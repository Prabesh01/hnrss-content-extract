<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 28 Jan 2026 17:53:24 +0000</lastBuildDate><item><title>Trayd (YC S23) is hiring senior engineers in NYC</title><link>https://news.ycombinator.com/item?id=46778846</link><description>&lt;doc fingerprint="b50dc15d5663cc54"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Trayd (YCS23) is building construction payroll + back office software. Payroll is one of the few domains where 99% accuracy is an F, so we obsess over correctness, reliability, and observability.&lt;/p&gt;
      &lt;p&gt;We recently hit a major growth milestone (57% month-over-month revenue growth, among other things), and we’re hiring senior engineers in NYC (on-site) to help scale the product and the systems behind it as volume ramps. You would be joining a small but mighty eng team, all based in NYC.&lt;/p&gt;
      &lt;p&gt;Stack: TypeScript, Node, Postgres, Prisma, React &amp;amp; React Native.&lt;/p&gt;
      &lt;p&gt;URL: https://www.ycombinator.com/companies/trayd/jobs/1Ez3l4b-sen...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46778846</guid><pubDate>Tue, 27 Jan 2026 12:00:22 +0000</pubDate></item><item><title>9 Mothers (YC X26, Defense Tech) Is Hiring</title><link>https://jobs.ashbyhq.com/9-mothers?utm_source=x8pZ4B3P3Q</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46779136</guid><pubDate>Tue, 27 Jan 2026 12:31:29 +0000</pubDate></item><item><title>Show HN: Build Web Automations via Demonstration</title><link>https://www.notte.cc/launch-week-i/demonstrate-mode</link><description>&lt;doc fingerprint="11ab23d28c35c2e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Launch Week I•Thursday, Dec 18 2025&lt;/p&gt;
    &lt;head rend="h1"&gt;Demonstrate Mode&lt;/head&gt;
    &lt;p&gt;Build your automation just by doing it manually once&lt;/p&gt;
    &lt;p&gt;Demonstrate Mode records your browser actions and generates automation code instantly. Perform any task manually once, and it captures every step and converts it into production-ready, editable code. No prompts, no syntax, just show your workflow and deploy instantly. Build now at https://console.notte.cc.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46779864</guid><pubDate>Tue, 27 Jan 2026 13:48:30 +0000</pubDate></item><item><title>430k-year-old well-preserved wooden tools are the oldest ever found</title><link>https://www.nytimes.com/2026/01/26/science/archaeology-neanderthals-tools.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46781530</guid><pubDate>Tue, 27 Jan 2026 15:46:29 +0000</pubDate></item><item><title>Artie (YC S23) Is Hiring a Founding Recruiter</title><link>https://www.ycombinator.com/companies/artie/jobs/MX163y2-founding-recruiter</link><description>&lt;doc fingerprint="a189c0db27d96528"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;About Artie&lt;/head&gt;
      &lt;p&gt;Artie is a real-time streaming platform that moves production data across systems in real-time, with zero maintenance. We make high-volume data replication simple, reliable, and scalable for engineering teams.&lt;/p&gt;
      &lt;p&gt;Our platform powers mission-critical use cases including fraud and risk monitoring, inventory visibility, customer-facing analytics, and AI workloads. Artie is built for engineers who care about performance, reliability, and operational simplicity — and we’re growing fast.&lt;/p&gt;
      &lt;p&gt;We’re trusted by teams like ClickUp, Substack, and Alloy, and backed by top-tier investors including Y Combinator, General Catalyst, Pathlight Ventures, and the founders of Dropbox and Mode.&lt;/p&gt;
      &lt;p&gt;We’re hiring our first in-house recruiter to own and build talent at Artie. This role is your chance to build our team from first principles.&lt;/p&gt;
      &lt;head rend="h3"&gt;About the Role&lt;/head&gt;
      &lt;p&gt;This is not a coordination role and not a “run the ATS” job.&lt;/p&gt;
      &lt;p&gt;You will be responsible for end-to-end recruiting across the company, with a focus on Engineering, Product, Operations, and Design (EPOD). You’ll partner directly with founders and hiring managers, define what “great” looks like for each role, and build the recruiting foundation we scale on top of.&lt;/p&gt;
      &lt;p&gt;You will also be the internal owner for our external recruiting partners — setting strategy, calibrating quality, and ensuring agencies complement our in-house motion.&lt;/p&gt;
      &lt;p&gt;If you view recruiting as a mix of sales, systems thinking, storytelling, and judgment, this role is for you.&lt;/p&gt;
      &lt;p&gt;This is a high-trust, high-ownership role, and you’ll have real influence over the shape, culture, and trajectory of the company.&lt;/p&gt;
      &lt;head rend="h3"&gt;What you’ll do&lt;/head&gt;
      &lt;p&gt;Own full-cycle recruiting across the company&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Run end-to-end hiring for Engineering, Product, Operations, and Design roles, and support other roles in GTM as needed&lt;/item&gt;
        &lt;item&gt;Partner with founders and hiring managers to: &lt;list rend="ul"&gt;&lt;item&gt;Define role scope, seniority, and success criteria&lt;/item&gt;&lt;item&gt;Calibrate on candidate quality and tradeoffs&lt;/item&gt;&lt;item&gt;Continuously refine interview loops and hiring signals&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Manage candidates through sourcing, screening, interviews, offers, and closing&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Be the engine for technical hiring&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Proactively source and engage senior technical talent (engineers, product, design) in a competitive market&lt;/item&gt;
        &lt;item&gt;Run outbound recruiting with creativity and rigor — LinkedIn, referrals, networks, events, cold outreach, and non-obvious channels&lt;/item&gt;
        &lt;item&gt;Confidently engage technical candidates and speak credibly about: &lt;list rend="ul"&gt;&lt;item&gt;Engineering culture and technical challenges&lt;/item&gt;&lt;item&gt;Artie’s product, architecture, and roadmap (with support from founders)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Sell Artie to candidates&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Craft and deliver a compelling narrative around: &lt;list rend="ul"&gt;&lt;item&gt;Why Artie exists&lt;/item&gt;&lt;item&gt;Why this team is special&lt;/item&gt;&lt;item&gt;Why this is a rare career opportunity&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Own candidate experience end-to-end - build a world-class recruiting process that delights candidates&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Build recruiting infrastructure from scratch&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Own and evolve our recruiting process, tools, and operating rhythm&lt;/item&gt;
        &lt;item&gt;Manage and improve our ATS and sourcing tools&lt;/item&gt;
        &lt;item&gt;Track and report on hiring progress, pipeline health, and bottlenecks&lt;/item&gt;
        &lt;item&gt;Continuously improve speed, quality, and candidate experience as we scale&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Manage external recruiting partners&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Act as the primary point of contact for agencies we work with&lt;/item&gt;
        &lt;item&gt;Set expectations, role briefs, and quality bars&lt;/item&gt;
        &lt;item&gt;Ensure agencies augment our hiring motion rather than define it&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What we’re looking for&lt;/head&gt;
      &lt;p&gt;Recruiting mastery in early-stage environments&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;4+ years of recruiting experience, with meaningful exposure to: &lt;list rend="ul"&gt;&lt;item&gt;Technical recruiting (engineering, product, design)&lt;/item&gt;&lt;item&gt;Early-stage startups (Series A–C preferred)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Experience owning full-cycle recruiting without large support teams&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Strong technical intuition&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You don’t need to code, but you can recruit engineers&lt;/item&gt;
        &lt;item&gt;Able to build credibility quickly with senior technical candidates&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Sales mindset&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You view recruiting as a sales and persuasion problem&lt;/item&gt;
        &lt;item&gt;Comfortable with outbound, rejection, and ambiguity&lt;/item&gt;
        &lt;item&gt;Strong written and verbal communicator&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Extreme ownership&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You take responsibility for outcomes, not just inputs&lt;/item&gt;
        &lt;item&gt;You proactively identify problems and fix them&lt;/item&gt;
        &lt;item&gt;You care deeply about quality and long-term team health&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Taste, judgment, and integrity&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You have a strong internal bar for talent&lt;/item&gt;
        &lt;item&gt;You know when to push, when to pause, and when to say no&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Logistics&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Willing to work in-person, 5 days/week at our SF office&lt;/item&gt;
        &lt;item&gt;Comfortable operating with speed, ambiguity, and very little structure&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What you’ll get&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Foundational impact: You will shape the team, culture, and hiring bar at Artie&lt;/item&gt;
        &lt;item&gt;Direct founder partnership: Work closely with the CEO/CTO and leadership team&lt;/item&gt;
        &lt;item&gt;End-to-end ownership: Strategy, execution, iteration — all yours&lt;/item&gt;
        &lt;item&gt;Growth path: Opportunity to grow into a Head of Talent / recruiting leader as we scale&lt;/item&gt;
        &lt;item&gt;High trust environment: You’ll be treated as a core operator, not a support function&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Compensation &amp;amp; Benefits&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Competitive salary (commensurate with experience) - our philosophy is P75-90 for base salary and P90+ for equity compared to benchmarks&lt;/item&gt;
        &lt;item&gt;Healthcare, 401(k) matching, unlimited PTO&lt;/item&gt;
        &lt;item&gt;Lunch &amp;amp; dinner provided&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46782763</guid><pubDate>Tue, 27 Jan 2026 17:01:37 +0000</pubDate></item><item><title>Prism</title><link>https://openai.com/index/introducing-prism</link><description>&lt;doc fingerprint="ab5a723abbf162df"&gt;
  &lt;main&gt;
    &lt;p&gt;Science shapes nearly every part of daily life—from the medicines we rely on, to the energy that powers our homes, to the systems that keep us safe. But the pace of scientific progress is still constrained by how research is done day to day. While AI has advanced rapidly, much of the everyday work of science still relies on tools that haven’t fundamentally changed in decades.&lt;/p&gt;
    &lt;p&gt;We’re introducing Prism, a free, AI-native workspace for scientists to write and collaborate on research, powered by GPT‑5.2. Prism offers unlimited projects and collaborators and is available today to anyone with a ChatGPT personal account.&lt;/p&gt;
    &lt;p&gt;Prism will be available soon to organizations using ChatGPT Business, Enterprise, and Education plans.&lt;/p&gt;
    &lt;p&gt;Over the past year, we’ve begun to see AI accelerate scientific work across domains. Advanced reasoning systems like GPT‑5 are helping push the frontiers of mathematics, accelerating the analysis of human immune-cell experiments, and speeding up experimental iteration in molecular biology.&lt;/p&gt;
    &lt;p&gt;We’re still early, but it’s clear that AI will play a meaningful role in how science advances.&lt;/p&gt;
    &lt;p&gt;At the same time, much of the everyday work of research—drafting papers, revising arguments, managing equations and citations, and coordinating with collaborators —remains fragmented across disconnected tools. Researchers often move between editors, PDFs, LaTeX compilers, reference managers, and separate chat interfaces, losing context and interrupting focus.&lt;/p&gt;
    &lt;p&gt;Prism is our first step toward addressing this fragmentation.&lt;/p&gt;
    &lt;p&gt;Prism is a free workspace for scientific writing and collaboration, with GPT‑5.2—our most advanced model for mathematical and scientific reasoning—integrated directly into the workflow.&lt;/p&gt;
    &lt;p&gt;It brings drafting, revision, collaboration, and preparation for publication into a single, cloud-based, LaTeX-native workspace. Rather than operating as a separate tool alongside the writing process, GPT‑5.2 works within the project itself—with access to the structure of the paper, equations, references, and surrounding context.&lt;/p&gt;
    &lt;p&gt;Prism builds on the foundation of Crixet, a cloud-based LaTeX platform that OpenAI acquired and has since evolved into Prism as a unified product. This allowed us to start with a strong base of a mature writing and collaboration environment, and integrate AI in a way that fits naturally into scientific workflows.&lt;/p&gt;
    &lt;p&gt;With Prism, researchers can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chat with GPT‑5.2 Thinking, to explore ideas, test hypotheses, and reason through complex scientific problems in context&lt;/item&gt;
      &lt;item&gt;Draft and revise papers with the full document as context, including surrounding text, equations, citations, figures, and overall structure&lt;/item&gt;
      &lt;item&gt;Search for and incorporate relevant literature (for example, from arXiv) in the context of the current manuscript, and revise text in light of newly identified related work&lt;/item&gt;
      &lt;item&gt;Create, refactor, and reason over equations, citations, and figures, with AI that understands how those elements relate across the paper&lt;/item&gt;
      &lt;item&gt;Turn whiteboard equations or diagrams directly into LaTeX, saving hours of time manipulating graphics pixel-by-pixel&lt;/item&gt;
      &lt;item&gt;Collaborate with co-authors, students, and advisors in real time, with edits, comments, and revisions reflected immediately&lt;/item&gt;
      &lt;item&gt;Make direct, in-place changes to the document when requested, without copying content between separate editors or chat tools&lt;/item&gt;
      &lt;item&gt;Use optional voice-based editing to make simple changes without interrupting writing or review&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scientific research is inherently collaborative. Papers are shaped over time by co-authors, students, advisors, and reviewers, often across institutions and geographies.&lt;/p&gt;
    &lt;p&gt;Prism supports unlimited collaborators, allowing research teams to work together without seat limits or access barriers. Because it’s cloud-based, there’s no local LaTeX installation or environment management required, making it easier for teams to collaborate in a shared workspace.&lt;/p&gt;
    &lt;p&gt;By reducing version conflicts, manual merging, and mechanical overhead, Prism helps teams spend less time managing files and more time engaging with the substance of their work.&lt;/p&gt;
    &lt;p&gt;Just as importantly, Prism is designed to expand access.&lt;/p&gt;
    &lt;p&gt;Prism is free to use, and anyone with a ChatGPT account can start writing immediately. There are no subscriptions or seat limits. By making high-quality scientific tools easier to adopt and broadly available, we hope to enable more researchers—across institutions, disciplines, and career stages—to participate fully in the scientific process.&lt;/p&gt;
    &lt;p&gt;More powerful AI features will be made available through paid ChatGPT plans over time.&lt;/p&gt;
    &lt;p&gt;In 2025, AI changed software development forever. In 2026, we expect a comparable shift in science, as AI begins to meaningfully accelerate discovery in several ways, one of which is reducing friction in day-to-day research work. Prism is an early step toward that future.&lt;/p&gt;
    &lt;p&gt;We’re excited to learn from researchers using Prism today and to continue building toward tools that help science move faster—together. Try Prism for free today at prism.openai.com(opens in a new window).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46783752</guid><pubDate>Tue, 27 Jan 2026 18:03:10 +0000</pubDate></item><item><title>Hypercubic (YC F25) Is Hiring a Founding SWE and COBOL Engineer</title><link>https://www.ycombinator.com/companies/hypercubic/jobs</link><description>&lt;doc fingerprint="3564976ca045d865"&gt;
  &lt;main&gt;
    &lt;p&gt;AI to maintain and modernize COBOL.&lt;/p&gt;
    &lt;p&gt;Hypercubic is an AI-native maintenance and modernization platform for COBOL and mainframes.&lt;/p&gt;
    &lt;p&gt;We help enterprises understand and preserve their mission-critical legacy systems. About 70% of the Fortune 500 companies still rely on them to run their core business applications in banking, insurance, telecom, airlines, retail, and more.&lt;/p&gt;
    &lt;p&gt;These systems, originally built in the 1960s–90s, still power trillions in global infrastructure today but have become increasingly opaque as original developers retire or leave the workforce.&lt;/p&gt;
    &lt;p&gt;We're laying the foundation to autonomously maintain and modernize these legacy systems to future-proof the backbone of the global economy.&lt;/p&gt;
    &lt;p&gt;Learn more at hypercubic.ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46784491</guid><pubDate>Tue, 27 Jan 2026 18:50:50 +0000</pubDate></item><item><title>A verification layer for browser agents: Amazon case study</title><link>https://sentienceapi.com/blog/verification-layer-amazon-case-study</link><description>&lt;doc fingerprint="3f4442764a60997b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A verification layer for browser agents: Amazon case study&lt;/head&gt;
    &lt;p&gt;How structured snapshots + Jest-style assertions make small local models reliable.&lt;/p&gt;
    &lt;p&gt;This post is a technical report on four runs of the same Amazon shopping flow. The purpose is to isolate one claim: reliability comes from verification, not from giving the model more pixels or more parameters.&lt;/p&gt;
    &lt;p&gt;Sentience is used here as a verification layer: each step is gated by explicit assertions over structured snapshots. This makes it feasible to use small local models as executors, while reserving larger models for planning (reasoning) when needed. No vision models are required for the core loop in the local runs discussed below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Key findings&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Finding&lt;/cell&gt;
        &lt;cell role="head"&gt;Evidence (from logs / report)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A fully autonomous run can complete with local models when verification gates every step.&lt;/cell&gt;
        &lt;cell&gt;Demo 3 re-run: &lt;code&gt;Steps passed: 7/7&lt;/code&gt; and &lt;code&gt;success: True&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Token efficiency can be engineered by interface design (structure + filtering), not by model choice.&lt;/cell&gt;
        &lt;cell&gt;Demo 0 report: estimated ~35,000 → 19,956 tokens (~43% reduction)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Verification &amp;gt; intelligence is the practical lesson.&lt;/cell&gt;
        &lt;cell&gt;Planner drift is surfaced as explicit FAIL/mismatch rather than silent progress&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Key datapoints:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Demo 0 (cloud baseline)&lt;/cell&gt;
        &lt;cell role="head"&gt;Demo 3 (local autonomy)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Success&lt;/cell&gt;
        &lt;cell&gt;1/1 run&lt;/cell&gt;
        &lt;cell&gt;7/7 steps (re-run)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Duration&lt;/cell&gt;
        &lt;cell&gt;~60,000ms&lt;/cell&gt;
        &lt;cell&gt;405,740ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tokens&lt;/cell&gt;
        &lt;cell&gt;19,956 (after filtering)&lt;/cell&gt;
        &lt;cell&gt;11,114&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Task (constant across runs): Amazon → Search “thinkpad” → Click first product → Add to cart → Proceed to checkout&lt;/p&gt;
    &lt;head rend="h2"&gt;First principles: structure &amp;gt; pixels&lt;/head&gt;
    &lt;p&gt;Screenshot-based agents use pixels as the control plane. That often fails in predictable ways: ambiguous click targets, undetected navigation failures, and “progress” without state change.&lt;/p&gt;
    &lt;p&gt;The alternative is to treat the page as a structured snapshot (roles, text, geometry, and a small amount of salience) and then require explicit pass/fail verification after each action. This is the “Jest for agents” idea: a step does not succeed because the model says it did; it succeeds because an assertion over browser state passes.&lt;/p&gt;
    &lt;head rend="h2"&gt;The “impossible benchmark”&lt;/head&gt;
    &lt;p&gt;The target configuration is a strong planner paired with a small, local executor, still achieving reliable end-to-end behavior. Concretely: DeepSeek-R1 (planner) + a ~3B-class local executor, with Sentience providing verification gates between steps.&lt;/p&gt;
    &lt;p&gt;Note on attribution: the run logs included in this post report outcomes (duration/tokens/steps) but do not consistently print model identifiers. Where specific model names are mentioned below, they are explicitly labeled as run configuration.&lt;/p&gt;
    &lt;p&gt;Why this is a useful benchmark:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The executor is intentionally “dumb”: it only needs to choose DOM actions (CLICK/TYPE) against a compact representation.&lt;/item&gt;
      &lt;item&gt;Reliability comes from the verification layer: each action is followed by snapshot + assertions that gate success and drive bounded retries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, the point is not that small models are magically capable; it’s that verification makes capability usable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup and dependencies&lt;/head&gt;
    &lt;p&gt;These demos use the Sentience Python SDK, Playwright for browser control, and local LLMs for planning/execution. The minimal install sequence is straight from the SDK README:&lt;/p&gt;
    &lt;code&gt;# Install from PyPI
pip install sentienceapi

# Install Playwright browsers (required)
playwright install chromium

# For local LLMs (optional)
pip install transformers torch  # For local LLMs&lt;/code&gt;
    &lt;p&gt;Video artifacts are generated from screenshots; if &lt;code&gt;ffmpeg&lt;/code&gt; is available, the run assembles a short summary clip (the logs show creation even when &lt;code&gt;ffmpeg&lt;/code&gt; errors occur in some runs).&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture: the 3-model stack (planner, executor, verifier)&lt;/head&gt;
    &lt;p&gt;Sentience separates planning from execution, and inserts verification in the middle:&lt;/p&gt;
    &lt;code&gt;Planner LLM → JSON Plan (steps + required verification)
          ↓
     AgentRuntime
(snapshot + verify + trace)
          ↓
   Executor LLM (CLICK/TYPE)
          ↓
 AsyncSentienceBrowser + Extension
          ↓
Trace pipeline → Sentience Studio&lt;/code&gt;
    &lt;p&gt;The key is the runtime: every action is wrapped in a snapshot + verification cycle that produces pass/fail evidence. Verification is inline runtime gating (it determines whether the step succeeds), not post-hoc analysis.&lt;/p&gt;
    &lt;p&gt;Clarifying terms used below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Planner (reasoning): produces a structured plan (steps + required verifications).&lt;/item&gt;
      &lt;item&gt;Executor (action): selects concrete DOM actions (CLICK/TYPE) against the current snapshot.&lt;/item&gt;
      &lt;item&gt;Verifier (assertions): evaluates explicit assertions over snapshots and gates step/task success (with deterministic overrides when intent is unambiguous).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Amazon shopping flow snapshot diff status between 2 steps (shows whether the LLM action succeeded)&lt;/p&gt;
    &lt;p&gt;Amazon shopping flow snapshot heatmap by importance of top elements&lt;/p&gt;
    &lt;head rend="h2"&gt;The verification layer (SDK code you can copy)&lt;/head&gt;
    &lt;p&gt;At the runtime level, assertions are first-class, recorded as events, and can gate success. From &lt;code&gt;AgentRuntime.assert_()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;def assert_(
  self,
  predicate: Predicate,
  label: str,
  required: bool = False,
) -&amp;gt; bool:
  """
  Evaluate an assertion against current snapshot state.

  The assertion result is:
  1. Accumulated for inclusion in step_end.data.verify.signals.assertions
  2. Emitted as a dedicated 'verification' event for Studio timeline
  """
  outcome = predicate(self._ctx())
  self._record_outcome(
      outcome=outcome,
      label=label,
      required=required,
      kind="assert",
      record_in_step=True,
  )
  if required and not outcome.passed:
      self._persist_failure_artifacts(reason=f"assert_failed:{label}")
  return outcome.passed&lt;/code&gt;
    &lt;p&gt;To support async UI, the runtime exposes fluent retry logic:&lt;/p&gt;
    &lt;code&gt;def check(self, predicate: Predicate, label: str, required: bool = False) -&amp;gt; AssertionHandle:
  """
  Create an AssertionHandle for fluent .once() / .eventually() usage.

  This does NOT evaluate the predicate immediately.
  """
  return AssertionHandle(runtime=self, predicate=predicate, label=label, required=required)&lt;/code&gt;
    &lt;p&gt;Predicates are explicit, composable, and operate over semantic snapshots. For example:&lt;/p&gt;
    &lt;code&gt;def url_contains(substring: str) -&amp;gt; Predicate:
  def _pred(ctx: AssertContext) -&amp;gt; AssertOutcome:
      url = ctx.url or ""
      ok = substring in url
      return AssertOutcome(
          passed=ok,
          reason="" if ok else f"url does not contain: {substring}",
          details={"substring": substring, "url": url[:200]},
      )
  return _pred&lt;/code&gt;
    &lt;code&gt;def exists(selector: str) -&amp;gt; Predicate:
  def _pred(ctx: AssertContext) -&amp;gt; AssertOutcome:
      snap = ctx.snapshot
      if snap is None:
          return AssertOutcome(
              passed=False,
              reason="no snapshot available",
              details={"selector": selector, "reason_code": "no_snapshot"},
          )
      from .query import query
      matches = query(snap, selector)
      ok = len(matches) &amp;gt; 0
      return AssertOutcome(
          passed=ok,
          reason="" if ok else f"no elements matched selector: {selector}",
          details={
              "selector": selector,
              "matched": len(matches),
              "reason_code": "ok" if ok else "no_match",
          },
      )
  return _pred&lt;/code&gt;
    &lt;p&gt;This is why the system behaves like a test harness: assertions are deterministic, and failures produce artifacts instead of silent drift.&lt;/p&gt;
    &lt;head rend="h2"&gt;A tiny comparison table (control plane and failure mode)&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;Control Plane&lt;/cell&gt;
        &lt;cell role="head"&gt;Failure Mode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vision agents&lt;/cell&gt;
        &lt;cell&gt;Screenshots&lt;/cell&gt;
        &lt;cell&gt;Silent retries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Raw DOM&lt;/cell&gt;
        &lt;cell&gt;HTML text&lt;/cell&gt;
        &lt;cell&gt;Hallucinated selectors&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Sentience&lt;/cell&gt;
        &lt;cell&gt;Structured snapshot + assertions&lt;/cell&gt;
        &lt;cell&gt;Explicit FAIL with artifacts&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Token efficiency is not a side effect here. The structure-first snapshot plus role filtering reduced prompt volume by ~43% in the cloud LLM baseline (Demo 0), while keeping the same deterministic verification loop.&lt;/p&gt;
    &lt;head rend="h2"&gt;The four demos (same task, different autonomy)&lt;/head&gt;
    &lt;head rend="h3"&gt;Demo 3 — Local autonomy (planner + executor) with verification gates&lt;/head&gt;
    &lt;head rend="h4"&gt;See Code&lt;/head&gt;
    &lt;p&gt;Inputs&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Planner (reasoning): DeepSeek-R1 family (run configuration; see note above)&lt;/item&gt;
      &lt;item&gt;Executor (action): local Qwen family (run configuration; the “impossible benchmark” target is ~3B-class)&lt;/item&gt;
      &lt;item&gt;Verifier (assertions): Sentience runtime gates (structured snapshots + explicit assertions + deterministic overrides)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Verification&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each step is gated by explicit assertions over snapshots (URL predicates, element existence, etc.)&lt;/item&gt;
      &lt;item&gt;Deterministic overrides are applied when intent is unambiguous (e.g., drawer dismissal)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Outcome The re-run completed end-to-end:&lt;/p&gt;
    &lt;code&gt;=== Run Summary ===
success: True
duration_ms: 405740
tokens_total: 11114
Steps passed: 7/7&lt;/code&gt;
    &lt;p&gt;The practical point is not that the executor “understands Amazon.” It selects actions inside a loop that proves outcomes (via assertions) and forces the correct branch when intent is unambiguous (via deterministic overrides).&lt;/p&gt;
    &lt;p&gt;Demo 3 is the latest result in this case study - the one we care about most (local autonomy with verification). To give the full context, the next sections go back to the earliest baseline (Demo 0) and then walk forward through Demo 1 and Demo 2, showing the evolution: how structure-first snapshots, verification gates, and element filtering made the system cheaper and more reliable before we reached full autonomy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Demo 0 — Cloud LLM + Sentience SDK (structured JSON)&lt;/head&gt;
    &lt;head rend="h4"&gt;See Code&lt;/head&gt;
    &lt;p&gt;Earlier baseline (Dec 2025 report). This run used a cloud model (GLM-4.6) with Sentience’s structured snapshot pipeline. The key contribution was reducing the problem to structured elements and gating each step with verification.&lt;/p&gt;
    &lt;p&gt;Token reduction from element filtering (as reported):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Before&lt;/cell&gt;
        &lt;cell role="head"&gt;After&lt;/cell&gt;
        &lt;cell role="head"&gt;Reduction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Prompt tokens (estimated)&lt;/cell&gt;
        &lt;cell&gt;~35,000&lt;/cell&gt;
        &lt;cell&gt;19,956&lt;/cell&gt;
        &lt;cell&gt;~43%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Demo 1 — Human steps + Local Qwen 2.5-3B executor&lt;/head&gt;
    &lt;head rend="h4"&gt;See Code&lt;/head&gt;
    &lt;p&gt;Inputs&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Planner: human-authored steps&lt;/item&gt;
      &lt;item&gt;Executor: Local Qwen 2.5-3B (run configuration)&lt;/item&gt;
      &lt;item&gt;Verifier: Sentience assertions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Outcome This isolates the verification layer and structured snapshot format with a small local executor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Demo 2 — Local Qwen 2.5-7B planner + Qwen 2.5-3B executor (autonomous)&lt;/head&gt;
    &lt;head rend="h4"&gt;See Code&lt;/head&gt;
    &lt;p&gt;Inputs&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Planner: Local Qwen 2.5-7B (run configuration)&lt;/item&gt;
      &lt;item&gt;Executor: Qwen 2.5-3B (run configuration)&lt;/item&gt;
      &lt;item&gt;Verifier: Sentience assertions + overrides&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Verification When the planner’s verification target becomes brittle under UI ambiguity, the runtime surfaces the mismatch via explicit verification failures (rather than silently proceeding).&lt;/p&gt;
    &lt;p&gt;Context: cross-demo outcome summary. The table below is a compact comparison of the four demos (cloud baseline → human-plan control → autonomous planning → local autonomy), highlighting duration, token usage, and step completion.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Demo&lt;/cell&gt;
        &lt;cell role="head"&gt;Planner / Executor&lt;/cell&gt;
        &lt;cell role="head"&gt;Duration&lt;/cell&gt;
        &lt;cell role="head"&gt;Tokens&lt;/cell&gt;
        &lt;cell role="head"&gt;Steps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Cloud LLM (run config) + Sentience SDK&lt;/cell&gt;
        &lt;cell&gt;~60,000ms&lt;/cell&gt;
        &lt;cell&gt;19,956&lt;/cell&gt;
        &lt;cell&gt;1/1 run&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Human plan + Local Qwen 2.5-3B (run config)&lt;/cell&gt;
        &lt;cell&gt;207,569ms&lt;/cell&gt;
        &lt;cell&gt;5,555&lt;/cell&gt;
        &lt;cell&gt;9/9 steps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;Local Qwen 2.5-7B + Qwen 2.5-3B (run config)&lt;/cell&gt;
        &lt;cell&gt;435,446ms&lt;/cell&gt;
        &lt;cell&gt;13,128&lt;/cell&gt;
        &lt;cell&gt;7/8 steps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;DeepSeek-R1 family + local Qwen family (run config)&lt;/cell&gt;
        &lt;cell&gt;Run A: 496,863ms / Run B: 405,740ms&lt;/cell&gt;
        &lt;cell&gt;12,922 / 11,114&lt;/cell&gt;
        &lt;cell&gt;7/8 → 7/7 steps&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;What the logs show (concrete evidence)&lt;/head&gt;
    &lt;head rend="h3"&gt;Deterministic overrides (first product)&lt;/head&gt;
    &lt;p&gt;The executor can be overruled when the runtime has a safer deterministic choice:&lt;/p&gt;
    &lt;code&gt;[fallback] first_product_link preselect -&amp;gt; CLICK(1022)
Executor decision: {"action": "click", "id": 884, "raw": "CLICK(884)"}
[override] first_product_link -&amp;gt; CLICK(1022)&lt;/code&gt;
    &lt;p&gt;This is not a model preference. It is an explicit guardrail that preserves the “first result” intent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Drawer handling (add-on upsell)&lt;/head&gt;
    &lt;p&gt;When Amazon opens the add-on drawer, the runtime dismisses it deterministically:&lt;/p&gt;
    &lt;code&gt;[fallback] add_to_cart drawer detected -&amp;gt; CLICK(6926)
Executor decision: {"action": "click", "id": 1214, "raw": "CLICK(1214)"}
[override] add_to_cart -&amp;gt; CLICK(6926)&lt;/code&gt;
    &lt;p&gt;Result:&lt;/p&gt;
    &lt;code&gt;result: PASS | add_to_cart_verified_after_drawer&lt;/code&gt;
    &lt;head rend="h3"&gt;Planner drift (brittle selector)&lt;/head&gt;
    &lt;p&gt;In the autonomous planner run, the verification target briefly drifted into a brittle selector; the runtime still enforces pass/fail deterministically, which is why the mismatch is surfaced rather than glossed over:&lt;/p&gt;
    &lt;code&gt;"intent": "checkout_button",
"verify": [
{
  "predicate": "exists",
  "args": [
    "data-test-id=nav-x-site-nav-button-checkout"
  ]
}
]&lt;/code&gt;
    &lt;p&gt;The system caught the mismatch with assertions, but it highlights why verification must be strong before model scale pays off.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run summaries (from logs)&lt;/head&gt;
    &lt;p&gt;Demo 0 (Cloud LLM + Sentience SDK; report excerpt):&lt;/p&gt;
    &lt;code&gt;success: True
duration_ms: ~60000
tokens_total: 19956
notes: token reduction ~43% (estimated ~35,000 → 19,956) via element filtering&lt;/code&gt;
    &lt;p&gt;Demo 1 (Human steps + Qwen 2.5-3B executor):&lt;/p&gt;
    &lt;code&gt;=== Run Summary ===
success: True
duration_ms: 207569
tokens_total: 5555
Steps passed: 9/9&lt;/code&gt;
    &lt;p&gt;Demo 2 (Qwen 2.5-7B planner + Qwen 2.5-3B executor):&lt;/p&gt;
    &lt;code&gt;=== Run Summary ===
success: True
duration_ms: 435446
tokens_total: 13128
Steps passed: 7/8&lt;/code&gt;
    &lt;p&gt;Demo 3 Run A (DeepSeek-R1-Distill-Qwen-14B planner + Qwen 2.5-7B executor): (Model identifiers are run configuration; the excerpted logs primarily report outcome metrics.)&lt;/p&gt;
    &lt;code&gt;=== Run Summary ===
success: True
duration_ms: 496863
tokens_total: 12922
Steps passed: 7/8&lt;/code&gt;
    &lt;p&gt;Demo 3 Re-run:&lt;/p&gt;
    &lt;code&gt;=== Run Summary ===
success: True
duration_ms: 405740
tokens_total: 11114
Steps passed: 7/7&lt;/code&gt;
    &lt;p&gt;The run also emits a trace upload confirmation:&lt;/p&gt;
    &lt;code&gt;✅ [Sentience] Trace uploaded successfully&lt;/code&gt;
    &lt;p&gt;Some runs use MLX backends (Apple Silicon). MLX does not expose token usage metrics, so token totals can be directional; plan stability and retry counts are the more reliable signal in those cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;Observability and artifacts&lt;/head&gt;
    &lt;p&gt;Each step produces structured traces, snapshots, and artifacts. The run automatically creates a video summary from screenshots:&lt;/p&gt;
    &lt;code&gt;Creating video from screenshots in .../screenshots/20260120_115426...
Found 7 screenshots
...
✅ Video created: .../demo.mp4
✅ [Sentience] Trace uploaded successfully&lt;/code&gt;
    &lt;p&gt;These traces power Sentience Studio: diff_status, evidence, and a timeline of verification events that explain why a step passed or failed.&lt;/p&gt;
    &lt;p&gt;See the screenshot below for a step with failed assertion due to URL not being changed after a click event:&lt;/p&gt;
    &lt;p&gt;Sentience Studio provides a time line to walk through traces for each run, making it easy to debug and understand agent behavior.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways (first principles)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verification &amp;gt; intelligence for reliability. A modest model operating inside strict, deterministic tests will beat a stronger model operating without tests, because failures are detected and bounded.&lt;/item&gt;
      &lt;item&gt;Vision is a fallback, not the control plane. Structure-first snapshots and assertions carry the flow; vision is used when necessary, not by default.&lt;/item&gt;
      &lt;item&gt;Structure + verification makes small models viable. The executor is constrained to checkable actions; retries are bounded; failures produce artifacts.&lt;/item&gt;
      &lt;item&gt;Local models become a rational default. Lower token usage enables better economics and improves privacy/deployment control.&lt;/item&gt;
      &lt;item&gt;Larger planners help after verification is in place. They reduce plan drift and replan churn; they do not replace verification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Sentience, “verification” means explicit assertions over structured snapshots, with deterministic overrides when intent is unambiguous.&lt;/p&gt;
    &lt;p&gt;This is the Sentience approach: treat the browser as structured data, assert outcomes explicitly, and keep vision as a fallback. The result is a deterministic core that makes local LLMs practical without sacrificing reliability.&lt;/p&gt;
    &lt;p&gt;This approach is designed for teams that care about cost, data privacy/compliance, reproducibility, and debuggability - not for demo-only agents.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46790127</guid><pubDate>Wed, 28 Jan 2026 02:08:14 +0000</pubDate></item><item><title>Rust at Scale: An Added Layer of Security for WhatsApp</title><link>https://engineering.fb.com/2026/01/27/security/rust-at-scale-security-whatsapp/</link><description>&lt;doc fingerprint="8e10dda3ac31549d"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WhatsApp has adopted and rolled out a new layer of security for users – built with Rust – as part of its effort to harden defenses against malware threats.&lt;/item&gt;
      &lt;item&gt;WhatsApp’s experience creating and distributing our media consistency library in Rust to billions of devices and browsers proves Rust is production ready at a global scale.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our Media Handling Strategy&lt;/head&gt;
    &lt;p&gt;WhatsApp provides default end-to-end encryption for over 3 billion people to message securely each and every day. Online security is an adversarial space, and to continue ensuring users can keep messaging securely, we’re constantly adapting and evolving our strategy against cyber-security threats – all while supporting the WhatsApp infrastructure to help people connect.&lt;/p&gt;
    &lt;p&gt;For example, WhatsApp, like many other applications, allows users to share media and other types of documents. WhatsApp helps protect users by warning about dangerous attachments like APKs, yet rare and sophisticated malware could be hidden within a seemingly benign file like an image or video. These maliciously crafted files might target unpatched vulnerabilities in the operating system, libraries distributed by the operating system, or the application itself.&lt;/p&gt;
    &lt;p&gt;To help protect against such potential threads, WhatsApp is increasingly using the Rust programming language, including in our media sharing functionality. Rust is a memory safe language offering numerous security benefits. We believe that this is the largest rollout globally of any library written in Rust.&lt;/p&gt;
    &lt;p&gt;To help explain why and how we rolled this out, we should first look back at a key OS-level vulnerability that sent an important signal to WhatsApp around hardening media-sharing defenses.&lt;/p&gt;
    &lt;head rend="h2"&gt;2015 Android Vulnerability: A Wake-up Call for Media File Protections&lt;/head&gt;
    &lt;p&gt;In 2015, Android devices, and the applications that ran on them, became vulnerable to the “Stagefright” vulnerability. The bug lay in the processing of media files by operating system-provided libraries, so WhatsApp and other applications could not patch the underlying vulnerability. Because it could often take months for people to update to the latest version of their software, we set out to find solutions that would keep WhatsApp users safe, even in the event of an operating system vulnerability.&lt;/p&gt;
    &lt;p&gt;At that time, we realized that a cross-platform C++ library already developed by WhatsApp to send and consistently format MP4 files (called “wamedia”) could be modified to detect files which do not adhere to the MP4 standard and might trigger bugs in a vulnerable OS library on the receiver side – hence putting a target’s security at risk. We rolled out this check and were able to protect WhatsApp users from the Stagefright vulnerability much more rapidly than by depending on users to update the OS itself.&lt;/p&gt;
    &lt;p&gt;But because media checks run automatically on download and process untrusted inputs, we identified early on that wamedia was a prime candidate for using a memory safe language.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Solution: Rust at Scale&lt;/head&gt;
    &lt;p&gt;Rather than an incremental rewrite, we developed the Rust version of wamedia in parallel with the original C++ version. We used differential fuzzing and extensive integration and unit tests to ensure compatibility between the two implementations.&lt;/p&gt;
    &lt;p&gt;Two major hurdles were the initial binary size increase due to bringing in the Rust standard library and the build system support required for the diverse platforms supported by WhatsApp. WhatsApp made a long-term bet to build that support. In the end, we replaced 160,000 lines of C++ (excluding tests) with 90,000 lines of Rust (including tests). The Rust version showed performance and runtime memory usage advantages over the C++. Given this success, Rust was fully rolled out to all WhatsApp users and many platforms: Android, iOS, Mac, Web, Wearables, and more. With this positive evidence in hand, memory safe languages will play an ever increasing part in WhatsApp’s overall approach to application and user security.&lt;/p&gt;
    &lt;p&gt;Over time, we’ve added more checks for non-conformant structures within certain file types to help protect downstream libraries from parser differential exploit attempts. Additionally, we check higher risk file types, even if structurally conformant, for risk indicators. For instance, PDFs are often a vehicle for malware, and more specifically, the presence of embedded files and scripting elements within a PDF further raise risks. We also detect when one file type masquerades as another, through a spoofed extension or MIME type. Finally, we uniformly flag known dangerous file types, such as executables or applications, for special handling in the application UX. Altogether, we call this ensemble of checks “Kaleidoscope.” This system protects people on WhatsApp from potentially malicious unofficial clients and attachments. Although format checks will not stop every attack, this layer of defense helps mitigate many of them.&lt;/p&gt;
    &lt;p&gt;Each month, these libraries are distributed to billions of phones, laptops, desktops, watches, and browsers running on multiple operating systems for people on WhatsApp, Messenger, and Instagram. This is the largest ever deployment of Rust code to a diverse set of end-user platforms and products that we are aware of. Our experience speaks to the production-readiness and unique value proposition of Rust on the client-side.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Rust Fits In To WhatsApp’s Approach to App Security&lt;/head&gt;
    &lt;p&gt;This is just one example of WhatsApp’s many investments in security. It’s why we built default end-to-end encryption for personal messages and calls, offer end-to-end encrypted backups, and use key transparency technology to verify a secure connection, provide additional calling protections, and more.&lt;/p&gt;
    &lt;p&gt;WhatsApp has a strong track record of being loud when we find issues and working to hold bad actors accountable. For example, WhatsApp reports CVEs for important issues we find in our applications, even if we do not find evidence of exploitation. We do this to give people on WhatsApp the best chance of protecting themselves by seeing a security advisory and updating quickly.&lt;/p&gt;
    &lt;p&gt;To ensure application security, we first must identify and quantify the sources of risk. We do this through internal and external audits like NCC Group’s public assessment of WhatsApp’s end-to-end encrypted backups, fuzzing, static analysis, supply chain management, and automated attack surface analysis. We also recently expanded our Bug Bounty program to introduce the WhatsApp Research Proxy – a tool that makes research into WhatsApp’s network protocol more effective.&lt;/p&gt;
    &lt;p&gt;Next, we reduce the identified risk. Like many others in the industry, we found that the majority of the high severity vulnerabilities we published were due to memory safety issues in code written in the C and C++ programming languages. To combat this we invest in three parallel strategies:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Design the product to minimize unnecessary attack surface exposure.&lt;/item&gt;
      &lt;item&gt;Invest in security assurance for the remaining C and C++ code.&lt;/item&gt;
      &lt;item&gt;Default the choice of memory safe languages, and not C and C++, for new code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;WhatsApp has added protections like CFI, hardened memory allocators, safer buffer handling APIs, and more. C and C++ developers have specialized security training, development guidelines, and automated security analysis on their changes. We also have strict SLAs for fixing issues uncovered by the risk identification process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Accelerating Rust Adoption to Enhance Security&lt;/head&gt;
    &lt;p&gt;Rust enabled WhatsApp’s security team to develop a secure, high performance, cross-platform library to ensure media shared on the platform is consistent and safe across devices. This is an important step forward in adding additional security behind the scenes for users and part of our ongoing defense-in-depth approach. Security teams at WhatsApp and Meta are highlighting opportunities for high impact adoption of Rust to interested teams, and we anticipate accelerating adoption of Rust over the coming years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46791742</guid><pubDate>Wed, 28 Jan 2026 06:21:07 +0000</pubDate></item><item><title>Make.ts</title><link>https://matklad.github.io/2026/01/27/make-ts.html</link><description>&lt;doc fingerprint="76c2d8f72f2572bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;make.ts&lt;/head&gt;
    &lt;p&gt;Up Enter Up Up Enter Up Up Up Enter&lt;/p&gt;
    &lt;p&gt; Sounds familiar? This is how I historically have been running benchmarks and other experiments requiring a repeated sequence of commands — type them manually once, then rely on shell history (and maybe some terminal splits) for reproduction. These past few years I’ve arrived at a much better workflow pattern — &lt;code&gt;make.ts&lt;/code&gt;.
          I was forced to adapt it once I started working with multiprocess
          applications, where manually entering commands is borderline
          infeasible. In retrospect, I should have adapted the workflow years
          earlier.
        &lt;/p&gt;
    &lt;head rend="h2"&gt;The Pattern&lt;/head&gt;
    &lt;p&gt; Use a (gitignored) file for interactive scripting. Instead of entering a command directly into the terminal, write it to a file first, and then run the file. For me, I type stuff into &lt;code&gt;make.ts&lt;/code&gt; and then run &lt;code&gt;./make.ts&lt;/code&gt; in my terminal
            (Ok, I need one Up Enter for that).
          &lt;/p&gt;
    &lt;p&gt;I want to be clear here, I am not advocating writing “proper” scripts, just capturing your interactive, ad-hoc command to a persistent file. Of course any command that you want to execute repeatedly belongs to the build system. The surprising thing is that even more complex one-off commands benefit from running through file, because it will take you several tries to get them right!&lt;/p&gt;
    &lt;p&gt;There are many benefits relative to Up Up Up workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real commands tend to get large, and it is so much nicer to use a real 2D text editor rather than shell’s line editor.&lt;/item&gt;
      &lt;item&gt; If you need more than one command, you can write several commands, and still run them all with a single key (before &lt;code&gt;make.ts&lt;/code&gt;, I was prone to constructing rather horrific &amp;amp;&amp;amp; conjuncts for this reason).&lt;/item&gt;
      &lt;item&gt;With a sequence of command outlined, you nudge yourself towards incrementally improving them, making them idempotent, and otherwise investing into your own workflow for the next few minutes, without falling into the YAGNI pit from the outset.&lt;/item&gt;
      &lt;item&gt;At some point you might realize after, say, running a series of ad-hoc benchmarks interactively, that you’d rather write a proper script which executes a collection of benchmarks with varying parameters. With the file approach, you already have the meat of the script implemented, and you only need to wrap in a couple of fors and ifs.&lt;/item&gt;
      &lt;item&gt;Finally, if you happen to work with multi-process projects, you’ll find it easier to manage concurrency declaratively, spawning a tree of processes from a single script, rather than switching between terminal splits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Details&lt;/head&gt;
    &lt;p&gt; Use a consistent filename for the script. I use &lt;code&gt;make.ts&lt;/code&gt;, and so there’s a &lt;code&gt;make.ts&lt;/code&gt; in the root
            of most projects I work on. Correspondingly, I have &lt;code&gt;make.ts&lt;/code&gt; line in project’s &lt;code&gt;.git/info/exclude&lt;/code&gt;
            — the &lt;code&gt;.gitignore&lt;/code&gt; file which is not shared. The fixed
            name reduces fixed costs — whenever I need complex interactivity I
            don’t need to come up with a name for a new file, I open my
            pre-existing &lt;code&gt;make.ts&lt;/code&gt;, wipe whatever was there and start
            hacking. Similarly, I have &lt;code&gt;./make.ts&lt;/code&gt; in my shell
            history, so
            fish autosuggestions
            work for me. At one point, I had a VS Code task to run &lt;code&gt;make.ts&lt;/code&gt;, though I now use
            terminal editor.
          &lt;/p&gt;
    &lt;p&gt; Start the script with hash bang, &lt;code&gt;#!/usr/bin/env -S deno run
                --allow-all&lt;/code&gt;
            in my case, and
            &lt;code&gt;chmod a+x make.ts&lt;/code&gt;
            the file, to make it easy to run.
          &lt;/p&gt;
    &lt;p&gt;Write the script in a language that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you are comfortable with,&lt;/item&gt;
      &lt;item&gt;doesn’t require huge setup,&lt;/item&gt;
      &lt;item&gt;makes it easy to spawn subprocesses,&lt;/item&gt;
      &lt;item&gt;has good support for concurrency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For me, that is TypeScript. Modern JavaScript is sufficiently ergonomic, and structural, gradual typing is a sweet spot that gives you reasonable code completion, but still allows brute-forcing any problem by throwing enough stringly dicts at it.&lt;/p&gt;
    &lt;p&gt;JavaScript’s tagged template syntax is brilliant for scripting use-cases:&lt;/p&gt;
    &lt;p&gt;prints&lt;/p&gt;
    &lt;p&gt; What happens here is that &lt;code&gt;$&lt;/code&gt; gets a list of literal
            string fragments inside the backticks, and then, separately, a list
            of values to be interpolated in-between. It could
            concatenate everything to just a single string, but it doesn’t have
            to. This is precisely what is required for process spawning, where
            you want to pass an array of strings to the &lt;code&gt;exec&lt;/code&gt;
            syscall.
          &lt;/p&gt;
    &lt;p&gt; Specifically, I use dax library with Deno, which is excellent as a single-binary batteries-included scripting environment (see &amp;lt;3 Deno). Bun has a dax-like library in the box and is a good alternative (though I personally stick with Deno because of &lt;code&gt;deno fmt&lt;/code&gt; and &lt;code&gt;deno lsp&lt;/code&gt;). You could also use
            famous zx, though be mindful that it
            uses your shell as a middleman, something I consider to be
            sloppy (explanation).
          &lt;/p&gt;
    &lt;p&gt; While &lt;code&gt;dax&lt;/code&gt; makes it convenient to spawn a single
            program, &lt;code&gt;async/await&lt;/code&gt; is excellent for herding a slither
            of processes:
          &lt;/p&gt;
    &lt;head rend="h2"&gt;Concrete Example&lt;/head&gt;
    &lt;p&gt;Here’s how I applied this pattern earlier today. I wanted to measure how TigerBeetle cluster recovers from the crash of the primary. The manual way to do that would be to create a bunch of ssh sessions for several cloud machines, format datafiles, start replicas, and then create some load. I almost started to split my terminal up, but then figured out I can do it the smart way.&lt;/p&gt;
    &lt;p&gt;The first step was cross-compiling the binary, uploading it to the cloud machines, and running the cluster (using my box from the other week):&lt;/p&gt;
    &lt;p&gt;Running the above the second time, I realized that I need to kill the old cluster first, so two new commands are “interactively” inserted:&lt;/p&gt;
    &lt;p&gt;At this point, my investment in writing this file and not just entering the commands one-by-one already paid off!&lt;/p&gt;
    &lt;p&gt;The next step is to run the benchmark load in parallel with the cluster:&lt;/p&gt;
    &lt;p&gt;I don’t need two terminals for two processes, and I get to copy-paste-edit the mostly same command.&lt;/p&gt;
    &lt;p&gt; For the next step, I actually want to kill one of the replicas, and I also want to capture live logs, to see in real-time how the cluster reacts. This is where &lt;code&gt;0-5&lt;/code&gt; multiplexing syntax
            of box falls short, but, given that this is JavaScript, I can just
            write a for loop:
          &lt;/p&gt;
    &lt;p&gt; At this point, I do need two terminals. One runs &lt;code&gt;./make.ts&lt;/code&gt; and shows the log from the benchmark itself, the
            other runs &lt;code&gt;tail -f logs/2.log&lt;/code&gt; to watch the next replica
            to become primary.
          &lt;/p&gt;
    &lt;p&gt;I have definitelly crossed the line where writing a script makes sense, but the neat thing is that the gradual evolution up to this point. There isn’t a discontinuity where I need to spend 15 minutes trying to shape various ad-hoc commands from five terminals into a single coherent script, it was in the file to begin with.&lt;/p&gt;
    &lt;p&gt; And then the script is easy to evolve. Once you realize that it’s a good idea to also run the same benchmark against a different, baseline version TigerBeetle, you replace &lt;code&gt;./tigerbeetle&lt;/code&gt;
            with
            &lt;code&gt;./${tigerbeetle}&lt;/code&gt; and wrap everything into
          &lt;/p&gt;
    &lt;p&gt;A bit more hacking, and you end up with a repeatable benchmark schedule for a matrix of parameters:&lt;/p&gt;
    &lt;p&gt;That’s the gist of it. Don’t let the shell history be your source, capture it into the file first!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46792194</guid><pubDate>Wed, 28 Jan 2026 07:35:51 +0000</pubDate></item><item><title>Virtual Boy on TV with Intelligent Systems Video Boy</title><link>https://hcs64.com/video-boy-vue/</link><description>&lt;doc fingerprint="2d351f13193782d2"&gt;
  &lt;main&gt;
    &lt;p&gt;The Video Boy plays Nintendo Virtual Boy games on a TV or monitor. As with many of Nintendo's development tools, it was made by Intelligent Systems. These were used to record video or screenshots; it's said that this particular unit was used by the venerable Nintendo Power.&lt;/p&gt;
    &lt;p&gt;Contents:&lt;/p&gt;
    &lt;p&gt;The Virtual Boy inputs are on the top and front of the unit. From left to right: cartridge slot, link cable port, and controller port. There's also a red power LED. A cartridge goes in with the label facing down; it plugs directly into a Virtual Boy main board.&lt;/p&gt;
    &lt;p&gt;Note that the name shown here is "Video Adapter VUE". "Video Boy" appears nowhere on this device, but it is found in the instructions as "VIDEO-BOY (VUE)" and on the Intelligent Systems site as ãããªãã¼ã¤ or videoboy.html.&lt;/p&gt;
    &lt;p&gt;The AV multi-out is PAL, which is 50 FPS like the Virtual Boy; this avoids a more complicated and lossy scan conversion to ~60 FPS NTSC.&lt;/p&gt;
    &lt;p&gt;The unit came with a DB9 to 3xRCA cable for the RGB OUT; I haven't tested it, lacking a monitor that takes raw RGB. The SCANNER port connects to a development headset; I don't have one.&lt;/p&gt;
    &lt;p&gt;This row of switches, SW1, is exposed on the bottom of the unit. The initial setting was: 1 off, 2 on, 3 on, 4 on, 5 off, 6 off, 7 off, 8 on.&lt;/p&gt;
    &lt;p&gt;7 and 8 control which display is shown: 8 controls the left, rendered in Virtual Boy red, 7 is the right, rendered in green. With both 7 and 8 on, left and right are combined, this was intended as anaglyph 3D. See the video output below.&lt;/p&gt;
    &lt;p&gt;Switching 5 on prevents anything from working. I didn't notice any effect from switching the others.&lt;/p&gt;
    &lt;p&gt;There's an image of the instructions which describes the switches, unfortunately it's low resolution. I think it says "don't use" for 5 and 6, and 1-4 are for setting some integer, "1=MSB, 4=LSB, ON=0, OFF=1".&lt;/p&gt;
    &lt;p&gt;There are nearby unpopulated jumper pads, and a set of pads marked "CL" which seem to be cut traces. This may have been hardwired when the switch wasn't installed.&lt;/p&gt;
    &lt;p&gt;The top label says "ããã¸ã§å 12å·", approximately "Project No. 12".&lt;/p&gt;
    &lt;p&gt;The bottom label says "VUE TV MONITOR", identifying the board inside. "Ver. C" indicates the third or fourth version, and "+æ¹é " is "plus retrofit", perhaps indicating the various jumper wire patches. "MAI-VUE-X8" identifies the internal Virtual Boy main board.&lt;/p&gt;
    &lt;p&gt;The monitor board (left) has a Virtual Boy main board mounted on top of it (center top between the metal braces). The right side of the case is taken up by the power supply.&lt;/p&gt;
    &lt;p&gt;The Virtual Boy generates an image by sweeping a column of light horizontally. To convert this to the rows of a PAL TV signal, at least one frame must be buffered and rotated; the monitor board performs this conversion.&lt;/p&gt;
    &lt;p&gt;Note that the monitor board has unpopulated connector pads on the left (CN1, label not visible) and lower right (CN2). I think this same board can be configured to go into a VUE-DEBUGGER development unit (see PAL monitor), CN1 would be where it plugs into the debugger bus.&lt;/p&gt;
    &lt;p&gt;The main board is the heart of a Virtual Boy. This seems to be an early or development board, MAI-VUE-X8, (c) 1994. (A production board is VUE-MAI-01, (c) 1995.)&lt;/p&gt;
    &lt;p&gt;For info on Virtual Boy hardware:&lt;/p&gt;
    &lt;p&gt;Under the MAI-VUE-X8 board there are a few stray ICs and the ribbon cables that carry video to the monitor board (left and right). The board name was hiding under here: "VUE TV MONITOR(C)", which matches "Ver. C" on the label.&lt;/p&gt;
    &lt;p&gt;The workhorses are these two big Xilinx XC3064-70 FPGAs, which get their configuration from the 1765DPC PROMs between them. Perhaps one stores input while the other scans output.&lt;/p&gt;
    &lt;p&gt;The big NEC chip on the left (D27C1024A-15) is a 1Mbit EPROM, with an Intelligent Systems metal sticker to prevent UV erasure. I guess that at least one of the FPGAs is configured as a DSP, running a program from the EPROM.&lt;/p&gt;
    &lt;p&gt;There are eight 32KB SRAMs across the board, numbered in two groups: U12 &amp;amp; U13 (64KB), and U17-U22 (192KB). The 64KB might be DSP work RAM. 192KB would exactly fit two 384x256 frames with 8 bits per pixel (384x256 is the size of the Virtual Boy framebuffer, though only the top 224 rows are used). Virtual Boy graphics are only 2 bits per pixel, but each of the three non-black brightness levels can be configured by an independent 8 bit register, so 8 bits per pixel is plausible. This could be a double buffer (one taking input while the other scans output), or it could be one buffer per eye, or a double buffer for each eye at only 4 bits per pixel.&lt;/p&gt;
    &lt;p&gt;The oscillator Y1 (left, below the EPROM) seems to be associated with a VCLK test point, probably Video Clock; it's labeled D177J4, which suggests the 17.734475 MHz PAL color subcarrier. There's an unpopulated space for a second oscillator, Y2, and support components; it's grouped with the SCLK test point, maybe the Servo Clock. This may have be used when the board was configured to plug into the VUE-DEBUGGER.&lt;/p&gt;
    &lt;p&gt;Output is produced here by two MB40778 8-bit DACs (bottom center), an S-RGB video encoder (center right), and numerous discrete components. On the left are the output connectors: CN10 at top is RGB, CN8 at bottom is AV multi-out. I guess that each DAC handles one channel, connected to the red and green inputs of the S-RGB.&lt;/p&gt;
    &lt;p&gt;There are three jumpers on the bottom, and one that goes to (and through) the Virtual Boy board, strategically glued. Version C might have still needed a few fixes, or maybe these are used to retrofit a particular MAI-VUE board, or they could be specific to the standalone Video Boy configuration.&lt;/p&gt;
    &lt;p&gt;These images of Virtual Boy Wario Land come through an Elgato dongle, deinterlaced with ffmpeg filter yadif=send_field.&lt;/p&gt;
    &lt;p&gt;The composite video output is a bit blurry. DIP switches 7 and 8 control how the two Virtual Boy displays are combined: 8 enables the left display in red, 7 the right in green, and they can be combined (center).&lt;/p&gt;
    &lt;p&gt;Here's the stereo effect in action, note how the colors separate on the backswing.&lt;/p&gt;
    &lt;p&gt;S-Video has nice crisp pixels, thanks to a higher luma resolution than composite. The Elgato isn't picking up chroma for some reason; this may be a flaw in the multi-out to S-Video cable I'm using, or the VUE Monitor may not output a color PAL S-Video signal. I use the brighter "green" output on the right when I occasionally stream Virtual Boy games.&lt;/p&gt;
    &lt;p&gt;Intelligent Systems had a ï¼¶ï¼µï¼¥ï¼ï¼¤ï¼¥ï¼¢ï¼µï¼§ï¼§ï¼¥ï¼²ã·ãªã¼ãºã®ãæ¡å site, with a section on the ãããªãã¼ã¤ (Video Boy) VUE and a connection diagram.&lt;/p&gt;
    &lt;p&gt;Here's the text of the Video Boy page, based on Google Translate:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Simply connect a TV monitor that has PAL video input or RGB input to the Video Boy VUE, and a simulated stereoscopic Virtual Boy image will be displayed. By playing the Virtual Boy game cartridge on the Video Boy VUE, you can display the left-eye image and the right-eye image on the TV monitor in red and green, respectively. It is also possible to select and display an image for the left eye only or an image for the right eye only.&lt;/p&gt;
      &lt;p&gt;Since the same screen can be checked by multiple people, it is very useful during demonstrations, specification meetings, debugging, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think the PALä»æ§ã¢ãã¿åºåãã¼ãVUE (PAL monitor output board) was the same VUE TV Monitor board that's used in the Video Boy, configured as an expansion board for the VUE-DEBUGGER development unit. The notes on that page are similar to the Video Boy, with these two added points:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You can record your debugging work on video. This makes it easier to reproduce and check bugs that occur only occasionally, and improves debugging efficiency.&lt;/p&gt;
      &lt;p&gt;The need for programmers to look into the scanner during development is drastically reduced, reducing the strain on the eyes of the developer and improving work efficiency.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;lettuce, a developer who had worked with the Video Boy, has a page with an image of the instructions, along with a longer post about working with the Virtual Boy.&lt;/p&gt;
    &lt;p&gt;Initially published 2021-05-14.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46792572</guid><pubDate>Wed, 28 Jan 2026 08:32:50 +0000</pubDate></item><item><title>Show HN: The HN Arcade</title><link>https://andrewgy8.github.io/hnarcade/</link><description>&lt;doc fingerprint="35d1c25da4e5eef"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to main content HN Arcade Games Tags How It Works GitHub HN Arcade Discover games from Hacker News Browse Games Submit a Game&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46793693</guid><pubDate>Wed, 28 Jan 2026 10:50:32 +0000</pubDate></item><item><title>Kyber (YC W23) Is Hiring a Staff Engineer</title><link>https://www.ycombinator.com/companies/kyber/jobs/GPJkv5v-staff-engineer-tech-lead</link><description>&lt;doc fingerprint="f15442ba0e572026"&gt;
  &lt;main&gt;
    &lt;p&gt;Instantly draft, review, and send complex regulatory notices.&lt;/p&gt;
    &lt;p&gt;At Kyber, we're building the next-generation document platform for enterprises. Today, our AI-native solution transforms regulatory document workflows, enabling insurance claims organizations to consolidate 80% of their templates, spend 65% less time drafting, and compress overall communication cycle times by 5x. Our vision is for every enterprise to seamlessly leverage AI templates to generate every document.&lt;/p&gt;
    &lt;p&gt;Over the past 18 months, we’ve:&lt;/p&gt;
    &lt;p&gt;Kyber is backed by top Silicon Valley VCs, including Y Combinator and Fellows Fund.&lt;/p&gt;
    &lt;p&gt;We're seeking a Staff Engineer with a clear line of sight to CTO. This role is ideal for someone who is already operating as a 10x engineer, thrives in early stage environments, and is excited to design and scale mission-critical AI systems from first principles.&lt;/p&gt;
    &lt;p&gt;Responsibilities:&lt;/p&gt;
    &lt;p&gt;What We’re Looking For in You:&lt;/p&gt;
    &lt;p&gt;Join us in building and scaling a game-changing enterprise product powered by state-of-the-art AI. At Kyber, your contributions will directly impact how businesses handle some of their most critical workflows and customer interactions.&lt;/p&gt;
    &lt;p&gt;If you’re obsessed with building, AI, and transforming enterprise workflows, we’d love to hear from you!&lt;/p&gt;
    &lt;p&gt;We want to hear from extraordinary individuals who are ready to shape the future of enterprise documents. To stand out, ask someone you’ve worked with to send your resume or LinkedIn profile, along with a brief 2-3 sentence endorsement, directly to arvind [at] askkyber.com.&lt;/p&gt;
    &lt;p&gt;Referrals matter. They help us understand the impact you’ve already had and the kind of teammate you’ll be. A strong referee can elevate your application, so choose someone who knows your skills and character well.&lt;/p&gt;
    &lt;p&gt;Apply today and help us bring enterprise documents into the AI-native age.&lt;/p&gt;
    &lt;p&gt;With Kyber, companies operating in regulated industries can quickly draft, review, and send complex regulatory notices. For example, when Branch Insurance's claims team has to settle a claim, instead of spending hours piecing together evidence to draft a complex notice, they can simply upload the details of the claim to Kyber, auto-generate multiple best in-class drafts, easily assign reviewers, collaborate on notices in real-time, and then send the letter to the individual the notice is for. Kyber not only saves these teams time, it also improves overall quality, accountability, and traceability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46794231</guid><pubDate>Wed, 28 Jan 2026 12:00:08 +0000</pubDate></item><item><title>Show HN: I built a small browser engine from scratch in C++</title><link>https://github.com/beginner-jhj/mini_browser</link><description>&lt;doc fingerprint="7c05a3807cd2fad"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built a small browser engine in C++ to understand how browsers render HTML to the screen.&lt;/item&gt;
      &lt;item&gt;Implemented core structures: HTML/CSS parsing, layout calculation, and rendering from scratch.&lt;/item&gt;
      &lt;item&gt;This is a learning-focused project, not a production browser.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hello, I'm a korean high school senior (Grade 12) planning to major in Computer Science.&lt;/p&gt;
    &lt;p&gt;I wanted to understand not just how to use technology, but how it works internally. I've built several websites with HTML, CSS, and JavaScript, but I never really understood how the browser executing this code actually works.&lt;/p&gt;
    &lt;p&gt;"How does a browser render HTML to the screen?"&lt;/p&gt;
    &lt;p&gt;To answer this question, I spent about 8 weeks building a browser engine from scratch.&lt;/p&gt;
    &lt;p&gt;C++ was entirely new to me, and I struggled with countless bugs, but I eventually completed a small browser that can parse HTML, apply CSS, and render images. While not perfect, it was sufficient to understand the core principles of how browsers work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Key Features&lt;/item&gt;
      &lt;item&gt;Tech Stack&lt;/item&gt;
      &lt;item&gt;Build &amp;amp; Run&lt;/item&gt;
      &lt;item&gt;Architecture&lt;/item&gt;
      &lt;item&gt;Project Structure&lt;/item&gt;
      &lt;item&gt;Supported CSS Properties&lt;/item&gt;
      &lt;item&gt;Challenges &amp;amp; Solutions&lt;/item&gt;
      &lt;item&gt;What I Learned&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This browser supports the following features:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;HTML Parsing: Tokenization, DOM tree construction, automatic error correction&lt;/item&gt;
      &lt;item&gt;CSS Rendering: Selector matching, style inheritance, Cascade application&lt;/item&gt;
      &lt;item&gt;Layout: Block/Inline layouts, Position property support&lt;/item&gt;
      &lt;item&gt;Images: Local/network/Data URL support, asynchronous loading, caching&lt;/item&gt;
      &lt;item&gt;Navigation: Link clicking, event bubbling, history (back/forward)&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Language&lt;/cell&gt;
        &lt;cell&gt;C++17&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;GUI Framework&lt;/cell&gt;
        &lt;cell&gt;Qt6 (Core, Gui, Network)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Build System&lt;/cell&gt;
        &lt;cell&gt;CMake 3.16+&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rendering&lt;/cell&gt;
        &lt;cell&gt;Qt Graphics View Framework&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS, Linux, or Windows (MSVC support)&lt;/item&gt;
      &lt;item&gt;CMake 3.16+&lt;/item&gt;
      &lt;item&gt;Qt6 installed (development libraries)&lt;/item&gt;
      &lt;item&gt;C++17-compatible compiler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install Qt6 on macOS (using Homebrew):&lt;/p&gt;
    &lt;code&gt;brew install qt6&lt;/code&gt;
    &lt;p&gt;Linux (Ubuntu/Debian):&lt;/p&gt;
    &lt;code&gt;sudo apt-get install qt6-base-dev cmake build-essential&lt;/code&gt;
    &lt;code&gt;# 1. Navigate to project directory
cd /path/to/small_browser

# 2. Create and enter build directory
mkdir -p build &amp;amp;&amp;amp; cd build

# 3. Configure with CMake
cmake ..

# 4. Build
make

# After build completes, executable: ./browser&lt;/code&gt;
    &lt;code&gt;# From build directory
./browser

# Or with full path
./build/browser&lt;/code&gt;
    &lt;p&gt;The GUI window will appear. You can test rendering by opening HTML files from the test_html_files directory.&lt;/p&gt;
    &lt;code&gt;# From build directory
cd test
ctest&lt;/code&gt;
    &lt;p&gt;Real browsers follow a 5-stage pipeline to render HTML to screen:&lt;/p&gt;
    &lt;p&gt;This project implements this pipeline as follows:&lt;/p&gt;
    &lt;p&gt;Goal: Convert HTML string into tokens&lt;/p&gt;
    &lt;p&gt;The first step breaks down HTML text into small units (tokens) that the parser can understand.&lt;/p&gt;
    &lt;p&gt;Core Struct:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;TOKEN&lt;/code&gt;(include/html/token.h)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;TOKEN_TYPE&lt;/code&gt;: START_TAG, END_TAG, TEXT&lt;/item&gt;&lt;item&gt;&lt;code&gt;value&lt;/code&gt;: tag name or text content&lt;/item&gt;&lt;item&gt;&lt;code&gt;attributes&lt;/code&gt;: tag attributes (key-value map)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTML_TOKENIZER&lt;/code&gt;(include/html/html_tokenizer.h, src/html/html_tokenizer.cpp)&lt;list rend="ul"&gt;&lt;item&gt;Role: Parse HTML string and convert to TOKEN vector&lt;/item&gt;&lt;item&gt;Key method: &lt;code&gt;tokenize()&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;Input: &amp;lt;div class="container"&amp;gt;Hello&amp;lt;/div&amp;gt;
Output: 
  - TOKEN{START_TAG, "div", {"class": "container"}}
  - TOKEN{TEXT, "Hello"}
  - TOKEN{END_TAG, "div"}
&lt;/code&gt;
    &lt;p&gt;Goal: Create DOM tree from tokens&lt;/p&gt;
    &lt;p&gt;Organize tokens into a hierarchical tree structure (DOM Tree).&lt;/p&gt;
    &lt;p&gt;Core Classes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;NODE&lt;/code&gt;(include/html/node.h, src/html/node.cpp)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;NODE_TYPE&lt;/code&gt;: ELEMENT, TEXT&lt;/item&gt;&lt;item&gt;Properties: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;m_tag_name&lt;/code&gt;: element's tag name (e.g., "div", "p")&lt;/item&gt;&lt;item&gt;&lt;code&gt;m_text&lt;/code&gt;: text node content&lt;/item&gt;&lt;item&gt;&lt;code&gt;m_children&lt;/code&gt;: child nodes&lt;/item&gt;&lt;item&gt;&lt;code&gt;m_attributes&lt;/code&gt;: attribute map (id, class, src, etc.)&lt;/item&gt;&lt;item&gt;&lt;code&gt;m_parent&lt;/code&gt;: parent node&lt;/item&gt;&lt;item&gt;&lt;code&gt;m_computed_style&lt;/code&gt;: calculated style information&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;HTML_PARSER&lt;/code&gt;(include/html/html_parser.h, src/html/html_parser.cpp)&lt;list rend="ul"&gt;&lt;item&gt;Role: Parse token stream and build DOM tree&lt;/item&gt;&lt;item&gt;Key method: &lt;code&gt;parse()&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Memory Structure:&lt;/p&gt;
    &lt;code&gt;root NODE
├── NODE (tag: html)
│   ├── NODE (tag: head)
│   │   └── NODE (tag: title)
│   │       └── NODE (text: "My Page")
│   └── NODE (tag: body)
│       ├── NODE (tag: div, attributes: {class: "container"})
│       │   └── NODE (text: "Hello World")
│       └── NODE (tag: img, attributes: {src: "image.png"})
&lt;/code&gt;
    &lt;p&gt;Goal: Parse CSS rules and apply styles to each node&lt;/p&gt;
    &lt;p&gt;Parse CSS file to extract style rules, then calculate final styles for each DOM node following CSS Cascade rules.&lt;/p&gt;
    &lt;p&gt;Core Structures/Classes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CSS_RULE&lt;/code&gt;(include/css/css_rule.h)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;selector&lt;/code&gt;: CSS selector (e.g., ".container", "#redBtn")&lt;/item&gt;&lt;item&gt;&lt;code&gt;properties&lt;/code&gt;: style properties (color, font-size, width, etc.)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;COMPUTED_STYLE&lt;/code&gt;(include/css/computed_style.h)&lt;list rend="ul"&gt;&lt;item&gt;Final style applied to each node&lt;/item&gt;&lt;item&gt;Property examples: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;color&lt;/code&gt;: text color&lt;/item&gt;&lt;item&gt;&lt;code&gt;font_size&lt;/code&gt;: font size&lt;/item&gt;&lt;item&gt;&lt;code&gt;font_weight&lt;/code&gt;: font weight&lt;/item&gt;&lt;item&gt;&lt;code&gt;display&lt;/code&gt;: BLOCK, INLINE, NONE&lt;/item&gt;&lt;item&gt;&lt;code&gt;margin_top/bottom/left/right&lt;/code&gt;: margin values&lt;/item&gt;&lt;item&gt;&lt;code&gt;padding_top/bottom/left/right&lt;/code&gt;: padding values&lt;/item&gt;&lt;item&gt;&lt;code&gt;background_color&lt;/code&gt;: background color&lt;/item&gt;&lt;item&gt;&lt;code&gt;position&lt;/code&gt;: Static, Relative, Absolute, Fixed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CSS_PARSER&lt;/code&gt;(include/css/css_parser.h, src/css/css_parser.cpp)&lt;list rend="ul"&gt;&lt;item&gt;CSS rule parsing&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CSSOM&lt;/code&gt;(include/css/cssom.h, src/css/cssom.cpp)&lt;list rend="ul"&gt;&lt;item&gt;CSS Object Model management&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;apply_style()&lt;/code&gt;(include/css/apply_style.h, src/css/apply_style.cpp)&lt;list rend="ul"&gt;&lt;item&gt;Apply CSS rules to DOM nodes&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Style Calculation Process:&lt;/p&gt;
    &lt;code&gt;1. Parse CSS rules → store as CSS_RULE
2. For each NODE:
   - Find matching selectors
   - Calculate specificity (I simplified this)
   - Apply Cascade rules
   - Create COMPUTED_STYLE
&lt;/code&gt;
    &lt;p&gt;Goal: Calculate position and size of each element&lt;/p&gt;
    &lt;p&gt;Based on styled nodes, calculate each element's position (x, y) and dimensions (width, height) on screen.&lt;/p&gt;
    &lt;p&gt;Core Structs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;LAYOUT_BOX&lt;/code&gt;(include/css/layout_tree.h)&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;node&lt;/code&gt;: corresponding DOM node&lt;/item&gt;&lt;item&gt;&lt;code&gt;style&lt;/code&gt;: applied COMPUTED_STYLE&lt;/item&gt;&lt;item&gt;&lt;code&gt;x, y&lt;/code&gt;: position&lt;/item&gt;&lt;item&gt;&lt;code&gt;width, height&lt;/code&gt;: dimensions&lt;/item&gt;&lt;item&gt;&lt;code&gt;children&lt;/code&gt;: child LAYOUT_BOX elements&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;LINE_STATE&lt;/code&gt;(include/css/layout_tree.h)&lt;list rend="ul"&gt;&lt;item&gt;Track current state during inline layout calculation&lt;/item&gt;&lt;item&gt;&lt;code&gt;current_x, current_y&lt;/code&gt;: current position&lt;/item&gt;&lt;item&gt;&lt;code&gt;line_height&lt;/code&gt;: current line height&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Helper Functions (include/css/layout_tree.h, src/css/layout_tree.cpp)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Role: Generate LAYOUT_BOX tree from DOM nodes&lt;/item&gt;
      &lt;item&gt;Key functions: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;layout_block_element()&lt;/code&gt;: calculate block layout&lt;/item&gt;&lt;item&gt;&lt;code&gt;layout_inline_element()&lt;/code&gt;: calculate inline layout&lt;/item&gt;&lt;item&gt;&lt;code&gt;layout_text_element()&lt;/code&gt;: calculate text node size&lt;/item&gt;&lt;item&gt;&lt;code&gt;layout_image_element()&lt;/code&gt;: calculate image element layout&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Layout Algorithm:&lt;/p&gt;
    &lt;code&gt;BLOCK elements:
  - Use full width
  - Children arranged vertically
  
INLINE elements:
  - Arranged horizontally
  - Flow like text
  
Margin/Padding handling:
  - total_width = margin_left + border + padding + content + padding + border + margin_right
&lt;/code&gt;
    &lt;p&gt;Goal: Draw calculated layout on screen&lt;/p&gt;
    &lt;p&gt;Traverse LAYOUT_BOX tree and render each element graphically.&lt;/p&gt;
    &lt;p&gt;Core Classes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Renderer&lt;/code&gt;(include/gui/renderer.h, src/gui/renderer.cpp)&lt;list rend="ul"&gt;&lt;item&gt;Role: Draw layout boxes to Qt Graphics&lt;/item&gt;&lt;item&gt;Key methods: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;paint_layout()&lt;/code&gt;: render entire layout box tree&lt;/item&gt;&lt;item&gt;&lt;code&gt;draw_element_box()&lt;/code&gt;: draw element box (background, border)&lt;/item&gt;&lt;item&gt;&lt;code&gt;draw_text_node()&lt;/code&gt;: render text&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Uses Qt's &lt;code&gt;QPainter&lt;/code&gt;to draw on screen&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rendering Order:&lt;/p&gt;
    &lt;code&gt;1. Fill background color
2. Draw borders
3. Handle padding area
4. Render text
5. Render images (using cached images)
6. Traverse child elements
&lt;/code&gt;
    &lt;code&gt;HTML String
    ↓
[HTML_TOKENIZER] - Tokenization
    ↓ TOKEN vector
[HTML_PARSER] - DOM Construction
    ↓ NODE tree
[CSS_PARSER + CSSOM + apply_style()] - Style Calculation
    ↓ NODE + COMPUTED_STYLE
[Helper Functions] - Layout
    ↓ LAYOUT_BOX tree
[RENDERER] - Painting
    ↓
Screen Output
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;IMAGE_CACHE_MANAGER (include/gui/image_cache_manager.h)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Cache downloaded images to prevent duplicate loads&lt;/item&gt;
          &lt;item&gt;Store QPixmap&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MAIN_WINDOW (include/gui/main_window.h, src/gui/main_window.cpp)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Qt main window&lt;/item&gt;
          &lt;item&gt;Provide user interface&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;include/
  ├── html/           # HTML parsing (token.h, node.h, html_tokenizer.h, html_parser.h)
  ├── css/            # CSS parsing &amp;amp; layout (computed_style.h, css_parser.h, layout_tree.h, etc.)
  └── gui/            # Rendering &amp;amp; UI (renderer.h, main_window.h, etc.)

src/
  ├── html/           # HTML parsing implementation
  ├── css/            # CSS &amp;amp; layout implementation
  └── gui/            # Rendering implementation
&lt;/code&gt;
    &lt;p&gt;This browser supports the following CSS properties. All property parsing is defined in the &lt;code&gt;COMPUTED_STYLE&lt;/code&gt; struct in include/css/computed_style.h, with implementation in src/css/computed_style.cpp.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Text color&lt;/cell&gt;
        &lt;cell&gt;Color name, hex (#RRGGBB)&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;font-size&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Font size&lt;/cell&gt;
        &lt;cell&gt;Number + px (e.g., 16px)&lt;/cell&gt;
        &lt;cell&gt;16px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;font-weight&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Font weight&lt;/cell&gt;
        &lt;cell&gt;normal, bold, 100-900&lt;/cell&gt;
        &lt;cell&gt;normal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;font-style&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Font style&lt;/cell&gt;
        &lt;cell&gt;normal, italic&lt;/cell&gt;
        &lt;cell&gt;normal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;font-family&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Font name&lt;/cell&gt;
        &lt;cell&gt;Font name&lt;/cell&gt;
        &lt;cell&gt;Arial&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;line-height&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Line height&lt;/cell&gt;
        &lt;cell&gt;Number (multiplier)&lt;/cell&gt;
        &lt;cell&gt;font-size * 1.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;background-color&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color&lt;/cell&gt;
        &lt;cell&gt;Color name, hex (#RRGGBB)&lt;/cell&gt;
        &lt;cell&gt;transparent&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;width&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Element width&lt;/cell&gt;
        &lt;cell&gt;Number + px, auto&lt;/cell&gt;
        &lt;cell&gt;auto (-1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;height&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Element height&lt;/cell&gt;
        &lt;cell&gt;Number + px, auto&lt;/cell&gt;
        &lt;cell&gt;auto (-1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;box-sizing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Box sizing method&lt;/cell&gt;
        &lt;cell&gt;content-box, border-box&lt;/cell&gt;
        &lt;cell&gt;content-box&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;margin-top&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top margin&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;margin-right&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Right margin&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;margin-bottom&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bottom margin&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;margin-left&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Left margin&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;margin&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Margin shorthand&lt;/cell&gt;
        &lt;cell&gt;1-4 values&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;padding-top&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top padding&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;padding-right&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Right padding&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;padding-bottom&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bottom padding&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;padding-left&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Left padding&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;padding&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Padding shorthand&lt;/cell&gt;
        &lt;cell&gt;1-4 values&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Shorthand Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;margin: 10px;&lt;/code&gt;→ 10px on all sides&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;margin: 10px 20px;&lt;/code&gt;→ 10px top/bottom, 20px left/right&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;margin: 10px 20px 30px;&lt;/code&gt;→ 10px top, 20px left/right, 30px bottom&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;margin: 10px 20px 30px 40px;&lt;/code&gt;→ 10px top, 20px right, 30px bottom, 40px left&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;border-width&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Border width&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;border-color&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Border color&lt;/cell&gt;
        &lt;cell&gt;Color name, hex (#RRGGBB)&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;border-style&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Border style&lt;/cell&gt;
        &lt;cell&gt;solid, dashed, dotted, etc.&lt;/cell&gt;
        &lt;cell&gt;solid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;border&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Border shorthand&lt;/cell&gt;
        &lt;cell&gt;width color style&lt;/cell&gt;
        &lt;cell&gt;0 black solid&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;display&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Display type&lt;/cell&gt;
        &lt;cell&gt;block, inline, none&lt;/cell&gt;
        &lt;cell&gt;inline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;position&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Position type&lt;/cell&gt;
        &lt;cell&gt;static, relative, absolute, fixed&lt;/cell&gt;
        &lt;cell&gt;static&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;top&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top position (relative, absolute, fixed)&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;right&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Right position (relative, absolute, fixed)&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;bottom&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bottom position (relative, absolute, fixed)&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;left&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Left position (relative, absolute, fixed)&lt;/cell&gt;
        &lt;cell&gt;Number + px&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;text-align&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Text alignment&lt;/cell&gt;
        &lt;cell&gt;left, center, right, justify&lt;/cell&gt;
        &lt;cell&gt;left&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;text-decoration&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Text decoration&lt;/cell&gt;
        &lt;cell&gt;none, underline, line-through, overline&lt;/cell&gt;
        &lt;cell&gt;none&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Property&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Possible Values&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;opacity&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Opacity&lt;/cell&gt;
        &lt;cell&gt;0 - 1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;visibility&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Element visibility&lt;/cell&gt;
        &lt;cell&gt;visible, hidden (true/false)&lt;/cell&gt;
        &lt;cell&gt;visible&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Property Parsing &amp;amp; Setting (src/css/computed_style.cpp):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;init_setters()&lt;/code&gt;: Register setter functions for all CSS properties&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_color()&lt;/code&gt;: Parse color values (hex, named colors)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_font_size()&lt;/code&gt;: Parse font size&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_string_to_float()&lt;/code&gt;: Parse numeric values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_display_type()&lt;/code&gt;: Parse display values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_text_align()&lt;/code&gt;: Parse text-align values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_box_sizing()&lt;/code&gt;: Parse box-sizing values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_text_decoration()&lt;/code&gt;: Parse text-decoration values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_position_type()&lt;/code&gt;: Parse position values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;parse_spacing_shorthand()&lt;/code&gt;: Parse margin/padding shorthand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Style Inheritance (src/css/computed_style.cpp):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_color()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_font_size()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_font_weight()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_font_style()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_font_family()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_line_height()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_text_align()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_visibility()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;inherit_text_decoration()&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was new to C++, and unexpected problems and bugs appeared continuously from start to finish. While some issues remain unresolved, I'll share the 3 most difficult challenges and how I overcame them.&lt;/p&gt;
    &lt;p&gt;The Challenge:&lt;/p&gt;
    &lt;p&gt;Writing parsers required managing many different states. HTML parsing needed to handle start tags, end tags, text, comments—so many cases. I had no idea how to even start.&lt;/p&gt;
    &lt;p&gt;The Solution:&lt;/p&gt;
    &lt;p&gt;I read two key articles to understand parser fundamentals:&lt;/p&gt;
    &lt;p&gt;After understanding the basic principles, I wrote a few simple parsers myself. Once comfortable with how parsers work, I could independently write the complete CSS parsing logic.&lt;/p&gt;
    &lt;p&gt;The Challenge:&lt;/p&gt;
    &lt;p&gt;Rendering required managing even more states than parsing. &lt;code&gt;LINE_STATE&lt;/code&gt;, &lt;code&gt;LAYOUT_BOX&lt;/code&gt;—complex state tracking. I had to implement recursion, which I wasn't familiar with. Plus, I needed to handle different element types (boxes, inline, text) separately.&lt;/p&gt;
    &lt;p&gt;The Solution:&lt;/p&gt;
    &lt;p&gt;I used Claude AI extensively to understand rendering logic and reviewed the generated code carefully. Rather than using AI code directly, I reviewed it thoroughly and asked questions to understand the underlying principles. After becoming comfortable with rendering logic, I could modify the codebase and fix bugs, which proved I truly understood it.&lt;/p&gt;
    &lt;p&gt;The Challenge:&lt;/p&gt;
    &lt;p&gt;Fetching HTTP/HTTPS images from external servers was the most complex part. I couldn't stop layout calculation while downloading images, so I needed to understand asynchronous processing. While I theoretically understood image caching and reflowing, implementation required considering so many details.&lt;/p&gt;
    &lt;p&gt;The Solution:&lt;/p&gt;
    &lt;p&gt;I invested 3-5 hours designing a solid, robust image caching/reflowing system. With proper architecture in place, implementation became much easier. Through this process, I clearly understood the difference between multithreading async and non-blocking I/O async.&lt;/p&gt;
    &lt;p&gt;Overcoming these three challenges taught me more than just technical skills—it taught me how to approach problems and the importance of design. While not perfect, this is the project's greatest value.&lt;/p&gt;
    &lt;p&gt;Through this project, I gained hands-on understanding of how real browsers work. Beyond the concept of "browsers render HTML," I now understand how tokenization, layout calculation, and final rendering stages interact. This will help me predict browser behavior and optimize performance in future web development.&lt;/p&gt;
    &lt;p&gt;But the most valuable lessons transcended this specific project:&lt;/p&gt;
    &lt;p&gt;1. Systematic Debugging&lt;/p&gt;
    &lt;p&gt;When facing endless bugs, I learned not to randomly fix code but to form hypotheses and test them. Through proper logging, I tracked program state and systematically identified where problems occurred. This debugging discipline will serve me across all programming languages.&lt;/p&gt;
    &lt;p&gt;2. Persistence &amp;amp; Grit&lt;/p&gt;
    &lt;p&gt;Many times I wanted to give up when problems remained unsolved for days. But by breaking problems into pieces and accumulating small progress, I eventually overcame them. I realized this persistence is essential for success in any field.&lt;/p&gt;
    &lt;p&gt;3. The Value of Pragmatism&lt;/p&gt;
    &lt;p&gt;Perfect software doesn't exist. This browser still lacks support for many CSS properties and HTML elements, and has various bugs. But I learned that shipping imperfect but working software beats chasing ideal perfection. This pragmatism is crucial in the real software industry.&lt;/p&gt;
    &lt;p&gt;4. The Power of "Why?"&lt;/p&gt;
    &lt;p&gt;When receiving code from AI or tutorials, I didn't just check if it worked. I constantly asked "Why does this work?", "Is this part really necessary?", "Are there alternative approaches?" This curiosity and deep exploration led to understanding principles, not just surface-level learning.&lt;/p&gt;
    &lt;p&gt;The greatest achievement of this project isn't the completed browser.&lt;/p&gt;
    &lt;p&gt;It's developing problem-solving ability.&lt;/p&gt;
    &lt;p&gt;Systematic debugging, persistent effort, "completion over perfection" pragmatism, and the habit of always asking "Why?"&lt;/p&gt;
    &lt;p&gt;These will help me tackle every problem I face ahead.&lt;/p&gt;
    &lt;p&gt;Honestly, experienced developers might complete this more elegantly,&lt;/p&gt;
    &lt;p&gt;but to a student new to C++, it seemed nearly impossible.&lt;/p&gt;
    &lt;p&gt;Yet I still took on the challenge. Why?&lt;/p&gt;
    &lt;p&gt;It was simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I was curious&lt;/item&gt;
      &lt;item&gt;It looked fun&lt;/item&gt;
      &lt;item&gt;I thought I could do it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So I created a folder, opened an editor, and started coding.&lt;/p&gt;
    &lt;code&gt;mkdir mini_browser
cd mini_browser
# Let's go&lt;/code&gt;
    &lt;p&gt;There were many difficulties. 5-hour debugging sessions. 3-day unsolved bugs.&lt;/p&gt;
    &lt;p&gt;But 8 weeks later, I have a working browser.&lt;/p&gt;
    &lt;p&gt;Are you facing something that seems impossible right now?&lt;/p&gt;
    &lt;p&gt;Just start.&lt;/p&gt;
    &lt;p&gt;Imperfect is okay. Slow is okay. Stuck is okay.&lt;/p&gt;
    &lt;p&gt;Without starting, it stays impossible forever. With starting, it becomes possible.&lt;/p&gt;
    &lt;p&gt;Thank you for reading this long journey. 🚀&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46795540</guid><pubDate>Wed, 28 Jan 2026 14:03:28 +0000</pubDate></item><item><title>Microsoft forced me to switch to Linux</title><link>https://www.himthe.dev/blog/microsoft-to-linux</link><description>&lt;doc fingerprint="3c17b9573b05fcde"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Microsoft to Microslop to Linux: Why I Made the Switch&lt;/head&gt;
    &lt;p&gt;January 26, 2026&lt;/p&gt;
    &lt;head rend="h2"&gt;What's better than a devil you don't know?&lt;lb/&gt;The devil you do.&lt;/head&gt;
    &lt;p&gt;I've used Windows for as long as I've been alive. At 6 years old, my first computer was a Windows 98 machine, with an Athlon XP 1900+ (Palomino core) and a GeForce 440 MX, blessed with a generous 256 megabytes of RAM.&lt;/p&gt;
    &lt;p&gt;Looking back, I kinda got scammed with that graphics card, but what could I do? I was a silly kid. (The missing shader support came back to bite me in the ass)&lt;/p&gt;
    &lt;p&gt;Also, is it weird that I still remember the specs of my first computer, 22 years later?&lt;/p&gt;
    &lt;p&gt;Anyway, Windows has been familiar and comfortable. I knew all the workarounds and how to extract maximum efficiency from it.&lt;/p&gt;
    &lt;p&gt;I was a happy user, for over 20 years, and Windows has been my go-to for everything computer-related.&lt;/p&gt;
    &lt;p&gt;Even after becoming a software developer and using a macbook, I'd still find myself reaching for Windows at times.&lt;/p&gt;
    &lt;p&gt;That is, until Microsoft decided to turn it into something completely unrecognizable and unusable.&lt;/p&gt;
    &lt;head rend="h2"&gt;It all came crashing down&lt;/head&gt;
    &lt;p&gt;I think it started with the Windows 10 full-screen ads.&lt;/p&gt;
    &lt;p&gt;You know, those friendly suggestions telling you to try OneDrive or to "use the recommended browser settings" (reads as "please try Edge and OneDrive, we're desperate").&lt;/p&gt;
    &lt;p&gt;Actually, scratch that, I think it really started with the non-consensual updates:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Oh you're doing work? That's so cute... we're gonna close whatever apps you had open, because we're updating now. We own your computer.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;You had unsaved work? Too bad, it's gone, get bent.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;At first I ignored it, and carried on as normal. Sure, I'd get mad from time to time and I'd complain.&lt;/p&gt;
    &lt;p&gt;But hey, nothing beats the convenience of being able to have all of your applications in one place&lt;/p&gt;
    &lt;head rend="h4"&gt;Right? Right?&lt;/head&gt;
    &lt;p&gt;My breaking point came with the 24H2 update. It installed on my system without my consent, like any other major update. I knew there were problems with it, people were already complaining on Reddit, so I just postponed it, and kept postponing it.&lt;/p&gt;
    &lt;p&gt;All it took was for me to leave my computer on and unattended for a while, and BOOM, just like that - the major OS update that nobody wanted, it was on my computer.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Chrome Seizure Incident&lt;/head&gt;
    &lt;p&gt;As soon as 24H2 landed on my machine, I encountered a bug so bizarre I thought I was losing my marbles.&lt;lb/&gt; If Chrome was positioned under any other window, it would start having what I can only describe as a visual seizure.&lt;lb/&gt; Here's Ableton Live with Chrome (Reddit) under it:&lt;/p&gt;
    &lt;p&gt;Worse, there was a decent chance this would trigger a full system lock, leaving me smashing my desk in impotent rage. I shit you not.&lt;/p&gt;
    &lt;p&gt;I tried to rollback. The rollback failed with an error. I reinstalled Windows. The bug persisted.&lt;lb/&gt; Like digital herpes, I just couldn't get rid of it.&lt;lb/&gt; The solution? Installing an Insider build. Yes, the solution to Microsoft's broken stable release was to use their unstable release.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Sequel I Never Wanted&lt;/head&gt;
    &lt;p&gt;The Insider build worked...sort of. But now I had a new bug: Chrome would randomly lock up for about 30 seconds when a video was playing. My options were to wait it out or press Ctrl+Alt+Delete and Esc to force my way back to a working browser. After some digging, I discovered this was caused by an NVIDIA-Microsoft driver incompatibility.&lt;/p&gt;
    &lt;p&gt;Links here:&lt;/p&gt;
    &lt;p&gt;I've found out that the flickers and the chrome lock-up issues are likely caused by the Multiplane Overlay (MPO) pipeline. Microsoft blamed NVIDIA for not correctly implementing it in their drivers. NVIDIA blamed Microsoft. What's clear is that if you were facing this issue, you were essentially screwed because these 2 companies would just pass the hot potato to each other.&lt;/p&gt;
    &lt;p&gt;I should mention that this bug persisted even after I went off the Insider build and on 25H2. And when I posted on r/Microsoft, they just deleted it.&lt;/p&gt;
    &lt;p&gt;The latest and greatest OS surely cannot be broken beyond repair, surely I'm using my PC wrong.&lt;/p&gt;
    &lt;p&gt;So there I was, finally grasping the reality of what you're up against, as a Windows user:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Random bugs that break basic functionality&lt;/item&gt;
      &lt;item&gt;Updates that install without permission and brick my system&lt;/item&gt;
      &lt;item&gt;Copilot and OneDrive ads appearing in every corner of the OS&lt;/item&gt;
      &lt;item&gt;Copilot buttons everywhere, coming for every application&lt;/item&gt;
      &lt;item&gt;Can't even make a local account without hacking the setup with Rufus (they even removed the terminal workaround)&lt;/item&gt;
      &lt;item&gt;Zero actionable fixes or even an aknowledgment of their fuckups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;People often say Linux is "too much work.".&lt;/p&gt;
    &lt;p&gt;And I agree. They're completely justified to complain. There's the documentation page diving, the forums, the reddit threads. And, most importantly, you have to basically rewire your brain and stop expecting it to behave like Windows used to.&lt;/p&gt;
    &lt;p&gt;But I looked at the list above and realized: Windows is now also too much work.&lt;lb/&gt; And the difference with Windows is that you're going to do all that work while actively fighting your computer only for it to be undone when the next surprise update comes and ruins everything.&lt;/p&gt;
    &lt;p&gt;You might be thinking "just disable updates, man" or "just install LTSC", or "just run some random debloat script off of GitHub". Why? Why would I jump through all these hoops? I'd rather put in the effort for an OS that knows what consent is and respects me as a user.&lt;/p&gt;
    &lt;head rend="h2"&gt;Could the grass actually be greener on the other side?&lt;/head&gt;
    &lt;p&gt;To set the stage: I'm a software developer and a musician.&lt;/p&gt;
    &lt;p&gt;As you can imagine, I was legitimately worried about app support on Linux, and how it would distrupt my workflow.&lt;/p&gt;
    &lt;p&gt;But after Chrome crashing for the 10000th time, I said "enough is enough", and decided to go big. I installed CachyOS, a performance-focused Arch-based distribution, on my main machine (9800X3D, RTX 5080).&lt;/p&gt;
    &lt;p&gt;It wasn't a painless process. In fact, sleep mode was broken from the start, and my system would fail to detect the monitor after waking up.&lt;/p&gt;
    &lt;p&gt;What's more, Ableton Live does not have a native Linux build, only Windows and macOS. So I couldn't use it anymore, at least not without fucking around with Wine (which doesn't fully support it), or without keeping a Windows VM and taking an L on audio latency.&lt;/p&gt;
    &lt;p&gt;But unlike Windows, on CachyOS I could actually fix my NVIDIA woes by following this thread on their forum.&lt;/p&gt;
    &lt;p&gt;All I had to do was add the NVIDIA modules to mkinitcpio. One config change, a command to rebuild the initramfs, and problem solved.&lt;/p&gt;
    &lt;p&gt;I also found a good native alternative to Ableton Live - Bitwig Studio, which bothered to release a native Linux Build.&lt;/p&gt;
    &lt;p&gt;Thanks to the constant progress that was made with Pipewire, I'm getting audio latency on par with Mac OS, and lower than Windows. And my workflow didn't even change that much, since Bitwig is made by ex-Ableton developers that seem to give a shit.&lt;/p&gt;
    &lt;p&gt;As for my development tools, on Windows you already accept the fact that you WILL use WSL or docker, so realistically I just cut the broken middleman.&lt;/p&gt;
    &lt;p&gt;Now compare that to the Windows fuckery above.&lt;/p&gt;
    &lt;head rend="h2"&gt;What You're Signing Up For&lt;/head&gt;
    &lt;p&gt;If 3 years ago you would have told me that Microsoft would singlehandedly sabotage their own OS, doing more Linux marketing than the most neckbearded Linux fanboy (or the most femboy Thinkpad enjoyer), I'd have laughed in your face, called you delusional, and then hurled some more insults your way.&lt;/p&gt;
    &lt;p&gt;Yet here we are, I've been dual-booting CachyOS for over a year, and in the last month I've been using it exclusively.&lt;/p&gt;
    &lt;p&gt;So what is the actual state of Linux in 2026, from my honest perspective?&lt;/p&gt;
    &lt;head rend="h5"&gt;Web Browsing&lt;/head&gt;
    &lt;p&gt;All major browsers (Chrome, Firefox, Edge, Brave) have native Linux builds. Full support. No compromises.&lt;lb/&gt; Video playback works flawlessly, with hardware acceleration even. On AMD, on NVidia and yes, on Intel too.&lt;/p&gt;
    &lt;head rend="h5"&gt;Software Development&lt;/head&gt;
    &lt;p&gt;Linux is the preferred platform for development.&lt;/p&gt;
    &lt;p&gt;Better terminal support, native package managers, Docker runs natively without the WSL overhead, and your production servers are probably running Linux anyway.&lt;/p&gt;
    &lt;p&gt;Hell, even Microsoft has their own Linux distro, Azure Linux (Formerly CBL-Mariner).&lt;/p&gt;
    &lt;head rend="h5"&gt;Content Creation&lt;/head&gt;
    &lt;p&gt;This is where people assume Linux falls short. And they're right, but not completely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adobe Suite: Runs via Winboat. Far from perfect (no video acceleration, laggy at times), but functional&lt;/item&gt;
      &lt;item&gt;DaVinci Resolve: Native Linux app. Professional-grade video editing, free tier available&lt;/item&gt;
      &lt;item&gt;Kdenlive: Native Linux app, completely free and open source&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Music Production&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bitwig Studio: Incredible DAW that was designed that runs natively on Linux&lt;/item&gt;
      &lt;item&gt;Ardour: Native, free, open-source DAW&lt;/item&gt;
      &lt;item&gt;Audio latency: Thanks to PipeWire, Linux audio latency is actually lower than Windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Gaming&lt;/head&gt;
    &lt;p&gt;Here's where things get interesting. The perception is that gaming on Linux is a compromise. In 2026, that's increasingly untrue:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proton/Wine: Pretty much all games without kernel-level anti-cheat work out of the box through Steam's Proton compatibility layer&lt;/item&gt;
      &lt;item&gt;Performance: For AMD GPUs, gaming performance is on par with Windows, on average&lt;/item&gt;
      &lt;item&gt;NVIDIA: There was a 10-30% performance penalty on Intel/NVIDIA GPU setups, but recent Vulkan extensions are taking care of that.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;NVIDIA has released beta drivers making use of these improvements, and once Wine/DXVK/Proton are updated to make use of the extensions, the performance delta should be essentially gone&lt;/p&gt;
    &lt;p&gt;The only real limitation is that some games with anti-cheat like Valorant, Call of Duty or League of Legends won't run. But honestly I think not being able to launch League of Legends is actually a feature - one final reason to install Linux.&lt;/p&gt;
    &lt;p&gt;It's not all bad, though. Arc Raiders makes use of Easy Anti-Cheat, yet runs flawlessly. In fact, I've been playing it like a madman. It goes to show that if the developers want to, it's possible.&lt;/p&gt;
    &lt;head rend="h5"&gt;3D Modeling&lt;/head&gt;
    &lt;p&gt;Still falls short compared to Windows and Mac OS (Autodesk, I'm looking at you).&lt;/p&gt;
    &lt;p&gt;The silver lining is that Blender has a native build. So if it's your main application, you're good to go.&lt;/p&gt;
    &lt;head rend="h5"&gt;General Usage&lt;/head&gt;
    &lt;p&gt;Basic operations are so much faster on Linux. Opening directories, launching applications, system responsiveness. It's like your computer took a line of coke, and is now ready to work.&lt;/p&gt;
    &lt;p&gt;No more waiting for the Start menu to decide it wants to open. No more File Explorer hanging when you need it the most.&lt;/p&gt;
    &lt;p&gt;Since we're on the topic of Linux improvements, I want to address the elephant in the room - people who keep saying "I want to switch", but keep moving the goalposts:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"I'll switch when Linux supports X."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux supports X.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;"Okay, but what about Y?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux supports Y.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;"Well, Z is still missing..."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you're always finding the next reason not to switch, you're not looking for solutions, you're looking for excuses to stay complacent.&lt;/p&gt;
    &lt;p&gt;I was that person, so I would know.&lt;/p&gt;
    &lt;p&gt;At the same time, I want to take it down a notch and say that there are still plenty of use cases (Especially creative work, and like stated previously, 3D modelling and also Game Dev) where it simply doesn't make sense to switch.&lt;/p&gt;
    &lt;p&gt;So if you're in that scenario, don't feel pressured, just wait for things to improve.&lt;/p&gt;
    &lt;p&gt;And if you don't plan on ever switching, more power to you.&lt;/p&gt;
    &lt;p&gt;I'm not here to judge, just here to vent my Microsoft frustrations.&lt;/p&gt;
    &lt;p&gt;And I didn't really want to switch either, because who wants to re-learn how their computer should be operated from scratch? What I really wanted was for Windows to work, but Microsoft didn't.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Windows Retrospective&lt;/head&gt;
    &lt;p&gt;While I'm enjoying my new Linux setup, Windows 11 is having a miserable year, and we're only a month in!&lt;/p&gt;
    &lt;p&gt;According to Windows Latest, there were over 20 major update problems in 2025 alone, and 2026 is starting off strong, with the January update causing black screens and Outlook crashes.&lt;/p&gt;
    &lt;p&gt;Here's a quick 2025 Spotify Wrapped of the bugs Windows users dealt with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;USB audio devices randomly stopped working&lt;/item&gt;
      &lt;item&gt;Webcams failed to be detected&lt;/item&gt;
      &lt;item&gt;BitLocker settings became inaccessible&lt;/item&gt;
      &lt;item&gt;Adobe Premiere Pro couldn't drag clips on the timeline&lt;/item&gt;
      &lt;item&gt;Cursor constantly spinning for no reason&lt;/item&gt;
      &lt;item&gt;Remote Desktop sessions randomly disconnecting&lt;/item&gt;
      &lt;item&gt;The Copilot app accidentally getting deleted (okay, this is actually a good change for once)&lt;/item&gt;
      &lt;item&gt;Blue screens of death in mandatory security updates&lt;/item&gt;
      &lt;item&gt;Windows Hello face recognition broken&lt;/item&gt;
      &lt;item&gt;File Explorer becoming unresponsive&lt;/item&gt;
      &lt;item&gt;FPS drops and system reboots while gaming&lt;/item&gt;
      &lt;item&gt;Task Manager spawning infinite copies of itself&lt;/item&gt;
      &lt;item&gt;Dark mode breaking with white flashes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And the company's response? Crickets. They're busy boasting that 30% of their code is currently being written by AI. Don't worry, Microsoft, we can definitely tell.&lt;/p&gt;
    &lt;p&gt;For the remainder of 2026, Microsoft is cooking up a big one: replacing more and more native apps with React Native. But don't let the name fool you, there's nothing "native" about it. These are projects designed to be easily ported across any machine and architecture, because underneath it all, it's just JavaScript.&lt;/p&gt;
    &lt;p&gt;And each one spawns its own Chromium process, gobbling up your RAM so you can enjoy the privilege of opening the Settings app.&lt;/p&gt;
    &lt;p&gt;I could maybe understand this for a weather widget. But when it's coming for core system apps, it's LAZY!&lt;/p&gt;
    &lt;p&gt;I'm gonna go full conspiracy nut here, but I bet it's because the LLM understands JavaScript better, and Microsoft can't be asked to pay actual humans to write proper native code.&lt;/p&gt;
    &lt;p&gt;Meanwhile, entire governments are abandoning Windows for Linux, the term "Microslop" is trending on social media, and Windows 11's reputation is at its lowest point ever.&lt;/p&gt;
    &lt;head rend="h2"&gt;Not Because I Wanted To, But Because Microsoft Forced My Hand&lt;/head&gt;
    &lt;p&gt;So here I am. Fully switched to Linux.&lt;/p&gt;
    &lt;p&gt;Not because I'm some open-source idealist or command-line warrior (I'm just some guy), but because Microsoft turned into Microslop.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Microsoft CEO Satya Nadella wrote a blog post asking people to stop calling AI-generated content "slop" and to think of AI as "bicycles for the mind."&lt;/p&gt;
    &lt;p&gt;Well, Mr Satya, I have a couple of bicycles that will blow your mind:&lt;/p&gt;
    &lt;p&gt;You are the biggest Linux evangelist there ever was, you single-handedly convinced countless people to ditch your buggy, ad-ridden, bloated, slop-infested mess of an OS.&lt;/p&gt;
    &lt;p&gt;And worst of all, you're like a pit bull that has lock-jawed onto OpenAI's ballsack, and you're not letting go, not matter how much we tell you to.&lt;/p&gt;
    &lt;p&gt;So we're calling slop for what it is: disgusting slop.&lt;/p&gt;
    &lt;p&gt;You're chasing profit like your life depends on it, yet you've completely forgotten the very thing that generates profit: user satisfaction.&lt;/p&gt;
    &lt;p&gt;Now you're stuck in a circlejerk of fake value in a fake bubble, and OpenAI's hand is so far up your ass they're playing shadow puppets with your tonsils.&lt;/p&gt;
    &lt;p&gt;The time to switch is now. The tools are ready. The only question is: are you?&lt;/p&gt;
    &lt;p&gt;Satya came down from his cloud in the sky,&lt;/p&gt;
    &lt;p&gt;With Copilot dreams and a gleam in his eye,&lt;/p&gt;
    &lt;p&gt;He sprinkled AI on each app, every field,&lt;/p&gt;
    &lt;p&gt;Till users cried "Fuck!", and the slop was revealed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46795864</guid><pubDate>Wed, 28 Jan 2026 14:28:21 +0000</pubDate></item><item><title>Airfoil (2024)</title><link>https://ciechanow.ski/airfoil/</link><description>&lt;doc fingerprint="b77d1a2b09aeb189"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Airfoil&lt;/head&gt;
    &lt;p&gt;The dream of soaring in the sky like a bird has captivated the human mind for ages. Although many failed, some eventually succeeded in achieving that goal. These days we take air transportation for granted, but the physics of flight can still be puzzling.&lt;/p&gt;
    &lt;p&gt;In this article we’ll investigate what makes airplanes fly by looking at the forces generated by the flow of air around the aircraft’s wings. More specifically, we’ll focus on the cross section of those wings to reveal the shape of an airfoil â you can see it presented in yellow below:&lt;/p&gt;
    &lt;p&gt;We’ll find out how the shape and the orientation of the airfoil helps airplanes remain airborne. We’ll also learn about the behavior and properties of air and other flowing matter. In the demonstration below, you can see a fluid flowing around a gray cube. Using the slider to change just one property of this substance, we can end up with vastly different effects on the liveliness of that flow:&lt;/p&gt;
    &lt;p&gt;Over the course of this blog post we’ll build some intuitions for why these different effects happen to airfoils and other objects placed in flowing air. We’ll start this journey by looking at some of the methods we can use to visualize the motion of the air.&lt;/p&gt;
    &lt;head rend="h1"&gt;Visualizing Flow&lt;/head&gt;
    &lt;p&gt;If you’ve ever been outside in a grassy area on a windy fall day, you may have witnessed something similar to the little scene seen below. The slider lets you control the speed of time to observe in detail how the falling leaves and the bending blades of grass are visibly affected by the wind sweeping through this area:&lt;/p&gt;
    &lt;p&gt;We intuitively understand that it’s the flowing air that pushes the vegetation around, but note that we only observe the effects that the wind has on other objects â we can’t see the motion of the air itself. I could show you a similarly windy scene without the grass and leaves, and I could try to convince you that there is something going on there, but that completely empty demonstration wouldn’t be very gratifying.&lt;/p&gt;
    &lt;p&gt;Since the air’s transparency prevents us from tracking its movement directly, we have to come up with some other ways that can help us see its motion. Thankfully, the little outdoor scene already provides us with some ideas.&lt;/p&gt;
    &lt;p&gt;Notice that as the wind hits a blade of grass, that blade naturally bends in the direction of the blowing gust, and the faster that gust, the stronger the bending. AÂ single blade indicates the direction and speed of the flow of air in that area.&lt;/p&gt;
    &lt;p&gt;In the next demonstration we’re looking at the same grassy field from above. When seen from this perspective, all the blades form short lines that are locally aligned with the wind. The more leaned over a blade of grass is, the longer the line it forms. We can mimic this behavior with a collection of small arrows placed all over the area, as seen on the right side:&lt;/p&gt;
    &lt;p&gt;Each arrow represents the direction and the speed of the flow of air at that location â the longer the arrow, the faster the flow. In these windy conditions the flow varies from place to place and it also changes over time, which we can clearly see in the motion of the arrows.&lt;/p&gt;
    &lt;p&gt;Note that we have some flexibility in how the speed of wind corresponds to the length of an arrow. I adjusted the lengths of the arrows to prevent them from visually overlapping, but I also made sure to maintain their relative lengths â if one arrow is twice as long as the other, then the flow at that location is also twice as fast.&lt;/p&gt;
    &lt;p&gt;For visual clarity I’m also not packing the arrows as densely as the blades of grass are placed, but it’s important to note that every point in the flow has its own velocity which contributes to the complete velocity field present in this area. If we wanted to, we could draw a velocity arrow at any of the seemingly empty spots on the right side.&lt;/p&gt;
    &lt;p&gt;The arrows are convenient, but the grassy scene also has another aid for visualizing flows. Many light objects like leaves, flower petals, dust, or smoke are very easily influenced by the motion of the surrounding air. They quickly change their velocity to match the flow of the wind. We can replicate the behavior of these light objects with little markers that are pushed around by that flow. You can see them on the right side:&lt;/p&gt;
    &lt;p&gt;These little markers also show us the motion of the air. Each marker represents an object so small and light that it instantly picks up the speed of the surrounding airflow. We’d have a hard time seeing these miniscule specks at their actual sizes, so I’m drawing the markers as visible dots.&lt;/p&gt;
    &lt;p&gt;In fact, the motion of each marker is equivalent to the motion of the parcel of air right around it. If you slow down time, you’ll be able to see how each marker just moves in the direction of the arrows underneath it. I also made each marker leave a little ghost trail behind it â this lets us track the path the air, as represented by the marker, took on the way to its current position.&lt;/p&gt;
    &lt;p&gt;Let’s pause for a second to emphasize what the grass-like arrows and leaf-like markers represent âÂ they both show the velocity of the flow of air, but in slightly different ways. An arrow is attached to its fixed point in space, so it represents the current direction and speed of the flow at that location. The whole collection of arrows lets us easily see what the entire flow is doing at the moment.&lt;/p&gt;
    &lt;p&gt;On the other hand, the little markers are actively following the flow, letting us see how the air is actually moving through space, with the ghosty trails giving us some historical overview of where this parcel of air has come from.&lt;/p&gt;
    &lt;p&gt;The two methods we’ve seen so far are very versatile, but sometimes we don’t care about the local direction of the flow, only its speed â in the middle of this grassy field one might get cold from a fast blowing wind regardless of the direction from which that wind is coming. This brings us the third way of visualizing flow:&lt;/p&gt;
    &lt;p&gt;In this method we show the speed of the airflow using colors of varying brightness â the faster the wind, the brighter the color. You can see the whole spectrum of colors in the scale below the plot.&lt;/p&gt;
    &lt;p&gt;This method shows the speed of the flow at all locations giving us a more fine-grained insight into the motion of air at the cost of the directional information. To help with that I’ll sometimes overlay the regular arrows on top to let us know where the flow is going as well.&lt;/p&gt;
    &lt;p&gt;You may have noticed that all these methods present a flat, two dimensional view of the flow. It’s based on the assumption that the wind in our little scene doesn’t change with elevation, and that it also doesn’t blow towards or away from the ground.&lt;/p&gt;
    &lt;p&gt;In reality, the air velocity could vary in all three dimensions, and that air could also flow upwards or downwards. Thankfully, the air flows we’ll consider in this article will be two dimensional and the simple flat drawings will suffice.&lt;/p&gt;
    &lt;p&gt;Before we finish this section, let me bring up visualization of a simple airflow, but this time I’ll give you some control over its direction, which you can change using the second slider. The first one once more controls the speed of time:&lt;/p&gt;
    &lt;p&gt;Don’t be misled by the frozen arrows, the wind is actually blowing there. Remember that the arrows represent the local velocity of the flow of air, so while the velocity doesn’t change, the position of each packet of air does. You can see those changes by tracking the markers moving around with the flow. This demonstration represents a steady flow, which means that its properties don’t change over time.&lt;/p&gt;
    &lt;p&gt;So far we’ve been exploring the notion of airflow’s velocity on a more intuitive level, with a general understanding that’s it’s “the air” moving around in some direction and at some speed. I illustrated that concept using simple arrowsâ, markersÂ â¢, and varying colors, but we’re now ready to investigate the details hiding behind those straightforward graphical representations.&lt;/p&gt;
    &lt;p&gt;To do that, we have to look at individual particles of air. Although I briefly discussed the particle nature of air before, this time around we’re going to take a closer look at the motion of these molecules, and what it means for airflow as a whole.&lt;/p&gt;
    &lt;head rend="h1"&gt;Velocity&lt;/head&gt;
    &lt;p&gt;Let’s take a look at the air particles in a small, marked out volume of space seen in the demonstration below â you can drag the cube around to change the viewing angle. The slider controls the speed of time:&lt;/p&gt;
    &lt;p&gt;You’re witnessing the motion of over twelve thousand air particles. It may seem like a lot, but this cube is extremely tiny, its sides are only 80 nanometers long. To put this in perspective using more familiar sizes, if that cube’s side measured just 1 inch1 centimeter, it would contain around 410 quintillion, or 4.1Ã102025 quintillion, or 2.5Ã1019 particles.&lt;/p&gt;
    &lt;p&gt;The particles are zipping around in random directions, constantly entering and leaving this region. However, despite all this motion what you’re seeing here is a simulation of still air.&lt;/p&gt;
    &lt;p&gt;To understand how all this movement ends up creating still conditions, we first have to look at the velocity of each particle â I’ll visualize it with a small arrow in the direction of motion. To make things a easier to see, I’ll also highlight a few of the particles while fading out the rest of them:&lt;/p&gt;
    &lt;p&gt;The length of an arrow is proportional to the speed of a particle, so when you freeze the time you should be able to see how some particles are slower and some are faster. This speed variation follows a certain distribution that’s related to temperature â the warmer the air, the faster the motion of its particles.&lt;/p&gt;
    &lt;p&gt;At room temperature the average speed of a particle in air is an astonishing 1030Â mph1650Â km/h, which is many times higher than even the most severe hurricanes. Given the size of the cube, this means that even at the fastest speed of simulation everything happens 11 billion times slower than in real life.&lt;/p&gt;
    &lt;p&gt;If you paid close attention, you may have also noticed that sometimes the particles randomly change direction and speed of their motion â this happens when molecules collide. Each particle experiences roughly ten billion collisions per second. We’ll get back to these interactions later on, but for now let’s try to figure out how all this turmoil creates still air.&lt;/p&gt;
    &lt;p&gt;Having just seen the small velocity arrows of individual particles, let’s calculate the average velocity of a group of three particles, using the process shown below. We first take the velocity arrows from each particle and place them head to toe, one after another. Then we connect the start of the first arrow with the end of the last arrow to create the sum of all velocities. Finally, we divide, or scale down, the length of this sum by the number of particles to get the average velocity:&lt;/p&gt;
    &lt;p&gt;In the next demonstration we’re repeating this whole procedure by tallying up all the particles inside the red box. You can change the size of that region with the second slider. The large arrow in the middle shows the average velocity of particles in the box. To make that central arrow visible, I’m making it much larger than the tiny arrows tied to particles:&lt;/p&gt;
    &lt;p&gt;The counter in the bottom part of the demonstration tracks the current number of particles in the red cube. That value fluctuates as the molecules enter and leave that region. While aggregating over a small number of particles creates a very noisy readout, it doesn’t take that many particles to get a much steadier measure.&lt;/p&gt;
    &lt;p&gt;Recall that the scale of the large central arrow is much larger than the scale of individual tiny arrows attached to each particle. Despite that increase in size, the arrow practically disappears when we average out a larger number of particles and we can clearly see that the average velocity of particles is more or less zero even in this extremely small volume.&lt;/p&gt;
    &lt;p&gt;In still conditions, all these motions in different directions average out to nothing. As some particles enter the area from a random direction, the others also leave it in a random way. The bulk of air doesn’t really go anywhere and the particles just meander in a random fashion.&lt;/p&gt;
    &lt;p&gt;An imperfect, but convenient analogy is to imagine a swarm of bees flying in the air. While all the individual insects are actively roaming around at different speeds, the group as a whole may steadily stay in one place.&lt;/p&gt;
    &lt;p&gt;All these experiments form the key to understanding what happens when wind sweeps through an area. In the demonstration below, we’re once again watching a small volume of space, but this time you can control the speed of the blowing wind:&lt;/p&gt;
    &lt;p&gt;Notice the mphkm/h speedometer in the bottom of the demonstration. This is not a mistake âÂ even with hurricane-level wind speeds it’s very hard to see any difference in the motion of the particles. Perhaps you’ve managed to see the tiniest shifts in the small particle arrows as you drag the second slider around with time paused, but it’s difficult to even perceive from which direction the wind is blowing.&lt;/p&gt;
    &lt;p&gt;However, when we use the procedure of averaging the velocity of all the particles, we can reveal the motion of their group in the box of a given size, at a specific speed of the flow:&lt;/p&gt;
    &lt;p&gt;Because the motion of each individual particle is so disordered, we have to look at many of them at once to discern any universal characteristics. And when we do just that, from all the chaos emerges order.&lt;/p&gt;
    &lt;p&gt;It’s important to note that with this approach we’re tracking the velocity of the flow within the same region of space outlined by the red box â the molecules keep entering and leaving this area as the flow moves and the arrow in the middle shows the average velocity of the air’s particles in that area.&lt;/p&gt;
    &lt;p&gt;This is exactly what the grass-like arrows we’ve played with in the previous section represent â each one shows the average velocity of air particles in that local region of space. The big arrow we just saw in the middle of the swarm in the averaging red box is equivalent to each of the arrows seen below:&lt;/p&gt;
    &lt;p&gt;Naturally, the averaging box needs to be large enough to avoid the jitteriness related to aggregation of too few particles, but at any scale that we could care about the noisy readout completely disappears.&lt;/p&gt;
    &lt;p&gt;The average motion of particles is very different than the motion of each individual molecule. Even in very fast flows, many of the molecules move in the opposite direction than what the arrow indicates, but if we tally up all the particle motion, the air as a whole does make forward progress in the direction of velocity.&lt;/p&gt;
    &lt;p&gt;Up to this point, we’ve mostly looked at the flow of air by looking at wind and the way it moves through space, but what we consider a motion of air is relative. Let’s see how, by merely changing the point of view, we can create a motion of air in otherwise windless conditions.&lt;/p&gt;
    &lt;head rend="h1"&gt;Relative Velocity&lt;/head&gt;
    &lt;p&gt;Let’s zoom away from the world of microscopic particles to look at the motion of larger bodies. In the demonstration below, you can see two different views of the same car driving in the left direction. In the top part, the camera stays firmly on the ground, but in the bottom part, the camera tracks the motion of the vehicle. If needed, you can restart the scene with the button in the bottom left corner or tweak the speed of time with the slider:&lt;/p&gt;
    &lt;p&gt;These two views show the exact same scene â we’re just changing what the camera is focusing on. As seen in the top part, from the perspective of the static camera, it’s only the car that has some velocity in the left direction.&lt;/p&gt;
    &lt;p&gt;On the other hand, from the perspective of the camera focused on the vehicle, the car doesn’t move, but everything else does. The poles and road markings all move to the right with a speed equal to that of the car. This shouldn’t come as a surprise from daily experience in any form of transportation â when you’re sitting in a moving vehicle, static things in the surrounding environment seem to move towards and past you.&lt;/p&gt;
    &lt;p&gt;The very same rules apply to any region of air â I’ve outlined some of them with dashed boxes up in the sky. For the observer on the ground that air is still, but from the car’s perspective, that air is moving.&lt;/p&gt;
    &lt;p&gt;With that in mind, let’s see the same scene, but this time I’ll add the familiar small arrows showing the air’s velocity as “seen” by the camera:&lt;/p&gt;
    &lt;p&gt;From the point of view of the car, as seen in the bottom view, the air is moving to the right, as if there was some wind blowing right at the vehicle. You’ve probably felt this many times by sticking your hand out the window â it feels no different than if you were standing still on the ground with the wind hitting your fingers.&lt;/p&gt;
    &lt;p&gt;In fact, there is absolutely no difference between “regular” wind and wind experienced by the car or your hand sticking out the window â both are simply a motion of air relative to some object. This means that we can use our arrows to represent any motion of air, as long as we note what that motion is relative to.&lt;/p&gt;
    &lt;p&gt;You may have also noticed that the moving car affects the motion of air in its vicinity. Let me bring up the previous demonstration one more time:&lt;/p&gt;
    &lt;p&gt;In the top view, we can see how the front of the vehicle pushes the air forward, and how the air “bends” and speeds up around the shape of the car to roughly follow its shape, only to end up circling right behind the machine.&lt;/p&gt;
    &lt;p&gt;The same effects are seen in the bottom view â they’re just experienced differently. For example, the air right in front of the car slows down, while the air on top moves even faster than the rest of the undisturbed, distant air.&lt;/p&gt;
    &lt;p&gt;We’ll soon explore why the air behaves this way when flowing around an object, but for now let’s raise above the ground to see the motion of an airplane flying in the sky. We’ll use the familiar setup of a camera kept steady relative the ground, as seen in the top part, and a camera that follows the airplane, seen in the bottom part:&lt;/p&gt;
    &lt;p&gt;Before we continue, notice that it’s getting a little hard to pay close attention to what happens to the moving objects in the ground-fixed camera view â the bodies quickly leave the field of view of the demonstrations. For the rest of this article I’ll stick to the camera style seen in the bottom part of the demonstration â this will let us directly track the interaction between the object and the air that flows around that object.&lt;/p&gt;
    &lt;p&gt;From the point of view of the airplane, it also experiences a flow of incoming air as seen by the air “boxes” approaching the plane, which is very similar to the car example. What’s completely different from the car example is the fact that the airplane somehow stays suspended in the air, despite gravity pulling it down towards the ground. This means that there must be some other force acting on it to prevent the plane from falling from the sky.&lt;/p&gt;
    &lt;p&gt;Let’s compare these two vehicles by looking at the basic forces affecting their motion, starting with the diagram of forces acting on the car:&lt;/p&gt;
    &lt;p&gt;The down-pulling gravity force is counteracted by the reaction forces from the ground â they act through the car’s tires to prevent the car from sinking. The air drag and other forms of resistance push the car back, but the car’s tires powered by the engine keep propelling the car forward.&lt;/p&gt;
    &lt;p&gt;In my previous article I presented a more elaborate description of the interplay between forces and objects, but to briefly recap here, if forces acting on an object are balanced, then that object will maintain its current velocity.&lt;/p&gt;
    &lt;p&gt;All forces on the car are balanced and the vehicle moves forward with constant speed, and it doesn’t move at all in the up or down direction â the object’s velocity is indeed constant.&lt;/p&gt;
    &lt;p&gt;Let’s draw a similar diagram of forces for the flying plane:&lt;/p&gt;
    &lt;p&gt;We still have the air drag that pushes the vehicle back, and the plane’s propeller powered by the engine keeps pushing it forward. As a result the plane moves forward with constant speed.&lt;/p&gt;
    &lt;p&gt;We also have the down-pulling gravity. This time, however, that gravity is not countered by the reaction forces from the ground, but instead it’s balanced by lift, a force that pushes the plane up. When gravity and lift are equalized, the plane doesn’t move up or down either.&lt;/p&gt;
    &lt;p&gt;Airplanes create most of their lift with wings, which are carefully designed to generate that force. While length, area, and the overall geometry of the wings are very important, in this article we’ll focus on the shape of the cross-section of a wing which I highlighted below in yellow:&lt;/p&gt;
    &lt;p&gt;This is an airfoil, the protagonist of this article. This airfoil has a smooth, rounded front and a sharp trailing edge. Let’s take a closer look at the flow of air around this airfoil using the grass-like arrows that show the velocity of air at that location:&lt;/p&gt;
    &lt;p&gt;These arrows paint an interesting picture, but in the demonstration below I’ve also added the little leaf-like markers that track the motion of air parcels in the flow. IÂ steadily release a whole line of them from the left side, but you can also clicktap anywhere in the flow to drop a marker at that location. You can do this in any demonstration that has a little hand symbol in the bottom right corner:&lt;/p&gt;
    &lt;p&gt;The markers show that the flow splits ahead of the airfoil, then it gently changes direction to glide above and below the shape. Moreover, the markers right in front of the airfoil gradually slow down and lag behind their neighbors. The air somehow senses the presence of the body.&lt;/p&gt;
    &lt;p&gt;It may be hard to see, but the top and bottom sections of this airfoil aren’t symmetric. This asymmetric design is very important, but right now it will needlessly complicate our discussion on how the flow around this shape arises.&lt;/p&gt;
    &lt;p&gt;To simplify things a little, let’s use a less complicated shape of a symmetric airfoil â you can see it in the demonstration below. I overlay the previous asymmetric shape with a dashed outline to show the difference between the two:&lt;/p&gt;
    &lt;p&gt;The motion of air around this airfoil is very similar â the flow changes its direction and speed when it passes around an object. Until now we’ve simply been observing that the flow changes to adapt to the shape of the body, but it’s finally time to understand why it happens. To explain that behavior we need to go back to the world of air particles to discuss the concept of pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Pressure&lt;/head&gt;
    &lt;p&gt;As we’ve discussed, even in the seemingly steady conditions the particles of air are zipping around at high speeds colliding with each other at an incredible rate. The surface of any object placed in the air will also experience these bounces.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, you can see air particles bombarding a small box. Every time a collision happens I briefly mark it with a dark spot on the surface of that cube:&lt;/p&gt;
    &lt;p&gt;To understand the implications of these collisions, let’s first take a look at objects with more ordinary sizes. In the demonstration below, tennis balls are hitting a large cardboard box from the left and right side. By dragging the slider you can change the intensity of both streams of balls:&lt;/p&gt;
    &lt;p&gt;When a tennis ball hits the box, the collision imparts some force on it, causing the box to move. However, in this simulation the collisions from all the balls on each side balance each other out, so the box doesn’t make any consistent progress in either direction.&lt;/p&gt;
    &lt;p&gt;In real air, the situation is similar, but at vastly different scales. The mass of each particle constituting air is absolutely miniscule, so the impact of an individual collision on any object of meaningful size is completely imperceptible.&lt;/p&gt;
    &lt;p&gt;Moreover, each air particle hitting an object has a different speed, and it strikes the surface of that object at a different angle â some hit the object straight on, but some barely graze it. Due to the enormous number of these collisions happening at every instant of time, all these variations average out, and even a small section of surface of any body experiences uniform bombardment.&lt;/p&gt;
    &lt;p&gt;In aggregate, we say that the air exerts pressure on any object present in that air. The magnitude of this pressure depends on the intensity of these collisions across an area.&lt;/p&gt;
    &lt;p&gt;Let’s see how this pressure manifests on our tiny cube. In the demonstration below, you can use the second slider to control the number of air molecules present in this volume:&lt;/p&gt;
    &lt;p&gt;The black arrows you see on the sides of the cube symbolize the magnitude of pressure on these walls. As we uniformly increase the number of particles in this volume, the intensity of collisions, and thus the pressure, also increases. Because the collisions happen at more or less the same rate on every side of the box, the net balance of forces is also maintained and the cube doesn’t move, regardless of how big or small the overall pressure is.&lt;/p&gt;
    &lt;p&gt;This is exactly what happens in the Earth’s atmosphere â everything is constantly squeezed by relatively high pressure caused by the barrage of countless air particles. That pressure is typically balanced either by an object’s material, which resists compression like a spring, or by the air itself that fills the insides of the object. When that inner air is removed, the seemingly innocuous atmospheric pressure reveals its might.&lt;/p&gt;
    &lt;p&gt;The underlying particle nature also shows us that pressure is never negative. Without any particle collisions, we reach the lowest possible pressure of zero. Beyond that, any impacts on the surface of an object create some amount of positive pressure.&lt;/p&gt;
    &lt;p&gt;In the demonstrations we’ve seen so far, the balanced number of collisions on each wall was very important for keeping the objects steady. Unsurprisingly, more interesting things happen when this harmony isn’t maintained. Let’s first investigate this scenario using the tennis balls. In the demonstration below, the slider controls if it’s the left side or the right side that’s shooting more balls:&lt;/p&gt;
    &lt;p&gt;As you can see, if one of the sides has a higher number of collisions, the forces acting on the box are no longer balanced and the box starts to move.&lt;/p&gt;
    &lt;p&gt;The very same situation happens in air, which you can witness in the simulation below. Notice that the volume in which the tiny cube exists has more particles on one side than the other. Observe what happens to cube once you let the time run using the slider:&lt;/p&gt;
    &lt;p&gt;The higher number of particle collisions on one side of the cube creates higher pressure forces on that wall. The uneven forces end up pushing the block to the side. In this demonstration, the pressure re-balances after a while and the cube stops moving.&lt;/p&gt;
    &lt;p&gt;Intuitively, the air exerts an imbalanced net force on the cube only when different parts of that object experience different pressure â it’s the spatial variation in pressure that creates an acting net force. When the difference in pressure between any two points increases, the net force acting on the object also grows.&lt;/p&gt;
    &lt;p&gt;It’s easy to see that a larger number of collisions on the left side of an object would start to exert a net force pushing that object to the right, but, perhaps surprisingly, the same rules apply to any chunk of air itself.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, I once again made one half of the test volume contain more particles than the other half. As you unpause the demonstration, observe the average velocity of molecules in the marked out section of air:&lt;/p&gt;
    &lt;p&gt;The particles on the more occupied side can easily travel to the less crowded side, because there are fewer particles there to collide with and bounce back from. Additionally, each particle in the less populated section is more likely to hit a particle in the more populated section, which will typically cause that particle from the desolate side to bounce back where it came from.&lt;/p&gt;
    &lt;p&gt;The particles end up, on average, traveling from the area of high pressure to the area of lower pressure. Even though we don’t have any clean borders between different sections, we can still see the bulk of particles getting accelerated towards the less dense section.&lt;/p&gt;
    &lt;p&gt;Once again, the initial pressure differences in the test volume dissipate after a while. On their own, these freely suspended pressure variations quickly disappear, but we will soon see how, with the aid of airflow, these areas of different pressure can be sustained indefinitely.&lt;/p&gt;
    &lt;p&gt;In the examples we’ve been playing with, the notion of increased pressure came from an increased number of collisions, which in turn came from an increased number of particles in the area. This shows that, all other things being equal, pressure is tied to the local density of the air, which was very easy to perceive in an increased concentration of molecules.&lt;/p&gt;
    &lt;p&gt;However, the pressure can also grow due to increased average speed of the particles, which in turn comes with increased temperature. As particles get faster, each collision gets more impactful and it pushes on an object or other particles a bit harder, causing the overall pressure to also increase. In the demonstration below, we can simulate this with tennis balls hitting the cardboard box at the same rate, but with different speeds, which you can control with the slider:&lt;/p&gt;
    &lt;p&gt;As we make the balls on one side of the box faster, their impacts also become stronger and the package starts moving to the right, even though the number of collisions per second is equal on both sides.&lt;/p&gt;
    &lt;p&gt;The important point from these discussions is that air pressure exerts force on everything inside it, be it a solid object or any parcel of air. It’s a little unintuitive that the air itself both exerts the pressure and it also “feels” the pressure, but it’s all just a consequence of very rapid motions of particles and the collisions between them happening at an enormous rate.&lt;/p&gt;
    &lt;p&gt;Recall that even in small volumes of air there are billions of billions of particles, and each particle experiences roughly ten billion collisions per second. What we’ve simulated at a micro scale and in slow motion as countable, individual interactions, very quickly smooths out into a uniform and uninterrupted notion of force-exerting pressure.&lt;/p&gt;
    &lt;p&gt;This fact lets us abandon the molecules and their collisions yet again. It’s not a big loss, since counting the number and intensity of collisions was never convenient in the first place, but we can now investigate some other ways of visualizing pressure in a region of air.&lt;/p&gt;
    &lt;head rend="h1"&gt;Visualizing Pressure&lt;/head&gt;
    &lt;p&gt;As we’ve seen in the particle simulations, pressure can vary from place to place. One of the most convenient ways to express this variation is to use colors of different intensities. Let’s see how that simple approach could work here. In the demonstration below, the dashed circles represent regions of high and low pressure â you can drag them around to change their position:&lt;/p&gt;
    &lt;p&gt;This map of pressure is colored with varying shades of red as indicated by the scale below â the redder the color, the higher the pressure. The small triangle â¼ in the middle of the scale indicates the location of the base, static pressure present in the atmosphere.&lt;/p&gt;
    &lt;p&gt;In this simulation we have complete control over where the different locations of lower and higher pressure are. To make things more interesting, each draggable pressure circle has a different strength and range. You can infer this variation from color changes around these points.&lt;/p&gt;
    &lt;p&gt;Let’s put an airfoil in this area to see how it’s affected by the pressure of the surrounding air. The arrows seen below symbolize the force that pressure exerts on the surface of the airfoil at that location. They’re the exact same arrows that we’ve seen acting on the walls of the tiny yellow cube, here we just see them at a larger scale:&lt;/p&gt;
    &lt;p&gt;As you move around the locations of lower and higher pressure, the forces acting on the surface of the airfoil also change, matching what we’ve seen with little cubes bombarded by air particles. The static pressure always exerts some base load, but in the areas of higher pressure the surface forces are higher, and in the areas of lower pressure the surface forces are lower than these base forces.&lt;/p&gt;
    &lt;p&gt;Note that you can also move the pressure circles into the airfoil, but it only serves as a convenience to let you customize the shape of the air pressure field around that body â we don’t particularly care about the pressure inside the solid itself.&lt;/p&gt;
    &lt;p&gt;When we tally up all the pressure forces acting on each piece of the airfoil’s surface, we end up with the net force acting on that object. In the demonstration below, I’m showing it with the big arrow at the center of the airfoil:&lt;/p&gt;
    &lt;p&gt;By changing the distribution of pressure around the airfoil, we can affect the total force that this object feels.&lt;/p&gt;
    &lt;p&gt;The reddish plots we’ve been looking at are correct, but a little inconvenient. Recall that final net force on the object depends only on the differences of pressure â when we uniformly increased the number of collisions on the walls of the tiny cube, it steadily remained in place.&lt;/p&gt;
    &lt;p&gt;This means that the static background pressure doesn’t matter for the cumulative forces acting on an object. It’s only the differences relative to that static pressure that affect the overall balance. This lets us overhaul our visual representation of pressure â we can use no color where the pressure has the static value, use blue color when the pressure is lower than the static pressure, and use red color when the pressure is higher than the static pressure:&lt;/p&gt;
    &lt;p&gt;This is the exact same distribution of pressure that we’ve just seen. All the pressure demos in this section are connected, and here we simply changed the reference point against which we present the pressure variation.&lt;/p&gt;
    &lt;p&gt;If we then throw in the airfoil back into the mix we can now also adjust the arrows representing the forces that the pressure exerts on the surface of that object:&lt;/p&gt;
    &lt;p&gt;The areas of higher pressure still seem to push on the surface of the airfoil, but the areas of lower pressure now seem to pull it. However, I need to emphasize once more that pressure always pushes on the object, and we can only talk about a pulling force when we discard that uniform, pushing contribution coming from the static pressure. In those “pulling” areas the pressure is still pushing, it just pushes less intensely.&lt;/p&gt;
    &lt;p&gt;I will also use the convenient terms of positive and negative pressure, but remember that this refers to their difference from the static pressure. The phrase “pressure lower than static pressure” is a mouthful, so the expression “negative pressure” is very handy, even when it hides the fact that pressure is always positive.&lt;/p&gt;
    &lt;p&gt;While the color variations used here show the true nature of the smoothly varying pressure changes, they make it a little hard to see how quickly those changes happen. To fix this, I’ll also draw the contour lines that join the locations of the same pressure â they’re very similar to lines of the same altitude you may have seen on maps:&lt;/p&gt;
    &lt;p&gt;Every point on one of those contour lines has the same value of pressure, and each subsequent line is drawn at the same increment of pressure â you can see this in the scale placed below the plot. This means that the closer the lines are together, the more quickly the pressure changes in that area.&lt;/p&gt;
    &lt;p&gt;The mathematical concept that describes the direction and rapidness of these changes is known as a gradient. Informally, gradient describes how some property changes from one point to another, and, thankfully, this notion tracks closely with how this word is used in graphic design to describe smooth color changes. Wherever you see a color gradient , this also implies that there is a pressure gradient â the pressure changes from place to place.&lt;/p&gt;
    &lt;p&gt;This spatial variation is particularly important for the motion of air. Recall that the air pressure differences don’t just exert forces on solid objects, but also on the air itself â any small parcel of air is subject to the same whims of pressure forces.&lt;/p&gt;
    &lt;p&gt;Those spatial variations in pressure end up pushing the air around, changing its velocity. Let’s see this in action using the little leaf-like markers that are moved around by pressure differences. In the demonstration below, I’m steadily releasing the markers from the left side âÂ notice how their trajectory changes when you modify the pressure field:&lt;/p&gt;
    &lt;p&gt;You may still find it a little difficult to grasp how pressure differences affect the motion of a parcel of air. Luckily, we can draw parallels between the contour lines of pressure seen on these pressure maps and the contour lines of elevation seen on traditional maps. This lets us build a little pressure-landscape analogy.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, the very same distribution of pressure is expressed as a mountainy landscape. Positive pressure lifts the ground above the base level and negative pressure depresses it below the base level. A parcel of air moves like a marble that loses speed when climbing uphill and accelerates when rolling downhill. You can drag the demo around to change the viewing angle:&lt;/p&gt;
    &lt;p&gt;Notice that when the pressure changes more rapidly and the contour lines are closer, the steepness of the corresponding hill or valley also increases, and so do the forces acting on a parcel of air. If the pressure is increasing by a large amount, it may even make the marker go back. This landscape analogy also shows that the static pressure doesn’t matter for the motion of air parcels, as any changes in static pressure would just lift all the areas by the same amount without changing their steepness.&lt;/p&gt;
    &lt;p&gt;When watching these air parcels move around, you may have noticed that things were a little bit off. For example, it’s possible for air parcels coming from different directions to arrive at the same location, and then continue to travel in different directions. You can see an example of that on the left side of the demonstration below, with the slider letting you scrub back and forth in time:&lt;/p&gt;
    &lt;p&gt;Recall that the markers always follow the local velocity of air, so the motion seen in the left part implies that the air at the location of the meetup of the two markers has two different velocities at the same time, which is not realistic.&lt;/p&gt;
    &lt;p&gt;It’s worth pointing out that the situation seen on right side, where one marker merely intersects the historical path of the other, can be realistic, as long as we’re dealing with an unsteady flow, where the velocity of the air at the crossing location has changed since the first marker was there. For steady conditions in which no changes occur over time, the scenario seen on the right is also not physically correct.&lt;/p&gt;
    &lt;p&gt;We’ll look at some unsteady flows later in the article, but for now we’re interested in steady conditions so the crossing paths of our markers indicate implausible velocities. Even more dubious result happen when we simulate the motion of these markers with an airfoil present in the flow:&lt;/p&gt;
    &lt;p&gt;For most distributions of pressure, the air markers will flow right through the body. This is clearly wrong! The demonstrations we’ve seen so far correctly represent what would happen to individual air parcels and bodies placed in these pressure fields, but those pressure fields themselves were completely made up and didn’t correspond to any physical reality. Our mistake was that we completely ignored any interactions between the pressure of the air and the motion of that air.&lt;/p&gt;
    &lt;p&gt;The flow of air, the pressure of air, and the shape of the objects placed in that air are all tied together â for a given incoming flow speed and the shape of the object, we can’t just arbitrarily arrange the pressure field like we did in our artificial demonstrations. Instead, that pressure field will arise on its own.&lt;/p&gt;
    &lt;p&gt;Let’s see a real distribution of pressure around this airfoil and witness how it affects the motion of air parcels around it:&lt;/p&gt;
    &lt;p&gt;The behavior of air parcels now matches our intuitive expectations â the markers don’t go through the body, and in these steady conditions they also don’t cross paths.&lt;/p&gt;
    &lt;p&gt;We’re now one step closer to understanding how the flow of air takes its shape to move around an airfoil â it’s the pressure differences that cause the flow to change its direction and speed.&lt;/p&gt;
    &lt;p&gt;The pressure field we’ve just seen clearly works â regions of lower and higher pressure guide the air around the airfoil. However, it’s still unclear how these areas emerged in the first place. Let’s try to follow nature’s path to see how this pressure distribution is created and sustained in a flow.&lt;/p&gt;
    &lt;head rend="h1"&gt;Airfoil Flow&lt;/head&gt;
    &lt;p&gt;Before we start building the correct pressure field from scratch, let’s first establish two guiding principles that the flow around any object has to follow.&lt;/p&gt;
    &lt;p&gt;Firstly, the air can’t penetrate solid walls. A valid pressure field should either completely stop the flow at the surface of the object, or redirect that flow to make it travel in the direction perpendicular to the walls. This means that the markers that we track can never get inside the object.&lt;/p&gt;
    &lt;p&gt;Secondly, we also have the restrictions on the relative motion of the markers. For now we’ll only be interested in steady conditions, which means that the markers can’t cross their paths â we expect the ghostly historical trails to never intersect.&lt;/p&gt;
    &lt;p&gt;Let’s first focus on the pressure field in front of the airfoil. In the demonstration below, I created an artificial pressure field in that frontal region, you can control it using the slider:&lt;/p&gt;
    &lt;p&gt;It should quickly become clear that to prevent the approaching air from getting into the object, the pressure in the frontal region has to be positive, so that it pushes the incoming air away.&lt;/p&gt;
    &lt;p&gt;If that positive pressure in front is too low the air can still erroneously flow through the object. If that pressure is too high, the air parcels arriving at the airfoil will turn back and incorrectly cross paths with the incoming air. When the pressure is just right, the air parcels don’t go through the wall, and, at least in front of the object, they also don’t cross their paths.&lt;/p&gt;
    &lt;p&gt;The faster the incoming flow, the higher the pushing force required to slow down and redirect the incoming air. In the demonstration below, you can also control the speed of that incoming air using the second slider:&lt;/p&gt;
    &lt;p&gt;While for slow flows, only a small amount of positive pressure is enough to stop the incoming air, for fast flows, the pressure in front of the airfoil has to become much higher.&lt;/p&gt;
    &lt;p&gt;The pressure needed to stop air at a given velocity is known as stagnation pressure and it’s proportional to the square of that velocity â twice as high speed requires four times larger pressure. Naturally, when there is no flow, no pressure is required as the air no longer tries to flow through the object.&lt;/p&gt;
    &lt;p&gt;In the previous two demonstrations, we manually adjusted the pressure to get the correct result, but in nature this process happens on its own â it’s the flow itself that creates this region of increased pressure in front of the object.&lt;/p&gt;
    &lt;p&gt;As the incoming parcels of air arrive at the surface of the airfoil, they can’t continue going forward, but air parcels from further up ahead continuously want to keep flowing into this region. This compresses the air close to the object, which causes the pressure in front to increase, which then helps to slow down the incoming flow.&lt;/p&gt;
    &lt;p&gt;This mechanism is self-balancing â if the pressure is too low to push away the incoming air parcels, the air parcels will compact the existing air more, causing an increase in pressure. If the pressure is too high, it will easily push the incoming air away, which relieves the frontal area, causing the pressure to decrease. Any fluctuations quickly settle to an equilibrium that balances the pressure in the entire frontal region.&lt;/p&gt;
    &lt;p&gt;Let’s look at the distribution of the positive frontal pressure once more:&lt;/p&gt;
    &lt;p&gt;Notice that the positive pressure isn’t limited to just the close vicinity of the airfoil, but it spreads out much further ahead to gradually reach the value of the static pressure, far away from the airfoil itself.&lt;/p&gt;
    &lt;p&gt;All in all, we have a large area of increasing pressure that starts far away from the body and ends at its surface. Those pressure differences create a pressure “hill” that not only gradually slows the incoming air down, but it also redirects that air to flow around the object.&lt;/p&gt;
    &lt;p&gt;It seems that with our frontal pressure field we’ve easily completed our goal of preventing the air from flowing through the walls of the body. However, our second guideline of non-crossing marker paths is still not fulfilled â this condition is broken above and below the airfoil.&lt;/p&gt;
    &lt;p&gt;Let’s first try to rectify this manually. In the demonstration below, you can control the pressure in these two regions using the slider:&lt;/p&gt;
    &lt;p&gt;While positive values of pressure in those zones make the problem worse, negative values get us much closer to the expected behavior â in the top and bottom areas the markers no longer veer off into different directions. However, that pressure can’t be too low, otherwise it will pull the markers back into the body.&lt;/p&gt;
    &lt;p&gt;In real flow, these regions of lower pressure arise on their own, but the explanation for this phenomenon is a little less straightforward than what I’ve described for the area of positive pressure in the frontal region. We can get some, albeit a bit hand-wavy, understanding by observing what happens to the air markers when those negative regions are missing.&lt;/p&gt;
    &lt;p&gt;In that scenario, the incoming air parcels no longer reach those areas above and below the airfoil, causing some local depletion of air that has since left those zones. This decreases the pressure in those regions, and that lower pressure attracts the surrounding air to flow into those less occupied spaces.&lt;/p&gt;
    &lt;p&gt;If that lower pressure is too negative, more air will come in and the pressure will rise. If the pressure is not negative enough, those region will get depleted again. Once again, it’s the flow itself that creates the balancing system â without the flow no pressure differences would arise.&lt;/p&gt;
    &lt;p&gt;As we’ll see later on, in more extreme scenarios that negative pressure can alter the flow more dramatically, and the regions of “missing” air get filled through other means, but for now let’s close things up by tweaking the pressure in the rear part of the airfoil:&lt;/p&gt;
    &lt;p&gt;Some amount of positive pressure in the rear prevents the air parcels from smashing into each other after leaving the airfoil. Intuitively, this pressure arises naturally from the flow, because as the air slides off from the ends of the top and bottom sides, it all arrives into the same region, creating some compression.&lt;/p&gt;
    &lt;p&gt;If that compressive pressure in the rear is too low, more air will manage to get in, which will further increase the pressure. If that pressure is too high, it will push the incoming air away, which depletes the area and the pressure decreases. The system balances itself yet again.&lt;/p&gt;
    &lt;p&gt;The quite informal description of these balances that I’ve presented can be formalized mathematically using the NavierâStokes equations. These equations describe the motion of liquids and gasses, collectively known as fluids, subject to various forces like gravity, or, most importantly for us, pressure.&lt;/p&gt;
    &lt;p&gt;NavierâStokes equations are notoriously difficult to solve analytically, but a lot of insight about the behavior of fluids can be gained with computer simulations with various degrees of complexity.&lt;/p&gt;
    &lt;p&gt;In this article, I’m also employing simulations to investigate the flow of air around objects. However, the computer models used here are quite simplified and they don’t reflect the full richness of physics involved in the motion of air. These slow-motion demonstrations are intended to present the broad strokes of the delicate interaction between the air and the airfoil, but I would advise against relying on them when building an airworthy airplane.&lt;/p&gt;
    &lt;p&gt;With all of these caveats in place, let’s get back to the pressure distribution around a symmetric airfoil. We’re done recreating the nature-made pressure field, but there is one small aspect that we haven’t yet accounted for.&lt;/p&gt;
    &lt;p&gt;For our experiments, I kept the pressure steady in time so that we could focus on its general outlines. In practice, a pressure field imposed by a fast flow around any object will experience some degree of instability, which you can see in the demonstration below. You can once more drop the markers at any location to track the flow in the area:&lt;/p&gt;
    &lt;p&gt;As the pressure builds up on one side, it redirects the flow, which changes the pressure again. The pressure ends up oscillating back and forth like a swing. The pressure distribution and the flow direction are once again at the mercy of their mutual balance, one affecting the other. We’ll soon see some other examples of these unstable behaviors.&lt;/p&gt;
    &lt;p&gt;As we’ve just seen, the variation in pressure doesn’t just happen in the close vicinity of the airfoil, but it stretches quite far away from the body itself. This means that the velocity of the flow is also affected quite far away from the shape.&lt;/p&gt;
    &lt;p&gt;However, when it comes to the forces exerted on the airfoil, it’s only the pressure right at the surface of the airfoil that matters. Let’s bring back the two tools we’ve used before: surface arrows that show how the air pushes or “pulls” on the airfoil, and the net force arrow that tallies up the net results of these forces:&lt;/p&gt;
    &lt;p&gt;As the pressure field fluctuates, the resulting net force also moves around. Let’s decompose this force into two different components, one perpendicular to the flow, and one parallel to it:&lt;/p&gt;
    &lt;p&gt;The force acting in the direction perpendicular to the flow is known as lift, and the one acting in the direction of the flow is known as pressure drag, or form drag. As the name implies, this component of drag is created by the distribution of pressure around the shape.&lt;/p&gt;
    &lt;p&gt;For this airfoil, the pressure drag is very tiny. While airfoils are specifically designed to minimize the overall drag, most of that force hindering their motion comes from another source â we’ll discuss it soon enough.&lt;/p&gt;
    &lt;p&gt;Notice that as this flow fluctuates, the lift force jumps around, but averaged over time the upward and downward swings of that force end up balancing each other. This airfoil in this configuration doesn’t generate any continuous lift.&lt;/p&gt;
    &lt;p&gt;This shouldn’t come as a surprise since this situation is completely symmetric, so the pressure forces on the upper and lower sides of the airfoil are, on average, completely balanced. However, there is an easy way to disturb that symmetry. In the demonstration below, we’re once again meeting the plain, symmetric airfoil, but this time we can gently tilt it using the slider:&lt;/p&gt;
    &lt;p&gt;The slider controls the so-called angle of attack, which is spanned between some reference line on the body, like the one joining the front and back, and the direction of the incoming flow. I’m showing this angle right in the middle of the airfoil.&lt;/p&gt;
    &lt;p&gt;As we change the angle of attack, the shape that the airflow “sees” is no longer symmetrical relative to the incoming direction of that flow. The velocity and pressure fields adapt in their mutual push and pull to form a new, asymmetric distribution. Notice that the stagnation point of high pressure has moved around, and the little markers that indicate the motion of air now travel on very different paths below and above and below the airfoil.&lt;/p&gt;
    &lt;p&gt;If we then put the pressure arrows back in, we can tally them all up to get the resulting lift and pressure drag. When compared to the previous simulation, I’m scaling down all the arrows to make them fit in the bounds of the demonstration:&lt;/p&gt;
    &lt;p&gt;When this symmetric airfoil is tilted up, the asymmetric pressure distribution generates a lift force that pushes the object up. Conversely, for a downward tilted airfoil, the pressure forces push the airfoil down.&lt;/p&gt;
    &lt;p&gt;Naturally, we’re typically interested in upward-pointing forces, and when the lift generated by the wings is equal to the weight of the plane, the plane will stay in the air without raising or falling to the ground â we’re finally flying.&lt;/p&gt;
    &lt;p&gt;Let’s plot the dependence between the lifting force and the angle of attack of an airfoil â you can see it in the right side of the demonstration below. Note that this plot presents time-averaged and settled values, so you may have to wait a little for the flow to normalize and the lift to start oscillating around the expected value:&lt;/p&gt;
    &lt;p&gt;Clearly, as the angle of attack increases, so does the generated lift. The same thing happens on the other end of the spectrum, where a more negative angle of attack creates more negative lift. Note that for this symmetric airfoil the positive and negative sides of the diagram are just mirror images of each other, so let’s focus only on positive angles of attack.&lt;/p&gt;
    &lt;p&gt;One could naively hope that we could keep increasing the angle of attack to generate more and more lift. Let’s see what happens in practice:&lt;/p&gt;
    &lt;p&gt;Initially, the lift force indeed keeps increasing with the angle of attack, but at some point it plateaus. Once that critical angle of attack is surpassed, the lift force starts to fall after the flow fully develops.&lt;/p&gt;
    &lt;p&gt;What we’re witnessing here is known as a stall. The onset of a stall imposes limits on how much lift the wings of an airplane can generate from merely increasing the angle of attack.&lt;/p&gt;
    &lt;p&gt;Notice that when the stall happens, the pressure distribution on the upper part of the airfoil becomes very erratic â it’s not only the surface pressure arrows that are changing rapidly, but the whole pressure field in that area is very disturbed.&lt;/p&gt;
    &lt;p&gt;Let’s bring in the velocity arrows and markers to get a better feel on what’s going on in that region:&lt;/p&gt;
    &lt;p&gt;At high angles of attack, the flow above the upper part of the airfoil becomes very complicated. If you clicktap in that region to drop a few markers, you’ll notice that the air is trapped in various swirling eddies that are eventually shed to fly away with rest of the flow.&lt;/p&gt;
    &lt;p&gt;We’re witnessing flow separation, where the main part of the flow detaches from the surface and doesn’t follow its shape anymore. The interactions in the complicated flow right above the airfoil affect the pressure field, which then decreases lift.&lt;/p&gt;
    &lt;p&gt;There is a lot going on there, but to understand how these effects arise we have to talk about a property that affects the flow of every fluid: viscosity.&lt;/p&gt;
    &lt;head rend="h1"&gt;Viscosity&lt;/head&gt;
    &lt;p&gt;You might have heard the term viscosity used to describe “thickness” of different liquids, with a classic example that contrasts the slowness of the flow of honey to the rapidness of the flow of water.&lt;/p&gt;
    &lt;p&gt;Viscosity is also a property of gasses like air, but before I describe this concept more formally, we’ll first build an intuitive understanding of what viscosity is and what it does to the flow of different fluids.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, the fluid flows in from the left side, but note that the flow in the top half is faster than the flow in the bottom half, which is reflected by the different lengths of the arrows. Dragging the slider to the left decreases the viscosity of the fluid, and dragging the slider to the right increases viscosity:&lt;/p&gt;
    &lt;p&gt;While we can see some changes to the arrows as we move the slider around, you probably agree that, for this flow, the arrow-based visualization isn’t very rewarding. Let’s add the color-based visualization of speed distribution in this flow:&lt;/p&gt;
    &lt;p&gt;We can now see how viscosity blends the speed variation between different sections of the fluid. For highly viscous fluids, this mixing behavior spreads very easily and the initially distinct velocities of the two layers average out quite rapidly.&lt;/p&gt;
    &lt;p&gt;At lower viscosity these two layers with different speeds remain quite separated. If you make the viscosity low enough, you may even notice that, after a while, the flow develops some interesting wave-like phenomena â we’ll get back to these soon.&lt;/p&gt;
    &lt;p&gt;All this mixing behavior may remind you of a diffusion process, where some quantity, like temperature or concentration, evens out over time. Let’s see some basic diffusion in action. In the simulation below, I filled half of the bottle with with red-dyed water, while the other half is filled with blue-dyed water. The slider lets you control the speed of time:&lt;/p&gt;
    &lt;p&gt;As time passes, the sharp difference between the two layer blends more and more to eventually completely disappear. Clearly, there is some similarity between the diffusion of differently colored dyes and the averaging of velocity that we’ve seen in the earlier example.&lt;/p&gt;
    &lt;p&gt;In our flow demonstrations, viscosity seemed to have controlled the diffusion of velocity. To define it more precisely, viscosity controls the diffusion of momentum, which is a product of velocity and mass. The simplified fluids we’re looking at have more or less constant density, so each equally-sized parcel of those fluids has the same mass. Therefore, if it makes things easier for you, wherever you see the word momentum you can think of velocity, but in more complex scenarios these differences can matter.&lt;/p&gt;
    &lt;p&gt;Let me bring in the previous flow simulation one more time:&lt;/p&gt;
    &lt;p&gt;You’ve probably noticed that, as the flow moves to the right, the size of this blended region increases. When the regions of fluid with different momentums meet for the first time, they barely have any time to average out, and the blending is minimal. As time passes, these regions of fluid get to average out more, similarly to how two different layers of dyed water mix more over time.&lt;/p&gt;
    &lt;p&gt;However, as time is passing, these parcels also keep moving, and that stronger blending happens further to the right. The downstream regions had more time to mix and average out, so the visible thickness of the blended region on the right side is also larger.&lt;/p&gt;
    &lt;p&gt;With higher viscosity, the size of blended region grows much more quickly, which lets us be more precise about our working definition â viscosity controls the rate of the diffusion of momentum.&lt;/p&gt;
    &lt;p&gt;So far we’ve only observed flows with nicely separated horizontal layers, but viscosity averages momentum between any two regions of fluids. In the demonstration below, you can witness how viscosity affects a swirly motion of fluid in a vortex:&lt;/p&gt;
    &lt;p&gt;Notice that with high viscosity any differences in velocity are very quickly diluted out into nothing, but with low viscosity the revolving motion can survive for quite a while.&lt;/p&gt;
    &lt;p&gt;Viscosity has a damping or smoothing effect that makes it much harder to sustain any large variation in a velocity field. Let’s see how this affects the motion of objects in fluids of various viscosity. In the demonstration below, we’re tracking a velocity field close to a very thin plate put directly in the stream of an incoming fluid of adjustable viscosity:&lt;/p&gt;
    &lt;p&gt;With high viscosity, there is a large region of slow down around the plate that regains its speed fairly quickly behind the object. At lower viscosity that surrounding region is much smaller, but it extends much further behind the plate. For very low viscosity we’re once again seeing some more unusual behavior that we’ll get back to in a minute.&lt;/p&gt;
    &lt;p&gt;From the dark colors we can easily see that right by the surface of the plate the fluid doesn’t move at all â it sticks to that surface. This velocity difference between the halted flow at the wall and the moving outer flow gets smoothed out over time by viscosity, similar to how it blended in the flow between two different layers of fluid.&lt;/p&gt;
    &lt;p&gt;As before, with higher viscosity, the velocity averaging process becomes more rapid, and the blended region becomes more widespread. This averaging effect doesn’t just alter the velocity of fluid, but it also affects the plate. In some sense, the viscosity also wants to make the velocity of the surface of the plate to be more like the velocity of the surrounding flow.&lt;/p&gt;
    &lt;p&gt;The viscosity makes the flow want to pull the plate with it, which creates a shearing force that tries to slide the surface of this object away. The net effect is that that viscosity creates additional drag known as skin friction drag that wants to slow down any object moving in it.&lt;/p&gt;
    &lt;p&gt;All of these effects underline why highly viscous fluids are “thick”. Viscosity not only quickly averages any local differences in velocity, which prevents those fluids from flowing easily, but it also represses motion of objects in those fluids â you’ve likely experienced the difficulty of moving a spoon through a jar of honey.&lt;/p&gt;
    &lt;p&gt;The flow of any fluid exhibits tiny, random disturbances. In fluids with high viscosity, these variations are very quickly dispersed, so their motion is rarely erratic. Fluids with low viscosity aren’t as effective at damping motion, and these disturbances can grow to create oscillatory patterns. We’ve seen glimpses of them in the previous simulations, but here is another example:&lt;/p&gt;
    &lt;p&gt;At lower viscosity the flow becomes quite wave-y. Those instabilities happen at the border of regions of fluid with different velocities, like where the slow wake behind a plate is in contact with the fast external flow. In those regions, any tiny random intrusion of slower flow into the faster flow can get magnified and rolled over like a wave.&lt;/p&gt;
    &lt;p&gt;In our discussion of the motion of air around an airfoil, we’ve seen how the flow, the pressure field, and the shape of the body have effects on each other. These influences can be quite dynamic in nature, with distributions of velocity and pressure swinging back and forth in a never-ending fight for dominance.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, we can see a more dramatic example of these battles, where, depending on the viscosity, the flow around a gray cube can take many different forms:&lt;/p&gt;
    &lt;p&gt;With very high viscosity, the flow is completely stable, but as viscosity decreases, it starts to regularly oscillate from side to side, shedding vortices in the process. At very low viscosity, the motion becomes even more erratic.&lt;/p&gt;
    &lt;p&gt;While I can’t easily simulate it here, with further decrease in viscosity, the flow can develop full featured turbulence in which highly irregular and chaotic mixing motions occur at different scales. Turbulent flow stands in contrast to laminar flow, in which neighboring areas of fluid move in an orderly way past each other without any varying fluctuations.&lt;/p&gt;
    &lt;p&gt;Although we’ve put most of our focus on viscosity, which is often denoted with the Greek letter Î¼, the general behavior of the flow also depends on its velocity u, density Ï, and the size L of the body or container involved in the flow. These parameters are tied together by the Reynolds number Re:&lt;/p&gt;
    &lt;p&gt;Flows with the same Reynolds numbers exhibit similar behavior, which means that if we make the obstacle size L twice as large and we halve the speed of the flow u, the Reynolds number won’t change and neither will the characteristics of the flow â it will exhibit the same smooth or oscillatory motion.&lt;/p&gt;
    &lt;p&gt;The Reynolds number also “predicts” the onset of turbulence. When we increase the speed of the flow u, or decrease the viscosity Î¼, the Reynolds number rises. When it reaches a high enough value, turbulence is likely to occur.&lt;/p&gt;
    &lt;p&gt;Let’s quantify the difference in viscosity between different fluids. The precise values aren’t that important to us, but to briefly be a bit more formal, viscosity is expressed in units of pascal-seconds, or PaÂ·s. To let us use more manageable numbers, the following table uses millipascal-seconds, or mPaÂ·s:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;honey&lt;/cell&gt;
        &lt;cell&gt;~10000 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;olive oil&lt;/cell&gt;
        &lt;cell&gt;~100 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;water&lt;/cell&gt;
        &lt;cell&gt;1.0 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;air&lt;/cell&gt;
        &lt;cell&gt;0.018 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These values are measured at 68 Â°F20 Â°C, but many fluids like oil get much less viscous with increased temperature. As expected, honey is significantly more viscous than water. Compared to water, the viscosity of air is around 50 times less still, but even a very low viscosity has effects on flow and its interaction with solid walls.&lt;/p&gt;
    &lt;p&gt;To understand how viscosity arises in gasses like air, we have to once more get back to the world of particles. So far we’ve been watching them from a distance, with individual collisions barely perceptible in the moving swarm. This time we’re going take a closer look at these interactions.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, you can experience a simplified simulation of two molecules colliding in space. Each molecule represents nitrogen or oxygen â these two elements constitute the vast majority of air, and, in normal conditions, each one consists of two atoms.&lt;/p&gt;
    &lt;p&gt;You can drag the orange particle around, and once you let go I’ll automatically aim it so that it hits the blue particle. The speed of the orange molecule is four times larger than the speed of the blue one:&lt;/p&gt;
    &lt;p&gt;Notice that after the collision, it’s the orange molecule that’s slow, and it’s the blue one that’s fast. In this demonstration the two particles have the same mass and they collide straight on, so they simply end up trading velocities.&lt;/p&gt;
    &lt;p&gt;More generally, particles of different masses that strike each other at different angles will exchange some amount of momentum. Recall that the heavier the particle, or the faster it moves, the higher its momentum.&lt;/p&gt;
    &lt;p&gt;Let’s see how this behavior ends up affecting the average velocities of larger quantities of molecules. In the paused demonstration below, air molecules are grouped into two different parts. The air in the blue region has higher velocity than the air in the red region, which you can see in the black arrows showing the average velocity in those regions. Notice what happens to these averages as you let time flow by dragging the slider:&lt;/p&gt;
    &lt;p&gt;At the very beginning, the average velocities in these two sections are visibly different, but they quickly even out when fast particles from the blue region flow into the slower red region, and the slower particles from the red region move into the faster blue region, balancing the initial velocity differences.&lt;/p&gt;
    &lt;p&gt;Moreover, some of the faster particles collide with slower particles in the red region and some of the slower particles collide with faster particles from above. The faster particles lose some of their higher momentum, while the slower particles gain some of the momentum. All of these effects “dilute” some of those average velocity differences between the two regions.&lt;/p&gt;
    &lt;p&gt;You may also remember that when we observed a flow of fluid around a flat plate, that fluid wasn’t moving at all right on the surface of that plate, because it was stuck to it. Let’s see how this behavior may arise on a microscopic scale.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, we’re watching the familiar air particles right next to the surface of an object. To make tracking easier, I’m highlighting some of the particles in the vicinity of this surface:&lt;/p&gt;
    &lt;p&gt;When seen at a very large magnification, this surface, like almost all surfaces, isn’t perfectly smooth and has various peaks and valleys. The particles hitting these irregularities get bounced in more or less random directions. Some of the unlucky molecules can even get stuck for a while in these local crevices.&lt;/p&gt;
    &lt;p&gt;Close to the surface, the random collisions with peaks and valleys prevent the particles from making bulk progress in any direction. The average velocity of the air flow by the wall is more or less zero. Some molecular interactions between the particles and the surface can also prevent the fluid from moving.&lt;/p&gt;
    &lt;p&gt;This sticking behavior is known as the noâslip condition and it holds true for most typical flows of fluids that we experience day to day. It’s only in extreme conditions of very rarified gasses in the upper parts of the atmosphere or flows in microscopic capillaries that can break this assumption.&lt;/p&gt;
    &lt;p&gt;Let’s leave the world of particles behind for the last time and see how these two effects play an important role of influencing the airflow close to the surface of any object.&lt;/p&gt;
    &lt;head rend="h1"&gt;Boundary Layer&lt;/head&gt;
    &lt;p&gt;Let’s take another look at a thin plate placed in the stream of incoming fluid:&lt;/p&gt;
    &lt;p&gt;From this broader perspective, it’s hard to see how the flow interacts with the surface of that plate, because the effects of viscosity are limited to the region close to that surface. Let’s focus our attention on the small area that I’ve outlined with a dashed line, right in the top part of the plate. Here it is zoomed up close:&lt;/p&gt;
    &lt;p&gt;We can once more see that, due to the no-slip condition, the velocity is zero at the wall, and then it grows to meet the velocity of the flow further away from the surface itself. What we’re seeing here is known as the boundary layer, which spans the region between the surface of the object and the “outer” flow, which is mostly unaffected by the presence of the object.&lt;/p&gt;
    &lt;p&gt;Because the velocity in the boundary layer smoothly approaches the speed of the outer flow, it doesn’t have a well-defined end point. One of the choices is to agree that the boundary layer ends where the speed reaches 99% of the speed of the surrounding flow far away from the solid surface. Let me visualize this boundary in the flow using a dashed line:&lt;/p&gt;
    &lt;p&gt;As we move with the flow along the distance of the plate, the viscosity keeps averaging out the velocity differences, making the boundary layer thicker â this is similar to what we’ve seen at larger scales with highly viscous flows around objects.&lt;/p&gt;
    &lt;p&gt;Let’s quantify the distribution of speed in the boundary layer a little more precisely. In the demonstration below, I put the velocity arrows back in. I then connected the ends of these arrows with a thin line to show a profile of velocity at that location along the surface:&lt;/p&gt;
    &lt;p&gt;Notice that, initially, the velocity close to the wall increases almost linearly, but then it smoothly tapers to reach the speed of the external flow. The velocity profile close to the surface has a certain steepness, which I’m showing with the white dotted line. This line determines the amount of skin friction drag at that spot â the closer to the surface, or more horizontal, the line is, the higher the skin drag.&lt;/p&gt;
    &lt;p&gt;As the differences in velocity become less severe, the force with which viscosity wants to drag the surface with the flow also decreases. In the conditions present in the demonstration, the skin friction drag decreases over distance.&lt;/p&gt;
    &lt;p&gt;At this point you hopefully have an intuitive grasp of how viscosity affects the flow close to the surface of the object. From our earlier discussion, you may also remember that pressure differences also affect how the flow behaves, with parcels of air slowing down when climbing the hill of increasing pressure and accelerating on the downhill of the decreasing pressure.&lt;/p&gt;
    &lt;p&gt;In the boundary layer flows we played with, the pressure distribution was more or less constant in the investigated region. Let’s see how the flow changes when we vary that pressure.&lt;/p&gt;
    &lt;p&gt;In the top part of the demonstration below we see the exact same view of velocity we’ve experimented with so far. In the bottom part of the demonstration below you can see the pressure distribution in the boundary layer, which you can change using the slider below.&lt;/p&gt;
    &lt;p&gt;If the pressure decreases in the direction of the flow in the boundary layer, we say that the pressure gradient is favorable. Favorable pressure gradient accelerates the air, and the boundary layer doesn’t grow as quickly, since the slowdown caused by viscosity is opposed by that acceleration.&lt;/p&gt;
    &lt;p&gt;When the pressure increases in the direction of the flow, we say that the pressure gradient is adverse. Adverse pressure gradient pushes against the direction of motion of the air. Far away from the surface, the air has enough momentum that the adverse pressure merely slows the flow down. However, close to the surface, the flow in the boundary layer was slow in the first place, so a pushing adverse pressure gradient may even reverse the direction of the flow.&lt;/p&gt;
    &lt;p&gt;When the flow in the boundary layer gets reversed, we say that the boundary layer separates. This region of reversed flow can form a sort of wedge that can lift the rest of the flow away from the surface.&lt;/p&gt;
    &lt;p&gt;Let’s take a step back from the subtleties of boundary layers to see how what we’ve learned corresponds to behavior of a flow around an airfoil. Let me once more bring up the demonstration that brought us here in the first place:&lt;/p&gt;
    &lt;p&gt;As we move across the surface of the airfoil, the high pressure at the stagnation point up front gradually decreases to reach minimum close to the “peak” of that curved surface. Across this transition the pressure gradient is favorable, and that distribution works in our favor â the boundary layer stays nicely attached to the surface.&lt;/p&gt;
    &lt;p&gt;However, as the air reaches the valley of the lowest pressure, it then has to start climbing back up to reach the slightly positive pressure in the rear of the airfoil. For small values of the angle of attack, the pressure pit from which the air has to climb out is not very deep and the adverse pressure gradient isn’t very strong, so the boundary layer remains attached.&lt;/p&gt;
    &lt;p&gt;As we increase the angle of attack of the airfoil, the pressure on top becomes lower and lower. For even higher angles, the adverse pressure gradient becomes so strong that it eventually reverses the flow in the boundary layer, creating separation. Let’s look at this region up close to see how the arrows of velocity in the separated region point in the other direction:&lt;/p&gt;
    &lt;p&gt;If you clicktap to add markers in the bottom right corner of the simulation you’ll notice that many of them move against the bulk of the flow â the boundary layer and the flow have separated.&lt;/p&gt;
    &lt;p&gt;We’ll get back to looking at airfoils soon enough, but we still have a few things to wrap up in the world of boundary layers.&lt;/p&gt;
    &lt;p&gt;The boundary layers we’ve looked at so far were laminar â the layers of fluid with different velocities flowed in an orderly way on top of each other. However, at higher flow speeds and over larger distances, or at high Reynolds numbers, the flow in the boundary layer transitions to a turbulent flow:&lt;/p&gt;
    &lt;p&gt;Be aware that what you’re seeing here is a very simplified simulation of a turbulent boundary layer. Turbulence is inherently three dimensional and it contains various evolving structures of different sizes that are extremely computationally expensive to evaluate in detail. Thankfully, you can find many videos of computer simulations and real flows showing turbulent boundary layers.&lt;/p&gt;
    &lt;p&gt;While the laminar boundary layers we’ve seen in the past exhibited very organized flows, the turbulent one is very chaotic, with large and small swirls causing the flow to mix very rapidly. The transition from laminar to turbulent boundary layer happens spontaneously, but for a given flow speed, the location of the transition depends on surface roughness, steadiness of the flow outside of the boundary layer, and presence of pressure gradients.&lt;/p&gt;
    &lt;p&gt;At any given moment, the velocity profile in the turbulent boundary layer is very unsteady, but it can be averaged over time to get the mean distribution of speed. Let’s compare the time-averaged profiles of the laminar and turbulent boundary layers:&lt;/p&gt;
    &lt;p&gt;In the dynamic simulation of the turbulent boundary layer, we saw how the slower flow close to the surface rapidly mixed with the upper regions of the flow. This slows down those faster sections, and we need to go farther away from the surface for these sluggish intrusions to stop affecting the flow. For this reason, the turbulent boundary layer is thicker and grows faster than a laminar boundary layer.&lt;/p&gt;
    &lt;p&gt;On the other hand, the strong turbulent mixing causes the fast external flow to get close to the body, so the overall velocity profile by the surface increases much more quickly in the turbulent case as opposed to laminar case â I’m showing that with white dotted lines.&lt;/p&gt;
    &lt;p&gt;Recall that the more horizontal the velocity profile at the surface of the object, the bigger the skin friction drag â a turbulent boundary layer has higher skin friction drag than a laminar layer. Despite the cost of increased friction drag, a turbulent boundary layer is often beneficial.&lt;/p&gt;
    &lt;p&gt;Because of that higher velocity closer to the surface, a turbulent boundary layer is more resistant to adverse pressure gradients and it can stay attached to the surface of an object for longer distances.&lt;/p&gt;
    &lt;p&gt;For some objects like golf balls, which purposefully make their boundary layer turbulent by roughing up the surface with little dimples, the delayed separation also decreases the pressure drag caused by uneven pressure distribution. That reduction more than compensates for the increased skin friction drag, making the dimply golf balls fly farther than equivalent smooth balls.&lt;/p&gt;
    &lt;p&gt;For airfoils, a turbulent boundary layer delays separation of the flow, which can help prevent stall at higher angles of attack, but at normal cruising conditions the increased skin friction becomes an important drawback. For many aerodynamic shapes in typical conditions, the skin friction drag is the primary contributor to the total drag that these objects experience.&lt;/p&gt;
    &lt;p&gt;As we’ve seen, by increasing the angle of attack on an airfoil, the lift force grows up to a certain limit, at which the boundary layer separates over most of the upper surface. By staying under this limit, a symmetric airfoil can safely generate lift force.&lt;/p&gt;
    &lt;p&gt;However, when it comes to angle of attack and lift, the shape of an airfoil isn’t particularly unique in its lift-creation capabilities. Most simple elongated shapes generate lift when put in a flow at an angle of attack. In the demonstration below, you can tilt a flat plate and see the forces exerted by the pressure field around it:&lt;/p&gt;
    &lt;p&gt;You may be surprised to see that, at small angles of attack, this flat plate also generates lift. An airfoil-like shape is not a requirement for lift generation. After all, paper airplanes with their flat wings can fly just fine. Lift is just an outcome of the pressure distribution created and sustained by the flow.&lt;/p&gt;
    &lt;p&gt;Although it doesn’t take a sophisticated shape to generate lift at an angle of attack, a well-designed airfoil can often create more lift and with lower drag. In the last section of this article, we’ll explore how other variations to the shape of an airfoil can affect its characteristics.&lt;/p&gt;
    &lt;head rend="h1"&gt;Airfoil Shapes&lt;/head&gt;
    &lt;p&gt;Let’s go back to the simple symmetric airfoil we’ve been playing with thus far. This time, however, we’re able to control its thickness using the slider:&lt;/p&gt;
    &lt;p&gt;Notice that as we increase the thickness of the airfoil, the pressure on the top and bottom sections of the shape becomes more negative. For this symmetric airfoil at 0Â° angle of attack the thickness doesn’t change much other than increasing the pressure drag.&lt;/p&gt;
    &lt;p&gt;However, if we break the symmetry of the shape, we can use thickness-dependence to make one side of the airfoil have a higher negative pressure than the other. In the demonstration below, you can control the “thickness” of the upper surface of the airfoil using the slider:&lt;/p&gt;
    &lt;p&gt;Notice that an asymmetric shape creates an asymmetric pressure distribution, which ends up creating lift without any changes to angle of attack. With some slight tweaking of this shape we finally recreated the asymmetric shape we first saw on the airplane in the early sections of this article.&lt;/p&gt;
    &lt;p&gt;Naturally, when combined with an increasing angle of attack, this airfoil will generate even more lift until it eventually reaches stalling conditions:&lt;/p&gt;
    &lt;p&gt;While symmetric airfoils are sometimes used in acrobatic airplanes, which often find themselves flying upside down, most typical planes use an asymmetric airfoil shape.&lt;/p&gt;
    &lt;p&gt;The underlying mechanism of lift generation by changing the angle of attack or by shaping the object differently is ultimately the same â we’re changing the placement and orientation of the surface of the body relative to the incoming flow. The flow reacts by changing the velocity and pressure distribution, and the resulting pressure field creates the forces on that object.&lt;/p&gt;
    &lt;p&gt;This all means that we have a lot of flexibility in how an airfoil is shaped, as long as the resulting pressure distribution fulfills the design goals of achieving a certain amount of lift while minimizing drag.&lt;/p&gt;
    &lt;p&gt;For example, in some applications it’s important to minimize the skin friction drag caused by a turbulent boundary layer. Some laminar flow airfoils achieve this by shaping the airfoil to move the “pit” of negative pressure further to the back of the airfoil:&lt;/p&gt;
    &lt;p&gt;The favorable pressure gradient between the front and the lowest pressure point extends over a longer distance across the surface of this airfoil, which, at least in principle, helps to keep the boundary layer laminar to keep the skin friction low.&lt;/p&gt;
    &lt;p&gt;Notice that even this unusual airfoil had a rounded front and a sharp back. The roundness of the front helps the air smoothly flow around this area at different angles of attack, and the sharp back reduces the pressure drag by avoiding the separation of the flow.&lt;/p&gt;
    &lt;p&gt;The velocity of the flow around the airfoil is also a contributing factor to the design of the shape. Let’s look at the speed distribution in the flow around a simple asymmetric airfoil using the varying colors and markers:&lt;/p&gt;
    &lt;p&gt;The flow above the airfoil is faster than the incoming flow as indicated by brighter colors. The markers that start in the same line don’t end up sliding off the airfoil in the same formation â the ones on top are further ahead. This is particularly visible for larger values of the angles of attack.&lt;/p&gt;
    &lt;p&gt;This acceleration in the upper part becomes another point of consideration for airfoil design. While commercial airliners don’t fly faster than the speed of sound, the accelerated flow in the top part of an airfoil can break that barrier. This creates a shockwave that can sometimes be seen in flight. Modern airliners use supercritical airfoils that are designed to reduce these drag-causing shockwaves by carefully controlling the speed of the flow around the wing.&lt;/p&gt;
    &lt;p&gt;Planes designed to fly above the speed of sound use supersonic airfoils that are quite different from the shapes we’ve seen. These airfoils have a thin profile and their front edge is sharp and not rounded. Supersonic flows of air are more complicated than what we’ve explored in this article, as variations in density and temperature become an important component of the behavior of the flow.&lt;/p&gt;
    &lt;p&gt;Many of the airfoils used today are designed specifically for the plane they’ll be used in. Moreover, that cross-sectional shape may change across the length of the wing. Real airplanes are three dimensional and the overall shape of the wings also significantly affects the lift and drag of an airplane, but ultimately all the resulting forces are an outcome of interactions between the flow and the body.&lt;/p&gt;
    &lt;head rend="h1"&gt;Further Reading and Watching&lt;/head&gt;
    &lt;p&gt;John Anderson’s Fundamentals of Aerodynamics is a very well-written textbook on aerodynamics. Over the course of over a thousand pages, the author presents a classic exposition of the motion of fluids and their interactions with bodies put in those flows.&lt;/p&gt;
    &lt;p&gt;Understanding Aerodynamics by Doug McLean is a great textbook that takes a different approach of explaining aerodynamic phenomena using physical reasoning. For me, the crowning achievement of the publication is showing that many popular explanations of the origins of lift are either incorrect or they’re based on merely mathematically convenient theorems. The author’s video lecture gives an overview of some of these misconceptions.&lt;/p&gt;
    &lt;p&gt;In this article, I’m using computational fluid dynamics to simulate the flow of air around different objects. For an approachable introduction to these methods I enjoyed Tony Saad’s series of lectures on the topic. For an alternative, and slightly more rigorous approach, Lorena Barba created 12 steps to Navier-Stokes. That website is also accompanied by video lectures.&lt;/p&gt;
    &lt;p&gt;Finally, YouTuber braintruffle created a series of beautiful videos that start with the behavior of fluids on a quantum scale and build up increasingly abstract models that can be used in more practical applications. The videos are packed with interesting takes on fluid mechanics, and they’re worth watching for their visuals alone.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;If you were to sit on a flying airplane and look out the window to glance at its wings, you’d often have a hard time seeing anything going on. However, in that crisp clearness of air whose invisible flow sustains the varied pressure field, lies the hidden source of lift that overcomes the might of gravity to keep the plane safely above the ground.&lt;/p&gt;
    &lt;p&gt;Since the first human flight, we’ve now mastered the art of soaring in the skies by bending the flow of air to our will, using physical quantities like pressure and velocity to help shape our designs. These tangible concepts are ultimately just a manifestation of motions and collisions of billions of inanimate air particles that somehow conspire to assemble the forces we need.&lt;/p&gt;
    &lt;p&gt;I hope this deeper, technical exploration of airfoils hasn’t diminished your appreciation of the greatness of flight. Perhaps paradoxically, by seeing how all the pieces fit together, you’ll find the whole thing even more magical.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46795908</guid><pubDate>Wed, 28 Jan 2026 14:32:30 +0000</pubDate></item><item><title>Amazon axes 16,000 jobs as it pushes AI and efficiency</title><link>https://www.reuters.com/legal/litigation/amazon-cuts-16000-jobs-globally-broader-restructuring-2026-01-28/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46796745</guid><pubDate>Wed, 28 Jan 2026 15:39:11 +0000</pubDate></item><item><title>Oban, the job processing framework from Elixir, has come to Python</title><link>https://www.dimamik.com/posts/oban_py/</link><description>&lt;doc fingerprint="2e18825df1a004c7"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Setting the Stage&lt;/head&gt;
    &lt;p&gt;I’ve used Oban in Elixir for almost as long as I’ve been writing software in Elixir, and it has always been an essential tool for processing jobs. I always knew Oban was cool, but I never dug deeper. This article is a collection of my notes and observations on how the Python implementation of Oban works and what I’ve learned while exploring its codebase. I’ll also try to compare it with the Elixir version and talk about concurrency in general.&lt;/p&gt;
    &lt;head rend="h2"&gt;Surface Level&lt;/head&gt;
    &lt;p&gt;Oban allows you to insert and process jobs using only your database. You can insert the job to send a confirmation email in the same database transaction where you create the user. If one thing fails, everything is rolled back.&lt;/p&gt;
    &lt;p&gt;Additionally, like most job processing frameworks, Oban has queues with local and global queue limits. But unlike others, it stores your completed jobs and can even keep their results if needed. It has built-in cron scheduling and many more features to control how your jobs are processed.&lt;/p&gt;
    &lt;p&gt;Oban comes in two versions - Open Source Oban-py and commercial Oban-py-pro.&lt;/p&gt;
    &lt;p&gt;OSS Oban has a few limitations, which are automatically lifted in the Pro version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-threaded asyncio execution - concurrent but not truly parallel, so CPU-bound jobs block the event loop.&lt;/item&gt;
      &lt;item&gt;No bulk inserts - each job is inserted individually.&lt;/item&gt;
      &lt;item&gt;No bulk acknowledgements - each job completion is persisted individually.&lt;/item&gt;
      &lt;item&gt;Inaccurate rescues - jobs that are long-running might get rescued even if the producer is still alive. Pro version uses smarter heartbeats to track producer liveness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition, Oban-py-pro comes with a few extra features you’d configure separately, like workflows, relay, unique jobs, and smart concurrency.&lt;/p&gt;
    &lt;p&gt;OSS Oban-py is a great start for your hobby project, or if you’d want to evaluate Oban philosophy itself, but for any bigger scale - I’d go with Oban Pro. The pricing seems very compelling, considering the amount of work put into making the above features work.&lt;/p&gt;
    &lt;p&gt;I obviously can’t walk you through the Pro version features, but let’s start with the basics. How Oban Py works under the hood, from the job insertion until the job execution. Stay tuned.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going Deeper - Job Processing Path&lt;/head&gt;
    &lt;p&gt;Let’s get straight to it. You insert your job:&lt;/p&gt;
    &lt;code&gt;from oban import job

@job(queue="default")
async def send_email(to: str, subject: str, body: str):
    # Simple and clean, but no access to job context
    await smtp.send(to, subject, body)

await send_email.enqueue("[email protected]", "Hello", "World")
&lt;/code&gt;
    &lt;p&gt;After the insertion, the job lands in the &lt;code&gt;oban_jobs&lt;/code&gt; database table with &lt;code&gt;state = 'available'&lt;/code&gt;. Oban fires off a PostgreSQL &lt;code&gt;NOTIFY&lt;/code&gt; on the &lt;code&gt;insert&lt;/code&gt; channel:&lt;/p&gt;
    &lt;code&gt;# oban.py:414-419
# Single inserts go through bulk insert path
result = await self._query.insert_jobs(jobs)
queues = {job.queue for job in result if job.state == "available"}
await self._notifier.notify("insert", [{"queue": queue} for queue in queues])
&lt;/code&gt;
    &lt;p&gt;Every Oban node listening on that channel receives the notification. The Stager on each node gets woken up, but each Stager only cares about queues it’s actually running. Be aware that each node decides which queues it runs, so if the current node runs this queue, the producer is notified:&lt;/p&gt;
    &lt;code&gt;# _stager.py:95-99
async def _on_notification(self, channel: str, payload: dict) -&amp;gt; None:
    queue = payload["queue"]

    if queue in self._producers:
        self._producers[queue].notify()
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;notify()&lt;/code&gt; call sets an &lt;code&gt;asyncio.Event&lt;/code&gt;, breaking the Producer out of its wait loop, so it can dispatch the jobs to the workers:&lt;/p&gt;
    &lt;code&gt;# _producer.py:244-262
async def _loop(self) -&amp;gt; None:
    while True:
        try:
            # &amp;lt;--- This is where the event is received ---&amp;gt;
            await asyncio.wait_for(self._notified.wait(), timeout=1.0)
        except asyncio.TimeoutError:
            continue
        except asyncio.CancelledError:
            break

        # &amp;lt;--- Reset the event so it can be triggered for the next batch ---&amp;gt;
        self._notified.clear()

        try:
            # &amp;lt;--- A little debounce to potentially process multiple jobs at once ---&amp;gt;
            await self._debounce()
            # &amp;lt;--- Dispatch (Produce) the jobs from the database to the workers ---&amp;gt;
            await self._produce()
        except asyncio.CancelledError:
            break
        except Exception:
            logger.exception("Error in producer for queue %s", self._queue)
&lt;/code&gt;
    &lt;p&gt;Before fetching the jobs, the producer persists all pre-existing job completions (acks) to the database to make sure queue limits are respected. Next, it fetches new jobs, transitioning their state to executing at the same time. A slightly more complex version of this SQL is used:&lt;/p&gt;
    &lt;code&gt;-- fetch_jobs.sql (simplified)
WITH locked_jobs AS (
  SELECT priority, scheduled_at, id
  FROM
  oban_jobs
  WHERE state = 'available' AND queue = %(queue)s
  ORDER BY priority ASC, scheduled_at ASC, id ASC
  LIMIT %(demand)s
  FOR UPDATE SKIP LOCKED
)
UPDATE oban_jobs oj
SET
  attempt = oj.attempt + 1,
  attempted_at = timezone('UTC', now()),
  attempted_by = %(attempted_by)s,
  state = 'executing'
FROM locked_jobs
WHERE oj.id = locked_jobs.id
&lt;/code&gt;
    &lt;p&gt;And this is the first really cool part.&lt;/p&gt;
    &lt;p&gt;Segue to FOR UPDATE SKIP LOCKED.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;FOR UPDATE&lt;/code&gt;- Locks the selected rows so no other transaction can modify them until this transaction completes. This prevents two producers from grabbing the same job.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SKIP LOCKED&lt;/code&gt;- If a row is already locked by another transaction, skip it instead of waiting. This is crucial for concurrency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this matters for job queues: Imagine two producer instances (A and B) trying to fetch jobs simultaneously:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Without SKIP LOCKED&lt;/cell&gt;
        &lt;cell role="head"&gt;With SKIP LOCKED&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A locks job #1&lt;/cell&gt;
        &lt;cell&gt;A locks job #1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B waits for job #1 to unlock&lt;/cell&gt;
        &lt;cell&gt;B skips job #1, takes job #2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Slow, sequential processing&lt;/cell&gt;
        &lt;cell&gt;Fast, parallel processing&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Back in Python, we know that the jobs we just fetched should be processed immediately. When we fetched the job, we already transitioned its state and respected the queue demand.&lt;/p&gt;
    &lt;p&gt;Each job gets dispatched as an async task:&lt;/p&gt;
    &lt;code&gt;jobs = await self._get_jobs()
for job in jobs:
    task = self._dispatcher.dispatch(self, job)
    task.add_done_callback(
        lambda _, job_id=job.id: self._on_job_complete(job_id)
    )

    self._running_jobs[job.id] = (job, task)
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;add_done_callback&lt;/code&gt; ensures that independent of success or failure, we can attach a callback to handle job completion.&lt;/p&gt;
    &lt;p&gt;The dispatcher controls how exactly the job is run. For the non-pro Oban version, it just uses &lt;code&gt;asyncio.create_task&lt;/code&gt; to run the job in the event loop:&lt;/p&gt;
    &lt;code&gt;# _producer.py:69-71
class LocalDispatcher:
    def dispatch(self, producer: Producer, job: Job) -&amp;gt; asyncio.Task:
        return asyncio.create_task(producer._execute(job))
&lt;/code&gt;
    &lt;p&gt;For pro version, local asyncio dispatcher is automatically replaced with a pool of processes, so you don’t need to do anything to have true parallelism across multiple cores.&lt;/p&gt;
    &lt;p&gt;After the job is dispatched, the Executor takes over. It resolves your worker class from the string name, runs it, and pattern-matches the result:&lt;/p&gt;
    &lt;code&gt;# _executor.py:73-83
async def _process(self) -&amp;gt; None:
  self.worker = resolve_worker(self.job.worker)()
  self.result = await self.worker.process(self.job)
&lt;/code&gt;
    &lt;code&gt;# _executor.py:95-133
match result:
    case Exception() as error:
        # Retry or discard based on attempt count
    case Cancel(reason=reason):
        # Mark cancelled
    case Snooze(seconds=seconds):
        # Reschedule with decremented attempt
    case _:
        # Completed successfully
&lt;/code&gt;
    &lt;p&gt;And that’s the second cool part! You see how similar it is to Elixir’s pattern matching? I love how it’s implemented!&lt;/p&gt;
    &lt;p&gt;When execution finishes, the result gets queued for acknowledgement:&lt;/p&gt;
    &lt;code&gt;# _producer.py:315
self._pending_acks.append(executor.action)
&lt;/code&gt;
    &lt;p&gt;The completion callback notifies the Producer to wake up again-fetch more jobs, and batch-ack the finished ones in a single query.&lt;/p&gt;
    &lt;p&gt;That’s the hot path: &lt;code&gt;Insert → Notify → Fetch (with locking) → Execute → Ack.&lt;/code&gt; Five hops from your code to completion. What about the background processes? What about errors and retries? What about periodic jobs, cron, and all these other pieces? Stay tuned.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Undercurrents - Background Processes&lt;/head&gt;
    &lt;p&gt;Oban runs several background loops that keep the system healthy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leader Election&lt;/head&gt;
    &lt;p&gt;In a cluster, you don’t want every node pruning jobs or rescuing orphans. Oban elects a single leader:&lt;/p&gt;
    &lt;code&gt;# _leader.py:107-113
async def _election(self) -&amp;gt; None:
    self._is_leader = await self._query.attempt_leadership(
        self._name, self._node, int(self._interval), self._is_leader
    )
&lt;/code&gt;
    &lt;code&gt;-- Cleanup expired leaders first
DELETE FROM
  oban_leaders
WHERE
  expires_at &amp;lt; timezone('UTC', now())
&lt;/code&gt;
    &lt;code&gt;-- If current node is a leader, it re-elects itself
INSERT INTO oban_leaders (name, node, elected_at, expires_at)
VALUES (
  %(name)s,
  %(node)s,
  timezone('UTC', now()),
  timezone('UTC', now()) + interval '%(ttl)s seconds'
)
ON CONFLICT (name) DO UPDATE SET
  -- Only update if we're the same node (i.e. current leader re-electing itself).
  -- Other nodes can't overwrite an active leader's lease.
  expires_at = EXCLUDED.expires_at
WHERE
  oban_leaders.node = EXCLUDED.node
RETURNING node
&lt;/code&gt;
    &lt;code&gt;-- Try to insert as a new leader if no leader exists
INSERT INTO oban_leaders (
  name, node, elected_at, expires_at
) VALUES (
  %(name)s,
  %(node)s,
  timezone('UTC', now()),
  timezone('UTC', now()) + interval '%(ttl)s seconds'
)
ON CONFLICT (name) DO NOTHING
RETURNING node
&lt;/code&gt;
    &lt;p&gt;The leader refreshes twice as often to hold onto the role:&lt;/p&gt;
    &lt;code&gt;# _leader.py:101-105
# Sleep for half interval if leader (to boost their refresh interval and allow them to
# retain leadership), full interval otherwise
sleep_duration = self._interval / 2 if self._is_leader else self._interval
&lt;/code&gt;
    &lt;p&gt;When a node shuts down cleanly, it resigns and notifies the cluster:&lt;/p&gt;
    &lt;code&gt;# _leader.py:83-87
if self._is_leader:
    payload = {"action": "resign", "node": self._node, "name": self._name}

    await self._notifier.notify("leader", payload)
    await self._query.resign_leader(self._name, self._node)
&lt;/code&gt;
    &lt;p&gt;And that’s the third cool part! Leader election is delegated entirely to PostgreSQL. Oban uses &lt;code&gt;INSERT ... ON CONFLICT&lt;/code&gt; with a TTL-based lease - no Raft, no consensus protocol, no external coordination service. If the leader dies, its lease expires and the next node to run the election query takes over. Simple, effective, and zero additional infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lifeline: Rescuing Orphaned Jobs&lt;/head&gt;
    &lt;p&gt;Workers crash. Containers get killed. When that happens, jobs can get stuck executing indefinitely. The Lifeline process (leader-only) rescues them:&lt;/p&gt;
    &lt;code&gt;# _lifeline.py:73-77
async def _rescue(self) -&amp;gt; None:
    if not self._leader.is_leader:
        return

    await use_ext("lifeline.rescue", _rescue, self._query, self._rescue_after)
&lt;/code&gt;
    &lt;p&gt;Oban-py rescue mechanics are purely time-based - any job in &lt;code&gt;executing&lt;/code&gt; state longer than &lt;code&gt;rescue_after&lt;/code&gt; (default: 5 minutes) gets moved back. Unlike the Oban Pro version, it doesn’t check whether the producer that owns the job is still alive. This means legitimately long-running jobs could be rescued and executed a second time.&lt;/p&gt;
    &lt;p&gt;The takeaway is that you should set &lt;code&gt;rescue_after&lt;/code&gt; higher than your longest expected job duration, and design workers to be idempotent.&lt;/p&gt;
    &lt;p&gt;The SQL itself is straightforward - jobs stuck executing get moved back to available or discarded if they’ve exhausted retries:&lt;/p&gt;
    &lt;code&gt;-- rescue_jobs.sql (simplified)
UPDATE oban_jobs
SET
  state = CASE
    WHEN attempt &amp;gt;= max_attempts THEN 'discarded'
    ELSE 'available'
  END,
  meta = CASE
    WHEN attempt &amp;gt;= max_attempts THEN meta
    ELSE meta || jsonb_build_object('rescued', coalesce((meta-&amp;gt;&amp;gt;'rescued')::int, 0) + 1)
  END
WHERE
  state = 'executing'
  AND attempted_at &amp;lt; timezone('UTC', now()) - make_interval(secs =&amp;gt; %(rescue_after)s)
&lt;/code&gt;
    &lt;p&gt;The rescued counter in meta lets you track how often jobs needed saving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pruner: Cleaning Up Old Jobs&lt;/head&gt;
    &lt;p&gt;Without pruning, your oban_jobs table grows forever. The Pruner (also leader-only) deletes terminal jobs older than max_age (default: 1 day):&lt;/p&gt;
    &lt;code&gt;-- prune_jobs.sql
WITH jobs_to_delete AS (
SELECT id FROM oban_jobs
WHERE
(state = 'completed' AND completed_at &amp;lt;= timezone('UTC', now()) - make_interval(secs =&amp;gt; %(max_age)s)) OR
(state = 'cancelled' AND cancelled_at &amp;lt;= timezone('UTC', now()) - make_interval(secs =&amp;gt; %(max_age)s)) OR
(state = 'discarded' AND discarded_at &amp;lt;= timezone('UTC', now()) - make_interval(secs =&amp;gt; %(max_age)s))
ORDER BY id ASC
LIMIT %(limit)s
)
DELETE FROM oban_jobs WHERE id IN (SELECT id FROM jobs_to_delete)
&lt;/code&gt;
    &lt;p&gt;The LIMIT prevents long-running deletes from blocking other operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Retry &amp;amp; Backoff Mechanics&lt;/head&gt;
    &lt;p&gt;When a job raises an exception, the Executor decides its fate:&lt;/p&gt;
    &lt;code&gt;# _executor.py:96-109
match result:
    case Exception() as error:
        if self.job.attempt &amp;gt;= self.job.max_attempts:
            self.action = AckAction(
                job=self.job,
                state="discarded",
                error=self._format_error(error),
            )
        else:
            self.action = AckAction(
                job=self.job,
                state="retryable",
                error=self._format_error(error),
                schedule_in=self._retry_backoff(),
            )
&lt;/code&gt;
    &lt;p&gt;Simple rule: under &lt;code&gt;max_attempts&lt;/code&gt; - retry, otherwise - discard.&lt;/p&gt;
    &lt;p&gt;The default backoff uses jittery-clamped exponential growth with randomness to prevent thundering herds:&lt;/p&gt;
    &lt;code&gt;# _backoff.py:66-87
def jittery_clamped(attempt: int, max_attempts: int, *, clamped_max: int = 20) -&amp;gt; int:
    if max_attempts &amp;lt;= clamped_max:
        clamped_attempt = attempt
    else:
        clamped_attempt = round(attempt / max_attempts * clamped_max)

    time = exponential(clamped_attempt, mult=1, max_pow=100, min_pad=15)

    return jitter(time, mode="inc")
&lt;/code&gt;
    &lt;p&gt;And that’s the fourth cool thing! Backoff includes jitter to prevent thundering herds - without it, all failed jobs from the same batch would retry at the exact same moment, spiking load all over again.&lt;/p&gt;
    &lt;p&gt;The formula: 15 + 2^attempt seconds, with up to 10% added jitter. Attempt 1 waits ~17s. Attempt 5 waits ~47s. Attempt 10 waits ~1039s (~17 minutes).&lt;/p&gt;
    &lt;p&gt;The clamping handles jobs with high &lt;code&gt;max_attempts&lt;/code&gt; - if you set &lt;code&gt;max_attempts=100&lt;/code&gt;, it scales the attempt number down proportionally so you don’t wait years between retries.&lt;/p&gt;
    &lt;p&gt;Workers can override this with custom backoff:&lt;/p&gt;
    &lt;code&gt;@worker(queue="default")
class MyWorker:
    async def process(self, job: Job):
        ...

    def backoff(self, job: Job) -&amp;gt; int:
        # Linear backoff: 60s, 120s, 180s...
        return job.attempt * 60
&lt;/code&gt;
    &lt;head rend="h2"&gt;Surfacing - Takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL does the heavy lifting. &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt;for concurrent job fetching,&lt;code&gt;LISTEN/NOTIFY&lt;/code&gt;for real-time signaling,&lt;code&gt;ON CONFLICT&lt;/code&gt;for leader election - the database isn’t just storage, it’s the coordination layer. There’s no Redis, no ZooKeeper, no external broker. One less thing to operate.&lt;/item&gt;
      &lt;item&gt;Oban-py is concurrent, but not parallel. Async IO allows multiple jobs to be in-flight, but the event loop is single-threaded. For I/O-bound workloads, this is fine. For CPU-bound tasks, consider using the Pro version with a process pool.&lt;/item&gt;
      &lt;item&gt;Leader election is simple and effective. No consensus protocol, no Raft - just an &lt;code&gt;INSERT ... ON CONFLICT&lt;/code&gt;with a TTL. The leader refreshes at 2x the normal rate to hold the lease. If it dies, the lease expires and another node takes over. Good enough for pruning and rescuing.&lt;/item&gt;
      &lt;item&gt;The codebase is a pleasure to read. Clear naming, consistent patterns, and well-separated concerns - exploring it felt more like reading a well-written book than understanding a library.&lt;/item&gt;
      &lt;item&gt;OSS gets you far, Pro fills the gaps. Bulk operations, smarter rescues, and true parallelism are all Pro-only - but for what you get, Pro license feels like a great deal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, Oban.py is a clean and well-structured port. If you’re coming from Elixir and miss Oban, or if you’re in Python and want a database-backed job queue that doesn’t require external infrastructure beyond PostgreSQL - it’s worth looking at.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46797594</guid><pubDate>Wed, 28 Jan 2026 16:32:00 +0000</pubDate></item><item><title>Will AIs Take All Our Jobs and End Human History–Or Not?</title><link>https://writings.stephenwolfram.com/2023/03/will-ais-take-all-our-jobs-and-end-human-history-or-not-well-its-complicated/</link><description>&lt;doc fingerprint="3e05995eb7e385f0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Shock of ChatGPT&lt;/head&gt;
    &lt;p&gt;Just a few months ago writing an original essay seemed like something only a human could do. But then ChatGPT burst onto the scene. And suddenly we realized that an AI could write a passable human-like essay. So now it’s natural to wonder: How far will this go? What will AIs be able to do? And how will we humans fit in?&lt;/p&gt;
    &lt;p&gt;My goal here is to explore some of the science, technology—and philosophy—of what we can expect from AIs. I should say at the outset that this is a subject fraught with both intellectual and practical difficulty. And all I’ll be able to do here is give a snapshot of my current thinking—which will inevitably be incomplete—not least because, as I’ll discuss, trying to predict how history in an area like this will unfold is something that runs straight into an issue of basic science: the phenomenon of computational irreducibility.&lt;/p&gt;
    &lt;p&gt;But let’s start off by talking about that particularly dramatic example of AI that’s just arrived on the scene: ChatGPT. So what is ChatGPT? Ultimately, it’s a computational system for generating text that’s been set up to follow the patterns defined by human-written text from billions of webpages, millions of books, etc. Give it a textual prompt and it’ll continue in a way that’s somehow typical of what it’s seen us humans write.&lt;/p&gt;
    &lt;p&gt;The results (which ultimately rely on all sorts of specific engineering) are remarkably “human like”. And what makes this work is that whenever ChatGPT has to “extrapolate” beyond anything it’s explicitly seen from us humans it does so in ways that seem similar to what we as humans might do.&lt;/p&gt;
    &lt;p&gt;Inside ChatGPT is something that’s actually computationally probably quite similar to a brain—with millions of simple elements (“neurons”) forming a “neural net” with billions of connections that have been “tweaked” through a progressive process of training until they successfully reproduce the patterns of human-written text seen on all those webpages, etc. Even without training the neural net would still produce some kind of text. But the key point is that it won’t be text that we humans consider meaningful. To get such text we need to build on all that “human context” defined by the webpages and other materials we humans have written. The “raw computational system” will just do “raw computation”; to get something aligned with us humans requires leveraging the detailed human history captured by all those pages on the web, etc.&lt;/p&gt;
    &lt;p&gt;But so what do we get in the end? Well, it’s text that basically reads like it was written by a human. In the past we might have thought that human language was somehow a uniquely human thing to produce. But now we’ve got an AI doing it. So what’s left for us humans? Well, somewhere things have got to get started: in the case of text, there’s got to be a prompt specified that tells the AI “what direction to go in”. And this is the kind of thing we’ll see over and over again. Given a defined “goal”, an AI can automatically work towards achieving it. But it ultimately takes something beyond the raw computational system of the AI to define what us humans would consider a meaningful goal. And that’s where we humans come in.&lt;/p&gt;
    &lt;p&gt;What does this mean at a practical, everyday level? Typically we use ChatGPT by telling it—using text—what we basically want. And then it’ll fill in a whole essay’s worth of text talking about it. We can think of this interaction as corresponding to a kind of “linguistic user interface” (that we might dub a “LUI”). In a graphical user interface (GUI) there’s core content that’s being rendered (and input) through some potentially elaborate graphical presentation. In the LUI provided by ChatGPT there’s instead core content that’s being rendered (and input) through a textual (“linguistic”) presentation.&lt;/p&gt;
    &lt;p&gt;You might jot down a few “bullet points”. And in their raw form someone else would probably have a hard time understanding them. But through the LUI provided by ChatGPT those bullet points can be turned into an “essay” that can be generally understood—because it’s based on the “shared context” defined by everything from the billions of webpages, etc. on which ChatGPT has been trained.&lt;/p&gt;
    &lt;p&gt;There’s something about this that might seem rather unnerving. In the past, if you saw a custom-written essay you’d reasonably be able to conclude that a certain irreducible human effort was spent in producing it. But with ChatGPT this is no longer true. Turning things into essays is now “free” and automated. “Essayification” is no longer evidence of human effort.&lt;/p&gt;
    &lt;p&gt;Of course, it’s hardly the first time there’s been a development like this. Back when I was a kid, for example, seeing that a document had been typeset was basically evidence that someone had gone to the considerable effort of printing it on a printing press. But then came desktop publishing, and it became basically free to make any document be elaborately typeset.&lt;/p&gt;
    &lt;p&gt;And in a longer view, this kind of thing is basically a constant trend in history: what once took human effort eventually becomes automated and “free to do” through technology. There’s a direct analog of this in the realm of ideas: that with time higher and higher levels of abstraction are developed, that subsume what were formerly laborious details and specifics.&lt;/p&gt;
    &lt;p&gt;Will this end? Will we eventually have automated everything? Discovered everything? Invented everything? At some level, we now know that the answer is a resounding no. Because one of the consequences of the phenomenon of computational irreducibility is that there’ll always be more computations to do—that can’t in the end be reduced by any finite amount of automation, discovery or invention.&lt;/p&gt;
    &lt;p&gt;Ultimately, though, this will be a more subtle story. Because while there may always be more computations to do, it could still be that we as humans don’t care about them. And that somehow everything we care about can successfully be automated—say by AIs—leaving “nothing more for us to do”.&lt;/p&gt;
    &lt;p&gt;Untangling this issue will be at the heart of questions about how we fit into the AI future. And in what follows we’ll see over and over again that what might at first essentially seem like practical matters of technology quickly get enmeshed with deep questions of science and philosophy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intuition from the Computational Universe&lt;/head&gt;
    &lt;p&gt;I’ve already mentioned computational irreducibility a couple of times. And it turns out that this is part of a circle of rather deep—and at first surprising—ideas that I believe are crucial to thinking about the AI future.&lt;/p&gt;
    &lt;p&gt;Most of our existing intuition about “machinery” and “automation” comes from a kind of “clockwork” view of engineering—in which we specifically build systems component by component to achieve objectives we want. And it’s the same with most software: we write it line by line to specifically do—step by step—whatever it is we want. And we expect that if we want our machinery—or software—to do complex things then the underlying structure of the machinery or software must somehow be correspondingly complex.&lt;/p&gt;
    &lt;p&gt;So when I started exploring the whole computational universe of possible programs in the early 1980s it was a big surprise to discover that things work quite differently there. And indeed even tiny programs—that effectively just apply very simple rules repeatedly—can generate great complexity. In our usual practice of engineering we haven’t seen this, because we’ve always specifically picked programs (or other structures) where we can readily foresee how they’ll behave, so that we can explicitly set them up to do what we want. But out in the computational universe it’s very common to see programs that just “intrinsically generate” great complexity, without us ever having to explicitly “put it in”.&lt;/p&gt;
    &lt;p&gt;And having discovered this, we realize that there’s actually a big example that’s been around forever: the natural world. And indeed it increasingly seems as if the “secret” that nature uses to make the complexity it so often shows is exactly to operate according to the rules of simple programs. (For about three centuries it seemed as if mathematical equations were the ultimate way to describe the natural world—but in the past few decades, and particularly poignantly with our recent Physics Project, it’s become clear that simple programs are in general a more powerful approach.)&lt;/p&gt;
    &lt;p&gt;How does all this relate to technology? Well, technology is about taking what’s out there in the world, and harnessing it for human purposes. And there’s a fundamental tradeoff here. There may be some system out in nature that does amazingly complex things. But the question is whether we can “slice off” certain particular things that we humans happen to find useful. A donkey has all sorts of complex things going on inside. But at some point it was discovered that we can use it “technologically” to do the rather simple thing of pulling a cart.&lt;/p&gt;
    &lt;p&gt;And when it comes to programs out in the computational universe it’s extremely common to see ones that do amazingly complex things. But the question is whether we can find some aspect of those things that’s useful to us. Maybe the program is good at making pseudorandomness. Or distributedly determining consensus. Or maybe it’s just doing its complex thing, and we don’t yet know any “human purpose” that this achieves.&lt;/p&gt;
    &lt;p&gt;One of the notable features of a system like ChatGPT is that it isn’t constructed in an “understand-every-step” traditional engineering way. Instead one basically just starts from a “raw computational system” (in the case of ChatGPT, a neural net), then progressively tweaks it until its behavior aligns with the “human-relevant” examples one has. And this alignment is what makes the system “technologically useful”—to us humans.&lt;/p&gt;
    &lt;p&gt;Underneath, though, it’s still a computational system, with all the potential “wildness” that implies. And free from the “technological objective” of “human-relevant alignment” the system might do all sorts of sophisticated things. But they might not be things that (at least at this time in history) we care about. Even though some putative alien (or our future selves) might.&lt;/p&gt;
    &lt;p&gt;OK, but let’s come back to the “raw computation” side of things. There’s something very different about computation from all other kinds of “mechanisms” we’ve seen before. We might have a cart that can move forward. And we might have a stapler that can put staples in things. But carts and staplers do very different things; there’s no equivalence between them. But for computational systems (at least ones that don’t just always behave in obviously simple ways) there’s my Principle of Computational Equivalence—which implies that all these systems are in a sense equivalent in the kinds of computations they can do.&lt;/p&gt;
    &lt;p&gt;This equivalence has many consequences. One of them is that one can expect to make something equally computationally sophisticated out of all sorts of different kinds of things—whether brain tissue or electronics, or some system in nature. And this is effectively where computational irreducibility comes from.&lt;/p&gt;
    &lt;p&gt;One might think that given, say, some computational system based on a simple program it would always be possible for us—with our sophisticated brains, mathematics, computers, etc.—to “jump ahead” and figure out what the system will do before it’s gone through all the steps to do it. But the Principle of Computational Equivalence implies that this won’t in general be possible—because the system itself can be as computationally sophisticated as our brains, mathematics, computers, etc. are. So this means that the system will be computationally irreducible: the only way to find out what it does is effectively just to go through the same whole computational process that it does.&lt;/p&gt;
    &lt;p&gt;There’s a prevailing impression that science will always eventually be able do better than this: that it’ll be able to make “predictions” that allow us to work out what will happen without having to trace through each step. And indeed over the past three centuries there’s been lots of success in doing this, mainly by using mathematical equations. But ultimately it turns out that this has only been possible because science has ended up concentrating on particular systems where these methods work (and then these systems have been used for engineering). But the reality is that many systems show computational irreducibility. And in the phenomenon of computational irreducibility science is in effect “deriving its own limitedness”.&lt;/p&gt;
    &lt;p&gt;Contrary to traditional intuition, try as we might, in many systems we’ll never be able find “formulas” (or other “shortcuts”) that describe what’s going to happen in the systems—because the systems are simply computationally irreducible. And, yes, this represents a limitation on science, and on knowledge in general. But while at first this might seem like a bad thing, there’s also something fundamentally satisfying about it. Because if everything were computationally reducible, we could always “jump ahead” and find out what will happen in the end, say in our lives. But computational irreducibility implies that in general we can’t do that—so that in some sense “something irreducible is being achieved” by the passage of time.&lt;/p&gt;
    &lt;p&gt;There are a great many consequences of computational irreducibility. Some—that I have particularly explored recently—are in the domain of basic science (for example, establishing core laws of physics as we perceive them from the interplay of computational irreducibility and our computational limitations as observers). But computational irreducibility is also central in thinking about the AI future—and in fact I increasingly feel that it adds the single most important intellectual element needed to make sense of many of the most important questions about the potential roles of AIs and humans in the future.&lt;/p&gt;
    &lt;p&gt;For example, from our traditional experience with engineering we’re used to the idea that to find out why something happened in a particular way we can just “look inside” a machine or program and “see what it did”. But when there’s computational irreducibility, that won’t work. Yes, we could “look inside” and see, say, a few steps. But computational irreducibility implies that to find out what happened, we’d have to trace through all the steps. We can’t expect to find a “simple human narrative” that “says why something happened”.&lt;/p&gt;
    &lt;p&gt;But having said this, one feature of computational irreducibility is that within any computationally irreducible systems there must always be (ultimately, infinitely many) “pockets of computational reducibility” to be found. So for example, even though we can’t say in general what will happen, we’ll always be able to identify specific features that we can predict. (“The leftmost cell will always be black”, etc.) And as we’ll discuss later we can potentially think of technological (as well as scientific) progress as being intimately tied to the discovery of these “pockets of reducibility”. And in effect the existence of infinitely many such pockets is the reason that “there’ll always be inventions and discoveries to be made”.&lt;/p&gt;
    &lt;p&gt;Another consequence of computational irreducibility has to do with trying to ensure things about the behavior of a system. Let’s say one wants to set up an AI so it’ll “never do anything bad”. One might imagine that one could just come up with particular rules that ensure this. But as soon as the behavior of the system (or its environment) is computationally irreducible one will never be able to guarantee what will happen in the system. Yes, there may be particular computationally reducible features one can be sure about. But in general computational irreducibility implies that there’ll always be a “possibility of surprise” or the potential for “unintended consequences”. And the only way to systematically avoid this is to make the system not computationally irreducible—which means it can’t make use of the full power of computation.&lt;/p&gt;
    &lt;head rend="h2"&gt;“AIs Will Never Be Able to Do That”&lt;/head&gt;
    &lt;p&gt;We humans like to feel special, and feel as if there’s something “fundamentally unique” about us. Five centuries ago we thought we lived at the center of the universe. Now we just tend to think that there’s something about our intellectual capabilities that’s fundamentally unique and beyond anything else. But the progress of AI—and things like ChatGPT—keep on giving us more and more evidence that that’s not the case. And indeed my Principle of Computational Equivalence says something even more extreme: that at a fundamental computational level there’s just nothing fundamentally special about us at all—and that in fact we’re computationally just equivalent to lots of systems in nature, and even to simple programs.&lt;/p&gt;
    &lt;p&gt;This broad equivalence is important in being able to make very general scientific statements (like the existence of computational irreducibility). But it also highlights how significant our specifics—our particular history, biology, etc.—are. It’s very much like with ChatGPT. We can have a generic (untrained) neural net with the same structure as ChatGPT, that can do certain “raw computation”. But what makes ChatGPT interesting—at least to us—is that it’s been trained with the “human specifics” described on billions of webpages, etc. In other words, for both us and ChatGPT there’s nothing computationally “generally special”. But there is something “specifically special”—and it’s the particular history we’ve had, particular knowledge our civilization has accumulated, etc.&lt;/p&gt;
    &lt;p&gt;There’s a curious analogy here to our physical place in the universe. There’s a certain uniformity to the universe, which means there’s nothing “generally special” about our physical location. But at least to us there’s still something “specifically special” about it, because it’s only here that we have our particular planet, etc. At a deeper level, ideas based on our Physics Project have led to the concept of the ruliad: the unique object that is the entangled limit of all possible computational processes. And we can then view our whole experience as “observers of the universe” as consisting of sampling the ruliad at a particular place.&lt;/p&gt;
    &lt;p&gt;It’s a bit abstract (and a long story, which I won’t go into in any detail here), but we can think of different possible observers as being both at different places in physical space, and at different places in rulial space—giving them different “points of view” about what happens in the universe. Human minds are in effect concentrated in a particular region of physical space (mostly on this planet) and a particular region of rulial space. And in rulial space different human minds—with their different experiences and thus different ways of thinking about the universe—are in slightly different places. Animal minds might be fairly close in rulial space. But other computational systems (like, say, the weather, which is sometimes said to “have a mind of its own”) are further away—as putative aliens might also be.&lt;/p&gt;
    &lt;p&gt;So what about AIs? It depends what we mean by “AIs”. If we’re talking about computational systems that are set up to do “human-like things” then that means they’ll be close to us in rulial space. But insofar as “an AI” is an arbitrary computational system it can be anywhere in rulial space, and it can do anything that’s computationally possible—which is far broader than what we humans can do, or even think about. (As we’ll talk about later, as our intellectual paradigms—and ways of observing things—expand, the region of rulial space in which we humans operate will correspondingly expand.)&lt;/p&gt;
    &lt;p&gt;But, OK, just how “general” are the computations that we humans (and the AIs that follow us) are doing? We don’t know enough about the brain to be sure. But if we look at artificial neural net systems—like ChatGPT—we can potentially get some sense. And in fact the computations really don’t seem to be that “general”. In most neural net systems data that’s given as input just “ripples once through the system” to produce output. It’s not like in a computational system like a Turing machine where there can be arbitrary “recirculation of data”. And indeed without such “arbitrary recirculation” the computation is necessarily quite “shallow” and can’t ultimately show computational irreducibility.&lt;/p&gt;
    &lt;p&gt;It’s a bit of a technical point, but one can ask whether ChatGPT, with its “re-feeding of text produced so far” can in fact achieve arbitrary (“universal”) computation. And I suspect that in some formal sense it can (or at least a sufficiently expanded analog of it can)—though by producing an extremely verbose piece of text that for example in effect lists successive (self-delimiting) states of a Turing machine tape, and in which finding “the answer” to a computation will take a bit of effort. But—as I’ve discussed elsewhere—in practice ChatGPT is presumably almost exclusively doing “quite shallow” computation.&lt;/p&gt;
    &lt;p&gt;It’s an interesting feature of the history of practical computing that what one might consider “deep pure computations” (say in mathematics or science) were done for decades before “shallow human-like computations” became feasible. And the basic reason for this is that for “human-like computations” (like recognizing images or generating text) one needs to capture lots of “human context”, which requires having lots of “human-generated data” and the computational resources to store and process it.&lt;/p&gt;
    &lt;p&gt;And, by the way, brains also seem to specialize in fundamentally shallow computations. And to do the kind of deeper computations that allow one to take advantage of more of what’s out there in the computational universe, one has to turn to computers. As we’ve discussed, there’s plenty out in the computational universe that we humans don’t (yet) care about: we just consider it “raw computation”, that doesn’t seem to be “achieving human purposes”. But as a practical matter it’s important to make a bridge between the things we humans do care about and think about, and what’s possible in the computational universe. And in a sense that’s at the core of the project I’ve put so much effort into in the Wolfram Language of creating a full-scale computational language that describes in computational terms the things we think about, and experience in the world.&lt;/p&gt;
    &lt;p&gt;OK, people have been saying for years: “It’s nice that computers can do A and B, but only humans can do X”. What X is supposed to be has changed—and narrowed—over the years. And ChatGPT provides us with a major unexpected new example of something more that computers can do.&lt;/p&gt;
    &lt;p&gt;So what’s left? People might say: “Computers can never show creativity or originality”. But—perhaps disappointingly—that’s surprisingly easy to get, and indeed just a bit of randomness “seeding” a computation can often do a pretty good job, as we saw years ago with our WolframTones music-generation system, and as we see today with ChatGPT’s writing. People might also say: “Computers can never show emotions”. But before we had a good way to generate human language we wouldn’t really have been able to tell. And now it already works pretty well to ask ChatGPT to write “happily”, “sadly”, etc. (In their raw form emotions in both humans and other animals are presumably associated with rather simple “global variables” like neurotransmitter concentrations.)&lt;/p&gt;
    &lt;p&gt;In the past people might have said: “Computers can never show judgement”. But by now there are endless examples of machine learning systems that do well at reproducing human judgement in lots of domains. People might also say: “Computers don’t show common sense”. And by this they typically mean that in a particular situation a computer might locally give an answer, but there’s a global reason why that answer doesn’t make sense, that the computer “doesn’t notice”, but a person would.&lt;/p&gt;
    &lt;p&gt;So how does ChatGPT do on this? Not too badly. In plenty of cases it correctly recognizes that “that’s not what I’ve typically read”. But, yes, it makes mistakes. Some of them have to do with it not being able to do—purely with its neural net—even slightly “deeper”computations. (And, yes, that’s something that can often be fixed by it calling Wolfram|Alpha as a tool.) But in other cases the problem seems to be that it can’t quite connect different domains well enough.&lt;/p&gt;
    &lt;p&gt;It’s perfectly capable of doing simple (“SAT-style”) analogies. But when it comes to larger-scale ones it doesn’t manage them. My guess, though, is that it won’t take much scaling up before it starts to be able to make what seem like very impressive analogies (that most of us humans would never even be able to make)—at which point it’ll probably successfully show broader “common sense”.&lt;/p&gt;
    &lt;p&gt;But so what’s left that humans can do, and AIs can’t? There’s—almost by definition—one fundamental thing: define what we would consider goals for what to do. We’ll talk more about this later. But for now we can note that any computational system, once “set in motion”, will just follow its rules and do what it does. But what “direction should it be pointed in”? That’s something that has to come from “outside the system”.&lt;/p&gt;
    &lt;p&gt;So how does it work for us humans? Well, our goals are in effect defined by the whole web of history—both from biological evolution and from our cultural development—in which we are embedded. But ultimately the only way to truly participate in that web of history is to be part of it.&lt;/p&gt;
    &lt;p&gt;Of course, we can imagine technologically emulating every “relevant” aspect of a brain—and indeed things like the success of ChatGPT may suggest that that’s easier to do than we might have thought. But that won’t be enough. To participate in the “human web of history” (as we’ll discuss later) we’ll have to emulate other aspects of “being human”—like moving around, being mortal, etc. And, yes, if we make an “artificial human” we can expect it (by definition) to show all the features of us humans.&lt;/p&gt;
    &lt;p&gt;But while we’re still talking about AIs as—for example—“running on computers” or “being purely digital” then, at least as far as we’re concerned, they’ll have to “get their goals from outside”. One day (as we’ll discuss) there will no doubt be some kind of “civilization of AIs”—which will form its own web of history. But at this point there’s no reason to think that we’ll still be able to describe what’s going on in terms of goals that we recognize. In effect the AIs will at that point have left our domain of rulial space. And—as we’ll discuss—they’ll be operating more like the kind of systems we see in nature, where we can tell there’s computation going on, but we can’t describe it, except rather anthropomorphically, in terms of human goals and purposes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will There Be Anything Left for the Humans to Do?&lt;/head&gt;
    &lt;p&gt;It’s been an issue that’s been raised—with varying degrees of urgency—for centuries: with the advance of automation (and now AI), will there eventually be nothing left for humans to do? Back in the early days of our species, there was lots of hard work of hunting and gathering to do, just to survive. But at least in the developed parts of the world, that kind of work is now at best a distant historical memory.&lt;/p&gt;
    &lt;p&gt;And yet at each stage in history—at least so far—there always seem to be other kinds of work that keep people busy. But there’s a pattern that increasingly seems to repeat. Technology in some way or another enables some new occupation. And eventually that occupation becomes widespread, and lots of people do it. But then there’s a technological advance, and the occupation gets automated—and people aren’t needed to do it anymore. But now there’s a new level of technology, that enables new occupations. And the cycle continues.&lt;/p&gt;
    &lt;p&gt;A century ago the increasingly widespread use of telephones meant that more and more people worked as switchboard operators. But then telephone switching was automated—and those switchboard operators weren’t needed anymore. But with automated switching there could be huge development of telecommunications infrastructure, opening up all sorts of new types of jobs, that in aggregate employ vastly more people than were ever switchboard operators.&lt;/p&gt;
    &lt;p&gt;Something somewhat similar happened with accounting clerks. Before there were computers, one needed to have people laboriously tallying up numbers. But with computers, that was all automated away. But with that automation came the ability to do more complex financial computations—which allowed for more complex financial transactions, more complex regulations, etc., which in turn led to all sorts of new types of jobs.&lt;/p&gt;
    &lt;p&gt;And across a whole range of industries, it’s been the same kind of story. Automation obsoletes some jobs, but enables others. There’s quite often a gap in time, and a change in the skills that are needed. But at least so far there always seems to have been a broad frontier of jobs that have been made possible—but haven’t yet been automated.&lt;/p&gt;
    &lt;p&gt;Will this at some point end? Will there come a time when everything we humans want (or at least need) is delivered automatically? Well, of course, that depends on what we want, and whether, for example, that evolves with what technology has made possible. But could we just decide that “enough is enough”; let’s stop here, and just let everything be automated?&lt;/p&gt;
    &lt;p&gt;I don’t think so. And the reason is ultimately because of computational irreducibility. We try to get the world to be “just so”, say set up so we’re “predictably comfortable”. Well, the problem is that there’s inevitably computational irreducibility in the way things develop—not just in nature, but in things like societal dynamics too. And that means that things won’t stay “just so”. There’ll always be something unpredictable that happens; something that the automation doesn’t cover.&lt;/p&gt;
    &lt;p&gt;At first we humans might just say “we don’t care about that”. But in time computational irreducibility will affect everything. So if there’s anything at all we care about (including, for example, not going extinct), we’ll eventually have to do something—and go beyond whatever automation was already set up.&lt;/p&gt;
    &lt;p&gt;It’s easy to find practical examples. We might think that when computers and people are all connected in a seamless automated network, there’d be nothing more to do. But what about the “unintended consequence” of computer security issues? What might have seemed like a case where “technology finished things” quickly creates a new kind of job for people to do. And at some level, computational irreducibility implies that things like this must always happen. There must always be a “frontier”. At least if there’s anything at all we want to preserve (like not going extinct).&lt;/p&gt;
    &lt;p&gt;But let’s come back to the situation here and now with AI. ChatGPT just automated all sorts of text-related tasks. It used to take lots of effort—and people—to write customized reports, letters, etc. But (at least so long as one’s dealing with situations where one doesn’t need 100% “correctness”) ChatGPT just automated a lot of that, so people aren’t needed for it anymore. But what will this mean? Well, it means that there’ll be a lot more customized reports, letters, etc. that can be produced. And that will lead to new kinds of jobs—managing, analyzing, validating etc. all that mass-customized text. Not to mention the need for prompt engineers (a job category that just didn’t exist until a few months ago), and what amount to AI wranglers, AI psychologists, etc.&lt;/p&gt;
    &lt;p&gt;But let’s talk about today’s “frontier” of jobs that haven’t been “automated away”. There’s one category that in many ways seems surprising to still be “with us”: jobs that involve lots of mechanical manipulation, like construction, fulfillment, food preparation, etc. But there’s a missing piece of technology here: there isn’t yet good general-purpose robotics (as there is general-purpose computing), and we humans still have the edge in dexterity, mechanical adaptability, etc. But I’m quite sure that in time—and perhaps quite suddenly—the necessary technology will be developed (and, yes, I have ideas about how to do it). And this will mean that most of today’s “mechanical manipulation” jobs will be “automated away”—and won’t need people to do them.&lt;/p&gt;
    &lt;p&gt;But then, just as in our other examples, this will mean that mechanical manipulation will become much easier and cheaper to do, and more of it will be done. Houses might routinely be built and dismantled. Products might routinely be picked up from wherever they’ve ended up, and redistributed. Vastly more ornate “food constructions” might become the norm. And each of these things—and many more—will open up new jobs.&lt;/p&gt;
    &lt;p&gt;But will every job that exists in the world today “on the frontier” eventually be automated? What about jobs where it seems like a large part of the value is just “having a human be there”? Jobs like flying a plane where one wants the “commitment” of the pilot being there in the plane. Caregiver jobs where one wants the “connection” of a human being there. Sales or education jobs where one wants “human persuasion” or “human encouragement”. Today one might think “only a human can make one feel that way”. But that’s typically based on the way the job is done now. And maybe there’ll be different ways found that allow the essence of the task to be automated, almost inevitably opening up new tasks to be done.&lt;/p&gt;
    &lt;p&gt;For example, something that in the past needed “human persuasion” might be “automated” by something like gamification—but then more of it can be done, with new needs for design, analytics, management, etc.&lt;/p&gt;
    &lt;p&gt;We’ve been talking about “jobs”. And that term immediately brings to mind wages, economics, etc. And, yes, plenty of what people do (at least in the world as it is today) is driven by issues of economics. But plenty is also not. There are things we “just want to do”—as a “social matter”, for “entertainment”, for “personal satisfaction”, etc.&lt;/p&gt;
    &lt;p&gt;Why do we want to do these things? Some of it seems intrinsic to our biological nature. Some of it seems determined by the “cultural environment” in which we find ourselves. Why might one walk on a treadmill? In today’s world one might explain that it’s good for health, lifespan, etc. But a few centuries ago, without modern scientific understanding, and with a different view of the significance of life and death, that explanation really wouldn’t work.&lt;/p&gt;
    &lt;p&gt;What drives such changes in our view of what we “want to do”, or “should do”? Some seems to be driven by the pure “dynamics of society”, presumably with its own computational irreducibility. But some has to do with our ways of interacting with the world—both the increasing automation delivered by the advance of technology, and the increasing abstraction delivered by the advance of knowledge.&lt;/p&gt;
    &lt;p&gt;And there seem to be similar “cycles” seen here as in the kinds of things we consider to be “occupations” or “jobs”. For a while something is hard to do, and serves as a good “pastime”. But then it gets “too easy” (“everybody now knows how to win at game X”, etc.), and something at a “higher level” takes its place.&lt;/p&gt;
    &lt;p&gt;About our “base” biologically driven motivations it doesn’t seem like anything has really changed in the course of human history. But there are certainly technological developments that could have an effect in the future. Effective human immortality, for example, would change many aspects of our motivation structure. As would things like the ability to implant memories or, for that matter, implant motivations.&lt;/p&gt;
    &lt;p&gt;For now, there’s a certain element of what we want to do that’s “anchored” by our biological nature. But at some point we’ll surely be able to emulate with a computer at least the essence of what our brains are doing (and indeed the success of things like ChatGPT makes it seems like the moment when that will happen is closer at hand than we might have thought). And at that point we’ll have the possibility of what amount to “disembodied human souls”.&lt;/p&gt;
    &lt;p&gt;To us today it’s very hard to imagine what the “motivations” of such a “disembodied soul” might be. Looked at “from the outside” we might “see the soul” doing things that “don’t make much sense” to us. But it’s like asking what someone from a thousand years ago would think about many of our activities today. These activities make sense to us today because we’re embedded in our whole “current framework”. But without that framework they don’t make sense. And so it will be for the “disembodied soul”. To us, what it does may not make sense. But to it, with its “current framework”, it will.&lt;/p&gt;
    &lt;p&gt;Could we “learn how to make sense of it”? There’s likely to be a certain barrier of computational irreducibility: in effect the only way to “understand the soul of the future” is to retrace its steps to get to where it is. So from our vantage point today, we’re separated by a certain “irreducible distance”, in effect in rulial space.&lt;/p&gt;
    &lt;p&gt;But could there be some science of the future that will at least tell us general things about how such “souls” behave? Even when there’s computational irreducibility we know that there will always be pockets of computational reducibility—and thus features of behavior that are predictable. But will those features be “interesting”, say from our vantage point today? Maybe some of them will be. Maybe they’ll show us some kind of metapsychology of souls. But inevitably they can only go so far. Because in order for those souls to even experience the passage of time there has to be computational irreducibility. If too much of what happens is too predictable, it’s as if “nothing is happening”—or at least nothing “meaningful”.&lt;/p&gt;
    &lt;p&gt;And, yes, this is all tied up with questions about “free will”. Even when there’s a disembodied soul that’s operating according to some completely deterministic underlying program, computational irreducibility means its behavior can still “seem free”—because nothing can “outrun it” and say what it’s going to be. And the “inner experience” of the disembodied soul can be significant: it’s “intrinsically defining its future”, not just “having its future defined for it”.&lt;/p&gt;
    &lt;p&gt;One might have assumed that once everything is just “visibly operating” as “mere computation” it would necessarily be “soulless” and “meaningless”. But computational irreducibility is what breaks out of this, and what allows there to be something irreducible and “meaningful” achieved. And it’s the same phenomenon whether one’s talking about our life now in the physical universe, or a future “disembodied” computational existence. Or in other words, even if absolutely everything—even our very existence—has been “automated by computation”, that doesn’t mean we can’t have a perfectly good “inner experience” of meaningful existence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generalized Economics and the Concept of Progress&lt;/head&gt;
    &lt;p&gt;If we look at human history—or, for that matter, the history of life on Earth—there’s a certain pervasive sense that there’s some kind of “progress” happening. But what fundamentally is this “progress”? One can view it as the process of things being done at a progressively “higher level”, so that in effect “more of what’s important” can happen with a given effort. This idea of “going to a higher level” takes many forms—but they’re all fundamentally about eliding details below, and being able to operate purely in terms of the “things one cares about”.&lt;/p&gt;
    &lt;p&gt;In technology, this shows up as automation, in which what used to take lots of detailed steps gets packaged into something that can be done “with the push of a button”. In science—and the intellectual realm in general—it shows up as abstraction, where what used to involve lots of specific details gets packaged into something that can be talked about “purely collectively”. And in biology it shows up as some structure (ribosome, cell, wing, etc.) that can be treated as a “modular unit”.&lt;/p&gt;
    &lt;p&gt;That it’s possible to “do things at a higher level” is a reflection of being able to find “pockets of computational reducibility”. And—as we mentioned above—the fact that (given underlying computational irreducibility) there are necessarily an infinite number of such pockets means that “progress can always go on forever”.&lt;/p&gt;
    &lt;p&gt;When it comes to human affairs we tend to value such progress highly, because (at least for now) we live finite lives, and insofar as we “want more to happen”, “progress” makes that possible. It’s certainly not self-evident that having more happen is “good”; one might just “want a quiet life”. But there is one constraint that in a sense originates from the deep foundations of biology.&lt;/p&gt;
    &lt;p&gt;If something doesn’t exist, then nothing can ever “happen to it”. So in biology, if one’s going to have anything “happen” with organisms, they’d better not be extinct. But the physical environment in which biological organisms exist is finite, with many resources that are finite. And given organisms with finite lives, there’s an inevitability to the process of biological evolution, and to the “competition” for resources between organisms.&lt;/p&gt;
    &lt;p&gt;Will there eventually be an “ultimate winning organism”? Well, no, there can’t be—because of computational irreducibility. There’ll in a sense always be more to explore in the computational universe—more “raw computational material for possible organisms”. And given any “fitness criterion” (like—in a Turing machine analog—“living longer before halting”) there’ll always be a way to “do better” with it.&lt;/p&gt;
    &lt;p&gt;One might still wonder, however, whether perhaps biological evolution—with its underlying process of random genetic mutation—could “get stuck” and never be able to discover some “way to do better”. And indeed simple models of evolution might give one the intuition that this would happen. But actual evolution seems more like deep learning with a large neural net—where one’s effectively operating in an extremely high-dimensional space where there’s typically always a “way to get there from here”, at least given enough time.&lt;/p&gt;
    &lt;p&gt;But, OK, so from our history of biological evolution there’s a certain built-in sense of “competition for scarce resources”. And this sense of competition has (so far) also carried over to human affairs. And indeed it’s the basic driver for most of the processes of economics.&lt;/p&gt;
    &lt;p&gt;But what if resources aren’t “scarce” anymore? What if progress—in the form of automation, or AI—makes it easy to “get anything one wants”? We might imagine robots building everything, AIs figuring everything out, etc. But there are still things that are inevitably scarce. There’s only so much real estate. Only one thing can be “the first ___”. And, in the end, if we have finite lives, we only have so much time.&lt;/p&gt;
    &lt;p&gt;Still, the more efficient—or high level—the things we do (or have) are, the more we’ll be able to get done in the time we have. And it seems as if what we perceive as “economic value” is intimately connected with “making things higher level”. A finished phone is “worth more” than its raw materials. An organization is “worth more” than its separate parts. But what if we could have “infinite automation”? Then in a sense there’d be “infinite economic value everywhere”, and one might imagine there’d be “no competition left”.&lt;/p&gt;
    &lt;p&gt;But once again computational irreducibility stands in the way. Because it tells us there’ll never be “infinite automation”, just as there’ll never be an ultimate winning biological organism. There’ll always be “more to explore” in the computational universe, and different paths to follow.&lt;/p&gt;
    &lt;p&gt;What will this look like in practice? Presumably it’ll lead to all sorts of diversity. So that, for example, a chart of “what the components of an economy are” will become more and more fragmented; it won’t just be “the single winning economic activity is ___”.&lt;/p&gt;
    &lt;p&gt;There is one potential wrinkle in this picture of unending progress. What if nobody cares? What if the innovations and discoveries just don’t matter, say to us humans? And, yes, there is of course plenty in the world that at any given time in history we don’t care about. That piece of silicon we’ve been able to pick out? It’s just part of a rock. Well, until we start making microprocessors out of it.&lt;/p&gt;
    &lt;p&gt;But as we’ve discussed, as soon as we’re “operating at some level of abstraction” computational irreducibility makes it inevitable that we’ll eventually be exposed to things that “require going beyond that level”.&lt;/p&gt;
    &lt;p&gt;But then—critically—there will be choices. There will be different paths to explore (or “mine”) in the computational universe—in the end infinitely many of them. And whatever the computational resources of AIs etc. might be, they’ll never be able to explore all of them. So something—or someone—will have to make a choice of which ones to take.&lt;/p&gt;
    &lt;p&gt;Given a particular set of things one cares about at a particular point, one might successfully be able to automate all of them. But computational irreducibility implies there will always be a “frontier”, where choices have to be made. And there’s no “right answer”; no “theoretically derivable” conclusion. Instead, if we humans are involved, this is where we get to define what’s going to happen.&lt;/p&gt;
    &lt;p&gt;How will we do that? Well, ultimately it’ll be based on our history—biological, cultural, etc. We’ll get to use all that irreducible computation that went into getting us to where we are to define what to do next. In a sense it’ll be something that goes “through us”, and that uses what we are. It’s the place where—even when there’s automation all around—there’s still always something us humans can “meaningfully” do.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Can We Tell the AIs What to Do?&lt;/head&gt;
    &lt;p&gt;Let’s say we want an AI (or any computational system) to do a particular thing. We might think we could just set up its rules (or “program it”) to do that thing. And indeed for certain kinds of tasks that works just fine. But the deeper the use we make of computation, the more we’re going to run into computational irreducibility, and the less we’ll be able to know how to set up particular rules to achieve what we want.&lt;/p&gt;
    &lt;p&gt;And then, of course, there’s the question of defining what “we want” in the first place. Yes, we could have specific rules that say what particular pattern of bits should occur at a particular point in a computation. But that probably won’t have much to do with the kind of overall “human-level” objective that we typically care about. And indeed for any objective we can even reasonably define, we’d better be able to coherently “form a thought” about it. Or, in effect, we’d better have some “human-level narrative” to describe it.&lt;/p&gt;
    &lt;p&gt;But how can we represent such a narrative? Well, we have natural language—probably the single most important innovation in the history of our species. And what natural language fundamentally does is to allow us to talk about things at a “human level”. It’s made of words that we can think of as representing “human-level packets of meaning”. And so, for example, the word “chair” represents the human-level concept of a chair. It’s not referring to some particular arrangement of atoms. Instead, it’s referring to any arrangement of atoms that we can usefully conflate into the single human-level concept of a chair, and from which we can deduce things like the fact that we can expect to sit on it, etc.&lt;/p&gt;
    &lt;p&gt;So, OK, when we’re “talking to an AI” can we expect to just say what we want using natural language? We can definitely get a certain distance—and indeed ChatGPT helps us get further than ever before. But as we try to make things more precise we run into trouble, and the language we need rapidly becomes increasingly ornate, as in the “legalese” of complex legal documents. So what can we do? If we’re going to keep things at the level of “human thoughts” we can’t “reach down” into all the computational details. But yet we want a precise definition of how what we might say can be implemented in terms of those computational details.&lt;/p&gt;
    &lt;p&gt;Well, there’s a way to deal with this, and it’s one that I’ve personally devoted many decades to: it’s the idea of computational language. When we think about programming languages, they’re things that operate solely at the level of computational details, defining in more or less the native terms of a computer what the computer should do. But the point of a true computational language (and, yes, in the world today the Wolfram Language is the sole example) is to do something different: to define a precise way of talking in computational terms about things in the world (whether concretely countries or minerals, or abstractly computational or mathematical structures).&lt;/p&gt;
    &lt;p&gt;Out in the computational universe, there’s immense diversity in the “raw computation” that can happen. But there’s only a thin sliver of it that we humans (at least currently) care about and think about. And we can view computational language as defining a bridge between the things we think about and what’s computationally possible. The functions in our computational language (7000 or so of them in the Wolfram Language) are in effect like words in a human language—but now they have a precise grounding in the “bedrock” of explicit computation. And the point is to design the computational language so it’s convenient for us humans to think and express ourselves in (like a vastly expanded analog of mathematical notation), but so it can also be precisely implemented in practice on a computer.&lt;/p&gt;
    &lt;p&gt;Given a piece of natural language it’s often possible to give a precise, computational interpretation of it—in computational language. And indeed this is exactly what happens in Wolfram|Alpha. Give a piece of natural language and the Wolfram|Alpha NLU system will try to find an interpretation of it as computational language. And from this interpretation, it’s then up to the Wolfram Language to do the computation that’s specified, and give back the results—and potentially synthesize natural language to express them.&lt;/p&gt;
    &lt;p&gt;As a practical matter, this setup is useful not only for humans, but also for AIs—like ChatGPT. Given a system that produces natural language, the Wolfram|Alpha NLU system can “catch” natural language it is “thrown”, and interpret it as computational language that precisely specifies a potentially irreducible computation to do.&lt;/p&gt;
    &lt;p&gt;With both natural language and computational language one’s basically “directly saying what one wants”. But an alternative approach—more aligned with machine learning—is just to give examples, and (implicitly or explicitly) say “follow these”. Inevitably there has to be some underlying model for how to do that following—typically in practice just defined by “what a neural net with a certain architecture will do”. But will the result be “right”? Well, the result will be whatever the neural net gives. But typically we’ll tend to consider it “right” if it’s somehow consistent with what we humans would have concluded. And in practice this often seems to happen, presumably because the actual architecture of our brains is somehow similar enough to the architecture of the neural nets we’re using.&lt;/p&gt;
    &lt;p&gt;But what if we want to “know for sure” what’s going to happen—or, for example, that some particular “mistake” can never be made? Well then we’re presumably thrust back into computational irreducibility, with the result that there’s no way to know, for example, whether a particular set of training examples can lead to a system that’s capable of doing (or not doing) some particular thing.&lt;/p&gt;
    &lt;p&gt;OK, but let’s say we’re setting up some AI system, and we want to make sure it “doesn’t do anything bad”. There are several levels of issues here. The first is to decide what we mean by “anything bad”. And, as we’ll discuss below, that in itself is very hard. But even if we could abstractly figure this out, how should we actually express it? We could give examples—but then the AI will inevitably have to “extrapolate” from them, in ways we can’t predict. Or we could describe what we want in computational language. It might be difficult to cover “every case” (as it is in present-day human laws, or complex contracts). But at least we as humans can read what we’re specifying. Though even in this case, there’s an issue of computational irreducibility: that given the specification it won’t be possible to work out all its consequences.&lt;/p&gt;
    &lt;p&gt;What does all this mean? In essence it’s just a reflection of the fact that as soon as there’s “serious computation” (i.e. irreducible computation) involved, one isn’t going to be immediately able to say what will happen. (And in a sense that’s inevitable, because if one could say, it would mean the computation wasn’t in fact irreducible.) So, yes, we can try to “tell AIs what to do”. But it’ll be like many systems in nature (or, for that matter, people): you can set them on a path, but you can’t know for sure what will happen; you just have to wait and see.&lt;/p&gt;
    &lt;head rend="h2"&gt;A World Run by AIs&lt;/head&gt;
    &lt;p&gt;In the world today, there are already plenty of things that are being done by AIs. And, as we’ve discussed, there’ll surely be more in the future. But who’s “in charge”? Are we telling the AIs what to do, or are they telling us? Today it’s at best a mixture: AIs suggest content for us (for example from the web), and in general make all sorts of recommendations about what we should do. And no doubt in the future those recommendations will be even more extensive and tightly coupled to us: we’ll be recording everything we do, processing it with AI, and continually annotating with recommendations—say through augmented reality—everything we see. And in some sense things might even go beyond “recommendations”. If we have direct neural interfaces, then we might be making our brains just “decide” they want to do things, so that in some sense we become pure “puppets of the AI”.&lt;/p&gt;
    &lt;p&gt;And beyond “personal recommendations” there’s also the question of AIs running the systems we use, or in fact running the whole infrastructure of our civilization. Today we ultimately expect people to make large-scale decisions for our world—often operating in systems of rules defined by laws, and perhaps aided by computation, and even what one might call AI. But there may well come a time when it seems as if AIs could just “do a better job than humans”, say at running a central bank or waging a war.&lt;/p&gt;
    &lt;p&gt;One might ask how one would ever know if the AI would “do a better job”. Well, one could try tests, and run examples. But once again one’s faced with computational irreducibility. Yes, the particular tests one tries might work fine. But one can’t ultimately predict everything that could happen. What will the AI do if there’s suddenly a never-before-seen seismic event? We basically won’t know until it happens.&lt;/p&gt;
    &lt;p&gt;But can we be sure the AI won’t do anything “crazy”? Could we—with some definition of “crazy”—effectively “prove a theorem” that the AI can never do that? For any realistically nontrivial definition of crazy we’ll again run into computational irreducibility—and this won’t be possible.&lt;/p&gt;
    &lt;p&gt;Of course, if we’ve put a person (or even a group of people) “in charge” there’s also no way to “prove” that they won’t do anything “crazy”—and history shows that people in charge quite often have done things that, at least in retrospect, we consider “crazy”. But even though at some level there’s no more certainty about what people will do than about what AIs might do, we still get a certain comfort when people are in charge if we think that “we’re in it together”, and that if something goes wrong those people will also “feel the effects”.&lt;/p&gt;
    &lt;p&gt;But still, it seems inevitable that lots of decisions and actions in the world will be taken directly by AIs. Perhaps it’ll be because this will be cheaper. Perhaps the results (based on tests) will be better. Or perhaps, for example, things will just have to be done too quickly and in numbers too large for us humans to be in the loop.&lt;/p&gt;
    &lt;p&gt;But, OK, if a lot of what happens in our world is happening through AIs, and the AIs are effectively doing irreducible computations, what will this be like? We’ll be in a situation where things are “just happening” and we don’t quite know why. But in a sense we’ve very much been in this situation before. Because it’s what happens all the time in our interaction with nature.&lt;/p&gt;
    &lt;p&gt;Processes in nature—like, for example, the weather—can be thought of as corresponding to computations. And much of the time there’ll be irreducibility in those computations. So we won’t be able to readily predict them. Yes, we can do natural science to figure out some aspects of what’s going to happen. But it’ll inevitably be limited.&lt;/p&gt;
    &lt;p&gt;And so we can expect it to be with the “AI infrastructure” of the world. Things are happening in it—as they are in the weather—that we can’t readily predict. We’ll be able to say some things—though perhaps in ways that are closer to psychology or social science than to traditional exact science. But there’ll be surprises—like maybe some strange AI analog of a hurricane or an ice age. And in the end all we’ll really be able to do is to try to build up our human civilization so that such things “don’t fundamentally matter” to it.&lt;/p&gt;
    &lt;p&gt;In a sense the picture we have is that in time there’ll be a whole “civilization of AIs” operating—like nature—in ways that we can’t readily understand. And like with nature, we’ll coexist with it.&lt;/p&gt;
    &lt;p&gt;But at least at first we might think there’s an important difference between nature and AIs. Because we imagine that we don’t “pick our natural laws”—yet insofar as we’re the ones building the AIs we imagine we can “pick their laws”. But both parts of this aren’t quite right. Because in fact one of the implications of our Physics Project is precisely that the laws of nature that we perceive are the way they are because we are observers who are the way we are. And on the AI side, computational irreducibility implies that we can’t expect to be able to determine the final behavior of the AIs just from knowing the underlying laws we gave them.&lt;/p&gt;
    &lt;p&gt;But what will the “emergent laws” of the AIs be? Well, just like in physics, it’ll depend on how we “sample” the behavior of the AIs. If we look down at the level of individual bits, it’ll be like looking at molecular dynamics (or the behavior of atoms of space). But typically we won’t do this. And just like in physics, we’ll operate as computationally bounded observers—measuring only certain aggregated features of an underlying computationally irreducible process. But what will the “overall laws of AIs” be like? Maybe they’ll show close analogies to physics. Or maybe they’ll seem more like psychological theories (superegos for AIs?). But we can expect them in many ways to be like large-scale laws of nature of the kind we know.&lt;/p&gt;
    &lt;p&gt;Still, there’s one more difference between at least our interaction with nature and with AIs. Because we have in effect been “co-evolving” with nature for billions of years—yet AIs are “new on the scene”. And through our co-evolution with nature we’ve developed all sorts of structural, sensory and cognitive features that allow us to “interact successfully” with nature. But with AIs we don’t have these. So what does this mean?&lt;/p&gt;
    &lt;p&gt;Well, our ways of interacting with nature can be thought of as leveraging pockets of computational reducibility that exist in natural processes—to make things seem at least somewhat predictable to us. But without having found such pockets for AIs, we’re likely to be faced with much more “raw computational irreducibility”—and thus much more unpredictability. It’s been a conceit of modern times that—particularly with the help of science—we’ve been able to make more and more of our world predictable to us, though in practice a large part of what’s led to this is the way we’ve built and controlled the environment in which we live, and the things we choose to do.&lt;/p&gt;
    &lt;p&gt;But for the new “AI world”, we’re effectively starting from scratch. And to make things predictable in that world may be partly a matter of some new science, but perhaps more importantly a matter of choosing how we set up our “way of life” around the AIs there. (And, yes, if there’s lots of unpredictability we may be back to more ancient points of view about the importance of fate—or we may view AIs as a bit like the Olympians of Greek mythology, duking it out among themselves and sometimes having an effect on mortals.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Governance in an AI World&lt;/head&gt;
    &lt;p&gt;Let’s say the world is effectively being run by AIs, but let’s assume that we humans have at least some control over what they do. Then what principles should we have them follow? And what, for example, should their “ethics” be?&lt;/p&gt;
    &lt;p&gt;Well, the first thing to say is that there’s no ultimate, theoretical “right answer” to this. There are many ethical and other principles that AIs could follow. And it’s basically just a choice which ones should be followed.&lt;/p&gt;
    &lt;p&gt;When we talk about “principles” and “ethics” we tend to think more in terms of constraints on behavior than in terms of rules for generating behavior. And that means we’re dealing with something more like mathematical axioms, where we ask things like what theorems are true according to those axioms, and what are not. And that means there can be issues like whether the axioms are consistent—and whether they’re complete, in the sense that they can “determine the ethics of anything”. But now, once again, we’re face to face with computational irreducibility, here in the form of Gödel’s theorem and its generalizations.&lt;/p&gt;
    &lt;p&gt;And what this means is that it’s in general undecidable whether any given set of principles is inconsistent, or incomplete. One might “ask an ethical question”, and find that there’s a “proof chain” of unbounded length to determine what the answer to that question is within one’s specified ethical system, or whether there is even a consistent answer.&lt;/p&gt;
    &lt;p&gt;One might imagine that somehow one could add axioms to “patch up” whatever issues there are. But Gödel’s theorem basically says that it’ll never work. It’s the same story as so often with computational irreducibility: there’ll always be “new situations” that can arise, that in this case can’t be captured by a finite set of axioms.&lt;/p&gt;
    &lt;p&gt;OK, but let’s imagine we’re picking a collection of principles for AIs. What criteria could we use to do it? One might be that these principles won’t inexorably lead to a simple state—like one where the AIs are extinct, or have to keep looping doing the same thing forever. And there may be cases where one can readily see that some set of principles will lead to such outcomes. But most of the time, computational irreducibility (here in the form of things like the halting problem) will once again get in the way, and one won’t be able to tell what will happen, or successfully pick “viable principles” this way.&lt;/p&gt;
    &lt;p&gt;So this means that there are going to be a wide range of principles that we could in theory pick. But presumably what we’ll want is to pick ones that make AIs give us humans some sort of “good time”, whatever that might mean.&lt;/p&gt;
    &lt;p&gt;And a minimal idea might be to get AIs just to observe what we humans do, and then somehow imitate this. But most people wouldn’t consider this the right thing. They’d point out all the “bad” things people do. And they’d perhaps say “let’s have the AIs follow not what we actually do, but what we aspire to do”.&lt;/p&gt;
    &lt;p&gt;But where should we get these aspirations from? Different people, and different cultures, can have very different aspirations—with very different resulting principles. So whose should we pick? And, yes, there are pitifully few—if any—principles that we truly find in common everywhere. (Though, for example, the major religions all tend to share things like respect for human life, the Golden Rule, etc.)&lt;/p&gt;
    &lt;p&gt;But do we in fact have to pick one set of principles? Maybe some AIs can have some principles, and some can have others. Maybe it should be like different countries, or different online communities: different principles for different groups or in different places.&lt;/p&gt;
    &lt;p&gt;Right now that doesn’t seem plausible, because technological and commercial forces have tended to make it seem as if powerful AIs always have to be centralized. But I expect that this is just a feature of the present time, and not something intrinsic to any “human-like” AI.&lt;/p&gt;
    &lt;p&gt;So could everyone (and maybe every organization) have “their own AI” with its own principles? For some purposes this might work OK. But there are many situations where AIs (or people) can’t really act independently, and where there have to be “collective decisions” made.&lt;/p&gt;
    &lt;p&gt;Why is this? In some cases it’s because everyone is in the same physical environment. In other cases it’s because if there’s to be social cohesion—of the kind needed to support even something like a language that’s useful for communication—then there has to be certain conceptual alignment.&lt;/p&gt;
    &lt;p&gt;It’s worth pointing out, though, that at some level having a “collective conclusion” is effectively just a way of introducing certain computational reducibility to make it “easier to see what to do”. And potentially it can be avoided if one has enough computation capability. For example, one might assume that there has to be a collective conclusion about which side of the road cars should drive on. But that wouldn’t be true if every car had the computation capability to just compute a trajectory that would for example optimally weave around other cars using both sides of the road.&lt;/p&gt;
    &lt;p&gt;But if we humans are going to be in the loop, we presumably need a certain amount of computational reducibility to make our world sufficiently comprehensible to us that we can operate in it. So that means there’ll be collective—“societal”—decisions to make. We might want to just tell the AIs to “make everything as good as it can be for us”. But inevitably there will be tradeoffs. Making a collective decision one way might be really good for 99% of people, but really bad for 1%; making it the other way might be pretty good for 60%, but pretty bad for 40%. So what should the AI do?&lt;/p&gt;
    &lt;p&gt;And, of course, this is a classic problem of political philosophy, and there’s no “right answer”. And in reality the setup won’t be as clean as this. It may be fairly easy to work out some immediate effects of different courses of action. But inevitably one will eventually run into computational irreducibility—and “unintended consequences”—and so one won’t be able to say with certainty what the ultimate effects (good or bad) will be.&lt;/p&gt;
    &lt;p&gt;But, OK, so how should one actually make collective decisions? There’s no perfect answer, but in the world today, democracy in one form or another is usually viewed as the best option. So how might AI affect democracy—and perhaps improve on it? Let’s assume first that “humans are still in charge”, so that it’s ultimately their preferences that matter. (And let’s also assume that humans are more or less in their “current form”: unique and unreplicable discrete entities that believe they have independent minds.)&lt;/p&gt;
    &lt;p&gt;The basic setup for current democracy is computationally quite simple: discrete votes (or perhaps rankings) are given (sometimes with weights of various kinds), and then numerical totals are used to determine the winner (or winners). And with past technology this was pretty much all that could be done. But now there are some new elements. Imagine not casting discrete votes, but instead using computational language to write a computational essay to describe one’s preferences. Or imagine having a conversation with a linguistically enabled AI that can draw out and debate one’s preferences, and eventually summarize them in some kind of feature vector. Then imagine feeding computational essays or feature vectors from all “voters” to some AI that “works out the best thing to do”.&lt;/p&gt;
    &lt;p&gt;Well, there are still the same political philosophy issues. It’s not like 60% of people voted for A and 40% for B, so one chose A. It’s much more nuanced. But one still won’t be able to make everyone happy all the time, and one has to have some base principles to know what to do about that.&lt;/p&gt;
    &lt;p&gt;And there’s a higher-order problem in having an AI “rebalance” collective decisions all the time based on everything it knows about people’s detailed preferences (and perhaps their actions too): for many purposes—like us being able to “keep track of what’s going on”—it’s important to maintain consistency over time. But, yes, one could deal with this by having the AI somehow also weigh consistency in figuring out what to do.&lt;/p&gt;
    &lt;p&gt;But while there are no doubt ways in which AI can “tune up” democracy, AI doesn’t seem—in and of itself—to deliver any fundamentally new solution for making collective decisions, and for governance in general.&lt;/p&gt;
    &lt;p&gt;And indeed, in the end things always seem to come down to needing some fundamental set of principles about how one wants things to be. Yes, AIs can be the ones to implement these principles. But there are many possibilities for what the principles could be. And—at least if we humans are “in charge”—we’re the ones who are going to have to come up with them.&lt;/p&gt;
    &lt;p&gt;Or, in other words, we need to come up with some kind of “AI constitution”. Presumably this constitution should basically be written in precise computational language (and, yes, we’re trying to make it possible for the Wolfram Language to be used), but inevitably (as yet another consequence of computational irreducibility) there’ll be “fuzzy” definitions and distinctions, that will rely on things like examples, “interpolated” by systems like neural nets. Maybe when such a constitution is created, there’ll be multiple “renderings” of it, which can all be applied whenever the constitution is used, with some mechanism for picking the “overall conclusion”. (And, yes, there’s potentially a certain “observer-dependent” multicomputational character to this.)&lt;/p&gt;
    &lt;p&gt;But whatever its detailed mechanisms, what should the AI constitution say? Different people and groups of people will definitely come to different conclusions about it. And presumably—just as there are different countries, etc. today with different systems of laws—there’ll be different groups that want to adopt different AI constitutions. (And, yes, the same issues about collective decision making apply again when those AI constitutions have to interact.)&lt;/p&gt;
    &lt;p&gt;But given an AI constitution, one has a base on which AIs can make decisions. And on top of this one imagines a huge network of computational contracts that are autonomously executed, essentially to “run the world”.&lt;/p&gt;
    &lt;p&gt;And this is perhaps one of those classic “what could possibly go wrong?” moments. An AI constitution has been agreed on, and now everything is being run efficiently and autonomously by AIs that are following it. Well, once again, computational irreducibility rears its head. Because however carefully the AI constitution is drafted, computational irreducibility implies that one won’t be able to foresee all its consequences: “unexpected” things will always happen—and some of them will undoubtedly be things “one doesn’t like”.&lt;/p&gt;
    &lt;p&gt;In human legal systems there’s always a mechanism for adding “patches”—filling in laws or precedents that cover new situations that have come up. But if everything is being autonomously run by AIs there’s no room for that. Yes, we as humans might characterize “bad things that happen” as “bugs” that could be fixed by adding a patch. But the AI is just supposed to be operating—essentially axiomatically—according to its constitution, so it has no way to “see that it’s a bug”.&lt;/p&gt;
    &lt;p&gt;Similar to what we discussed above, there’s an interesting analogy here with human law versus natural law. Human law is something we define and can modify. Natural law is something the universe just provides us (notwithstanding the issues about observers discussed above). And by “setting an AI constitution and letting it run” we’re basically forcing ourselves into a situation where the “civilization of the AIs” is some “independent stratum” in the world, that we essentially have to take as it is, and adapt to.&lt;/p&gt;
    &lt;p&gt;Of course, one might wonder if the AI constitution could “automatically evolve”, say based on what’s actually seen to happen in the world. But one quickly returns to the exact same issues of computational irreducibility, where one can’t predict whether the evolution will be “right”, etc.&lt;/p&gt;
    &lt;p&gt;So far, we’ve assumed that in some sense “humans are in charge”. But at some level that’s an issue for the AI constitution to define. It’ll have to define whether AIs have “independent rights”—just like humans (and, in many legal systems, some other entities too). Closely related to the question of independent rights for AIs is whether an AI can be considered autonomously “responsible for its actions”—or whether such responsibility must always ultimately rest with the (presumably human) creator or “programmer” of the AI.&lt;/p&gt;
    &lt;p&gt;Once again, computational irreducibility has something to say. Because it implies that the behavior of the AI can go “irreducibly beyond” what its programmer defined. And in the end (as we discussed above) this is the same basic mechanism that allows us humans to effectively have “free will” even when we’re ultimately operating according to deterministic underlying natural laws. So if we’re going to claim that we humans have free will, and can be “responsible for our actions” (as opposed to having our actions always “dictated by underlying laws”) then we’d better claim the same for AIs.&lt;/p&gt;
    &lt;p&gt;So just as a human builds up something irreducible and irreplaceable in the course of their life, so can an AI. As a practical matter, though, AIs can presumably be backed up, copied, etc.—which isn’t (yet) possible for humans. So somehow their individual instances don’t seem as valuable, even if the “last copy” might still be valuable. As humans, we might want to say “those AIs are something inferior; they shouldn’t have rights”. But things are going to get more entangled. Imagine a bot that no longer has an identifiable owner but that’s successfully befriending people (say on social media), and paying for its underlying operation from donations, ads, etc. Can we reasonably delete that bot? We might argue that “the bot can feel no pain”—but that’s not true of its human friends. But what if the bot starts doing “bad” things? Well, then we’ll need some form of “bot justice”—and pretty soon we’ll find ourselves building a whole human-like legal structure for the AIs.&lt;/p&gt;
    &lt;head rend="h2"&gt;So Will It End Badly?&lt;/head&gt;
    &lt;p&gt;OK, so AIs will learn what they can from us humans, then they’ll fundamentally just be running as autonomous computational systems—much like nature runs as an autonomous computational system—sometimes “interacting with us”. What will they “do to us”? Well, what does nature “do to us”? In a kind of animistic way, we might attribute intentions to nature, but ultimately it’s just “following its rules” and doing what it does. And so it will be with AIs. Yes, we might think we can set things up to determine what the AIs will do. But in the end—insofar as the AIs are really making use of what’s possible in the computational universe—there’ll inevitably be computational irreducibility, and we won’t be able to foresee what will happen, or what consequences it will have.&lt;/p&gt;
    &lt;p&gt;So will the dynamics of AIs in fact have “bad” effects—like, for example, wiping us out? Well, it’s perfectly possible nature could wipe us out too. But one has the feeling that—extraterrestrial “accidents” aside—the natural world around us is at some level enough in some kind of “equilibrium” that nothing too dramatic will happen. But AIs are something new. So maybe they’ll be different.&lt;/p&gt;
    &lt;p&gt;And one possibility might be that AIs could “improve themselves” to produce a single “apex intelligence” that would in a sense dominate everything else. But here we can see computational irreducibility as coming to the rescue. Because it implies that there can never be a “best at everything” computational system. It’s a core result of the emerging field of metabiology: that whatever “achievement” you specify, there’ll always be a computational system somewhere out there in the computational universe that will exceed it. (A simple example is that there’s always a Turing machine that can be found that will exceed any upper bound you specify on the time it takes to halt.)&lt;/p&gt;
    &lt;p&gt;So what this means is that there’ll inevitably be a whole “ecosystem” of AIs—with no single winner. Of course, while that might be an inevitable final outcome, it might not be what happens in the shorter term. And indeed the current tendency to centralize AI systems has a certain danger of AI behavior becoming “unstabilized” relative to what it would be with a whole ecosystem of “AIs in equilibrium”.&lt;/p&gt;
    &lt;p&gt;And in this situation there’s another potential concern as well. We humans are the product of a long struggle for life played out over the course of the history of biological evolution. And insofar as AIs inherit our attributes we might expect them to inherit a certain “drive to win”—perhaps also against us. And perhaps this is where the AI constitution becomes important: to define a “contract” that supersedes what AIs might “naturally” inherit from effectively observing our behavior. Eventually we can expect the AIs to “independently reach equilibrium”. But in the meantime, the AI constitution can help break their connection with our “competitive” history of biological evolution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preparing for an AI World&lt;/head&gt;
    &lt;p&gt;We’ve talked quite a bit about the ultimate future course of AIs, and their relation to us humans. But what about the short term? How today can we prepare for the growing capabilities and uses of AIs?&lt;/p&gt;
    &lt;p&gt;As has been true throughout history, people who use tools tend to do better than those who don’t. Yes, you can go on doing by direct human effort what has now been successfully automated, but except in rare cases you’ll increasingly be left behind. And what’s now emerging is an extremely powerful combination of tools: neural-net-style AI for “immediate human-like tasks”, along with computational language for deeper access to the computational universe and computational knowledge.&lt;/p&gt;
    &lt;p&gt;So what should people do with this? The highest leverage will come from figuring out new possibilities—things that weren’t possible before but have now “come into range” as a result of new capabilities. And as we discussed above, this is a place where we humans are inevitably central contributors—because we’re the ones who must define what we consider has value for us.&lt;/p&gt;
    &lt;p&gt;So what does this mean for education? What’s worth learning now that so much has been automated? I think the fundamental answer is how to think as broadly and deeply as possible—calling on as much knowledge and as many paradigms as possible, and particularly making use of the computational paradigm, and ways of thinking about things that directly connect with what computation can help with.&lt;/p&gt;
    &lt;p&gt;In the course of human history a lot of knowledge has been accumulated. But as ways of thinking have advanced, it’s become unnecessary to learn directly that knowledge in all its detail: instead one can learn things at a higher level, abstracting out many of the specific details. But in the past few decades something fundamentally new has come on the scene: computers and the things they enable.&lt;/p&gt;
    &lt;p&gt;For the first time in history, it’s become realistic to truly automate intellectual tasks. The leverage this provides is completely unprecedented. And we’re only just starting to come to terms with what it means for what and how we should learn. But with all this new power there’s a tendency to think something must be lost. Surely it must still be worth learning all those intricate details—that people in the past worked so hard to figure out—of how to do some mathematical calculation, even though Mathematica has been able to do it automatically for more than a third of a century?&lt;/p&gt;
    &lt;p&gt;And, yes, at the right time it can be interesting to learn those details. But in the effort to understand and best make use of the intellectual achievements of our civilization, it makes much more sense to leverage the automation we have, and treat those calculations just as “building blocks” that can be put together in “finished form” to do whatever it is we want to do.&lt;/p&gt;
    &lt;p&gt;One might think this kind of leveraging of automation would just be important for “practical purposes”, and for applying knowledge in the real world. But actually—as I have personally found repeatedly to great benefit over the decades—it’s also crucial at a conceptual level. Because it’s only through automation that one can get enough examples and experience that one’s able to develop the intuition needed to reach a higher level of understanding.&lt;/p&gt;
    &lt;p&gt;Confronted with the rapidly growing amount of knowledge in the world there’s been a tremendous tendency to assume that people must inevitably become more and more specialized. But with increasing success in the automation of intellectual tasks—and what we might broadly call AI—it becomes clear there’s an alternative: to make more and more use of this automation, so people can operate at a higher level, “integrating” rather than specializing.&lt;/p&gt;
    &lt;p&gt;And in a sense this is the way to make the best use of our human capabilities: to let us concentrate on setting the “strategy” of what we want to do—delegating the details of how to do it to automated systems that can do it better than us. But, by the way, the very fact that there’s an AI that knows how to do something will no doubt make it easier for humans to learn how to do it too. Because—although we don’t yet have the complete story—it seems inevitable that with modern techniques AIs will be able to successfully “learn how people learn”, and effectively present things an AI “knows” in just the right way for any given person to absorb.&lt;/p&gt;
    &lt;p&gt;So what should people actually learn? Learn how to use tools to do things. But also learn what things are out there to do—and learn facts to anchor how you think about those things. A lot of education today is about answering questions. But for the future—with AI in the picture—what’s likely to be more important is to learn how to ask questions, and how to figure out what questions are worth asking. Or, in effect, how to lay out an “intellectual strategy” for what to do.&lt;/p&gt;
    &lt;p&gt;And to be successful at this, what’s going to be important is breadth of knowledge—and clarity of thinking. And when it comes to clarity of thinking, there’s again something new in modern times: the concept of computational thinking. In the past we’ve had things like logic, and mathematics, as ways to structure thinking. But now we have something new: computation.&lt;/p&gt;
    &lt;p&gt;Does that mean everyone should “learn to program” in some traditional programming language? No. Traditional programming languages are about telling computers what to do in their terms. And, yes, lots of humans do this today. But it’s something that’s fundamentally ripe for direct automation (as examples with ChatGPT already show). And what’s important for the long term is something different. It’s to use the computational paradigm as a structured way to think not about the operation of computers, but about both things in the world and abstract things.&lt;/p&gt;
    &lt;p&gt;And crucial to this is having a computational language: a language for expressing things using the computational paradigm. It’s perfectly possible to express simple “everyday things” in plain, unstructured natural language. But to build any kind of serious “conceptual tower” one needs something more structured. And that’s what computational language is about.&lt;/p&gt;
    &lt;p&gt;One can see a rough historical analog in the development of mathematics and mathematical thinking. Up until about half a millennium ago, mathematics basically had to be expressed in natural language. But then came mathematical notation—and from it a more streamlined approach to mathematical thinking, that eventually made possible all the various mathematical sciences. And it’s now the same kind of thing with computational language and the computational paradigm. Except that it’s a much broader story, in which for basically every field or occupation “X” there’s a “computational X” that’s emerging.&lt;/p&gt;
    &lt;p&gt;In a sense the point of computational language (and all my efforts in the development of the Wolfram Language) is to be able to let people get “as automatically as possible” to computational X—and to let people express themselves using the full power of the computational paradigm.&lt;/p&gt;
    &lt;p&gt;Something like ChatGPT provides “human-like AI” in effect by piecing together existing human material (like billions of words of human-written text). But computational language lets one tap directly into computation—and gives the ability to do fundamentally new things, that immediately leverage our human capabilities for defining intellectual strategy.&lt;/p&gt;
    &lt;p&gt;And, yes, while traditional programming is likely to be largely obsoleted by AI, computational language is something that provides a permanent bridge between human thinking and the computational universe: a channel in which the automation is already done in the very design (and implementation) of the language—leaving in a sense an interface directly suitable for humans to learn, and to use as a basis to extend their thinking.&lt;/p&gt;
    &lt;p&gt;But, OK, what about the future of discovery? Will AIs take over from us humans in, for example, “doing science”? I, for one, have used computation (and many things one might think of as AI) as a tool for scientific discovery for nearly half a century. And, yes, many of my discoveries have in effect been “made by computer”. But science is ultimately about connecting things to human understanding. And so far it’s taken a human to knit what the computer finds into the whole web of human intellectual history.&lt;/p&gt;
    &lt;p&gt;One can certainly imagine, though, that an AI—even one rather like ChatGPT—could be quite successful in taking a “raw computational discovery” and “explaining” how it might relate to existing human knowledge. One could also imagine that the AI would be successful at identifying what aspects of some system in the world could be picked out to describe in some formal way. But—as is typical for the process of modeling in general—a key step is to decide “what one cares about”, and in effect in what direction to go in extending one’s science. And this—like so much else—is inevitably tied into the specifics of the goals we humans set ourselves.&lt;/p&gt;
    &lt;p&gt;In the emerging AI world there are plenty of specific skills that won’t make sense for (most) humans to learn—just as today the advance of automation has obsoleted many skills from the past. But—as we’ve discussed—we can expect there to “be a place” for humans. And what’s most important for us humans to learn is in effect how to pick “where next to go”—and where, out of all the infinite possibilities in the computational universe, we should take human civilization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Afterword: Looking at Some Actual Data&lt;/head&gt;
    &lt;p&gt;OK, so we’ve talked quite a bit about what might happen in the future. But what about actual data from the past? For example, what’s been the actual history of the evolution of jobs? Conveniently, in the US, the Census Bureau has records of people’s occupations going back to 1850. Of course, many job titles have changed since then. Switchmen (on railroads), chainmen (in surveying) and sextons (in churches) aren’t really things anymore. And telemarketers, aircraft pilots and web developers weren’t things in 1850. But with a bit of effort, it’s possible to more or less match things up—at least if one aggregates into large enough categories.&lt;/p&gt;
    &lt;p&gt;So here are pie charts of different job categories at 50-year intervals:&lt;/p&gt;
    &lt;p&gt;And, yes, in 1850 the US was firmly an agricultural economy, with just over half of all jobs being in agriculture. But as agriculture got more efficient—with the introduction of machinery, irrigation, better seeds, fertilizers, etc.—the fraction dropped dramatically, to just a few percent today.&lt;/p&gt;
    &lt;p&gt;After agriculture, the next biggest category back in 1850 was construction (along with other real-estate-related jobs, mainly maintenance). And this is a category that for a century and a half hasn’t changed much in size (at least so far), presumably because, even though there’s been greater automation, this has just allowed buildings to be more complex.&lt;/p&gt;
    &lt;p&gt;Looking at the pie charts above, we can see a clear trend towards greater diversification in jobs (and indeed the same thing is seen in the development of other economies around the world). It’s an old theory in economics that increasing specialization is related to economic growth, but from our point of view here, we might say that the very possibility of a more complex economy, with more niches and jobs, is a reflection of the inevitable presence of computational irreducibility, and the complex web of pockets of computational reducibility that it implies.&lt;/p&gt;
    &lt;p&gt;Beyond the overall distribution of job categories, we can also look at trends in individual categories over time—with each one in a sense providing a certain window onto history:&lt;/p&gt;
    &lt;p&gt;One can definitely see cases where the number of jobs decreases as a result of automation. And this happens not only in areas like agriculture and mining, but also for example in finance (fewer clerks and bank tellers), as well as in sales and retail (online shopping). Sometimes—as in the case of manufacturing—there’s a decrease of jobs partly because of automation, and partly because the jobs move out of the US (mainly to countries with lower labor costs).&lt;/p&gt;
    &lt;p&gt;There are cases—like military jobs—where there are clear “exogenous” effects. And then there are cases like transportation+logistics where there’s a steady increase for more than half a century as technology spreads and infrastructure gets built up—but then things “saturate”, presumably at least partly as a result of increased automation. It’s a somewhat similar story with what I’ve called “technical operations”—with more “tending to technology” needed as technology becomes more widespread.&lt;/p&gt;
    &lt;p&gt;Another clear trend is an increase in job categories associated with the world becoming an “organizationally more complicated place”. Thus we see increases in management, as well as administration, government, finance and sales (which all have recent decreases as a result of computerization). And there’s also a (somewhat recent) increase in legal.&lt;/p&gt;
    &lt;p&gt;Other areas with increases include healthcare, engineering, science and education—where “more is known and there’s more to do” (as well as there being increased organizational complexity). And then there’s entertainment, and food+hospitality, with increases that one might attribute to people leading (and wanting) “more complex lives”. And, of course, there’s information technology which takes off from nothing in the mid-1950s (and which had to be rather awkwardly grafted into the data we’re using here).&lt;/p&gt;
    &lt;p&gt;So what can we conclude? The data seems quite well aligned with what we discussed in more general terms above. Well-developed areas get automated and need to employ fewer people. But technology also opens up new areas, which employ additional people. And—as we might expect from computational irreducibility—things generally get progressively more complicated, with additional knowledge and organizational structure opening up more “frontiers” where people are needed. But even though there are sometimes “sudden inventions”, it still always seems to take decades (or effectively a generation) for there to be any dramatic change in the number of jobs. (The few sharp changes visible in the plots seem mostly to be associated with specific economic events, and—often related—changes in government policies.)&lt;/p&gt;
    &lt;p&gt;But in addition to the different jobs that get done, there’s also the question of how individual people spend their time each day. And—while it certainly doesn’t live up to my own (rather extreme) level of personal analytics—there’s a certain amount of data on this that’s been collected over the years (by getting time diaries from randomly sampled people) in the American Heritage Time Use Study. So here, for example, are plots based on this survey for how the amount of time spent on different broad activities has varied over the decades (the main line shows the mean—in hours—for each activity; the shaded areas indicate successive deciles):&lt;/p&gt;
    &lt;p&gt;And, yes, people are spending more time on “media &amp;amp; computing”, some mixture of watching TV, playing videogames, etc. Housework, at least for women, takes less time, presumably mostly as a result of automation (appliances, etc.). (“Leisure” is basically “hanging out” as well as hobbies and social, cultural, sporting events, etc.; “Civic” includes volunteer, religious, etc. activities.)&lt;/p&gt;
    &lt;p&gt;If one looks specifically at people who are doing paid work&lt;/p&gt;
    &lt;p&gt;one notices several things. First, the average number of hours worked hasn’t changed much in half a century, though the distribution has broadened somewhat. For people doing paid work, media &amp;amp; computing hasn’t increased significantly, at least since the 1980s. One category in which there is systematic increase (though the total time still isn’t very large) is exercise.&lt;/p&gt;
    &lt;p&gt;What about people who—for one reason or another—aren’t doing paid work? Here are corresponding results in this case:&lt;/p&gt;
    &lt;p&gt;Not so much increase in exercise (though the total times are larger to begin with), but now a significant increase in media &amp;amp; computing, with the average recently reaching nearly 6 hours per day for men—perhaps as a reflection of “more of life going online”.&lt;/p&gt;
    &lt;p&gt;But looking at all these results on time use, I think the main conclusion that over the past half century, the ways people (at least in the US) spend their time have remained rather stable—even as we’ve gone from a world with almost no computers to a world in which there are more computers than people.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46797865</guid><pubDate>Wed, 28 Jan 2026 16:48:48 +0000</pubDate></item><item><title>Show HN: I Built a Sandbox for Agents</title><link>https://github.com/vrn21/bouvet.com</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46797895</guid><pubDate>Wed, 28 Jan 2026 16:50:43 +0000</pubDate></item></channel></rss>