<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 25 Jan 2026 09:12:02 +0000</lastBuildDate><item><title>Memory layout in Zig with formulas</title><link>https://raymondtana.github.io/math/programming/2026/01/23/zig-alignment-and-sizing.html</link><description>&lt;doc fingerprint="eaf8dac1609182cb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Memory Layout in Zig with Formulas&lt;/head&gt;&lt;p&gt;I was recently encouraged to watch A Practical Guide to Applying Data Oriented Design (DoD) by Andrew Kelley, the creator of Zig1. Just 10 minutes into the talk, I was confronted with a skill I had never formally learned… the arithmetic behind memory layout of types.&lt;/p&gt;&lt;p&gt;Throughout the talk, Andrew tested the audience’s ability to compute the alignment and sizes of various types, starting with primitives like &lt;code&gt;u32&lt;/code&gt; and &lt;code&gt;bool&lt;/code&gt;, and ending with some more complex structures involving enums, unions, and more. As far as I can tell, the exact rules for computing the alignment and size of a type in Zig are not made explicit in any documentation, but are understood by those in-the-know.&lt;/p&gt;&lt;p&gt;As a late-comer to low-level programming myself, I thought I’d collect here some formulas &amp;amp; explanations I landed on while wrestling with alignment and sizing in Zig.&lt;/p&gt;&lt;head rend="h2"&gt;Memory Layout Principles&lt;/head&gt;&lt;p&gt;For any piece of data stored in memory on a computer, the data must have some natural alignment and size dimensions according to its type. Intuitively, its size captures how many bytes it would take to specify the information that should be contained in any instance of that type. Whereas, its alignment captures the spacing the compiler must obey when choosing valid addresses at which to place data of this type.&lt;/p&gt;&lt;p&gt;I imagine just about any computer science major would have learned the rules of memory layout according to some kind of C-like compiler. I guess the motivation would go something like: “CPUs fetch data from memory in fixed-size blocks of so-many bytes, and performance degrades when data is misaligned. So, compilers automatically pad and align data types.”&lt;/p&gt;&lt;p&gt;In particular, types that don’t “fill up” all of the space in memory allocated to them will be padded with extra bits/bytes to make up for the difference.&lt;/p&gt;&lt;p&gt;Andrew’s whole message in his DoD talk centered on designing your data types to take up as little space in memory as possible, which includes reducing the size, alignment, and padding required to represent the same information.&lt;/p&gt;&lt;p&gt;Probably, the formulas I propose below apply to similar languages beyond Zig and my machine’s (Apple) ABI, but I make no claims.&lt;/p&gt;&lt;p&gt;The Zig language exposes two builtin functions relevant to our discussion:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;@alignOf(comptime T: type): #bytes required for aligning this type in memory (valid addresses for this type will be multiples of this value);&lt;/item&gt;&lt;item&gt;@sizeOf(comptime T: type): #bytes for storing the type in memory (including padding).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I’ll use the following function &lt;code&gt;memory_printout&lt;/code&gt; to report these values for any type:&lt;/p&gt;&lt;code&gt;const std = @import("std");

fn memory_printout(T: type) void {
    std.debug.print("@alignOf( {s} ): {d}\t", .{ @typeName(T), @alignOf(T) });
    std.debug.print("@sizeOf( {s} ): {d}\n", .{ @typeName(T), @sizeOf(T) });
}
&lt;/code&gt;&lt;head rend="h2"&gt;Memory Layout Formulas&lt;/head&gt;&lt;p&gt;To start, one helpful invariant is offered in Zig’s documentation. For any type &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;&lt;head rend="h3"&gt;Primitives&lt;/head&gt;&lt;p&gt;Already, the primitive data types will teach us a good bit about memory layout. Try guessing the results of the following Zig code before looking at the answer:&lt;/p&gt;&lt;code&gt;pub fn main() void {
    const types = [_]type{ bool, c_char, u8, *u8, u16, u17, i32, f64, usize };
    inline for (types) |T| memory_printout(T);
}
&lt;/code&gt;&lt;head&gt;The Output:&lt;/head&gt;&lt;code&gt;@alignOf( bool ): 1       @sizeOf( bool ): 1
@alignOf( c_char ): 1     @sizeOf( c_char ): 1
@alignOf( u8 ): 1         @sizeOf( u8 ): 1
@alignOf( *u8 ): 8        @sizeOf( *u8 ): 8
@alignOf( u16 ): 2        @sizeOf( u16 ): 2
@alignOf( u17 ): 4        @sizeOf( u17 ): 4
@alignOf( i32 ): 4        @sizeOf( i32 ): 4
@alignOf( f64 ): 8        @sizeOf( f64 ): 8
@alignOf( usize ): 8      @sizeOf( usize ): 8
&lt;/code&gt;&lt;p&gt;This suggests the following formula for primitive data types:&lt;/p&gt;\[\texttt{@sizeOf(primitive)} = \texttt{@alignOf(primitive)}.\]&lt;p&gt;Most of these make sense. A &lt;code&gt;c_char&lt;/code&gt; truly requires 8 bits, or 1 byte to specify.&lt;/p&gt;&lt;p&gt;Whereas, a &lt;code&gt;bool&lt;/code&gt; comprises a single bit (information-theoretically). But, alignment and size are measured in whole bytes, so we should round up to the nearest byte (and pad with 7 bits to fill up that byte).&lt;/p&gt;&lt;p&gt;Similarly, any unsigned integer &lt;code&gt;u{b}&lt;/code&gt;, signed integer &lt;code&gt;i{b}&lt;/code&gt;, or floating-point number &lt;code&gt;f{b}&lt;/code&gt; contains $b$ bits of information. So, counting in bytes, we will have to round $b / 8$ up, somehow. But, look at &lt;code&gt;u17&lt;/code&gt;: despite $2 &amp;lt; 17 / 8 \leq 3$, the size of &lt;code&gt;u17&lt;/code&gt; is not 3 bytes. Instead, it’s 4 bytes. In general, alignment and size must be powers-of-2 bytes. This is another desirable property half-dictated by architecture and half-related to the convenience of powers of two. So, we actually always round up to the nearest power-of-2 bytes when converting from bits.&lt;/p&gt;&lt;p&gt;Let’s formalize this conversion from bits to bytes for good:&lt;/p&gt;\[\begin{aligned} \texttt{bytes}(\texttt{bits}) &amp;amp;:= \max\left\{1, 2^{\left\lceil\log_2(\frac{\texttt{bits}}{8}) \right\rceil}\right\}. \end{aligned}\]&lt;p&gt;Another consequence of alignment and size being powers of two is the following, stronger invariant. For any type &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;That is, the size of the type is always a multiple of its alignment.&lt;/p&gt;&lt;p&gt;Next up, depending on your architecture, &lt;code&gt;usize&lt;/code&gt; will either match &lt;code&gt;u32&lt;/code&gt; or &lt;code&gt;u64&lt;/code&gt;. I’m working on a 64-bit machine, so that’s why we see its size and alignment as 8 bytes. Moreover, any pointer (such as &lt;code&gt;*u8&lt;/code&gt;) represents an address, which is guaranteed by Zig to match &lt;code&gt;usize&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;For primitive data types, remember: their size and align values agree and equal the smallest power-of-2 many bytes required to represent that type in memory.&lt;/p&gt;&lt;head rend="h3"&gt;Structs&lt;/head&gt;&lt;p&gt;In Zig, a &lt;code&gt;struct&lt;/code&gt; combines many fields into a single data type. How does memory layout work when many fields are combined together?&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Note: Zig automatically minimizes the memory footprint of a struct by possibly shuffling around its fields. To force the Zig compiler to respect the order of the fields as we’ve defined them, we may use the&lt;/p&gt;&lt;code&gt;extern&lt;/code&gt;keyword as shown below. Really, this forces the compiler to obey C ABI compatibility.&lt;/quote&gt;&lt;p&gt;First, rest assured that structs add no overhead. That is, if &lt;code&gt;T&lt;/code&gt; is a type, and we define:&lt;/p&gt;&lt;code&gt;const struct_T = struct {
  field: T,
};
&lt;/code&gt;&lt;p&gt;then &lt;code&gt;@alignOf(struct_T) == @alignOf(T)&lt;/code&gt; and &lt;code&gt;@sizeOf(struct_T) == @sizeOf(T)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Now, when a struct envelops two fields, such as&lt;/p&gt;&lt;code&gt;const pair = extern struct {
  a: u8,
  b: u32,
};
&lt;/code&gt;&lt;p&gt;we should consider the meanings of align and size again:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The alignment of the struct &lt;code&gt;pair&lt;/code&gt;should be such that any offset by this alignment value does not break the alignment of its constituent fields&lt;code&gt;a&lt;/code&gt;and&lt;code&gt;b&lt;/code&gt;.&lt;list rend="ul"&gt;&lt;item&gt;Here, field &lt;code&gt;b&lt;/code&gt;has a stricter alignment of 4 bytes, whereas&lt;code&gt;a&lt;/code&gt;permits offsets of 1 byte. So,&lt;code&gt;pair&lt;/code&gt;better also only permit alignments of 4 bytes.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Here, field &lt;/item&gt;&lt;item&gt;The size of the struct &lt;code&gt;pair&lt;/code&gt;is dictated by its alignment. Its memory will necessarily take up a number of bytes which is a multiple of its alignment. To figure out exactly how many, we iterate through the fields in order, trying our best to greedily pack those fields while still respecting their own alignments.&lt;list rend="ul"&gt;&lt;item&gt;Here, the size of &lt;code&gt;a&lt;/code&gt;is just one byte, so it fits into a single memory chunk of size 4 bytes (the alignment of&lt;code&gt;pair&lt;/code&gt;is 4 bytes), with three bytes to spare. Now, we consider the next field:&lt;code&gt;b&lt;/code&gt;. As its alignment is 4 bytes, we can only write&lt;code&gt;b&lt;/code&gt;at an address which is a multiple of its own alignment. The soonest we can accomplish this is by padding three bytes after&lt;code&gt;a&lt;/code&gt;and writing&lt;code&gt;b&lt;/code&gt;at the fourth byte. This already places&lt;code&gt;b&lt;/code&gt;into another 4-byte memory chunk, in which it fits entirely as its size is 4 bytes. So, the total size of&lt;code&gt;pair&lt;/code&gt;is 8 bytes.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Here, the size of &lt;/item&gt;&lt;/list&gt;&lt;p&gt;Now, what do you expect the output of the following Zig code to be?&lt;/p&gt;&lt;code&gt;pub fn main() void {
    memory_printout(ABAB);
    memory_printout(ABBA);
}

const ABAB = extern struct {
    a1: u8,
    b1: u16,
    a2: u8,
    b2: u16,
};

const ABBA = extern struct {
    a1: u8,
    a2: u8,
    b1: u16,
    b2: u16,
};
&lt;/code&gt;&lt;head&gt;The Answer:&lt;/head&gt;&lt;code&gt;@alignOf( ABAB ): 2      @sizeOf( ABAB ): 8
@alignOf( ABBA ): 2      @sizeOf( ABBA ): 6
&lt;/code&gt;&lt;p&gt;In general, deciding the placement of a struct field follows this rule:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Rule: Each field is placed after the previous field at the next smallest multiple of its own alignment.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In general, the formula for rounding a number $N$ up to the nearest multiple of some other number $m$ looks like:&lt;/p&gt;\[\texttt{next_mult}(N, m) := \left\lceil \frac{N}{m} \right\rceil \cdot m.\\\]&lt;p&gt;Given this, try to follow the next example. We make use of another Zig builtin &lt;code&gt;@offsetOf(&amp;lt;type&amp;gt;, &amp;lt;field_name&amp;gt;)&lt;/code&gt; to display exactly where each field is placed in memory relative to the beginning address of &lt;code&gt;S&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;fn printOffset(T: type, comptime f: [:0]const u8) void {
    std.debug.print("@offsetOf( {s} ) = {d}\n", .{ f, @offsetOf(T, f) });
}

pub fn main() void {
  memory_printout(S);
  printOffset(S, "a");
  printOffset(S, "b");
  printOffset(S, "c");
}

const S = extern struct {
    // align = size = 2
    a: u16,
    // align = 2, size = 6
    b: extern struct { b1: u16, b2: u16, b3: u16 },
    // align = size = 4
    c: u32,
};
&lt;/code&gt;&lt;head&gt;Output:&lt;/head&gt;&lt;code&gt;@alignOf( S ): 4        @sizeOf( S ): 12
@offsetOf( a ) = 0
@offsetOf( b ) = 2
@offsetOf( c ) = 8
&lt;/code&gt;&lt;p&gt;In general, alignment for structs is quite simple to formulate:&lt;/p&gt;\[\texttt{@alignOf(struct)} := \max_{\texttt{fields}} \texttt{@alignOf(field)}.\]&lt;p&gt;In contrast, the size of the (externed) struct is more complicated: $\texttt{@sizeOf(struct)}$ is the smallest multiple of $\texttt{@alignOf(struct)}$ many bytes required to fit the fields of a struct (in order) such that:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;No two fields overlap in memory, and&lt;/item&gt;&lt;item&gt;Each field lies at an address which is a multiple of its own alignment: $\texttt{@alignOf(field)}$.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Without the &lt;code&gt;extern&lt;/code&gt; keyword, Zig minimizes the size of the struct under all permutations of its fields. Zig also offers &lt;code&gt;packed struct&lt;/code&gt;, which eliminates padding entirely and lays out fields in strict bit-order. While this can reduce memory usage further, it comes with performance trade-offs and restrictions on field access.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Bonus question: Try to explain why the choice to make alignments and sizes be powers of two is vital for these rules to be well-defined for assessing the alignment and size of structs.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;Enums&lt;/head&gt;&lt;p&gt;Under the hood, an enum works on indices, not labels. So, the alignment and size of an enum will equal the minimal number of bytes to express the type of its indices. Suppose &lt;code&gt;T = enum (u{b}) { ... }&lt;/code&gt; is an arbitrary enum indexed by unsigned integers of type &lt;code&gt;u{b}&lt;/code&gt;, where &lt;code&gt;b&lt;/code&gt; is measured in bits.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Note: we may use the Zig standard library to access the number of options in the enum as follows:&lt;/p&gt;&lt;code&gt;std.meta.fields(T).len&lt;/code&gt;. I’ll call this $\texttt{T.len}$, below.&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;Note: By default, Zig’s compiler assigns indices starting at zero and counting up by one. So, by default, Zig sets&lt;/p&gt;&lt;code&gt;b&lt;/code&gt;to $\lceil \log_2 (\texttt{T.len}) \rceil$.&lt;/quote&gt;&lt;p&gt;Then,&lt;/p&gt;\[\begin{aligned} \texttt{@alignOf(enum(u{b}))} &amp;amp;= \texttt{@alignOf(u{b})} = \texttt{bytes}(\texttt{b}),\\ \texttt{@sizeOf(enum(u{b}))} &amp;amp;= \texttt{@sizeOf(u{b})} = \texttt{bytes}(\texttt{b}). \end{aligned}\]&lt;p&gt;We’ve made use of the $\texttt{bytes}$ function, again2.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;const T_default = enum { a, b, c, d, e };
const T_long = enum(u64) { a, b, c, d, e };

memory_printout(T_default);
memory_printout(T_long);
&lt;/code&gt;&lt;head&gt;Output:&lt;/head&gt;&lt;code&gt;@alignOf( T_default ): 1    @sizeOf( T_default ): 1
@alignOf( T_long ): 8       @sizeOf( T_long ): 8
&lt;/code&gt;&lt;head rend="h3"&gt;Arrays and Slices&lt;/head&gt;&lt;p&gt;For an array in Zig, its alignment inherits from that of its elements, and its size is just length$\times$size of the type:&lt;/p&gt;\[\begin{aligned} \texttt{@alignOf([N]T)} &amp;amp;= \texttt{@alignOf(T)},\\ \texttt{@sizeOf([N]T)} &amp;amp;= \texttt{N} \cdot \texttt{@sizeOf(T)}. \end{aligned}\]&lt;p&gt;In contrast, a slice in Zig is just a special case of a struct which contains one pointer (&lt;code&gt;usize&lt;/code&gt;) and a length (&lt;code&gt;usize&lt;/code&gt;). So,&lt;/p&gt;&lt;p&gt;Example (using another builtin &lt;code&gt;@TypeOf&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;pub fn main() void {
    memory_printout(@TypeOf(digits_array));
    memory_printout(@TypeOf(digits_slice));
}

const digits_array = [10]u8{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 };
const digits_slice: []const u8 = digits_array[0..]; 
&lt;/code&gt;&lt;head&gt;Output:&lt;/head&gt;&lt;code&gt;@alignOf( [10]u8 ): 1         @sizeOf( [10]u8 ): 10
@alignOf( []const u8 ): 8     @sizeOf( []const u8 ): 16
&lt;/code&gt;&lt;head rend="h3"&gt;(Untagged) Unions&lt;/head&gt;&lt;p&gt;An untagged, bare union in Zig (accomplished with the &lt;code&gt;extern&lt;/code&gt; keyword) essentially acts as a switch statement over a number of mutually-exclusive options of various types without the ability to report which option is active. To express a bare union type in memory, we just need enough memory to store the largest option(s).&lt;/p&gt;&lt;p&gt;However, untagged unions in Zig without the &lt;code&gt;extern&lt;/code&gt; keyword seem to require extra memory, essentially one more alignment’s worth. So, untagged unions should satisfy:&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;pub fn main() void {
    memory_printout(U_bare);
    memory_printout(U);
}

const U_bare = extern union {
    a: i64,
    b: extern struct { c: i64, d: i64, e: i64 },
};

const U = union {
    a: i64,
    b: struct { c: i64, d: i64, e: i64 },
};
&lt;/code&gt;&lt;head&gt;Output:&lt;/head&gt;&lt;code&gt;@alignOf( U_bare ): 8   @sizeOf( U_bare ): 24
@alignOf( U ): 8        @sizeOf( U ): 32
&lt;/code&gt;&lt;head rend="h3"&gt;Tagged Unions&lt;/head&gt;&lt;p&gt;Tagged unions are unions which use an additional enum to detect which field is active in the union. Alignment for a tagged union must additionally consider the alignment of the tag too, while the size must treat the tag and fields together.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Note: there are no “bare” tagged unions. So, the&lt;/p&gt;&lt;code&gt;extern&lt;/code&gt;keyword doesn’t work on tagged unions.&lt;/quote&gt;&lt;p&gt;Suppose &lt;code&gt;U(E)&lt;/code&gt; is of type &lt;code&gt;union(enum)&lt;/code&gt;. Then, we can compute the alignment and size of this type depending on those of its union and enum components:&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;pub fn main() void {
    memory_printout(UE);
    memory_printout(UF);
    memory_printout(UG);
}

const E = enum { a, b };
const F = enum(u64) { a, b };
const G = enum(u128) { a, b };

const UE = union(E) {
    a: i64, b: struct { c: i64, d: i64 }
};
const UF = union(F) {
    a: i64, b: struct { c: i64, d: i64 }
};
const UG = union(G) {
    a: i64, b: struct { c: i64, d: i64 }
};
&lt;/code&gt;&lt;head&gt;Output:&lt;/head&gt;&lt;code&gt;@alignOf( UE ): 8       @sizeOf( UE ): 24
@alignOf( UF ): 8       @sizeOf( UF ): 24
@alignOf( UG ): 16      @sizeOf( UG ): 32
&lt;/code&gt;&lt;head rend="h3"&gt;ArrayLists and MultiArrayLists&lt;/head&gt;&lt;p&gt;An &lt;code&gt;ArrayList(T)&lt;/code&gt; in Zig is the standard library’s dynamic array implementation. This is comparable to the notions of a “vector” in C++ or a “list” in Python, otherwise understood as the Array of Structs (AoS) memory layout. Elements of type &lt;code&gt;T&lt;/code&gt; are stored contiguously in memory each with their full size and padding.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;ArrayList(T)&lt;/code&gt; type itself is a struct containing:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;items&lt;/code&gt;: a slice&lt;code&gt;[]T&lt;/code&gt;, and&lt;/item&gt;&lt;item&gt;&lt;code&gt;capacity&lt;/code&gt;: a&lt;code&gt;usize&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;So, the &lt;code&gt;ArrayList(T)&lt;/code&gt; type has a fixed memory footprint:&lt;/p&gt;&lt;p&gt;However, the data that the ArrayList manages lives on the heap. When iterating over an ArrayList, you traverse memory in strides of $\texttt{@sizeOf(T)}$ bytes. The memory consumed by the backing buffer is simply:&lt;/p&gt;\[\begin{aligned} \texttt{buffer_alignment} &amp;amp;= \texttt{@alignOf(T)},\\ \texttt{buffer_size} &amp;amp;= \texttt{buffer.capacity} \cdot \texttt{@sizeOf(T)}. \end{aligned}\]&lt;p&gt;In contrast, a &lt;code&gt;MultiArrayList(T)&lt;/code&gt; in Zig follows the Struct of Arrays (SoA) memory layout. Instead of storing complete &lt;code&gt;T&lt;/code&gt; instances contiguously, it stores each field of &lt;code&gt;T&lt;/code&gt; in its own separate, tightly-packed array.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;MultiArrayList(T)&lt;/code&gt; type itself is a struct containing:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;ptr&lt;/code&gt;: a single pointer to the backing buffer,&lt;/item&gt;&lt;item&gt;&lt;code&gt;len&lt;/code&gt;: a&lt;code&gt;usize&lt;/code&gt;,&lt;/item&gt;&lt;item&gt;&lt;code&gt;capacity&lt;/code&gt;: a&lt;code&gt;usize&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;So:&lt;/p&gt;\[\begin{aligned} \texttt{@alignOf(MultiArrayList(T))} &amp;amp;= \texttt{@alignOf(usize)} = 8 \text{ bytes (on 64-bit)},\\ \texttt{@sizeOf(MultiArrayList(T))} &amp;amp;= 3 \cdot \texttt{@sizeOf(usize)} = 24 \text{ bytes (on 64-bit)}. \end{aligned}\]&lt;p&gt;The backing buffer stores all field arrays contiguously. Padding between fields is not needed; instead, each field array is packed according to the field’s alignment. Summing over the fields of $T$, the total buffer size would thus be:&lt;/p&gt;\[\texttt{buffer_size} = \texttt{buffer.capacity} \cdot \sum_{\texttt{fields}} \texttt{@sizeOf}(\texttt{field}).\]&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const ArrayList = std.ArrayList;
const MultiArrayList = std.MultiArrayList;

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    const allocator = gpa.allocator();

    const T = struct { a: u8, b: u16 };

    var list: ArrayList(T) = .{};
    defer list.deinit(allocator);

    var multiList: MultiArrayList(T) = .{};
    defer multiList.deinit(allocator);

    for (0..10_000) |_| {
        try list.append(allocator, .{ .a = 0, .b = 1 });
        try multiList.append(allocator, .{ .a = 0, .b = 1 });
    }

    memory_printout(T);
    memory_printout(ArrayList(T));
    memory_printout(MultiArrayList(T));
    std.debug.print("list capacity: {d}\n", .{list.capacity});
    std.debug.print("list buffer size: {d}\n", .{list.capacity * @sizeOf(T)});
    std.debug.print("multiList capacity: {d}\n", .{multiList.capacity});
    std.debug.print("multiList buffer size: {d}\n", .{multiList.capacity * (@sizeOf(@FieldType(T, "a")) + @sizeOf(@FieldType(T, "b")))});
}
&lt;/code&gt;&lt;head&gt;Output:&lt;/head&gt;&lt;code&gt;@alignOf( T ): 2                  @sizeOf( T ): 4
@alignOf( ArrayList(T) ): 8       @sizeOf( ArrayList(T) ): 24
@alignOf( MultiArrayList(T) ): 8  @sizeOf( MultiArrayList(T) ): 24
list capacity: 12854
list buffer size: 51416
multiList capacity: 11150
multiList buffer size: 33450
&lt;/code&gt;&lt;head rend="h2"&gt;Battle Testing&lt;/head&gt;&lt;p&gt;Let’s test our formulas against the video that inspired this post: Andrew Kelley’s talk on DoD.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Note: In order to get these examples to actually compile and execute in Zig (0.16.0), I had to throw in some allocators and extra helper methods.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h3"&gt;ArrayList of Monsters (19:05)&lt;/head&gt;&lt;code&gt;const Monster = struct {
  anim: *Animation, 
  kind: Kind,

  const Kind = enum { snake, bat, wolf, dingo, human };
};

var monsters: ArrayList(Monster) = .{};
var i: usize = 0;
while (i &amp;lt; 10_000) : (i += 1) {
  try monsters.append(.{
    .anim = getAnimation(),
    .kind = rng.enumValue(Monster.Kind),
  });
}
&lt;/code&gt;&lt;head&gt;Actual Memory Use:&lt;/head&gt;&lt;code&gt;Monster size: 16 bytes 
ArrayList(Monster) size: 24 bytes
monsters size: 160_000 bytes        // Total memory size of 10_000 monsters
&lt;/code&gt;&lt;p&gt;Do our formulas agree?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;code&gt;Monster&lt;/code&gt;Struct:&lt;list rend="ul"&gt;&lt;item&gt;The fields of &lt;code&gt;Monster&lt;/code&gt;satisfy $\texttt{@alignOf(anim)} = 8$ and $\texttt{@alignOf(kind)} = 1$. So, we expect the alignment of the struct to be $\texttt{@alignOf(Monster)} = 8$.&lt;/item&gt;&lt;item&gt;No matter the ordering, it takes two memory chunks of size 8 bytes to fit these fields (since $\texttt{@sizeOf(anim)} = 8$ and $\texttt{@sizeOf(kind)} = 1$). So, we expect $\texttt{@sizeOf(Monster)} = 16$, exactly as we observed.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The fields of &lt;/item&gt;&lt;item&gt;ArrayList(Monster) Type: as a type, it takes a bit of overhead to specify &lt;code&gt;ArrayList(Monster)&lt;/code&gt;, since an ArrayList is really a slice&lt;code&gt;[]Monster&lt;/code&gt;plus a capacity (&lt;code&gt;usize&lt;/code&gt;). On my 64-bit machine, that adds up to 24 bytes of memory.&lt;/item&gt;&lt;item&gt;&lt;code&gt;monsters&lt;/code&gt;ArrayList: remember, ArrayLists act like “arrays of structs”.&lt;list rend="ul"&gt;&lt;item&gt;The natural size and alignment of the &lt;code&gt;Monster&lt;/code&gt;struct dictate the layout of the&lt;code&gt;monsters&lt;/code&gt;ArrayList.&lt;/item&gt;&lt;item&gt;The size of an ArrayList in memory should equal its length times the size of each &lt;code&gt;Monster&lt;/code&gt;element type. So, we expect&lt;code&gt;monsters&lt;/code&gt;to take up $10,000 \times \texttt{@sizeOf(Monster)} = 160,000$ bytes, as observed.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The natural size and alignment of the &lt;/item&gt;&lt;/list&gt;&lt;p&gt;Footnotes&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Zig is a modern, C-like programming language which offers a safer, more memory-explicit experience for systems programming, without sacrificing low-level control or C interoperability. Notably, Zig makes it straightforward to manage memory allocation by treating allocators as first-class values rather than hidden globals. Instead of relying on an implicit runtime or a process-wide allocator, you pass explicit allocator objects into the code that needs them. This makes ownership and lifetimes much clearer, encourages you to design APIs around who is responsible for allocating and freeing memory, and makes it easy to swap in custom allocation strategies (e.g., arenas, scratch, tracking, etc.). ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Recall: $\displaystyle \texttt{bytes}(\texttt{bits}) := \max(1, 2^{\lceil\log_2(\frac{\texttt{bits}}{8})\rceil})$ whenever $\texttt{bits} &amp;gt; 0$. Otherwise, $\texttt{bytes}(0) = 0$. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46744647</guid><pubDate>Sat, 24 Jan 2026 15:57:45 +0000</pubDate></item><item><title>Ask HN: Gmail spam filtering suddenly marking everything as spam?</title><link>https://news.ycombinator.com/item?id=46744807</link><description>&lt;doc fingerprint="7ff280131b22bafd"&gt;
  &lt;main&gt;
    &lt;p&gt;Almost all transactional emails are being marked as suspicious even when their SPF/DKIM records are fine and they’ve been whitelisted before. Did Google break something in gmail/spam filtering?&lt;/p&gt;
    &lt;p&gt;Briefly, this morning, I had the opposite effect happen to my Gmail inbox in which things that would normally land in the social and updates folders ended up in my primary folder. I don't know which I'd be more freaked out by: a broken Gmail spam filter or 18 inches of snow.&lt;/p&gt;
    &lt;p&gt;Yes, me as well. Thought I had mistakenly changed something but I hadn't. Have also noticed that ad blockers stopped working last week; now as well as the wrong routing in Gmail, that casting to Chromecast from Chrome stopped working today.&lt;/p&gt;
    &lt;p&gt;That. Why in 2062 are so many people still reduced to merely describing what happens, and have no power to fix it themselves? These big corporations can roll out anything and we’ll all roll over and take it. Neuralink 7.0 completely changed everyone’s personality and people bitched for a while but ultimately no one can do anything about it. They promised to bring back choice of features but it’s been a decade and so far just more enshittification&lt;/p&gt;
    &lt;p&gt;It's a great reminder of how good this feature is that we take for granted. I think this outage has actually improved my appreciation for Gmail (a service I normally only complain about).&lt;/p&gt;
    &lt;p&gt;Seriously. I didn't even realize this was a wide issue, but I couldn't find a school enrolment email I was looking for this morning, and found it in the spam folder. The fact that I basically never have to do this is actually amazing.&lt;/p&gt;
    &lt;p&gt;I wonder about difference in experience that different people have with gmail’s spam filter. In my case, the majority of emails that go to my gmail spam folder are legitimate. I don’t actually receive much spam, a single-digit number of emails per month (in the past 30 days, 2 emails), so any time I see anything in my spam folder I have to check so that I can rescue the email if legitimate.&lt;/p&gt;
    &lt;p&gt;This is my experience also. Closely guarded email, haven't received _any_ spam to it to date, but a large volume of false positives. This, among other reasons, actually led to my setting up my own email server again. Gmail is a great product if you don't know what you're doing or have avaliable to you. It's like a McDonalds burger. Not inpressive, not good, bot bad either, and certainly won't offend anyone while being accessible --- but calling it good is a bit out of touch with what good looks like.&lt;/p&gt;
    &lt;p&gt;I kinda expect there are a lot of false positives that people just never notice because they've got also thousands of unread (non-spam) emails in their inbox and never check their spam to see if there's anything legitimate there.&lt;/p&gt;
    &lt;p&gt;I have run my own mail server for years and I rarely see spam. I'm running a classic Bayesian filter as outlined in the legendary PG post "A Plan For Spam" and it works very well. I don't really get all the fuss about this issue. When I do see a piece of unclassified spam I simply classify it and continue. For me this is a far better tradeoff than having all my most private mail on some bigcorp server where any nerd can rifle through it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; For me this is a far better tradeoff than having all my most private mail on some bigcorp server where any nerd can rifle through it.&lt;/p&gt;
    &lt;p&gt;You've functionally given yourself very little extra privacy because the vast majority of emails you send or receive will still cross through BigCorp servers (whether Google, Microsoft, Intuit, or other).&lt;/p&gt;
    &lt;p&gt;You can do the work to run your own mail server, but so few other people do that one end of the conversation is still almost always feeding a corporation's data lake.&lt;/p&gt;
    &lt;p&gt;I agree with you but I still run my own mail server. If people like me stopped doing that, we would cede the entire email landscape to BigCorp. A sad fate to happen to one of the true decentralized protocols. It's like if we all just went back to AOL&lt;/p&gt;
    &lt;p&gt;And yet, if you're communicating with someone else who does the same (or uses a niche hosted provider), that entire conversation is outside their "data lake".&lt;/p&gt;
    &lt;p&gt;As someone who's run my own email for 25 years or so (I'm really getting old...) my biggest problem is not that I receive spam (spamassassin mostly takes care of it) but that my sent emails get marked as spam by big email providers. Yahoo is the worst offender and seems to do so at some base despite my best efforts (spf, dkim, arc, and jumping through their registration hoops)&lt;/p&gt;
    &lt;p&gt;I'm running my own mail server for longer than I'd like to admit, but not for my critical/key email addresses. Looking at the spam filtering I get in Gmail and knowing my endless fights with spamassassin and DSBLs I know I could never achieve that.&lt;/p&gt;
    &lt;p&gt;The only upside of having an actual mail server is the ability to say "this is incorrect, no one ever tried to send an email to this address/from this IP" or custom 55x messages.&lt;/p&gt;
    &lt;p&gt;I disabled google's spam filter years ago for this sort of issue. The final straw was that it flagged a response from my boss, to an email I had sent earlier that day, resulting in a missed meeting. Installed thunderbird to access the company mail, never looked back. Private gmail is unused, only ever created it for android sign-up back in the day.&lt;/p&gt;
    &lt;p&gt;As with search, I don't get why people still use google.&lt;/p&gt;
    &lt;p&gt;Yes, my Gmail inbox is full of regular senders being flagged as "possibly unsafe" and I need to click a button "Looks Safe" to accept them. They are not being spamboxed, but they are definitely flagged. Even official communications from the USPS!&lt;/p&gt;
    &lt;p&gt;The reason given is that "Gmail hasn't scanned this message", so I suppose the scanners are unavailable/disabled for the time being.&lt;/p&gt;
    &lt;p&gt;They should also be tagged as "Important" but they are not. I believe this is a heuristic-based designation, and it has not been working too great lately. My most important mail is coming through as "unimportant".&lt;/p&gt;
    &lt;p&gt;They are not being marked as "Suspicious" but they are showing an infobox that explains they could not be scanned at all.&lt;/p&gt;
    &lt;p&gt;You could click "Seems Safe" on these messages, but they are not scanned by Google, and they are simply adding a disclaimer that they currently can't vouch for the safety of a message that they couldn't scan. It seems to me that this is a prudent and helpful course of action.&lt;/p&gt;
    &lt;p&gt;This has been “down” for me for a few months now, ever since Google tied this functionality to the same toggle that opts you in for using your email data for AI training. So now you can’t filter this stuff without also agreeing to a whole swath of unrelated and opt-ins.&lt;/p&gt;
    &lt;p&gt;Ive since gone on an unsubscribe campaign, and things seem bearable now.&lt;/p&gt;
    &lt;p&gt;It works with most real companies. If you signed up for it, you can generally unsubscribe from it. It’s easy to do by mistake, and some default to yes with no option during sign up.&lt;/p&gt;
    &lt;p&gt;I don’t care about whatever new shows Netflix has. Unsubscribe.&lt;/p&gt;
    &lt;p&gt;I don’t care about my DNS registrar having a sale. Unsubscribe.&lt;/p&gt;
    &lt;p&gt;Google postmaster notices when you hit the one-click unsubscribe button and severely punishes senders that continue to send. It's worth using as long as you understand that some senders will never allow you to sign up again.&lt;/p&gt;
    &lt;p&gt;Not just you, widespread reports on /r/gmail and Twitter since ~12 hours ago. Likely a bad model push on Google's end. Workaround: check spam folder for legit mail, mark as "not spam" + star important senders to retrain your filter faster. Usually resolves in 24-48h when they rollback.&lt;/p&gt;
    &lt;p&gt;Google's spam filter is having a moment. Even emails with perfect auth records are getting flagged - clearly a broken model deployment. Mark legitimate emails as "not spam" aggressively. They'll either rollback or your local filter will adapt. This happens every 6-12 months with Gmail.&lt;/p&gt;
    &lt;p&gt;Same here. Until recently I would get maybe 1-2 spams a month, and I just got 30 in the span of a few days.&lt;/p&gt;
    &lt;p&gt;They’re the very obvious, very obnoxious kind of spam, and Gmail still correctly sends them to the junk bin, so I wonder if they were shadowbanned before and Google simply decided to make the process more explicit (which I don’t hate on principle).&lt;/p&gt;
    &lt;p&gt;Either that or my address was scrapped from somewhere by a spam bot and the timing is coincidental.&lt;/p&gt;
    &lt;p&gt;Google just let through an email spoofed from my own domain (via a mailgun server). It was a phishing attack about the domain being shut down. The connection between the domain name and my personal email address have never been published. Either google or Squarespace leaked the info.&lt;/p&gt;
    &lt;p&gt;Thank goodness. My Gmail address is my first name so I typically get many hundreds of spam’s a day which are almost all caught. Dozens in my inbox today so I figured something was up. Glad it’s not that the spam pedlars have suddenly gotten clever.&lt;/p&gt;
    &lt;p&gt;I see nothing amiss on my oldest Gmail account. But then, I get probably &amp;lt;1 spam email a day on average, and even less legitimate mail, and even less that isn't an automatic notification of something or other that's already filtered and categorized by sender.&lt;/p&gt;
    &lt;p&gt;Today gmail labelled an email coming from google search console as potentially dangerous, however it was because it couldn't properly do spam filtering on the email.&lt;/p&gt;
    &lt;p&gt;I have been receiving a large number of spam emails in my "Important and Unread" areas which is anomalous. I was wondering exactly why and this helps. thanks!&lt;/p&gt;
    &lt;p&gt;Step zero. Never disclose your email address to anyone.&lt;/p&gt;
    &lt;p&gt;This is very easy and straightforward. I operate 6 Gmail accounts, and three are "alts" where I've basically never given the address out to anyone at all, and they receive zero spam, zero UCE, zero marketing emails.&lt;/p&gt;
    &lt;p&gt;Of course, on my "main" I've disclosed the address to many entities and I use it for sign-in and shipping and many things. And yes, I do receive spam and scam emails there, but wcyd?&lt;/p&gt;
    &lt;p&gt;I recently had a "role" Google account terminated because I was (paraphrasing) "violating Google policies" by having multiple accounts. I didn't know they were sticklers about that.&lt;/p&gt;
    &lt;p&gt;(I don't much care because the account was just used for interacting with somebody else's Google-hosted junk but, if I had been using it for something serious, I have probably been frustrated.)&lt;/p&gt;
    &lt;p&gt;There is no way, no possible way that Google prohibits the use of multiple accounts. They do not. They cannot. I just asked Gemini and I checked the actual TOS. It does not, in any way, prohibit these uses.&lt;/p&gt;
    &lt;p&gt;In fact, this is plainly evident by the way they give you tools to operate them in a systematic way. You can add multiple accounts to a single Android "user". You can add them to a single Google Chromebook account under one signed-in account. You can add multiple accounts separately to the same Chromebook.&lt;/p&gt;
    &lt;p&gt;You can add multiple accounts with the same names, the same birthdates, and the same Driver License. I've validated at least two YouTube channels by showing exactly the same ID.&lt;/p&gt;
    &lt;p&gt;Google did not terminate your account for the reason you state. You are not telling us all the background information.&lt;/p&gt;
    &lt;p&gt;Google may indeed terminate multiple accounts for the same person because of TOS violations. They will definitely link and associate your accounts, so making an "alt account" for misbehavior is not safe. If my "alt account" is compromised or violates TOS, then I can expect they will discipline all 6 equally, because they're all linked.&lt;/p&gt;
    &lt;p&gt;But operating multiple accounts is very explicitly supported by Google, and by Microsoft as well, I will say. I don't know about Apple. Facebook definitely prohibited this in the past, although you can maintain multiple "profiles" and "pages" that have unique settings and personalities.&lt;/p&gt;
    &lt;p&gt;Apparently my account "violated TOS" (though I don't see how). The other account was used to interact with a Google Workspace in 2016, and hasn't been used since. I don't particularly care to waste mental energy trying to figure out the methodology behind Google's decision here.&lt;/p&gt;
    &lt;p&gt;This happening seemed kinda sketchy to me (because I've heard of people having several Google accounts) but, like I said, I didn't really care too much.&lt;/p&gt;
    &lt;p&gt;Anyway, here's how it went down:&lt;/p&gt;
    &lt;p&gt;In 2016 I was working w/ a Customer who was using some Google product (I believe Workspace) and I have to have a Google account to interact w/ it. Because I didn't care for them to see my "personal" Google account I make this one-off account.&lt;/p&gt;
    &lt;p&gt;This account is a Google account w/o Gmail (i.e. the username is not "@gmail.com"). That may be a factor.&lt;/p&gt;
    &lt;p&gt;Over the years I'd receive notifications that Google was going to delete the account for inactivity. I'd logon again to keep it active.&lt;/p&gt;
    &lt;p&gt;On 2026-01-12 I got a notification that my old "role" Google account was going to be deleted for being inactive for two years. I decided I wanted to keep it so I attempted to logon. The password in my vault didn't work. I found that perplexing, so I did a "Forgot password" workflow. As part of that I was offered an SMS option. I used the telephone number I use for my main Google account. For sure they "know" I'm the same operator of both accounts.&lt;/p&gt;
    &lt;p&gt;I don't believe somebody guessed the password on this account and was using it because (a) I was notified it was inactive, and (b) the password was a random 16 character alphanumeric string used only for this account. Something was clearly sketchy about the password being "wrong", though.&lt;/p&gt;
    &lt;p&gt;I completed the "Forgot password" workflow on the "role" account and got access. I decided to enable TOTP and my "real" Gmail account as the recovery contact. Everything seemed fine.&lt;/p&gt;
    &lt;p&gt;On 2026-01-13 I received a message as-follows:&lt;/p&gt;
    &lt;p&gt;&amp;gt; From: Google &amp;lt;no-reply@accounts.google.com&amp;gt;&lt;/p&gt;
    &lt;p&gt;&amp;gt; To: MyUsername@NotGmail.example.com&lt;/p&gt;
    &lt;p&gt;&amp;gt; Subject: Your Google Account has been disabled&lt;/p&gt;
    &lt;p&gt;&amp;gt; It looks like this account was created or used with multiple other accounts to violate Google's policies. The account might have been created by a computer program or bot.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If you think your account was disabled by mistake, submit an appeal as soon as possible.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Disabled accounts are eventually deleted. You’ll need to submit an appeal soon to keep your emails, contacts, photos, and other data saved in your Google Account.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If you live in the European Union (EU) or are an EU citizen, there may be additional resolution options available to you.&lt;/p&gt;
    &lt;p&gt;I did a “reset” a few years ago where I moved to a fresh gmail address, forwarded my old one, and updated all my accounts to use Apple’s Hide My Email service, unique per sender.&lt;/p&gt;
    &lt;p&gt;After a few years of updating addresses that I’d missed whenever something showed up that was forwarded from my old gmail account, I shut down my old account.&lt;/p&gt;
    &lt;p&gt;No more spam, whenever I start receiving spam to a Hide My Email address, I deactivate it.&lt;/p&gt;
    &lt;p&gt;I feel like an easier solution to having six different email addresses is to use Gmail aliases - I've caught a few less-than-honest companies either selling my email address, or been breached without disclosing such, simply by using an alias along the lines of '+service_name'. If any alias starts to receive spam you can setup rules to automatically delete everything that comes in with that. You also get the added benefit of significantly easier and more accurate search.&lt;/p&gt;
    &lt;p&gt;I don't think y'all understand why I have separate Google accounts.&lt;/p&gt;
    &lt;p&gt;I use them for different purposes. They are "role accounts" for projects I am doing, such as geneaology and astronomy.&lt;/p&gt;
    &lt;p&gt;In order to use YouTube sanely, and store different stuff in Drive, I separate them into unique accounts. I use those accounts for specific things, and my YouTube subscriptions, playlists, etc. are tailored for each role, for example.&lt;/p&gt;
    &lt;p&gt;This is not about email at all. Obviously, I can access all those email accounts through the one app on my smartphone or the one PWA on my Chromebook. They are easily manageable but separate.&lt;/p&gt;
    &lt;p&gt;I also run 3 Outlook/Microsoft accounts, and for the same reason. (One of them is my academic account from community college, and the other two are personal.)&lt;/p&gt;
    &lt;p&gt;I don't need to give out email addresses for the "role accounts" except where I "Sign In With Google" to various services. So I don't really send/receive email from them at all, except where I'm sharing links or documents with myself (the best way to do this cross-account is still by using email, oftentimes.)&lt;/p&gt;
    &lt;p&gt;Well, spam is no big deal, and any scam that comes via email should not affect anyone who is educated and prepared for them.&lt;/p&gt;
    &lt;p&gt;Of course, with a well-known email address, you could run a higher risk of credential stuffing, and an account takeover by someone who hijacks your email account, and then pivots from there to taking other accounts.&lt;/p&gt;
    &lt;p&gt;But this seems to be a risk we all take: email addresses are meant to be shared, to be public, and to be well-known to anyone to correspond with us.&lt;/p&gt;
    &lt;p&gt;I will say that disclosing my email address to certain parties has had noticeable effects. For example, I used "MYADDRESS+Echovita@gmail.com" once, and only once. My godfather had passed away, and I ordered some flowers for his funeral. And I put that order through with that email address.&lt;/p&gt;
    &lt;p&gt;Well, Echovita themselves had a data breach shortly afterwards, and I was inundated with scam emails. Just all sorts of attackers and they were basically all using the same M.O. But they were readily identifiable because I had used that "+Echovita" to identify it uniquely. And they really haven't stopped coming in. It's been 5 years since that breach.&lt;/p&gt;
    &lt;p&gt;So yes, especially with untrusted parties, it may help to tag your email address. I don't worry about receiving spam anywhere. But like I said, since I've never ever disclosed the addresses of 2-3 of my "alt accounts" they simply never receive any mail at all, spam or no spam.&lt;/p&gt;
    &lt;p&gt;I did a wildcard acceptance for years, but it doesn’t scale as well something like Apple’s Hide My Email (or other comparable service) - with a catchall you have to then start keeping a blacklist of bad emails, and I started getting spam to generic addresses like info@customdomain.com or admin@customdomain.com - with @icloud.com addresses you can just delete an address and forget about it once it’s burned.&lt;/p&gt;
    &lt;p&gt;I have an absurd and overwrought system involving Gmail, and client-side rspamd and SpamSieve on my Mac. Gmail is (was?) overly aggressive flagging things as spam, so I have the client-side Bayesian filter check Gmail’s spam folder and rescue good email, so long as rspamd also says it’s not phishing. And then add sender to a Gmail whitelisting rule. All rescued email is flagged such that if I later manually move any of it back to junk, it stays there as spam and updates the corpus.&lt;/p&gt;
    &lt;p&gt;I now never get good email in the spam folder, and never get undetected spam in the inbox, and very occasionally get a spam erroneously rescued, but still visually flagged as iffy-but-maybe-ham.&lt;/p&gt;
    &lt;p&gt;If Gmail has been lax at filtering spam lately, I haven’t noticed, but perhaps the Bayesian filter has been picking up the slack.&lt;/p&gt;
    &lt;p&gt;I should consider this - I run my own domains, and for years I just forwarded it to gmail, but I had so many cases when mails were put into spam, even replies to emails I had sent in the middle of a long conversation between myself and 1 other person, that I went to just self-hosted IMAP. Then for years I couldn't reliably send to google or yahoo or MS; I added SPF a while ago which help, but recently buckled down and put in SRS and DMARC and DKIM (and rspamd while I was at it); now I get the mail I want, and can mostly send mail without it being rejected (still have to ask people to check spam, but anyways many people I have to tell them I'm emailing them anyways if its important). However I have a lot of non-spam "promotion" emails that I don't want to see. If I could train gmail to not block legit stuff reliably, that would be worth trying again (I would say except for the privacy implications, but since so much email involves gmail on one side or the other, they probably get most of it anyways).&lt;/p&gt;
    &lt;p&gt;Multiple accounts as others have said. The most powerful is to switch to a provider that permits custom domains and allows you to construct topic specific wildcard addresses on the fly. These can't be flagged as invalid or stripped like Google '+' suffixes and when compromised, you can filter them into oblivion and move on to something else. You also get the bonus of having the entire namespace to yourself and can select short addresses.&lt;/p&gt;
    &lt;p&gt;You don’t need an email service provider for that, plenty of DNS providers also offer email forwarding. I can recommend easyDNS, but various others are probably also good.&lt;/p&gt;
    &lt;p&gt;I use Gmail since the beta (I got invite from a googler) and I don't remember when they began adding spam control but in my experience the GMail spam check works usually exceptionally well: I very rarely need to add a custom filter.&lt;/p&gt;
    &lt;p&gt;My email, over two decades+ (2004?), hasn't been in a many public leaks (only one on https://haveibeenpwned.com/ ) but obviously has made its way to various spammy actors but thankfully nearly everything is caught by GMail's spam filter.&lt;/p&gt;
    &lt;p&gt;If anything I'd say GMail's spam filter works too well: I get more legit emails in my spam folder than spam in my regular inbox. As in: one in a rare while vs about zero spam in my regular inbox.&lt;/p&gt;
    &lt;p&gt;I'm having the exact opposite issue. 30+ emails today that clearly belong in spam (fake package delivery, "failed payment for your cloud subscription", etc) have landed in my inbox.&lt;/p&gt;
    &lt;p&gt;There's your confirmation, then. It must be either a localized failure to some subgroup of users, or triggered by some combination of settings, if some people are seeing it and others are not.&lt;/p&gt;
    &lt;p&gt;It's been happening for about a month for me. I had to start monitoring spam because legit emails end up there. Funnily enough I started having the opposite problem too - plenty of obvious spam and phishing attempt ending up in my mailbox.&lt;/p&gt;
    &lt;p&gt;I have seen a spam button show up I haven't seen in a long time.&lt;/p&gt;
    &lt;p&gt;It might be a new round of AI training featuring the labour of customers as free employees doing training. Every time we click, we consent to sharing private email data.&lt;/p&gt;
    &lt;p&gt;Its really slow. Too slow to use 2FA or in some cases, verify email addresses or recover passwords.&lt;/p&gt;
    &lt;p&gt;Most people can't handle a notification on their watch every minute, or several spam every five minutes, so "large numbers of people" are shutting off notifications on their phones. And human nature being what it is, they're not going to be turned back on again. So the era of getting a notification when you get an email is coming to a close. "Important Immediate Attention Stuff" moved to text messages a long time ago anyway, at least for me. The list of technologies you can no longer reach me on, always increases over time...&lt;/p&gt;
    &lt;p&gt;I don't understand why spam detection is so complicated. I can tell with high accuracy if an email is spam just by the subject line. I'd think even basic ML could do this very reliably you don't need a bleeding-edge LLM to do this.&lt;/p&gt;
    &lt;p&gt;Phishing is tricker because it can be very deceptive especially if you're being targeted specifically. But also usually pretty obvious.&lt;/p&gt;
    &lt;p&gt;* Are you available? * Paul, can we have a zoom meeting with you on Monday? * Assistance for donation * Greetings!!! * some ideas for you * Refund request * Somethings not working * Manuel Montoya for roof work contractor * proposals for print * Invite Connection&lt;/p&gt;
    &lt;p&gt;Half of the above are actual spam, half are not. Tell me which is which ...&lt;/p&gt;
    &lt;p&gt;This only applies to spam which requires significant follow-up effort from the spammer to respond to potential victims; effectively just 419 "advance-fee" fraud scams.&lt;/p&gt;
    &lt;p&gt;For spam which only does not require manual effort on the other side, there is no reason to filter out potential victims and all the more reason to make it look as legit as possible to maximize conversion rates.&lt;/p&gt;
    &lt;p&gt;&amp;gt; For spam which only does not require manual effort on the other side, there is no reason to filter out potential victims and all the more reason to make it look as legit as possible to maximize conversion rates.&lt;/p&gt;
    &lt;p&gt;Unless there's a trade-off. Saying "respond now or your account will be erased!" doesn't sound very legit. But the number of additional victims the phisher gets by doing probably outweighs the number of more sophisticated victims he loses.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46744807</guid><pubDate>Sat, 24 Jan 2026 16:16:02 +0000</pubDate></item><item><title>Metriport (YC S22) is hiring a security eng to harden healthcare data infra</title><link>https://www.ycombinator.com/companies/metriport/jobs/XC2AF8s-senior-security-engineer</link><description>&lt;doc fingerprint="47704651dcb5720e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Metriport is an open-source data intelligence platform that helps healthcare organizations access and exchange patient data in real-time. We integrate with all major US healthcare IT systems and tap into comprehensive medical data for 300+ million individuals.&lt;/p&gt;
      &lt;p&gt;We've found product-market fit with multi-million ARR, 100+ customers (including Strive Health, Circle Medical, and Brightside Health), backing from top VCs, and years of runway. We're ready to scale. We're a tight-knit, high-performing team of mostly former founders (including two YC alumni). We're engineering-heavy, operate with minimal bureaucracy and high autonomy, and hire based on competence, not prestige. We push hard—founders work six days a week from our SF office—but give everyone freedom to craft their schedule. We measure output and we're committed to sustainable intensity.&lt;/p&gt;
      &lt;head rend="h2"&gt;About us&lt;/head&gt;
      &lt;p&gt;The following points are an assortment of the most relevant bits that will give you the gist of where we’re at, why we’ll win, and our company culture:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Well funded with a massive recent infusion of capital, found PMF, multi-million ARR, 80+ customers (including Strive Health, Circle Medical, and Brightside Health), funded by top VCs and angels, have years of runway - and we’re just getting started.&lt;/item&gt;
        &lt;item&gt;We’re a tight-knit, high performing, and passionate team - we work with a consistent intensity and have become a leader in our industry with a fraction of the resources of our competitors.&lt;/item&gt;
        &lt;item&gt;Consistency means we push as hard as humanly possible, while keeping our health and personal lives in check.&lt;/item&gt;
        &lt;item&gt;Meaningful work is what gets us out of bed, and we just wouldn’t be satisfied by building yet another CRM company.&lt;/item&gt;
        &lt;item&gt;By pedigree, we’re a group of underdogs - we don’t hire based on prestige, but on demonstrated competence and perceived potential.&lt;/item&gt;
        &lt;item&gt;We’re engineering heavy, and most of our engineers are former founders (including 2 ex-YC founders).&lt;/item&gt;
        &lt;item&gt;We operate as a relatively flat structure with little red tape, forced structure, or bureaucracy. We just opt to get shit done and foster a collaborative environment with high autonomy - our GitHub commit history and product velocity is a testament to this.&lt;/item&gt;
        &lt;item&gt;The founders set the pace by working 6 days a week in our SF office, but everyone is given full freedom to craft a schedule that’s best for both the team and themselves - team output is measured.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;&lt;lb/&gt; About you&lt;/p&gt;
      &lt;p&gt;In a nutshell, we're looking for a security engineer with the following specific qualities:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You’re entrepreneurial-minded, with an olympian-level work ethic (nearly our entire engineering team consists of former founders).&lt;/item&gt;
        &lt;item&gt;You are passionate about security and are excited to own security related projects within the company end-to-end.&lt;/item&gt;
        &lt;item&gt;You are confident in your ability to build scalable systems across the full stack, and people usually come to you for technical guidance.&lt;/item&gt;
        &lt;item&gt;You believe you can solve any problem that comes at you, and don't shy away from diving deep into areas where you may lack domain expertise.&lt;/item&gt;
        &lt;item&gt;You have a strong sense of ownership over your work, and have demonstrated ability to lead others.&lt;/item&gt;
        &lt;item&gt;You know how to move fast - while still maintaining a strong security posture.&lt;/item&gt;
        &lt;item&gt;You care more about the end result and delivering value, rather than what new and frilly tech is being used under the hood for a given feature.&lt;/item&gt;
        &lt;item&gt;When someone scopes out a project with an ETA of 3 weeks, you ask yourself "why can't it be done in 3 days?".&lt;/item&gt;
        &lt;item&gt;You’re a hacker at heart, and have a good sense of what rules should, and shouldn’t, be broken.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;What you'll be doing&lt;/head&gt;
      &lt;p&gt;After quickly ramping up using our comprehensive onboarding materials to get familiar with our domain, product, and codebase, the goal would be to get you shipping product directly to customers as quickly as possible. Specifically, day to day, this looks like:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Evangelizing security across Metriport’s growing team - we will look to you for guidance, and training.&lt;/item&gt;
        &lt;item&gt;Driving full-stack security projects , big and small, end-to-end from ideation to production rollout.These projects could include things like: &lt;list rend="ul"&gt;&lt;item&gt;Implement an enterprise-grade audit logging solution for a new national healthcare network infrastructure stack.&lt;/item&gt;&lt;item&gt;Implement fine grained RBAC on the API key access layer, and more robust roles on our UIs.&lt;/item&gt;&lt;item&gt;Help us revamp our internal security policies and put tools in place to keep the platform, and employees, secure while still allowing the team to be efficient.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Helping the engineering team with PR reviews with a security-focused lens.&lt;/item&gt;
        &lt;item&gt;Work with the Go to Market team to complete customer security assessments and questionnaires.&lt;/item&gt;
        &lt;item&gt;Work with the engineering team to harden security across the development lifecycle - think secret management, access controls, and vulnerability scanning.&lt;/item&gt;
        &lt;item&gt;Managing your own work in Linear.&lt;/item&gt;
        &lt;item&gt;Participating in bi-weekly sprint planning / retro sessions, and quarterly planning sessions.&lt;/item&gt;
        &lt;item&gt;Attending a daily 30 minute remote stand-up at 7:30am PST Mon-Fri (our only regular mandatory meeting).&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;&lt;lb/&gt; Requirements&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You have 6+ years experience in security engineering and information security.&lt;/item&gt;
        &lt;item&gt;You’re located in San Francisco or the Bay Area (or willing to relocate).&lt;/item&gt;
        &lt;item&gt;Familiar with HIPAA compliant environments.&lt;/item&gt;
        &lt;item&gt;Experience rolling out and maintaining security frameworks like SOC 2, NIST, HITRUST, FedRAMP, etc.&lt;/item&gt;
        &lt;item&gt;Experience rolling out data protection technologies like SSO, MFA, VPN, FIPS, etc.&lt;/item&gt;
        &lt;item&gt;Experience with organizational secret management.&lt;/item&gt;
        &lt;item&gt;Experience implementing SCA, SAST, DAST in CICD workflows.&lt;/item&gt;
        &lt;item&gt;Experience with Mobile Device Management (MDM).&lt;/item&gt;
        &lt;item&gt;Proficiency in cloud security &amp;amp; networking on AWS - IAM, WAF, KMS, etc.&lt;/item&gt;
        &lt;item&gt;Proficiency in authentication, cryptography, encryption, and security protocols such as: mTLS, RSA, SSL, HMAC, RBAC, etc.&lt;/item&gt;
        &lt;item&gt;Bonus: experience with IHE profiles (ATNA, CT, XUA).&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Benefits&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Competitive equity + compensation package 🚀&lt;/item&gt;
        &lt;item&gt;Salary range: $160,000,00 - $220,000.00&lt;/item&gt;
        &lt;item&gt;Full family Platinum health insurance, dental, and vision coverage 🦷&lt;/item&gt;
        &lt;item&gt;401(k) retirement plan + matching 💰&lt;/item&gt;
        &lt;item&gt;Flexible work from home or in-office 🏢&lt;/item&gt;
        &lt;item&gt;Healthy lunches are complimentary when working in-office (and breakfast + dinners as needed) 🍏&lt;/item&gt;
        &lt;item&gt;Quarterly company off-sites with the team ⛷️&lt;/item&gt;
        &lt;item&gt;MacBook provided by us 💻&lt;/item&gt;
        &lt;item&gt;Unlimited PTO (we work hard, but trust you to take time you need to be at your best) 🧘♂️&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;&lt;lb/&gt; Our tech&lt;/p&gt;
      &lt;p&gt;On the frontend, we use React - on the backend, we rely on Node.js and TypeScript for writing core business logic. We deploy a wide range of AWS cloud services (ie ECS, Fargate, Lambda, etc), and manage our infrastructure as code with AWS CDK. Data lives in PostgreSQL, DynamoDB, S3, Snowflake, FHIR servers, and more. We use Oneleet for security and compliance.&lt;/p&gt;
      &lt;p&gt;Metriport provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity, or gender expression. We are committed to a diverse and inclusive workforce and welcome people from all backgrounds, experiences, perspectives, and abilities.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745224</guid><pubDate>Sat, 24 Jan 2026 17:00:07 +0000</pubDate></item><item><title>Raspberry Pi Drag Race: Pi 1 to Pi 5 – Performance Comparison</title><link>https://the-diy-life.com/raspberry-pi-drag-race-pi-1-to-pi-5-performance-comparison/</link><description>&lt;doc fingerprint="be10987704b616b6"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we’re going to be taking a look at what almost 13 years of development has done for the Raspberry Pi. I have one of each generation of Pi from the original Pi that was launched in 2012 through to the Pi 5 which was released just over a year ago.&lt;/p&gt;
    &lt;p&gt;We’ll take a look at what has changed between each generation and how their performance and power consumption has improved by running some tests on them.&lt;/p&gt;
    &lt;p&gt;Here’s my video of the testing process and results, read on for the write-up;&lt;/p&gt;
    &lt;head rend="h2"&gt;Purchase Links For Components Used In These Tests&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raspberry Pi 5 – Buy Here&lt;/item&gt;
      &lt;item&gt;Raspberry Pi 4 – Buy Here&lt;/item&gt;
      &lt;item&gt;Pi 5 Ice Tower Cooler – Buy Here&lt;/item&gt;
      &lt;item&gt;Pi 5 Power Supply – Buy Here&lt;/item&gt;
      &lt;item&gt;USB C to MicroUSB Adaptor – Buy Here&lt;/item&gt;
      &lt;item&gt;Sandisk Ultra MicroSD Card – Buy Here&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Equipment Used&lt;/head&gt;
    &lt;p&gt;Some of the above parts are affiliate links. By purchasing products through the above links, you’ll be supporting this channel, at no additional cost to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware Changes Through Each Generation&lt;/head&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 1&lt;/head&gt;
    &lt;p&gt;This is the original Raspberry Pi, which was launched in February 2012.&lt;/p&gt;
    &lt;p&gt;This Pi has a Broadcom BCM2835 SOC which features a single ARM1176JZF-S core running at 700MHz along with a VideoCore IV GPU. It has 512 MB of DDR RAM.&lt;/p&gt;
    &lt;p&gt;In terms of connectivity, it only has 100Mb networking and 2 x USB 2.0 ports. Video output is 1080P through a full-size HDMI port or analogue video out through a composite video connector and audio output is provided through a 3.5mm audio jack. It doesn’t have any WiFi or Bluetooth connectivity but it does have some of the features that we still have on more recent models like DSI and CSI ports, a full size SD card reader for the operating system and GPIO pins, although only 26 of them at this stage.&lt;/p&gt;
    &lt;p&gt;Power is supplied through a micro USB port and it is rated for 5V and 700mA.&lt;/p&gt;
    &lt;p&gt;It was priced at $35 – which at the time was incredibly cheap for what was essentially a palm-sized computer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 2&lt;/head&gt;
    &lt;p&gt;The Raspberry Pi 2 was launched 3 years later, in February 2015 and this Pi looked quite different to the original and similar to the Pi’s we know today.&lt;/p&gt;
    &lt;p&gt;The Pi 2 has a significantly better processor than the original. The Broadcom BCM2836 SOC has 4 Cortex-A7 cores running at 900 MHz and it retained the same VideoCore IV GPU. RAM was also bumped up to 1GB.&lt;/p&gt;
    &lt;p&gt;It added another 2 x USB 2.0 ports alongside the 100Mb Ethernet port. The composite video port disappeared and the analogue video output was moved into the audio jack.&lt;/p&gt;
    &lt;p&gt;The GPIO pins were increased to 40 pins which has followed the same pin layout since – which has really helped in maintaining compatibility with hats and accessories. The SD card reader was also changed to a microSD card reader.&lt;/p&gt;
    &lt;p&gt;The power circuitry was bumped up to 800mA to accommodate the more powerful CPU.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 3&lt;/head&gt;
    &lt;p&gt;The Raspberry Pi 3 was launched just a year later, in February 2016.&lt;/p&gt;
    &lt;p&gt;The Pi 3’s new Broadcom BCM2837 SOC retained the same 4-core architecture but these were changed to 64-bit Cortex A53 cores running at 1.2Ghz.&lt;/p&gt;
    &lt;p&gt;RAM was kept at 1GB but was now DDR2.&lt;/p&gt;
    &lt;p&gt;There was no change to the USB or Ethernet connectivity on the original Pi 3 but we did see WiFi and Bluetooth added for the first time. WiFi was single band 2.4GHz and we had Bluetooth 4.1.&lt;/p&gt;
    &lt;p&gt;The version that I have is actually the 3B+, which was launched a little later. The main improvements over the original Pi 3 were a 0.2GHz boost to the clock speed and the upgrade to Gigabit networking with PoE (Power over Ethernet) support and dual-band WiFi.&lt;/p&gt;
    &lt;p&gt;The power circuitry was again improved, still running at 5V but now up to 1.34A, which was almost double the Pi 2.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 4&lt;/head&gt;
    &lt;p&gt;Next came the Pi 4 in June 2019. This Pi came at one of the worst times for global manufacturing and was notoriously difficult to get hold of due to the impact of COVID on the global supply chain. Quite ironically, this hard-to-get Pi is the one that I’ve got the most of, mainly due to my water-cooled Pi cluster build.&lt;/p&gt;
    &lt;p&gt;The Pi 4 has a Broadcom BCM2711 SOC with 4 Cortex-A72 cores running at 1.5GHz. So again a slight clock speed increase over the Pi 3 but still retaining 4 cores. It also includes a bump up to a VideoCore VI GPU.&lt;/p&gt;
    &lt;p&gt;This was the first model to feature different RAM configurations. It was originally available in 1, 2, 4GB variants featuring LPDDR4 RAM and in March of 2020 an 8GB variant was added to the linup as well. This obviously resulted in a few different price points but impressively they still managed to keep a $35 offering 7 years after the launch of the first Pi.&lt;/p&gt;
    &lt;p&gt;It retained the same form factor as the Pi 3 but with the network and Ethernet ports switched around. Notably, two of the USB ports were upgraded to USB 3.0, networking was now gigabit ethernet like the 3B+, WiFi was dual-band and it had Bluetooth 5.0.&lt;/p&gt;
    &lt;p&gt;They also changed the single full-size HDMI port to two micro HDMI ports. Most people I know don’t like this change and find it annoying to have to use adaptors to work with common displays and these micro HDMI ports are prone to breaking when they are used often. I think general hobbyists and makers would prefer this to still be a single full-size port but Pi’s are often used in commercial display applications so I guess that’s why they went with this dual micro HDMI configuration.&lt;/p&gt;
    &lt;p&gt;The power circuit was actually reduced in this model, from 1.34 down to 1.25A and the port was changed to USB C.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;p&gt;Lastly and most recently we have the Pi 5 which was launched in October 2023.&lt;/p&gt;
    &lt;p&gt;This Pi features a Broadcom BCM2712 SOC with 4 Cortex A76 cores running at a significantly faster 2.4Ghz and a VideoCore VII GPU running at 800MHz.&lt;/p&gt;
    &lt;p&gt;So quite a bump up in CPU and GPU performance.&lt;/p&gt;
    &lt;p&gt;It is offered in 3 RAM configurations but the drop in a 1GB offering means that they’re no longer available at the $35 price point. There is a fairly significant increase in price up to $50 for the base 2GB variant.&lt;/p&gt;
    &lt;p&gt;Some other notable changes are the inclusion of a PCIe port which enables IO expansion and a much improved power circuit. The PCIe port is quite commonly used to add an NVMe SSD instead of a microSD card for the operating system.&lt;/p&gt;
    &lt;p&gt;The power circuit was upgraded to handle the PCIe port addition, now stepping up to 5V at up to 5A, along with a power button for the first time.&lt;/p&gt;
    &lt;p&gt;The change in power supply requirements to 5V and 5A is a bit annoying as most power delivery capable supplies cap out 2.5 or 3A at 5V. It would have been more universal to require a 9V 3A supply to meet the Pis power requirements. I assume they steered away from this because the Pi’s circuitry runs at 5V and 3.3V and they would have then needed to add another onboard DC-DC converter which increases complexity, size and potentially the cost, it would also have made it a bit less efficient. But this does mean that you most likely need to buy a USB C power supply that has been purpose-built for the Pi 5.&lt;/p&gt;
    &lt;p&gt;The Pi 5 is also the first Pi to have its own dedicated fan socket.&lt;/p&gt;
    &lt;p&gt;So that’s a summary of the hardware changes, now let’s boot them up and take a look at their performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing The Performance Of Each Generation Of Pi&lt;/head&gt;
    &lt;p&gt;To compare the performance between the Pi’s, I’m going to run the following tests.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I’m going to attempt to playback a 1080P YouTube video in the browser, although I expect we’ll have problems with this up to the Pi 4.&lt;/item&gt;
      &lt;item&gt;We’ll then run a Sysbench CPU benchmark which I’ll do both for a single-core and multicore.&lt;/item&gt;
      &lt;item&gt;Then we’ll run a GLMark2 GPU benchmark.&lt;/item&gt;
      &lt;item&gt;Then test the storage speed using James Chambers Pi Benchmark script.&lt;/item&gt;
      &lt;item&gt;Then we’ll run an iPerf3 Network Speed test.&lt;/item&gt;
      &lt;item&gt;Lastly, we’ll look at Power Consumption, both at idle and with the CPU maxed out.&lt;/item&gt;
      &lt;item&gt;And then use that data to determine each Pi’s Performance per Watt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To keep things as consistent as possible I’m going to be running the latest available version of Pi OS from Raspberry Pi Imager for each Pi. I was pleasantly surprised to find that you can still flash an OS image for the original Pi in their latest version of Imager.&lt;/p&gt;
    &lt;p&gt;I’ll be testing them all running on a 32GB Sandisk Ultra microSD card. I’ll also be using an Ice Tower cooler on each to ensure they don’t run anywhere near thermal throttling.&lt;/p&gt;
    &lt;head rend="h3"&gt;1080P YouTube Video Playback&lt;/head&gt;
    &lt;p&gt;I started with the original Pi and its first boot and setup process was a lesson in patience. It took me the best part of two hours to get the first boot complete, the Pi updated and the testing utilities installed but I got there in the end.&lt;/p&gt;
    &lt;p&gt;Even once set up it takes about 8 minutes to boot up to the desktop and the CPU stays pegged at 100% for another two to three minutes before dropping down to about 20% at idle.&lt;/p&gt;
    &lt;p&gt;The original Pi refused to open up the browser, so that’s where my YouTube video playback test ended.&lt;/p&gt;
    &lt;p&gt;The Pi 2 managed to open the browser and actually started playing back a 1080P video, which was surprising, but playback was terrible. It dropped pretty much all of the frames both in the window and fullscreen.&lt;/p&gt;
    &lt;p&gt;The Pi 3 played video back noticeably better than the Pi 2, but it’s still quite a long way away from being usable and still drops a lot of frames.&lt;/p&gt;
    &lt;p&gt;The Pi 4 handled 1080P video reasonably well. It had some initial trouble but then settled down. Fullscreen is also a bit choppy but is also usable.&lt;/p&gt;
    &lt;p&gt;The Pi 5 handled 1080P playback well without any significant issues both in the window and fullscreen.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sysbench CPU Benchmark&lt;/head&gt;
    &lt;p&gt;Next was the Sysbench CPU benchmark. I ran three tests on each and averaged the scores and I did this for both single-core and multicore.&lt;/p&gt;
    &lt;p&gt;In single core, the Pi 1 managed a rather dismal score of 68, the Pi 2 got a bit more than double this score but the real step up was with the Pi 3 which managed 18 times higher than the Pi 2. The Pi 4 and Pi 5 also offered good improvements on the previous generations.&lt;/p&gt;
    &lt;p&gt;Similarly in multicore, the Pi 3 scored over 18 times the score of the Pi2 and the Pi 4 and 5 provided good improvements on the Pi 3’s score.&lt;/p&gt;
    &lt;p&gt;Comparing the combined multicore score of the Pi 5 to what the single core on the Pi 1 can do, the Pi 5 is a little over 600 times faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;GLmark2 GPU Benchmark&lt;/head&gt;
    &lt;p&gt;Next, I tried running a GLMark2 GPU benchmark on them. I used the GLMark2-es2-wayland version which is designed for OpenGL ES so that the Pi 1 was supported.&lt;/p&gt;
    &lt;p&gt;I was surprised that the Pi 1 was even able to run GLMark2 – it did complete the benchmark, although the score wasn’t all that impressive.&lt;/p&gt;
    &lt;p&gt;These results really show how the Pi’s GPU has improved in the last two generations. Prior to these tests, I had never seen a score below 100 and the Pi 1, 2 and 3 managed to fall short of triple digits. Pi 5 scored over 2.5 times higher than the Pi 4.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage Speed Test&lt;/head&gt;
    &lt;p&gt;Next was the storage speed test using James Chambers Pi Benchmarks script. The bus speed has increased over the years from 25MHz on the Pi 1 to 100MHz on the Pi 5, so I expect we’ll see these reflected in the benchmark scores.&lt;/p&gt;
    &lt;p&gt;The storage speed test’s results aren’t as dramatic as the CPU and GPU results but show a steady improvement between generations. The Pi 3 did a bit worse than the Pi 2 but this small difference is likely just due to variability in the tests.&lt;/p&gt;
    &lt;head rend="h3"&gt;iPerf Network Speed Test&lt;/head&gt;
    &lt;p&gt;Next, I ran the iPerf network speed test on each.&lt;/p&gt;
    &lt;p&gt;The Pi 1 doesn’t quite get close to its theoretical 100Mbps but the Pi 2 does. The Pi 3 B+ although having Gigabit Ethernet is limited by this running over USB 2.0 which only has a theoretical maximum of 300MBps, so it came quite close. Both the Pi 4 and 5 expectedly come close to theoretical Gigabit speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Consumption Test&lt;/head&gt;
    &lt;p&gt;Lastly, I tested the power consumption of each Pi at idle and under load.&lt;/p&gt;
    &lt;p&gt;I used the same Pi 5 power adaptor to test all of the Pis to keep things consistent and I just used a USB C to micro USB adaptor for the Pi 1, 2 and 3.&lt;/p&gt;
    &lt;p&gt;The idle results were closer than I expected. The Pi 2 had the lowest idle power draw and the Pi 5 the highest, but all were within a watt or two of each other. At full load, you can see the increase in CPU power draw more physical power with the Pi 5 drawing almost three times the Pi 1 and Pi 2.&lt;/p&gt;
    &lt;p&gt;Converted to performance per watt using the Sysbench results, we can again see how much better the Pi 4 and 5 are over the Pi 1 and 2. There is a clear improvement in the performance that each generation of Pi is able to get per watt of power, which is essentially its efficiency. Although the Pi 5 draws more power than the Pi 1 under full load, you’re getting almost 200 times more power out of it per watt.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts On The Drag Race And Future Pis&lt;/head&gt;
    &lt;p&gt;I really enjoyed working through this project to see how much Pi’s have changed over the years, particularly in terms of performance. I still remember being amazed at the size and price of the original Pi when it came out and it’s great that they’re still fully supported and can still be used for projects – albeit with less CPU-intensive projects.&lt;/p&gt;
    &lt;p&gt;Let me know what you think has been the biggest improvement to the Pi over the years and what you’d still like to see added to future models in the comments section below.&lt;/p&gt;
    &lt;p&gt;I personally really like the addition of the PCIe port on the Pi 5 and I’d like to see 2.5Gb networking and a DisplayPort or USB C with DisplayPort added to a future generation of Pi.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745922</guid><pubDate>Sat, 24 Jan 2026 18:06:00 +0000</pubDate></item><item><title>Understanding Rust Closures</title><link>https://antoine.vandecreme.net/blog/rust-closures/</link><description>&lt;doc fingerprint="bc31cda920a59147"&gt;
  &lt;main&gt;
    &lt;p&gt;While reading the Explicit capture clauses blog post, I realized that my understanding of rust closures was very superficial. This article is an attempt at explaining what I learned while reading and experimenting on the subject. It starts from the very basics and then explore more complex topics. Note that each title is a link to a rust playground where you can experiment with the code in the section.&lt;/p&gt;
    &lt;head rend="h1"&gt;Closures basics&lt;/head&gt;
    &lt;p&gt;You probably already know that a closure in rust is a function written with the following syntax:&lt;/p&gt;
    &lt;code&gt;let double_closure = |x| x * 2;
assert_eq!(4, double_closure(2));
&lt;/code&gt;
    &lt;p&gt;Written as a regular function it looks like:&lt;/p&gt;
    &lt;code&gt;fn double_function(x: u32) -&amp;gt; u32 {
    x * 2
}
assert_eq!(4, double_function(2));
&lt;/code&gt;
    &lt;p&gt;Very similar. There is actually a small difference between the two, the &lt;code&gt;double_function&lt;/code&gt; parameter and return type are &lt;code&gt;u32&lt;/code&gt;.
On the other hand, because we did not specify any type in &lt;code&gt;double_closure&lt;/code&gt;, the
default integer type has been picked, namely &lt;code&gt;i32&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can fix that like this:&lt;/p&gt;
    &lt;code&gt;let double_typed_closure = |x: u32| -&amp;gt; u32 { x * 2 };
assert_eq!(4, double_typed_closure(2));
assert_eq!(4, double_typed_closure(2u32));
// assert_eq!(4, double_typed_closure(2u16)); // This would be an error.
&lt;/code&gt;
    &lt;p&gt;And for a classic example usage of closures, we can use the &lt;code&gt;Option::map&lt;/code&gt; method:&lt;/p&gt;
    &lt;code&gt;assert_eq!(Some(4), Some(2).map(|x| x * 2));
assert_eq!(Some(4), Some(2).map(double_closure)); // double_closure from above
assert_eq!(Some(4), Some(2).map(double_function)); // Passing double_function works too!
&lt;/code&gt;
    &lt;p&gt;So, it seems closures are just a shorter syntax for functions with type inference.&lt;/p&gt;
    &lt;head rend="h1"&gt;Capture&lt;/head&gt;
    &lt;p&gt;The main difference between closures and functions is that closures can capture variables from their environment while functions can't:&lt;/p&gt;
    &lt;code&gt;let hello = "Hello ";
let greeter_closure = |x| String::new() + hello + x;

assert_eq!("Hello world", greeter_closure("world"));
assert_eq!(
    Some("Hello world".to_owned()),
    Some("world").map(greeter_closure)
);
&lt;/code&gt;
    &lt;p&gt;Notice how the &lt;code&gt;hello&lt;/code&gt; variable is used within the body of the &lt;code&gt;greeter_closure&lt;/code&gt;.
Let's try that with a function:&lt;/p&gt;
    &lt;code&gt;let hello = "Hello ";

fn greeter_function(x: &amp;amp;str) -&amp;gt; String {
    String::new() + hello + x
}
&lt;/code&gt;
    &lt;code&gt;error[E0434]: can't capture dynamic environment in a fn item
 --&amp;gt; src/main.rs:7:25
  |
7 |         String::new() + hello + x
  |                         ^^^^^
  |
  = help: use the `|| { ... }` closure form instead
&lt;/code&gt;
    &lt;p&gt;This does not work and the compiler helpfully suggest to use a closure instead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Capture by shared reference&lt;/head&gt;
    &lt;p&gt;In the &lt;code&gt;greeter_closure&lt;/code&gt; example above, the &lt;code&gt;hello&lt;/code&gt; variable was captured by
shared reference because the variable is only read.
As shown below, we can still use that variable after the closure declaration and usage:&lt;/p&gt;
    &lt;code&gt;let hello = "Hello ";
let greeter_closure = |x| String::new() + hello + x;

// We can still use the `hello` variable here
assert_eq!("Hello ", hello);

assert_eq!("Hello world", greeter_closure("world"));

// And here
assert_eq!("Hello ", hello);
&lt;/code&gt;
    &lt;head rend="h2"&gt;Capture by mutable reference&lt;/head&gt;
    &lt;p&gt;It is also possible to capture by mutable reference so that the closure can alter the value of the captured variable. See this naive way to compute the sum of integers from 1 to 10:&lt;/p&gt;
    &lt;code&gt;let mut total = 0;
let add_mut_closure = |x| total += x;

// We can't access total here:
// assert_eq!(0, total);
// error[E0502]: cannot borrow `total` as immutable because it is also borrowed as mutable

(1..=10).for_each(add_mut_closure);

// But we can access total here, now that `add_mut_closure` is out of scope.
assert_eq!(55, total);
&lt;/code&gt;
    &lt;head rend="h2"&gt;Capture by value&lt;/head&gt;
    &lt;p&gt;Finally, one can capture by value:&lt;/p&gt;
    &lt;code&gt;let last_word = "last word: ".to_owned();
let drop_closure = |sigh| {
    let res = String::new() + &amp;amp;last_word + sigh;
    drop(last_word); // Forcing the capture by value
    res
};

// We can't access `last_word` here:
// assert_eq!("last word: ".to_owned(), last_word);
// error[E0382]: borrow of moved value: `last_word`

assert_eq!("last word: sigh!", drop_closure("sigh!"));

// We can't access `last_word` here either
// assert_eq!("last word: ".to_owned(), last_word);
// error[E0382]: borrow of moved value: `last_word`

// And we can't call drop_closure again
// assert_eq!("last word: sigh!", drop_closure("sigh!"));
// error[E0382]: use of moved value: `drop_closure`
&lt;/code&gt;
    &lt;head rend="h1"&gt;FnOnce trait&lt;/head&gt;
    &lt;p&gt;In the previous example, notice the last error when trying to call &lt;code&gt;drop_closure&lt;/code&gt; twice.
Here is the full error:&lt;/p&gt;
    &lt;code&gt;error[E0382]: use of moved value: `drop_closure`
  --&amp;gt; src/main.rs:18:32
   |
12 | assert_eq!("last word: sigh!", drop_closure("sigh!"));
   |                                --------------------- `drop_closure` moved due to this call
...
18 | assert_eq!("last word: sigh!", drop_closure("sigh!"));
   |                                ^^^^^^^^^^^^ value used here after move
   |
note: closure cannot be invoked more than once because it moves the variable `last_word` out of its environment
  --&amp;gt; src/main.rs:5:10
   |
 5 |     drop(last_word);
   |          ^^^^^^^^^
note: this value implements `FnOnce`, which causes it to be moved when called
  --&amp;gt; src/main.rs:12:32
   |
12 | assert_eq!("last word: sigh!", drop_closure("sigh!"));
   |                                ^^^^^^^^^^^^
&lt;/code&gt;
    &lt;p&gt;The interesting note is:&lt;/p&gt;
    &lt;code&gt;note: this value implements `FnOnce`, which causes it to be moved when called
&lt;/code&gt;
    &lt;p&gt;What is that &lt;code&gt;FnOnce&lt;/code&gt; implementation the compiler is talking about?&lt;/p&gt;
    &lt;p&gt;It is a trait automatically implemented by the compiler which state that the closure can be called at least once.&lt;/p&gt;
    &lt;p&gt;That trait is a bit special because it cannot be implemented manually in stable rust.&lt;lb/&gt; However, if we switch to unstable and enable some features, we can play with it and try to desugar how closures are actually implemented by the compiler.&lt;/p&gt;
    &lt;p&gt;Let's try to desugar the &lt;code&gt;drop_closure&lt;/code&gt; above.&lt;/p&gt;
    &lt;p&gt;First, make sure to switch to the nightly channel and to enable the following features (for example by putting them at the top of your &lt;code&gt;main.rs&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;#![feature(fn_traits)]
#![feature(unboxed_closures)]
&lt;/code&gt;
    &lt;p&gt;Next, we need to define a struct having the captured variables as fields:&lt;/p&gt;
    &lt;code&gt;struct DropStruct {
    last_word: String,
}
&lt;/code&gt;
    &lt;p&gt;Simple enough, we are capturing only one variable so our struct has one field.&lt;/p&gt;
    &lt;p&gt;Now the &lt;code&gt;FnOnce&lt;/code&gt; implementation:&lt;/p&gt;
    &lt;code&gt;impl FnOnce&amp;lt;(&amp;amp;str,)&amp;gt; for DropStruct {
    type Output = String;
    extern "rust-call" fn call_once(self, (sigh,): (&amp;amp;str,)) -&amp;gt; Self::Output {
        let res = String::new() + &amp;amp;self.last_word + sigh;
        drop(self.last_word);
        res
    }
}
&lt;/code&gt;
    &lt;p&gt;That is some weird trait!&lt;/p&gt;
    &lt;p&gt;Let's go step by step.&lt;code&gt;impl FnOnce&amp;lt;(&amp;amp;str,)&amp;gt;&lt;/code&gt; means that we are implementing a closure which takes one parameter which is a &lt;code&gt;&amp;amp;str&lt;/code&gt;.&lt;lb/&gt; If the closure took two arguments of type &lt;code&gt;i32&lt;/code&gt; and &lt;code&gt;i64&lt;/code&gt; we would have &lt;code&gt;impl FnOnce&amp;lt;(i32, i64)&amp;gt;&lt;/code&gt;. &lt;code&gt;(&amp;amp;str,)&lt;/code&gt; is the definition of a tuple of one element.
See the reference on tuple types for details.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;for DropStruct&lt;/code&gt; should not be too surprising.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;type Output = String&lt;/code&gt; specifies that our closure returns a &lt;code&gt;String&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;extern "rust-call"&lt;/code&gt; is some magic which I won't explain mostly because I don't know exactly why it is required.&lt;/p&gt;
    &lt;p&gt;The rest of the implementation should be self explanatory. We just took the content of the closure and replaced &lt;code&gt;last_word&lt;/code&gt; by &lt;code&gt;self.last_word&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's try it:&lt;/p&gt;
    &lt;code&gt;let last_word = "last word: ".to_owned();
let drop_struct = DropStruct { last_word };

// We could call `call_once`:
// assert_eq!("last word: sigh!", drop_struct.call_once(("sigh!",)));

// But more simply, we can use the function call syntax:
assert_eq!("last word: sigh!", drop_struct("sigh!"));

// And we still can't call it twice
// assert_eq!("last word: sigh!", drop_struct("sigh!"));
// error[E0382]: use of moved value: `drop_struct`
&lt;/code&gt;
    &lt;head rend="h1"&gt;FnMut trait&lt;/head&gt;
    &lt;p&gt;What about our &lt;code&gt;add_mut_closure&lt;/code&gt; from before? We were able to call it
multiple times and even mutate the capture variables.&lt;/p&gt;
    &lt;p&gt;That kind of closure implements the &lt;code&gt;FnMut&lt;/code&gt; trait.&lt;/p&gt;
    &lt;p&gt;Let's try to desugar the following closure which push elements in a vector:&lt;/p&gt;
    &lt;code&gt;let mut v = vec![];
let push_closure = |x| v.push(x);

(1..=5).for_each(push_closure);
assert_eq!(vec![1, 2, 3, 4, 5], v);
&lt;/code&gt;
    &lt;p&gt;First we need to define a struct:&lt;/p&gt;
    &lt;code&gt;struct PusherStruct&amp;lt;'a&amp;gt; {
    v: &amp;amp;'a mut Vec&amp;lt;i32&amp;gt;,
}
&lt;/code&gt;
    &lt;p&gt;Because we are capturing by reference, we need to introduce a lifetime.&lt;/p&gt;
    &lt;p&gt;Now the &lt;code&gt;FnMut&lt;/code&gt; implementation:&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;'a&amp;gt; FnMut&amp;lt;(i32,)&amp;gt; for PusherStruct&amp;lt;'a&amp;gt; {
    extern "rust-call" fn call_mut(&amp;amp;mut self, (x,): (i32,)) -&amp;gt; Self::Output {
        self.v.push(x)
    }
}
&lt;/code&gt;
    &lt;p&gt;It is very similar to the &lt;code&gt;FnOnce&lt;/code&gt; trait except that the function is called &lt;code&gt;call_mut&lt;/code&gt; instead of &lt;code&gt;call_once&lt;/code&gt; and that it takes &lt;code&gt;&amp;amp;mut self&lt;/code&gt; instead of &lt;code&gt;self&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's try to compile that:&lt;/p&gt;
    &lt;code&gt;error[E0277]: expected a `FnOnce(i32)` closure, found `PusherStruct&amp;lt;'a&amp;gt;`
 --&amp;gt; src/main.rs:8:5
  |
8 |     extern "rust-call" fn call_mut(&amp;amp;mut self, args: (i32,)) -&amp;gt; Self::Output {
  |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ expected an `FnOnce(i32)` closure, found `PusherStruct&amp;lt;'a&amp;gt;`
  |
help: the trait `FnOnce(i32)` is not implemented for `PusherStruct&amp;lt;'a&amp;gt;`
&lt;/code&gt;
    &lt;p&gt;Turns out we need to implement &lt;code&gt;FnOnce&lt;/code&gt; too. Remember that &lt;code&gt;FnOnce&lt;/code&gt; defines functions which can be called at least once.
In the example above, we called our closure 5 times, so it can definitely be called at least once.&lt;/p&gt;
    &lt;p&gt;Let's implement it:&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;'a&amp;gt; FnOnce&amp;lt;(i32,)&amp;gt; for PusherStruct&amp;lt;'a&amp;gt; {
    type Output = ();
    extern "rust-call" fn call_once(mut self, args: (i32,)) -&amp;gt; Self::Output {
        self.call_mut(args)
    }
}
&lt;/code&gt;
    &lt;p&gt;Our closure does not return anything so the &lt;code&gt;Output&lt;/code&gt; is the unit.&lt;lb/&gt; As for the &lt;code&gt;call_once&lt;/code&gt; implementation, we can just call &lt;code&gt;call_mut&lt;/code&gt; to avoid repetition.&lt;/p&gt;
    &lt;p&gt;This should compile and we can now use it like so:&lt;/p&gt;
    &lt;code&gt;let mut v = vec![];
let pusher_struct = PusherStruct { v: &amp;amp;mut v };

(1..=5).for_each(pusher_struct);
assert_eq!(vec![1, 2, 3, 4, 5], v);
&lt;/code&gt;
    &lt;head rend="h1"&gt;Fn trait&lt;/head&gt;
    &lt;p&gt;Finally, there is a third trait implemented by closures which can be called multiple times and don't need a mutable reference; the Fn trait.&lt;/p&gt;
    &lt;p&gt;To see that let's try to desugar the &lt;code&gt;greeter_closure&lt;/code&gt; from before:&lt;/p&gt;
    &lt;code&gt;let hello = "Hello ";
let greeter_closure = |x| String::new() + hello + x;

assert_eq!("Hello world", greeter_closure("world"));
assert_eq!("Hello rust", greeter_closure("rust")); // Can be called multiple times
&lt;/code&gt;
    &lt;p&gt;As usual, we need to define our struct:&lt;/p&gt;
    &lt;code&gt;struct GreeterStruct&amp;lt;'a&amp;gt; {
    hello: &amp;amp;'a str,
}
&lt;/code&gt;
    &lt;p&gt;Let's not make the same mistake as before, and remember to implement &lt;code&gt;FnOnce&lt;/code&gt; and &lt;code&gt;FnMut&lt;/code&gt; first. The same way an &lt;code&gt;FnMut&lt;/code&gt; closures are also &lt;code&gt;FnOnce&lt;/code&gt; because they can be called at least once. &lt;code&gt;Fn&lt;/code&gt; closures are also &lt;code&gt;FnMut&lt;/code&gt; because if given a mutable reference, they can still perform their work which does not mutate the reference.&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;'a, 'b&amp;gt; FnOnce&amp;lt;(&amp;amp;'b str,)&amp;gt; for GreeterStruct&amp;lt;'a&amp;gt; {
    type Output = String;
    extern "rust-call" fn call_once(self, args: (&amp;amp;'b str,)) -&amp;gt; Self::Output {
        self.call(args)
    }
}

impl&amp;lt;'a, 'b&amp;gt; FnMut&amp;lt;(&amp;amp;'b str,)&amp;gt; for GreeterStruct&amp;lt;'a&amp;gt; {
    extern "rust-call" fn call_mut(&amp;amp;mut self, args: (&amp;amp;'b str,)) -&amp;gt; Self::Output {
        self.call(args)
    }
}
&lt;/code&gt;
    &lt;p&gt;This should be pretty straightforward. &lt;code&gt;call_once&lt;/code&gt; and &lt;code&gt;call_mut&lt;/code&gt; are just calling &lt;code&gt;call&lt;/code&gt; which is defined in &lt;code&gt;Fn&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;impl&amp;lt;'a, 'b&amp;gt; Fn&amp;lt;(&amp;amp;'b str,)&amp;gt; for GreeterStruct&amp;lt;'b&amp;gt; {
    extern "rust-call" fn call(&amp;amp;self, (x,): (&amp;amp;'b str,)) -&amp;gt; Self::Output {
        String::new() + &amp;amp;self.hello + &amp;amp;x
    }
}
&lt;/code&gt;
    &lt;p&gt;And we can use it like this:&lt;/p&gt;
    &lt;code&gt;let hello = "Hello";
let greeter_struct = GreeterStruct {
    hello,
};

assert_eq!("Hello world", greeter_struct("world"));
assert_eq!("Hello rust", greeter_struct("rust")); // Can be called multiple times
&lt;/code&gt;
    &lt;head rend="h1"&gt;The move keyword&lt;/head&gt;
    &lt;p&gt;You may already know that one can add the &lt;code&gt;move&lt;/code&gt; keyword in front of a closure to force the closure to take ownership of the capture variables even if the closure only need a reference to it.&lt;lb/&gt; For example:&lt;/p&gt;
    &lt;code&gt;let hello = "Hello ".to_owned();
let greeter_closure = move |x| String::new() + &amp;amp;hello + x;

// We can't access `hello` here
// assert_eq!("Hello ", hello);
// error[E0382]: borrow of moved value: `hello`

assert_eq!("Hello world", greeter_closure("world"));

// Nor here
// assert_eq!("Hello ", hello);
// error[E0382]: borrow of moved value: `hello`
&lt;/code&gt;
    &lt;p&gt;In order to clearly understand what we can do depending on whether the closure needs a shared reference, a mutable reference or a value and if there is &lt;code&gt;move&lt;/code&gt; keyword or not, let's introduce those small dummy functions:&lt;/p&gt;
    &lt;code&gt;fn by_ref(_data: &amp;amp;String) {}

fn by_mut(_data: &amp;amp;mut String) {}

fn by_value(_data: String) {}
&lt;/code&gt;
    &lt;p&gt;Now, let's see what we can do with different combination of move / not move and by_ref / by_mut / by_value:&lt;/p&gt;
    &lt;code&gt;let data = "by_ref".to_owned();
let by_ref_closure = || by_ref(&amp;amp;data);

// Access data while the closure is still in scope
assert_eq!("by_ref", data);

// Call the closure once
by_ref_closure();

// Call the closure multiple times
by_ref_closure();

// Access data once the closure is out of scope
assert_eq!("by_ref", data);
&lt;/code&gt;
    &lt;p&gt;Now with move:&lt;/p&gt;
    &lt;code&gt;let data = "move_by_ref".to_owned();
let move_by_ref_closure = move || by_ref(&amp;amp;data);

// Access data while the closure is still in scope
// assert_eq!("move_by_ref", data);
// error[E0382]: borrow of moved value: `data`

// Call the closure once
move_by_ref_closure();

// Call the closure multiple times
move_by_ref_closure();

// Access data once the closure is out of scope
// assert_eq!("move_by_ref", data);
// error[E0382]: borrow of moved value: `data`
&lt;/code&gt;
    &lt;p&gt;This makes sense, since the closure took ownership of &lt;code&gt;data&lt;/code&gt; we can't access it anymore from outside.&lt;/p&gt;
    &lt;p&gt;Similarly we can define the following closures:&lt;/p&gt;
    &lt;code&gt;let mut data = "by_mut".to_owned();
let by_mut_closure = || by_mut(&amp;amp;mut data);

let mut data = "move_by_mut".to_owned();
let move_by_mut_closure = move || by_mut(&amp;amp;mut data);

let data = "by_value".to_owned();
let by_value_closure = || by_value(data);

let data = "move_by_value".to_owned();
let move_by_value_closure = move || by_value(data);
&lt;/code&gt;
    &lt;p&gt;I will let you play with them, here what you should see:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;by_ref&lt;/cell&gt;
        &lt;cell role="head"&gt;by_mut&lt;/cell&gt;
        &lt;cell role="head"&gt;by_value&lt;/cell&gt;
        &lt;cell role="head"&gt;move by_ref&lt;/cell&gt;
        &lt;cell role="head"&gt;move by_mut&lt;/cell&gt;
        &lt;cell role="head"&gt;move by_value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Access when in scope&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Call once&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Call multiple times&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Access when out of scope&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;And the trait implemented by each closures:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;by_ref&lt;/cell&gt;
        &lt;cell role="head"&gt;by_mut&lt;/cell&gt;
        &lt;cell role="head"&gt;by_value&lt;/cell&gt;
        &lt;cell role="head"&gt;move by_ref&lt;/cell&gt;
        &lt;cell role="head"&gt;move by_mut&lt;/cell&gt;
        &lt;cell role="head"&gt;move by_value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;FnOnce&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;FnMut&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We can see that the &lt;code&gt;move&lt;/code&gt; keyword has no impact on the implemented trait. It only changes the capture to be from reference to value.&lt;/p&gt;
    &lt;p&gt;For example, the desugaring of &lt;code&gt;by_ref_closure&lt;/code&gt; is:&lt;/p&gt;
    &lt;code&gt;struct ByRefStruct&amp;lt;'a&amp;gt; {
    data: &amp;amp;'a String,
}

impl&amp;lt;'a&amp;gt; FnOnce&amp;lt;()&amp;gt; for ByRefStruct&amp;lt;'a&amp;gt; {
    type Output = ();
    extern "rust-call" fn call_once(self, args: ()) -&amp;gt; Self::Output {
        self.call(args)
    }
}

impl&amp;lt;'a&amp;gt; FnMut&amp;lt;()&amp;gt; for ByRefStruct&amp;lt;'a&amp;gt; {
    extern "rust-call" fn call_mut(&amp;amp;mut self, args: ()) -&amp;gt; Self::Output {
        self.call(args)
    }
}

impl&amp;lt;'a&amp;gt; Fn&amp;lt;()&amp;gt; for ByRefStruct&amp;lt;'a&amp;gt; {
    extern "rust-call" fn call(&amp;amp;self, (): ()) -&amp;gt; Self::Output {
        by_ref(self.data)
    }
}
&lt;/code&gt;
    &lt;p&gt;whereas for &lt;code&gt;move_by_ref_closure&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;struct MoveByRefStruct {
    data: String,
}

impl FnOnce&amp;lt;()&amp;gt; for MoveByRefStruct {
    type Output = ();
    extern "rust-call" fn call_once(self, args: ()) -&amp;gt; Self::Output {
        self.call(args)
    }
}

impl FnMut&amp;lt;()&amp;gt; for MoveByRefStruct {
    extern "rust-call" fn call_mut(&amp;amp;mut self, args: ()) -&amp;gt; Self::Output {
        self.call(args)
    }
}

impl Fn&amp;lt;()&amp;gt; for MoveByRefStruct {
    extern "rust-call" fn call(&amp;amp;self, (): ()) -&amp;gt; Self::Output {
        by_ref(&amp;amp;self.data)
    }
}
&lt;/code&gt;
    &lt;p&gt;Notice how the &lt;code&gt;data&lt;/code&gt; field changed from &lt;code&gt;&amp;amp;'a String&lt;/code&gt; to &lt;code&gt;String&lt;/code&gt; and the call to &lt;code&gt;by_ref&lt;/code&gt; from &lt;code&gt;self.data&lt;/code&gt; to &lt;code&gt;&amp;amp;self.data&lt;/code&gt; eventhough in the closure forms we had &lt;code&gt;by_ref(&amp;amp;data)&lt;/code&gt; in both cases.&lt;/p&gt;
    &lt;p&gt;So we now hopefully understand what the &lt;code&gt;move&lt;/code&gt; keyword does but you might wonder why that can be useful? After all, the first table above shows that we only removed flexbility.&lt;/p&gt;
    &lt;p&gt;Spawning a thread:&lt;/p&gt;
    &lt;code&gt;let data = "by_ref".to_owned();
std::thread::spawn(|| by_ref(&amp;amp;data)).join().unwrap();
&lt;/code&gt;
    &lt;p&gt;Without &lt;code&gt;move&lt;/code&gt;, we get the following compiler error which helpfully suggest adding &lt;code&gt;move&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;error[E0373]: closure may outlive the current function, but it borrows `data`, which is owned by the current function
 --&amp;gt; src/main.rs:9:20
  |
9 | std::thread::spawn(|| by_ref(&amp;amp;data)).join().unwrap();
  |                    ^^         ---- `data` is borrowed here
  |                    |
  |                    may outlive borrowed value `data`
  |
note: function requires argument type to outlive `'static`
 --&amp;gt; src/main.rs:9:1
  |
9 | std::thread::spawn(|| by_ref(&amp;amp;data)).join().unwrap();
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
help: to force the closure to take ownership of `data` (and any other referenced variables), use the `move` keyword
  |
9 | std::thread::spawn(move || by_ref(&amp;amp;data)).join().unwrap();
  |                    ++++
&lt;/code&gt;
    &lt;p&gt;Creating a function returning a closure:&lt;/p&gt;
    &lt;code&gt;fn make_greeter(greeter: &amp;amp;str) -&amp;gt; impl Fn(&amp;amp;str) -&amp;gt; String {
    move |name| format!("{greeter} {name}")
}

let hello_greeter = make_greeter("Hello");
let hi_greeter = make_greeter("Hi");

assert_eq!(hello_greeter("rust"), "Hello rust");
assert_eq!(hi_greeter("rust"), "Hi rust");
&lt;/code&gt;
    &lt;p&gt;Here too we need &lt;code&gt;move&lt;/code&gt; otherwise we get the same borrow checker error.&lt;/p&gt;
    &lt;head rend="h1"&gt;Last word&lt;/head&gt;
    &lt;p&gt;
      &lt;del&gt;Sigh&lt;/del&gt;
    &lt;/p&gt;
    &lt;p&gt;This article is long enough as is, so I am stopping here for now. I plan to publish a follow up article for async closures later. If you want to read more on the subject I recommend:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The closure chapter in the rust book.&lt;/item&gt;
      &lt;item&gt;The closure chapter in the rust reference.&lt;/item&gt;
      &lt;item&gt;Finding Closure in Rust by Huon Wilson.&lt;/item&gt;
      &lt;item&gt;The article from the baby steps blog about adding an explicit capture clause.&lt;/item&gt;
      &lt;item&gt;The Rust Unstable book on fn_traits and unboxed_closures.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And to discuss this article, you can head over to the Hacker News thread or the reddit thread.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46746266</guid><pubDate>Sat, 24 Jan 2026 18:42:13 +0000</pubDate></item><item><title>BirdyChat becomes first European chat app that is interoperable with WhatsApp</title><link>https://www.birdy.chat/blog/first-to-interoperate-with-whatsapp</link><description>&lt;doc fingerprint="8e0f2119ee313c8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;BirdyChat becomes the first European chat app that is interoperable with WhatsApp&lt;/head&gt;
    &lt;p&gt;November 14, 2025&lt;/p&gt;
    &lt;p&gt;Today we are excited to share a big milestone. BirdyChat is now the first chat app in Europe that can exchange messages with WhatsApp under the Digital Markets Act. This brings us closer to our mission of giving work conversations a proper home.&lt;/p&gt;
    &lt;p&gt;WhatsApp is currently rolling out interoperability support across Europe. As this rollout continues, the feature will become fully available to both BirdyChat and WhatsApp users in the coming months.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why this matters&lt;/head&gt;
    &lt;p&gt;Until now, you could only message people who already had a BirdyChat account. If someone was not on the app, they had to download it before you could talk. It slowed down adoption and made it harder to move real work conversations into BirdyChat.&lt;/p&gt;
    &lt;p&gt;With the new WhatsApp interface mandated by the DMA, any BirdyChat user in the EEA will be able to start a chat with any WhatsApp user in the region simply by knowing their phone number. Your contacts keep using WhatsApp. You stay on BirdyChat. Messages flow both ways.&lt;/p&gt;
    &lt;p&gt;This removes a big barrier to adopting BirdyChat for work. You no longer need to ask people to switch apps. You can keep work neatly organised in BirdyChat while staying connected to everyone who still relies on WhatsApp.&lt;/p&gt;
    &lt;head rend="h2"&gt;What interoperability lets you do&lt;/head&gt;
    &lt;p&gt;Interoperability lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Start 1:1 chats with WhatsApp users using their phone number&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Send messages, photos and files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Communicate over an encrypted connection&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use your work email as your identity instead of a personal phone number&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes it much easier to keep work and personal life separate while staying fully reachable.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the integration works&lt;/head&gt;
    &lt;p&gt;WhatsApp introduced Third-Party Chats in Europe earlier this year. BirdyChat connects through this official DMA interface and does not use any workarounds. All communication between BirdyChat and WhatsApp users is end-to-end encrypted.&lt;/p&gt;
    &lt;p&gt;Currently, BirdyChat supports 1:1 chats, with group chat interoperability coming in a future update.&lt;/p&gt;
    &lt;head rend="h2"&gt;Availability&lt;/head&gt;
    &lt;p&gt;This feature will roll out gradually to BirdyChat users across the European Economic Area. For interoperability to work, both you and your WhatsApp contacts need to be based in the EEA. Since WhatsApp is releasing interoperability as part of a gradual rollout, availability may differ slightly from country to country.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get early access&lt;/head&gt;
    &lt;p&gt;BirdyChat is invite-only while we scale access. Join the waitlist with your work email to be among the first to try WhatsApp interoperability.&lt;/p&gt;
    &lt;p&gt;Productively yours,&lt;lb/&gt;Team BirdyChat&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46746476</guid><pubDate>Sat, 24 Jan 2026 19:04:08 +0000</pubDate></item><item><title>Agent orchestration for the timid</title><link>https://substack.com/inbox/post/185649875</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46746681</guid><pubDate>Sat, 24 Jan 2026 19:25:53 +0000</pubDate></item><item><title>Postmortem: Our first VLEO satellite mission (with imagery and flight data)</title><link>https://albedo.com/post/clarity-1-what-worked-and-where-we-go-next</link><description>&lt;doc fingerprint="ae1dcadd78b58dbb"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Clarity-1: What Worked, and Where We Go Next&lt;/head&gt;
    &lt;p&gt;On March 14, 2025, Albedo's first satellite, Clarity-1, launched on SpaceX Transporter-13. We took a big swing with our pathfinder. The mission goals:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove sustainable orbit operations in VLEO — an orbital regime long considered too harsh for commercial satellites — by overcoming thick atmospheric drag, dangerous atomic oxygen, and extreme speeds.&lt;/item&gt;
      &lt;item&gt;Prove our mid-size, high-performance Precision bus — designed and built in-house in just over two years.&lt;/item&gt;
      &lt;item&gt;Capture 10 cm resolution visible imagery and 2-meter thermal infrared imagery, a feat previously achieved only by exquisite, billion dollar government systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We proved a ton. We learned a ton.&lt;/p&gt;
    &lt;p&gt;We achieved the first two goals definitively and validated 98% of the technology required for the third. This was an extraordinarily ambitious first satellite. We designed and built a high-performance bus on time and on budget, integrated a large-aperture telescope, and operated in an environment no commercial company had sustained operations in, funded entirely by private capital.&lt;/p&gt;
    &lt;p&gt;This is the full story.&lt;/p&gt;
    &lt;head rend="h1"&gt;VLEO Works&lt;/head&gt;
    &lt;p&gt;Let's start with the result that matters most: VLEO works. And it works better than even we expected.&lt;/p&gt;
    &lt;p&gt;For decades, Very Low Earth Orbit was written off as impractical for normal satellite lifetimes. The atmosphere is thicker, creating drag that would deorbit normal satellites in weeks. If the drag didn't kill you, atomic oxygen would erode your solar arrays and surfaces. To succeed in VLEO required a fundamentally different satellite design.&lt;/p&gt;
    &lt;p&gt;Clarity-1 proved that our design works.&lt;/p&gt;
    &lt;p&gt;The drag coefficient was the headline: 12% better than our design target. Measured multiple times at altitudes between 350 km - 380 km with a repeatable result, this validates our models producing a satellite lifespan of five years at 275 km altitude, averaged across the solar cycle. This was one of our most critical assumptions, and we exceeded it.&lt;/p&gt;
    &lt;p&gt;Atomic oxygen (AO) is the silent killer in VLEO. The deeper you go, the more AO you encounter. It degrades solar arrays and other traditional satellite materials. We developed a new class of solar arrays with unique measures designed to mitigate AO degradation. They work. Even as we descended deeper into VLEO and AO fluence increased logarithmically, our power generation stayed constant. The solar arrays are holding up as designed.&lt;/p&gt;
    &lt;p&gt;Clarity-1 demonstrated over 100 km of controlled altitude descent, stationkeeping in VLEO, and survived a solar storm that temporarily spiked atmospheric density — the impact on Clarity's descent rate was barely noticeable. Momentum management worked. Fault detection worked. Our thrust planning model was validated against GOCE data (a 2009 VLEO R&amp;amp;D mission) with sub-meter accuracy. Radiation tolerance was excellent, with 4x fewer single-event upsets than expected. Orbit determination was dialed.&lt;/p&gt;
    &lt;p&gt;We proved sustainable VLEO operations.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Precision Bus is Flight-Proven&lt;/head&gt;
    &lt;p&gt;Developed and built in just over two years, our in-house bus Precision is now TRL-9: flight-proven on-orbit.&lt;/p&gt;
    &lt;p&gt;Every bus subsystem worked. Every piece of in-house technology we developed performed: our CMG steering law, our operational modes, flight and ground software, electronics boards, and our novel thermal management system. We hit our embedded software GNC timing deadlines, we converged our attitude and orbit determination estimators, we saw 4π steradian command and telemetry antenna coverage, and we got on-orbit actuals for our power generation and loads.&lt;/p&gt;
    &lt;p&gt;Our cloud-native ground system was incredible. Contact planning across 25 ground stations was completely automated. Mission scheduling updated every 15 minutes to incorporate new tasking and the latest satellite state information, smoothly transitioning to updated on-board command loads with visual tracking of each schedule and its status. Automated thrust planning to achieve our desired orbital trajectory supported 30+ maneuvers per day. Our engineers could track and command the satellite from anywhere with internet and a secure VPN.&lt;/p&gt;
    &lt;p&gt;We pushed 14 successful flight software feature updates on-orbit — and even executed one FPGA update, which is exceptionally rare. The ability to continuously improve throughout Clarity's operational life proved essential — every major solution to challenges we faced involved flight software updates. On-orbit software upgrades are exceedingly tricky to get right, but Clarity-1 was designed from day one around this foundational capability.&lt;/p&gt;
    &lt;head rend="h1"&gt;Four Weeks of Perfection&lt;/head&gt;
    &lt;p&gt;The first month of the mission was magic.&lt;/p&gt;
    &lt;p&gt;An hour after launch, we watched Clarity-1 deploy from the premium caketopper slot into LEO, giving us an incredible view of the Nile River as she separated from the rocket.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;First contact came just three hours later at 5:11am MT. Imagine sitting in Mission Control, watching two ground station passes with no data, then on the third: heaps of green, healthy telemetry streaming into all of the subsystem dashboards. Clarity had nailed her autonomous boot-up sequence and rocket separation rate capture. Stuck the landing.&lt;/p&gt;
    &lt;p&gt;The next milestone — and the one many of us were most anxious about — was our autonomous Protect Mode, basically our VLEO version of Safe Mode.&lt;/p&gt;
    &lt;p&gt;We estimated a week.&lt;/p&gt;
    &lt;p&gt;We nailed it 14 hours after launch.&lt;/p&gt;
    &lt;p&gt;By 6:45pm that same day, Clarity was in Operational mode, ready for commissioning.&lt;/p&gt;
    &lt;p/&gt;
    &lt;quote&gt;"Gotta say it: the last 16 hours have been incredible. I started my shift last night hoping to see one bit of data. I wouldn't have believed it if someone told me we'd be in Protect within 14 hours from launch."— Albedo GNC Engineer&lt;/quote&gt;
    &lt;p/&gt;
    &lt;p&gt;The days that followed were a blur of checkboxes turning green. 4-CMG commissioning complete. Payload power-on and checkout validated. Thermal balance for both visible and thermal sensors confirmed. Our first on-orbit software update went flawlessly.&lt;/p&gt;
    &lt;p&gt;Clarity uses Control Moment Gyroscopes (CMGs) to steer the satellite, giving us more agility than more commonly used reaction wheels. We moved onto validating GNC modes such as GroundTrack, which we use to point at communication ground terminals.&lt;/p&gt;
    &lt;p&gt;We moved on to commissioning our X-band radio — the high-rate link to downlink imagery. After we uncovered an issue with our ground station provider’s pointing mode, the 800 Mbps link began pumping down data on every pass. The waveforms were clean. Textbook. A direct representation of how locked in our precision CMG pointing was.&lt;/p&gt;
    &lt;p&gt;With our first satellite at this level of complexity, we couldn't believe how smoothly it had gone. Years of developing new technologies had been validated in a fraction of the commissioning time we'd anticipated.&lt;/p&gt;
    &lt;head rend="h1"&gt;Maneuvering over 100 km to VLEO&lt;/head&gt;
    &lt;p&gt;Next up was maneuvering from our LEO drop-off altitude down to VLEO, where it would be safe to eject the telescope contamination cover and start snapping pictures.&lt;/p&gt;
    &lt;p&gt;Then came April 14.&lt;/p&gt;
    &lt;p&gt;One of our four CMGs experienced a temperature spike in the flywheel bearing. Our Fault Detection, Isolation, and Recovery (FDIR) logic caught it immediately, spun it down, and executed automated recovery actions. But it wouldn't spin back up. Manual recovery attempts followed. Also unsuccessful.&lt;/p&gt;
    &lt;p&gt;Rushing back into CMG operations without understanding the failure mechanism risked killing the mission entirely, so we turned off the other three and put the satellite in two-axis stabilization using the magnetic torque rods.&lt;/p&gt;
    &lt;p&gt;We had a choice. Hack together novel 3-CMG control algorithms as fast as possible and risk losing another, or figure out how to leverage only the torque rods to achieve 3-axis control with sufficient accuracy to navigate the maneuver to VLEO.&lt;/p&gt;
    &lt;p&gt;We went with the torque rods.&lt;/p&gt;
    &lt;p&gt;On satellites this size (~600 kg), magnetic torque rods are typically used for momentum dumping, not attitude control. But we'd built Clarity with unusually beefy torque rods due to the elevated momentum management needs in VLEO. Our GNC team went heads down and developed algorithms to achieve 3-axis attitude control using only torque rods.&lt;/p&gt;
    &lt;p&gt;Within a month, we had it working.&lt;/p&gt;
    &lt;p&gt;Both of our electric thrusters commissioned quickly and were working well. But with torque rods only, our attitude control had 15 to 20 degrees of error, sometimes reaching ~45 degrees. And maneuvering to VLEO isn’t “point into the wind and fire” — it’s continuous vector and trajectory management across an orbit. That kind of control error meant inefficient burns and a much harder descent plan.&lt;/p&gt;
    &lt;p&gt;As the descent progressed, however, the team learned and iterated. With more iteration and flight software updates, we uploaded onboard logic informed by several sources of live data that dialed in our thrust vector control to within 5 degrees of the target. The autonomous thrust planning system we built enabled us to claw back performance that nearly matched our originally projected descent speed.&lt;/p&gt;
    &lt;p&gt;We maneuvered safely past the ISS and entered VLEO. Eager to pop off the contamination cover.&lt;/p&gt;
    &lt;head rend="h1"&gt;Lens Cap Jettison&lt;/head&gt;
    &lt;p&gt;Once we reached safe altitude, it was time to jettison the contamination cover protecting our telescope.&lt;/p&gt;
    &lt;p&gt;There are horror stories about contamination covers getting stuck after months of temperature fluctuations.&lt;/p&gt;
    &lt;p&gt;Clarity's was flawless. I'll never forget seeing this blip in telemetry live — confirming through Newton's third law that the jettison was successful. Shortly after, LeoLabs confirmed tracking of two separate objects.&lt;/p&gt;
    &lt;p&gt;We were ready to start imaging.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Imaging Journey&lt;/head&gt;
    &lt;p&gt;Here's where it got complicated.&lt;/p&gt;
    &lt;p&gt;Our GNC and FSW teams were close but not yet finished with the new 3-CMG control law. CMGs are rarely used in commercial space, let alone by a startup. Then take one more step: singularity-prone 3-CMG control that to our knowledge has not been attempted on a non-exquisite satellite, and certainly not developed and uploaded on-orbit. Traditional algorithms require at least four CMGs to provide capability volumes free of singularities.&lt;/p&gt;
    &lt;p&gt;We were eager to make some amount of progress, so we started imaging on torque rods even though there would be severe limitations: 50+ pixels of smear, large mispointing from the wobble of torque rod control due to earth's magnetic field, and downlink limited to at best two small images per day. The last two constraints meant we were at risk of spending precious downlink capacity on clouds.&lt;/p&gt;
    &lt;p&gt;Sure enough, the first two days of pixels were mostly clouds, but we were happy to peek through a little in this image.&lt;/p&gt;
    &lt;p&gt;Although we couldn't control attitude accurately, we did still have good attitude knowledge after the fact. AyJay whipped up a clever idea with Claude Code that automated posting weather conditions in Slack for each collection. We analyzed that to determine which images were likely clear, and selected those for downlink.&lt;/p&gt;
    &lt;p&gt;Boom:&lt;/p&gt;
    &lt;p&gt;We adjusted the focus position a few times, and images continued getting better.&lt;/p&gt;
    &lt;p&gt;Then, 3-CMG control was ready.&lt;/p&gt;
    &lt;p&gt;Out of the box, the new algorithms and software performed perfectly.&lt;/p&gt;
    &lt;p&gt;This visualization shows real telemetry of Clarity performing seven back-to-back imaging maneuvers, with limited 3-CMG agility, followed by an X-band downlink over Iceland minutes later. The satellite was executing sophisticated attitude profiles with very low control error. Fiber-optic gyro measurements showed exquisite jitter performance.&lt;/p&gt;
    &lt;p&gt;In real time, collecting and downlinking those seven images took ten minutes.&lt;/p&gt;
    &lt;p&gt;And this is where our ground software really showed its teeth. On most missions, “data on the ground” is just the start — turning raw bits into something viewable is a slow chain of handoffs and batch processing. For us, within seconds of the downlink finishing, the image product pipeline was already posting processed snippets into our company Slack. Literally seconds.&lt;/p&gt;
    &lt;p&gt;That end-to-end loop — photons in orbit to a viewable product on the ground, within minutes — is a capability that’s still rare in this industry.&lt;/p&gt;
    &lt;p&gt;As expected with smear reduced, image quality improved immediately.&lt;/p&gt;
    &lt;p&gt;We were ready to execute focus calibration.&lt;/p&gt;
    &lt;p&gt;Large telescope optics experience hygroscopic dryout during the first few months on-orbit — moisture trapped in materials during ground assembly slowly releases in the vacuum of space, causing the focus position to drift. Dialing in best focus requires dozens of iterations: capture images, analyze sharpness, adjust focus position, repeat. Each cycle gets you closer to the optical performance the system was designed for, and our telescope’s on-ground alignment was verified to spec.&lt;/p&gt;
    &lt;p&gt;We continued to iterate.&lt;/p&gt;
    &lt;p&gt;After a few iterations of this, we could start to see cars.&lt;/p&gt;
    &lt;p&gt;Even this early into imaging, the infrared images blew us away. Using a low-cost microbolometer — a fraction of the price of cooled IR sensors — we captured thermal signatures that showed ships in Tokyo Bay, steel processing facilities where we could distinguish individual coke ovens from their smokestacks, and distinct signatures between real vegetation and turf — a good proxy for camouflage detection. Day or night, clear as day.&lt;/p&gt;
    &lt;p&gt;Three days into the excitement, CMG problems started again.&lt;/p&gt;
    &lt;p&gt;A second CMG began showing the same telemetry signatures we now recognized as warning signs.&lt;/p&gt;
    &lt;p&gt;What we had learned from the investigation: the allowable temperature specifications of the CMGs were much higher than the true limit, constrained by what the lubricant inside the flywheel could handle. A straightforward fix for the future — an unfortunate corner case to learn about in hindsight.&lt;/p&gt;
    &lt;p&gt;The second CMG showing issues was also on the hot side of the satellite. While we had overhauled the vehicle and CMG operations to prevent additional bearing wear, the damage had already been done in the first month of the mission.&lt;/p&gt;
    &lt;p&gt;We spent months trying everything we could to get the CMGs to operate sustainably. The team attempted many clever solutions, one of which revived the first CMG that had locked up. We uploaded a feature to select any 3 of the 4 CMGs for operator commanding. But we weren't able to get sustained, reliable operation.&lt;/p&gt;
    &lt;p&gt;Despite the CMG challenges, here's what the imaging journey proved.&lt;/p&gt;
    &lt;p&gt;The full end-to-end image chain works. Photons hit our optics, get captured by our sensor, processed through payload electronics, packetized and encrypted, transmitted via our X-band radio, received on the ground, and processed into image products. The entire chain is validated.&lt;/p&gt;
    &lt;p&gt;The end-to-end loop is fast. Within 30 seconds of a downlink, processed image snippets were already posting to our company Slack.&lt;/p&gt;
    &lt;p&gt;Sensor performance exceeded expectations. Dynamic range, radiometry, color balance, band-to-band alignment — all look great, even on uncalibrated imagery.&lt;/p&gt;
    &lt;p&gt;We can scan out long images. Our line-scanning approach produced strips 20-30 kilometers long, exactly as designed.&lt;/p&gt;
    &lt;p&gt;Pointing accuracy and high quality telemetry validates the ingredients for precise geolocation. The data we need to pinpoint where each pixel lands on Earth to &amp;lt;5m (closed-loop CE90) is there.&lt;/p&gt;
    &lt;p&gt;Jitter and smear are low. Fiber-optic gyro measurements confirmed 3x lower smear and 11x lower jitter compared to our goal — a critical ingredient for exquisite imagery.&lt;/p&gt;
    &lt;p&gt;Our proprietary image scheduler works. The automated system that plans collections, manages constraints, and optimizes what we capture each day performed as designed.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where Clarity Is Now&lt;/head&gt;
    &lt;p&gt;Nine months into the mission, we lost contact with Clarity-1.&lt;/p&gt;
    &lt;p&gt;By that point, we had largely exhausted our options on the CMGs. The path to further image quality improvement had effectively closed.&lt;/p&gt;
    &lt;p&gt;We had been tracking intermittent memory issues in our TT&amp;amp;C radio throughout the mission, working around them as they appeared. Our best theory is that one of these issues escalated in a way that corrupted onboard memory and is preventing reboots. We've tried several recovery approaches. So far, none have worked, and the likelihood of recovery looks low at this point.&lt;/p&gt;
    &lt;p&gt;But here's what matters: the VLEO validation data we collected is sufficient.&lt;/p&gt;
    &lt;p&gt;We combined a state-of-the-art atmospheric density model, our high-fidelity orbital dynamics force models, and months of natural orbit decay data from 350 to 380 km altitude to determine Clarity’s coefficient of drag — with repeatable results at different altitudes. That drag coefficient, paired with our demonstrated ability to maintain altitude in VLEO for months using high-efficiency thrusters, tells us exactly how the vehicle behaves under aerodynamic drag across the VLEO regime — and validates an average five-year lifespan at 275 km across the solar cycle. Telemetry from our solar arrays, together with onboard atomic oxygen sensor data, shows peak power generation stayed constant after exposure to VLEO levels of AO fluence — proving our AO mitigation worked.&lt;/p&gt;
    &lt;p&gt;Thanks to our friends at LeoLabs, we've validated that Clarity is maintaining attitude autonomously. She's still up there, still oriented, still descending through VLEO. Just not talking to us.&lt;/p&gt;
    &lt;p&gt;Even before this, we had started developing an in-house TT&amp;amp;C radio for our systems moving forward, rather than reusing this radio that was procured from a third party. We’ll incorporate learnings from this reliability issue into that.&lt;/p&gt;
    &lt;p&gt;We're still working the problem. This chapter isn't over yet. But even if it is, Clarity-1 gave us what we needed to build what comes next.&lt;/p&gt;
    &lt;head rend="h1"&gt;98% of The 10 cm Imagery Pyramid&lt;/head&gt;
    &lt;p&gt;If you think about exquisite imagery as a pyramid, we needed 100% of the systems working together to achieve the pinnacle: 10 cm visible imagery. We got to about 98%. Everything else in that pyramid — the entire foundation — is proven and retired.&lt;/p&gt;
    &lt;p&gt;Our drag coefficient. Our atomic oxygen resilience. Our solar arrays. Our thermal management. Our flight software. Our ground software. Our CMG steering laws. Our precision pointing algorithms. Our payload electronics. Our sensor performance. Our image processing chain. Our ability to operate sustainably in VLEO. Our team.&lt;/p&gt;
    &lt;p&gt;All validated.&lt;/p&gt;
    &lt;p&gt;We know exactly what to fix. It’s straight forward: operate the CMGs at lower temperature. The system thermal design is already updated in the next build to maximize CMG life going forward.&lt;/p&gt;
    &lt;p&gt;Beyond the CMGs, there were a handful of learnings on the margins. We learned our secondary mirror structure could be stiffer — already in the updated design. We learned we could use more heater capacity in some payload zones — already fixed.&lt;/p&gt;
    &lt;p&gt;We learned from the things that worked, too. We're well down the development path for next-gen flight software, avionics, and power distribution. Orbit determination and geolocation will be even better. Additional surface treatments will improve drag coefficient further. Power-generation will increase while maintaining the proven atomic oxygen resilience. The list goes on.&lt;/p&gt;
    &lt;p&gt;The path to exquisite imagery is clear. And that’s only one of many exciting capabilities unlocked by sustainable operations in VLEO.&lt;/p&gt;
    &lt;head rend="h1"&gt;What’s Next&lt;/head&gt;
    &lt;p&gt;Our next VLEO mission will incorporate these learnings and demonstrate new features that enable missions beyond imaging — we’ll share more details soon. In parallel, imaging remains a core focus: we’re continuing to build optical payloads for EO/IR missions as part of a broader VLEO roadmap.&lt;/p&gt;
    &lt;p&gt;The successes of Clarity-1 reinforced our core conviction: VLEO isn’t just a better orbit for imaging — it’s the next productive orbital layer.&lt;/p&gt;
    &lt;p&gt;The physics are unforgiving, but that’s exactly why it matters. Go lower and you unlock a step-change in performance: sharper sensing, faster links, lower latency, and a new level of responsiveness. The reason VLEO has been written off for decades isn’t lack of upside — it’s that most satellites simply can’t survive there long enough to matter.&lt;/p&gt;
    &lt;p&gt;Now we know they can.&lt;/p&gt;
    &lt;p&gt;Clarity proved the hard parts: sustainable VLEO operations, validated drag and lifetime models, atomic oxygen resilience, and a flight-proven high-performance bus. We’re not speculating about VLEO. We’re operating in it, learning in it, and capitalized to scale it.&lt;/p&gt;
    &lt;p&gt;Onward,&lt;/p&gt;
    &lt;p&gt;Topher &amp;amp; Team Albedo&lt;/p&gt;
    &lt;p/&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46747119</guid><pubDate>Sat, 24 Jan 2026 20:03:39 +0000</pubDate></item><item><title>I added a Bluesky comment section to my blog</title><link>https://micahcantor.com/blog/bluesky-comment-section.html</link><description>&lt;doc fingerprint="bc4caf037f2e3bf8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;I added a Bluesky comment section to my blog&lt;/head&gt;Published on&lt;p&gt;You can now view replies to this blog post made on Bluesky directly on this website. Check it out here!&lt;/p&gt;&lt;p&gt;I've always wanted to host a comment section on my site, but it's difficult because the content is statically generated and hosted on a CDN. I could host comments on a separate VPS or cloud service. But maintaining a dynamic web service like this can be expensive and time-consuming — in general, I'm not interested in being an unpaid, part-time DevOps engineer.&lt;/p&gt;&lt;p&gt;Recently, however, I read a blog post by Cory Zue about how he embedded a comment section from Bluesky on his blog. I immediately understood to benefits of this approach. With this approach, Bluesky could handle all of the difficult work involved in managing a social media like account verification, hosting, storage, spam, and moderation. Meanwhile because Bluesky is an open platform with a public API, it's easy to directly embed comments on my own site.&lt;/p&gt;&lt;p&gt;There are other services that could be used for this purpose instead. Notably, I could embed replies from the social media formerly known as Twitter. Or I could use a platform like Disqus or even giscus, which hosts comments on GitHub Discussions. But I see Bluesky as a clearly superior choice among these options. For one, Bluesky is built on top of an open social media platform in AT Proto, meaning it can't easily be taken over by an authoritarian billionaire creep. Moreover, Bluesky is a full-fledged social media platform, which naturally makes it a better option for hosting a conversation than GitHub.&lt;/p&gt;&lt;p&gt;Zue published a standalone package called bluesky-comments that allows embedding comments in a React component as he did. But I decided to build this feature myself instead. Mainly this is because I wanted to make a few styling changes anyway to match the rest of my site. But I also wanted to leave the option open to adding more features in the future, which would be easier to do if I wrote the code myself. The entire implementation is small regardless, amounting to only ~200 LOC between the UI components and API functions.&lt;/p&gt;&lt;p&gt;Initially, I planned to allow people to directly post on Bluesky via my site. This would work by providing an OAuth flow that gives my site permission to post on Bluesky on behalf of the user. I actually did get the auth flow working, but building out a UI for posting and replying to existing comments is difficult to do well. Going down this path quickly leads to building what is essentially a custom Bluesky client, which I didn't have the time or interest in doing right now. Moreover, because the user needs to go through the auth flow and sign-in to their Bluesky account, the process is not really much easier than posting directly on a linked Bluesky post.&lt;/p&gt;&lt;p&gt;Without the requirement of allowing others to directly post on my site, the implementation became much simpler. Essentially, my task was to specify a Bluesky post that corresponds to the article in the site's metadata. Then, when the page loads I fetch the replies to that post from Bluesky, parse the response, and display the results in a simple comment section UI.&lt;/p&gt;&lt;p&gt;As explained in my last post, this site is built using React Server Components and Parcel. The content of my articles are written using MDX, an extension to Markdown that allows directly embedding JavaScript and JSX. In each post, I export a &lt;code&gt;metadata&lt;/code&gt; object that I validate using a Zod schema. For instance, the metadata for this post looks like this:&lt;/p&gt;&lt;p&gt;The value of &lt;code&gt;bskyPostId&lt;/code&gt; references the Bluesky post from which I'll pull replies to display in the comment section. Because my project is built in TypeScript, it was easy to integrate with the Bluesky TypeScript SDK (&lt;code&gt;@bluesky/api&lt;/code&gt; on NPM). Reading the Bluesky API documentation and Zue's implementation led me to the &lt;code&gt;getPostThread&lt;/code&gt; endpoint. Given an AT Protocol URI, this endpoint returns an object with data on the given post and its replies.&lt;/p&gt;&lt;p&gt;I could have interacted directly with the Bluesky API from my React component using &lt;code&gt;fetch&lt;/code&gt; and &lt;code&gt;useEffect&lt;/code&gt;. However, it can be a bit tricky to correctly handle loading and a error states, even for a simple feature like this. Because of this, I decided to use the Tanstack &lt;code&gt;react-query&lt;/code&gt; package to manage the API request/response cycle. This library takes care of the messy work of handling errors, retries, and loading states while I simply provide it a function to fetch the post data.&lt;/p&gt;&lt;p&gt;Once I obtain the Bluesky response, the next task is parsing out the content and metadata for the replies. Bluesky supports a rich content structure in its posts for representing markup, references, and attachments. Building out a UI that fully respects this rich content would be difficult. Instead, I decided to keep it simple by just pulling out the text content from each reply.&lt;/p&gt;&lt;p&gt;Even so, building a UI that properly displays threaded comments, particularly one that is formatted well on small mobile devices, can be tricky. For now, my approach was to again keep it simple. I indented each reply and added a left border to make it easier to follow reply threads. Otherwise, I mostly copied design elements for layout of the profile picture and post date from Bluesky.&lt;/p&gt;&lt;p&gt;Lastly, I added a UI component linking to the parent post on Bluesky, and encouraging people to add to the conversation there. With this, the read-only comment section implementation was complete. If there's interest, I could publish my version of Bluesky comments as a standalone package. But several of the choices I made were relatively specific to my own site. Moreover, the implementation is simple enough that others could probably build their own version from reading the source code, just as I did using Zue's version.&lt;/p&gt;&lt;p&gt;Let me know what you think by replying on Bluesky. Hopefully this can help increase engagement with my blog posts, but then again, my last article generated no replies, so maybe not 😭.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46747366</guid><pubDate>Sat, 24 Jan 2026 20:33:40 +0000</pubDate></item><item><title>First Design Engineer Hire – Build Games at Gym Class (YC W22)</title><link>https://www.ycombinator.com/companies/gym-class-by-irl-studios/jobs/ywXHGBv-design-engineer-senior-staff-principal</link><description>&lt;doc fingerprint="29966ae1094f87a6"&gt;
  &lt;main&gt;
    &lt;p&gt;Connecting the world by simulating it&lt;/p&gt;
    &lt;p&gt;Gym Class is a top rated social game on Meta Quest - millions of downloads, 79,000+ reviews, and a 4.9-star rating. We’re hiring our founding Design Engineer to drive the development of our upcoming mobile web app (embedded in native), and web surfaces inside our flagship, social VR experience.&lt;/p&gt;
    &lt;p&gt;You’ll own key web surfaces end-to-end - crafting in Figma, then building responsive, production-grade UI with React/Node/CSS - and you’ll set a clear quality bar for speed, polish, and accessibility. If you love living at the intersection of consumer design and front-end engineering, owning a high-compact roadmap for a startup, and shipping to a highly engaged social audience - this role is for you.&lt;/p&gt;
    &lt;p&gt;WHAT YOU'LL DO&lt;/p&gt;
    &lt;p&gt;QUALIFICATIONS - you are…&lt;/p&gt;
    &lt;p&gt;Salary ranges may be inclusive of several career levels and will be narrowed during the interview process based on a number of factors, including the candidate’s experience, qualifications, and location. Additional benefits for this role include: equity; and medical, dental, and vision benefits; and 401k retirement plan with matching.&lt;/p&gt;
    &lt;p&gt;Gym Class is a top-rated social VR game on Meta Quest with over +79,000 reviews and a 4.9-star rating. Millions of players join to connect with friends, explore social worlds, and engage in competitive games. The community launched around basketball, and is now expanding rapidly into more categories.&lt;/p&gt;
    &lt;p&gt;The company's mission is to connect the world by simulating it. To achieve this, Gym Class recently raised over $8M from the world's top technology investors, including Andreessen Horowitz (a16z), Y Combinator, the National Basketball Association, the Golden State Warriors, top NBA players like Kevin Durant, Lonzo Ball, Andre Iguodala, Soma Capital, Founders Inc., Danny Green, Zaza Pachulia, Todd and Rahul's Angel Fund, Balaji Srinivasan, and more. In 2023, the company announced a licensing relationship with the NBA, and in 2025, its licensing relationship with the MLB.&lt;/p&gt;
    &lt;p&gt;Learn more at gymclass.com or follow @gymclassvr.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46747625</guid><pubDate>Sat, 24 Jan 2026 21:01:02 +0000</pubDate></item><item><title>Poland's energy grid was targeted by never-before-seen wiper malware</title><link>https://arstechnica.com/security/2026/01/wiper-malware-targeted-poland-energy-grid-but-failed-to-knock-out-electricity/</link><description>&lt;doc fingerprint="bd763819a871cdd2"&gt;
  &lt;main&gt;
    &lt;p&gt;Researchers on Friday said that Poland’s electric grid was targeted by wiper malware, likely unleashed by Russia state hackers, in an attempt to disrupt electricity delivery operations.&lt;/p&gt;
    &lt;p&gt;A cyberattack, Reuters reported, occurred during the last week of December. The news organization said it was aimed at disrupting communications between renewable installations and the power distribution operators but failed for reasons not explained.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wipers R Us&lt;/head&gt;
    &lt;p&gt;On Friday, security firm ESET said the malware responsible was a wiper, a type of malware that permanently erases code and data stored on servers with the goal of destroying operations completely. After studying the tactics, techniques, and procedures (TTPs) used in the attack, company researchers said the wiper was likely the work of a Russian government hacker group tracked under the name Sandworm.&lt;/p&gt;
    &lt;p&gt;“Based on our analysis of the malware and associated TTPs, we attribute the attack to the Russia-aligned Sandworm APT with medium confidence due to a strong overlap with numerous previous Sandworm wiper activity we analyzed,” said ESET researchers. “We’re not aware of any successful disruption occurring as a result of this attack.”&lt;/p&gt;
    &lt;p&gt;Sandworm has a long history of destructive attacks waged on behalf of the Kremlin and aimed at adversaries. Most notable was one in Ukraine in December 2015. It left roughly 230,000 people without electricity for about six hours during one of the coldest months of the year. The hackers used general purpose malware known as BlackEnergy to penetrate power companies’ supervisory control and data acquisition systems and, from there, activate legitimate functionality to stop electricity distribution. The incident was the first known malware-facilitated blackout.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46747827</guid><pubDate>Sat, 24 Jan 2026 21:24:13 +0000</pubDate></item><item><title>We X-Rayed a Suspicious FTDI USB Cable</title><link>https://eclypsium.com/blog/xray-counterfeit-usb-cable/</link><description>&lt;doc fingerprint="ad38505b1b402ed0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We X-Rayed A Suspicious FTDI USB Cable&lt;/head&gt;
    &lt;p&gt;We recently got an industrial X-Ray machine in the Eclypsium office to use to &lt;del&gt;make the next Doctor Manhattan&lt;/del&gt; do serious cybersecurity research. In between X-raying yet-to-be released industrial IT technologies on behalf of giant companies whose names we cannot reveal, we have done some other fun experiments.&lt;/p&gt;
    &lt;p&gt;One thing we’ve done with it so far was to x-ray some FTDI USB to UART cables. We had an old cable lying around that seemed a little suspicious and dysfunctional. It worked at slow speeds but it failed when transferring firmware images from a product. These failures drove us to purchase the known good cables from DigiKey, which worked as expected. It is possible that this older cable came from a factory which also produced older generations of authentic FTDI cables, but this particular chip didn’t meet the performance requirements for the FTDI brand. Or maybe it was just a production run based on stolen FTDI IP. Or it is actually completely unrelated to any FTDI IC but has been programmed to claim to be FTDI in software. Unless we could match the silicon exactly to a known supply chain, we can really only speculate.&lt;/p&gt;
    &lt;p&gt;In either case, we wanted to see the difference between the suspicious cable and a newer, more obviously “legit” one that cost about $20 from DigiKey. It is not a stretch to assume that a suspicious looking cable is a counterfeit. FTDI has publicly announced issues with counterfeit devices. They have even fought back with drivers which brick counterfeit chips. Some people have even referred to this as vendor sanctioned malware.&lt;/p&gt;
    &lt;p&gt;Here’s what the two cables look like to the naked eye:&lt;/p&gt;
    &lt;p&gt;Take a look at the two x-ray images below and see if you can tell which one is suspicious, and which one is authentic. Then scroll down and we’ll tell you what we see.&lt;/p&gt;
    &lt;p&gt;Before we tell you the answer, here are some clues to look out for in each picture. The authentic cable has the following features visible in the X-Ray image, not shared with the suspicious cable:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ground pours (reduces impedance and ground loops while improving EMI resistance and thermal dissipation). While there is some debate about the actual value of copper ground pours, they are still used by reputable manufacturers.&lt;/item&gt;
      &lt;item&gt;Ground stapling&lt;/item&gt;
      &lt;item&gt;Decoupling passives nearer to the main integrated circuit (IC)&lt;/item&gt;
      &lt;item&gt;More isolation passives for USB data pins&lt;/item&gt;
      &lt;item&gt;Thermal pad under IC&lt;/item&gt;
      &lt;item&gt;Engineered strain relief for wire connections&lt;/item&gt;
      &lt;item&gt;More solder for mechanical tabs on USB A connector&lt;/item&gt;
      &lt;item&gt;Smaller/newer silicon process&lt;/item&gt;
      &lt;item&gt;Better passive alignment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Big Reveal, and the Implications for Supply Chain Security&lt;/head&gt;
    &lt;p&gt;OK, the top image above is the authentic cable. The bottom image is the more questionable one.&lt;/p&gt;
    &lt;p&gt;Did you get it right? If not, go back and see if you can pinpoint the various clues.&lt;/p&gt;
    &lt;p&gt;The point is that, even when you know what to look for, spotting a counterfeit isn’t necessarily easy. The consequences for a consumer buying a shady USB cable likely aren’t too bad. But what happens when an enterprise gets counterfeit network gear with a backdoor pre-installed? Or when a major bank receives grey market servers with another company’s data on them? Eclypsium has helped major worldwide organizations discover exactly these types of supply chain issues.&lt;/p&gt;
    &lt;p&gt;Supply chain risk is growing rapidly. As AI data center projects capture more and more of the global supply for chips, memory, storage, and other key resources, the secondary market for all of these is heating up. The speed and complexity of these supply chains leaves gaps that cyber adversaries can exploit to introduce vulnerable components and backdoors into tech that makes its way into critical infrastructure.&lt;/p&gt;
    &lt;p&gt;To learn more, grab our white paper on Why Supply Chain Security Demands Focus on Hardware&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46749053</guid><pubDate>Sat, 24 Jan 2026 23:55:10 +0000</pubDate></item><item><title>Adoption of EVs tied to real-world reductions in air pollution: study</title><link>https://keck.usc.edu/news/adoption-of-electric-vehicles-tied-to-real-world-reductions-in-air-pollution-study-finds/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46749198</guid><pubDate>Sun, 25 Jan 2026 00:14:40 +0000</pubDate></item><item><title>Two Weeks Until Tapeout</title><link>https://essenceia.github.io/projects/two_weeks_until_tapeout/</link><description>&lt;doc fingerprint="37b09448082b2888"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Living under rocks#&lt;/head&gt;
    &lt;p&gt;As anyone that hasn’t been living under a rock might have heard, AI accelerators are the coolest kids in town these days. And although I have never been part of the “in” crowd, this time at least, I get the appeal.&lt;/p&gt;
    &lt;p&gt;So when the opportunity arose to join an experimental shuttle using global foundries 180nm for FREE I jumped onto the opportunity and designed my own JTAG!&lt;/p&gt;
    &lt;p&gt;…&lt;/p&gt;
    &lt;p&gt;I’m sorry? Is this not what you were expecting?&lt;/p&gt;
    &lt;p&gt;Frankly, I would love to tell you a great story about how I went into this wanting to design a free and open source silicon proven AI accelerator that the community could freely extend and re-use in their own projects. But in truth this project started out first as me wanting to design some far less sexy, in-silicon debug infrastructure and only later came to include the systolic matrix matrix multiplication accelerator … to serve as the design under test.&lt;/p&gt;
    &lt;p&gt;No wonder I was never one of the cool kids.&lt;/p&gt;
    &lt;p&gt;Also, I’m designing everything from scratch and have only two weeks left, welcome to the crunch.&lt;/p&gt;
    &lt;head rend="h3"&gt;Experimental shuttle#&lt;/head&gt;
    &lt;p&gt;Once again this tapeout was done as part of a Tiny Tapeout shuttle, but this time, it went out as not part of a public but an experimental shuttle.&lt;/p&gt;
    &lt;p&gt;These experimental shuttles are used as testing grounds for new nodes and flows in order to iron out issues before they are opened to the public.&lt;/p&gt;
    &lt;p&gt;Participation in these experimental shuttles is commonly reserved to contributors having previously submitted to Tiny Tapeout.&lt;/p&gt;
    &lt;p&gt;This limitation was set in place in order to help select for veteran designers as these experimental tapeout typically have less stable tooling and the final chip doesn’t feature the same level of inter design isolation as in a public tapeout.&lt;/p&gt;
    &lt;p&gt;Contributions to these shuttles are done with the understanding that the resulting chip might not be functional for some reason outside of the designers control.&lt;/p&gt;
    &lt;p&gt;Given these limitations, the Tiny Tapeout program is generously making submissions to these shuttles free of charge.&lt;/p&gt;
    &lt;p&gt;In practice, this makes area effectively free, explaining the higher occurrence of absolutely massive designs being submitted.&lt;/p&gt;
    &lt;p&gt;If the final chip is deemed sufficiently functional, the resulting ASICs along with the dev board will be available for purchase at the Tiny Tapeout store.&lt;/p&gt;
    &lt;p&gt;Given you need to have taped out a chip with Tiny Tapeout before to be eligible, I became eligible 14 days before the experimental shuttle submission deadline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Combo#&lt;/head&gt;
    &lt;p&gt;Before we start, let us acknowledge what I am attempting to do, alone.&lt;lb/&gt;If this were any normal corporate setting, it would be the deadline equivalent to a one way ticket straight to the ever-lengthening queue outside the gates of Hell.&lt;lb/&gt;Do not try this at work!&lt;/p&gt;
    &lt;p&gt;Now that safety precautions are out of the way:&lt;/p&gt;
    &lt;p&gt;Welcome to a tale of two designs.&lt;/p&gt;
    &lt;p&gt;The first, is the systolic array, typically found at the heart of any AI inference accelerator, its function is to perform matrix-matrix multiplications.&lt;/p&gt;
    &lt;p&gt;The other is our silicon debug infrastructure, namely the all ubiquitous JTAG TAP component. Its goal is to provide something to latch onto when my ASIC comes back as an expensive brick and I am frantically trying to figure out where I fucked up. Given I wish to trust it not to be broken, having something that is proven early is very important since my masterplan is to slap it on all my future tapeouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Project Roadmap#&lt;/head&gt;
    &lt;p&gt;Alright, I have to admit something: &lt;del&gt;I embellished reality in order to make myself look good&lt;/del&gt; I lied. Although initially I had 2 weeks to do this tapeout, because I spent the first 4 days reeling from the after effects of my previous tapeout (aka: sleeping, going outside and talking to another human being) and “deciding on a technical direction” which is corporate speech for describing a mixture between “procrastinating” and “figuring out what I could build without sacrificing too much of my remaining sanity”, I now had 10 days left. You’re welcome.&lt;/p&gt;
    &lt;p&gt;Given my self-imposed dire straits of a timeline, if I wanted to have any hope whatsoever of meeting the tapeout deadline I needed a battle plan: here is the roadmap.&lt;/p&gt;
    &lt;quote&gt;--- config: logLevel: 'debug' theme: 'default' gitGraph: showBranches: true mainBranchName: 'architecture' mainBranchOrder: 6 --- gitGraph TB: commit id: "idea " commit id: "figured it out " branch design order: 4 checkout design commit id: 'basic RTL ' branch simulation order: 5 checkout simulation commit id: 'this RTL is broken ' checkout design commit id: 'starts to work ' branch implementation order: 3 checkout implementation commit id: 'RIP timing ' commit id: 'RIP area ' checkout design commit id: 'looking good ' branch emulation order: 2 checkout emulation commit id: 'bitstream acquired ' branch firmware commit id: 'firmware bringup ' checkout emulation merge firmware checkout design merge emulation merge simulation checkout implementation merge design commit id: 'tapeout! '&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;architecture: what I need to build and how components would interface with one another.&lt;/item&gt;
      &lt;item&gt;design: where most of the RTL design takes place.&lt;/item&gt;
      &lt;item&gt;simulation: I write tests for simulating and validating that my design is behaving correctly and without any identifiable bugs. I used the classic Cocotb wrapper around iverilog for this.&lt;/item&gt;
      &lt;item&gt;FPGA emulation: when the design has mostly taken shape is when emulation takes place. This is the step where I port the design to an FPGA.&lt;/item&gt;
      &lt;item&gt;firmware: Alongside the emulation I bring up the firmware to interface with my design. This allows me to validate there are no issues when interfacing between the MCU and the ASIC.&lt;/item&gt;
      &lt;item&gt;implementation: this is when the ASIC flow is run and is the longest running task. It starts being run when the RTL design starts becoming functional to identify and fix timing and area utilization issues. And once all the verification is finished, it is run one final time to generate the final ASIC GDSII (manufacturing files).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most astute readers might have noticed that this grand strategy is pretty much the same roadmap I always use.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If it aint broken dont fix it. ~ a wise man&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Flow saves the day#&lt;/head&gt;
    &lt;p&gt;So, why am I doing this to myself? Well, self delusion is a powerful force, and it was telling me this timeline would be possible if I leveraged my previous experience with the Tiny Tapeout/Librelane/OpenROAD flows, my existing personal linting/simulation/fpga/firmware flows, my existing code bases, and my own &lt;del&gt;ingrained knowledge of the dark arts&lt;/del&gt; experience.&lt;/p&gt;
    &lt;p&gt;But, let us not delude ourselves, the saving grace of this terrible idea was really just how great the Tiny Tapeout/Librelane/OpenROAD ASIC flow is.&lt;/p&gt;
    &lt;p&gt;The following section assumes readers are already familiar with the Open Source Silicon ecosystem.&lt;/p&gt;
    &lt;p&gt;For those not already familiar with Tiny Tapeout, Librelane and OpenROAD, you can find a short description of these in my previous hashing accelerator ASIC article, where I introduce some of the great tools that the open source silicon ecosystem has created.&lt;/p&gt;
    &lt;p&gt;The OpenROAD project was conceived with a no-human-in-the-loop (NHIL) target, and the goal of enabling 24-hour-or-less design turnaround times.&lt;/p&gt;
    &lt;p&gt;Librelane, the master coordinator of the flow itself, brings together OpenROAD, Yoysy, ABC, Magic, and many more amazing open source tools, building on top of this philosophy. Creating a process that takes you from your verilog and a few configurations all the way to the tapeout ready artifacts, in an extremely streamlined and fast fashion, requiring minimal human intervention.&lt;/p&gt;
    &lt;p&gt;Tiny Tapeout then completes the loop, running your testbenches on top of the entire implementation, and then allowing you to automatically upload your GDSII for integration into the shuttle chip.&lt;/p&gt;
    &lt;p&gt;This deeply resonates with my personal beliefs that faster iteration times are central to higher quality and more efficient design :&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Since I believe a low iteration time is paramount to project velocity and getting big things done, I also want to automatize all of the Vivado flow from taking the rtl to the SVF generation.&lt;/p&gt;&lt;lb/&gt;~ Previous article: Alibaba cloud FPGA: the 200$ Kintex UltraScale+&lt;/quote&gt;
    &lt;p&gt;Thus, not only was I building on top of a legacy of efficient design flows, but I had previously &lt;del&gt;spent&lt;/del&gt; invested a lot of time streamlining my tasks, especially repetitive ones.&lt;/p&gt;
    &lt;p&gt;A classic example would be my FPGA build flow used for emulation. It only requires a single command to create the Vivado project, read all the rtl and constraint files, synthesize, optionally add an ILA core and connect any wires marked for debug to it, implement, and then flash the bitstream :&lt;/p&gt;
    &lt;code&gt;make fpga_prog debug=1  
&lt;/code&gt;
    &lt;p&gt;Essentially, the bottleneck to making this design, from scratch in 10 days, wasn’t going to be the tools, but the squishy human between the chair and the keyboard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design#&lt;/head&gt;
    &lt;p&gt;Without further ado, let’s talk design !&lt;/p&gt;
    &lt;head rend="h2"&gt;Systolic Array Design#&lt;/head&gt;
    &lt;p&gt;The goal of this systolic array is to perform a 2×2 matrix-matrix multiply on 8-bit integer numbers.&lt;/p&gt;
    &lt;p&gt;Without going too much into detail on why systolic arrays are the recurring stars at the heart of modern AI inference accelerators, their main strength is achieving a high ratio of compute to everything else. And since memory operations are the most expensive family of operations by far, a hh ratio of compute to memory operations.&lt;/p&gt;
    &lt;p&gt;Data is recirculated directly within the array and reused across multiple consecutive operations rather than being repeatedly fetched/written from/to memory. This matters because memory accesses are expensive. SRAM accesses cost time and significant power, while DRAM accesses cost eternities of time and egregious amounts of power. Compute operations, even 64 bit floating point multiplications, are by comparison, cheap.&lt;/p&gt;
    &lt;p&gt;Thus, the larger the systolic array, the deeper the chain of compute, the better this compute to memory ratio becomes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Energy ratio#&lt;/head&gt;
    &lt;p&gt;As an illustrative example of this evolution of compute ratios, let us compare the power cost of 64 bit floating point multiply-add (MAC) operations in a hypothetical systolic array designed on a 45nm node running at 0.9V.&lt;/p&gt;
    &lt;p&gt;Compared with the energy expenditure needed to access this data using 256 bit wide reads to the 16nm DRAM. DRAM access costs will include the 10mm of wire, interface and access costs.&lt;/p&gt;
    &lt;p&gt;In this example, we will very generously assume the weights are stored in place and do not need to be updated, so will assume only the input data matrix needs to be read from DRAM.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;45 nm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16-bit integer multiply&lt;/cell&gt;
        &lt;cell&gt;2 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;64-bit floating-point multiply-add&lt;/cell&gt;
        &lt;cell&gt;50 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;64-bit access to 8-Kbyte SRAM&lt;/cell&gt;
        &lt;cell&gt;14 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;256-bit access to 1-Mbyte SRAM&lt;/cell&gt;
        &lt;cell&gt;566 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;256-bit 10 mm wire&lt;/cell&gt;
        &lt;cell&gt;310 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;256-bit DRAM interface&lt;/cell&gt;
        &lt;cell&gt;5,120 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;256-bit DRAM access&lt;/cell&gt;
        &lt;cell&gt;2,048 pJ&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;source: ENERGY PROPORTIONAL MEMORY SYSTEMS&lt;/p&gt;
    &lt;p&gt;Even though I purposefully chose 64 bit floats, the most energy intensive arithmetic operation, we still required a 64x64 systolic array before the cost of compute started exceeding the cost of the initial DRAM value reads.&lt;/p&gt;
    &lt;p&gt;aka: For those not living in 2026, we have uncovered a new clue to the mystery of where all the low-power DRAM chips have suddenly vanished to!&lt;/p&gt;
    &lt;head rend="h4"&gt;Scaling#&lt;/head&gt;
    &lt;p&gt;Another great feature of systolic arrays is how regular they are and how well their design scales. So, although today I am designing a small 2x2 array, without much re-work this can be scaled up to a larger 256x256 array.&lt;/p&gt;
    &lt;p&gt;Some of you might be wondering: if area is free and systolic arrays scale so well, why am I limiting myself to only making a 2x2 array ?&lt;/p&gt;
    &lt;p&gt;Well, because I am a good neighbor and any additional tile of area I use is potentially depriving someone else from having the area available to submit their project.&lt;/p&gt;
    &lt;p&gt;Since the initial motivation for this project was to design some proven in silicon debug infrastructure, a 2x2 array is sufficient for my needs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Constraints#&lt;/head&gt;
    &lt;p&gt;Since this ASIC is once again taping out as part of a Tiny Tapeout shuttle, it has to grapple with the similar limitations as my previous hashing accelerator, namely: the eternal I/O bandwidth limitation!&lt;/p&gt;
    &lt;p&gt;This limitations comes in two flavors:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Pin count: eight input pins, eight output pins, and eight configurable I/O pins, limiting my parallel buses in and out of the accelerator.&lt;/item&gt;
      &lt;item&gt;Maximum operating frequency: unlike with the well-trodden path that was the public Skywater 130nm shuttle tapeout, for this experimental shuttle the maximum switching frequency of these pins hasn’t been characterized yet. And since I haven’t yet figured out how to do this characterization using a simulator myself, I’ve hand-wavingly assumed that both the input and output directions have a sustainable switching frequency of 50 MHz, and have sized the path timings around that assumption.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, I once again have the constraint of not having any SRAM. Though unlike with the Skywater 130 tapeout, the GlobalFoundries 180nm PDK does include a proven SRAM macro!&lt;/p&gt;
    &lt;p&gt;Hooray! Human progress is unstoppable !&lt;/p&gt;
    &lt;p&gt;This time, the limitation was actually imposed by the experimental nature of this shuttle.&lt;/p&gt;
    &lt;p&gt;Firstly, the flow didn’t initially support these macros out of the box. More specifically in the way they were laid out, which led to a slew of DRC failures that needed to be fixed.&lt;/p&gt;
    &lt;p&gt;Secondly, this experimental shuttle didn’t provide individual project power gating. As such, integrating this SRAM macro was commonly deemed not the best idea, and the community collectively agreed to wait for a future shuttle, which would include per-project macro power gating, before including it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Details#&lt;/head&gt;
    &lt;p&gt;This system is broken into two parts :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the compute units&lt;/item&gt;
      &lt;item&gt;the main array controller&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Compute units#&lt;/head&gt;
    &lt;p&gt;Since this is a 2×2 systolic array, there are four compute units. Each unit takes in an 8 bit signed integer and performs a multiply, addition, and a clamping operation, producing 8 bit signed integers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multiplication#&lt;/head&gt;
    &lt;p&gt;The multiply is done using a custom (from scratch) implementation of a Booth Radix-4 multiplier with Wallace trees.&lt;/p&gt;
    &lt;p&gt;This multiplier architecture strikes a good balance between area, power and performance, and can be regarded as the multiplier equivalent of what “Rental White™"[1] is to interior design.&lt;/p&gt;
    &lt;p&gt;Meaning it is a solid, well rounded from a PPA’s perspective, multiplier option, that without being anything novel or groundbreaking, is probably good enough for your use case (and given my +2ns of worst negative slack, and comfortable area occupancy, it was plenty good enough for mine).&lt;/p&gt;
    &lt;p&gt;Another advantage of booth radix-4 is that, since we are performing a signed multiplication, we can optimize out a level in the Wallace tree we are using for the partial product additions given we only have 4 partial products, unlike the 5 needed for unsigned operations.&lt;/p&gt;
    &lt;p&gt;The original article plan included an in-depth explanation of the booth radix-4 multiplication implementation and optimization, resulting in a very interesting but also very dry multi-page explanation covered with boolean algebra. Resulting in its canning.&lt;/p&gt;
    &lt;p&gt;If you are looking for a good explanation of this multiplier and its optimization see chapter 11 of ‘CMOS VLSI Design: A Circuits and Systems Perspective’.&lt;/p&gt;
    &lt;head rend="h4"&gt;Clamping#&lt;/head&gt;
    &lt;p&gt;The clamping operation occurs after both the multiply and addition operations. At which point the data is now 17 bit wide, and since we want to prevent our data size from exploding, the clamping operation is needed to clamp the outgoing data back down to eight bits.&lt;/p&gt;
    &lt;p&gt;$$ clamp_{i8}(x) = \begin{cases} \phantom{-}127,\,\text{if}\,x &amp;gt; 127, \\ \phantom{-12}x,\,\text{if}\,x \in [-128,127], \\ -128,\,\text{if}\,x &amp;lt; -128 \end{cases} $$&lt;/p&gt;
    &lt;head rend="h4"&gt;In place weight storage#&lt;/head&gt;
    &lt;p&gt;Given that the weights have high temporal and spatial locality, meaning the same set of weights can be reused over multiple unique input data matrices, in order to save on bandwidth, the choice was made to store an 8 bit weight in place inside of each unit.&lt;/p&gt;
    &lt;p&gt;A separate control sequence allows the user to load a new set of weights inside each of the units. The weight packet is sent over four consecutive cycles using the same input interface as the data matrix.&lt;/p&gt;
    &lt;head rend="h3"&gt;Array controller#&lt;/head&gt;
    &lt;p&gt;Given the way in which the matrix matrix multiplication is performed using a systolic array, the input data matrix needs to be shaped and fed to the array in a staggered manner.&lt;/p&gt;
    &lt;p&gt;Understandably readers might not instinctively get what I mean by “the data needs to be shaped”.&lt;/p&gt;
    &lt;p&gt;In order to help better illustrate this point, I would like to bring your attention to a great animation to show how data flows through the array.&lt;/p&gt;
    &lt;p&gt;An added bonus is that this animation is also of a 2×2 systolic array, such that it shares the same data with the same arrival constraints as my accelerator.&lt;/p&gt;
    &lt;p&gt;This animation is not a carbon copy of my accelerator. Rather, readers should use this animation as a tool to better understand how data flows in a systolic array and how it results in the computation of a matrix-matrix multiplication. For the sake of completeness, I would like to point out the major differences with my implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;this animation uses floating-point numbers, I am using 8-bit integers&lt;/item&gt;
      &lt;item&gt;there is no clamping step&lt;/item&gt;
      &lt;item&gt;the timings are not entirely identical. In my accelerator, many more operations occur on the same cycle. Granted, this was probably done in an effort to help make the animation more legible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition to helping shape the input data to the systolic array, this controller also helps coordinate data transfers around my I/O limitations.&lt;/p&gt;
    &lt;p&gt;Given the parallel data bus allows only 8 bits of input data to arrive per cycle, it is used to control the input buffers in order to accumulate enough data to create the next wave.&lt;/p&gt;
    &lt;p&gt;Similarly on the output side, when the accelerator produces two 8-bit results per cycle, the controller stores the results in an output buffer in order to only stream out 8 bits per cycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validation#&lt;/head&gt;
    &lt;p&gt;Validation was an extremely important step for this systolic array, particularly for validating my custom implementation of the Booth Radix-4 multiplication.&lt;/p&gt;
    &lt;p&gt;Once again, I used Cocotb as the abstracting layer allowing me to interface with multiple different simulators. Namely, icarus verilog for my standard verification and CVC for the post implementation timing annotated netlist.&lt;lb/&gt;This validation followed my standard approach of providing a set of input data matrices and weights over the standard accelerators interfaces and comparing the produced results to the expected values.&lt;/p&gt;
    &lt;p&gt;Given the nature of the problem, I made extensive use of randomization in order to get better testing coverage and attempt to hit corner cases that would not have been revealed using a directed approach. This randomization randomized both the values of the inputs weight and data, and also the timings with which this incoming data was fed over the parallel bus to the accelerator.&lt;/p&gt;
    &lt;head rend="h3"&gt;Firmware#&lt;/head&gt;
    &lt;p&gt;The firmware used to interface with this accelerator was designed to run on the RP2040 Raspberry Pi silicon and used the PIO hardware block to drive the parallel port.&lt;/p&gt;
    &lt;p&gt;I followed a similar approach as with my hashing accelerator, co-designing with the PIO imposed limitation when designing the parallel bus protocol.&lt;/p&gt;
    &lt;p&gt;Although this was designed for an RP2040, I later learned that the Tiny Tapeout boards would be evolving to a new version of the chip. These new PCBs would use a different pinout layout for communicating with the ASIC chips.&lt;/p&gt;
    &lt;p&gt;Although this will necessitate to re-adapt parts of the firmware for the new dev boards, this shouldn’t cause any major incompatibilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;JTAG TAP Design#&lt;/head&gt;
    &lt;p&gt;The second major component of this design and admittedly the actual principle piece of this ASIC is the JTAG TAP.&lt;/p&gt;
    &lt;p&gt;As stated earlier when my ASIC comes back as a glorified paperweight I will suddenly have a strong and immediate need for some kind of hardware block providing some in-silicon observability.&lt;/p&gt;
    &lt;p&gt;This is absolutely crucial, as it isn’t a question of if but when one of my ASIC will have hardware issues.&lt;lb/&gt;And, when that day comes, not having a view of the internal behavior will only make it that much more painful to identify the root issue in order to fix it for the upcoming generation.&lt;/p&gt;
    &lt;p&gt;As such, this TAP is actually a part of my larger efforts to help produce a set of DFT (Design for Test) proven IPs/tools that I can integrate into all of my future ASICs. As such, I have quite an incentive to have these designs proven early.&lt;/p&gt;
    &lt;p&gt;JTAG was chosen as it is a very common debug protocol. Not only is it well-defined but has good off-the-shelf support, both on the hardware (debug probes) and software side.&lt;/p&gt;
    &lt;head rend="h3"&gt;JTAG Implementation#&lt;/head&gt;
    &lt;p&gt;In addition to implementing all of the basic JTAG features in order to be compliant with the protocol, I additionally added custom instructions, extending it to fit my personal requirements.&lt;/p&gt;
    &lt;p&gt;Required instructions :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;EXTEST&lt;/code&gt;, opcode&lt;code&gt;0x0&lt;/code&gt;, boundary scan operation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;IDCODE&lt;/code&gt;, opcode&lt;code&gt;0x1&lt;/code&gt;, read JTAG tap identifiers, which allows the hardware to advertise itself on the JTAG scan chain&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SAMPLE_PRELOAD&lt;/code&gt;, opcode&lt;code&gt;0x2&lt;/code&gt;, boundary scan operation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BYPASS&lt;/code&gt;, opcode&lt;code&gt;0x7&lt;/code&gt;, set the TAP in bypass mode&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Custom instructions :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;USER_REG&lt;/code&gt;, opcode&lt;code&gt;0x3&lt;/code&gt;, custom instruction used to probe the internal registers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The systolic array is quite a deep structure in the sense that the data is directly reused as it is recirculated within the array.&lt;/p&gt;
    &lt;p&gt;As such, it is very easy to lose track of what the internal state is, having only the input and output observable to the user. Although this might stay manageable with a 2 x 2 array. As the structure grows larger, this will start to become more and more of an issue.&lt;/p&gt;
    &lt;p&gt;In order to help address this future pain point, I added the custom &lt;code&gt;USER_REG&lt;/code&gt; JTAG
instruction to read the current state of a target compute units internal registers.&lt;/p&gt;
    &lt;p&gt;Link to the datasheet of the &lt;code&gt;USER_REG&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design#&lt;/head&gt;
    &lt;p&gt;The JTAG TAP design itself is quite straightforward as JTAG was conceived as a hardware first protocol, and this shows in its implementation. The design clearly flows from the JTAG specification to the RTL (unlike you BLAKE2 I am looking at you!). As such I do not think it is worthwhile to discuss it in detail.&lt;/p&gt;
    &lt;p&gt;What makes this design more interesting is how the JTAG and the systolic array live in two different clock domains. Making this accelerator have not one but two separate clock trees.&lt;/p&gt;
    &lt;p&gt;On the altar of this noble cause, I sacrificed one of my precious data input pins to serve as the JTAG clock input (TCK).&lt;/p&gt;
    &lt;p&gt;The SDC script I used to generate these two clock trees drew heavy inspiration from the official LibreLane clock generation SDC, but was re-adapted from my contorted used case:&lt;/p&gt;
    &lt;code&gt;# modified librelane base.sdc to support 2 clocks

# custom env variable
set ::env(JTAG_CLOCK_PERIOD) 500


if { [info exists ::env(CLOCK_PORT)] } {
    set port_count [llength $::env(CLOCK_PORT)]
    puts "/gc[INFO] Found ${port_count} clocks : $::env(CLOCK_PORT)%"
    if { $port_count == "0" } {
        puts "/gc[ERROR] No CLOCK_PORT found."
        error
    }

    # set both clock ports
    set ::clock_port [lindex $::env(CLOCK_PORT) 0]
    set ::jtag_clock_port [lindex $::env(CLOCK_PORT) 1]
}


set port_args [get_ports $clock_port]
set jtag_port_args [get_ports $jtag_clock_port]

puts "/gc[INFO] Using clock $clock_port… with args $port_args"
puts "/gc[INFO] Using jtag clock $jtag_clock_port… with args $jtag_port_args"


create_clock {*}$port_args -name $clock_port -period $::env(CLOCK_PERIOD)
create_clock {*}$jtag_port_args -name $jtag_clock_port -period $::env(JTAG_CLOCK_PERIOD)

set input_delay_value [expr $::env(CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
set output_delay_value [expr $::env(CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
puts "/gc[INFP] for clk $clock_port :"
puts "/gc[INFO] Setting output delay to: $output_delay_value"
puts "/gc[INFO] Setting input delay to: $input_delay_value"


# keep the same io delay constraints for jtag 
set jtag_input_delay_value [expr $::env(JTAG_CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
set jtag_output_delay_value [expr $::env(JTAG_CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
puts "/gc[INFP] for clk $jtag_clock_port :"
puts "/gc[INFO] Setting output delay to: $jtag_output_delay_value"
puts "/gc[INFO] Setting input delay to: $jtag_input_delay_value"


set_max_fanout $::env(MAX_FANOUT_CONSTRAINT) [current_design]
if { [info exists ::env(MAX_TRANSITION_CONSTRAINT)] } {
    set_max_transition $::env(MAX_TRANSITION_CONSTRAINT) [current_design]
}
if { [info exists ::env(MAX_CAPACITANCE_CONSTRAINT)] } {
    set_max_capacitance $::env(MAX_CAPACITANCE_CONSTRAINT) [current_design]
} 

# clk
set clk_input [get_port $clock_port]
set clk_indx [lsearch [all_inputs] $clk_input]
set all_inputs_wo_clk [lreplace [all_inputs] $clk_indx $clk_indx ""]

# jtag clk
set jtag_clk_input [get_port $jtag_clock_port]
set jtag_clk_indx [lsearch [all_inputs] $jtag_clk_input]
set jtag_all_inputs_wo_clk [lreplace [all_inputs] $jtag_clk_indx $jtag_clk_indx ""]

# rst
set all_inputs_wo_clk_rst $all_inputs_wo_clk

# jtag has no trst so there is no need to define another rst path 

# correct resetn
set clocks [get_clocks $clock_port]

set_input_delay $input_delay_value -clock $clocks $all_inputs_wo_clk_rst
set_output_delay $output_delay_value -clock $clocks [all_outputs]

if { ![info exists ::env(SYNTH_CLK_DRIVING_CELL)] } {
    set ::env(SYNTH_CLK_DRIVING_CELL) $::env(SYNTH_DRIVING_CELL)
}

set_driving_cell /gc
    -lib_cell [lindex [split $::env(SYNTH_DRIVING_CELL) "/"] 0] /gc
    -pin [lindex [split $::env(SYNTH_DRIVING_CELL) "/"] 1] /gc
    $all_inputs_wo_clk_rst

set_driving_cell /gc
    -lib_cell [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 0] /gc
    -pin [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 1] /gc
    $clk_input

set_driving_cell /gc
    -lib_cell [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 0] /gc
    -pin [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 1] /gc
    $jtag_clk_input

set cap_load [expr $::env(OUTPUT_CAP_LOAD) / 1000.0]
puts "/gc[INFO] Setting load to: $cap_load"
set_load $cap_load [all_outputs]

puts "/gc[INFO] Setting clock uncertainty to: $::env(CLOCK_UNCERTAINTY_CONSTRAINT)"
set_clock_uncertainty $::env(CLOCK_UNCERTAINTY_CONSTRAINT) $clocks

puts "/gc[INFO] Setting clock transition to: $::env(CLOCK_TRANSITION_CONSTRAINT)"
set_clock_transition $::env(CLOCK_TRANSITION_CONSTRAINT) $clocks

puts "/gc[INFO] Setting timing derate to: $::env(TIME_DERATING_CONSTRAINT)%"
set_timing_derate -early [expr 1-[expr $::env(TIME_DERATING_CONSTRAINT) / 100]]
set_timing_derate -late [expr 1+[expr $::env(TIME_DERATING_CONSTRAINT) / 100]]

if { [info exists ::env(OPENLANE_SDC_IDEAL_CLOCKS)] &amp;amp;&amp;amp; $::env(OPENLANE_SDC_IDEAL_CLOCKS) } {
    unset_propagated_clock [all_clocks]
} else {
    set_propagated_clock [all_clocks]
}


set_clock_groups -asynchronous -group $clock_port -group $jtag_clock_port
&lt;/code&gt;
    &lt;p&gt;Because the official JTAG spec lives behind the impregnable IEEE paywall, a castle in which I am not permitted to set foot as a result of not having paid its lord my dues, the verification of the JTAG TAP was actually quite interesting.&lt;/p&gt;
    &lt;p&gt;Since JTAG is such a common protocol, its easy to find free resources online to build a good mental model of the internal FSM and TAP structure. My initial implementation and test bench was derived from this best effort understanding.&lt;lb/&gt;As such emulation actually became a critical step for validating this JTAG TAP.&lt;/p&gt;
    &lt;p&gt;Recall how earlier I mentioned that JTAG was a well-supported protocol with good off-the-shelf support on the software side?&lt;lb/&gt;Did I mention OpenOCD has great support for JTAG?&lt;/p&gt;
    &lt;p&gt;If I wasn’t going to get access to the official spec on how JTAG is expected to behave, plan B was to let my implementation be guided by how OpenOCD expected JTAG to behave.&lt;/p&gt;
    &lt;p&gt;To the purists clutching their pearls flabbergasted by the idea of not implementing per the spec: I’m not sorry. But if you do have the official spec here is my email.&lt;/p&gt;
    &lt;p&gt;In practice, whenever OpenOCD flagged my JTAG behavior as problematic, I assumed that my implementation was at fault. And trust me, there were issues. I called this&lt;/p&gt;
    &lt;p&gt;$$Designed\, By\, Support\,^{TM}$$&lt;/p&gt;
    &lt;p&gt;Then came the matter of supporting my custom TAP’s unholy instructions.&lt;/p&gt;
    &lt;p&gt;Luckily for me, OpenOCD allows you to finetune its behavior, through custom TCL scripts (did I mention I have PTSD and now love TCL?).&lt;/p&gt;
    &lt;p&gt;Thanks to already having gone through the pain of learning to create custom OpenOCD scripts during my Alibaba accelerator salvage project, this was a breeze.&lt;/p&gt;
    &lt;p&gt;These scripts allowed me to bring up my custom TAP and add support for my own godforsaken instructions.&lt;/p&gt;
    &lt;p&gt;The script can be found here, and below is &lt;del&gt;proof of my crime&lt;/del&gt; a log of me interfacing with my design during emulation:&lt;/p&gt;
    &lt;code&gt;Open On-Chip Debugger 0.12.0+dev-02171-g11dc2a288 (2025-11-23-19:25)  
Licensed under GNU GPL v2  
For bug reports, read  
	http://openocd.org/doc/doxygen/bugs.html  
Info : J-Link V10 compiled Jan 30 2023 11:28:07  
Info : Hardware version: 10.10  
Info : VTarget /gc= 3.380 V  
Info : clock speed 2000 kHz  
Info : JTAG tap: tpu.tap tap/device found: 0x1beef0d7 (mfg: 0x06b (Transwitch), part: 0xbeef, ver: 0x1)  
&lt;/code&gt;
    &lt;p&gt;Readers having reached enlightenment in their level of familiarity with the JTAG protocol might notice that the chip actually advertises itself as an Nvidia accelerator (mfg &lt;code&gt;0x06b&lt;/code&gt;.)&lt;/p&gt;
    &lt;p&gt;It’s good to have dreams.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;And somehow, against all odds, I made it! I met this absurd self-imposed deadline and the chip is now in fabrication!&lt;/p&gt;
    &lt;p&gt;This project was a battle against time. But making it fit in the end is something I’m honestly very proud of.&lt;/p&gt;
    &lt;p&gt;One of the big difficulties was to not get side-tracked. Even if it was frustrating, I had to commit to only adding the minimal set of features needed to get the project rolling. And trust me, the temptation to add, just that one extra little teeny-tiny feature, either to the systolic array or the debug system, was excruciating.&lt;/p&gt;
    &lt;p&gt;The good news is that, with this upcoming tapeout (v2 has already started), I can finally lash out !! My grand strategy is for the next project to be an improvement of both concepts.&lt;/p&gt;
    &lt;p&gt;Firstly, I’d like to finally have a rematch with an old adversary of mine: Floating Point arithmetics! Less for the TOPS-&amp;gt;FLOPS bragging rights, but rather to finally pierce its deep mysteries through an optimized hardware implementation of my own.&lt;/p&gt;
    &lt;p&gt;Secondly, I will continue extending my debug infrastructure: adding scan chains and an ATPG flow not only to help identify silicon manufacturing issues, but also as a convenient way to extract the current state of all flops for usage debugging.&lt;/p&gt;
    &lt;p&gt;Yet, this is but another step towards my greater long term objective: to tape out my own chip, not as part of Tiny Tapeout, nor as part of a shuttle chip, but entirely on my own.&lt;/p&gt;
    &lt;p&gt;There are many minutiea of the ASIC’s design that for now (and for good reason) abstracted away by Tiny Tapeout. So, my objective is to use these shuttle programs as a harness while I master the missing skills.&lt;/p&gt;
    &lt;p&gt;I would like to finish with a thanks to a company, which has not been sponsoring me in any way, is totally unaware of my existence, but who, never the less, never fail to provide the level of reward desperately needed after an all nighter spent trying to wrap up my design:&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes#&lt;/head&gt;
    &lt;p&gt;[1] - All uses and attributions of the designation ‘rental white’™ are the sole property of John and are used hereby with acknowledgment of his proprietary rights.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46749671</guid><pubDate>Sun, 25 Jan 2026 01:25:37 +0000</pubDate></item><item><title>David Patterson: Challenges and Research Directions for LLM Inference Hardware</title><link>https://arxiv.org/abs/2601.05047</link><description>&lt;doc fingerprint="bc9e340b55f380cd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Hardware Architecture&lt;/head&gt;&lt;p&gt; [Submitted on 8 Jan 2026 (v1), last revised 14 Jan 2026 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Challenges and Research Directions for Large Language Model Inference Hardware&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Xiaoyu Ma [view email]&lt;p&gt;[v1] Thu, 8 Jan 2026 15:52:11 UTC (832 KB)&lt;/p&gt;&lt;p&gt;[v2] Wed, 14 Jan 2026 20:37:46 UTC (983 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AR&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46750214</guid><pubDate>Sun, 25 Jan 2026 02:48:36 +0000</pubDate></item><item><title>Second Win11 emergency out of band update to address disastrous Patch Tuesday</title><link>https://www.windowscentral.com/microsoft/windows-11/windows-11-second-emergency-out-of-band-update-kb5078127-released-address-outlook-bugs</link><description>&lt;doc fingerprint="9537e9523ae654d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft issues SECOND emergency out of band update for Windows 11 to address disastrous Patch Tuesday bugs — KB5078127 released globally&lt;/head&gt;
    &lt;p&gt;Another out of band update has been issued to Windows 11 users to address a major bug that caused Outlook, Dropbox, and more to become inoperable after January's disastrous Patch Tuesday updates.&lt;/p&gt;
    &lt;p&gt;Microsoft's January 2026 Patch Tuesday updates for Windows 11 have been a total disaster, seemingly introducing more problems than it fixed. The botched update has already forced the company to issue one emergency out of band update to fix two major issues, and now a second emergency out of band update has been released to address another critical problem.&lt;/p&gt;
    &lt;p&gt;This time around, the out of band update is addressing an issue that caused Outlook and "cloud-backed" applications such as OneDrive and Dropbox to be rendered inoperable after Patch Tuesday. The temporary workaround was to uninstall the latest security updates, but now the KB5078127 update for version 24H2 and 25H2 should mitigate this need.&lt;/p&gt;
    &lt;p&gt;"An out-of-band (OOB) update was released today, January 24, 2026, to address this issue," Microsoft says in newly published documentation. "This cumulative update includes all protections and improvements from the January 2026 Windows security update released January 13, 2026, as well as from the OOB update released on January 17, 2026 (which introduced fixes for two known issues: remote desktop connections and hibernation failures)."&lt;/p&gt;
    &lt;p&gt;Here's the official changelog for the KB5078127 update from Microsoft:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fixed: After installing the Windows update released on and after January 13, 2026, some applications became unresponsive or encountered unexpected errors when opening files from or saving files to cloud-based storage, such as OneDrive or Dropbox. In certain Outlook configurations that store PST files on OneDrive, Outlook may hang and fail to reopen unless the process is terminated or the system is restarted. Users may also see missing sent Items or previously downloaded emails being re‑downloaded.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This same fix is being rolled out to various other versions and editions of Windows too, including Windows 11 version 23H2, Windows Server editions, and more. Be sure to check out the Windows release health dashboard for the latest information impacting your version of Windows.&lt;/p&gt;
    &lt;p&gt;This is the second emergency out of band update that Microsoft has pushed out for Windows 11 users in a week. The first arrived on January 17, and fixed two major issues that began appearing for users after installing this month's Patch Tuesday updates released on January 13.&lt;/p&gt;
    &lt;p&gt;The original Patch Tuesday release introduced an issue that caused PCs running version 23H2 to fail to shutdown or hibernate, and also broke signing into Windows 11 PCs using Remote Desktop. Microsoft addressed these issues in the out of band update issued on January 17, but in the process broke apps like Outlook, OneDrive, and Dropbox.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;It's clear that the quality bar for Windows is at an all time low right now, marking an absolutely dreadful start to the year for Microsoft and Windows. It feels like there's nowhere to go but up at this point, so hopefully things improve with the next Patch Tuesday release.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46750358</guid><pubDate>Sun, 25 Jan 2026 03:17:43 +0000</pubDate></item><item><title>Nvidia-smi hangs indefinitely after ~66 days</title><link>https://github.com/NVIDIA/open-gpu-kernel-modules/issues/971</link><description>&lt;doc fingerprint="25161cdf1c066734"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 1.6k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;NVIDIA Open GPU Kernel Modules Version&lt;/head&gt;
    &lt;p&gt;[root@A11-R42-I61-42-5504045 ~]# cat /proc/driver/nvidia/params ResmanDebugLevel: 4294967295 RmLogonRC: 1 ModifyDeviceFiles: 1 DeviceFileUID: 0 DeviceFileGID: 0 DeviceFileMode: 438 InitializeSystemMemoryAllocations: 1 UsePageAttributeTable: 4294967295 EnableMSI: 1 EnablePCIeGen3: 0 MemoryPoolSize: 0 KMallocHeapMaxSize: 0 VMallocHeapMaxSize: 0 IgnoreMMIOCheck: 0 EnableStreamMemOPs: 0 EnableUserNUMAManagement: 1 NvLinkDisable: 0 RmProfilingAdminOnly: 1 PreserveVideoMemoryAllocations: 0 EnableS0ixPowerManagement: 0 S0ixPowerManagementVideoMemoryThreshold: 256 DynamicPowerManagement: 3 DynamicPowerManagementVideoMemoryThreshold: 200 RegisterPCIDriver: 1 EnablePCIERelaxedOrderingMode: 0 EnableResizableBar: 0 EnableGpuFirmware: 18 EnableGpuFirmwareLogs: 2 RmNvlinkBandwidthLinkCount: 0 EnableDbgBreakpoint: 0 OpenRmEnableUnsupportedGpus: 1 DmaRemapPeerMmio: 1 ImexChannelCount: 2048 CreateImexChannel0: 0 GrdmaPciTopoCheckOverride: 0 RegistryDwords: "" RegistryDwordsPerDevice: "" RmMsg: "" GpuBlacklist: "" TemporaryFilePath: "" ExcludedGpus: ""&lt;/p&gt;
    &lt;head rend="h3"&gt;Please confirm this issue does not happen with the proprietary driver (of the same version). This issue tracker is only for bugs specific to the open kernel driver.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I confirm that this does not happen with the proprietary driver package.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Operating System and Version&lt;/head&gt;
    &lt;p&gt;[root@A11-R42-I61-42-5504045 ~]# cat /etc/openeuler-release openeuler release 2.0 (LTS-SP2) [root@A11-R42-I61-42-5504045 ~]#&lt;/p&gt;
    &lt;head rend="h3"&gt;Kernel Release&lt;/head&gt;
    &lt;p&gt;[root@A11-R42-I61-42-5504045 ~]# uname -a Linux A11-R42-I61-42-5504045. 6.6.0-100. SMP Fri Aug 22 10:50:04 CST 2025 x86_64 x86_64 x86_64 GNU/Linux&lt;lb/&gt; [root@A11-R42-I61-42-5504045 ~]# uname -r 6.6.0-100&lt;/p&gt;
    &lt;head rend="h3"&gt;Please confirm you are running a stable release kernel (e.g. not a -rc). We do not accept bug reports for unreleased kernels.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I am running on a stable kernel release.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hardware: GPU&lt;/head&gt;
    &lt;p&gt;B200&lt;/p&gt;
    &lt;head rend="h3"&gt;Describe the bug&lt;/head&gt;
    &lt;p&gt;nvidia-smi hangs indefinitely after ~66 days 12 hours uptime with driver 570.133.20 OpenRM on B200&lt;/p&gt;
    &lt;p&gt;[root@A11-R42-I61-42-5504045 ~]# dmesg -T | grep -i nvrm | head -n 10&lt;lb/&gt; [Sat Nov 22 05:08:50 2025] NVRM: knvlinkUpdatePostRxDetectLinkMask_IMPL: Failed to update Rx Detect Link mask!&lt;lb/&gt; [Sat Nov 22 05:08:50 2025] NVRM: knvlinkDiscoverPostRxDetLinks_GH100: Getting peer1's postRxDetLinkMask failed!&lt;lb/&gt; [Sat Nov 22 05:08:54 2025] NVRM: knvlinkUpdatePostRxDetectLinkMask_IMPL: Failed to update Rx Detect Link mask!&lt;lb/&gt; [Sat Nov 22 05:08:54 2025] NVRM: knvlinkDiscoverPostRxDetLinks_GH100: Getting peer1's postRxDetLinkMask failed!&lt;lb/&gt; [Sat Nov 22 05:08:58 2025] NVRM: knvlinkUpdatePostRxDetectLinkMask_IMPL: Failed to update Rx Detect Link mask!&lt;lb/&gt; [Sat Nov 22 05:08:58 2025] NVRM: knvlinkDiscoverPostRxDetLinks_GH100: Getting peer1's postRxDetLinkMask failed!&lt;lb/&gt; [Sat Nov 22 05:09:02 2025] NVRM: knvlinkUpdatePostRxDetectLinkMask_IMPL: Failed to update Rx Detect Link mask!&lt;lb/&gt; [Sat Nov 22 05:09:02 2025] NVRM: knvlinkDiscoverPostRxDetLinks_GH100: Getting peer0's postRxDetLinkMask failed!&lt;lb/&gt; [Sat Nov 22 05:09:06 2025] NVRM: knvlinkUpdatePostRxDetectLinkMask_IMPL: Failed to update Rx Detect Link mask!&lt;lb/&gt; [Sat Nov 22 05:09:06 2025] NVRM: knvlinkDiscoverPostRxDetLinks_GH100: Getting peer1's postRxDetLinkMask failed!&lt;lb/&gt; [root@A11-R42-I61-42-5504045 ~]#&lt;/p&gt;
    &lt;p&gt;[root@A11-R42-I61-42-5504045 ~]# uptime&lt;lb/&gt; 22:50:02 up 67 days, 6:11, 2 users, load average: 17.40, 16.73, 18.67&lt;lb/&gt; [root@A11-R42-I61-42-5504045 ~]# last reboot&lt;lb/&gt; reboot system boot 6.6.0-100. Tue Sep 16 16:38 still running&lt;lb/&gt; reboot system boot 6.6.0-100 Tue Sep 9 17:02 - 16:34 (6+23:32)&lt;/p&gt;
    &lt;head rend="h3"&gt;To Reproduce&lt;/head&gt;
    &lt;p&gt;nvidia-smi hangs indefinitely after ~66 days 12 hours uptime with driver 570.133.20 OpenRM on B200 and kernel 6.6.0&lt;/p&gt;
    &lt;head rend="h3"&gt;Bug Incidence&lt;/head&gt;
    &lt;p&gt;Once&lt;/p&gt;
    &lt;head rend="h3"&gt;nvidia-bug-report.log.gz&lt;/head&gt;
    &lt;p&gt;no&lt;/p&gt;
    &lt;head rend="h3"&gt;More Info&lt;/head&gt;
    &lt;p&gt;No response&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46750425</guid><pubDate>Sun, 25 Jan 2026 03:33:20 +0000</pubDate></item><item><title>Show HN: VM-curator – a TUI alternative to libvirt and virt-manager</title><link>https://github.com/mroboff/vm-curator</link><description>&lt;doc fingerprint="51ef87b4d96f2fe"&gt;
  &lt;main&gt;
    &lt;p&gt;A fast and friendly Rust TUI for managing QEMU/KVM virtual machines. Discover, create, organize, launch, and manage VMs with an intuitive interface.&lt;/p&gt;
    &lt;p&gt;Para-virtualized 3D acceleration works with NVIDIA GPUs in VMs created by &lt;code&gt;vm-curator&lt;/code&gt;! Huzzah!&lt;/p&gt;
    &lt;p&gt;This was extensively tested by the developer on an RTX-4090 in Arch Linux using NVIDIA driver 590.48.01. The guest OS still has to support QEMU 3D-accelleration. &lt;code&gt;(virtio-vga-gl with gl=on.)&lt;/code&gt; Note this is not the same as full GPU Passthrough (the kind requiring multiple GPUs and/or jumping through many, many hoops.) Support for full GPU passthrough is being worked on.&lt;/p&gt;
    &lt;p&gt;VM Discovery &amp;amp; Organization&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically scans your VM library for directories containing &lt;code&gt;launch.sh&lt;/code&gt;scripts&lt;/item&gt;
      &lt;item&gt;Hierarchical organization by OS family (Windows, Linux, macOS, BSD, etc.)&lt;/item&gt;
      &lt;item&gt;Parses QEMU launch scripts to extract configuration (emulator, memory, CPU, VGA, audio, disks)&lt;/item&gt;
      &lt;item&gt;Smart categorization based on configurable hierarchy patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;VM Creation Wizard&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;5-step guided wizard for creating new VMs&lt;/item&gt;
      &lt;item&gt;50+ pre-configured OS profiles with optimal QEMU settings&lt;/item&gt;
      &lt;item&gt;Automatic UEFI firmware detection across Linux distributions (Arch, Debian, Fedora, NixOS, etc.)&lt;/item&gt;
      &lt;item&gt;ISO file browser for selecting installation media&lt;/item&gt;
      &lt;item&gt;Configurable disk size, memory, CPU cores, and QEMU options&lt;/item&gt;
      &lt;item&gt;Support for custom OS entries with user metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Snapshot Management&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create, restore, and delete snapshots for qcow2 disk images&lt;/item&gt;
      &lt;item&gt;Visual snapshot list with timestamps and sizes&lt;/item&gt;
      &lt;item&gt;Background operations with progress feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Launch Script Editor&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit &lt;code&gt;launch.sh&lt;/code&gt;scripts directly in the TUI&lt;/item&gt;
      &lt;item&gt;Syntax-aware display with line numbers&lt;/item&gt;
      &lt;item&gt;Automatic QEMU configuration re-parsing after saves&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;USB Passthrough&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;USB device enumeration via libudev&lt;/item&gt;
      &lt;item&gt;Select devices for passthrough to VMs&lt;/item&gt;
      &lt;item&gt;Persistent passthrough configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional Features&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vim-style navigation (j/k, arrows, mouse)&lt;/item&gt;
      &lt;item&gt;Search and filter VMs&lt;/item&gt;
      &lt;item&gt;Multiple boot modes (normal, install, custom ISO)&lt;/item&gt;
      &lt;item&gt;OS metadata with historical blurbs and fun facts&lt;/item&gt;
      &lt;item&gt;ASCII art logos for classic operating systems&lt;/item&gt;
      &lt;item&gt;Configurable settings with persistence&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt; VM Curator (QEMU VM Library in ~/vm-space)
┌─────────────────────────────────────────────────────────────────────┐
│ ┌─────────────────────────┐  ┌────────────────────────────────────┐ │
│ │ VMs (35)                │  │       _    _ _           _        │ │
│ │ ──────────────────────  │  │      | |  | (_)         | |       │ │
│ │ 🪟 Microsoft            │  │      | |/\| |_ _ __   __| | ___   │ │
│ │   ▼ DOS                 │  │      \  /\  / | '_ \ / _` |/ _ \  │ │
│ │     &amp;gt; MS-DOS 6.22   [*] │  │       \/  \/|_|_| |_|\__,_|\___/  │ │
│ │     &amp;gt; Windows 3.11      │  │                                   │ │
│ │   ▼ Windows 9x          │  │   Windows 95 OSR2.5               │ │
│ │     &amp;gt; Windows 95        │  │   Microsoft | August 1995 | i386  │ │
│ │     &amp;gt; Windows 98        │  │                                   │ │
│ │ 🐧 Linux                │  │   The OS that changed everything  │ │
│ │   ▼ Debian-based        │  │   with the Start Menu, taskbar,   │ │
│ │     &amp;gt; Debian 12         │  │   and 32-bit computing for all.   │ │
│ │     &amp;gt; Ubuntu 24.04      │  │                                   │ │
│ └─────────────────────────┘  └────────────────────────────────────┘ │
├─────────────────────────────────────────────────────────────────────┤
│ [Enter] Launch  [m] Manage  [c] Create  [s] Settings  [?] Help     │
└─────────────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;Prerequisites&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust 1.70+&lt;/item&gt;
      &lt;item&gt;QEMU (&lt;code&gt;qemu-system-*&lt;/code&gt;binaries)&lt;/item&gt;
      &lt;item&gt;libudev-dev (Debian/Ubuntu) or libudev (Arch/Fedora)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd vm-curator
cargo build --release&lt;/code&gt;
    &lt;p&gt;The binary will be at &lt;code&gt;target/release/vm-curator&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;vm-curator&lt;/code&gt;
    &lt;code&gt;# List all VMs
vm-curator list

# Launch a VM
vm-curator launch windows-95
vm-curator launch windows-95 --install    # Boot in install mode
vm-curator launch windows-95 --cdrom /path/to/image.iso

# View VM configuration
vm-curator info windows-95

# Manage snapshots
vm-curator snapshot windows-95 list
vm-curator snapshot windows-95 create my-snapshot
vm-curator snapshot windows-95 restore my-snapshot
vm-curator snapshot windows-95 delete my-snapshot

# List available QEMU emulators
vm-curator emulators&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;j/k&lt;/code&gt; or &lt;code&gt;Down/Up&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Navigate VM list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Enter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Launch selected VM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;m&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Open management menu&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Open VM creation wizard&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;s&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Open settings&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Search/filter VMs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show help&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;PgUp/PgDn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Scroll info panel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Esc&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Back / Cancel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;q&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Quit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Enter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Select menu option&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;e&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Edit launch script&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;u&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Configure USB passthrough&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;Tab&lt;/code&gt; / &lt;code&gt;Shift+Tab&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Next/previous field&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Enter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Select / Continue&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;n&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Next step&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;p&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Previous step&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;Esc&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cancel wizard&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Settings are stored in &lt;code&gt;~/.config/vm-curator/config.toml&lt;/code&gt; and can be edited via the Settings screen (&lt;code&gt;s&lt;/code&gt; key).&lt;/p&gt;
    &lt;code&gt;# VM library location
vm_library_path = "~/vm-space"

# Default values for new VMs
default_memory_mb = 4096
default_cpu_cores = 2
default_disk_size_gb = 64
default_display = "gtk"      # gtk, sdl, spice
default_enable_kvm = true

# Behavior
confirm_before_launch = true&lt;/code&gt;
    &lt;p&gt;VMs are expected in your library directory (default &lt;code&gt;~/vm-space/&lt;/code&gt;) with this structure:&lt;/p&gt;
    &lt;code&gt;~/vm-space/
├── windows-95/
│   ├── launch.sh      # QEMU launch script (required)
│   └── disk.qcow2     # Disk image (qcow2 recommended for snapshots)
├── linux-debian/
│   ├── launch.sh
│   ├── disk.qcow2
│   └── install.iso    # Optional: installation media
└── macos-tiger/
    ├── launch.sh
    └── disk.qcow2
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;launch.sh&lt;/code&gt; script should invoke QEMU. VM Curator parses this script to extract configuration and can generate new scripts via the creation wizard.&lt;/p&gt;
    &lt;p&gt;The creation wizard includes pre-configured profiles for 50+ operating systems:&lt;/p&gt;
    &lt;p&gt;Microsoft: DOS, Windows 3.x, 95, 98, ME, 2000, XP, Vista, 7, 8, 10, 11, Server editions&lt;/p&gt;
    &lt;p&gt;Apple: Classic Mac OS (System 6-9), Mac OS X (10.4-10.15), macOS (11+)&lt;/p&gt;
    &lt;p&gt;Linux: Arch, Debian, Ubuntu, Fedora, openSUSE, Mint, CentOS, RHEL, Gentoo, Slackware, Alpine, NixOS, Void, EndeavourOS, Manjaro, and more&lt;/p&gt;
    &lt;p&gt;BSD: FreeBSD, OpenBSD, NetBSD, DragonFly BSD&lt;/p&gt;
    &lt;p&gt;Unix: Solaris, OpenIndiana, illumos&lt;/p&gt;
    &lt;p&gt;Other: Haiku, ReactOS, FreeDOS, Plan 9, Minix, TempleOS&lt;/p&gt;
    &lt;p&gt;Each profile includes optimal QEMU settings for that OS (emulator, machine type, VGA, audio, network, etc.).&lt;/p&gt;
    &lt;p&gt;OS Information: Override or add OS metadata in &lt;code&gt;~/.config/vm-curator/metadata/&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# ~/.config/vm-curator/metadata/my-os.toml
[my-custom-os]
name = "My Custom OS"
publisher = "My Company"
release_date = "2024-01-01"
architecture = "x86_64"

[my-custom-os.blurb]
short = "A brief description"
long = "A longer description with history and details."

[my-custom-os.fun_facts]
facts = ["Fact 1", "Fact 2"]&lt;/code&gt;
    &lt;p&gt;ASCII Art: Add custom ASCII art in &lt;code&gt;~/.config/vm-curator/ascii/&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;QEMU Profiles: Override profiles in &lt;code&gt;~/.config/vm-curator/qemu_profiles.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Runtime: QEMU, qemu-img (for snapshots), libudev&lt;/item&gt;
      &lt;item&gt;Build: Rust 1.70+, libudev-dev&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;VM Curator automatically detects OVMF/UEFI firmware paths across Linux distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arch Linux: &lt;code&gt;/usr/share/edk2/x64/OVMF_CODE.4m.fd&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Debian/Ubuntu: &lt;code&gt;/usr/share/OVMF/OVMF_CODE.fd&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Fedora/RHEL: &lt;code&gt;/usr/share/edk2/ovmf/OVMF_CODE.fd&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NixOS: Multiple search paths supported&lt;/item&gt;
      &lt;item&gt;And more...&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! If you find a bug or have an idea for an improvement, feel free to open an issue or submit a Pull Request.&lt;/p&gt;
    &lt;p&gt;Help Wanted: ASCII Art As a TUI application, &lt;code&gt;vm-curator&lt;/code&gt; relies on visual flair to stand out. I am specifically looking for help with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logo/Banner Art: A cool ASCII banner for the startup screen.&lt;/item&gt;
      &lt;item&gt;Iconography: Small, recognizable ASCII/block character icons for the TUI menus (e.g., stylized hard drives, network cards, or GPU icons).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a knack for terminal aesthetics, your PRs are highly appreciated!&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;vm-curator&lt;/code&gt; was built to solve a specific, painful problem: getting high-performance, 3D-accelerated Linux VMs (via QEMU) without the overhead and complexity of &lt;code&gt;libvirt&lt;/code&gt; or &lt;code&gt;virt-manager&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is a personal passion project that I am sharing with the community. While I use this tool daily and will fix critical bugs as I encounter them, please note:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Development Pace: This project is maintained in my spare time. Feature requests will be considered but are not guaranteed.&lt;/item&gt;
      &lt;item&gt;The "As-Is" Philosophy: The goal is a lean, transparent TUI. I prioritize stability and performance over comprehensive enterprise feature parity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If this tool saved you time or helped you get 3D Acceleration working without having to resort to passthrough:&lt;/p&gt;
    &lt;p&gt;If you'd like to say thanks, you can support the project below. Donations are a "thank you" for existing work, not a payment for future support.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Sponsors: Best for one-time contributions (Goes to the RTX-Pro 6000 fund!)&lt;/item&gt;
      &lt;item&gt;Ko-fi: Buy me a coffee (or a generic energy drink).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46750437</guid><pubDate>Sun, 25 Jan 2026 03:36:38 +0000</pubDate></item><item><title>What Ralph Wiggum loops are missing</title><link>https://xr0am.substack.com/p/what-ralph-wiggum-loops-are-missing</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46750937</guid><pubDate>Sun, 25 Jan 2026 05:09:38 +0000</pubDate></item><item><title>Like digging 'your own grave': The translators grappling with losing work to AI</title><link>https://www.cnn.com/2026/01/23/tech/translation-language-jobs-ai-automation-intl</link><description>&lt;doc fingerprint="7f10d89d8a39232f"&gt;
  &lt;main&gt;
    &lt;p&gt;As a rare Irish-language translator, Timothy McKeon enjoyed steady work for European Union institutions for years. But the rise of artificial intelligence tools that can translate text and, increasingly, speech nearly instantly has upended his livelihood and that of many others in his field.&lt;/p&gt;
    &lt;p&gt;He says he lost about 70% of his income when the EU translation work dried up. Now, available work consists of polishing machine-generated translations, jobs he refuses “on principle” because they help train the software taking work away from human translators. When the edited text is fed back into the translation software, “it learns from your work.”&lt;/p&gt;
    &lt;p&gt;“The more it learns, the more obsolete you become,” he said. “You’re essentially expected to dig your own professional grave.”&lt;/p&gt;
    &lt;p&gt;While workers worldwide ponder how AI might affect their livelihoods – a topic on the agenda at the World Economic Forum in Davos this week – that question is no longer hypothetical in the translation industry. Apps like Google Translate already reduced the need for human translators, and increased adoption of generative AI has only accelerated that trend.&lt;/p&gt;
    &lt;p&gt;A 2024 survey of writing professionals by the United Kingdom’s Society of Authors showed that more than a third of translators had lost work due to generative AI, which can create sophisticated text, as well as images and audio, from users’ prompts. And 43% of translators said their income had dropped because of the technology.&lt;/p&gt;
    &lt;p&gt;In the United States, data from 2010-23 analyzed by Carl Frey and Pedro Llanos-Paredes at Oxford University showed that regions where Google Translate was in greater use saw slower growth in the number of translator jobs. Originally powered by statistical translation, Google Translate shifted to a technique called neural translation in 2016, resulting in more natural-sounding text and bringing it closer to today’s AI tools.&lt;/p&gt;
    &lt;p&gt;“Our best baseline estimate is that roughly 28,000 more jobs for translators would’ve been added in the absence of machine translation,” Frey told CNN.&lt;/p&gt;
    &lt;p&gt;“It’s not a story of mass displacement but I think that’s very likely to follow.”&lt;/p&gt;
    &lt;p&gt;The story is similar globally, suggests McKeon: He is part of the Guerrilla Media Collective, an international group of translators and communications professionals, and says everyone in the collective supplements their income with other work due to the impact of AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘The entire US is looking at Wisconsin’&lt;/head&gt;
    &lt;p&gt;Christina Green is president of Green Linguistics, a provider of language services, and a court interpreter in Wisconsin.&lt;/p&gt;
    &lt;p&gt;She worries her court role could soon vanish because of a bill that would allow courts to use AI or other machine translation in civil or criminal proceedings, and in certain other cases.&lt;/p&gt;
    &lt;p&gt;Green and other language professionals have been fighting the proposal since it was introduced in May. “The entire US is looking at Wisconsin” as a precedent, Green said, noting that the bill’s opponents had so far succeeded in stalling it.&lt;/p&gt;
    &lt;p&gt;While Green still has her court job, her company recently lost a major Fortune 10 corporate client, which she said opted to use a company offering AI translation instead. The client accounted for such an outsized share of her company’s business that she had to make layoffs.&lt;/p&gt;
    &lt;p&gt;“People and companies think they’re saving money with AI, but they have absolutely no clue what it is, how privacy is affected and what the ramifications are,” Green said.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘Governments are not doing enough’&lt;/head&gt;
    &lt;p&gt;Fardous Bahbouh, based in London, is an Arabic-language translator and interpreter for international media organizations, including CNN. She has seen a considerable reduction in written work in recent years, which she attributes to technological developments and the financial pressures facing media outlets.&lt;/p&gt;
    &lt;p&gt;Bahbouh is also studying for a PhD focusing on the translation industry. Her research shows that technology, including AI, is “hugely impacting” translators and interpreters.&lt;/p&gt;
    &lt;p&gt;“I worry a great deal that governments are not doing enough to help them transition into other work, which could lead to greater inequality, in-work poverty and child poverty,” she told CNN.&lt;/p&gt;
    &lt;p&gt;Many translators are indeed looking to retrain “because translation isn’t generating the income it previously did,” according to Ian Giles, a translator and chair of the Translators Association at the UK’s Society of Authors. The picture is similar in the United States: Many translators are leaving the profession, Andy Benzo, president of the American Translators Association, told CNN.&lt;/p&gt;
    &lt;p&gt;And Kristalina Georgieva, the head of the International Monetary Fund, said in Davos Thursday that the number of translators and interpreters at the fund had gone down to 50 from 200 due to greater use of technology.&lt;/p&gt;
    &lt;p&gt;Governments should also do more for those remaining in the translation industry, by introducing stronger labor protections, Bahbouh argued.&lt;/p&gt;
    &lt;head rend="h2"&gt;Human professionals still needed&lt;/head&gt;
    &lt;p&gt;Despite advances in machine translation and interpretation, technology can’t replace human language workers entirely just yet.&lt;/p&gt;
    &lt;p&gt;While using AI tools for everyday tasks like finding directions is “low-risk,” human translators will likely need to be involved for the foreseeable future in diplomatic, legal, financial and medical contexts where the risks are “humungous,” according to Benzo.&lt;/p&gt;
    &lt;p&gt;“I’m a translator and a lawyer and in both professions the nuance of each word is very specific and the (large language models powering AI tools) aren’t there yet, by far,” she said.&lt;/p&gt;
    &lt;p&gt;Another field relatively untouched by machine translation tools is literary translation.&lt;/p&gt;
    &lt;p&gt;Giles, who translates commercial fiction from Scandinavian languages into English, used to supplement his income with translation work from companies, but that has now disappeared. Meanwhile, literary commissions have continued to come in, he said.&lt;/p&gt;
    &lt;p&gt;There’s also one key element of communication that AI can’t replace, according to Oxford University’s Frey: Human connection.&lt;/p&gt;
    &lt;p&gt;“The fact that machine translation is pervasive doesn’t mean you can build a relationship with somebody in France without speaking a word of French,” he said.&lt;/p&gt;
    &lt;p&gt;Ana Nicolaci da Costa contributed reporting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751835</guid><pubDate>Sun, 25 Jan 2026 08:08:43 +0000</pubDate></item></channel></rss>