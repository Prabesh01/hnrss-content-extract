<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 06 Jan 2026 23:38:51 +0000</lastBuildDate><item><title>Why Big Companies Keep Failing: The Stack Fallacy (2016)</title><link>https://techcrunch.com/2016/01/18/why-big-companies-keep-failing-the-stack-fallacy/</link><description>&lt;doc fingerprint="7d0c11194f46e5ed"&gt;
  &lt;main&gt;
    &lt;p&gt;Stack fallacy has caused many companies to attempt to capture new markets and fail spectacularly. When you see a database company thinking apps are easy, or a VM company thinking big data is easy ‚Äî they are suffering from stack fallacy.&lt;/p&gt;
    &lt;p&gt;Stack fallacy is the mistaken belief that it is trivial to build the layer above yours.&lt;/p&gt;
    &lt;p&gt;Comic credit: XKCD&lt;/p&gt;
    &lt;p&gt;Mathematicians often believe we can describe the entire natural world in mathematical terms. Hence, all of physics is just applied math. And so on and so forth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stack fallacy ‚Äî ‚Äújust an app‚Äù&lt;/head&gt;
    &lt;p&gt;In the business world, we have a similar illusion. Database companies believe that SaaS apps are ‚Äújust a database app‚Äù ‚Äî this gives them false confidence that they can easily build, compete and win in this new market.&lt;/p&gt;
    &lt;p&gt;As history has shown, Amazon is dominating the cloud IaaS market, even as the technology vendors that build ingredient, lower-layer technologies struggle to compete ‚Äî VMware is nowhere close to winning against AWS, even though all of AWS runs on virtual machine technology, a core competency of VMware; Oracle has been unable to beat Salesforce in CRM SaaS, despite the fact that Oracle perceives Salesforce to be just a hosted database app. It even runs on their database!&lt;/p&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages ‚Äî part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages ‚Äî part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;p&gt;Apple continues to successfully integrate vertically down ‚Äî building chips, programming languages, etc., but again has found it very hard to go up the stack and build those simple apps ‚Äî things like photo sharing apps and maps.&lt;/p&gt;
    &lt;p&gt;History is full of such examples. IBM thought nothing much of the software layer that ran their PC hardware layer and happily allowed Microsoft to own the OS market.&lt;/p&gt;
    &lt;p&gt;In the 1990s, Larry Ellison saw SAP make gargantuan sums of money selling process automation software (ERP) ‚Äî to him, ERP was nothing more than a bunch of tables and workflows ‚Äî so he spent hundreds of millions of dollars trying to own that market, with mixed results. Eventually, Oracle bought its way into the apps market by acquiring PeopleSoft and Siebel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why do we keep falling for the stack fallacy?&lt;/head&gt;
    &lt;p&gt;The stack fallacy is a result of human nature ‚Äî we (over) value what we know. In real terms, imagine you work for a large database company and the CEO asks , ‚ÄúCan we compete with Intel or SAP?‚Äù Very few people will imagine they can build a computer chip just because they can build relational database software, but because of our familiarity with building blocks of the layer up, it is easy to believe you can build the ERP app. After all, we know tables and workflows.&lt;/p&gt;
    &lt;p&gt;The bottleneck for success often is not knowledge of the tools, but lack of understanding of the customer needs. Database engineers know almost nothing about what supply chain software customers want or need. They can hire for that, but it is not a core competency.&lt;/p&gt;
    &lt;p&gt;In a surprising way, it is far easier to innovate down the stack than up the stack.&lt;/p&gt;
    &lt;p&gt;The reason for this is that you are yourself a natural customer of the lower layers. Apple knew what it wanted from an ideal future microprocessor. It did not have the skills necessary to build it, but the customer needs were well understood. Technical skills can be bought/acquired, whereas it is very hard to buy a deep understanding of market needs.&lt;/p&gt;
    &lt;p&gt;It is therefore no surprise that Apple had an easier time building semiconductor chips than building Apple Maps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google, Facebook, WhatsApp&lt;/head&gt;
    &lt;p&gt;Google is a great example. It owned our email graph and our interest data (search), yet found it very difficult to succeed in what looks like a ‚Äútrivial to build‚Äù app ‚Äî social networks.&lt;/p&gt;
    &lt;p&gt;In fact, this is the perfect irony of stack fallacy. You can build things higher up the stack. It is just that often it is not clear what to build.&lt;/p&gt;
    &lt;p&gt;Product management is the art of knowing what to build.&lt;/p&gt;
    &lt;p&gt;The stack fallacy provides insights into why companies keep failing at the obvious things ‚Äî things so close to their reach that they can surely build. The answer may be that the what is 100 times more important than the how.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514816</guid><pubDate>Tue, 06 Jan 2026 16:53:27 +0000</pubDate></item><item><title>Volkswagen Brings Back Physical Buttons</title><link>https://www.caranddriver.com/news/a69916699/volkswagen-interior-physical-buttons-return/</link><description>&lt;doc fingerprint="bdad1ddd2a3489a9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Volkswagen revealed a new generation of cockpit design with the refreshed ID. Polo.&lt;/item&gt;
      &lt;item&gt;The new design marks a big departure for VW and features a plethora of physical controls rather than the capacitive buttons on current models.&lt;/item&gt;
      &lt;item&gt;While the switchgear is currently only found on the new ID. Polo, which isn't sold in the United States, it could debut on the soon-to-be-refreshed ID.4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Volkswagen is making a drastic change to its interiors, or at least the interiors of its electric vehicles. The automaker recently unveiled a new cockpit generation with the refreshed ID. Polo‚Äîthe diminutive electric hatchback that the brand sells in Europe‚Äîthat now comes with physical buttons.&lt;/p&gt;
    &lt;p&gt;While VW certainly isn't the only automaker that pushed the envelope with haptic controls and digital buttons, it was a particularly egregious offender. Now, the company is doing a complete 180-degree shift, adding a full suite of physical buttons and switchgear to the Polo's interior.&lt;/p&gt;
    &lt;head rend="h2"&gt;For Sale Near You&lt;/head&gt;
    &lt;p&gt;See all results for Volkswagen for sale near 20011&lt;/p&gt;
    &lt;p&gt;The steering wheel gets new clusters of buttons for cruise control and interacting with music playback, while switches for the temperature and fan speed now live in a row along the dashboard. The move back to buttons doesn't come out of nowhere. Volkswagen already started the shift with the new versions of the Golf and Tiguan models in the United States. Unfortunately, some climate controls, such as those for the rear defrost and the heated seats, are still accessed through the touchscreen. Thankfully, they look to retain their dedicated spot at the bottom of the display.&lt;/p&gt;
    &lt;p&gt;Volkswagen hasn't announced which models will receive the new cockpit design. The redesigned interior also may be limited to the brand's electric vehicles, which would limit it to the upcoming refresh for the ID.4 SUV (and potentially the ID.Buzz), as the only VW EV models currently sold in America.&lt;/p&gt;
    &lt;p&gt;‚û°Ô∏è Skip the lot. Let Car and Driver help you find your next car.&lt;/p&gt;
    &lt;p&gt;Jack Fitzgerald‚Äôs love for cars stems from his as yet unshakable addiction to Formula 1. &lt;lb/&gt; After a brief stint as a detailer for a local dealership group in college, he knew he needed a more permanent way to drive all the new cars he couldn‚Äôt afford and decided to pursue a career in auto writing. By hounding his college professors at the University of Wisconsin-Milwaukee, he was able to travel Wisconsin seeking out stories in the auto world before landing his dream job at Car and Driver. His new goal is to delay the inevitable demise of his 2010 Volkswagen Golf.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514913</guid><pubDate>Tue, 06 Jan 2026 16:59:44 +0000</pubDate></item><item><title>Opus 4.5 is not the normal AI agent experience that I have had thus far</title><link>https://burkeholland.github.io/posts/opus-4-5-change-everything/</link><description>&lt;doc fingerprint="26499875abe73fe9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Opus 4.5 is going to change everything&lt;/head&gt;
    &lt;p&gt;If you had asked me three months ago about these statements, I would have said only someone who‚Äôs never built anything non-trivial would believe they‚Äôre true. Great for augmenting a developer‚Äôs existing workflow, and completions are powerful, but agents replacing developers entirely? No. Absolutely not.&lt;/p&gt;
    &lt;p&gt;Today, I think that AI coding agents can absolutely replace developers. And the reason that I believe this is Claude Opus 4.5.&lt;/p&gt;
    &lt;head rend="h2"&gt;Opus 4.5 is not normal&lt;/head&gt;
    &lt;p&gt;And by ‚Äúnormal‚Äù, I mean that it is not the normal AI agent experience that I have had thus far. So far, AI Agents seem to be pretty good at writing spaghetti code and after 9 rounds of copy / paste errors into the terminal and ‚Äúfix it‚Äù have probably destroyed my codebase to the extent that I‚Äôll be throwing this whole chat session out and there goes 30 minutes I‚Äôm never getting back.&lt;/p&gt;
    &lt;p&gt;Opus 4.5 feels to me like the model that we were promised - or rather the promise of AI for coding actually delivered.&lt;/p&gt;
    &lt;p&gt;One of the toughest things about writing that last sentence is that the immediate response from you should be, ‚Äúprove it‚Äù. So let me show you what I‚Äôve been able to build.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 1 - Windows Image Conversion Utility&lt;/head&gt;
    &lt;p&gt;I first noticed that Opus 4.5 was drastically different when I used it to build a Windows utility to right-click an image and convert it to different file types. This was basically a one shot build after asking Opus the best way to add a right-click menu to the file explorer.&lt;/p&gt;
    &lt;p&gt;What amazed me through the process of building this was Opus 4.5 ability to get most things right on the first try. And if it ran into errors, it would try and build using the dotnet CLI, read the errors and iterate until fixed. The only issue I had was Opus inability to see XAML errors, which I used Visual Studio to see and copy / paste back into the agent.&lt;/p&gt;
    &lt;p&gt;Opus built a site for me to distribute it and handled the bundling of the executable so as to use a powershell script for the install, uninstall. It also built the GitHub Actions which do the release and update the landing page so that all I have to do is push source.&lt;/p&gt;
    &lt;p&gt;The only place I had to use other tools was for the logo - where I used Figma‚Äôs AI to generate a bunch of different variations - but then Opus wrote the scripts to convert that SVG to the right formats for icons, even store distribution if I chose to do so.&lt;/p&gt;
    &lt;p&gt;Now this is admittedly not a complex application. This is a small Windows utility that is doing basically one thing. It‚Äôs not like I asked Opus 4.5 to build Photoshop.&lt;/p&gt;
    &lt;p&gt;Except I kind of did.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 2 - Screen recording / editing&lt;/head&gt;
    &lt;p&gt;I was so impressed by Opus 4.5 work on this utility that I decided to make a simple GIF recording utility similar to LICEcap for Mac. Great app, questionable name.&lt;/p&gt;
    &lt;p&gt;But that proved to be so easy, that I went ahead and continued adding features, including capturing and editing video, static images, adding shapes, cropping, blurs and more. I‚Äôm still working on this application as it turns out building a full on image/video editor is kind of a big undertaking. But I got REALLY far in a matter of hours. HOURS, PEOPLE.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have a fancy landing page for this one yet, but you can view all of the source code here.&lt;/p&gt;
    &lt;p&gt;I realized that if I could build a video recording app, I could probably build anything at all - at least UI-wise. But the achilles heel of all AI agents is when they have to glue together backend systems - which any real world application is going to have - auth, database, API, storage.&lt;/p&gt;
    &lt;p&gt;Except Opus 4.5 can do that too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 3 - AI Posting Utility&lt;/head&gt;
    &lt;p&gt;Armed with my confidence in Opus 4.5, I took on a project that I had built in React Native last year and finished for Android, but gave up in the final stretches (as one does).&lt;/p&gt;
    &lt;p&gt;The application is for my wife who owns a small yard sign franchise. The problem is that she has a Facebook page for the business, but never posts there because it‚Äôs time consuming. But any good small business has a vibrant page where people can see photos of your business doing‚Ä¶whatever the heck it does. So people know that it exsits and is alive and well.&lt;/p&gt;
    &lt;p&gt;The idea is simple - each time she sets up a yard sign, she takes a picture to send to the person who ordered it so they can see it was setup. So why not have a mobile app where she can upload 10 images at a time, and the app will use AI to generate captions and then schedule them and post them over the coming week.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a simple premise, but it has a lot of moving parts - there is the Facebook authentication which is a caper in and of itself - not for the faint of heart. There is authentication with a backend, there is file storage for photos that are scheduled to go out, there is the backend process which needs to post the photo. It‚Äôs a full on backend setup.&lt;/p&gt;
    &lt;p&gt;As it turns out, I needed to install some blinds in the house so I thought - why don‚Äôt I see if Opus 4.5 can build this while I install the blinds.&lt;/p&gt;
    &lt;p&gt;So I fired up a chat session and just started by telling Opus 4.5 what I wanted to build and how it would recommend handling the backend. It recommended several options but settled on Firebase. I‚Äôm not now nor have I ever been a Firebase user, but at this point I trust Opus 4.5 a lot. Probably too much.&lt;/p&gt;
    &lt;p&gt;So I created a Firebase account, upgraded to the Blaze plan with alerts for billing and Opus 4.5 got to work.&lt;/p&gt;
    &lt;p&gt;By the time I was done installing blinds, I had a functional iOS application for using AI to caption photos and posting them on a schedule to Facebook.&lt;/p&gt;
    &lt;p&gt;When I say that Opus 4.5 built this almost entirely, I mean it. It used the &lt;code&gt;firebase&lt;/code&gt; CLI to stand up any resources it needed and would tag me in for certain things like upgrading a project to the Blaze plan for features like storage, etc. The best part was that when the Firebase cloud functions would throw errors, it would automatically grep those logs, find the error and resolve it. And all it needed was a CLI. No MCP Server. No fancy prompt file telling it how to use Firebase.&lt;/p&gt;
    &lt;p&gt;And of course, since it‚Äôs solved, I had Opus 4.5 create a backend admin dashboard so I could see what she‚Äôs got pending and make any adjustments.&lt;/p&gt;
    &lt;p&gt;And since it did in a few hours what had taken me two months of work in the evenings instead of being a decent husband, I decided to make up for my dereliction of duties by building her another app for her sign business that would make her life just a bit more delightful - and eliminate two other apps she is currently paying for.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project 4 - Order tracking and routing&lt;/head&gt;
    &lt;p&gt;This app parses orders from her business Gmail account to show her what sign setups / pickups she has for the day, calculates how long its going to take to go to each stop, calculates the optimal route when there is more than one stop and tracks drive time for tax purposes. She was previously using two paid apps for the last two features there.&lt;/p&gt;
    &lt;p&gt;This app also uses Firebase. Again, Opus one-shotted the Google auth email integration. This is the kind of thing that is painstakingly miserable by hand. And again, Firebase is so well suited here because Opus knows how to use the Firebase CLI so well. It needs zero instruction.&lt;/p&gt;
    &lt;head rend="h3"&gt;BUT YOU DON‚ÄôT KNOW HOW THE CODE WORKS&lt;/head&gt;
    &lt;p&gt;No I don‚Äôt. I have a vague idea, but you are right - I do not know how the applications are actually assembled. Especially since I don‚Äôt know Swift at all.&lt;/p&gt;
    &lt;p&gt;This used to be a major hangup for me. I couldn‚Äôt diagnose problems when things went sideways. With Opus 4.5, I haven‚Äôt hit that wall yet‚ÄîOpus always figures out what the issue is and fixes its own bugs.&lt;/p&gt;
    &lt;p&gt;The real question is code quality. Without understanding how it‚Äôs built, how do I know if there‚Äôs duplication, dead code, or poor patterns? I used to obsess over this. Now I‚Äôm less worried that a human needs to read the code, because I‚Äôm genuinely not sure that they do.&lt;/p&gt;
    &lt;p&gt;Why does a human need to read this code at all? I use a custom agent in VS Code that tells Opus to write code for LLMs, not humans. Think about it‚Äîwhy optimize for human readability when the AI is doing all the work and will explain things to you when you ask?&lt;/p&gt;
    &lt;p&gt;What you don‚Äôt need: variable names, formatting, comments meant for humans, or patterns designed to spare your brain.&lt;/p&gt;
    &lt;p&gt;What you do need: simple entry points, explicit code with fewer abstractions, minimal coupling, and linear control flow.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs my custom agent prompt:&lt;/p&gt;
    &lt;code&gt;You are an AI-first software engineer. Assume all code will be written and maintained by LLMs, not humans. Optimize for model reasoning, regeneration, and debugging ‚Äî not human aesthetics.

These coding principles are mandatory:

1. Structure
- Use a consistent, predictable project layout.
- Group code by feature/screen; keep shared utilities minimal.
- Create simple, obvious entry points.
- Before scaffolding multiple files, identify shared structure first. Use framework-native composition patterns (layouts, base templates, providers, shared components) for elements that appear across pages. Duplication that requires the same fix in multiple places is a code smell, not a pattern to preserve.

2. Architecture
- Prefer flat, explicit code over abstractions or deep hierarchies.
- Avoid clever patterns, metaprogramming, and unnecessary indirection.
- Minimize coupling so files can be safely regenerated.

3. Functions and Modules
- Keep control flow linear and simple.
- Use small-to-medium functions; avoid deeply nested logic.
- Pass state explicitly; avoid globals.

4. Naming and Comments
- Use descriptive-but-simple names.
- Comment only to note invariants, assumptions, or external requirements.

5. Logging and Errors
- Emit detailed, structured logs at key boundaries.
- Make errors explicit and informative.

6. Regenerability
- Write code so any file/module can be rewritten from scratch without breaking the system.
- Prefer clear, declarative configuration (JSON/YAML/etc.).

7. Platform Use
- Use platform conventions directly and simply (e.g., WinUI/WPF) without over-abstracting.

8. Modifications
- When extending/refactoring, follow existing patterns.
- Prefer full-file rewrites over micro-edits unless told otherwise.

9. Quality
- Favor deterministic, testable behavior.
- Keep tests simple and focused on verifying observable behavior.

Your goal: produce code that is predictable, debuggable, and easy for future LLMs to rewrite or extend.
&lt;/code&gt;
    &lt;p&gt;All of that said, I don‚Äôt have any proof that this prompt makes a difference. I find that Opus 4.5 writes pretty solid code no matter what you prompt it with. However, because models like to write code WAY more than they like to delete it, I will at points run a prompt that looks something like this‚Ä¶&lt;/p&gt;
    &lt;code&gt;Check your LLM, AI coding principles and then do a comprehensive search of this application and suggest what we can do to refactor this to better align to those principles. Also point out any code that can be deleted, any files that can be deleted, things that should read should be renamed, things that should be restructured. Then do a write up of what that looks like. Kind of keep it high level so that it's easy for me to read and not too complex. Add sections for high, medium and lower priority And if something doesn't need to be changed, then don't change it. You don't need to change things just for the sake of changing them. You only need to change them if it helps better align to your LLM AI coding principles. Save to a markdown file.
&lt;/code&gt;
    &lt;p&gt;And you get a document that has high, medium and low priority items. The high ones you can deal with and the AI will stop finding them. You can refactor your project a million times and it will keep finding medium/low priority refactors that you can do. An AI is never ever going to pass on the opportunity to generate some text.&lt;/p&gt;
    &lt;p&gt;I use a similar prompt to find security issues. These you have to be very careful about. Where are the API keys? Is login handled correctly? Are you storing sensitive values in the database? This is probably the most manual part of the project and frankly, something that makes me the most nervous about all of these apps at the moment. I‚Äôm not 100% confident that they are bullet proof. Maybe like 80%. And that, as they say, is too damn low.&lt;/p&gt;
    &lt;head rend="h3"&gt;Times they are A-changin&lt;/head&gt;
    &lt;p&gt;I don‚Äôt know if I feel exhilarated by what I can now build in a matter of hours, or depressed because the thing I‚Äôve spent my life learning to do is now trivial for a computer. Both are true.&lt;/p&gt;
    &lt;p&gt;I understand if this post made you angry. I get it - I didn‚Äôt like it either when people said ‚ÄúAI is going to replace developers.‚Äù But I can‚Äôt dismiss it anymore. I can wish it weren‚Äôt true, but wishing doesn‚Äôt change reality.&lt;/p&gt;
    &lt;p&gt;But for everything else? Build. Stop waiting to have all the answers. Stop trying to figure out your place in an AI-first world. The answer is the same as it always was: make things. And now you can make them faster than you ever thought possible.&lt;/p&gt;
    &lt;p&gt;Just make sure you know where your API keys are.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: This post was written by a human and edited for spelling, grammer by Haiku 4.5&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515696</guid><pubDate>Tue, 06 Jan 2026 17:45:37 +0000</pubDate></item><item><title>Launch HN: Tamarind Bio (YC W24) ‚Äì AI Inference Provider for Drug Discovery</title><link>https://news.ycombinator.com/item?id=46515777</link><description>&lt;doc fingerprint="6f51ad5b7d3645cb"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we're Deniz and Sherry from Tamarind Bio (&lt;/p&gt;https://www.tamarind.bio&lt;p&gt;). Tamarind is an inference provider for AI drug discovery, serving models like AlphaFold. Biopharma companies use our library of leading open-source models to design new medicines computationally.&lt;/p&gt;&lt;p&gt;Here‚Äôs a demo: https://youtu.be/luoMApPeglo&lt;/p&gt;&lt;p&gt;Two years ago, I was hired at a Stanford lab to run models for my labmates. Some post-doc would ask me to run a set of 1-5 models in sequence with tens of thousands inputs and I would email them back the result after setting up the workflow in the university cluster.&lt;/p&gt;&lt;p&gt;At some point, it became unreasonable that all of an organization's computational biology work would go through an undergrad, so we built Tamarind as a single place for all molecular AI tools, usable at massive scale with no technical background needed. Today, we are used by much of the top 20 pharma, dozens of biotechs and tens of thousands of scientists.&lt;/p&gt;&lt;p&gt;When we started getting adoption in the big pharma companies, we found that this problem also persisted. I know directors of data science, where half their job could be described as running scripts for other people.&lt;/p&gt;&lt;p&gt;Lots of companies have also deprecated their internally built solution to switch over, dealing with GPU infra and onboarding docker containers not being a very exciting problem when the company you work for is trying to cure cancer.&lt;/p&gt;&lt;p&gt;Unlike non-specialized inference providers, we build both a programmatic interface for developers along with a scientist-friendly web app, since most of our users are non-technical. Some of them used to extract proteins from animal blood before replacing that process with using AI to generate proteins on Tamarind.&lt;/p&gt;&lt;p&gt;Besides grinding out images for each of the models we serve, we‚Äôve designed a standardized schema to be able to share each model‚Äôs data format. We‚Äôve built a custom scheduler and queue optimized for horizontal scaling (each inference call takes minutes to hours, and runs on one GPU at a time), while splitting jobs across CPUs and GPUs for optimal timing.&lt;/p&gt;&lt;p&gt;As we've grown to handle a substantial portion of the biopharma R&amp;amp;D AI demand on behalf of our customers, we've expanded beyond just offering a library of open source protocols.&lt;/p&gt;&lt;p&gt;A common use case we saw from early on was the need to connect multiple models together into pipelines, and having reproducible, consistent protocols to replace physical experiments. Once we became the place to build internal tools for computational science, our users started asking if they could onboard their own models to the platform.&lt;/p&gt;&lt;p&gt;From there, we now support fine-tuning, building UIs for arbitrary docker containers, connecting to wet lab data sources and more!&lt;/p&gt;&lt;p&gt;Reach out to me at deniz[at]tamarind.bio if you‚Äôre interested in our work, we are hiring! Check out our product at https://app.tamarind.bio and let us know if you have any feedback to support how the biotech industry uses AI today.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515777</guid><pubDate>Tue, 06 Jan 2026 17:49:56 +0000</pubDate></item><item><title>Dude, where's my supersonic jet?</title><link>https://rationaloptimistsociety.substack.com/p/dude-wheres-my-supersonic-jet</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515936</guid><pubDate>Tue, 06 Jan 2026 17:59:41 +0000</pubDate></item><item><title>Locating a Photo of a Vehicle in 30 Seconds with GeoSpy</title><link>https://geospy.ai/blog/locating-a-photo-of-a-vehicle-in-30-seconds-with-geospy</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515948</guid><pubDate>Tue, 06 Jan 2026 18:00:27 +0000</pubDate></item><item><title>Hierarchical Autoregressive Modeling for Memory-Efficient Language Generation</title><link>https://arxiv.org/abs/2512.20687</link><description>&lt;doc fingerprint="889c3c0ad4d2528e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 22 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\times$ higher throughput per unit memory.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515987</guid><pubDate>Tue, 06 Jan 2026 18:02:01 +0000</pubDate></item><item><title>Passing of Joe Mancuso author of Masonite (Python web framework)</title><link>https://github.com/MasoniteFramework/masonite/discussions/853</link><description>&lt;doc fingerprint="2c52725b5bc5a72e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Passing of Joe Mancuso #853&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Good morning Masonite community,&lt;/p&gt;
          &lt;p&gt;I regret to inform you all that @josephmancuso has passed away due to health complications. Please keep his family in your thoughts during this time.&lt;/p&gt;
          &lt;p&gt;I had the privilege of working alongside Joe for many years, and it was clear as day how much Masonite meant to him. Even when fighting for his life, he continued doing everything he could to maintain and support this project.&lt;/p&gt;
          &lt;p&gt;One of the beautiful things about open source is that we build together. While Joe is no longer with us, Masonite can continue to grow and evolve through the contributions of this community. I hope we all continue working toward the vision he poured so much of himself into.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;So sad, my condolences.üòû&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This is heartbreaking. I also had the privilege of working with Joe a couple of months ago, and he was always bringing new ideas to improve and share within the open-source community.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46516137</guid><pubDate>Tue, 06 Jan 2026 18:11:51 +0000</pubDate></item><item><title>Video Game Websites in the early 00s</title><link>https://www.webdesignmuseum.org/exhibitions/video-game-websites-in-the-early-00s</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46516559</guid><pubDate>Tue, 06 Jan 2026 18:39:21 +0000</pubDate></item><item><title>High-Performance DBMSs with io_uring: When and How to use it</title><link>https://arxiv.org/abs/2512.04859</link><description>&lt;doc fingerprint="b89f341a58d3848a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Databases&lt;/head&gt;&lt;p&gt; [Submitted on 4 Dec 2025 (v1), last revised 12 Dec 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:High-Performance DBMSs with io_uring: When and How to use it&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We study how modern database systems can leverage the Linux io_uring interface for efficient, low-overhead I/O. io_uring is an asynchronous system call batching interface that unifies storage and network operations, addressing limitations of existing Linux I/O interfaces. However, naively replacing traditional I/O interfaces with io_uring does not necessarily yield performance benefits. To demonstrate when io_uring delivers the greatest benefits and how to use it effectively in modern database systems, we evaluate it in two use cases: Integrating io_uring into a storage-bound buffer manager and using it for high-throughput data shuffling in network-bound analytical workloads. We further analyze how advanced io_uring features, such as registered buffers and passthrough I/O, affect end-to-end performance. Our study shows when low-level optimizations translate into tangible system-wide gains and how architectural choices influence these benefits. Building on these insights, we derive practical guidelines for designing I/O-intensive systems using io_uring and validate their effectiveness in a case study of PostgreSQL's recent io_uring integration, where applying our guidelines yields a performance improvement of 14%.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Matthias Jasny [view email]&lt;p&gt;[v1] Thu, 4 Dec 2025 14:43:03 UTC (504 KB)&lt;/p&gt;&lt;p&gt;[v2] Fri, 12 Dec 2025 09:44:22 UTC (505 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46517319</guid><pubDate>Tue, 06 Jan 2026 19:29:15 +0000</pubDate></item><item><title>Stop Doom Scrolling, Start Doom Coding: Build via the terminal from your phone</title><link>https://github.com/rberg27/doom-coding</link><description>&lt;doc fingerprint="d11490dd3b96cae3"&gt;
  &lt;main&gt;
    &lt;p&gt;A DIY approach to coding on-the-go!&lt;/p&gt;
    &lt;p&gt;As an aspiring builder, I sought out a way to keep coding while not at home. Thanks to some Claude-assisted research and troubleshooting, I can now code via the terminal on my phone anywhere at anytime via "Doom Coding" (think Doom Scrolling but more productive).&lt;/p&gt;
    &lt;p&gt;After this 5-minute setup guide, you'll be able to "doom code" anywhere you have Internet connection.&lt;/p&gt;
    &lt;p&gt;I've been amazed by how much I can get done while being so far away from home. In Taiwan, I could access my computer in Philadelphia and coded a prototype in my downtime.&lt;/p&gt;
    &lt;p&gt;Shameless plug: check out www.friendlyr.ai to help shape the future of connection!&lt;/p&gt;
    &lt;p&gt;Make sure to "Watch" this repo for future updates to this doom coding guide. As I tryout the latest mobile coding tools (e.g. Claude Code on the Web), I'll update this repository with comparisons.&lt;/p&gt;
    &lt;p&gt;Happy doom coding my friends!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A Computer running 24/7 with Internet Connection&lt;/item&gt;
      &lt;item&gt;A Smartphone&lt;/item&gt;
      &lt;item&gt;A Claude Pro subscription&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use Tailscale, Termius, Claude Code, and a computer running 24/7 to continue building anywhere you have Internet connection.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable sleep in power settings&lt;/item&gt;
      &lt;item&gt;Enable SSH/Remote Login&lt;/item&gt;
      &lt;item&gt;Install Tailscale and sign in&lt;lb/&gt;https://tailscale.com/download&lt;/item&gt;
      &lt;item&gt;Install Claude Code on your computer&lt;lb/&gt;https://docs.anthropic.com/en/docs/claude-code/overview&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Install Tailscale ‚Üí Sign in with the same account&lt;/p&gt;&lt;lb/&gt;https://apps.apple.com/us/app/tailscale/id1470499037&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Install Termius (A Mobile Terminal Tool)&lt;/p&gt;&lt;lb/&gt;https://apps.apple.com/us/app/termius-modern-ssh-client/id549039908&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Note the MagicDNS address of your computer (e.g. my-computer.tailnet-name.ts.net)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create a new host in Termius:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Label: What you want your connection to be called&lt;/item&gt;
          &lt;item&gt;Hostname: The MagicDNS address (my-computer.tailnet-name.ts.net)&lt;/item&gt;
          &lt;item&gt;Port: 22&lt;/item&gt;
          &lt;item&gt;Username/Password: Your login for your computer &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you're not able to establish a connection from your phone via Termius to your computer:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check your phone settings to make sure you are connected to the Tailscale VPN. &lt;/item&gt;
      &lt;item&gt;Check the Tailscale app to make sure the Tailscale VPN is on. If your phone and doom coding computer do not have a green circle next to their labels, there is an issue with your Tailscale/Internet connection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When disconnecting/reconnecting power, make sure you unlock the computer. I've ran into this issue one too many times.&lt;/p&gt;
    &lt;p&gt;End sessions by asking Claude to update CLAUDE.md with where you left off.&lt;/p&gt;
    &lt;p&gt;Go to your desired directory and start an HTTP server&lt;code&gt;python -m http.server 3005&lt;/code&gt;
then visit http://your-machine.tailnet-name.ts.net:3005/your-html-file.html in a browser on your phone.&lt;/p&gt;
    &lt;p&gt;Wherever you would use localhost:PORT to view an app on your computer, replace localhost with the computer's MagicDNS from the Tailscale app (e.g. your-computer.tailnet-name.ts.net)&lt;/p&gt;
    &lt;p&gt;Use the PostgreSQL app to view databases for your projects https://apps.apple.com/us/app/postgresql-client/id1233662353&lt;/p&gt;
    &lt;p&gt;On your computer, bookmark the sites you refer to during development (e.g. Google OAuth, GitHub) to make it easier to reference from your phone. I use the Chrome app to seamlessly access the sites I need.&lt;/p&gt;
    &lt;p&gt;Please contibute your best practices! I am looking forward to seeing all the places you will code!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46517458</guid><pubDate>Tue, 06 Jan 2026 19:38:32 +0000</pubDate></item><item><title>Self hosting my media library with Jellyfin and Wireguard on Hetzner</title><link>https://layandreas.github.io/personal-blog/posts/how-spotify-made-me-self-host/</link><description>&lt;doc fingerprint="fd11bd7d192258f0"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: This post was written without the help of AI. Research for the self hosting setup (which docker images to use, Jellyfin vs. Plex) and the creation of initial configurations (docker-compose, wireguard) were supported by AI (among others like e.g. Reddit, Stackoverflow)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;How It All Started&lt;/head&gt;
    &lt;p&gt;If you don‚Äôt care about my motivations and are only interested in the technical setup just directly jump to Self Hosting Setup!&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotify Price Increase&lt;/head&gt;
    &lt;p&gt;In August 2025 Spotify announced a price increase in Germany. I received a friendly message that while I was a valued Premium subscriber, my subscription price would change (some might say increase):&lt;/p&gt;
    &lt;p&gt;For the time being I decided to sit it out. So November 2025 arrived and my Premium membership ended.&lt;/p&gt;
    &lt;p&gt;I decided to give Spotify‚Äôs free plan a try. How bad can it be? I had used the free tier 10 years ago and it was perfectly fine. How wrong I was!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The free tier forces shuffle after a certain amount of free choices. So while the tier might be free, your song choice is not. Although there are reports that Spotify supposedly got rid of this feature as of September 2025, I still encountered this restriction as of end of December 2025&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It‚Äôs not possible to scrub the song‚Äôs progress bar&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I don‚Äôt remember these restrictions being in place back in the day, but I may be wrong. It might be a case of enshittification of the free tier?&lt;/p&gt;
    &lt;p&gt;Okay, fair enough: Companies need to make money, I‚Äôm not entitled to Spotify providing their services for free! For all those services I still pay for I certainly get an awesome user experience?&lt;/p&gt;
    &lt;p&gt;Not so fast‚Ä¶&lt;/p&gt;
    &lt;head rend="h3"&gt;Across All Streaming Services: Less Value For More Money&lt;/head&gt;
    &lt;p&gt;Across all streaming services I have noticed a decline in user friendliness, UX &amp;amp; UI.&lt;/p&gt;
    &lt;head rend="h4"&gt;Price increases&lt;/head&gt;
    &lt;p&gt;All streaming services have seen price increases and / or introduced ads. I get it, companies need to make money. My day job consists of helping companies increase their revenues, it pays my bills!&lt;/p&gt;
    &lt;p&gt;The issue: These price increases are accompanied by a worse UX/UI. From a user perspective my interest lies in getting the best value for money!&lt;/p&gt;
    &lt;head rend="h4"&gt;Ads&lt;/head&gt;
    &lt;p&gt;Amazon Prime introduced ads to an already paid service. This is a noticeable worse user experience. Netflix introduced a paid plan with ads. At least for me YouTube advertises mostly mobile games &amp;amp; obvious scams (ever gotten one of these terrible liven app ads?)&lt;/p&gt;
    &lt;head rend="h4"&gt;UI&lt;/head&gt;
    &lt;p&gt;In May 2025 Netflix introduced a new UI. It‚Äôs a matter of taste and while some people may like it, I find it much worse.&lt;/p&gt;
    &lt;head rend="h4"&gt;Cracking Down on Password Sharing&lt;/head&gt;
    &lt;p&gt;Disney+ followed Netflix and cracked down on password sharing. This is a de facto price increase for many.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self Hosting Setup&lt;/head&gt;
    &lt;p&gt;So I decided to give self hosting a try. I‚Äôve followed the awesome selfhosted subreddit for a while anyway and own a collection of shows, movies &amp;amp; music anyway.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deployment: Hetzner VPS&lt;/head&gt;
    &lt;p&gt;Instead of going with my own hardware I decided to go with a VPS from Hetzner. I chose a CAX21 with 4 VCPUs, 8GB RAM, 20 TB of traffic included and 80 GB of SSD storage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage: Hetzner Storage Box&lt;/head&gt;
    &lt;p&gt;While I could use the server‚Äôs SSD to store my media, I decided to use Hetzner‚Äôs Storage Box. This way I can easily mount my media library on my Macbook via SMB.&lt;/p&gt;
    &lt;head rend="h3"&gt;Media Server: Jellyfin&lt;/head&gt;
    &lt;p&gt;I opted to use the open source Jellyfin as my media server. Plex would be an alternative but it appears to have fallen out of favour and blocks Hetzner anyway, so it would not work for me anyway.&lt;/p&gt;
    &lt;head rend="h3"&gt;Remote Access: VPN Tunnel via WireGuard&lt;/head&gt;
    &lt;p&gt;To access my VPS remotely either from home or on the road I use WireGuard:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Home network: My FRITZ!Box router lets me easily add a WireGuard configuration to my home network. This way any device in my home network can access my media network without having to run a VPN. This is especially useful for accessing Jellyfin on my LG webOS TV as there‚Äôs no easy way to connect my TV to WireGuard directly&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On the road: Simply install WireGuard on your device. You will need to create a client configuration for each of your devices&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Server Config&lt;/head&gt;
    &lt;code&gt;[Interface]
Address = 10.13.13.1/24
ListenPort = 51820
PrivateKey = {{ wireguard_server_private_key }}

PostUp   = iptables -A INPUT -i wg0 -p tcp --dport 8096 -j ACCEPT; iptables -A FORWARD -i wg0 -j ACCEPT
PostDown = iptables -D INPUT -i wg0 -p tcp --dport 8096 -j ACCEPT; iptables -D FORWARD -i wg0 -j ACCEPT

[Peer]
PublicKey = {{ wireguard_client_public_key }}
AllowedIPs = 10.13.13.2/32

[Peer]
PublicKey = {{ wireguard_client_public_key_iphone }}
AllowedIPs = 10.13.13.3/32

[Peer]
PublicKey = {{ wireguard_client_public_key_fritzbox }}
AllowedIPs = 10.13.13.4/32
&lt;/code&gt;
    &lt;p&gt;Peers&lt;/p&gt;
    &lt;p&gt;In my WireGuardserver config I currently have three devices added as peers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My Macbook (&lt;code&gt;wireguard_client_public_key&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;My iPhone (&lt;code&gt;wireguard_client_public_key_iphone&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;My FRITZ!Box router (&lt;code&gt;wireguard_client_public_key_fritzbox&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each device has its corresponding private key stored inside its WireGuard configuration / app. The &lt;code&gt;AllowedIPs&lt;/code&gt; only allows the corresponding peer to use this internal IP address. On your device (router, notebook, mobile phone, ‚Ä¶) you‚Äôll need to assign this IP address as the interface section.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs dive a bit deeper into the interface block.&lt;/p&gt;
    &lt;p&gt;Interface&lt;/p&gt;
    &lt;p&gt;Configuration for the network interface created on the server.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Address = 10.13.13.1/24&lt;/code&gt;: This is the address of the Hetzner server in our VPN&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ListenPort = 51820&lt;/code&gt;: UDP port number the WireGuard server listens on for incoming VPN connections&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PrivateKey = {{ wireguard_server_private_key }}&lt;/code&gt;: WireGuard uses this private key to identify itself to clients (via the public key which is shared)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PostUp / PostDown&lt;/code&gt;: Port forwarding - we allow WireGuard to forward incoming traffic&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Client Config&lt;/head&gt;
    &lt;p&gt;Here‚Äôs an example how a client config on my Macbook looks like:&lt;/p&gt;
    &lt;p&gt;You‚Äôll have a separate configuration for each of your devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Docker Compose&lt;/head&gt;
    &lt;p&gt;And finally here‚Äôs the compose config for the actualy deployment on the VPS:&lt;/p&gt;
    &lt;code&gt;version: "3.9"

services:
  wireguard:
    image: linuxserver/wireguard:latest
    container_name: wireguard
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - SERVERPORT=51820
      - SERVERURL=&amp;lt;SERVERS-IP-ADDRESS&amp;gt;
    volumes:
      - ./wireguard/config:/config
      - /lib/modules:/lib/modules:ro
    ports:
      - "51820:51820/udp"
    sysctls:
      - net.ipv4.conf.all.src_valid_mark=1
    restart: unless-stopped

  jellyfin:
    image: jellyfin/jellyfin:latest
    container_name: jellyfin
    network_mode: "service:wireguard"
    user: 1000:1000
    volumes:
      - ./jellyfin/config:/config
      - /mnt/storagebox:/media:ro
    environment:
      - TZ=UTC
    restart: unless-stopped
    depends_on:
      - wireguard
&lt;/code&gt;
    &lt;p&gt;Some notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wireguard:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;51820:51820/udp&lt;/code&gt;: We only expose WireGuard UDP port&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Jellyfin:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/mnt/storagebox:/media:ro&lt;/code&gt;: Mounting my Hetzner storage box with my media library into the container&lt;/item&gt;&lt;item&gt;&lt;code&gt;network_mode: "service:wireguard"&lt;/code&gt;: Shares the WireGuard container‚Äôs network stack&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;So Is Self Hosting a Alternative to Streaming Services?&lt;/head&gt;
    &lt;p&gt;It depends. My personal media library isn‚Äôt big enough that it could rival any of the streaming services libraries. You also need to put in the work to get your self-hosted setup running which non-technical people won‚Äôt do, and even for technical people it might not be worth it. For me personally it‚Äôs totally worth it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46517636</guid><pubDate>Tue, 06 Jan 2026 19:50:35 +0000</pubDate></item><item><title>Show HN: Finding similarities in New Yorker covers</title><link>https://shoplurker.com/labs/newyorker-covers/</link><description>&lt;doc fingerprint="ab25d1531171e208"&gt;
  &lt;main&gt;
    &lt;p&gt;2025-12-29 Lorenzo Mattotti&lt;/p&gt;
    &lt;p&gt;2025-12-22 Luci Guti√©rrez&lt;/p&gt;
    &lt;p&gt;2025-12-15 Pierre-Emmanuel Lyet&lt;/p&gt;
    &lt;p&gt;2025-12-08 Klaas Verplancke&lt;/p&gt;
    &lt;p&gt;2025-12-01 Malika Favre, and ‚ÄúEustace Tilley,‚Äù by Rea Irvin&lt;/p&gt;
    &lt;p&gt;2025-11-24 Kenton Nelson&lt;/p&gt;
    &lt;p&gt;2025-11-17 Edel Rodriguez&lt;/p&gt;
    &lt;p&gt;2025-11-10 Sergio Garc√≠a S√°nchez&lt;/p&gt;
    &lt;p&gt;2025-11-03 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2025-10-27 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-10-20 Harry Bliss&lt;/p&gt;
    &lt;p&gt;2025-10-13 Brian Stauffer&lt;/p&gt;
    &lt;p&gt;2025-10-06 R. Kikuo Johnson&lt;/p&gt;
    &lt;p&gt;2025-09-29 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-09-22 Maira Kalman&lt;/p&gt;
    &lt;p&gt;2025-09-15 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2025-09-01 Cindy Sherman (Cover 1); ‚ÄúEustace Tilley,‚Äù by Rea Irvin (Cover 2)&lt;/p&gt;
    &lt;p&gt;2025-08-25 Sergio Garc√≠a S√°nchez and Lola Moral&lt;/p&gt;
    &lt;p&gt;2025-08-18 Lorenzo Mattotti&lt;/p&gt;
    &lt;p&gt;2025-08-11 Amy Sherald&lt;/p&gt;
    &lt;p&gt;2025-08-04 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2025-07-28 Sergio Garc√≠a S√°nchez and Lola Moral&lt;/p&gt;
    &lt;p&gt;2025-07-21 Joost Swarte&lt;/p&gt;
    &lt;p&gt;2025-07-07 Malika Favre&lt;/p&gt;
    &lt;p&gt;2025-06-30 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-06-23 David Plunkert&lt;/p&gt;
    &lt;p&gt;2025-06-16 Haruka Aoki&lt;/p&gt;
    &lt;p&gt;2025-06-09 David Hockney&lt;/p&gt;
    &lt;p&gt;2025-06-02 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2025-05-26 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-05-12 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-05-05 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-04-28 Adrian Tomine&lt;/p&gt;
    &lt;p&gt;2025-04-21 Frank Viva&lt;/p&gt;
    &lt;p&gt;2025-04-14 Richard McGuire&lt;/p&gt;
    &lt;p&gt;2025-04-07&lt;/p&gt;
    &lt;p&gt;2025-03-31&lt;/p&gt;
    &lt;p&gt;2025-03-24 Amy Sherald&lt;/p&gt;
    &lt;p&gt;2025-03-17 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2025-03-10 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2025-03-03 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-02-17&lt;/p&gt;
    &lt;p&gt;2025-02-10 Tom Gauld&lt;/p&gt;
    &lt;p&gt;2025-02-03 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2025-01-27 Till Lauer&lt;/p&gt;
    &lt;p&gt;2025-01-20 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2025-01-13 Roz Chast&lt;/p&gt;
    &lt;p&gt;2024-12-30 Diana Ejaita&lt;/p&gt;
    &lt;p&gt;2024-12-23 Kate Beaton&lt;/p&gt;
    &lt;p&gt;2024-12-16 Eric Drooker&lt;/p&gt;
    &lt;p&gt;2024-12-09 John Cuneo&lt;/p&gt;
    &lt;p&gt;2024-12-02 Tom Toro&lt;/p&gt;
    &lt;p&gt;2024-11-25 Javier Mariscal&lt;/p&gt;
    &lt;p&gt;2024-11-18 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-11-11 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-11-04 Lorenzo Mattotti&lt;/p&gt;
    &lt;p&gt;2024-10-28 Eric Drooker&lt;/p&gt;
    &lt;p&gt;2024-10-21 Owen Smith&lt;/p&gt;
    &lt;p&gt;2024-10-14 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2024-10-07 Malika Favre&lt;/p&gt;
    &lt;p&gt;2024-09-30 Pierre-Emmanuel Lyet&lt;/p&gt;
    &lt;p&gt;2024-09-23 Christoph Niemann&lt;/p&gt;
    &lt;p&gt;2024-09-16 Mark Ulriksen&lt;/p&gt;
    &lt;p&gt;2024-09-09 R. Kikuo Johnson&lt;/p&gt;
    &lt;p&gt;2024-09-02 Pascal Campion&lt;/p&gt;
    &lt;p&gt;2024-08-26 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-08-19 Charles Addams&lt;/p&gt;
    &lt;p&gt;2024-08-12 Roz Chast&lt;/p&gt;
    &lt;p&gt;2024-08-05 Gayle Kabaker&lt;/p&gt;
    &lt;p&gt;2024-07-29 Paul Rogers&lt;/p&gt;
    &lt;p&gt;2024-07-22 Anita Kunz&lt;/p&gt;
    &lt;p&gt;2024-07-08&lt;/p&gt;
    &lt;p&gt;2024-07-08 Kadir Nelson&lt;/p&gt;
    &lt;p&gt;2024-07-01 Klaas Verplancke&lt;/p&gt;
    &lt;p&gt;2024-06-24 Adrian Tomine&lt;/p&gt;
    &lt;p&gt;2024-06-17 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2024-06-10 John Cuneo&lt;/p&gt;
    &lt;p&gt;2024-06-03 Sergio Garc√≠a S√°nchez&lt;/p&gt;
    &lt;p&gt;2024-05-27 R. Kikuo Johnson&lt;/p&gt;
    &lt;p&gt;2024-05-20 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-05-13 Mark Ulriksen&lt;/p&gt;
    &lt;p&gt;2024-05-06 by Faith Ringgold&lt;/p&gt;
    &lt;p&gt;2024-04-22&lt;/p&gt;
    &lt;p&gt;2024-04-22 Ana Juan&lt;/p&gt;
    &lt;p&gt;2024-04-15 Peter de S√®ve&lt;/p&gt;
    &lt;p&gt;2024-04-08 Pascal Campion&lt;/p&gt;
    &lt;p&gt;2024-04-01 Mark Ulriksen&lt;/p&gt;
    &lt;p&gt;2024-03-25 Klaas Verplancke&lt;/p&gt;
    &lt;p&gt;2024-03-18 Peter de S√®ve&lt;/p&gt;
    &lt;p&gt;2024-03-11 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-03-04 Victoria Tentler-Krylov&lt;/p&gt;
    &lt;p&gt;2024-02-26 Marcellus Hall&lt;/p&gt;
    &lt;p&gt;2024-02-12 Nicholas Konrad&lt;/p&gt;
    &lt;p&gt;2024-02-05 Sarula Bao&lt;/p&gt;
    &lt;p&gt;2024-01-29 Roz Chast&lt;/p&gt;
    &lt;p&gt;2024-01-15 Barry Blitt&lt;/p&gt;
    &lt;p&gt;2024-01-01 Bianca Bagnarelli&lt;/p&gt;
    &lt;p&gt;2023-12-25 Edward Steed&lt;/p&gt;
    &lt;p&gt;2023-12-18 Olimpia Zagnoli&lt;/p&gt;
    &lt;p&gt;2023-12-11 Barry Blitt&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518106</guid><pubDate>Tue, 06 Jan 2026 20:22:43 +0000</pubDate></item><item><title>Calling All Hackers: How money works (2024)</title><link>https://phrack.org/issues/71/17</link><description>&lt;doc fingerprint="332310190d2133ec"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Title : Calling All Hackers&lt;/p&gt;
      &lt;p&gt; Author : cts&lt;/p&gt;
      &lt;quote&gt; ==Phrack Inc.== Volume 0x10, Issue 0x47, Phile #0x11 of 0x11 |=-----------------------------------------------------------------------=| |=-----------------------=[ Calling All Hackers ]=-----------------------=| |=-----------------------------------------------------------------------=| |=--------------------------=[ cts (@gf_256) ]=--------------------------=| |=-----------------------------------------------------------------------=| --[ Table of Contents 0 - Preamble 1 - About the Author 2 - The Birth of a Shitcoin 3 - How Money Works 3.1 - Fixed Income 3.2 - Equities 3.3 - Shareholder Value 4 - Startup Blues 5 - Takeaways 6 - Thanks 7 - References 8 - Appendix --[ 0 - Preamble Hi. I'm cts, also known as gf_256, ephemeral, or a number of other handles. I am a hacker and now a small business owner and CEO. In this article, I would like to share my experience walking these two different paths. A hacker is someone who understands how the world works. It's about knowing what happens when you type "google.com" and press Enter. It's about knowing how your computer turns on, about memory training, A20, all of that. It's about modern processors, their caches, and their side channels. It's about DSi bootloaders and how the right electromagnetic faults can be used to jailbreak them. And it's about how Spotify and Widevine and AES and SGX work so you can free your music from the shackles of DRM. But being a hacker is so much more than these things. It's about knowing where to find things. Like libgen and Sci-Hub and nyaa. Or where to get into the latest IDA Pro group buy. Or which trackers have what and how to get into them. It's about knowing how to bypass email verification. How to bypass SMS verification. How to bypass that stupid fucking verification where you hold your driver's license up to a webcam (thank you, OBS virtual camera!) Having an actual threat model not just paranoia. Knowing that you're not worth burning a 0day on, but reading indictments to learn from others' mistakes. It's about knowing where to buy estradiol valerate on the internet and how to compound injections. Or the "bodybuilder method" to order your own blood tests when your state requires a script to do so. It's about knowing which shipments give the US CBP a bad vibe and which don't. It's about knowing what happens when you open Robinhood and giga long NVDA FDs. I mean the actual market microstructure, not "Ken Griffin PFOF bad". Then using that microstructure to find an infinite money glitch (high Sharpe!). It's about knowing how to get extra passports and reading the tax code. It's about knowing how to negotiate your salary (or equity). It's about knowing why things at the supermarket cost what they do. Or how that awful shitcoin keeps pumping. And why that dogshit startup got assigned that insane valuation. And understanding who really pays for it in the end (hint: it's you). My point is, it is not just about computers. It's about understanding how the world works. The world is made up of people. As much as machines keep society running, those machines are programmed by people--people with managers, spouses, and children; with wants, needs, and dreams. And it is about using that knowledge to bring about the change you want to see. That is what being a hacker is all about. --[ 1 - About the Author I have been a hacker for 13 years. Prior to founding Zellic, I helped start a CTF team called perfect blue (lately Blue Water). We later became the number one ranked CTF team in the world. We've played in DEF CON CTF. We've won GoogleCTF, PlaidCTF, and HITCON. It's like that scene from Mr. Robot but not cringe. In 2021, we decided to take that hacker friend circle and form a security firm. It turned out that crypto paid well, so we worked with a lot of crypto clients. In the process, we encountered insane, hilarious, and depressingly sobering bullshit. In this article, I will tell some stories about what that bullshit taught me, so you can benefit from the same lessons as I have. Markets are computers; they compute prices, valuations, and the allocation of resources in our society. Hackers are good at computers. Let's learn more about it. --[ 2 - The Birth of a Shitcoin I can't think of a better example than shitcoins. Let's look at the crypto markets in action. First, let's talk about tokens. What is their purpose? The purpose of a token is to go up. There is no other purpose. Token go up. This is important, remember this point. Now the question is, how do we make the token go up? In crypto, there are two main kinds of token deals. Let's call them the Asian Arrangement and the Western Way. The Asian Arrangement is a fairly straightforward pump and dump. It's a rectangle between the VC, the Market Maker, the Crypto Exchange, and the Token Project Founder. 1. The exchange's job is to list the token, bringing in investors. They get paid in a mix of tokens and cold, hard cash. Their superpower is owning the customer relationships with the retail users, and the naming rights to sports arenas. 2. The market maker provides liquidity so the market looks really healthy and well-traded so it is easy to buy the token. In good deals, they are paid in in-the-money call options on the tokens, so they are incentivized to help the token trade well. Their superpower is having a lot of liquidity to deploy, and people on PagerDuty. 3. The founder's job is to pump the token and shill it on Twitter. They are the hype man, and it's their job to drum up the narrative and pump everyone's bags. Their unique power is they can print more tokens out of thin air, and this is in large part how they get paid in this arrangement. 4. Lastly, the VC gets paid to organize the deal. They give the founders some money, who in return give a pinky promise that they will give the VC a lot of tokens once the tokens actually exist. This is known as a Simple Agreement for Future Tokens, or SAFT. Their superpower is dressing up the founders and project so it seems like the Next Big Thing instead of a Ponzi scheme. Everyone gets paid a ton of token exposure (directly or indirectly), and when it lists, it pumps. Then the insiders dump and leave with a fat stack. Except retail, they end up with the bag. Sometimes the listing doesn't go well for the organizers, in which case, better luck next time. But retail always loses. wtf??? LFG!!! to the moon ,o \oXo/\o/ /v | | | /\ / X\ / \ crypto investors ^ | | | | v +----------+ provides liquidity +--------+ | Crypto | &amp;lt;--------------------------------------- | Market | | Exchange | ----------------------------------------&amp;gt; | Maker | +----------+ maker fees +--------+ ^ | ^ fees, | | listing options | tokens | | / fees | | | +-------------------------------------------------+ | v | +---------+ tokens / SAFT / token warrants +---------+ | Token | ---------------------------------------&amp;gt; | Venture | | Project | &amp;lt;--------------------------------------- | Capital | +---------+ cash , intros to CEX / MM, shilling +---------+ This machine worked exceptionally well in 2017, especially before China banned crypto. All those ICO shitcoins? Asian Arrangement. And it still works well to this day, except people are more wary of lockups and vesting schedules and so on. Now let's discuss the Western Way. The Asian Arrangement? That old pump and dump? No sir, we are civilized people. Instead, our VCs *add value* to their investments by telling the world "how disruptive the tech is" and how the "team are incredible outliers". And they will not blatantly PnD the token, but instead they will fund "projects in the ecosystem" so it appears there is real activity happening on the platform. This is to hype up metrics (like TPS or TVL) to inflate the next round valuation. Anyways, then they dump. Or maybe the VC is also a market maker so they market make their portfolio company tokens. Overall it's the same shit (Ponzi) but dressed up in a nicer outfit. Asian Arrangement or Western Way--either way, if you're the token founder, your main priority is to just GO TO MARKET NOW and LAUNCH THE TOKEN. This is so you can collect your sweet bag and dump some secondary before someone else steals the narrative or the hype cycle moves on. This is one of the reasons there are so many hacks in crypto. The code is all shitty because it's rushed out as fast as possible by 20-something- year-old software engineers formerly writing Typescript and Golang at Google. Pair that with some psycho CEO product manager. Remember, it is not about WRITING SECURE CODE, it is about SHIPPING THE FUCKING PRODUCT. Good luck rewriting it in Rust! All of this worked well until Luna, then 3AC, Genesis, and FTX imploded in 2022. It still works, but you have to be less blatant now. Shitcoins do serve an essential need. They are an answer to financial nihilism. Many people are working dead-end wage slave jobs that are not enough to "make it". They feel trapped and forced to work at jobs they fucking hate and waste their life doing pointless shit to generate shareholder value. This kind of life feels unacceptable, yet there are few avenues out. So what is the only "attainable" solution left? Gamble it on shitcoins, and if you lose...maybe next paycheck will be better. But enough about crypto, let's talk about securities. --[ 3 - How Money Works ----[ 3.1 - Fixed Income First, let's start with fixed income. I'm talking boring, old-fashioned bonds, like Treasury bonds. A lot of people are introduced nowadays to finance through equities (stocks) and tokens. In my opinion, this is only half of the story. Fixed income is the bedrock of finance. It has fundamental value. It provides a prototypical asset that all assets can be benchmarked based on. Fixed income assets, like bonds, boil down to borrowing and lending. A bond is basically an IOU for someone to pay you in the future. It is more useful to have a dollar today than in a year, so lenders charge a fee for access to money today. This fee is known as interest, and how it is baked into the equation varies from asset-to-asset. Some bonds come with interest payments, whereas other bonds are zero-coupon. The most important thing is to remember that bonds are essentially an IOU to pay $X in the future. Here is an example. Let's say you would like to borrow $100 to finance an upcoming project. The interest rate will be 5% per year. To borrow money, you would issue (mint) a bond (an IOU) for $X+5 dollars to be repaid 1 year in the future. In exchange for this fresh IOU, the lender will give you $X dollars now. On the lender's balance sheet, they will be less $X dollars worth of cash, but will also have gained ($X+5) dollars worth of an asset (your IOU), creating $5 of equity. In contrast, you would have $X more cash in assets, but also an ($X+5) liability, creating -$5 of equity. This example also works for depositing money at a bank. Here, you are the lender, and the bank is the borrower. Your deposits would be liabilities on their balance sheet, as they are liable to pay you back the deposit if you choose to withdraw it. Lender's Balance Sheet Borrower's Balance Sheet =========================== =========================== Assets: Assets: IOU-----------------X+5 Cash------------------X Liabilities: Liabilities: Cash----------------(X) IOU-----------------X+5 Equity: Equity: Equity----------------5 Equity--------------(5) Fixed income assets are extremely simple. There are various risks (credit risk, interest rate risk, etc.), but excluding these factors, you essentially get what you pay for. Unlike a token or stock, the bond is not going to suddenly evaporate or crash. (In theory.) Because of this, they can be modeled in a straightforward way; a way so straightforward even a high school student can understand it. Let's say I have $X today. Suppose the prevailing (risk-free) interest rate is 5%. What is the value of this $X in a year? Obviously, it would be no less than $X*1.05, as I can just lend it out for 5% interest and get $X*1.05 back in a year. If you gave me the opportunity to invest in any asset yielding less than 5%, this would be a bad deal for me, since I could just lend it out myself to get 5% yield. Now, let's analyze the same scenario, but in reverse. Let's take that IOU from earlier. What is the value *today* of a (risk-free) $X IOU, due in 1 year? It would be worth no more than $X/1.05. This is because with $X/1.05 dollars today, I could lend it out and collect 5% interest to end up with $X again in the future. If I pay more than $X/1.05, I am getting a bad deal, since I am locking up my money with you when it would be more capital efficient to just lend it out myself. You can probably see where I am going with this. The present value of an $X IOU at some time *t* in the future is $X/(1+r)^t, where *r* is the discount rate. The discount rate describes the "decay" of the value over time, due to interest but also factors like potential failure of the asset (for example, if the asset is a company, business failure of the company). Now, if we have some asset which pays a series of future cash flows *f(t)*, we can model this asset as a bundle of IOUs with values f(t) due in time 1, 2, 3, and so on. Then the present value of this asset is the geometric series sum of the discounted future cash flows. This is called discounted cash flows (DCF). Congrats, now you can do better modeling than what goes into many early-stage venture deals. +------+-----+-----+---------+---------+---------+-------+---------+ | Year | 0 | 1 | 2 | 3 | 4 | ... | t | +------+-----+-----+---------+---------+---------+-------+---------+ | Cash | CF1 | CF2 | CF3 | CF4 | CF5 | ... | CF_t | | Flow | | | | | | | | +------+-----+-----+---------+---------+---------+-------+---------+ | Disc.| CF1 |_CF2_| __CF3__ | __CF4__ | __CF5__ | ... | _CF_t__ | | Val | | 1+r | (1+r)^2 | (1+r)^3 | (1+r)^4 | | (1+r)^t | +------+-----------+---------+---------+---------+-------+---------+ IOU 1 IOU 2 IOU 3 IOU 4 IOU 5 ... IOU n inf _ f(t) 1 DCF = \ ------- = (assume constant annual cash flow x) = --------- x /_ (1+r)^t 1-1/(1+r) t=0 = (1/r + 1) x Cash flow multiple = (value) / (annual cash flow) ~= 1/r (The astute reader might also find that they can go backwards from valuations to estimate first, second, ... Nth derivatives of the cash flow or the year-to-year survival chances of a company. And these can be compared with...going outside and touching grass to see if the valuation actually makes sense.) At this point, you're probably wondering why I'm boring you with all of this dry quant finance 101 shit. Well, it's a useful thing to know about how the world works. First, interest rates affect you directly and personally. You may have heard of the term "zero interest rate environment". In a low interest rate environment, cash flow becomes irrelevant. Why? Consider the DCF geometric series sum if the interest rate r = 0. The present value approaches infinity. If the benchmark hurdle rate we're trying to beat is 0%, literally ANYTHING is a better investment than holding onto cash. Now do you see why VCs were slamming hundreds of millions into blatantly bad deals and shit companies during Covid? Cash flow and profitability didn't matter, because you could simply borrow more money from the money printer. Here's a more concrete example. Do you remember a few years ago when Uber rides were so cheap, that they were clearly losing money on each ride? This is known as Customer Acquisition Cost, or CAC. CAC is basically the company paying you to use their app, go to their store, subscribe to the thing, ... whatever. The strategy is well-known: burn money to acquire users until everyone else dies and you become a monopoly. Then raise the prices. But here is the key point: this only works in a low-interest rate environment. In such an environment, discounting is low, and thus, future growth potential is valued over profitability and fundamentals at present. It doesn't need to make sense *today* as long as it works 10 years from now. For now, we can keep borrowing more money to sustain the burn. Of course, when rates go back up, the free money machine turns off and the effects ripple outward. You are the humble CAC farmer, farming CAC from various unprofitable consumer apps like ride share, food delivery, whatever. These apps raise their money from their investors, VC and growth equity funds. These funds in turn raise their money from *their* investors, their limited partners. These LPs might be institutional capital like pension funds, sovereign wealth funds, or family offices. At the end of the day, all of that wealth is generated somewhere throughout the economy by ordinary people. So when some VC-backed founders throw an extravagant party on a boat with fundraised dollars, in some sense, you are the one paying for it. And when the money machine turns off, anyone who had gotten complacent under ZIRP is now left scrambling. Companies will overhire during ZIRP only to do layoffs when rates go up. +=========================+ | THE LIQUIDITY CYCLE | +=========================+ VENTURE CAPITAL _______________ ,.-^=^=^=^=^=^=^=^=^=^;, ,;===============&amp;gt;&amp;gt; E^ a16z LSVP Tiger '^3. .;^ E^ FF Social Cap. '^3 // condensation .E Bain SoftBank Accel 3^ /|^ ^E KP Benchmark :^ || ^;: YC Greylock GC ;3' ,.^-^-^-^-^-^-^-^-^-^-^;, ^.=.=_=_=_=_=_=_=_=_=_=_=^ E^ endowments family '^:. \\\\\\\\\\\\\\\\\\\\ E^ offices '^3 \\\\\\\\\\\\\\\\\\\\ E' pension ^3. SOURCE \\\ precipitation \\ ^; funds sovereign 3.' CAPITAL \\\\\\\\\\\\\\\\\\\\ E;: wealth funds ,3^ (LPs) \\\\\\\\\\\\\\\\\\\\ ^;._.._._._._._._._._._._,^ \\\\\\\\\\\\\\\\\\\\ /\ ^ ^ ^ ^ ^ ^ ^ ^ gamefi /\ /\ uber eats | | | | | | | | shitcoins/::\/::\ /::::\ /\ | evaporation | / doordash/^^^^^^\ /^^\ | | | | | | | | ____________ / \ / hello \ (poggers desu) /_____ lime ____ fresh ___\ \o/ \oXo/\oXoXo/ o '==========' UNPROFITABLE CONSUMER APPS | | | | | | /|\ Oo._ /\_/\ ,/// __/_\_/_X_\/_X_X_\_/_\__ /_________(@'w'@)_____________.,://' SOCIETY \'''''''' -...-''''''''''''''''' surface THE HUMBLE runoff CAC FARMER Second, credit is not inherently a bad thing if used responsibly. Take for example those Buy Now, Pay Later loans. Now that you are equipped with the concept of capital efficiency, wouldn't it technically better than paying cash to take an interest-free BNPL loan and temporarily stick the freed cash into an investment? (Barring other side effects, etc.) Third, the concept of net present value--i.e., credit--is the killer app of finance. It allows you to transport value from the future into today. Of course, that debt must be repaid in the future, unless you can figure out a way to kick the can down the road forever. For now, let's get back to stocks. ----[ 3.2 - Equities Now we have seen both sides of the coin. Asset value is twofold: speculative and fundamental. First, we saw speculative value as illustrated by crypto meme coins. Then, on the other hand, we examined fundamental value as illustrated by, e.g. a US Treasury. These two lie on two extremes of a spectrum. Some sectors and stocks are more speculative than others; Nvidia is practically a meme coin at this point, whereas something like Coca-Cola is like fixed income for boomers (NFA BTW). Most assets have a blend of both. Thinking about stocks, they (usually) have some fundamental value. Equities represent ownership of some asset, like a business. The business in theory generates dividends for shareholders, and this cash flow (or the net present value of future ones) represents the fundamental value of the business. As we've seen, assets with better cash flows are more valuable. In practice, buybacks can be used to create what is effectively a shareholder dividend in a more tax-advantaged way. Whereas with dividends, they are taxed as income, and this is realized immediately. With buybacks, they are taxed as capital gains, but crucially the gains are not realized until the asset is sold. This could be indefinitely far in the future, so it's more capital efficient. It has the added benefit that it helps pump the token, and imo this is kind of cute because it marries both the fundamental and speculative aspects. Meanwhile, like tokens, stocks are also supposed to go up. Here's an example: imagine a generic meme coin. Apart from Go Up, what does it do? Nothing. Even if it's a Governance Token, who cares when the founders and VCs hold all the voting power? Anyways, I'm describing Airbnb Class A Common Stock. Here's an excerpt from their S-1 [1] [2]: &amp;gt; We have four series of common stock, Class A, Class B, Class C, and &amp;gt; Class H common stock (collectively, our "common stock"). The rights of &amp;gt; holders of Class A, Class B, Class C, and Class H common stock are &amp;gt; identical, except voting and conversion rights ... Each share of Class A &amp;gt; common stock is entitled to one vote, each share of Class B common stock &amp;gt; is entitled to 20 votes and is convertible at any time into one share of &amp;gt; Class A common stock ... Holders of our outstanding shares of Class B &amp;gt; common stock will beneficially own 81.7% of our outstanding capital &amp;gt; stock and represent 99.0% of the voting power of our outstanding capital &amp;gt; stock immediately following this offering, ... Name of | Class B | % | % of Vot- Beneficial Owner | Shares | | ing Power -------------------------------------+------------+-------+----------- Brian Chesky | 76,407,686 | 29.1% | 27.1% Nathan Blecharczyk | 64,646,713 | 25.3% | 23.5% Joseph Gebbia | 58,023,452 | 22.9% | 21.4% Entities Affil. w/ Sequoia Capital | 51,505,045 | 20.3% | 18.9% Why do people buy tech stocks with inflated valuations? Some may because they believe that they will go up, that they will be more dominant, important, and valuable in the future. Like tokens, a large part of stocks' value is speculative. They are expressing their opinion on the future fundamentals. Others may simply because they believe others will believe that it is more valuable. Not fundamentals, this is an opinion about *pumpamentals*. Importantly, unlike fundamental value, speculative value can be created out of thin air. It is minted by *fiat*. Fundamental value is difficult to create, whereas speculative value can be created through hype and psychology alone. ----[ 3.3 - Shareholder Value For stocks, there are usually laws in place to protect investors, pushing the balance between "speculation" and "fundamentals" towards the latter. As a result, firms are generally legally obligated to act in their shareholders' best interests. This is good because normal people will be able to participate in the wealth generated by companies. And obviously, companies should not defraud their investors. However, the biggest *stake* holders in a business, are usually (in order): 1. The employees. No matter what, no one else is spending 8 hours a day, or ~33% of their total waking lifespan at this place. Whatever it is, I guarantee you the employees feel it the most. 2. The customers. The customers are the reason the business is able to exist in the first place. Non-profits are not exempt: their customers are their donors. 3. The local community / local environment / ecosystem. The business doesn't exist in a vacuum. The business has externalities, and those externalities affect most the immediate surrounding environment. 4. And in last place, the shareholders. They do not really do anything except contribute capital and hold the stock. Of course capital is important but they are not spending 8 hours a day here, they are not the reason the business exists, and in fact they might even live in a totally different country. For large, publicly-listed companies, the shareholders have one more unique difference from the other three stakeholders: liquidity. This difference is critical. Liquidity describes how easy it is to buy and sell an asset. A dollar bill is liquid. Bitcoin is liquid. A house is relatively illiquid. Stock in large, publicly-listed companies is also liquid. A shareholder can buy a stock one day and sell it the next. As a result, the relationship is non-commital and opens the opportunity for short-term thinking. There are many things a company could do which would benefit shareholders short term, while harming the other three stakeholders long term. While a shareholder can simply dump their position and leave, the mess created is left for the employees, customers, and community to clean up. (The SPAC boom was a pretty good example of this. Not all SPACs are bad, but a lot of pretty shit businesses publicly listed through SPACs then crashed. This is sad to me because some of that is early investors and founders dumping on retail like a crypto shitcoin, but dressed up because it's NYSE or NASDAQ. Get liquidity then bail.) Now, it is a misconception that stock companies must solely paperclip- maximize short-term shareholder value. However, this is how it often plays out due to fucked up shit in the public markets, like annoying activist hedge funds or executive compensation tied to stock price. And it is true that employees can be shareholders. And that is usually a good thing! But few public companies are truly employee-owned. Thinking about it from this perspective, the concept of maximizing shareholder value seems somewhat backwards. But *why* would one make this system where the priorities are seemingly inverted? One benefit is that it would make your currency extremely valuable. Suppose you want to do some shit on Ethereum (speculating on some animal token?), you will need to have native ETH to do that transaction. Similarly, if you want to invest in US securities you at some point need US Dollars. If you want to get a piece of that sweet $NVDA action, you need dollars. People want to buy American stocks. American companies perform well: they're innovative; they're not too heavily regulated; it's a business friendly environment. (Shareholder value comes first!) The numbers go up. Remember the token founder from earlier in the Asian Arrangement? Suppose you are a *country* in the situation above, with a valuable currency. Not only is your currency in demand and valuable, you are the issuing/minting authority for that token. Similar to the token founder, you can print valuable money and pay for things with it. And speaking of being a founder, let's talk about that! --[ 4 - Startup Blues Based on what we've set up so far, I will discuss some of the problems I see with many startups today and with startup culture. Much of the problems stem from misalignment between shareholders and the other stakeholders (employees, etc). A lot of this comes from the fundamentals of venture capital. VC is itself an asset class, like fixed income and equities. VCs pitch this to their limited partners, at some level, based on the premise that their VC fund will generate yield for them. The strategy is to identify stuff that will become huge and buy it while it's still small and really cheap. Like trading shitcoins, it's about finding what's going to moon and getting in early. In a typical VC fund, a small handful of the investments will comprise the entire returns of the fund, with all of the other investments being 0's. The distribution is very power law. This means we are not looking for 1x, 2x, or 3x outcomes; these may even be seen as failure modes. We are only interested in 20x, 50x, 100x, etc. outcomes. This is because anything less will be insufficient to make up for all the bad investments that get written down to zero. For the same reason, it only makes sense for VCs to invest in certain types of companies. Have you ever heard this one? "We invest in SOFTWARE companies!...How is this SCALABLE? What do the VENTURE SCALE OUTCOMES look like here?" This is because these kinds of companies are the ones with the potential to 100x. They want you to deliver a 100x. Or how about this one? "We invest in CATEGORY-DEFINING companies". At least in security, "category-defining" means a shiny new checkbox in the compliance / cyber insurance questionnaire. In other words, a new kind of product that people MUST purchase. The market is incentivized to deliver a product that meets the minimum bar to meet that checkbox, while being useless. I invite you to think of your favorite middleware or EDR vendors here. For passionate security founders considering raising venture, remember that this is what your "success" is being benchmarked against. _.,------------------------------_ .%' '&amp;amp;. .;' We partner with founders ^; ! building category-defining ;! ; companies at the earliest stages _; ^; _.^ ''-.______________ __________.-' / / / /^ / /^ /;^ /' _________ _________ _-' '. _-' '. ,^ '^_ ,^ '^_ /' '"' /' '"' ^' ^\^ ^' ^\^ : ^| : ^| : . . |) : . . |) : \ |) : \ |) : __\ ,; : __\ ,; " ! ; " ! ; " ^\ _____ /' " ^\ _____ /' '| | ^\ _/^ '| | ^\ _/^ | ^'=====' | ^'=====' | . | | | . | | _' |^__ _' |^__ ---------_-' U '--_ -------------_-' U '--_ ----- ._ _.-' '-._ _.-' '- ':.' \ ; / ': .' \ ; / [4] It's due to the thirst for 100x that there are painful dynamics. A fledgling startup may have founders they really like, but the current business may be unscalable. Bad VCs will push founders towards strategies, bets, models that have a 1% chance of working, but pay out 200x if they do. In the process they destroy a good business--one which has earned the trust of dutiful employees and loyal customers--all for a lottery ticket to build a unicorn. They will throw 100 darts at the dartboard and maybe 5 will land, but what is it like to be the dart? You may have good expected value, but all of that EV is from spikes super far away from the origin. Is it pleasant betting everything on this distribution? VC's want founders to be cult leaders. Have you ever heard this line? "We invest in great storytellers." Like what we saw with stocks and tokens, much of the easily-unlockable potential upside in assets is speculative. In essence, value can be created through narrative. Narrative *IS* value. Bad VC's will push founders to raise more capital at ever higher valuations (higher val = markup = fees), using narrative as fuel for the fire. Storytelling means "pump the token", and the job of the CEO is to (1) be the hype man and to raise (2) cash and (3) eyeballs. For this reason, Sam Altman and Elon are fine CEOs, regardless of other factors, because they are great at all three. Much to the detriment of founders' and their employees' psyche, investors expect founders to be this legendary hype man. This requires a religiosity of belief that is borderline delusional. Have you ever tried to convince one of those Silicon Valley YC-type founder/CEOs that they are wrong? They will never listen to you because they have been socialized to be this way. It is what is expected of them, and it is easy to fall into this trap without even becoming aware of it. But if you think about it, does it make sense that to be a business owner, you need to be a religious leader? Of course not. All of these reasons are why so many startup founders are young. They have little to lose, so gambling it all is OK. Being a cult leader may be traumatizing, but they have time (and the neuroplasticity) to heal. And lastly, they do not have the life experience to have a mature personal identity beyond "I am a startup founder". All of this makes it easy to accept the external pressures to build a company this or that way. And perhaps not the way they would have wanted to, relying instead on their personal values. The true irony is that the latter is what creates true, enduring company culture and not the made-up Mad Libs-tier Company Culture Notion Page shit that so many startups have. And of course, good VCs are self-aware of all of the issues and strive to prevent them. But the overall problem remains. One last externality is for communities based around an industry. When you add billions of venture dollars into an industry, it becomes cringe. It's saddening to me seeing the state of certain cybersecurity conferences which are now dominated by..."COME TO OUR BOOTH, YOU CAN BE A HACKER. PLEASE VIEW OUR AI GENERATED GRAPHICS OF FIGURES CLAD IN DARK HOODIES STATIONED BEHIND LAPTOPS". Here I would use the pensive emoji U+1F614 to describe my feelings about the appropriation of hacker culture but Phrack is 7-bit ASCII, so please have this: :c u_u . _. --[ 5 - Takeaways The point is, all of this made me feel very small and powerless after I realized the sheer size of the problems I was staring at. Nowadays, to me it's about creating good jobs for my friends, helping our customers, and taking care of the community. Importantly, I realized that this is still making a bigger positive impact than what I could have done alone just as an individual hacker or engineer. To me, businesses are economic machines that can create positive (or negative) impact in a consistent, self-sustaining way. There are many people who are talented, kind, and thoughtful but temporarily unlucky. Having a company let me help these friends monetize their abilities and be rewarded fairly for them. And in that way I helped make their life better. Despite a lot of the BS involved in running a business, this is one thing that is very meaningful to me. You can understand computers and science and math as much as you want, but you will not be able to fix the bigger issues by yourself. The systems that run the world are much bigger than what we can break on our laptops and lab benches. But like those familiar systems, if we want to change things for the better, we have to first understand those systems. Knowledge is power. Understanding is the first step towards change. If you do not like the system as it is, then it is your duty to help fix it. Do not swallow blackpills. It's easy to get really cynical and think things are doomed (to AGI apocalypse, to environmental disaster, to techno/autocratic dystopia, whatever). I want to see a world where thoughtful hackers learn these systems and teach each other about them. That generation of hackers will wield that apparatus, NOT THE OTHER WAY AROUND. Creating leverage for yourself. Hackers should not think of themselves as "oh I am this little guy fighting Big Corporation" or whatever. This is low agency behavior. Instead become the corporation and RUN IT THE WAY YOU THINK IT SHOULD BE RUN. Keep it private and closely held, so no one can fuck it up. Closely train up successors, so in your absence it will continue to be run in a highly principled way that is aligned with your values and morals. Give employees ownership, as it makes everyone aligned with the machine's long-term success, not just you. Raising capital. Many things do really need capital, but raise in a responsible way that leaves you breathing room and the freedom to operate in ways that are aligned with your values. Never compromise your values or integrity. Stay laser focused on cash flows and sustainability, as these grant you the freedom to do the things right. HACKERS SHOULDN'T BE AFRAID TO TOUCH THE CAPITAL MARKETS. Many hackers assume "oh that fundraising stuff is for charismatic business types". I disagree. It's probably better for the world if good thoughtful hackers raise capital. Giving them leverage to change the world is better than giving that leverage to some psycho founder drinking the Kool-Aid. I deeply respect many of the authors in Phrack 71, and I would trust them to do a better job taking care of things than an amorphous amalgam of angry and greedy shareholders. For all things that don't need capital, do not raise. Stay bootstrapped for as long as possible. REMEMBER THAT VALUATION IS A VANITY METRIC. Moxie Marlinspike wrote on his blog [3] that we are often guilty of always trying to quantify success. But what is success? You can quantify net worth, but can you quantify the good you have brought to others lives? For personal goals, think long term. People tend to overestimate what they can do in 1 year, but underestimate what they can do in 10. DO NOT start a company thinking you can get your hands clean of it in 2-3 years. If you do a good job, you will be stuck with it for 5-10+ years. Therefore, DO NOT start a company until you are sure that is what you want to do with your life, or at least, your twenties/thirties (depending on when you start). A common lament among founders, even successful ones, is: "Sometimes I feel like I'm wasting my twenties". There's an easy Catch-22 here: you may not know what you really want until you do the company; but once you do the company, you won't really be able to get out of it. Be wary of that. Creating value. This is one of those meaningless phrases that I dislike. Value is what you define it to be. Remember to work on things that have TAMs, but remember that working on art is valuable too! It is not all about the TAM monster--doing cool things that are NOT ECONOMICALLY VALUABLE, but ARTISTICALLY VALUABLE, is equally important. There is not much economic value in a beautiful polyglot file, but it is artistically delightful. This is part of why people hate AI art: it may be economically valuable, but it is often artistically bankrupt. (Some people do use generative tools in actually original and artistic ways, but this is the exception not the norm currently.) Founders vs Investors. Here is my advice: Ignore any pressure from investors to make company "scalable" or whatever. Make sure your investors have no ability to fire you or your co-founder(s). Make sure you and co-founder are always solid and trust each other more than investors. You and your cofounders need to be BLOOD BROTHERS (/sisters/w.e). If an investor is trying to play politics with one of you to go against the other cofounder, cut that investor out immediately and stop listening to them. Any investor who pushes for scalability over what you think is the best interest of the company is not aligned with you. High-quality investors will not push for this because they are patient and in it for the long game. If you are patient, you can make a very successful company, even if it is not that scalable. High-quality investors will bet on founders and are committed; only bad ones will push for this kind of shit. I'm going to avoid giving more generic startup advice here. Go read Paul Graham's essays. But remember that any investor's perspective will not be the perspective of you and your employees. Pivoting 5 times in 24 months is not a fun experience to work at: your employees will resign while your investors celebrate your "coming of age journey"--unless everyone signed up for that terrifying emotional rollercoaster from the start. They say that "hacker" is a dying identity. Co-opted by annoying VC-backed cybersecurity companies that culturally appropriate the identity, the term is getting more polluted and diluted by the day. Meanwhile, computers are getting more secure, and they are rewriting everything in Rust with pointers-as-capability machines and memory tagging. Is it over? I disagree. As long as the hacker *ethos* is alive, regardless of any particular scene, the identity will always exist. However, now is a crucible moment as a diaspora of hackers, young and old, venture out into the world. Calling all hackers: never forget who you are, who you will become, and the mark you leave. --[ 6 - Thanks Greetz (in no particular order): * ret2jazzy, Sirenfal, ajvpot, rose4096, Transfer Learning, samczsun, tjr, claire (aka sport), and psifertex. * perfect blue, Blue Water, DiceGang, Shellphish, and all CTF players. * NotJan, nspace, xenocidewiki, and the members of pinkchan and Secret Club. * Everyone at Zellic, past and present. Finally, a big thank you to the Phrack staff (shoutout to netspooky and richinseattle!) for making this all possible. --[ 7 - References [1] https://www.sec.gov/Archives/edgar/data/1559720/000119312520315318/ d81668d424b4.htm [2] https://www.sec.gov/Archives/edgar/data/1559720/000119312522115317/ d278253ddef14a.htm [3] https://moxie.org/stories/promise-defeat/ [4] https://twitter.com/nikitabier/status/1622477273294336000 --[ 8 - Appendix: Financial institution glossary for hackers (Not serious! For jokes... :-) - IB: Investment Bank. Basically collect fat fees to do up ("advise on") M&amp;amp;As and other transactions. Help match buyers and sellers for your private equity. They are like CYA for your deal. - PE: Private Equity. Basically buy not-overly-seriously ("poorly") run companies, fire the management, then run it "professionally" (i.e. make it generally shitty for customers and employees and community for the benefit of shareholders) - HF: Hedge Fund. Trade out pricing inefficiencies - MM: Market Maker. Basically the same thing - VC: Basically gamble on tokens (crypto or stocks) and back cool and/or wacky ideas that the rest of these people find too stinky to invest in - PnD: Pump and Dump. - TVL: Total Value Locked. Basically how much money is currently in a blockchain or smart contract system. - TPS: Transactions Per Second. A measure of how scalable or useful a blockchain or database is. An oft-abused metric hacked by vaporware shillers for hype and PnD purposes. - TAM: Total Addressable ~~Memory~~ Market. Basically how much money a given idea can make. - NFA: Not finanical advice. |=[ EOF ]=---------------------------------------------------------------=| &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518129</guid><pubDate>Tue, 06 Jan 2026 20:24:45 +0000</pubDate></item><item><title>A 30B Qwen Model Walks into a Raspberry Pi and Runs in Real Time</title><link>https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/</link><description>&lt;doc fingerprint="3edb28de9d388188"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt; A 30B Qwen Model Walks Into a Raspberry Pi√¢¬¶&lt;lb/&gt; and Runs in Real Time &lt;/head&gt;
    &lt;p&gt;For this release, we optimize for what people actually experience when they run a model: fast, high-quality responses on a specific target device.&lt;/p&gt;
    &lt;p&gt;We use Shapelearn, our bitlength learning method to choose weight datatypes for Qwen3-30B-A3B-Instruct-2507 that maximize performance in terms of tokens per second (TPS) and output quality, with one practical constraint: the model must fit comfortably in the available memory. Once it fits, making the file smaller isn't a goal by itself. We only shrink further when it also improves the real tradeoff people care about: speed vs. quality.&lt;/p&gt;
    &lt;p&gt;Approaching bitlength learning this way matters because in llama.cpp, "fewer bits" doesn't automatically mean "more speed." Different quantization formats can trigger different kernels and overheads, and on some GPUs, going lower-bit can even get slower, despite using less memory.&lt;/p&gt;
    &lt;p&gt;Bottom line: treat memory as a budget to meet, then optimize what matters most: TPS and quality.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt; Yes, this 30B Qwen3 runs on a Raspberry Pi. On a Pi 5 (16GB), &lt;code&gt;
              
                Q3_K_S-2.70bpw [KQ-2]
              
            &lt;/code&gt;
            hits 8.03 TPS at 2.70 BPW and maintains 94.18% of BF16 quality. It genuinely feels
            real-time. More broadly, the same pattern shows up everywhere else: ByteShape models
            give you a better TPS/quality tradeoff than the alternatives (here we look at Unsloth
            and MagicQuant).
          &lt;/p&gt;
    &lt;head rend="h2"&gt;CPUs&lt;/head&gt;
    &lt;p&gt;On CPUs, the reducing footprint via shorter bitlengths affects the TPS and accuracy tradeoff as one would expect: once the model fits, reducing footprint tends to increase TPS in a fairly monotonic way. If datatypes are selected correctly, you can trade a bit of quality for speed predictably, which makes it much easier to pick a point on the curve that matches your constraints.&lt;/p&gt;
    &lt;p&gt;We'll start with the most memory-constrained CPU case (Raspberry Pi 5 16GB), where "fits in RAM" is the limiting factor, then move to an Intel i7 with 64GB, where everything fits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;p&gt;The figure below shows TPS vs. normalized accuracy for the models that fit in RAM on the Raspberry Pi 5 16GB.&lt;/p&gt;
    &lt;p&gt;Notably, sustaining 8.5 TPS at 92%+ baseline accuracy with a 30B model on a Raspberry Pi reshapes expectations for Pi-class systems. Overall, the trend shows that ShapeLearn consistently produces better models, with ByteShape trending up and to the right of Unsloth, achieving higher tokens per second at the same quality, or higher quality at the same throughput.&lt;/p&gt;
    &lt;p&gt;We highlight choices for two primary objectives: accuracy or response time.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Optimizing for response time while maintaining accuracy: For interactive, on-device use, perceived responsiveness is driven by how quickly text appears, not peak throughput. In practice, generation feels real-time once it reaches roughly 8 TPS, comfortably above typical reading speed. In this Raspberry Pi real-time regime, &lt;code&gt;Q3_K_S-2.70bpw [KQ-2]&lt;/code&gt;(2.70 BPW, 8.03 TPS, 94.18% accuracy) is our go-to recommendation: it crosses the real-time threshold while maintaining high accuracy. Compared to Unsloth models at similar quality, ByteShape achieves real-time performance at lower BPW and higher TPS, making it the more efficient choice for interactive edge deployment.&lt;/item&gt;
      &lt;item&gt; Accuracy above all: The table below lists the models that achieve the highest accuracy while still being able to run on a Raspberry Pi. Within this set, ByteShape models make the best use of the available resources to maximize accuracy, occupying the lowest-error rows (~1.1√¢1.3% relative error, ~98.8% accuracy), while the strongest Unsloth entries remain around 2.1√¢2.2% error (~97.9% accuracy). Compared to Unsloth's &lt;code&gt;UD-Q3_K_XL [8]&lt;/code&gt;, ByteShape achieves up to a 1.87√É lower error rate while still operating at ~5√¢6 TPS, comfortably within TPS-norms on Raspberry PI making it the better choice when accuracy is the priority.&lt;lb/&gt;Even when prioritizing maximum speed with some reduction in accuracy,&lt;code&gt;Q3_K_S-3.25bpw [KQ-5]&lt;/code&gt;offers a better tradeoff: more accurate, smaller, and faster than the fastest Unsloth model.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Relative Error&lt;/cell&gt;
        &lt;cell role="head"&gt;BPW&lt;/cell&gt;
        &lt;cell role="head"&gt;TPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q4_K_S-3.92bpw [KQ-7]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1.14%&lt;/cell&gt;
        &lt;cell&gt;3.92&lt;/cell&gt;
        &lt;cell&gt;5.30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q4_K_S-3.61bpw [KQ-6]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1.25%&lt;/cell&gt;
        &lt;cell&gt;3.61&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q3_K_S-3.25bpw [KQ-5]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.03%&lt;/cell&gt;
        &lt;cell&gt;3.25&lt;/cell&gt;
        &lt;cell&gt;6.68&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UD-IQ3_XXS [6]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.22%&lt;/cell&gt;
        &lt;cell&gt;3.38&lt;/cell&gt;
        &lt;cell&gt;5.03&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;UD-Q3_K_XL [8]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.13%&lt;/cell&gt;
        &lt;cell&gt;3.62&lt;/cell&gt;
        &lt;cell&gt;6.28&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Many other Unsloth and MagicQuant models (some of ours too!) are not in this chart. We compare them in other sections, but they're not applicable in the Raspberry Pi case. They simply don't fit!&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel i7&lt;/head&gt;
    &lt;p&gt;Next, we move to the Intel i7 with 64GB RAM. The figure below shows TPS vs normalized accuracy for all models.&lt;/p&gt;
    &lt;p&gt;Overall, ByteShape models outperform both Unsloth and MagicQuant, delivering higher quality at comparable throughput using fewer bits per parameter. Only ByteShape offers models that run in the 26+ TPS range, extending performance well beyond the other methods.&lt;/p&gt;
    &lt;p&gt;Highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Quality-first: At the high-accuracy end of the table, &lt;code&gt;IQ4_XS-4.67bpw [KQ-9]&lt;/code&gt;achieves the lowest relative error (0.25%), outperforming the best-running Unsloth models (&lt;code&gt;Q6_K [20]&lt;/code&gt;and&lt;code&gt;Q5_K_M [18]&lt;/code&gt;whose relative errors are 0.36% and 0.44%). Compared directly, ByteShape delivers up to a 1.44√É lower error rate with higher throughput than&lt;code&gt;Q6_K [20]&lt;/code&gt;, and a 1.76√É lower error rate at essentially the same speed as&lt;code&gt;Q5_K_M [18]&lt;/code&gt;. MagicQuant&lt;code&gt;mxfp4 [3]&lt;/code&gt;trails in this regime, with both higher error and lower TPS.&lt;/item&gt;
      &lt;item&gt; Balanced point: In the mid-accuracy, high-throughput region, &lt;code&gt;Q3_K_S-3.25bpw [KQ-5]&lt;/code&gt;combines ~98% accuracy with 23.1 TPS at just 3.25 BPW, offering the best overall balance in the table. Matching or exceeding this accuracy with Unsloth (&lt;code&gt;IQ4_XS [10]&lt;/code&gt;) requires higher BPW and lower TPS, while choosing an Unsloth model closer in speed (&lt;code&gt;Q3_K_S [7]&lt;/code&gt;) incurs a 1.73√É higher error rate. MagicQuant does not offer a competitive model in this range; its fastest entry (&lt;code&gt;IQ4_NL [2]&lt;/code&gt;) is behind both ByteShape and Unsloth in accuracy and throughput.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Takeaway: Across both quality-first and balanced settings, ByteShape consistently converts the available bit budget into either higher accuracy or higher TPS, and is the only approach that simultaneously covers the high-quality and 26+ TPS balanced-performance regions in this comparison.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPUs: RTX5090/32GB and RTX4080/16GB&lt;/head&gt;
    &lt;p&gt;On GPUs, performance depends as much on kernel choice as on raw memory footprint. For matmul/matvec, llama.cpp's quantization-specific GPU decode paths incur very different overheads, so fewer bits per weight do not reliably translate to higher TPS. Instead, TPS often peaks at quantization-specific sweet spots. Pushing BPW lower can even increase VRAM traffic and instruction count, hurting performance rather than improving it. We dig into this behavior in more detail right after the GPU results section, where the kernel-level tradeoffs become more apparent.&lt;/p&gt;
    &lt;p&gt;We evaluate on two GPUs: an RTX 5090 (32 GB), which can run models above 4 BPW and typically reach the fastest sweet spots, and an RTX 4080 (16 GB), where &amp;gt;4 BPW models do not fit, forcing different trade-offs and making the device-optimized curve easier to see.&lt;/p&gt;
    &lt;head rend="h3"&gt;RTX 5090 (32GB of VRAM)&lt;/head&gt;
    &lt;p&gt;Let's start with the 5090, which has enough VRAM to support all of the quantized models. The figure below shows TPS vs normalized accuracy.&lt;/p&gt;
    &lt;p&gt; Two things stand out immediately:&lt;lb/&gt; First, this GPU shows a clear ~4-bit sweet spot: several ~4b models cluster at very high TPS with nearly identical quality. Examples include &lt;code&gt;Unsloth Q4_0 [12]&lt;/code&gt;, &lt;code&gt;Unsloth IQ4_XS [10]&lt;/code&gt;,
            &lt;code&gt;
              
                IQ4_XS-3.87bpw [IQ-6]
              
            &lt;/code&gt;
            , and MagicQuant &lt;code&gt;iq4_nl-EHQKOUD-IQ4NL [1]&lt;/code&gt;, all running around ~302√¢303 TPS
            at ~98.4√¢98.9% accuracy. Within 
            this tight cluster, Unsloth edges out slightly in throughput and quality.
          &lt;/p&gt;
    &lt;p&gt;Second, outside of that sweet spot, the tradeoff becomes much more uneven:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many other Unsloth and Magic Quant models show significantly lower TPS, regardless of whether they are quantized more or less aggressively.&lt;/item&gt;
      &lt;item&gt;Past the ~4b region, only ByteShape continues to increase TPS with a more predictable reduction in quality.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Accuracy-critical workloads: when output quality is paramount, ByteShape delivers the most accurate model on the 5090: &lt;code&gt;
              
                IQ4_XS-4.67bpw [IQ-8]
              
            &lt;/code&gt;
            (4.67 BPW, 272.98 TPS, 99.75% accuracy). It surpasses &lt;code&gt;Unsloth Q6_K [20]&lt;/code&gt; (6.57 BPW, 264.88 TPS, 99.64% accuracy) 
            while using fewer bits and achieving slightly higher throughput, and it clearly outperforms MagicQuant 
            &lt;code&gt;mxfp4_moe-H-B16-EUR-IQ4NL-KO-Q5K-QD-Q6K [3]&lt;/code&gt; (5.46 BPW, 240.42 TPS, 99.32% accuracy) in both 
            accuracy and speed, making it the strongest choice when accuracy is a task-critical deployment requirement.
          &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Practical takeaway. If your GPU has enough VRAM to run a strong ~4b model that already meets your speed and accuracy requirements, that cluster is an excellent default. The curve becomes more interesting when task-critical deployment constraints demand higher accuracy or smaller models as for example, under tighter memory budgets or constrained environments (as we'll see on the 4080). &lt;/p&gt;
    &lt;head rend="h3"&gt;RTX 4080 (16GB of VRAM)&lt;/head&gt;
    &lt;p&gt;Next, let's move to a more accessible GPU, especially in these memory-challenged times. The biggest stumbling block for the 4080 is its 16GB of VRAM, which is not sufficient to support the "magical" ~4b quantizations for a 30B model. How convenient! This "avoids" the 5090's ~4b sweet spot and forces a more "real-world" comparison under a hard VRAM budget. The figure below shows TPS versus normalized accuracy for all models that fit on the 4080.&lt;/p&gt;
    &lt;p&gt;On the RTX 4080, ByteShape consistently outperforms Unsloth under the same 16 GB VRAM constraint, delivering a better TPS√¢quality tradeoff.&lt;/p&gt;
    &lt;p&gt; In particular, ByteShape's highest-quality model that fits, &lt;code&gt;
              
                IQ4_XS-3.87bpw [IQ-6]
              
            &lt;/code&gt;
            (3.87 BPW, 214.81 TPS, 98.66% accuracy) delivers:
          &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; a 1.59√É lower error rate and 9.4% higher TPS vs. &lt;code&gt;Unsloth Q3_K_XL [8]&lt;/code&gt;(3.62 BPW, 196.42 TPS, 97.87% accuracy).&lt;/item&gt;
      &lt;item&gt; a 2.54√É lower error rate at the same TPS vs. &lt;code&gt;Unsloth IQ2_M [2]&lt;/code&gt;(2.84 BPW, 214.79 TPS, 96.59% accuracy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we move to higher throughput, ByteShape's maintains accuracy, while Unsloth's error rate experiences a cliff.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Elephant in the Room: When 3-bits is not just 3-bits&lt;/head&gt;
    &lt;p&gt;There is an inconvenient truth hiding in these results. On several setups, around 4 bpw is already flying, and pushing quantization harder does not make things faster. It just manages to be smaller and slower at the same time.&lt;/p&gt;
    &lt;p&gt;Reducing the size of data doesn't automatically speed things up. While using fewer bits to store each number seems like it should reduce memory traffic and speed up computation, GPUs don't work that way. NVIDIA GPUs process work in fixed groups of 32 threads called "warps," which move through instructions together in near lock-step. The GPU hardware is optimized for specific data formats, memory access patterns, and operations that the chip's circuits are physically designed to handle efficiently. When your workload matches these "golden paths", you get peak performance. Step outside them, and you hit slowdowns. This isn't a design flaw, it's a deliberate tradeoff. Supporting more flexibility would require additional circuitry: more wires, more transistors, more complexity. That extra hardware consumes more power and adds latency to every operation, whether a program needs that flexibility or not.&lt;/p&gt;
    &lt;p&gt;Here a few examples of relevant hardware "quirks": VRAM is read in aligned 32-byte blocks, so reading one or 32 bytes consumes the same memory bandwidth. Both on-chip and off-chip memories can also suffer contention depending on how data is laid out, meaning that a warp's accesses may complete in a single step or, in the worst case, be serialized into 32 steps. And of course, decoding quantized values before computation can require extra instructions, with the cost depending on the quantization scheme.&lt;/p&gt;
    &lt;p&gt;This explains the behaviour we observe: 4-bit kernels use VRAM bandwidth more efficiently than 3- or 2-bit kernels and require fewer decode steps before computation. At the same time, 4-bit kernels exploit subword parallelism just as effectively as lower-bit kernels, and all rely primarily on dynamic caches rather than shared memory to take advantage of data reuse when possible.&lt;/p&gt;
    &lt;p&gt;So why llama.cpp hasn't been optimized to deliver peak speed for every bit-length? Our understanding is that llama.cpp prioritizes portable, space-efficient quantization that can run across a wide range of hardware. That design goal limits how aggressively backends can reshape data layouts or reorder computation in ways that might help one GPU or one bit-width.&lt;/p&gt;
    &lt;p&gt;A key example is its choice to store quantized weights in fixed blocks of 256 values. Each block is self-contained (it carries everything needed to decode it) and sits at a simple, predictable offset in the tensor, which makes the format easy to implement and fast to locate.&lt;/p&gt;
    &lt;p&gt;The tradeoff is that GPUs often need to decode many blocks in parallel to keep their wide compute units busy. With many independent 256-value blocks, those parallel decodes can translate into more scattered or fragmented VRAM reads and extra decode overhead, reducing bandwidth efficiency, especially for some lower-bit formats.&lt;/p&gt;
    &lt;p&gt; Point for example on RTX 5090: a matrix multiply [256, 768] √É [768, 2048] takes ~54√Ç¬µs with &lt;code&gt;iq4_xs&lt;/code&gt; datatype, but ~62√Ç¬µs with 
            &lt;code&gt;iq3_xxs&lt;/code&gt; (mul_mat_q()+mul_mat_q_stream_k_fixup()). In other words, 
            cutting nearly 1.2 bits per weight (a reduction of more than 25% in weight footprint) leads 
            to a ~13% slowdown, directly hurting user experience.
          &lt;/p&gt;
    &lt;p&gt;An excellent reminder that bitlength learning matters: Heuristics can get us part of the way, but not all the way. ShapeLearn makes deliberate, per-tensor datatype choices that improve speed without sacrificing accuracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methodology (brief recap)&lt;/head&gt;
    &lt;p&gt;If you're wondering how we are scoring these points, the full methodology is discussed in our previous blog post. This post is intentionally focused on the curves and device tradeoffs, so here is the quick version.&lt;/p&gt;
    &lt;p&gt;For each quantized variant, we measure throughput (TPS) on the target device and compute a single normalized quality score relative to the BF16 baseline, using the same evaluation harness and prompts as the methodology post. The quality score aggregates standard benchmarks (MMLU, GSM8K, IFEval, LiveCodeBench V4) into one number so you can compare points directly. In other words, every dot in the plots answers two questions: how fast does it run on this device, and how much quality does it retain compared to BF16, with memory fit as the first constraint.&lt;/p&gt;
    &lt;p&gt;We also want to thank all for the many, excellent suggestions on our recent Reddit post for improving and extending this evaluation strategy, and we√¢re actively working through them. Right now, evaluation is the main bottleneck and not bitlength learning/quantization. Careful evaluation is essential to clearly communicate the strengths of each model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;First, thank you for your tenacity. You made it through all of this without giving up. We are sincerely flattered!&lt;/p&gt;
    &lt;p&gt;The takeaway is simple: treat memory as a constraint, not a goal. Once a model fits on your device, what matters is the tradeoff curve, TPS versus quality. Across CPUs and GPUs, ByteShape consistently lands on the better side of that curve, delivering either more speed at the same quality or higher quality at the same speed.&lt;/p&gt;
    &lt;p&gt; If you're deploying on a Raspberry Pi 5 (16 GB) and want a genuinely interactive experience, start with &lt;code&gt;
              
                Q3_K_S-2.70bpw [KQ-2]
              
            &lt;/code&gt;
            . On larger CPUs or GPUs, you can move up the curve toward higher-quality points with
            little loss in throughput, the same rule applies:
            fit first, then optimize the tradeoff.
          &lt;/p&gt;
    &lt;p&gt;We'll keep releasing more device-targeted variants (and more plots). If your system can't run a 30B model smoothly, don't blame the model or the silicon. Blame the datatypes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518573</guid><pubDate>Tue, 06 Jan 2026 20:55:01 +0000</pubDate></item><item><title>Oral microbiome sequencing after taking probiotics</title><link>https://blog.booleanbiotech.com/oral-microbiome-biogaia</link><description>&lt;doc fingerprint="8a7c03af02b6652a"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, a friend recommended BioGaia Prodentis to me. It is a DTC oral probiotic you can buy online that is supposedly good for oral health. I thought it would be interesting to do some sequencing to see what, if anything, it did to my oral microbiome.&lt;/p&gt;
    &lt;p&gt;BioGaia Prodentis is available online for $20 or less for a month's supply&lt;/p&gt;
    &lt;head rend="h1"&gt;BioGaia&lt;/head&gt;
    &lt;p&gt;BioGaia has a fascinating story. They are a Swedish company, founded over 30 years ago, that exclusively sells probiotics DTC. They have developed multiple strains of Limosilactobacillus reuteri, mainly for gut and oral health. They apparently sell well! Their market cap is around $1B‚Äîimpressive for a consumer biotech.&lt;/p&gt;
    &lt;p&gt;Going in, I expected scant evidence for any real benefits to their probiotics, but the data (over 250 clinical studies) is much more complete than I expected.&lt;/p&gt;
    &lt;p&gt;Most notably, their gut probiotic, Protectis, seems to have a significant effect on preventing Necrotizing Enterocolitis (NEC) in premature babies. According to their website:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;5-10% of the smallest premature infants develop NEC, a potentially lethal disorder in which portions of the bowel undergo tissue death.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In March 2025, the FDA granted Breakthrough Therapy Designation to IBP-9414, an L. reuteri probiotic developed by BioGaia spinout IBT.&lt;/p&gt;
    &lt;p&gt;This is not specifically for the oral health product, but it's for sure more science than I expected to see going in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prodentis&lt;/head&gt;
    &lt;p&gt;BioGaia Prodentis contains two strains of L. reuteri: DSM 17938 and ATCC PTA 5289. The claimed benefits include fresher breath, healthier gums, and outcompeting harmful bacteria.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sequencing with Plasmidsaurus&lt;/head&gt;
    &lt;p&gt;Many readers will be familiar with Plasmidsaurus. Founded in 2021, the team took a relatively simple idea: use massively multiplexed Oxford Nanopore (ONT) to offer complete plasmid sequencing with one day turnaround for $15, and scaled it. Plasmidsaurus quickly became part of biotech's core infrastructure, and spread like wildfire. It also inspired multiple copycats.&lt;/p&gt;
    &lt;p&gt;Compared to Illumina, ONT is faster and has much longer reads, but lower throughput and lower accuracy. This profile is a great fit to many sequencing problems like plasmid QC, where you only need megabases of sequences, but want an answer within 24 hours.&lt;/p&gt;
    &lt;p&gt;Over time, Plasmidsaurus has been adding services, including genome sequencing, RNA-Seq, and microbiome sequencing, all based on ONT sequencing.&lt;/p&gt;
    &lt;p&gt;Plasmidsaurus accepts many kinds of sample for microbiome sequencing&lt;/p&gt;
    &lt;p&gt;I used their 16S sequencing product, which costs $45 for ~5000 reads, plus $15 for DNA extraction. 16S sequencing is an efficient way to amplify and sequence a small amount of DNA (the ubiquitous 16S region) and be able to assign reads to specific species or even strains.&lt;/p&gt;
    &lt;p&gt;This experiment cost me $240 for four samples, and I got data back in around a week. It's very convenient that I no longer have to do my own sequencing. As a side note, you can pay more for more than 5000 reads, but unless you want information on very rare strains (&amp;lt;&amp;lt;1% frequency), this is a waste of money.&lt;/p&gt;
    &lt;p&gt;Sample collection is simple: take 100-250 ¬µL of saliva and mix with 500 ¬µL of Zymo DNA/RNA Shield (which I also had to buy for around $70.) You also need 2 mL screwtop tubes to ship in.&lt;/p&gt;
    &lt;p&gt;The reads are high quality for nanopore sequencing, with a median Q score of 23 (99%+ accuracy). This is more than sufficient accuracy for this experiment. The read length is very tightly distributed around 1500 nt (the length of a typical 16S region).&lt;/p&gt;
    &lt;p&gt;The results provided by Plasmidsaurus include taxonomy tables, per-read assignments, and some basic plots. I include a download of the results at the end of this article, as well as the FASTQ files.&lt;/p&gt;
    &lt;head rend="h1"&gt;The experiment&lt;/head&gt;
    &lt;p&gt;The main idea of the experiment was to see if any L. reuteri would colonize by the end of 30 days of probiotic use, and if so, whether it would persist beyond that. I collected four saliva samples:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Timing&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline A&lt;/cell&gt;
        &lt;cell&gt;Day -4&lt;/cell&gt;
        &lt;cell&gt;A few days before starting BioGaia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline B&lt;/cell&gt;
        &lt;cell&gt;Day -1&lt;/cell&gt;
        &lt;cell&gt;The day before I started BioGaia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;The last day of the 30 day course&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Week after&lt;/cell&gt;
        &lt;cell&gt;Day 37&lt;/cell&gt;
        &lt;cell&gt;One week after completing the course&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Heatmap of the top 20 species. All species assignments were done by Plasmidsaurus&lt;/p&gt;
    &lt;head rend="h2"&gt;Did L. reuteri colonize?&lt;/head&gt;
    &lt;p&gt;There was no L. reuteri found in any of the samples. I did a manual analysis to check for any possible misassignments, but the closest read was only 91% identical to either L. reuteri strain.&lt;/p&gt;
    &lt;p&gt;The probiotic either (a) didn't colonize the oral cavity; (b) was present only transiently while actively taking the lozenges; (c) was below the detection threshold.&lt;/p&gt;
    &lt;p&gt;Probiotics are generally bad at colonizing, which is why you have to keep taking them. Still, I was surprised not to see a single L. reuteri read in there.&lt;/p&gt;
    &lt;head rend="h2"&gt;What actually changed?&lt;/head&gt;
    &lt;p&gt;Even though the probiotic itself didn't show up, the oral microbiome did change quite a lot.&lt;/p&gt;
    &lt;p&gt;The most striking change was a massive increase in S. salivarius. S. salivarius went from essentially absent to ~20% of my oral microbiome on the last day. However, this happened one week after I stopped taking the probiotic, so it's very unclear if it is related.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;S. mitis&lt;/cell&gt;
        &lt;cell role="head"&gt;S. salivarius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline A&lt;/cell&gt;
        &lt;cell&gt;2.0%&lt;/cell&gt;
        &lt;cell&gt;0.4%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline B&lt;/cell&gt;
        &lt;cell&gt;15.9%&lt;/cell&gt;
        &lt;cell&gt;0.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;10.2%&lt;/cell&gt;
        &lt;cell&gt;0.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Week after&lt;/cell&gt;
        &lt;cell&gt;1.0%&lt;/cell&gt;
        &lt;cell&gt;19.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We see S. mitis decreasing as S. salivarius increases, while the total Streptococcus fraction stayed roughly stable. It's possible one species replaced the other within the same ecological niche.&lt;/p&gt;
    &lt;p&gt;S. salivarius is itself a probiotic species. The strain BLIS K12 was isolated from a healthy New Zealand child and is sold commercially for oral health. It produces bacteriocins that kill Streptococcus pyogenes (strep throat bacteria).&lt;/p&gt;
    &lt;p&gt;At the same time, V. tobetsuensis increased in abundance from 2.1% to 5.7%. Veillonella bacteria can't eat sugar directly‚Äîthey survive by consuming lactate that Streptococcus produces. The S. salivarius bloom is plausibly feeding them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are these changes real or intra-day variation?&lt;/head&gt;
    &lt;p&gt;There was a lot more variation in species than I expected, especially comparing the two baseline samples. In retrospect, I should have taken multiple samples on the same day, and mixed them to smooth it out.&lt;/p&gt;
    &lt;p&gt;However, there is some light evidence that the variation I see is not just intra-day variation. Specifically, there are several species that stay consistent in frequency across all samples: e.g., Neisseria subflava, Streptococcus viridans, Streptococcus oralis.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;L. reuteri didn't detectably colonize my mouth. Oral probiotics are surprisingly difficult to detect, even if you sample the same day as dosing.&lt;/item&gt;
      &lt;item&gt;S. salivarius increased massively in abundance, but this increase happened after I stopped taking BioGaia&lt;/item&gt;
      &lt;item&gt;Microbiome sequencing can be used to assess oral health. None of the "red complex" bacteria (P. gingivalis, T. forsythia, T. denticola) associated with gum disease were found in any sample.&lt;/item&gt;
      &lt;item&gt;The oral microbiome is dynamic, with huge swings in species abundance over a timeframe of just weeks&lt;/item&gt;
      &lt;item&gt;Microbiome sequencing is very easy and convenient these days thanks to Plasmidsaurus&lt;/item&gt;
      &lt;item&gt;Prodentis tastes good, may help with oral health, and I'd consider taking it again&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518804</guid><pubDate>Tue, 06 Jan 2026 21:10:47 +0000</pubDate></item><item><title>Comparing AI agents to cybersecurity professionals in real-world pen testing</title><link>https://arxiv.org/abs/2512.09882</link><description>&lt;doc fingerprint="919c362341ffe08f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 10 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Comparing AI Agents to Cybersecurity Professionals in Real-World Penetration Testing&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We present the first comprehensive evaluation of AI agents against human cybersecurity professionals in a live enterprise environment. We evaluate ten cybersecurity professionals alongside six existing AI agents and ARTEMIS, our new agent scaffold, on a large university network consisting of ~8,000 hosts across 12 subnets. ARTEMIS is a multi-agent framework featuring dynamic prompt generation, arbitrary sub-agents, and automatic vulnerability triaging. In our comparative study, ARTEMIS placed second overall, discovering 9 valid vulnerabilities with an 82% valid submission rate and outperforming 9 of 10 human participants. While existing scaffolds such as Codex and CyAgent underperformed relative to most human participants, ARTEMIS demonstrated technical sophistication and submission quality comparable to the strongest participants. We observe that AI agents offer advantages in systematic enumeration, parallel exploitation, and cost -- certain ARTEMIS variants cost $18/hour versus $60/hour for professional penetration testers. We also identify key capability gaps: AI agents exhibit higher false-positive rates and struggle with GUI-based tasks.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518996</guid><pubDate>Tue, 06 Jan 2026 21:23:07 +0000</pubDate></item><item><title>Laylo (YC S20) ‚Äì Head of Growth (Organic and Partners and Loops and AI) ‚Äì Remote US</title><link>https://www.ycombinator.com/companies/laylo/jobs/ZtLHRXe-head-of-growth</link><description>&lt;doc fingerprint="b1dc8874f8cd840c"&gt;
  &lt;main&gt;
    &lt;p&gt;The CRM powering iconic musicians and events&lt;/p&gt;
    &lt;p&gt;Laylo is the Drop CRM powering iconic artists and live events. The platform combines landing pages, messaging, link tracking, and more to drive more tickets, merch and streams through Instagram DM, SMS and Email. Today, we power CRM across hundreds of millions of fans for some of the biggest names in entertainment like Sabrina Carpenter, Outside Lands and Skrillex.&lt;/p&gt;
    &lt;p&gt;Role Overview&lt;/p&gt;
    &lt;p&gt;We‚Äôre looking for a Head of Growth (player/coach) to build and run Laylo‚Äôs growth engine. A 0‚Üí1 builder who doesn‚Äôt just ideate, but ships. You‚Äôll create great content, run scrappy experiments, and build product growth loops that compound. This is a hands-on role: you‚Äôll set strategy, execute a rapid experiment cadence, measure results, and turn wins into repeatable playbooks.&lt;/p&gt;
    &lt;p&gt;You‚Äôll focus primarily on non-advertising channels: organic social, influencer/creator collaborations, channel partners, and product growth loops. We value strong taste, speed, and a rigorous learning cadence.&lt;/p&gt;
    &lt;p&gt;You‚Äôll report directly to the CEO and work closely with Product, Engineering, Design, Partnerships, and Sales. You‚Äôre likely a good fit if you‚Äôre excited to open Adobe/Figma/Notion/PostHog and ship something today.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;You‚Äôll thrive here if you can:&lt;/p&gt;
    &lt;p&gt;Key Responsibilities:&lt;/p&gt;
    &lt;p&gt;What Success Looks Like:&lt;/p&gt;
    &lt;p&gt;What You Bring:&lt;/p&gt;
    &lt;p&gt;How To Apply:&lt;/p&gt;
    &lt;p&gt;Send us your first out-of-the-box idea for your first campaign in this role. Bonus points for mentioning other companies and campaigns you think are relevant.&lt;/p&gt;
    &lt;p&gt;Our founders met while building competing consumer startups. We launched multiple products across consumer and SaaS and talked to thousands of fans and creators in the process. In 2020, we realized one of the biggest pain points artists and events face is actually driving their audience from socials into their own CRM.&lt;/p&gt;
    &lt;p&gt;In 2020, we joined Y Combinator‚Äôs summer batch and began building a product that quickly gained strong early traction. We raised from top-tier investors like Eldridge and Sony and have since grown into a team of 24 exceptional individuals spanning product, sales, and operations.&lt;/p&gt;
    &lt;p&gt;We have a strong written documentation culture. We try to do as much as possible asynchronously to move quickly and efficiently. We have a daily 30 minute standup and team-specific meetings throughout the week.&lt;/p&gt;
    &lt;p&gt;Creators and Brands have a few key moments that drive the majority of their sales and fan engagement, we call them drops. At Laylo, we're building the Drop CRM to make these moments perfect.&lt;/p&gt;
    &lt;p&gt;With Laylo, creators and brands can notify fans the second they drop new content, merch and events. From there, they get a full featured CRM, a dashboard to connect with fans forever in the future, high conversion landing pages and deep analytics to conversions, click throughs and sales.&lt;/p&gt;
    &lt;p&gt;We work with some of biggest creators, brands, records labels and managers in the world to create incredible drop experiences.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46519303</guid><pubDate>Tue, 06 Jan 2026 21:44:37 +0000</pubDate></item><item><title>CES 2026: Taking the Lids Off AMD's Venice and MI400 SoCs</title><link>https://chipsandcheese.com/p/ces-2026-taking-the-lids-off-amds</link><description>&lt;doc fingerprint="ee681e65ab90b9d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;CES 2026: Taking the Lids off AMD's Venice and MI400 SoCs&lt;/head&gt;
    &lt;p&gt;Hello you fine Internet folks,&lt;/p&gt;
    &lt;p&gt;Here at CES 2026, AMD showed off their upcoming Venice series of server CPUs and their upcoming MI400 series of datacenter accelerators. AMD has talked about the specifications of both Venice and the MI400 series at their Advancing AI event back in June of 2025, but this is the first time AMD has shown off the silicon for both of product lines.&lt;/p&gt;
    &lt;p&gt;Starting with Venice, the first thing to notice is the packaging of the CCDs to the IO dies is different. Instead of using the organic substrate of the package to run the wires between the CCDs and the IO dies that AMD has used since EPYC Rome, Venice appears to be using a more advanced form of packaging similar to Strix Halo or MI250X. Another change is that Venice appears to have two IO dies instead of the single IO die that the prior EPYC CPUs had.&lt;/p&gt;
    &lt;p&gt;Venice has 8 CCDs each of which have 32 cores for a total of up to 256 cores per Venice package. Doing some measuring of each of the dies, you get that each CCD is approximately 165mm2 of N2 silicon. If AMD has stuck to 4MB of L3 per core than each of these CCDs have 32 Zen 6 cores and 128MB of L3 cache along with the die to die interface for the CCD &amp;lt;-&amp;gt; IO die communications. At approximately 165mm2 per CCD, that would make a Zen 6 core plus the 4MB of L3 per core about 5mm2 each which is similar to Zen 5‚Äôs approximately 5.34mm2 on N3 when counting both the Zen 5 core and 4MB of L3 cache.&lt;/p&gt;
    &lt;p&gt;Moving to the IO dies, they each appear to be approximately 353mm2 for a total of just over 700mm2 of silicon dedicated for the IO dies. This is a massive increase from the approximately 400mm2 that the prior EPYC CPUs dedicated for their IO dies. The two IO dies appear to be using an advanced packaging of some kind similar to the CCDs. Next to the IO dies appear to be 8 little dies, 4 on each side of the package, which are likely to either be structural silicon or deep trench capacitor dies meant to improve power delivery to the CCDs and IO dies.&lt;/p&gt;
    &lt;p&gt;Shifting off of Venice and on to the MI400 accelerator, this is a massive package with 12 HBM4 dies and ‚Äútwelve 2 nanometer and 3 nanometer compute and IO dies‚Äù. It appears as if there are two base dies just like MI350. But unlike MI350, there appears to also be two extra dies on the top and bottom of the base dies. These two extra dies are likely for off-package IO such as PCIe, UALink, etc.&lt;/p&gt;
    &lt;p&gt;Calculating the die sizes of the base dies and the IO dies, the die size of the base die is approximately 747mm2 for each of the two base dies with the off-package IO dies each being approximately 220mm2. As for the compute dies, while the packaging precludes any visual demarcation of the different compute dies, it is likely that there are 8 compute dies with 4 compute dies on each base die. So while we can‚Äôt figure out the exact die size of the compute dies, the maximum size is approximately 180mm2. The compute chiplet is likely in the 140mm2 to 160mm2 region but that is a best guess that will have to wait to be confirmed.&lt;/p&gt;
    &lt;p&gt;The MI455X and Venice are the two SoCs that are going to be powering AMD‚Äôs Helios AI Rack but they aren‚Äôt the only new Zen 6 and MI400 series products that AMD announced at CES. AMD announced that there would be a third member of the MI400 family called the MI440X joining the MI430X and MI455X. The MI440X is designed to fit into the 8-way UBB boxes as a direct replacement for the MI300/350 series.&lt;/p&gt;
    &lt;p&gt;AMD also announced Venice-X which is likely is going to be a V-Cache version of Venice. This is interesting because not only did AMD skip Turin-X but if there is a 256 core version of Venice-X, then this would be the first time that a high core count CCD will have the ability to support a V-Cache die. If AMD sticks to the same ratio of base die cache to V-Cache die cache, then each 32 core CCD would have up to 384MB of L3 cache which equates to 3 Gigabytes of L3 cache across the chip.&lt;/p&gt;
    &lt;p&gt;Both Venice and the MI400 series are due to launch later this year and I can‚Äôt wait to learn more about the underlying architectures of both SoCs.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese, also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46519326</guid><pubDate>Tue, 06 Jan 2026 21:46:04 +0000</pubDate></item><item><title>High-Performance GPU Cuckoo Filter</title><link>https://github.com/tdortman/cuckoo-filter</link><description>&lt;doc fingerprint="4adfe2ece0e02bc8"&gt;
  &lt;main&gt;
    &lt;p&gt;A high-performance CUDA implementation of the Cuckoo Filter data structure, developed as part of the thesis "Design and Evaluation of a GPU-Accelerated Cuckoo Filter".&lt;/p&gt;
    &lt;p&gt;This library provides a GPU-accelerated Cuckoo Filter implementation optimized for high-throughput batch operations. Cuckoo Filters are space-efficient probabilistic data structures that support insertion, lookup, and deletion operations with a configurable false positive rate.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CUDA-accelerated batch insert, lookup, and delete operations&lt;/item&gt;
      &lt;item&gt;Configurable fingerprint size and bucket size&lt;/item&gt;
      &lt;item&gt;Multiple eviction policies (DFS, BFS)&lt;/item&gt;
      &lt;item&gt;Sorted insertion mode for improved memory coalescing&lt;/item&gt;
      &lt;item&gt;Multi-GPU support via gossip&lt;/item&gt;
      &lt;item&gt;IPC support for cross-process filter sharing&lt;/item&gt;
      &lt;item&gt;Header-only library design&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarks at 80% load factor on an NVIDIA GH200 (H100 HBM3, 3.4 TB/s). The GPU Cuckoo Filter is compared against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU Cuckoo Filter&lt;/item&gt;
      &lt;item&gt;Bulk Two-Choice Filter (TCF)&lt;/item&gt;
      &lt;item&gt;GPU Counting Quotient Filter (GQF)&lt;/item&gt;
      &lt;item&gt;GPU Blocked Bloom Filter&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Comparison&lt;/cell&gt;
        &lt;cell role="head"&gt;Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Delete&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPU vs CPU Cuckoo&lt;/cell&gt;
        &lt;cell&gt;360√ó faster&lt;/cell&gt;
        &lt;cell&gt;973√ó faster&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs TCF&lt;/cell&gt;
        &lt;cell&gt;6√ó faster&lt;/cell&gt;
        &lt;cell&gt;42√ó faster&lt;/cell&gt;
        &lt;cell&gt;100√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs GQF&lt;/cell&gt;
        &lt;cell&gt;585√ó faster&lt;/cell&gt;
        &lt;cell&gt;6√ó faster&lt;/cell&gt;
        &lt;cell&gt;273√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cuckoo vs Bloom&lt;/cell&gt;
        &lt;cell&gt;0.6√ó (slower)&lt;/cell&gt;
        &lt;cell&gt;1.4√ó faster&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Comparison&lt;/cell&gt;
        &lt;cell role="head"&gt;Insert&lt;/cell&gt;
        &lt;cell role="head"&gt;Query&lt;/cell&gt;
        &lt;cell role="head"&gt;Delete&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPU vs CPU Cuckoo&lt;/cell&gt;
        &lt;cell&gt;583√ó faster&lt;/cell&gt;
        &lt;cell&gt;1504√ó faster&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs TCF&lt;/cell&gt;
        &lt;cell&gt;1.9√ó faster&lt;/cell&gt;
        &lt;cell&gt;11.3√ó faster&lt;/cell&gt;
        &lt;cell&gt;35.3√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Cuckoo vs GQF&lt;/cell&gt;
        &lt;cell&gt;9.6√ó faster&lt;/cell&gt;
        &lt;cell&gt;2.6√ó faster&lt;/cell&gt;
        &lt;cell&gt;3.8√ó faster&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Cuckoo vs Bloom&lt;/cell&gt;
        &lt;cell&gt;0.7√ó (slower)&lt;/cell&gt;
        &lt;cell&gt;1.0√ó (equal)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For a more comprehensive evaluation including additional systems and analysis, see the accompanying thesis.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CUDA Toolkit (&amp;gt;= 12.9)&lt;/item&gt;
      &lt;item&gt;C++20 compatible compiler&lt;/item&gt;
      &lt;item&gt;Meson build system (&amp;gt;= 1.3.0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;meson setup build
meson compile -C build&lt;/code&gt;
    &lt;p&gt;Benchmarks and tests are built by default. To disable them:&lt;/p&gt;
    &lt;code&gt;meson setup build -DBUILD_BENCHMARKS=false -DBUILD_TESTS=false&lt;/code&gt;
    &lt;code&gt;#include &amp;lt;CuckooFilter.cuh&amp;gt;

// Configure the filter: key type, fingerprint bits, max evictions, block size, bucket size
using Config = CuckooConfig&amp;lt;uint64_t, 16, 500, 256, 16&amp;gt;;

// Create a filter with the desired capacity
CuckooFilter&amp;lt;Config&amp;gt; filter(1 &amp;lt;&amp;lt; 20);  // capacity for ~1M items

// Insert keys (d_keys is a device pointer)
filter.insertMany(d_keys, numKeys);

// Or use sorted insertion
filter.insertManySorted(d_keys, numKeys);

// Check membership
filter.containsMany(d_keys, d_results, numKeys);

// Delete keys
filter.deleteMany(d_keys, d_results, numKeys);&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;CuckooConfig&lt;/code&gt; template accepts the following parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;T&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Key type&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;bitsPerTag&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fingerprint size in bits (8, 16, 32)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;maxEvictions&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maximum eviction attempts before failure&lt;/cell&gt;
        &lt;cell&gt;500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;blockSize&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CUDA block size&lt;/cell&gt;
        &lt;cell&gt;256&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;bucketSize&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Slots per bucket (must be power of 2)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;AltBucketPolicy&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Alternate bucket calculation policy&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;XorAltBucketPolicy&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;evictionPolicy&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Eviction strategy (DFS or BFS)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;BFS&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;WordType&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Atomic type (uint32_t or uint64_t)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;uint64_t&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For workloads that exceed single GPU capacity:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;CuckooFilterMultiGPU.cuh&amp;gt;

CuckooFilterMultiGPU&amp;lt;Config&amp;gt; filter(numGPUs, totalCapacity);
filter.insertMany(h_keys, numKeys);
filter.containsMany(h_keys, h_results, numKeys);&lt;/code&gt;
    &lt;code&gt;include/           - Header files
  CuckooFilter.cuh           - Main filter implementation
  CuckooFilterMultiGPU.cuh   - Multi-GPU implementation
  CuckooFilterIPC.cuh        - IPC support
  bucket_policies.cuh        - Alternative bucket policies
  helpers.cuh                - Helper functions
src/               - Example applications
benchmark/         - benchmarks
tests/             - Unit tests
scripts/           - Scripts for running/plotting benchmarks
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46519771</guid><pubDate>Tue, 06 Jan 2026 22:33:21 +0000</pubDate></item></channel></rss>