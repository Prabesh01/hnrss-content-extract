<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Sep 2025 14:38:49 +0000</lastBuildDate><item><title>Defeating Nondeterminism in LLM Inference</title><link>https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/</link><description>&lt;doc fingerprint="1758903540291f97"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Defeating Nondeterminism in LLM Inference&lt;/head&gt;&lt;p&gt;Reproducibility is a bedrock of scientific progress. However, it’s remarkably difficult to get reproducible results out of large language models.&lt;/p&gt;&lt;p&gt;For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves “sampling”, a process that converts the language model’s output into a probability distribution and probabilistically selects a token.&lt;/p&gt;&lt;p&gt;What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn’t deterministic (see here or here).&lt;/p&gt;&lt;p&gt;But why aren’t LLM inference engines deterministic? One common hypothesis is that some combination of floating-point non-associativity and concurrent execution leads to nondeterminism based on which concurrent core finishes first. We will call this the “concurrency + floating point” hypothesis for LLM inference nondeterminism. For example, a recent arXiv preprint writes:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Floating-point arithmetic in GPUs exhibits non-associativity, meaning $(a + b) + c \neq a + (b + c)$ due to finite precision and rounding errors. This property directly impacts the computation of attention scores and logits in the transformer architecture, where parallel operations across multiple threads can yield different results based on execution order.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;You can also find the “concurrency + floating point” hypothesis repeated by others, like here (“There are speed tradeoffs, and in order to make the endpoints fast GPUs are used, which do parallel [nondeterministic] calculations. Any modern GPU neural net calculations will be subject to these."), or here (“Because GPUs are highly parallelized, the ordering of additions or multiplications might be different on each execution, which can cascade into small differences in output.").&lt;/p&gt;&lt;p&gt;While this hypothesis is not entirely wrong, it doesn’t reveal the full picture. For example, even on a GPU, running the same matrix multiplication on the same data repeatedly will always provide bitwise equal results. We’re definitely using floating-point numbers. And our GPU definitely has a lot of concurrency. Why don’t we see nondeterminism in this test?&lt;/p&gt;&lt;code&gt;A = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
B = torch.randn(2048, 2048, device='cuda', dtype=torch.bfloat16)
ref = torch.mm(A, B)
for _ in range(1000):
    assert (torch.mm(A, B) - ref).abs().max().item() == 0
&lt;/code&gt;&lt;p&gt;To understand the true cause of LLM inference nondeterminism, we must look deeper.&lt;/p&gt;&lt;p&gt;Unfortunately, even defining what it means for LLM inference to be deterministic is difficult. Perhaps confusingly, the following statements are all simultaneously true:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Some kernels on GPUs are nondeterministic.&lt;/item&gt;&lt;item&gt;However, all the kernels used in a language model’s forward pass are deterministic.&lt;/item&gt;&lt;item&gt;Moreover, the forward pass of an LLM inference server (like vLLM) can also be claimed to be deterministic.&lt;/item&gt;&lt;item&gt;Nevertheless, from the perspective of anybody using the inference server, the results are nondeterministic.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In this post, we will explain why the “concurrency + floating point” hypothesis misses the mark, unmask the true culprit behind LLM inference nondeterminism, and explain how to defeat nondeterminism and obtain truly reproducible results in LLM inference.&lt;/p&gt;&lt;head rend="h2"&gt;The original sin: floating-point non-associativity&lt;/head&gt;&lt;p&gt;Before talking about nondeterminism, it’s useful to explain why there are numerical differences at all. After all, we typically think of machine learning models as mathematical functions following structural rules such as commutativity or associativity. Shouldn’t there be a “mathematically correct” result that our machine learning libraries should provide us?&lt;/p&gt;&lt;p&gt;The culprit is floating-point non-associativity. That is, with floating-point numbers:&lt;/p&gt;$$ (a + b) + c \neq a + (b + c) $$&lt;code&gt;(0.1 + 1e20) - 1e20
&amp;gt;&amp;gt;&amp;gt; 0
0.1 + (1e20 - 1e20)
&amp;gt;&amp;gt;&amp;gt; 0.1
&lt;/code&gt;&lt;p&gt;Ironically, breaking associativity is what makes floating-point numbers useful.&lt;/p&gt;&lt;p&gt;Floating-point numbers are useful because they allow for a “dynamic” level of precision. For the purposes of explanation, we will use base 10 (instead of binary), where floating-point numbers are in the format $\text{mantissa} * 10^\text{exponent}$. We will also use 3 digits for the mantissa and 1 digit for the exponent.&lt;/p&gt;&lt;p&gt;For example, for the value 3450, we can represent it exactly as $3.45 * 10^3$. We can also represent much smaller values like 0.486 as $4.86 * 10^{-1}$. In this way, floating point allows us to represent both very small as well as very large values. In the sciences, we might say that floating point allows us to maintain a constant number of “significant figures”.&lt;/p&gt;&lt;p&gt;If you add together two floating-point numbers with the same exponent, it looks similar to integer addition. For example, 123 ($1.23 * 10^2$) + 456 ($4.56 * 10^2$) results in 579 ($5.79 * 10^2$).&lt;/p&gt;&lt;p&gt;But what happens when we add two floating-point numbers with different exponents, such as 1230 and 23.4? In this case, the exact result is 1253.4. However, we can only maintain 3 digits of precision at a time. Floating-point addition will thus drop the last 2 digits and obtain the value $1.25 * 10^3$ (or 1250).&lt;/p&gt;&lt;p&gt;At this point, however, we’ve destroyed information. Note that this can happen every time we add two floating-point numbers with different “scales” (i.e. different exponents). And adding together floating-point numbers with different exponents happens all of the time. In fact, if we could guarantee that we never needed different exponents, we could just use integers!&lt;/p&gt;&lt;p&gt;In other words, every time we add together floating-point numbers in a different order, we can get a completely different result. To take an extreme example, there are 102 possible different results for summing this array depending on the order.&lt;/p&gt;&lt;code&gt;import random

vals = [1e-10, 1e-5, 1e-2, 1]
vals = vals + [-v for v in vals]

results = []
random.seed(42)
for _ in range(10000):
    random.shuffle(vals)
    results.append(sum(vals))

results = sorted(set(results))
print(f"There are {len(results)} unique results: {results}")

# Output:
# There are 102 unique results: [-8.326672684688674e-17, -7.45931094670027e-17, ..., 8.326672684688674e-17]
&lt;/code&gt;&lt;p&gt;Although this is the underlying cause for non-identical outputs, it does not directly answer where the nondeterminism comes from. It doesn’t help us understand why floating-point values get added in different orders, when that happens, nor how it can be avoided.&lt;/p&gt;&lt;p&gt;The answers lie in how kernels are implemented.&lt;/p&gt;&lt;head rend="h2"&gt;Why don’t kernels always add numbers in the same order?&lt;/head&gt;&lt;p&gt;As mentioned above, one common explanation for why kernels add numbers in different orders is the “concurrency + floating point” hypothesis. The hypothesis states that if the order in which concurrent threads finish is nondeterministic and the accumulation order depends on the order in which concurrent threads finish (such as with an atomic add), our accumulation order will be nondeterministic as well.&lt;/p&gt;&lt;p&gt;Confusingly, although this can lead to nondeterministic kernels, concurrency (and atomic adds) end up being completely uninvolved in LLM inference nondeterminism! To explain what the real culprit is, let’s first understand why modern GPU kernels rarely need atomic adds.&lt;/p&gt;&lt;head rend="h2"&gt;When are atomic adds needed?&lt;/head&gt;&lt;p&gt;Typically a GPU launches a program concurrently across many “cores” (i.e. SMs). As the cores have no inherent synchronization among them, this poses a challenge if the cores need to communicate among each other. For example, if all cores must accumulate to the same element, you can use an “atomic add” (sometimes known as a “fetch-and-add”). The atomic add is “nondeterministic” — the order in which the results accumulate is purely dependent on which core finishes first.&lt;/p&gt;&lt;p&gt;Concretely, imagine that you are reducing a 100-element vector with 100 cores (e.g. &lt;code&gt;torch.sum()&lt;/code&gt;). Although you can load all 100 elements in parallel, we must eventually reduce down to a single element. One way to accomplish this is with some kind of “atomic add” primitive, where the hardware guarantees that all additions will be processed but does not guarantee the order.&lt;/p&gt;&lt;p&gt;This is usually what folks mean by “nondeterminism” — you execute the same kernel twice with exactly the same inputs and you get a different result out. This is known as run-to-run nondeterminism, where you run the same python script twice with the exact same dependencies but get a different result.&lt;/p&gt;&lt;p&gt;Although concurrent atomic adds do make a kernel nondeterministic, atomic adds are not necessary for the vast majority of kernels. In fact, in the typical forward pass of an LLM, there is usually not a single atomic add present.&lt;/p&gt;&lt;p&gt;This may be surprising, given that parallelizing a reduction can benefit from atomic adds. There are two main reasons why atomic adds do not end up being needed.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;There is often sufficient parallelism along the “batch” dimension that we don’t need to parallelize along the reduction dimension. For example, let’s say that instead of reducing a single 100-dim vector we were reducing 500 vectors in parallel. In this case, we can reduce an entire vector in each core and allow every core to operate on a different vector.&lt;/item&gt;&lt;item&gt;Over time, most neural network libraries have adopted a variety of strategies for achieving determinism without sacrificing performance. For example, we can perform a “split” (or tree) reduction, where we split the 100-element reduction into five 20-element reductions (thus achieving five-way parallelism). Then, to combine the remaining five elements, we can either perform a separate “clean-up” reduction (which isn’t parallelized, but operates over few enough elements to be cheap) or utilize a semaphore (which ensures that each concurrent thread-block will accumulate in a deterministic order).The semaphore strategy can be found described here.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Due to these two factors, avoiding atomics adds is a negligible performance penalty for the vast majority of neural network operations.&lt;/p&gt;&lt;p&gt;There are still a couple of common operations that have significant performance penalties for avoiding atomics. For example, &lt;code&gt;scatter_add&lt;/code&gt; in PyTorch (&lt;code&gt;a[b] += c&lt;/code&gt;). The only one commonly used in LLMs, however, is FlashAttention backward.Fun fact: did you know that the widely used Triton implementations of FlashAttention backward actually differ algorithmically from Tri Dao’s FlashAttention-2 paper? The standard Triton implementation does additional recomputation in the backward pass, avoiding atomics but costing 40% more FLOPs!&lt;/p&gt;&lt;p&gt;However, the forward pass of an LLM involves no operations that require atomic adds. Thus, the forward pass in an LLM is in fact “run-to-run deterministic.”&lt;/p&gt;&lt;p&gt;Wikipedia writes that “a deterministic algorithm is an algorithm that, given a particular input, will always produce the same output.” And in this case, given the exact same inputs (i.e. the exact requests the inference server is processing), the forward pass always produces the exact same outputs.&lt;/p&gt;&lt;p&gt;However, the forward pass itself being “deterministic” is not sufficient to ensure that a system that includes it is deterministic. For example, what if our request’s output depended on the parallel user requests (e.g. batch-norm)? Since each individual request has no way of knowing what the parallel requests will be, from their perspective our overall LLM inference is also nondeterministic!&lt;/p&gt;&lt;p&gt;As it turns out, our request’s output does depend on the parallel user requests. Not because we’re somehow leaking information across batches — instead, it’s because our forward pass lacks “batch invariance”, causing our request’s output to depend on the batch size of our forward pass.&lt;/p&gt;&lt;head rend="h3"&gt;Batch invariance and “determinism”&lt;/head&gt;&lt;p&gt;To explain batch invariance, let’s simplify the system and look solely at matmuls. You can assume that all matmul implementations are “run-to-run deterministic."This is not totally true, but most common matmul implementations do have this property. However, they are not “batch-invariant.” In other words, when the batch size changes, each element in the batch can get different results.&lt;/p&gt;&lt;p&gt;This is a fairly unusual property from a mathematical perspective. Matrix multiplication should be “independent” along every element in the batch — neither the other elements in the batch nor how large the batch is should affect the computation results of a specific element in the batch.&lt;/p&gt;&lt;p&gt;However, as we can observe empirically, this isn’t true.&lt;/p&gt;&lt;code&gt;import torch
torch.set_default_device('cuda') 

B = 2048
D = 4096
a = torch.linspace(-1000, 1000, B*D).reshape(B, D)
b = torch.linspace(-1000, 1000, D*D).reshape(D, D)
# Doing a matrix vector multiplication by taking
# the first element of the batch
out1 = torch.mm(a[:1], b)
# Doing a matrix matrix multiplication and then taking
# the first element of the batch
out2 = torch.mm(a, b)[:1]
print((out1 - out2).abs().max()) # tensor(1669.2500, device='cuda:0')
&lt;/code&gt;&lt;p&gt;Note that this is “run-to-run deterministic.” If you run the script multiple times, it will deterministically return the same result.It is not “hardware/software version invariant” — your GPU/PyTorch version may return a different value, but it should deterministically return the same value.&lt;/p&gt;&lt;p&gt;However, when a non-batch-invariant kernel is used as part of a larger inference system, the system can become nondeterministic. When you make a query to an inference endpoint, the amount of load the server is under is effectively “nondeterministic” from the user’s perspective. The load determines the batch size that the kernels are run under, and thus changes the eventual result of each individual request!&lt;/p&gt;&lt;p&gt;If you compose some property under which the kernel is not invariant (i.e. batch-size) with nondeterminism of that property (i.e. the load the server is under), you get a nondeterministic system.&lt;/p&gt;&lt;p&gt;In other words, the primary reason nearly all LLM inference endpoints are nondeterministic is that the load (and thus batch-size) nondeterministically varies! This nondeterminism is not unique to GPUs — LLM inference endpoints served from CPUs or TPUs will also have this source of nondeterminism.&lt;/p&gt;&lt;p&gt;So, if we’d like to avoid nondeterminism in our inference servers, we must achieve batch invariance in our kernels. In order to understand how that can be achieved, let’s first take a look at why kernels don’t have batch invariance in the first place.&lt;/p&gt;&lt;head rend="h2"&gt;How do we make kernels batch-invariant?&lt;/head&gt;&lt;p&gt;In order to make a transformer implementation batch-invariant, we must make every kernel batch-invariant. Luckily, we can assume that every pointwise operation is batch-invariant.Although this is true for all kernels in say, PyTorch, it’s not inherently true. For example, there are some kernel implementations on CPU that will use vectorized intrinsics on some parts of the array and non-vectorized intrinsics on other parts, and these intrinsics don’t necessarily always have bitwise identical numerics. Thus, we only need to worry about the 3 operations that involve reductions — RMSNorm, matrix multiplication, and attention.Reductions related to parallelism are out of the scope of this discussion, but the same principles apply. One factoid that may be useful is that NVLink-Sharp in-switch reductions are deterministic on Blackwell as well as Hopper with CUDA 12.8+. As is the case with many things, this information can be found on NCCL’s github issues&lt;/p&gt;&lt;p&gt;Conveniently, these are also ordered in ascending levels of difficulty. Each one requires some additional considerations to achieve batch invariance with reasonable performance. Let’s talk about RMSNorm first.&lt;/p&gt;&lt;head rend="h3"&gt;Batch-invariant RMSNorm&lt;/head&gt;RMSNorm can be implemented as:&lt;code&gt;# x: [batch_size, hidden_dim]
# weight: [hidden_dim]
def rms_norm(x, weight):
    return x * torch.rsqrt(torch.mean(x ** 2, dim=-1, keepdim=True)) * weight
&lt;/code&gt;&lt;p&gt;The requirement for batch invariance is that the reduction order for each element must be fixed regardless of the batch-size of the kernel. Note that this doesn’t mean we must always use the same reduction strategy. For example, if we change the number of elements we’re reducing over, we can still be batch-invariant even if our reduction strategy changes.The Quack blog post has some nice examples showing the hierarchy of various reduction strategies you can do (e.g. thread reduction, warp reduction, block reduction, cluster reduction).&lt;/p&gt;&lt;p&gt;Thus, we only break batch invariance when our batch-size affects the reduction strategy.&lt;/p&gt;&lt;p&gt;Let’s look at the standard parallelism strategy for RMSNorm. Generally, parallel algorithms benefit from minimizing communication across cores. For the purpose of this discussion you can assume that when we refer to “cores” we mean SMs. More specifically, the property here that’s important is that the # of threadblocks our kernel launches is greater than the # of SMs. So, one strategy we can start with is to assign each batch element to one core, as seen in the above figure.&lt;/p&gt;&lt;p&gt;Increasing our batch size doesn’t affect our reduction strategy; if a batch size of 200 provides sufficient parallelism to our kernel then a batch size of 2000 will definitely provide sufficient parallelism.&lt;/p&gt;&lt;p&gt;On the other hand, decreasing the batch size can pose challenges. Because we assign each batch element to one core, decreasing our batch size will eventually lead to having more cores than batch elements, leaving some cores idle.&lt;/p&gt;&lt;p&gt;Upon encountering this situation, a good kernel engineer would reach for one of the solutions mentioned in the prior section (atomic adds or split reductions), maintaining good parallelism and thus, good performance. Unfortunately, this changes the reduction strategy, preventing this kernel from being batch-invariant.&lt;/p&gt;&lt;p&gt;The easiest solution is to simply ignore these cases altogether. This is not completely unreasonable — a small batch size means that the kernel is likely to execute quickly anyways, and so a slowdown may not be catastrophic.&lt;/p&gt;&lt;p&gt;If we were compelled to optimize this use case, one approach would be to consistently use a reduction strategy that has enough parallelism even for very small batch sizes. Such a reduction strategy would lead to an excess amount of parallelism for larger batch sizes but would allow us to achieve decent (but not peak) performance across the entire range of sizes.&lt;/p&gt;&lt;head rend="h3"&gt;Batch-invariant matrix multiplication&lt;/head&gt;&lt;p&gt;At its core, you can view matrix multiplication as simply a pointwise operation followed by a reduction. Then, if we parallelize our matrix multiplication by chunking the output into tiles, we have an analogous “data-parallel” kernel strategy that keeps each reduction within one core.&lt;/p&gt;&lt;p&gt;Also similar to RMSNorm, it is possible for our “batch” dimensions (M and N) to become too small, forcing us to split along the reduction dimension (K). Despite having two “batch” dimensions, matmuls also require us to have much more “work” per core in order to leverage tensorcores effectively. For example, if you have a [1024, K] x [K, 1024] matmul and a standard 2D tile size of [128, 128], a data-parallel strategy would only be able to split this matmul into 64 cores, insufficient to saturate the GPU.&lt;/p&gt;&lt;p&gt;Splitting along the reduction dimension in a matmul is known as a Split-K Matmul. And just like RMSNorm, using this strategy breaks batch invariance. Another interesting parallelism strategy for matmuls is stream-k. Stream-k is interesting because it has even less invariance than typical matmuls. As discussed, most matmul libraries are not batch-invariant, but they’re at least what you could call batch-position-invariant (i.e. changing the position of the element within the batch does not affect numerics). However, stream-k is not batch-position-invariant either! Its core insight is that you can get cleaner load-balancing by splitting along k in different ways for different output tiles, but taking advantage of this makes our kernel not batch-position-invariant either.&lt;/p&gt;&lt;p&gt;There’s an additional complexity with matmuls — tensor core instructions. Whereas with reductions we could simply operate on one row at a time, efficient matrix multiplication kernels must operate on an entire “tile” at a time.&lt;/p&gt;&lt;p&gt;Each tensor-core instruction (like say, &lt;code&gt;wgmma.mma_async.sync.aligned.m64n128k16&lt;/code&gt;) may have a different reduction order internally. One reason to use a different tensor-core instruction might be that the batch size is very small. For example, if we use a tensor-core PTX instruction that operates on a tile of length 256 but the batch size is only 32, we’re wasting almost all of that compute! At a batch-size of 1, the fastest kernels usually don’t use tensor cores at all.&lt;/p&gt;&lt;p&gt;So, the easiest way to ensure batch invariance for matmuls is to compile one kernel configuration and use that for all shapes. Although we will lose some performance, this isn’t typically disastrous in LLM inference. In particular, split-k is most needed when both M and N are small, and luckily in our case, N (i.e. the model dim) is usually pretty large!&lt;/p&gt;&lt;head rend="h3"&gt;Batch-invariant attention&lt;/head&gt;&lt;p&gt;After obtaining batch invariance for matmuls, attention introduces two additional wrinkles — fittingly, because it contains two matmuls.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;As opposed to only reducing over the feature dimension like both RMSNorm and matmuls, we now reduce over the feature dimension and sequence dimension.&lt;/item&gt;&lt;item&gt;Due to the above, attention must deal with a variety of inference optimizations that affect how sequences get processed (chunked prefill, prefix caching, etc.).&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Thus, to achieve determinism in LLM inference our numerics must be invariant to both how many requests are processed at once and how each request gets sliced up in the inference engine.&lt;/p&gt;&lt;p&gt;Let’s first walk through the standard parallelism strategy for attention, first introduced in FlashAttention2. Similar to RMSNorm and Matmul, the default strategy is a “data-parallel” strategy. Since we reduce along the key/value tensors, a data-parallel strategy can only parallelize along the query tensor.&lt;/p&gt;&lt;p&gt;For example, depending on the inference engine’s choices, it’s possible that a sequence might get processed in several parts (such as in chunked prefill) or perhaps all at once (if the prefill isn’t split up). In order to achieve “batch invariance”, it’s necessary that the reduction order for a given token does not depend on how many other tokens from its sequence are being simultaneously processed. If you reduce over the K/V values in the KV cache separately from the K/V values in the current tokens being processed (like in vLLM’s Triton attention kernel), this can’t be achieved. For example, when processing the 1000th query token in a sequence, the reduction order must be identical regardless of whether 0 tokens are in the KV cache (prefill) or 999 tokens are in the KV cache (decoding).&lt;/p&gt;&lt;p&gt;To resolve this, we can just update the KV cache and page table before the attention kernel itself, ensuring that our keys and values are always consistently laid out regardless of how many tokens are being processed.&lt;/p&gt;&lt;p&gt;With this additional detail (as well as all the things mentioned in the previous section, like consistent tile sizes), we are able to achieve a batch-invariant attention implementation!&lt;/p&gt;&lt;p&gt;However, there is a significant problem here. Unlike with matrix multiplication, the attention shapes we see in LLM inference often do require a split-reduction kernel, often known as Split-KV or FlashDecoding. This is because if we don’t parallelize along the reduction, we can only parallelize along the batch dimension, head dimension, and “query length” dimension. In the decode stage of attention, query length is very small, and so unless we have a very large batch size we are often unable to saturate the GPU.&lt;/p&gt;&lt;p&gt;Unfortunately, it’s not as easy to ignore this case as it was for RMSNorm and Matmuls. For example, if you have a very long KV cache, the attention kernel may take a very long time despite only processing one request.&lt;/p&gt;&lt;p&gt;Furthermore, the split-reduction strategies commonly used for attention also pose challenges for batch invariance. For example, FlashInfer’s “balanced scheduling algorithm” chooses the largest split-size that can still saturate all the GPU’s cores, thus making the reduction strategy not “batch-invariant”. However, unlike with RMSNorm/Matmuls, it’s not sufficient to choose a fixed number of splits regardless of the batch size.&lt;/p&gt;&lt;p&gt;Instead, to achieve batch invariance, we must adopt a “fixed split-size” strategy. In other words, instead of fixing the # of splits, we fix the size of each split and then end up with a varying number of splits. In this manner, we can guarantee that regardless of how many tokens we’re processing, we always perform the identical reduction order. This requires some internal FlexAttention changes that are not included in our code release. We will upstream them in the near future!&lt;/p&gt;&lt;head rend="h2"&gt;Implementation&lt;/head&gt;&lt;p&gt;We provide a demonstration of deterministic inference on top of vLLM by leveraging its FlexAttention backend as well as torch.Library. Through torch.Library, we’re able to substitute out most of the relevant PyTorch operators in an unintrusive way. You can find the library of “batch-invariant” kernels at thinking-machines-lab/batch-invariant-ops, as well as the vLLM example of running in “deterministic” mode.&lt;/p&gt;&lt;head rend="h2"&gt;Experiments&lt;/head&gt;&lt;head rend="h3"&gt;How nondeterministic are completions?&lt;/head&gt;&lt;p&gt;We use &lt;code&gt;Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/code&gt; and sample 1000 completions at temperature 0 with the prompt “Tell me about Richard Feynman” (non-thinking mode), generating 1000 tokens each. Surprisingly, we generate 80 unique completions, with the most common of these occuring 78 times.&lt;/p&gt;&lt;p&gt;Looking at where the completions differ, we see that the completions are actually identical for the first 102 tokens! The first instance of diverging completions occurs at the 103rd token. All completions generate the sequence “Feynman was born on May 11, 1918, in” However, 992 of the completions go on to generate “Queens, New York” whereas 8 of the completions generate “New York City”.&lt;/p&gt;&lt;p&gt;On the other hand, when we enable our batch-invariant kernels, all of our 1000 completions are identical. This is what we would mathematically expect from our sampler, but we aren’t able to achieve deterministic results without our batch-invariant kernels.&lt;/p&gt;&lt;head rend="h3"&gt;Performance&lt;/head&gt;&lt;p&gt;We have not put a significant effort into optimizing the performance of the batch-invariant kernels here. However, let’s run some experiments to verify that our performance remains usable.&lt;/p&gt;&lt;p&gt;We will set up an API server with one GPU running Qwen-3-8B, and request 1000 sequences with an output length of between 90 and 110.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Configuration&lt;/cell&gt;&lt;cell role="head"&gt;Time (seconds)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;vLLM default&lt;/cell&gt;&lt;cell&gt;26&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Unoptimized Deterministic vLLM&lt;/cell&gt;&lt;cell&gt;55&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;+ Improved Attention Kernel&lt;/cell&gt;&lt;cell&gt;42&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Much of the slowdown comes from the fact that the FlexAttention integration in vLLM has not been heavily optimized yet. Nevertheless, we see that performance is not disastrous.&lt;/p&gt;&lt;head rend="h3"&gt;True on-policy RL&lt;/head&gt;&lt;p&gt;As researchers have noted, the different numerics between training and inference implicitly turns our on-policy RL into off-policy RL.&lt;/p&gt;&lt;p&gt;Of course, it is impossible to get bitwise identical results between training and inference if we can’t even get bitwise identical results from two identical inference requests. Then, deterministic inference enables us to also modify our training stack to obtain bitwise identical results between sampling and training, thus resulting in true on-policy RL.&lt;/p&gt;&lt;p&gt;We run experiments in a RLVR setup on Bigmath with the RL policy initialized from the Qwen 2.5-VL instruct 8B with a max rollout length of 4096.&lt;/p&gt;&lt;p&gt;If we train without off-policy correction (i.e. importance weighting), our reward collapses partway through training, whereas adding an off-policy correction term allows training to proceed smoothly. But, if we achieve bitwise identical results between our sampler and trainer, we are fully on policy (i.e. 0 KL divergence) and can also train smoothly.&lt;/p&gt;&lt;p&gt;We can also plot the KL-divergence in logprobs between our sampler and trainer, where all 3 runs have notably different behavior. When running with importance weighting, it stays around 0.001 with occasional spikes. However, running without importance weighting eventually leads to a spike in KL-divergence around the same time that reward crashes. And, of course, when running “True On-Policy RL”, our KL-divergence stays flat at 0, indicating that there is no divergence between the training policy and sampling policy.&lt;/p&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;Modern software systems contain many layers of abstractions. In machine learning, when we run into nondeterminism and subtle numerical differences it can often be tempting to paper over them. After all, our systems are already “probabilistic”, so what’s wrong with a little more nondeterminism? What’s wrong with bumping up the atol/rtol on the failing unit test? The difference in logprobs between the trainer and the sampler probably isn’t a real bug, right?&lt;/p&gt;&lt;p&gt;We reject this defeatism. With a little bit of work, we can understand the root causes of our nondeterminism and even solve them! We hope that this blog post provides the community with a solid understanding of how to resolve nondeterminism in our inference systems and inspires others to obtain a full understanding of their systems.&lt;/p&gt;&lt;head rend="h2"&gt;Citation&lt;/head&gt;&lt;p&gt;Please cite this work as:&lt;/p&gt;&lt;code&gt;He, Horace and Thinking Machines Lab, "Defeating Nondeterminism in LLM Inference", 
Thinking Machines Lab: Connectionism, Sep 2025.
&lt;/code&gt;&lt;p&gt;Or use the BibTeX citation:&lt;/p&gt;&lt;code&gt;@article{he2025nondeterminism,
  author = {Horace He and Thinking Machines Lab},
  title = {Defeating Nondeterminism in LLM Inference},
  journal = {Thinking Machines Lab: Connectionism},
  year = {2025},
  note = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  doi = {10.64434/tml.20250910}
}
&lt;/code&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45200925</guid></item><item><title>KDE launches its own distribution</title><link>https://lwn.net/SubscriberLink/1037166/caa6979c16a99c9e/</link><description>&lt;doc fingerprint="3d1f381ad9a5a4b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;KDE launches its own distribution (again)&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;At Akademy 2025, the KDE Project released an alpha version of KDE Linux, a distribution built by the project to "&lt;quote&gt;include the best implementation of everything KDE has to offer, using the most advanced technologies&lt;/quote&gt;". It is aimed at providing an operating system suitable for home use, business use, OEM installations, and more "&lt;quote&gt;eventually&lt;/quote&gt;". For now there are many rough edges and missing features that users should be aware of before taking the plunge; but it is an interesting look at the kind of complete Linux system that KDE developers would like to see.&lt;/p&gt;
    &lt;head rend="h4"&gt;Development and goals&lt;/head&gt;
    &lt;p&gt;KDE contributor Nate Graham wrote an announcement blog post on September 6 to accompany the release of KDE Linux. Harald Sitter had introduced the project as "Project Banana" during a talk (video, slides) at Akademy in 2024, and has been leading its development along with major contributions from Hadi Chokr, Lasath Fernando, Justin Zobel, Graham, and others.&lt;/p&gt;
    &lt;p&gt;KDE Linux is an immutable distribution that uses Arch Linux packages as its base, but Graham notes that it is "&lt;quote&gt;definitely not an 'Arch-based distro!'&lt;/quote&gt;" Pacman is not included, and Arch is used only for the base operating system. Everything else, he said, is either compiled from source using KDE Builder or installed using Flatpak.&lt;/p&gt;
    &lt;p&gt;Some may wonder why another Linux distribution is needed; Graham said that he has expressed that sentiment himself in the past regarding other distributions, but he thinks that KDE Linux is justified:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;KDE is a huge producer of software. It's awkward for us to not have our own method of distributing it. Yes, KDE produces source code that others distribute, but we self-distribute our apps on app stores like Flathub and the Snap and Microsoft stores, so I think it's a natural thing for us to have our own platform for doing that distribution too, and that's an operating system. I think all the major producers of free software desktop environments should have their own OS, and many already do: Linux Mint and ElementaryOS spring to mind, and GNOME is working on one too.&lt;/p&gt;
      &lt;p&gt;Besides, this matter was settled 10 years ago with the creation of KDE neon, our first bite at the "in-house OS" apple. The sky did not fall; everything was beautiful and nothing hurt.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Speaking of neon, Graham points out that it is "&lt;quote&gt;being held together by a heroic volunteer&lt;/quote&gt;" (singular) and that no decision has been made as of yet about its future. Neon has "&lt;quote&gt;served admirably for a decade&lt;/quote&gt;", he said, but it "&lt;quote&gt;has somewhat reached its limit in terms of what we can do with it&lt;/quote&gt;" because of its Ubuntu base. According to the wiki page, neon's Ubuntu LTS base is built on old technology and requires "&lt;quote&gt;a lot of packaging busywork&lt;/quote&gt;". It also becomes less stable as time goes on, "&lt;quote&gt;because it needs to be tinkered with to get Plasma to build on it, breaking the LTS promise&lt;/quote&gt;".&lt;/p&gt;
    &lt;head rend="h4"&gt;Architecture and plans&lt;/head&gt;
    &lt;p&gt;KDE Linux, on the other hand, is designed to be a greenfield project that allows KDE to make use of newer technologies and more modern approaches to a Linux distribution unhampered by the needs of a general-purpose distribution. If KDE Linux's technology choices are not appealing, Graham says, "&lt;quote&gt;feel free to ignore KDE Linux and continue using the operating system of your choice. There are plenty of them!&lt;/quote&gt;"&lt;/p&gt;
    &lt;p&gt;KDE Linux is Wayland-only; there is no X.org session and no plan to add one. Users with some of the older NVIDIA cards will need to manually configure the system to work properly with KDE Linux. The distribution also only supports UEFI systems, and there are no plans to add support for BIOS-only systems.&lt;/p&gt;
    &lt;p&gt;The root filesystem (/) is a read/write Btrfs volume, while /usr is a read-only Enhanced Read-Only File System (EROFS) volume backed by a single file. The system is updated atomically by swapping out the EROFS volume; currently KDE Linux caches up to five of the files to allow users to roll back to previous versions if the most recent updates are broken.&lt;/p&gt;
    &lt;p&gt;The files have names like kde-linux_202509082242.erofs and are stored in /system. The most recent releases are about 4.8GB in size. The project uses systemd-sysupdate under the hood, which does not have support for delta updates yet. Users should expect to set aside at least 30GB just to cache the EROFS files for now.&lt;/p&gt;
    &lt;p&gt;Unlike Fedora's image-based Atomic Desktops, KDE Linux does not supply a way for users to add packages to the base system. So, for example, users have no way to add packages with additional kernel modules. Users can add applications packaged as Flatpaks using KDE's Discover graphical software manager; the Snap format is also supported, but it is not integrated with Discover—the snap command-line utility can be used to do install Snaps for now. KDE Linux also includes Distrobox, which allows users to set up a container with the distribution of their choice and install software in the container that is integrated with the system. LWN touched on Distrobox in our coverage of the Bluefin image-based operating system in December 2023.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it looks like users are not set up correctly for Podman, which Distrobox needs, on KDE Linux; trying to set up a new container gives a "potentially insufficient UIDs or GIDs available in user namespace" error when trying to test Distrobox on the latest KDE Linux build. This comment in the Podman repository on GitHub set me on the right path to fix the problem. This kind of bug is to be expected in an alpha release; no doubt it will be ironed out in the coming weeks or months.&lt;/p&gt;
    &lt;p&gt;System updates are also performed using Discover: when a new system image is available, it will show up in the Updates tab and can be installed from there. (Or using "sudo updatectl update" from the command line, for those who prefer doing it that way.) Likewise, installed Flatpaks with updates will show up in the Updates tab. For now, at least, users will have to manually manage any applications installed in a Distrobox container.&lt;/p&gt;
    &lt;p&gt;The default software selection is a good start for a desktop distribution; it includes the Gwenview image viewer, Okular document viewer, Haruna media player, Kate text editor, and Konsole for terminal emulation. Firefox is the only prominent non-KDE application included with the default install. The base system currently includes GNU Bash 5.3.3, curl 8.15, Linux 6.16.5, GCC 15.2.1, Perl 5.42, Python 3.13.7, Vim 9.1, and wget 1.25. It does not include some utilities users might want or expect, such as GNU Screen, Emacs, tmux, pip, or alternate shells like Fish.&lt;/p&gt;
    &lt;p&gt;KDE Linux's base packages are not meant to be user-customizable, but it should be possible to create custom images using systemd's mkosi tool, which is what is used by the project itself. The mkosi.conf.d directory in the KDE Linux repository contains the various configuration files for managing the packages included in the system image.&lt;/p&gt;
    &lt;head rend="h4"&gt;Development and what's next&lt;/head&gt;
    &lt;p&gt;The plan, longer term, is to have three editions of KDE Linux: the testing edition, which is what is available now, an enthusiast edition, and a stable edition. The testing edition is meant for developers and quality assurance folks; it is to be built daily from Git and to be similar in quality to KDE neon's unstable release. The enthusiast edition will include beta or released software, depending on the status of a given application at the time; this edition is aimed at "&lt;quote&gt;KDE enthusiasts, power users, and influencers&lt;/quote&gt;". The stable edition, as the name suggests, will include only released software that meets quality metrics (which are not yet defined), indicating it's ready for users not in the other categories.&lt;/p&gt;
    &lt;p&gt;KDE Linux can be installed on bare metal or in a virtual machine using virt-manager. Support for UEFI Secure Boot is currently missing. Since KDE Linux uses a lot of space for cached images, users should provision more disk space for a virtual machine than they might ordinarily; I allocated 50GB, but probably should have gone with 75GB or more.&lt;/p&gt;
    &lt;p&gt;Those wishing to follow along with KDE Linux development can check out the milestone trackers for the enthusiast and stable editions. All of the milestones have been reached for the testing edition. There are quite a few items to complete before KDE Linux reaches beta status; for example, the project is currently using packages from the Arch User Repository (AUR) but the plan is to move away from using AUR soon. The project also needs to move production to official KDE infrastructure rather than depending on Sitter's personal systems.&lt;/p&gt;
    &lt;p&gt;At the moment, the project does not have a security announcement mailing list or other notification mechanism; those using KDE Linux for more than testing should keep an eye on Arch's security tracker and KDE security advisories. Since KDE Linux is an immutable derivative of Arch Linux, with no way to immediately pull updated Arch packages, users should remember that they will be at a disadvantage when there are security vulnerabilities in the base operating system. Any security update would need to be created by Arch Linux, pushed out as an Arch package, and then incorporated into a build for KDE Linux. Conservatively, that will add at least a day for any security updates to reach KDE Linux users.&lt;/p&gt;
    &lt;p&gt;One of the downsides of having no package manager is that there is no easy way to take stock of what is installed on the system. Normally, one might do an inventory of software using a package manager's query tools; a quick "rpm -qa" shows all of the system software on my desktop's Fedora 42 install. There is no such mechanism for KDE Linux, and it's not clear that there are any plans for that type of feature long term. To be suitable for some of the target audiences, KDE Linux will need (for example) ways to manage the base operating system and easily query what is installed.&lt;/p&gt;
    &lt;p&gt;The project's governance is described as a "&lt;quote&gt;'Council of elders' model with major contributors being the elders&lt;/quote&gt;". Sitter has final decision-making authority in cases of disagreement.&lt;/p&gt;
    &lt;p&gt;Obviously the team working on KDE Linux wants the project to succeed, but it has put some thought into what will happen if the distribution is put out to pasture at some point. There is an end-of-life contingency plan to "&lt;quote&gt;push a final update shipping an OS image that transforms the system into a completely different distro&lt;/quote&gt;". The successor distribution has not been chosen yet; it would be picked based on the KDE Linux team's relationship with the other distribution and its ability to take on all of the new users.&lt;/p&gt;
    &lt;p&gt;Part of the rationale for KDE Linux is to satisfy an impulse that is common to many open-source developers: the desire to ship software directly to users without an intermediary tampering with it. The process of creating and refining KDE Linux will satisfy that for KDE developers, but it may also serve another purpose: to demonstrate just how difficult it is to create and maintain a desktop distribution for widespread use. Whether KDE Linux succeeds as a standalone distribution or not, it may be a useful exercise to illustrate why projects like Debian, Fedora, openSUSE, Ubuntu, and others make choices that ultimately frustrate application developers.&lt;/p&gt;
    &lt;p&gt; Posted Sep 10, 2025 21:09 UTC (Wed) by pauldoo (subscriber, #124140) [Link] (2 responses) Is there a reason for that? Is what KDE Linux has done better suited than those tools in some way? Posted Sep 11, 2025 2:23 UTC (Thu) by salvesefu (guest, #179283) [Link] [1] https://conf.kde.org/event/6/contributions/202/attachment... Posted Sep 11, 2025 14:09 UTC (Thu) by jzb (editor, #7867) [Link] it sounds like KDE Linux isn’t using existing solutions for delivering an immutable Linux As mentioned in the article, KDE Linux uses systemd's mkosi tooling to build the distribution, and systemd's update tooling as well. I am not sure why they chose that over bootc or others, but it isn't a built-from-scratch solution. Posted Sep 10, 2025 23:53 UTC (Wed) by gerdesj (subscriber, #5446) [Link] Posted Sep 11, 2025 8:27 UTC (Thu) by nadir (subscriber, #154506) [Link] (2 responses) Posted Sep 11, 2025 12:12 UTC (Thu) by CChittleborough (subscriber, #60775) [Link] (1 responses) Posted Sep 11, 2025 14:04 UTC (Thu) by jzb (editor, #7867) [Link] Posted Sep 11, 2025 10:14 UTC (Thu) by NN (subscriber, #163788) [Link] (1 responses) Posted Sep 11, 2025 11:36 UTC (Thu) by anselm (subscriber, #2796) [Link] This is supposed to be an image-based distribution. Image-based distributions don't need package managers because there are no packages. Posted Sep 11, 2025 13:49 UTC (Thu) by raven667 (subscriber, #5198) [Link] (2 responses) Wow. This is ultimately the original idea for "DevOps" which is that you need feedback from direct experience to better understand the pain points and severity of issues when actually _using_ and _maintaining_ software as part of a larger system, where "it works on my machine" might not be the most effective strategy for prioritizing issues. This might also be a good test to see how useful the `systemd-sysupdate` infrastructure is, as I'm not sure how much uptake there has been for it (maybe embedded devices that fly under our collective awareness) as most desktop-user-facing distros have their own methods which predate systemd. I'd need to read the linked mail threads but I wonder about the decision to move away from Arch, isn't SteamOS shipping KDE on an Arch-derived base, making that one of the most common configurations of KDE that people who aren't software developers or IT professionals would encounter? ISTM that aligning testing and dogfooding around the ecosystem of what I presume is their largest and growing userbase would make for the best experience for the most people. If they make their development process have less friction when using their own self-hosted distro, but more friction when distributed on Debian, Fedora, SteamOS/Arch, Ubuntu, SuSE, etc. then I'm not sure that's winning. As you say though if a wider base of KDE developers have the experience of shaping a distro then they may shave off some of the friction which improves the experience across all distros. I don't know which is more likely, we'll see. Posted Sep 11, 2025 13:58 UTC (Thu) by raven667 (subscriber, #5198) [Link] &amp;gt; Simplicity: Violates the "no packaging knowledge required to develop it" aspiration, because now you do need to understand something about Arch PKGBUILD files and how they're produced and consumed. I don't see how you develop a distro without needing to learn some way to manage the underlying base OS dependency tree, so I don't think you can develop a whole distribution mechanism without learning something specific to that workflow. &amp;gt; Breaks distro-agnosticism: In principle KDE Linux aspires to be agnostic about the base distro, so that swapping it out for another one is easy. With the packages pipeline producing a local AUR repo and PKGBUILD files, that isn't true anymore. I don't think this is achievable either, there are going to be systems which are better tested and aligned with the needs of KDE and systems which are less tested and aligned, creating their own distro just adds another flavor to support, it doesn't make it more agnostic. They should pick a mechanism for maintaining their base OS that works well for them and not be so concerned about the "purity" of it and of not picking favorites. At least that's my opinion, I'm not a KDE developer (or user anymore) so I don't have a stake in the outcome, just random opinions that I'm willing to share ;-) Posted Sep 11, 2025 14:02 UTC (Thu) by jzb (editor, #7867) [Link] I'd need to read the linked mail threads but I wonder about the decision to move away from Arch It's possible I didn't make this part clear enough or put in enough detail: the KDE Linux team wants to move away from using the AUR packages, not Arch itself. Being distribution-agnostic is a goal, but AFAIK they are not trying to move away from Arch right now, just end the dependency on AUR for security reasons and also because the AUR has had outages lasting multiple days. Apologies that I didn't make that more clear. &lt;head&gt;Not using pre-existing immutable tools?&lt;/head&gt;&lt;head&gt;Not using pre-existing immutable tools?&lt;/head&gt;&lt;lb/&gt; [2] https://www.freedesktop.org/software/systemd/man/latest/s...&lt;head&gt;Not using pre-existing immutable tools?&lt;/head&gt;&lt;head/&gt; "Whether KDE Linux succeeds as a standalone distribution or not, it may be a useful exercise to illustrate why projects like Debian, Fedora, openSUSE, Ubuntu, and others make choices that ultimately frustrate application developers." I've used all of the components of this thing at one time or another but I would not use them like this as a whole. I remember when KDE (around 1998) compiled and ran on my Slackware effort. Time has moved on. This beast starts off with Arch - fine choice of rolling distro. No pacman ... hmm we are off piste now ... OK. It has an unusual partitioning scheme involving Butter Fuss and something-else-fs (EROFS). Arch is not normally deployed on that sort of combo but it probably has been done because Archers are pretty &lt;head&gt;Hmmm&lt;/head&gt;&lt;del&gt;weird&lt;/del&gt; adventurous. Then it gets cute and immutable and all that stuff. I hope it works out but I don't think that what I might charitably call a Window Manager flogger, should be fiddling with the rest of the OS. Stick with what you do well and leave the OS floggers to do their thing. A demo OS build is obviously a great way to advertise your wares. For starters this distro will never be a corporate thing which is sad. I can get Kubuntu (int al) through all audits - ISO 9000, 27000, UK - Cyber Essentials and Essentials+. *sigh* &lt;head&gt;Very nice article&lt;/head&gt;&lt;head&gt;Very nice article&lt;/head&gt;&lt;head&gt;Very nice article&lt;/head&gt;&lt;head&gt;Package manager.&lt;/head&gt;&lt;head&gt;Package manager.&lt;/head&gt;&lt;head&gt;DevOps for Desktop Environments&lt;/head&gt;&lt;head&gt;DevOps for Desktop Environments&lt;/head&gt;&lt;head&gt;DevOps for Desktop Environments&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45204393</guid></item><item><title>Intel's E2200 "Mount Morgan" IPU at Hot Chips 2025</title><link>https://chipsandcheese.com/p/intels-e2200-mount-morgan-ipu-at</link><description>&lt;doc fingerprint="275bddf9500866c1"&gt;
  &lt;main&gt;
    &lt;p&gt;Intel’s IPUs, or Infrastructure Processing Units, evolved as network adapters developed increasingly sophisticated offload capabilities. IPUs take things a step further, aiming to take on a wide variety of infrastructure services in a cloud environment in addition to traditional software defined networking functions. Infrastructure services are run by the cloud operator and orchestrate tasks like provisioning VMs or collecting metrics. They won’t stress a modern server CPU, but every CPU core set aside for those tasks is one that can’t be rented out to customers. Offloading infrastructure workloads also provides an extra layer of isolation between a cloud provider’s code and customer workloads. If a cloud provider rents out bare metal servers, running infrastructure services within the server may not even be an option.&lt;/p&gt;
    &lt;p&gt;Intel’s incoming “Mount Morgan” IPU packs a variety of highly configurable accelerators alongside general purpose CPU cores, and aims to capture as many infrastructure tasks as possible. It shares those characteristics with its predecessor, “Mount Evans”. Flexibility is the name of the game with these IPUs, which can appear as a particularly capable network card to up to four host servers, or run standalone to act as a small server. Compared to Mount Evans, Mount Morgan packs more general purpose compute power, improved accelerators, and more off-chip bandwidth to support the whole package.&lt;/p&gt;
    &lt;head rend="h1"&gt;Compute Complex&lt;/head&gt;
    &lt;p&gt;Intel includes a set of Arm cores in their IPU, because CPUs are the ultimate word in programmability. They run Linux and let the IPU handle a wide range of infrastructure services, and ensure the IPU stays relevant as infrastructure requirements change. Mount Morgan’s compute complex gets an upgrade to 24 Arm Neoverse N2 cores, up from 16 Neoverse N1 cores in Mount Evans. Intel didn’t disclose the exact core configuration, but Mount Evans set its Neoverse N1 cores up with 512 KB L2 caches and ran them at 2.5 GHz. It’s not the fastest Neoverse N1 configuration around, but it’s still nothing to sneeze at. Mount Morgan of course takes things further. Neoverse N1 is a 5-wide out-of-order core with a 160 entry ROB, ample execution resources, and a very capable branch predictor. Each core is already a substantial upgrade over Neoverse N1. 24 Neoverse N2 cores would be enough to handle some production server workloads, let alone a collection of infrastructure services.&lt;/p&gt;
    &lt;p&gt;Mount Morgan gets a memory subsystem upgrade to quad channel LPDDR5-6400 to feed the more powerful compute complex. Mount Evans had a triple channel LPDDR4X-4267 setup, connected to 48 GB of onboard memory capacity. If Intel keeps the same memory capacity per channel, Mount Morgan would have 64 GB of onboard memory. Assuming Intel’s presentation refers to 16-bit LPDDR4/5(X) channels, Mount Morgan would have 51.2 GB/s of DRAM bandwidth compared to 25.6 GB/s in Mount Evans. Those figures would be doubled if Intel refers to 32-bit data buses to LPDDR chips, rather than channels. A 32 MB System Level Cache helps reduce pressure on the memory controllers. Intel didn’t increase the cache’s capacity compared to the last generation, so 32 MB likely strikes a good balance between hitrate and die area requirements. The System Level Cache is truly system level, meaning it services the IPU’s various hardware acceleration blocks in addition to the CPU cores.&lt;/p&gt;
    &lt;p&gt;A Lookaside Crypto and Compression Engine (LCE) sits within the compute complex, and shares lineage with Intel’s Quickassist (QAT) accelerator line. Intel says the LCE features a number of upgrades over QAT targeted towards IPU use cases. But perhaps the most notable upgrade is getting asymmetric crypto support, which was conspicuously missing from Mount Evans’s LCE block. Asymmetric cryptography algorithms like RSA and ECDHE are used in TLS handshakes, and aren’t accelerated by special instructions on many server CPUs. Therefore, asymmetric crypto can consume significant CPU power when a server handles many connections per second. It was a compelling use case for QAT, and it’s great to see Mount Morgan get that as well. The LCE block also supports symmetric crypto and compression algorithms, capabilities inherited from QAT.&lt;/p&gt;
    &lt;p&gt;A programmable DMA engine in the LCE lets cloud providers move data as part of hardware accelerated workflows. Intel gives an example workflow for accessing remote storage, where the LCE helps move, compress, and encrypt data. Other accelerator blocks located in the IPU’s network subsystem help complete the process.&lt;/p&gt;
    &lt;head rend="h1"&gt;Network Subsystem&lt;/head&gt;
    &lt;p&gt;Networking bandwidth and offloads are a core function of the IPU, and its importance can’t be understated. Cloud servers need high network and storage bandwidth. The two are often two sides of the same coin, because cloud providers might use separate storage servers accessed over datacenter networking. Mount Morgan has 400 Gbps of Ethernet throughput, double Mount Evans’s 200 Gbps.&lt;/p&gt;
    &lt;p&gt;True to its smart NIC lineage, Mount Morgan uses a large number of inline accelerators to handle cloud networking tasks. A programmable P4-based packet processing pipeline, called the FXP, sits at the heart of the network subsystem. P4 is a packet processing language that lets developers express how they want packets handled. Hardware blocks within the FXP pipeline closely match P4 demands. A parser decodes packet headers and translates the packet into a representation understood by downstream stages. Downstream stages can check for exact or wildcard matches. Longest prefix matches can be carried out in hardware too, which is useful for routing.&lt;/p&gt;
    &lt;p&gt;The FXP can handle a packet every cycle, and can be configured to perform multiple passes per packet. Intel gives an example where one pass processes outer packet layers to perform decapsulation and checks against access control lists. A second pass can look at the inner packet, and carry out connection tracking or implement firewall rules.&lt;/p&gt;
    &lt;p&gt;An inline crypto block sits within the network subsystem as well. Unlike the LCE in the compute complex, this crypto block is dedicated to packet processing and focuses on symmetric cryptography. It includes its own packet parsers, letting it terminate IPSec and PSP connections and carry out IPSec/PSP functions like anti-replay window protection, sequence number generation, and error checking in hardware. IPSec is used for VPN connections, which are vital for letting customers connect to cloud services. PSP is Google’s protocol for encrypting data transfers internal to Google’s cloud. Compared to Mount Evans, the crypto block’s throughput has been doubled to support 400 Gbps, and supports 64 million flows.&lt;/p&gt;
    &lt;p&gt;Cloud providers have to handle customer network traffic while ensuring fairness. Customers only pay for a provisioned amount of network bandwidth. Furthermore, customer traffic can’t be allowed to monopolize the network and cause problems with infrastructure services. The IPU has a traffic shaper block, letting it carry out quality of service measures completely in hardware. One mode uses a mutli-level hierarchical scheduler to arbitrate between packets based on source port, destination port, and traffic class. Another “timing wheel” mode does per-flow packet pacing, which can be controlled by classification rules set up at the FXP. Intel says the timing wheel mode gives a pacing resolution of 512 nanoseconds per slot.&lt;/p&gt;
    &lt;p&gt;RDMA traffic accounts for a significant portion of datacenter traffic. For example, Azure says RDMA accounts for 70% of intra-cloud network traffic, and is used for disk IO. Mount Morgan has a RDMA transport option to provide hardware offload for that traffic. It can support two million queue pairs across multiple hosts, and can expose 1K virtual functions per host. The latter should let a cloud provider directly expose RDMA acceleration capabilities to VMs. To ensure reliable transport, the RDMA transport engine supports the Falcon and Swift transport protocols. Both protocols offer improvements over TCP, and Intel implements congestion control for those protocols completely in hardware. To reduce latency, the RDMA block can bypass the packet processing pipeline and handle RDMA connections on its own.&lt;/p&gt;
    &lt;p&gt;All of the accelerator blocks above are clients of the system level cache. Some hardware acceleration use cases, like connection tracking with millions of flows, can have significant memory footprints. The system level cache should let the IPU keep frequently accessed portions of accelerator memory structures on-chip, reducing DRAM bandwidth needs.&lt;/p&gt;
    &lt;head rend="h1"&gt;Host Fabric and PCIe Switch&lt;/head&gt;
    &lt;p&gt;Mount Morgan’s PCIe capabilities have grown far beyond what a normal network card may offer. It has 32 PCIe Gen 5 lanes, providing more IO bandwidth than some recent desktop CPUs. It’s also a huge upgrade over the 16 PCIe Gen 4 lanes in Mount Evans.&lt;/p&gt;
    &lt;p&gt;Traditionally, a network card sits downstream of a host, and thus appears as a device attached to a server. The host fabric and PCIe subsystem is flexible to let the IPU wear many hats. It can appear as a downstream device to up to four server hosts, each of which sees the IPU as a separate, independent device. Mount Evans supported this “multi-host” mode as well, but Mount Morgan’s higher PCIe bandwidth is necessary to utilize its 400 Gigabit networking.&lt;/p&gt;
    &lt;p&gt;Mount Morgan can run in a “headless” mode, where it acts as a standalone server and a lightweight alternative to dedicating a traditional server to infrastructure tasks. In this mode, Mount Morgan’s 32 PCIe lanes can let it connect to many SSDs and other devices. The IPU’s accelerators as well as the PCIe lanes appear downstream of the IPU’s CPU cores, which act as a host CPU.&lt;/p&gt;
    &lt;p&gt;A “converged” mode can use some PCIe lanes to connect to upstream server hosts, while other lanes connect to downstream devices. In this mode, the IPU shows up as a PCIe switch to connected hosts, with downstream devices visible behind it. A server could connect to SSDs and GPUs through the IPU. The IPU’s CPU cores can sit on top of the PCIe switch and access downstream devices, or can be exposed as a downstream device behind the PCIe switch.&lt;/p&gt;
    &lt;p&gt;The IPU’s multiple modes are a showcase of IO flexibility. It’s a bit like how AMD uses the same die as an IO die within the CPU and a part of the motherboard chipset on AM4 platforms. The IO die’s PCIe lanes can connect to downstream devices when it’s serving within the CPU, or be split between an upstream host and downstream devices when used in the chipset. Intel is also no stranger to PCIe configurability. Their early QAT PCIe cards reused their Lewisburg chipset, exposing it as a downstream device with three QAT devices appearing behind a PCIe switch.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;Cloud computing plays a huge role in the tech world today. It originally started with commodity hardware, with similar server configurations to what customers might deploy in on-premise environments. But as cloud computing expanded, cloud providers started to see use cases for cloud-specific hardware accelerators. Examples include "Nitro" cards in Amazon Web Services, or smart NICs with FPGAs in Microsoft Azure. Intel has no doubt seen this trend, and IPUs are the company's answer.&lt;/p&gt;
    &lt;p&gt;Mount Morgan tries to service all kinds of cloud acceleration needs by packing an incredible number of highly configurable accelerators, in recognition of cloud providers’ diverse and changing needs. Hardware acceleration always runs the danger of becoming obsolete as protocols change. Intel tries to avoid this by having very generalized accelerators, like the FXP, as well as packing in CPU cores that can run just about anything under the sun. The latter feels like overkill for infrastructure tasks, and could let the IPU remain relevant even if some acceleration capabilities become obsolete.&lt;/p&gt;
    &lt;p&gt;At a higher level, IPUs like Mount Morgan show that Intel still has ambitions to stretch beyond its core CPU market. Developing Mount Morgan must have been a complex endeavor. It’s a showcase of Intel’s engineering capability even when their CPU side goes through a bit of a rough spot. It’ll be interesting to see whether Intel’s IPUs can gain ground in the cloud market, especially with providers that have already developed in-house hardware offload capabilities tailored to their requirements.&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45204838</guid></item><item><title>DOOMscrolling: The Game</title><link>https://ironicsans.ghost.io/doomscrolling-the-game/</link><description>&lt;doc fingerprint="3ea0f9566b458649"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Doomscrolling: The Game&lt;/head&gt;
    &lt;p&gt;Can a game work where all you do is scroll?&lt;/p&gt;
    &lt;p&gt;This post has gotten popular! Welcome to the newsletter. If you enjoy this, please subscribe. It’s free!&lt;/p&gt;
    &lt;p&gt;We’re all familiar with doomscrolling, spending too much time scrolling endless feeds of content that make you feel bad about everything.&lt;/p&gt;
    &lt;p&gt;But sometimes when I hear the word “doomscrolling” it makes me think of two other things: the classic video game Doom and, well, scrolling.&lt;/p&gt;
    &lt;p&gt;That got me wondering if I could make a Doom-inspired game in a web browser where the only thing you do to play is scroll. No moving your character laterally, no jumping. Just scrolling.&lt;/p&gt;
    &lt;p&gt;So I made it. And it’s fun! Here’s a small clip:&lt;/p&gt;
    &lt;p&gt;That’s what the game looks like. But here’s what playing it feels like:&lt;/p&gt;
    &lt;p&gt;You can go play it right now on desktop or mobile. The rest of this newsletter is the non-technical story of how I made the game.&lt;/p&gt;
    &lt;head rend="h3"&gt;The first time was a failure&lt;/head&gt;
    &lt;p&gt;As readers know, I’m not a coder, but I enjoy how vibe coding lets me turn an idea into something real. So naturally, I turned to vibe coding for this.&lt;/p&gt;
    &lt;p&gt;It didn’t work.&lt;/p&gt;
    &lt;p&gt;This was around nine months ago. I tried and tried, but none of the LLMs were able to help me go from idea to a playable game at all. Like, not even close. GPT-4 absolutely refused to understand that scrolling down a page means that the background should move up the page. I ended up with something kinda pathetic that didn’t even resemble a game. So I gave up, figuring this was either beyond an LLM’s skills, beyond my skills, or both.&lt;/p&gt;
    &lt;p&gt;But then GPT-5 came out a few weeks ago, and I tried again to see how much better it might be at coding. In just two hours I had a very good prototype. I even made a little title screen for my prototype so it felt more like a real game:&lt;/p&gt;
    &lt;p&gt;I described the game design to ChatGPT very broadly at first. I said it should work kinda like Galaga turned upside-down. But I explained that unlike Galaga, the player moves forward and backward rather than side to side, and that the monster’s position should remain relative to the floor. That and a few more details got me surprisingly far as a first step.&lt;/p&gt;
    &lt;p&gt;For prototyping purposes, I asked ChatGPT to just come up with five different monsters, each one with a two-frame animation, like Space Invaders aliens. They were little more than basic shapes, and I figured at some point I’d replace them with actual pre-rendered monster sprites. But this worked for now.&lt;/p&gt;
    &lt;p&gt;Then I went on vacation. I spent an hour or two each morning over coffee working on this game until the kids woke up, gradually adding and refining features one at a time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Improving the game&lt;/head&gt;
    &lt;p&gt;I focused on making the game more fun to play, with incentives to keep moving but also things to stop you from racing through it too fast. I added things like weapon upgrades for every 100 monsters you kill, a wall of fire that chases you if you stay in one place too long, and obstacles to slow you down, like brick walls and spider webs.&lt;/p&gt;
    &lt;p&gt;Some other little quality-of-life features I came up with include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Five different background textures so you can have a different visual experience each time you play.&lt;/item&gt;
      &lt;item&gt;Health potions. The first one appears when you’re down to three health points. After that, they are more rare and may require weapon upgrades to reach.&lt;/item&gt;
      &lt;item&gt;A visual marker when you pass your record distance&lt;/item&gt;
      &lt;item&gt;A pause screen with some stats&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Making it really Doomscrolling&lt;/head&gt;
    &lt;p&gt;I was pretty happy with the game and ready to share it. But then at the last minute I got another nagging idea in the back of my mind: What if it was somehow more like actual doomscrolling?&lt;/p&gt;
    &lt;p&gt;It would be easy to get an RSS Feed of headlines from a news site. Could I make them appear in the game as you scroll, in a way that felt integrated with the game?&lt;/p&gt;
    &lt;p&gt;First I tried having headlines just appear on the floor as you play, but it felt very tacked-on. So I came up with some lore for the game that helped.&lt;/p&gt;
    &lt;p&gt;I decided that the game takes place in the future, in a lair that was sealed up today, whatever day you happen to be playing the game. And the civilization that sealed up the cave left behind plaques featuring today’s headlines for some unexplained reason. So as you play, you encounter these plaques that each has a bit of news from when the cave was sealed up. Today’s actual news.&lt;/p&gt;
    &lt;p&gt;The plaques have no effect on the game, except to the extent that they tempt you to read them and get distracted from the gameplay. But they’re just decorative elements. Feel free to ignore them. If you can.&lt;/p&gt;
    &lt;p&gt;The headlines all come from the New York Times front page RSS feed. So in a sense, this game is actually an extremely complex single-feed RSS reader.&lt;/p&gt;
    &lt;head rend="h3"&gt;Working with AI is still a pain. This was my solution.&lt;/head&gt;
    &lt;p&gt;If you’ve ever tried to work with AI, you’ve likely run into a roadblock where you’re describing something over and over and it’s simply not getting it. “No, don’t do it that way, for the umpteenth time, do it this way.”&lt;/p&gt;
    &lt;p&gt;I still planned on making the monsters as pre-rendered sprites, but the background textures, plaques, and decorative items like torches could still be rendered in-game if I could just get GPT-5 to understand what I want them to look like. An LLM isn’t really good at making artwork like this.&lt;/p&gt;
    &lt;p&gt;So I simplified things. I had the AI set up simple “labs,” standalone test pages where we could work on different designs, using the style from the game. For example, here’s one “lab” I made for testing how some in-world elements would look on different backgrounds:&lt;/p&gt;
    &lt;p&gt;Everything above is rendered on the fly. One big advantage of that approach is that I could randomize some visual elements in the game. For example, look at the spider webs above. They all follow the same rules but they’re all slightly different. The game’s background textures are also slightly different each time you play.&lt;/p&gt;
    &lt;p&gt;Next, I set about making pre-rendered monsters. But wow, small-scale pixel art is hard. I went through a lot of versions of different monsters. Eventually, I had a few I felt were good enough. The game looked like this:&lt;/p&gt;
    &lt;p&gt;It had its own charm, but in the end, I didn’t love it. Somehow, my pre-rendered monsters made the game feel worse. Maybe it’s because I just am not a good pixel artist.&lt;/p&gt;
    &lt;p&gt;So I decided to see what I could do with ChatGPT in a “lab” like I did for other in-game items, but focused on monster designs. After a lot of experimentation, I settled on the simple monsters that ended up in the game:&lt;/p&gt;
    &lt;p&gt;I assume doing all this computationally is more processor-intensive than using pre-rendered monsters, but it’s very smooth for me on both desktop and phone, so it must not be too intensive. I guess I’ll hear from people if it’s choppy on their device.&lt;/p&gt;
    &lt;p&gt;Sometimes I still needed a little more control over how something looked. So in those cases I had ChatGPT build labs with sliders that I could adjust to decide how I want things to appear, instead of getting frustrated with the chatbot. This way I could quickly settle on a look and feel.&lt;/p&gt;
    &lt;p&gt;Here for example is the lab page for the plaques. I wanted them to look integrated into the game world, so I described the parameters I wanted to play with for the text styling and plaque itself. We put a “copy settings” button that I could use to report back to the LLM once I liked it, so I could say “Okay, let’s go with these settings in the game.”&lt;/p&gt;
    &lt;p&gt;I’ve made this lab page live for you to play with if you’re curious.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ship it&lt;/head&gt;
    &lt;p&gt;There are still features and adjustments I’d like to add, but I’m not on vacation anymore, so think I just need to stop here. I may still tweak it, but for now I’m calling version 1.0 ready to ship. It has successfully scratched my itch to see if I could make a fun scrolling-only game, which was really all I wanted.&lt;/p&gt;
    &lt;p&gt;It should play equally well on mobile and desktop. The only difference is that with a taller device you can see more of the world at a time, which makes it a little easier.&lt;/p&gt;
    &lt;p&gt;Oh, and you can save it to your home screen so it acts like a standalone app rather than a browser game. That’s how I play.&lt;/p&gt;
    &lt;p&gt;And that’s it for another newsletter!&lt;/p&gt;
    &lt;p&gt;Here’s where I remind you that you can support my endeavors by becoming a paid member, or making a one-time donation. Every little bit matters.&lt;/p&gt;
    &lt;p&gt;And if not, that’s okay, too. But maybe you can share the newsletter? Moving from Beehiiv to Ghost resulted in a subscriber hit, which I anticipated because the same thing happened when I first left Substack. It took a while to begin growing again after the first move, and you can help me get back to positive growth by spreading the word now that I’ve landed at Ghost. It would mean the world to me.&lt;/p&gt;
    &lt;p&gt;Until next time, thanks as always for reading!&lt;/p&gt;
    &lt;p&gt;David&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45205232</guid></item><item><title>Where did the Smurfs get their hats (2018)</title><link>https://www.pipelinecomics.com/beginning-bd-smurfs-hats-origin/</link><description>&lt;doc fingerprint="76e296f6dee65d27"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Beginning BD: Where Did the Smurfs Get Their Hats?&lt;/head&gt;
    &lt;p&gt;That white floppy hat on top of every Smurf’s head.&lt;/p&gt;
    &lt;p&gt;What is it? Where did it come from?&lt;/p&gt;
    &lt;p&gt;Do Smurfs have bald spots?&lt;/p&gt;
    &lt;p&gt;We’ve got a history lesson coming up here, folks. Sit back.&lt;/p&gt;
    &lt;p&gt;But, first, let’s answer a side question:&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s Under a Smurf’s Hat?&lt;/head&gt;
    &lt;p&gt;There’s only one true source for information on this question, and that’s Peyo’s comics. Accept no substitutes. Yes, there are probably moments in various movies and cartoons where a Smurf can been seen without their hat on. That’s not Peyo.&lt;/p&gt;
    &lt;p&gt;Peyo showed us one example of a Smurf without a hat. Go to the classic story, “The Purple Smurf.” At one point, Papa Smurf’s lab blows up, along with his hat.&lt;/p&gt;
    &lt;p&gt;Papa Smurf is bald. This doesn’t necessarily mean that Brainy Smurf isn’t sporting a pompadour under his hat or that Hefty Smurf doesn’t have a buzz cut, but…&lt;/p&gt;
    &lt;p&gt;We know Smurfs are capable of having hair. Smurfette has luxurious blonde locks, but she’s also not really a natural born Smurf. Technically, she’s a concoction of Gargamel’s. But let’s give his spell-making the benefit of the doubt. Maybe Smurfs can have hair.&lt;/p&gt;
    &lt;p&gt;I mean, where do Smurfs come from? What’s the genetics at work that would determine–&lt;/p&gt;
    &lt;p&gt;–wait, no, I’m not going down that rabbit hole.&lt;/p&gt;
    &lt;p&gt;Papa Smurf is definitely bald. I bet you already assumed that, though. Let’s move on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Do Smurfs Get Such Wonderful Hats?&lt;/head&gt;
    &lt;p&gt;It’s called a Phrygian cap, which is about as much fun to type as Vercingetorix, who is an amazing story for another day.&lt;/p&gt;
    &lt;p&gt;The headgear is over 2000 years old. Here’s proof:&lt;/p&gt;
    &lt;p&gt;Handsome fella, isn’t he? But check out that hat! He’s totally a Smurf, right?&lt;/p&gt;
    &lt;p&gt;His name is Attis, and this sculpture comes from somewhere in the 100s AD, though Attis lived even further back, in the 4th century BC.&lt;/p&gt;
    &lt;p&gt;In that time period, you know who else worse a Smurfs hat? King Midas. Yes the guy with the thing for gold. (Croesus is also involved in this story, but let’s not get completely off topic.)&lt;/p&gt;
    &lt;p&gt;Phrygis, in case your curious, is the name of an ancient group of people who lived in the Balkans region of eastern Europe — Greece, Turkey, Romania, etc. Their language and culture went extinct by the 5th century AD. Near the end, the Romans thought of them as being lazy and dull.&lt;/p&gt;
    &lt;p&gt;Those Phrygian caps do kind of resemble a dunce cap, don’t they? It’s completely unrelated, though. That’s a dead end.&lt;/p&gt;
    &lt;head rend="h2"&gt;Liberty and Freedom&lt;/head&gt;
    &lt;p&gt;The hat is often associated with liberty and freedom. Why?&lt;/p&gt;
    &lt;p&gt;It was adopted during the French Revolution (at the end of the 18th century AD) as “the red cap of liberty” by the revolutionaries.&lt;/p&gt;
    &lt;p&gt;You can see one such cap on the French personification of “Liberté”. Here she is:&lt;/p&gt;
    &lt;p&gt;Looks just like Papa Smurf’s hat. And while Peyo was Belgian, he was working with French publishers, so drawing inspiration from a French symbol isn’t too crazy.&lt;/p&gt;
    &lt;p&gt;The thing that Peyo maybe didn’t realize and the French revolutionaries definitely didn’t realize, though, is that it’s the wrong hat.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Right Hat&lt;/head&gt;
    &lt;p&gt;This is the pileus hat. It’s a brimless hat, often made of felt or maybe wool. It started in Ancient Greece as a sailor’s hat, and eventually found its way to Ancient Rome, too.&lt;/p&gt;
    &lt;p&gt;In Rome, a freed slave had his head shaved. Then, they would wear a pileus, in part to keep their head warm. The hat was a sign of the slave’s freedom/liberty.&lt;/p&gt;
    &lt;p&gt;Somewhere along the line in the French Revolution, they adopted the freed slaves’ head gear as their own symbol of freedom, but picked the wrong one.&lt;/p&gt;
    &lt;p&gt;They didn’t have Google Images back then, so don’t be too hard on them.&lt;/p&gt;
    &lt;p&gt;I’m sure Peyo picked that hat because it looked good on the Smurfs, helped further set them apart from all the other characters, and would someday make for an awesome free giveaway at Angouleme.&lt;/p&gt;
    &lt;head rend="h2"&gt;Side By Side&lt;/head&gt;
    &lt;p&gt;Wikipedia has an image of the Phrygian and Pileus next to each other on the bottom shelf here:&lt;/p&gt;
    &lt;p&gt;On the bottom shelf, the Smurfs’ Phrygian cap on the left is next to the pileus in the center there.&lt;/p&gt;
    &lt;p&gt;Yes, there were metal versions of both that were used as head gear during war times. Imagine an army of Smurfs coming over the hill to attack!&lt;/p&gt;
    &lt;head rend="h2"&gt;Glad You Asked?&lt;/head&gt;
    &lt;p&gt;To sum it all up: Some Greeks had a hat that the Romans borrowed and that their slaves used to represent their freedom. 2000 years later, some French revolutionaries confused that hat for a different one from the Phrygis folks and made it their own sign of freedom.&lt;/p&gt;
    &lt;p&gt;150 years after that, Peyo created the Smurfs and gave them that hat, but in white.&lt;/p&gt;
    &lt;p&gt;Here endeth the lesson.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45206311</guid></item><item><title>Court rejects Verizon claim that selling location data without consent is legal</title><link>https://arstechnica.com/tech-policy/2025/09/court-rejects-verizon-claim-that-selling-location-data-without-consent-is-legal/</link><description>&lt;doc fingerprint="fb2479b85417625e"&gt;
  &lt;main&gt;
    &lt;p&gt;Verizon lost an attempt to overturn a $46.9 million fine for selling customer location data without its users' consent. The US Court of Appeals for the 2nd Circuit rejected Verizon's challenge in a ruling issued today.&lt;/p&gt;
    &lt;p&gt;The Federal Communications Commission fined the three major carriers last year for violations revealed in 2018. The companies sued the FCC in three different courts, with varying results.&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T beat the FCC in the reliably conservative US Court of Appeals for the 5th Circuit, while T-Mobile lost in the District of Columbia Circuit. Although FCC Chairman Brendan Carr voted against the fine last year, when the commission had a Democratic majority, his FCC urged the courts to uphold the Biden-era decisions.&lt;/p&gt;
    &lt;p&gt;A ruling against the FCC could gut the agency's ability to issue financial penalties. The different rulings from different circuits raise the odds of the cases being taken up by the Supreme Court.&lt;/p&gt;
    &lt;p&gt;Today's 2nd Circuit ruling against Verizon was issued unanimously by a panel of three judges, and it comes to the same legal conclusions as the DC Circuit did in the T-Mobile case. The court did not accept the carrier's argument that the fine violated its Seventh Amendment right to a jury trial and that the location data wasn't protected under the law used by the FCC to issue the penalties.&lt;/p&gt;
    &lt;p&gt;"We disagree [with Verizon]," the 2nd Circuit ruling said. "The customer data at issue plainly qualifies as customer proprietary network information, triggering the Communication Act's privacy protections. And the forfeiture order both soundly imposed liability and remained within the strictures of the penalty cap. Nothing about the Commission's proceedings, moreover, transgressed the Seventh Amendment's jury trial guarantee. Indeed, Verizon had, and chose to forgo, the opportunity for a jury trial in federal court. Thus, we DENY Verizon's petition."&lt;/p&gt;
    &lt;head rend="h2"&gt;Verizon claimed law doesn’t cover device location data&lt;/head&gt;
    &lt;p&gt;Until 2019, Verizon "ran a 'location-based services' program that sold access to certain kinds of wireless customer location data," the ruling said. "As part of that program, Verizon contracted with 'location information aggregators,' which collected customer data and resold it to third-party location-based services providers. Verizon had arrangements with two aggregators, LocationSmart and Zumigo, which in turn contracted with 63 third-party entities."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45206567</guid></item><item><title>GrapheneOS accessed Android security patches but not allowed to publish sources</title><link>https://grapheneos.social/@GrapheneOS/115164133992525834</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45208925</guid></item><item><title>PgEdge Goes Open Source</title><link>https://www.pgedge.com/blog/pgedge-goes-open-source</link><description>&lt;doc fingerprint="abc381b5b51065b2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;pgEdge goes Open Source&lt;/head&gt;
    &lt;p&gt;In November last year after nearly two decades at my previous gig, I came to the conclusion that I didn’t want to work at what seemed to be rapidly becoming an AI-focused company and moved to pgEdge where the focus is well and truly on distributed PostgreSQL and Postgres generally. Distributed databases (and particularly Postgres of course) have always been a passion of mine – even being a key topic of my master’s dissertation many years ago.&lt;/p&gt;
    &lt;p&gt;Moving to pgEdge was a breath of fresh air. Not only did I get to work with some outstanding engineers and other folks on Postgres, but a good number of them were friends and colleagues that I’d worked with in the past. I’ve since had the privilege of hiring even more colleagues from the Postgres world, and look forward to expanding the team even further with more fantastic engineers from the PostgreSQL and wider database communities.&lt;/p&gt;
    &lt;p&gt;There was a wrinkle in my ideal view of how things should be though - the key components of pgEdge were “source available” and not Open Source. That means the source code to our replication engine known as Spock and key extensions such as Snowflake which provides cluster-wide unique sequence values and Lolor which enables logical replication of large objects, had a proprietary licence – known as the pgEdge Community License – which allowed you to view and modify the source code, but limited how you could actually use it.&lt;/p&gt;
    &lt;p&gt;Well, I’m pleased to be able to say that that is no longer the case. All the core components of pgEdge Distributed Postgres, along with any other pgEdge repositories that previously used the pgEdge Community License have now been re-licenced under the permissive PostgreSQL License, as approved by the Open Source Initiative!&lt;/p&gt;
    &lt;p&gt;We’re proud to be able to make this change to support Open Source software and contribute to the PostgreSQL ecosystem, and I’m looking forward to seeing us continue to expand our contributions as much as we can.&lt;/p&gt;
    &lt;p&gt;So, if you want to try out multimaster distributed Postgres, and get involved with the development of the technology, head on over to GitHub and in particular check out the spock, snowflake, and lolor repositories.&lt;/p&gt;
    &lt;p&gt;If you just want to use the tech without having to build it yourself or are looking for supported builds for production use, then we have cloud, container, and VM options you can try out on our website.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45209065</guid></item><item><title>Germany is not supporting ChatControl – blocking minority secured</title><link>https://digitalcourage.social/@echo_pbreyer/115184350819592476</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45209366</guid></item><item><title>DeepCodeBench: Real-World Codebase Understanding by Q&amp;A Benchmarking</title><link>https://www.qodo.ai/blog/deepcodebench-real-world-codebase-understanding-by-qa-benchmarking/</link><description>&lt;doc fingerprint="2f422a9369cb45c6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DeepCodeBench: Real-World Codebase Understanding by Q&amp;amp;A Benchmarking&lt;/head&gt;
    &lt;p&gt;At Qodo, we’ve created a new benchmark dataset of real-world questions derived from large, complex code repositories. We are excited to release the dataset, methodology, and prompts used in its creation to support further research and development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;Enterprises often maintain massive codebases that are difficult for any individual developer to navigate and fully understand. Whether onboarding, doing routine development, or using AI-assisted workflows, teams often have questions about their codebase. To effectively address this, we’ve developed specialized retrieval capabilities within our research agents. However, to benchmark and validate these systems effectively, we require a robust set of real-world questions and answers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prior Work&lt;/head&gt;
    &lt;p&gt;Existing benchmarks, such as CodeQA, primarily contain artificially generated code with questions limited to provided code snippets, requiring no retrieval from broader contexts. Another recent work (arXiv:2407.02883) involves real-world scenarios but focuses on retrieval from databases rather than code repositories, which does not adequately represent common real-world use-cases.&lt;/p&gt;
    &lt;p&gt;To address this gap, we propose a new approach. We introduce a benchmark based on realistic questions derived from pull requests that require retrieval across multiple files in a codebase.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dataset Generation&lt;/head&gt;
    &lt;p&gt;To effectively challenge retrieval systems, questions in our benchmark must:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Require deep retrieval, often spanning multiple interconnected files.&lt;/item&gt;
      &lt;item&gt;Reflect realistic questions developers encounter when solving actual issues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We identified that pull requests (PRs) are good sources for complex code changes with proper context that can be used for question and answer generation. PRs naturally link related code, not always through explicit imports or function calls, but through functional changes made together. We leveraged this insight to generate context:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each code change within a PR, we retrieved its containing method, class or file from the current default branch.&lt;/item&gt;
      &lt;item&gt;We bundled these retrieved code snippets along with the PR’s title and description to form a meaningful context..&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using the above mentioned PR data as context, we prompt large language models (LLMs) to generate questions relevant to real developer workflows, ensuring authenticity and practical value (see prompt in Appendix). The same context is also used to generate the ground-true answer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;Consider PR 39363 of Hugging Face’s Transformers repository. This PR touches 4 files in different places, including methods such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;BaseImageProcessorFast.__init__ (src/transformers/image_processing_utils_fast.py)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;BaseVideoProcessor.__init__ (src/transformers/video_processing_utils.py)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are not concerned with the exact code changes in the PR. Instead, we use the PR as a signal to locate blocks of code that are functionally related and should be considered together.&lt;/p&gt;
    &lt;p&gt;We try to find these methods in the updated code to avoid asking questions about irrelevant code. We don’t always find them, due to renames and refactors, but in this PR we found both BaseImageProcessorFast and BaseVideoProcessor.&lt;/p&gt;
    &lt;p&gt;In the next step, we pass the complete methods, along with the PR title, description, and the prompt in Appendix A, to an LLM, asking it to generate a question.&lt;/p&gt;
    &lt;p&gt;For this PR, the generated question is:&lt;/p&gt;
    &lt;p&gt;How do the fast image and video processor base classes prevent shared mutable state when instantiating multiple instances?&lt;/p&gt;
    &lt;p&gt;And the answer is:&lt;/p&gt;
    &lt;p&gt;Both the fast image and video processor bases deep-copy their mutable defaults when you instantiate them. In BaseImageProcessorFast.__init__ (src/transformers/image_processing_utils_fast.py) and BaseVideoProcessor.__init__ (src/transformers/video_processing_utils.py) they loop over all valid kwargs and do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you passed a value, use it;&lt;/item&gt;
      &lt;item&gt;Otherwise, setattr(self, key, deepcopy(getattr(self, key))), and they rebuild size/crop_size via get_size_dict.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By deep-copying every default dict/list on init, no two instances share the same mutable object.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dataset Statistics&lt;/head&gt;
    &lt;p&gt;We generated 1,144 questions derived from eight open-source repositories. Below are detailed insights into the characteristics of the dataset:&lt;/p&gt;
    &lt;head rend="h3"&gt;Context Distribution&lt;/head&gt;
    &lt;p&gt;The histograms illustrate the distribution of context used for each question:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Number of Context Blocks: Indicates how many individual code blocks were involved in generating each question.&lt;/item&gt;
      &lt;item&gt;Number of Context Files: Reflects the number of distinct files utilized per question.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the example above, there are two blocks across two files. However, PRs often touch multiple methods within the same file, resulting in more blocks than files.&lt;/p&gt;
    &lt;head rend="h3"&gt;Categorical Breakdown&lt;/head&gt;
    &lt;p&gt;Scope:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deep: Questions focusing on specific, detailed aspects of a single block of code.&lt;/item&gt;
      &lt;item&gt;Broad: Questions involving interactions or relationships across multiple code blocks or files.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Core Questions: Questions targeting fundamental, core functionality versus those focusing on peripheral technical details.&lt;/p&gt;
    &lt;p&gt;Searchable Questions: Questions containing specific keywords or identifiers that facilitate direct searches within the codebase.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluation Mechanism: LLM as a Judge&lt;/head&gt;
    &lt;p&gt;Evaluating model predictions requires an objective and scalable approach.Rather than relying solely on subjective LLM judgment, we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extracted discrete, verifiable facts from each ground-truth (GT) answer.&lt;/item&gt;
      &lt;item&gt;Checked whether each fact appeared in the predicted answer using a simple LLM call.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This method, that we call “fact recall,” was introduced in the 2003 TREC (Text REtrieval Conference) QA Track (paper, overview) and is widely used today – for example in Google/DeepMind’s SAFE and in the TREC 2024 RAG Track (e.g., MSR/Waterloo’s AutoNuggetizer). It ensures robust, objective, and scalable assessment of model performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Baselines&lt;/head&gt;
    &lt;p&gt;To better understand our dataset, we established several baseline evaluations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ground Truth (GT) answers: Verifies both the accuracy of fact extraction and the reliability of the automated fact verification method&lt;/item&gt;
      &lt;item&gt;LLM with full context: Provides an LLM with all context used to generate the questions, setting an upper-bound performance baseline.&lt;/item&gt;
      &lt;item&gt;LLM with no context: Evaluates how well an LLM could answer questions using only the repository name, capturing inherent model knowledge and setting a lower-bound baseline.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These baselines help evaluate the quality of the dataset, validate our evaluation methods, and measure the inherent knowledge of different LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;We evaluated Codex CLI, Claude Code, and our Deep Research agent in Qodo Aware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Overall: Qodo’s deep-research agent achieves the best fact recall (~76%), just ahead of OpenAI’s Codex (~74%), while being about twice as fast. Also, with the high reasoning feature, we reached (~80%) with a tradeoff on runtimes, where we see a 10-second optimization for our agent. Both outperform Claude (~64%) and Gemini (~45%).&lt;/item&gt;
      &lt;item&gt;Searchable: All agents improved with searchable keywords in the question, but our DeepResearch’s gain was smallest thanks to strong semantic search.&lt;/item&gt;
      &lt;item&gt;Scope: Codex and Claude preferred deep over broad questions, while DeepResearch performed equally well on both due to wide search capabilities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Overall results&lt;/head&gt;
    &lt;head rend="h2"&gt;Results by data segment&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;scope&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;codex-cli&lt;/cell&gt;
        &lt;cell&gt;claude-code&lt;/cell&gt;
        &lt;cell&gt;gemini-cli&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;deep-research (Qodo)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;broad&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0.72&lt;/cell&gt;
        &lt;cell&gt;0.6&lt;/cell&gt;
        &lt;cell&gt;0.41&lt;/cell&gt;
        &lt;cell&gt;0.76&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;deep&lt;/cell&gt;
        &lt;cell&gt;0.76&lt;/cell&gt;
        &lt;cell&gt;0.67&lt;/cell&gt;
        &lt;cell&gt;0.48&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.77&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Searchable&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;codex-cli&lt;/cell&gt;
        &lt;cell&gt;claude-code&lt;/cell&gt;
        &lt;cell&gt;gemini-cli&lt;/cell&gt;
        &lt;cell&gt;deep-research(Qodo)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;False&lt;/cell&gt;
        &lt;cell&gt;0.73&lt;/cell&gt;
        &lt;cell&gt;0.59&lt;/cell&gt;
        &lt;cell&gt;0.43&lt;/cell&gt;
        &lt;cell&gt;0.76&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;True&lt;/cell&gt;
        &lt;cell&gt;0.76&lt;/cell&gt;
        &lt;cell&gt;0.68&lt;/cell&gt;
        &lt;cell&gt;0.47&lt;/cell&gt;
        &lt;cell&gt;0.77&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;What We’re Releasing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dataset: 1,144 carefully curated question-answer pairs – deep_code_bench.&lt;/item&gt;
      &lt;item&gt;Metadata and context: Each question is linked to the pull request (PR) it was generated from and tagged with category labels (e.g., broad/deep, is searchable).&lt;/item&gt;
      &lt;item&gt;Prompts: The exact prompts used to guide question and answer generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Appendix A – prompt for question generation&lt;/head&gt;
    &lt;head rend="h3"&gt;System Prompt&lt;/head&gt;
    &lt;p&gt;You are helping build a high-quality dataset of real-world codebase questions to test our search AI agents. Each question should require the agent to search through the codebase to find the relevant code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Guidelines&lt;/head&gt;
    &lt;p&gt;Your task is to generate exactly ONE onboarding question adhering to these guidelines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The question must be clearly grounded in the provided code context.&lt;/item&gt;
      &lt;item&gt;Do not include exact file paths, line numbers, or raw code snippets in the question text.&lt;/item&gt;
      &lt;item&gt;Prefer questions involving relationships across multiple functions, components, or files.&lt;/item&gt;
      &lt;item&gt;Keep the wording concise, clear, and readable.&lt;/item&gt;
      &lt;item&gt;Avoid vague reference to code elements like ‘the function’ or ‘that class’.&lt;/item&gt;
      &lt;item&gt;Don’t make identifier references (function names, class names, variables, etc.) too obvious, so that the search will be as challenging as possible.&lt;/item&gt;
      &lt;item&gt;Despite the above, the question should still be answerable, and the context should be unambiguous.&lt;/item&gt;
      &lt;item&gt;The question should be answerable with a short, concise response—ideally, a single short sentence.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Scopes&lt;/head&gt;
    &lt;p&gt;There are 2 kinds of scopes. When provided with only 1–2 short code blocks, generate a DEEP question: a highly specific question that explores internal logic, error handling, edge cases, or detailed behaviors. When provided with multiple code blocks or a larger context, generate a BROAD question: a higher-level question about architecture, overall flow, interactions between modules, or general system design.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core questions&lt;/head&gt;
    &lt;p&gt;Core questions targeting fundamental, core functionality versus non-core questions which are focusing on peripheral technical aspects.&lt;/p&gt;
    &lt;head rend="h3"&gt;PR details&lt;/head&gt;
    &lt;p&gt;If a PR title and description are provided, use them only to infer the high-level subject of the question. Think of questions that the developer needs to know in order to address the PR. The question must still be answerable using the code context. If the PR text lacks details, base the question solely on the code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Examples&lt;/head&gt;
    &lt;p&gt;Here are examples to illustrate the desired style and scope:&lt;/p&gt;
    &lt;head rend="h4"&gt;Broad question examples:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What is the general workflow for training and deploying a transformer-based language model?&lt;/item&gt;
      &lt;item&gt;Can you describe the internal steps involved in performing hyperparameter tuning with a grid search?&lt;/item&gt;
      &lt;item&gt;What’s the end-to-end flow involved in generating images using diffusion-based models?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Deep question examples:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How are gradient updates managed when training gradient-boosted decision trees on sparse data?&lt;/item&gt;
      &lt;item&gt;Which parameter directly controls the number of leaves permitted in each decision tree of a gradient boosting algorithm?&lt;/item&gt;
      &lt;item&gt;How does a functional deep learning API internally handle merging layers with multiple input tensors?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Core question examples:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How are token and positional embeddings combined and fed into the BERT model?&lt;/item&gt;
      &lt;item&gt;How does the Keras Layer base class manage weight creation and the build/call lifecycle?&lt;/item&gt;
      &lt;item&gt;What happens in one XGBoost boosting iteration—how are new trees grown and combined?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Output format&lt;/head&gt;
    &lt;p&gt;Return the question, its type, whether it is a core question, and the relevant NODE IDENTIFIER headers from the context as a JSON object with keys ‘question’, ‘scope’, ‘is_core_question’, and ‘nodes’ (a list of strings). Wrap the JSON in triple backticks.&lt;/p&gt;
    &lt;head rend="h3"&gt;User message prompt&lt;/head&gt;
    &lt;p&gt;PR info:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;{pr_context}&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Code context:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;{context}&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Based on the PR information and code above, write ONE question and return only the requested JSON.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45209532</guid></item><item><title>Reshaped is now open source</title><link>https://reshaped.so/blog/reshaped-oss</link><description>&lt;doc fingerprint="b5483351fd033f36"&gt;
  &lt;main&gt;
    &lt;p&gt;About five years ago, I started Reshaped. I built it for myself, since I had a few projects in urgent need of a component library for both React and Figma. Having worked in the design systems space for over a decade, I had developed a clear sense of what a good design system should be — and what tends to go wrong in others.&lt;/p&gt;
    &lt;p&gt;I noticed that no matter how trends evolve, around 80% of the web is still built on the same core design practices. So I set out to build a system that covers that 80%, while giving developers the flexibility to handle the last 20% with low-level utilities. From the start, I didn’t want to focus only on accessibility or only on design. Instead, I prioritized alignment between design and engineering, while also solving common UI challenges like theming, dark mode, and micro-animations.&lt;/p&gt;
    &lt;p&gt;To keep the project sustainable, I made it a paid product: one-time licenses for individuals, and source code licenses for larger teams. This allowed me to focus on supporting a smaller community and dive deeply into every bug report and feature request.&lt;/p&gt;
    &lt;p&gt;While this model kept me motivated and financially supported, I always hoped to remove the paywall one day.&lt;/p&gt;
    &lt;p&gt;Two years ago, I took the first step by making the React package free. That unlocked new possibilities—not only did indie developers gain free access, but teams using Reshaped with source code licenses could now install it directly from npm.&lt;/p&gt;
    &lt;p&gt;Today, I’m taking the next step: making all of Reshaped fully open source. The React library source code is now on GitHub and the Figma library is available in the Figma Community. I’m especially excited because Reshaped bridges both design and engineering, and I hope it helps both communities learn best practices for building design systems that scale while staying minimal.&lt;/p&gt;
    &lt;p&gt;Making both libraries public also opens the door for me to share more behind-the-scenes work as new features roll out. I think this is particularly valuable when it comes to integration with other tools. Imagine Figma or React releasing new features – you’ll be able to see how they’re implemented in Reshaped before you even need to migrate your company’s design system.&lt;/p&gt;
    &lt;p&gt;This is a leap of faith for me after five years of working closed-source. It feels like the right time to give everything back to the community—and to have some fun along the way ❤️&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45209558</guid></item><item><title>Brussels faces privacy crossroads over encryption backdoors</title><link>https://www.theregister.com/2025/09/11/eu_chat_control/</link><description>&lt;doc fingerprint="afed19d53821b34d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Brussels faces privacy crossroads over encryption backdoors&lt;/head&gt;
    &lt;head rend="h2"&gt;Over 600 security boffins say planned surveillance crosses the line&lt;/head&gt;
    &lt;p&gt;Europe, long seen as a bastion of privacy and digital rights, will debate this week whether to enforce surveillance on citizens' devices.&lt;/p&gt;
    &lt;p&gt;Representatives from member states will meet on Friday to consider legislation critics call Chat Control, aka "laying down rules to prevent and combat child sexual abuse," which seeks to require ISPs or messaging app providers to scan user content or backdoor encryption so that intelligence agencies can do it themselves. It's the latest attempt in a three-year campaign by some in the community to allow government agencies unprecedented access to private communications.&lt;/p&gt;
    &lt;p&gt;The proposed legislation has been in the works since 2022 but immediately drew fire from security professionals. After being rejected by EU member states repeatedly, this latest attempt has come at the request of the Danish delegation, which currently holds the EU presidency, and should go to a full vote next month.&lt;/p&gt;
    &lt;p&gt;An open letter signed by more than 600 security academics, practitioners, and stakeholders has called on the proposals to be dropped and claimed they are unworkable and highly intrusive. It also points out that the false positive detection rate for such a serious crime is unacceptable and could lead to many people being unfairly smeared.&lt;/p&gt;
    &lt;p&gt;One signatory, Matthew Green, associate professor of computer science at the Johns Hopkins Information Security Institute, told The Register that the plans, if implemented, would be a "national security disaster."&lt;/p&gt;
    &lt;p&gt;He pointed out that if encryption backdoors were implemented, adversarial nations would see it as a "Manhattan Project" which could be used to expose all data, and if client-side scanning was used then it would create a privacy nightmare.&lt;/p&gt;
    &lt;p&gt;The revised legislative proposals call for systems to be set up to find all current "and new" forms of CSAM, but decline to give any guidance as to how this seemingly impossible task would be achieved. Government and military communications would be exempt from the plan.&lt;/p&gt;
    &lt;p&gt;"It is science fiction," fellow signatory Bart Preneel, the Belgian cryptographer and former president of the International Association for Cryptologic Research, told us. "The latest draft extends the detection order to new CSAM – it is assumed that AI can do this in a reliable way 'quod non.'" This is a Latin term loosely translated as "which it does not."&lt;/p&gt;
    &lt;p&gt;While there are plenty of companies that would love to provide this service, they lack the technical expertise to do so, he pointed out. Also, the best estimates show around a 10 percent false positive rate for client-side scanning – which could see a huge number of people accused of crimes they didn't commit.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;EU attempt to sneak through new encryption-eroding law slammed by Signal, politicians&lt;/item&gt;
      &lt;item&gt;European Court of Human Rights declares backdoored encryption is illegal&lt;/item&gt;
      &lt;item&gt;German Digital Affairs Committee hearing heaps scorn on Chat Control&lt;/item&gt;
      &lt;item&gt;Scanning phones to detect child abuse evidence is harmful, 'magical' thinking&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If passed, the legislation would require encrypted app makers like WhatsApp, iMessage, Signal, Telegram, and Tuta to find ways to enforce such scanning – something they have neither the ability nor the desire to do.&lt;/p&gt;
    &lt;p&gt;Similar legislation has passed in the UK, but with an admission that the plans for message scanning are unworkable at the moment. Attempts to enforce them have failed, and drawn the ire of the US government, which has warned it would not look on such proposals favorably.&lt;/p&gt;
    &lt;p&gt;Signal, possibly the gold standard of end-to-end encrypted services, has said it will fight any moves to enforce such rules. Tuta spokesperson Hanna Bozakov told us that the company would not comply and would consider moving outside the EU if the legislation passed, but only after fighting it in the courts.&lt;/p&gt;
    &lt;p&gt;"First of all, we will sue, because we are pretty certain that this will not stand up in court," she said. "You can't do this because we have privacy rights in the EU Constitution, and you can't just overwrite this."&lt;/p&gt;
    &lt;p&gt;However, sources told The Register that some EU members might be getting cold feet about the plans. Two people told us that the German delegation, which has previously been highly skeptical of the proposals, could ask for a delay for further consideration. We'll see on Friday what happens. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210442</guid></item><item><title>Piramidal (YC W24) Is Hiring Back End Engineer</title><link>https://www.ycombinator.com/companies/piramidal/jobs/1HvdaXs-full-stack-engineer-platform</link><description>&lt;doc fingerprint="7e375edefd2d2b68"&gt;
  &lt;main&gt;
    &lt;p&gt;Foundation Model for the Brain&lt;/p&gt;
    &lt;p&gt;We are looking for a software engineer to help us enable interactions and automations with Piramidal’s newest technologies. We value proactive, customer-centric engineers who prioritize foundational details (data models, architecture, security) to enable excellent products.&lt;/p&gt;
    &lt;p&gt;We are building a first-of-its-kind foundation model for electrophysiological brain data. Our goal is to create scaled neural decoders that enable humans to understand and control neural syntax.&lt;/p&gt;
    &lt;p&gt;We are dedicated to redirecting technology to maximize human potential. At the heart of our mission is support for cognitive liberty - the fundamental right to freedom of thought, mental privacy, and self-determination.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210539</guid></item><item><title>Gregg Kellogg has passed away</title><link>https://lists.w3.org/Archives/Public/public-json-ld-wg/2025Sep/0012.html</link><description>&lt;doc fingerprint="f238957fba6b279f"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;From: Coralie Mercier &amp;lt;coralie@w3.org&amp;gt;&lt;/item&gt;
      &lt;item&gt;Date: Thu, 11 Sep 2025 13:58:34 +0200&lt;/item&gt;
      &lt;item&gt;To: public-json-ld-wg@w3.org&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;Dear Members of the group, We were informed yesterday of the passing last Saturday of Gregg Kellogg. It is terrible news but Gregg had been open about his health &amp;lt;https://greggkellogg.net/health-faq.html&amp;gt;. Gregg had been a prolific and appreciated W3C contributor for many years as a W3C Invited Expert, most recently as co-chair of the JSON-LD Working Group; he was also Chair of several data-related Community Groups. He was said to truly value the energy, brilliance, and camaraderie he found in his groups, and to feel rewarded for being part of such an inventive community. Over the past 13 years [1], Gregg has been co-editor of 9 published recommendations, and a dozen other W3C specifications (CSV2RDF suite, a large portion of the RDF 1.2 suite, 1.0 and 1.1 versions of the JSON-LD suite, RCH, etc.) Remarkably, Gregg also provided open-source implementations of all these specifications (and more), as well a numerous test suites that the relevant Working Groups are still using. His implication in the JSON-LD Working Group was instrumental in the huge success of this W3C technology. For all this, but also and foremost for his friendliness and good nature, he will be missed. If any of you would like to pay tribute to honor Gregg Kellogg, please get in touch with the JSON-LD Working Group Staff Contact, Pierre-Antoine Champin &amp;lt;pierre-antoine@w3.org&amp;gt;, as this group is currently making plans. With kind regards, Coralie Mercier, Director of W3C Marketing &amp;amp; Communications [1] https://api.w3.org/users/8nomt3nm1n8cw4s44koggo4wcoo8w4s/specifications [This message was distributed to: * the Members mailing list; * the W3C Groups chairs mailing list; * the public mailing lists of the WG and CGs that Gregg was chairing: public-json-ld-wg public-json-ld public-rdf-tests public-schemaorg] -- Coralie Mercier (she/her) - Director of W3C Marketing &amp;amp; Communications mailto:coralie@w3.org - https://www.w3.org/People/Coralie/&lt;/quote&gt;
    &lt;p&gt;Received on Thursday, 11 September 2025 11:58:56 UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210564</guid></item><item><title>Show HN: I built a minimal Forth-like stack interpreter library in C</title><link>https://news.ycombinator.com/item?id=45210654</link><description>&lt;doc fingerprint="3cd5c7a8f7e753dc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;This weekend I created stacklib.h - a single-header library that brings Forth-style stack operations to C. It implements a basic interpreter with:&lt;/p&gt;
      &lt;p&gt;- Stack operations (push/pop/dup/swap/over/drop) - Arithmetic (+, -, *, /) - Output (., emit, cr) - Stack inspection (.s, depth)&lt;/p&gt;
      &lt;p&gt;Example usage: Stack s; stack_init(&amp;amp;s); dict_init(); exec(&amp;amp;s, "10 20 + ."); // Prints "30" exec(&amp;amp;s, "1 2 3 4 .s"); // Shows stack contents&lt;/p&gt;
      &lt;p&gt;The library is self-contained, requires no dependencies, and handles basic error checking. It was inspired by wanting to understand how Forth works at a fundamental level while keeping the simplicity of C.&lt;/p&gt;
      &lt;p&gt;I'm curious what other stack-based or concatenative programming enthusiasts think about this approach. Has anyone else built something similar? What features would you add to make it more useful?&lt;/p&gt;
      &lt;p&gt;GitHub: https://github.com/Ferki-git-creator/stacklib&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210654</guid></item><item><title>The Rise of Async Programming</title><link>https://www.braintrust.dev/blog/async-programming</link><description>&lt;doc fingerprint="3df0b2e968b31c89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The rise of async programming&lt;/head&gt;
    &lt;p&gt;I spend a decent amount of time reviewing code I didn't write. An AI agent takes a detailed problem description, writes code (primarily Typescript, Rust, and Python), adds tests, and commits the changes to a branch. I tap back in when everything's ready for review.&lt;/p&gt;
    &lt;p&gt;This used to feel like a futuristic scenario, but it's how I work now, and it's how many developers are starting to work. The shift is subtle but powerful: instead of writing code line by line, we're learning to describe problems clearly and let tools solve them in the background.&lt;/p&gt;
    &lt;head rend="h2"&gt;The async programming workflow&lt;/head&gt;
    &lt;p&gt;This version of "async programming" is different from the classic definition. It's about how developers approach building software.&lt;/p&gt;
    &lt;p&gt;The workflow looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define the problem clearly. Write a detailed specification of what needs to be built, including edge cases, constraints, and success criteria.&lt;/item&gt;
      &lt;item&gt;Hand it off. Delegate the implementation to an AI agent, a teammate, or even your future self with comprehensive notes.&lt;/item&gt;
      &lt;item&gt;Return later. Come back to review results, provide feedback, and decide on next steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key difference from traditional programming is the time separation between problem definition and implementation. Instead of immediate feedback loops, you have background problem solving driven by clear requirements and automated verification.&lt;/p&gt;
    &lt;p&gt;Async programming is not vibe coding. Vibe coding enables you to write code without getting into the nitty gritty details. Async programming is a workflow for developers to solve more complex problems simultaneously, while still understanding the details of the code being written. You're still architecting solutions, reviewing implementations, and maintaining a codebase. You're just not typing a vast majority of characters yourself.&lt;/p&gt;
    &lt;head rend="h2"&gt;The three pillars of async programming&lt;/head&gt;
    &lt;p&gt;For async programming to work in practice, you need three things: a clear definition of the problem you're solving, a way to automatically verify your results, and human-driven code review.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Clear problem definitions&lt;/head&gt;
    &lt;p&gt;The quality of your problem statement determines everything else. Vague requirements produce vague results. Precise specifications produce working code.&lt;/p&gt;
    &lt;p&gt;Here's the difference:&lt;/p&gt;
    &lt;p&gt;Vague: "Make the search faster"&lt;/p&gt;
    &lt;p&gt;Precise: "My goal is to reduce search latency from about 800ms to around 200ms. I suspect the root cause is the heap allocation I'm doing on each batch of rows. Can you try refactoring the allocation to happen once per search, instead, and measure the impact?"&lt;/p&gt;
    &lt;p&gt;The precise version includes the current state, target outcome, proposed approach, and acceptance criteria. An AI agent (or human teammate) can work independently because the requirements are unambiguous.&lt;/p&gt;
    &lt;p&gt;Effective async programming specs read like technical documentation: they include context, constraints, examples, and explicit success criteria. If you can't explain the problem clearly, you probably don't understand it well enough to delegate it.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Automated verification&lt;/head&gt;
    &lt;p&gt;Async programming only works if you can verify results without manual testing every edge case. You need systems that can check the work automatically.&lt;/p&gt;
    &lt;p&gt;This might include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unit and integration tests that validate core functionality&lt;/item&gt;
      &lt;item&gt;Type checking that catches interface mismatches&lt;/item&gt;
      &lt;item&gt;Performance benchmarks that ensure code meets speed requirements&lt;/item&gt;
      &lt;item&gt;Linting and formatting that enforce style guidelines&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The goal is developing a process that agents can use to validate their work independently. This takes time. You'll provide significant guidance initially, then develop patterns that allow agents to work autonomously. Setting this up in CI is challenging but enables background agents to perform work outside your development environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Detailed code review&lt;/head&gt;
    &lt;p&gt;Once you're not typing every character yourself, code review becomes absolutely crucial. I regularly find PRs that solve the completely wrong problem, make poor design decisions, or have large amounts of code duplication.&lt;/p&gt;
    &lt;p&gt;Reviewing AI-generated code is valuable, similar to traditional code review. Expect to spend significantly more time on code review than before.&lt;/p&gt;
    &lt;p&gt;The code may not be yours line by line, but the system design and technical decisions should still reflect your judgment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why async programming works&lt;/head&gt;
    &lt;p&gt;My workflow has changed since adopting async programming. I now work on four or five tasks simultaneously: one complex problem synchronously and three or four in the background. When I context switch, I review in-progress work on each background task, provide guidance, and return to synchronous work or code review.&lt;/p&gt;
    &lt;head rend="h2"&gt;Async programming at Braintrust&lt;/head&gt;
    &lt;p&gt;We've been using async programming to build Braintrust itself, and now we're building tools to translate these ideas to AI engineering.&lt;/p&gt;
    &lt;p&gt;Traditional prompt engineering is manual. You write a prompt, test it against examples, observe failures, make small adjustments, and repeat. The process requires expertise but involves significant iteration.&lt;/p&gt;
    &lt;p&gt;Our agent, Loop, lets you describe the evaluation problem you're trying to solve and spends time in the background analyzing experiment results, identifying patterns in failed test cases, and suggesting improvements to prompts, datasets, and scorers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where this is heading&lt;/head&gt;
    &lt;p&gt;The implications of working this way are still emerging. This changes what I optimize for as a developer: less time on IDE shortcuts and typing speed, more time explaining problems clearly and reviewing solutions thoroughly.&lt;/p&gt;
    &lt;p&gt;The implementation work can happen in parallel with other thinking. More developers will likely adopt this approach as tools improve. AI isn't replacing programming, but the most valuable parts of programming are becoming more prominent while routine tasks move to the background.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210693</guid></item><item><title>Behind the Scenes of Bun Install</title><link>https://bun.com/blog/behind-the-scenes-of-bun-install</link><description>&lt;doc fingerprint="b279a80a8ce08e80"&gt;
  &lt;main&gt;
    &lt;p&gt;Running &lt;code&gt;bun install&lt;/code&gt; is fast, very fast. On average, it runs ~7× faster than npm, ~4× faster than pnpm, and ~17× faster than yarn. The difference is especially noticeable in large codebases. What used to take minutes now takes (milli)seconds.&lt;/p&gt;
    &lt;p&gt;These aren't just cherry-picked benchmarks. Bun is fast because it treats package installation as a systems programming problem, not a JavaScript problem.&lt;/p&gt;
    &lt;p&gt;In this post we’ll explore what that means: from minimizing syscalls and caching manifests as binary, to optimizing tarball extraction, leveraging OS-native file copying, and scaling across CPU cores.&lt;/p&gt;
    &lt;p&gt;But to understand why this matters, we first have to take a small step back in time.&lt;/p&gt;
    &lt;p&gt;It's the year 2009. You're installing jQuery from a &lt;code&gt;.zip&lt;/code&gt; file, your iPhone 3GS has 256MB of RAM. GitHub was just a year old, SSDs cost $700 for 256GB. Your laptop's 5400RPM hard drive maxes out at 100MB/s, and "broadband" means 10 Mbps (if you're lucky).&lt;/p&gt;
    &lt;p&gt;But more importantly: Node.js just launched! Ryan Dahl is on stage explaining why servers spend most of their time waiting.&lt;/p&gt;
    &lt;p&gt;In 2009, a typical disk seek takes 10ms, a database query 50–200ms, and an HTTP request to an external API 300ms+. During each of these transactions, traditional servers would just... wait. Your server would start reading a file, and then just freeze for 10ms.&lt;/p&gt;
    &lt;p&gt;Now multiply that by thousands of concurrent connections each doing multiple I/O operations. Servers spent ~95% of their time waiting for I/O operations.&lt;/p&gt;
    &lt;p&gt;Node.js figured that JavaScript's event loop (originally designed for browser events) was perfect for server I/O. When code makes an async request, the I/O happens in the background while the main thread immediately moves to the next task. Once complete, a callback gets queued for execution.&lt;/p&gt;
    &lt;p&gt;JavaScript's event loop was a great solution for a world where waiting for data was the primary bottleneck.&lt;/p&gt;
    &lt;p&gt;For the next 15 years, Node's architecture shaped how we built tools. Package managers inherited Node's thread pool, event loop, async patterns; optimizations that made sense when disk seeks took 10ms.&lt;/p&gt;
    &lt;p&gt;But hardware evolved. It's not 2009 anymore, we're 16 years into the future, as hard as that is to believe. The M4 Max MacBook I'm using to write this would've ranked among the 50 fastest supercomputers on Earth in 2009. Today's NVMe drives push 7,000 MB/s, 70× faster than what Node.js was designed for! The slow mechanical drives are gone, internet speeds stream 4K video, and even low-end smartphones have more RAM than high-end servers had in 2009.&lt;/p&gt;
    &lt;p&gt;Yet today's package managers still optimize for the last decade's problems. In 2025, the real bottleneck isn't I/O anymore. It's system calls.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem with System Calls&lt;/head&gt;
    &lt;p&gt;Every time your program wants the operating system to do something (read a file, open a network connection, allocate memory), it makes a system call. Each time you make a system call, the CPU has to perform a mode switch.&lt;/p&gt;
    &lt;p&gt;Your CPU can run programs in two modes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;user mode&lt;/code&gt;, where your application code runs. Programs in&lt;code&gt;user mode&lt;/code&gt;cannot directly access your device's hardware, physical memory addresses, etc. This isolation prevents programs from interfering with each other or crashing the system.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;kernel mode&lt;/code&gt;, where the operating system's kernel runs. The kernel is the core component of the OS that manages resources like scheduling processes to use the CPU, handling memory, and hardware like disks or network devices. Only the kernel and device drivers operate in&lt;code&gt;kernel mode&lt;/code&gt;!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you want to open a file, (e.g. &lt;code&gt;fs.readFile()&lt;/code&gt;) in your program, the CPU running in &lt;code&gt;user mode&lt;/code&gt; cannot directly read from disk. It first has to switch to &lt;code&gt;kernel mode&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;During this mode switch, the CPU stops executing your program → saves all its state → switches into kernel mode → performs the operation → then switches back to user mode.&lt;/p&gt;
    &lt;p&gt;However, this mode switching is expensive! Just this switch alone costs 1000-1500 CPU cycles in pure overhead, before any actual work happens.&lt;/p&gt;
    &lt;p&gt;Your CPU operates on a clock that ticks billions of times per second. A 3GHz processor completes 3 billion cycles per second. During each cycle the CPU can execute instructions: add numbers, move data, make comparisons, etc. Each cycle takes 0.33ns.&lt;/p&gt;
    &lt;p&gt;On a 3GHz processor, 1000-1500 cycles is about 500 nanoseconds. This might sound negligibly fast, but modern SSDs can handle over 1 million operations per second. If each operation requires a system call, you're burning 1.5 billion cycles per second just on mode switching.&lt;/p&gt;
    &lt;p&gt;Package installation makes thousands of these system calls. Installing React and its dependencies might trigger 50,000+ system calls: that's seconds of CPU time lost to mode switching alone! Not even reading files or installing packages, just switching between user and kernel mode.&lt;/p&gt;
    &lt;p&gt;This is why Bun treats package installation as a systems programming problem. Fast install speeds come from minimizing system calls and leveraging every OS-specific optimization available.&lt;/p&gt;
    &lt;p&gt;You can see the difference when we trace the actual system calls made by each package manager:&lt;/p&gt;
    &lt;code&gt;Benchmark 1: strace -c -f npm install
    Time (mean ± σ):  37.245 s ±  2.134 s [User: 8.432 s, System: 4.821 s]
    Range (min … max):   34.891 s … 41.203 s    10 runs

    System calls: 996,978 total (108,775 errors)
    Top syscalls: futex (663,158),  write (109,412), epoll_pwait (54,496)

  Benchmark 2: strace -c -f bun install
    Time (mean ± σ):      5.612 s ±  0.287 s [User: 2.134 s, System: 1.892 s]
    Range (min … max):    5.238 s …  6.102 s    10 runs

    System calls: 165,743 total (3,131 errors)
    Top syscalls: openat(45,348), futex (762), epoll_pwait2 (298)

  Benchmark 3: strace -c -f yarn install
    Time (mean ± σ):     94.156 s ±  3.821 s    [User: 12.734 s, System: 7.234 s]
    Range (min … max):   89.432 s … 98.912 s    10 runs

    System calls: 4,046,507 total (420,131 errors)
    Top syscalls: futex (2,499,660), epoll_pwait (326,351), write (287,543)

  Benchmark 4: strace -c -f pnpm install
    Time (mean ± σ):     24.521 s ±  1.287 s    [User: 5.821 s, System: 3.912 s]
    Range (min … max):   22.834 s … 26.743 s    10 runs

    System calls: 456,930 total (32,351 errors)
    Top syscalls: futex (116,577), openat(89,234), epoll_pwait (12,705)

  Summary
    'strace -c -f bun install' ran
      4.37 ± 0.28 times faster than 'strace -c -f pnpm install'
      6.64 ± 0.51 times faster than 'strace -c -f npm install'
     16.78 ± 1.12 times faster than 'strace -c -f yarn install'

  System Call Efficiency:
    - bun:  165,743 syscalls (29.5k syscalls/s)
    - pnpm: 456,930 syscalls (18.6k syscalls/s)
    - npm:  996,978 syscalls (26.8k syscalls/s)
    - yarn: 4,046,507 syscalls (43.0k syscalls/s)
&lt;/code&gt;
    &lt;p&gt;We can see that Bun installs much faster, but it also makes far fewer system calls. For a simple install, yarn makes over 4 million system calls, npm almost 1 million, pnpm close to 500k, and bun 165k.&lt;/p&gt;
    &lt;p&gt;At 1000-1500 cycles per call, yarn's 4 million system calls means it's spending billions of CPU cycles just on mode switching. On a 3GHz processor, that's seconds of pure overhead!&lt;/p&gt;
    &lt;p&gt;And it's not just the amount of system calls. Look at those &lt;code&gt;futex&lt;/code&gt; calls! Bun made 762 &lt;code&gt;futex&lt;/code&gt; calls (only 0.46% of total system calls), whereas npm made 663,158 (66.51%), yarn made 2,499,660 (61.76%), and pnpm made 116,577 (25.51%).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;futex&lt;/code&gt; (fast userspace mutex) is a Linux system call used for thread synchronization. Threads are smaller units of a program that run simultaneously that often share access to memory or resources, so they must coordinate to avoid conflicts.&lt;/p&gt;
    &lt;p&gt;Most of the time, threads coordinate using fast atomic CPU instructions in &lt;code&gt;user mode&lt;/code&gt;. There's no need to switch to &lt;code&gt;kernel mode&lt;/code&gt;, so it's very efficient!&lt;/p&gt;
    &lt;p&gt;But if a thread tries to acquire a lock that's already taken, it makes a &lt;code&gt;futex&lt;/code&gt; syscall to ask the kernel to put it to sleep until the lock becomes available. A high number of &lt;code&gt;futex&lt;/code&gt; calls is an indicator that many threads are waiting on one another, causing delays.&lt;/p&gt;
    &lt;p&gt;So what's Bun doing differently here?&lt;/p&gt;
    &lt;head rend="h2"&gt;Eliminating JavaScript overhead&lt;/head&gt;
    &lt;p&gt;npm, pnpm and yarn are all written in Node.js. In Node.js, system calls aren’t made directly: when you call &lt;code&gt;fs.readFile()&lt;/code&gt;, you’re actually going through several layers before reaching the OS.&lt;/p&gt;
    &lt;p&gt;Node.js uses libuv, a C library that abstracts platform differences and manages async I/O through a thread pool.&lt;/p&gt;
    &lt;p&gt;The result is that when Node.js has to read a single file, it triggers a pretty complex pipeline. For a simple &lt;code&gt;fs.readFile('package.json', ...)&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;JavaScript validates arguments and converts strings from UTF-16 to UTF-8 for libuv's C APIs. This briefly blocks the main thread before any I/O even starts.&lt;/item&gt;
      &lt;item&gt;libuv queues the request for one of 4 worker threads. If all threads are busy, your request waits.&lt;/item&gt;
      &lt;item&gt;A worker thread picks up the request, opens the file descriptor, and makes the actual &lt;code&gt;read()&lt;/code&gt;system call.&lt;/item&gt;
      &lt;item&gt;The kernel switches to &lt;code&gt;kernel mode&lt;/code&gt;, fetches the data from disk, and returns it to the worker thread.&lt;/item&gt;
      &lt;item&gt;The worker pushes the file data back to the main thread through the event loop, which eventually schedules and runs your callback.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every single &lt;code&gt;fs.readFile()&lt;/code&gt; call goes through this pipeline. Package installation involves reading thousands of &lt;code&gt;package.json&lt;/code&gt; files: scanning directories, processing dependency metadata, and so on. Each time threads coordinate (e.g., when accessing the task queue or signaling back to the event loop), a &lt;code&gt;futex&lt;/code&gt; system call can be used to manage locks or waits.&lt;/p&gt;
    &lt;p&gt;The overhead of making thousands of these system calls can take longer than the actual data movement itself!&lt;/p&gt;
    &lt;p&gt;Bun does it differently. Bun is written in Zig, a programming language that compiles to native code with direct system call access:&lt;/p&gt;
    &lt;code&gt;// Direct system call, no JavaScript overhead
var file = bun.sys.File.from(try bun.sys.openatA(
    bun.FD.cwd(),
    abs,
    bun.O.RDONLY,
    0,
).unwrap());
&lt;/code&gt;
    &lt;p&gt;When Bun reads a file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Zig code directly invokes the system call (e.g., &lt;code&gt;openat()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;The kernel immediately executes the system call and returns data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's it. There's no JavaScript engine, thread pools, event loops or marshaling between different runtime layers. Just native code making direct system calls to the kernel.&lt;/p&gt;
    &lt;p&gt;The performance difference speaks for itself:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Runtime&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Files/Second&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;v1.2.20&lt;/cell&gt;
        &lt;cell&gt;146,057&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;v24.5.0&lt;/cell&gt;
        &lt;cell&gt;66,576&lt;/cell&gt;
        &lt;cell&gt;2.2x slower&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;v22.18.0&lt;/cell&gt;
        &lt;cell&gt;64,631&lt;/cell&gt;
        &lt;cell&gt;2.3x slower&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In this benchmark, Bun processes 146,057 &lt;code&gt;package.json&lt;/code&gt; files per second, while Node.js v24.5.0 manages 66,576 and v22.18.0 handles 64,631. That's over 2x faster!&lt;/p&gt;
    &lt;p&gt;Bun's 0.019ms per file represents the actual I/O cost, so how long it takes to read data when you make direct system calls without any runtime overhead. Node.js takes 0.065ms for the same operation. Package managers written in Node.js are "stuck" with Node's abstractions; they use the thread pool whether they need it or not. But they pay this cost on every file operation.&lt;/p&gt;
    &lt;p&gt;Bun's package manager is more like a native application that happens to understand JavaScript packages, not a JavaScript application trying to do systems programming.&lt;/p&gt;
    &lt;p&gt;Even though Bun isn't written in Node.js, you can use &lt;code&gt;bun install&lt;/code&gt; in any Node.js project without switching runtimes. Bun's package manager respects your existing Node.js setup and tooling, you just get faster installs!&lt;/p&gt;
    &lt;p&gt;But at this point we haven't even started installing packages yet. Let's see the optimizations Bun applies to the actual installation.&lt;/p&gt;
    &lt;p&gt;When you type &lt;code&gt;bun install&lt;/code&gt;, Bun first figures out what you're asking it to do. It reads any flags you've passed, and finds your &lt;code&gt;package.json&lt;/code&gt; to read your dependencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Async DNS Resolution&lt;/head&gt;
    &lt;p&gt;⚠️ Note: This optimization is specific to macOS&lt;/p&gt;
    &lt;p&gt;Working with dependencies means working with network requests, and network requests require DNS resolution to convert domain names like &lt;code&gt;registry.npmjs.org&lt;/code&gt; into IP addresses.&lt;/p&gt;
    &lt;p&gt;As Bun is parsing the &lt;code&gt;package.json&lt;/code&gt;, it already starts to prefetch the DNS lookups. This means network resolution begins even before dependency analysis is even complete.&lt;/p&gt;
    &lt;p&gt;For a Node.js-based package managers, one way to do it is by using &lt;code&gt;dns.lookup()&lt;/code&gt;. While this looks async from JavaScript's perspective, it's actually implemented as a blocking &lt;code&gt;getaddrinfo()&lt;/code&gt; call under the hood, running on &lt;code&gt;libuv&lt;/code&gt;'s thread pool. It still blocks a thread, just not the main thread.&lt;/p&gt;
    &lt;p&gt;As a nice optimization, Bun takes a different approach on macOS by making it truly asynchronous at the system level. Bun uses Apple's "hidden" async DNS API (&lt;code&gt;getaddrinfo_async_start()&lt;/code&gt;), which isn't part of the POSIX standard, but it allows bun to make DNS requests that run completely asynchronously using mach ports, Apple's inter-process communication system.&lt;/p&gt;
    &lt;p&gt;While DNS resolution happens in the background, Bun can continue processing other operations like file I/O, network requests, or dependency resolution without any thread blocking. By the time it needs to download React, the DNS lookup is already done.&lt;/p&gt;
    &lt;p&gt;It's a small optimization (and not benchmarked), but it shows Bun's attention to detail: optimize at every layer!&lt;/p&gt;
    &lt;head rend="h2"&gt;Binary Manifest Caching&lt;/head&gt;
    &lt;p&gt;Now that Bun has established a connection to the npm registry, it needs the package manifests.&lt;/p&gt;
    &lt;p&gt;A manifest is a JSON file containing all versions, dependencies, and metadata for each package. For popular packages like React with 100+ versions, these manifests can be several megabytes!&lt;/p&gt;
    &lt;p&gt;A typical manifest can look something like this:&lt;/p&gt;
    &lt;code&gt;{
  "name": "lodash",
  "versions": {
    "4.17.20": {
      "name": "lodash",
      "version": "4.17.20",
      "description": "Lodash modular utilities.",
      "license": "MIT",
      "repository": {
        "type": "git",
        "url": "git+https://github.com/lodash/lodash.git"
      },
      "homepage": "https://lodash.com/"
    },
    "4.17.21": {
      "name": "lodash",
      "version": "4.17.21",
      "description": "Lodash modular utilities.",
      "license": "MIT",
      "repository": {
        "type": "git",
        "url": "git+https://github.com/lodash/lodash.git"
      },
      "homepage": "https://lodash.com/"
    }
    // ... 100+ more versions, nearly identical
  }
}
&lt;/code&gt;
    &lt;p&gt;Most package managers cache these manifests as JSON files in their cache directories. When you run &lt;code&gt;npm install&lt;/code&gt; again, instead of downloading the manifest, they read it from the cache.&lt;/p&gt;
    &lt;p&gt;That all makes sense, but the issue is that on every install (even if it's cached), they still need to parse the JSON file. This includes validating the syntax, building the object tree, managing garbage collection, and so on. A lot of parsing overhead.&lt;/p&gt;
    &lt;p&gt;And it's not just the JSON parsing overhead. Looking at lodash: the string &lt;code&gt;"Lodash modular utilities."&lt;/code&gt; appears in every single version—that's 100+ times. &lt;code&gt;"MIT"&lt;/code&gt; appears 100+ times. &lt;code&gt;"git+https://github.com/lodash/lodash.git"&lt;/code&gt; is duplicated for every version, the URL &lt;code&gt;"https://lodash.com/"&lt;/code&gt; appears in every version. Overall, lots of repeated strings.&lt;/p&gt;
    &lt;p&gt;In memory, JavaScript creates a separate string object for each string. This wastes memory and makes comparisons slower. Every time the package manager checks if two packages use the same version of postcss, it's comparing separate string objects rather than pointing to the same interned string.&lt;/p&gt;
    &lt;p&gt;Bun stores package manifests in a binary format. When Bun downloads package information, it parses the JSON once and stores it as binary files (&lt;code&gt;.npm&lt;/code&gt; files in &lt;code&gt;~/.bun/install/cache/&lt;/code&gt;). These binary files contain all the package information (versions, dependencies, checksums, etc.) stored at specific byte offsets.&lt;/p&gt;
    &lt;p&gt;When Bun accesses the name &lt;code&gt;lodash&lt;/code&gt;, it's just pointer arithmetic: &lt;code&gt;string_buffer + offset&lt;/code&gt;. No allocations, no parsing, no object traversal, just reading bytes at a known location.&lt;/p&gt;
    &lt;code&gt;// Pseudocode

// String buffer (all strings stored once)
string_buffer = "lodash\0MIT\0Lodash modular utilities.\0git+https://github.com/lodash/lodash.git\0https://lodash.com/\04.17.20\04.17.21\0..."
                 ^0     ^7   ^11                        ^37                                      ^79                   ^99      ^107

// Version entries (fixed-size structs)
versions = [
  { name_offset: 0, name_len: 6, version_offset: 99, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... },  // 4.17.20
  { name_offset: 0, name_len: 6, version_offset: 107, version_len: 7, desc_offset: 11, desc_len: 26, license_offset: 7, license_len: 3, ... }, // 4.17.21
  // ... 100+ more version structs
]
&lt;/code&gt;
    &lt;p&gt;To check if packages need updating, Bun stores the responses's &lt;code&gt;ETag&lt;/code&gt; , and sends &lt;code&gt;If-None-Match&lt;/code&gt; headers. When npm responds with &lt;code&gt;"304 Not Modified"&lt;/code&gt;, Bun knows the cached data is fresh without parsing a single byte.&lt;/p&gt;
    &lt;p&gt;Looking at the benchmarks:&lt;/p&gt;
    &lt;code&gt;Benchmark 1: bun install # fresh install
  Time (mean ± σ):      35.7 ms ±  86.6 ms    [User: 8.4 ms, System: 13.4 ms]
  Range (min … max):     4.1 ms … 280.5 ms    10 runs

Benchmark 2: bun install # cached
  Time (mean ± σ):       4.8 ms ±   0.7 ms    [User: 4.7 ms, System: 3.9 ms]
  Range (min … max):     3.7 ms …   6.2 ms    482 runs

Benchmark 3: npm install # fresh install
  Time (mean ± σ):     815.1 ms ± 976.9 ms    [User: 730.1 ms, System: 130.4 ms]
  Range (min … max):   478.8 ms … 3595.1 ms    10 runs

Summary
  bun install # cached ran
    1.02 ± 0.02 times faster than bun install # fresh, no cache
    3.73 ± 0.12 times faster than npm install # cached
&lt;/code&gt;
    &lt;p&gt;Here you can see that a cached(!!) &lt;code&gt;npm install&lt;/code&gt; is slower than a fresh Bun install. That's how much overhead JSON parsing the cached files can add (among other factors).&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized Tarball Extraction&lt;/head&gt;
    &lt;p&gt;Now that Bun has fetched the package manifests, it needs to download and extract compressed tarballs from the npm registry.&lt;/p&gt;
    &lt;p&gt;Tarballs are compressed archive files (like &lt;code&gt;.zip&lt;/code&gt; files) that contain all the actual source code and files for each package.&lt;/p&gt;
    &lt;p&gt;Most package managers stream the tarball data as it arrives, and decompress as it streams in. When you extract a tarball that's streaming in, the typical pattern assumes the size is unknown, and looks something like this:&lt;/p&gt;
    &lt;code&gt;let buffer = Buffer.alloc(64 * 1024); // Start with 64KB
let offset = 0;

function onData(chunk) {
  while (moreDataToCome) {
    if (offset + chunk.length &amp;gt; buffer.length) {
      // buffer full → allocate bigger one
      const newBuffer = Buffer.alloc(buffer.length * 2);

      // copy everything we’ve already written
      buffer.copy(newBuffer, 0, 0, offset);

      buffer = newBuffer;
    }

    // copy new chunk into buffer
    chunk.copy(buffer, offset);
    offset += chunk.length;
  }

  // ... decompress from buffer ...
}
&lt;/code&gt;
    &lt;p&gt;Start with a small buffer, and let it grow as more decompressed data arrives. When the buffer fills up, you allocate a larger buffer, copy all the existing data over, and continue.&lt;/p&gt;
    &lt;p&gt;This seems reasonable, but it creates a performance bottleneck: you end up copying the same data multiple times as the buffer repeatedly outgrows its current size.&lt;/p&gt;
    &lt;p&gt;When we have a 1MB package:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with 64KB buffer&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 128KB → Copy 64KB over&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 256KB → Copy 128KB over&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 512KB → Copy 256KB over&lt;/item&gt;
      &lt;item&gt;Fill up → Allocate 1MB → Copy 512KB over&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You just copied 960KB of data unnecessarily! And this happens for every single package. The memory allocator has to find contiguous space for each new buffer, while the old buffer stays allocated during the copy operation. For large packages, you might copy the same bytes 5-6 times.&lt;/p&gt;
    &lt;p&gt;Bun takes a different approach by buffering the entire tarball before decompressing. Instead of processing data as it arrives, Bun waits until the entire compressed file is downloaded into memory.&lt;/p&gt;
    &lt;p&gt;Now you might think "Wait, aren't they just wasting RAM keeping everything in memory?" And for large packages like TypeScript (which can be 50MB compressed), you'd have a point.&lt;/p&gt;
    &lt;p&gt;But the vast majority of npm packages are tiny, most are under 1MB. For these common cases, buffering the whole thing eliminates all the repeated copying. Even for those larger packages, the temporary memory spike is usually fine on modern systems, and avoiding 5-6 buffer copies more than makes up for it.&lt;/p&gt;
    &lt;p&gt;Once Bun has the complete tarball in memory, it can read the last 4 bytes of the gzip format. These bytes are special since store the uncompressed size of the file! Instead of having to guess how large the uncompressed file will be, Bun can pre-allocate memory to eliminate buffer resizing entirely:&lt;/p&gt;
    &lt;code&gt;{
  // Last 4 bytes of a gzip-compressed file are the uncompressed size.
  if (tgz_bytes.len &amp;gt; 16) {
    // If the file claims to be larger than 16 bytes and smaller than 64 MB, we'll preallocate the buffer.
    // If it's larger than that, we'll do it incrementally. We want to avoid OOMing.
    const last_4_bytes: u32 = @bitCast(tgz_bytes[tgz_bytes.len - 4 ..][0..4].*);
    if (last_4_bytes &amp;gt; 16 and last_4_bytes &amp;lt; 64 * 1024 * 1024) {
      // It's okay if this fails. We will just allocate as we go and that will error if we run out of memory.
      esimated_output_size = last_4_bytes;
      if (zlib_pool.data.list.capacity == 0) {
          zlib_pool.data.list.ensureTotalCapacityPrecise(zlib_pool.data.allocator, last_4_bytes) catch {};
      } else {
          zlib_pool.data.ensureUnusedCapacity(last_4_bytes) catch {};
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Those 4 bytes tell Bun "this gzip will decompress to exactly 1,048,576 bytes", so it can pre-allocate exactly this amount of memory upfront. There's no repeated resizing or copying of data; just one memory allocation.&lt;/p&gt;
    &lt;p&gt;To do the actual decompression, Bun uses &lt;code&gt;libdeflate&lt;/code&gt;. This is a high-performance lib that decompresses tarballs faster than the standard &lt;code&gt;zlib&lt;/code&gt; used by most package managers. It's optimized specifically for modern CPUs with SIMD instructions.&lt;/p&gt;
    &lt;p&gt;Optimized tarball extraction would've been difficult to for package managers written in Node.js. You'd need to create a separate read stream, seek to the end, read 4 bytes, parse them, close the stream, then start over with your decompression. Node's APIs aren't designed for this pattern.&lt;/p&gt;
    &lt;p&gt;In Zig it's pretty straight-forward: you just seek to the end and read the last four bytes, that's it!&lt;/p&gt;
    &lt;p&gt;Now that Bun has all the package data, it faces another challenge: how do you efficiently store and access thousands of (interdependent) packages?&lt;/p&gt;
    &lt;head rend="h2"&gt;Cache-Friendly Data Layout&lt;/head&gt;
    &lt;p&gt;Dealing with thousands of packages can be tricky. Each package has dependencies, which have their own dependencies, creating a pretty complex graph.&lt;/p&gt;
    &lt;p&gt;During installation, package managers have to traverse this graph to check the package versions, resolve any conflicts, and determine which version to install. They also need to "hoist" dependencies by moving them to higher levels so multiple packages can share them.&lt;/p&gt;
    &lt;p&gt;But the way that this dependency graph is stored has a big impact on performance. Traditional package managers store dependencies like this:&lt;/p&gt;
    &lt;code&gt;const packages = {
  next: {
    name: "next",
    version: "15.5.0",
    dependencies: {
      "@swc/helpers": "0.5.15",
      "postcss": "8.4.31",
      "styled-jsx": "5.1.6",
    },
  },
  postcss: {
    name: "postcss",
    version: "8.4.31",
    dependencies: {
      nanoid: "^3.3.6",
      picocolors: "^1.0.0",
    },
  },
};
&lt;/code&gt;
    &lt;p&gt;This looks clean as JavaScript code, but it's not ideal for modern CPU architectures.&lt;/p&gt;
    &lt;p&gt;In JavaScript, each object is stored on the heap. When accessing &lt;code&gt;packages["next"]&lt;/code&gt;, the CPU accesses a pointer that tells it where Next's data is located in memory. This data then contains yet another pointer to where its dependencies live, which in turn contains more pointers to the actual dependency strings.&lt;/p&gt;
    &lt;p&gt;The key issue is how JavaScript allocates objects in memory. When you create objects at different times, the JavaScript engine uses whatever memory is available at that moment:&lt;/p&gt;
    &lt;code&gt;// These objects are created at different moments during parsing
packages["react"] = { name: "react", ... }  	  // Allocated at address 0x1000
packages["next"] = { name: "next", ... }     		// Allocated at address 0x2000
packages["postcss"] = { name: "postcss", ... }  // Allocated at address 0x8000
// ... hundreds more packages
&lt;/code&gt;
    &lt;p&gt;These addresses are basically just random. There is no locality guarantee - objects can just be scattered across RAM, even objects that are related to each other!&lt;/p&gt;
    &lt;p&gt;This random scattering matters because of how modern CPUs actually fetch data.&lt;/p&gt;
    &lt;p&gt;Modern CPUs are incredibly fast at processing data (billions of operations per second), but fetching data from RAM is slow. To bridge this gap, CPUs have multiple cache levels:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;L1 cache, small storage, but extremely fast (~4 CPU cycles)&lt;/item&gt;
      &lt;item&gt;L2 cache, medium storage, a bit slower (~12 CPU cycles)&lt;/item&gt;
      &lt;item&gt;L3 cache: 8-32MB storage, requires ~40 CPU cycles&lt;/item&gt;
      &lt;item&gt;RAM: Lots of GB, requires ~300 cycles (slow!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Visualizing CPU cache speeds vs RAM. Cache optimization matters! pic.twitter.com/q2rkGqSUAG&lt;/p&gt;— Ben Dicken (@BenjDicken) Oct 18, 2024&lt;/quote&gt;
    &lt;p&gt;The "issue" is that caches work with cache lines. When you access memory, the CPU doesn't just load that one byte: it loads the entire 64-byte chunk in which that byte appears. It figures that if you need one byte, you'll probably need nearby bytes soon (this is called spatial locality).&lt;/p&gt;
    &lt;p&gt;This optimization works great for data that's stored sequentially, but it backfires when your data is scattered randomly across memory.&lt;/p&gt;
    &lt;p&gt;When the CPU loads &lt;code&gt;packages["next"]&lt;/code&gt; at address &lt;code&gt;0x2000&lt;/code&gt;, it actually loads all the bytes within that cache line. But the next package, &lt;code&gt;packages["postcss"]&lt;/code&gt;, is at address &lt;code&gt;0x8000&lt;/code&gt; . This is a completely different cache line! The other 56 bytes the CPU loaded in the cache line are just completely wasted, they're just random memory from whatever happened to be allocated nearby; maybe garbage, maybe parts of unrelated objects.&lt;/p&gt;
    &lt;p&gt;But you paid the cost of loading 64 bytes but only used 8...&lt;/p&gt;
    &lt;p&gt;By the time it's accessed 512 different packages (32KB / 64 bytes), you've filled your entire L1 cache already. Now every new package access evicts a previously loaded cache line to make space. The package you just accessed will be evicted soon, and that dependency it needs to check in 10 microseconds is already gone. Cache hit rate drops, and every access becomes a ~300 cycle trip to RAM instead of a 4 cycle L1 hit, far from optimal.&lt;/p&gt;
    &lt;p&gt;The nested structure of objects creates whats called "pointer chasing", a common anti-pattern in system programming. The CPU can't predict where to load next because each pointer could point anywhere. It simply cannot know where &lt;code&gt;next.dependencies&lt;/code&gt; lives until it finishes loading the &lt;code&gt;next&lt;/code&gt; object.&lt;/p&gt;
    &lt;p&gt;When traversing Next's dependencies, the CPU has to perform multiple dependent memory loads:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Load &lt;code&gt;packages["next"]&lt;/code&gt;pointer → Cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
      &lt;item&gt;Follow that pointer to load &lt;code&gt;next.dependencies&lt;/code&gt;pointer → Another cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
      &lt;item&gt;Follow that to find &lt;code&gt;"postcss"&lt;/code&gt;in the hash table → Cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
      &lt;item&gt;Follow that pointer to load the actual string data → Cache miss → RAM fetch (~300 cycles)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can end up with many cache misses since we're working with hundreds of dependencies, all scattered across memory. Each cache line we load (64 bytes) might contain data for just one object. With all those objects spread across GBs of RAM, the working set easily exceeds the L1 cache (32KB), L2 (256KB) and even the L3 cache (8-32MB). By the time we need an object again, it's likely that it's been evicted from all cache levels.&lt;/p&gt;
    &lt;p&gt;That's ~1200 cycles (400ns on a 3GHz CPU) just to read one dependency name! For a project with 1000 packages averaging 5 dependencies each, that's 2ms of pure memory latency.&lt;/p&gt;
    &lt;p&gt;Bun uses Structure of Arrays. Instead of each package storing its own dependency array, Bun keeps all dependencies in one big shared array, all package names in another shared array, and so on:&lt;/p&gt;
    &lt;code&gt;// ❌ Traditional Array of Structures (AoS) - lots of pointers
packages = {
  next: { dependencies: { "@swc/helpers": "0.5.15", "postcss": "8.4.31" } },
};

// ✅ Bun's Structure of Arrays (SoA) - cache friendly
packages = [
  {
    name: { off: 0, len: 4 },
    version: { off: 5, len: 6 },
    deps: { off: 0, len: 2 },
  }, // next
];

dependencies = [
  { name: { off: 12, len: 13 }, version: { off: 26, len: 7 } }, // @swc/helpers@0.5.15
  { name: { off: 34, len: 7 }, version: { off: 42, len: 6 } }, // postcss@8.4.31
];

string_buffer = "next\015.5.0\0@swc/helpers\00.5.15\0postcss\08.4.31\0";
&lt;/code&gt;
    &lt;p&gt;Instead of each package storing pointers to its own data scattered across memory, Bun just uses large contiguous buffers, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;packages&lt;/code&gt;stores lightweight structs that specify where to find this package's data using offsets&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;dependencies&lt;/code&gt;stores the actual dependency relationships for all packages in one place&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;string_buffer&lt;/code&gt;stores all text (names, versions, etc.) sequentially in one massive string&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;versions&lt;/code&gt;stores all parsed semantic versions as compact structs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now, accessing Next's dependencies just becomes arithmetic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;packages[0]&lt;/code&gt;tells us that Next's dependencies start at position&lt;code&gt;0&lt;/code&gt;in the&lt;code&gt;dependencies&lt;/code&gt;array, and there's 2 dependencies:&lt;code&gt;{ name_offset: 0, deps_offset: 0, deps_count: 2 }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go to &lt;code&gt;dependencies[1]&lt;/code&gt;which tells us that postcss's name starts at position&lt;code&gt;34&lt;/code&gt;in the string&lt;code&gt;string_buffer&lt;/code&gt;, and version at position&lt;code&gt;42&lt;/code&gt;:&lt;code&gt;{ name_offset: 34, version_offset: 42 }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go to position 34 in &lt;code&gt;string_buffer&lt;/code&gt;and read&lt;code&gt;postcss&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Go to position 42 in &lt;code&gt;string_buffer&lt;/code&gt;and read&lt;code&gt;"8.4.31"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;… and so on&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now when you access &lt;code&gt;packages[0]&lt;/code&gt;, the CPU doesn't just load those 8 bytes: it loads an entire 64-byte cache line. Since each package is 8 bytes, and 64 ÷ 8 = 8, you get &lt;code&gt;packages[0]&lt;/code&gt; through &lt;code&gt;packages[7]&lt;/code&gt; in a single memory fetch.&lt;/p&gt;
    &lt;p&gt;So when your code processes the &lt;code&gt;react&lt;/code&gt; dependency (&lt;code&gt;packages[0]&lt;/code&gt;, &lt;code&gt;packages[1]&lt;/code&gt; through &lt;code&gt;packages[7]&lt;/code&gt; are already sitting in your L1 cache, ready to be accessed with zero additional memory fetches. That's why sequential access is so fast: you're getting 8 packages just by accessing memory once.&lt;/p&gt;
    &lt;p&gt;Instead of the many small, scattered allocations throughout memory that we saw in the previous example, we now have just ~6 large allocations in total, regardless of how many packages you have. This is completely different from the pointer-based approach, which required a separate memory fetch for each object.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimized Lockfile Format&lt;/head&gt;
    &lt;p&gt;Bun also applies the Structure of Arrays approach to its &lt;code&gt;bun.lock&lt;/code&gt; lockfile.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;bun install&lt;/code&gt;, Bun has to parse the existing lockfile to determine what's already installed and what needs updating. Most package managers store lockfiles as nested JSON (npm) or YAML (pnpm, yarn). When npm parses &lt;code&gt;package-lock.json&lt;/code&gt;, it's processing deeply nested objects:&lt;/p&gt;
    &lt;code&gt;{
  "dependencies": {
    "next": {
      "version": "15.5.0",
      "requires": {
        "@swc/helpers": "0.5.15",
        "postcss": "8.4.31"
      }
    },
    "postcss": {
      "version": "8.4.31",
      "requires": {
        "nanoid": "^3.3.6",
        "picocolors": "^1.0.0"
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Each package becomes its own object with nested dependency objects. JSON parsers must allocate memory for every object, validate syntax, and build complex nested trees. For projects with thousands of dependencies, this creates the same pointer-chasing problem we saw earlier!&lt;/p&gt;
    &lt;p&gt;Bun applies the Structure of Arrays approach to its lockfile, in a human-readable format:&lt;/p&gt;
    &lt;code&gt;{
  "lockfileVersion": 0,
  "packages": {
    "next": [
      "next@npm:15.5.0",
      { "@swc/helpers": "0.5.15", "postcss": "8.4.31" },
      "hash123"
    ],
    "postcss": [
      "postcss@npm:8.4.31",
      { "nanoid": "^3.3.6", "picocolors": "^1.0.0" },
      "hash456"
    ]
  }
}
&lt;/code&gt;
    &lt;p&gt;This again deduplicates strings, and stores dependencies in a cache-friendly layout. They're stored following dependency order rather than alphabetically or in a nested hierarchy. This means that a parser can read memory more efficiently (sequentially), avoiding random jumps between objects.&lt;/p&gt;
    &lt;p&gt;And not only that, Bun also pre-allocates memory based on the lockfile size. Just like with tarball extraction, this avoids the repeated resize-and-copy cycles that create performance bottlenecks during parsing.&lt;/p&gt;
    &lt;p&gt;As a sidenote: Bun originally used a binary lockfile format (&lt;code&gt;bun.lockb&lt;/code&gt;) to avoid JSON parsing overhead entirely, but binary files are impossible to review in pull requests and can't be merged when conflicts happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;File copying&lt;/head&gt;
    &lt;p&gt;After the packages are installed and cached in &lt;code&gt;~/.bun/install/cache/&lt;/code&gt;, Bun must copy the files into &lt;code&gt;node_modules&lt;/code&gt;. This is where we see most of Bun's performance impact!&lt;/p&gt;
    &lt;p&gt;Traditional file copying traverses each directory and copies files individually. This requires multiple system calls per file:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;opening the source file (&lt;code&gt;open()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;creating and opening the destination file (&lt;code&gt;open()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;repeatedly reading chunks from the source and writing them to the destination until complete (&lt;code&gt;read()&lt;/code&gt;/&lt;code&gt;write()&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;finally, closing both files &lt;code&gt;close()&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of these steps requires that expensive mode switch between user mode and the kernel.&lt;/p&gt;
    &lt;p&gt;For a typical React app with thousands of package files, this generates hundreds of thousands to millions of system calls! This is exactly the systems programming problem we described earlier: the overhead of making all these system calls becomes more expensive than actually moving the data.&lt;/p&gt;
    &lt;p&gt;Bun uses different strategies depending on your operating system and filesystem, leveraging every OS-specific optimization available. Bun supports several file copying backends, each with different performance characteristics:&lt;/p&gt;
    &lt;head rend="h3"&gt;macOS&lt;/head&gt;
    &lt;p&gt;On macOS, Bun uses Apple's native &lt;code&gt;clonefile()&lt;/code&gt; copy-on-write system call.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;clonefile&lt;/code&gt; can clone entire directory trees in a single system call. This system call creates new directory and file metadata entries that reference the same physical disk blocks as the original files. Instead of writing new data to disk, the filesystem just creates new "pointers" to existing data.&lt;/p&gt;
    &lt;code&gt;// Traditional approach: millions of syscalls
for (each file) {
  copy_file_traditionally(src, dst);  // 50+ syscalls per file
}

// Bun's approach: ONE syscall
clonefile("/cache/react", "/node_modules/react", 0);
&lt;/code&gt;
    &lt;p&gt;SSD stores data in fixed-size blocks. When you normally copy a file (&lt;code&gt;copy()&lt;/code&gt;), the filesystem allocates new blocks and writes duplicate data. With &lt;code&gt;clonefile&lt;/code&gt;, both the original and "copied" file have metadata that points to the exact same physical blocks on your SSD.&lt;/p&gt;
    &lt;p&gt;Copy-on-write means data is only duplicated when modified. This results in an &lt;code&gt;O(1)&lt;/code&gt; operation vs. the &lt;code&gt;O(n)&lt;/code&gt; of traditional copying.&lt;/p&gt;
    &lt;p&gt;The metadata of both files point to the same data blocks until you modify one of them.&lt;/p&gt;
    &lt;p&gt;When you modify the contents of one of the files, the filesystem automatically allocates new blocks for the edited parts, and updates the file metadata to point to the new blocks.&lt;/p&gt;
    &lt;p&gt;However, this rarely happens since &lt;code&gt;node_modules&lt;/code&gt; files are typically read-only after installation; we don't actively modify modules from within our code.&lt;/p&gt;
    &lt;p&gt;This makes copy-on-write extremely efficient: multiple packages can share identical dependency files without using additional disk space.&lt;/p&gt;
    &lt;code&gt;Benchmark 1: bun install --backend=copyfile
  Time (mean ± σ):      2.955 s ±  0.101 s    [User: 0.190 s, System: 1.991 s]
  Range (min … max):    2.825 s …  3.107 s    10 runs

Benchmark 2: bun install --backend=clonefile
  Time (mean ± σ):      1.274 s ±  0.052 s    [User: 0.140 s, System: 0.257 s]
  Range (min … max):    1.184 s …  1.362 s    10 runs

Summary
  bun install --backend=clonefile ran
    2.32 ± 0.12 times faster than bun install --backend=copyfile
&lt;/code&gt;
    &lt;p&gt;When &lt;code&gt;clonefile&lt;/code&gt; fails (due to lack of filesystem support), Bun falls back to &lt;code&gt;clonefile_each_dir&lt;/code&gt; for per-directory cloning. If that also fails, Bun uses traditional &lt;code&gt;copyfile&lt;/code&gt; as the final fallback.&lt;/p&gt;
    &lt;head rend="h3"&gt;Linux&lt;/head&gt;
    &lt;p&gt;Linux doesn't have &lt;code&gt;clonefile()&lt;/code&gt;, but it has something even older and more powerful: hardlinks. Bun implements a fallback chain that tries increasingly less optimal approaches until one works:&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Hardlinks&lt;/head&gt;
    &lt;p&gt;On Linux, Bun's default strategy is hardlinks. A hardlink doesn't create a new file at all, it only creates a new name for an existing file, and references this existing file.&lt;/p&gt;
    &lt;code&gt;link("/cache/react/index.js", "/node_modules/react/index.js");
&lt;/code&gt;
    &lt;p&gt;To understand hardlinks, you need to understand inodes. Every file on Linux has an inode, which is a data structure that contains all the file's metadata (permissions, timestamps, etc.). The filename is just a pointer to an inode:&lt;/p&gt;
    &lt;p&gt;Both paths point to the same inode. If you delete one path, the other remains. However, if you modify one, both see changes (because they're the same file!).&lt;/p&gt;
    &lt;p&gt;This results in great performance gains because there's zero data movement. Creating a hard link requires a single system call that completes in microseconds, regardless of whether you're linking a 1KB file or a 100MB bundle. Much more efficient than traditional copying, which has to read and write every single byte.&lt;/p&gt;
    &lt;p&gt;They're also extremely efficient for disk space, since there's only ever one copy of the actual data on disk, no matter how many packages reference the same dependency files&lt;/p&gt;
    &lt;p&gt;However, hardlinks have limitations. They can't cross filesystem boundaries (e.g. your cache is in a different location than your &lt;code&gt;node_modules&lt;/code&gt;), some filesystems don't support them, and certain file types or permission configurations can cause hardlink creation to fail.&lt;/p&gt;
    &lt;p&gt;When hardlinks aren't possible, Bun has some fallbacks:&lt;/p&gt;
    &lt;head rend="h4"&gt;2. &lt;code&gt;ioctl_ficlone&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;It starts with &lt;code&gt;ioctl_ficlone&lt;/code&gt;, which enables copy-on-write on filesystems like Btrfs and XFS. This is very similar to &lt;code&gt;clonefile&lt;/code&gt;'s copy-on-write system in the way that it also creates a new file references that share the same disk data. Unlike hardlinks, these are separate files; they just happen to share storage until modified.&lt;/p&gt;
    &lt;head rend="h4"&gt;3. &lt;code&gt;copy_file_range&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;If copy-on-write isn't available, Bun tries to at least keep the copying in kernel space and falls back to &lt;code&gt;copy_file_range&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In a traditional copy, the kernel reads from disk into a kernel buffer, then copies that data to your program's buffer in user space. Later when you call &lt;code&gt;write()&lt;/code&gt;, it copies it back to a kernel buffer before writing to disk. That's four memory operations and multiple context switches!&lt;/p&gt;
    &lt;p&gt;With &lt;code&gt;copy_file_range&lt;/code&gt;, the kernel reads from disk into a kernel buffer and writes directly to disk. Just two operations and zero context switches for the data movement.&lt;/p&gt;
    &lt;head rend="h4"&gt;4. &lt;code&gt;sendfile&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;If that's unavailable, Bun uses &lt;code&gt;sendfile&lt;/code&gt;. This is a system call that was originally designed for network transfers, but it's also effective for copying data directly between two files on disk.&lt;/p&gt;
    &lt;p&gt;This command also keeps data in kernel space: the kernel reads data from one destination (a reference to an open file on disk, e.g. a source file in &lt;code&gt;~/.bun/install/cache/&lt;/code&gt;) and writes it to another destination (like a destination file in &lt;code&gt;node_modules&lt;/code&gt;), all within the kernel's memory space.&lt;/p&gt;
    &lt;p&gt;This process is called disk-to-disk copying, as it moves data between files stored on the same or different disks without touching your program's memory. It's an older API but more widely supported, making it a reliable fallback when newer system calls aren't available while still reducing the number of memory calls.&lt;/p&gt;
    &lt;head rend="h4"&gt;5. &lt;code&gt;copyfile&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;As a last resort, Bun uses traditional file copying; the same approach most package managers use. This creates entirely separate copies of each file by reading data from the cache and writing it to the destination using a &lt;code&gt;read()&lt;/code&gt;/&lt;code&gt;write()&lt;/code&gt; loop. This uses multiple system calls, which is exactly what Bun is trying to minimize. It's the least efficient option, but it's universally compatible.&lt;/p&gt;
    &lt;code&gt;Benchmark 1: bun install --backend=copyfile
  Time (mean ± σ):     325.0 ms ±   7.7 ms    [User: 38.4 ms, System: 295.0 ms]
  Range (min … max):   314.2 ms … 340.0 ms    10 runs

Benchmark 2: bun install --backend=hardlink
  Time (mean ± σ):     109.4 ms ±   5.1 ms    [User: 32.0 ms, System: 86.8 ms]
  Range (min … max):   102.8 ms … 119.0 ms    19 runs

Summary
  bun install --backend=hardlink ran
    2.97 ± 0.16 times faster than bun install --backend=copyfile
&lt;/code&gt;
    &lt;p&gt;These file copying optimizations address the primary bottleneck: system call overhead. Instead of using a one-size-fits-all approach, Bun chooses the most efficient file copying specifically tailored to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multi-Core Parallelism&lt;/head&gt;
    &lt;p&gt;All the above-mentioned optimizations are great, but they aim to reduce the workload for a single CPU core. However, modern laptops have 8, 16, even 24 CPU cores!&lt;/p&gt;
    &lt;p&gt;Node.js has a thread pool, but all the actual work (e.g. figuring out which version of React works with which version of webpack, building the dependency graph, deciding what to install) happens on one thread and one CPU core. When npm runs on your M3 Max, one core works really hard while the other 15 are idle.&lt;/p&gt;
    &lt;p&gt;A CPU core can independently execute instructions. Early computers had one core, they could only do one thing at a time, but modern CPUs pack multiple cores onto a single chip. A 16-core CPU can execute 16 different instruction streams simultaneously, not just switching between them really fast.&lt;/p&gt;
    &lt;p&gt;This is yet another fundamental bottleneck for traditional package managers: no matter how many cores you have, the package manager can only use one CPU core.&lt;/p&gt;
    &lt;p&gt;Bun takes a different approach with a lock-free, work-stealing thread pool architecture.&lt;/p&gt;
    &lt;p&gt;Work-stealing means that idle threads can "steal" pending tasks from busy threads' queues. When a thread finishes its work, it checks its local queue, then the global queue, then steals from other threads. No thread sits idle when there's still work to do.&lt;/p&gt;
    &lt;p&gt;Instead of being limited to JavaScript's event loop, Bun spawns native threads that can fully utilize every CPU core. The thread pool automatically scales to match your device's CPU's core count, allowing Bun to maximize parallelizing the I/O-heavy parts of the installation process. One thread can be extracting &lt;code&gt;next&lt;/code&gt;'s tarball, another is resolving &lt;code&gt;postcss&lt;/code&gt; dependencies, a third applying patches to &lt;code&gt;webpack&lt;/code&gt;, and so on.&lt;/p&gt;
    &lt;p&gt;But multi-threading often comes with synchronization overhead. Those hundreds of thousands of &lt;code&gt;futex&lt;/code&gt; calls npm made were just threads constantly waiting for each other. Each time a thread wants to add a task to a shared queue, it has to lock it first, blocking all other threads.&lt;/p&gt;
    &lt;code&gt;// Traditional approach: Locks
mutex.lock();                   // Thread 1 gets exclusive access
queue.push(task);               // Only Thread 1 can work
mutex.unlock();                 // Finally releases lock
// Problem: Threads 2-8 blocked, waiting in line
&lt;/code&gt;
    &lt;p&gt;Bun uses lock-free data structures instead. These use special CPU instructions called atomic operations that allow threads to safely modify shared data without locks:&lt;/p&gt;
    &lt;code&gt;pub fn push(self: *Queue, batch: Batch) void {
  // Atomic compare-and-swap, happens instantly
  _ = @cmpxchgStrong(usize, &amp;amp;self.state, state, new_state, .seq_cst, .seq_cst);
}
&lt;/code&gt;
    &lt;p&gt;In an earlier benchmark we saw that Bun was able to process 51,685 &lt;code&gt;package.json&lt;/code&gt; files/second versus Node.js's 15,471. That's the impact of using all cores instead of one.&lt;/p&gt;
    &lt;p&gt;Bun also runs network operations differently. Traditional package managers often block. When downloading a package, the CPU sits idle waiting for the network.&lt;/p&gt;
    &lt;p&gt;Bun maintains a pool of 64(!) concurrent HTTP connections (configurable via &lt;code&gt;BUN_CONFIG_MAX_HTTP_REQUESTS&lt;/code&gt;) on dedicated network threads. The network thread runs independently with its own event loop, handling all downloads while CPU threads handle the extraction and processing. Neither waits for the other.&lt;/p&gt;
    &lt;p&gt;Bun also gives each thread its own memory pool. An issue with "traditional" multi-threading is that all threads compete for the same memory allocator. This creates contention: if 16 threads all need memory at once, they have to wait for each other.&lt;/p&gt;
    &lt;code&gt;// Traditional: all threads share one allocator
Thread 1: "I need 1KB for package data"    // Lock allocator
Thread 2: "I need 2KB for JSON parsing"    // Wait...
Thread 3: "I need 512B for file paths"     // Wait...
Thread 4: "I need 4KB for extraction"      // Wait...
&lt;/code&gt;
    &lt;p&gt;Bun instead gives each thread its own large chunk of pre-allocated memory that the thread manages independently. There's no sharing or waiting, each thread works with its own data whenever possible.&lt;/p&gt;
    &lt;code&gt;// Bun: each thread has its own allocator
Thread 1: Allocates from pool 1    // Instant
Thread 2: Allocates from pool 2    // Instant
Thread 3: Allocates from pool 3    // Instant
Thread 4: Allocates from pool 4    // Instant
&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The package managers we benchmarked weren't built wrong, they were solutions designed for the constraints of their time.&lt;/p&gt;
    &lt;p&gt;npm gave us a foundation to build on, yarn made managing workspaces less painful, and pnpm came up with a clever way to save space and speed things up with hardlinks. Each worked hard to solve the problems developers were actually hitting at the time.&lt;/p&gt;
    &lt;p&gt;But that world no longer exists. SSDs are 70× faster, CPUs have dozens of cores, and memory is cheap. The real bottleneck shifted from hardware speed to software abstractions.&lt;/p&gt;
    &lt;p&gt;Buns approach wasn't revolutionary, it was just willing to look at what actually slows things down today. When SSDs can handle a million operations per second, why accept thread pool overhead? When you're reading the same package manifest for the hundredth time, why parse JSON again? When the filesystem supports copy-on-write, why duplicate gigabytes of data?&lt;/p&gt;
    &lt;p&gt;The tools that will define the next decade of developer productivity are being written right now, by teams who understand that performance bottlenecks shifted when storage got fast and memory got cheap. They're not just incrementally improving what exists; they're rethinking what's possible.&lt;/p&gt;
    &lt;p&gt;Installing packages 25x faster isn't "magic": it's what happens when tools are built for the hardware we actually have.&lt;/p&gt;
    &lt;p&gt;→ Experience software built for 2025 at bun.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210850</guid></item><item><title>GrapheneOS and Forensic Extraction of Data (2024)</title><link>https://discuss.grapheneos.org/d/13107-grapheneos-and-forensic-extraction-of-data</link><description>&lt;doc fingerprint="b7661d918043d3e8"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi,&lt;lb/&gt; I am writing an article and I am sharing a draft with you. I will be glad if you share your thoughts and suggestions with me.&lt;/p&gt;
      &lt;p&gt;GrapheneOS is an Android-based, open source, privacy and security-focused mobile operating system for mobile phones. It is one of the most secure and privacy protecting operating systems (and yes, it does this task comparable and in some scenarios even better than iOS, but we will come to that later).&lt;/p&gt;
      &lt;p&gt;However, in the beginning of May, someone started an attack on GrapheneOS across social media platforms. The perpetrators were misrepresenting consent-based data extraction as GrapheneOS being compromised. Which would be funny if it wasn't so stupid. So let's see what happened and what actually consent-based data extraction means.&lt;/p&gt;
      &lt;head rend="h2"&gt;Digital forensics&lt;/head&gt;
      &lt;p&gt;Digital forensics is the process of uncovering and analysing electronic data in order to gather evidence for legal proceedings.&lt;lb/&gt; It involves the use of various techniques and tools to examine digital devices such as computers, smartphones, and storage&lt;lb/&gt; media to identify, preserve, analyse, and present digital evidence.&lt;/p&gt;
      &lt;p&gt;Digital evidence refers to any data or information that is stored or transmitted in digital form and can be used as evidence in a legal investigation or trial. Digital evidence is often used in criminal investigations to help establish a suspect's guilt or innocence, and can also be used in civil litigation, regulatory investigations, and other legal proceedings.&lt;/p&gt;
      &lt;p&gt;Unfortunately, sometimes digital forensics can be abused. It can be used against investigative journalists or political activists, it can be used for privacy violation, to intimidate or harass someone, to tamper with evidence, etc. That is why GrapheneOS developers are working hard to protect mobile phones from tampering and try to make data extraction without user's consent as hard as possible.&lt;/p&gt;
      &lt;head rend="h2"&gt;Cellebrite&lt;/head&gt;
      &lt;p&gt;Cellebrite is a leading Israeli company in the digital intelligence and digital forensics field. Their main digital forensics tool is called Universal Forensic Extraction Device (UFED), and is used to extract and analyze data from mobile devices for investigations.&lt;/p&gt;
      &lt;p&gt;They are selling their equipment to governments all around the world, and their tools are mostly used for legitimate purposes. Unfortunately they are selling their tools to authoritarian regimes too. Cellebrite's customer list has included authoritarian regimes in Belarus, Russia, Venezuela, and China, death squads in Bangladesh, military juntas in Myanmar and those seeking to abuse and oppress in Turkey, UAE, and elsewhere.&lt;/p&gt;
      &lt;head rend="h2"&gt;Data extraction&lt;/head&gt;
      &lt;p&gt;As mentioned, digital forensics tools first try to extract data from mobile device. This is the initial step in a digital forensics investigation.&lt;/p&gt;
      &lt;p&gt;The problem for digital forensics is, what if mobile phone is locked? How to extract data from a locked device?&lt;/p&gt;
      &lt;p&gt;There are several options, but basically three approaches exist.&lt;/p&gt;
      &lt;p&gt;First option is so called consent-based data extraction. This simply means that user voluntary unlocks their device (or provides PIN code or password), and forensic tool then extracts data from unlocked device.&lt;/p&gt;
      &lt;p&gt;Why would someone voluntary unlock their device? Well, maybe the owner of the device wants to cooperate with the investigators. Maybe he is a victim of criminal activity and wants to provide evidence against the perpetrators. Maybe he knows that data will prove his innocence. Or something else.&lt;/p&gt;
      &lt;p&gt;The question is of course, what if user do not want to provide PIN code or password or to unlock their phone?&lt;/p&gt;
      &lt;p&gt;In that case, there are two general approaches of digital forensic examination. The first option is to try to hack a mobile device so it gets unlocked (and then extract the data). And the other is to try to guess PIN code or password in order to unlock the device and the extract the data.&lt;/p&gt;
      &lt;p&gt;Companies like Cellebrite are offering various tools, that can try to hack into the locked mobile phone and then extract the data. And their tools also offer a possibility to "guess" PIN code or password, in order to unlock the device.&lt;/p&gt;
      &lt;p&gt;But first we need to understand that from the digital forensics point of view, mobile phone can basically be in two states.&lt;/p&gt;
      &lt;p&gt;First is called BFU (Before First Unlock), and it simply means a device that has been turned off or rebooted and never subsequently unlocked by entering the correct PIN or passcode. The second is called AFU (After First Unlock), and it means that device has been unlocked after reboot (meaning that encryption keys are stored in internal memory of the device).&lt;/p&gt;
      &lt;p&gt;Technically, there is important difference between the two. BFU devices (that hasn't been unlocked with a PIN or passcode) mostly contains encrypted data. Since the first unlock of the device also decrypts the device (technically: unlocks the decryption key, so device can access the data stored in internal storage), most data on the device in that case is inaccessible for forensic analysis. Technically that means that data are encrypted at rest.&lt;/p&gt;
      &lt;p&gt;AFU devices (that has been unlocked with the correct passcode after powering on, or restarting) contains decryption key in it's internal memory, and that key is used to decrypt files in internal storage. In that case most data on the device becomes accessible for forensic analysis, because data in that state are decrypted for normal use. However, in that case screen lock could still been activated, meaning, that forensic investigator needs to unlock the screen first, and then can extract the data.&lt;/p&gt;
      &lt;p&gt;From the user's point of view this simply means that a locked device in BFU state presents significant challenges for data extraction, while an unlocked device (in AFU state) offers greater access to extract the data.&lt;/p&gt;
      &lt;head rend="h2"&gt;Data extraction approaches&lt;/head&gt;
      &lt;p&gt;As already mentioned, AFU devices are easily approached for extraction. General approach here is to hack into the locked mobile phone (by exploiting some software vulnerability) in order to disable or bypass a screen lock, and then extract the data.&lt;/p&gt;
      &lt;p&gt;In case of BFU devices, where data in internal storage are still encrypted, the forensic examinator needs to "guess" PIN code or password, in order to unlock the device. Usually this is done by so called brute forcing. This simply means that a forensic tool tries to guess the correct PIN or password by going through all possible combination, until the correct one is found.&lt;/p&gt;
      &lt;head rend="h2"&gt;Cellebrite's capabilities&lt;/head&gt;
      &lt;p&gt;In April 2024 Cellebrite published a list of their capabilities provided to customers. The list shows that they can successfully exploit every non-GrapheneOS Android device brand. They can do this for devices in AFU and BFU states. This means, that Cellebrite's tools can unlock (and then extract data) every Android device on the market.&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/757/183/372/025/original/992254912340eeaf.png&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/757/581/168/086/original/a2c40bcc6a083183.png&lt;/p&gt;
      &lt;p&gt;According to Cellebrite's documents, they have similar capabilities for iOS devices too. Not all, but for many of them. In fact, it is only the latest device generation and OS versions which are not fully supported yet (however, it is fair to mention, that most iPhone users are getting iOS updates automatically). Will they be able to develop exploits for the later iOS devices too? We do not know that, but we know, that NSO (an Israeli company developing Pegasus spyware) already did that, right after iOS 17 has been released in September 2023.&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/760/076/651/069/original/abb6bfdb2d3cbc6a.png&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/760/480/507/923/original/eaff65050cbc6d1c.png&lt;/p&gt;
      &lt;p&gt;Which is interesting, because Apple is advertising iOS as "the world’s most personal and secure mobile operating system".&lt;/p&gt;
      &lt;p&gt;What about GrapheneOS? According to the documents, Cellebrite admits they can not hack GrapheneOS if users had installed updates since late 2022. This is important, because GrapheneOS is releasing security updates and improvements quite frequently - sometimes even several times a month. And GrapheneOS is designed in such a way, that updates are automatically enabled, and if users want to disable them, they need to do that manually. This means that very likely vast majority of users have GrapheneOS updated to the last version automatically and their phones can not be hacked by Cellebrite's tools.&lt;/p&gt;
      &lt;p&gt;https://grapheneos.social/system/media_attachments/files/112/462/757/581/168/086/original/a2c40bcc6a083183.png&lt;/p&gt;
      &lt;p&gt;On the other side, Cellebrite claims that they can do so called consent-based full filesystem extraction with iOS, Android and GrapheneOS. As already mentioned, this simply means they can extract data from the device once the user voluntary unlocks their device. For GrapheneOS that means, that when they get unlocked phone, they just enable developer options and use standard ADB tool (ADB lets you communicate with Android devices through a computer) to extract the data.&lt;/p&gt;
      &lt;p&gt;So, according to Cellebrite documents, they can not unlock fully patched GrapheneOS phone, unless user voluntary unlocks the phone. In fact, analysis of Cellebrite's documents shows, that they even can not brute force a random 6-digit PIN on Pixel 6 and later phones (which are the phones supported by GrapheneOS). Cellebrite's documents reveal, that Pixel 6 phones and later with GrapheneOS (and the latest iPhones also), are the only devices where a random 6 digit PIN can not be brute forced.&lt;/p&gt;
      &lt;head rend="h2"&gt;The attack on GrapheneOS on social media?&lt;/head&gt;
      &lt;p&gt;As we mentioned at the beginning, in the beginning of May, someone started an attack on GrapheneOS across social media platforms. The perpetrators claimed that GrapheneOS has been compromised, and the "proof" has been, that data extraction from GrapheneOS is successful when it is consent-based.&lt;/p&gt;
      &lt;p&gt;It is unclear who has been behind this social media attack, but in December 2020 something similar happened. At that time, various media (including BBC) reported, that Cellebrite claimed to have cracked Signal's encryption. Signal is a free, encrypted messaging application, which is widely considered one of the most secure messaging apps due to its strong encryption and focus on privacy.&lt;/p&gt;
      &lt;p&gt;However, at that time it turned out, that the claims were completely false - Cellebrite has been able to extract Signal messages only if user unlocked the phone and Signal app and hand it to the forensic examinator. Which is by definition consent-based extraction and does not really require some specific technical excellence from the forensic acquisition tool.&lt;/p&gt;
      &lt;head rend="h2"&gt;Defense against forensic hacking tools in GrapheneOS&lt;/head&gt;
      &lt;p&gt;Now let's dive into GrapheneOS security countermeasures against described attacks.&lt;/p&gt;
      &lt;head rend="h3"&gt;Protection against hacking into the phone&lt;/head&gt;
      &lt;p&gt;GrapheneOS has implemented many security features. You can get a glimpse of them from my presentation on GrapheneOS security.&lt;/p&gt;
      &lt;p&gt;However, regarding various forensic tools, it is important to mention, that GrapheneOS has a special feature that disallows new USB connections in AFU mode (After First Unlock) after the device is locked, and fully disables USB data at a hardware level once there aren't active USB connections. This means that if an attacker would connect GrapheneOS device to the computer through USB, GrapheneOS device will not allow any connection at all.&lt;/p&gt;
      &lt;p&gt;Users can set it to do this in BFU (Before First Unlock) mode or even when the phone is fully unlocked. And users with a high threat model can even fully disable USB, including USB-PD/charging, while the OS is booted. So they can decide to only allow charging while powered off or booted into the fastboot/fastbootd/recovery/charging modes. This is the feature no other phone has, and can be completely customized by the user.&lt;/p&gt;
      &lt;p&gt;GrapheneOS is constantly improving security, and since beginning of 2024 they massively improved security against various possible exploits. In April 2024 they also helped to implement several firmware improvements for Pixel phones.&lt;/p&gt;
      &lt;head rend="h3"&gt;Protection against brute force attacks&lt;/head&gt;
      &lt;p&gt;As we mentioned, in case of BFU devices, where data in internal storage are still encrypted, the forensic examinator needs to "guess" PIN code or password, in order to unlock the device. This is done by so called brute force attack (guessing every possible combination of PIN code or password).&lt;/p&gt;
      &lt;p&gt;However, Pixel 6 and later phones contain a dedicated security chip Titan M2, also called hardware security module, which is storing the decryption keys. This chip will unlock the internal phone storage only if user will enter the correct PIN or password.&lt;/p&gt;
      &lt;p&gt;But here is the catch. If an attacker try to perform brute force attack (i. e. try to go through all possible PIN/password combinations), the chip will limit the number of attempts. After 5 failed attempts, chip will add 30 second delay before next guessing attempt is allowed. Then delay gets increased (after 30 failed attempts the delay doubles after every 10 attempts), and after 140 failed attempts, GrapheneOS and its secure element will limit brute force to 1 attempt per day. This is called secure element throttling.&lt;/p&gt;
      &lt;p&gt;So if an attacker would like to test all different combinations to guess a 6-digit PIN, there are one million possible combinations, so brute forcing would take a long, long time. Unless, the attacker is extremely lucky and guesses the correct PIN at the few first attempts.&lt;/p&gt;
      &lt;p&gt;Of course, the question is, is it possible to somehow hack this secure element or unlock the limited number of attempts? The answer is - very unlikely. Why? Because this secure element has been developed specifically to protect against those types of attacks. And it has passed the highest hardware vulnerability assessment (AVA_VAN.5) by an independent and accredited evaluation lab.&lt;/p&gt;
      &lt;p&gt;In fact, GrapheneOS is so successful in this area, because it is doing far more hardening than iOS against these attacks. iPhones also have security element, but the companies developing attacks, had successfully bypassed secure element throttling from Apple for years (and are doing the same with Samsung and Qualcomm implementation of secure element). These companies were successfully bypassing the secure element throttling on 2nd through 5th generation Pixels. Pixel 2 used NXP secure element and Pixel 3 moved to a custom ARM secure element. But Pixel 6 and later phones are using a custom RISC-V secure element. It seems that moving away from the standard ARM Cortex secure element cores was the correct decision, because it blocked these companies from successfully exploiting the secure element for several years.&lt;/p&gt;
      &lt;head rend="h3"&gt;Auto reboot feature&lt;/head&gt;
      &lt;p&gt;GrapheneOS also has an auto-reboot feature, meaning that after some time, phone gets automatically rebooted. Default auto reboot time is 18 hours (if phone is not unlocked in that time, it will reboot), but user can set it to a different interval, even as low as 10 minutes.&lt;/p&gt;
      &lt;p&gt;This technically means that the data after this period are put back to rest, or, to put it differently, phone gets from AFU to BFU state. And as we already explained, a locked device in BFU state presents significant challenges for data extraction, much more than unlocked device in AFU state.&lt;/p&gt;
      &lt;p&gt;After reboot, it is almost impossible to extract decrypted data from the phone. So the focus of GrapheneOS's team is defending against exploitation long enough for auto-reboot to work, and after that your data are even safer than before. That means that if an attacker develops a successful exploit, their window of opportunity to use it to get data from user profiles is until next reboot from when the phone was locked.&lt;/p&gt;
      &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
      &lt;p&gt;In the last year, GrapheneOS started to put much more effort into defending your phone against these attacks. Users who need their data secure, should definitely use a strong passphrase. To make that more convenient, GrapheneOS is developing 2-factor fingerprint unlock feature, which will allow people to combine a strong passphrase with convenient fingerprint and PIN unlock. They are also planning to offer an UI for generating random passphrases automatically. This will vastly improve security and make access to the user's data on a phone much more difficult.&lt;/p&gt;
      &lt;p&gt;The actors that want to hack into GrapheneOS are rightfully desperate. So it is no surprise, that they try to play dirty, by spreading misinformation. But knowledge is power and misinformation could be successfully fought with facts.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45210910</guid></item><item><title>An Engineering History of the Manhattan Project</title><link>https://www.construction-physics.com/p/an-engineering-history-of-the-manhattan</link><description>&lt;doc fingerprint="fde891156d158f6d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An Engineering History of the Manhattan Project&lt;/head&gt;
    &lt;p&gt;The Manhattan Project, the US program to build an atomic bomb during WWII, is one of the most famous and widely known major government projects: a survey in 1999 ranked the dropping of the atomic bomb as the top news story of the 20th century. Virtually everyone knows that the project built the bombs that were dropped on Hiroshima and Nagasaki. And most of us probably know that the bomb was built by some of the world’s best physicists, working under Robert Oppenheimer at Los Alamos in New Mexico. But the Manhattan Project was far more than just a science project: building the bombs required an enormous industrial effort of unprecedented scale and complexity. Enormous factory complexes were built using hundreds of millions of dollars worth of never-before-constructed equipment. Scores of new machines, analytical techniques, and methods of working with completely novel substances had to be invented. Materials which had never been produced at all, or only produced in tiny amounts, suddenly had to be manufactured in vast quantities.&lt;/p&gt;
    &lt;p&gt;This massive effort was required in part because of the enormous difficulty in producing fissile material, and in part because of the enormous uncertainty facing the project: it wasn’t known what the best method for manufacturing the fissile material needed for the bomb would be, what the design of the bomb should be, or whether a workable bomb could even be built. Developing the bomb required resolving this uncertainty, and the project needed to rapidly push forward knowledge and capabilities in many fields: not merely in the realm of nuclear chain reactions and atomic physics, but also in areas like precision explosives, metallurgy, welding, chemical separation, and electronics.&lt;/p&gt;
    &lt;p&gt;Because of the exigencies of war, this work needed to be done extremely rapidly. There wasn’t time to investigate promising approaches sequentially, or wait for more information before picking a particular course. Thus, multiple possible routes to the bomb — different fuels (and different fuel production techniques), different bomb designs, different components like triggers and tampers — were pursued simultaneously. Major commitments, like factories that cost hundreds of millions of dollars, were made before it was known whether they would even be useful. Design work began on the bombs when the nuclear fuel they would use hadn’t been produced in more than microscopic amounts.&lt;/p&gt;
    &lt;p&gt;Normally when trying to create a new technology, funding constraints and the need for economic returns determine how much time and effort can be spent on development. Efforts to create some new technology will often be small-scale until the surrounding conditions are right — until knowledge has caught up, or the necessary supporting infrastructure exists, or the input materials are cheap enough — and risk can be minimized. But with the Manhattan Project, these constraints didn’t exist. Funding was virtually unlimited in service of ending the war sooner, and the biggest perceived risk was that Germany would beat the US to the bomb. As a result, an extremely robust development effort could be justified, which thoroughly explored virtually every promising path to an atomic weapon (no matter how expensive or uncertain).&lt;/p&gt;
    &lt;head rend="h4"&gt;Beginnings of the project&lt;/head&gt;
    &lt;p&gt;The Manhattan Project began in June of 1942, when Colonel James Marshall of the Army Corps of Engineers was directed to create a new engineering district to lead the army’s efforts to develop an atomic weapon. Shortly after, Colonel Leslie Groves (who would soon be promoted to brigadier general) was selected to lead the project. At the time, the official name of the project was “Laboratory for the Development of Substitute Materials” (DSM for short), but Groves felt that this name would attract curiosity, and so a new name was selected based on the location of Marshall’s New York office: the Manhattan Engineer District.&lt;/p&gt;
    &lt;p&gt;By the time the Manhattan Project officially formed, the US was already at work developing an atomic bomb. Following the discovery of fission in 1938 by Otto Hahn and Fritz Strassmann, physicists began to speculate that a nuclear chain reaction might be possible, and that such a reaction could be used to build a bomb of unprecedented magnitude. In August the following year, Albert Einstein and physicist Leo Szilard sent a letter to president Roosevelt, warning him that a nuclear chain reaction might be used to build an extremely powerful bomb, and that the US should research atomic energy. Two months later, Roosevelt ordered the creation of an advisory committee on uranium, and by early 1940 US researchers (most notably Enrico Fermi) were working to create a sustained nuclear chain reaction.&lt;/p&gt;
    &lt;p&gt;In July of 1941, a report from the British MAUD Committee concluded that it was likely feasible to build an atomic bomb. It reached the US, and in October Roosevelt authorized expediting atomic bomb work. Bomb efforts accelerated following Japan’s attack on Pearl Harbor in December of 1941, and in February of 1942 the Metallurgical Laboratory was formed at the University of Chicago to study nuclear chain reactions and the chemistry of newly-created element plutonium. There, a team working under Enrico Fermi continued their work to create nuclear chain reactions, ultimately resulting in Chicago Pile-1, the world’s first self-sustaining nuclear reaction, in December of that year.&lt;/p&gt;
    &lt;head rend="h4"&gt;The path to the bomb&lt;/head&gt;
    &lt;p&gt;When the Manhattan Engineering District was formed in 1942, there was still a great deal of uncertainty surrounding the construction of an atomic bomb. Based on what was known at the time, it was believed that a bomb was probably feasible, and that due to the risks of Germany developing one it should be pursued. But it was far from clear what the surest path to success was.&lt;/p&gt;
    &lt;p&gt;The first major challenge came in producing sufficient fissile material to build a bomb. Fissile material splits and releases neutrons when struck by slow, “thermal” neutrons, making a nuclear chain reaction possible. At the time there were two major candidate materials: a rare isotope of uranium known as uranium-235 (U235), and plutonium, an element first synthesized by Glenn Seaborg in late 1940.1&lt;/p&gt;
    &lt;p&gt;Using either would be very challenging. U235 makes up less than 1% of naturally-occurring uranium, and using it as a bomb material required separating it from the far more common U238. But the two isotopes were only distinguished by a tiny difference in their weights (U235 weighs about 1.3% less than U238). Some sort of filtering mechanism was needed that could act on this difference and create concentrations of U235 high enough for a bomb (a process known as enrichment).&lt;/p&gt;
    &lt;p&gt;There were several potential methods considered for separating U235:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the electromagnetic method, a beam of charged uranium tetrachloride particles would be fired through a magnetic field, which would alter their paths. Because of the difference in weight, the paths of U235 and U238 would be slightly different, and the U235 could be gathered at an appropriately placed collector.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the gaseous diffusion method, gaseous uranium hexafluoride would diffuse through a barrier with microscopic pores. The lighter U235 would diffuse more readily through the barrier due to Graham’s Law.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the liquid thermal diffusion method, a thermal gradient created in a uranium solution would cause a slight migration of heavier U238 to the cold side and the lighter U235 to the warm side.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the centrifuge method, uranium spun in a high-speed centrifuge would cause the heavier U238 to concentrate on the outer edge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of these methods, the electromagnetic method was the most developed, thanks to the efforts of Ernest Lawrence at the University of California, but it wasn’t clear if any of them (either alone or in combination) could actually produce U235 at the scale and speed needed. And no matter the method selected, an enormous industrial facility would be required: in 1939 Danish physicist Niels Bohr insisted that an atomic bomb could never be built “unless you turned the US into one huge factory.”&lt;/p&gt;
    &lt;p&gt;The main alternative to U235 was plutonium. As with U235, the main challenge would be collecting enough of it to build a bomb. Plutonium only occurs in trace amounts in nature (1 atom per 100 billion in uranium ore): collecting enough to build a bomb requires synthesizing it. Plutonium could be produced in a nuclear reactor (then called a “pile,” as it was essentially chunks of uranium piled high enough to create a self-sustaining nuclear reaction), but only in microscopic amounts. Producing a pound of plutonium required around 4000 pounds of uranium fuel, and producing enough for a bomb would require an enormous industrial facility, as with U235. Exacerbating this difficulty was the fact that while U235 and U238 could be handled comparatively easily, plutonium and other nuclear reactor byproducts were highly radioactive, requiring special handling.&lt;/p&gt;
    &lt;p&gt;Once enough fissile material had been collected, it then needed to be turned into a bomb. When enough fissile material is brought together in a small enough volume (the so-called critical mass), it can start a nuclear chain reaction, releasing enormous amounts of energy as more and more fissions were triggered. Because a chain reaction in critical mass could be started by spontaneous fission (fissile elements randomly splitting and releasing neutrons) or by cosmic rays, a bomb would have to start with a sub-critical mass of fissile material, turning it into a critical mass at detonation.&lt;/p&gt;
    &lt;p&gt;The most straightforward way to do this, it was thought, was to use a gun that would fire a sub-critical “bullet” into another sub-critical “target,” the combination of which would exceed the critical mass. But there were other mechanisms considered, including using an explosion to compress a sphere of fissile material (the so-called implosion method), as well as “autocatalytic” mechanisms in which “the chain reaction itself, as it proceeded, increased the neutron number for a time.”&lt;/p&gt;
    &lt;p&gt;In mid-1942, a gun-based plutonium bomb was generally considered most promising, but due to the lack of information and the great urgency, many promising paths were investigated simultaneously. Early on in the project, when resolving a debate about pile cooling systems, Leslie Groves stated that “The War Department considers this project important. There is no objection to a wrong decision with quick results. If there is a choice between two methods, one of which is good and the other looks promising, then build both.”&lt;/p&gt;
    &lt;p&gt;Perhaps no phrase better summarizes the philosophy of Manhattan Project than “build both.” It was ultimately decided to pursue both U235 and plutonium-based bombs. To produce the necessary U235, a production facility would be built near Knoxville, Tennessee, employing both electromagnetic separation and gaseous diffusion (and, eventually, liquid thermal diffusion). This plant, initially referred to as the Clinton Engineer Works, would later be named Oak Ridge. To produce the plutonium, another facility, the Hanford Engineer Works, would be built in southeast Washington. And while these plants were being built and producing fissile material, the design of the bombs themselves would be done at Los Alamos, New Mexico.&lt;/p&gt;
    &lt;head rend="h4"&gt;Oak Ridge and Uranium 235&lt;/head&gt;
    &lt;p&gt;The acquisition of the site for Oak Ridge was authorized in September of 1942, and construction of the electromagnetic separation plant began a few months later in February of 1943 by the firm Stone and Webster. To produce U235, the plant would use modified versions of Ernest Lawrence’s Nobel prize-winning cyclotron particle accelerators. Lawrence had been working on the devices, which he referred to as “calutrons” (after the University of California) since the spring of 1942, and while he confident that calutrons could be used for large-scale production of U235, “he stood almost alone in his optimism”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The method called for a large number of extremely complicated, and as yet undesigned and undeveloped devices involving high vacuums, high voltages and intense magnetic fields. As a large-scale method of separating Uranium-235, it seemed almost impossible. Dr. George T. Felbeck, who was in charge of the gaseous diffusion process for Union Carbide, once said it was like trying to find needles in a haystack while wearing boxing gloves. - Now It Can Be Told&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(According to Richard Rhodes, the total volume of high-vacuum required by the calutrons would eventually exceed the amount of vacuum produced everywhere else on earth at the time.)&lt;/p&gt;
    &lt;p&gt;The calutrons, arranged around a series of several “racetracks” with several dozen collection tanks attached, were divided into two stages. Partly enriched material from “alpha” racetracks would be fed into “beta” racetracks to be further enriched, eventually (it was hoped) producing 90% enriched U235. Each calutron only produced a tiny amount of U235 — Lawrence estimated that 2000 calutrons could produce 100 grams of enriched U235 per day — so a huge number of them were needed: the alpha and beta calutron buildings eventually occupied an area greater than 20 football fields, and the entire electromagnetic separation facility grew to 268 buildings, requiring 20,000 workers to build:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…The calutron structures of steel and brick and tile, chemistry laboratories, a distilled water plant, sewage treatment plants, pump houses, a shop, a service station, warehouses, cafeterias, gatehouses, change houses and locker rooms, a paymaster’s office, a foundry, a generator building, eight electric substations, nineteen water-cooling towers - for an output measured in the best of times in grams per day. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Building this enormous facility was rife with challenges. Copper, traditionally used for winding electromagnets, was in short supply due to the war, and so was substituted with silver (also a good conductor) borrowed from the US Treasury. Altogether 13,540 tons, worth $300 million ($6 billion in 2025 dollars) was borrowed, 99.964% of which was eventually returned. Because construction started so early, constant changes to already manufactured and installed equipment were required, a process that continued “long after the first major units of the plant began production operations.” Little of what was required to build the plant was off the shelf or standard:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Discouragingly few items were commercially available. Tanks, magnets, vacuum pumps, cubicles, and most of the chemical equipment, for example, were either completely new in design or so much larger or so much greater in capacity that nothing of the kind previously had been manufactured. Many less obvious items also carried performance specifications that far exceeded anything ever attempted on a commercial scale. For instance, the calutrons required electrical cable that could carry a high-voltage load continuously. The only commercial product that came near meeting this specification was the heaviest X-ray cable, and it was designed to operate intermittently. Even when the commercial equipment could be used, suppliers often had to add to their productive capacity or build entire new plants to furnish the items required in the enormous quantities they were needed. Thus, in the first equipping of the racetracks some eighty-five thousand vacuum tubes were required. In the case of one type of tube, procurement officials ordered in advance the entire national output for 1943 as well as that from a plant still under construction. In the early months of plant operation, when tubes burned out faster than predicted, some feared the racetracks might prove inoperable simply through inability to maintain the tube supply.&lt;/p&gt;
      &lt;p&gt;New methods had to be developed for machining and shaping the graphite in those parts of the calutron subject to intense heat. No standard material would endure the high potentials, mechanical strain, and temperature changes to which bushings in the high-voltage elements in the sources were continuously subjected. After months of investigation, Stone and Webster found an insulator made of zirconium oxide, a new and still very expensive substance. Similarly, use of large quantities of liquid nitrogen to condense moisture created a demand for a substance hitherto not produced on a commercial scale anywhere in the country. - Manhattan: The Army and the Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These difficulties didn’t stop after construction was completed. When the first calutrons were turned on for testing in November of 1943, the extremely powerful magnets caused the equipment to “walk” by several inches: this was eventually resolved by tying them down with heavy steel straps. Testing also showed intermittent electrical shorts and unexpectedly high variation in the strength of the magnetic fields, problems that was eventually traced to dirt and rust within the electromagnets bridging the gap between the closely spaced silver windings. To fix this required rebuilding and redesigning the magnets, delaying production by a month.&lt;/p&gt;
    &lt;p&gt;As production came online in early 1944, the electromagnetic separation plant continued to deal with numerous problems: equipment and mechanical failures, electrical short circuits, vacuum leaks and various breakdowns. The operation “skirted the edge of chaos for months.” As late as August of 1944, the electromagnetic plant had only produced a small fraction of the expected U235, and it was unclear if enough would be produced to build a wartime bomb. But eventually these problems were ironed out, and by September of 1945 the alpha and beta calutrons had produced 88 kilograms of 84.5% enriched U235.&lt;/p&gt;
    &lt;p&gt;In addition to the electromagnetic separation plant, Oak Ridge was also the site of the gaseous diffusion plant, which began construction in May 1943. As with the electromagnetic process, the gaseous diffusion process only produced U235 in tiny amounts, and an enormous facility was needed to manufacture it in sufficient quantities to build a bomb. Gaseous diffusion worked on the principle that lighter U235 would be more likely to diffuse through a porous barrier than heavier U238, but the difference in diffusion rates was miniscule. A single gaseous diffusion step would only increase the fraction of U235 by a factor of 1.0043: to produce 90% enriched U235, the initial design of the plant called for a series of 4,600 gaseous diffusion stages. Upon completion, the gaseous diffusion plant was one of the largest buildings in the world.&lt;/p&gt;
    &lt;p&gt;As with the electromagnetic plant, construction began before the design of the diffusion process had been finished, and it was unclear if the plant would work at scale. The greatest challenge was finding an appropriate barrier for the gas to diffuse through. The barrier needed to have numerous microscopic pores, be robust enough to withstand exposure to extremely corrosive uranium hexafluoride gas, and be mass-producible. Researchers had experimented with “a great many materials” between 1941 and 1942, but none were suitable. The only common material sufficiently corrosion-resistant was nickel, but no form of nickel seemed to do the trick. An electro-deposited nickel mesh, invented by Edward Norris (a “self-educated Anglo-American interior decorator”) and Edward Adler appeared most promising, but it was so brittle that manufacturing it was incredibly difficult. A modified version of the Norris-Adler barrier, produced by a team from Kellex, Bell Labs, and Bakelite, appeared to work even better, though it too had problems. Work proceeded to further develop both barriers simultaneously, but progress was slow and by the end of 1943 “morale had plummeted”. It wasn’t until early 1944 that satisfactory barriers were being produced in sufficient quantities.&lt;/p&gt;
    &lt;p&gt;While the diffusion barrier was the biggest challenge with the gaseous diffusion process, it wasn’t the only one. The plant required a level of vacuum-tightness that had previously only been achieved in labs, demanding the development of novel methods of pipe welding and leak detection. Upon completion, it took 406 workers eight months to test the plant for leaks. More than 130,000 measuring instruments, many of them novel, were installed in the plant. It was likely the greatest number of instruments ever used in any plant in the world till that date, and they required months of testing and calibration. The plant had to be incredibly reliable, as “even slight variations in such factors as temperature and pressure could produce adverse effects.” Initially, it was believed that a slight power interruption could bring the diffusion plant offline for months, so a dedicated power plant was built specifically at Oak Ridge for the process. And because any contaminants could prove disastrous, the cleanliness standards for the plant approached surgical:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…Construction workers had to cleanse all pipes, valves, pumps, converters, and other items of equipment thoroughly before installation. Workmen in a special unit performed this vast operation in the large conditioning building, using equipment for solvent degreasing, alkaline cleaning, acid pickling, scratch brushing, surface passivation, and a variety of other procedures. When they finished, they sealed all openings to interior surfaces and kept them sealed until installation teams put the equipment into place.&lt;/p&gt;
      &lt;p&gt;To make certain no dust or other foreign matter polluted the system during installation, J. A. Jones instituted a rigid schedule of surgical cleanliness in installation areas. Isolating these areas with temporary partitions, the workers installed pressure ventilation, using filtered air. Then they cleaned the areas thoroughly, and inspectors carefully checked all personnel and material that entered them. Maintenance crews with mops and vacuum cleaners continued to remove any foreign substances that seeped in. When trucks had to enter, workers hosed them down at the entrances.&lt;/p&gt;
      &lt;p&gt;Workers wore special clothes and lintless gloves. Because certain work on equipment to be used in plant installations could not be done in the dirt-free areas, such as welding pipes and other small jobs, J.A. Jones installed special inflatable canvas balloons and the work was done inside them. The cleanliness control measures required many additional guards, inspectors, and supervisors… - Manhattan: The Army and the Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This level of cleanliness extended to the design of the equipment itself:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[Uranium hexafluoride] attacked organic materials ferociously: not a speck of grease could be allowed to ooze into the gas stream anywhere along the miles and miles of pipes and pumps and barriers. Pump seals therefore had to be devised that were both gastight and greaseless, a puzzle no one had ever solved before that required the development of new kinds of plastics. (The seal material that eventually served at Oak Ridge came into its own after the war under the brand name Teflon.) A single pinhole leak anywhere in the miles of pipes would confound the entire system; Alfred O. Nier developed portable mass spectrometers to serve as subtle leak detectors. Since pipes of solid nickel would exhaust the entire U.S. production of that valuable resource, Groves found a company willing to nickel-plate all the pipe interiors, a difficult new process accomplished by filling the pipes themselves with plating solution and rotating them as the plating current did its work. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Because of these difficulties, the gaseous diffusion plant didn’t begin operating until February of 1945. While the plant was originally planned to produce uranium to the roughly 90% U235 enrichment needed to build a bomb, it was discovered that beyond 36.6% enrichment, different types of barrier and different types of pumps being designed would be required. In the latter half of 1943 the plant was thus redesigned to produce 36.6% enriched uranium (using 2892 diffusion stages) that would then be fed into the electromagnetic process. By the end of the war, the gaseous diffusion plant had “contributed substantially to the manufacture of the fissionable material used in the fabrication of atomic weapons”, and would become the primary method of producing enriched uranium in the early post-war years.&lt;/p&gt;
    &lt;p&gt;In addition to the electromagnetic and gaseous diffusion separation processes, a plant to separate U235 by liquid thermal diffusion was also built. Thermal diffusion had been considered by the Manhattan Project early on, but it appeared insufficiently promising and there were no initial plans to build a thermal diffusion plant. However, work on the process continued by the Navy as a method of producing fissile material for nuclear reactors. By late 1942, it appeared much more promising as a feasible separation method, and Leslie Groves recommended that it continue to be developed by the Navy. Eventually, in June of 1944 it was decided to build a thermal diffusion plant at Oak Ridge to produce partially-enriched uranium as an input to the electromagnetic separation process, as doing so would speed up overall U235 production. The plant came online in late 1944.&lt;/p&gt;
    &lt;p&gt;Like the other separation plants, there were struggles getting the plant built and operational. Early on there were numerous steam leaks and other equipment failures, and the “results scarcely seemed to justify the risks.” But eventually the plant “served its wartime purpose,” providing enough slightly enriched U235 to the electromagnetic separation plant and (later) the gaseous diffusion plant to build a uranium bomb by July 1945. After the war, however, it was found that the thermal diffusion plant was less economical than gaseous diffusion in producing enriched uranium, and it was shut down in September 1945, less than a year after starting operation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Hanford and plutonium&lt;/head&gt;
    &lt;p&gt;As with U235, when plutonium was being considered as a possible fissile material for an atomic bomb there was a great deal of uncertainty around its large-scale production. When plans began to be formulated for plutonium manufacture in late 1942, the element had only been produced in microscopic amounts in cyclotrons — as late as December 1943, only two milligrams of plutonium had been manufactured. Producing the pounds of plutonium needed for a bomb would require a self-sustaining nuclear chain reaction in a nuclear reactor, which would create plutonium as a fission byproduct. The first such chain reaction was created in Chicago Pile-1 in December 1942, shortly after Du Pont had been (reluctantly) brought on as the contractor to build and operate a plutonium production plant.&lt;/p&gt;
    &lt;p&gt;As with uranium separation, there were a variety of potential ways to build a plutonium-producing nuclear reactor. Every design considered used uranium as a fuel, but there were a variety of options for cooling (water, helium, diphenyl, bismuth) and for moderators to slow down the neutrons (heavy water, graphite).2 The initial reactor design used helium-cooling with a graphite moderator: helium wouldn’t absorb neutrons, and was an inert gas that wouldn’t corrode any of the reactor materials. But it was later shown that the neutron multiplication factor in a reactor, k, was high enough that coolants which absorbed more neutrons (such as water) could be made to work. Because a water-cooled reactor appeared far simpler to build, the design was changed to use water cooling. Du Pont, worried that pursuing a single reactor design was too risky, also continued to develop other designs, chiefly one moderated and cooled by heavy water, and several heavy water plants were built around the country for these purposes.&lt;/p&gt;
    &lt;p&gt;Plutonium production began with a smaller-scale, air-cooled test reactor, named X-10, built at Oak Ridge between February 1943 and January 1944. Full-scale production would take place at a remote facility built in southeastern Washington known as the Hanford Engineer Works. The first plutonium production reactor, B Reactor, began construction at Hanford in August of 1943, and first achieved criticality in September of the following year.&lt;/p&gt;
    &lt;p&gt;Like with uranium, plutonium production demanded a massive industrial facility, and Hanford was the biggest facility that Du Pont had ever constructed. An oral history of the Hanford site notes that “the building effort at Hanford from 1943 to 1945 can only be measured in superlatives”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the following: Project building crews used 1800 vehicles, including sedans, pick-up trucks, jeeps, and ambulances; 900 buses that had a total seating capacity of over 30,000; 1900 dump trucks and flat bed trucks; 240 tractor trailers; 44 railway locomotives and 460 railway cars; 5 locomotive cranes and 4 stiff leg derricks. The various construction teams built 386 miles of highways, 158 miles of track, poured 780,000 cubic yards of concrete, and erected housing for 5,000 women and 24,000 men. Excavation crews moved 25 million cubic yards of earth in the process. The overall cost was $350 million. - Working on the Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The three production reactors built at Hanford (B, D, and F Reactors) were far more powerful than anything that had come before. Fermi’s Chicago Pile-1 produced a maximum of 0.2 kilowatts of power. The X-10 reactor at Oak Ridge produced just 500 kilowatts when it was first turned on (though output would later be raised to 4,000 kilowatts). The Hanford reactors were designed to produce 250,000 kilowatts.&lt;/p&gt;
    &lt;p&gt;But the largest structures at Hanford were the separation facilities to extract plutonium out of the soup of radioactive waste products the reactors generated: these were three 800 foot long, 8-story tall buildings that resembled “an ancient mausoleum”; they were so large they were referred to as “Queen Marys.” Within these enormous structures, plutonium was extracted from radioactive “slugs” of spent uranium fuel from the reactors:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Irradiated slugs ejected from a production pile would be stored in pools of water 16.5 feet deep to remain until the most intense and therefore short-lived of their fission-product radioactivities decayed away, the water glowing blue around them with Cerenkov radiation, a sort of charged-particle sonic boom. The slugs would then move in shielded casks on special railroad cars to one of the Queen Marys, where they would first be dissolved in hot nitric acid. A standard equipment group occupied two cells: a centrifuge, a catch tank, a precipitator and a solution tank, all made of specially fabricated corrosion-resistant stainless steel. The liquid solution that the slugs had become would move through these units by steam-jet syphoning, a low-maintenance substitute for pumps. There were three necessary steps to the separation process: solution, precipitation and centrifugal removal of the precipitate. These would repeat from equipment group to equipment group down the canyon of the separation building. The end products would be radioactive wastes, stored on site in underground tanks, and small quantities of highly purified plutonium nitrate. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As at Oak Ridge, there were numerous challenges in building a mammoth industrial facility employing completely novel production processes. The reactors “presented construction problems never encountered before, even by Du Pont's highly competent field forces.” Graphite bars of exceptional purity had to be fabricated and then machined to remove any sort of surface imperfections. Supplying cooling water for the reactors required “installation of a complex system of river pumps; purification, aeration, and distillation units; and retention basins for holding radioactive water until natural decay permitted its return to the Columbia.” New machines for fabricating the uranium fuel rods had to be designed and built, and a method for shielding the unprecedentedly large nuclear reactors had to be developed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Ten months of work went into this before we could even begin to build it, with three more months before the first unit was completed…In the course of this, a special high-density pressed-wood sheet was developed in collaboration with an outside supplier. Then special sharp tools and operating techniques were required to cut various shapes from the standard manufactured widths…At the same time very detailed specifications for assembly, prescribing the closest of tolerances, were written. Some sixty manufacturers were invited to bid and refused, presumably because of the complexity of construction and the close tolerances required…but after methods were developed and prototypes were fabricated at du Pont’s shops in Wilmington eventually satisfactory suppliers were found. - Now It Can Be Told&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Even seemingly simple items often proved enormously complex. Finding a way to prevent the uranium fuel (which was packaged into aluminum-clad slugs) from corroding took an enormous amount of effort:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Two years of trial-and-error effort had not produced canning technology adequate to seal the uranium slugs, which quickly oxidized upon exposure to air or water, away from corrosion. Only in August had the crucial step been devised, by a young research chemist who had followed the problem from Du Pont in Wilmington to Chicago and then to Hanford: putting aside elaborate dips and baths he tried soaking the bare slugs in molten solder, lowering the aluminum cans into the solder with tongs and canning the slugs submerged. The melting point of the aluminum was not much higher than the melting point of the solder, but with careful temperature control the canning technique worked. - The Making of the Atomic Bomb&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Similarly, Leslie Groves notes that “seven months of persistent effort” were required to produce simple aluminum tubes that met the required specifications.&lt;/p&gt;
    &lt;p&gt;After enough plutonium had collected in the canned uranium slugs, it needed to be separated. The chemical process for this was comparatively straightforward to develop compared to the novel methods of mass-based separation developed for U235 (after experimenting with several possibilities, a method based on using bismuth phosphate was employed), but actually implementing it was a challenge. The materials to be processed were radioactive enough that the entire facility needed to be operable and maintainable remotely:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…Periscopes and other special mechanisms were incorporated into the plant design; all operations could thus be carried out in complete safety from behind the heavy concrete walls. The need for shielding and the possibility of having to replace parts by indirect means required unusually close tolerances, both in fabrication and in installation. This was true even for such items as the special railroad cars that moved the irradiated uranium between the piles and the separation plants. The tracks over which these cars moved were built with extreme care so as to minimize the chances of an accident. Under no circumstances could we plan on human beings directly repairing highly radioactive equipment. - Now It Can Be Told&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Among the technologies developed to make remote operation possible were pipe flanges that could be connected by a remotely operated wrench, and the world’s first use of closed circuit TV.&lt;/p&gt;
    &lt;p&gt;The biggest crisis at Hanford came shortly after the first production reactor came online: a few hours after beginning operation the reaction began to slow, and within a few days it had shut itself down completely. Investigation revealed that this was being caused by a fission byproduct, an isotope of xenon known as Xenon 135 that had a massively greater probability of absorbing neutrons (known as a “neutron cross section”) than any previously discovered material. (Its cross section was 70 times larger than the previous largest measured cross section). This “xenon poisoning” hadn’t been noticed in earlier reactors because they hadn’t been run for long enough at a high enough power output.&lt;/p&gt;
    &lt;p&gt;The problem was ultimately resolved thanks to the conservative reactor design of the Du Pont engineers. The original Hanford reactor, designed chiefly by physicist Eugene Wigner, consisted of 1,500 “channels” for uranium fuel arranged in a cylindrical shape. Du Pont had squared this cylinder, adding more channels to the edges and bringing the total to 2004. When fuel was loaded into these extra channels, it was sufficient to overcome the xenon poisoning effect.&lt;/p&gt;
    &lt;p&gt;The xenon poisoning problem was overcome in December 1944, and by March 1945 the Hanford site had achieved full-scale plutonium production of around a pound and a half of plutonium per day.&lt;/p&gt;
    &lt;head rend="h4"&gt;Los Alamos&lt;/head&gt;
    &lt;p&gt;As design and construction of the huge facilities at Hanford and Oak Ridge began, work also proceeded on designing the bomb itself. This would chiefly be done at Los Alamos, New Mexico, the third of the atomic cities built for the Manhattan Project. Scientists led by Robert Oppenheimer began to arrive at Los Alamos in March of 1943 (though most of the facility was still under construction).&lt;/p&gt;
    &lt;p&gt;When work began at Los Alamos, the most promising method for building a bomb appeared to be the gun method: firing a sub-critical uranium or plutonium bullet into another sub-critical mass. But there were a variety of ways this might be done. Different gun arrangements, ranging from somewhat conventional gun mechanisms of various sizes and shapes to more exotic layouts like double-guns, rockets, and spherical guns were considered. As with the rest of the program, every design choice was mired in uncertainty. Since plutonium and U235 hadn’t yet been produced in large quantities, it wasn’t known exactly how much fissile material would be needed to create a critical mass, and thus how big the bullet and target needed to be. The neutron-reflecting properties of steel gun barrels (which would affect critical mass requirements) hadn’t yet been measured.&lt;/p&gt;
    &lt;p&gt;More generally, while the designers were reasonably confident that a gun-type bomb would work, because various nuclear properties and constants had at best been measured very imprecisely (if they’d been measured at all), they couldn’t be sure. A history of Los Alamos notes that in 1942, “the main obstacle to a theoretical understanding of the fission bomb was the uncertainty surrounding existing experimental data, in part the result of inadequate instrumentation and a lack of experience in the new field.” Things were so uncertain that it was not even 100% clear at the beginning of the program that plutonium would produce neutrons. A great deal of effort at Los Alamos was thus devoted to more accurate measurements and better understanding of nuclear physics: the neutron cross sections of various materials, the number of neutrons produced per fission, rates of spontaneous fission, and so on. These measurements were “constantly in flux” for much of the project, and the Los Alamos scientists were “plagued by worry about some unpredicted or overlooked mechanism of nuclear physics which might render our program unsound.”&lt;/p&gt;
    &lt;p&gt;Thus in 1943, while most design efforts were directed to developing a gun-type assembly for both uranium and plutonium, there were also parallel efforts on other types of bomb. Chief among these was the implosion method, which would use an explosion to compress a sphere of material enough to create a critical mass, though there were also investigations into the autocatalytic methods. Work also proceeded on what was referred to as the Super (better known today as the hydrogen bomb): using a fission explosion to trigger an even more destructive fusion explosion.&lt;/p&gt;
    &lt;p&gt;Beyond the mechanism for creating the critical mass, there were many other aspects of the bomb that needed to be figured out. To minimize the amount of fissile material needed to create a critical mass, the core needed to be surrounded by some type of material that would reflect neutrons back into the core and prevent them from escaping, but what material would work best and how it should be arranged wasn’t yet known. To ensure that the bomb detonated at the right time, it would also need some type of initiator: a mechanism that, when triggered, would create a sudden burst of neutrons to start the nuclear chain reaction.&lt;/p&gt;
    &lt;p&gt;More generally, everything about the atomic bomb was new, and almost nothing about its various aspects was known or could be assumed. Every step involved from taking the fissile material from Oak Ridge and Hanford and turning it into a bomb had to be worked out for the first time.&lt;/p&gt;
    &lt;p&gt;Fissile material would arrive from Hanford and Oak Ridge not as pure plutonium or uranium but as compounds — plutonium nitrate and uranium tetrafluoride, respectively — and methods for turning these into metallic plutonium and uranium needed to be created. Because impurities might affect the functioning of the bomb, purification methods also needed to be developed. It was initially believed that the plutonium in particular would need to be exceptionally pure, with no more than one part per hundred billion of light elements. Preventing contamination required, among other things, reagents that were “unbelievably purified,” electronic air cleaners, and an extensive lab-cleaning procedure performed by a dedicated service team. Creating effective plutonium purification methods took roughly a year, in part because initially only microscopic quantities of plutonium were available for experiments. Because purification required the ability to detect extremely small quantities of impurities, novel methods of “sub-micro” chemical analysis had to be developed.&lt;/p&gt;
    &lt;p&gt;Once metallic plutonium and uranium had been produced, methods for shaping the material — casting, rolling, pressing — also needed to be created. The need for purity, combined with the extreme reactivity of molten uranium and plutonium, meant that new crucible materials were needed: MIT spent an enormous amount of “time, effort, and expense” to develop a cerium sulfide crucible that could withstand plutonium's extreme reactivity and high expected melting point without introducing impurities.&lt;/p&gt;
    &lt;p&gt;Shaping metallic plutonium — never before produced — required understanding its material properties, which were found to be extremely strange: plutonium has been dubbed “the most complicated metal known to man.” Researchers discovered that plutonium had six different allotropes (physical arrangement of atoms), more than any other metal. Plutonium’s complexity made basic facts such as its melting point surprisingly difficult to determine, and the exotic cerium sulfide crucibles proved to be unnecessary when an unexpectedly low melting point was found. Most of what’s known about plutonium metallurgy was initially worked out at Los Alamos during the Manhattan Project.&lt;/p&gt;
    &lt;p&gt;In addition to uranium and plutonium, novel methods and processes had to be developed for a variety of other materials. Polonium, used in the initiator, needed to be procured in large quantities, extensively purified, and deposited on metallic foils. The investigation into polonium’s material properties has been described as “novel as that of plutonium.” Fabrication techniques were also developed for other materials with useful nuclear properties, such as Boron 10 (an isotope of boron with a large neutron cross section) and beryllium.&lt;/p&gt;
    &lt;p&gt;While a great deal was known about the mechanics of guns and projectiles at the beginning of the project, nothing like a gun needed to fire a sub-critical nuclear bullet had ever been built. The bullet needed to be fired as fast as possible to avoid the problem of predetonation — spontaneous fission starting a chain reaction before the pieces had been joined, leading to a “fizzle” — but the target also needed to stay roughly intact after the projectile’s impact. The gun also needed to be light and small enough that the resulting bomb wouldn’t be too heavy to carry, a requirement that was greatly aided by the realization that the gun would only need to fire once, and could be much less robust than conventional guns. A great deal of calculation and testing on guns of various calibers, propellants, and geometries of targets and bullets was required.&lt;/p&gt;
    &lt;p&gt;But novel as it was, this work on gun development was far more straightforward than what was required for the implosion bomb. Virtually nothing was known about the behavior of materials when imploded, or how to create an explosion that would compress a sphere of material symmetrically. Resolving this lack of knowledge began simply — setting off explosives on the outside of pipes and seeing how they deformed — but quickly ramped up in complexity. A variety of methods and instruments had to be created to study the interior of materials as they were being imploded. Some of these, such as using X-rays and high-speed cameras, were adaptations of existing measurement techniques, pushed to much higher levels of performance: getting high-speed cameras to work, for instance, required months of experiments with multiple camera designs to create one that was fast enough.&lt;/p&gt;
    &lt;p&gt;Other implosion analysis methods, such as the “RaLa” method, were far more novel. The RaLa method involved placing an isotope of radioactive lanthanum (hence RaLa), an emitter of gamma rays, at the center of the material to be imploded. When the material began to compress, it would become denser, resulting in fewer gamma rays penetrating. By surrounding the implosion with gamma ray detectors, a detailed progression of the geometric changes in the imploded material could be recorded.&lt;/p&gt;
    &lt;p&gt;While the RaLa method was a valuable source of implosion information, implementing it was fiendishly complicated. Even gathering and manipulating the lanthanum was difficult due to its intense radioactivity. Lanthanum was shipped from Oak Ridge in special lead-lined trucks, which were driven 24 hours a day. The assembled material reached 100 curies of radioactivity at a time when most radioactive experiments didn't exceed a fraction of a curie: a Los Alamos chemist noted that “no one ever worked with radiation levels like these before, ever, anywhere in the world.” Using RaLa required things like a “mechanical chemist” to remotely manipulate the radioactive material, and, initially, a mobile laboratory built inside repurposed M4 tanks.&lt;/p&gt;
    &lt;p&gt;It was initially hoped that a symmetrical compression could be created simply by adding enough explosive detonation points around the spherical bomb core, but RaLa and other measurement methods revealed “jets” of core material shooting ahead of the rest of the collapsing mass. Dealing with the jets and other asymmetries, and creating a symmetrical collapse of the bomb core, would be one of the main difficulties of the implosion bomb program.&lt;/p&gt;
    &lt;p&gt;New analytic techniques and devices weren’t limited to measuring implosions. New devices were made for, among other things, counting neutrons, collecting electrons, discriminating between different electronic pulses, and measuring projectiles with microwaves. Over a thousand pieces of electronic equipment were built at Los Alamos, many of them novel or higher performance than any other equipment then available, including better amplifiers, oscilloscopes, and counting circuits.&lt;/p&gt;
    &lt;p&gt;As with Oak Ridge and Hanford, even seemingly straightforward project elements were often enormously complex development efforts due to the novel requirements of the bomb. The triggering mechanism, for instance, couldn’t rely on off the shelf components, as they were considered insufficiently reliable: even a 1% chance of failure was far too high for a bomb in which hundreds of millions of dollars had been invested (the acceptable failure rate was eventually decided to be 0.01%). And when dropped, the bomb needed to automatically trigger at a specific elevation, a capability which didn’t yet exist. After a great deal of testing and development, a trigger circuit using radar altimeters, barometric switches, and electronic clocks was eventually created.&lt;/p&gt;
    &lt;p&gt;All this development work took place in largely the same form as the rest of the project: for nearly every decision or device, multiple promising paths were investigated, often at great expense, in the hopes that one or more could be made to work. This often led to a cascade of branching investigations, where each path branched into several possible paths, each of which might branch into more possible paths, and so on. The implosion method was just one of multiple bomb designs investigated, and within the implosion investigation RaLa was just one of multiple analytic techniques created to study implosions (a history of Los Alamos described the implosion studies as a “seven-pronged experimental program”). Bombs using both plutonium and U235 fuels were pursued, and for each material multiple methods of processing them were studied.&lt;/p&gt;
    &lt;p&gt;Because so little was known, and progress needed to be made quickly, these investigations often relied on brute force empiricism: running dozens or hundreds of experiments while systematically varying different experimental parameters. Early implosion studies repeatedly tested pipes surrounded by explosives, systematically varying the size of pipes, explosive arrangement, and the type of explosive used. These systematic investigations continued when more advanced diagnostic methods became available: to determine implosion parameters like symmetry, collapse velocity, and amount of compression, an “exhaustive” test program was initiated, where “every possible parameter was varied”. Systematic trial-and-error testing was also used for the design of the gun, the projectiles, and the target.&lt;/p&gt;
    &lt;head rend="h4"&gt;The pivot to implosion at Los Alamos&lt;/head&gt;
    &lt;p&gt;The strategy of investigating multiple promising paths proved its worth when the first shipments of reactor-produced plutonium began to arrive at Los Alamos from Oak Ridge in the spring of 1944. Prior to this Los Alamos had worked only with plutonium produced in cyclotrons, which consisted chiefly of the isotope Plutonium-239. However, reactor-produced plutonium was found to also have significant amounts of a different isotope, Plutonium-240. This isotope was found to undergo spontaneous fission much more readily than Plutonium-239 or U235. Its rate of spontaneous fission — a million times higher that of U235 — was so high in fact that the presence of Plutonium-240 made a gun-type plutonium bomb infeasible: so many neutrons would be produced by spontaneous fission that a chain reaction would be triggered before the bullet met the target, blowing the bomb apart and creating a fizzle.&lt;/p&gt;
    &lt;p&gt;The situation seemed dire. There was no time to design and build a separation plant to remove Plutonium-239 from Plutonium-240. To make use of the Hanford plutonium, the only options appeared to be build a composite bomb that mixed plutonium and uranium together (which would be low efficiency and have a comparatively small yield), or to use a different bomb mechanism capable of creating critical mass much more quickly than a gun could. That meant the implosion method, but in early 1944 work on the implosion bomb was far behind behind the gun bomb: until then the implosion bomb had been of secondary importance, a backup in case the gun didn’t work. It was still far from certain whether a workable implosion bomb could be built.&lt;/p&gt;
    &lt;p&gt;In July 1944, Robert Oppenheimer ordered a halt on further work on the plutonium gun, and to step up efforts on a plutonium implosion bomb. Within two weeks Los Alamos had been completely reorganized to focus on solving the problems of the implosion bomb. Work on the various implosion analysis methods accelerated, and the first RaLa test was completed in late September. An extensive exploration of solutions to the problem of jets and asymmetrical compression was undertaken, and the development of plutonium purification and metal fabrication methods continued (made easier by the fact that an implosion bomb could tolerate a much higher level of plutonium impurities). By early 1945, aided by the discovery plutonium’s unexpectedly low melting point (which made finding a workable crucible much easier), plutonium was successfully being purified, reduced to a metal, and worked by various methods, shortly before the first “batch” (a mere 80 grams) of plutonium arrived from Hanford.&lt;/p&gt;
    &lt;p&gt;The asymmetrical compression problem was eventually solved by the use of explosive lenses, and by changing from a hollow to a solid core of fissile material. Explosive lenses — shaped explosives that would focus the explosion and create a converging pressure wave — were first suggested by James Tuck, who arrived at Los Alamos in May 1944, but using them to build a bomb was fiercely difficult. No theory yet existed for analyzing and predicting the behavior of explosive lenses, and no methods existing for fabricating the carefully shaped explosives to the level of precision required. To design the lenses, there was no choice but to take an iterative approach: designers made guesses about effective lens shapes, tested them, and used the feedback to refine their designs.&lt;/p&gt;
    &lt;p&gt;At the same time, methods had to be developed for casting and machining the explosives. This was both enormously difficult (since the explosives had to be extremely uniform and precisely shaped) and dangerous (since machining risked explosion). A history of Los Alamos describes the challenges of explosive casting:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was particularly difficult to cast the high explosives accurately and avoid cracks, bubbles, and other imperfections. Cooling cycles had to be long to minimize thermal stress cracks. Castings had to be wrapped in insulation before being transported between buildings…Casting technology developed slowly and painfully at Los Alamos, by a succession of reasonable steps, that consistently failed to give completely satisfactory results. Eventually, the problems were overcome… - Critical Assembly&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Machining the explosives was similarly difficult. The jigs and fixtures required to hold the explosives required several months of development, and the explosive machining methods created were considered “revolutionary.”&lt;/p&gt;
    &lt;p&gt;The work of figuring out casting and machining methods, and of creating workable lens shapes, demanded enormous amounts of explosives: over the course of the project James Tuck noted that “well over twenty thousand castings were delivered to the firing sites, while the number of castings rejected because of poor quality or destroyed for other reasons is several times this figure.” At the peak of the implosion program, Los Alamos was using 100,000 pounds of high explosive a month.&lt;/p&gt;
    &lt;p&gt;Creating symmetrical compression in the bomb core also required very precise detonating of the explosives. Early tests were done using primacord detonators (a cord of high explosive surrounded by textiles), but these were found to be far too imprecise, and investigation into other types of detonators was pursued. After extensive experimentation, a spark gap switch (a sort of explosive spark plug) combined with an exploding bridgewire detonator was developed.&lt;/p&gt;
    &lt;p&gt;Another major design problem on the implosion device was the initiator. The implosion bomb would require a new, more precise type of initiator than used on the gun bomb, one that would be triggered at the moment of highest compression in the bomb core. As late as early 1945, it wasn’t clear whether such an initiator could be built. But continued experiments and testing eventually resulted in what appeared to be a workable design. Named “the urchin,” it consisted of a beryllium sphere and pellet, with polonium between the two. When the core was compressed, the sphere would be crushed, mixing the beryllium and polonium and emitting neutrons.&lt;/p&gt;
    &lt;p&gt;Until very late in the Manhattan Project, it remained unclear if a workable implosion bomb could be built. In the last weeks of 1944, James Conant, President of Harvard and chair of the National Research Defense Council which oversaw the Manhattan Project, stated that the “difficulties were still enormous” and “my own bets very much against it.” At that time, the problems of the modulated initiator and of sufficiently precise and accurate detonation still hadn’t been solved. But as the researchers continued to run down various problems over the following months, the outlook improved considerably. By April the head of the explosives division at Los Alamos could report that its major research and design gambles “had been won”, and there was growing confidence that a bomb of the design chosen — solid plutonium core, explosive lenses, with electric detonators and a modulated initiator — could be made to work. Development work was by no means complete, and there were many problems yet to be solved (design changes to the bomb continued to be made until a few days before the Trinity test on July 16 1945), but by spring of 1945 the “research” portion of research and development had largely concluded.&lt;/p&gt;
    &lt;p&gt;As Los Alamos scrambled to build an effective implosion bomb, work also continued on the uranium gun weapon. Because the gun device was considered much less risky and much more certain to work, its development didn’t have the same fervor as the plutonium bomb. (In fact, cancelling the plutonium gun made the uranium gun program easier: a uranium bullet could be fired at a much lower velocity, reducing the difficulties of building a working gun device). But there were still numerous development problems that needed to be overcome. As with plutonium, uranium metal reduction and working methods were finalized by late 1944, and by early 1945 the design of the gun was completed and assembly of it was being tested. Unanswered questions remained until surprisingly late in the program (as late as December 1944 the critical mass of U235 still hadn’t been precisely determined), but there was little uncertainty around whether the bomb would function (so little, in fact, that testing the uranium gun bomb was considered unnecessary). By May of 1945, the uranium gun weapon, code named “Little Boy,” was “ready for combat”.&lt;/p&gt;
    &lt;head rend="h4"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The Manhattan Project has become synonymous with a difficult, expensive, and ambitious technological development project, and you often hear folks advocating for a “Manhattan Project for X.” So it's worth understanding why, specifically, the Manhattan Project was so difficult and expensive.&lt;/p&gt;
    &lt;p&gt;First, there were inherent physical difficulties in many of the tasks. The bombs required pounds of fissile material, and there was no easy way to produce it: any method chosen would require an enormous industrial-scale production facility. The Hanford Site for producing plutonium cost $350 million ($6.4 billion in 2025 dollars), and the Oak Ridge site cost $304 million ($5.5 billion in 2025 dollars), not including the cost of the borrowed silver for the electromagnets. Hundreds of millions more were spent on operating the facilities. Part of the expense and difficulty of the Manhattan Project came simply because manufacturing fissile material is expensive and difficult (and remains so today.)&lt;/p&gt;
    &lt;p&gt;The second difficulty with the Manhattan Project was that because of the great urgency, work had to proceed on the basis of very little information. Resolving the uncertainty often entailed expensive efforts that would have been greatly simplified (or eschewed altogether) had a slower pace been acceptable. Plants were built before the processes they would use had been completely defined, often requiring extensive rework after parts of them had been built. Time and effort was invested in creating a plutonium gun bomb that could have been avoided had the designers waited until reactor-produced plutonium (which due to the presence of Plutonium-240 wouldn’t work in a gun bomb) was available.&lt;/p&gt;
    &lt;p&gt;The third difficulty was that because so little knowledge existed around the nature of atomic physics and nuclear chain reactions, it was far from clear what the best route to an atomic weapon was. Because the field was so new, using only recently-discovered natural phenomena that were poorly understood, a great deal of effort was needed to resolve this uncertainty along numerous technological axes. Thus the Manhattan Project involved a large amount of trial and error experimentation, and of pursuing multiple paths of technological development — different bomb types, different fuels, different uranium separation methods, different tampers, different triggers, different implosion analysis methods — to create a workable bomb.&lt;/p&gt;
    &lt;p&gt;It’s this last difficulty that is most relevant for other technological development projects. Developing other technologies doesn’t necessarily require building enormous, industrial scale industrial facilities to even begin, and doesn’t necessarily require rapidly proceeding before the proper information and supporting technologies are available. But it will almost certainly require investigating various promising paths of development, partially-informed groping around until the right combination of methods and components is discovered. Indeed, this sort of exploration is the very essence of technological development.&lt;/p&gt;
    &lt;p&gt;Edison’s light bulb provides a useful comparison: inventing it didn’t require building an enormous, multi-million dollar factory to produce the components to experiment with. And Edison wasn’t forced to invent every single predecessor technology that a light bulb required. One of the reasons why an incandescent bulb wasn’t invented earlier is that, prior to the 1860s, vacuum pumps weren’t good enough. Edison’s bulb relied on the invention of the Sprengel mercury pump by Hermann Sprengel in 1865, which could create a high enough vacuum that incandescent lamps became feasible. But Edison was forced to explore a variety of different potential methods for creating a bulb until he created one that worked.&lt;/p&gt;
    &lt;p&gt;One thing that the Manhattan Project shows is that resolving this uncertainty, and figuring out what a technology should actually be, is hard. The Manhattan Project had some of the most brilliant scientific minds in the world working on it, but even with this collective brainpower it was far from clear what the best route to the bomb was. For almost every major design decision (including the successful ones), there was at least one genius or expert highly skeptical that it would work. Future Nobel Prize winner Hans Bethe initially resisted joining the program because he believed that building an atomic bomb wasn’t feasible at all. Early in the program, many (such as physicist Alfred Nier) felt that electromagnetic separation wouldn’t be a feasible way to isolate U235 (and indeed, until very late it looked like it wouldn’t be). British scientists were similarly pessimistic about the gaseous diffusion process, with some believing that the plant “would be inoperable” due to surges and fluctuations in the gas flow. Du Pont thought that a graphite moderated, water-cooled reactor might not work, and insisted on a heavy water reactor as a backup. Explosives expert William Parsons, head of the uranium gun program, was skeptical that explosive lenses would work, and argued that the only way to have an implosion bomb ready by summer of 1945 was with a non-lensed design. Enrico Fermi initially believed that a modulated initiator wouldn’t work, and would come up with a new reason why “every second day or so.”&lt;/p&gt;
    &lt;p&gt;It’s also notable that many of the options chosen didn’t turn out to be the best long-term. Post-war atomic weapons almost entirely eschewed gun-type mechanisms in favor of more efficient implosions. And of all the uranium separation methods explored, it was the one that was deemed least promising and not used at all by the Manhattan Project — gas centrifuges — that is primarily used for uranium enrichment today.&lt;/p&gt;
    &lt;p&gt;Not all technologies will require expensive physical facilities to produce, or require extremely rapid, expensive development. But resolving the uncertainty inherent in a new technology — figuring out what, exactly, the arrangement of phenomena needs to be to achieve some goal, and how that arrangement can be achieved — is part of the fundamental nature of creating a new technology. The Manhattan Project required an unusually large amount of this (advancing the state of the art in many different disciplines), but will always be required to some degree.&lt;/p&gt;
    &lt;p&gt;Consideration was also given to another isotope of uranium, U233.&lt;/p&gt;
    &lt;p&gt;Slowing down the neutrons made them more likely to be captured by the U235 and induce fission.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45211127</guid></item><item><title>CRISPR Offers New Hope for Treating Diabetes</title><link>https://www.wired.com/story/no-more-injections-crispr-offers-new-hope-for-treating-diabetes/</link><description>&lt;doc fingerprint="1654c01a1a0f0b13"&gt;
  &lt;main&gt;
    &lt;p&gt;All products featured on WIRED are independently selected by our editors. However, we may receive compensation from retailers and/or from purchases of products through these links. Learn more.&lt;/p&gt;
    &lt;p&gt;Crispr gene-editing technology has demonstrated its revolutionary potential in recent years: It has been used to treat rare diseases, to adapt crops to withstand the extremes of climate change, or even to change the color of a spider’s web. But the greatest hope is that this technology will help find a cure for a global disease, such as diabetes. A new study points in that direction.&lt;/p&gt;
    &lt;p&gt;For the first time, researchers succeeded in implanting Crispr-edited pancreatic cells in a man with type 1 diabetes, an autoimmune disease where the immune system attacks insulin-producing cells in the pancreas. Without insulin, the body is then unable to regulate blood sugar. If steps aren’t taken to manage glucose levels by other means (typically, by injecting insulin), this can lead to damage to the nerves and organs—particularly the heart, kidneys, and eyes. Roughly 9.5 million people worldwide have type 1 diabetes.&lt;/p&gt;
    &lt;p&gt;In this experiment, edited cells produced insulin for months after being implanted, without the need for the recipient to take any immunosuppressive drugs to stop their body attacking the cells. The Crispr technology allowed the researchers to endow the genetically modified cells with camouflage to evade detection.&lt;/p&gt;
    &lt;p&gt;The study, published last month in The New England Journal of Medicine, details the step-by-step procedure. First, pancreatic islet cells were taken from a deceased donor without diabetes, and then altered with the gene-editing technique Crispr-Cas12b to allow them to evade the immune response of the diabetes patient. Cells altered like this are said to be “hypoimmune,” explains Sonja Schrepfer, a professor at Cedars-Sinai Medical Center in California and the scientific cofounder of Sana Biotechnology, the company that developed this treatment.&lt;/p&gt;
    &lt;p&gt;The edited cells were then implanted into the forearm muscle of the patient, and after 12 weeks, no signs of rejection were detected. (A subsequent report from Sana Biotechnology notes that the implanted cells were still evading the patient’s immune system after six months.)&lt;/p&gt;
    &lt;p&gt;Tests run as part of the study recorded that the cells were functional: The implanted cells secreted insulin in response to glucose levels, representing a key step toward controlling diabetes without the need for insulin injections. Four adverse events were recorded during follow-ups with the patient, but none of them were serious or directly linked to the modified cells.&lt;/p&gt;
    &lt;p&gt;The researchers’ ultimate goal is to apply immune-camouflaging gene edits to stem cells—which have the ability to reproduce and differentiate themselves into other cell types inside the body—and then to direct their development into insulin-secreting islet cells. “The advantage of engineering hypoimmune stem cells is that when these stem cells proliferate and create new cells, the new cells are also hypoimmune,” Schrepfer explained in a Cedars-Sinai Q+A earlier this year.&lt;/p&gt;
    &lt;p&gt;Traditionally, transplanting foreign cells into a patient has required suppressing the patient’s immune system to avoid them being rejected. This carries significant risks: infections, toxicity, and long-term complications. “Seeing patients die from rejection or severe complications from immunosuppression was frustrating to me, and I decided to focus my career on developing strategies to overcome immune rejection without immunosuppressive drugs,” Schrepfer told Cedars-Sinai.&lt;/p&gt;
    &lt;p&gt;Although the research marks a milestone in the search for treatments of type 1 diabetes, it’s important to note that the study involved one one participant, who received a low dose of cells for a short period—not enough for the patient to no longer need to control their blood sugar with injected insulin. An editorial by the journal Nature also says that some independent research groups have failed in their efforts to confirm that Sana’s method provides edited cells with the ability to evade the immune system.&lt;/p&gt;
    &lt;p&gt;Sana will be looking to conduct more clinical trials starting next year. Without overlooking the criticisms and limitations of the current study, the possibility of transplanting cells modified to be invisible to the immune system opens up a very promising horizon in regenerative medicine.&lt;/p&gt;
    &lt;p&gt;This story originally appeared on WIRED en Español and has been translated from Spanish.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45211596</guid></item></channel></rss>