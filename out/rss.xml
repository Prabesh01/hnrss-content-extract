<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 24 Jan 2026 19:08:52 +0000</lastBuildDate><item><title>Unrolling the Codex agent loop</title><link>https://openai.com/index/unrolling-the-codex-agent-loop/</link><description>&lt;doc fingerprint="f64eccd3469e8c9d"&gt;
  &lt;main&gt;
    &lt;p&gt;Codex CLI(opens in a new window) is our cross-platform local software agent, designed to produce high-quality, reliable software changes while operating safely and efficiently on your machine. We’ve learned a tremendous amount about how to build a world-class software agent since we first launched the CLI in April. To unpack those insights, this is the first post in an ongoing series where we’ll explore various aspects of how Codex works, as well as hard-earned lessons. (For an even more granular view on how the Codex CLI is built, check out our open source repository at https://github.com/openai/codex(opens in a new window). Many of the finer details of our design decisions are memorialized in GitHub issues and pull requests if you’d like to learn more.)&lt;/p&gt;
    &lt;p&gt;To kick off, we’ll focus on the agent loop, which is the core logic in Codex CLI that is responsible for orchestrating the interaction between the user, the model, and the tools the model invokes to perform meaningful software work. We hope this post gives you a good view into the role our agent (or “harness”) plays in making use of an LLM.&lt;/p&gt;
    &lt;p&gt;Before we dive in, a quick note on terminology: at OpenAI, “Codex” encompasses a suite of software agent offerings, including Codex CLI, Codex Cloud, and the Codex VS Code extension. This post focuses on the Codex harness, which provides the core agent loop and execution logic that underlies all Codex experiences and is surfaced through the Codex CLI. For ease here, we’ll use the terms “Codex” and “Codex CLI” interchangeably.&lt;/p&gt;
    &lt;p&gt;At the heart of every AI agent is something called “the agent loop.” A simplified illustration of the agent loop looks like this:&lt;/p&gt;
    &lt;p&gt;To start, the agent takes input from the user to include in the set of textual instructions it prepares for the model known as a prompt.&lt;/p&gt;
    &lt;p&gt;The next step is to query the model by sending it our instructions and asking it to generate a response, a process known as inference. During inference, the textual prompt is first translated into a sequence of input tokens(opens in a new window)—integers that index into the model’s vocabulary. These tokens are then used to sample the model, producing a new sequence of output tokens.&lt;/p&gt;
    &lt;p&gt;The output tokens are translated back into text, which becomes the model’s response. Because tokens are produced incrementally, this translation can happen as the model runs, which is why many LLM-based applications display streaming output. In practice, inference is usually encapsulated behind an API that operates on text, abstracting away the details of tokenization.&lt;/p&gt;
    &lt;p&gt;As the result of the inference step, the model either (1) produces a final response to the user’s original input, or (2) requests a tool call that the agent is expected to perform (e.g., “run &lt;code&gt;ls&lt;/code&gt; and report the output”). In the case of (2), the agent executes the tool call and appends its output to the original prompt. This output is used to generate a new input that’s used to re-query the model; the agent can then take this new information into account and try again.&lt;/p&gt;
    &lt;p&gt;This process repeats until the model stops emitting tool calls and instead produces a message for the user (referred to as an assistant message in OpenAI models). In many cases, this message directly answers the user’s original request, but it may also be a follow-up question for the user.&lt;/p&gt;
    &lt;p&gt;Because the agent can execute tool calls that modify the local environment, its “output” is not limited to the assistant message. In many cases, the primary output of a software agent is the code it writes or edits on your machine. Nevertheless, each turn always ends with an assistant message—such as “I added the &lt;code&gt;architecture.md&lt;/code&gt; you asked for”—which signals a termination state in the agent loop. From the agent’s perspective, its work is complete and control returns to the user.&lt;/p&gt;
    &lt;p&gt;The journey from user input to agent response shown in the diagram is referred to as one turn of a conversation (a thread in Codex). Though this conversation turn can include many iterations between the model inference and tool calls). Every time you send a new message to an existing conversation, the conversation history is included as part of the prompt for the new turn, which includes the messages and tool calls from previous turns:&lt;/p&gt;
    &lt;p&gt;This means that as the conversation grows, so does the length of the prompt used to sample the model. This length matters because every model has a context window, which is the maximum number of tokens it can use for one inference call. Note this window includes both input and output tokens. As you might imagine, an agent could decide to make hundreds of tool calls in a single turn, potentially exhausting the context window. For this reason, context window management is one of the agent’s many responsibilities. Now, let’s dive in to see how Codex runs the agent loop.&lt;/p&gt;
    &lt;p&gt;The Codex CLI sends HTTP requests to the Responses API(opens in a new window) to run model inference. We’ll examine how information flows through Codex, which uses the Responses API to drive the agent loop.&lt;/p&gt;
    &lt;p&gt;The Responses API endpoint that the Codex CLI uses is configurable(opens in a new window), so it can be used with any endpoint that implements the Responses API(opens in a new window):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When using ChatGPT login(opens in a new window) with the Codex CLI, it uses &lt;code&gt;https://chatgpt.com/backend-api/codex/responses&lt;/code&gt;as the endpoint&lt;/item&gt;
      &lt;item&gt;When using API-key authentication(opens in a new window) with OpenAI hosted models, it uses &lt;code&gt;https://api.openai.com/v1/responses&lt;/code&gt;as the endpoint&lt;/item&gt;
      &lt;item&gt;When running Codex CLI with &lt;code&gt;--oss&lt;/code&gt;to use gpt-oss with ollama 0.13.4+(opens in a new window) or LM Studio 0.3.39+(opens in a new window), it defaults to&lt;code&gt;http://localhost:11434/v1/responses&lt;/code&gt;running locally on your computer&lt;/item&gt;
      &lt;item&gt;Codex CLI can be used with the Responses API hosted by a cloud provider such as Azure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s explore how Codex creates the prompt for the first inference call in a conversation.&lt;/p&gt;
    &lt;p&gt;As an end user, you don’t specify the prompt used to sample the model verbatim when you query the Responses API. Instead, you specify various input types as part of your query, and the Responses API server decides how to structure this information into a prompt that the model is designed to consume. You can think of the prompt as a “list of items”; this section will explain how your query gets transformed into that list.&lt;/p&gt;
    &lt;p&gt;In the initial prompt, every item in the list is associated with a role. The &lt;code&gt;role&lt;/code&gt; indicates how much weight the associated content should have and is one of the following values (in decreasing order of priority): &lt;code&gt;system&lt;/code&gt;, &lt;code&gt;developer&lt;/code&gt;, &lt;code&gt;user&lt;/code&gt;, &lt;code&gt;assistant&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The Responses API(opens in a new window) takes a JSON payload with many parameters. We’ll focus on these three:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;instructions&lt;/code&gt;(opens in a new window): system (or developer) message inserted into the model’s context&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tools&lt;/code&gt;(opens in a new window): a list of tools the model may call while generating a response&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;input&lt;/code&gt;(opens in a new window): a list of text, image, or file inputs to the model&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Codex, the &lt;code&gt;instructions&lt;/code&gt; field is read from the &lt;code&gt;model_instructions_file&lt;/code&gt;(opens in a new window) in &lt;code&gt;~/.codex/config.toml&lt;/code&gt;, if specified; otherwise, the &lt;code&gt;base_instructions&lt;/code&gt; associated with a model(opens in a new window) are used. Model-specific instructions live in the Codex repo and are bundled into the CLI (e.g., &lt;code&gt;gpt-5.2-codex_prompt.md&lt;/code&gt;(opens in a new window)).&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;tools&lt;/code&gt; field is a list of tool definitions that conform to a schema defined by the Responses API. For Codex, this includes tools that are provided by the Codex CLI, tools that are provided by the Responses API that should be made available to Codex, as well as tools provided by the user, usually via MCP servers:&lt;/p&gt;
    &lt;p&gt;Finally, the &lt;code&gt;input&lt;/code&gt; field of the JSON payload is a list of items. Codex inserts the following items(opens in a new window) into the &lt;code&gt;input&lt;/code&gt; before adding the user message:&lt;/p&gt;
    &lt;p&gt;1. A message with &lt;code&gt;role=developer&lt;/code&gt; that describes the sandbox that applies only to the Codex-provided &lt;code&gt;shell&lt;/code&gt; tool defined in the &lt;code&gt;tools&lt;/code&gt; section. That is, other tools, such as those provided from MCP servers, are not sandboxed by Codex and are responsible for enforcing their own guardrails.&lt;/p&gt;
    &lt;p&gt;The message is built from a template where the key pieces of content come from snippets of Markdown bundled into the Codex CLI, such as &lt;code&gt;workspace_write.md&lt;/code&gt;(opens in a new window) and &lt;code&gt;on_request.md&lt;/code&gt;(opens in a new window):&lt;/p&gt;
    &lt;p&gt;2. (Optional) A message with &lt;code&gt;role=developer&lt;/code&gt; whose contents are the &lt;code&gt;developer_instructions&lt;/code&gt; value read from the user’s &lt;code&gt;config.toml&lt;/code&gt; file.&lt;/p&gt;
    &lt;p&gt;3. (Optional) A message with &lt;code&gt;role=user&lt;/code&gt; whose contents are the “user instructions,” which are not sourced from a single file but are aggregated across multiple sources(opens in a new window). In general, more specific instructions appear later:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Contents of &lt;code&gt;AGENTS.override.md&lt;/code&gt;and&lt;code&gt;AGENTS.md&lt;/code&gt;in&lt;code&gt;$CODEX_HOME&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Subject to a limit (32 KiB, by default), look in each folder from the Git/project root of the &lt;code&gt;cwd&lt;/code&gt;(if it it exists) up to the&lt;code&gt;cwd&lt;/code&gt;itself: add the contents of any of&lt;code&gt;AGENTS.override.md&lt;/code&gt;,&lt;code&gt;AGENTS.md&lt;/code&gt;, or any filename specified by&lt;code&gt;project_doc_fallback_filenames in config.toml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;If any skills(opens in a new window) have been configured:&lt;list rend="ul"&gt;&lt;item&gt;a short preamble about skills&lt;/item&gt;&lt;item&gt;the skill metadata(opens in a new window) for each skill&lt;/item&gt;&lt;item&gt;a section on how to use skills(opens in a new window)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;4. A message with &lt;code&gt;role=user&lt;/code&gt; that describes the local environment in which the agent is currently operating. This specifies the current working directory and the user’s shell(opens in a new window):&lt;/p&gt;
    &lt;p&gt;Once Codex has done all of the above computation to initialize the &lt;code&gt;input&lt;/code&gt;, it appends the user message to start the conversation.&lt;/p&gt;
    &lt;p&gt;The previous examples focused on the content of each message, but note that each element of &lt;code&gt;input&lt;/code&gt; is a JSON object with &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;role&lt;/code&gt;(opens in a new window), and &lt;code&gt;content&lt;/code&gt; as follows:&lt;/p&gt;
    &lt;p&gt;Once Codex builds up the full JSON payload to send to the Responses API, it then makes the HTTP POST request with an &lt;code&gt;Authorization&lt;/code&gt; header depending on how the Responses API endpoint is configured in &lt;code&gt;~/.codex/config.toml&lt;/code&gt; (additional HTTP headers and query parameters are added if specified).&lt;/p&gt;
    &lt;p&gt;When an OpenAI Responses API server receives the request, it uses the JSON to derive the prompt for the model as follows (to be sure, a custom implementation of the Responses API could make a different choice):&lt;/p&gt;
    &lt;p&gt;As you can see, the order of the first three items in the prompt is determined by the server, not the client. That said, of those three items, only the content of the system message is also controlled by the server, as the &lt;code&gt;tools&lt;/code&gt; and &lt;code&gt;instructions&lt;/code&gt; are determined by the client. These are followed by the &lt;code&gt;input&lt;/code&gt; from the JSON payload to complete the prompt.&lt;/p&gt;
    &lt;p&gt;Now that we have our prompt, we are ready to sample the model.&lt;/p&gt;
    &lt;p&gt;This HTTP request to the Responses API initiates the first “turn” of a conversation in Codex. The server replies with a Server-Sent Events (SSE(opens in a new window)) stream. The &lt;code&gt;data&lt;/code&gt; of each event is a JSON payload with a &lt;code&gt;"type"&lt;/code&gt; that starts with &lt;code&gt;"response"&lt;/code&gt;, which could be something like this (a full list of events can be found in our API docs(opens in a new window)):&lt;/p&gt;
    &lt;p&gt;Codex consumes the stream of events(opens in a new window) and republishes them as internal event objects that can be used by a client. Events like &lt;code&gt;response.output_text.delta&lt;/code&gt; are used to support streaming in the UI, whereas other events like &lt;code&gt;response.output_item.added&lt;/code&gt; are transformed into objects that are appended to the &lt;code&gt;input&lt;/code&gt; for subsequent Responses API calls.&lt;/p&gt;
    &lt;p&gt;Suppose the first request to the Responses API includes two &lt;code&gt;response.output_item.done&lt;/code&gt; events: one with &lt;code&gt;type=reasoning&lt;/code&gt; and one with &lt;code&gt;type=function_call&lt;/code&gt;. These events must be represented in the &lt;code&gt;input&lt;/code&gt; field of the JSON when we query the model again with the response to the tool call: &lt;/p&gt;
    &lt;p&gt;The resulting prompt used to sample the model as part of the subsequent query would look like this:&lt;/p&gt;
    &lt;p&gt;In particular, note how the old prompt is an exact prefix of the new prompt. This is intentional, as this makes subsequent requests much more efficient because it enables us to take advantage of prompt caching (which we’ll discuss in the next section on performance).&lt;/p&gt;
    &lt;p&gt;Looking back at our first diagram of the agent loop, we see that there could be many iterations between inference and tool calling. The prompt may continue to grow until we finally receive an assistant message, indicating the end of the turn:&lt;/p&gt;
    &lt;p&gt;In the Codex CLI, we present the assistant message to the user and focus the composer to indicate to the user that it’s their “turn” to continue the conversation. If the user responds, both the assistant message from the previous turn, as well as the user’s new message, must be appended to the &lt;code&gt;input&lt;/code&gt; in the Responses API request to start the new turn:&lt;/p&gt;
    &lt;p&gt;Once again, because we are continuing a conversation, the length of the &lt;code&gt;input&lt;/code&gt; we send to the Responses API keeps increasing:&lt;/p&gt;
    &lt;p&gt;Let’s examine what this ever-growing prompt means for performance.&lt;/p&gt;
    &lt;p&gt;You might be asking yourself, “Wait, isn’t the agent loop quadratic in terms of the amount of JSON sent to the Responses API over the course of the conversation?” And you would be right. While the Responses API does support an optional &lt;code&gt;previous_response_id&lt;/code&gt;(opens in a new window) parameter to mitigate this issue, Codex does not use it today, primarily to keep requests fully stateless and to support Zero Data Retention (ZDR) configurations.&lt;/p&gt;
    &lt;p&gt;Avoiding &lt;code&gt;previous_response_id&lt;/code&gt; simplifies things for the provider of the Responses API because it ensures that every request is stateless. This also makes it straightforward to support customers who have opted into Zero Data Retention (ZDR)(opens in a new window), as storing the data required to support &lt;code&gt;previous_response_id&lt;/code&gt; would be at odds with ZDR. Note that ZDR customers do not sacrifice the ability to benefit from proprietary reasoning messages from prior turns, as the associated &lt;code&gt;encrypted_content&lt;/code&gt; can be decrypted on the server. (OpenAI persists a ZDR customer’s decryption key, but not their data.) See PRs #642(opens in a new window) and #1641(opens in a new window) for the related changes to Codex to support ZDR.&lt;/p&gt;
    &lt;p&gt;Generally, the cost of sampling the model dominates the cost of network traffic, making sampling the primary target of our efficiency efforts. This is why prompt caching is so important, as it enables us to reuse computation from a previous inference call. When we get cache hits, sampling the model is linear rather than quadratic. Our prompt caching (opens in a new window)documentation explains this in more detail:&lt;/p&gt;
    &lt;p&gt;Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests.&lt;/p&gt;
    &lt;p&gt;With this in mind, let’s consider what types of operations could cause a “cache miss” in Codex:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Changing the &lt;code&gt;tools&lt;/code&gt;available to the model in the middle of the conversation.&lt;/item&gt;
      &lt;item&gt;Changing the &lt;code&gt;model&lt;/code&gt;that is the target of the Responses API request (in practice, this changes the third item in the original prompt, as it contains model-specific instructions).&lt;/item&gt;
      &lt;item&gt;Changing the sandbox configuration, approval mode, or current working directory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Codex team must be diligent when introducing new features in the Codex CLI that could compromise prompt caching. As an example, our initial support for MCP tools introduced a bug where we failed to enumerate the tools in a consistent order(opens in a new window), causing cache misses. Note that MCP tools can be particularly tricky because MCP servers can change the list of tools they provide on the fly via a &lt;code&gt;notifications/tools/list_changed&lt;/code&gt;(opens in a new window) notification. Honoring this notification in the middle of a long conversation can cause an expensive cache miss.&lt;/p&gt;
    &lt;p&gt;When possible, we handle configuration changes that happen mid-conversation by appending a new message to &lt;code&gt;input&lt;/code&gt; to reflect the change rather than modifying an earlier message:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the sandbox configuration or approval mode changes, we insert(opens in a new window) a new &lt;code&gt;role=developer&lt;/code&gt;message with the same format as the original&lt;code&gt;&amp;lt;permissions instructions&amp;gt;&lt;/code&gt;item.&lt;/item&gt;
      &lt;item&gt;If the current working directory changes, we insert(opens in a new window) a new &lt;code&gt;role=user&lt;/code&gt;message with the same format as the original&lt;code&gt;&amp;lt;environment_context&amp;gt;&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We go to great lengths to ensure cache hits for performance. There’s another key resource we have to manage: the context window.&lt;/p&gt;
    &lt;p&gt;Our general strategy to avoid running out of context window is to compact the conversation once the number of tokens exceeds some threshold. Specifically, we replace the &lt;code&gt;input&lt;/code&gt; with a new, smaller list of items that is representative of the conversation, enabling the agent to continue with an understanding of what has happened thus far. An early implementation of compaction(opens in a new window) required the user to manually invoke the &lt;code&gt;/compact&lt;/code&gt; command, which would query the Responses API using the existing conversation plus custom instructions for summarization(opens in a new window). Codex used the resulting assistant message containing the summary as the new &lt;code&gt;input&lt;/code&gt;(opens in a new window) for subsequent conversation turns.&lt;/p&gt;
    &lt;p&gt;Since then, the Responses API has evolved to support a special &lt;code&gt;/responses/compact&lt;/code&gt; endpoint(opens in a new window) that performs compaction more efficiently. It returns a list of items(opens in a new window) that can be used in place of the previous &lt;code&gt;input&lt;/code&gt; to continue the conversation while freeing up the context window. This list includes a special &lt;code&gt;type=compaction&lt;/code&gt; item with an opaque &lt;code&gt;encrypted_content&lt;/code&gt; item that preserves the model’s latent understanding of the original conversation. Now, Codex automatically uses this endpoint to compact the conversation when the &lt;code&gt;auto_compact_limit&lt;/code&gt;(opens in a new window) is exceeded.&lt;/p&gt;
    &lt;p&gt;We’ve introduced the Codex agent loop and walked through how Codex crafts and manages its context when querying a model. Along the way, we highlighted practical considerations and best practices that apply to anyone building an agent loop on top of the Responses API.&lt;/p&gt;
    &lt;p&gt;While the agent loop provides the foundation for Codex, it’s only the beginning. In upcoming posts, we’ll dig into the CLI’s architecture, explore how tool use is implemented, and take a closer look at Codex’s sandboxing model.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46737630</guid><pubDate>Fri, 23 Jan 2026 20:42:36 +0000</pubDate></item><item><title>TrueVault (YC W14) is hiring a Growth Lead to test different growth channels</title><link>https://www.ycombinator.com/companies/truevault/jobs/njvSGDj-growth-lead</link><description>&lt;doc fingerprint="7d1cd05f220d14af"&gt;
  &lt;main&gt;
    &lt;p&gt;We make Privacy Software for SMBs.&lt;/p&gt;
    &lt;p&gt;At TrueVault, we build software that solves real problems for real customers, and we approach growth with the same discipline and curiosity we bring to product. We’re looking for a Growth Lead who wants to build from first principles: someone who enjoys experimenting, uncovering real signal, and turning insight into momentum.&lt;/p&gt;
    &lt;p&gt;This is a hands-on role for a builder. Someone who doesn’t want to inherit a playbook, but help write the first version of one.&lt;/p&gt;
    &lt;p&gt;Privacy is one of the most fundamental rights we have.&lt;/p&gt;
    &lt;p&gt;At TrueVault, we believe that when businesses are given practical, well-designed tools, respecting consumer privacy becomes the obvious choice. That belief drives everything we build.&lt;/p&gt;
    &lt;p&gt;We create software that helps brands comply with complex privacy laws without drowning in legal costs or operational overhead. No armies of lawyers. No brittle workflows. Just software that works.&lt;/p&gt;
    &lt;p&gt;TrueVault is a Y Combinator–backed company based in San Francisco. We’re obsessive about product quality, thoughtful about the problems we choose to solve, and relentless about making our customers successful.&lt;/p&gt;
    &lt;p&gt;As Growth Lead, your work will have immediate, visible impact on the company’s trajectory. This is not a role focused on incremental optimization. You will help establish how growth works at TrueVault.&lt;/p&gt;
    &lt;p&gt;The role reports directly to the CEO, ensuring you have the support, context, and access needed to move quickly and decisively. The experiments you run and the signals you uncover will directly influence how we reach customers, how we invest our time, and where the company goes next.&lt;/p&gt;
    &lt;p&gt;If you thrive on identifying what drives growth in early-stage startups, where the product is strong but the growth playbook is still being written, this is the role.&lt;/p&gt;
    &lt;p&gt;We’re looking for someone who thrives on experimentation. Someone who sees growth as a series of questions to answer, signals to uncover, and systems to build.&lt;/p&gt;
    &lt;p&gt;As TrueVault’s first Growth Lead, your initial mission is to identify viable acquisition channels that can reliably and repeatedly drive growth. You’ll explore a wide range of channels including paid, outbound, partnerships, co-marketing, SEO, and more, running structured experiments to determine what truly works.&lt;/p&gt;
    &lt;p&gt;Your early focus will be on Acquisition, but the role expands over time. As viable channels emerge and stabilize, you’ll help shape growth across the broader AAARRR framework, influencing awareness, activation, revenue, retention, and referral.&lt;/p&gt;
    &lt;p&gt;Experimentation is central to this role. Being genuinely data-driven is critical. You'll be expected to form clear hypotheses, measure outcomes rigorously, and let evidence guide decisions.&lt;/p&gt;
    &lt;p&gt;We believe in setting clear, concrete expectations. Success in this role will be measured by outcomes such as:&lt;/p&gt;
    &lt;p&gt;This role starts with hands-on ownership and grows with impact. Initially, you'll focus on discovery and execution: running experiments, identifying viable channels, and building repeatable growth motions. As those motions scale, your scope expands.&lt;/p&gt;
    &lt;p&gt;Strong performance typically leads to broader ownership across the funnel, strategic influence, and eventually leadership of the growth function—including hiring and managing a team. The trajectory depends on the impact you create and where your strengths emerge.&lt;/p&gt;
    &lt;p&gt;This is a fully remote role, but you must be based in the United States.&lt;/p&gt;
    &lt;p&gt;This role is leveled as a Principal Individual Contributor (Pave level: P6/Carta level: L7)&lt;/p&gt;
    &lt;p&gt;Final offers may vary slightly based on experience.&lt;/p&gt;
    &lt;p&gt;To apply, please submit your application through this form. Applications submitted outside of this form will not be reviewed.&lt;/p&gt;
    &lt;p&gt;We take applicant privacy seriously. Please review our Job Applicant Privacy Policy to understand how we collect and process your personal information in accordance with applicable laws, including the CCPA.&lt;/p&gt;
    &lt;p&gt;TrueVault builds software tools that help businesses comply with consumer data privacy laws. We believe if businesses have access to products that make getting and staying compliant simple, straightforward, and fully automated, respecting consumers' data privacy becomes the sensible default. And we all benefit from that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46737870</guid><pubDate>Fri, 23 Jan 2026 21:01:34 +0000</pubDate></item><item><title>Noora Health (YC W14) Is Hiring AI/ML Engineer</title><link>https://www.ycombinator.com/companies/noora-health/jobs/2B4RxLG-ai-ml-engineer</link><description>&lt;doc fingerprint="2e02a1e67629ad2b"&gt;
  &lt;main&gt;
    &lt;p&gt;Training patients and their families with health skills&lt;/p&gt;
    &lt;p&gt;WHO WE ARE&lt;/p&gt;
    &lt;p&gt;Noora Health India Private Limited is a key partner in Noora Health’s mission is to improve outcomes and strengthen health systems by equipping family caregivers with the skills they need to care for their loved ones. They develop content, technology platforms, new products, and strengthen other operational functions that support the scale and impact of Noora Health’s programs.&lt;/p&gt;
    &lt;p&gt;Founded in 2014, Noora Health turns hospital hallways and waiting rooms into classrooms by tapping into the most compassionate resources available for the patient’s care: their own family.&lt;/p&gt;
    &lt;p&gt;With support from governments and partners in India, Bangladesh, Indonesia, and Nepal, Noora Health has trained more than 43 million caregivers and patients across 12,800+ facilities using their flagship caregiver education and training curriculum, the Care Companion Program (CCP).&lt;/p&gt;
    &lt;p&gt;In a cohort of patients, the CCP reduced post-surgical cardiac complications by 71%, maternal complications by 12%, newborn complications by 16%, and newborn mortality by 18%.&lt;/p&gt;
    &lt;p&gt;Noora Health is an Audacious Project Grantee and received the Skoll Foundation Award for Social Innovation. To learn more, watch our TED Talk, Skoll feature, or read about our partnership with the World Health Organization.&lt;/p&gt;
    &lt;p&gt;WHAT YOU WILL DO&lt;/p&gt;
    &lt;p&gt;WHO WE ARE LOOKING FOR&lt;/p&gt;
    &lt;p&gt;WHAT WE VALUE&lt;/p&gt;
    &lt;p&gt;We value diversity, equity, and inclusion, and we understand the value of developing a team with different perspectives, educational backgrounds, and life experiences. We prioritize diversity within our team, and we welcome candidates from all gender identities, castes, religious practices, sexual orientations, and abilities — among many others.&lt;/p&gt;
    &lt;p&gt;We encourage people from all backgrounds to apply.&lt;/p&gt;
    &lt;p&gt;HOW TO APPLY&lt;/p&gt;
    &lt;p&gt;Please submit your application using this link.&lt;/p&gt;
    &lt;p&gt;Noora Health trains patient families with high-impact health skills that improve outcomes and save lives. Our model provides basic yet vital care knowledge through trusted providers by creating a scalable program for caregiving education and training within the established healthcare system. This model expands the care umbrella to include those closest to the patient — their family and community. Noora Health has trained over 30 million caregivers and patients across over 12,400 healthcare facilities in India, Bangladesh, Indonesia, and Nepal. The program reduces cardiac surgery complications by 71%, newborn readmissions by 56%, and neonatal mortality by 18%. By 2028, Noora Health will expand to reach over 70 million caregivers and patients.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46740028</guid><pubDate>Sat, 24 Jan 2026 01:00:48 +0000</pubDate></item><item><title>“Let people help” – Advice that made a big difference to a grieving widow</title><link>https://www.npr.org/2026/01/20/nx-s1-5683170/let-them-the-small-bit-of-advice-that-made-a-big-difference-to-a-grieving-widow</link><description>&lt;doc fingerprint="5776d1cd93b7c751"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'Let them.' The small bit of advice that made a big difference to a grieving widow&lt;/head&gt;
    &lt;head rend="h4"&gt;'Let them.' The small bit of advice that made a big difference to a grieving widow&lt;/head&gt;
    &lt;p&gt;In 2020, Connie Sherburne's husband of 31 years, Peter, died in a crash while piloting their small plane.&lt;/p&gt;
    &lt;p&gt;Sherburne, a practical person, immediately focused on what needed to be done.&lt;/p&gt;
    &lt;p&gt;"I went home that evening, and the next morning, I got up and I thought, 'Hit the ground running. You've got all this to take care of.'"&lt;/p&gt;
    &lt;p&gt;She drove to their insurance company in Escondido, Calif., to transfer the insurance for Peter's truck into her name. Because of the pandemic, the office was staffed by just one person. The woman opened the door to let Sherburne inside, and they sat across from each other at a desk.&lt;/p&gt;
    &lt;p&gt;"It didn't take her but a few minutes to take care of why I was there," Sherburne recalled.&lt;/p&gt;
    &lt;p&gt;"When she got done, she stopped and she looked at me across her desk, and she made sure that I was looking at her — that she had my full attention."&lt;/p&gt;
    &lt;p&gt;"And she said, 'OK, so now that we've finished with that, people are going to stop and ask you, "How can I help?"' And then she gave this pregnant pause, and she said, 'Let them.'"&lt;/p&gt;
    &lt;p&gt;Sherburne took in the advice, absorbing the moment. She wasn't the kind of person to reach out to others for help.&lt;/p&gt;
    &lt;p&gt;"But because she said it with such force, it really, really made sense to me," Sherburne said.&lt;/p&gt;
    &lt;p&gt;As soon as she got home, she realized that she did need help. The firewood she used to heat her home was running low, and winter was around the corner. Chopping wood was something Peter had always done. But this time, she asked a friend to take care of it.&lt;/p&gt;
    &lt;p&gt;"So many, many people did little things and big things to help me," Sherburne said. "One of the neighbors actually cooked for me for four years — dinners — and her husband delivered the dinners to me."&lt;/p&gt;
    &lt;p&gt;Sherburne says she would have never reached out for support if not for the advice of the woman at the insurance company.&lt;/p&gt;
    &lt;p&gt;"In the back of my mind, I kept hearing her voice, you know, 'Let them.'"&lt;/p&gt;
    &lt;p&gt;A few years later, Sherburne went back to the insurance office to tell the woman how deeply her words had affected her. But the agent didn't work there anymore.&lt;/p&gt;
    &lt;p&gt;"So I just wanted to tell her how much that meant to me," Sherburne said.&lt;/p&gt;
    &lt;p&gt;"It was such a little thing for this woman just to say that to me. But she didn't realize what a huge thing it was going to be to help me through all this."&lt;/p&gt;
    &lt;p&gt;My Unsung Hero is also a podcast — new episodes are released every Tuesday. To share the story of your unsung hero with the Hidden Brain team, record a voice memo on your phone and send it to myunsunghero@hiddenbrain.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46740748</guid><pubDate>Sat, 24 Jan 2026 03:20:51 +0000</pubDate></item><item><title>80386 Multiplication and Division</title><link>https://nand2mario.github.io/posts/2026/80386_multiplication_and_division/</link><description>&lt;doc fingerprint="877cd3abcb32029b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;80386 Multiplication and Division&lt;/head&gt;
    &lt;p&gt;When Intel released the 80386 in October 1985, it marked a watershed moment for personal computing. The 386 was the first 32-bit x86 processor, increasing the register width from 16 to 32 bits and vastly expanding the address space compared to its predecessors. This wasn't just an incremental upgrade—it was the foundation that would carry the PC architecture for decades to come.&lt;/p&gt;
    &lt;p&gt;The timing was significant. By the mid-1980s, the IBM PC had established x86 as the dominant PC architecture, but the 16-bit 8086/286 processors were hitting their limits. Memory was constrained to 1MB (or 16MB with the 286's limited protected mode). Competing 32-bit architectures like the Motorola 68020 threatened Intel's dominance. The 386 was Intel's answer: full 32-bit computing with backward compatibility for the massive library of existing DOS software.&lt;/p&gt;
    &lt;p&gt;The 386 introduced important and long-lasting x86 features: a flat 4GB address space, virtual memory with paging, and a protected mode that actually worked. It would go on to run Windows 3.0, Windows 95, early Linux, and countless other operating systems that shaped modern computing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Faster arithmetic&lt;/head&gt;
    &lt;p&gt;In addition to its architectural advances, the 386 delivered a major jump in arithmetic performance. On the earlier 8086, multiplication and division were slow — 16-bit multiplication typically required 120–130 cycles, with division taking even longer at over 150 cycles. The 286 significantly improved on this by introducing faster microcode routines and modest hardware enhancements.&lt;/p&gt;
    &lt;p&gt;The 386 pushed performance further with dedicated hardware that processes multiplication and division at the rate of one bit per cycle, combined with a native 32-bit datapath width. The microcode still orchestrates the operation, but the heavy lifting happens in specialized datapath logic that advances every cycle.&lt;/p&gt;
    &lt;p&gt;Here are the actual cycle counts from the Intel 386 Programmer's Reference Manual:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;8-bit&lt;/cell&gt;
        &lt;cell role="head"&gt;16-bit&lt;/cell&gt;
        &lt;cell role="head"&gt;32-bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MUL&lt;/cell&gt;
        &lt;cell&gt;9-14&lt;/cell&gt;
        &lt;cell&gt;9-22&lt;/cell&gt;
        &lt;cell&gt;9-38&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;IMUL&lt;/cell&gt;
        &lt;cell&gt;9-14&lt;/cell&gt;
        &lt;cell&gt;9-22&lt;/cell&gt;
        &lt;cell&gt;9-38&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DIV&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;22&lt;/cell&gt;
        &lt;cell&gt;38&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IDIV&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The ranges for MUL/IMUL reflect an "early-out" optimization—the loop exits early when the remaining multiplier bits are all zeros (or all ones for signed). Division has no early-out, so cycle counts are fixed at roughly &lt;code&gt;width + overhead&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To save silicon, the 386 reuses the main ALU for the per-iteration add/subtract work rather than having a separate multiplier unit. The microcode controls the iteration, while dedicated datapath logic handles the shifting and loop termination. Let's look at how these algorithms work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Add-and-shift multiplication&lt;/head&gt;
    &lt;p&gt;The classic multiplication algorithm in processors is the Booth algorithm. However, the 80386 does not use that. Instead, an "add-and-shift" multiplication algorithm is used. This is similar to grade-school long multiplication. The difference is that instead of moving from lower digits to higher, we shift to the right. Here's the data layout:&lt;/p&gt;
    &lt;p&gt;Three key internal registers participate in multiplication: MULTMP, TMPB, and SIGMA. A notable challenge is that x86 instructions support 8-bit, 16-bit, and 32-bit operands. Consistent with the design philosophy of the 8086, the 386 achieves this flexibility by reusing the same registers and microcode routines for all operand sizes. In most cases, the identical hardware and microcode sequence accommodate different widths seamlessly. The diagram above shows how, for example, the result of multiplying two 16-bit numbers is arranged within a 32-bit product: it occupies the lower half of the SIGMA register and the upper half of TMPB.&lt;/p&gt;
    &lt;p&gt;Here is the multiplication algorithm in pseudocode:&lt;/p&gt;
    &lt;code&gt;1: COUNTR = width-1
2: while (true):
3:   if (TMPB[0]) SIGMA &amp;lt;= SIGMA + MULTMP
4:   {SIGMA, TMPB} &amp;gt;&amp;gt;= 1      // arithmetic shift for signed
5:   if (--COUNTR==0) break
6:   if (remaining TMPB bits are all 0 or all 1 for signed) break
7: {SIGMA, TMPB} &amp;gt;&amp;gt;= COUNTR   // compensate for early exit
8: correction for signed multiplication
&lt;/code&gt;
    &lt;p&gt;Shifting to the right rather than the left simplifies the hardware circuits. Line 6 implements the important "early-out" optimization, which means the loop can terminate early if the remaining multiplier bits are all zeros—or all ones, in the case of signed multiplication. When this happens, line 7 adjusts for the early exit by shifting the accumulated result right by the number of remaining COUNTR bits.&lt;/p&gt;
    &lt;p&gt;Lines 1–7 fully describe unsigned multiplication. To extend this to signed multiplication, only a few tweaks are needed: use arithmetic (not logical) shifts on lines 4 and 7, and, as a final correction in line 8, subtract the multiplicand from the upper product register (SIGMA) if the multiplier was negative. For a deeper dive into the mathematics, see college-level computer organization resources such as this one.&lt;/p&gt;
    &lt;p&gt;The 80386 multiplication microcode closely mirrors the algorithm described above, and shows both the timing and the likely underlying hardware involved. The microcode routine shown here handles register-based multiplication—both unsigned and signed—and supports all three operand sizes: 8, 16, and 32 bits. Other forms, such as multiplying with a memory operand, are implemented similarly.&lt;/p&gt;
    &lt;p&gt;Before we examine the code, it’s helpful to quickly review the 80386’s microcode syntax and conventions. While the 8086 used 21-bit micro-operations, the 80386 expanded these to 37 bits, adding fields to control more complex hardware functionality. Moves are written as &lt;code&gt;src-&amp;gt;dest&lt;/code&gt;, which simply means copying data from one register to another. The &lt;code&gt;alujmp&lt;/code&gt; field directs either the ALU (using &lt;code&gt;src&lt;/code&gt; and &lt;code&gt;alu_src&lt;/code&gt; as inputs) or the microcode control flow, handling everything from arithmetic to jumps to indirect operations (&lt;code&gt;alu_src&lt;/code&gt; as the jump target). Pay special attention to the &lt;code&gt;RPT&lt;/code&gt; keyword found on the third line of the upcoming listing: this signals the microcode sequencer to repeatedly execute a micro-instruction, decrementing the COUNTR register each time, and continuing until COUNTR reaches zero, i.e. looping for COUNTR+1 iterations.&lt;/p&gt;
    &lt;code&gt;; MUL/IMUL r
; src     dest    alu_src        alujmp  uop sub busop
DSTREG -&amp;gt; MULTMP  BITS_V         LDCNTR          ; MULTMP=r (multiplicand), COUNTR=width-1
eAX_AL -&amp;gt; TMPB    0              PASS2           ; TMPB=multiplier (AL/AX/EAX)
SIGMA             TMPB           IMUL3   RPT DLY ; hardware mult loop with early-out 
SIGMA                            PASS            ; pass through SIGMA
COUNTR -&amp;gt; TMPD                                   ; save remaining COUNTR
RESULT -&amp;gt; TMPC    TMPD           LDBSR8          ; load shift count: right shift, COUNTR
SIGMA  -&amp;gt; TMPD    TMPC           SHIFT           ; shift {SIGMA,RESULT} to get low result
SIGMA  -&amp;gt; eAX_AL  TMPD           MULFIX          ; write low result, set flags, signed mult correction
SIGMA             TMPD           SHIFT   RNI     ; shift {0,ProdU} to get high result
SIGMA  -&amp;gt; eDX_AH                                 ; write high result
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;RESULT&lt;/code&gt; register is used by both multiplication and division. For multiplication, it accumulates the lower half of the product as bits shift right out of TMPB during the loop. &lt;code&gt;MULFIX&lt;/code&gt; is the correction for signed multiplication on pseudocode line 8.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other variants&lt;/head&gt;
    &lt;p&gt;The 386 introduced two new forms of IMUL beyond the original single-operand form:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two-operand: &lt;code&gt;IMUL reg, r/m&lt;/code&gt;- multiplies reg by r/m, stores in reg (single-width result)&lt;/item&gt;
      &lt;item&gt;Three-operand: &lt;code&gt;IMUL reg, r/m, imm&lt;/code&gt;- multiplies r/m by immediate, stores in reg (single-width result)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These variants are interesting because they only produce a single-width result (discarding the upper half), making them faster for common cases where overflow isn't expected. The microcode for these uses a slightly different entry point that skips writing the upper result to EDX/DX/AH.&lt;/p&gt;
    &lt;head rend="h2"&gt;Division&lt;/head&gt;
    &lt;p&gt;80386 uses the standard non-restoring division algorithm for division. Here's the data layout:&lt;/p&gt;
    &lt;p&gt;The dividend is {SIGMA, DIVTMP} (max 64 bits), while the divisor is TMPB (max 32 bits). Each iteration shifts the dividend left by one bit and either adds or subtracts the divisor, building up the quotient in RESULT one bit at a time.&lt;/p&gt;
    &lt;code&gt;1: do:                               // loop body is DIV7
2:     {SIGMA,DIVTMP} &amp;lt;&amp;lt;= 1;
3:     if (SIGMA &amp;lt; 0) SIGMA += TMPB;
4:     else           SIGMA -= TMPB;
5:     RESULT = (RESULT &amp;lt;&amp;lt; 1) | (SIGMA &amp;gt;= 0 ? 1 : 0)
6:     COUNTR--;
7: while (COUNTR &amp;gt; 0)
8: if (SIGMA &amp;lt; 0) SIGMA += TMPB;     // DIV5
&lt;/code&gt;
    &lt;p&gt;Let's look at the division routine (&lt;code&gt;DIV r&lt;/code&gt; at F6.6/F7.6) directly.&lt;/p&gt;
    &lt;code&gt;; DIV r
eAX_AL -&amp;gt; DIVTMP  BITS_V         LDCNTR          ; DIVTMP = lower half of dividend, COUNTR=width-1
eDX_AH                           PASS            ; SIGMA = upper half of dividend
DSTREG -&amp;gt; TMPB                                   ; TMPB = divisor
SIGMA             TMPB            DIV7   RPT DLY ; Loop: dividend={SIGMA,DIVTMP}, divisor=TMPB
SIGMA             TMPB            DIV5           ; Final correction
SIGMA                            PASS            ; Preserve remainder through ALU
RESULT -&amp;gt; eAX_AL                         RNI     ; accumulator = quotient 
SIGMA  -&amp;gt; eDX_AH                                 ; upper-half reg = remainder
&lt;/code&gt;
    &lt;p&gt;DIV7 and DIV5 are both single-cycle micro-operations. DIV7 implements the core of the division loop, corresponding to pseudocode lines 2–5 (excluding the COUNTR decrement). With each iteration, DIV7 updates SIGMA (the remainder) and RESULT (the quotient accumulator). The loop is controlled by the RPT instruction, which keeps the sequencer repeatedly executing DIV7 for COUNTR+1 iterations—there’s no early exit for division. After completing the main loop, DIV5 performs the final correction required by the non-restoring division algorithm (pseudocode line 8).&lt;/p&gt;
    &lt;head rend="h3"&gt;Signed division (IDIV)&lt;/head&gt;
    &lt;p&gt;IDIV is more complex than DIV because it must handle signs. The approach is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Convert dividend and divisor to absolute values&lt;/item&gt;
      &lt;item&gt;Perform unsigned division&lt;/item&gt;
      &lt;item&gt;Adjust signs of quotient and remainder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here's the IDIV microcode:&lt;/p&gt;
    &lt;code&gt;; IDIV r
-1                BITS_V         ADD             ; COUNTR=width-2
SIGMA  -&amp;gt; COUNTR
eDX_AH                           PASS            ; SIGMA=upper dividend
eAX_AL -&amp;gt; DIVTMP                                 ; DIVTMP=lower dividend
DSTREG -&amp;gt; TMPB                                   ; TMPB=divisor
SIGMA             TMPB           PREDIV          ; |dividend|divisor|, save signs, first iteration
SIGMA             TMPB            DIV7   RPT DLY ; main division loop
SIGMA             TMPB            DIV5           ; non-restoring correction
SIGMA             TMPB           IDIV1           ; correct remainder sign
SIGMA                            PASS
SIGMA  -&amp;gt; TMPB                                   ; save remainder
RESULT                           IDIV2           ; correct quotient sign -&amp;gt; SIGMA
TMPB   -&amp;gt; eDX_AH                         RNI     ; write remainder
SIGMA  -&amp;gt; eAX_AL                                 ; write quotient
&lt;/code&gt;
    &lt;p&gt;The key micro-ops are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PREDIV: Computes absolute values of dividend and divisor, saves their signs in internal flip-flops, and performs the first division iteration&lt;/item&gt;
      &lt;item&gt;IDIV1: Corrects the remainder's sign (remainder has same sign as dividend)&lt;/item&gt;
      &lt;item&gt;IDIV2: Corrects the quotient's sign (negative if operand signs differ)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This explains why IDIV takes 5 more cycles than DIV - the extra cycles handle sign computation and correction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional notes&lt;/head&gt;
    &lt;p&gt;One of the biggest hurdles in deciphering CPUs is interpreting the role and meaning of each micro-operation and constant. Their interdependence often makes the process both challenging and fascinating. Consider BITS_V: at first glance, seeing it used in LDCNTR and loop logic, you might assume it simply represents the instruction’s bit width—such as 8, 16, or 32—meaning the RPT instruction would run for that number of COUNTR cycles. This approach seems to suffice for MUL and DIV. However, when applied to IDIV and AAM, the microcode repeatedly failed to function as expected. After many hours spent troubleshooting, I finally came across a clue in a seemingly unrelated part of the microcode:&lt;/p&gt;
    &lt;code&gt;; PUSHAd
ESP               BITS_V         SUB         DLY      0    
SIGMA     INDSTK  -1             ADD             IN=+      
...
SIGMA  -&amp;gt; eSP                                DLY           
&lt;/code&gt;
    &lt;p&gt;This finally gave me the hint that BITS_V is &lt;code&gt;width-1&lt;/code&gt; instead of &lt;code&gt;width&lt;/code&gt;. Here PUSHA pushes 8 registers to the stack, so SP should be subtracted by &lt;code&gt;8*2=16&lt;/code&gt; or &lt;code&gt;8*4=32&lt;/code&gt; bytes. The existence of &lt;code&gt;SIGMA-1&lt;/code&gt; (SIGMA, -1, ADD) after &lt;code&gt;SIGMA=ESP-BITS_V&lt;/code&gt; (ESP, BITS_V, SUB) clearly indicates that BITS_V is one less than 16 or 32.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparison with modern CPUs&lt;/head&gt;
    &lt;p&gt;The 386's iterative approach to multiplication and division was state-of-the-art for its time, but modern x86 processors have moved far beyond it:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Era&lt;/cell&gt;
        &lt;cell role="head"&gt;Processor&lt;/cell&gt;
        &lt;cell role="head"&gt;32-bit MUL&lt;/cell&gt;
        &lt;cell role="head"&gt;32-bit DIV&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1985&lt;/cell&gt;
        &lt;cell&gt;80386&lt;/cell&gt;
        &lt;cell&gt;9-38 cycles&lt;/cell&gt;
        &lt;cell&gt;38 cycles&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1993&lt;/cell&gt;
        &lt;cell&gt;Pentium&lt;/cell&gt;
        &lt;cell&gt;10 cycles&lt;/cell&gt;
        &lt;cell&gt;41 cycles&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2000s&lt;/cell&gt;
        &lt;cell&gt;Core 2&lt;/cell&gt;
        &lt;cell&gt;3-4 cycles&lt;/cell&gt;
        &lt;cell&gt;17-41 cycles&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2020s&lt;/cell&gt;
        &lt;cell&gt;Zen 3/Alder Lake&lt;/cell&gt;
        &lt;cell&gt;3-4 cycles&lt;/cell&gt;
        &lt;cell&gt;13-19 cycles&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Modern CPUs use dedicated multiplier arrays (often Booth-encoded Wallace trees) that can multiply 64-bit numbers in just a few cycles. Division remains slower because it's inherently sequential - each quotient bit depends on the previous remainder. However, modern CPUs use radix-4 or radix-16 division (computing 2-4 bits per cycle) and sophisticated prediction to speed things up.&lt;/p&gt;
    &lt;p&gt;The 386's "one bit per cycle" approach is elegant in its simplicity and its reuse of the main ALU. For an FPGA implementation, this microcode-driven design is actually quite practical - it minimizes hardware while still achieving reasonable performance.&lt;/p&gt;
    &lt;p&gt;Follows me on X (@nand2mario) for updates, or use RSS.&lt;/p&gt;
    &lt;p&gt;Credits: This analysis of the 80386 draws on the microcode disassembly and silicon reverse engineering work of reenigne, gloriouscow, smartest blob, and Ken Shirriff. For a detailed examination of the silicon itself, see Ken Shirriff’s silicon reverse engineering series on the 80386.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46741482</guid><pubDate>Sat, 24 Jan 2026 06:11:42 +0000</pubDate></item><item><title>Telli (YC F24) is hiring eng, design, growth [on-site, Berlin]</title><link>https://careers.telli.com/</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46741643</guid><pubDate>Sat, 24 Jan 2026 07:01:13 +0000</pubDate></item><item><title>Doing gigabit Ethernet over my British phone wires</title><link>https://thehftguy.com/2026/01/22/doing-gigabit-ethernet-over-my-british-phone-wires/</link><description>&lt;doc fingerprint="e1c9085f6b475eff"&gt;
  &lt;main&gt;
    &lt;p&gt;Disclaimer: None of this is written by AI, I’m still a real person writing my own blog like its 1999&lt;/p&gt;
    &lt;p&gt;I finally figured out how to do Gigabit Ethernet over my existing phone wires.&lt;/p&gt;
    &lt;head rend="h2"&gt;Powerline adapter and misery&lt;/head&gt;
    &lt;p&gt;I’ve mostly lived with powerline adapters over recent years. Some worked well, some did not (try few and return what doesn’t work in your home). One I had for a while gave me stable 30 Mbps, which was little but good enough for internet at the time. I care very much about having stable low latency for gaming, more than bandwidth.&lt;/p&gt;
    &lt;p&gt;Fast forward to my current situation, that powerline adapter regularly lost connection which was a major problem. I got some new ones with the latest and greatest G.hn 2400 standard. The final contender served around 180 Mbps to my office (with high variance 120 to 280 Mbps), or around 80 Mbps to the top floor. It’s good enough to watch YouTube/TV yet it’s far from impressive.&lt;/p&gt;
    &lt;p&gt;One peculiar thing from the UK: Internet providers don’t truly offer gigabit internet. They have a range of deals like 30 Mbps – 75 Mbps – 150 Mbps – 300 Mbps – 500 Mbps – 900 Mbps, each one costing a few more pounds per month than the last. This makes the UK simultaneously one of the cheapest and one of the most expensive countries to get Internet.&lt;/p&gt;
    &lt;p&gt;Long story short, new place, new hardware, new deals, the internet has been running at 500 Mbps for some time now.&lt;/p&gt;
    &lt;p&gt;Every 50 GB of Helldivers 2 update (because these idiots shipped the same content in duplicate 5 times) is a painful reminder that the setup is not operating at capacity.&lt;/p&gt;
    &lt;p&gt;Problem: How to get 500 Mbps to my room?&lt;/p&gt;
    &lt;head rend="h2"&gt;A Fetish for Phone Sockets&lt;/head&gt;
    &lt;p&gt;I’ve been looking for a way to reuse phone wires for a while, because British houses are full of phone sockets. There are 2 sockets in my office room.&lt;/p&gt;
    &lt;p&gt;I can’t stress enough how much we love our phone sockets. It’s not uncommon to have a one bed flat with 2 phone sockets in the living room and 2 phone sockets in the bedroom and a master socket in the technical room. It’s ridiculous.&lt;/p&gt;
    &lt;p&gt;A new house bought today could have 10 phone sockets and 0 Ethernet sockets. There is still no regulation that requires new build to get Ethernet wiring (as far as I know).&lt;/p&gt;
    &lt;p&gt;There’s got to be a way to use the existing phone infrastructure.&lt;/p&gt;
    &lt;p&gt;I know the technology exists. It’s one of the rare cases where the technology exists and is mature, but nobody can be bothered to make products for it.&lt;/p&gt;
    &lt;p&gt;The standards that run powerline adapters (HomePlug AV200, AV500, G.hn 2400) can work with any pair of wires. It should work ten times better on dedicated phone wires instead of noisy power wires, if only manufacturers could be bothered to pull their fingers out of their arse and make the products that are needed.&lt;/p&gt;
    &lt;p&gt;After countless years of research, I finally found one German manufacturer that’s making what needs to be made https://www.gigacopper.net/wp/en/home-networking/&lt;/p&gt;
    &lt;head rend="h2"&gt;Ordering&lt;/head&gt;
    &lt;p&gt;It’s made and shipped from Germany.&lt;/p&gt;
    &lt;p&gt;I was lazy so I ordered online in self-service (which is definitely the wrong way to go about it). It’s available on Ebay DE and Amazon DE, it’s possible to order from either with a UK account, make sure to enter a UK address for delivery (some items don’t allow it).&lt;/p&gt;
    &lt;p&gt;The better approach is almost certainly to speak to the seller to get a quote, with international shipping and the import invoice excluding VAT (to avoid paying VAT on VAT).&lt;/p&gt;
    &lt;head rend="h2"&gt;Delivery Hell&lt;/head&gt;
    &lt;p&gt;The package got the usual Royal Mail treatment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The package was shipped by DHL Germany&lt;/item&gt;
      &lt;item&gt;The package was transferred to Royal Mail when entering the UK&lt;/item&gt;
      &lt;item&gt;After some days, the DHL website said they tried to deliver but nobody home, this is bullshit&lt;/item&gt;
      &lt;item&gt;Royal website said the package reached the depot and was awaiting delivery, this is bullshit&lt;/item&gt;
      &lt;item&gt;In reality, the package was stuck at the border, as usual&lt;/item&gt;
      &lt;item&gt;Google to find “website to pay import fee on parcel”&lt;/item&gt;
      &lt;item&gt;https://www.royalmail.com/receiving-mail/pay-a-fee&lt;/item&gt;
      &lt;item&gt;Entered the DHL tracking number into the Royal Mail form for a Royal Mail tracking number&lt;/item&gt;
      &lt;item&gt;The website said that the parcel had import fees to pay, this is correct&lt;/item&gt;
      &lt;item&gt;Paid the fee online, 20% VAT + a few pounds of handling fees&lt;/item&gt;
      &lt;item&gt;The package will be scheduled for delivery a few days later&lt;/item&gt;
      &lt;item&gt;Royal Mail and DHL updated their status another two or three times with false information&lt;/item&gt;
      &lt;item&gt;Royal Mail delivered a letter saying there was a package waiting on fees, though it was paid&lt;/item&gt;
      &lt;item&gt;The package finally arrived&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Basically, you need to follow the tracking regularly until the package is tagged as lost or failed delivery, which is the cue to pay import fees.&lt;/p&gt;
    &lt;p&gt;It’s the normal procedure to buy things from Europe since Brexit 2020. It’s actually quite shocking that Royal Mail still hasn’t updated their tracking system to be able to give a status “waiting on import fees to be paid online”. They had 6 years!&lt;/p&gt;
    &lt;head rend="h2"&gt;Items&lt;/head&gt;
    &lt;p&gt;This is the gigacopper G4201TM: 1 RJ11 phone line, 1 RJ45 gigabit Ethernet port, 1 power&lt;/p&gt;
    &lt;p&gt;Shopping list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A pair of gigacopper G4201TM&lt;/item&gt;
      &lt;item&gt;The device has a German power socket (expected)&lt;/item&gt;
      &lt;item&gt;It came with a German to UK power adapter (unexpected and useful)&lt;/item&gt;
      &lt;item&gt;It came with a standard RJ11 cable (expected and useless)&lt;/item&gt;
      &lt;item&gt;Found BT631A to RJ11 cables online (the standard UK phone socket)&lt;/item&gt;
      &lt;item&gt;Found Ethernet cables in my toolbox&lt;/item&gt;
      &lt;item&gt;3M removable hanging strip to stick to the wall, the device is very light&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is a gigacopper G4202TM: with an RJ45 to connect to the phone line instead of a RJ11 (not sure if it’s a newer model or just a variant, as that one has two gigabit Ethernet ports). Don’t be confused by having a RJ45 port that is not a RJ45 port.&lt;/p&gt;
    &lt;p&gt;There is a gigacopper G4201C (1 port) and G4204C (4 port) for Ethernet over coaxial. Some countries have coax in every room for TV/satellite. This may be of interest to some readers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing&lt;/head&gt;
    &lt;p&gt;Plugged it and it works!&lt;/p&gt;
    &lt;p&gt;Full speed achieved.&lt;/p&gt;
    &lt;p&gt;Reminder, this is a 500 Mbps internet connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;InHome Variant&lt;/head&gt;
    &lt;p&gt;I discovered soon afterwards that I bought the wrong item. There is an InHome and a Client/Server variant of the product. Make sure to buy the InHome variant.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The InHome variant can have up to 16 devices, communicating to any peer on the medium, with sub millisecond latency.&lt;/item&gt;
      &lt;item&gt;The client-server variant is preconfigured as a pair, splitting the bandwidth 70% download / 30% upload, with few milliseconds latency. I think it’s a use case for ISP and long range connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thankfully the difference is only the firmware. I spoke to the vendor who was very helpful and responsive. They sent me the firmware and the tools to patch.&lt;/p&gt;
    &lt;p&gt;I have a fetish for low latency. This screenshot is oddly satisfying.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gigabit&lt;/head&gt;
    &lt;p&gt;The web interface says 1713 Mbps on the physical layer, the debugging tool says PHONE 200MHz – Connected 1385 Mbps.&lt;/p&gt;
    &lt;p&gt;I wanted to verify whether the device can do a full Gigabit. Unfortunately I realized I don’t have any device that can test that.&lt;/p&gt;
    &lt;p&gt;Phones are wireless, which is too slow to test anything. I checked out of curiosity, my phone did 100 Mbps to 400 Mbps right next to the router. Grabbed two laptops only to realize they didn’t have any Ethernet port. I dug up an old laptop from storage with an Ethernet port. The laptop couldn’t boot, the CPU fan didn’t start and the laptop refused to boot with a dead fan.&lt;/p&gt;
    &lt;p&gt;There is a hard lesson here: 1 Gbps ought to be enough for any home. Using the phone line is as good as having Ethernet wiring through the house if it can deliver a (shared) 1.7 Gbps link to multiple rooms.&lt;/p&gt;
    &lt;p&gt;Still, I really wanted to verify that the device can do a full Gbps, I procured an USB-C to Ethernet adapter.&lt;/p&gt;
    &lt;p&gt;It works!&lt;/p&gt;
    &lt;p&gt;Full speed achieved, testing from a phone to a computer with iperf3.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wiring&lt;/head&gt;
    &lt;p&gt;Some readers might wonder about the wiring.&lt;/p&gt;
    &lt;p&gt;I didn’t check the wiring before buying anything because it’s pointless. British sockets are always daisy chained in an incomprehensible maze.&lt;/p&gt;
    &lt;p&gt;Phone sockets need 2 wires and can be daisy chained. Ethernet sockets need 8 wires. They often use the same Cat5 cable because it’s the most widely available (8 wires cable, the 6 extra wires can remain unconnected).&lt;/p&gt;
    &lt;p&gt;It’s possible to swap the phone socket for an RJ45 socket, if you only have 2 sockets connected with the right cable. It’s not possible when sockets are daisy chained. (You could put a double or triple RJ45 socket with a switch to break a daisy chain, but it quickly becomes impractical in a British house with 5 to 10 sockets in an arbitrary layout.)&lt;/p&gt;
    &lt;p&gt;I opened one socket in the office room. There are two Cat5 cables daisy chained. There are 3 wires connected.&lt;/p&gt;
    &lt;p&gt;It’s probably daisy chained with the other socket in the room, or it’s daisy chained with the socket in the other room that’s closer. Who knows.&lt;/p&gt;
    &lt;p&gt;I opened the BT master socket in the technical room. It should have the cables coming from the other rooms. It should connect the internal phone wires with the external phone line.&lt;/p&gt;
    &lt;p&gt;There is one single Cat5 cable. There are 4 wires connected. It’s definitely not a master socket. WTF?!&lt;/p&gt;
    &lt;p&gt;It’s interesting that this socket has 4 wires connected but the socket in the office has 3 wires connected. The idiot who did the wiring was inconsistent. The gigacopper device can operate over 2 wires (200 MHz Phone SISO) or over 4 wires (100 MHz Phone MIMO). I can try the other modes if I finish the job.&lt;/p&gt;
    &lt;p&gt;The search for the master socket continues. The cables from the other floors should all be coming down somewhere around here. There is a blank plate next to it (right).&lt;/p&gt;
    &lt;p&gt;This might be the external phone line? A bunch of wires are crimped together, colours do not match. It’s the hell of a mess.&lt;/p&gt;
    &lt;p&gt;Only sure thing, they are different cables because they are different colours. They might be going to a junction box somewhere else. Probably behind a wall that’s impossible to access!&lt;/p&gt;
    &lt;p&gt;Conclusion: There is zero chance to get proper Ethernet wiring out of this mess.&lt;/p&gt;
    &lt;p&gt;The gigacopper device to do gigabit Ethernet over phone line is a miracle!&lt;/p&gt;
    &lt;p&gt;There is an enormous untapped market for gigabit Ethernet over phone sockets in the UK.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46742362</guid><pubDate>Sat, 24 Jan 2026 10:14:31 +0000</pubDate></item><item><title>How I Estimate Work as a Staff Software Engineer</title><link>https://www.seangoedecke.com/how-i-estimate-work/</link><description>&lt;doc fingerprint="bd5fd94228e61343"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I estimate work as a staff software engineer&lt;/head&gt;
    &lt;p&gt;There’s a kind of polite fiction at the heart of the software industry. It goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Estimating how long software projects will take is very hard, but not impossible. A skilled engineering team can, with time and effort, learn how long it will take for them to deliver work, which will in turn allow their organization to make good business plans.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is, of course, false. As every experienced software engineer knows, it is not possible to accurately estimate software projects. The tension between this polite fiction and its well-understood falseness causes a lot of strange activity in tech companies.&lt;/p&gt;
    &lt;p&gt;For instance, many engineering teams estimate work in t-shirt sizes instead of time, because it just feels too obviously silly to the engineers in question to give direct time estimates. Naturally, these t-shirt sizes are immediately translated into hours and days when the estimates make their way up the management chain.&lt;/p&gt;
    &lt;p&gt;Alternatively, software engineers who are genuinely trying to give good time estimates have ridiculous heuristics like “double your initial estimate and add 20%“. This is basically the same as giving up and saying “just estimate everything at a month”.&lt;/p&gt;
    &lt;p&gt;Should tech companies just stop estimating? One of my guiding principles is that when a tech company is doing something silly, they’re probably doing it for a good reason. In other words, practices that appear to not make sense are often serving some more basic, illegible role in the organization. So what is the actual purpose of estimation, and how can you do it well as a software engineer?&lt;/p&gt;
    &lt;head rend="h3"&gt;Why estimation is impossible&lt;/head&gt;
    &lt;p&gt;Before I get into that, I should justify my core assumption a little more. People have written a lot about this already, so I’ll keep it brief.&lt;/p&gt;
    &lt;p&gt;I’m also going to concede that sometimes you can accurately estimate software work, when that work is very well-understood and very small in scope. For instance, if I know it takes half an hour to deploy a service1, and I’m being asked to update the text in a link, I can accurately estimate the work at something like 45 minutes: five minutes to push the change up, ten minutes to wait for CI, thirty minutes to deploy.&lt;/p&gt;
    &lt;p&gt;For most of us, the majority of software work is not like this. We work on poorly-understood systems and cannot predict exactly what must be done in advance. Most programming in large systems is research: identifying prior art, mapping out enough of the system to understand the effects of changes, and so on. Even for fairly small changes, we simply do not know what’s involved in making the change until we go and look.&lt;/p&gt;
    &lt;p&gt;The pro-estimation dogma says that these questions ought to be answered during the planning process, so that each individual piece of work being discussed is scoped small enough to be accurately estimated. I’m not impressed by this answer. It seems to me to be a throwback to the bad old days of software architecture, where one architect would map everything out in advance, so that individual programmers simply had to mechanically follow instructions. Nobody does that now, because it doesn’t work: programmers must be empowered to make architectural decisions, because they’re the ones who are actually in contact with the code2. Even if it did work, that would simply shift the impossible-to-estimate part of the process backwards, into the planning meeting (where of course you can’t write or run code, which makes it near-impossible to accurately answer the kind of questions involved).&lt;/p&gt;
    &lt;p&gt;In short: software engineering projects are not dominated by the known work, but by the unknown work, which always takes 90% of the time. However, only the known work can be accurately estimated. It’s therefore impossible to accurately estimate software projects in advance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Estimates do not come from engineers&lt;/head&gt;
    &lt;p&gt;Estimates do not help engineering teams deliver work more efficiently. Many of the most productive years of my career were spent on teams that did no estimation at all: we were either working on projects that had to be done no matter what, and so didn’t really need an estimate, or on projects that would deliver a constant drip of value as we went, so we could just keep going indefinitely3.&lt;/p&gt;
    &lt;p&gt;In a very real sense, estimates aren’t even made by engineers at all. If an engineering team comes up with a long estimate for a project that some VP really wants, they will be pressured into lowering it (or some other, more compliant engineering team will be handed the work). If the estimate on an undesirable project - or a project that’s intended to “hold space” for future unplanned work - is too short, the team will often be encouraged to increase it, or their manager will just add a 30% buffer.&lt;/p&gt;
    &lt;p&gt;One exception to this is projects that are technically impossible, or just genuinely prohibitively difficult. If a manager consistently fails to pressure their teams into giving the “right” estimates, that can send a signal up that maybe the work can’t be done after all. Smart VPs and directors will try to avoid taking on technically impossible projects.&lt;/p&gt;
    &lt;p&gt;Another exception to this is areas of the organization that senior leadership doesn’t really care about. In a sleepy backwater, often the formal estimation process does actually get followed to the letter, because there’s no director or VP who wants to jump in and shape the estimates to their ends. This is one way that some parts of a tech company can have drastically different engineering cultures to other parts. I’ll let you imagine the consequences when the company is re-orged and these teams are pulled into the spotlight.&lt;/p&gt;
    &lt;p&gt;Estimates are political tools for non-engineers in the organization. They help managers, VPs, directors, and C-staff decide on which projects get funded and which projects get cancelled.&lt;/p&gt;
    &lt;head rend="h3"&gt;Estimates define the work, not the other way around&lt;/head&gt;
    &lt;p&gt;The standard way of thinking about estimates is that you start with a proposed piece of software work, and you then go and figure out how long it will take. This is entirely backwards. Instead, teams will often start with the estimate, and then go and figure out what kind of software work they can do to meet it.&lt;/p&gt;
    &lt;p&gt;Suppose you’re working on a LLM chatbot, and your director wants to implement “talk with a PDF”. If you have six months to do the work, you might implement a robust file upload system, some pipeline to chunk and embed the PDF content for semantic search, a way to extract PDF pages as image content to capture formatting and diagrams, and so on. If you have one day to do the work, you will naturally search for simpler approaches: for instance, converting the PDF to text client-side and sticking the entire thing in the LLM context, or offering a plain-text “grep the PDF” tool.&lt;/p&gt;
    &lt;p&gt;This is true at even at the level of individual lines of code. When you have weeks or months until your deadline, you might spend a lot of time thinking airily about how you could refactor the codebase to make your new feature fit in as elegantly as possible. When you have hours, you will typically be laser-focused on finding an approach that will actually work. There are always many different ways to solve software problems. Engineers thus have quite a lot of discretion about how to get it done.&lt;/p&gt;
    &lt;head rend="h3"&gt;How I estimate work&lt;/head&gt;
    &lt;p&gt;So how do I estimate, given all that?&lt;/p&gt;
    &lt;p&gt;I gather as much political context as possible before I even look at the code. How much pressure is on this project? Is it a casual ask, or do we have to find a way to do this? What kind of estimate is my management chain looking for? There’s a huge difference between “the CTO really wants this in one week” and “we were looking for work for your team and this seemed like it could fit”.&lt;/p&gt;
    &lt;p&gt;Ideally, I go to the code with an estimate already in hand. Instead of asking myself “how long would it take to do this”, where “this” could be any one of a hundred different software designs, I ask myself “which approaches could be done in one week?“.&lt;/p&gt;
    &lt;p&gt;I spend more time worrying about unknowns than knowns. As I said above, unknown work always dominates software projects. The more “dark forests” in the codebase this feature has to touch, the higher my estimate will be - or, more concretely, the tighter I need to constrain the set of approaches to the known work.&lt;/p&gt;
    &lt;p&gt;Finally, I go back to my manager with a risk assessment, not with a concrete estimate. I don’t ever say “this is a four-week project”. I say something like “I don’t think we’ll get this done in one week, because X Y Z would need to all go right, and at least one of those things is bound to take a lot more work than we expect. Ideally, I go back to my manager with a series of plans, not just one:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We tackle X Y Z directly, which might all go smoothly but if it blows out we’ll be here for a month&lt;/item&gt;
      &lt;item&gt;We bypass Y and Z entirely, which would introduce these other risks but possibly allow us to hit the deadline&lt;/item&gt;
      &lt;item&gt;We bring in help from another team who’s more familiar with X and Y, so we just have to focus on Z&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, I don’t “break down the work to determine how long it will take”. My management chain already knows how long they want it to take. My job is to figure out the set of software approaches that match that estimate.&lt;/p&gt;
    &lt;p&gt;Sometimes that set is empty: the project is just impossible, no matter how you slice it. In that case, my management chain needs to get together and figure out some way to alter the requirements. But if I always said “this is impossible”, my managers would find someone else to do their estimates. When I do that, I’m drawing on a well of trust that I build up by making pragmatic estimates the rest of the time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Addressing some objections&lt;/head&gt;
    &lt;p&gt;Many engineers find this approach distasteful. One reason is that they don’t like estimating in conditions of uncertainty, so they insist on having all the unknown questions answered in advance. I have written a lot about this in Engineers who won’t commit and How I provide technical clarity to non-technical leaders, but suffice to say that I think it’s cowardly. If you refuse to estimate, you’re forcing someone less technical to estimate for you.&lt;/p&gt;
    &lt;p&gt;Some engineers think that their job is to constantly push back against engineering management, and that helping their manager find technical compromises is betraying some kind of sacred engineering trust. I wrote about this in Software engineers should be a little bit cynical. If you want to spend your career doing that, that’s fine, but I personally find it more rewarding to find ways to work with my managers (who have almost exclusively been nice people).&lt;/p&gt;
    &lt;p&gt;Other engineers might say that they rarely feel this kind of pressure from their directors or VPs to alter estimates, and that this is really just the sign of a dysfunctional engineering organization. Maybe! I can only speak for the engineering organizations I’ve worked in. But my suspicion is that these engineers are really just saying that they work “out of the spotlight”, where there’s not much pressure in general and teams can adopt whatever processes they want. There’s nothing wrong with that. But I don’t think it qualifies you to give helpful advice to engineers who do feel this kind of pressure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;I think software engineering estimation is generally misunderstood.&lt;/p&gt;
    &lt;p&gt;The common view is that a manager proposes some technical project, the team gets together to figure out how long it would take to build, and then the manager makes staffing and planning decisions with that information. In fact, it’s the reverse: a manager comes to the team with an estimate already in hand (though they might not come out and admit it), and then the team must figure out what kind of technical project might be possible within that estimate.&lt;/p&gt;
    &lt;p&gt;This is because estimates are not by or for engineering teams. They are tools used for managers to negotiate with each other about planned work. Very occasionally, when a project is literally impossible, the estimate can serve as a way for the team to communicate that fact upwards. But that requires trust. A team that is always pushing back on estimates will not be believed when they do encounter a genuinely impossible proposal.&lt;/p&gt;
    &lt;p&gt;When I estimate, I extract the range my manager is looking for, and only then do I go through the code and figure out what can be done in that time. I never come back with a flat “two weeks” figure. Instead, I come back with a range of possibilities, each with their own risks, and let my manager make that tradeoff.&lt;/p&gt;
    &lt;p&gt;It is not possible to accurately estimate software work. Software projects spend most of their time grappling with unknown problems, which by definition can’t be estimated in advance. To estimate well, you must therefore basically ignore all the known aspects of the work, and instead try and make educated guesses about how many unknowns there are, and how scary each unknown is.&lt;/p&gt;
    &lt;p&gt;edit: I should thank one of my readers, Karthik, who emailed me to ask about estimates, thus revealing to me that I had many more opinions than I thought.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;For anyone wincing at that time, I mean like three minutes of actual deployment and twenty-seven minutes of waiting for checks to pass or monitors to turn up green.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I write a lot more about this in You can’t design software you don’t work on.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For instance, imagine a mandate to improve the performance of some large Rails API, one piece at a time. I could happily do that kind of work forever.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;How I provide technical clarity to non-technical leaders&lt;/p&gt;&lt;p&gt;My mission as a staff engineer is to provide technical clarity to the organization.&lt;/p&gt;&lt;p&gt;Of course, I do other stuff too. I run projects, I ship code, I review PRs, and so on. But the most important thing I do - what I’m for - is to provide technical clarity.&lt;/p&gt;&lt;p&gt;In an organization, technical clarity is when non-technical decision makers have a good-enough practical understanding of what changes they can make to their software systems.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46742389</guid><pubDate>Sat, 24 Jan 2026 10:22:32 +0000</pubDate></item><item><title>I Like GitLab</title><link>https://www.whileforloop.com/en/blog/2026/01/21/i-like-gitlab/</link><description>&lt;doc fingerprint="3e435f7c5cb20c4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I like GitLab&lt;/head&gt;
    &lt;p&gt;Iâve been using GitLab for my private projects for years now. I just started using it at some point and never had a reason to switch.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I ended up here&lt;/head&gt;
    &lt;p&gt;Back when GitHub still charged for private repositories, GitLab offered them for free. That was the initial hook. I had a bunch of small projects and experiments that I didnât want to publish but also didnât want to pay for. GitLab was the obvious choice.&lt;/p&gt;
    &lt;p&gt;GitHub eventually made private repos free too, but by then my workflow was already built around GitLab. All my CI pipelines, Docker images, deployment scripts - everything pointed there.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Docker registry thing&lt;/head&gt;
    &lt;p&gt;Every GitLab project comes with a Container Registry. This is probably the feature I use most.&lt;/p&gt;
    &lt;p&gt;The workflow is simple. Build an image locally or in CI, push it to the registry, pull it wherever you need it.&lt;/p&gt;
    &lt;p&gt;No separate Docker Hub account. No thinking about pull rate limits (remember when Docker Hub introduced those and broke half the internetâs CI pipelines?). No managing access tokens for yet another service.&lt;/p&gt;
    &lt;p&gt;For my private projects this is perfect. I donât need the discoverability of Docker Hub. I just need a place to store images that integrates with my existing authentication.&lt;/p&gt;
    &lt;p&gt;The 10GB limit per project sounds small on paper but Iâve never come close to hitting it. Old tags get cleaned up, base layers are shared, and most of my images arenât that big anyway.&lt;/p&gt;
    &lt;head rend="h2"&gt;CI/CD&lt;/head&gt;
    &lt;p&gt;GitLab CI was one of the earlier âCI config as codeâ implementations that I used. You drop a &lt;code&gt;.gitlab-ci.yml&lt;/code&gt; in your repo and things start happening. The config is versioned with everything else, which means you can see exactly what your pipeline looked like six months ago.&lt;/p&gt;
    &lt;p&gt;Nothing fancy. Build the image, push it, optionally deploy. The manual trigger on deploy is nice - I can build automatically but still control when things go to production.&lt;/p&gt;
    &lt;p&gt;The shared runners GitLab provides handle most of my workloads. Theyâre not fast, but theyâre free and reliable. For the times when I needed something specific - like a runner with more memory or access to my private network - setting up my own runner on a cheap VPS was straightforward. Install the runner, register it with a token, done.&lt;/p&gt;
    &lt;p&gt;The documentation for CI/CD is extensive. Almost too extensive. There are so many features and options that finding what you need can take a while. But once you figure out your patterns, you mostly copy-paste from your own previous configs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The annoying parts&lt;/head&gt;
    &lt;p&gt;Letâs talk about whatâs not great.&lt;/p&gt;
    &lt;p&gt;Speed. The GitLab web interface has always felt sluggish to me. Click on a merge request, wait. Switch to the pipeline view, wait. Open the job log, wait. Itâs not terrible, but thereâs this constant friction that adds up over a long session.&lt;/p&gt;
    &lt;p&gt;Iâve noticed improvements recently. Either they optimized things or Iâve just gotten used to it - hard to say. But itâs still not as snappy as GitHub.&lt;/p&gt;
    &lt;p&gt;Feature overload. GitLab tries to be everything. Issue tracking, project management, wiki, snippets, package registry, container registry, security scanning, infrastructure management, monitoringâ¦ The sidebar menu goes on forever.&lt;/p&gt;
    &lt;p&gt;I use maybe 10% of whatâs available. Repositories, merge requests, CI/CD, container registry. Thatâs pretty much it.&lt;/p&gt;
    &lt;p&gt;This is a double-edged sword. On one hand, feature bloat. On the other, if I ever need something like a private NPM registry or security scanning, itâs already there. I just havenât needed it yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;The value proposition&lt;/head&gt;
    &lt;p&gt;I run about a dozen private projects ranging from active side projects to abandoned experiments I keep around âjust in case.â None of this costs me anything. Itâs crazy.&lt;/p&gt;
    &lt;head rend="h2"&gt;My workflow&lt;/head&gt;
    &lt;p&gt;All my private projects live on GitLab. Prototypes, experiments, half-finished ideas, things Iâm actively working on but not ready to share. Itâs my digital workshop where I can make a mess without anyone watching.&lt;/p&gt;
    &lt;p&gt;GitHub is different. Thatâs where I put things I want people to see.&lt;/p&gt;
    &lt;p&gt;This split works well for me. I get the collaboration and visibility benefits of GitHub for public stuff while keeping my private mess organized on GitLab with proper CI/CD and container registries.&lt;/p&gt;
    &lt;p&gt;Some people do everything on GitHub. Some do everything on GitLab. Having a foot in both camps might seem redundant but honestly, they serve different purposes in my workflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46742432</guid><pubDate>Sat, 24 Jan 2026 10:32:26 +0000</pubDate></item><item><title>Many Small Queries Are Efficient in SQLite</title><link>https://www.sqlite.org/np1queryprob.html</link><description>&lt;doc fingerprint="2f2e541fc3b4ade"&gt;
  &lt;main&gt;
    &lt;p&gt;200 SQL statements per webpage is excessive for client/server database engines like MySQL, PostgreSQL, or SQL Server.&lt;/p&gt;
    &lt;p&gt;But with SQLite, 200 or more SQL statement per webpage is not a problem.&lt;/p&gt;
    &lt;p&gt;SQLite can also do large and complex queries efficiently, just like client/server databases. But SQLite can do many smaller queries efficiently too. Application developers can use whichever technique works best for the task at hand.&lt;/p&gt;
    &lt;p&gt;The Appropriate Uses For SQLite page says that dynamic pages on the SQLite website typically do about 200 SQL statements each. This has provoked criticism from readers. Examples:&lt;/p&gt;
    &lt;p&gt;"200 SQL statements is a ridiculously high number for a single page"&lt;/p&gt;
    &lt;p&gt;"For most sites, 200 queries is way, way, way too much."&lt;/p&gt;
    &lt;p&gt;"[This is] bad design"&lt;/p&gt;
    &lt;p&gt;Such criticism would be well-founded for a traditional client/server database engine, such as MySQL, PostgreSQL, or SQL Server. In a client/server database, each SQL statement requires a message round-trip from the application to the database server and back to the application. Doing over 200 round-trip messages, sequentially, can be a serious performance drag. This is sometimes called the "N+1 Query Problem" or the "N+1 Select Problem" and it is an anti-pattern.&lt;/p&gt;
    &lt;p&gt;SQLite is not client/server, however. The SQLite database runs in the same process address space as the application. Queries do not involve message round-trips, only a function call. The latency of a single SQL query is far less in SQLite. Hence, using a large number of queries with SQLite is not the problem.&lt;/p&gt;
    &lt;p&gt;The dynamic webpages on the SQLite website are mostly generated by the Fossil version control system. A typical dynamic page would be a timeline such as https://sqlite.org/src/timeline. A log of all SQL used by the timeline is shown below.&lt;/p&gt;
    &lt;p&gt;The first group of queries in the log are extracting display options from the "config" and "global_config" tables of the Fossil database. Then there is a single complex query that extracts a list of all elements to be displayed on the timeline. This "timeline" query demonstrates that SQLite can easily process complex relational database queries involving multiple tables, subqueries, and complex WHERE clause constraints, and it can make effective use of indexes to solve the queries with minimal disk I/O.&lt;/p&gt;
    &lt;p&gt;Following the single big "timeline" query, there are additional queries for each timeline element. Fossil is using the "N+1 Query" pattern rather than trying to grab all the information in as few queries as possible. But that is ok because there is no unnecessary IPC overhead. At the bottom of each timeline page, Fossil shows approximately how long it took to generate the page. For a 50-entry timeline, the latency is usually less than 25 milliseconds. Profiling shows that few of those milliseconds were spent inside the database engine.&lt;/p&gt;
    &lt;p&gt;Using the N+1 Query pattern in Fossil does not harm the application. But the N+1 Query pattern does have benefits. For one, the section of the code that creates the timeline query can be completely separate from the section that prepares each timeline entry for display. This provides a separation of responsibility that helps keep the code simple and easy to maintain. Secondly, the information needed for display, and the queries needed to extract that information, vary according to what type of objects are to be shown. Check-ins need one set of queries. Tickets need another set of queries. Wiki pages need a different query. And so forth. By implementing these queries on-demand and in the part of the code dealing with the various entities, there is further separation of responsibility and simplification of the overall code base.&lt;/p&gt;
    &lt;p&gt;So, SQLite is able to do one or two large and complex queries, or it can do many smaller and simpler queries. Both are efficient. An application can use either or both techniques, depending on what works best for the situation at hand.&lt;/p&gt;
    &lt;p&gt;The following is a log of all SQL used to generate one particular timeline (captured on 2016-09-16):&lt;/p&gt;
    &lt;quote&gt;-- sqlite3_open: /home/drh/sqlite/sqlite/.fslckout PRAGMA foreign_keys=OFF; SELECT sql FROM localdb.sqlite_schema WHERE name=='vfile'; -- sqlite3_open: /home/drh/.fossil PRAGMA foreign_keys=OFF; SELECT value FROM vvar WHERE name='repository'; ATTACH DATABASE '/home/drh/www/repos/sqlite.fossil' AS 'repository' KEY ''; SELECT value FROM config WHERE name='allow-symlinks'; SELECT value FROM global_config WHERE name='allow-symlinks'; SELECT value FROM config WHERE name='aux-schema'; SELECT 1 FROM config WHERE name='baseurl:http://'; SELECT value FROM config WHERE name='ip-prefix-terms'; SELECT value FROM global_config WHERE name='ip-prefix-terms'; SELECT value FROM config WHERE name='localauth'; SELECT value FROM vvar WHERE name='default-user'; SELECT uid FROM user WHERE cap LIKE '%s%'; SELECT login FROM user WHERE uid=1; SELECT cap FROM user WHERE login = 'nobody'; SELECT cap FROM user WHERE login = 'anonymous'; SELECT value FROM config WHERE name='public-pages'; SELECT value FROM global_config WHERE name='public-pages'; SELECT value FROM config WHERE name='header'; SELECT value FROM config WHERE name='project-name'; SELECT value FROM config WHERE name='th1-setup'; SELECT value FROM global_config WHERE name='th1-setup'; SELECT value FROM config WHERE name='redirect-to-https'; SELECT value FROM global_config WHERE name='redirect-to-https'; SELECT value FROM config WHERE name='index-page'; SELECT mtime FROM config WHERE name='css'; SELECT mtime FROM config WHERE name='logo-image'; SELECT mtime FROM config WHERE name='background-image'; CREATE TEMP TABLE IF NOT EXISTS timeline( rid INTEGER PRIMARY KEY, uuid TEXT, timestamp TEXT, comment TEXT, user TEXT, isleaf BOOLEAN, bgcolor TEXT, etype TEXT, taglist TEXT, tagid INTEGER, short TEXT, sortby REAL ) ; INSERT OR IGNORE INTO timeline SELECT blob.rid AS blobRid, uuid AS uuid, datetime(event.mtime,toLocal()) AS timestamp, coalesce(ecomment, comment) AS comment, coalesce(euser, user) AS user, blob.rid IN leaf AS leaf, bgcolor AS bgColor, event.type AS eventType, (SELECT group_concat(substr(tagname,5), ', ') FROM tag, tagxref WHERE tagname GLOB 'sym-*' AND tag.tagid=tagxref.tagid AND tagxref.rid=blob.rid AND tagxref.tagtype&amp;gt;0) AS tags, tagid AS tagid, brief AS brief, event.mtime AS mtime FROM event CROSS JOIN blob WHERE blob.rid=event.objid AND NOT EXISTS(SELECT 1 FROM tagxref WHERE tagid=5 AND tagtype&amp;gt;0 AND rid=blob.rid) ORDER BY event.mtime DESC LIMIT 50; -- SELECT value FROM config WHERE name='timeline-utc'; SELECT count(*) FROM timeline WHERE etype!='div'; SELECT min(timestamp) FROM timeline; SELECT julianday('2016-09-15 14:54:51',fromLocal()); SELECT EXISTS (SELECT 1 FROM event CROSS JOIN blob WHERE blob.rid=event.objid AND mtime&amp;lt;=2457647.121412037); SELECT max(timestamp) FROM timeline; SELECT julianday('2016-09-24 17:42:43',fromLocal()); SELECT EXISTS (SELECT 1 FROM event CROSS JOIN blob WHERE blob.rid=event.objid AND mtime&amp;gt;=2457656.238009259); SELECT value FROM config WHERE name='search-ci'; SELECT value FROM vvar WHERE name='checkout'; SELECT value FROM config WHERE name='timeline-max-comment'; SELECT value FROM global_config WHERE name='timeline-max-comment'; SELECT value FROM config WHERE name='timeline-date-format'; SELECT value FROM config WHERE name='timeline-truncate-at-blank'; SELECT value FROM global_config WHERE name='timeline-truncate-at-blank'; SELECT * FROM timeline ORDER BY sortby DESC; SELECT value FROM config WHERE name='hash-digits'; SELECT value FROM global_config WHERE name='hash-digits'; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68028; SELECT pid FROM plink WHERE cid=68028 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM tagxref WHERE rid=68028 AND tagid=9 AND tagtype&amp;gt;0; SELECT value FROM config WHERE name='timeline-block-markup'; SELECT value FROM config WHERE name='timeline-plaintext'; SELECT value FROM config WHERE name='wiki-use-html'; SELECT value FROM global_config WHERE name='wiki-use-html'; SELECT 1 FROM private WHERE rid=68028; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68026; SELECT pid FROM plink WHERE cid=68026 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68026; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68024; SELECT pid FROM plink WHERE cid=68024 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68024; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68018; SELECT pid FROM plink WHERE cid=68018 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68018; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68012; SELECT pid FROM plink WHERE cid=68012 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68012; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68011; SELECT value FROM config WHERE name='details'; SELECT pid FROM plink WHERE cid=68011 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM tagxref WHERE rid=68011 AND tagid=9 AND tagtype&amp;gt;0; SELECT 1 FROM private WHERE rid=68011; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68008; SELECT pid FROM plink WHERE cid=68008 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68008; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68006; SELECT pid FROM plink WHERE cid=68006 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68006; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=68000; SELECT pid FROM plink WHERE cid=68000 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=68000; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67997; SELECT pid FROM plink WHERE cid=67997 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67997; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67992; SELECT pid FROM plink WHERE cid=67992 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67992; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67990; SELECT pid FROM plink WHERE cid=67990 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67990; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67989; SELECT pid FROM plink WHERE cid=67989 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67989; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67984; SELECT pid FROM plink WHERE cid=67984 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67984; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67983; SELECT pid FROM plink WHERE cid=67983 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67983; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67979; SELECT pid FROM plink WHERE cid=67979 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67979; SELECT value FROM config WHERE name='ticket-closed-expr'; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='1ec41379c9c1e400' AND tkt_uuid&amp;lt;'1ec41379c9c1e401'; SELECT 1 FROM private WHERE rid=67980; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67977; SELECT pid FROM plink WHERE cid=67977 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='1ec41379c9c1e400' AND tkt_uuid&amp;lt;'1ec41379c9c1e401'; SELECT 1 FROM private WHERE rid=67977; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='1ec41379c9c1e400' AND tkt_uuid&amp;lt;'1ec41379c9c1e401'; SELECT 1 FROM private WHERE rid=67974; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67971; SELECT pid FROM plink WHERE cid=67971 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67971; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67972; SELECT pid FROM plink WHERE cid=67972 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67972; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67969; SELECT pid FROM plink WHERE cid=67969 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67969; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67966; SELECT pid FROM plink WHERE cid=67966 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67966; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67962; SELECT pid FROM plink WHERE cid=67962 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67962; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67960; SELECT pid FROM plink WHERE cid=67960 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67960; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67957; SELECT pid FROM plink WHERE cid=67957 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67957; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67955; SELECT pid FROM plink WHERE cid=67955 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67955; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67953; SELECT pid FROM plink WHERE cid=67953 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='5990a1bdb4a073' AND tkt_uuid&amp;lt;'5990a1bdb4a074'; SELECT 1 FROM blob WHERE uuid&amp;gt;='5990a1bdb4a073' AND uuid&amp;lt;'5990a1bdb4a074'; SELECT 1 FROM private WHERE rid=67953; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67941; SELECT pid FROM plink WHERE cid=67941 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67941; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67940; SELECT pid FROM plink WHERE cid=67940 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67940; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67938; SELECT pid FROM plink WHERE cid=67938 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67938; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67935; SELECT pid FROM plink WHERE cid=67935 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67935; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67934; SELECT pid FROM plink WHERE cid=67934 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67934; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67932; SELECT pid FROM plink WHERE cid=67932 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67932; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67930; SELECT pid FROM plink WHERE cid=67930 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67930; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67928; SELECT pid FROM plink WHERE cid=67928 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM tagxref WHERE rid=67928 AND tagid=9 AND tagtype&amp;gt;0; SELECT 1 FROM private WHERE rid=67928; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='0eab1ac7591f511d' AND tkt_uuid&amp;lt;'0eab1ac7591f511e'; SELECT 1 FROM private WHERE rid=67919; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='01874d252ac44861' AND tkt_uuid&amp;lt;'01874d252ac44862'; SELECT 1 FROM blob WHERE uuid&amp;gt;='01874d252ac44861' AND uuid&amp;lt;'01874d252ac44862'; SELECT 1 FROM private WHERE rid=67918; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67916; SELECT pid FROM plink WHERE cid=67916 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='0eab1ac759' AND tkt_uuid&amp;lt;'0eab1ac75:'; SELECT 1 FROM private WHERE rid=67916; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='a49bc0a8244feb08' AND tkt_uuid&amp;lt;'a49bc0a8244feb09'; SELECT 1 FROM blob WHERE uuid&amp;gt;='a49bc0a8244feb08' AND uuid&amp;lt;'a49bc0a8244feb09'; SELECT 1 FROM private WHERE rid=67914; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67913; SELECT pid FROM plink WHERE cid=67913 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='0eab1ac7591f' AND tkt_uuid&amp;lt;'0eab1ac7591g'; SELECT 1 FROM private WHERE rid=67913; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67911; SELECT pid FROM plink WHERE cid=67911 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67911; SELECT status='Closed' OR status='Fixed' FROM ticket WHERE tkt_uuid&amp;gt;='0eab1ac7591f511d' AND tkt_uuid&amp;lt;'0eab1ac7591f511e'; SELECT 1 FROM private WHERE rid=67909; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67907; SELECT pid FROM plink WHERE cid=67907 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67907; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67899; SELECT pid FROM plink WHERE cid=67899 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67899; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67897; SELECT pid FROM plink WHERE cid=67897 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67897; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67895; SELECT pid FROM plink WHERE cid=67895 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67895; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67893; SELECT pid FROM plink WHERE cid=67893 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67893; SELECT value FROM tagxref WHERE tagid=8 AND tagtype&amp;gt;0 AND rid=67891; SELECT pid FROM plink WHERE cid=67891 AND pid NOT IN phantom ORDER BY isprim DESC; SELECT 1 FROM private WHERE rid=67891; SELECT count(*) FROM plink WHERE pid=67928 AND isprim AND coalesce((SELECT value FROM tagxref WHERE tagid=8 AND rid=plink.pid), 'trunk') =coalesce((SELECT value FROM tagxref WHERE tagid=8 AND rid=plink.cid), 'trunk') ; SELECT count(*) FROM plink WHERE pid=68011 AND isprim AND coalesce((SELECT value FROM tagxref WHERE tagid=8 AND rid=plink.pid), 'trunk') =coalesce((SELECT value FROM tagxref WHERE tagid=8 AND rid=plink.cid), 'trunk') ; SELECT count(*) FROM plink WHERE pid=68028 AND isprim AND coalesce((SELECT value FROM tagxref WHERE tagid=8 AND rid=plink.pid), 'trunk') =coalesce((SELECT value FROM tagxref WHERE tagid=8 AND rid=plink.cid), 'trunk') ; SELECT value FROM config WHERE name='show-version-diffs'; SELECT value FROM config WHERE name='adunit-omit-if-admin'; SELECT value FROM global_config WHERE name='adunit-omit-if-admin'; SELECT value FROM config WHERE name='adunit-omit-if-user'; SELECT value FROM global_config WHERE name='adunit-omit-if-user'; SELECT value FROM config WHERE name='adunit'; SELECT value FROM global_config WHERE name='adunit'; SELECT value FROM config WHERE name='auto-hyperlink-delay'; SELECT value FROM global_config WHERE name='auto-hyperlink-delay'; SELECT value FROM config WHERE name='footer'; PRAGMA database_list; PRAGMA database_list; PRAGMA localdb.freelist_count; PRAGMA localdb.page_count;&lt;/quote&gt;
    &lt;p&gt;This page was last updated on 2025-05-31 13:08:22Z&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46742635</guid><pubDate>Sat, 24 Jan 2026 11:15:15 +0000</pubDate></item><item><title>JVIC: New web-based Commodore VIC 20 emulator</title><link>https://vic20.games/#/basic/24k</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46742828</guid><pubDate>Sat, 24 Jan 2026 11:53:55 +0000</pubDate></item><item><title>MS confirms it will give the FBI your Windows PC data encryption key if asked</title><link>https://www.windowscentral.com/microsoft/windows-11/microsoft-bitlocker-encryption-keys-give-fbi-legal-order-privacy-nightmare</link><description>&lt;doc fingerprint="3616ba097f2ad7d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft confirms it will give the FBI your Windows PC data encryption key if asked — you can thank Windows 11's forced online accounts for that&lt;/head&gt;
    &lt;p&gt;Windows 11's online Microsoft Account requirement means your PC is automatically backing up its data encryption key to the cloud, and Microsoft says it will hand those over to the FBI if requested via legal order.&lt;/p&gt;
    &lt;p&gt;Microsoft has confirmed in a statement to Forbes that the company will provide the FBI access to BitLocker encryption keys if a valid legal order is requested. These keys enable the ability to decrypt and access the data on a computer running Windows, giving law enforcement the means to break into a device and access its data.&lt;/p&gt;
    &lt;p&gt;The news comes as Forbes reports that Microsoft gave the FBI the BitLocker encryption keys to access a device in Guam that law enforcement believed to have "evidence that would help prove individuals handling the island’s Covid unemployment assistance program were part of a plot to steal funds" in early 2025.&lt;/p&gt;
    &lt;p&gt;This was possible because the device in question had its BitLocker encryption key saved in the cloud. By default, Windows 11 forces the use of a Microsoft Account, and the OS will automatically tie your BitLocker encryption key to your online account so that users can easily recover their data in scenarios where they might get locked out. This can be disabled, letting you choose where to save them locally, but the default behavior is to store the key in Microsoft's cloud when setting up a PC with a Microsoft Account.&lt;/p&gt;
    &lt;p&gt;"While key recovery offers convenience, it also carries a risk of unwanted access, so Microsoft believes customers are in the best position to decide... how to manage their keys,” Microsoft spokesperson Charles Chamberlayne said in a statement to Forbes.&lt;/p&gt;
    &lt;p&gt;Microsoft told Forbes that it receives around 20 requests for BitLocker encryption keys from the FBI a year, but the majority of requests are unable to be met because the encryption key was never uploaded to the company's cloud.&lt;/p&gt;
    &lt;p&gt;This is notable as other tech companies, such as Apple, have famously refused to provide law enforcement with access to encrypted data stored on their products. Apple has openly fought against the FBI in the past when it was asked to provide a backdoor into an iPhone. Other tech giants, such as Meta, will store encryption keys in the cloud, but use zero-knowledge architectures and encrypt the keys server-side so that only the user can access them.&lt;/p&gt;
    &lt;p&gt;It's frankly shocking that the encryption keys that do get uploaded to Microsoft aren't encrypted on the cloud side, too. That would prevent Microsoft from seeing the keys, but it seems that, as things currently stand, those keys are available in an unencrypted state, and it is a privacy nightmare for customers.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;To see Microsoft so willingly hand over the keys to encrypted Windows PCs is concerning, and should make everybody using a modern Windows computer think twice before backing up their keys to the cloud. You can see which PCs have their BitLocker keys stored on Microsoft's servers on the Microsoft Account website here, which will let you delete them if present.&lt;/p&gt;
    &lt;p&gt;Follow Windows Central on Google News to keep our latest news, insights, and features at the top of your feeds!&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46743154</guid><pubDate>Sat, 24 Jan 2026 12:55:22 +0000</pubDate></item><item><title>Claude Code's new hidden feature: Swarms</title><link>https://twitter.com/NicerInPerson/status/2014989679796347375</link><description>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46743908</guid><pubDate>Sat, 24 Jan 2026 14:35:47 +0000</pubDate></item><item><title>Ask HN: Gmail spam filtering suddenly marking everything as spam?</title><link>https://news.ycombinator.com/item?id=46744807</link><description>&lt;doc fingerprint="7fb321031f7612f5"&gt;
  &lt;main&gt;
    &lt;p&gt;Almost all transactional emails are being marked as suspicious even when their SPF/DKIM records are fine and they’ve been whitelisted before. Did Google break something in gmail/spam filtering?&lt;/p&gt;
    &lt;p&gt;It's a great reminder of how good this feature is that we take for granted. I think this outage has actually improved my appreciation for Gmail (a service I normally only complain about).&lt;/p&gt;
    &lt;p&gt;Seriously. I didn't even realize this was a wide issue, but I couldn't find a school enrolment email I was looking for this morning, and found it in the spam folder. The fact that I basically never have to do this is actually amazing.&lt;/p&gt;
    &lt;p&gt;This has been “down” for me for a few months now, ever since Google tied this functionality to the same toggle that opts you in for using your email data for AI training. So now you can’t filter this stuff without also agreeing to a whole swath of unrelated and opt-ins.&lt;/p&gt;
    &lt;p&gt;Ive since gone on an unsubscribe campaign, and things seem bearable now.&lt;/p&gt;
    &lt;p&gt;Same here. Until recently I would get maybe 1-2 spams a month, and I just got 30 in the span of a few days.&lt;/p&gt;
    &lt;p&gt;They’re the very obvious, very obnoxious kind of spam, and Gmail still correctly sends them to the junk bin, so I wonder if they were shadowbanned before and Google simply decided to make the process more explicit (which I don’t hate on principle).&lt;/p&gt;
    &lt;p&gt;Either that or my address was scrapped from somewhere by a spam bot and the timing is coincidental.&lt;/p&gt;
    &lt;p&gt;Yes, my Gmail inbox is full of regular senders being flagged as "possibly unsafe" and I need to click a button "Looks Safe" to accept them. They are not being spamboxed, but they are definitely flagged. Even official communications from the USPS!&lt;/p&gt;
    &lt;p&gt;The reason given is that "Gmail hasn't scanned this message", so I suppose the scanners are unavailable/disabled for the time being.&lt;/p&gt;
    &lt;p&gt;They should also be tagged as "Important" but they are not. I believe this is a heuristic-based designation, and it has not been working too great lately. My most important mail is coming through as "unimportant".&lt;/p&gt;
    &lt;p&gt;They are not being marked as "Suspicious" but they are showing an infobox that explains they could not be scanned at all.&lt;/p&gt;
    &lt;p&gt;You could click "Seems Safe" on these messages, but they are not scanned by Google, and they are simply adding a disclaimer that they currently can't vouch for the safety of a message that they couldn't scan. It seems to me that this is a prudent and helpful course of action.&lt;/p&gt;
    &lt;p&gt;I don't understand why spam detection is so complicated. I can tell with high accuracy if an email is spam just by the subject line. I'd think even basic ML could do this very reliably you don't need a bleeding-edge LLM to do this.&lt;/p&gt;
    &lt;p&gt;Phishing is tricker because it can be very deceptive especially if you're being targeted specifically. But also usually pretty obvious.&lt;/p&gt;
    &lt;p&gt;* Are you available? * Paul, can we have a zoom meeting with you on Monday? * Assistance for donation * Greetings!!! * some ideas for you * Refund request * Somethings not working * Manuel Montoya for roof work contractor * proposals for print * Invite Connection&lt;/p&gt;
    &lt;p&gt;Half of the above are actual spam, half are not. Tell me which is which ...&lt;/p&gt;
    &lt;p&gt;I have been receiving a large number of spam emails in my "Important and Unread" areas which is anomalous. I was wondering exactly why and this helps. thanks!&lt;/p&gt;
    &lt;p&gt;Step zero. Never disclose your email address to anyone.&lt;/p&gt;
    &lt;p&gt;This is very easy and straightforward. I operate 6 Gmail accounts, and three are "alts" where I've basically never given the address out to anyone at all, and they receive zero spam, zero UCE, zero marketing emails.&lt;/p&gt;
    &lt;p&gt;Of course, on my "main" I've disclosed the address to many entities and I use it for sign-in and shipping and many things. And yes, I do receive spam and scam emails there, but wcyd?&lt;/p&gt;
    &lt;p&gt;I recently had a "role" Google account terminated because I was (paraphrasing) "violating Google policies" by having multiple accounts. I didn't know they were sticklers about that.&lt;/p&gt;
    &lt;p&gt;(I don't much care because the account was just used for interacting with somebody else's Google-hosted junk but, if I had been using it for something serious, I have probably been frustrated.)&lt;/p&gt;
    &lt;p&gt;There is no way, no possible way that Google prohibits the use of multiple accounts. They do not. They cannot. I just asked Gemini and I checked the actual TOS. It does not, in any way, prohibit these uses.&lt;/p&gt;
    &lt;p&gt;In fact, this is plainly evident by the way they give you tools to operate them in a systematic way. You can add multiple accounts to a single Android "user". You can add them to a single Google Chromebook account under one signed-in account. You can add multiple accounts separately to the same Chromebook.&lt;/p&gt;
    &lt;p&gt;You can add multiple accounts with the same names, the same birthdates, and the same Driver License. I've validated at least two YouTube channels by showing exactly the same ID.&lt;/p&gt;
    &lt;p&gt;Google did not terminate your account for the reason you state. You are not telling us all the background information.&lt;/p&gt;
    &lt;p&gt;Google may indeed terminate multiple accounts for the same person because of TOS violations. They will definitely link and associate your accounts, so making an "alt account" for misbehavior is not safe. If my "alt account" is compromised or violates TOS, then I can expect they will discipline all 6 equally, because they're all linked.&lt;/p&gt;
    &lt;p&gt;But operating multiple accounts is very explicitly supported by Google, and by Microsoft as well, I will say. I don't know about Apple. Facebook definitely prohibited this in the past, although you can maintain multiple "profiles" and "pages" that have unique settings and personalities.&lt;/p&gt;
    &lt;p&gt;I feel like an easier solution to having six different email addresses is to use Gmail aliases - I've caught a few less-than-honest companies either selling my email address, or been breached without disclosing such, simply by using an alias along the lines of '+service_name'. If any alias starts to receive spam you can setup rules to automatically delete everything that comes in with that. You also get the added benefit of significantly easier and more accurate search.&lt;/p&gt;
    &lt;p&gt;I don't think y'all understand why I have separate Google accounts.&lt;/p&gt;
    &lt;p&gt;I use them for different purposes. They are "role accounts" for projects I am doing, such as geneaology and astronomy.&lt;/p&gt;
    &lt;p&gt;In order to use YouTube sanely, and store different stuff in Drive, I separate them into unique accounts. I use those accounts for specific things, and my YouTube subscriptions, playlists, etc. are tailored for each role, for example.&lt;/p&gt;
    &lt;p&gt;This is not about email at all. Obviously, I can access all those email accounts through the one app on my smartphone or the one PWA on my Chromebook. They are easily manageable but separate.&lt;/p&gt;
    &lt;p&gt;I also run 3 Outlook/Microsoft accounts, and for the same reason. (One of them is my academic account from community college, and the other two are personal.)&lt;/p&gt;
    &lt;p&gt;I don't need to give out email addresses for the "role accounts" except where I "Sign In With Google" to various services. So I don't really send/receive email from them at all, except where I'm sharing links or documents with myself (the best way to do this cross-account is still by using email, oftentimes.)&lt;/p&gt;
    &lt;p&gt;I use Gmail since the beta (I got invite from a googler) and I don't remember when they began adding spam control but in my experience the GMail spam check works usually exceptionally well: I very rarely need to add a custom filter.&lt;/p&gt;
    &lt;p&gt;My email, over two decades+ (2004?), hasn't been in a many public leaks (only one on https://haveibeenpwned.com/ ) but obviously has made its way to various spammy actors but thankfully nearly everything is caught by GMail's spam filter.&lt;/p&gt;
    &lt;p&gt;If anything I'd say GMail's spam filter works too well: I get more legit emails in my spam folder than spam in my regular inbox. As in: one in a rare while vs about zero spam in my regular inbox.&lt;/p&gt;
    &lt;p&gt;I have seen a spam button show up I haven't seen in a long time.&lt;/p&gt;
    &lt;p&gt;It might be a new round of AI training featuring the labour of customers as free employees doing training. Every time we click, we consent to sharing private email data.&lt;/p&gt;
    &lt;p&gt;There's your confirmation, then. It must be either a localized failure to some subgroup of users, or triggered by some combination of settings, if some people are seeing it and others are not.&lt;/p&gt;
    &lt;p&gt;Its really slow. Too slow to use 2FA or in some cases, verify email addresses or recover passwords.&lt;/p&gt;
    &lt;p&gt;Most people can't handle a notification on their watch every minute, or several spam every five minutes, so "large numbers of people" are shutting off notifications on their phones. And human nature being what it is, they're not going to be turned back on again. So the era of getting a notification when you get an email is coming to a close. "Important Immediate Attention Stuff" moved to text messages a long time ago anyway, at least for me. The list of technologies you can no longer reach me on, always increases over time...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46744807</guid><pubDate>Sat, 24 Jan 2026 16:16:02 +0000</pubDate></item><item><title>Are we all plagiarists now?</title><link>https://www.economist.com/culture/2026/01/22/are-we-all-plagiarists-now</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46744968</guid><pubDate>Sat, 24 Jan 2026 16:34:14 +0000</pubDate></item><item><title>Metriport (YC S22) is hiring a security eng to harden healthcare data infra</title><link>https://www.ycombinator.com/companies/metriport/jobs/XC2AF8s-senior-security-engineer</link><description>&lt;doc fingerprint="47704651dcb5720e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Metriport is an open-source data intelligence platform that helps healthcare organizations access and exchange patient data in real-time. We integrate with all major US healthcare IT systems and tap into comprehensive medical data for 300+ million individuals.&lt;/p&gt;
      &lt;p&gt;We've found product-market fit with multi-million ARR, 100+ customers (including Strive Health, Circle Medical, and Brightside Health), backing from top VCs, and years of runway. We're ready to scale. We're a tight-knit, high-performing team of mostly former founders (including two YC alumni). We're engineering-heavy, operate with minimal bureaucracy and high autonomy, and hire based on competence, not prestige. We push hard—founders work six days a week from our SF office—but give everyone freedom to craft their schedule. We measure output and we're committed to sustainable intensity.&lt;/p&gt;
      &lt;head rend="h2"&gt;About us&lt;/head&gt;
      &lt;p&gt;The following points are an assortment of the most relevant bits that will give you the gist of where we’re at, why we’ll win, and our company culture:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Well funded with a massive recent infusion of capital, found PMF, multi-million ARR, 80+ customers (including Strive Health, Circle Medical, and Brightside Health), funded by top VCs and angels, have years of runway - and we’re just getting started.&lt;/item&gt;
        &lt;item&gt;We’re a tight-knit, high performing, and passionate team - we work with a consistent intensity and have become a leader in our industry with a fraction of the resources of our competitors.&lt;/item&gt;
        &lt;item&gt;Consistency means we push as hard as humanly possible, while keeping our health and personal lives in check.&lt;/item&gt;
        &lt;item&gt;Meaningful work is what gets us out of bed, and we just wouldn’t be satisfied by building yet another CRM company.&lt;/item&gt;
        &lt;item&gt;By pedigree, we’re a group of underdogs - we don’t hire based on prestige, but on demonstrated competence and perceived potential.&lt;/item&gt;
        &lt;item&gt;We’re engineering heavy, and most of our engineers are former founders (including 2 ex-YC founders).&lt;/item&gt;
        &lt;item&gt;We operate as a relatively flat structure with little red tape, forced structure, or bureaucracy. We just opt to get shit done and foster a collaborative environment with high autonomy - our GitHub commit history and product velocity is a testament to this.&lt;/item&gt;
        &lt;item&gt;The founders set the pace by working 6 days a week in our SF office, but everyone is given full freedom to craft a schedule that’s best for both the team and themselves - team output is measured.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;&lt;lb/&gt; About you&lt;/p&gt;
      &lt;p&gt;In a nutshell, we're looking for a security engineer with the following specific qualities:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You’re entrepreneurial-minded, with an olympian-level work ethic (nearly our entire engineering team consists of former founders).&lt;/item&gt;
        &lt;item&gt;You are passionate about security and are excited to own security related projects within the company end-to-end.&lt;/item&gt;
        &lt;item&gt;You are confident in your ability to build scalable systems across the full stack, and people usually come to you for technical guidance.&lt;/item&gt;
        &lt;item&gt;You believe you can solve any problem that comes at you, and don't shy away from diving deep into areas where you may lack domain expertise.&lt;/item&gt;
        &lt;item&gt;You have a strong sense of ownership over your work, and have demonstrated ability to lead others.&lt;/item&gt;
        &lt;item&gt;You know how to move fast - while still maintaining a strong security posture.&lt;/item&gt;
        &lt;item&gt;You care more about the end result and delivering value, rather than what new and frilly tech is being used under the hood for a given feature.&lt;/item&gt;
        &lt;item&gt;When someone scopes out a project with an ETA of 3 weeks, you ask yourself "why can't it be done in 3 days?".&lt;/item&gt;
        &lt;item&gt;You’re a hacker at heart, and have a good sense of what rules should, and shouldn’t, be broken.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;What you'll be doing&lt;/head&gt;
      &lt;p&gt;After quickly ramping up using our comprehensive onboarding materials to get familiar with our domain, product, and codebase, the goal would be to get you shipping product directly to customers as quickly as possible. Specifically, day to day, this looks like:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Evangelizing security across Metriport’s growing team - we will look to you for guidance, and training.&lt;/item&gt;
        &lt;item&gt;Driving full-stack security projects , big and small, end-to-end from ideation to production rollout.These projects could include things like: &lt;list rend="ul"&gt;&lt;item&gt;Implement an enterprise-grade audit logging solution for a new national healthcare network infrastructure stack.&lt;/item&gt;&lt;item&gt;Implement fine grained RBAC on the API key access layer, and more robust roles on our UIs.&lt;/item&gt;&lt;item&gt;Help us revamp our internal security policies and put tools in place to keep the platform, and employees, secure while still allowing the team to be efficient.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Helping the engineering team with PR reviews with a security-focused lens.&lt;/item&gt;
        &lt;item&gt;Work with the Go to Market team to complete customer security assessments and questionnaires.&lt;/item&gt;
        &lt;item&gt;Work with the engineering team to harden security across the development lifecycle - think secret management, access controls, and vulnerability scanning.&lt;/item&gt;
        &lt;item&gt;Managing your own work in Linear.&lt;/item&gt;
        &lt;item&gt;Participating in bi-weekly sprint planning / retro sessions, and quarterly planning sessions.&lt;/item&gt;
        &lt;item&gt;Attending a daily 30 minute remote stand-up at 7:30am PST Mon-Fri (our only regular mandatory meeting).&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;&lt;lb/&gt; Requirements&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You have 6+ years experience in security engineering and information security.&lt;/item&gt;
        &lt;item&gt;You’re located in San Francisco or the Bay Area (or willing to relocate).&lt;/item&gt;
        &lt;item&gt;Familiar with HIPAA compliant environments.&lt;/item&gt;
        &lt;item&gt;Experience rolling out and maintaining security frameworks like SOC 2, NIST, HITRUST, FedRAMP, etc.&lt;/item&gt;
        &lt;item&gt;Experience rolling out data protection technologies like SSO, MFA, VPN, FIPS, etc.&lt;/item&gt;
        &lt;item&gt;Experience with organizational secret management.&lt;/item&gt;
        &lt;item&gt;Experience implementing SCA, SAST, DAST in CICD workflows.&lt;/item&gt;
        &lt;item&gt;Experience with Mobile Device Management (MDM).&lt;/item&gt;
        &lt;item&gt;Proficiency in cloud security &amp;amp; networking on AWS - IAM, WAF, KMS, etc.&lt;/item&gt;
        &lt;item&gt;Proficiency in authentication, cryptography, encryption, and security protocols such as: mTLS, RSA, SSL, HMAC, RBAC, etc.&lt;/item&gt;
        &lt;item&gt;Bonus: experience with IHE profiles (ATNA, CT, XUA).&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Benefits&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Competitive equity + compensation package 🚀&lt;/item&gt;
        &lt;item&gt;Salary range: $160,000,00 - $220,000.00&lt;/item&gt;
        &lt;item&gt;Full family Platinum health insurance, dental, and vision coverage 🦷&lt;/item&gt;
        &lt;item&gt;401(k) retirement plan + matching 💰&lt;/item&gt;
        &lt;item&gt;Flexible work from home or in-office 🏢&lt;/item&gt;
        &lt;item&gt;Healthy lunches are complimentary when working in-office (and breakfast + dinners as needed) 🍏&lt;/item&gt;
        &lt;item&gt;Quarterly company off-sites with the team ⛷️&lt;/item&gt;
        &lt;item&gt;MacBook provided by us 💻&lt;/item&gt;
        &lt;item&gt;Unlimited PTO (we work hard, but trust you to take time you need to be at your best) 🧘♂️&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;&lt;lb/&gt; Our tech&lt;/p&gt;
      &lt;p&gt;On the frontend, we use React - on the backend, we rely on Node.js and TypeScript for writing core business logic. We deploy a wide range of AWS cloud services (ie ECS, Fargate, Lambda, etc), and manage our infrastructure as code with AWS CDK. Data lives in PostgreSQL, DynamoDB, S3, Snowflake, FHIR servers, and more. We use Oneleet for security and compliance.&lt;/p&gt;
      &lt;p&gt;Metriport provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, genetics, sexual orientation, gender identity, or gender expression. We are committed to a diverse and inclusive workforce and welcome people from all backgrounds, experiences, perspectives, and abilities.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745224</guid><pubDate>Sat, 24 Jan 2026 17:00:07 +0000</pubDate></item><item><title>Tao Te Ching – Translated by Ursula K. Le Guin</title><link>https://github.com/nrrb/tao-te-ching/blob/master/Ursula%20K%20Le%20Guin.md</link><description>&lt;doc fingerprint="eb7ad05e09566404"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745233</guid><pubDate>Sat, 24 Jan 2026 17:01:31 +0000</pubDate></item><item><title>December in Servo: multiple windows, proxy support, better caching, and more</title><link>https://servo.org/blog/2026/01/23/december-in-servo/</link><description>&lt;doc fingerprint="660f40f72340fbc4"&gt;
  &lt;main&gt;
    &lt;p&gt;Servo 0.0.4 and our December nightly builds now support multiple windows (@mrobinson, @mukilan, #40927, #41235, #41144)! This builds on features that landed in Servo’s embedding API last month. We’ve also landed support for several web platform features, both old and new:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‘contrast-color()’ in CSS color values (@webbeef, #41542)&lt;/item&gt;
      &lt;item&gt;partial support for &amp;lt;meta charset&amp;gt; (@simonwuelker, #41376)&lt;/item&gt;
      &lt;item&gt;partial support for encoding sniffing (@simonwuelker, #41435)&lt;/item&gt;
      &lt;item&gt;‘background’ and ‘bgcolor’ attributes on &amp;lt;table&amp;gt;, &amp;lt;thead&amp;gt;, &amp;lt;tbody&amp;gt;, &amp;lt;tfoot&amp;gt;, &amp;lt;tr&amp;gt;, &amp;lt;td&amp;gt;, &amp;lt;th&amp;gt; (@simonwuelker, #41272)&lt;/item&gt;
      &lt;item&gt;tee() on readable byte streams (@Taym95, #35991)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For better compatibility with older web content, we now support vendor-prefixed CSS properties like ‘-moz-transform’ (@mrobinson, #41350), as well as window.clientInformation (@Taym95, #41111).&lt;/p&gt;
    &lt;p&gt;We’ve continued shipping the SubtleCrypto API, with full support for ChaCha20-Poly1305, RSA-OAEP, RSA-PSS, and RSASSA-PKCS1-v1_5 (see below), plus importKey() for ML-KEM (@kkoyung, #41585) and several other improvements (@kkoyung, @PaulTreitel, @danilopedraza, #41180, #41395, #41428, #41442, #41472, #41544, #41563, #41587, #41039, #41292):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Algorithm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ChaCha20-Poly1305&lt;/cell&gt;
        &lt;cell&gt;(@kkoyung, #40978, #41003, #41030)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RSA-OAEP&lt;/cell&gt;
        &lt;cell&gt;(@kkoyung, @TimvdLippe, @jdm, #41225, #41217, #41240, #41316)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;RSA-PSS&lt;/cell&gt;
        &lt;cell&gt;(@kkoyung, @jdm, #41157, #41225, #41240, #41287)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RSASSA-PKCS1-v1_5&lt;/cell&gt;
        &lt;cell&gt;(@kkoyung, @jdm, #41172, #41225, #41240, #41267)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;When using servoshell on Windows, you can now see &lt;code&gt;--help&lt;/code&gt; and log output, as long as servoshell was started in a console (@jschwe, #40961).&lt;/p&gt;
    &lt;p&gt;Servo diagnostics options are now accessible in servoshell via the &lt;code&gt;SERVO_DIAGNOSTICS&lt;/code&gt; environment variable (@atbrakhi, #41013), in addition to the usual &lt;code&gt;-Z&lt;/code&gt; / &lt;code&gt;--debug=&lt;/code&gt; arguments.&lt;/p&gt;
    &lt;p&gt;Servo’s devtools now partially support the Network &amp;gt; Security tab (@jiang1997, #40567), allowing you to inspect some of the TLS details of your requests. We’ve also made it compatible with Firefox 145 (@eerii, #41087), and use fewer IPC resources (@mrobinson, #41161).&lt;/p&gt;
    &lt;p&gt;We’ve fixed rendering bugs related to ‘float’, ‘order’, ‘max-width’, ‘max-height’, ‘:link’ selectors, &amp;lt;audio&amp;gt; layout, and getClientRects(), affecting intrinsic sizing (@Loirooriol, #41513), anonymous blocks (@Loirooriol, #41510), incremental layout (@Loirooriol, #40994), flex item sizing (@Loirooriol, #41291), selector matching (@andreubotella, #41478), replaced element layout (@Loirooriol, #41262), and empty fragments (@Loirooriol, #41477).&lt;/p&gt;
    &lt;p&gt;Servo now fires ‘toggle’ events on &amp;lt;dialog&amp;gt; (@lukewarlow, #40412). We’ve also improved the conformance of ‘wheel’ events (@mrobinson, #41182), ‘hashchange’ events (@Taym95, #41325), ‘dblclick’ events on &amp;lt;input&amp;gt; (@Taym95, #41319), ‘resize’ events on &amp;lt;video&amp;gt; (@tharkum, #40940), ‘seeked’ events on &amp;lt;video&amp;gt; and &amp;lt;audio&amp;gt; (@tharkum, #40981), and the ‘transform’ property in getComputedStyle() (@mrobinson, #41187).&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedding API&lt;/head&gt;
    &lt;p&gt;Servo now has basic support for HTTP proxies (@Narfinger, #40941). You can set the proxy URL in the &lt;code&gt;http_proxy&lt;/code&gt; (@Narfinger, #41209) or &lt;code&gt;HTTP_PROXY&lt;/code&gt; (@treeshateorcs, @yezhizhen, #41268) environment variables, or via &lt;code&gt;--pref network_http_proxy_uri&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We now use the system root certificates by default (@Narfinger, @mrobinson, #40935, #41179), on most platforms. If you don’t want to trust the system root certificates, you can instead continue to use Mozilla’s root certificates with &lt;code&gt;--pref network_use_webpki_roots&lt;/code&gt;.
As always, you can also add your own root certificates via &lt;code&gt;Opts&lt;/code&gt;::&lt;code&gt;certificate_path&lt;/code&gt; (&lt;code&gt;--certificate-path=&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;We have a new &lt;code&gt;SiteDataManager&lt;/code&gt; API for managing localStorage, sessionStorage, and cookies (@janvarga, #41236, #41255, #41378, #41523, #41528), and a new &lt;code&gt;NetworkManager&lt;/code&gt; API for managing the cache (@janvarga, @mrobinson, #41255, #41474, #41386).
To clear the cache, call &lt;code&gt;NetworkManager&lt;/code&gt;::&lt;code&gt;clear_cache&lt;/code&gt;, and to list cache entries, call &lt;code&gt;NetworkManager&lt;/code&gt;::&lt;code&gt;cache_entries&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Simple dialogs – that is alert(), confirm(), and prompt() – are now exposed to embedders via a new &lt;code&gt;SimpleDialog&lt;/code&gt; type in &lt;code&gt;EmbedderControl&lt;/code&gt; (@mrobinson, @mukilan, #40982).
This new interface is harder to misuse, and no longer requires boilerplate for embedders that wish to ignore simple dialogs.&lt;/p&gt;
    &lt;p&gt;Web console messages, including messages from the Console API, are now accessible via &lt;code&gt;ServoDelegate&lt;/code&gt;::&lt;code&gt;show_console_message&lt;/code&gt; and &lt;code&gt;WebViewDelegate&lt;/code&gt;::&lt;code&gt;show_console_message&lt;/code&gt; (@atbrakhi, #41351).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Servo&lt;/code&gt;, the main handle for controlling Servo, is now cloneable for sharing within the same thread (@mukilan, @mrobinson, #41010).
To shut down Servo, simply drop the last &lt;code&gt;Servo&lt;/code&gt; handle or let it go out of scope.
&lt;code&gt;Servo&lt;/code&gt;::&lt;code&gt;start_shutting_down&lt;/code&gt; and &lt;code&gt;Servo&lt;/code&gt;::&lt;code&gt;deinit&lt;/code&gt; have been removed (@mukilan, @mrobinson, #41012).&lt;/p&gt;
    &lt;p&gt;Several interfaces have also been renamed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Servo&lt;/code&gt;::&lt;code&gt;clear_cookies&lt;/code&gt;is now&lt;code&gt;SiteDataManager&lt;/code&gt;::&lt;code&gt;clear_cookies&lt;/code&gt;(@janvarga, #41236, #41255)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;DebugOpts&lt;/code&gt;::&lt;code&gt;disable_share_style_cache&lt;/code&gt;is now&lt;code&gt;Preferences&lt;/code&gt;::&lt;code&gt;layout_style_sharing_cache_enabled&lt;/code&gt;(@atbrakhi, #40959)&lt;/item&gt;
      &lt;item&gt;The rest of &lt;code&gt;DebugOpts&lt;/code&gt;has been moved to&lt;code&gt;DiagnosticsLogging&lt;/code&gt;, and the options have been renamed (@atbrakhi, #40960)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Perf and stability&lt;/head&gt;
    &lt;p&gt;We can now evict entries from our HTTP cache (@Narfinger, @gterzian, @Taym95, #40613), rather than having it grow forever (or get cleared by an embedder). about:memory now tracks SVG-related memory usage (@d-kraus, #41481), and we’ve fixed memory leaks in &amp;lt;video&amp;gt; and &amp;lt;audio&amp;gt; (@tharkum, #41131).&lt;/p&gt;
    &lt;p&gt;Servo now does less work when matching selectors (@webbeef, #41368), when focus changes (@mrobinson, @Loirooriol, #40984), and when reflowing boxes whose size did not change (@Loirooriol, @mrobinson, #41160).&lt;/p&gt;
    &lt;p&gt;To allow for smaller binaries, gamepad support is now optional at build time (@WaterWhisperer, #41451).&lt;/p&gt;
    &lt;p&gt;We’ve fixed some undefined behaviour around garbage collection (@sagudev, @jdm, @gmorenz, #41546, mozjs#688, mozjs#689, mozjs#692). To better avoid other garbage-collection-related bugs (@sagudev, mozjs#647, mozjs#638), we’ve continued our work on defining (and migrating to) safer interfaces between Servo and the SpiderMonkey GC (@sagudev, #41519, #41536, #41537, #41520, #41564).&lt;/p&gt;
    &lt;p&gt;We’ve fixed a crash that occurs when &amp;lt;link rel=“shortcut icon”&amp;gt; has an empty ‘href’ attribute, which affected chiptune.com (@webbeef, #41056), and we’ve also fixed crashes in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‘background-repeat’ (@mrobinson, #41158)&lt;/item&gt;
      &lt;item&gt;&amp;lt;audio&amp;gt; layout (@Loirooriol, #41262)&lt;/item&gt;
      &lt;item&gt;custom elements (@mrobinson, #40743)&lt;/item&gt;
      &lt;item&gt;AudioBuffer (@WaterWhisperer, #41253)&lt;/item&gt;
      &lt;item&gt;AudioNode (@Taym95, #40954)&lt;/item&gt;
      &lt;item&gt;ReportingObserver (@Taym95, #41261)&lt;/item&gt;
      &lt;item&gt;Uint8Array (@jdm, #41228)&lt;/item&gt;
      &lt;item&gt;the fonts system, on FreeType platforms (@simonwuelker, #40945)&lt;/item&gt;
      &lt;item&gt;IME usage, on OpenHarmony (@jschwe, #41570)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Donations&lt;/head&gt;
    &lt;p&gt;Thanks again for your generous support! We are now receiving 7110 USD/month (+10.5% over November) in recurring donations. This helps us cover the cost of our speedy CI and benchmarking servers, one of our latest Outreachy interns, and funding maintainer work that helps more people contribute to Servo.&lt;/p&gt;
    &lt;p&gt;Servo is also on thanks.dev, and already 30 GitHub users (+2 over November) that depend on Servo are sponsoring us there. If you use Servo libraries like url, html5ever, selectors, or cssparser, signing up for thanks.dev could be a good way for you (or your employer) to give back to the community.&lt;/p&gt;
    &lt;p&gt;We now have sponsorship tiers that allow you or your organisation to donate to the Servo project with public acknowlegement of your support. A big thanks from Servo to our newest Bronze Sponsors: Anthropy, Niclas Overby, and RxDB! If you’re interested in this kind of sponsorship, please contact us at [email protected].&lt;/p&gt;
    &lt;p&gt;Use of donations is decided transparently via the Technical Steering Committee’s public funding request process, and active proposals are tracked in servo/project#187. For more details, head to our Sponsorship page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conference talks and blogs&lt;/head&gt;
    &lt;p&gt;We’ve recently published one talk and one blog post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Web engine CI on a shoestring budget (slides; transcript) – Delan Azabani (@delan) spoke about the CI system that keeps our builds and tryjobs moving fast, running nearly two million tests in under half an hour.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Servo 2025 Stats – Manuel Rego (@mrego) wrote about the growth of the Servo project, and how our many new contributors have enabled that.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We also have two upcoming talks at FOSDEM 2026 in Brussels later this month:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Servo project and its impact on the web platform ecosystem – Manuel Rego (@mrego) is speaking on Saturday 31 January at 14:00 local time (13:00 UTC), about Servo’s impact on spec issues, interop bugs, test cases, and the broader web platform ecosystem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Implementing Streams Spec in Servo web engine – Taym Haddadi (@Taym95) is speaking on Saturday 31 January at 17:45 local time (16:45 UTC), about our experiences writing a new implementation of the Streams API that is independent of the one in SpiderMonkey.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Servo developers Martin Robinson (@mrobinson) and Delan Azabani (@delan) will also be attending FOSDEM 2026, so it would be a great time to come along and chat about Servo!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745259</guid><pubDate>Sat, 24 Jan 2026 17:03:24 +0000</pubDate></item><item><title>Raspberry Pi Drag Race: Pi 1 to Pi 5 – Performance Comparison</title><link>https://the-diy-life.com/raspberry-pi-drag-race-pi-1-to-pi-5-performance-comparison/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745922</guid><pubDate>Sat, 24 Jan 2026 18:06:00 +0000</pubDate></item><item><title>US Vaccine Panel Chair Says Polio and Other Shots Should Be Optional</title><link>https://www.nytimes.com/2026/01/23/health/milhoan-vaccines-optional-polio.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46745998</guid><pubDate>Sat, 24 Jan 2026 18:14:28 +0000</pubDate></item></channel></rss>