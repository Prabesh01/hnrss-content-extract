<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 02 Nov 2025 19:32:07 +0000</lastBuildDate><item><title>Claude Code can debug low-level cryptography</title><link>https://words.filippo.io/claude-debugging/</link><description>&lt;doc fingerprint="c6650996270f1fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Claude Code Can Debug Low-level Cryptography&lt;/head&gt;
    &lt;p&gt;Over the past few days I wrote a new Go implementation of ML-DSA, a post-quantum signature algorithm specified by NIST last summer. I livecoded it all over four days, finishing it on Thursday evening. Except… Verify was always rejecting valid signatures.&lt;/p&gt;
    &lt;code&gt;$ bin/go test crypto/internal/fips140/mldsa
--- FAIL: TestVector (0.00s)
    mldsa_test.go:47: Verify: mldsa: invalid signature
    mldsa_test.go:84: Verify: mldsa: invalid signature
    mldsa_test.go:121: Verify: mldsa: invalid signature
FAIL
FAIL     crypto/internal/fips140/mldsa   2.142s
FAIL
&lt;/code&gt;
    &lt;p&gt;I was exhausted, so I tried debugging for half an hour and then gave up, with the intention of coming back to it the next day with a fresh mind.&lt;/p&gt;
    &lt;p&gt;On a whim, I figured I would let Claude Code take a shot while I read emails and resurfaced from hyperfocus. I mostly expected it to flail in some maybe-interesting way, or rule out some issues.&lt;/p&gt;
    &lt;p&gt;Instead, it rapidly figured out a fairly complex low-level bug in my implementation of a relatively novel cryptography algorithm. I am sharing this because it made me realize I still don’t have a good intuition for when to invoke AI tools, and because I think it’s a fantastic case study for anyone who’s still skeptical about their usefulness.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Full disclosure: Anthropic gave me a few months of Claude Max for free. They reached out one day and told me they were giving it away to some open source maintainers. Maybe it’s a ploy to get me hooked so I’ll pay for it when the free coupon expires. Maybe they hoped I’d write something like this. Maybe they are just nice. Anyway, they made no request or suggestion to write anything public about Claude Code. Now you know.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Finding the bug&lt;/head&gt;
    &lt;p&gt;I started Claude Code v2.0.28 with Opus 4.1 and no system prompts, and gave it the following prompt (typos included):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I implemented ML-DSA in the Go standard library, and it all works except that verification always rejects the signatures. I know the signatures are right because they match the test vector.&lt;/p&gt;
      &lt;p&gt;YOu can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Look for potential reasons the signatures don’t verify. ultrathink&lt;/p&gt;
      &lt;p&gt;I spot-checked and w1 is different from the signing one.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;To my surprise, it pinged me a few minutes later with a complete fix.&lt;/p&gt;
    &lt;p&gt;Maybe I shouldn’t be surprised! Maybe it would have been clear to anyone more familiar with AI tools that this was a good AI task: a well-scoped issue with failing tests. On the other hand, this is a low-level issue in a fresh implementation of a complex, relatively novel algorithm.&lt;/p&gt;
    &lt;p&gt;It figured out that I had merged &lt;code&gt;HighBits&lt;/code&gt; and &lt;code&gt;w1Encode&lt;/code&gt; into a single function for using it from Sign, and then reused it from Verify where &lt;code&gt;UseHint&lt;/code&gt; already produces the high bits, effectively taking the high bits of w1 twice in Verify.&lt;/p&gt;
    &lt;p&gt;Looking at the log, it loaded the implementation into the context and then immediately figured it out, without any exploratory tool use! After that it wrote itself a cute little test that reimplemented half of verification to confirm the hypothesis, wrote a mediocre fix, and checked the tests pass.&lt;/p&gt;
    &lt;p&gt;I threw the fix away and refactored &lt;code&gt;w1Encode&lt;/code&gt; to take high bits as input, and changed the type of the high bits, which is both clearer and saves a round-trip through Montgomery representation. Still, this 100% saved me a bunch of debugging time.&lt;/p&gt;
    &lt;head rend="h2"&gt;A second synthetic experiment&lt;/head&gt;
    &lt;p&gt;On Monday, I had also finished implementing signing with failing tests. There were two bugs, which I fixed in the following couple evenings.&lt;/p&gt;
    &lt;p&gt;The first one was due to somehow computing a couple hardcoded constants (1 and -1 in the Montgomery domain) wrong. It was very hard to find, requiring a lot of deep printfs and guesswork. Took me maybe an hour or two.&lt;/p&gt;
    &lt;p&gt;The second one was easier: a value that ends up encoded in the signature was too short (32 bits instead of 32 bytes). It was relatively easy to tell because only the first four bytes of the signature were the same, and then the signature lengths were different.&lt;/p&gt;
    &lt;p&gt;I figured these would be an interesting way to validate Claude’s ability to help find bugs in low-level cryptography code, so I checked out the old version of the change with the bugs (yay Jujutsu!) and kicked off a fresh Claude Code session with this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector it looks like it goes into an infinite loop, probably because it always rejects in the Fiat-Shamir with Aborts loop.&lt;/p&gt;
      &lt;p&gt;You can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Figure out why it loops forever, and get the tests to pass. ultrathink&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It spent some time doing printf debugging and chasing down incorrect values very similarly to how I did it, and then figured out and fixed the wrong constants. Took Claude definitely less than it took me. Impressive.&lt;/p&gt;
    &lt;p&gt;It gave up after fixing that bug even if the tests still failed, so I started a fresh session (on the assumption that the context on the wrong constants would do more harm than good investigating an independent bug), and gave it this prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am implementing ML-DSA in the Go standard library, and I just finished implementing signing, but running the tests against a known good test vector they don’t match.&lt;/p&gt;
      &lt;p&gt;You can run the tests with “bin/go test crypto/internal/fips140/mldsa”&lt;/p&gt;
      &lt;p&gt;You can find the code in src/crypto/internal/fips140/mldsa&lt;/p&gt;
      &lt;p&gt;Figure out what is going on. ultrathink&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It took a couple wrong paths, thought for quite a bit longer, and then found this one too. I honestly expected it to fail initially.&lt;/p&gt;
    &lt;p&gt;It’s interesting how Claude found the “easier” bug more difficult. My guess is that maybe the large random-looking outputs of the failing tests did not play well with its attention.&lt;/p&gt;
    &lt;p&gt;The fix it proposed was updating only the allocation’s length and not its capacity, but whatever, the point is finding the bug, and I’ll usually want to throw away the fix and rewrite it myself anyway.&lt;/p&gt;
    &lt;p&gt;Three out of three one-shot debugging hits with no help is extremely impressive. Importantly, there is no need to trust the LLM or review its output when its job is just saving me an hour or two by telling me where the bug is, for me to reason about it and fix it.&lt;/p&gt;
    &lt;p&gt;As ever, I wish we had better tooling for using LLMs which didn’t look like chat or autocomplete or “make me a PR.” For example, how nice would it be if every time tests fail, an LLM agent was kicked off with the task of figuring out why, and only notified us if it did before we fixed it?&lt;/p&gt;
    &lt;p&gt;For more low-level cryptography &lt;del&gt;bugs&lt;/del&gt; implementations, follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert. I promise I almost never post about AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;Enjoy the silliest floof. Surely this will help redeem me in the eyes of folks who consider AI less of a tool and more of something to be hated or loved.&lt;/p&gt;
    &lt;p&gt;My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.) Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45784179</guid><pubDate>Sat, 01 Nov 2025 18:41:56 +0000</pubDate></item><item><title>Visopsys: OS maintained by a single developer since 1997</title><link>https://visopsys.org/</link><description>&lt;doc fingerprint="a1fd7da091abb4c3"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome!&lt;/head&gt;
    &lt;p&gt;Visopsys is an alternative operating system for PC compatible computers. In development since 1997, this system is small, fast, and open source. It features a simple but functional graphical interface, pre-emptive multitasking, and virtual memory. Though it attempts to be compatible in a number of ways, Visopsys is not a clone of any other operating system. You can demo the distribution from a “live” USB stick, CD/DVD, or floppy disk … (read more).&lt;/p&gt;
    &lt;head rend="h4"&gt;Features of Visopsys&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Small &amp;amp; fast&lt;/item&gt;
      &lt;item&gt;Graphical user interface&lt;/item&gt;
      &lt;item&gt;Fully multitasking&lt;/item&gt;
      &lt;item&gt;100% protected mode&lt;/item&gt;
      &lt;item&gt;Open source, free software&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;News&lt;/head&gt;
    &lt;p&gt;[21.09.2023] – Version 0.92 is now available from the download page&lt;lb/&gt; [30.07.2021] – Version 0.91 was released&lt;lb/&gt; [16.04.2020] – Version 0.9 was released&lt;lb/&gt; [21.01.2020] – Version 0.85 was released&lt;lb/&gt; [15.05.2019] – Version 0.84 was released&lt;lb/&gt; [09.08.2018] – Version 0.83 was released&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45785858</guid><pubDate>Sat, 01 Nov 2025 22:07:49 +0000</pubDate></item><item><title>Pomelli</title><link>https://blog.google/technology/google-labs/pomelli/</link><description>&lt;doc fingerprint="71941a302cb5a8f1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Create on-brand marketing content for your business with Pomelli&lt;/head&gt;
    &lt;p&gt;Creating impactful, on-brand content can often require significant investment in time, budget, and design expertise. For small to medium-sized businesses (SMBs), this can be a major obstacle. That’s where Pomelli, our newest experiment from Google Labs in partnership with Google DeepMind, comes in. Pomelli is an AI marketing tool designed to help SMBs more easily generate scalable, on-brand social media campaigns to help grow their businesses.&lt;/p&gt;
    &lt;p&gt;Pomelli uses AI to understand your unique business and generate effective, tailored campaigns in just three steps.&lt;/p&gt;
    &lt;p&gt;1. Build your business DNA&lt;/p&gt;
    &lt;p&gt;Enter your website, and Pomelli will analyze it and create a "Business DNA" profile for your brand. By analyzing your website and existing images, Pomelli is able to automatically extract and understand your business’ unique brand identity. This profile includes your tone of voice, custom fonts, images and color palette.&lt;/p&gt;
    &lt;p&gt;All content Pomelli generates is grounded in this DNA, ensuring your content feels more authentic and consistent across all channels.&lt;/p&gt;
    &lt;p&gt;2. Generate tailored campaign ideas&lt;/p&gt;
    &lt;p&gt;Once your Business DNA is established, Pomelli generates tailored campaign ideas specifically for your business. This feature tackles the common pain point of coming up with fresh, strategic ideas, allowing you to quickly pick a campaign focus. If you have your own idea, you can type in a prompt to create content tailored exactly to your vision.&lt;/p&gt;
    &lt;p&gt;3. Edit and create high-quality, branded creatives&lt;/p&gt;
    &lt;p&gt;Finally, Pomelli creates a set of high-quality, on-brand marketing assets designed to help grow your brand across various channels, like your social media, your site and your ads. Browse through the generations and select the assets that best fit your campaign goals. You're in full control to make edits to the text or images right inside the tool. All assets can be downloaded and are ready to be used across your channels.&lt;/p&gt;
    &lt;p&gt;Pomelli is launching today as a public beta experiment in the United States, Canada, Australia and New Zealand in English. It’s an early experiment and it might take some time to get things right. Our goal is to build the highest quality experiments, so your feedback is appreciated. Give it a try and let us know what you think.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45786324</guid><pubDate>Sat, 01 Nov 2025 23:09:25 +0000</pubDate></item><item><title>How I use every Claude Code feature</title><link>https://blog.sshh.io/p/how-i-use-every-claude-code-feature</link><description>&lt;doc fingerprint="36cf0efb2d9d82af"&gt;
  &lt;main&gt;
    &lt;p&gt;I use Claude Code. A lot.&lt;/p&gt;
    &lt;p&gt;As a hobbyist, I run it in a VM several times a week on side projects, often with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; to vibe code whatever idea is on my mind. Professionally, part of my team builds the AI-IDE rules and tooling for our engineering team that consumes several billion tokens per month just for codegen.&lt;/p&gt;
    &lt;p&gt;The CLI agent space is getting crowded and between Claude Code, Gemini CLI, Cursor, and Codex CLI, it feels like the real race is between Anthropic and OpenAI. But TBH when I talk to other developers, their choice often comes down to what feels like superficials—a “lucky” feature implementation or a system prompt “vibe” they just prefer. At this point these tools are all pretty good. I also feel like folks often also over index on the output style or UI. Like to me the “you’re absolutely right!” sycophancy isn’t a notable bug; it’s a signal that you’re too in-the-loop. Generally my goal is to “shoot and forget”—to delegate, set the context, and let it work. Judging the tool by the final PR and not how it gets there.&lt;/p&gt;
    &lt;p&gt;Having stuck to Claude Code for the last few months, this post is my set of reflections on Claude Code’s entire ecosystem. We’ll cover nearly every feature I use (and, just as importantly, the ones I don’t), from the foundational &lt;code&gt;CLAUDE.md&lt;/code&gt; file and custom slash commands to the powerful world of Subagents, Hooks, and GitHub Actions. This post ended up a bit long and I’d recommend it as more of a reference than something to read in entirety. &lt;/p&gt;
    &lt;head rend="h2"&gt;CLAUDE.md&lt;/head&gt;
    &lt;p&gt;The single most important file in your codebase for using Claude Code effectively is the root &lt;code&gt;CLAUDE.md&lt;/code&gt;. This file is the agent’s “constitution,” its primary source of truth for how your specific repository works.&lt;/p&gt;
    &lt;p&gt;How you treat this file depends on the context. For my hobby projects, I let Claude dump whatever it wants in there.&lt;/p&gt;
    &lt;p&gt;For my professional work, our monorepo’s &lt;code&gt;CLAUDE.md&lt;/code&gt; is strictly maintained and currently sits at 13KB (I could easily see it growing to 25KB). &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It only documents tools and APIs used by 30% (arbitrary) or more of our engineers (else tools are documented in product or library specific markdown files)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We’ve even started allocating effectively a max token count for each internal tool’s documentation, almost like selling “ad space” to teams. If you can’t explain your tool concisely, it’s not ready for the&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Tips and Common Anti-Patterns&lt;/head&gt;
    &lt;p&gt;Over time, we’ve developed a strong, opinionated philosophy for writing an effective &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Start with Guardrails, Not a Manual. Your&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;should start small, documenting based on what Claude is getting wrong.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t&lt;/p&gt;&lt;code&gt;@&lt;/code&gt;-File Docs. If you have extensive documentation elsewhere, it’s tempting to&lt;code&gt;@&lt;/code&gt;-mention those files in your&lt;code&gt;CLAUDE.md&lt;/code&gt;. This bloats the context window by embedding the entire file on every run. But if you just mention the path, Claude will often ignore it. You have to pitch the agent on why and when to read the file. “For complex … usage or if you encounter a&lt;code&gt;FooBarError&lt;/code&gt;, see&lt;code&gt;path/to/docs.md&lt;/code&gt;for advanced troubleshooting steps.”&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Don’t Just Say “Never.” Avoid negative-only constraints like “Never use the&lt;/p&gt;&lt;code&gt;--foo-bar&lt;/code&gt;flag.” The agent will get stuck when it thinks it must use that flag. Always provide an alternative.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use&lt;/p&gt;&lt;code&gt;CLAUDE.md&lt;/code&gt;as a Forcing Function. If your CLI commands are complex and verbose, don’t write paragraphs of documentation to explain them. That’s patching a human problem. Instead, write a simple bash wrapper with a clear, intuitive API and document that. Keeping your&lt;code&gt;CLAUDE.md&lt;/code&gt;as short as possible is a fantastic forcing function for simplifying your codebase and internal tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a simplified snapshot:&lt;/p&gt;
    &lt;code&gt;# Monorepo

## Python
- Always ...
- Test with &amp;lt;command&amp;gt;
... 10 more ...

## &amp;lt;Internal CLI Tool&amp;gt;
... 10 bullets, focused on the 80% of use cases ...
- &amp;lt;usage example&amp;gt;
- Always ...
- Never &amp;lt;x&amp;gt;, prefer &amp;lt;Y&amp;gt;

For &amp;lt;complex usage&amp;gt; or &amp;lt;error&amp;gt; see path/to/&amp;lt;tool&amp;gt;_docs.md

...&lt;/code&gt;
    &lt;p&gt;Finally, we keep this file synced with an &lt;code&gt;AGENTS.md&lt;/code&gt; file to maintain compatibility with other AI IDEs that our engineers might be using.&lt;/p&gt;
    &lt;p&gt;If you are looking for more tips for writing markdown for coding agents see “AI Can’t Read Your Docs”, “AI-powered Software Engineering”, and “How Cursor (AI IDE) Works”.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Treat your &lt;code&gt;CLAUDE.md&lt;/code&gt; as a high-level, curated set of guardrails and pointers. Use it to guide where you need to invest in more AI (and human) friendly tools, rather than trying to make it a comprehensive manual.&lt;/p&gt;
    &lt;head rend="h2"&gt;Compact, Context, &amp;amp; Clear&lt;/head&gt;
    &lt;p&gt;I recommend running &lt;code&gt;/context&lt;/code&gt; mid coding session at least once to understand how you are using your 200k token context window (even with Sonnet-1M, I don’t trust that the full context window is actually used effectively). For us a fresh session in our monorepo costs a baseline ~20k tokens (10%) with the remaining 180k for making your change — which can fill up quite fast.&lt;/p&gt;
    &lt;p&gt;I have three main workflows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/compact&lt;/code&gt;(Avoid): I avoid this as much as possible. The automatic compaction is opaque, error-prone, and not well-optimized.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/clear&lt;/code&gt;+&lt;code&gt;/catchup&lt;/code&gt;(Simple Restart): My default reboot. I&lt;code&gt;/clear&lt;/code&gt;the state, then run a custom&lt;code&gt;/catchup&lt;/code&gt;command to make Claude read all changed files in my git branch.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;“Document &amp;amp; Clear” (Complex Restart): For large tasks. I have Claude dump its plan and progress into a&lt;/p&gt;&lt;code&gt;.md&lt;/code&gt;,&lt;code&gt;/clear&lt;/code&gt;the state, then start a new session by telling it to read the&lt;code&gt;.md&lt;/code&gt;and continue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Don’t trust auto-compaction. Use &lt;code&gt;/clear&lt;/code&gt; for simple reboots and the “Document &amp;amp; Clear” method to create durable, external “memory” for complex tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Slash Commands&lt;/head&gt;
    &lt;p&gt;I think of slash commands as simple shortcuts for frequently used prompts, nothing more. My setup is minimal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/catchup&lt;/code&gt;: The command I mentioned earlier. It just prompts Claude to read all changed files in my current git branch.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/pr&lt;/code&gt;: A simple helper to clean up my code, stage it, and prepare a pull request.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IMHO if you have a long list of complex, custom slash commands, you’ve created an anti-pattern. To me the entire point of an agent like Claude is that you can type almost whatever you want and get a useful, mergable result. The moment you force an engineer (or non-engineer) to learn a new, documented-somewhere list of essential magic commands just to get work done, you’ve failed.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use slash commands as simple, personal shortcuts, not as a replacement for building a more intuitive &lt;code&gt;CLAUDE.md&lt;/code&gt; and better-tooled agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Subagents&lt;/head&gt;
    &lt;p&gt;On paper, custom subagents are Claude Code’s most powerful feature for context management. The pitch is simple: a complex task requires &lt;code&gt;X&lt;/code&gt; tokens of input context (e.g., how to run tests), accumulates &lt;code&gt;Y&lt;/code&gt; tokens of working context, and produces a &lt;code&gt;Z&lt;/code&gt; token answer. Running &lt;code&gt;N&lt;/code&gt; tasks means &lt;code&gt;(X + Y + Z) * N&lt;/code&gt; tokens in your main window.&lt;/p&gt;
    &lt;p&gt;The subagent solution is to farm out the &lt;code&gt;(X + Y) * N&lt;/code&gt; work to specialized agents, which only return the final &lt;code&gt;Z&lt;/code&gt; token answers, keeping your main context clean.&lt;/p&gt;
    &lt;p&gt;I find they are a powerful idea that, in practice, custom subagents create two new problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;They Gatekeep Context: If I make a&lt;/p&gt;&lt;code&gt;PythonTests&lt;/code&gt;subagent, I’ve now hidden all testing context from my main agent. It can no longer reason holistically about a change. It’s now forced to invoke the subagent just to know how to validate its own code.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They Force Human Workflows: Worse, they force Claude into a rigid, human-defined workflow. I’m now dictating how it must delegate, which is the very problem I’m trying to get the agent to solve for me.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My preferred alternative is to use Claude’s built-in &lt;code&gt;Task(...)&lt;/code&gt; feature to spawn clones of the general agent.&lt;/p&gt;
    &lt;p&gt;I put all my key context in the &lt;code&gt;CLAUDE.md&lt;/code&gt;. Then, I let the main agent decide when and how to delegate work to copies of itself. This gives me all the context-saving benefits of subagents without the drawbacks. The agent manages its own orchestration dynamically.&lt;/p&gt;
    &lt;p&gt;In my “Building Multi-Agent Systems (Part 2)” post, I called this the “Master-Clone” architecture, and I strongly prefer it over the “Lead-Specialist” model that custom subagents encourage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Custom subagents are a brittle solution. Give your main agent the context (in &lt;code&gt;CLAUDE.md&lt;/code&gt;) and let it use its own &lt;code&gt;Task/Explore(...)&lt;/code&gt; feature to manage delegation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resume, Continue, &amp;amp; History&lt;/head&gt;
    &lt;p&gt;On a simple level, I use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue&lt;/code&gt; frequently. They’re great for restarting a bugged terminal or quickly rebooting an older session. I’ll often &lt;code&gt;claude --resume&lt;/code&gt; a session from days ago just to ask the agent to summarize how it overcame a specific error, which I then use to improve our &lt;code&gt;CLAUDE.md&lt;/code&gt; and internal tooling.&lt;/p&gt;
    &lt;p&gt;More in the weeds, Claude Code stores all session history in &lt;code&gt;~/.claude/projects/&lt;/code&gt; to tap into the raw historical session data. I have scripts that run meta-analysis on these logs, looking for common exceptions, permission requests, and error patterns to help improve agent-facing context.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use &lt;code&gt;claude --resume&lt;/code&gt; and &lt;code&gt;claude --continue &lt;/code&gt;to restart sessions and uncover buried historical context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooks&lt;/head&gt;
    &lt;p&gt;Hooks are huge. I don’t use them for hobby projects, but they are critical for steering Claude in a complex enterprise repo. They are the deterministic “must-do” rules that complement the “should-do” suggestions in &lt;code&gt;CLAUDE.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We use two types:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Block-at-Submit Hooks: This is our primary strategy. We have a&lt;/p&gt;&lt;code&gt;PreToolUse&lt;/code&gt;hook that wraps any&lt;code&gt;Bash(git commit)&lt;/code&gt;command. It checks for a&lt;code&gt;/tmp/agent-pre-commit-pass&lt;/code&gt;file, which our test script only creates if all tests pass. If the file is missing, the hook blocks the commit, forcing Claude into a “test-and-fix” loop until the build is green.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hint Hooks: These are simple, non-blocking hooks that provide “fire-and-forget” feedback if the agent is doing something suboptimal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We intentionally do not use “block-at-write” hooks (e.g., on &lt;code&gt;Edit&lt;/code&gt; or &lt;code&gt;Write&lt;/code&gt;). Blocking an agent mid-plan confuses or even “frustrates” it. It’s far more effective to let it finish its work and then check the final, completed result at the commit stage.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use hooks to enforce state validation at commit time (&lt;code&gt;block-at-submit&lt;/code&gt;). Avoid blocking at write time—let the agent finish its plan, then check the final result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planning Mode&lt;/head&gt;
    &lt;p&gt;Planning is essential for any “large” feature change with an AI IDE.&lt;/p&gt;
    &lt;p&gt;For my hobby projects, I exclusively use the built-in planning mode. It’s a way to align with Claude before it starts, defining both how to build something and the “inspection checkpoints” where it needs to stop and show me its work. Using this regularly builds a strong intuition for what minimal context is needed to get a good plan without Claude botching the implementation.&lt;/p&gt;
    &lt;p&gt;In our work monorepo, we’ve started rolling out a custom planning tool built on the Claude Code SDK. Its similar to native plan mode but heavily prompted to align its outputs with our existing technical design format. It also enforces our internal best practices—from code structure to data privacy and security—out of the box. This lets our engineers “vibe plan” a new feature as if they were a senior architect (or at least that’s the pitch).&lt;/p&gt;
    &lt;p&gt;The Takeaway: Always use the built-in planning mode for complex changes to align on a plan before the agent starts working.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills&lt;/head&gt;
    &lt;p&gt;I agree with Simon Willison’s: Skills are (maybe) a bigger deal than MCP.&lt;/p&gt;
    &lt;p&gt;If you’ve been following my posts, you’ll know I’ve drifted away from MCP for most dev workflows, preferring to build simple CLIs instead (as I argued in “AI Can’t Read Your Docs”). My mental model for agent autonomy has evolved into three stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Single Prompt: Giving the agent all context in one massive prompt. (Brittle, doesn’t scale).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tool Calling: The “classic” agent model. We hand-craft tools and abstract away reality for the agent. (Better, but creates new abstractions and context bottlenecks).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scripting: We give the agent access to the raw environment—binaries, scripts, and docs—and it writes code on the fly to interact with them.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this model in mind, Agent Skills are the obvious next feature. They are the formal productization of the “Scripting” layer.&lt;/p&gt;
    &lt;p&gt;If, like me, you’ve already been favoring CLIs over MCP, you’ve been implicitly getting the benefit of Skills all along. The &lt;code&gt;SKILL.md&lt;/code&gt; file is just a more organized, shareable, and discoverable way to document these CLIs and scripts and expose them to the agent.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Skills are the right abstraction. They formalize the “scripting”-based agent model, which is more robust and flexible than the rigid, API-like model that MCP represents.&lt;/p&gt;
    &lt;head rend="h2"&gt;MCP (Model Context Protocol)&lt;/head&gt;
    &lt;p&gt;Skills don’t mean MCP is dead (see also “Everything Wrong with MCP”). Previously, many built awful, context-heavy MCPs with dozens of tools that just mirrored a REST API (&lt;code&gt;read_thing_a()&lt;/code&gt;, &lt;code&gt;read_thing_b()&lt;/code&gt;, &lt;code&gt;update_thing_c()&lt;/code&gt;). &lt;/p&gt;
    &lt;p&gt;The “Scripting” model (now formalized by Skills) is better, but it needs a secure way to access the environment. This to me is the new, more focused role for MCP.&lt;/p&gt;
    &lt;p&gt;Instead of a bloated API, an MCP should be a simple, secure gateway that provides a few powerful, high-level tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;download_raw_data(filters…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;take_sensitive_gated_action(args…)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;execute_code_in_environment_with_state(code…)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this model, MCP’s job isn’t to abstract reality for the agent; its job is to manage the auth, networking, and security boundaries and then get out of the way. It provides the entry point for the agent, which then uses its scripting and &lt;code&gt;markdown&lt;/code&gt; context to do the actual work.&lt;/p&gt;
    &lt;p&gt;The only MCP I still use is for Playwright, which makes sense—it’s a complex, stateful environment. All my stateless tools (like Jira, AWS, GitHub) have been migrated to simple CLIs.&lt;/p&gt;
    &lt;p&gt;The Takeaway: Use MCPs that act as data gateways. Give the agent one or two high-level tools (like a raw data dump API) that it can then script against.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code SDK&lt;/head&gt;
    &lt;p&gt;Claude Code isn’t just an interactive CLI; it’s also a powerful SDK for building entirely new agents—for both coding and non-coding tasks. I’ve started using it as my default agent framework over tools like LangChain/CrewAI for most new hobby projects.&lt;/p&gt;
    &lt;p&gt;I use it in three main ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Massive Parallel Scripting: For large-scale refactors, bug fixes, or migrations, I don’t use the interactive chat. I write simple bash scripts that call&lt;/p&gt;&lt;code&gt;claude -p “in /pathA change all refs from foo to bar”&lt;/code&gt;in parallel. This is far more scalable and controllable than trying to get the main agent to manage dozens of subagent tasks.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Building Internal Chat Tools: The SDK is perfect for wrapping complex processes in a simple chat interface for non-technical users. Like an installer that, on error, falls back to the Claude Code SDK to just fix the problem for the user. Or an in-house “v0-at-home” tool that lets our design team vibe-code mock frontends in our in-house UI framework, ensuring their ideas are high-fidelity and the code is more directly usable in frontend production code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rapid Agent Prototyping: This is my most common use. It’s not just for coding. If I have an idea for any agentic task (e.g., a “threat investigation agent” that uses custom CLIs or MCPs), I use the Claude Code SDK to quickly build and test the prototype before committing to a full, deployed scaffolding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: The Claude Code SDK is a powerful, general-purpose agent framework. Use it for batch-processing code, building internal tools, and rapidly prototyping new agents before you reach for more complex frameworks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Claude Code GHA&lt;/head&gt;
    &lt;p&gt;The Claude Code GitHub Action (GHA) is probably one of my favorite and most slept on features. It’s a simple concept: just run Claude Code in a GHA. But this simplicity is what makes it so powerful.&lt;/p&gt;
    &lt;p&gt;It’s similar to Cursor’s background agents or the Codex managed web UI but is far more customizable. You control the entire container and environment, giving you more access to data and, crucially, much stronger sandboxing and audit controls than any other product provides. Plus, it supports all the advanced features like Hooks and MCP.&lt;/p&gt;
    &lt;p&gt;We’ve used it to build custom “PR-from-anywhere” tooling. Users can trigger a PR from Slack, Jira, or even a CloudWatch alert, and the GHA will fix the bug or add the feature and return a fully tested PR1.&lt;/p&gt;
    &lt;p&gt;Since the GHA logs are the full agent logs, we have an ops process to regularly review these logs at a company level for common mistakes, bash errors, or unaligned engineering practices. This creates a data-driven flywheel: Bugs -&amp;gt; Improved CLAUDE.md / CLIs -&amp;gt; Better Agent.&lt;/p&gt;
    &lt;code&gt;$ query-claude-gha-logs --since 5d | claude -p “see what the other claudes were getting stuck on and fix it, then put up a PR“&lt;/code&gt;
    &lt;p&gt;The Takeaway: The GHA is the ultimate way to operationalize Claude Code. It turns it from a personal tool into a core, auditable, and self-improving part of your engineering system.&lt;/p&gt;
    &lt;head rend="h2"&gt;settings.json&lt;/head&gt;
    &lt;p&gt;Finally, I have a few specific &lt;code&gt;settings.json&lt;/code&gt; configurations that I’ve found essential for both hobby and professional work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HTTPS_PROXY&lt;/code&gt;/&lt;code&gt;HTTP_PROXY&lt;/code&gt;: This is great for debugging. I’ll use it to inspect the raw traffic to see exactly what prompts Claude is sending. For background agents, it’s also a powerful tool for fine-grained network sandboxing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MCP_TOOL_TIMEOUT&lt;/code&gt;/&lt;code&gt;BASH_MAX_TIMEOUT_MS&lt;/code&gt;: I bump these. I like running long, complex commands, and the default timeouts are often too conservative. I’m honestly not sure if this is still needed now that bash background tasks are a thing, but I keep it just in case.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ANTHROPIC_API_KEY&lt;/code&gt;: At work, we use our enterprise API keys (via apiKeyHelper). It shifts us from a “per-seat” license to “usage-based” pricing, which is a much better model for how we work.&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;It accounts for the massive variance in developer usage (We’ve seen 1:100x differences between engineers).&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It lets engineers to tinker with non-Claude-Code LLM scripts, all under our single enterprise account.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;“permissions”&lt;/code&gt;: I’ll occasionally self-audit the list of commands I’ve allowed Claude to auto-run.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Takeaway: Your &lt;code&gt;settings.json&lt;/code&gt; is a powerful place for advanced customization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That was a lot, but hopefully, you find it useful. If you’re not already using a CLI-based agent like Claude Code or Codex CLI, you probably should be. There are rarely good guides for these advanced features, so the only way to learn is to dive in.&lt;/p&gt;
    &lt;p&gt;To me, a fairly interesting philosophical question is how many reviewers should a PR get that was generated directly from a customer request (no internal human prompter)? We’ve settled on 2 human approvals for any AI-initiated PR for now, but it is kind of a weird paradigm shift (for me at least) when it’s no longer a human making something for another human to review.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45786738</guid><pubDate>Sun, 02 Nov 2025 00:13:27 +0000</pubDate></item><item><title>LM8560, the eternal chip from the 1980 years</title><link>https://www.tycospages.com/other-themes/lm8560-the-eternal-chip-from-the-1980-years/</link><description>&lt;doc fingerprint="c6401d002374c3f4"&gt;
  &lt;main&gt;
    &lt;p&gt;Quick jumps to:&lt;lb/&gt;– What digital alarm clocks before it were like&lt;lb/&gt;– Why is the LM8560 so costs effective? The trick, how it works&lt;lb/&gt;– Typical issues&lt;lb/&gt;– Its weak point is also its strength&lt;lb/&gt;– Its “hidden” functions&lt;lb/&gt;– Some unwanted behaviors&lt;lb/&gt;– That evil beep beep!&lt;lb/&gt;– Limitations of use&lt;lb/&gt;– Make a quartz clock / DC supply clock with the LM8560&lt;lb/&gt;– Operating modes&lt;lb/&gt;– Its relatives and predecessors&lt;lb/&gt;– The Duplex display. A not versatile but clever solution&lt;lb/&gt;– Not the best chip for electronics hobbyists&lt;lb/&gt;– My alarm clocks&lt;lb/&gt;– My first clock radio in 1986, by Majestic&lt;lb/&gt;– Sony ICF-C102&lt;lb/&gt;– Sony ICF-C290&lt;lb/&gt;– Thomson CR61, my final clock radio&lt;lb/&gt;– The modern clock radios&lt;lb/&gt;– LM8560 manufacturers list and alternative ICs&lt;/p&gt;
    &lt;p&gt;Almost everybody has had one, or even has still one of it at home. Almost everybody who had it, kept it in his/her immediate proximity while sleeping…&lt;/p&gt;
    &lt;p&gt;The LM8560 is the integrated circuit (IC), which was built in almost all the digital alarm clocks and clock radios, with numeric LED display, that have been produced from around 1985, or even the first 1980 years, up to the 2010 years. A very few and last models are still produced in year 2023. But they are fading out.&lt;lb/&gt;Alarm clocks and clock radios with numeric LED displays are disappearing today, being replaced by LCD displays, merely because of trend reasons. Furthermore, the red color for the LED displays is also becoming rare, because of trend reasons as well, being replaced by green, or worse, by blue LEDs.&lt;lb/&gt;It doesn’t matter whether it was a major brand, like Sony or a no-name brand sold for 10€ at a discounter market, the brain of the alarm clock was almost always the same. The now ‘obsolete’ but timeless LM8560, originally made by the Japanese Sanyo.&lt;lb/&gt;The LM8560 is a low power consumption MOS integrated circuit.&lt;/p&gt;
    &lt;p&gt;The letters before the number 8560 printed on the IC vary depending by the chip manufacturer. “LM” were used by the original manufacturer Sanyo, which was also the inventor of the chip. The letters after the number, if present, are of virtually no importance. Today some illegal clones made by Chinese manufacturers are labeled with “LM”, but they are not the original by Sanyo and I have no idea about the quality and durability of these chips.&lt;lb/&gt;The TMS3450NL was the clone made by Texas Instruments.&lt;/p&gt;
    &lt;p&gt;The modern alarm clocks with LED display are made with micro controller chips. I will show it later.&lt;/p&gt;
    &lt;p&gt;– Why am I talking here about this ‘obsolete’ chip?&lt;/p&gt;
    &lt;p&gt;Because it had its debut in the early or mid 1980s and it is one of the few things from those years that has survived intact to this day, still working. This chip played an important role in the falling of price of the digital alarm clocks.&lt;lb/&gt;Although it is now considered an obsolete chip, its modern successors based on a programmed micro controller, built in modern budged clock radios, do not offer much more functionalities. Sanyo and other licensed occidental manufacturers have discontinued the production of the LM8560, but it is still produced today by some Chinese chip manufacturers, under license of Sanyo. It is also copied by not authorized Chinese manufactures, of unknown quality. Therefore the LM8560 has earned a place in the list of the longest-living chips, still being produced the same as it was, for about 40 years, today in 2023. I don’t know the year of its first introduction, but it might be around 1985. If anybody has any info about this, please let me know.&lt;lb/&gt;In some alarm clocks with LED display, also a slightly more updated version of the LM8560 was used, the LM8562 or an equivalent chip. It offered the possibility to set 2 different alarm times, and it had “Slow”, “Fast” buttons to set the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;– What digital alarm clocks before it were like&lt;/head&gt;
    &lt;p&gt;Until the late 1970s and early 1980s, digital alarm clocks were somewhat more challenging cost devices than today. “Digital” doesn’t mean necessarily “electronic”. It means that the time is show actively by digits and not by hands.&lt;lb/&gt;The most primitive models did not have lighting digits. The numbers were printed on thin plastic plates, which were flipped by the slow rotation of a step by step electric motor.&lt;/p&gt;
    &lt;p&gt;re LEDs became more affordable, the alarm clocks with luminous digits were made using fluorescent displays, like the displays of many DVD-Blu Ray players of today, or many Hi-Fi sets. Usually the fluorescent display used in alarm clocks had a light greenish color. Fluorescent displays were, and are, much more expensive to build and complicate to drive than LEDs. In addition, alarm clocks counted the time mostly using a quartz crystal. Often the word “Quartz” was printed proudly on the case of the clock. The control circuit of the display and the quartz circuit involved some construction complexity and a significant number of components. The lower price for a clock radio in the early 1980s was not cheap. Let’s say about €70 today. The size of the alarm clocks was also considerably larger.&lt;/p&gt;
    &lt;p&gt;Clocks of this kind never appealed me. They tastes too much “mechanical” and rudimentary, whereas I like electronic since ever.&lt;/p&gt;
    &lt;p&gt;Before LEDs became more affordable, the alarm clocks with luminous digits were made using fluorescent displays, like the displays of many DVD-Blu Ray players of today, or many Hi-Fi sets. Usually the fluorescent display used in alarm clocks had a light greenish color. Fluorescent displays were, and are, much more expensive to build and complicate to drive than LEDs. In addition, alarm clocks counted the time mostly using a quartz crystal. Often the word “Quartz” was printed proudly on the case of the clock. The control circuit of the display and the quartz circuit involved some construction complexity and a significant number of components. The lower price for a clock radio in the early 1980s was not cheap. Let’s say about €70 today. The size of the alarm clocks was also considerably larger.&lt;/p&gt;
    &lt;p&gt;With the progress of electronics and the decreasing price of LEDs, the LM8560 came. A less than half a Euro chip (adjusted to today inflation), that contains all the functions you need to build a basic digital alarm clock. You just need some LEDs to arrange the display and very few external components around it. The radio is to be added separately, if needed.&lt;lb/&gt;The arrival of the LM8560 made possible to build digital alarm clocks for half the price, or less, of those with fluorescent display and quartz. The manufacturing became so much cheaper and simplified, that for the first time clock radios were even being offered as prizes by some supermarkets for the collection of their purchase points.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Why is the LM8560 so cost effective? The trick, how it works&lt;/head&gt;
    &lt;p&gt;The LM8560 is not a programmed micro controller. In the ’80s programmable micro controllers were no cheap ware at all.&lt;lb/&gt;The LM8560 is a logic chip, made of fixed integrated components, as logical ports, flip-flop circuits, comparators, operational amplifiers, etc. All them fitted in the same chip and connected among each others in order to execute the desired functions. The functions can’t be changed, without to change physically the content of the chip.&lt;/p&gt;
    &lt;p&gt;Being not a programmed micro controller, the LM8560 is also a virtually eternal component. Many modern micro controllers incorporate a flash memory to store the software that let the controller work and execute the desired functions. Flash memories retain their content not for an unlimited lifespan. It may be several decades, but before or later it comes the day when they begin to lose their content, and the micro controller stops to work. This can’t happen to LM8560, because it doesn’t contain any flash memory.&lt;/p&gt;
    &lt;p&gt;An electronic clock consists essentially of 2 different parts. A frequency generator, that generates a stable frequency and one part counts the waves coming from the generated frequency. Counting the waves that pass, the passing of seconds is counted. In quartz watches, the frequency generator is a quartz oscillator. In addition, if you want obtain a very good accuracy in quartz clocks, it is necessary to add some components to make the quartz frequency adjustable. A calibration process has to be performed individually during the manufacturing of each clock. This increases the time and cost for the workers needed for the manufacturing.&lt;lb/&gt;The LM8560 does not need any calibration operation. As soon as it is assembled, it is ready to use with no mistakes! It contains only the counter and the logic for to manage the alarm and clock functions. It has no frequency generator.&lt;lb/&gt;The LM8560 simply counts the waves coming from the AC power outlet. It can be 50 Hz or 60 Hz, depending on the Countries. In my case it is 50 Hz. It means that 50 waves per second come in. Every 50 counted waves, one second of time is counted. Every 60 counted seconds the minute digits are increased by one. Every 60 minutes the hour digits are increased by one.&lt;lb/&gt;In addition to the counter and the logic for the clock/alarm functions, the LM8560 contains the circuit to drive directly the LED display.&lt;/p&gt;
    &lt;p&gt;The alarm clocks with LCD displays, even those from the ’80s years, are instead quartz-based, like the wristwatches, and they use different components.&lt;/p&gt;
    &lt;p&gt;An important function integrated in the LM8560 is the power backup system. It keeps the time running in the event of a power blackout. It is made switching the power supply of the chip to a 9V battery. During the blackout, a local simple RC (resistance-capacitor) oscillator is used. The frequency generated by the RC oscillator is very approximated, up to +/-10% error. It means that while the AC power is missing, the clock can run forwards or backwards up to 1 minute every 10 minutes! If the AC power is missing for a long time, it is normal to find the time several minutes shifted. It seems that the LM8560 was the first IC clock to incorporate this important function, making it a very smart and convenient chip. Only the resistance and the capacitor for the backup oscillator, are required to be mounted externally to the chip.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Typical Issues of clocks with the LM8560&lt;/head&gt;
    &lt;p&gt;You might experience that some clocks with the LM8560 may run too fast in comparison with other clocks, also with the LM8560. Cheap models are more subjected to this fault. It is due to the missing of proper filtering on the 50/60Hz input signal for the LM8560. Spikes and disturbs present in the AC line can be wrongly counted as clock waves by the chip. Just add a 0,1 or 01,01 µF ceramic capacitor in parallel to each rectifier diode of the power supply of the clock, and one between the pin 25 (50/60Hz input) and pin 15 (VSS) of the LM8560.&lt;/p&gt;
    &lt;p&gt;If power blackouts are frequent in your zone and your alarm clock runs excessively fast or too slow while there is no AC power, you can fix it easy. Simply adjust with a trimmer the value of the resistor that is in parallel to the capacitor of the backup oscillator (see the datasheet), in order to obtain a frequency of exact 900 Hz, or adjust it simply by trying. You can roughly calculate the required Ohm variation in %, observing how many seconds the time gets shifted in 1 or few minutes without power.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Its weak point is also its strength&lt;/head&gt;
    &lt;p&gt;As mentioned before, the LM8560 merely counts the waves of the AC power. But there is nothing that lets it know whether the waves are coming exactly 50 times per second or not. If 100 waves would come per second, the clock would run forwards half an hour every hour!&lt;lb/&gt;Normally, the frequency of the AC power in our sockets has a tolerance of +/- 2%. It means that the clock could run forward or backwards up to 14 minutes a day! Far away from the constancy of a quartz oscillator. Therefore, this would make the frequency of the AC power unsuitable as source to drive a clock, since very small variations in the frequency result in large hourly variations.&lt;lb/&gt;Why doesn’t this happen instead? Frequency variations are caused by small random variations in the big generators in the power plants. Sometimes the frequency is a little higher than the standard value by a small amount, sometimes it is a little lower. These variations will never be the exactly equal between each other, so the errors cancel among each other and roughly the average frequency remains constant, very close to its standard value.&lt;lb/&gt;Furthermore, in power plants the frequency is constantly monitored and kept under control. When it is detected that over a certain time, the average frequency has been higher by a certain %, the frequency is then lowered by the same % for the same time, so the overall balance is brought back to equal.&lt;lb/&gt;In fact, if you compare second accurately, the time of an alarm clock with the LM8560 to a reference clock, like a radio-controlled clock, you will find that the time difference is not constant. It varies day by day, and even during the same day. Sometimes you will find that the alarm clock is 30 seconds, or max 1-2 minutes in advance. Meanwhile some days you will find it 1-2 minutes back. Some days it will be almost aligned. Therefore, normally you never need to adjust the clock. But it depends by the quality and stability of the AC frequency coming from the power plant where you are supplied from. Moving from Italy to Germany, I noticed that my clock radio with the LM8560 is considerably more accurate in Germany than Italy. Here, I never need to adjust it.&lt;lb/&gt;Anyway, even the quartz clocks are not so accurate as they could be. A quartz clock will run constantly a little bit in advance or in delay, forcing you to adjust it at regular intervals. In theory, a well-calibrated quartz clock could provide an accuracy of +/-1 second per month, which is very good. I could obtain even better, with a wristwatch that I had in the past. But in the facts, the most quartz clocks sold on the market are built to provide an accuracy of +/-30 seconds per month. A not good performance at all.&lt;lb/&gt;I mentioned about comparing “second accurately” the time of an LM8560 clock… But how can you display the running seconds on the display of a clock with an LM8560? Don’t you have yet find out it by yourself, playing with the clock’s buttons? Below I will explain all the “hidden” tricks of the LM8560.&lt;/p&gt;
    &lt;p&gt;As first, if you want to know, without opening it, whether your alarm clock is built with the chip LM8560, just check some of these points:&lt;/p&gt;
    &lt;p&gt;1) The display is strictly 4-digits LEDs, not with liquid crystals.&lt;lb/&gt;2) It has only one alarm time that can be set. If your alarm clock is old, it has 2 alarm times, it should be one of its bigger brother chips, rare to find, like the LM8562 or the LC85632.&lt;lb/&gt;3) It has the store place for a 9V backup battery. Not any batteries with lower voltage.&lt;lb/&gt;4) It has this 6 or 7 function buttons: Time, Alarm, Hour, Minute, Sleep, Snooze. Eventually there is also an “Alarm reset” button, but normally the two buttons are wired together, so that the “Alarm” button has also the function of alarm reset.&lt;lb/&gt;5) It has some “hidden functions”,like the possibility to display seconds, as I will describe below.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Its “hidden” functions&lt;/head&gt;
    &lt;p&gt;The hidden key combinations:&lt;/p&gt;
    &lt;p&gt;1) Display seconds: Hold down the “Alarm” button and press “Sleep”. Or do vice-versa as well. Then you can release one of the 2 buttons. On the left side of the display, the digit of the minute units is shown. On the right part, the seconds runs. This is my favorite function and no manufacturer of alarm clocks has ever implemented a dedicated button to display the seconds. All this to save few Eurocents on the final cost. I modified my clock radio, so I can keep the seconds displayed as long as I want, and it is very convenient to adjust the clock, second accurately.&lt;/p&gt;
    &lt;p&gt;2) While displaying seconds, press the “Hour” button: the seconds will be set to zero. If you press it between the 30-59th second, the minute will be increased by one.&lt;/p&gt;
    &lt;p&gt;3) While displaying seconds, press the “Minute” button: the time stops! This function is also very useful to align the clock, seconds accurately.&lt;/p&gt;
    &lt;p&gt;4) While displaying seconds, press both the “Hour” and “Minute” buttons at the same time:&lt;lb/&gt;The clock time resets to 0:00&lt;/p&gt;
    &lt;p&gt;5) Hold down the “Time” (or “Clock”) button and then keep pressed “Hours” and “Minutes” together: both hours and minutes increase at the same time at the rate of 2 digits per second. This is useful if you need to set up the clock time from zero, when you are in a time with high digits numbers.&lt;/p&gt;
    &lt;p&gt;6) Hold down “Alarm” and then press “Hour” and “Minute” together: the alarm time is set to 0:00&lt;/p&gt;
    &lt;p&gt;7) Hold down “Sleep” and then press “Hours”: the timer for the radio sets to 1 hour and 59 minutes, the max possible time for the sleep function. This setting possibility sometime is written in the user manual, sometimes not.&lt;/p&gt;
    &lt;p&gt;One thing I didn’t notice, and I always missed to read in the user manual, is that to turn off the alarm until the next day, just press the “Alarm” button (my clock radio had no dedicated “Alarm reset” button). Instead, I used to move the alarm selector back to the OFF position and then moving it again to the Alarm position, every day. So if anyone else missed it, just press the Alarm button, or the “Alarm reset” if present.&lt;/p&gt;
    &lt;head rend="h2"&gt;And some unwanted behaviors…&lt;/head&gt;
    &lt;p&gt;Due to the way how the buttons are wired to the LM8560 on the most clocks, the clocks with this chip have also some unwanted behaviors. Behaviors that can have unpleasant consequences, when the next day the alarm clock has to wake you!&lt;/p&gt;
    &lt;p&gt;1) Hold down the “Hour” button and then press “Alarm”. You would expect that only the hours of the alarm time get increased. In reality, the hours of the clock time increase too!&lt;/p&gt;
    &lt;p&gt;2) Hold down the “Minute” key and then press “Alarm”. The minutes of the clock increase too!&lt;/p&gt;
    &lt;p&gt;3) The same happens if you hold down the “Minute” or “Hour” button and then press “Sleep”.&lt;lb/&gt;Therefore be carefully when adjusting the alarm or the sleep time! If you inadvertently decrease the pressure on the “Alarm” or “Sleep” button and its electric contact opens even for a fraction of second, also the clock time will be increased! It is easy to happen when you set the alarm or sleep time using only one hand.&lt;lb/&gt;If the mist happens and you don’t notice that the clock time has increased… the next day the alarm will ring a few minutes earlier, in the best case, or at worst, a few hours earlier!&lt;/p&gt;
    &lt;p&gt;4) When the alarm rings (buzzer or radio), independently by whether you have already temporarily snoozed it, press the Sleep button and then the Snooze button. The alarm will be deactivated completely, until the next day. As if you press the Alarm or “Alarm reset” button! This seems to be a bug or “feature” of the chip itself. It is not related to the wiring of the buttons. Therefore, be carefully what keys you press, when you want just snooze the alarm!&lt;/p&gt;
    &lt;p&gt;5) When you press the Snooze button after that you have already snoozed the alarm, the snooze timer resets. Example: The alarm rings at 8:00 and you snooze it immediately. The alarm now is silenced and it would ring again at 8:09. But if at 8:08 you press the snooze button, it will ring again at 8:17, not at 8:09! Bug or feature, keep it in mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical curiosities&lt;/head&gt;
    &lt;p&gt;The evil beep beep!&lt;/p&gt;
    &lt;p&gt;I hope no one really uses to wake up that evil loud and crazy unpleasant beep! beep! buzzer. A friend of mine didn’t know that you could also wake up listening the radio, and for years he had kept the alarm switch on that evil beep set, instead of the radio! But where did that beep come from? That beep is the same in all radio alarm clocks with the LM8560 and it has a frequency of 900 Hz. This frequency is generated by the RC oscillator that is used for the power back up system.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Limitations of use&lt;/head&gt;
    &lt;p&gt;Clocks based on counting the frequency of the AC power are not suitable to operate in places where the AC voltage does not come directly from the fixed national power grid. For example, portable emergency generators, camping generators, or large UPS power backup system, which have no active control over frequency changes over time. The alarm clock will therefore tend to shift the time by several minutes in a few hours. The could happens onboard ships. Who takes one of these alarm clocks on a cruise could do the experiment. Just synchronize it with a wristwatch and check after a few hours or days, how much the time will be shifted.&lt;lb/&gt;Quartz alarm clocks have not this limitation, because the oscillation frequency of the quartz is totally independent from the accuracy of the AC frequency.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Make a quartz clock of it&lt;/head&gt;
    &lt;p&gt;You can make a quartz clock using an LM8560, by adding a small circuit which contains the quartz oscillator and a frequency divisor, which downclocks the quartz frequency to 50 or 60Hz, the value that the LM8560 expects to count. You can do it using a quartz with frequency 3,579 MHz and the frequency divisor IC MM5369, as shown in this diagram of a clock built with the LM8365, where the pin 36 is the input of the 50/60Hz frequency.&lt;lb/&gt;Simply connect the 50/60Hz output coming from the R7 on the pin 1 of the quartz oscillator built with the MM5369, to the 50/60Hz input pin 25 of the LM8560.&lt;/p&gt;
    &lt;p&gt;Once you have added a quartz, It is also possible to feed the clock directly with DC power supply, if you need. You have to add 2 or 3 transistors to use the 50/60 Hz clock signal generated by the quartz, to switch the DC power alternately between the two groups of cathodes of the duplex display. If the segments are not displayed correctly, exchange the 2 lines to the display.&lt;lb/&gt;Look at this schematic. To feed the whole circuit with DC, simply the components on the left of the C1, and apply the DC in parallel to C1, which should be no more necessary too. See also this schematic.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Operating modes&lt;/head&gt;
    &lt;p&gt;The LM8560 cm be set to display the time in 12 or 24 hour format, and it can be set to count the time using an AC power frequency of 50Hz (as in Europe) or 60 Hz (as in north America). These settings are made by adding or not, bridges to the related pins of the LM8560.&lt;/p&gt;
    &lt;head rend="h2"&gt;– Its relatives and predecessors&lt;/head&gt;
    &lt;p&gt;The LM8560 has a family of important predecessors, but these have been dropped almost immediately in favor of the 8560. This was the MM53xx family, such as the MM5309 or the -11-12-13-14-15. All of these chips (except I think the MM5312) had the interesting ability to control 6-digit displays, so the seconds could be displayed together with hours and minutes on the display.&lt;lb/&gt;Also the family MM53xx worked on the same principle as the LM8560, counting the AC frequency, but it did not have a backup system built in for power blackout. The MM53xx could drive a standard 7 segments display common cathode, multiplexed. It needed also a few more external components to drive the display. Probably to save on all those extra components, and because of the missing of a power backup system, this chip family was dropped in favor of the successor LM8560.&lt;/p&gt;
    &lt;head rend="h2"&gt;– The duplex Display. A not versatile but clever solution&lt;/head&gt;
    &lt;p&gt;This is where it gets a little too technical, and only the electronics hobbyists may be interested in. I try to explain the important points in a simple way, for those who want to know a little more about how this duplex LED display works, that was so popular in alarm clocks.&lt;/p&gt;
    &lt;p&gt;The LM8560’s display works differently than the numerical LED displays found commonly in other appliances, where the digits are “multiplexed”. The display which works with the LM8560 is a custom-made display for this chip. It is a so called “Duplex” display. It is a smart solution for the industrial manufacturing of alarm clocks, but it makes the LM8560 not suitable for the hobbyists who want to build a their own alarm clock using a standard LED display. Let’s see why.&lt;/p&gt;
    &lt;p&gt;Normally, the numeric LED displays with more than one digit, are driven using the Multiplex system. The digits are not lighted all together at the same time. They are lighted in turn one at once, just for a fraction of a second each one. After the first digit has been turned off, the second digit is turned on. It is repeated in cycle through all the digits. The process happens so fast that the eye sees the digits as all lit on, but they aren’t.&lt;lb/&gt;It is done in this way in order to reduce the number of the output connections (wires or leading tracks) necessary to connect the chip to the display.&lt;lb/&gt;To notice if a display is multiplexed, you can simply move it quickly horizontally in front of your eyes, without follow it with the eyes. If it is multiplexed you will see the digits as broken and flickering. If, the digits are really continuously lighted, they leave a continuous trail without breaks.&lt;lb/&gt;In some LED multiplexed displays, the multiplexing frequency is enough low that you can perceive the flickering, especially when watching them with the corner of the eyes.&lt;lb/&gt;In numeric LCD displays the digits are multiplexed too, but since the monochromatic LCD displays have a very high persistence, the effect of multiplexing is not visible even by moving the display quickly.&lt;/p&gt;
    &lt;p&gt;With the multiplexing, regardless of how many digits the display has, the chip needs to have only 7 output pins to drive the 7 segments for a single digit. All the digits are connected to these same 7 outputs. Just a few output pins more are needed to select the digit to be lighted on. The Multiplex circuit, built inside the chip, connect the power supply only to the digit that has to be lighted during its turn, while all the other digits will be disconnected from power.&lt;/p&gt;
    &lt;p&gt;Using only 4 digits, the LM8560 in order to reduce the number of the output connections to the display, uses a low-cost, ‘smart’ 2-phases multiplexing system. The 4 digits are not turned on/off one at a time. The whole display is divided in only two groups of segments, spread among all the 4 digits. &lt;lb/&gt;The cathode of the segments of each group are connected together, meanwhile the anodes of the segments are connected together in couples, two by two segments. The 2 cathodes of each couple are connected to the 2 different groups of the common cathodes. In this way, exploiting the dual power supply coming from the AC transformer, a single output pin of the LM8560 can drive 2 segments, but not at the same time. One segment of the couple will be turned on during the positive half-wave coming from the AC transformer, and the other segment during the negative half-wave. The LM8560 will connect the anode of a segment to the power supply, only when that segment has to be lighted. See it in the following diagram. Don’t get confused by the presence of 2 LEDs in parallel in each segment. Normally in small displays there is only one LED inside of each segment, but in this project there are 2.&lt;/p&gt;
    &lt;p&gt;Thanks to the Mousa Simple Projects blog for this diagram&lt;lb/&gt;https://mousa-simple-projects.blogspot.com/2020/10/24hr-digital-led-clock-without-using.html&lt;/p&gt;
    &lt;p&gt;Please note that in this project the clock is feed with DC power and the 50Hz clock signal is generate by a RC oscillator, therefore it will be not accurate and it will be subjected to variations also due to the tolerances of the RC components. For a better accuracy a quartz oscillator is needed (see the paragraph “Make a quartz clock of it” above).&lt;/p&gt;
    &lt;p&gt;Here there are two schematics of the duplex display which is commonly used with the LM8560:&lt;/p&gt;
    &lt;p&gt;Here you can find a PCB in editable form, where you can modify the layout of the connections to fit the LEDs that you want use: . https://oshwlab.com/Trajectory/tests If you login in that site you can also export the PCB in svg format.&lt;lb/&gt;You can also modify it to accommodate 1 LED per segment with SMD LED format to replace entirely an old display. The SMD LED form factor 0805 (2,0 x 1,0mm) should be most suitable size to replace faulty LEDs on the old displays, or to rebuilt the PCB with the same size of the original display.&lt;/p&gt;
    &lt;p&gt;The 2 groups of segments are switched on/off alternately, accordingly to the half-waves coming from the transformer (50 or 60 Hz) transformer. Therefore no additional components are required to select and drive the on/off switching of the digits.&lt;lb/&gt;Being divided in only 2 groups of segments, alternatively switched on/off, the display is called ‘Duplex’.&lt;/p&gt;
    &lt;p&gt;As you can see in the images bottom, taken with a shutter speed shorter than 1/00 sec., you can clearly see the two groups of segments that compose the digits. Despite the appearances they, are never turned on all together.&lt;/p&gt;
    &lt;p&gt;=&lt;/p&gt;
    &lt;p&gt;+&lt;/p&gt;
    &lt;head rend="h2"&gt;Not the best chip for the electronics hobbyists&lt;/head&gt;
    &lt;p&gt;Requiring a single block specific duplex display, the LM8560 is not the best clocks’ chip for the electronic hobbyists. The Duplex display for the LM8560 is discontinued, it is almost impossible to find as spare part for sale today, and it can’t be replaced with standard 7-segments display modules with common cathode. In the standard modules, the cathodes of all segments of a digit are connected together. You can’t recreate the internal connections among the segments as they are in the duplex display, because the cathode of some segments of a digit has to be connected with the cathode of a segments of another digit. To do this, you need a display modules in which all the segments have their anodes and cathodes free, not connected in common. But such digits modules do not exist, as far I know.&lt;lb/&gt;Therefore you can create a duplex display only by using single LEDs, as Mousa did in his self-made clock with the LM8560. Sure, in this way you can create digits of the size and shape you want.&lt;/p&gt;
    &lt;p&gt;Another problem for the hobbyists is that the package of the LM8560 is a shrink package. In order to reduce the space occupied on the boards of small clocks, the spacing of the pins of the LM8560 is 1,78mm, not 2,54mm as the common standard. It means that the pins of the LM8560 don’t fit in the standard 28pin sockets or in the pre-holed boards. But if you make a custom board for your circuit, this is not a problem. Sockets with spacing 1,78mm are today extremely hard to find, if you want mount it on a socket.&lt;lb/&gt;For the hobbyists, clock ICs that can drive the standard 7-segments modules with common cathodes are more suitable, like the LM8365. It has also 2 alarm times.&lt;/p&gt;
    &lt;head rend="h2"&gt;My alarm clocks&lt;/head&gt;
    &lt;p&gt;– An advice first: Why red LED displays are my favorite:&lt;/p&gt;
    &lt;p&gt;For stupid trend reasons, red LED displays are disappearing, being replaced by other colors, such as green and even worse, blue. Since 2007 Sony produces no more alarm clocks with red LED display. Red LEDs were the first to have been invented and for a long time, red was also the only LED color available. Only because of that reason, red LEDs are perceived today as something “retro”, but in alarm clocks the red color has also its own functionality.&lt;lb/&gt;The highest sensitivity of the human eye is to yellow light, then to green, then to red, and as last, to blue light. When you look at the alarm clock during the day, the color of the display may not make a big difference, except for blue, which can be really hard to read. But in the night, if you sleep in an almost completely dark room, the color of the display makes a huge difference.&lt;lb/&gt;A red display in the darkness, even a quite bright one, will not make visible the whole room to your eyes. A green display (or worse a yellow one), even if with a weak brightness, will also showy you the whole room. This is why I want only red LED displays for my alarm clock.&lt;lb/&gt;LCD radio clocks have usually a yellowish or green backlight display. Right due to the reason explained above, in some LCD clocks the backlight is normally off, and in the night you have to turn it on manually when you wish to read the time! For me, this makes no sense in an alarm clock to be used in a bedroom.&lt;lb/&gt;The red backlight used in LCD displays (rare to find) is of a lighter tone then the LEDs display and it illuminates also the rest of the room, even if less strongly than how the green LEDs do.&lt;/p&gt;
    &lt;head rend="h2"&gt;My first clock radio, by Majestic 1986&lt;/head&gt;
    &lt;p&gt;As mentioned before, thanks to the LM8560 the price of clock radios dropped. In the mid 1980s some supermarkets even offered a clock radio as prize for the collection of their purchase-points. Back then, prizes for purchase points were normally something boring, like a set of glasses, cups, or dishes with table baskets. They were no electronic gadgets. A digital clock radio appeared like a super cool and technological prize! Never seen before. It seemed impossible that they could give a so much electronic device “for free”.&lt;lb/&gt;One of these supermarkets was in my little town, where I lived in Italy in 1986. I was 15 years old and I always wanted an alarm clock radio all for me, with luminous digits.&lt;lb/&gt;Between September and November 1986, I don’t remember exactly, it was the time to pick up the prize. I remember the bike ride to the supermarket to pick up the clock radio. It was a model branded by Majestic, an unknown retailer of rebranded cheap electronic ware made in Asia (Hong Kong in this case). Here below a photo of the same identical model that I found here in Germany in year 2017, in incredibly new conditions. The only difference is the inscriptions in German language, instead than in English, and the brand name Maximal instead of Majestic.&lt;/p&gt;
    &lt;p&gt;Maximal 8688 (same as Majestic)&lt;/p&gt;
    &lt;p&gt;Arrived at home, as soon as I checked that it worked, I opened it to see what it looked like inside. I remember the disappointment seeing that the clock was made with only a single chip, with few other components around it (apart the radio section), and without a quartz, which I liked much to have. Quartz back then were a symbol of precision and anything with a quartz built in, looked to be prestigious! Today with 15 € you can buy a radio-controlled clock, constantly synchronized via radio with an atomic clock, which is accurate to one millionth of a second.&lt;/p&gt;
    &lt;p&gt;One day, talking with the owner of a little electronics shop about my disappointment that my new alarm clock had no quartz inside, he said to me, “But what? As prize for the supermarket points did you wanted a quartz radio clock?! It counts the 50Hz and let’s go! It’s already much if they gave it!”&lt;/p&gt;
    &lt;p&gt;This glorious clock radio lasted in service until 1998. In the meantime, I played with the keys so much that after 11 years, the paint and marks were completely worn and the black plastic underneath had been completely exposed. &lt;lb/&gt;After 11 years of honored service I had gotten a little bored to have always the same clock radio, and I decided it was time to replace it. &lt;lb/&gt;My original Majestic unfortunately no longer exists. In its last months, it happened two times that the alarm didn’t ringed, despite being properly set. The only cause I could guess at than time, was a faulty contact in the selector switch. But although I had cleaned it with proper cleaning spray, for a third time it did not ring. Then I had fun demolishing that radio clock. What a mistake! Today I would keep it as a vintage heirloom! I kept as memory of it, only the PC board broken in 2 parts, without display and without the IC LM8560. I cut away the IC, thinking that it was faulty. If I had the datasheet of the LM8560 at that time, I could have a better clue about what what could be the failure and repair it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Sony ICF-C102&lt;/head&gt;
    &lt;p&gt;Compared to the Sony’s clock radios (my sister got already one) the audio quality of the Majestic was poorer. I still didn’t knew, that the blame was of low quality speaker, rather than of the radio. So I decided to afford a Sony too, and doing that, my alarm clock odyssey began, and lasted until August 2007, right before my departure from Italy.&lt;/p&gt;
    &lt;p&gt;The first Sony clock radio I purchased was a cube-shaped model with green LED display. The Digicube ICF-C102. I wanted experiment the green LED trend too:&lt;/p&gt;
    &lt;p&gt;I brought it home, and after verifying that it worked, I opened it to see what there was inside. Given the price of more than twice as much as a ‘budged’ clock radio, I was sure that all the Sony’s clock radios were based on a quartz clock… Instead not… I opened it and the clock was made with the chip LM8365. It works with the same principle of the LM8560, counting the AC frequency, but it drives a standard 7-segments display with multiplexing. I think to remember, that the flickering of the display was in some conditions, somewhat more perceivable than on the clocks with the LM8560. Also once again, this was not the time to get a quartz clock radio.&lt;/p&gt;
    &lt;p&gt;Beside this surprise, I noticed that its green display, behind the dark glass, was much less bright than the red display of the destroyed Majestic, and during the daytime it was less readable. Instead, at night I had the surprise that the green light illuminates well my whole room! And even worst, I noticed that the radio is never completely switched off!!! I place my alarm clock very close to the bed, practically almost sticking to my head. When the radio was switched off, it was actually switched on, just kept with the volume set to minimum. If you put the speaker in proximity of your ear, you can distinctly hear the music from the radio station in the background, in addition to the background hiss that is always present when the volume is set to zero! During the daytime you can’t hear it, but in the night, in a silent room, you hear it clearly, if the clock is not enough distant from your head.&lt;/p&gt;
    &lt;p&gt;To me, as an electronic technician, this is an absurd design choice beyond any reason. I guess that the reason for this absurd choice is to don’t let you hear that classic small “stock!” noise, when you switch on/off the radio, that is usually audible in clock radios where the radio is completely turned of when it is not in use.&lt;lb/&gt;Compliments to the Sony’s engineers! To don’t let your hear that short “Stock” sound, which anyway can be minimized by designing better the audio amplifier, they let you hear the sound of the radio and the background noise of the amplifier all night long! Truly clever engineering!&lt;lb/&gt;And there was also another bad surprise. There is no alarm indicator on the display (usually a dot), that shows you whether the alarm function is inserted or not, as it is usually present even on budget alarm clocks!&lt;/p&gt;
    &lt;p&gt;The audio quality of the radio was high as expected. It mounted the famous radio chip Sony CXA1019S, mounted also in some small good portable radios by Sony, like the ICF-380 (the ICF-12 had a similar IC). The buttons were real micro switch buttons, not like those hard to press, that you find on the cheap alarm clocks. But the issue with the radio never turned off was so deleterious, that after 1 or 2 months trying to live with it, I placed that clock radio in another room, and I bought another one.&lt;/p&gt;
    &lt;p&gt;Given the good sound quality of the radio, I wanted try with another Sony, but this time I switched back to the red display, forever. I choose this time a more classic model, not a cube. I thought that Sony engineers were not so stupid to handle the turning off of the radio, in the same way on all Sony’s models…&lt;/p&gt;
    &lt;head rend="h2"&gt;My second and last Sony was the ICF-C290&lt;/head&gt;
    &lt;p&gt;It was still in the year 1998&lt;/p&gt;
    &lt;p&gt;The audio quality of the radio was good as expected. The radio chip was the same Sony CXA1019S, as in the Digicube ICF-C102. But also in this clock radio, the radio is never completely turned off! It is just kept at the minimum volume. And also here there is no LED dot that indicates that the alarm function is inserted! Looking closely at the display, the spot of the dot was present, so I drove the 10Km back to the store, thinking that my clock radio was faulty. The clerk tested it on the same model exhibited on the shelf, and also that one show no alarm dot. The clerk was quite surprised too.&lt;lb/&gt;In the darkness before to sleep, how I can know whether the alarm is active or not? How much would it cost more to Sony, to use that single led, that all the budget 10€ alarm clocks use? The Sony’s engineer have really absurd concepts ideas for their products. I have really no understanding for such choices.&lt;/p&gt;
    &lt;p&gt;Then I drove back to home, I opened the clock radio, and what was inside? The LM8560, as in my 11 years older supermarket clock radio prize! I was astonished. The giant Sony Company bought that low cost IC from Sanyo, instead to develop an its own custom clock IC, as they did with the IC of the radio?&lt;lb/&gt;This time I definitely gave up the idea about getting a ‘quartz’ alarm clock and I began to appreciate this immortal integrated circuit, companion of thousand nights, more or less sleepless! A low cost but well made, long living chip.&lt;/p&gt;
    &lt;p&gt;Since the audio quality was really good and I didn’t want an LCD clock radio, I modified the Sony ICF-C290 by myself, as the best I could do at that time. I made a hole in the front panel and I added a LED that indicates the activation of the alarm. I added a small reed relay that disconnected the speaker when the radio was off.&lt;lb/&gt;In 1998 it was not easy to get datasheets of electronic components, as it is searching in internet today. In 1998 I had even not yet internet connection and anyway there were very few contents. Make all those modifications without having any schematic diagram of the device and without the pinout of the LM8560 was not easy and not a fun job at all. But finally, my high branded Sony clock radio had the most basic functions that the budget clocks already have as standard. Congratulations Sony, see your clock radios never again!&lt;/p&gt;
    &lt;p&gt;But this is not the end of the story… After a few years the radio began to have tuning problems. I find that the variable capacitor of the radio was made of poor material and oxide has formed between the blades, and on the rotating contact. Right in a Sony, which should use only high quality components…&lt;lb/&gt;The Sony ICF-C290 lasted until August 2007, when the tuning problem had become so big that I finally destroyed the clock radio tearing it apart. What a satisfaction! I had enough of that crap made by Sony. I kept only The LM8560 and the radio chip as souvenir. The only 2 parts that deserved to be kept.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thomson CR61, my final clock radio&lt;/head&gt;
    &lt;p&gt;In August 2007 I was now in the mid of preparations for my moving to Germany and I had to look once again for a clock radio… I went around a lot of stores and with red display there were only very budget models, very cheap, very low quality and of unknown brands. Then in a big store I saw the Thomson CR61. I didn’t like much that model aesthetically, because I don’t like rounded shapes, but it seemed to be solid and well made. I could feel that the buttons were made, as usual on the budged clocks, with big slats bounded with adhesive plastic film. But they were very good to use. Too bad it didn’t have two alarm times, but by that time I knew well that I need a red display, so I picked it bought it. Price was 30€. Not a budged model, and not a Sony.&lt;/p&gt;
    &lt;p&gt;It was also the only model remained with a design still focused on functionality. For example, the buttons are good spaced between each other and are not grouped in a single circular mess of buttons, as it was trendy in those years. Much to my surprise, the audio quality of the radio is really good as the Sony! It lacks a bit in the bass tones, but the sound is really very clear and detailed.&lt;lb/&gt;Another surprise, the display was very bright. The brightest I found so far in an alarm clock, but in spite of that, it didn’t create any problem in the dark, right because it is red and not green. Now after 15 years, the brightness has dropped much, but it is still good very good in the night.&lt;lb/&gt;The only design flaw, indeed common to many clock radios of not large size, is that the speaker is so close to the AC transformer, that due to the induction, it plays a little noise of the 50 Hz generated by the transformer. If they placed the speaker on the right of the case and the clock buttons on the left, the unwanted effect would not happen. In smaller clock radios, the problem is unavoidable due to lack of space inside. The noise, though very weak in the Thomson CR61, was in my conditions of use still audible, so I mounted the transformer in an external box outside the clock and the problem is solved. Needless to say, the radio here is turned off completely when not in use, as it is logical to be done, unlike on the Sony’s, sold at double the price and more.&lt;/p&gt;
    &lt;p&gt;A remarkable safety feature of this model, is that the transformer has a 120°C thermal fuse built in. In case of overheating due to a short circuit, internally or externally to the transformer, it interrupts the AC supply. A very good feature for a device that stays plugged in all the time.&lt;/p&gt;
    &lt;p&gt;I modded the clock radio so that I can keep the seconds displayed permanently. I added only a switch which keeps the “Sleep” button pressed. If you keep the “Alarm” button pressed instead, the alarm will not ring,&lt;lb/&gt;if the button Is pressed during the alarm time. This applies to the clocks model which have not a separated “Alarm reset” button.&lt;lb/&gt;I have added also 2 LEDs that light the indicator of the radio scale, when the radio is switched on.&lt;/p&gt;
    &lt;p&gt;Finally, I promised myself that this will be my last clock radio and I will never threw it away! It is so well made and functional to me, that I bought other 3 pieces of the same model as replacement, to be sure it will last forever.&lt;/p&gt;
    &lt;head rend="h2"&gt;The modern clock radios&lt;/head&gt;
    &lt;p&gt;The modern clock radios in year 2023, also the most cheap models with LED display, are made with a PLL tuner made with SDR technology. There is no more mixer, intermediate frequency, etc… The radio frequency is directly sampled and completely digitally processed and demodulated. The processor unit has 10 or 20 memories for the stations. Cool, but it is made just to save the costs of the variable capacitor and other components. Like all the cheap SDR tuner, the have very low selectivity, low sensitivity and low audio quality.&lt;lb/&gt;Since they use a PLL tuner and a digital processor unit, a quartz oscillator is present in the circuit, to let the microprocessor and PLL work. This means also that the clock is quartz based, differently than the clocks made with LM8560.&lt;lb/&gt;But above all, they are all Chinese garbage electronic products. They will not last 30-40 years or more like many clock radios made in the ’80s, ’90s and early 2000s do.&lt;lb/&gt;Therefore, if you have a clock radio with the legendary LM8560 inside, keep it!&lt;/p&gt;
    &lt;p&gt;I bought one of those modern cheap clock radios, just to see what there is inside.&lt;/p&gt;
    &lt;p&gt;iCES ICR-210 (22€ in year 2023)&lt;/p&gt;
    &lt;p&gt;It looks fine, but the audio quality is poor. It has a very cheap digital SDR tuner. The quality of the tuner is quite poor too. The 10 memories for the radio stations are not comfortable to manage. The clock can’t display seconds, but it has 2 alarm times.&lt;lb/&gt;And… there is nothing inside!&lt;/p&gt;
    &lt;p&gt;It is not a clock with radio. It is a display with integrated clock and radio. 3 ICs mounted on a mini board, with a bunch of SMD components. That’s all! If the display or the clock IC fails, you have to throw away al.&lt;lb/&gt;The IC on the left is the IC of the radio (used only for FM on this model, to save the cost of the AM antenna). The square chip in the middle is the IC of the clock. There is nothing printed on it. Probably it is a programmed microcontroller, with integrated memory. The IC on the right is the audio amplifier.&lt;lb/&gt;If they were all good components, a such device should last forever (excluding the life of the flash memory, if the memory if of flash type). But welcome in the capitalism and in the reign of the infinite growth! &lt;lb/&gt;If you read the customer reviews of it, many users complain failures after 2 or 3 months, even on multiple pieces of the same model! About the display, radio, or audio, always the same failures. It indicates a real low quality of the components and a clear design based on programmed obsolescence.&lt;/p&gt;
    &lt;p&gt;The only good note. When disconnected from the AC power it sucks almost no current from the 3V (2xAAA size) backups battery. Only 4µA! It means that you can leave it unplugged for the whole time and plug it only when you need it. Being a quartz clock, it will not run faster or slower than when it is powered by the AC.&lt;/p&gt;
    &lt;p&gt;On the back side there is finally the quartz, which generates the clock frequency for the micro controller. But now it is too late for my wish from the ’80s about a quartz clock radio. Now I prefer the eternal LM8560…&lt;/p&gt;
    &lt;head rend="h2"&gt;Some LM8560 manufacturers:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Designation&lt;/cell&gt;
        &lt;cell&gt;Manufacturer&lt;/cell&gt;
        &lt;cell&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SC8560&lt;/cell&gt;
        &lt;cell&gt;Silanic Integrated Cicuits (Silan Microelectronics, Hangzhou – China)&lt;/cell&gt;
        &lt;cell&gt;Still produced in 2023?&lt;p&gt;Mounted in the Thomson CR61 and in some Watson clock radios. Good quality.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SP8560&lt;/cell&gt;
        &lt;cell&gt;Si-Power&lt;p&gt;(Silicon Power Micro-Electronics), China&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Mounted in some Chinese garbage clock radios. Quality unknown.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;IL8560&lt;/cell&gt;
        &lt;cell&gt;BMS – Belmicrosystems&lt;p&gt;(INTEGRAL JSC), Belarus&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Quality unknown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NTE2062&lt;/cell&gt;
        &lt;cell&gt;NTE Electronics, USA&lt;/cell&gt;
        &lt;cell&gt;It looks to be still in production in 2023. Quality unknown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LM8560&lt;/cell&gt;
        &lt;cell&gt;UTC (Unisonic Technologies), China.&lt;/cell&gt;
        &lt;cell&gt;Still produced in 2023? Quality unknown, but supposedly&lt;p&gt;good.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LM8560&lt;/cell&gt;
        &lt;cell&gt;?&lt;/cell&gt;
        &lt;cell&gt;Fake LM8560 made by unknown Chinese manufacturers.&lt;p&gt;Quality unknown. Still in production.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;W8560&lt;/cell&gt;
        &lt;cell&gt;?&lt;/cell&gt;
        &lt;cell&gt;Unknown manufacturer from the 1990 years. Likely discontinued.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TMS3450NL&lt;/cell&gt;
        &lt;cell&gt;Texas Instruments&lt;/cell&gt;
        &lt;cell&gt;Discontinued&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;LM8560&lt;/cell&gt;
        &lt;cell&gt;Sanyo, Japan&lt;/cell&gt;
        &lt;cell&gt;Original by Sanyo. Discontinued.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Similar ICs and alternative ICs for hobbyists&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Designation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8365&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 2 alarm times. It has a dedicated&lt;p&gt;pin to display seconds.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8360&lt;p&gt;LM8361&lt;/p&gt;&lt;p&gt;NTE2060&lt;/p&gt;&lt;p&gt;NTE2061&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 1alarm time. It has a dedicated&lt;p&gt;pin to display seconds. “Fast” and “Slow” speed set buttons.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8362&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 1alarm time. It has a dedicated&lt;p&gt;pin to display seconds. “Fast” and “Slow” speed set buttons.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LM8363&lt;/cell&gt;
        &lt;cell&gt;It can drive standard 7-segments common cathode LED displays. 2 alarm times. It has a dedicated&lt;p&gt;pin to display seconds. Calendar with month/day.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LC85632&lt;/cell&gt;
        &lt;cell&gt;2 alarm times. It can’t display seconds. Calendar with month/day. Dimmer function for the display.&lt;p&gt;Made for duplex display.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;LM8562&lt;/cell&gt;
        &lt;cell&gt;2 alarm times. It can display seconds. “Fast” and “Slow” speed set buttons. Made for duplex display.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45787842</guid><pubDate>Sun, 02 Nov 2025 04:27:31 +0000</pubDate></item><item><title>Backpropagation is a leaky abstraction (2016)</title><link>https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b</link><description>&lt;doc fingerprint="f70910553d224cb1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Yes you should understand backprop&lt;/head&gt;
    &lt;p&gt;When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Why do we have to write the backward pass when frameworks in the real world, such as TensorFlow, compute them for you automatically?”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is seemingly a perfectly sensible appeal - if you’re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of “it’s worth knowing what’s under the hood as an intellectual curiosity”, or perhaps “you might want to improve on the core algorithm later”, but there is a much stronger and practical argument, which I wanted to devote a whole post to:&lt;/p&gt;
    &lt;p&gt;&amp;gt; The problem with Backpropagation is that it is a leaky abstraction.&lt;/p&gt;
    &lt;p&gt;In other words, it is easy to fall into the trap of abstracting away the learning process — believing that you can simply stack arbitrary layers together and backprop will “magically make them work” on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vanishing gradients on sigmoids&lt;/head&gt;
    &lt;p&gt;We’re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can “saturate” and entirely stop learning — your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):&lt;/p&gt;
    &lt;code&gt;z = 1/(1 + np.exp(-np.dot(W, x))) # forward pass&lt;lb/&gt;dx = np.dot(W.T, z*(1-z)) # backward pass: local gradient for x&lt;lb/&gt;dW = np.outer(z*(1-z), x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (“vanish”), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.&lt;/p&gt;
    &lt;p&gt;Another non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you’re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.&lt;/p&gt;
    &lt;p&gt;TLDR: if you’re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn’t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dying ReLUs&lt;/head&gt;
    &lt;p&gt;Another fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:&lt;/p&gt;
    &lt;code&gt;z = np.maximum(0, np.dot(W, x)) # forward pass&lt;lb/&gt;dW = np.outer(z &amp;gt; 0, x) # backward pass: local gradient for W&lt;/code&gt;
    &lt;p&gt;If you stare at this for a while you’ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn’t “fire”), then its weights will get zero gradient. This can lead to what is called the “dead ReLU” problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron’s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It’s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and your network has ReLUs, you’re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exploding gradients in RNNs&lt;/head&gt;
    &lt;p&gt;Vanilla RNNs feature another good example of unintuitive effects of backpropagation. I’ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):&lt;/p&gt;
    &lt;p&gt;This RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you’ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.&lt;/p&gt;
    &lt;p&gt;What happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b…)? This sequence either goes to zero if |b| &amp;lt; 1, or explodes to infinity when |b|&amp;gt;1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.&lt;/p&gt;
    &lt;p&gt;TLDR: If you understand backpropagation and you’re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spotted in the Wild: DQN Clipping&lt;/head&gt;
    &lt;p&gt;Lets look at one more — the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector — turns out this trivial operation is not supported in TF). Anyway, I searched “dqn tensorflow”, clicked the first link, and found the core code. Here is an excerpt:&lt;/p&gt;
    &lt;p&gt;If you’re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \gamma \argmax_a Q(s’,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.&lt;/p&gt;
    &lt;p&gt;The problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.&lt;/p&gt;
    &lt;p&gt;The clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:&lt;/p&gt;
    &lt;code&gt;def clipped_error(x): &lt;lb/&gt;  return tf.select(tf.abs(x) &amp;lt; 1.0, &lt;lb/&gt;                   0.5 * tf.square(x), &lt;lb/&gt;                   tf.abs(x) - 0.5) # condition, true, false&lt;/code&gt;
    &lt;p&gt;It’s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can’t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.&lt;/p&gt;
    &lt;p&gt;I submitted an issue on the DQN repo and this was promptly fixed.&lt;/p&gt;
    &lt;head rend="h3"&gt;In conclusion&lt;/head&gt;
    &lt;p&gt;Backpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because “TensorFlow automagically makes my networks learn”, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.&lt;/p&gt;
    &lt;p&gt;The good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.&lt;/p&gt;
    &lt;p&gt;That’s it for now! I hope you’ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I’m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45787993</guid><pubDate>Sun, 02 Nov 2025 05:20:12 +0000</pubDate></item><item><title>Notes by djb on using Fil-C</title><link>https://cr.yp.to/2025/fil-c.html</link><description>&lt;doc fingerprint="afe1387bca053cc2"&gt;
  &lt;main&gt;&lt;p&gt;I'm impressed with the level of compatibility of the new memory-safe C/C++ compiler Fil-C (filcc, fil++). Many libraries and applications that I've tried work under Fil-C without changes, and the exceptions haven't been hard to get working.&lt;/p&gt;&lt;p&gt;I've started accumulating miscellaneous notes on this page regarding usage of Fil-C. My selfish objective here is to protect various machines that I manage by switching them over to code compiled with Fil-C, but maybe you'll find something useful here too.&lt;/p&gt;&lt;p&gt;Timings below are from a mini-PC named phoenix except where otherwise mentioned. This mini-PC has a 6-core (12-thread) AMD Ryzen 5 7640HS (Zen 4) CPU, 12GB RAM, and 36GB swap. The OS is Debian 13. (I normally run LTS software, periodically upgrading from software that's 4â5 years old such as Debian 11 today to software that's 2â3 years old such as Debian 12 today; but some of the packages included in Fil-C expect newer utilities to be available.)&lt;/p&gt;&lt;p&gt;Related:&lt;/p&gt;&lt;p&gt;Another way to run Fil-C is via Filnix from Mikael Brockman. For example, an unprivileged user under Debian 12 with about 10GB of free disk space can download, compile, and install Fil-C, and run a Fil-C-compiled Nethack, as follows:&lt;/p&gt;&lt;code&gt;unshare --user --pid echo YES # just to test
git clone https://github.com/nix-community/nix-user-chroot
cd nix-user-chroot
cargo build --release
mkdir -m 0755 ~/nix
~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  bash -c 'curl -L https://nixos.org/nix/install | sh'
env TERM=vt102 \
  ~/nix-user-chroot/target/release/nix-user-chroot ~/nix \
  ~/nix/store/*-nix-2*/bin/nix \
  --extra-experimental-features 'nix-command flakes' \
  run 'github:mbrock/filnix#nethack'
&lt;/code&gt;&lt;p&gt;Current recommendations for things to do at the beginning as root:&lt;/p&gt;&lt;code&gt;mkdir -p /var/empty
apt install \
  autoconf-dickey build-essential bison clang cmake flex gawk \
  gettext ninja-build patchelf quilt ruby texinfo time
&lt;/code&gt;&lt;p&gt;I created an unprivileged filc user. Everything else is as that user.&lt;/p&gt;&lt;p&gt;I downloaded the Fil-C source package:&lt;/p&gt;&lt;code&gt;git clone https://github.com/pizlonator/fil-c.git
cd fil-c
&lt;/code&gt;&lt;p&gt;This isn't just the compiler; there's also glibc and quite a few higher-level libraries and applications. There are also binary Fil-C packages, but I've worked primarily with the source package at this point.&lt;/p&gt;&lt;p&gt;I compiled Fil-C and glibc:&lt;/p&gt;&lt;code&gt;time ./build_all_fast_glibc.sh
&lt;/code&gt;&lt;p&gt;There are also options to use musl instead of glibc, but musl is incompatible with some of the packages shipped with Fil-C: attr needs basename, elfutils needs argp_parse, sed's test suite needs the glibc variant of calloc, and vim's build needs iconv to be able to convert from CP932 to UTF-8.&lt;/p&gt;&lt;p&gt;I had originally configured the server phoenix with only 12GB swap. I then had to restart ./build_all_fast_glibc.sh a few times because the Fil-C compilation ran out of memory. Switching to 36GB swap made everything work with no restarts; monitoring showed that almost 19GB swap (plus 12GB RAM) was used at one point. A larger server, 128 cores with 512GB RAM, took 8 minutes for Fil-C plus 6 minutes for musl, with no restarts needed.&lt;/p&gt;&lt;p&gt;Fil-C includes a ./build_all_slow.sh that builds many more libraries and applications (sometimes with patches from the Fil-C author). I wrote a replacement script https://cr.yp.to/2025/build-parallel-20251023.py with the following differences:&lt;/p&gt;&lt;p&gt;On phoenix, running time PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" ./build-parallel.py went through 61 targets in 101 minutes real time (467 minutes user time, 55 minutes system time), successfully compiling 60 of them.&lt;/p&gt;&lt;p&gt;libcap. This is the one that didn't compile: /home/filc/fil-c/pizfix/bin/ld: /usr/libexec/gcc/x86_64-linux-gnu/14/liblto_plugin.so: error loading plugin: libc.so.6: cannot open shared object file: No such file or directory&lt;/p&gt;&lt;p&gt;util-linux. I skipped this one. It does compile, but the compiled taskset utility needs to be patched to use sched_getaffinity and sched_setaffinity as library functions rather than via syscall, or Fil-C needs to be patched for those syscalls. This is an issue for build-parallel since build-parallel relies on taskset; maybe build-parallel should instead use Python's affinity functions.&lt;/p&gt;&lt;p&gt;attr, bash, benchmarks, binutils, bison, brotli, bzip2, bzip3, check, cmake, coreutils, cpython, curl, dash, diffutils, elfutils, emacs, expat, ffi, gettext, git, gmp, grep, icu, jpeg-6b, libarchive, libcap, libedit, libevent, libpipeline, libuev, libuv, lua, lz4, m4, make, mg, ncurses, nghttp2, openssh, openssl, pcre2, pcre, perl, pkgconf, procps, quickjs, sed, shadow, simdutf, sqlite, tcl, tmux, toybox, vim, wg14_signals, xml_parser, xz, zlib, zsh, zstd. No problems encountered so far (given whatever patches were already applied from the Fil-C author!). The benchmarks package is supplied with Fil-C and does a few miscellaneous measurements.&lt;/p&gt;&lt;p&gt;I did export PATH="$HOME/bin:$HOME/fil-c/build/bin:$HOME/fil-c/pizfix/bin:$PATH" before these.&lt;/p&gt;&lt;p&gt;boost 1.89.0: Seems to mostly work. Most of the package is header-only; a few simple tests worked fine.&lt;/p&gt;&lt;p&gt;I also looked a bit at the compiled parts. Running ./bootstrap.sh --with-toolset=clang --prefix=$HOME ran into vfork, which Fil-C doesn't support, but editing tools/build/src/engine/execunix.cpp to use defined(__APPLE__) || defined(__FILC__) for the no-fork test got past this.&lt;/p&gt;&lt;p&gt;Running ./b2 install --prefix=$HOME toolset=clang address-model=64 architecture=x86_64 binary-format=elf produced an error message since I should have said x86 instead of x86_64; Fil-C said it caught a safety issue in the b2 program after the error message: filc safety error: argument size mismatch (actual = 8, expected = 16). I didn't compile with debugging so Fil-C didn't say where this is in b2.&lt;/p&gt;&lt;p&gt;cdb-20251021: Seems to work. One regression test, an artificial out-of-memory regression test, currently produces a different error message with Fil-C: filc panic: src/libpas/pas_compact_heap_reservation.c:65: pas_aligned_allocation_result pas_compact_heap_reservation_try_allocate(size_t, size_t): assertion page_result.result failed.&lt;/p&gt;&lt;p&gt;libcpucycles-20250925: Seems to work. I commented out the first three lines of cpucycles/options.&lt;/p&gt;&lt;p&gt;libgc: I replaced this with a small gcshim package (https://cr.yp.to/2025/gcshim-20251022.tar.gz) that simply calls malloc etc. So far this seems to be an adequate replacement. (Fil-C includes a garbage collector.)&lt;/p&gt;&lt;p&gt;libntruprime-20241021: Seems to work after a few tweaks but I didn't collect full notes yet. chmod +t crypto_hashblocks/sha512/avx2 disables assembly and makes things compile; configured with --no-valgrind since Fil-C doesn't support valgrind; did a bit more tweaking to make cpuid work.&lt;/p&gt;&lt;p&gt;lpeg-1.1.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://www.inf.puc-rio.br/~roberto/lpeg/lpeg-1.1.0.tar.gz
tar -xf lpeg-1.1.0.tar.gz
cd lpeg-1.1.0
make CC=`which filcc` DLLFLAGS='-shared -fPIC' test
cp lpeg.so $PREFIX/lib
&lt;/code&gt;&lt;p&gt;luv-1.51.0: Compiles, maybe works (depends on lua, dependency of neovim):&lt;/p&gt;&lt;code&gt;cd
PREFIX=$(dirname $(dirname $(which lua)))
wget https://github.com/luvit/luv/releases/download/1.51.0-1/luv-1.51.0-1.tar.gz
tar -xf luv-1.51.0-1.tar.gz
cd luv-1.51.0-1
mkdir build
cd build
LUA_DIR=$HOME/fil-c/projects/lua-5.4.7
# lua install should probably do this:
cp $LUA_DIR/lua.h $PREFIX/include/
cp $LUA_DIR/lauxlib.h $PREFIX/include/
cp $LUA_DIR/luaconf.h $PREFIX/include/
cp $LUA_DIR/lualib.h $PREFIX/include/
# and then:
cmake -DCMAKE_C_COMPILER=`which filcc` -DCMAKE_INSTALL_PREFIX=$PREFIX -DWITH_LUA_ENGINE=Lua -DLUA_DIR=$HOME/fil-c/projects/lua-5.4.7/ ..
make test
make install
&lt;/code&gt;&lt;p&gt;mutt-2-2-15-rel (depends on ncurses):&lt;/p&gt;&lt;code&gt;wget https://github.com/muttmua/mutt/archive/refs/tags/mutt-2-2-15-rel.tar.gz
tar -xf mutt-2-2-15-rel.tar.gz
cd mutt-mutt-2-2-15-rel
CC=`which clang` ./prepare --prefix=$HOME/fil-c/pizfix --with-homespool
make -j12 install
&lt;/code&gt;
Seems to work, at least for reading email.


&lt;p&gt;tig (depends on ncurses and maybe more):&lt;/p&gt;&lt;code&gt;wget https://github.com/jonas/tig/releases/download/tig-2.6.0/tig-2.6.0.tar.gz
tar -xf tig-2.6.0.tar.gz
cd tig-2.6.0
CC=`which filcc` ./configure --prefix=$(dirname $(dirname $(which git)))
make -j12
make test
make -j12 install
&lt;/code&gt;
Seems to work, at least for viewing the Fil-C repo.


&lt;p&gt;w3m (depends on gcshim and ncurses): Seems to work. I tried the Debian version: git clone https://salsa.debian.org/debian/w3m.git. I used CFLAGS=-Wno-incompatible-function-pointer-types (which is probably needed for clang anyway even without Fil-C).&lt;/p&gt;&lt;p&gt;I've built and installed some replacement Debian packages using Fil-C as the compiler on a Debian 13 machine, as explained below. Hopefully this can rapidly scale to many packages, taking advantage of the basic compile-install-test knowledge already built into Debian source packages, although some packages will take more work because they need extra patches to work with Fil-C.&lt;/p&gt;&lt;p&gt;Structure. Debian already understands how to have packages for multiple architectures (ABIs; Debian "ports") installed at once. For example, dpkg --add-architecture i386; apt update; apt install bash:i386 installs a 32-bit version of bash, replacing the usual 64-bit version; you can do apt install bash:amd64 to revert to the 64-bit version. Meanwhile the 32-bit libraries and 64-bit libraries are installed in separate locations, basically /lib/i386-linux-gnu or /usr/lib/i386-linux-gnu vs. /lib/x86_64-linux-gnu or /usr/lib/x86_64-linux-gnu. (On Debian 11 and newer, and on Ubuntu 22.04 and newer, /lib is symlinked to /usr/lib.)&lt;/p&gt;&lt;p&gt;I'm following this model for plugging Fil-C into Debian: the goal is for apt install bash:amd64fil0 to install a Fil-C-compiled (amd64fil0) version of bash, replacing the usual (amd64) version of bash, while the amd64 and amd64fil0 libraries are installed in separate locations.&lt;/p&gt;&lt;p&gt;The include-file complication. Debian expects library packages compiled for multiple ABIs to all provide the same include files: for example, /usr/include/ncurses.h is provided by libncurses-dev:i386, libncurses-dev:amd64, etc. This is safe because Debian forces libncurses-dev:i386 and libncurses-dev:amd64 and so on to all have the same version. An occasional package with ABI-dependent include files can still use /usr/include/x86_64-linux-gnu etc.&lt;/p&gt;&lt;p&gt;Fil-C instead omits /usr/include in favor of a Fil-C-specific directory (which will typically be different from /usr/include: even if Fil-C is compiled with glibc, probably the glibc version won't be the same as in /usr/include). This difference is the top source of messiness below. I'm planning to tweak the Fil-C driver to use /usr/include on Debian. [This is done in the filian-install-compiler script.]&lt;/p&gt;&lt;p&gt;Something else I'm planning to tweak is Fil-C's glibc compilation, so that it uses the final system prefix. [This is also done in the filian-install-compiler script.] The approach described below instead requires /home/filian/fil-c to stay in place for compiling and running programs.&lt;/p&gt;&lt;p&gt;Building Debian packages. How does Debian package building work? First, more packages to install as root:&lt;/p&gt;&lt;code&gt;apt install dpkg-dev devscripts docbook2x \
  dh-exec dh-python python3-setuptools fakeroot \
  sbuild mmdebstrap uidmap piuparts
&lt;/code&gt;
&lt;p&gt;Debian has multiple options for building a package. The option that has the best isolation, and that Debian uses to continually build new packages for distribution, is sbuild, but for fast development I'll focus on directly using the lower-level dpkg-buildpackage.&lt;/p&gt;&lt;p&gt;Baseline 1: using sbuild without Fil-C. In case you do want to try sbuild, here's the basic setup, and then an example of building a small package (tinycdb):&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/sbuild
time mmdebstrap --include=ca-certificates --skip=output/dev --variant=buildd unstable ~/shared/sbuild/unstable-amd64.tar.zst https://deb.debian.org/debian

mkdir -p ~/.config/sbuild
cat &amp;lt;&amp;lt; "EOF" &amp;gt; ~/.config/sbuild/config.pl
$chroot_mode = 'unshare';
$external_commands = { "build-failed-commands" =&amp;gt; [ [ '%SBUILD_SHELL' ] ] };
$build_arch_all = 1;
$build_source = 1;
$source_only_changes = 1;
$run_lintian = 1;
$lintian_opts = ['--display-info', '--verbose', '--fail-on', 'error,warning', '--info'];
$run_autopkgtest = 1;
$run_piuparts = 1;
$piuparts_opts = ['--no-eatmydata', '--distribution=%r', '--fake-essential-packages=systemd-sysv'];
EOF

mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time sbuild
&lt;/code&gt;

&lt;p&gt;Baseline 2: using dpkg-buildpackage without Fil-C. Here's what it looks like compiling the same small package with dpkg-buildpackage:&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
time dpkg-buildpackage -us -uc -b
&lt;/code&gt;

&lt;p&gt;The goal: Using dpkg-buildpackage with Fil-C. As root, teach dpkg basic features of the new architecture, imitating the current line amd64 x86_64 (amd64|x86_64) 64 little in the same file:&lt;/p&gt;&lt;code&gt;echo amd64fil0 x86_64+fil0 amd64fil0 64 little &amp;gt;&amp;gt; /usr/share/dpkg/cputable
&lt;/code&gt;

&lt;p&gt;Also, allow apt to install packages compiled for this architecture (beware that this will also later make apt update look for that architecture on servers, and whimper a bit for not finding it, but nothing breaks):&lt;/p&gt;&lt;code&gt;dpkg --add-architecture amd64fil0
&lt;/code&gt;
&lt;p&gt;Also, teach autoconf to accept amd64fil0 (the third of these lines is what's critical for Debian builds):&lt;/p&gt;&lt;code&gt;sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/autoconf/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/libtool/build-aux/config.sub
sed -i '/| x86_64 / a| x86_64+fil0 \\' /usr/share/misc/config.sub
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As a filian user, compile Fil-C and its standard library:&lt;/p&gt;&lt;code&gt;cd
git clone https://github.com/pizlonator/fil-c.git
cd fil-c
time ./build_all_fast_glibc.sh
&lt;/code&gt;
&lt;p&gt;[Not necessary if you've used filian-install-compiler:] As root, copy Fil-C and its standard library into system locations:&lt;/p&gt;&lt;code&gt;mkdir -p /usr/libexec/fil/amd64/compiler
time cp -r /home/filian/fil-c/pizfix /usr/libexec/fil/amd64/
rm -rf /usr/lib/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/lib /usr/lib/x86_64+fil0-linux-gnu
ln -s /usr/lib/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/lib
rm -rf /usr/include/x86_64+fil0-linux-gnu
mv /usr/libexec/fil/amd64/pizfix/include /usr/include/x86_64+fil0-linux-gnu
ln -s /usr/include/x86_64+fil0-linux-gnu /usr/libexec/fil/amd64/pizfix/include
time cp -r /home/filian/fil-c/build/bin /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/include /usr/libexec/fil/amd64/compiler/
time cp -r /home/filian/fil-c/build/lib /usr/libexec/fil/amd64/compiler/
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/filcc "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-gcc
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-gcc
( echo '#!/bin/sh'
  echo 'exec /usr/libexec/fil/amd64/compiler/bin/fil++ "$@"' ) &amp;gt; /usr/bin/x86_64+fil0-linux-gnu-g++
chmod 755 /usr/bin/x86_64+fil0-linux-gnu-g++
ln -s /usr/libexec/fil/amd64/compiler/bin/llvm-objdump /usr/bin/x86_64+fil0-linux-gnu-objdump
ln -s x86_64+fil0-linux-gnu-gcc /usr/bin/filcc
ln -s x86_64+fil0-linux-gnu-g++ /usr/bin/fil++
&lt;/code&gt;
&lt;p&gt;Now, as user filian (or whichever other user), let's make a little helper script to adjust a Debian source package:&lt;/p&gt;&lt;code&gt;mkdir -p $HOME/bin
( echo '#!/bin/sh'
  echo 'sed -i '\''s/^ \([^"]*\)$/ pizlonated_\1/'\'' debian/*.symbols'
  echo 'find . -name '\''*.map'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if ($1 == "local:") global = 0'
  echo '    if ($1 == "}") global = 0'
  echo '    if (global &amp;amp;&amp;amp; NF &amp;gt; 0 &amp;amp;&amp;amp; !index($0,"c++")) $1 = "pizlonated_"$1'
  echo '    if ($1 == "global:") global = 1'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
  echo 'find debian -name '\''*.install'\'' | while read fn'
  echo 'do'
  echo '  awk '\''{'
  echo '    if (NF == 2 &amp;amp;&amp;amp; $2 == "usr/include") $2 = $2"/${DEB_HOST_MULTIARCH}"'
  echo '    if (NF == 1 &amp;amp;&amp;amp; $1 == "usr/include") { $2 = $1"/${DEB_HOST_MULTIARCH}"; $1 = $1"/*" }'
  echo '    print'
  echo '  }'\'' &amp;lt; $fn &amp;gt; $fn.tmp'
  echo '  mv $fn.tmp $fn'
  echo 'done'
) &amp;gt; $HOME/bin/fillet
chmod 755 $HOME/bin/fillet
&lt;/code&gt;
And now let's try building a small package:

&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source tinycdb
cd tinycdb-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
&lt;/code&gt;

&lt;p&gt;Explanation of the differences from a normal build:&lt;/p&gt;&lt;p&gt;For me this worked and produced three ../*.deb packages. Installing them as root also worked:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/*.deb
# some sanity checks:
apt list | grep tinycdb
# prints "tinycdb/stable 0.81-2 amd64" (available package)
# and prints "tinycdb/now 0.81-2 amd64fil0 [installed,local]"
dpkg -L tinycdb:amd64fil0
# lists various files such as /usr/bin/cdb
nm /usr/bin/cdb
# shows various symbols including "pizlonated" (Fil-C) symbols
ldd /usr/bin/cdb
# shows dependence on libraries in /usr/libexec/fil
/usr/bin/cdb -h
# prints a help message: "cdb: Constant DataBase" etc.
&lt;/code&gt;
&lt;p&gt;Compiling a deliberately wrong test program with the newly installed library also works, and triggers Fil-C's run-time protection:&lt;/p&gt;&lt;code&gt;cd /root
( echo '#include &amp;lt;cdb.h&amp;gt;'
  echo 'int main() { cdb_init(0,0); return 0; }' ) &amp;gt; usecdb.c
filcc -o usecdb usecdb.c -lcdb
./usecdb &amp;lt; /bin/bash
# ... "filc panic: thwarted a futile attempt to violate memory safety."
&lt;/code&gt;

&lt;p&gt;libc-dev. Some packages depend on libc-dev, so let's build a fake libc-dev package (probably there's an easier way to do this):&lt;/p&gt;&lt;code&gt;FAKEPACKAGE=libc-dev
mkdir -p ~/shared/packages/$FAKEPACKAGE/debian
cd ~/shared/packages/$FAKEPACKAGE
( echo $FAKEPACKAGE' (0.0) unstable; urgency=medium'
  echo ''
  echo '  * Initial Release.'
  echo ''
  echo ' -- djb &amp;lt;djb@cr.yp.to&amp;gt;  Sun, 26 Oct 2025 16:05:17 +0000'
) &amp;gt; debian/changelog
( echo 'Source: '$FAKEPACKAGE
  echo 'Build-Depends: debhelper-compat (= 13)'
  echo 'Maintainer: djb &lt;/code&gt;
&lt;p&gt;ncurses.&lt;/p&gt;&lt;code&gt;mkdir -p ~/shared/packages
cd ~/shared/packages
apt source ncurses
cd ncurses-*/
$HOME/bin/fillet
time env DPKG_GENSYMBOLS_CHECK_LEVEL=0 \
  DEB_BUILD_OPTIONS='crossbuildcanrunhostbinaries nostrip' \
  dpkg-buildpackage -d -us -uc -b -a amd64fil0
rm ../ncurses-*deb # apt won't let us touch the binaries
&lt;/code&gt;
&lt;p&gt;As root, install the above libraries:&lt;/p&gt;&lt;code&gt;apt install /home/filian/shared/packages/lib*.deb
&lt;/code&gt;
&lt;p&gt;libmd. Seems to work. At first this didn't install since the compiled version (for amd64fil0) was 1.1.0-2 while the installed version (for amd64) was 1.1.0-2+b1. Debian requires the same version number across architectures (see above regarding include-file compatibility), so apt said that 1.1.0-2+b1 breaks 1.1.0-2. I resolved this by compiling and installing 1.1.0-2 for both amd64 and amd64fil0. This is a downgrade since "+b" refers to a "binNMU", a "binary-only non-maintainer upload", a patch beyond the official source; I don't know what the patch is.&lt;/p&gt;&lt;p&gt;readline. Needs ln -s /usr/include/readline /usr/include/x86_64+fil0-linux-gnu/readline after installation. Could have tweaks in debian/rules (which seems to predate *.install), but this is in any case an example of the messiness that I'm planning to get rid of.&lt;/p&gt;&lt;p&gt;lua5.4. Seems to work. Depends on readline.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45788040</guid><pubDate>Sun, 02 Nov 2025 05:32:02 +0000</pubDate></item><item><title>Using FreeBSD to make self-hosting fun again</title><link>https://jsteuernagel.de/posts/using-freebsd-to-make-self-hosting-fun-again/</link><description>&lt;doc fingerprint="3f1431577be6acd1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using FreeBSD to make self-hosting fun again&lt;/head&gt;
    &lt;p&gt;2025-11-01 - Feeling like a kid in a candy store, once more&lt;/p&gt;
    &lt;p&gt;As evident by my last blog post "A prison of my own making", I needed to change something about my relationship with technology. How I was doing things didn't work anymore, but I also felt unable to change anything about it, as the way I was doing things seemed like the way that I was supposed to use.&lt;/p&gt;
    &lt;p&gt;What I needed was a fresh start. And I managed to find that fresh start in the BSD family of operating systems.&lt;/p&gt;
    &lt;p&gt;I had already given FreeBSD and OpenBSD a try at the time and I liked what I saw. OpenBSD had already established itself in my workflow as an easy to use and reliable router and general OS for single-purpose VMs. But it isn't able to fullfill my needs for a multi-purpose system, where I'd want to run multiple separated workloads in something like a container or VM. But FreeBSD could.&lt;/p&gt;
    &lt;p&gt;I know that I generally operate best by just committing to using a thing and then figuring out what I need, as I need it. So I committed to using FreeBSD and found a really nice server to do just that on the Hetzner server auction.&lt;/p&gt;
    &lt;p&gt;I started setting it up with BastilleBSD for jails and vm-bhyve for VMs. I didn't know how to do most things and felt kinda lost. But there it was again, that feeling of excitement to learn something new, which got my into self-hosting in the first place.&lt;/p&gt;
    &lt;p&gt;After some trial and error I managed to find a setup that works for me. As per usual, it deviates a bit from what might be the most common setup, but it's undoubtedly me (I'll probably explain more about it in the future, when things have settled).&lt;/p&gt;
    &lt;p&gt;What I've come to appreciate about FreeBSD, and the BSD operating systems in general, is their simplicity and good documentation. Most tasks are just a few commands to run via SSH and if that isn't the case, someone has probably written a decent wrapper around it. If I need to find a piece of information, I still instinctively search online for it, just to be greeted by an online version of the corresponding man page. So I could also have just gathered that information on the CLI, oh well.&lt;/p&gt;
    &lt;p&gt;I also love the focus on long-term compatibility. I can find a solution to a problem in a forum post from 2008 and not even for a second do I have to doubt whether it will work, because it always does. At the same time, that doesn't mean there are no new features. The system doesn't feel old.&lt;/p&gt;
    &lt;p&gt;Sure, not everything was all roses and some of that was probably due to my way of just jumping into a problem and digging myself through it one step at a time, instead of reading up on it a lot beforehand. For example I was confused for a long time about the release cycle of the base system and whether that somehow related to pkg and ports (It does not). And I was not able to properly phrase the question in a way that would result in a helpful result while searching. Luckily the BSD community has been nothing but kind and helpful so far. I've had multiple people on the Fediverse offer their help and when I had a specific question, I would always get multiple solid answers explaining it to me. Thanks to everyone that replied, it's genuinely a blast to feel like a newbie again!&lt;/p&gt;
    &lt;p&gt;I don't know whether I will actually stick with all of what I'm doing right now, in the long term. But that's not important. What is important is that I'm having fun, learning a new thing, right now. I'll see what sticks long-term.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;@Joel: See? I wrote a blog post! :D&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789424</guid><pubDate>Sun, 02 Nov 2025 11:01:23 +0000</pubDate></item><item><title>URLs are state containers</title><link>https://alfy.blog/2025/10/31/your-url-is-your-state.html</link><description>&lt;doc fingerprint="abc21e28a1a26d89"&gt;
  &lt;main&gt;
    &lt;p&gt;Couple of weeks ago when I was publishing The Hidden Cost of URL Design I needed to add SQL syntax highlighting. I headed to PrismJS website trying to remember if it should be added as a plugin or what. I was overwhelmed with the amount of options in the download page so I headed back to my code. I checked the file for PrismJS and at the top of the file, I found a comment containing a URL:&lt;/p&gt;
    &lt;code&gt;/* https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript+bash+css-extras+markdown+scss+sql&amp;amp;plugins=line-highlight+line-numbers+autolinker */
&lt;/code&gt;
    &lt;p&gt;I had completely forgotten about this. I clicked the URL, and it was the PrismJS download page with every checkbox, dropdown, and option pre-selected to match my exact configuration. Themes chosen. Languages selected. Plugins enabled. Everything, perfectly reconstructed from that single URL.&lt;/p&gt;
    &lt;p&gt;It was one of those moments where something you once knew suddenly clicks again with fresh significance. Here was a URL doing far more than just pointing to a page. It was storing state, encoding intent, and making my entire setup shareable and recoverable. No database. No cookies. No localStorage. Just a URL.&lt;/p&gt;
    &lt;p&gt;This got me thinking: how often do we, as frontend engineers, overlook the URL as a state management tool? We reach for all sorts of abstractions to manage state such as global stores, contexts, and caches while ignoring one of the webâs most elegant and oldest features: the humble URL.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, I want to flip that perspective and talk about the immense value of good URL design. Specifically, how URLs can be treated as first-class state containers in modern web applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Overlooked Power of URLs&lt;/head&gt;
    &lt;p&gt;Scott Hanselman famously said âURLs are UIâ and heâs absolutely right. URLs arenât just technical addresses that browsers use to fetch resources. Theyâre interfaces. Theyâre part of the user experience.&lt;/p&gt;
    &lt;p&gt;But URLs are more than UI. Theyâre state containers. Every time you craft a URL, youâre making decisions about what information to preserve, what to make shareable, and what to make bookmarkable.&lt;/p&gt;
    &lt;p&gt;Think about what URLs give us for free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shareability: Send someone a link, and they see exactly what you see&lt;/item&gt;
      &lt;item&gt;Bookmarkability: Save a URL, and youâve saved a moment in time&lt;/item&gt;
      &lt;item&gt;Browser history: The back button just works&lt;/item&gt;
      &lt;item&gt;Deep linking: Jump directly into a specific application state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs make web applications resilient and predictable. Theyâre the webâs original state management solution, and theyâve been working reliably since 1991. The question isnât whether URLs can store state. Itâs whether weâre using them to their full potential.&lt;/p&gt;
    &lt;p&gt;Before we dive into examples, letâs break down how URLs encode state. Hereâs a typical stateful URL:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For many years, these were considered the only components of a URL. That changed with the introduction of Text Fragments, a feature that allows linking directly to a specific piece of text within a page. You can read more about it in my article Smarter than âCtrl+Fâ: Linking Directly to Web Page Content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Different parts of the URL encode different types of state:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Path Segments (&lt;code&gt;/path/to/myfile.html&lt;/code&gt;). Best used for hierarchical resource navigation:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;/users/123/posts&lt;/code&gt;- User 123âs posts&lt;/item&gt;&lt;item&gt;&lt;code&gt;/docs/api/authentication&lt;/code&gt;- Documentation structure&lt;/item&gt;&lt;item&gt;&lt;code&gt;/dashboard/analytics&lt;/code&gt;- Application sections&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Query Parameters (&lt;code&gt;?key1=value1&amp;amp;key2=value2&lt;/code&gt;). Perfect for filters, options, and configuration:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;?theme=dark&amp;amp;lang=en&lt;/code&gt;- UI preferences&lt;/item&gt;&lt;item&gt;&lt;code&gt;?page=2&amp;amp;limit=20&lt;/code&gt;- Pagination&lt;/item&gt;&lt;item&gt;&lt;code&gt;?status=active&amp;amp;sort=date&lt;/code&gt;- Data filtering&lt;/item&gt;&lt;item&gt;&lt;code&gt;?from=2025-01-01&amp;amp;to=2025-12-31&lt;/code&gt;- Date ranges&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Anchor&lt;/del&gt;Fragment (&lt;code&gt;#SomewhereInTheDocument&lt;/code&gt;). Ideal for client-side navigation and page sections:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;#L20-L35&lt;/code&gt;- GitHub line highlighting&lt;/item&gt;&lt;item&gt;&lt;code&gt;#features&lt;/code&gt;- Scroll to section&lt;/item&gt;&lt;item&gt;&lt;code&gt;#/dashboard&lt;/code&gt;- Single-page app routing (though itâs rarely used these days)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Common Patterns That Work for Query Parameters&lt;/head&gt;
    &lt;head rend="h4"&gt;Multiple values with delimiters&lt;/head&gt;
    &lt;p&gt;Sometimes youâll see multiple values packed into a single key using delimiters like commas or plus signs. Itâs compact and human-readable, though it requires manual parsing on the server side.&lt;/p&gt;
    &lt;code&gt;?languages=javascript+typescript+python
?tags=frontend,react,hooks
&lt;/code&gt;
    &lt;head rend="h4"&gt;Nested or structured data&lt;/head&gt;
    &lt;p&gt;Developers often encode complex filters or configuration objects into a single query string. A simple convention uses keyâvalue pairs separated by commas, while others serialize JSON or even Base64-encode it for safety.&lt;/p&gt;
    &lt;code&gt;?filters=status:active,owner:me,priority:high
?config=eyJyaWNrIjoicm9sbCJ9==  (base64-encoded JSON)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Boolean flags&lt;/head&gt;
    &lt;p&gt;For flags or toggles, itâs common to pass booleans explicitly or to rely on the keyâs presence as truthy. This keeps URLs shorter and makes toggling features easy.&lt;/p&gt;
    &lt;code&gt;?debug=true&amp;amp;analytics=false
?mobile  (presence = true)
&lt;/code&gt;
    &lt;head rend="h4"&gt;Arrays (Bracket notation)&lt;/head&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
&lt;/code&gt;
    &lt;p&gt;Another old pattern is bracket notation, which represents arrays in query parameters. It originated from early web frameworks like PHP where appending &lt;code&gt;[]&lt;/code&gt; to a parameter name signals that multiple values should be grouped together.&lt;/p&gt;
    &lt;code&gt;?tags[]=frontend&amp;amp;tags[]=react&amp;amp;tags[]=hooks
?ids[0]=42&amp;amp;ids[1]=73
&lt;/code&gt;
    &lt;p&gt;Many modern frameworks and parsers (like Nodeâs &lt;code&gt;qs&lt;/code&gt; library or Express middleware) still recognize this pattern automatically. However, itâs not officially standardized in the URL specification, so behavior can vary depending on the server or client implementation. Notice how it even breaks the syntax highlighting on my website.&lt;/p&gt;
    &lt;p&gt;The key is consistency. Pick patterns that make sense for your application and stick with them.&lt;/p&gt;
    &lt;head rend="h2"&gt;State via URL Parameters&lt;/head&gt;
    &lt;p&gt;Letâs look at real-world examples of URLs as state containers:&lt;/p&gt;
    &lt;p&gt;PrismJS Configuration&lt;/p&gt;
    &lt;code&gt;https://prismjs.com/download.html#themes=prism&amp;amp;languages=markup+css+clike+javascript&amp;amp;plugins=line-numbers
&lt;/code&gt;
    &lt;p&gt;The entire syntax highlighter configuration encoded in the URL. Change anything in the UI, and the URL updates. Share the URL, and someone else gets your exact setup. This one uses anchor and not query parameters, but the concept is the same.&lt;/p&gt;
    &lt;p&gt;GitHub Line Highlighting&lt;/p&gt;
    &lt;code&gt;https://github.com/zepouet/Xee-xCode-4.5/blob/master/XeePhotoshopLoader.m#L108-L136
&lt;/code&gt;
    &lt;p&gt;It links to a specific file while highlighting lines 108 through 136. Click this link anywhere, and youâll land on the exact code section being discussed.&lt;/p&gt;
    &lt;p&gt;Google Maps&lt;/p&gt;
    &lt;code&gt;https://www.google.com/maps/@22.443842,-74.220744,19z
&lt;/code&gt;
    &lt;p&gt;Coordinates, zoom level, and map type all in the URL. Share this link, and anyone can see the exact same view of the map.&lt;/p&gt;
    &lt;p&gt;Figma and Design Tools&lt;/p&gt;
    &lt;code&gt;https://www.figma.com/file/abc123/MyDesign?node-id=123:456&amp;amp;viewport=100,200,0.5
&lt;/code&gt;
    &lt;p&gt;Before shareable design links, finding an updated screen or component in a large file was a chore. Someone had to literally show you where it lived, scrolling and zooming across layers. Today, a Figma link carries all that context like canvas position, zoom level, selected element. Literally everything needed to drop you right into the workspace.&lt;/p&gt;
    &lt;p&gt;E-commerce Filters&lt;/p&gt;
    &lt;code&gt;https://store.com/laptops?brand=dell+hp&amp;amp;price=500-1500&amp;amp;rating=4&amp;amp;sort=price-asc
&lt;/code&gt;
    &lt;p&gt;This is one of the most common real-world patterns youâll encounter. Every filter, sort option, and price range preserved. Users can bookmark their exact search criteria and return to it anytime. Most importantly, they can come back to it after navigating away or refreshing the page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frontend Engineering Patterns&lt;/head&gt;
    &lt;p&gt;Before we discuss implementation details, we need to establish a clear guideline for what should go into the URL. Not all state belongs in URLs. Hereâs a simple heuristic:&lt;/p&gt;
    &lt;p&gt;Good candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search queries and filters&lt;/item&gt;
      &lt;item&gt;Pagination and sorting&lt;/item&gt;
      &lt;item&gt;View modes (list/grid, dark/light)&lt;/item&gt;
      &lt;item&gt;Date ranges and time periods&lt;/item&gt;
      &lt;item&gt;Selected items or active tabs&lt;/item&gt;
      &lt;item&gt;UI configuration that affects content&lt;/item&gt;
      &lt;item&gt;Feature flags and A/B test variants&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Poor candidates for URL state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sensitive information (passwords, tokens, PII)&lt;/item&gt;
      &lt;item&gt;Temporary UI states (modal open/closed, dropdown expanded)&lt;/item&gt;
      &lt;item&gt;Form input in progress (unsaved changes)&lt;/item&gt;
      &lt;item&gt;Extremely large or complex nested data&lt;/item&gt;
      &lt;item&gt;High-frequency transient states (mouse position, scroll position)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are not sure if a piece of state belongs in the URL, ask yourself: If someone else clicking this URL, should they see the same state? If so, it belongs in the URL. If not, use a different state management approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using Plain JavaScript&lt;/head&gt;
    &lt;p&gt;The modern &lt;code&gt;URLSearchParams&lt;/code&gt; API makes URL state management straightforward:&lt;/p&gt;
    &lt;code&gt;// Reading URL parameters
const params = new URLSearchParams(window.location.search);
const view = params.get('view') || 'grid';
const page = params.get('page') || 1;

// Updating URL parameters
function updateFilters(filters) {
  const params = new URLSearchParams(window.location.search);

  // Update individual parameters
  params.set('status', filters.status);
  params.set('sort', filters.sort);

  // Update URL without page reload
  const newUrl = `${window.location.pathname}?${params.toString()}`;
  window.history.pushState({}, '', newUrl);

  // Now update your UI based on the new filters
  renderContent(filters);
}

// Handling back/forward buttons
window.addEventListener('popstate', () =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  const filters = {
    status: params.get('status') || 'all',
    sort: params.get('sort') || 'date'
  };
  renderContent(filters);
});
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;popstate&lt;/code&gt; event fires when the user navigates with the browserâs Back or Forward buttons. It lets you restore the UI to match the URL, which is essential for keeping your appâs state and history in sync. Usually your frameworkâs router handles this for you, but itâs good to know how it works under the hood.&lt;/p&gt;
    &lt;head rend="h3"&gt;Implementation using React&lt;/head&gt;
    &lt;p&gt;React Router and Next.js provide hooks that make this even cleaner:&lt;/p&gt;
    &lt;code&gt;import { useSearchParams } from 'react-router-dom';
// or for Next.js 13+: import { useSearchParams } from 'next/navigation';

function ProductList() {
  const [searchParams, setSearchParams] = useSearchParams();

  // Read from URL (with defaults)
  const color = searchParams.get('color') || 'all';
  const sort = searchParams.get('sort') || 'price';

  // Update URL
  const handleColorChange = (newColor) =&amp;gt; {
    setSearchParams(prev =&amp;gt; {
      const params = new URLSearchParams(prev);
      params.set('color', newColor);
      return params;
    });
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;select value={color} onChange={e =&amp;gt; handleColorChange(e.target.value)}&amp;gt;
        &amp;lt;option value="all"&amp;gt;All Colors&amp;lt;/option&amp;gt;
        &amp;lt;option value="silver"&amp;gt;Silver&amp;lt;/option&amp;gt;
        &amp;lt;option value="black"&amp;gt;Black&amp;lt;/option&amp;gt;
      &amp;lt;/select&amp;gt;

      {/* Your filtered products render here */}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Best Practices for URL State Management&lt;/head&gt;
    &lt;p&gt;Now that weâve seen how URLs can hold application state, letâs look at a few best practices that keep them clean, predictable, and user-friendly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Defaults Gracefully&lt;/head&gt;
    &lt;p&gt;Donât pollute URLs with default values:&lt;/p&gt;
    &lt;code&gt;// Bad: URL gets cluttered with defaults
?theme=light&amp;amp;lang=en&amp;amp;page=1&amp;amp;sort=date

// Good: Only non-default values in URL
?theme=dark  // light is default, so omit it
&lt;/code&gt;
    &lt;p&gt;Use defaults in your code when reading parameters:&lt;/p&gt;
    &lt;code&gt;function getTheme(params) {
  return params.get('theme') || 'light'; // Default handled in code
}
&lt;/code&gt;
    &lt;head rend="h4"&gt;Debouncing URL Updates&lt;/head&gt;
    &lt;p&gt;For high-frequency updates (like search-as-you-type), debounce URL changes:&lt;/p&gt;
    &lt;code&gt;import { debounce } from 'lodash';

const updateSearchParam = debounce((value) =&amp;gt; {
  const params = new URLSearchParams(window.location.search);
  if (value) {
    params.set('q', value);
  } else {
    params.delete('q');
  }
  window.history.replaceState({}, '', `?${params.toString()}`);
}, 300);

// Use replaceState instead of pushState to avoid flooding history
&lt;/code&gt;
    &lt;head rend="h4"&gt;pushState vs. replaceState&lt;/head&gt;
    &lt;p&gt;When deciding between &lt;code&gt;pushState&lt;/code&gt; and &lt;code&gt;replaceState&lt;/code&gt;, think about how you want the browser history to behave. &lt;code&gt;pushState&lt;/code&gt; creates a new history entry, which makes sense for distinct navigation actions like changing filters, pagination, or navigating to a new view â users can then use the Back button to return to the previous state. On the other hand, &lt;code&gt;replaceState&lt;/code&gt; updates the current entry without adding a new one, making it ideal for refinements such as search-as-you-type or minor UI adjustments where you donât want to flood the history with every keystroke.&lt;/p&gt;
    &lt;head rend="h2"&gt;URLs as Contracts&lt;/head&gt;
    &lt;p&gt;When designed thoughtfully, URLs become more than just state containers. They become contracts between your application and its consumers. A good URL defines expectations for humans, developers, and machines alike&lt;/p&gt;
    &lt;head rend="h3"&gt;Clear Boundaries&lt;/head&gt;
    &lt;p&gt;A well-structured URL draws the line between whatâs public and whatâs private, client and server, shareable and session-specific. It clarifies where state lives and how it should behave. Developers know whatâs safe to persist, users know what they can bookmark, and machines know whats worth indexing.&lt;/p&gt;
    &lt;p&gt;URLs, in that sense, act as interfaces: visible, predictable, and stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Communicating Meaning&lt;/head&gt;
    &lt;p&gt;Readable URLs explain themselves. Consider the difference between the two URLs below.&lt;/p&gt;
    &lt;code&gt;https://example.com/p?id=x7f2k&amp;amp;v=3
https://example.com/products/laptop?color=silver&amp;amp;sort=price
&lt;/code&gt;
    &lt;p&gt;The first one hides intent. The second tells a story. A human can read it and understand what theyâre looking at. A machine can parse it and extract meaningful structure.&lt;/p&gt;
    &lt;p&gt;Jim Nielsen calls these âexamples of great URLsâ. URLs that explain themselves.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching and Performance&lt;/head&gt;
    &lt;p&gt;URLs are cache keys. Well-designed URLs enable better caching strategies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Same URL = same resource = cache hit&lt;/item&gt;
      &lt;item&gt;Query params define cache variations&lt;/item&gt;
      &lt;item&gt;CDNs can cache intelligently based on URL patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can even visualize a userâs journey without any extra tracking code:&lt;/p&gt;
    &lt;quote&gt;graph LR A["/products"] --&amp;gt; |selects category| B["/products?category=laptops"] B --&amp;gt; |adds price filter| C["/products?category=laptops&amp;amp;price=500-1000"] style A fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style B fill:#e9edf7,stroke:#455d8d,stroke-width:2px; style C fill:#e9edf7,stroke:#455d8d,stroke-width:2px;&lt;/quote&gt;
    &lt;p&gt;Your analytics tools can track this flow without additional instrumentation. Every URL parameter becomes a dimension you can analyze.&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning and Evolution&lt;/head&gt;
    &lt;p&gt;URLs can communicate API versions, feature flags, and experiments:&lt;/p&gt;
    &lt;code&gt;?v=2                   // API version
?beta=true             // Beta features
?experiment=new-ui     // A/B test variant
&lt;/code&gt;
    &lt;p&gt;This makes gradual rollouts and backwards compatibility much more manageable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anti-Patterns to Avoid&lt;/head&gt;
    &lt;p&gt;Even with the best intentions, itâs easy to misuse URL state. Here are common pitfalls:&lt;/p&gt;
    &lt;head rend="h3"&gt;âState Only in Memoryâ SPAs&lt;/head&gt;
    &lt;p&gt;The classic single-page app mistake:&lt;/p&gt;
    &lt;code&gt;// User hits refresh and loses everything
const [filters, setFilters] = useState({});
&lt;/code&gt;
    &lt;p&gt;If your app forgets its state on refresh, youâre breaking one of the webâs fundamental features. Users expect URLs to preserve context. I remember a viral video from years ago where a Reddit user vented about an e-commerce site: every time she hit âBack,â all her filters disappeared. Her frustration summed it up perfectly. If users lose context, they lose patience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensitive Data in URLs&lt;/head&gt;
    &lt;p&gt;This one seems obvious, but itâs worth repeating:&lt;/p&gt;
    &lt;code&gt;// NEVER DO THIS
?password=secret123
&lt;/code&gt;
    &lt;p&gt;URLs are logged everywhere: browser history, server logs, analytics, referrer headers. Treat them as public.&lt;/p&gt;
    &lt;head rend="h3"&gt;Inconsistent or Opaque Naming&lt;/head&gt;
    &lt;code&gt;// Unclear and inconsistent
?foo=true&amp;amp;bar=2&amp;amp;x=dark

// Self-documenting and consistent
?mobile=true&amp;amp;page=2&amp;amp;theme=dark
&lt;/code&gt;
    &lt;p&gt;Choose parameter names that make sense. Future you (and your team) will thank you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overloading URLs with Complex State&lt;/head&gt;
    &lt;code&gt;?config=eyJtZXNzYWdlIjoiZGlkIHlvdSByZWFsbHkgdHJpZWQgdG8gZGVjb2RlIHRoYXQ_IiwiZmlsdGVycyI6eyJzdGF0dXMiOlsiYWN0aXZlIiwicGVuZGluZyJdLCJwcmlvcml0eSI6WyJoaWdoIiwibWVkaXVtIl0sInRhZ3MiOlsiZnJvbnRlbmQiLCJyZWFjdCIsImhvb2tzIl0sInJhbmdlIjp7ImZyb20iOiIyMDI0LTAxLTAxIiwidG8iOiIyMDI0LTEyLTMxIn19LCJzb3J0Ijp7ImZpZWxkIjoiY3JlYXRlZEF0Iiwib3JkZXIiOiJkZXNjIn0sInBhZ2luYXRpb24iOnsicGFnZSI6MSwibGltaXQiOjIwfX0==
&lt;/code&gt;
    &lt;p&gt;If you need to base64-encode a massive JSON object, the URL probably isnât the right place for that state.&lt;/p&gt;
    &lt;head rend="h3"&gt;URL Length Limits&lt;/head&gt;
    &lt;p&gt;Browsers and servers impose practical limits on URL length (usually between 2,000 and 8,000 characters) but the reality is more nuanced. As this detailed Stack Overflow answer explains, limits come from a mix of browser behavior, server configurations, CDNs, and even search engine constraints. If youâre bumping against them, itâs a sign you need to rethink your approach.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the Back Button&lt;/head&gt;
    &lt;code&gt;// Replacing state incorrectly
history.replaceState({}, '', newUrl); // Used when pushState was needed
&lt;/code&gt;
    &lt;p&gt;Respect browser history. If a user action should be âundoableâ via the back button, use &lt;code&gt;pushState&lt;/code&gt;. If itâs a refinement, use &lt;code&gt;replaceState&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thought&lt;/head&gt;
    &lt;p&gt;That PrismJS URL reminded me of something important: good URLs donât just point to content. They describe a conversation between the user and the application. They capture intent, preserve context, and enable sharing in ways that no other state management solution can match.&lt;/p&gt;
    &lt;p&gt;Weâve built increasingly sophisticated state management libraries like Redux, MobX, Zustand, Recoil and others. They all have their place but sometimes the best solution is the one thatâs been there all along.&lt;/p&gt;
    &lt;p&gt;In my previous article, I wrote about the hidden costs of bad URL design. Today, weâve explored the flip side: the immense value of good URL design. URLs arenât just addresses. Theyâre state containers, user interfaces, and contracts all rolled into one.&lt;/p&gt;
    &lt;p&gt;If your app forgets its state when you hit refresh, youâre missing one of the webâs oldest and most elegant features.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789474</guid><pubDate>Sun, 02 Nov 2025 11:12:58 +0000</pubDate></item><item><title>Mock – An API creation and testing utility: Examples</title><link>https://dhuan.github.io/mock/latest/examples.html</link><description>&lt;doc fingerprint="9c9aac8da400f802"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How-tos &amp;amp; Examples¶&lt;/head&gt;
    &lt;head rend="h2"&gt;Delaying specific endpoints¶&lt;/head&gt;
    &lt;p&gt;Making an existing API slow can be easily accomplished combining mock’s Base APIs and the delay option.&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --delay 2000
&lt;/code&gt;
    &lt;p&gt;You may want however to make a specific endpoint slow instead of the whole API. This can be achieved using middlewares:&lt;/p&gt;
    &lt;code&gt;$ mock serve -p 8000 --base example.com --middleware '
if [ "${MOCK_REQUEST_ENDPOINT}" = "some/endpoint" ]
then
    sleep 2 # wait two seconds
fi
'
&lt;/code&gt;
    &lt;p&gt;With that last example, our API at &lt;code&gt;localhost:8000&lt;/code&gt; will act as a proxy to
&lt;code&gt;example.com&lt;/code&gt;. All requests will be responded immediately except
&lt;code&gt;some/endpoint&lt;/code&gt; which will have a delay of 2 seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;An API powered by multiple languages¶&lt;/head&gt;
    &lt;code&gt;$ mock serve -p 3000 \
    --route js \
    --exec '
node &amp;lt;&amp;lt;EOF | mock write
console.log("Hello from Node.js!")
EOF
' \
    --route python \
    --exec '
python3 &amp;lt;&amp;lt;EOF | mock write
print("Hello from Python!")
EOF
' \
    --route php \
    --exec '
php &amp;lt;&amp;lt;EOF | mock write
&amp;lt;?php
echo "Hello from PHP!\n";
?&amp;gt;
EOF
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/js
# Prints out: Hello from Node.js!
$ curl localhost:3000/python
# Prints out: Hello from Python!
$ curl localhost:3000/php
# Prints out: Hello from PHP!
&lt;/code&gt;
    &lt;head rend="h2"&gt;A stateful API¶&lt;/head&gt;
    &lt;code&gt;$ export TMP=$(mktemp)
$ printf "0" &amp;gt; "${TMP}"

$ mock serve -p 3000 \
    --route '/hello' \
    --exec '
printf "%s + 1\n" "$(cat ${TMP})" | bc | sponge "${TMP}"

printf "This server has received %s request(s) so far." "$(cat '"${TMP}"')" | mock write
'
&lt;/code&gt;
    &lt;p&gt;Let’s test it:&lt;/p&gt;
    &lt;code&gt;$ curl localhost:3000/hello
# Prints out: This server has received 1 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 2 request(s) so far.
$ curl localhost:3000/hello
# Prints out: This server has received 3 request(s) so far.
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789556</guid><pubDate>Sun, 02 Nov 2025 11:30:50 +0000</pubDate></item><item><title>HyperRogue – A non-Euclidean roguelike</title><link>https://roguetemple.com/z/hyper/</link><description>&lt;doc fingerprint="39a78a5a89f4b621"&gt;
  &lt;main&gt;
    &lt;div&gt;See the Gallery for the high quality images of all lands.&lt;p&gt; You are a lone adventurer in a strange world, where geometry does not work in the expected way. Gather as much treasure as you can before the nasty monsters get you. Explore about 50 different worlds, each with its own unique treasures, enemies, and terrain obstacles. Your quest is to find the legendary treasure, the Orbs of Yendor. Collect one of them to win! Or just ignore your quest and collect smaller treasures. &lt;/p&gt;&lt;p&gt; The twist is the unique, unusual geometry of the world: it is one of just few games which takes place on the &lt;/p&gt;hyperbolic plane&lt;p&gt;. Witness a grid composed of hexagons and heptagons, straight lines which seem to be parallel, but then they diverge and never cross, triangles whose angles add up to less than 180 degrees, how extremely unlikely is it to reach the same place twice, and how the world seems to be rotated when you do return. All this matters for the gameplay. The game is inspired by the roguelike genre (although in a very minimalist way), works of &lt;/p&gt;M. C. Escher&lt;p&gt;, and by puzzle games such as &lt;/p&gt;Deadly Rooms of Death&lt;p&gt;. &lt;/p&gt;&lt;head rend="h3"&gt;A very infinite world&lt;/head&gt;&lt;p&gt; With more space than anything Euclidean. The game dynamically generates new parts of the world as you move. No previous understanding of hyperbolic geometry is required -- actually, playing HyperRogue is probably the best way to learn about this, much better and deeper than any mathematical formulas. It is virtually impossible to get back to a place where you have been before, unless you go back exactly the same way. Show your true mastery of hyperbolic navigation by finding the &lt;/p&gt;Orb of Yendor&lt;p&gt;, &lt;/p&gt;Holy Grail&lt;p&gt;, rescuing the &lt;/p&gt;Prince(ss)&lt;p&gt;! &lt;/p&gt;&lt;head rend="h3"&gt;Lots of variety&lt;/head&gt; 72 lands&lt;p&gt; (72 in the free version), each with unique theme, mechanics, graphics, terrain features, native monsters, treasure type, and magical Orb power. The ultimate &lt;/p&gt;Hyperstone Quest&lt;p&gt; requires you to get 10 treasures in each of the lands! &lt;/p&gt;&lt;head rend="h3"&gt;Simple but hard to master mechanics&lt;/head&gt;&lt;p&gt; In many ways, HyperRogue is closer to boardgames like Chess, than to mainstream computer games -- except that its "chessboard" is a hyperbolic plane, with randomly generated features. Enemies move predictably, and most can be killed simply by moving into them -- however, they could kill your character with a single attack too! Even though the game disallows you from making moves which would lead to this immediately ("check" in Chess), fighting large groups is still a challenge. &lt;/p&gt;&lt;head rend="h3"&gt;Even more challenge!&lt;/head&gt;&lt;p&gt; If you want even more challenge, you will get it easily, due to HyperRogue's difficulty/high score system. The more treasures you collect in a given land, the more monster chase you there. Collect 10 treasures in the given land to show the basic understanding of it, 25 treasures to show that you have mastered it, or go for even more! The game never ends, but it gets harder and harder. &lt;/p&gt;&lt;head rend="h3"&gt;Multiple special modes&lt;/head&gt;&lt;p&gt; Enable the shoot'em up mode, and the game is no longer turn-based or grid-based. Play together with your friend (shmup mode is recommended). Try the Euclidean, elliptic, or spherical modes, to see why the &lt;/p&gt;geometry&lt;p&gt; matters, or enable the heptagonal mode to make the hyperbolic effects stronger. Try extra challenges such as the Yendor Challenge or the Pure Tactics Mode, or make the game &lt;/p&gt;look differently&lt;p&gt; with the Hypersian Rug or Conformal mode. The recently added Orb Strategy mode emphasizes the resource management by giving you harder challenges while allowing you to use your limited magical powers in difficult situations. &lt;/p&gt;&lt;head rend="h3"&gt;Great game, educational thing, or maybe an artistic or research tool?&lt;/head&gt;&lt;p&gt; HyperRogue has started as a small, weird technical experiment, but it turned out that hyperbolic geometry combined with basic roguelike rules makes for exceptionally great gameplay, even if you do not care about geometry! Further work improved the gameplay, but also turned HyperRogue into probably the most fully featured engine for truly non-Euclidean geometry in existence. Even if you do not care about roguelikes, roguelites and block puzzles, you can play the tutorial as an explorable explanation about hyperbolic geometry, use HyperRogue for &lt;/p&gt;research in applied hyperbolic geometry&lt;p&gt;, or use the texture mode and vector graphics editor to create mathematical art. The possibilities are endless! &lt;/p&gt;&lt;head rend="h2"&gt;How to get it&lt;/head&gt;&lt;p&gt; HyperRogue can be &lt;/p&gt;downloaded freely&lt;p&gt; from this website, or bought on &lt;/p&gt;Steam&lt;p&gt; or &lt;/p&gt;itch.io&lt;p&gt;; the paid versions are updated more frequently and include social features such as achievements and leaderboards. There are also Android and iOS versions. &lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789596</guid><pubDate>Sun, 02 Nov 2025 11:40:47 +0000</pubDate></item><item><title>Tongyi DeepResearch – open-source 30B MoE Model that rivals OpenAI DeepResearch</title><link>https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</link><description>&lt;doc fingerprint="efd86393ad1b861a"&gt;
  &lt;main&gt;
    &lt;p&gt;GITHUB HUGGINGFACE MODELSCOPE SHOWCASE&lt;/p&gt;
    &lt;head rend="h2"&gt;From Chatbot to Autonomous Agent&lt;/head&gt;
    &lt;p&gt;We are proud to present Tongyi DeepResearch, the first fully open‑source Web Agent to achieve performance on par with OpenAI’s DeepResearch across a comprehensive suite of benchmarks. Tongyi DeepResearch demonstrates state‑of‑the‑art results, scoring 32.9 on the academic reasoning task Humanity’s Last Exam (HLE), 43.4 on BrowseComp and 46.7 on BrowseComp‑ZH in extremely complex information‑seeking tasks, and achieving a score of 75 on the user‑centric xbench‑DeepSearch benchmark, systematically outperforming all existing proprietary and open‑source Deep Research agents.&lt;/p&gt;
    &lt;p&gt;Beyond the model, we share a complete and battle‑tested methodology for creating such advanced agents. Our contribution details a novel data synthesis solution applied across the entire training pipeline, from Agentic Continual Pre‑training (CPT) and Supervised Fine‑Tuning (SFT) for cold‑starting, to the final Reinforcement Learning (RL) stage. For RL, we provide a full‑stack solution, including algorithmic innovations, automated data curation, and robust infrastructure. For inference, the vanilla ReAct framework showcases the model’s powerful intrinsic capabilities without any prompt engineering, while the advanced Heavy Mode (test‑time‑scaling) demonstrates the upper limits of its complex reasoning and planning potential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continual Pre‑training and Post‑training Empowered by Fully Synthetic Data&lt;/head&gt;
    &lt;head rend="h3"&gt;Continual Pre‑training Data&lt;/head&gt;
    &lt;p&gt;We introduce Agentic CPT to deep research agent training, creating powerful agentic foundation models for post‑training. We propose AgentFounder, a systematic and scalable solution for large‑scale data synthesis that creates a data flywheel with data from the post‑training pipeline.&lt;/p&gt;
    &lt;p&gt;Data Reorganization and Question Construction. We continuously collect data from various sources, including documents, publicly available crawled data, knowledge graphs, and historical trajectories and tool invocation records (e.g., search results with links). As shown in the figure, these diverse data sources are restructured into an entity‑anchored open‑world knowledge memory. Based on randomly sampled entities and their corresponding knowledge, we generate multi‑style (question,answer) pairs.&lt;/p&gt;
    &lt;p&gt;Action Synthesis. Based on diverse problems and historical trajectories, we construct first‑order action synthesis data and higher‑order action synthesis data. Our method enables large‑scale and comprehensive exploration of the potential reasoning‑action space within offline environments, thereby thereby eliminating the need for additional commercial tool API calls. Specifically, for the higher‑order action synthesis, we remodel trajectories as multi‑step decision‑making processes to enhance the model’s decision‑making capabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Post-training Data&lt;/head&gt;
    &lt;p&gt;High-quality synthetic QA pairs&lt;/p&gt;
    &lt;p&gt;We develop an end‑to‑end solution for synthetic data generation. This fully automated process requires no human intervention to construct super‑human quality datasets, designed to push the boundaries of AI agent performance. Through long‑term exploration and iteration‑from early methods like reverse‑engineering QA pairs from clickstreams (WebWalker) to the more systematic graph‑based synthesis (WebSailor and WebSailor‑V2), then the formalized task modeling (WebShaper)‑our approach ensures both exceptional data quality and massive scalability, breaking through the upper limits of model capabilities.&lt;/p&gt;
    &lt;p&gt;To address complex, high‑uncertainty questions, we synthesize web‑based QA data through a novel pipeline. The process begins by constructing a highly interconnected knowledge graph via random walks and isomorphic tables towards tabular data fusion from real‑world websites , ensuring a realistic information structure. We then sample subgraphs and subtables to generate initial questions and answers. The crucial step involves intentionally increasing difficulty by strategically obfuscating or blurring information within the question. This practical approach is grounded in a complete theoretical framework, where we formally model QA difficulty as a series of controllable “atomic operations” (e.g., merging entities with similar attributes) on entity relationships, allowing us to systematically increase complexity.&lt;/p&gt;
    &lt;p&gt;To further reduce inconsistencies between the organized information structure and the reasoning structure of QA, enable more controllable difficulty and structure scaling of reasoning, we proposed a formal modeling of the information‑seeking problem based on set theory. With this formalization, we developed agents that expands the problem in a controlled manner, and minimizes reasoning shortcuts and structural redundancy, leading to further improved QA quality. Moreover, this formal modeling also allows for efficient verification of QA correctness, effectively addressing the challenge of validating synthetic information‑seeking data for post‑training.&lt;/p&gt;
    &lt;p&gt;Furthermore, we have developed an automated data engine to scale up the creation of PhD‑level research questions. This engine begins with a multi‑disciplinary knowledge base, generating “seed” QA pairs that require multi‑source reasoning. Each seed then enters a self‑guided loop of “iterative complexity upgrades”, where a question‑crafting agent is equipped with a powerful toolset including web search, academic retrieval, and a Python execution environment. In each iteration, the agent expands knowledge boundaries, deepens conceptual abstraction, and even constructs computational tasks, creating a virtuous cycle where the output of one round becomes the more complex input for the next, ensuring a controllable and systematic escalation of task difficulty.&lt;/p&gt;
    &lt;p&gt;Unleashing Agent Capabilities with Diverse Reasoning Pattern&lt;/p&gt;
    &lt;p&gt;To bootstrap the model’s initial capabilities, we constructed a set of trajectories via rejection sampling, based on the ReAct and IterResearch frameworks (for details, see below). On one hand, ReAct, as a classic and foundational multi-turn reasoning format, instills rich reasoning behaviors and reinforces the model’s ability to adhere to structured formats.&lt;/p&gt;
    &lt;p&gt;On the other hand, we introduce IterResearch, an innovative agent paradigm (detailed below). It unleashes the model’s full reasoning potential by dynamically reconstructing a streamlined workspace in each turn, ensuring that every decision is deliberate and well-considered. Leveraging IterResearch, we constructed a set of trajectories that integrate reasoning, planning, and tool-use, thereby strengthening the model’s capacity for sustained planning when confronted with Long-Horizon tasks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rollout Mode&lt;/head&gt;
    &lt;p&gt;We have conducted extensive exploration into the rollout paradigms for DeepResearch‑type agents. As a result, our final model supports multiple rollout formats, including the native ReAct Mode and the context‑managing Heavy Mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native ReAct Mode&lt;/head&gt;
    &lt;p&gt;Our model demonstrates excellent performance using the native ReAct reasoning paradigm without any prompt engineering. It strictly adheres to the Thought‑Action‑Observation cycle, performing multiple iterations to solve problems. With a model context length of 128K, it can handle a large number of interaction rounds, fully achieving scaling in its interaction with the environment. ReAct’s simplicity and universality provide the clearest benchmark for a model’s intrinsic capabilities and the efficacy of our training pipeline.&lt;/p&gt;
    &lt;p&gt;Our choice of ReAct is heavily informed by “The Bitter Lesson”, which posits that general methods leveraging scalable computation ultimately outperform approaches that rely on complex, human‑engineered knowledge and intricate designs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heavy Mode&lt;/head&gt;
    &lt;p&gt;In addition to the native ReAct mode, we have developed a “Heavy Mode” for complex, multi‑step research tasks. This mode is built on our new IterResearch paradigm, designed to push the agent’s capabilities to their limit.&lt;/p&gt;
    &lt;p&gt;The IterResearch paradigm was created to solve the “cognitive suffocation” and “noise pollution” that occurs when agents accumulate all information into a single, ever‑expanding context. Instead, IterResearch deconstructs a task into a series of “research rounds”.&lt;/p&gt;
    &lt;p&gt;In each round, the agent reconstructs a streamlined workspace using only the most essential outputs from the previous round. Within this focused workspace, the agent analyzes the problem, integrates key findings into a continuously evolving central report, and then decides its next action‑either gathering more information or providing a final answer. This iterative process of “synthesis and reconstruction” allows the agent to maintain a clear “cognitive focus” and high reasoning quality throughout long tasks.&lt;/p&gt;
    &lt;p&gt;Building on this, we propose the Research‑Synthesis framework. In this model, multiple Research Agents use the IterResearch process to explore a problem in parallel. A final Synthesis Agent then integrates their refined reports and conclusions to produce a more comprehensive final answer. This parallel structure enables the model to consider a wider range of research paths within a limited context window, pushing its performance to the limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;End-to‑End Agent Training Pipeline&lt;/head&gt;
    &lt;p&gt;Training an agentic model like this required us to rethink the entire model training pipeline, from pre‑training to fine‑tuning to reinforcement learning. We established a new paradigm for agent model training that connects Agentic CPT → Agentic SFT → Agentic RL, creating a seamless end‑to‑end training loop for an AI agent. Here’s how we tackled the final stage with reinforcement learning, which was crucial for aligning the agent’s behavior with high‑level goals:&lt;/p&gt;
    &lt;head rend="h3"&gt;On‑Policy Agent Reinforcement Learning (RL)&lt;/head&gt;
    &lt;p&gt;Constructing a high‑quality agent through RL is a complex system engineering challenge; if this entire development process is viewed as a “reinforcement learning” loop, any instability or lack of robustness in its components can lead to erroneous “reward” signals. We will now share our practices in RL, covering both the algorithmic and infrastructure sides.&lt;/p&gt;
    &lt;p&gt;For RL algorithm, we made several algorithmic breakthroughs, using a customized on‑policy Group Relative Policy Optimization (GRPO). We employ a strictly on‑policy training regimen, ensuring that the learning signal is always relevant to the model’s current capabilities. The training objective is optimized using a token‑level policy gradient loss. Second, to further reduce variance in the advantage estimation, we adopt a leave‑one‑out strategy. Furthermore, we employ a conservative strategy for negative samples, having observed that an unfiltered set of negative trajectories significantly degrades training stability. This can manifest as a “format collapse” phenomenon after extended training. To mitigate this, we selectively exclude certain negative samples from the loss calculation, for instance, those that do not yield a final answer because they exceed a length limit. For the sake of efficiency, we do not employ dynamic sampling. We instead leverage larger batch and group sizes, which serve to maintain smaller variance and provide adequate supervision.&lt;/p&gt;
    &lt;p&gt;The training dynamics demonstrate effective learning, with a consistent upward trend in reward. Meanwhile, policy entropy remains consistently high, indicating sustained exploration and preventing premature convergence. We attribute this to the non‑stationary nature of the web environment, which naturally fosters a robust, adaptive policy and obviates the need for explicit entropy regularization.&lt;/p&gt;
    &lt;p&gt;We consider that the algorithm is important but not the only decisive factor in the success of Agentic RL. We have experimented with many different algorithms and tricks, and find that data and stability of the training environment are likely the more critical components in determining whether the RL works. Interestingly, we have tested to train the model directly on the BrowseComp testing set, but the results are substantially poorer than when using our synthetic data. We hypothesize that this disparity arises because the synthetic data offers a more consistent distribution, which allows the model to be more effectively tailored. Conversely, the human‑annotated data (such as BrowseComp) is inherently noisier. Given its limited scale, it is difficult to approximate a learnable underlying distribution, which consequently hinders the model to learn and generalize from it.&lt;/p&gt;
    &lt;p&gt;On the infrastructure side, training an agent with tools required us to develop a highly stable and efficient environment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Synthetic Training Environment: Relying on live web APIs for development is expensive, slow, and inconsistent. We addressed this by creating a simulated training environment using an offline Wikipedia database and a custom tool suite. By adapting our data pipeline to generate high‑quality, complex tasks for this environment, we created a cost‑effective, fast, and controllable platform that dramatically accelerates our research and iteration.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stable &amp;amp; Efficient Tool Sandbox: To ensure reliable tool use during agent training and evaluation, we developed a unified sandbox. The sandbox handles concurrency and failure gracefully by caching results, retrying failed calls, and using redundant providers as fallbacks (e.g., a backup search API). This provides the agent with a fast and deterministic experience, which is crucial for preventing tool errors from corrupting its learning trajectory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Data Curation: Data is the core driver of model capability enhancement; its importance even surpasses that of the algorithm. The quality of the data directly determines the upper bound on the model’s ability to generalize to out‑of‑distribution scenarios through self‑exploration. To address this challenge, we optimize data in real time, guided by training dynamics. This optimization is achieved through a fully automated data synthesis and filtering pipeline that dynamically adjusts the training set. By closing the loop between data generation and model training, this approach not only ensures training stability but also delivers substantial performance gains.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;On‑Policy Asynchronous Framework: We implemented a custom step‑level asynchronous RL training loop on top of rLLM. Multiple agent instances interact with the (simulated or real) environment in parallel, each producing trajectories.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these measures, we “closed the loop” on agent training. Starting from a raw model, we did Agentic pre‑training to initialize tool‑use skills, then supervised finetuning on expert‑like data to cold start, and finally on‑policy RL to let the model conduct self‑evolution. This full‑stack approach ‑ now proven with Tongyi DeepResearch ‑ presents a new paradigm for training AI agents that can robustly solve complex tasks in dynamic environments.&lt;/p&gt;
    &lt;p&gt;(Our RL approach is inspired by several past work from Agentica. We adapt their rLLM framework and extend it to train our web agents.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Real‑World Applications and Impact&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch is not just a research showcase; it’s already powering real applications within Alibaba and beyond, demonstrating its value in practical scenarios:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gaode Mate (Map &amp;amp; Navigation Agent): Collaborating with Amap (Gaode) Team, we co‑developed “Xiao Gao,” an AI copilot that leverages the app’s rich toolset. It can execute complex travel planning commands, like creating a multi‑day driving tour that includes specific scenic spots and pet‑friendly hotels. Through multi‑step reasoning, Xiao Gao autonomously researches and integrates information to produce a detailed, personalized itinerary, offering an intelligent planning experience that far surpasses standard navigation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tongyi FaRui (Legal Research Agent): Empowered by our DeepResearch architecture, FaRui now functions as a true legal agent. It autonomously executes complex, multi‑step research tasks that mirror a junior attorney’s workflow‑systematically retrieving case law, cross‑referencing statutes, and synthesizing analysis. Crucially, all conclusions are grounded in verifiable judicial sources and delivered with precise case and statute citations, ensuring professional‑grade accuracy and credibility.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Our future work will address three key limitations. First, the current 128k context length is still insufficient for the most complex long‑horizon tasks, requiring us to explore expanded context windows and more sophisticated information management. Second, our training pipeline’s scalability remains unproven on foundation models significantly larger than our 30B‑scale MoE, and we plan to validate our methods on larger‑scale models. Lastly, we aim to improve the efficiency of our reinforcement learning framework by investigating techniques like partial rollouts, which will necessitate solving the challenges of off‑policy training, such as distributional shift.&lt;/p&gt;
    &lt;head rend="h2"&gt;Series Work&lt;/head&gt;
    &lt;p&gt;Tongyi DeepResearch also has an extensive deep research agent family. You can find more information in the following papers:&lt;/p&gt;
    &lt;p&gt;[1] WebWalker: Benchmarking LLMs in Web Traversal&lt;/p&gt;
    &lt;p&gt;[2] WebDancer: Towards Autonomous Information Seeking Agency&lt;/p&gt;
    &lt;p&gt;[3] WebSailor: Navigating Super‑human Reasoning for Web Agent&lt;/p&gt;
    &lt;p&gt;[4] WebShaper: Agentically Data Synthesizing via Information‑Seeking Formalization&lt;/p&gt;
    &lt;p&gt;[5] WebWatcher: Breaking New Frontier of Vision‑Language Deep Research Agent&lt;/p&gt;
    &lt;p&gt;[6] WebResearch: Unleashing reasoning capability in Long‑Horizon Agents&lt;/p&gt;
    &lt;p&gt;[7] ReSum: Unlocking Long‑Horizon Search Intelligence via Context Summarization&lt;/p&gt;
    &lt;p&gt;[8] WebWeaver: Structuring Web‑Scale Evidence with Dynamic Outlines for Open‑Ended Deep Research&lt;/p&gt;
    &lt;p&gt;[10] Scaling Agents via Continual Pre‑training&lt;/p&gt;
    &lt;p&gt;[11] Towards General Agentic Intelligence via Environment Scaling&lt;/p&gt;
    &lt;p&gt;Our team has a long‑standing commitment to the research and development of deep research agents. Over the past six months, we have consistently published one technical report per month, totaling five to date. Today, we are excited to simultaneously release six new reports and share our Tongyi DeepResearch‑30B‑A3B model with the community.&lt;/p&gt;
    &lt;p&gt;Stay tuned for our next generation of agentic models.&lt;/p&gt;
    &lt;code&gt;@misc{tongyidr,
  author={Tongyi DeepResearch Team},
  title={Tongyi DeepResearch: A New Era of Open-Source AI Researchers},
  year={2025},
  howpublished={\url{https://github.com/Alibaba-NLP/DeepResearch}}
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45789602</guid><pubDate>Sun, 02 Nov 2025 11:43:26 +0000</pubDate></item><item><title>X.org Security Advisory: multiple security issues X.Org X server and Xwayland</title><link>https://lists.x.org/archives/xorg-announce/2025-October/003635.html</link><description>&lt;doc fingerprint="981a9199600a0298"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;X.Org Security Advisory: multiple security issues X.Org X server and Xwayland&lt;/head&gt; Olivier Fourdan ofourdan at redhat.com &lt;lb/&gt;Tue Oct 28 13:22:18 UTC 2025&lt;quote&gt;====================================================================== X.Org Security Advisory: October 28, 2025 Issues in X.Org X server prior to 21.1.18 and Xwayland prior to 24.1.8 ====================================================================== Multiple issues have been found in the X server and Xwayland implementations published by X.Org for which we are releasing security fixes for in xorg-server-21.1.19 and xwayland-24.1.9. 1) CVE-2025-62229: Use-after-free in XPresentNotify structures creation Using the X11 Present extension, when processing and adding the notifications after presenting a pixmap, if an error occurs, a dangling pointer may be left in the error code path of the function causing a use-after-free when eventually destroying the notification structures later. Introduced in: Xorg 1.15 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/5a4286b1 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 2) CVE-2025-62230: Use-after-free in Xkb client resource removal When removing the Xkb resources for a client, the function XkbRemoveResourceClient() will free the XkbInterest data associated with the device, but not the resource associated with it. As a result, when the client terminates, the resource delete function triggers a use-after-free. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/99790a2c https://gitlab.freedesktop.org/xorg/xserver/-/commit/10c94238 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. 3) CVE-2025-62231: Value overflow in Xkb extension XkbSetCompatMap() The XkbCompatMap structure stores some of its values using an unsigned short, but fails to check whether the sum of the input data might overflow the maximum unsigned short value. Introduced in: X11R6 Fixed in: xorg-server-21.1.19 and xwayland-24.1.9 Fix: https://gitlab.freedesktop.org/xorg/xserver/-/commit/475d9f49 Found by: Jan-Niklas Sohn working with Trend Micro Zero Day Initiative. ------------------------------------------------------------------------ X.Org thanks all of those who reported and fixed these issues, and those who helped with the review and release of this advisory and these fixes. -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_0x14706DBE1E4B4540.asc Type: application/pgp-keys Size: 2988 bytes Desc: OpenPGP public key URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.key&amp;gt; -------------- next part -------------- A non-text attachment was scrubbed... Name: OpenPGP_signature.asc Type: application/pgp-signature Size: 203 bytes Desc: OpenPGP digital signature URL: &amp;lt;https://lists.x.org/archives/xorg-announce/attachments/20251028/ff11c77e/attachment.sig&amp;gt; &lt;/quote&gt;&lt;lb/&gt;More information about the xorg-announce
mailing list&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790015</guid><pubDate>Sun, 02 Nov 2025 13:07:31 +0000</pubDate></item><item><title>Writing FreeDOS Programs in C</title><link>https://www.freedos.org/books/cprogramming/</link><description>&lt;doc fingerprint="3ceffed571b967d9"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;This project was backed by Patreon supporters&lt;/head&gt;
    &lt;p&gt;This web programming guide started out as a video series on YouTube, supported through Patreon. Patrons at the "C programming" level and above (Patreon) got access to these extras:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Early access to the "C programming" videos&lt;/item&gt;
      &lt;item&gt;Exclusive access to the rest of the "programming guide" with more detail and information that didn't make it into the videos&lt;/item&gt;
      &lt;item&gt;A weekly Patreon forum to ask questions about that week's "C programming" topics (if you were following along with the videos and need help, this was the place to ask)&lt;/item&gt;
      &lt;item&gt;After the video series was finished, I edited the programming guide into a "teach yourself programming" book, via publishing partner Lulu. Patrons could purchase the book at cost.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790293</guid><pubDate>Sun, 02 Nov 2025 13:43:08 +0000</pubDate></item><item><title>Show HN: Anki-LLM – Bulk process and generate Anki flashcards with LLMs</title><link>https://github.com/raine/anki-llm</link><description>&lt;doc fingerprint="9c78ccdf08b67ab0"&gt;
  &lt;main&gt;
    &lt;p&gt;A CLI toolkit for bulk-processing and generating Anki flashcards with LLMs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bulk-verify translations – End-to-end pipeline for cleaning large decks. Read more&lt;/item&gt;
      &lt;item&gt;Add a Key Vocabulary field – Create a per-note field highlighting 1–3 key words with readings, meanings, and HTML context. Read more&lt;/item&gt;
      &lt;item&gt;Generate new cards – Interactively create multiple contextual flashcards for a vocabulary word or concept from a single command. Read more&lt;/item&gt;
      &lt;item&gt;Scriptable collection access – Query AnkiConnect directly from the CLI or AI agents. Command reference&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Manually editing hundreds or thousands of Anki cards is tedious, error-prone, and time-consuming. Whether it's verifying translations, adding grammar notes, or generating contextual examples, doing it by hand doesn't scale.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;anki-llm&lt;/code&gt; provides a bridge between your Anki collection and modern AI models.&lt;/p&gt;
    &lt;p&gt;Batch processing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File-based: Export deck to file, process with LLM, import results back to Anki.&lt;/item&gt;
      &lt;item&gt;Direct: Process and update notes in-place.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Card generation&lt;/p&gt;
    &lt;p&gt;Generate multiple contextual flashcard examples for a term, review interactively, and add selected cards to your deck.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Batch processing workflows: File-based (with resume) or direct-to-Anki (one command).&lt;/item&gt;
      &lt;item&gt;Export Anki decks to clean CSV or YAML files.&lt;/item&gt;
      &lt;item&gt;Batch process note fields using OpenAI or Google Gemini models.&lt;/item&gt;
      &lt;item&gt;Custom prompts: Use flexible template files to define exactly how the LLM should process your cards.&lt;/item&gt;
      &lt;item&gt;Concurrent processing: Make multiple parallel API requests to speed up large jobs.&lt;/item&gt;
      &lt;item&gt;Resilient: Automatically retries failed requests and saves progress incrementally (file mode).&lt;/item&gt;
      &lt;item&gt;Automatic resume: Pick up where you left off if processing is interrupted (file mode).&lt;/item&gt;
      &lt;item&gt;Copy mode: Alternatively, generate cards without API keys by pasting LLM responses from browser interfaces (ChatGPT, Claude, etc.).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install globally via npm:&lt;/p&gt;
    &lt;code&gt;npm install -g anki-llm&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Node.js (v18 or higher)&lt;/item&gt;
      &lt;item&gt;Anki Desktop must be running.&lt;/item&gt;
      &lt;item&gt;The AnkiConnect add-on must be installed in Anki.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;anki-llm&lt;/code&gt; uses LLM APIs to process your notes. You need to configure an API key
for the model provider you want to use.&lt;/p&gt;
    &lt;p&gt;The tool supports two API providers:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-4.1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$2.50/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-4o&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$2.50/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-4o-mini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.15/M&lt;/cell&gt;
        &lt;cell&gt;$0.60/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$1.25/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5-mini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.25/M&lt;/cell&gt;
        &lt;cell&gt;$2.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gpt-5-nano&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.05/M&lt;/cell&gt;
        &lt;cell&gt;$0.40/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Google Gemini models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.0-flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.10/M&lt;/cell&gt;
        &lt;cell&gt;$0.40/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.5-flash&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.30/M&lt;/cell&gt;
        &lt;cell&gt;$2.50/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.5-flash-lite&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$0.10/M&lt;/cell&gt;
        &lt;cell&gt;$0.40/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;gemini-2.5-pro&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;$1.25/M&lt;/cell&gt;
        &lt;cell&gt;$10.00/M&lt;/cell&gt;
        &lt;cell&gt;🔗&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pricing is per million tokens (M). Check the latest prices on the provider's website to be sure.&lt;/p&gt;
    &lt;p&gt;Set the appropriate environment variable for your chosen model provider:&lt;/p&gt;
    &lt;p&gt;For OpenAI models:&lt;/p&gt;
    &lt;code&gt;export OPENAI_API_KEY="your-api-key-here"&lt;/code&gt;
    &lt;p&gt;Get your API key from: https://platform.openai.com/api-keys&lt;/p&gt;
    &lt;p&gt;For Gemini models:&lt;/p&gt;
    &lt;code&gt;export GEMINI_API_KEY="your-api-key-here"&lt;/code&gt;
    &lt;p&gt;Get your API key from: https://aistudio.google.com/api-keys&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;anki-llm config&lt;/code&gt; to store defaults (for example, the model) so you don't
have to repeat flags on every command.&lt;/p&gt;
    &lt;code&gt;# Set or override defaults
anki-llm config set model gpt-4o-mini&lt;/code&gt;
    &lt;p&gt;Config file lives at &lt;code&gt;~/.config/anki-llm/config.json&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;export&lt;/code&gt;- Export deck to file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;import&lt;/code&gt;- Import data to deck&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;process-file&lt;/code&gt;- Process notes from file with AI&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;process-deck&lt;/code&gt;- Process notes from deck with AI&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;generate-init&lt;/code&gt;- Create prompt template for generate&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;generate&lt;/code&gt;- Generate new cards for a term&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;query&lt;/code&gt;- Query AnkiConnect API&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Exports notes from an Anki deck.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;deck&amp;gt;&lt;/code&gt;: The name of the Anki deck to export (must be in quotes if it contains spaces).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[output]&lt;/code&gt;: Optional output file path. If omitted, automatically generates a filename from the deck name (e.g.,&lt;code&gt;"My Deck"&lt;/code&gt;→&lt;code&gt;my-deck.yaml&lt;/code&gt;). You can also provide just a file extension (e.g.,&lt;code&gt;.csv&lt;/code&gt;) to auto-generate the filename with your preferred format.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Auto-generate filename with default .yaml format
anki-llm export "Japanese Core 1k"
# → japanese-core-1k.yaml

# Auto-generate filename with .csv format
anki-llm export "Japanese Core 1k" .csv
# → japanese-core-1k.csv

# Specify custom filename
anki-llm export "Japanese Core 1k" my-custom-name.yaml&lt;/code&gt;
    &lt;p&gt;Imports data from a file into an Anki deck. Existing notes (matched by key field) are updated, while new entries create new notes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;: Path to the data file to import (CSV or YAML).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-d, --deck&lt;/code&gt;: The name of the target Anki deck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-n, --note-type&lt;/code&gt;: The Anki note type to use when creating new notes. If not specified, it will be inferred from existing notes in the deck.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-k, --key-field&lt;/code&gt;: Field to use for identifying existing notes. If not specified, auto-detects using this priority: (1)&lt;code&gt;noteId&lt;/code&gt;column if present, (2) first field of the note type, (3) error if neither found.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Batch-process notes from a CSV/YAML file using an LLM and user-defined prompts. This command saves the transformed results to an output file and features automatic resume, allowing it to safely skip completed notes if interrupted or re-run.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;: Input file path (CSV or YAML).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-o, --output&lt;/code&gt;: Output file path (CSV or YAML).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-p, --prompt&lt;/code&gt;: Path to the prompt template text file.&lt;/item&gt;
      &lt;item&gt;Either &lt;code&gt;--field&lt;/code&gt;or&lt;code&gt;--json&lt;/code&gt;(mutually exclusive):&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;--field &amp;lt;name&amp;gt;&lt;/code&gt;: Update a single field with the AI response.&lt;/item&gt;&lt;item&gt;&lt;code&gt;--json&lt;/code&gt;: Expect JSON response and merge all fields into the note.&lt;/item&gt;&lt;item&gt;See Understanding &lt;code&gt;--field&lt;/code&gt;vs&lt;code&gt;--json&lt;/code&gt;modes for more details.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: AI model to use (required unless set via&lt;code&gt;config set model&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-b, --batch-size&lt;/code&gt;: Number of concurrent API requests (default:&lt;code&gt;5&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r, --retries&lt;/code&gt;: Number of retries for failed requests (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --dry-run&lt;/code&gt;: Preview the operation without making API calls (recommended for testing).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-f, --force&lt;/code&gt;: Re-process all rows, ignoring existing output.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--limit&lt;/code&gt;: Limit the number of new rows to process (useful for testing prompts on a small sample before processing large datasets).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Only extracts content from within&lt;code&gt;&amp;lt;result&amp;gt;&amp;lt;/result&amp;gt;&lt;/code&gt;tags in the AI response.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--log&lt;/code&gt;: Generate a log file with detailed debug information.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--very-verbose&lt;/code&gt;: Log full LLM responses to the log file (automatically enables&lt;code&gt;--log&lt;/code&gt;). Useful for debugging prompts and understanding model outputs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Export deck to file: &lt;code&gt;anki-llm export "My Deck" notes.yaml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Process file: &lt;code&gt;anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt -m gpt-4o-mini&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Import results: &lt;code&gt;anki-llm import output.yaml -d "My Deck"&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Process a file and update a single field
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt -m gpt-4o-mini

# Process with JSON mode (update multiple fields)
anki-llm process-file notes.yaml -o output.yaml --json -p prompt.txt -m gpt-4o-mini

# Test on 10 notes first (dry run)
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt --limit 10 --dry-run -m gpt-4o-mini

# Resume processing after interruption (automatic - just re-run the same command)
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt -m gpt-4o-mini

# Force re-process all notes (ignore existing output)
anki-llm process-file notes.yaml -o output.yaml --field Translation -p prompt.txt --force -m gpt-4o-mini&lt;/code&gt;
    &lt;p&gt;Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Automatic resume: Skips already-processed notes&lt;/item&gt;
      &lt;item&gt;✅ Incremental saves: Progress saved continuously&lt;/item&gt;
      &lt;item&gt;✅ Review before import: You can inspect/edit the output file before importing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use this command:&lt;/p&gt;
    &lt;p&gt;This command provides a file-based workflow for batch processing notes. It is the primary alternative to the &lt;code&gt;process-deck&lt;/code&gt; command, which modifies notes directly in your Anki collection.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;process-file&lt;/code&gt; instead of &lt;code&gt;process-deck&lt;/code&gt; when you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Require a manual review step. The command outputs to a file, creating a safe staging area to inspect results before you commit them to your Anki deck.&lt;/item&gt;
      &lt;item&gt;Need to process a large number of notes where interruptions are possible. Its resume capability ensures you don't lose progress if the process fails midway.&lt;/item&gt;
      &lt;item&gt;Are operating in an environment without a running Anki instance. This command is fully self-contained and does not need to connect to the Anki application.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Batch-process notes directly from an Anki deck using an LLM and user-defined prompts, updating them in-place. No intermediate files needed. This is faster and more convenient when you've tested your prompt and know the end result is safe to run.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;deck&amp;gt;&lt;/code&gt;: Name of the Anki deck to process (must be in quotes if it contains spaces).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-p, --prompt&lt;/code&gt;: Path to the prompt template text file.&lt;/item&gt;
      &lt;item&gt;Either &lt;code&gt;--field&lt;/code&gt;or&lt;code&gt;--json&lt;/code&gt;(mutually exclusive):&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;--field &amp;lt;name&amp;gt;&lt;/code&gt;: Update a single field with the AI response.&lt;/item&gt;&lt;item&gt;&lt;code&gt;--json&lt;/code&gt;: Expect JSON response and merge all fields into the note.&lt;/item&gt;&lt;item&gt;See Understanding &lt;code&gt;--field&lt;/code&gt;vs&lt;code&gt;--json&lt;/code&gt;modes for more details.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: AI model to use (required unless set via&lt;code&gt;config set model&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-b, --batch-size&lt;/code&gt;: Number of concurrent API requests (default:&lt;code&gt;5&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r, --retries&lt;/code&gt;: Number of retries for failed requests (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --dry-run&lt;/code&gt;: Preview the operation without making API calls (recommended for testing).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--limit&lt;/code&gt;: Limit the number of notes to process (useful for testing prompts on a small sample before processing entire deck).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Only extracts content from within&lt;code&gt;&amp;lt;result&amp;gt;&amp;lt;/result&amp;gt;&lt;/code&gt;tags in the AI response.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--log&lt;/code&gt;: Generate a log file with detailed debug information.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--very-verbose&lt;/code&gt;: Log full LLM responses to the log file (automatically enables&lt;code&gt;--log&lt;/code&gt;). Useful for debugging prompts and understanding model outputs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Prerequisites:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Anki Desktop must be running&lt;/item&gt;
      &lt;item&gt;AnkiConnect add-on must be installed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single command: &lt;code&gt;anki-llm process-deck "My Deck" --field Translation -p prompt.txt -m gpt-4o-mini&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Process a deck directly and update a single field
anki-llm process-deck "Japanese Core 1k" --field Translation -p prompt.txt

# Direct mode with JSON (update multiple fields)
anki-llm process-deck "Vocabulary" --json -p prompt.txt

# Test on 10 notes first (recommended before processing entire deck)
anki-llm process-deck "My Deck" --field Notes -p prompt.txt --limit 10 --dry-run

# Use a different model for a specific run
anki-llm process-deck "Spanish" --field Translation -p prompt.txt&lt;/code&gt;
    &lt;p&gt;Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ No intermediate files: Process and update in one step&lt;/item&gt;
      &lt;item&gt;✅ Batch updates: Efficient bulk updates to Anki&lt;/item&gt;
      &lt;item&gt;✅ Error logging: Failed notes logged to &lt;code&gt;[deck-name]-errors.jsonl&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;❌ No resume support: Must complete in one run (use &lt;code&gt;process-file&lt;/code&gt;for large datasets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both &lt;code&gt;process-file&lt;/code&gt; and &lt;code&gt;process-deck&lt;/code&gt; support two response formats for the LLM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--field&lt;/code&gt;mode (single field update): The LLM response is saved to the specified field.&lt;quote&gt;anki-llm process-file notes.yaml -o out.yaml -p prompt.txt --field Translation&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--json&lt;/code&gt;mode (multi-field merge): The LLM must return valid JSON. All fields in the JSON are merged into your note.&lt;quote&gt;anki-llm process-file notes.yaml -o out.yaml -p prompt.txt --json&lt;/quote&gt;&lt;p&gt;Example: If your note has&lt;/p&gt;&lt;code&gt;Japanese&lt;/code&gt;and&lt;code&gt;Grammar&lt;/code&gt;fields, and the LLM returns:&lt;quote&gt;{ "Japanese": "こんにちは", "Grammar": "greeting" }&lt;/quote&gt;&lt;p&gt;Both fields will be updated. Only fields present in the JSON are updated (partial updates are allowed). If the response is not valid JSON, the operation will fail and retry.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Interactively creates a prompt template file for the &lt;code&gt;generate&lt;/code&gt; command. The
wizard guides you through selecting a deck and note type, then uses an LLM to
analyze your existing cards and generate a tailored prompt that matches your
deck's style and formatting. This is the recommended way to get started with
card generation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[output]&lt;/code&gt;: Optional output file path. If omitted, automatically generates a filename from the deck name.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: The LLM model to use for the smart prompt generation step (e.g.,&lt;code&gt;gemini-2.5-pro&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t, --temperature&lt;/code&gt;: Temperature for LLM generation (0.0-2.0, default varies by model). Lower values produce more consistent output.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--copy&lt;/code&gt;: Copy the LLM prompt to clipboard and wait for manual response pasting. Useful when you don't have API access and want to use a browser LLM interface like ChatGPT.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Using a more capable reasoning model like &lt;code&gt;gemini-2.5-pro&lt;/code&gt; for the
&lt;code&gt;generate-init&lt;/code&gt; step can produce higher-quality prompt templates that better
capture the nuances and style of your existing cards.&lt;/p&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the wizard: &lt;code&gt;anki-llm generate-init&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Follow the interactive steps to select a deck and note type.&lt;/item&gt;
      &lt;item&gt;A prompt file (e.g., &lt;code&gt;my-deck-prompt.md&lt;/code&gt;) is created for you.&lt;/item&gt;
      &lt;item&gt;Review and customize the generated prompt file.&lt;/item&gt;
      &lt;item&gt;Use the file with the &lt;code&gt;generate&lt;/code&gt;command:&lt;code&gt;anki-llm generate "term" -p my-deck-prompt.md&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generates multiple new Anki card examples for a given term, lets you review and select which ones to keep, and adds them directly to your deck.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;term&amp;gt;&lt;/code&gt;: The word or phrase to generate cards for (must be in quotes if it contains spaces).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-p, --prompt&lt;/code&gt;: Path to the prompt template file (created with&lt;code&gt;generate-init&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-c, --count&lt;/code&gt;: Number of card examples to generate (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-m, --model&lt;/code&gt;: AI model to use (defaults to&lt;code&gt;gpt-5-mini&lt;/code&gt;or&lt;code&gt;gemini-2.5-flash&lt;/code&gt;depending on your API key; can also be set via&lt;code&gt;config set model&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --dry-run&lt;/code&gt;: Display generated cards without starting the interactive selection or import process.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-r, --retries&lt;/code&gt;: Number of retries for failed requests (default:&lt;code&gt;3&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t, --temperature&lt;/code&gt;: LLM temperature, a value between 0 and 2 that controls creativity (default:&lt;code&gt;1.0&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--max-tokens&lt;/code&gt;: Set a maximum number of tokens for the LLM response.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-o, --output&lt;/code&gt;: Export cards to a file instead of importing to Anki (e.g.,&lt;code&gt;cards.yaml&lt;/code&gt;,&lt;code&gt;cards.csv&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--log&lt;/code&gt;: Enable logging of LLM responses to a file (useful for debugging).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--copy&lt;/code&gt;: Copy the LLM prompt to clipboard and wait for manual response pasting. Useful when you don't have API access and want to use a browser LLM interface like ChatGPT.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;--prompt&lt;/code&gt; file is a text or markdown file that contains two parts: YAML
frontmatter for configuration and a prompt body with instructions for the LLM.&lt;/p&gt;
    &lt;p&gt;Frontmatter (Required)&lt;/p&gt;
    &lt;p&gt;The frontmatter is a YAML block at the top of the file enclosed by &lt;code&gt;---&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;deck&lt;/code&gt;: The target Anki deck name.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;noteType&lt;/code&gt;: The name of the Anki note type (model) to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fieldMap&lt;/code&gt;: Maps the keys from the LLM's JSON output to your actual Anki field names. The LLM will be instructed to generate JSON with the keys on the left, and&lt;code&gt;anki-llm&lt;/code&gt;will use them to populate the Anki fields on the right.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Prompt Body&lt;/p&gt;
    &lt;p&gt;The body contains your instructions for the LLM. It must:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Include the &lt;code&gt;{term}&lt;/code&gt;placeholder, which will be replaced by the&lt;code&gt;&amp;lt;term&amp;gt;&lt;/code&gt;you provide on the command line.&lt;/item&gt;
      &lt;item&gt;Include the &lt;code&gt;{count}&lt;/code&gt;placeholder, which will be replaced by the number of cards requested.&lt;/item&gt;
      &lt;item&gt;Instruct the LLM to return a JSON array of objects, where each object represents one card and uses the keys defined in &lt;code&gt;fieldMap&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Include a "one-shot" example showing the exact JSON array structure and desired formatting (e.g., HTML for bolding or lists).&lt;/item&gt;
      &lt;item&gt;Encourage the LLM to generate diverse cards that highlight different nuances, contexts, or usage examples of the term.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example Prompt File (&lt;code&gt;japanese-vocab-prompt.md&lt;/code&gt;)&lt;/p&gt;
    &lt;code&gt;---
deck: Japanese::Vocabulary
noteType: Japanese (recognition)
fieldMap:
  en: English
  jp: Japanese
  context: Context
---

You are an expert assistant who creates {count} distinct Anki flashcards for a
Japanese vocabulary word. The term to create cards for is: **{term}**

IMPORTANT: Your output must be a single, valid JSON array of objects and nothing
else. Each object in the array should represent a unique flashcard. All field
values must be strings.

Follow the structure shown in this example precisely:

```json
[
  {
    "en": "How was your day?",
    "jp": "今日はどうでしたか？",
    "context": "A natural and common way to ask about someone's day politely. You can say 「今日どうだった？」 in casual speech."
  }
]
```

Return only a valid JSON array matching this structure. Ensure you generate
{count} varied and high-quality cards that highlight different nuances, contexts,
or usage examples of the term.&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;--copy&lt;/code&gt; flag allows you to generate cards without API keys by manually
copying prompts to a browser-based LLM interface (like ChatGPT, Claude, Gemini,
etc.) and pasting responses back.&lt;/p&gt;
    &lt;p&gt;Workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run the command with &lt;code&gt;--copy&lt;/code&gt;:&lt;code&gt;anki-llm generate "今日" -p prompt.md --copy&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;The program automatically copies the LLM prompt to your clipboard.&lt;/item&gt;
      &lt;item&gt;Paste the prompt into your preferred LLM interface (ChatGPT, Claude, etc.).&lt;/item&gt;
      &lt;item&gt;Copy the complete JSON response from the LLM.&lt;/item&gt;
      &lt;item&gt;Paste it into the terminal.&lt;/item&gt;
      &lt;item&gt;Type &lt;code&gt;END&lt;/code&gt;on a new line and press Enter to submit.&lt;/item&gt;
      &lt;item&gt;The program validates and processes your cards normally.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No API key required&lt;/item&gt;
      &lt;item&gt;Use any LLM interface you prefer&lt;/item&gt;
      &lt;item&gt;Works with free-tier LLM services&lt;/item&gt;
      &lt;item&gt;Full control over the LLM interaction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Generate 3 cards for a term using a prompt file
anki-llm generate "新しい" -p japanese-vocab-prompt.md

# Generate 5 cards and preview them without importing
anki-llm generate "ambiguous" -p english-vocab-prompt.md --count 5 --dry-run

# Use a different model for a specific run
anki-llm generate "maison" -p french-prompt.md -m gemini-2.5-pro

# Generate cards and export to YAML for later review/import
anki-llm generate "今日" -p japanese-vocab-prompt.md -o cards.yaml

# Import the exported cards when ready
anki-llm import cards.yaml --deck "Japanese::Vocabulary"

# Enable logging for debugging
anki-llm generate "新しい" -p prompt.md --log

# Use manual copy-paste mode (no API key required)
anki-llm generate "今日" -p japanese-vocab-prompt.md --copy&lt;/code&gt;
    &lt;p&gt;Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Interactive selection: Review and choose which generated cards to keep.&lt;/item&gt;
      &lt;item&gt;✅ Duplicate detection: Automatically flags cards that may already exist in your deck.&lt;/item&gt;
      &lt;item&gt;✅ Export option: Save generated cards to YAML/CSV for review before importing.&lt;/item&gt;
      &lt;item&gt;✅ Highly customizable: Full control over card generation via the prompt file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Query the AnkiConnect API directly with any supported action. This command is especially useful for AI agents (like Claude Code) to explore and interact with your Anki collection programmatically.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;&amp;lt;action&amp;gt;&lt;/code&gt;: The AnkiConnect API action to perform (e.g.,&lt;code&gt;deckNames&lt;/code&gt;,&lt;code&gt;findNotes&lt;/code&gt;,&lt;code&gt;cardsInfo&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[params]&lt;/code&gt;: Optional JSON string of parameters for the action.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this is useful for AI agents:&lt;/p&gt;
    &lt;p&gt;AI assistants can use this command to dynamically query your Anki collection without you having to manually provide information. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"List all my decks" → &lt;code&gt;anki-llm query deckNames&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;"Show me statistics for my Japanese deck" → &lt;code&gt;anki-llm query getDeckStats '{"decks":["Japanese"]}'&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;"Find all cards with tag 'vocabulary'" → &lt;code&gt;anki-llm query findNotes '{"query":"tag:vocabulary"}'&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The command outputs clean JSON that AI agents can parse and reason about, making it easy to build custom workflows or answer questions about your Anki collection.&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# Get all deck names
anki-llm query deckNames

# Get all model (note type) names
anki-llm query modelNames

# Find notes in a specific deck
anki-llm query findNotes '{"query":"deck:Japanese"}'

# Get detailed information about specific cards
anki-llm query cardsInfo '{"cards":[1498938915662]}'

# Get statistics for a deck
anki-llm query getDeckStats '{"decks":["Default"]}'

# Check AnkiConnect version
anki-llm query version

# Get full AnkiConnect API documentation (useful for AI agents to understand available actions)
anki-llm query docs&lt;/code&gt;
    &lt;p&gt;Example: Sampling random cards from decks&lt;/p&gt;
    &lt;p&gt;AI agents can use &lt;code&gt;anki-llm query&lt;/code&gt; to discover information about your
collection and then take action. Here's an example of Claude Code using the
&lt;code&gt;query&lt;/code&gt; command to sample random cards from multiple decks. Given the
instruction: "Use anki-llm to pick random cards from Glossika decks, and print
the English and Japanese fields for each, pick 10 cards from each deck, and
save to a markdown file"&lt;/p&gt;
    &lt;p&gt;This demonstrates how the &lt;code&gt;query&lt;/code&gt; command enables AI agents to build custom
scripts for data analysis and extraction tasks autonomously.&lt;/p&gt;
    &lt;p&gt;Special actions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;docs&lt;/code&gt;or&lt;code&gt;help&lt;/code&gt;: Returns the complete AnkiConnect API documentation. This is especially useful for AI agents that need to understand what actions are available and how to use them. The agent can query this once to get the full documentation and then use that context to make informed decisions about which API calls to make.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See ANKI_CONNECT.md for the complete list of available actions and their parameters.&lt;/p&gt;
    &lt;p&gt;Let's say you have an Anki deck named "Japanese Core 1k" with 1000 notes. Each note has a &lt;code&gt;Japanese&lt;/code&gt; field with a sentence and a &lt;code&gt;Translation&lt;/code&gt; field with an
English translation that you suspect is inaccurate. We'll use &lt;code&gt;anki-llm&lt;/code&gt; and
GPT-4o mini to generate better translations for all 1000 notes.&lt;/p&gt;
    &lt;p&gt;First, export the notes from your Anki deck into a YAML file. YAML is great for multiline text fields and for using &lt;code&gt;git diff&lt;/code&gt; to see what has changed after
processing is complete.&lt;/p&gt;
    &lt;code&gt;anki-llm export "Japanese Core 1k" notes.yaml&lt;/code&gt;
    &lt;p&gt;This command will connect to Anki, find all notes in that deck, and save them to a YAML file.&lt;/p&gt;
    &lt;code&gt;============================================================
Exporting deck: Japanese Core 1k
============================================================

✓ Found 1000 notes in 'Japanese Core 1k'.

Discovering model type and fields...
✓ Model type: Japanese Model
✓ Fields: Japanese, Translation, Reading, Sound, noteId

Fetching note details...
✓ Retrieved information for 1000 notes.

Writing to notes.yaml...
✓ Successfully exported 1000 notes to notes.yaml
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;notes.yaml&lt;/code&gt; file will look something like this:&lt;/p&gt;
    &lt;code&gt;- noteId: 1512345678901
  Japanese: 猫は机の上にいます。
  Translation: The cat is on the desk.
- noteId: 1512345678902
  Japanese: 彼は毎日公園を散歩します。
  Translation: He strolls in the park every day.
# ... 998 more notes&lt;/code&gt;
    &lt;p&gt;Next, create a prompt file (&lt;code&gt;prompt-ja-en.txt&lt;/code&gt;) to instruct the AI. Use
&lt;code&gt;{field_name}&lt;/code&gt; syntax for variables that will be replaced with data from each
note. We want to process the &lt;code&gt;Japanese&lt;/code&gt; field.&lt;/p&gt;
    &lt;p&gt;File: &lt;code&gt;prompt-ja-en.txt&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;You are an expert Japanese-to-English translator.

Translate this Japanese sentence to English: {Japanese}

Guidelines:
- Translate accurately while preserving nuance and meaning.
- Be natural and idiomatic in English.
- If possible, structure the English so the original Japanese grammar can be inferred.

Instructions:
1. First, analyze the sentence structure and key elements.
2. Think through the translation choices and any nuances.
3. Provide your final translation wrapped in &amp;lt;result&amp;gt;&amp;lt;/result&amp;gt; XML tags.

Format your response like this:
- Analysis: [your analysis of the sentence]
- Translation considerations: [your thought process]
- &amp;lt;result&amp;gt;[your final English translation here]&amp;lt;/result&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;&amp;lt;result&amp;gt;&lt;/code&gt; tag (used with &lt;code&gt;--require-result-tag&lt;/code&gt;) is optional. You could instruct the LLM to respond with only the translation directly. However, asking the model to "think out loud" by analyzing the sentence first tends to produce higher-quality translations, as it encourages deeper reasoning before generating the final output.&lt;/p&gt;
    &lt;p&gt;Now, run the &lt;code&gt;process-file&lt;/code&gt; command. We'll tell it to use our &lt;code&gt;notes.yaml&lt;/code&gt; file
as input, write to a new &lt;code&gt;notes-translated.yaml&lt;/code&gt; file, process the &lt;code&gt;Translation&lt;/code&gt;
field, and use our prompt template.&lt;/p&gt;
    &lt;p&gt;The tool will read the &lt;code&gt;Japanese&lt;/code&gt; field from each note to fill the prompt, then
the AI's response will overwrite the &lt;code&gt;Translation&lt;/code&gt; field.&lt;/p&gt;
    &lt;code&gt;anki-llm process-file notes.yaml \
  --output notes-translated.yaml \
  --field Translation \
  --prompt prompt-ja-en.txt \
  --model gemini-2.5-flash \
  --batch-size 10 \
  --require-result-tag&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;notes.yaml&lt;/code&gt;: The input file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--output notes-translated.yaml&lt;/code&gt;: The output file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--field Translation&lt;/code&gt;: The field we want the AI to generate and place its result into.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--prompt prompt-ja-en.txt&lt;/code&gt;: Our instruction template.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--model gemini-2.5-flash&lt;/code&gt;: The AI model to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--batch-size 10&lt;/code&gt;: Process 10 notes concurrently for speed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Ensures the tool only saves the content inside the&lt;code&gt;&amp;lt;result&amp;gt;&lt;/code&gt;tag, ignoring the AI's analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You will see real-time progress as it processes the notes:&lt;/p&gt;
    &lt;code&gt;============================================================
File-Based Processing
============================================================
Input file:        notes.yaml
Output file:       notes-translated.yaml
Field to process:  Translation
Model:             gpt-4o-mini
Batch size:        10
...
============================================================

Reading notes.yaml...
✓ Found 1000 rows in YAML

Loading existing output...
✓ Found 0 already-processed rows

Processing 1000 rows...
Processing |████████████████████████████████████████| 100% | 1000/1000 rows | Cost: $0.0234 | Tokens: 152340

✓ Processing complete

============================================================
Summary
============================================================
- Successes:         1000
- Failures:          0
- Total Processed:   1000
- Total Time:        85.32s
- Model:             gpt-4o-mini
- Dry Run:           false
---
- Total Tokens:      152,340
- Input Tokens:      120,100
- Output Tokens:     32,240
- Est. Cost:         $0.02
============================================================
&lt;/code&gt;
    &lt;p&gt;The final step is to import the newly generated translations back into Anki. The tool uses the &lt;code&gt;noteId&lt;/code&gt; to find and update the existing notes.&lt;/p&gt;
    &lt;code&gt;anki-llm import notes-translated.yaml --deck "Japanese Core 1k"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;notes-translated.yaml&lt;/code&gt;: The file with our improved translations.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--deck "Japanese Core 1k"&lt;/code&gt;: The destination deck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model type will be automatically inferred from the existing notes in the deck. You can also explicitly specify it with &lt;code&gt;--model "Japanese Model"&lt;/code&gt; if
needed.&lt;/p&gt;
    &lt;code&gt;============================================================
Importing from notes-translated.yaml to deck: Japanese Core 1k
Model: Japanese Model
Key field: noteId
============================================================

✓ Found 1000 rows in notes-translated.yaml.

✓ Valid fields to import: Japanese, Translation, Reading, Sound

✓ Found 1000 existing notes with a 'noteId' field.

✓ Partitioning complete:
  - 0 new notes to add.
  - 1000 existing notes to update.

Updating 1000 existing notes...
✓ Update operation complete: 1000 notes updated successfully.

Import process finished.
&lt;/code&gt;
    &lt;p&gt;That's it! All 1000 notes in your Anki deck have now been updated with high-quality translations.&lt;/p&gt;
    &lt;p&gt;Sentence flashcards often benefit from a focused vocabulary breakdown. You can use &lt;code&gt;anki-llm&lt;/code&gt; to populate a dedicated &lt;code&gt;Key Vocabulary&lt;/code&gt; field with structured
HTML that spotlights the most important words in each sentence.&lt;/p&gt;
    &lt;p&gt;Create a prompt that instructs the model to reason about the sentence, pick the top 1–3 items, and return clean HTML. This example assumes your notes have &lt;code&gt;Japanese&lt;/code&gt; and &lt;code&gt;English&lt;/code&gt; fields. You can start from the full prompt example in
&lt;code&gt;examples/key_vocabulary.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;File: &lt;code&gt;prompt-key-vocab.txt&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;You are an expert Japanese vocabulary AI assistant designed for language learners. Your primary role is to analyze Japanese sentences, identify the most significant vocabulary words, and produce clear, concise, and educational explanations formatted in clean, semantic HTML.

The user is an intermediate learner who uses sentence flashcards to practice. Your output will populate a "Key Vocabulary" field on their Anki flashcard. The HTML you generate must be well-structured to allow for easy and flexible styling with CSS.

English: {English}
Japanese: {Japanese}

Analysis: Explain which vocabulary items you chose and why they matter for an intermediate learner.
Always produce between 1 and 3 key vocabulary entries using the following HTML structure (use dictionary form in the heading and include the kana reading in parentheses):

&amp;lt;h3&amp;gt;WORD (reading)&amp;lt;/h3&amp;gt;
&amp;lt;dl class="vocab-entry"&amp;gt;
  &amp;lt;dt&amp;gt;Type&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;Part of speech&amp;lt;/dd&amp;gt;

  &amp;lt;dt&amp;gt;Meaning&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;Concise English definition&amp;lt;/dd&amp;gt;

  &amp;lt;dt&amp;gt;Context&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;Sentence-specific explanation, including any conjugation or nuance notes.&amp;lt;/dd&amp;gt;
&amp;lt;/dl&amp;gt;

Replace the placeholder content with the actual vocabulary analysis. Within the `&amp;lt;result&amp;gt;` tags, output only the completed HTML entries with no additional commentary.

&amp;lt;result&amp;gt;
&amp;lt;/result&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Process your exported notes and overwrite the &lt;code&gt;Key Vocabulary&lt;/code&gt; field with the
HTML generated by the prompt:&lt;/p&gt;
    &lt;code&gt;anki-llm process-file sentences.yaml \
  --output sentences-key-vocab.yaml \
  --field "Key Vocabulary" \
  --prompt prompt-key-vocab.txt \
  --model gemini-2.5-flash-lite \
  --require-result-tag&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--field "Key Vocabulary"&lt;/code&gt;: Updates that specific field on each note.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--require-result-tag&lt;/code&gt;: Keeps only the HTML between&lt;code&gt;&amp;lt;result&amp;gt;&lt;/code&gt;tags and drops the analysis from the prompt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you open the processed YAML/CSV, the generated field will look like this:&lt;/p&gt;
    &lt;code&gt;Key Vocabulary: |
  &amp;lt;h3&amp;gt;控える (ひかえる)&amp;lt;/h3&amp;gt;
  &amp;lt;dl class="vocab-entry"&amp;gt;
    &amp;lt;dt&amp;gt;Type&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Ichidan verb&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Meaning&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;To refrain; to hold back&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Context&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Appears as 控えていて, the te-form plus いる to show an ongoing act of self-restraint in the scene.&amp;lt;/dd&amp;gt;
  &amp;lt;/dl&amp;gt;

  &amp;lt;h3&amp;gt;さっぱり (さっぱり)&amp;lt;/h3&amp;gt;
  &amp;lt;dl class="vocab-entry"&amp;gt;
    &amp;lt;dt&amp;gt;Type&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Adverb&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Meaning&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Completely; entirely (with a nuance of 'not at all' when paired with negatives)&amp;lt;/dd&amp;gt;

    &amp;lt;dt&amp;gt;Context&amp;lt;/dt&amp;gt;
    &amp;lt;dd&amp;gt;Modifies わからない to emphasize that the speaker has absolutely no understanding.&amp;lt;/dd&amp;gt;
  &amp;lt;/dl&amp;gt;&lt;/code&gt;
    &lt;p&gt;After verifying the results, import the updated file back into Anki to add the structured vocabulary explanations to your cards.&lt;/p&gt;
    &lt;p&gt;Let's create several new example flashcards for the Japanese word &lt;code&gt;会議&lt;/code&gt;
(meeting) and add them to our "Japanese::Vocabulary" deck.&lt;/p&gt;
    &lt;p&gt;First, run the &lt;code&gt;generate-init&lt;/code&gt; wizard. It will ask you to select your deck and
note type, then use an LLM to analyze your existing cards and generate a prompt
file tailored to your collection.&lt;/p&gt;
    &lt;code&gt;anki-llm generate-init&lt;/code&gt;
    &lt;p&gt;Follow the interactive prompts. The wizard will use an AI model (defaults to &lt;code&gt;gpt-5&lt;/code&gt; or &lt;code&gt;gemini-2.5-flash&lt;/code&gt; depending on your API key) to analyze existing
cards in your deck and create a smart prompt that matches their style and
formatting. When it's done, it will save a new file, for example
&lt;code&gt;japanese-vocabulary-prompt.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The generated file will look something like this:&lt;/p&gt;
    &lt;p&gt;File: &lt;code&gt;japanese-vocabulary-prompt.md&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;---
deck: Japanese::Vocabulary
noteType: Japanese (recognition)
fieldMap:
  en: English
  jp: Japanese
  context: Context
---

You are an expert Japanese language assistant. Your goal is to create {count}
distinct, high-quality, contextual Anki flashcards for a vocabulary term. The
term to create cards for is: **{term}**

IMPORTANT: Your output must be a single, valid JSON array of objects and nothing
else. Each object in the array should represent a unique flashcard. All field
values must be strings. Use `&amp;lt;b&amp;gt;` tags to highlight the term in the example
sentence.

Follow the structure shown in this example precisely:

```json
[
  {
    "en": "The &amp;lt;b&amp;gt;meeting&amp;lt;/b&amp;gt; is scheduled for 3 PM.",
    "jp": "&amp;lt;b&amp;gt;会議&amp;lt;/b&amp;gt;は午後3時に予定されています。",
    "context": "Used in a formal business context."
  }
]
```

Return only a valid JSON array matching this structure. Ensure you generate
{count} varied cards that highlight different nuances, contexts, or usage
examples of the term.&lt;/code&gt;
    &lt;p&gt;You can now edit this file to further refine the instructions for the AI.&lt;/p&gt;
    &lt;p&gt;Now, use the &lt;code&gt;generate&lt;/code&gt; command with your new prompt file to create card
examples for &lt;code&gt;会議&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;anki-llm generate "会議" -p japanese-vocabulary-prompt.md -m gemini-2.5-flash&lt;/code&gt;
    &lt;p&gt;The tool will make a single API call asking the LLM to generate 3 distinct cards and show progress:&lt;/p&gt;
    &lt;code&gt;🔄 Generating 3 card candidates for "会議"...
✓ Generation complete: 3 succeeded, 0 failed
  Cost: $0.0027 (930 input + 954 output tokens)

🔍 Checking for duplicates...
&lt;/code&gt;
    &lt;p&gt;After generation, an interactive checklist appears in your terminal. You can use the arrow keys and spacebar to select the cards you want to add to Anki.&lt;/p&gt;
    &lt;code&gt;📋 Select cards to add to Anki (use Space to select, Enter to confirm):

❯ ◯ Card 1
│     English: The *meeting* was very productive.
│     Japanese: その*会議*は非常に生産的でした。
│     Context: General business context.
│
│ ◯ Card 2
│     English: I have a *meeting* with a client tomorrow.
│     Japanese: 明日、クライアントとの*会議*があります。
│     Context: A common phrase for scheduling.
│
│ ◯ Card 3 (⚠️  Duplicate)
│     English: The *meeting* is scheduled for 3 PM.
│     Japanese: *会議*は午後3時に予定されています。
│     Context: Used in a formal business context.
&lt;/code&gt;
    &lt;p&gt;Here, we see three options. Card 3 has been flagged as a potential duplicate because a similar card already exists in the deck. Let's select the first two cards and press Enter.&lt;/p&gt;
    &lt;p&gt;The selected cards are immediately added to your Anki deck.&lt;/p&gt;
    &lt;code&gt;📥 Adding 2 card(s) to Anki...

✓ Successfully added 2 new note(s) to "Japanese::Vocabulary"
&lt;/code&gt;
    &lt;p&gt;That's it! You have successfully generated, reviewed, and imported multiple high-quality, contextual Anki cards from a single command.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;tsx&lt;/code&gt; to run the CLI directly from TypeScript source without rebuilding:&lt;/p&gt;
    &lt;code&gt;pnpm tsx src/cli.ts export "My Deck" notes.yaml&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;pnpm link&lt;/code&gt; to test the command globally:&lt;/p&gt;
    &lt;code&gt;pnpm link --global
anki-llm export "My Deck" notes.yaml&lt;/code&gt;
    &lt;p&gt;Note: The linked command uses compiled JavaScript from &lt;code&gt;dist/&lt;/code&gt;. Run
&lt;code&gt;pnpm run build&lt;/code&gt; after making changes to see them reflected.&lt;/p&gt;
    &lt;p&gt;To unlink: &lt;code&gt;pnpm unlink --global&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;pnpm run check&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790443</guid><pubDate>Sun, 02 Nov 2025 14:04:07 +0000</pubDate></item><item><title>Why don't you use dependent types?</title><link>https://lawrencecpaulson.github.io//2025/11/02/Why-not-dependent.html</link><description>&lt;doc fingerprint="8548021f0f0bb7f0"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;"Why don't you use dependent types?"&lt;/head&gt;[&lt;code&gt;&lt;nobr&gt;memories&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;AUTOMATH&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;LCF&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;Martin-Löf type theory&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;NG de Bruijn&lt;/nobr&gt;&lt;/code&gt; 
  
    
    &lt;code&gt;&lt;nobr&gt;ALEXANDRIA&lt;/nobr&gt;&lt;/code&gt; 
  
]

&lt;p&gt;To be fair, nobody asks me this exact question. But people have regularly asked why Isabelle dispenses with proof objects. The two questions are essentially the same, because proof objects are intrinsic to all the usual type theories. They are also completely unnecessary and a huge waste of space. As described in an earlier post, type checking in the implementation language (rather than in the logic) can ensure that only legitimate proof steps are executed. Robin Milner had this fundamental insight 50 years ago, giving us the LCF architecture with its proof kernel. But the best answer to the original question is simply this: I did use dependent types, for years.&lt;/p&gt;&lt;head rend="h3"&gt;My time with AUTOMATH&lt;/head&gt;&lt;p&gt;I was lucky enough to get some personal time with N G de Bruijn when he came to Caltech in 1977 to lecture about AUTOMATH. I never actually got to use this system. Back then, researchers used the nascent Internet (the ARPAnet) not to download software so much as to run software directly on the host computer, since most software was not portable. But Eindhoven University was not on the ARPAnet, and AUTOMATH was configured to run on a computer we did not have:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Until September 1973, the computer was the Electrologica X8, after that Burroughs 6700. In both cases the available multiprogranming systems required the use of ALGOL 60.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I did however read many of the research reports, including the PhD dissertation by LS Jutting, where he presents his translation of Landau’s text Grundlagen der Analysis (described last time) from German into AUTOMATH. It is no coincidence that many of my papers, from the earliest to the latest, copied the idea of formalising a text and attempting to be faithful to it, if possible line by line.&lt;/p&gt;&lt;p&gt;As an aside, note that while AUTOMATH was a system of dependent types, it did not embody the Curry–Howard correspondence (sometimes wrongly called the Curry–Howard–de Bruijn correspondence). That correspondence involves having a type theory strong enough to represent the predicate calculus directly in the form of types. In AUTOMATH you had to introduce the symbols and inference rules of your desired calculus in the form of axioms, much as you do with Isabelle. In short, AUTOMATH was a logical framework:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;like a big restaurant that serves all sorts of food: vegetarian, kosher, or anything else the customer wants&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;De Bruijn did not approve of the increasingly powerful type theories being developed in the 1990s. AUTOMATH was a weak language, a form of λ-calculus including a general product construction just powerful enough to express the inference rules of a variety of formalisms and to make simple definitions, again clearly the inspiration for Isabelle. Isabelle aims to be generic, like the big AUTOMATH restaurant. Only these days everybody prefers the same cuisine, higher-order logic, so Isabelle/HOL has become dominant. Unfortunately, I last spoke to Dick (as he was known to friends) when I was putting all my effort into Isabelle/ZF. He simply loathed set theory and saw mathematics as essentially typed. He never lived to see the enormous amount of advanced mathematics that would be formalised using types in Isabelle/HOL.&lt;/p&gt;&lt;p&gt;I annoyed him in another way. I kept asking, AUTOMATH looks natural, but how do we know that it is right? He eventually sent me a 300 page volume entitled The Language Theory of Automath. It describes AUTOMATH’s formal properties such as strong normalisation and Church–Rosser properties, but this was not the answer I wanted at all. I got that answer for a quite different type theory.&lt;/p&gt;&lt;head rend="h3"&gt;Martin-Löf type theory&lt;/head&gt;&lt;p&gt;In response to kind invitations from Bengt Nordström and Kent Petersson, I paid a number of visits to Chalmers University in Gothenburg to learn about Martin-Löf type theory. I was particularly impressed by its promise of a systematic and formal approach to program synthesis. I had already encountered intuitionism through a course on the philosophy of mathematics at Stanford University, as I recall taught by Ian Hacking. The “rightness” of Martin-Löf type theory was obvious, because it directly embodied the principles of intuition truth as outlined by Heyting: for example, that a proof of $A\land B$ consists of a proof of $A$ paired with a proof of $B$.&lt;/p&gt;&lt;p&gt;I devoted several years of research to Martin-Löf type theory. This included a whole year of intricate hand derivations to produce a paper that I once thought would be important, and the very first version of Isabelle. Yes: Isabelle began as an implementation of Martin-Löf type theory, which is still included in the distribution even today as Isabelle/CTT. But eventually I tired of what seemed to me a doctrinaire attitude bordering on a cult of personality around Per Martin-Löf. The sudden switch to intensional equality (everyone was expected to adopt the new approach) wrecked most of my work. Screw that.&lt;/p&gt;&lt;p&gt;You might ask, what about the calculus of constructions, which arose during that time and eventually gave us Rocq and Lean? (Not to mention LEGO.) To me they raised, and continue to raise, the same question I had put to de Bruijn. Gérard Huet said something like “it is nothing but function application”, which did not convince me. It’s clear that I am being fussy,1 because thousands of people find these formalisms perfectly natural and believable. But it is also true that the calculus of constructions underwent numerous changes over the past four decades. There seem to be several optional axioms that people sometimes adopt while attempting to minimise their use, like dieters enjoying an occasional croissant.&lt;/p&gt;&lt;head rend="h3"&gt;Decisions, decisions&lt;/head&gt;&lt;p&gt;We can see all this as an example of the choices we make in research. People were developing new formalisms. This specific fact was the impetus for making Isabelle generic in the first place. But we have to choose whether to spend our time developing formalisms or instead to choose a fixed formalism and see how far you can push it. Both are legitimate research goals.&lt;/p&gt;&lt;p&gt;For example, already in 1985, Mike Gordon was using higher-order logic to verify hardware. He was not distracted by the idea that some dependent type theory might work better for n-bit words and the like. The formalism that he implemented was essentially the same as the simple theory of types outlined by Alonzo Church in 1940. He made verification history using this venerable formalism, and John Harrison later found a clever way to encode the dimension of vector types including words. Isabelle/HOL also implements Church’s simple type theory, with one extension: axiomatic type classes. Isabella users also derive much power from the locale concept, a kind of module sysstem that lies outside any particular logic.&lt;/p&gt;&lt;p&gt;During all this time, both Martin-Löf type theory and the calculus of constructions went through several stages of evolution. It’s remarkable how the Lean community, by running with a certain version of the calculus, quickly formalised a vast amount of mathematics.&lt;/p&gt;&lt;head rend="h3"&gt;Pushing higher-order logic to its limit&lt;/head&gt;&lt;p&gt;I felt exceptionally lucky to win funding from the European Research Council for the advanced grant ALEXANDRIA. When I applied, homotopy type theory was still all the rage, so the proposal emphasised Isabelle’s specific advantages: its automation, its huge libraries and the legibility of its proofs.&lt;/p&gt;&lt;p&gt;The team started work with enthusiasm. Nevertheless, I fully expected that we would hit a wall, reaching mathematical material that could not easily be formalised in higher-order logic. Too much of Isabelle’s analysis library identified topological spaces with types. Isabelle’s abstract algebra library was old and crufty. A number of my research colleagues were convinced that higher-logic was not adequate for serious mathematics. But Anthony Bordg took up the challenge, leading a subproject to formalise Grothendieck schemes.&lt;/p&gt;&lt;p&gt;For some reason I had a particular fear of the field extension $F[a]$, which extends the field $F$ with some $a$ postulated to be a root of some polynomial over $F$. (For example, the field of complex numbers is precisely $\mathbb{R}[i]$, where $i$ is postulated to be a root a root of $x^2+1=0$.) And yet an early outcome of ALEXANDRIA was a proof, by Paulo Emílio de Vilhena and Martin Baillon, that every field admits an algebraically closed extension. This was the first proof of that theorem in any proof assistant, and its proof involves an infinite series of field extensions.&lt;/p&gt;&lt;p&gt;We never hit any wall. As our group went on to formalise more and more advanced results, such as the Balog–Szemerédi–Gowers theorem, people stopped saying “you can’t formalise mathematics without dependent types” and switched to saying “dependent types give you nicer proofs”. But they never proved this claim.&lt;/p&gt;&lt;p&gt;Now that dependent type theory has attained maturity and has an excellent tool in the form of Lean, shall I go back to dependent types? I am not tempted. The only aspects of Lean that I envy are its huge community and the Blueprint tool. I hear too many complaints about Lean’s performance. I’ve heard of too many cases where dependent types played badly with intensional equality – I sat through an entire talk on this topic – or otherwise made life difficult. Quite a few people have told me that the secret of dependent types is knowing when not to use them. And so, to me, they have too much in common with Tesla’s Full Self-Driving.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Especially as regards constructive mathematics. To its founders, intuitionism is a philosophy suspicious of language, which it relegates to the purpose of recording and communicating mathematical thoughts. This is the opposite of today’s “constructive mathematics”, which refers the use of a formalism satisfying certain syntactic properties. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790827</guid><pubDate>Sun, 02 Nov 2025 15:06:36 +0000</pubDate></item><item><title>New South Korean national law will turn large parking lots into solar farms</title><link>https://electrek.co/2025/11/02/new-national-law-will-turn-large-parking-lots-into-solar-power-farms/</link><description>&lt;doc fingerprint="2003f15ebfb7b5ba"&gt;
  &lt;main&gt;
    &lt;p&gt;Starting this month, parking lots in South Korea with more than 80 spaces will be required to install solar canopies and carports. But, unlike similar laws that have been proposed in the US, this new law doesn’t just apply to new construction – existing lots will have to comply as well!&lt;/p&gt;
    &lt;p&gt;South Korea’s Ministry of Trade, Industry and Energy announced in August that it has prepared an amendment to the Enforcement Decree of the Act on the Promotion of the Development, Use, and Diffusion of New and Renewable Energy to the effect that all publicly- and privately-owned parking lots in the Asian country with room for more than 80 vehicles will be compelled to add solar panels to their lots in a move designed to proactively expand renewable energy and create more solar and construction jobs.&lt;/p&gt;
    &lt;p&gt;In addition to creating jobs and working to stabilize the local grid with more renewable energy, the proposed solar canopies will offer a number of practical, day-to-day benefits for Korean drivers, as well.&lt;/p&gt;
    &lt;p&gt;The shaded structures will protect vehicles from heavy rain, snow, and the blistering summer sun — keeping interiors cooler, extending the life of plastics and upholstery, and even helping to preserve battery range in EVs and PHEVs by reducing their AC loads (and, of course, provide charging while the cars are parked).&lt;/p&gt;
    &lt;p&gt;To their credit, Ministry officials absolutely get it. “Through this mandatory installation,” one unnamed official told Asia Business Daily, “we expect to expand the distribution of eco-friendly renewable energy generation facilities while providing tangible benefits to the public. By utilizing idle land such as parking lots, we can maximize land use efficiency. In addition, installing canopy-type solar panels can provide shade underneath, offering noticeable comfort to people using parking lots during hot weather.”&lt;/p&gt;
    &lt;p&gt;The new rule was approved in late September, and is expected to go into effect later this month, with new installation projects set to begin immediately.&lt;/p&gt;
    &lt;head rend="h2"&gt;It could work here&lt;/head&gt;
    &lt;p&gt;South Korea is proving that an idea like is practical. Here in the US, we’re proving that out, too – the Northwest Fire District in Arizona partnered with Standard Solar to build a conceptually similar, 657 kW solar carport system across 12 parking lots (shown, above) that delivers more than 1.23 million kWh of clean, emissions-free power annually and offsets the equivalent of 185,000 vehicles’ worth of harmful carbon emissions.&lt;/p&gt;
    &lt;p&gt;That’s just Arizona. In New York, a new initiative to help expand solar into parking lots has more than doubled commercially zoned land where EV charging stations can be sited, “freeing up” an additional 400 million square feet of space throughout the city.&lt;/p&gt;
    &lt;p&gt;Sun-rich states like Texas, New Mexico, and Florida could also benefit, and even if we’re “just” adding fresh energy sources to municipal parking, dealer lots, and public schools, we could do a lot to reduce the cost of energy generation for the entire community. And, for what it’s worth, that seems to be right in line with the big reasons why people are choosing to add solar to their homes today.&lt;/p&gt;
    &lt;p&gt;What do you guys think – would something like this work in the US, or are we too far gone down the sophomoric, pseudo-libertarian rabbit hole to ever dig our way out? Let us know your take in the comments.&lt;/p&gt;
    &lt;p&gt;SOURCE | IMAGES: Asia Business Daily, via LinkedIn; Standard Solar.&lt;/p&gt;
    &lt;p&gt;If you’re considering going solar, it’s always a good idea to get quotes from a few installers. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out EnergySage, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it’s free to use, and you won’t get sales calls until you select an installer and share your phone number with them.&lt;/p&gt;
    &lt;p&gt;Your personalized solar quotes are easy to compare online and you’ll get access to unbiased Energy Advisors to help you every step of the way. Get started here.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45790867</guid><pubDate>Sun, 02 Nov 2025 15:12:08 +0000</pubDate></item><item><title>At the end you use Git bisect</title><link>https://kevin3010.github.io/git/2025/11/02/At-the-end-you-use-git-bisect.html</link><description>&lt;doc fingerprint="96385cbb607c7f9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;At the end you use `git bisect`&lt;/head&gt;
    &lt;p&gt;People rant about having to learn algorithmic questions for interviews. I get it — interview system is broken, but you ought to learn binary search at least.&lt;/p&gt;
    &lt;p&gt;Anyways, yet again I came across a real life application of Algorithms. This time in the OG tool &lt;code&gt;git&lt;/code&gt;. &lt;code&gt;git bisect - Use binary search to find the commit that introduced a bug&lt;/code&gt; ref. And Leetcode wanted you to know it First Bad Version&lt;/p&gt;
    &lt;p&gt;We use a monorepo at work. And people tend to make hundreds, if not thousands, commit in a single repo a day. On this day, our tests started failing, and the logs weren’t enough to debug or trace the root cause. The failing method depended on a configuration file that made a remote call using a specific role to obtain a token for running the tests. At some point, that configuration had been changed — a string was updated to reference a different account — which caused the failure.&lt;/p&gt;
    &lt;p&gt;Somehow, the bad change slipped through integration tests unnoticed. It was difficult to manually find the exact file or commit that introduced the issue since many commits had been made across the repository over the past few days.&lt;/p&gt;
    &lt;p&gt;That’s when a teammate from another team — who was facing the same test failures — ran a few “magical” commands and quickly identified the exact commit where things started to break. The basic idea was simple but brilliant: pick a known good commit and a known bad one, then run a binary search to find the exact commit that caused the failure.&lt;/p&gt;
    &lt;p&gt;It took a while since each test run was time-consuming, but eventually, it pinpointed the precise commit that introduced the issue. And sure enough, after reverting that commit, everything went back to green.&lt;/p&gt;
    &lt;p&gt;Here’s a small demo repository that shows how &lt;code&gt;git bisect&lt;/code&gt; finds the first bad commit.&lt;/p&gt;
    &lt;p&gt;File tree:&lt;/p&gt;
    &lt;code&gt;git-bisect-demo/
├── calc.py           # the library under test
├── test_calc.py      # a pytest test for calc.add
└── test_script.sh    # wrapper used by `git bisect run`
&lt;/code&gt;
    &lt;p&gt;Good version of &lt;code&gt;calc.py&lt;/code&gt; (commit where tests pass):&lt;/p&gt;
    &lt;code&gt;def add(a, b):
    return a + b

if __name__ == "__main__":
    print(add(2, 3))
&lt;/code&gt;
    &lt;p&gt;Bad version of &lt;code&gt;calc.py&lt;/code&gt; (commit that introduced the bug):&lt;/p&gt;
    &lt;code&gt;def add(a, b):
    # accidental string concatenation when inputs are coerced to str
    return str(a) + str(b)

if __name__ == "__main__":
    print(add(2, 3))
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;test_calc.py&lt;/code&gt; (pytest):&lt;/p&gt;
    &lt;code&gt;import calc

def test_add():
    assert calc.add(2, 3) == 5, "Addition failed!"
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;test_script.sh&lt;/code&gt; — used by &lt;code&gt;git bisect run&lt;/code&gt; to return exit code 0 on success and non-zero on failure:&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env bash
set -e
pytest -q
&lt;/code&gt;
    &lt;p&gt;Example commit history (chronological):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Commit 1: Initial commit (good)&lt;/item&gt;
      &lt;item&gt;Commit 2: Small refactor (still good)&lt;/item&gt;
      &lt;item&gt;Commit 3: Bug introduced (bad)&lt;/item&gt;
      &lt;item&gt;Commits 4..10: Non-functional edits / comments (remain bad)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can run &lt;code&gt;git bisect&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;git bisect start
git bisect bad HEAD
git bisect good HEAD~9
git bisect run ./test_script.sh
&lt;/code&gt;
    &lt;code&gt;status: waiting for both good and bad commits
status: waiting for good commit(s), bad commit known
Bisecting: 4 revisions left to test after this (roughly 2 steps)
[8dad374fd7c097c4fa3521c0b259e1eefe533520] Commit 5: more changes
running  './test_script.sh'
Bisecting: 1 revision left to test after this (roughly 1 step)
[b982ed9373fe235fe61c74b15faf264bc7142398] Commit 3: introduced bug
running  './test_script.sh'
Bisecting: 0 revisions left to test after this (roughly 0 steps)
[7b59759ca785572797e04f6b313bb0b735c22529] Commit 2: minor refactor
running  './test_script.sh'
b982ed9373fe235fe61c74b15faf264bc7142398 is the first bad commit
commit b982ed9373fe235fe61c74b15faf264bc7142398
Author: Kevin
Date:   Sun Nov 2 10:54:47 2025 -0500

    Commit 3: introduced bug

 calc.py | 10 +---------
 1 file changed, 1 insertion(+), 9 deletions(-)
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;git bisect&lt;/code&gt; will checkout intermediate commits and run &lt;code&gt;./test_script.sh&lt;/code&gt; until it finds the first commit that makes the tests fail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45791882</guid><pubDate>Sun, 02 Nov 2025 17:24:39 +0000</pubDate></item><item><title>Anti-cybercrime laws are being weaponized to repress journalism</title><link>https://www.cjr.org/analysis/nigeria-pakistan-jordan-cybercrime-laws-journalism.php</link><description>&lt;doc fingerprint="1480d19f3aa57a8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign up for the daily CJR newsletter.&lt;/p&gt;
    &lt;p&gt;In May 2024, Daniel Ojukwu, a twenty-six-year-old reporter for the Foundation for Investigative Journalism, a Nigerian nonprofit, was grabbed off the streets of Lagos by armed police and bundled into a vehicle. For the next several days, he was held in a cell incommunicado—first in Lagos, and later in the federal capital, Abuja—without being told exactly what he’d been arrested for. “It was more of an abduction,” Ojukwu recalled recently, via WhatsApp. Finally, on the fourth day, the authorities informed him that he was being accused of breaching a 2015 law known as the Cybercrime Act. His violation: an article he wrote about alleged corruption in the office of the president.&lt;/p&gt;
    &lt;p&gt;The Cybercrime Act was introduced to combat a growing trend of internet fraud and other criminal activity within Nigeria, but it has instead frequently been used to suppress journalism published online. One provision in particular—Section 24, which made it illegal to publish false information online that was deemed to be “grossly offensive,” “indecent,” or even merely an “annoyance”—has been especially ripe for abuse. In 2019, for instance, Agba Jalingo, a journalist and publisher of CrossRiverWatch, in Nigeria’s Cross River State, was arrested and charged under Section 24 after he published articles accusing the state’s governor of corruption. (He was later acquitted.) In February 2024, Nigerian lawmakers amended Section 24 to remove some of its most egregious elements, but the new language still makes it illegal, and punishable by up to three years in jail, to “knowingly or intentionally” communicate online anything that is “false, for the purpose of causing a breakdown of law and order [or] posing a threat to life.”&lt;/p&gt;
    &lt;p&gt;“This vague text is still used to unfairly prosecute journalists, particularly those who regularly publish investigative reports implicating political or institutional forces,” said Sadibou Marong, the sub-Saharan Africa bureau director of Reporters Without Borders. “Authorities are intent on gagging investigative journalism uncovering corruption and governance issues in the country. The continued implementation of this law constitutes a real threat.”&lt;/p&gt;
    &lt;p&gt;Nigeria is not the only country using laws designed to legitimately combat online misbehavior to instead repress journalism. In neighboring Niger, Abdourahamane Tchiani, who seized power in a coup in 2023, signed an order amending three articles of the country’s cybercrime law to reinstate prison sentences for “defamation,” “insults,” and the “dissemination of data likely to disturb public order or undermine human dignity” when these offenses are committed electronically. The law, originally enacted in 2019, had previously been softened to remove prison sentences for such offenses, in part owing to how the law had been abused to repress journalists. In Pakistan, Georgia, and Turkey, among others, recent laws meant to limit nefarious activity online, or the spread of misinformation, have been used to restrict acts of journalism. According to Amnesty International, at least fifteen people in Jordan have been prosecuted under a 2023 expansion of the country’s Cybercrimes Law, for offenses ranging from “spreading fake news” to “threatening society peace.”&lt;/p&gt;
    &lt;p&gt;“Unfortunately, most of the laws being passed will have little effect in actually curbing misinformation, but instead may give governments far more authority to control content they deem false or misleading,” said Gabrielle Lim, a doctoral fellow at the Citizen Lab at the University of Toronto, who recently coauthored a paper tracking the misuse of “fake news” laws around the world. “For some governments, the threat of misinformation provides a convenient justification for censorship. This is compounded by the fact that liberal democracies are also considering or passing similar laws, which can give cover to authoritarian regimes who want to do the same.”&lt;/p&gt;
    &lt;p&gt;In Nigeria, more than two dozen journalists have faced prosecution under the Cybercrime Act, according to the Committee to Protect Journalists. In most cases, the journalists have been accused of cyberbullying, cyberstalking, or attempting to overthrow the government. On February 16, 2024—two weeks before the amended Cybercrime Act was signed into law—four journalists of The Informant247, an independent online newspaper based in Nigeria’s Kwara State, were arrested and briefly detained after they published a two-part investigative series that alleged a corrupt atmosphere at a state-run polytechnic institute. “The experience was profoundly disturbing,” said Salihu Ayatullahi, the publication’s editor in chief and one of the arrested journalists. “We were locked in a dark, cramped cell with hardened criminals. The psychological impact was heavier than the physical discomfort: I couldn’t sleep, not because of the poor conditions, but because I couldn’t stop thinking about how broken our system had become and how the corrupt could illegally summon the police to punish those who expose them.” The case was dismissed eleven months later without any evidence presented against the reporters.&lt;/p&gt;
    &lt;p&gt;Solomon Okedara, a Nigerian digital rights lawyer and researcher, notes that the use of the Cybercrime Act has created a chilling effect in the nation’s civic space. “It is even more worrisome that most of the time, the prosecution cannot establish ingredients of the offense to the point of conviction,” Okedara said. “Knowing a fellow journalist has faced arrest, harassment, and detention, or endless trials, can force others to drop an investigative story idea.”&lt;/p&gt;
    &lt;p&gt;Despite their ordeals, both Ojukwu and Ayatullahi say they are more determined than ever to use their craft to hold public officials accountable. “As a journalist, the whole experience has made me understand that there is more work to do,” Ojukwu said. “And since there is no limit to which the corrupt are willing to go, there is also none for me. The Cybercrime Act remains a thorn in the flesh of journalists in Nigeria.”&lt;/p&gt;
    &lt;p&gt;Has America ever needed a media defender more than now? Help us by joining CJR today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792209</guid><pubDate>Sun, 02 Nov 2025 18:12:49 +0000</pubDate></item><item><title>Lisp: Notes on its Past and Future (1980)</title><link>https://www-formal.stanford.edu/jmc/lisp20th/lisp20th.html</link><description>&lt;doc fingerprint="9449101996ab135c"&gt;
  &lt;main&gt;
    &lt;p&gt; John McCarthy &lt;lb/&gt; Computer Science Department &lt;lb/&gt; Stanford University &lt;lb/&gt; Stanford, CA 94305 &lt;lb/&gt; jmc@cs.stanford.edu &lt;lb/&gt; http://www-formal.stanford.edu/jmc/&lt;/p&gt;
    &lt;p&gt; JanFebMarAprMayJun JulAugSepOctNovDec , :&amp;lt; 10 0 &lt;/p&gt;
    &lt;p&gt;LISP has survived for 21 years because it is an approximate local optimum in the space of programming languages. However, it has accumulated some barnacles that should be scraped off, and some long-standing opportunities for improvement have been neglected. It would benefit from some co-operative maintenance especially in creating and maintaining program libraries. Computer checked proofs of program correctness are now possible for pure LISP and some extensions, but more theory and some smoothing of the language itself are required before we can take full advantage of LISP's mathematical basis.&lt;/p&gt;
    &lt;p&gt;1999 note: This article was included in the 1980 Lisp conference held at Stanford. Since it almost entirely corresponds to my present opinions, I should have asked to have it reprinted in the 1998 Lisp users conference proceedings at which I gave a talk with the same title.&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45792579</guid><pubDate>Sun, 02 Nov 2025 19:05:32 +0000</pubDate></item></channel></rss>