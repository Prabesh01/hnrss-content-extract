<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 22 Dec 2025 10:12:27 +0000</lastBuildDate><item><title>Depot (YC W23) Is Hiring an Enterprise Support Engineer (Remote/US)</title><link>https://www.ycombinator.com/companies/depot/jobs/jhGxVjO-enterprise-support-engineer</link><description>&lt;doc fingerprint="7c7cbd4bdf24670a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Depot is the fastest place to build software. Our platform integrates with services and tools like GitHub Actions, Docker, Bazel, Gradle, Turborepo, and more to make builds exponentially faster, saving our customers collective decades of build time every month.&lt;/p&gt;
        &lt;p&gt;To support our rapidly growing customer base, we are looking for an Enterprise Support Engineer working Pacific Time hours to expand our support team.&lt;/p&gt;
        &lt;head rend="h3"&gt;About the job&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;You will handle technical support requests for all Depot offerings and advise customers on best practices&lt;/item&gt;
          &lt;item&gt;You will become a customer-facing subject-matter expert on CI/CD optimization, Docker, and build tools across a variety of languages and technologies&lt;/item&gt;
          &lt;item&gt;You will identify product and resource gaps in customer contexts and work with the engineering team to prioritize and escalate them&lt;/item&gt;
          &lt;item&gt;You will assist customers migrating from legacy build infrastructure to take full advantage of Depot’s platform&lt;/item&gt;
          &lt;item&gt;You'll be joining our growing support team and helping to shape how support works at Depot&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;About you&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;You are interested in the CI/build space and are comfortable with a fair amount of context-switching. We assist customers with their GitHub Actions workflows, give recommendations for Dockerfile optimizations, debug performance regressions, and generally see a wide range of build and CI configurations&lt;/item&gt;
          &lt;item&gt;You are able to debug and troubleshoot complex issues, combining experience and intuition to find root causes quickly&lt;/item&gt;
          &lt;item&gt;You are comfortable in front of customers, able to assist but also empower them to become successful on their own&lt;/item&gt;
          &lt;item&gt;You can operate independently, but know when to flag for help&lt;/item&gt;
          &lt;item&gt;You know how to work under pressure in times of urgency. Blocked build pipelines can mean being unable to deploy critical fixes to production or failing deadlines, so we and our customers take issues seriously&lt;/item&gt;
          &lt;item&gt;You enjoy preventing issues from recurring through documentation work&lt;/item&gt;
          &lt;item&gt;You are available to work under a public holiday rotation basis to ensure continuous support coverage&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;Benefits&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Remote work with async-friendly culture&lt;/item&gt;
          &lt;item&gt;Compensation package with equity available&lt;/item&gt;
          &lt;item&gt;Health insurance/benefits&lt;/item&gt;
          &lt;item&gt;Unlimited PTO policy&lt;/item&gt;
          &lt;item&gt;Free access to Claude Code and Cursor&lt;/item&gt;
          &lt;item&gt;Yearly team offsites&lt;/item&gt;
          &lt;item&gt;Cool Depot swag&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;What you will need&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Able to work Pacific Time hours and participate in public holiday rotation&lt;/item&gt;
          &lt;item&gt;Technical experience with DevOps consulting or similar customer-facing role&lt;/item&gt;
          &lt;item&gt;Working knowledge of CI/CD platforms, such as GitHub Actions&lt;/item&gt;
          &lt;item&gt;Working knowledge of Docker and Dockerfile optimization&lt;/item&gt;
          &lt;item&gt;Working knowledge of major cloud platforms, such as AWS, Azure or GCP&lt;/item&gt;
          &lt;item&gt;Working knowledge of some common build tools and package managers, such as NPM, Maven, UV or Cargo&lt;/item&gt;
          &lt;item&gt;Strong ability to communicate directly with customers, primarily via email, Slack, or via calls when required&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h3"&gt;What would be helpful&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Experience with GitHub Actions optimization&lt;/item&gt;
          &lt;item&gt;Experience with BuildKit and/or advanced Dockerfile features (buildx, multi-stage builds)&lt;/item&gt;
          &lt;item&gt;API integration experience, to help our customers integrate our programmatic build API into their platforms&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;Depot is a build acceleration and developer productivity platform that saves companies like PostHog, Wistia, Semgrep, and Secoda thousands of hours in build time every week.&lt;/p&gt;
      &lt;p&gt;We are developers. We started Depot because we were frustrated with the constant pain of slow build performance. We were fed up waiting for builds and annoyed by the lack of tooling and providers that actually made builds performant. So, we went and built the solution we had always wanted.&lt;/p&gt;
      &lt;p&gt;Slow builds are the dam standing in the way between mediocrity and innovation. They’re wasteful, expensive, and a drain on developer happiness &amp;amp; productivity. They slow down innovation.&lt;/p&gt;
      &lt;p&gt;Taking a 40-minute build down to a minute, changes everything. We help folks save literal years in build time every single week.&lt;/p&gt;
      &lt;p&gt;And we’re just getting started. For us, it’s all about iteration speed and keeping developers in their flow state. Our mission is to be relentless in accelerating software development.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46338156</guid><pubDate>Sat, 20 Dec 2025 18:06:04 +0000</pubDate></item><item><title>Programming languages used for music</title><link>https://timthompson.com/plum/cgi/showlist.cgi?sort=name&amp;concise=yes</link><description>&lt;doc fingerprint="f4ab408dc8bb0e94"&gt;
  &lt;main&gt;
    &lt;p&gt;This notation language was originally designed for transcribing Irish folk tunes, but has since evolved into a considerably richer language allowing, for example, polymetric output on multiple staves. This music notation format has the advantage of being extremely concise and fairly readable.&lt;/p&gt;
    &lt;p&gt;The AC Toolbox is a Macintosh PPC application to assist the algorithmic composition of music. A legacy version for older, 68K-based computers is also available. Several models for defining musical events are included. They can be used by defining objects such as sections, shapes, masks, or note structures. It is also possible to play, plot, modify, and examine objects in a number of ways. Extensive online help is available. In addition to Midi input and output, the AC Toolbox can also produce text files suitable for use as data in other programs. In particular, score files for Csound, note list files for Common Lisp Music, and tables for MAX can be produced. An important method of creating data in the Toolbox is the use of generators. A number of generators have been included reflecting various approaches to the creation of musical material including tendency masks, stochastic functions, chaotic systems, transition tables, recursive subdivisions, metric indispensabilities, morphological mutations, etc. The AC Toolbox is implemented in Lisp and input syntax often reflects the conventions of this language. It is also possible for a user to extend the Toolbox by adding Lisp functions. For example, additional generators can be defined in Lisp to use with the Toolbox.&lt;/p&gt;
    &lt;p&gt;Alphanumeric Language for Music Analysis, implemented at the Institute for Computer Research in the Humanities (NYU). The idea was to incorporate more than Western staff notations. They implemented some crude translators, proof-listeners (analgous to proof-reading), etc.&lt;/p&gt;
    &lt;p&gt;AML consisted of an interpreter and a compiler. The compiler was written in and used a lobotomized version of the Digital Research MAC assembler. The compiler generated code that was read by the interpreter. The AML interpreter was written in Intel 8080 assembly language. The interpreter created up to 8 virtual machines that drove analog synthesizers using various D/A and A/D hardware. Each virtual machine consisted of a stack oriented computer that processed code specifically designed for generating music. The instruction set consisted various operators for manipulating the stacks, reading note lists, computing note lists in real-time and drawing pseudo random numbers including fractals. A version supporting MIDI was the final development, and this version ran on an Apple 2 computer equipped with an Intel 8080 board which talked to a Roland MPU-401 MIDI interface. A wide range of students and composers in the LA area used this system, many through the UCLA extension program. LA Composer Jeff Rona wrote several works for AML, and it was first demonstrated in Denton TX with his "Step Music" with dancers Sean Green and Dianna McNeil. There is an article in the ICMC proceedings for the Denton conference.&lt;/p&gt;
    &lt;p&gt;Forth-like stack-based language, in which letters a-g are the musical notes. A change to capitals indicates a change up an octave, to lowercase is down an octave (symbols &amp;lt; and &amp;gt; do this explicitly). Also is multitasking.&lt;/p&gt;
    &lt;p&gt;MARS is a development system for realtime Digital Signal Processing techniques, sound synthesis, filters and sound effects. Sound and MIDI environments can be developped which allow it to be used as a MIDI musical instrument. MARS is a system for audio research, musical production and the education of computer music, for people who like a programmable and flexible sound machine with realtime performance.&lt;/p&gt;
    &lt;p&gt;A lisp-like language that can manipulate MIDI data and do other sequencer-related operations (creating new tracks, etc.) within the Cakewalk sequencer. See http://www.cakewalk.com/devxchange/cal.asp for examples of CAL programs.&lt;/p&gt;
    &lt;p&gt;Common Lisp Music (CLM) is a sound synthesis package in the Music V family written primarily in Common Lisp. The instrument design language is a subset of Lisp, extended with a large number of generators: oscil, env, table-lookup, and so on. The run-time portion of an instrument can be compiled into C or Lisp code. Since CLM instruments are lisp functions, a CLM note list is just a lisp expression that happens to call those functions. Recent additions to CLM include support for real-time interactions and integration with the Snd sound editor.&lt;/p&gt;
    &lt;p&gt;Cmix is a package of routines for editing, processing, and creating soundfiles. It also includes a library of routines designed to make it easier to write c programs which deal with soundfiles. A version for Linux, called RTcmix, is maintained by Dave Topper.&lt;/p&gt;
    &lt;p&gt;Common Music Notation (CMN) is a music notation package written in Common Lisp, using CLOS and the Sonata font. It provides for all the usual needs of music notation in a fully customizable, programmable environment.&lt;/p&gt;
    &lt;p&gt;Common Music (CM) is an object-oriented music composition environment. It produces sound by transforming a high-level representation of musical structure into low-level control statements for a number of different synthesis targets: MIDI, CSound, Common Lisp Music (CLM), Music Kit, CMix, CMusic, RT, Mix and Common Music Notation (CMN). Common Music provides an extensive library of compositional objects and encourages the user to modify and extend the system through subclassing and specialization. Common Music is implemented in Common Lisp and runs on a variety of computers, including NeXT, Macintosh, SGI, SUN, and 386.&lt;/p&gt;
    &lt;p&gt;Environment with Stackbased language for music composition. Algorithmic composition, sequences. Multitasking. Outputs through custombuilt interface, for control of analogue synthesizer hardware.&lt;/p&gt;
    &lt;p&gt;Provides a C++ class library for representing music scores. Works with scores in ALMA, *kern, NIFF, and Esac. Provides a visualisation and analysis paradigm for music. Huge corpus available, with translators from practially all major encodings.&lt;/p&gt;
    &lt;p&gt;Csound is a popular and widely used software synthesis package in the tradition of so-called music-N languages, among which the best-known is Music V. It consists of an orchestra- and score-driven executable, written in C for portability. Basically Csound reads some files and creates the result as a soundfile on disk or, on faster machines, realtime through a DAC.&lt;/p&gt;
    &lt;p&gt;CYBIL is a compositional language for the efficient specification of arbitrarily complex Csound scores. It is integrated into CECILIA and can be used to generate scores for any kind of Csound orchestra. The syntax of CYBIL owes to Leland Smith's SCORE language and, to an extent, to Heinrich Taub's Common-Music. Scores are specified as parameter lines with the help of a number of data generators such as sequences, masks, lines and exponentials. These generators can be further modified by the use of functions such as random, urns and constrained random and parameters cross-referencing.&lt;/p&gt;
    &lt;p&gt;JSFX audio effects for Reaper are written in EEL2, a scripting language that is compiled on the fly and allows you to modify and/or generate audio and MIDI, as well as draw custom vector based UI and analysis displays. EEL2 is based on AVS's EEL. AVS is a programmable visualization plugin for Winamp&lt;/p&gt;
    &lt;p&gt;Elody is a music composition environment that proposes Lambda-abstraction on musical structures as a fundamental mechanism to represent user-defined programs&lt;/p&gt;
    &lt;p&gt;FAUST (Functional Audio Stream) is a functional programming language specifically designed for real-time signal processing and synthesis. A distinctive characteristic of FAUST is to be fully compiled. The FAUST compiler translates DSP specifications into very efficient C++ code that works at sample level. The generated code is self contained and doesn't depend on any library or runtime. Moreover a same FAUST specification can be used to generate native implementations for most OS (Linux, OSX, Android, iOS) or platforms (LV2, Ladspa, VST, PD, Csound, SC,..) Faust distribution can be downloaded at: http://sourceforge.net/projects/faudiostream The GIT repository can be cloned with the following command : git clone git://git.code.sf.net/p/faudiostream/code faust&lt;/p&gt;
    &lt;p&gt;The Foo environment consists of the Foo Kernel layer and the Foo Control layer. The Foo Kernel layer is implemented in Objective-C and is made accessible to Scheme through a set of types and primitives added to the Elk Scheme interpreter. The Foo Control layer is implemented in Scheme and OOPS, an object-oriented extension to Scheme. Whereas the Foo Kernel layer implements the generic sound synthesis and processing modules as well as a patch description and execution language, the Foo Control layer offers a symbolic interface to the kernel and implements musically salient control abstractions. The user interacts with the Foo environment by writing Scheme programs which eventually will define and execute synthesis patches in non-real-time.&lt;/p&gt;
    &lt;p&gt;FORMULA is a language/multitasking OS for the Atari ST and Mac. It is based on (and built on top of) Forth. The basic idea of FORMULA is to represent music as cooperating processes. For instance, each part of a symphony might be a different process. Also, the generation of pitches, durations, velocities, and tempo can similarly be controlled by separate processes.&lt;/p&gt;
    &lt;p&gt;Haskore is a collection of Haskell modules designed for expressing musical structures in the high-level, declarative style of functional programming. In Haskore, musical objects consist of primitive notions such as notes and rests, operations to transform musical objects such as transpose and tempo-scaling, and operations to combine musical objects to form more complex ones, such as concurrent and sequential composition. From these simple roots, much richer musical ideas can easily be developed.&lt;/p&gt;
    &lt;p&gt;HMSL (Hierarchical Music Specification Language) is an object-oriented software environment for experimental music composition, with an emphasis on real-time user-machine interaction.&lt;/p&gt;
    &lt;p&gt;Hyperlisp is a real-time MIDI programming environment embedded in Macintosh Common Lisp. The environment was developed specifically for the Hyperinstruments project at the MIT Media Laboratory, and is optimized for interactive systems which require fast response times. Hyperlisp provides two main services for the music programmer: routines for MIDI processing and primitives for scheduling the application of functions. Programs written in Macintosh Common Lisp can use these services for a wide variety of real-time MIDI applications.&lt;/p&gt;
    &lt;p&gt;A graphical editor for multidimensional and multi-formal scores, sort of poly-temporal meta-sequencer, based on the former UPIC created by Iannis Xenakis.&lt;/p&gt;
    &lt;p&gt;The Microsoft Interactive Music control is an ActiveX component used to create and play music dynamically on Web pages. It is used as an object within VBScript or JavaScript programs.&lt;/p&gt;
    &lt;p&gt;Interactor is a graphic authoring tool created by composers Mark Coniglio and Morton Subotnick that enables artists to realize realtime interactive performances based on MIDI.&lt;/p&gt;
    &lt;p&gt;JFugue is a set of Java classes for music programming. It uses simple strings to represent musical data, including notes, chords, and instrument changes. JFugue also allows you to define music using patterns, and you can do interesting transformations on those patterns to come up with new musical segments that are derived from existing pieces of music. JFugue can write MIDI files. The JFugue webpage is full of clear examples and instructions. JFugue makes music programming incredible easy!&lt;/p&gt;
    &lt;p&gt;Java Music Specification Language (JMSL) is a programming environment for experiments in music performance, composition, and intelligent instrument design. Based on HMSL (Hierarchical Music Specification Language), JMSL is a Java package which affords the composer all the functionality of the Java programming language as well as the hierarchical structuring, scheduling, and philosophy of HMSL.&lt;/p&gt;
    &lt;p&gt;JSyn uses native methods written in 'C' to provide real-time audio synthesis for Java programmers. It is based on the traditional model of unit generators which can be connected together to form complex sounds. For example, you could connect a white noise generator to a low pass filter that is modulated by random ramp generators to create a wind sound.&lt;/p&gt;
    &lt;p&gt;Interpreted multi-tasking awk-like language designed for MIDI algorithmic and realtime manipulation. Multi-window GUI with pull-off menus and buttons is implemented using the language, and includes a multi-track sequencer, and drum pattern editor. Source code for all tools is included and can be customized easily.&lt;/p&gt;
    &lt;p&gt;Interpreted programming language and GUI, algorithmic and realtime MIDI processing, music editor written in Keynote itself, hence customizable, piano-roll style with pop-up menus. See also: KeyKit.&lt;/p&gt;
    &lt;p&gt;Object-oriented compositional environment based on Nasal, a clean and flexible dynamically typed scripting language with garbage collection. Suitable for live performance (real-time recompiling of objects), algorithmic composition and experiments.&lt;/p&gt;
    &lt;p&gt;A language for specifying and manipulating sound. It is a visual language and is based on units called "sound objects" rather than the "notes" of standard music notation. Structures specified in Kyma can be compiled for real-time samples generation on a digital signal processor. Kyma is described in an issue of CMJ devoted to object-oriented music applications.&lt;/p&gt;
    &lt;p&gt;Loki is a text to MIDI converter. It was developed to help transcribe music to MIDI. It contains facilities to harmonise and manipulate melodies as well. The next release will provide interactive facilities. Shareware and Fun.&lt;/p&gt;
    &lt;p&gt;A graphical, object-oriented language in which precompiled input/output primitives of specific function can be 'patched' together graphically onscreen to create large interactive systems. Primarily but not exclusively MIDI-oriented. User primitives can be compiled in C.&lt;/p&gt;
    &lt;p&gt;Macro language for configuration and control of analog oscillators, filters, VCAs, LFOs, amp, mixer, etc., music notation, SMPTE sync, but especially rich in traditional compositional vocabulary. (Nestable) macros could be triggered by name, parameter value (e.g. pitch), time (abs or rel in min:sec:frames or measures:beats:ticks), controller activity, alphanumeric input. (No published documentation or description.)&lt;/p&gt;
    &lt;p&gt;MML is a Macro Music Language used by the BASIC interpreter on MSX home computers. It allows to define up to nine channels of MIDI or internal sounds (FM chips or AY-3-8910 chip).&lt;/p&gt;
    &lt;p&gt;The Musical Object Development Environment (MODE) is a Smalltalk-80 framework and tool kit for music description, score editing, interactive performance, and digital signal processing.&lt;/p&gt;
    &lt;p&gt;MPL is a collection of functions in APL that manipulate note and conductor data in matrices. Deveoped at Oberlin. Used extensively at U. Melbourne in Australia. Currently used only by the author on Mac OSX with APLX from microAPL in UK. Listen to Goss on http://www.timara.oberlin.edu/~gnelson/mp3s/Long.mp3s.html&lt;/p&gt;
    &lt;p&gt;THE MSQ PROJECT provides an open, easily readable and editable file format qualified for algorithmic manipulation and composition as well as for real time controlling MIDI instruments. The MSQ file format, a plain ASCII text file format, represents sequences of MIDI commands in strictly chronological order even when multiple MIDI tracks are present. It does not only allow the translation of MIDI files to some readable text but is a well defined and MIDI compatible file format itself.&lt;/p&gt;
    &lt;p&gt;Creates high-quality PostScript or MIDI output from a text description of music. Available for DOS and UNIX, plus in source form for compiling on many other systems.&lt;/p&gt;
    &lt;p&gt;8-bit sampling, graphical additive synthesis and command line sequencing. Information able to be entered through keyboard or light-pen. 'Film Music Processor' allowed editing to be carried out in a variety of time codes, working in frames, music cues and sync points.&lt;/p&gt;
    &lt;p&gt;MusicDNA Composer is a web-based application for the creation of tonal music. Composers use a simple API to create melodies, harmonies and counterpoint, from the simplest single-voice piano song all the way up to a symphony. Code is compiled into both MIDI and sheet music in PNG and PDF format, which can be downloaded or displayed directly through the browser. The output can therefore be easily synthesized or performed by instrumentalists. The language "knows" basic harmony and counterpoint, and can take care of much of the calculation necessary in programming if the composer chooses. Themes or melodies can be pulled out into functions and shared among composers, thus allowing them to collaborate in the same way that programmers do. Since the songs are stored on the MusicDNA servers, you can also edit your compositions from anywhere. The site also allows you to store MP3s, MIDIs, sheet music or other media, so that you may give out the URL to your creations.&lt;/p&gt;
    &lt;p&gt;The Music Kit is an object-oriented software system for building music, sound, signal processing, and MIDI applications in the NEXTSTEP programming environment. It has been used in such diverse commercial applications as music sequencers, computer games, and document processors. Professors and students have used the Music Kit in a host of areas, including music performance, scientific experiments, computer-aided instruction, and physical modeling.&lt;/p&gt;
    &lt;p&gt;MusicXML is a universal translator for common Western musical notation from the 17th century onwards. It is designed as an interchange format for notation, analysis, retrieval, and performance applications.&lt;/p&gt;
    &lt;p&gt;There was an Algol based language MUSIGOL fashioned after the Bell Labs MUSIC I-V programs. It was created at the University of Virginia. MUSIGOL ran on a Burroughs B5500 and used an Adage Ambilog 200 as a DAC.&lt;/p&gt;
    &lt;p&gt;NetSound is a structured-audio compositor and synthesizer which renders sound in real-time using a variety of synthesis algorithms. It is being developed by the Machine Listening Group at the Media Lab.&lt;/p&gt;
    &lt;p&gt;A functional programming language for composition and sound synthesis. Uses a Lisp syntax, a signal processing and signal representation core, and a rich semantics dealing with time and transformations.&lt;/p&gt;
    &lt;p&gt;OpenMusic is a highly visual environment for the composer on the Macintosh. While drawing benefit from the huge amount of knowledge and experience gathered around the PatchWork software, OpenMusic implements a set of radically new features that make it a second generation compositional software. Based on Digitool Macintosh Common Lisp, OpenMusic provides a visual programming interface to Lisp programming as well as to CLOS (Common Lisp Object System). Thus OpenMusic is an Object Oriented (OO) environment. Objects are symbolized by icons that may be dragged and dropped all around. Most operations are then performed by dragging an icon from a particular place and dropping it to an other place. These places include the OpenMusic Workspace as well as the Macintosh finder itself. A lot of classes implementing musical data / behaviour are provided. They are associated with graphical editors and may be extended by the user to meet specifical needs. Different representation of a musical process are handled among which common notation, midi piano-roll, sound signal. High level in-time organisation of the music material is proposed through the " maquette " concept.&lt;/p&gt;
    &lt;p&gt;Opusmodus is aimed at composers of all kinds - of art music, concert music, choral music, film music, jazz, electroacoustic music, music for games and new media, songwriters. Opusmodus is a comprehensive computer-aided environment for the whole work of music composition a virtual space where a composer can develop ideas and experiments for projects large and small. Opusmodus allows you to explore more than one structure at the same time. It also allows the composer to study the interaction between the different structures with more meaningful outcome.&lt;/p&gt;
    &lt;p&gt;Open Sound World, or OSW, is a scalable, extensible programming environment that allows musicians, sound designers and researchers to process sound in response to expressive real-time control. OSW combines a familiar visual patching paradigm with solid programming-language features such as a strong type system and hierarchical name spaces. OSW also includes an intuitive model for specifying new components using a graphical interface and high-level C++ expressions, making it easy to develop and share new music and signal-processing algorithms.&lt;/p&gt;
    &lt;p&gt;Patchwork is a graphical interactive environment for computer assisted composition which is aimed at helping composers generate, represent and manipulate musical material. Its general, extendible environment can be easily adapted to suit radically different aesthetic needs.&lt;/p&gt;
    &lt;p&gt;pcmusic takes a text (ASCII) file written in the pcmusic input language and creates a soundfile (in .wav format) that corresponds to it. For this reason, pcmusic is a member of the class of programs sometimes known as "acoustic compilers". pcmusic is a version of the cmusic sound synthesis program for the IBM PC and compatibles.&lt;/p&gt;
    &lt;p&gt;Rosegarden is an integrated MIDI sequencer and musical notation editor. It is free software for Unix and X, running on systems such as Linux, FreeBSD and SGI IRIX, and now also on OpenVMS.&lt;/p&gt;
    &lt;p&gt;PMML is a musical event description/manipulation language designed for computer-controlled performances with MIDI instruments. Direct music description, algorithmic composition, and music transformation are all supported.&lt;/p&gt;
    &lt;p&gt;A python module for generating and manipulating musical events. Output is currently only in csound sco format, but the goal of pysco is to support MIDI streams and files as well, and user-extensible support for other kinds of output.&lt;/p&gt;
    &lt;p&gt;Q is a modern functional programming language based on the term rewriting calculus. Programs are simply collections of equations which are used to evaluate expressions in a symbolic fashion. Q offers an elaborate interface to Grame's MidiShare, and also has a basic audio interface. The latter will be improved over time (additional modules for doing modular synth and dsp stuff are in the planning stage). The MidiShare interface already makes Q a nice environment for (realtime) MIDI programming.&lt;/p&gt;
    &lt;p&gt;Quasimodo is an advanced, real-time, extensible, MIDI-controllable environment for generating and processing audio and MIDI data. Quasimodo supports the Csound programming language, plugin opcodes, themeable graphics, a simple scripting language for user-interface design, and an intuitive graphical user interface for real-time manipulation. It supports both Csound scorefiles, real-time MIDI input, and its own user interface for playing audio and MIDI compositions.&lt;/p&gt;
    &lt;p&gt;This library - a collection of patches for MAX (an interactive graphical programming environment for multimedia, music, and MIDI) - offers the possibility to experiment with a number of compositional techniques, such as serial procedures, permutations and controlled randomness. Most of these objects are geared towards straightforward processing of data. By using these specialized objects together in a patch, programming becomes much more clear and easy. Many functions that are often useful in algorithmic composition are provided with this library - therefore the composer could concentrate rather on the composition than the programming aspects. The Real Time Composition Library (RTC-lib) was developed during my extensive work on Lexikon-Sonate (1992 ff.), an interactive realtime composition for computer-controlled MIDI-piano. Regardless the fact that it was conceived for a concrete project it became more and more obvious that its functionalities are open and generic enough to be used by other composers in different compositional contexts. Although - from the theoretical point of view - based on paradigms which have been extracted from serial thinking - and its further developments until nowadays it does not force towards a certain aesthetic, but provides a programming environment for testing and developing musical strategies. Please note that "serial" has here another connotation than it normally has (especially in the US): "serial" here refers to a certain way of musical thinking rather then dodecaphonic techniques, which have been abandoned by the serial theory itself (cf. Gottfried Michael Koenig and Karlheinz Stockhausen).&lt;/p&gt;
    &lt;p&gt;SAOL is a music-synthesis and effects-processing language which is a component of the MPEG-4 standard (ISO/IEC 14496-3). It follows a Music-N paradigm, but has a number of novel extensions, most notably the ability to define new unit generators within the language. In MPEG-4, SAOL is used to transmit synthesis descriptions controllable with MIDI or by a new lightweight score format called SASL, and to transmit effects-processing algorithms which apply to natural (waveform-encoded) audio within the MPEG-4 audio scene.&lt;/p&gt;
    &lt;p&gt;An acoustic compiler - a program which takes a source file written in the sapphire programming language and generates a sample. Sapphire can create sounds of arbitrary complexity, although it may take several hours for it to do so. Think of it as a ray-tracer for noises.&lt;/p&gt;
    &lt;p&gt;Scala is an editor, librarian, and analysis tool for musical tunings. One can create, manipulate and combine scales in many ways using the Scala command language. It can also tune synthesizers and retune MIDI files.&lt;/p&gt;
    &lt;p&gt;The Sound Description Interchange Format (SDIF) is a recently adopted standard that can store a variety of sound representations including spectal, time domain, and higher-level models. SDIF consists of a data format and a set of standard sound descriptions and their official representation. SDIF is flexible in that new sound descriptions can be represented, and new kinds of data can be added to existing sound descriptions.&lt;/p&gt;
    &lt;p&gt;Silence is an extensible system for making music on computers by means of software alone. It implements Music Modeling Language (MML), which represents music as a directed acyclic graph of nodes that can be notes, groups of notes, transformations of notes, or processes generating notes. MML is to sounds as VRML is to pictures. Silence currently uses its own Java interface to Csound as a synthesis engine.&lt;/p&gt;
    &lt;p&gt;Synthesis toolKit Instrument Network Interface - a language designed to be MIDI compatible and extend MIDI in incremental but profound ways. It uses text-based messages. SKINI was designed to be extensable and hackable for a number of applications: imbedded synthesis in a game or VR simulation, scoring and mixing tasks, real-time and non-real time applications which could benefit from a controllable sound synthesis, JAVA controlled synthesis, or eventually maybe JAVA synthesis.&lt;/p&gt;
    &lt;p&gt;The Standard Music Description Language (SMDL) is defined (in ISO/IEC Draft International Standard 10743) as "an architecture for the representation of music information, either alone, or in conjunction with text, graphics, or other information needed for publishing or business purposes. Multimedia time sequencing information is also supported." SMDL is a HyTime application conforming to International Standard ISO/IEC 10744 Hypermedia/Time-based Structuring Language.&lt;/p&gt;
    &lt;p&gt;The Smalltalk music object kernel (Smoke) music representation language facilitates the formal description of low-level musical data such as note events, and also of higher-level structures such as chord progressions and musical form "objects." In object-oriented software terms, the representation is described in terms of software class hierarchies of objects that share state and behavior and implement the description language as their protocol.&lt;/p&gt;
    &lt;p&gt;SOUL is a language for audio development focused on portability and speed, but also on accessibility for audio enthusiasts and professionals. Conceived as "an audio equivalent of OpenGLSL or OpenCL", its goal is to produce audio programs than can run not only on the CPU but also on heterogeneous or remote hardware, without an impact on the complexity for the developer. Claiming that the CPU is not the best place to run these programs, the authors are confident that domain-specific hardware will be the future of sound programming, and accessible and portable tools to exploit these will be necessary. Like graphic shaders, SOUL programs run within another "host" application, that can be written with any language or framework without bearing on its performance. SOUL is currently in beta.&lt;/p&gt;
    &lt;p&gt;Common Composer's Programming Language (CCPL) is a computer programming environment aiming at composers and researchers in the field of electroacoustic music. SoundModel is part of CCPL, does not depend on any particular synthesis method, and can be interfaced to many different methods as long as the parameters of the method are closely related to the acoustical result. The language can be used to describe a large universe of sonic-structures with complex dynamic behaviour and complicated interdependencies which makes it useful in a computer music environment.&lt;/p&gt;
    &lt;p&gt;A successor to SuperCollider 2 for Macintosh OS9. Synthesis takes place in a separate process (the server), controlled by OSC messages generated by a client. Any software that can produce OSC messages can be used to control the client, however. The library of sound-processing functions is now implemented as C++ plugins to the server, allowing users to write their own using the server API. SC Server includes a client whose language is similar to SC 2, but is extended with some borrowing from functional programming notation. It features powerful data manipulation classes and multi-threading for algorithmic composition. GUI, MIDI in/out, HID (joystick/other) input, Wacom tablet input are also supported. Optimized for real-time, live-performance scenarios.&lt;/p&gt;
    &lt;p&gt;SuperCollider is an environment for real time audio synthesis which runs on a Power Macintosh with no additional hardware. SuperCollider features: a built in programming language with real time incremental garbage collection, first class functions/closures, a small object oriented class system, a mini GUI builder for creating a patch control panel, a graphical interface for creating wave tables and breakpoint envelopes, MIDI control, and a large library of signal processing and synthesis functions. It is an extended version of Pyrite, but no longer runs under Max.&lt;/p&gt;
    &lt;p&gt;SC allows the user to create music from almost any source: text, fractals, association-structures... in almost any style.... It is quite a difficult language to learn, but it offers almost endless possibilities.&lt;/p&gt;
    &lt;p&gt;VMM is a programming language that allows you to send and receive raw midi messages and has built-in libraries for higher level MIDI functions. VMM makes Multithreading super easy, and that's what makes it so powerfull.&lt;/p&gt;
    &lt;p&gt;Zel is a computer language for creating MIDI data. Its features include&lt;/p&gt;
    &lt;p&gt;-low language overhead--"a b c" plays a b c . -powerful macro capabilities with parameter passing, -automatic distribution of notes into multiple tracks, -file inclusion, -controller/tempo/velocity sequence generation, -automatic pitch-bend generation, -integer/fractional/decimal/MBT/SMPTE duration formats, -fine control of note displacement, -unlimited tracks, -attribute inheritance ( track-&amp;gt;chord-&amp;gt;note ) -random or sequential pick from a list of weighted macros -can automatically apply macro based on note timing -sysex file inclusion and sub-parser -musical thread isolation using parentheses -looping -define and transpose sets of notes and reference them -supports MIDI text and meta-events&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46338437</guid><pubDate>Sat, 20 Dec 2025 18:40:45 +0000</pubDate></item><item><title>How I protect my Forgejo instance from AI web crawlers</title><link>https://her.esy.fun/posts/0031-how-i-protect-my-forgejo-instance-from-ai-web-crawlers/index.html</link><description>&lt;doc fingerprint="44e6f9513b184b8e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;Put that in your nginx config:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;location / {
  # needed to still allow git clone from http/https URLs
  if ($http_user_agent ~* "git/|git-lfs/") {
        set $bypass_cookie 1;
  }
  # If we see the expected cookie; we could also bypass the blocker page
  if ($cookie_Yogsototh_opens_the_door = "1") {
        set $bypass_cookie 1;
  }
  # Redirect to 418 if neither condition is met
  if ($bypass_cookie != 1) {
     add_header Content-Type text/html always;
     return 418 '&amp;lt;script&amp;gt;document.cookie = "Yogsototh_opens_the_door=1; Path=/;"; window.location.reload();&amp;lt;/script&amp;gt;';
  }
  # rest of your nginx config
&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Preferably run a string replace from &lt;code&gt;Yogsototh_opens_the_door&lt;/code&gt; to your own personal Cookie
name.&lt;/p&gt;
    &lt;p&gt;Main advantage, is that it is almost invisible to the users of my website compartively to other solutions like Anubis.&lt;/p&gt;
    &lt;head rend="h1"&gt;More detail&lt;/head&gt;
    &lt;p&gt;Not so long ago I started to host my code to forgejo. There is a promise that in the future it will support federation and forgejo is the same project that is used for codeberg.&lt;/p&gt;
    &lt;p&gt;The only problem I had was one day, I discovered that my entire node was down. At first I didn't investigate and just restarted the node. But soon after a few hours, it was down again. Looking at the reason, clearly thousands of requests that looked at every commit which put too much pressure on the system. Who could be so interested in using the web API to look at every commit instead, of… you know, clone the repository locally and explore it. Quickly, yep, like so many of you, I discovered that tons of crawlers that did not respect the &lt;code&gt;robots.txt&lt;/code&gt;
are crawling my forgejo instance until death ensues.&lt;/p&gt;
    &lt;p&gt;So I had no choice, I first used a radical approach and blocked my website entirely except from me. But hey, why having a public forge if not for people to be able to look into it time to time?&lt;/p&gt;
    &lt;p&gt;I then installed Anubis, but it wasn't really for me. It is way too heavy for my needs, not as easy as I would have hoped to configure and install.&lt;/p&gt;
    &lt;p&gt;Then I saw this article You don't need anubis on lobste.rs using a simple configuration in caddy that should block these pesky crawlers. I made some adjustments to adapt it to nginx. For now, this is working perfectly well, my users are just redirected once, without really noticing it. And they could use forgejo as they could before. And this puts the crawlers away.&lt;/p&gt;
    &lt;p&gt;The strategy is pretty basic; in fact, a lot less advanced than the strategy adopted by Anubis. For every access of my website, I just check if the user has a specific cookie set. If not, I redirect the user to a 418 HTML page containing some js code to execute that set this specific cookie and reload the page.&lt;/p&gt;
    &lt;p&gt;That's it.&lt;/p&gt;
    &lt;p&gt;I also tried to return a 302 and add a cookie from the response without javascript, but the crawlers are immune to that second strategy. Unfortunately this means, my website could only be seen if you enable javascript in your browser. I feel this is acceptable. I guess, someday this very basic protection will not be enough and my forgejo instance will break again, and I will be forced to use more advanced system like Anubis or perhaps even iocaine.&lt;/p&gt;
    &lt;p&gt;I hope this could be helpful, because, I recently saw many discussions on that subject where people were not totally happy to use Anubis, while at least for me, this quick dirty fix does the trick. And I am fully aware that this would be very easy to bypass. But for now, I think the volume is more important than the quality for these crawlers and it may take a while for them to need to adapt. Also, by publishing this, I know if too many people use the same trick, quickly, these crawlers will adapt.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345205</guid><pubDate>Sun, 21 Dec 2025 14:46:53 +0000</pubDate></item><item><title>CO2 batteries that store grid energy take off globally</title><link>https://spectrum.ieee.org/co2-battery-energy-storage</link><description>&lt;doc fingerprint="267d315709b6a538"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grid-Scale Bubble Batteries Will Soon Be Everywhere&lt;/head&gt;
    &lt;p&gt;When the sun sets on solar panels, these gas-filled domes take over&lt;/p&gt;
    &lt;p&gt;This giant bubble on the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasn’t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the dome’s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until it’s needed.&lt;/p&gt;
    &lt;p&gt;Developed by the Milan-based company Energy Dome, the bubble and its surrounding machinery demonstrate a first-of-its-kind “CO2 Battery,” as the company calls it. The facility compresses and expands CO2 daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.&lt;/p&gt;
    &lt;p&gt;We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere there’s 5 hectares of flat land.&lt;/p&gt;
    &lt;p&gt;The first to build one outside of Sardinia will be one of India’s largest power companies, NTPC Limited. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility Alliant Energy received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.&lt;/p&gt;
    &lt;p&gt;And Google likes the concept so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling data centers with round-the-clock clean energy, even when the sun isn’t shining or the wind isn’t blowing. The partnership with Energy Dome, announced in July, marked Google’s first investment in long-duration energy storage.&lt;/p&gt;
    &lt;p&gt;“We’ve been scanning the globe seeking different solutions,” says Ainhoa Anda, Google’s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. “So standardization is really important, and this is one of the aspects that we really like” about Energy Dome, she says. “They can really plug and play this.”&lt;/p&gt;
    &lt;p&gt;Google will prioritize placing the Energy Dome facilities where they’ll have the most impact on decarbonization and grid reliability, and where there’s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Google’s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.&lt;/p&gt;
    &lt;p&gt;Anda says Google expects to help the technology “reach a massive commercial stage.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting creative with long-duration energy storage&lt;/head&gt;
    &lt;p&gt;All this excitement is based on Energy Dome’s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transition’s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.&lt;/p&gt;
    &lt;p&gt;When sun and wind are abundant, solar and wind farms tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.&lt;/p&gt;
    &lt;p&gt;The problem is that even the best new grid-scale storage systems on the market—mainly lithium-ion batteries—provide only about 4 to 8 hours of storage. That’s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.&lt;/p&gt;
    &lt;p&gt;After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually aren’t economically viable. Other grid-scale battery chemistries and approaches are in development, such as sodium-based, iron-air, and vanadium redox flow batteries. But the energy density, costs, degradation, and funding complications have challenged the developers of those alternatives.&lt;/p&gt;
    &lt;p&gt;Researchers have also experimented with storing energy by compressing air, heating up blocks or sand, using hydrogen or methanol, pressurizing water deep underground, and even dangling heavy objects in the air and dropping them. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.&lt;/p&gt;
    &lt;p&gt;The tried-and-true grid-scale storage option—pumped hydro, in which water is pumped between reservoirs at different elevations—lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.&lt;/p&gt;
    &lt;p&gt;CO2 Batteries check a lot of boxes that other approaches don’t. They don’t need special topography like pumped-hydro reservoirs do. They don’t need critical minerals like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as lithium-ion batteries. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.&lt;/p&gt;
    &lt;p&gt;China has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO2-based energy-storage facility in the Xinjiang region of northwest China. Media reports show renderings of domes but give widely varying storage capacities—including 100 MW and 1,000 MW. The Chinese companies did not respond to IEEE Spectrum’s requests for information.&lt;/p&gt;
    &lt;p&gt;“What I can say is that they are developing something very, very similar [to Energy Dome’s CO2 Battery] but quite large in scale,” says Claudio Spadacini, Energy Dome’s founder and CEO. The Chinese companies “are good, they are super fast, and they have a lot of money,” he says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Google investing in CO2 Batteries?&lt;/head&gt;
    &lt;p&gt;When I visited Energy Dome’s Sardinia facility in October, the CO2 had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO2, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.&lt;/p&gt;
    &lt;p&gt;“This is incredible,” I said to my guide, Mario Torchio, Energy Dome’s global marketing and communications director.&lt;/p&gt;
    &lt;p&gt;“It is. But it’s physics,” he said.&lt;/p&gt;
    &lt;p&gt;Outside the dome, a series of machines connected by undulating pipes moves the CO2 out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO2 to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.&lt;/p&gt;
    &lt;p&gt;To discharge the battery, the process reverses. The liquid CO2 is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.&lt;/p&gt;
    &lt;p&gt;Energy Dome engineers inspect the dryer system, which keeps the gaseous CO₂ in the dome at optimal dryness levels at all times.Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;It’s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. “How we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensing…can really cut costs and increase the efficiency,” he says.&lt;/p&gt;
    &lt;p&gt;The company uses pure, purpose-made CO2 instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happens if the dome is punctured?&lt;/head&gt;
    &lt;p&gt;On the downside, Energy Dome’s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.&lt;/p&gt;
    &lt;p&gt;And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a day’s warning of severe weather, the company can just compress and store the CO2 in the tanks and then deflate the outer dome, he says.&lt;/p&gt;
    &lt;p&gt;If the worst happens and the dome is punctured, 2,000 tonnes of CO2 will enter the atmosphere. That’s equivalent to the emissions of about 15 round-trip flights between New York and London on a Boeing 777. “It’s negligible compared to the emissions of a coal plant,” Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.&lt;/p&gt;
    &lt;p&gt;Worth the risk? The companies lining up to build these systems seem to think so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid-Scale Battery Stabilizes Scottish Power Supply ›&lt;/item&gt;
      &lt;item&gt;Backing Up the Power Grid With Green Methanol ›&lt;/item&gt;
      &lt;item&gt;DOE Places Compressed-Air Energy Storage Loan Under Review ›&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345506</guid><pubDate>Sun, 21 Dec 2025 15:27:36 +0000</pubDate></item><item><title>Show HN: WalletWallet – create Apple passes from anything</title><link>https://walletwallet.alen.ro/</link><description>&lt;doc fingerprint="6fc1f19d954373"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WalletWallet&lt;/head&gt;
    &lt;p&gt;A simple utility to convert physical barcodes into digital passes for Apple Wallet®. Entirely free and runs directly from your browser.&lt;/p&gt;
    &lt;p&gt; 1 Enter your membership or loyalty card barcode data. &lt;/p&gt;
    &lt;p&gt; 2 Configure the appearance and titles for your pass. &lt;/p&gt;
    &lt;p&gt; 3 Download and open the file to add it to your Wallet. &lt;/p&gt;
    &lt;p&gt; No Sign-up &lt;/p&gt;
    &lt;p&gt; Private &lt;/p&gt;
    &lt;p&gt; No Install &lt;/p&gt;
    &lt;p&gt; Pass Configuration &lt;/p&gt;
    &lt;p&gt;Generates a standard .pkpass file&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345745</guid><pubDate>Sun, 21 Dec 2025 16:04:05 +0000</pubDate></item><item><title>Show HN: Books mentioned on Hacker News in 2025</title><link>https://hackernews-readings-613604506318.us-west1.run.app</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345897</guid><pubDate>Sun, 21 Dec 2025 16:21:04 +0000</pubDate></item><item><title>The Going Dark initiative or ProtectEU is a Chat Control 3.0 attempt</title><link>https://mastodon.online/@mullvadnet/115742530333573065</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46347080</guid><pubDate>Sun, 21 Dec 2025 18:39:46 +0000</pubDate></item><item><title>Rue: Higher level than Rust, lower level than Go</title><link>https://rue-lang.dev/</link><description>&lt;doc fingerprint="d268deffe87878fc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Memory Safe&lt;/head&gt;
    &lt;p&gt;No garbage collector, no manual memory management. A work in progress, though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple Syntax&lt;/head&gt;
    &lt;p&gt;Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast Compilation&lt;/head&gt;
    &lt;p&gt;Direct compilation to native code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, Rue&lt;/head&gt;
    &lt;code&gt;// It's a classic for a reason
 

 
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348262</guid><pubDate>Sun, 21 Dec 2025 20:46:02 +0000</pubDate></item><item><title>More on whether useful quantum computing is “imminent”</title><link>https://scottaaronson.blog/?p=9425</link><description>&lt;doc fingerprint="6f11911b6025ddfc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;More on whether useful quantum computing is “imminent”&lt;/head&gt;
    &lt;p&gt;These days, the most common question I get goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A decade ago, you told people that scalable quantum computing wasn’t imminent. Now, though, you claim it plausibly is imminent. Why have you reversed yourself??&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I appreciated the friend of mine who paraphrased this as follows: “A decade ago you said you were 35. Now you say you’re 45. Explain yourself!”&lt;/p&gt;
    &lt;p&gt;A couple weeks ago, I was delighted to attend Q2B in Santa Clara, where I gave a keynote talk entitled “Why I Think Quantum Computing Works” (link goes to the PowerPoint slides). This is one of the most optimistic talks I’ve ever given. But mostly that’s just because, uncharacteristically for me, here I gave short shrift to the challenge of broadening the class of problems that achieve huge quantum speedups, and just focused on the experimental milestones achieved over the past year. With every experimental milestone, the little voice in my head that asks “but what if Gil Kalai turned out to be right after all? what if scalable QC wasn’t possible?” grows quieter, until now it can barely be heard.&lt;/p&gt;
    &lt;p&gt;Going to Q2B was extremely helpful in giving me a sense of the current state of the field. Ryan Babbush gave a superb overview (I couldn’t have improved a word) of the current status of quantum algorithms, while John Preskill’s annual where-we-stand talk was “magisterial” as usual (that’s the word I’ve long used for his talks), making mine look like just a warmup act for his. Meanwhile, Quantinuum took a victory lap, boasting of their recent successes in a way that I considered basically justified.&lt;/p&gt;
    &lt;p&gt;After returning from Q2B, I then did an hour-long podcast with “The Quantum Bull” on the topic “How Close Are We to Fault-Tolerant Quantum Computing?” You can watch it here:&lt;/p&gt;
    &lt;p&gt;As far as I remember, this is the first YouTube interview I’ve ever done that concentrates entirely on the current state of the QC race, skipping any attempt to explain amplitudes, interference, and other basic concepts. Despite (or conceivably because?) of that, I’m happy with how this interview turned out. Watch if you want to know my detailed current views on hardware—as always, I recommend 2x speed.&lt;/p&gt;
    &lt;p&gt;Or for those who don’t have the half hour, a quick summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In quantum computing, there are the large companies and startups that might succeed or might fail, but are at least trying to solve the real technical problems, and some of them are making amazing progress. And then there are the companies that have optimized for doing IPOs, getting astronomical valuations, and selling a narrative to retail investors and governments about how quantum computing is poised to revolutionize optimization and machine learning and finance. Right now, I see these two sets of companies as almost entirely disjoint from each other.&lt;/item&gt;
      &lt;item&gt;The interview also contains my most direct condemnation yet of some of the wild misrepresentations that IonQ, in particular, has made to governments about what QC will be good for (“unlike AI, quantum computers won’t hallucinate because they’re deterministic!”)&lt;/item&gt;
      &lt;item&gt;The two approaches that had the most impressive demonstrations in the past year are trapped ions (especially Quantinuum but also Oxford Ionics) and superconducting qubits (especially Google but also IBM), and perhaps also neutral atoms (especially QuEra but also Infleqtion and Atom Computing).&lt;/item&gt;
      &lt;item&gt;Contrary to a misconception that refuses to die, I haven’t dramatically changed my views on any of these matters. As I have for a quarter century, I continue to profess a lot of confidence in the basic principles of quantum computing theory worked out in the mid-1990s, and I also continue to profess ignorance of exactly how many years it will take to realize those principles in the lab, and of which hardware approach will get there first.&lt;/item&gt;
      &lt;item&gt;But yeah, of course I update in response to developments on the ground, because it would be insane not to! And 2025 was clearly a year that met or exceeded my expectations on hardware, with multiple platforms now boasting &amp;gt;99.9% fidelity two-qubit gates, at or above the theoretical threshold for fault-tolerance. This year updated me in favor of taking more seriously the aggressive pronouncements—the “roadmaps”—of Google, Quantinuum, QuEra, PsiQuantum, and other companies about where they could be in 2028 or 2029.&lt;/item&gt;
      &lt;item&gt;One more time for those in the back: the main known applications of quantum computers remain (1) the simulation of quantum physics and chemistry themselves, (2) breaking a lot of currently deployed cryptography, and (3) eventually, achieving some modest benefits for optimization, machine learning, and other areas (but it will probably be a while before those modest benefits win out in practice). To be sure, the detailed list of quantum speedups expands over time (as new quantum algorithms get discovered) and also contracts over time (as some of the quantum algorithms get dequantized). But the list of known applications “from 30,000 feet” remains fairly close to what it was a quarter century ago, after you hack away the dense thickets of obfuscation and hype.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m going to close this post with a warning. When Frisch and Peierls wrote their now-famous memo in March 1940, estimating the mass of Uranium-235 that would be needed for a fission bomb, they didn’t publish it in a journal, but communicated the result through military channels only. As recently as February 1939, Frisch and Meitner had published in Nature their theoretical explanation of recent experiments, showing that the uranium nucleus could fission when bombarded by neutrons. But by 1940, Frisch and Peierls realized that the time for open publication of these matters had passed.&lt;/p&gt;
    &lt;p&gt;Similarly, at some point, the people doing detailed estimates of how many physical qubits and gates it’ll take to break actually deployed cryptosystems using Shor’s algorithm are going to stop publishing those estimates, if for no other reason than the risk of giving too much information to adversaries. Indeed, for all we know, that point may have been passed already. This is the clearest warning that I can offer in public right now about the urgency of migrating to post-quantum cryptosystems, a process that I’m grateful is already underway.&lt;/p&gt;
    &lt;p&gt;Update: Someone on Twitter who’s “long $IONQ” says he’ll be posting about and investigating me every day, never resting until UT Austin fires me, in order to punish me for slandering IonQ and other “pure play” SPAC IPO quantum companies. And also, because I’ve been anti-Trump and pro-Biden. He confabulates that I must be trying to profit from my stance (eg by shorting the companies I criticize), it being inconceivable to him that anyone would say anything purely because they care about what’s true.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348318</guid><pubDate>Sun, 21 Dec 2025 20:53:34 +0000</pubDate></item><item><title>A guide to local coding models</title><link>https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</link><description>&lt;doc fingerprint="2f539dd2343f89db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Revised] You Don’t Need to Spend $100/mo on Claude Code: Your Guide to Local Coding Models&lt;/head&gt;
    &lt;head rend="h3"&gt;What you need to know about local model tooling and the steps for setting one up yourself&lt;/head&gt;
    &lt;p&gt;[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.&lt;/p&gt;
    &lt;p&gt;[Edit 2] This hypothesis was actually wrong and thank you to everyone who commented!&lt;/p&gt;
    &lt;p&gt;Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.&lt;/p&gt;
    &lt;p&gt;I’m not editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below.&lt;/p&gt;
    &lt;p&gt;There is one takeaway this article provides that definitely holds true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local models are far more capable than they’re given credit for, even for coding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.&lt;/p&gt;
    &lt;p&gt;But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.&lt;/p&gt;
    &lt;p&gt;This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. I would not recommend running local models as a company instead of giving employees access to a tool like Claude Code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.&lt;/p&gt;
    &lt;p&gt;Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).&lt;/p&gt;
    &lt;p&gt;So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.&lt;/p&gt;
    &lt;p&gt;After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that my hypothesis was &lt;del&gt;correct, with nuance&lt;/del&gt;, not correct [see edit 2 above] which I’ll get into later in this article.&lt;/p&gt;
    &lt;p&gt;In this article, we cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why local models matter and the benefits they provide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Walk through setting up your own local coding model and tool step-by-step.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.&lt;/p&gt;
    &lt;p&gt;If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local coding models are very capable. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. [Edited to add in this next part] Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools matter a lot. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There’s a lot to consider when you’re actually working within hardware constraints. We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google threw a wrench into my hypothesis. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why local models?&lt;/head&gt;
    &lt;p&gt;You might wonder why local models are worth investing in at all. The obvious answer is cost. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.&lt;/p&gt;
    &lt;p&gt;First: Reliability. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.&lt;/p&gt;
    &lt;p&gt;Second: Local models can apply to far more applications. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.&lt;/p&gt;
    &lt;p&gt;With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.&lt;/p&gt;
    &lt;p&gt;Finally: Availability. Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).&lt;/p&gt;
    &lt;p&gt;While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding memory&lt;/head&gt;
    &lt;p&gt;To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.&lt;/p&gt;
    &lt;p&gt;Local AI has two parts that eat up your memory: The model itself and the model’s context window.&lt;/p&gt;
    &lt;p&gt;The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).&lt;/p&gt;
    &lt;p&gt;The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.&lt;/p&gt;
    &lt;p&gt;When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.&lt;/p&gt;
    &lt;p&gt;The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.&lt;/p&gt;
    &lt;p&gt;This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).&lt;/p&gt;
    &lt;p&gt;Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works here.&lt;/p&gt;
    &lt;p&gt;The second trick is quantizing the values you’re working with. Quantization means converting a continuous set of values into a smaller amount of distinct values. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.&lt;/p&gt;
    &lt;p&gt;You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.&lt;/p&gt;
    &lt;p&gt;We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it causes the model to forget details in long reasoning traces. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.&lt;/p&gt;
    &lt;p&gt;In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.&lt;/p&gt;
    &lt;p&gt;Here are a few more factors to understand when setting up a local coding model on your hardware:&lt;/p&gt;
    &lt;head rend="h3"&gt;Instruct versus non-instruct&lt;/head&gt;
    &lt;p&gt;Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).&lt;/p&gt;
    &lt;head rend="h3"&gt;Serving tools&lt;/head&gt;
    &lt;p&gt;You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.&lt;/p&gt;
    &lt;p&gt;Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.&lt;/p&gt;
    &lt;p&gt;MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.&lt;/p&gt;
    &lt;p&gt;Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time-to-first-token and tokens per second&lt;/head&gt;
    &lt;p&gt;In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.&lt;/p&gt;
    &lt;p&gt;This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance trade-offs&lt;/head&gt;
    &lt;p&gt;There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.&lt;/p&gt;
    &lt;p&gt;The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding tools&lt;/head&gt;
    &lt;p&gt;There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are OpenCode, Aider, Qwen Code, Roo Code, and Continue. Make sure to use a tool compatible with OpenAI’s API standard. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting set up&lt;/head&gt;
    &lt;p&gt;I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that tooling matters a lot. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.&lt;/p&gt;
    &lt;p&gt;If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.&lt;/p&gt;
    &lt;p&gt;For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.&lt;/p&gt;
    &lt;p&gt;For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.&lt;/p&gt;
    &lt;p&gt;I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with a ton of RAM. For serving local coding models, more is always better.&lt;/p&gt;
    &lt;p&gt;I’ve shared my modelfiles repo for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install MLX or download Ollama (the rest of this guide will continue with MLX but details for serving on Ollama can be found here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run pip install -U mlx-lm to install MLX for serving community models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download Qwen Code. You might need to install Node Package Manager for this. I recommend using Node Version Manager (nvm) for managing your npm version.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Set up your tool to access an OpenAI compatible API by entering the following settings:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Base URL: http://localhost:8080/v1 (should be the default MLX serves your model at)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;API Key: mlx&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Voila! Your coding model tool should be working with your local coding model.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run very slowly.&lt;/p&gt;
    &lt;p&gt;One tip I have for using local coding models: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was my hypothesis correct?&lt;/head&gt;
    &lt;p&gt;My original hypothesis was: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.&lt;/p&gt;
    &lt;p&gt;I would argue that&lt;del&gt;—yes!—&lt;/del&gt;no [see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.&lt;/p&gt;
    &lt;p&gt;[This paragraph was added in after initial release of this article] It’s important to note that local models will not reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.&lt;/p&gt;
    &lt;p&gt;It’s also important to note that local models are only going to get better and smaller. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.&lt;/p&gt;
    &lt;p&gt;From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.&lt;/p&gt;
    &lt;p&gt;One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.&lt;/p&gt;
    &lt;p&gt;Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.&lt;/p&gt;
    &lt;p&gt;However, this is foiled a bit now that Gemini 3 Flash was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.&lt;/p&gt;
    &lt;p&gt;I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Always be (machine) learning,&lt;/p&gt;
    &lt;p&gt;Logan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348329</guid><pubDate>Sun, 21 Dec 2025 20:55:15 +0000</pubDate></item><item><title>Show HN: Rust/WASM lighting data toolkit – parses legacy formats, generates SVGs</title><link>https://eulumdat.icu</link><description>&lt;doc fingerprint="714bd42804c754b4"&gt;
  &lt;main&gt;
    &lt;p&gt;This application requires JavaScript to run.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348344</guid><pubDate>Sun, 21 Dec 2025 20:56:54 +0000</pubDate></item><item><title>Disney Imagineering Debuts Next-Generation Robotic Character, Olaf</title><link>https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</link><description>&lt;doc fingerprint="ebf30b7df696007b"&gt;
  &lt;main&gt;
    &lt;p&gt;Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ Frozen.&lt;/p&gt;
    &lt;p&gt;This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation at the Core: From Screen to Reality&lt;/head&gt;
    &lt;p&gt;From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&lt;/p&gt;
    &lt;p&gt;Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technology Behind the Magic&lt;/head&gt;
    &lt;p&gt;Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&lt;/p&gt;
    &lt;p&gt;Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&lt;/p&gt;
    &lt;p&gt;While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists.&lt;/p&gt;
    &lt;p&gt;It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&lt;/p&gt;
    &lt;p&gt;Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&lt;/p&gt;
    &lt;p&gt;Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&lt;/p&gt;
    &lt;p&gt;The BDX Droids, self-balancing H.E.R.B.I.E., and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Guests Can See Olaf&lt;/head&gt;
    &lt;p&gt;Olaf will soon venture out into the unknown, eager to see guests at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.&lt;/item&gt;
      &lt;item&gt;Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp;amp; Development, came to life at in the latest episode of We Call It Imagineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348847</guid><pubDate>Sun, 21 Dec 2025 21:46:20 +0000</pubDate></item><item><title>ONNX Runtime and CoreML May Silently Convert Your Model to FP16</title><link>https://ym2132.github.io/ONNX_MLProgram_NN_exploration</link><description>&lt;doc fingerprint="6d59dd17b5120e18"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ONNX Runtime &amp;amp; CoreML May Silently Convert Your Model to FP16 (And How to Stop It)&lt;/head&gt;
    &lt;p&gt;Running an ONNX model in ONNX RunTime (ORT) with the CoreMLExecutionProvider may change the predictions your model makes implicitly and you may observe differences when running with PyTorch on MPS or ONNX on CPU. This is because the default arguments ORT uses when converting your model to CoreML will cast the model to FP16.&lt;/p&gt;
    &lt;p&gt;The fix is to use the following setup when creating the InferenceSession:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;p&gt;This ensures your model remains in FP32 when running on a Mac GPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncovering an Issue in ONNX Runtime - Benchmarking the EyesOff Model&lt;/head&gt;
    &lt;p&gt;Having trained the EyesOff model, I began evaluating the model and its run time. I was looking into the ONNX format and using it to run the model efficiently. I setup a little test bench in which I ran the model using PyTorch and ONNX with ONNX Runtime (ORT), both using MPS and CPU. While checking the outputs, I noticed that the metrics from the model ran on ONNX on MPS had a different output to those on ONNX CPU and PyTorch CPU and MPS. Note, the metrics from PyTorch on CPU and MPS were the same.&lt;/p&gt;
    &lt;p&gt;When I say ORT and MPS, this is achieved through ORT’s execution providers. To run an ONNX model on the Mac GPU you have to use the CoreMLExecutionProvider (more on this to come).&lt;/p&gt;
    &lt;p&gt;Now in Figure 1 and 2, observe the metric values - the PyTorch ones (Figure 1) are the same across CPU and MPS, this isn’t the same story for ONNX (Figure 2):&lt;/p&gt;
    &lt;p&gt;Wow, look at the diff in Figure 2! When I saw this it was quite concerning, floating point math can lead to differences in the calculations carried out across the GPU and CPU but the values here don’t appear to be a result of floating point math, the values are too large.&lt;/p&gt;
    &lt;p&gt;Given the difference in metrics, I was worried that running the model with ORT was changing the output of the model and hence the behaviour. The reason the metrics change is because some of the model predictions around the threshold flipped to the opposite side of the threshold (which is 0.5), this can be seen in the confusion matrices for the ONNX CPU run and MPS run:&lt;/p&gt;
    &lt;head rend="h4"&gt;FP32 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;207 (TN)&lt;/cell&gt;
        &lt;cell&gt;24 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;69 (FN)&lt;/cell&gt;
        &lt;cell&gt;164 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;FP16 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;206 (TN)&lt;/cell&gt;
        &lt;cell&gt;25 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;68 (FN)&lt;/cell&gt;
        &lt;cell&gt;165 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So two predictions flipped from negative to positive.&lt;/p&gt;
    &lt;p&gt;Having said that, the first thing I did was to make my life easier, by simplifying the scenario from the large EyesOff model to a simple one layer MLP and using that to run the experiments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why am I Using ONNX and ONNX RunTime?&lt;/head&gt;
    &lt;p&gt;Before going on it’s worth discussing what ONNX and ORT are, and why I’m even using them in the first place.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX1&lt;/head&gt;
    &lt;p&gt;ONNX stands for Open Neural Network Exchange. It can be thought of as a common programming language in which to describe ML models. Under the hood ONNX models are represented as graphs, these graphs outline the computation path of a model and it shows the operators and transformations required to get from input to prediction. These graphs are called ONNX graphs.&lt;/p&gt;
    &lt;p&gt;The use of a common language to describe models makes deployment easier and in some cases can add efficiency in terms of resource usage + or inference speed. Firstly, the ONNX graph itself can be optimised. Take PyTorch for example, you train the model in it and sure PyTorch is very mature and extremely optimised but it’s such a large package some things can be overlooked or difficult to change. By converting the model to ONNX, you take advantage of the fact that ONNX was built specifically with inference in mind and with that comes optimisations which the PyTorch team could implement but have not yet.&lt;/p&gt;
    &lt;p&gt;Furthermore, ONNX models can be ran cross platform in specialised runtimes. These runtimes are specially optimised for different architectures and add another layer of efficiency gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX RunTime (ORT)2&lt;/head&gt;
    &lt;p&gt;ORT is one of these runtimes. ORT actually runs the model, it can be thought of as an interpreter, it takes the ONNX graph and actually implements the operators and runs them on the specified hardware. ORT has a lot of magic built into it, the operators are extremely optimised and through the use of execution providers they target a wide range of hardware. Each execution provider is optimised for the specific hardware it refers to, this enables the ORT team to implement extremely efficient operators giving us another efficiency gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML3&lt;/head&gt;
    &lt;p&gt;As mentioned before, I used the CoreMLExecutionProvider to run the model on a Mac GPU. This execution provider informs ORT to make use of CoreML. CoreML is an apple developed framework which lets models (neural networks and classical ML models) run on Apple hardware, CPU, GPU and ANE. ORT’s purpose in this phase is to take the ONNX graph and convert it to a CoreML model. CoreML is Apple’s answer to running efficient on device models on Apple hardware.&lt;/p&gt;
    &lt;p&gt;Note, that all of this doesn’t always mean the model will run faster. Some models may run faster in PyTorch, TensorRT or any other framework. This is why it is important to benchmark and test as many approaches as makes sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Source of the CPU vs MPS Difference - With an MLP&lt;/head&gt;
    &lt;p&gt;The MLP used is very simple it has a single layer, with 4 inputs, 3 outputs and the bias turned off. So, I pretty much created a fancy matrix multiplication.&lt;/p&gt;
    &lt;p&gt;To understand where the issue was coming from I ran this MLP through some different setups:&lt;/p&gt;
    &lt;code&gt;- PyTorch CPU
- PyTorch MPS
- ORT CPU
- ORT MPS
- CoreML FP32
- CoreML FP16&lt;/code&gt;
    &lt;p&gt;The goal of this exercise is to find out if 1 - the difference in outputs is seen in a simple model and 2 - to figure out where exactly the issue arises.&lt;/p&gt;
    &lt;p&gt;Before showing the full results, I want to explain why I included the CoreML FP16 and FP32 runs - specifically why the FP16 one. When I initially ran the MLP experiment I only ran PyTorch, ORT and CoreML FP32 but the output numbers of ORT MPS looked like FP16 numbers. So, I tested if they were and also if the outputs from the other runs were true FP32 numbers. You can do this with a “round trip” test, by converting a number to FP16 and back to FP32. If after this process the number is unchanged then it is an FP16 number but if it changes then it was a true FP32. The number changes as FP16 can represent fewer floating point numbers than FP32. It’s a very simple check to carry out:&lt;/p&gt;
    &lt;code&gt;import numpy as np

= np.array([0.6480752, -0.34015813, 1.4329923], dtype=np.float32)
 onnx_cpu = np.array([0.6484375, -0.34033203, 1.4326172], dtype=np.float32)  # We cast the ort MPS numbers up to FP32, if they were FP16 this has no effect
 onnx_coreml 
= onnx_cpu.astype(np.float16).astype(np.float32)
 cpu_roundtrip = onnx_coreml.astype(np.float16).astype(np.float32)
 coreml_roundtrip 
print("ORT CPU values:")
print("  Original:", onnx_cpu)
print("  fp16 roundtrip:", cpu_roundtrip)
print("  Changed?", not np.allclose(onnx_cpu, cpu_roundtrip, atol=0))

print("\nORT CoreML values:")
print("  Original:", onnx_coreml)
print("  fp16 roundtrip:", coreml_roundtrip)
print("  Changed?", not np.allclose(onnx_coreml, coreml_roundtrip, atol=0))&lt;/code&gt;
    &lt;p&gt;The output of this is:&lt;/p&gt;
    &lt;p&gt;The CPU values change and the MPS values don’t! Now it’s getting interesting - perhaps when using the CoreML execution provider the output is FP16? This prompted adding the CoreML direct run in FP16 precision.&lt;/p&gt;
    &lt;p&gt;I tested this theory with an experiment. Originally, when benchmarking it was all about inference speed, now it’s about floating point precision and figuring out where the diffs come from.&lt;/p&gt;
    &lt;p&gt;Running on PyTorch CPU and MPS gives a strong baseline, PyTorch is a very mature ecosystem and I used the results from that as my ground truth. It being so close together is what drove me to understand what caused ORT runs on different hardware to have a difference. Then using CoreML FP32 and FP16 aimed to show if the issue was an ONNX one or a CoreML one.&lt;/p&gt;
    &lt;p&gt;Check Figure 4 for the outputs and Figure 5 for differences in the outputs here:&lt;/p&gt;
    &lt;p&gt;Wow, would you look at that - once again PyTorch + ORT CPU match and so does PyTorch CPU + CoreML FP32. Also note that CoreML FP16 and ORT MPS match! This is a big insight into what is happening and why the metrics output differed before. Along with the round trip experiment this proves our model is being ran in FP16 when using the CoreML execution provider in ORT!&lt;/p&gt;
    &lt;p&gt;Floating points numbers are defined by three values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign: 1 bit to define if the number is positive or negative&lt;/item&gt;
      &lt;item&gt;Significand: Contains the numbers digits&lt;/item&gt;
      &lt;item&gt;Exponent: This says where the decimal place should be placed relative to the beginning of the significand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Floating point numbers are often expressed in scientific notation, e.g:&lt;/p&gt;
    &lt;p&gt;FP16 and FP32 specifically, have the following specification:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Total bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Significand bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Exponent bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Smallest number&lt;/cell&gt;
        &lt;cell role="head"&gt;Largest number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Single precision&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;23 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;\(1.2 * 10^{-38}\)&lt;/cell&gt;
        &lt;cell&gt;\(3.4 * 10^{38}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Half precision&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;10 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;\(5.96 * 10^{-8}\)&lt;/cell&gt;
        &lt;cell&gt;\(6.55 * 10^{4}\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So as FP16 is half the size it affords a couple benefits, firstly it requires half the memory to store and secondly it can be quicker to do computations with too. However, this comes at a cost of precision, FP16 cannot represent very small numbers and the distances between small numbers as accurately as FP32.&lt;/p&gt;
    &lt;p&gt;An example of FP16 vs FP32 - The Largest Number Below 1&lt;/p&gt;
    &lt;p&gt;As you see FP32 can represent a value much closer to 1.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Link to the ONNX Issue&lt;/head&gt;
    &lt;p&gt;Having said all that, going back to the issue at hand we observe a ~\(1.17*e^{-7}\) error between PyTorch and CoreML FP32 which is typical of FP32. But, then ORT and CoreML when ran on MPS have a difference of ~\(3.7*e{-4}\) which is much more representative of FP16, this is what prompted the round trip experiment.&lt;/p&gt;
    &lt;p&gt;If you need a quick refresher on FP values, please expand the box above. If you already read that or you know enough about FP already let’s look at why some predictions flip.&lt;/p&gt;
    &lt;p&gt;In my model the base threshold for a 0 or 1 class is 0.5. Both FP16 and FP32 can represent 0.5 exactly:&lt;/p&gt;
    &lt;code&gt;= np.array([0.5], dtype=np.float32)
 fp_32_05 = np.array([0.5], dtype=np.float16)
 fp_16_05 
 fp_32_05.item(), fp_16_05.item()
0.5, 0.5) (&lt;/code&gt;
    &lt;p&gt;But we know that FP representations cannot represent every single number, so there will be some values around 0.5 which cannot be represented and hence will get rounded either up or down. Let’s look into that and find the threshold, this will show why some predictions of the EyesOff model were flipped when changing the model to run in FP16. Also, note by flipped we mean they go from a negative (0) prediction to a positive (1) class prediction, the rounding means it’d have to be below 0.5 and then be rounded up to cross the threshold boundary. Any other scenario would keep the label the same, i.e if it’s above 0.5 and gets rounded to 0.5 that’s fine as the predicted class is still the same.&lt;/p&gt;
    &lt;p&gt;The first step is to find the next representable number below 0.5:&lt;/p&gt;
    &lt;code&gt;# Show the representable values just below 0.5
= np.nextafter(np.float32(0.5), np.float32(0.0))
 fp32_below = np.nextafter(np.float16(0.5), np.float16(0.0))
 fp16_below 
= 0.5 - fp32_below
 fp32_gap = 0.5 - fp16_below
 fp16_gap 
print(f"\nClosest value BELOW 0.5:")
print(f"FP32: {fp32_below:.20f}")
print(f"FP16: {fp16_below:.20f}")

print(f"\nGap from threshold (0.5):")
print(f"FP32: {fp32_gap:.2e}")
print(f"FP16: {fp16_gap:.2e}")&lt;/code&gt;
    &lt;code&gt;Closest value BELOW 0.5:
FP32: 0.49999997019767761230
FP16: 0.49975585937500000000

Gap from threshold (0.5):
FP32: 2.98e-08
FP16: 2.44e-04&lt;/code&gt;
    &lt;p&gt;Taking this gap between 0.5 and the next representable number below 0.5 in FP16 we can calculate the threshold for values which will get rounded up to 0.5:&lt;/p&gt;
    &lt;code&gt;# Given the gap is 2.44e-04, we need to divide it by 2 and calculate the midpoint between 0.499755859375 and 0.5. This midpoint determines whether the FP16 value will be rounded down if below it or up it equal to or greater than. 

# Convert to FP32 as the midpoint is not representable in FP16
= np.float32(fp16_below)
 fp_16_below_fp32 
# Calculate the gap and midpoint
= 0.5 - fp16_below
 fp16_gap = fp_16_below_fp32 + (fp16_gap / 2)
 midpoint 
print(f"  Midpoint (rounding boundary): {midpoint:.15f}")&lt;/code&gt;
    &lt;code&gt;Midpoint: 0.499877929687500&lt;/code&gt;
    &lt;p&gt;Finally let’s see some examples of numbers being rounded up to 0.5 if they are above the midpoint between the representable values of FP16:&lt;/p&gt;
    &lt;code&gt;# Firstly, the midpoint itself is rounded up
0.499877929687500).item() -&amp;gt; 0.5
 np.float16(0.4999).item() -&amp;gt; 0.5
 np.float16(
# For completeness here's a number slightly smaller than the midpoint which gets rounded down
0.4998779296874).item() -&amp;gt; 0.499755859375 np.float16(&lt;/code&gt;
    &lt;p&gt;In short, any number between \([0.4998779296875, 0.5)\) will be rounded up to 0.5. This means, the predictions which were flipped were in this range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Does the Model Switch to FP16?&lt;/head&gt;
    &lt;p&gt;Now that we know what the issue is, it’s time to find out what caused it.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Bug or Intended Behaviour - It Must be a Bug Right…?&lt;/head&gt;
    &lt;p&gt;Originally I thought this behaviour was a bug with the ORT repo. Knowing the cast occured in the phase where ORT takes the ONNX model and converts it to a CoreML model my initial thinking was either ORT casts it to FP16 somewhere or calls CoreML with a hardcode FP16 flag or something similar.&lt;/p&gt;
    &lt;p&gt;Having little background in cpp, Claude came in useful here. I gave it the structure of the repo and it told me where I ought to place breakpoints to debug the ORT package (turns out you can debug a cpp package from python, clone the repo, build from src and then link it to your python code using the PID). However, upon running the code the breakpoints weren’t being hit. I was puzzled for a bit, but then I realised why the code wasn’t being hit. It turns out CoreML has two model formats “NeuralNetwork” and “MLProgram”, I will call them NN and MLP formats respectively. The behaviour of the ORT repo changes depending on which you want, as does the behaviour of CoreML, with the default being the NN format. So, the breakpoints weren’t hit as the code was regarding the MLP format whereas I was not setting this so the code flowed through the default NN code. Knowing this I took a step back and began experimenting with NN vs MLP format.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix - NeuralNetwork vs MLProgram CoreML Format&lt;/head&gt;
    &lt;p&gt;So, CoreML has two model formats, these represent how the model is stored and ran with CoreML. The NeuralNetwork (NN) format is older and the MLProgram (MLP) format is newer. ORT specifies NN format by default, but it does allow you to pass a flag to use MLP format.&lt;/p&gt;
    &lt;p&gt;Testing the MLP format revealed it as the solution! See below in figure 6 the final output, which includes both ORT MLP and NN format ran on the GPU.&lt;/p&gt;
    &lt;p&gt;So ORT on MPS with NN format has the same difference from the PyTorch CPU baseline as CoreML FP16, whereas ORT with MLP format matches - this is exactly what I wanted. Mystery solved! By setting the model format to be the newer MLProgram format no implicit cast to FP16 takes place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MLProgram Format Worked and Neural Network Didnt?&lt;/head&gt;
    &lt;p&gt;To understand the difference in behaviours of these two models formats we need to take a deep dive on the internals of CoreML, its goals and the two formats themselves. Let’s begin with CoreML.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML&lt;/head&gt;
    &lt;p&gt;ORT implements methods to convert the ONNX graph into CoreML model formats. CoreML has two types of model format, this defines how the model is represented in the CoreML framework, how it’s stored and how it’s ran. The older is the NeuralNetwork format and the newer one which solved our issue is the MLProgram format. The reason MLProgram keeps the model at FP32 when running on MPS is due to the differences in model representation in these two formats. Let’s take a look at both of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Neural Network Format&lt;/head&gt;
    &lt;p&gt;As stated, the NN format is the older one, it came out in 2017. It stored models as a Directed Acyclic Graph (DAG). Each layer in the model is a node in the DAG, and they encode information on layer type, list of input names, output names and a collection of parameters specific to the layer type7. We can observe the model which is created by ORT’s InferenceSession call with the following code:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir
= ort.InferenceSession(
 ort =["CoreMLExecutionProvider"]
     onnx_path, providers
 )= ort.run(None, {"input": numpy_input})[0]
 nn_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# NeuralNetwork models are .mlmodel files
= list(temp_dir.glob("*.mlmodel"))
 nn_files 
for model_path in nn_files:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;This outputs the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-40975D85-7412-4309-A6F7-4E51CA3D2FE8-7682-0000BF11C24C3150.model.mlmodel:
 specificationVersion: 4
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
neuralNetwork {
  layers {
    name: "node_linear"
    input: "input"
    output: "output"
    innerProduct {
      inputChannels: 4
      outputChannels: 3
      weights {
        floatValue: 0.0349225402
        floatValue: -0.301196814
        floatValue: 0.159211695
        floatValue: 0.156890273
        floatValue: -0.267238438
        floatValue: -0.0749385953
        floatValue: -0.292913973
        floatValue: 0.129736364
        floatValue: -0.134683847
        floatValue: 0.351268351
        floatValue: 0.354943156
        floatValue: 0.0509352088
      }
    }
  }
  arrayInputShapeMapping: EXACT_ARRAY_MAPPING
}&lt;/code&gt;
    &lt;p&gt;NeuralNetwork format has typed input and output to the model, but the nodes themselves are not typed. This is why the model gets cast to FP16, in the NN format the default behaviour is to run in FP16 on the MPS GPU. This quirk of the NN format is what threw off my results8. The CoreML runtime also specifies which parts of the model operators can run on which hardware9 and each hardware has different abilities in terms of what FP values it can handle with the NN format. Take a look at Figure 7 for Apple’s guide on the hardware and FP types they can handle:&lt;/p&gt;
    &lt;p&gt;When running on CPU the NN format model will run in FP32, as we observed. However, on GPU it is implicitly cast to FP16 even though the input and output are specified to be FP32 as you see in the inspection code above. This is an inherent limitation of the NN format. The DAG structure of the model does not store any information on the types of intermediate layers. You can see this in the inspection output, the part beginning neuralNetwork stored info on the actual layer node, in our case a single linear layer. Observe that there is no information on the FP precision of the node itself, hence CoreML implicitly sets it to FP16.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Does CoreML Implicitly Use FP16&lt;/head&gt;
    &lt;p&gt;From the typed execution docs for coremltools, the goal of CoreML is to run ML models in the most performant way and FP16 happens to be more performant than FP32 (which makes sense as it’s half the precision) on Apple GPUs. Also, they state that most of the time the reduced precision doesn’t matter for inference - this whole blog post shows why this is false and a pitfall of the NN format, the user should choose which precision the model is ran in, it should never be implicit.&lt;/p&gt;
    &lt;head rend="h5"&gt;MatMul Test - Is FP16 Faster on Apple Hardware?&lt;/head&gt;
    &lt;p&gt;To test Apple’s claim that FP16 is more performant on Apple hardware I carried out a large matmul. Taking a 16384x16384 matrix and multiplying it with another 16384x16384 matrix should show us if FP16 is faster. The size is arbitrary I just wanted something large.&lt;/p&gt;
    &lt;p&gt;The matmul was ran 10 times, in both FP32 and FP16 on the MPS hardware, and we take the average:&lt;/p&gt;
    &lt;code&gt;FP32 Average Time: 8.6521 seconds
FP16 Average Time: 6.7691 seconds

Speedup Factor: 1.28x faster&lt;/code&gt;
    &lt;p&gt;So FP16 is quicker, which sheds a bit of light on why the NN format has implicit casting to FP16, on paper if you only care about speed then it’s the better option.&lt;/p&gt;
    &lt;p&gt;Final point on the NeuralNetwork format, it’s surprising as the weights themselves are stored as FP32 values (a roundtrip test verifies this) but it still executes that layer in FP16, once again showing the NN format doesn’t respect the FP precision of the layer but just casts it to FP16.&lt;/p&gt;
    &lt;p&gt;All that is to say this, this was not a bug but rather an explicit design choice, which funnily enough involves implicitly going against what the user wants. The NN format has its downsides, which is why Apple introduced the MLProgram format, let’s look into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MLProgram (MLP) Format&lt;/head&gt;
    &lt;p&gt;The MLP format is the newer and better model format in CoreML, released in 2021, the core thing we care about is that the intermediate tensors are typed, i.e. there is no implicit casting when using the MLP format - the user controls whether the model is ran in FP16 or FP32.&lt;/p&gt;
    &lt;p&gt;MLP format allows for this as it uses a different representation of ML models, instead of a DAG it uses a programmatic representation of the models. By representing the model as code, it allows for greater control over the operations.&lt;/p&gt;
    &lt;p&gt;Let’s see what this looks like in the stored model format and how it differs to the NN format inspection.&lt;/p&gt;
    &lt;p&gt;The code to do so is pretty similar:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir. Also
# this time we specify the ModelFormat to be MLProgram 
= ort.InferenceSession(
 ort_mlp =[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]
     onnx_path, providers
 )= ort_mlp.run(None, {"input": numpy_input})[0]
 mlp_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# MLProgram models are in onnxruntime-* directories (not .mlmodelc)
= [d for d in temp_dir.glob("onnxruntime-*")
 mlp_dirs if d.is_dir() and not str(d).endswith('.mlmodelc')]
             
for model_path in mlp_dirs:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;The output of this is the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-752039B9-BA73-47E3-9ED4-AE029184DA69-9443-0000BF278CD8396E:
 specificationVersion: 8
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
mlProgram {
  version: 1
  functions {
    key: "main"
    value {
      inputs {
        name: "input"
        type {
          tensorType {
            dataType: FLOAT32
            rank: 2
            dimensions {
              constant {
                size: 1
              }
            }
            dimensions {
              constant {
                size: 4
              }
            }
          }
        }
      }
      opset: "CoreML7"
      block_specializations {
        key: "CoreML7"
        value {
          outputs: "output"
          operations {
            type: "const"
            outputs {
              name: "linear_weight"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                  dimensions {
                    constant {
                      size: 4
                    }
                  }
                }
              }
            }
            attributes {
              key: "val"
              value {
                type {
                  tensorType {
                    dataType: FLOAT32
                    rank: 2
                    dimensions {
                      constant {
                        size: 3
                      }
                    }
                    dimensions {
                      constant {
                        size: 4
                      }
                    }
                  }
                }
                blobFileValue {
                  fileName: "@model_path/weights/weight.bin"
                  offset: 64
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "linear_weight"
                    }
                  }
                }
              }
            }
          }
          operations {
            type: "linear"
            inputs {
              key: "x"
              value {
                arguments {
                  name: "input"
                }
              }
            }
            inputs {
              key: "weight"
              value {
                arguments {
                  name: "linear_weight"
                }
              }
            }
            outputs {
              name: "output"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 1
                    }
                  }
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "node_linear__0"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you see in the inspection of the MLP format model the linear layer is explicitly typed. To make it a bit easier to see let’s bring back the NeuralNetwork format inspection and compare the linear layer setup in both:&lt;/p&gt;
    &lt;head rend="h4"&gt;Linear Layer in NeuralNetwork &amp;amp; MLProgram Format&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;NeuralNetwork Linear Layer&lt;/cell&gt;
        &lt;cell role="head"&gt;MLProgram Linear Layer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;neuralNetwork { layers { name: "node_linear" input: "input" output: "output" innerProduct { inputChannels: 4 outputChannels: 3 weights { floatValue: 0.0349225402 floatValue: -0.301196814 floatValue: 0.159211695 floatValue: 0.156890273 floatValue: -0.267238438 floatValue: -0.0749385953 floatValue: -0.292913973 floatValue: 0.129736364 floatValue: -0.134683847 floatValue: 0.351268351 floatValue: 0.354943156 floatValue: 0.0509352088 } } }&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;operations { type: "linear" inputs { key: "x" value { arguments { name: "input" } } } inputs { key: "weight" value { arguments { name: "linear_weight" } } } outputs { name: "output" type { tensorType { dataType: FLOAT32 rank: 2 dimensions { constant { size: 1 } } dimensions { constant { size: 3 } } } } }&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Observe in the NN format, there is no explicit mention of the input or output type also the model weights are stored with the layer. Now, in the MLProgram layer, the output is explicitly typed as FP32. No more pesky implicit casting to FP16! This is one the big changes in MLProgram vs NN format, secondly notice how the layer weights are not stored along with the spec, they’re stored elsewhere. This aspect also makes the MLP format more efficient as the actual model spec is lighter.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Why Does MLProgram Have Typed Layers?&lt;/head&gt;
    &lt;p&gt;So we’ve come to the end of the journey, we found that NeuralNetwork format lacks types in the intermediate layers of the model and MLProgram doesn’t. So, setting ORT to use MLProgram keeps the model at FP32 and our output predictions remain the same when running in PyTorch and ORT. But why, why does NeuralNetwork not include types? Answering this requires a look into how ML models have been represented in the past and how this has evolved over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Choices, Design Choices - How Goals of ML Optimisation Evolved Over Time&lt;/head&gt;
    &lt;p&gt;When the NeuralNetwork format was released in 2017, it came into a much different environment than the one MLProgram was born into in 2021. The goal of NeuralNetwork was to act as a configuration file to be ran by hardware, as we saw above it defines the layers and the weights without much other info and lets the hardware figure out the rest. This is indicative of the trends in ML at the time, models were still being optimised so the added complexity wasn’t yet needed, the DAG representation worked well.&lt;/p&gt;
    &lt;p&gt;In essence, the NN format assumes that if the weights are stored in FP32, the input is FP32 and the output is too then the intermediate layers will also be FP32 - but as it doesn’t explicitly type these intermediate layers the hardware is free to choose and the Apple GPU chooses FP16 by default!&lt;/p&gt;
    &lt;p&gt;As time went on the demands in the ML world changed, these hardware based quirks became known, optimisations advanced and overall the industry moved away from the splintered (splintered in the sense that many frameworks implemented their own) config style DAGs and began to utilise learnings from the world of compilers&lt;/p&gt;
    &lt;head rend="h3"&gt;Changes From 2017 to 2021 Which Lead to Greater Adoption of Intermediate Representations&lt;/head&gt;
    &lt;p&gt;Firstly, for Apple specifically the hardware available expanded, now you have the CPU, GPU and ANE chips - making it very difficult to assume any given piece of hardware will run a specific FP type. Also, the lack of typing leads to other issues namely the compiler cannot make some optimisations, as they depend on knowing the types before runtime. Furthermore, things like mixed FP training and quantization became a thing, once again highlighting the need for explicit typing.&lt;/p&gt;
    &lt;p&gt;Lastly, in 2017 DAGs and other forms of model compilers were very fragmented and modern times have seen a push towards standardisation10, as the compiler community consolidated on tools like LLVM the ML community has too. Intermediate Representations(IR) began to be used in ML, an IR is a hardware agnostic specification of a program which a compiler can optimise. CoreML introduced their own IR, called MIL (Model Intermediate Language) and it implements the output we see in the stored MLProgram output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MIL Approach&lt;/head&gt;
    &lt;p&gt;MIL and IRs in general afford a lot of benefits. They are inherently designed for optimisation and by providing a general framework you can extract maximal value as all optimisation engineers can work on a common goal. In MIL specifically, some of the changes we’ve discussed between NN and MLProgram format, are implemented by it. Namely, each variable within the model has an explicit dtype.&lt;/p&gt;
    &lt;p&gt;Note, the MLProgram serialises and stores the output of the MIL phase, we’ve already observed how it differs to the the NeuralNetwork model, with the biggest difference being in the explicit types.&lt;/p&gt;
    &lt;head rend="h4"&gt;Further Reading on ML Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;head rend="h3"&gt;The Fix&lt;/head&gt;
    &lt;p&gt;The solution to all the issues we discussed today is, if you are using the CoreMLExecutionProvider in ORT then be sure to specify ModelFormat is MLProgram, this will ensure that whatever precision your model was trained it will be ran with that - which in my case was FP32 (whereas the default ModelFormat NeuralNetwork casts the model to FP16).&lt;/p&gt;
    &lt;p&gt;You can implement this as such:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;head rend="h3"&gt;The Cause&lt;/head&gt;
    &lt;p&gt;The issue was the differing model formats employed by CoreML to represent ML models. The NeuralNetwork format utilised a more historic DAG based approach which was developed during a time in which types and precision wasn’t a huge concern in the ML community and hardware decisions were left to the hardware. Whereas the MLProgram format used a programmatic approach, in which types are explicit letting the software influence how the model is run on the hardware.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lessons?&lt;/head&gt;
    &lt;p&gt;This whole thing taught me the importance of being thorough, it’s not acceptable to test your model in one setup and deploy it in another. We really need to test our model runs across all the platforms we intend to deploy to. Secondly, implicit defaults can be particularly damaging, in my case it wasn’t a huge issue but it easily could have been. Implicit defaults in this case also killed reproducibility, which can be problematic.&lt;/p&gt;
    &lt;p&gt;Lastly, I leave you with this:&lt;/p&gt;
    &lt;p&gt;1https://onnx.ai/onnx/intro/concepts.html&lt;/p&gt;
    &lt;p&gt;3https://developer.apple.com/documentation/coreml&lt;/p&gt;
    &lt;p&gt;4https://en.wikipedia.org/wiki/Single-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;5https://en.wikipedia.org/wiki/Half-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;6https://floating-point-gui.de/formats/fp/&lt;/p&gt;
    &lt;p&gt;7https://apple.github.io/coremltools/mlmodel/Format/NeuralNetwork.html&lt;/p&gt;
    &lt;p&gt;8https://apple.github.io/coremltools/docs-guides/source/typed-execution.html&lt;/p&gt;
    &lt;p&gt;9https://github.com/microsoft/onnxruntime/issues/21271#issuecomment-3637845056&lt;/p&gt;
    &lt;p&gt;10https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350075</guid><pubDate>Mon, 22 Dec 2025 00:27:04 +0000</pubDate></item><item><title>Build Android apps using Rust and Iced</title><link>https://github.com/ibaryshnikov/android-iced-example</link><description>&lt;doc fingerprint="3d82cc104505dfd0"&gt;
  &lt;main&gt;
    &lt;p&gt;There are NativeActivity and GameActivity examples here.&lt;/p&gt;
    &lt;p&gt;Based on several other examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;na-mainloop&lt;/code&gt;and&lt;code&gt;agdk-mainloop&lt;/code&gt;from android-activity&lt;/item&gt;
      &lt;item&gt;na-winit-wgpu from &lt;code&gt;rust-android-examples&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;integration from &lt;code&gt;iced&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;iced integration example&lt;/p&gt;
    &lt;p&gt;You can also run most of the examples from iced. For this omit the scene rendering part and set the background of the root container.&lt;/p&gt;
    &lt;p&gt;Text input partially works, unresolved issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;window doesn't resize on show/hide soft keyboard&lt;/item&gt;
      &lt;item&gt;how to change input language of soft keyboard&lt;/item&gt;
      &lt;item&gt;ime is not supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copy/paste and show/hide soft keyboard is implemented by calling Java&lt;/p&gt;
    &lt;p&gt;Check &lt;code&gt;android-activity&lt;/code&gt; crate for detailed instructions.
During my tests I was running the following command and using android studio afterwards:&lt;/p&gt;
    &lt;code&gt;export ANDROID_NDK_HOME="path/to/ndk"
export ANDROID_HOME="path/to/sdk"

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build&lt;/code&gt;
    &lt;p&gt;My setup is the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;archlinux 6.9.6&lt;/item&gt;
      &lt;item&gt;jdk-openjdk 22&lt;/item&gt;
      &lt;item&gt;target api 35&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to &lt;code&gt;android-activity&lt;/code&gt; we can already build android apps in Rust, and
key crates such as &lt;code&gt;winit&lt;/code&gt; and &lt;code&gt;wgpu&lt;/code&gt; also support building for android.
&lt;code&gt;iced&lt;/code&gt; doesn't support android out of the box, but it can be integrated with
existing graphics pipelines, as shown in
integration example.
As a result, it was possible to convert existing example running &lt;code&gt;winit&lt;/code&gt; + &lt;code&gt;wgpu&lt;/code&gt; to
use &lt;code&gt;iced&lt;/code&gt; on top.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350641</guid><pubDate>Mon, 22 Dec 2025 02:14:32 +0000</pubDate></item><item><title>Cursed circuits #3: true mathematics</title><link>https://lcamtuf.substack.com/p/cursed-circuits-3-true-mathematics</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46351345</guid><pubDate>Mon, 22 Dec 2025 04:34:36 +0000</pubDate></item><item><title>I announced my divorce on Instagram and then AI impersonated me</title><link>https://eiratansey.com/2025/12/20/i-announced-my-divorce-on-instagram-and-then-ai-impersonated-me/</link><description>&lt;doc fingerprint="6f1a115f08a28503"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;I announced my divorce on Instagram and then AI impersonated me&lt;/head&gt;
    &lt;p&gt;After maintaining a total stance of public silence for months, I recently publicly announced my unexpected divorce on Instagram. I shared a picture of the divorce cake that my friends got for me, and shared a brief essay I had drafted the day before about the news. I had to edit it down slightly from my original draft to fit Instagram’s character limits. You can see the Instagram post here. And for the record, here is the photo of the cake and what I wrote:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Two weeks before our ninth anniversary in April my husband told me he wanted a divorce. I was completely blindsided. Going through an unexpected divorce has been the most brutal, humiliating, and traumatizing process I’ve ever experienced. In an instant my life trajectory changed. The markers of security that I had clung to following the previous year’s burst appendix hospitalization and my Dad dying in the same hospital where I was being treated suddenly vanished. Being forced to sell our house, the address I lived in longer than any other place in my life because of my complicated childhood, was one of the most devastating parts of this hellish timeline.&lt;/p&gt;&lt;lb/&gt;I don’t know if I would have survived the last several months were it not for my friends, those from home or around the world, those I’ve known for decades and those I’ve only recently met in the process of beginning to rebuild my life. Having friends from all ages, backgrounds, and circumstances has been a lifesaver because it turns out a lot of people have also gone through similar traumas that turned their lives upside down, who have a lot of counsel and camaraderie to offer.&lt;lb/&gt;Enough people have received the memo about divorce etiquette to ask whether a person’s divorce is a “congratulations” or “sorry” situation. Until recently the latter has been more applicable, but now that the state has decreed that my divorce is official I mostly feel a sense of overwhelming relief tempering my deep grief, and trying to stay focused on what’s next.&lt;lb/&gt;A divorce you don’t see coming really does a number on your sense of worth and identity. I did not choose to end my marriage, but I am humbled and struck by how many choices I get to make about my future. I am spending a lot of time thinking about the people and places, principles and pleasures that I want to prioritize in this next phase of my life.&lt;lb/&gt;Over the last several months I’ve been grateful for my friends’ unwavering belief that things will be better on the other side of this journey. And who I am to argue with my friends? They are smart and I think they love me and I definitely love them. So with that, I’m officially in my hot divorcee era. WATCH THIS SPACE.&lt;/quote&gt;
    &lt;p&gt;I hit post, and cross-posted it to Facebook. I also shared the Instagram link on glammr.us (the main Mastodon server that caters to GLAM workers), which is where I took a lot of my post-Twitter energy a few years ago.&lt;/p&gt;
    &lt;p&gt;A few days after posting, I was looking at the Mastodon app, Toot, on my phone (yes, all of the terminology around Mastodon is that embarrassing). And I noticed something I definitely did not remember writing. In the app, all the original text from my post was pulled in along with some additional text following my sign-off line of WATCH THIS SPACE that I certainly didn’t author. It also sounded very AI-generated. Here’s what I saw:&lt;/p&gt;
    &lt;p&gt;Here’s the AI-generated text I didn’t write:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Going through a divorce can be a life-altering experience that leaves you feeling lost and uncertain about your future. In this post, I share my personal journey of self-discovery and rebuilding after a sudden divorce.&lt;/p&gt;&lt;lb/&gt;From finding support from friends to prioritizing what matters most, I explore the themes of identity, worth, and finding happiness after divorce.&lt;lb/&gt;Whether you’re navigating a divorce or simply looking for inspiration, this post is for you. Follow along as I share my story and insights on rebuilding, self-care, and finding happiness after divorce.&lt;/quote&gt;
    &lt;p&gt;After talking with my friend Ruth (one of the glammr.us admins and a much smarter tech person than myself) and finding this recent story from 404 Media, I realized that somehow my post had been targeted for Instagram’s AI-generated SEO bait. If you go to the original Instagram post and view the page source, you’ll see that there is both the description text from above, along with a whole bevvy of keywords like “self-discovery” and “finding happiness.”&lt;/p&gt;
    &lt;p&gt;While I am sure buried deep in some EULA there is some bullshit allowing Meta to get away with this, I was never explicitly informed about this possibility or asked to review this AI-generated text. Meanwhile, as of today when I look at Instagram’s posting settings, I see that there is a button that requires you to label any AI-generated content you post as the end user, but obviously there is no such obligation in the other direction.&lt;/p&gt;
    &lt;p&gt;One of the things I gave up long ago about being an extremely online woman who is a micro-public figure with a weird name are any assumptions about privacy. It’s why I drafted the brief essay for my announcement in advance, because I know that once I hit post on anything on the internet I can never unring that bell. I have been writing in the public sphere long enough to know that things that I write are routinely misrepresented by others (a lesson I relearn every time I check some of the citations of my work via Google Scholar updates).&lt;/p&gt;
    &lt;p&gt;But what I vehemently object to in this situation is the use of the first-person voice without my review or permission. The language used in the description makes it sound as if I wrote it (“In this post, I share my personal journey…”). Because I have fiercely protected my authorship throughout my life and what my name is attached to, any generative AI writing that purports to be in my voice without my informed consent is a profound violation of my authorial voice, agency, and frankly it feels like fraud or impersonation. As an archivist who has spent almost twenty years thinking about accuracy in information, it makes my skin crawl that there is a metadata field with the sole purpose of generating SEO-engagement purporting to be my voice that doesn’t disclose the authorship was actually non-consensual AI.&lt;/p&gt;
    &lt;p&gt;In recent days I’ve noticed this kind of “some people are saying” text showing up in my search results, especially related to Reddit threads. For example, a list of Google search results showing Reddit threads might show some description like “Some users are discussing the challenges of making a mug cake that doesn’t taste rubbery.” But even if this description had been in the third-person voice such as “This woman discusses her divorce and its impact on her….” my anger would remain.&lt;/p&gt;
    &lt;p&gt;Because what this AI-generated SEO slop formed from an extremely vulnerable and honest place shows is that women’s pain is still not taken seriously. The tone of the post is laden in the absolute worst therapy/wellness culture cliches (“Follow along as I share my story and insights on rebuilding, self-care, and finding happiness after divorce”) which is language that I abhor since it often trivializes women’s pain. I also object to the flattening of the contours of my particular divorce. There are really important distinctions between the experiences of women who initiate their own divorces versus women who come to a mutual agreement with their spouses to end the marriage versus women, like me, who are completely blindsided by their husbands’ decisions to suddenly end the marriage. All divorces do involve self-discovery and rebuilding your life, but the ways in which you begin down that path often involve dramatically different circumstances.&lt;/p&gt;
    &lt;p&gt;My story is absolutely layered through with trauma, humiliation, and sudden financial insecurity and I truly resent that this AI-generated garbage erases the deliberately uncomfortable and provocative words I chose to include in my original framing. If my story ends up becoming inspiring to others, that’s great, but that’s not why I share my pain on the internet. I share my pain publicly as a gesture of solidarity with other people, but especially women, who have been profoundly traumatized by those they thought they could love and trust. Having my pain witnessed and acknowledged is part of how I am healing while also letting other people know they are not alone. I have to externalize my pain in regular intervals right now because if I internalize it any more it might actually destroy me. Forcing an “inspiration” vibe onto what I posted feels tone deaf at best, and like toxic positivity at worst.&lt;/p&gt;
    &lt;p&gt;We already know that in a patriarchal society, women’s pain is dismissed, belittled, and ignored. This kind of AI-generated language also depoliticizes patriarchal power dynamics. Thinkers like Dr. Safiya Noble have been warning us for years that the prejudices carried by the people who design and maintain internet infrastructure continually shapes how we view and think about women and girls in society. I already felt immense pain and anger by the decision of my husband to suddenly end our marriage. And now I feel a double sense of violation that the men who design and maintain and profit from the internet have literally impersonated my voice behind the closed doors of hidden metadata to tell a more palatable version of the story they think will sell.&lt;/p&gt;
    &lt;p&gt;Categorised as: Uncategorized&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352004</guid><pubDate>Mon, 22 Dec 2025 07:13:33 +0000</pubDate></item><item><title>QBasic64 Phoenix 4.3.0 Released</title><link>https://qb64phoenix.com/forum/showthread.php?tid=4244</link><description>&lt;doc fingerprint="7600925a85c73bd8"&gt;
  &lt;main&gt;
    &lt;p&gt; 10 hours ago &lt;/p&gt;
    &lt;p&gt; Download from GitHub&lt;lb/&gt;Enhancements&lt;lb/&gt;- #647, #648, #649, #650, #652 - Implements the $USELIBRARY meta-command and functionality - @RhoSigma-QB64&lt;lb/&gt;- For use of our brand new QB64-PE Libraries Pack add-on.&lt;lb/&gt;- The add-on is optional to ease library usage, but the traditional way of $INCLUDEing libraries still works without changes.&lt;lb/&gt;- #651 - Implements three new IDE editing features - @FellippeHeitor&lt;lb/&gt;- Auto-closing of brackets and quotes.&lt;lb/&gt;- Ctrl+D for line/selection block duplication (Edit -&amp;gt; Duplicate lines).&lt;lb/&gt;- Alt+Shift+Up and Alt+Shift+Down to move lines/selection blocks up and down.&lt;lb/&gt;- #662 - Implements a feature request by @Bhsdfa to define another EXE output folder, this closes issue #661 - @RhoSigma-QB64&lt;lb/&gt;Library Updates&lt;lb/&gt;- #645, #654 - Update and refactoring of various libraries - @a740g&lt;lb/&gt;- Replaces `image_log_error` with `image_log_warn` to reduce unnecessary error-level log output.&lt;lb/&gt;- Reorders image format loading sequence to: `stb_image`, `sg_pcx`, `qoi`, `sg_curico`, and finally `nanosvg`.&lt;lb/&gt;- Updates `nanosvg`, `miniaudio`, `libcurl`, `FreeType`, and `clip` to their latest versions. These updates include important bug fixes and security patches.&lt;lb/&gt;- Removes obsolete logging code in `libqb`, now superseded by the recently added logging system.&lt;lb/&gt;- Switches the build to `C++20`, preparing for upcoming features that will rely on the newer standard.&lt;lb/&gt;- #655 - Further relaxing ALIAS name validation, building on the work introduced in #535 - @a740g&lt;lb/&gt;- The previous update did not account for several critical C++ operators, which are essential for leveraging the underlying C++ compiler in more advanced and novel ways. By including these operators, developers can now use techniques that enable direct use of the C++ compiler without the need for additional header libraries.&lt;lb/&gt;Bug Fixes&lt;lb/&gt;- #615 - Fixed a rather subtle IDE module error - @RhoSigma-QB64&lt;lb/&gt;- Reported at Discord&lt;lb/&gt;- If there was a program with a line count close below the next 1000s boundary and a search was performed, then if the found position was close to the program end and the IDE window height was bigger than the remaining program lines after the found match, then empty line padding could lead to a "subscribt out of range" error in an internal array in case the number of empty padding lines was bigger than the difference of the next 1000s boundary minus total program lines.&lt;lb/&gt;- #657 - Fixes a bug in the font library function `UTF32::ConvertUTF16()` - @a740g&lt;lb/&gt;- The function incorrectly attempted to consume a UTF-16 BOM even when none was present.&lt;lb/&gt;Full Changelog: https://github.com/QB64-Phoenix-Edition/...0...v4.3.0&lt;/p&gt;
    &lt;p&gt;Enhancements&lt;/p&gt;
    &lt;p&gt;- #647, #648, #649, #650, #652 - Implements the $USELIBRARY meta-command and functionality - @RhoSigma-QB64&lt;/p&gt;
    &lt;p&gt;- For use of our brand new QB64-PE Libraries Pack add-on.&lt;/p&gt;
    &lt;p&gt;- The add-on is optional to ease library usage, but the traditional way of $INCLUDEing libraries still works without changes.&lt;/p&gt;
    &lt;p&gt;- #651 - Implements three new IDE editing features - @FellippeHeitor&lt;/p&gt;
    &lt;p&gt;- Auto-closing of brackets and quotes.&lt;/p&gt;
    &lt;p&gt;- Ctrl+D for line/selection block duplication (Edit -&amp;gt; Duplicate lines).&lt;/p&gt;
    &lt;p&gt;- Alt+Shift+Up and Alt+Shift+Down to move lines/selection blocks up and down.&lt;/p&gt;
    &lt;p&gt;- #662 - Implements a feature request by @Bhsdfa to define another EXE output folder, this closes issue #661 - @RhoSigma-QB64&lt;/p&gt;
    &lt;p&gt;Code: (Select All)&lt;/p&gt;
    &lt;code&gt;    Where is my EXE saved?&lt;lb/&gt;
    Master behavior determined by IDE config as follows:&lt;lb/&gt;
    |&lt;lb/&gt;
    +-&amp;gt; Save EXE to Source Folder?&lt;lb/&gt;
          |        |&lt;lb/&gt;
        OFF        ON&lt;lb/&gt;
          |        |&lt;lb/&gt;
          |        +-&amp;gt; EXE goes to source folder, except new unsaved code,&lt;lb/&gt;
          |            which goes to default location (no known source&lt;lb/&gt;
          |            path yet), hence always save new code before first&lt;lb/&gt;
          |            compiling to avoid the default location fallback.&lt;lb/&gt;
          |&lt;lb/&gt;
          +-&amp;gt; Default EXE location set?    (IDE Run menu &amp;gt; Set Default EXE Folder...)&lt;lb/&gt;
                |              |&lt;lb/&gt;
              YES              NO&lt;lb/&gt;
                |              |&lt;lb/&gt;
                |              +-&amp;gt; EXE goes to 'qb64pe' folder.&lt;lb/&gt;
                |&lt;lb/&gt;
                +-&amp;gt; EXE goes to the user chosen location if existing,&lt;lb/&gt;
                    else fallback to 'qb64pe' folder. The exist check is&lt;lb/&gt;
                    performed once at start time, hence a removable USB&lt;lb/&gt;
                    drive or virtual RAM DISK must be accessible at start&lt;lb/&gt;
                    of QB64-PE. However, the fallback is temporary, if the&lt;lb/&gt;
                    user location is accessible at next start, then it is&lt;lb/&gt;
                    used again.&lt;lb/&gt;
&lt;lb/&gt;
      Command line compiling follows the logic above, except if the '-o'&lt;lb/&gt;
      option is used, in fact the only method to override this logic.&lt;lb/&gt;
      A given absolute path here will save the EXE exactly there, any&lt;lb/&gt;
      relative path assumes _STARTDIR$ as root.&lt;lb/&gt;
&lt;lb/&gt;
      To change the default location from command line use the new switch:&lt;lb/&gt;
        -s:ExeDefaultDir=&amp;lt;path&amp;gt;  (quotes may be required)&lt;lb/&gt;
      The path may be absolute or relative assuming _STARTDIR$ as root.&lt;lb/&gt;
      Note the new location is saved and will be used by the IDE too then.&lt;lb/&gt;
   &lt;/code&gt;
    &lt;p&gt;Library Updates&lt;/p&gt;
    &lt;p&gt;- #645, #654 - Update and refactoring of various libraries - @a740g&lt;/p&gt;
    &lt;p&gt;- Replaces `image_log_error` with `image_log_warn` to reduce unnecessary error-level log output.&lt;/p&gt;
    &lt;p&gt;- Reorders image format loading sequence to: `stb_image`, `sg_pcx`, `qoi`, `sg_curico`, and finally `nanosvg`.&lt;/p&gt;
    &lt;p&gt;- Updates `nanosvg`, `miniaudio`, `libcurl`, `FreeType`, and `clip` to their latest versions. These updates include important bug fixes and security patches.&lt;/p&gt;
    &lt;p&gt;- Removes obsolete logging code in `libqb`, now superseded by the recently added logging system.&lt;/p&gt;
    &lt;p&gt;- Switches the build to `C++20`, preparing for upcoming features that will rely on the newer standard.&lt;/p&gt;
    &lt;p&gt;- #655 - Further relaxing ALIAS name validation, building on the work introduced in #535 - @a740g&lt;/p&gt;
    &lt;p&gt;- The previous update did not account for several critical C++ operators, which are essential for leveraging the underlying C++ compiler in more advanced and novel ways. By including these operators, developers can now use techniques that enable direct use of the C++ compiler without the need for additional header libraries.&lt;/p&gt;
    &lt;p&gt;Bug Fixes&lt;/p&gt;
    &lt;p&gt;- #615 - Fixed a rather subtle IDE module error - @RhoSigma-QB64&lt;/p&gt;
    &lt;p&gt;- Reported at Discord&lt;/p&gt;
    &lt;p&gt;- If there was a program with a line count close below the next 1000s boundary and a search was performed, then if the found position was close to the program end and the IDE window height was bigger than the remaining program lines after the found match, then empty line padding could lead to a "subscribt out of range" error in an internal array in case the number of empty padding lines was bigger than the difference of the next 1000s boundary minus total program lines.&lt;/p&gt;
    &lt;p&gt;- #657 - Fixes a bug in the font library function `UTF32::ConvertUTF16()` - @a740g&lt;/p&gt;
    &lt;p&gt;- The function incorrectly attempted to consume a UTF-16 BOM even when none was present.&lt;/p&gt;
    &lt;p&gt;Full Changelog: https://github.com/QB64-Phoenix-Edition/...0...v4.3.0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352047</guid><pubDate>Mon, 22 Dec 2025 07:25:43 +0000</pubDate></item><item><title>Inverse Parentheses</title><link>https://kellett.im/a/inverse-parentheses</link><description>&lt;doc fingerprint="1f1817d1eda55258"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you ever noticed that lots of programming languages let you use parentheses to group operands, but none use them to ungroup them? No? Well letâs pretend this is a normal thing to be thinking about, and see what we can do about it.&lt;/p&gt;
    &lt;p&gt;Grouping with parentheses is relatively easy to add to a language grammar. The rule that accepts atomic things like &lt;code&gt;37&lt;/code&gt; simply needs to also accept an opening paren,1 at which point it will recursively parse an entire expression, and then eat a closing paren.&lt;/p&gt;
    &lt;code&gt;def parse_atom(lex):
    r = next(lex)
    if r[0] == 'integer':
        return int(r[1])
    elif r[0] == '(':
        expr = parse_expr(lex)
        s = next(lex)
        if s[0] != ')':
            raise ParseError("missing close paren")
        return expr
    else:
        raise ParseError(f"unexpected {r[0]}")
&lt;/code&gt;
    &lt;p&gt;Anti-grouping isnât quite as straightforward. Our parser canât follow the structure of the parentheses, because then it wouldnât be following the structure of the expressionâthe whole point is that these are dissimilar.&lt;/p&gt;
    &lt;p&gt;I donât know if itâs possible to write a pure parser that does this. But purity is overrated anyway. I decided to take inspiration from another language with a weird grouping system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python&lt;/head&gt;
    &lt;p&gt;Did you know that Pythonâs grammar has braces? You just donât type them. The tokeniser3 keeps track of the indentation level and inserts special tokens when it changes. The parser itself doesnât need to worry about counting whitespace; it just sees blocks of statements bracketed by &lt;code&gt;INDENT&lt;/code&gt; and &lt;code&gt;DEDENT&lt;/code&gt;,4 which are easy to parse.&lt;/p&gt;
    &lt;p&gt;As it happens, Pythonâs tokeniser also knows when itâs inside a parenthesised expression. Indentation inside parens is not significant, and this is implemented by tracking the paren nesting depth and suppressing &lt;code&gt;INDENT&lt;/code&gt; and &lt;code&gt;DEDENT&lt;/code&gt; while itâs non-zero.&lt;/p&gt;
    &lt;p&gt;What if we used the same trick? Instead of trying to do all this in the parser somehow, the tokeniser could track its nesting depth, and emit a âfriendlinessâ score for each token. Then we can simply parse operators in ascending order of friendliness.&lt;/p&gt;
    &lt;p&gt;In this model &lt;code&gt;1 + (2 * 3)&lt;/code&gt; will yield the following token stream:&lt;/p&gt;
    &lt;code&gt;1
+ (0)
(
2
* (-1)
3
)
&lt;/code&gt;
    &lt;p&gt;Weâll leave the parentheses in the token stream, but all the parser needs to do with them is generate a syntax error if it finds one in the wrong place. Grouping will be handled entirely by the precedence levels embedded in the token stream.5&lt;/p&gt;
    &lt;head rend="h2"&gt;A not-so-infinite climb&lt;/head&gt;
    &lt;p&gt;The tokeniser hack solves our parsing problem, but it creates another one: our language now has infinitely many precedence levels. I donât feel like trying to do that with handrolled recursive descent, but a rummage through school textbooks suggests a precedence climbing parser is what we need. It deals with operators in the order it meets them, so having infinitely many possible precedences wonât bother it.&lt;/p&gt;
    &lt;p&gt;I hacked this together and itâs appropriately silly:&lt;/p&gt;
    &lt;code&gt;&amp;gt; (1 + 2) * 3
1 + 2 * 3
&amp;gt; 1 + (2 * 3)
(1 + 2) * 3
&lt;/code&gt;
    &lt;p&gt;Something I particularly enjoy about the implementation I landed on is that if you increase friendliness instead of decreasing it, you end up with an ordinary6 parser. Itâs also a good platform for other questionable syntactic innovations, like a language with no parentheses at all, using whitespace to weaken binding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;While weâve achieved a lot here today,7 weâve also raised some important new questions. For instance, is it always necessary to double-parenthesise expressions in more complex cases?&lt;/p&gt;
    &lt;code&gt;&amp;gt; ((1 * 2)) + (3 * 4)
1 * ((2 + 3) * 4)
&lt;/code&gt;
    &lt;p&gt;And is it possible to have an anti-grouping parser that gives an involution when hooked up to an ordinary printer?&lt;/p&gt;
    &lt;p&gt;These are promising avenues for deeper study, and Iâd love to hear from anyone who chooses to take them on.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Does it bother anyone else that âparenthesesâ (meaning round brackets) doesnât seem to have a satisfactory2 singular form? ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To me, âparenthesisâ sounds like a pair of parentheses, possibly together with the text they bracket, rather than a single paren. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Also called a lexer or occasionally a scanner. Iâm going to stick with âtokeniserâ because it feels the most like a real word. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sadly these are just internal names and not spellings, so you canât use them to write exciting one-liners. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I have ignored the order of operations in this post to keep things simple, but my implementation does handle it. Operators are looked up in a table and their ânaturalâ precedence is paired with their friendliness, so in fact the precedences of&lt;/p&gt;&lt;code&gt;+&lt;/code&gt;and&lt;code&gt;*&lt;/code&gt;from the example would be (0,0) and (-1,1) respectively. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Outwardly. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually early 2024, but it took me until today to get around to writing it up. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352248</guid><pubDate>Mon, 22 Dec 2025 08:29:03 +0000</pubDate></item><item><title>Perl articles are being memory wiped from Wikipedia</title><link>https://old.reddit.com/r/perl/comments/1psj81k/perlmonks_is_being_memory_wiped_on_https_and/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352498</guid><pubDate>Mon, 22 Dec 2025 09:20:26 +0000</pubDate></item><item><title>The ancient monuments saluting the winter solstice</title><link>https://www.bbc.com/culture/article/20251219-the-ancient-monuments-saluting-the-winter-solstice</link><description>&lt;doc fingerprint="bd08521d25e3a112"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'It's a moment of death and rebirth': The ancient monuments saluting the winter solstice&lt;/head&gt;
    &lt;p&gt;Dozens of mysterious structures across the Northern Hemisphere – some nearly 5,000 years old – align precisely to frame the rising and setting Sun during midwinter's shortest day. What motivated people to construct these solar-calibrated masterpieces?&lt;/p&gt;
    &lt;p&gt;The winter solstice, which usually falls on 21 or 22 December in the Northern Hemisphere each year, marks the moment that one yearly cycle comes to an end and another is born. It is the day with the smallest number of sunlight hours in the calendar, and once it's over, the days lengthen again incrementally until the summer solstice in June.&lt;/p&gt;
    &lt;p&gt;The significance of this day is manifested in ancient monuments that were designed to acknowledge and celebrate its passing. One example is Maeshowe tomb in Orkney. To the untrained eye this burial cairn, created around 2800BC, looks like a grassy hillock – but it conceals a cuboid, stone-clad sepulchre and a 33ft (10m) long entry corridor oriented to the south-west. During midwinter, three weeks either side of the winter solstice, the setting Sun aims directly down the corridor and emanates its light into the tomb.&lt;/p&gt;
    &lt;p&gt;When the sky is cloudless, the light seems to carve a golden aperture into the tomb's rear wall – a sacrament of pure light. These days of radiance are interrupted by the solstice itself, when blackness temporarily takes over. But daylight reappears soon after, to blaze for another few days as if in celebration of the promise of nature's rejuvenation in spring.&lt;/p&gt;
    &lt;p&gt;We will probably never know the specific beliefs and rituals that inspired Maeshowe tomb. But it's nonetheless possible to understand the enormous significance of the winter solstice as the "year's midnight", both as the darkest moment in the calendar and the pivot to six future months of greater illumination. It was a moment of death and rebirth, and a reminder of the cyclical nature of time.&lt;/p&gt;
    &lt;p&gt;In the deep past, understanding the markers of nature's clockwork – including solstices – was a matter of survival. Predicting the recurrent patterns of animal migration, for example, could help successful hunting and fishing. Knowing when the climate was likely to change meant being able to adapt and survive. In pre-agricultural societies, it helped people anticipate the availability and location of edible roots, nuts and plants.&lt;/p&gt;
    &lt;p&gt;After the introduction of farming, around 9000BC, it was essential – for successful planting and harvesting – to anticipate the timing of seasonal changes. Monuments that calculated time had practical value, but it's likely that they also embodied spiritual beliefs in Neolithic times too, with the winter solstice being of particular importance. This very ancient recognition of the solstice's significance even echoes through to the modern world. The word "Yule", now associated with the winter holiday period, derives from the historic Norse festival of Jól, which was based around the winter solstice. Modern Christmas traditions recall bygone midwinter celebrations like the Roman holiday of Saturnalia, which involved feasting and gift-giving. And the solstice continues to be acknowledged in hundreds of traditions across the world, such as the Inca celebration of Inti Raymi, and the Dōngzhì festival in China.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Nature's sublime power'&lt;/head&gt;
    &lt;p&gt;Alongside Maeshowe tomb, archaeologists have discovered dozens of Neolithic monuments that stare directly at the Sun on the winter solstice. There's Stonehenge (England), whose tallest trilithon once framed the setting sun; Newgrange (Ireland), which has a passageway aligned to sunrise on this auspicious day; and the standing stones at Callanish (Outer Hebrides) which create similar solar sightlines. In Brittany, north-western France, is La Roche aux Fées: a megalithic passageway constructed from 41 blocks of stone, some of which weigh over 40 tonnes (40,000kg). At sunrise on the winter solstice, it breathes in its annual dose of restorative midwinter light. Legends once told that fairies constructed it over the course of one night, but it is actually a dolmen (tomb) created by Neolithic architects around 2750BC.&lt;/p&gt;
    &lt;p&gt;In the 20th and 21st Centuries there has been a resurgence of Neolithic-inspired solar-oriented artworks. Nancy Holt's seminal land art piece, Sun Tunnels (1973-76) is one example, set in the Great Basin Desert of Utah, and comprising of four 22-tonne (22,000kg) concrete tubes arranged in an X-shape formation. The view down each of them perfectly frames the Sun as it rises and sets on the winter and summer solstices. Holt bought the land in 1975 and created her artwork with the help of engineers, an astrophysicist, an astronomer and a team of contractors.&lt;/p&gt;
    &lt;p&gt;It's best to understand Sun Tunnels in the context of the Land Art movement of the 1960s and 70s. Artists like Holt who are associated with this movement worked with the landscape rather than within traditional studios and galleries, and aimed to reconnect people with the awe of nature. Unlike its Neolithic predecessors, Sun Tunnels has no religious significance – Holt explained that she simply wanted "to bring the vast space of the desert back to human scale". It is also a response to modern concerns about nature. In an age where humans seem hell-bent on despoiling and exploiting nature, Sun Tunnels turns our attention back to its sublime power and rhythmic patterns.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• Seven of the greatest rivalries in art history&lt;/p&gt;
    &lt;p&gt;Another masterpiece of Land Art, James Turrell's Roden Crater (begun 1979), does this on an even more epic scale than Sun Tunnels. It occupies a volcanic cinder cone in the Painted Desert region of northern Arizona and houses multiple spaces from which to watch celestial phenomena. One of them is a 900ft- (274m) long tunnel drilled through the volcanic cone. It acts like a camera obscura, focusing an image of the midwinter sun (via a glass lens halfway down the passage) on to a slab of white marble in a central chamber. Like Maeshowe tomb's passage, it aligns with the Sun's position around 21 December each year, and drinks down the Sun's light from 10 days before the solstice to the 10th day after it.&lt;/p&gt;
    &lt;p&gt;Enoura Observatory in Kanagawa Prefecture, Japan (completed 2017) was designed by photographer and architect Hiroshi Sugimoto. Its various buildings are all calibrated towards the movement of the Sun, to create what the artist describes as a "new Neolithic aesthetic". He wanted to correct what he saw as a lack of purpose in contemporary art by exploring the primal concerns of our ancient ancestors – our status within the infinite wilderness of the cosmos, our sense of time, and our notion of a human identity within the natural order.&lt;/p&gt;
    &lt;p&gt;One of its structures, the "Winter Solstice Light-Worship Tunnel", points directly at the spot on the horizon where the Sun rises at about 06:48 local time on 21 December each year. The solstice sunlight floods this 230ft-(70m) long chamber made of Corten steel and illuminates a stone medieval wellhead that is situated halfway along its length. It passes underneath another structure which aligns with the Sun on the summer solstice. The entire site, which took a decade to build, was intended by Sugimoto to act like a living clock, and to make an artwork with the ancient function of helping humans "identify their place within the vastness of the universe".&lt;/p&gt;
    &lt;p&gt;Holt's, Turrell's and Sugimoto's structures put us back in contact with seasonal patterns and the rhythms of nature, just as Maeshowe tomb and La Roche aux Fées once did. These monuments and artworks orient us in time, to the landscape, to our place within nature and to reoccurring celestial events. The winter solstice – which they all respond to directly – has always been of critical importance to humans, enshrining the significance of light, and honouring death and rebirth in the annual calendar. If the spectacle of these solar-aligned structures lining up perfectly with the rising and setting solstice Sun quickens the soul, it's because it triggers a primal recognition that the darkest hours of the year have passed. It's the first sign of spring's promised return, and future days of increased lightness and warmth.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for the Essential List newsletter – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352565</guid><pubDate>Mon, 22 Dec 2025 09:30:58 +0000</pubDate></item></channel></rss>