<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 28 Sep 2025 19:07:19 +0000</lastBuildDate><item><title>IBM Intellistation 185 AIX workstation (2016)</title><link>http://www.ibmfiles.com/pages/intellipower185.htm</link><description>&lt;doc fingerprint="b78ab9122b85b98d"&gt;
  &lt;main&gt;&lt;p&gt;IBM IntelliStation POWER 185&lt;/p&gt;&lt;p&gt;System Type: 7047&lt;/p&gt;&lt;p&gt;Released in 2006 after the 285, this system used the PowerPC 970 processors (sometimes erroneously categorized under POWER5 by 3rd parties and at IBM--it would technically be closer to a POWER4). Despite POWER5+ and DDR2 already being used on the 285, the 185 took a few steps back and used DDR1 and didn't bother with the POWER chips. While this may seem rather strange, it can be explained with System p5 185 (7037-A50). Perhaps IBM wanted to create a machine of their own with the PPC 970 or it was stuck in developmental hell for a bit, whatever the case IBM used the 7037 guts to be modified into the IntelliStation 185. This move could have been done to offer a workstation that used less power and was quieter / while still maintaining AIX compatibility. Hundreds went to use in government offices and engineering firms; as a result they mainly show up in the U.S. and Japan.&lt;/p&gt;&lt;p&gt;Most of the POWER and AIX people don't even know that IBM badged the 970 under POWER and popped it into a workstation; therefore making the IntelliStation "POWER" 185 a source of confusion, horror and intrigue--and the fact it's relatively unknown means you'll get different responses as to what it can and cannot do. It is still a POWER system in firmware, and supports hardware virtualization like any other POWER system, which makes it completely different from a Power Mac G5 which uses the same CPU. If it didn't support hardware virtualization IBM wouldn't have been able to call the original System p5 185 a System p5... it would just be a plain PowerPC box. But this wasn't too unfamiliar as other AIX systems in the past also leveraged PowerPC chips like RS/6000s and PowerPC ThinkPads. I suspect the whole reason why the PowerPC 970MP had virtualization in the first place was because of IBM's exact requirement for this on their System p5 that they were intending to use it in (which Apple never did on the G5 as far as I know), and then of course Apple wanted AltiVec (which IBM didn't use in AIX as far as I know, but that doesn't mean you couldn't write AIX software to leverage it).&lt;/p&gt;&lt;p&gt;Downloads&lt;/p&gt;&lt;p&gt;--&amp;gt; Latest System Firmware (AT071-156, RPM package) - 05/11/2007&lt;/p&gt;&lt;p&gt;Due to IBM's licensing agreements I am not entitled to distribute the firmware update as it's licensed code. Therefore the above link redirects to IBM's legacy firmware page where it can be downloaded. If the firmware ever gets pulled from the website I will consider mirroring it.&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM IntelliStation POWER 185 Hardware Announcement - February 14, 2006&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM IntelliStation POWER 185 RedBook Overview&lt;/p&gt;&lt;p&gt;--&amp;gt; Quick Start Guide for IntelliStation POWER 185&lt;/p&gt;&lt;p&gt;--&amp;gt; Installing SUSE Linux Enterprise Server 9 SP3 on POWER 185&lt;/p&gt;&lt;p&gt;--&amp;gt; Installing Red Hat Enterprise Linux AS 4 Update 3 on POWER 185&lt;/p&gt;&lt;p&gt;--&amp;gt; TAGITT-CATIA V4/ENOVIA DMU Evaluation&lt;/p&gt;&lt;p&gt;---&lt;/p&gt;&lt;p&gt;--&amp;gt; UNIX Workstations Facts and Features (2006)&lt;/p&gt;&lt;p&gt;--&amp;gt; System i &amp;amp; p PCI adapters (44 MB!)&lt;/p&gt;&lt;p&gt;--&amp;gt; System i &amp;amp; p fans&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM IntelliStation POWER and IBM RS/6000 Graphics Performance Report&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM Installation Toolkit 4.2 Release Notes&lt;/p&gt;&lt;p&gt;--&amp;gt; IBM Installation Toolkit 4.2 User's Manual&lt;/p&gt;&lt;p&gt;--&amp;gt; RS/6000 SpaceBall Announcement on AIX&lt;/p&gt;&lt;p&gt;---&lt;/p&gt;&lt;p&gt;--&amp;gt; 3Dconnexion 3DxWare AIX Driver (1.6.0 11/29/2011)&lt;/p&gt;&lt;p&gt;--&amp;gt; 3Dconnexion Xdriver AIX 5+ (4.66 11/18/2005)&lt;/p&gt;&lt;p&gt;Note: it seems that the 3DxWare driver should support the SpaceBall and SpaceMouse, along with offering new support for the SpacePilot and SpaceExplorer on AIX. However if that's not the case the older Xdriver can be used.&lt;/p&gt;&lt;p&gt;--&amp;gt; Open Sound System for AIX (OSS/AIX v3.9.8g)&lt;/p&gt;&lt;p&gt;Note: you must purchase a license from 4Front Technologies to activate OSS/AIX (while it's released as open source the old UNIX versions are not).&lt;/p&gt;&lt;p&gt;PowerPC 970MP running in single-core?&lt;/p&gt;&lt;p&gt;It may sound strange but the IntelliStation POWER 185 is in fact running the PowerPC 970MP "dual core" chip in single core mode. This is undocumented on the PPC 970MP datasheet and not implemented on anything else as far as I can tell. I'm not exactly sure how this was achieved but it's either a hidden switch on the PPC970MP or a special switch done through firmware. The reason why this was done was to reduce the thermal requirements for a smaller system and smaller heatsink; as a work-around IBM then offered adding a second PowerPC 970MP in the IntelliStation 185 (also running in single-core mode) to simulate a "dual-core" PowerPC 970MP setup through two discreet chips instead of one.&lt;/p&gt;&lt;p&gt;7037-A50 or 7047-185, which came first&lt;/p&gt;&lt;p&gt;It's easy to tell the 7037-A50 came first based on the fact documentation always shows the system with a 7037 bezel instead of the characteristic one present on 7047: even on the IntelliStation POWER 185's service diagram on the metal door. Furthermore based on the timing, where IBM felt they needed a lower-end IntelliStation POWER to complement the already released 285 / why create a system from scratch when the upcoming 7037 design could be used. It gets somewhat interesting because this means the IntelliStation 185's bezel was designed after-the-fact, as a result that's why it is so thick on the front to compensate for the slanting that's characeristic of (most) IntelliStations. However, vendors and IBM constantly mix 7037 and 7047 in stock photos and documentation. The photo below shows a 7037 and you can see how it is inspired by 9228--oh and the bezel is flush against the front:&lt;/p&gt;&lt;p&gt;The story doesn't end there... System p5 185 sold *so* poorly (and was more work for IBM to support due to the PPC 970 that really should have never been considered p5) that IBM discontinued it a year after its release in 2007, while the IntelliStation 185 continued to be sold until 2009.&lt;/p&gt;&lt;p&gt;Deceptive as a Greyghost (PC Server 500)&lt;/p&gt;&lt;p&gt;The IntelliStation 185 POWER was aimed towards CAD running under AIX, and had a specific emphasis on quiet operations, as mentioned in the hardware announcement "A quiet deskside form factor". Unlike System x and the IntelliStation 9228 (which is based on a System x), these run amazingly quiet for what they are--which begs the question why couldn't have IBM calibrated the cooling of System x like the POWER 185 as the design (internally) is very similar to 9228?* In fact the original external appearance of 7037-A50 could be confused with 9228; so there was some inspiration between the two. A lot of attention was put into the acoustics as the system is lined extensively with foam, unlike the 9228 or even 7382 which hails as one of the poorest 'acoustic' performers and makes a similar sound to an IntelliStation POWER 285. The characteristic front fold baffle is also meant to reduce noise (and is actually a separate piece clipped in to the main bezel: the design would be rather complex to injection mold as a single piece).in).&lt;/p&gt;&lt;p&gt;*I have confirmed System x towers can run fine with noctua fans / of course I suppose System x is not design to reside as a workstation, but still. The servers have thermal sensors so it would be VERY little effort for IBM to calibrate the fans at a slower RPM instead of hard-lining at a higher-than-necessary value--'desktop mode' could even be added to the UEFI. Even worse, the IntelliStation 6225 was never shipped with a rear fan when it's REALLY ideal to have one (else the FSB can get up to 90C), and when a rear fan is installed the controller throttles it down to a few *hundred* RPM. I suppose different engineers were involved with the RPM tuning on 7047-185, 7382, 9228, 9229 and 6225 so I shouldn't be that critical.&lt;/p&gt;&lt;p&gt;Here with the IntelliStation POWER 185 opened up into pieces you can see the foam lining behind the front bezel along with the interior baffles being lined with foam. The foam on the front bezel was actually manually cut near the intake bezel to angle it off:&lt;/p&gt;&lt;p&gt;It's a workstation not a server&lt;/p&gt;&lt;p&gt;Some mistake the IntelliStation POWER 185 as a "server" or try to say that IntelliStation POWERs in general are servers, but it was always meant as an end-user workstation typically involving CAD or industrial design. The correct "server" product would be system type 7037-A50, the A50 was modified to perform as a server and not a CAD workstation / and also features some differences supported PCI add-ons. It's still hard to distinguish the two when the workstations are directly derived from the servers.&lt;/p&gt;&lt;p&gt;By all means you can use an IntelliStation POWER as a server, but then it's being under-utilizing for what it can really do and missing an opportunity for getting the most out of AIX as a "desktop" OS. Something which is rarely exhibited with the phasing out of the CAD components the IntelliStations first brought to the table.&lt;/p&gt;&lt;p&gt;UNIX high-end CAD meets its death&lt;/p&gt;&lt;p&gt;When the ancients roamed the earth (2006), Dassault Systemes ported CATIA V5 and V6 to AIX, most likely by request from IBM. What's weird about this is that you'll be hard-pressed to find any references to CATIA being run on UNIX now, Dassault completely removed it but there's probably a story behind this as to why. If you go back to the Dassault Systemes website in early 2007, you'll see that they still reference support for Windows and all major flavours of UNIX--but there's one key phrase near the end where they say: "Dassault Systemes has set up a certification program... (blah blah blah)... since the launch of V5, natively developed on windows NT Platform". At that point Windows XP 32-bit was the most commonly used variant, and while you could run XP 64-bit (and IBM did have native support for it on the IntelliStation 9228), XP 64-bit had so many problems so most users were stuck with 3.9 GB of RAM. Therefore if we were to assume that UNIX and said UNIX hardware offered way more memory, it starts to make sense (not to mention that the drivers and video cards would be extremely optimized and had features nVidia and ATI didn't offer*--at the time--). This may be even supported further from the POWER 185 RedBook where IBM states: "The addition of native 64-bit capability brings significant performance enhancements of CATIA V5 and ENOVIA DMU Navigator. Initial benchmarks indicate that for memory-intensive operations, such as analysis of large models, global performance is significantly increased. In addition, improvements to clash analysis greatly reduces processing times for analysis of large assemblies." It's also worth pointing out that CATIA predates Windows, so Dassault probably rewrote V5 from scratch on NT.&lt;/p&gt;&lt;p&gt;*HP's HP-UX hardware being an exception since they just sloppily hacked in standard ATI cards / which means you wouldn't get the extra benefits of running a GXT6500 on AIX as you would with a FireGL X3 on HP-UX. HP probably had the lowest share of the UNIX CAD market so they probably felt little need to invest much R&amp;amp;D: not to mention HP can't make a proper enterprise workstation or server ANYWAYS.&lt;/p&gt;&lt;p&gt;While the IntelliStation lineup was never involved with the sale of the PC division to Lenovo (and continued to press on for awhile), IBM dropped the x86 IntelliStations in 2006 and the POWER IntelliStations in 2009. Why did IBM bother to dedicate CAD resources to AIX when they were deliberately planning to phase out the platform? As established already, UNIX workstations were 'still a thing' back then and had advantages over the terrible x86 machines at the time (Windows XP with Pentium 4), and it was profitable for IBM (along with HP, SGI and Sun) to manufacture such computers. When x64 Windows 7 rolled by various changes in the market happened alongside it: Sun and SGI went out of business, HP started downsizing and split their enterprise into HPE, and Dassault Systemes dropped support for CAITA on UNIX. The UNIX CAD market had the carpet pulled from underneath and now UNIX was set aside for even more specialized applications.&lt;/p&gt;&lt;p&gt;The IntelliStation POWER lineup left a few remnants which are still gawked at by users of newer POWER7/8 systems, such as the high-performance CAD GPUs. AIX has never again received high performance video cards / apparently some IBM customers have complained of this, but not enough to warrant development on better AIX graphics cards. As a result, for more realistic graphics performance AIX users continue to use the IntelliStation cards.&lt;/p&gt;&lt;p&gt;There is a story underneath the death march of AIX workstations, if you look carefully you'll notice the GXT6500P was originaly announced in 2002 (along with the IntelliStation POWER 265). IBM never refreshed any of the GXT135P, GXT4500P and GXT6500P cards, still keeping them as "current" offerings for the IntelliStation POWER 185 and 285. And soon as the last two of the POWER IntelliStations were made, other OEMs stopped making UNIX CAD stations and IBM quietly killed all IntelliStations, only leaving System x and System i behind (of course IBM has now even sold System x off in 2014 after the U.S. government reviewed the case as they used System x servers internally).&lt;/p&gt;&lt;p&gt;POWER Linux vs AIX on the IntelliStation POWER 185&lt;/p&gt;&lt;p&gt;Let's get it out of the way: many think AIX will be dead soon and stipulate IBM is wanting clients to all mass migrate to POWER Linux (in fact, Watson was even programmed in POWER Linux and not AIX). The truth of the matter is that AIX contains more features that IBM or anyone else does not add in Linux. For instance, going through the RedBooks and Hardware Announcement, the higher-end CAD cards are not supported under Linux, RAS is not supported, the sound card is not supported and it goes on and on. In other words you're not going to use be using Linux for CAD, graphical operations or sound any time soon. While it may 'seem' IBM wants everyone on POWER Linux and not AIX, you can't use it as a workstation operating system unlike AIX.&lt;/p&gt;&lt;p&gt;Because the IntelliStation POWER 185 uses the aforementioned PowerPC 970, POWER Linux itself has limited support on the system. In the June 23 2009 hardware announcement, IBM confirms that SUSE Linux Enterprise Server 11 has dropped support for the PowerPC 970:&lt;/p&gt;&lt;p&gt;"SUSE Linux Enterprise Server 11 (SLES 11) for POWER supports all POWER5 and POWER6 technology-based systems with the exception of the OpenPower line of servers. It does not support any 970 based systems, which includes JS20, JS21, IBM System p5 185, and the IBM Intellistation POWER 185."&lt;/p&gt;&lt;p&gt;Whereas on AIX 7.1 PPC 970 is still supported (note that AIX 7.2 dropped it, but so was the IntelliStation POWER 285):&lt;/p&gt;&lt;p&gt;"Only 64-bit Common Hardware Reference Platform (CHRP) machines running selected PowerPC 970, POWER4, POWER5, POWER6, and POWER7 processors that implement the POWER architecture Platform Requirements (PAPR) are supported."&lt;/p&gt;&lt;p&gt;It's therefore far more logical to use AIX to get the most out of the IntelliStation's workstation-oriented hardware, since POWER Linux has virtually nothing to offer for graphics support. While AIX and UNIX workstations in general are dead, you won't see such features revisted on newer AIX hardware / and it's *completely* omitted under POWER Linux.&lt;/p&gt;&lt;p&gt;Meanwhile HPE and Intel seem to be having a hard time maintaining their Itanium contracts and want HP-UX to die / whereas AIX and POWER are in a much better position due to IBM's contributions to Linux and (now) OpenPOWER.&lt;/p&gt;&lt;p&gt;OS Support&lt;/p&gt;&lt;p&gt;AIX&lt;/p&gt;AIX 7.2 Not Supported&lt;p&gt;I'm not sure why (some) IBM documentation claims AIX 7.1 is not supported on the IntelliStation POWER 185 when it in fact is (yes! You can in fact run AIX 7.1 on a PowerPC 970 CPU).&lt;/p&gt;&lt;p&gt;WARNING: AIX 7.1 wasn't designed with the System p5 185, or AIX workstations in mind anymore; so if you run AIX 7.1 on a system with a single PSU, you will get the rc.powerfail:2 being generated in the console every so often because AIX 7.1 is looking for a redundant power supply that does not exist. Kind of strange considering the System p5 185 lacks a second power supply as well and was sold as a server rack unit.&lt;/p&gt;&lt;p&gt;POWER Linux&lt;/p&gt;&lt;p&gt; SUSE Linux Enterprise SLES 9 SP3&lt;lb/&gt; Red Hat Enterprise AS4U3&lt;/p&gt;&lt;p&gt;The Great Purge of 2016&lt;/p&gt;&lt;p&gt;The U.S. Department of Defense was using a mass horde of IntelliStation POWER 185 units / and much of them have now been tossed back to IBM reselling channels. So, in early 2016 many appeared and obtaining one became a LOT cheaper. The timing is not a coincidence, IBM is phasing out the 185 in mid 2017 so corporate customers are being told to upgrade, or their leases are expiring (requiring a forced upgrade). The fact that some of these computers have been in service for 10 years is an example of how UNIX moves slowly and that the hardware was well built. Unlike other IBM systems, the IntelliStation POWER 185 is uniquely elusive for its age, I don't know if they will continue to drop in price or get more expensive as they reach unobtanium.&lt;/p&gt;&lt;p&gt;In one of IBM's client case studies, it is explained that they sold the Polish government 400 IntelliStation POWER 185 workstations / so we can assume a similar amount was also sold to the U.S. government (and in other places, too):&lt;/p&gt;&lt;p&gt;"The organization teamed with IBM Global Technology Services to implement a solution based on IBM Lotus Domino Collaboration Express software and the Lotus software-based iDoc offering from IBM Business Partner Advatech. The IBM staff loaded the new software on the client's fleet of 400 newly purchased IBM IntelliStation(R) POWER(TM) 185 Express workstations."&lt;/p&gt;&lt;p&gt;I'm not sure how many of these IntelliStations were sold, maybe a few thousand at best? But if we were to take into consideration that a typical system would probably be around $8,000, times that by four hundred and you're looking at $3.2 million! Later the document explains that operating costs for the client were reduced as a result of using that IBM solution. I suppose in the grand scheme of things they would be--since they probably used them for 10 years / and since the hardware was so robust IBM wouldn't have to do any work on the service agreement.&lt;/p&gt;&lt;p&gt;GPU options and caveats&lt;/p&gt;&lt;p&gt;As mentioned throughout, the IntelliStation 185 was geared towards CAD design and as such supported GPU options. Here are the three main GPUs that the system supports, it may be possible to run others but I have not tried it:&lt;/p&gt;&lt;p&gt; GXT4500P (FC 2842 - IBM)&lt;lb/&gt; GXT6500P (FC 2843 - IBM)&lt;lb/&gt; GXT135P (FC 1980 - Matrox)&lt;/p&gt;&lt;p&gt;The GXT4500P and GXT6500P are the premier cards designed in-house by IBM; they only run properly on AIX and no Linux support exists. The GXT135P is a rebadged Matrox GT series card and runs under both AIX and Linux, but is far more limited in graphical capabilities. Apparently two GXT4500Ps can be ran in the IntelliStation, however I don't know if this will enable dual-monitor support. Only one GXT6500P can be ran in a system at a time. Up to FOUR GXT135Ps can be stuffed into an IntelliStation, this could theoreitcally allow eight monitors to be connected, however again, that would all depend if AIX could support it.&lt;/p&gt;&lt;p&gt;Firmware options&lt;/p&gt;&lt;p&gt;The IntelliStation POWER 185 has an array of its own unique firmware options, similar to a BIOS/UEFI on a regular x86 computer. Below is what options you get when selecting to go into the firmware menu upon when the machine first starts up:&lt;/p&gt;&lt;p&gt; Main Menu&lt;lb/&gt; 1 Select Language&lt;lb/&gt; 2 Setup Remote IPL (Initial Program Load)&lt;lb/&gt; 3 Change SCSI Settings&lt;lb/&gt; 4 Select Console&lt;lb/&gt; 5 Select Boot Options&lt;lb/&gt; 6 Power/Restart Control&lt;lb/&gt; 7 System Service Aids&lt;lb/&gt; 8 Set Beep Volume&lt;lb/&gt; 9 Select Keyboard&lt;lb/&gt; --------------------------------------------------------------------------------&lt;lb/&gt; Navigation keys:&lt;lb/&gt; M = return to Main Menu N = Next page of list&lt;lb/&gt; ESC key = return to previous screen X = eXit System Management Services&lt;lb/&gt; --------------------------------------------------------------------------------&lt;lb/&gt; Type menu item number and press Enter or select Navigation key&lt;/p&gt;&lt;p&gt;Basically all of the options are self-explanatory really. I had to change the sound of the system beep since it was WAY too loud on its default setting. I chose 6.&lt;/p&gt;&lt;p&gt;PCI adapter placement&lt;/p&gt;&lt;p&gt;Unlike most computers, the IntelliStation POWER series can be sensitive what adapter is placed where: this is due to the way each slot is tied to the north bridge via the hyper transport tunnels and individual sub PCI bridges. There are a total of four PCI-X slots with a regular 32-bit PCI slot at the very top. Slots 2 &amp;amp; 3 have direct access and operate at 133 Mhz, slots 4 &amp;amp; 5 are shared and operate at 100 Mhz, slot 1 is a regular PCI slot and operates at 33 Mhz. Slot 5 can operate at 133 Mhz but only if slot 4 is empty, otherwise it runs at 100 Mhz.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Feature code (FC)&lt;/cell&gt;&lt;cell role="head"&gt;Base unit slot priority&lt;/cell&gt;&lt;cell role="head"&gt;7047-185 max. allowed adapters&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2842 (GXT4500P)&lt;/cell&gt;&lt;cell&gt;2, 3&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2843 (GXT6500P)&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1954*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5740*&lt;/cell&gt;&lt;cell&gt;2, 3&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1984*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5706*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5707*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1983*&lt;/cell&gt;&lt;cell&gt;2, 3, 5/4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1978*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1979*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5700*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5701*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5759**&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4 3&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1910**&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4 3&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1905*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5758*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1986*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1987*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5713*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5714*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1977, 5716* (Fibre Channel Adapter)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1912* (Ultra320 LVD SCSI Adapter)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4, 1&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5736*&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4, 1&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;1913* (Ultra320 LVD SCSI RAID Adapter)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5737* (Disk Controller)&lt;/cell&gt;&lt;cell&gt;2, 3, 5, 4&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2849, 1980 (GXT135P)&lt;/cell&gt;&lt;cell&gt;2, 3, 4, 5&lt;/cell&gt;&lt;cell&gt;4&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2947 (4-Port Multiprotocol Adapter)&lt;/cell&gt;&lt;cell&gt;4, 5, 3, 2&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;5723 (2-Port asynchronous Serial Adapter)&lt;/cell&gt;&lt;cell&gt;1, 4, 5, 3, 2&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;2943 (8-Port asynchronous Serial Adapter)&lt;/cell&gt;&lt;cell&gt;1, 4, 5, 3, 2&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;8244 (Crystal PCI Audio Adapter)&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;** Extra-high bandwidth (EHB) adapter. See the Performance notes before installing this adapter.&lt;lb/&gt; * High bandwidth (HB) adapter. See the Performance notes before installing this adapter. &lt;/p&gt;&lt;p&gt; Performance notes (for optimum performance)&lt;lb/&gt; System unit information:&lt;lb/&gt; -No more than three EHB adapters can be placed in the system. If an EHB adapter is placed in the system, it must be the only EHB or HB adapter attached to the PHB it uses.&lt;lb/&gt; -No more than four HB adapters can be placed in the system&lt;lb/&gt; -No more than three Gb Ethernet ports per PHB.&lt;lb/&gt; -No more than three 1 Gb Ethernet ports per one CPU in a system. More Ethernet adapters can be added for connectivity.&lt;lb/&gt; -If an adapter lists slot 5/4, this indicates the adapter can go in slot 5 or 4, but not both 5 and 4.&lt;/p&gt;&lt;p&gt;Control Panel Functions&lt;/p&gt;&lt;p&gt;On the front of the IntelliStation POWER 185 the control panel pops out (pushing the tab sideways) and there are a fair bit of functions to choose from / the default boot (or IPL) would be set to 'N, F, P', then you simply press the power button and away it goes; you can also choose the 'service IPL' mode from the Open Firmware menu as well without ever having to touch the panel. Since some of these functions are specific to the IntelliStation 185 they're not entirely agnostic to other "IPL" systems or mainframes. I've extracted the function help from the IBM knowledgebase below. Quick trivia: the IntelliStation POWER 185's specific control panel has been borrowed and re-used in later POWER systems such as the Power 720 and Power S814, it's funny because not many know of this particular machine, yet parts of it live on, photo of it below.&lt;/p&gt;&lt;p&gt; Function 01: Display selected system operating mode, IPL speed, and firmware IPL mode&lt;lb/&gt; This function displays the selected system operating mode, speed, and firmware mode for the next IPL on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. This function displays the following information:&lt;/p&gt;&lt;p&gt; The valid logical key modes (N).&lt;lb/&gt; The IPL speed (F).&lt;lb/&gt; The firmware mode (P or T).&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 1 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 01.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 1 _ _ _ _ _ N _ _ _ _ F _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ P _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Valid system operating mode is N.&lt;p&gt;Valid IPL speed display is F.&lt;/p&gt;&lt;p&gt;Valid firmware IPL modes are P and T.&lt;/p&gt;&lt;p&gt;P = permanent side boot&lt;/p&gt;&lt;p&gt;T = temporary side boot&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 1 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 02: Select firmware IPL mode&lt;lb/&gt; This function allows you to select the firmware IPL mode on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 02..&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ P _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 02. The current firmware mode is displayed.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ T &amp;lt; _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the firmware IPL modes. Valid firmware IPL modes are P and T.&lt;p&gt;P = permanent side boot&lt;/p&gt;&lt;p&gt;T = temporary side boot&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to select the firmware IPL mode and exit function 02.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 04: Lamp test&lt;lb/&gt; This function performs a lamp test on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. This function shows whether any control panel indicators are burned out and whether characters that are displayed in the Function/Data display on the control panel are valid. When you activate this test, all the control panel lights and indicators are lit.&lt;/p&gt;&lt;p&gt;The lamp test continues on the system control panel for four minutes. Use this procedure to verify that the lights on the system control panel are working correctly. If you cannot complete these steps, see "Starting Point for All Problems" in the Problem Analysis information for your system to start problem analysis.&lt;/p&gt;&lt;p&gt; 1. Power on the system.&lt;lb/&gt; 2. Press the Increment (up) or Decrement (down) buttons on the control panel to display function 04.&lt;lb/&gt; Press Enter on the control panel.&lt;lb/&gt; 3. Do all of the lights and indicators on the system control panel come on?&lt;lb/&gt; Yes No&lt;lb/&gt; -&amp;gt; Exchange the control panel or the replaceable unit that contains the control panel function [system unit backplane (MB1) or tower card (CB1)]. See "Removal and Installation Procedures" in the Problem Analysis information for your system.&lt;lb/&gt; 4. Do the expansion unit control panel lights all come on?&lt;lb/&gt; Yes No&lt;lb/&gt; -&amp;gt; Exchange the control panel on the expansion unit.&lt;/p&gt;&lt;p&gt;The lights on the system control panel are working correctly. This ends the procedure.&lt;/p&gt;&lt;p&gt; Function 05: Remind mode&lt;lb/&gt; This function allows you to place the system fault-indicator LED in remind mode on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode.&lt;/p&gt;&lt;p&gt;When the system fault-indicator LED is on solid, an error condition exists on the system. If you want to defer the repair of the error, you can place the system fault-indicator LED in remind mode. Placing the system in remind mode causes the system fault-indicator LED to flash instead of being on solid. The remind mode lets you know that a system fault that you have deferred still exists on the system. If any other serviceable event occurs on the system, the remind mode is changed back to system fault mode, where the LED is on solid.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 05.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;R E M I N D M O D E O N _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Press Enter to start function 05. Valid options are:&lt;p&gt;Remind mode ON&lt;/p&gt;&lt;p&gt;Remind mode OFF&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;R E M I N D M O D E O F F _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to toggle the option on or off.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;&lt;lb/&gt; Light path diagnostic card indicator LED layout for the 7037-A50 and 7047-185 models&lt;/p&gt;&lt;p&gt; 1 Power supply fault-indicator LED&lt;lb/&gt; 2 Voltage-regulator module fault-indicator LED&lt;lb/&gt; 3 Disk-drive bay fan fault-indicator LED&lt;lb/&gt; 4 Optical-media bay fault-indicator LEDs&lt;lb/&gt; 5 Disk-drive bay fault-indicator LEDs&lt;lb/&gt; 6 System backplane fault-indicator LED&lt;lb/&gt; 7 Front fan fault-indicator LED&lt;lb/&gt; 8 Battery fault-indicator LED&lt;lb/&gt; 9 PCI adapter fault-indicator LED&lt;lb/&gt; 10 Thermal fault-indicator LED&lt;lb/&gt; 11 Rear fan fault-indicator LED&lt;lb/&gt; 12 Memory fault-indicator LED&lt;/p&gt;&lt;p&gt; Function 06: Display the BMC version&lt;lb/&gt; This function displays the base motherboard controller (BMC) version on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 6 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 06.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;B M C: A W 8 T x x A _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 06. An example of the BMC version is AW8T23A.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 6 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 09: Display the BMC fan speed&lt;lb/&gt; This function displays the base motherboard controller (BMC) fan speed on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. The display alternates every two seconds between MAIN, DASD, and PCI fan speed.&lt;lb/&gt; The following table provides details about this function. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 0 9 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 09.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;M A I N: 7 b 0 _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 09. The main fan speed is listed in hexadecimal (rpm).&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;D A S D: 7 0 0 _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 09. The DASD fan speed is listed in hexadecimal (rpm).&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;P C I: 7 b 0 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 09. The PCI (I/O) fan speed is listed in hexadecimal (rpm).&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0 9 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 10: Display the temperature&lt;lb/&gt; This function displays the temperature on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. The display alternates every two seconds between ambient, CPU1, and CPU2 temperature. The following table provides details about this function. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt; 1 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 10.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;A m b i e n t : 3 e , 3 e _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Press Enter to start function 10. The ambient temperature is listed in hexadecimal (degrees Celsius).&lt;p&gt;The first value is the average temperature over a time span.&lt;/p&gt;&lt;p&gt;The last value is the most recent temperature reading.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;C P U 1 : 5 0 , 6 f _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt; Press Enter to start function 10. The CPU1 temperature is listed in hexadecimal (degrees Celsius).&lt;p&gt;The first value is the average temperature over a time span.&lt;/p&gt;&lt;p&gt;The last value is the most recent temperature reading.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;C P U 2 : 0 , 0 _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 10. The CPU2 temperature is listed in hexadecimal (degrees Celsius).&lt;p&gt;The first value is the average temperature over a time span.&lt;/p&gt;&lt;p&gt;The last value is the most recent temperature reading.&lt;/p&gt;&lt;p&gt;The reading is 0 if the system is one-way.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;1 0 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll through the control panel functions.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt; Function 20: System type and model&lt;lb/&gt; This function displays the machine type and model on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. The machine type and model is displayed in the following format:&lt;lb/&gt; p p p p - m m m _ _ _ _ _ _ _ _&lt;lb/&gt; The values are indicated as follows:&lt;lb/&gt; Values for p indicate the machine type.&lt;lb/&gt; Values for m indicate the machine model.&lt;/p&gt;&lt;p&gt; Function 22: Partition dump&lt;lb/&gt; This function initiates a dump of a partition's operating system data on the 7037-A50 server and the 7047-185 workstation. This function is available in both normal and manual operating mode. You must perform two consecutive function 22 selections to initiate a partition dump. The following table shows an example of function 22. &lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Function/Data&lt;/cell&gt;&lt;cell role="head"&gt;Action or description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 22.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2 2 _ _ _ _ 0 0 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 22.&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;A 1 0 0 3 0 2 2 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Displays the partition dump verification system reference code (SRC).&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;2 2 _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Use the Increment or Decrement buttons to scroll to function 22.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;2 2 _ _ _ _ 0 0 _ _ _ _ _ _ _ _&lt;p&gt;_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;Press Enter to start function 22.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45401907</guid><pubDate>Sun, 28 Sep 2025 05:04:39 +0000</pubDate></item><item><title>Beyond OpenMP in C++ and Rust: Taskflow, Rayon, Fork Union</title><link>https://ashvardanian.com/posts/beyond-openmp-in-cpp-rust/</link><description>&lt;doc fingerprint="776ed3c34da494ba"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR: Most C++ and Rust thread-pool libraries leave significant performance on the table - often running 10× slower than OpenMP on classic fork-join workloads and micro-benchmarks. So I’ve drafted a minimal ~300-line library called Fork Union that lands within 20% of OpenMP. It does not use advanced NUMA tricks; it uses only the C++ and Rust standard libraries and has no other dependencies.&lt;/p&gt;&lt;p&gt;Update (Sep 2025): Since the v2 release, Fork Union supports NUMA and Huge Pages, as well as&lt;/p&gt;&lt;code&gt;tpause&lt;/code&gt;,&lt;code&gt;wfet&lt;/code&gt;, and other “pro” features. Check the README for details.&lt;/quote&gt;
    &lt;p&gt;OpenMP has been the industry workhorse for coarse-grain parallelism in C and C++ for decades. I lean on it heavily in projects like USearch, yet I avoid it in larger systems because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fine-grain parallelism with independent subsystems doesn’t map cleanly to OpenMP’s global runtime.&lt;/item&gt;
      &lt;item&gt;Portability of the C++ STL and the Rust standard library is better than OpenMP.&lt;/item&gt;
      &lt;item&gt;Meta-programming with OpenMP is a pain - mixing &lt;code&gt;#pragma omp&lt;/code&gt;with templates quickly becomes unmaintainable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So I went looking for ready-made thread pools in C++ and Rust — only to realize most of them implement asynchronous task queues, a much heavier abstraction than OpenMP’s fork-join model. Those extra layers introduce what I call the four horsemen of low performance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Locks &amp;amp; mutexes with syscalls in the hot path.&lt;/item&gt;
      &lt;item&gt;Heap allocations in queues, tasks, futures, and promises.&lt;/item&gt;
      &lt;item&gt;Compare-and-swap (CAS) stalls in the pessimistic path.&lt;/item&gt;
      &lt;item&gt;False sharing unaligned counters thrashing cache lines.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With today’s dual-socket AWS machines pushing 192 physical cores, I needed something leaner than Taskflow, Rayon, or Tokio. Enter Fork Union.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;Hardware: AWS Graviton 4 metal (single NUMA node, 96× Arm v9 cores, 1 thread/core). Workload: “ParallelReductionsBenchmark” - summing single-precision floats in parallel. In this case, just one cache line (&lt;code&gt;float[16]&lt;/code&gt;) per core—small enough to stress synchronization cost of the thread pool rather than arithmetic throughput of the CPU.
In other words, we are benchmarking kernels similar to:&lt;/p&gt;
    &lt;p&gt;Google Benchmark numbers for the C++ version of Fork Union, compared to OpenMP, Taskflow, and allocating 96× &lt;code&gt;std::thread&lt;/code&gt; objects on-demand, are as follows:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;I’ve cleaned up the output, focusing only on the relevant rows and the reduction throughput.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Criterion.rs numbers for the Rust version of Fork Union, compared to Rayon, Tokio, and Smol’s Async Executors, are as follows:&lt;/p&gt;
    &lt;p&gt;The timing methods used in those two executables are different, but the relative observations should hold.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spawning new threads is obviously too expensive.&lt;/item&gt;
      &lt;item&gt;Most reusable thread pools are still 10x slower to sync than OpenMP.&lt;/item&gt;
      &lt;item&gt;OpenMP isn’t easy to compete with and still outperforms Fork Union by 20%.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This clearly shows, how important it is to chose the right tool for the job. Don’t pick an asynchronous task pool for a fork-join blocking workload!&lt;/p&gt;
    &lt;head rend="h2"&gt;Four Horsemen of Performance&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;This article won’t be a deep dive into those topics. Each deserves its own article and a proper benchmark, with some good ones already available and linked.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Locks and Mutexes&lt;/head&gt;
    &lt;p&gt;Unlike the &lt;code&gt;std::atomic&lt;/code&gt;, the &lt;code&gt;std::mutex&lt;/code&gt; update may result in a system call, and it can be expensive to acquire and release.
Its implementations generally have 2 executable paths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the fast path, where the mutex is not contended, where it first tries to grab the mutex via a compare-and-swap operation, and if it succeeds, it returns immediately.&lt;/item&gt;
      &lt;item&gt;the slow path, where the mutex is contended, and it has to go through the kernel to block the thread until the mutex is available.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Linux, the latter translates to a “futex” syscall and an expensive context switch. In Rust, the same applies to &lt;code&gt;std::async::atomic&lt;/code&gt; and &lt;code&gt;std::sync::Mutex&lt;/code&gt;.
Prefer the former when possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory Allocations&lt;/head&gt;
    &lt;p&gt;Most thread-pools use classes like &lt;code&gt;std::future&lt;/code&gt;, &lt;code&gt;std::packaged_task&lt;/code&gt;, &lt;code&gt;std::function&lt;/code&gt;, &lt;code&gt;std::queue&lt;/code&gt;, &lt;code&gt;std::conditional_variable&lt;/code&gt;.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In Rust land, there will often be a&lt;/p&gt;&lt;code&gt;std::Box&lt;/code&gt;,&lt;code&gt;std::Arc&lt;/code&gt;,&lt;code&gt;std::collections::VecDeque&lt;/code&gt;,&lt;code&gt;std::sync::mpsc&lt;/code&gt;or even&lt;code&gt;std::sync::mpmc&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;Most of those, I believe, aren’t unusable in Big-Data applications, where you always operate in memory-constrained environments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raising a &lt;code&gt;std::bad_alloc&lt;/code&gt;exception when there is no memory left and just hoping that someone up the call stack will catch it is not a great design idea for Systems Engineering.&lt;/item&gt;
      &lt;item&gt;The threat of having to synchronize ~200 physical CPU cores across 2-8 sockets and potentially dozens of NUMA nodes around a shared global memory allocator practically means you can’t have predictable performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we focus on a simpler &lt;del&gt;concurrency&lt;/del&gt; parallelism model, we can avoid the complexity of allocating shared states, wrapping callbacks into some heap-allocated “tasks”, and a lot of other boilerplates.&lt;/p&gt;
    &lt;p&gt;Less work = more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Atomics and CAS&lt;/head&gt;
    &lt;p&gt;Once you get to the lowest-level primitives on concurrency, you end up with the &lt;code&gt;std::atomic&lt;/code&gt; and a small set of hardware-supported atomic instructions.
Hardware implements it differently:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;x86 is built around the “Total Store Order” (TSO) memory consistency model and provides &lt;code&gt;LOCK&lt;/code&gt;variants of the&lt;code&gt;ADD&lt;/code&gt;and&lt;code&gt;CMPXCHG&lt;/code&gt;. These variants act as full-blown “fences” — no loads or stores can be reordered across them. This makes atomic operations on x86 straightforward but heavyweight.&lt;/item&gt;
      &lt;item&gt;Arm, on the other hand, has a “weak” memory model and provides a set of atomic instructions that are not fenced and match the C++ concurrency model. It offers &lt;code&gt;acquire&lt;/code&gt;,&lt;code&gt;release&lt;/code&gt;, and&lt;code&gt;acq_rel&lt;/code&gt;variants of each atomic instruction — such as&lt;code&gt;LDADD&lt;/code&gt;,&lt;code&gt;STADD&lt;/code&gt;, and&lt;code&gt;CAS&lt;/code&gt;— which allow precise control over visibility and order, especially with the introduction of “Large System Extension” (LSE) instructions in Armv8.1-A.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A locked atomic on x86 requires the cache line in the Exclusive state in the requester’s L1 cache. This would incur a coherence transaction (Read-for-Ownership) if another core had the line. Both Intel and AMD handle this similarly.&lt;/p&gt;
    &lt;p&gt;It makes Arm and Power much more suitable for lock-free programming and concurrent data structures, but some observations hold for both platforms. Most importantly, “Compare and Swap” (CAS) is costly and should be avoided at all costs.&lt;/p&gt;
    &lt;p&gt;On x86, for example, the &lt;code&gt;LOCK ADD&lt;/code&gt; can easily take 50 CPU cycles.
It is 50x slower than a regular &lt;code&gt;ADD&lt;/code&gt; instruction but still easily 5-10x faster than a &lt;code&gt;LOCK CMPXCHG&lt;/code&gt; instruction.
Once the contention rises, the gap naturally widens, further amplified by the increased “failure” rate of the CAS operation when the value being compared has already changed.
That’s why, for the “dynamic” mode, we resort to using an additional atomic variable rather than more typical CAS-based implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alignment&lt;/head&gt;
    &lt;p&gt;Assuming a thread pool is a heavy object anyway, nobody will care if it’s a bit larger than expected. That allows us to over-align the internal counters to &lt;code&gt;std::hardware_destructive_interference_size&lt;/code&gt; or &lt;code&gt;std::max_align_t&lt;/code&gt; to avoid false sharing.
In that case, even on x86, where the entire cache will be exclusively owned by a single thread, in eager mode, we end up effectively “pipelining” the execution, where one thread may be incrementing the “in-flight” counter while the other is decrementing the “remaining” counter.
Others are executing the loop body in between.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparing APIs&lt;/head&gt;
    &lt;head rend="h3"&gt;Fork Union&lt;/head&gt;
    &lt;p&gt;Fork Union has a straightforward goal, so its API is equally clear. There are only 4 core interfaces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;for_each_thread&lt;/code&gt;- to dispatch a callback per thread, similar to&lt;code&gt;#pragma omp parallel&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;for_each_static&lt;/code&gt;- for individual evenly-sized tasks, similar to&lt;code&gt;#pragma omp for schedule(static)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;for_each_slice&lt;/code&gt;- for slices of evenly-sized tasks, similar to nested&lt;code&gt;#pragma omp for schedule(static)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;for_each_dynamic&lt;/code&gt;- for individual unevenly-sized tasks, similar to&lt;code&gt;#pragma omp for schedule(dynamic, 1)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They all receive a C++ lambda or a Rust closure and a range of tasks to execute. The construction of the thread pool itself is a bit trickier than typically in standard libraries, as “exceptions” and “panics” are not allowed. So, the constructor can’t perform any real work. In C++, the &lt;code&gt;try_spawn&lt;/code&gt; method can be called to allocate all the threads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;As you may have noticed, the lambdas are forced to be&lt;/p&gt;&lt;code&gt;noexcept&lt;/code&gt;and can’t return anything. This is a design choice that vastly simplifies the implementation.&lt;/quote&gt;
    &lt;p&gt;In Rust, similarly, the &lt;code&gt;try_spawn&lt;/code&gt; method can be used:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Assuming Rust has no function overloading, there are a few alternatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;try_spawn&lt;/code&gt;- to spawn a thread pool with the main allocator.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;try_spawn_in&lt;/code&gt;- to spawn a thread pool with a custom allocator.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;try_named_spawn&lt;/code&gt;- to spawn a thread pool with the main allocator and a name.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;try_named_spawn_in&lt;/code&gt;- to spawn a thread pool with a custom allocator and a name.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rayon&lt;/head&gt;
    &lt;p&gt;Rayon is the go-to Rust library for data parallelism. It suffers from the same core design issues as every other thread pool I’ve looked at on GitHub, but it’s fair to say that at the high level, it provides outstanding coverage for various parallel iterators! As such, there is an open call to explore similar “Map-Reduce” and “Map-Fork-Reduce” patterns in Fork Union to see if they can be implemented efficiently.&lt;/p&gt;
    &lt;p&gt;The default &lt;code&gt;.par_iter()&lt;/code&gt; API of Rayon, at the start of the README.md, is not how I’ve used it in “Parallel Reductions Benchmark”.
To ensure that we are benchmarking the actual synchronization cost of the thread pool, I’ve gone directly to the underlying &lt;code&gt;rayon::ThreadPool&lt;/code&gt; API:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Taskflow&lt;/head&gt;
    &lt;p&gt;Taskflow is one of the most popular C++ libraries for parallelism. It has many features, including async execution graphs on CPUs and GPUs. The most common example looks like this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Despite being just an example, it clearly shows how different Taskflow’s core objectives are from OpenMP and Fork Union. It is still probably mainly used for simple static parallelism, similar to our case without complex dependencies and the &lt;code&gt;taskflow&lt;/code&gt; can be reused.
Here is how “Parallel Reductions Benchmark” wraps Taskflow:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Only the &lt;code&gt;operator()&lt;/code&gt; method is timed, leaving the construction costs out of the equation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions &amp;amp; Observations&lt;/head&gt;
    &lt;p&gt;Fork Union shows that a lean, 300-line fork-join pool can sit within ~20% of OpenMP, while more functional pools trail by an order of magnitude. That margin will shift as more workloads, CPUs, and compilers are tested, so treat today’s numbers as directional, not gospel. There may still be subtle memory-ordering bugs lurking in Fork Union, but the core observations should hold: dodge mutexes, dynamic queues, likely-pessimistic CAS paths, and false sharing — regardless of language or framework.&lt;/p&gt;
    &lt;p&gt;Rust is still new territory for me. The biggest surprise is the missing allocator support in &lt;code&gt;std::collections&lt;/code&gt; on the stable toolchain.
Nightly’s &lt;code&gt;Vec::try_reserve_in&lt;/code&gt; helps, but until stable lands, ergonomic custom allocation remains tricky.
The machinery exists in C++, yet most projects ignore it — so the culture needs to catch up.&lt;/p&gt;
    &lt;p&gt;PS: Spot dubious memory-ordering? Open an issue. Want to close the remaining 20% gap? Happy forking 🤗&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Fork Union, arguably the most unusual parallel-processing library on GitHub, just crossed its first 100 stars — my 12th project to reach that milestone 🥳&lt;/p&gt;— Ash Vardanian (@ashvardanian) September 7, 2025&lt;lb/&gt;Repository: https://t.co/Gyg43GW6d7&lt;lb/&gt;Unlike typical thread-pools, it avoids not only mutexes but even Compare-and-Swap… pic.twitter.com/H2H7fgaXl4&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45402820</guid><pubDate>Sun, 28 Sep 2025 08:53:36 +0000</pubDate></item><item><title>Show HN: Curated gamedev specific search engine</title><link>https://gamedevtorch.com/</link><description>&lt;doc fingerprint="1d0eae37a1e6ef04"&gt;
  &lt;main&gt;
    &lt;p&gt;GameDev Torch is a small gamedev specific search engine.&lt;/p&gt;
    &lt;p&gt;Search gamedev related articles, game engines, frameworks, blog posts and more from a manually curated set of websites. Use it to complement your general purpose search engines to easily find niche resources.&lt;/p&gt;
    &lt;head rend="h4"&gt;What can I do?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Looking for inspiration? Learn more about "npc ai"&lt;/item&gt;
      &lt;item&gt;Something's missing? Suggest a new resource to index&lt;/item&gt;
      &lt;item&gt;Want to know more? Learn how to query effectively&lt;/item&gt;
      &lt;item&gt;Or browse the list of all indexed websites&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Who made this?&lt;/head&gt;
    &lt;p&gt;You can find me on bsky&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45403288</guid><pubDate>Sun, 28 Sep 2025 10:36:23 +0000</pubDate></item><item><title>Privacy Badger is a free browser extension made by EFF to stop spying</title><link>https://privacybadger.org/</link><description>&lt;doc fingerprint="6511c9540fb41697"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Privacy Badger&lt;/head&gt;
    &lt;head rend="h2"&gt;Frequently Asked Questions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What is Privacy Badger?&lt;/item&gt;
      &lt;item&gt;How is Privacy Badger different from other blocking extensions?&lt;/item&gt;
      &lt;item&gt;Who makes Privacy Badger?&lt;/item&gt;
      &lt;item&gt;How does Privacy Badger work?&lt;/item&gt;
      &lt;item&gt;What is a third party tracker?&lt;/item&gt;
      &lt;item&gt;What do the red, yellow and green sliders in the Privacy Badger menu mean?&lt;/item&gt;
      &lt;item&gt;Why does Privacy Badger block ads?&lt;/item&gt;
      &lt;item&gt;Why doesn't Privacy Badger block all ads?&lt;/item&gt;
      &lt;item&gt;What is Global Privacy Control (GPC)?&lt;/item&gt;
      &lt;item&gt;What about tracking by the sites I actively visit, like NYTimes.com or Facebook.com?&lt;/item&gt;
      &lt;item&gt;Does Privacy Badger contain a list of blocked sites?&lt;/item&gt;
      &lt;item&gt;How was the cookie blocking yellowlist created?&lt;/item&gt;
      &lt;item&gt;Does Privacy Badger prevent fingerprinting?&lt;/item&gt;
      &lt;item&gt;Does Privacy Badger consider every cookie to be a tracking cookie?&lt;/item&gt;
      &lt;item&gt;Will you be supporting any other browsers besides Chrome, Firefox, Edge and Opera?&lt;/item&gt;
      &lt;item&gt;Can I download Privacy Badger directly from eff.org?&lt;/item&gt;
      &lt;item&gt;I run a domain that uses cookies or other tracking. How do I stop Privacy Badger from blocking me?&lt;/item&gt;
      &lt;item&gt;Where can I find general information about Privacy Badger that I can use for a piece I'm writing?&lt;/item&gt;
      &lt;item&gt;As an administrator, how do I configure Privacy Badger on my managed devices?&lt;/item&gt;
      &lt;item&gt;What is the Privacy Badger license? Where is the Privacy Badger source code?&lt;/item&gt;
      &lt;item&gt;How can I support Privacy Badger?&lt;/item&gt;
      &lt;item&gt;How does Privacy Badger handle social media widgets?&lt;/item&gt;
      &lt;item&gt;How do I uninstall/remove Privacy Badger?&lt;/item&gt;
      &lt;item&gt;Is Privacy Badger compatible with other extensions, including adblockers?&lt;/item&gt;
      &lt;item&gt;Is Privacy Badger compatible with Firefox's built-in privacy protections?&lt;/item&gt;
      &lt;item&gt;Why does my browser connect to fastly.com IP addresses on startup after installing Privacy Badger?&lt;/item&gt;
      &lt;item&gt;Why does Privacy Badger need access to my data for all websites?&lt;/item&gt;
      &lt;item&gt;Why aren't videos loading on YouTube? Why isn't Privacy Badger blocking ads on YouTube?&lt;/item&gt;
      &lt;item&gt;I need help! I found a bug! What do I do now?&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What is Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Privacy Badger is a browser extension that stops advertisers and other third-party trackers from secretly tracking where you go and what pages you look at on the web. If an advertiser seems to be tracking you across multiple websites without your permission, Privacy Badger automatically blocks that advertiser from loading any more content in your browser. To the advertiser, it’s like you suddenly disappeared.&lt;/p&gt;
    &lt;head rend="h3"&gt;How is Privacy Badger different from other blocking extensions?&lt;/head&gt;
    &lt;p&gt;Privacy Badger was born out of our desire to be able to recommend a single extension that would:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically analyze and block any tracker or ad that violated the principle of user consent&lt;/item&gt;
      &lt;item&gt;Function well without any settings, knowledge, or configuration by the user&lt;/item&gt;
      &lt;item&gt;Use algorithmic methods to decide what is and isn’t tracking&lt;/item&gt;
      &lt;item&gt;Be produced by an organization that is unambiguously working for its users rather than for profit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a result, Privacy Badger differs from traditional ad-blocking extensions in two key ways. First, while most other blocking extensions prioritize blocking ads, Privacy Badger doesn’t block ads unless they happen to be tracking you; in fact, one of our goals is to incentivize advertisers to adopt better privacy practices.&lt;/p&gt;
    &lt;p&gt;Second, most other blockers rely on a human-curated list of domains or URLs to block. Privacy Badger is an algorithmic tracker blocker – we define what âtrackingâ looks like, and then Privacy Badger blocks or restricts domains that it observes tracking in the wild. What is and isnât considered a tracker is entirely based on how a specific domain acts, not on human judgment.&lt;/p&gt;
    &lt;p&gt;Privacy Badger sends the Global Privacy Control signal to opt you out of data sharing and selling, and the Do Not Track signal to tell companies not to track you. If trackers ignore these signals, Privacy Badger will learn to block them.&lt;/p&gt;
    &lt;p&gt;Beyond this, Privacy Badger comes with other advantages like cookie blocking, click-to-activate placeholders for potentially useful tracker widgets (video players, comments widgets, etc.), and outgoing link click tracking removal on Facebook and Google.&lt;/p&gt;
    &lt;p&gt;By using Privacy Badger, you support the Electronic Frontier Foundation and help fight for a better Web for everybody.&lt;/p&gt;
    &lt;head rend="h3"&gt;Who makes Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Privacy Badger was created by the Electronic Frontier Foundation, a nonprofit organization that protects your privacy and free expression online. We make free tools like Privacy Badger, publish educational guides, testify before lawmakers about technology, and fight for the public interest in courtâall thanks to support from EFFâs members. If you want a better internet and a strong democracy, join the fight against creepy online surveillance.&lt;/p&gt;
    &lt;head rend="h3"&gt;How does Privacy Badger work?&lt;/head&gt;
    &lt;p&gt;When you view a webpage, that page will often be made up of content from many different sources. For example, a news webpage might load the actual article from the news company, ads from an ad company, and the comments section from a different company that’s been contracted out to provide that service.&lt;/p&gt;
    &lt;p&gt;Privacy Badger keeps track of all of this. If the same source seems to be tracking across different websites, then Privacy Badger springs into action, telling the browser not to load any more content from that source. And when your browser stops loading content from a source, that source can no longer track you. Voila!&lt;/p&gt;
    &lt;p&gt;At a more technical level, Privacy Badger keeps track of the “third party” domains that embed images, scripts and advertising in the pages you visit. Privacy Badger looks for tracking techniques like uniquely identifying cookies, local storage “supercookies,” and canvas fingerprinting. If it observes the same third-party host tracking on three separate sites, Privacy Badger will automatically disallow content from that third-party tracker.&lt;/p&gt;
    &lt;p&gt;By default, Privacy Badger receives periodic learning updates from Badger Sett, our Badger training project. This “remote learning” automatically discovers trackers present on thousands of the most popular sites on the Web.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is a third party tracker?&lt;/head&gt;
    &lt;p&gt;When you visit a webpage parts of the page may come from domains and servers other than the one you asked to visit. This is an essential feature of hypertext. On the modern Web, embedded images and code often use cookies and other methods to track your browsing habits â often to display advertisements. The domains that do this are called “third party trackers”, and you can read more about how they work here.&lt;/p&gt;
    &lt;head rend="h3"&gt;What do the red, yellow and green sliders in the Privacy Badger menu mean?&lt;/head&gt;
    &lt;p&gt;Red means that content from this third party domain has been completely disallowed.&lt;/p&gt;
    &lt;p&gt;Yellow means that the third party domain appears to be trying to track you, but it is on Privacy Badger’s cookie-blocking “yellowlist” of third party domains that, when analyzed, seemed to be necessary for Web functionality. In that case, Privacy Badger will load content from the domain but will try to screen out third party cookies and referrers from it.&lt;/p&gt;
    &lt;p&gt;Green means “no action”; Privacy Badger will leave the domain alone.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does Privacy Badger block ads?&lt;/head&gt;
    &lt;p&gt;Actually, nothing in the Privacy Badger code is specifically written to block ads. Rather, it focuses on disallowing any visible or invisible “third party” scripts or images that appear to be tracking you even though you specifically denied consent by sending Do Not Track and Global Privacy Control signals. It just so happens that most (but not all) of these third party trackers are advertisements. When you see an ad, the ad sees you, and can track you. Privacy Badger is here to stop that.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why doesn't Privacy Badger block all ads?&lt;/head&gt;
    &lt;p&gt;Because Privacy Badger is primarily a privacy tool, not an ad blocker. Our aim is not to block ads, but to prevent non-consensual invasions of people’s privacy because we believe they are inherently objectionable. We also want to create incentives for advertising companies to do the right thing. Of course, if you really dislike ads, you can also install a traditional ad blocker.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is Global Privacy Control (GPC)?&lt;/head&gt;
    &lt;p&gt;Global Privacy Control (GPC) is a new specification that allows users to tell companies they’d like to opt out of having their data shared or sold. By default, Privacy Badger sends the GPC signal to every company you interact with alongside the Do Not Track (DNT) signal.&lt;/p&gt;
    &lt;p&gt;What’s the difference? Do Not Track is meant to tell companies that you don’t want to be tracked in any way (learn more about what we mean by “tracking” here). Privacy Badger gives third-party companies a chance to comply with DNT by adopting our DNT policy, and blocks those that look like they’re tracking you anyway.&lt;/p&gt;
    &lt;p&gt;When DNT was developed, many websites simply ignored usersâ requests not to be tracked. That’s why Privacy Badger has to act as an enforcer: trackers that don’t want to comply with your wishes get blocked. Today, users in many jurisdictions have the legal right to opt out of some kinds of tracking. That’s where GPC comes in.&lt;/p&gt;
    &lt;p&gt;GPC is meant to be a legally-binding request to all companies in places with applicable privacy laws. For example, the California Consumer Privacy Act gives California residents the right to opt out of having their data sold. By sending the GPC signal, Privacy Badger is telling companies that you would like to exercise your rights.&lt;/p&gt;
    &lt;p&gt;The CCPA and other laws are not perfect, which is why Privacy Badger uses both approaches. It asks websites to respect your privacy, and it blocks known trackers from loading at all.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about tracking by the sites I actively visit, like NYTimes.com or Facebook.com?&lt;/head&gt;
    &lt;p&gt;At present, Privacy Badger primarily protects you against tracking by third party sites. As far as privacy protections for “first party” sites (sites that you visit directly), Privacy Badger removes outgoing link click tracking on Facebook and Google. We plan on adding more first party privacy protections in the future.&lt;/p&gt;
    &lt;p&gt;We are doing things in this order because the most scandalous, intrusive and objectionable form of online tracking is that conducted by companies you’ve often never heard of and have no relationship with. First and foremost, Privacy Badger is there to enforce Do Not Track against these domains by providing the technical means to restrict access to their tracking scripts and images. The right policy for whether nytimes.com, facebook.com or google.com can track you when you visit that site â and the technical task of preventing it â is more complicated because often tracking is interwoven with the features the site offers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does Privacy Badger contain a list of blocked sites?&lt;/head&gt;
    &lt;p&gt;Unlike other blocking tools, we have not made decisions about which sites to block, but rather about which behavior is objectionable. Domains will only be blocked if Privacy Badger observes the domain collecting unique identifiers after it was sent Do Not Track and Global Privacy Control signals.&lt;/p&gt;
    &lt;p&gt;Privacy Badger does contain a “yellowlist” of some sites that are known to provide essential third party resources; those sites show up as yellow and have their cookies blocked rather than being blocked entirely. This is a compromise with practicality, and in the long term we hope to phase out the yellowlist as these third parties begin to explicitly commit to respecting Do Not Track. The criteria for including a domain on the yellowlist can be found here.&lt;/p&gt;
    &lt;head rend="h3"&gt;How was the cookie blocking yellowlist created?&lt;/head&gt;
    &lt;p&gt;The initial list of domains that should be cookie blocked rather than blocked entirely was derived from a research project on classifying third party domains as trackers and non-trackers. We will make occasional adjustments to it as necessary. If you find domains that are under- or over-blocked, please file a bug on GitHub.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does Privacy Badger prevent fingerprinting?&lt;/head&gt;
    &lt;p&gt;Browser fingerprinting is an extremely subtle and problematic method of tracking, which we documented with the Cover Your Tracks project. Privacy Badger can detect canvas-based fingerprinting, and will block third party domains that use it. Detection of other forms of fingerprinting and protections against first-party fingerprinting are ongoing projects. Of course, once a domain is blocked by Privacy Badger, it will no longer be able to fingerprint you.&lt;/p&gt;
    &lt;head rend="h3"&gt;Does Privacy Badger consider every cookie to be a tracking cookie?&lt;/head&gt;
    &lt;p&gt;No. Privacy Badger analyzes the cookies from each site; unique cookies that contain tracking IDs are disallowed, while “low entropy” cookies that perform other functions are allowed. For instance a cookie like LANG=fr that encodes the user’s language preference, or a cookie that preserves a very small amount of information about ads the user has been shown, would be allowed provided that individual or small groups of users’ reading habits could not be collected with them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Will you be supporting any other browsers besides Chrome, Firefox, Edge and Opera?&lt;/head&gt;
    &lt;p&gt;We are working towards Safari on macOS support. Safari on iOS seems to lack certain extension capabilities required by Privacy Badger to function properly.&lt;/p&gt;
    &lt;p&gt;Chrome on Android does not support extensions. To use Privacy Badger on Android, install Firefox for Android.&lt;/p&gt;
    &lt;p&gt;Privacy Badger does not work with Microsoft Edge Legacy. Please switch to the new Microsoft Edge browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I download Privacy Badger directly from eff.org?&lt;/head&gt;
    &lt;p&gt;If you use Google Chrome, you have to install extensions from Chrome Web Store. To install Privacy Badger in Chrome, visit Privacy Badger’s Chrome Web Store listing and click the “Add to Chrome” button there.&lt;/p&gt;
    &lt;p&gt;Otherwise, you can use the following links to get the latest version of Privacy Badger directly from eff.org:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Firefox: https://www.eff.org/files/privacy-badger-latest.xpi&lt;/item&gt;
      &lt;item&gt;Chromium: https://www.eff.org/files/privacy_badger-chrome.crx&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;I run a domain that uses cookies or other tracking. How do I stop Privacy Badger from blocking me?&lt;/head&gt;
    &lt;p&gt;One way is to stop tracking users who have turned on Global Privacy Control or Do Not Track signals (i.e., stop collecting cookies, supercookies or fingerprints from them). Privacy Badger will stop learning to block that domain. The next version of Privacy Badger to ship with an updated pre-trained list will no longer include that domain in the list. Most Privacy Badger users will then update to that list.&lt;/p&gt;
    &lt;p&gt;You can also unblock yourself by promising to meaningfully respect the Do Not Track signal. To do so, post a verbatim copy of EFF’s Do Not Track policy to the URL https://example.com/.well-known/dnt-policy.txt, where “example.com” is replaced by your domain. Posting EFF’s DNT policy on a domain is a promise of compliance with EFF’s DNT Policy by that domain.&lt;/p&gt;
    &lt;p&gt;If your domain is compliant with EFF’s DNT policy and declares this compliance, most Privacy Badgers will see this declaration the next time they encounter your domain. Also, the next version of Privacy Badger to ship with an updated pre-trained list will probably include your declaration of compliance in the list.&lt;/p&gt;
    &lt;p&gt;Note that the domain must support HTTPS, to protect against tampering by network attackers. The path contains “.well-known” per RFC 5785. Also note that you must post a copy of the policy at each compliant subdomain you control. For example, if you wish to declare compliance by both sub1.example.com and sub2.example.com, you must post EFF’s DNT policy on each domain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where can I find general information about Privacy Badger that I can use for a piece I'm writing?&lt;/head&gt;
    &lt;p&gt;Glad you asked! Check out this downloadable press kit that we’ve put together.&lt;/p&gt;
    &lt;head rend="h3"&gt;As an administrator, how do I configure Privacy Badger on my managed devices?&lt;/head&gt;
    &lt;p&gt;Please see our enterprise deployment and configuration document.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is the Privacy Badger license? Where is the Privacy Badger source code?&lt;/head&gt;
    &lt;p&gt;Privacy Badger’s source code is licensed under GPLv3+. This website’s source code is licensed under AGPLv3+.&lt;/p&gt;
    &lt;head rend="h3"&gt;How can I support Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Thanks for asking! Individual donations make up about half of EFF’s support, which gives us the freedom to work on user-focused projects. If you want to support the development of Privacy Badger and other projects like it, you can throw us a few dollars here. Thank you.&lt;/p&gt;
    &lt;p&gt;If you want to help directly with the project, we appreciate that as well. Please see Privacy Badger’s CONTRIBUTING document for ways to get started.&lt;/p&gt;
    &lt;head rend="h3"&gt;How does Privacy Badger handle social media widgets?&lt;/head&gt;
    &lt;p&gt;Social media widgets (such as the Facebook Like button) often track your reading habits. Even if you don’t click them, the social media companies often see exactly which pages you’re seeing the widget on. When blocking social buttons and other potentially useful (video, audio, comments) widgets, Privacy Badger can replace them with click-to-activate placeholders. You will not be tracked by these replacements unless you explicitly choose to click them.&lt;/p&gt;
    &lt;head rend="h3"&gt;How do I uninstall/remove Privacy Badger?&lt;/head&gt;
    &lt;p&gt;Firefox: See the Disable or remove Add-ons Mozilla help page.&lt;/p&gt;
    &lt;p&gt;Chrome: See the Install and manage extensions Chrome Web Store help page.&lt;/p&gt;
    &lt;p&gt;Edge: See the Add or remove browser add-ons, extensions, and toolbars Microsoft help page.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is Privacy Badger compatible with other extensions, including adblockers?&lt;/head&gt;
    &lt;p&gt;Privacy Badger should be compatible with other extensions.&lt;/p&gt;
    &lt;p&gt;While there is likely to be overlap between the various manually-edited advertising/tracker lists and Privacy Badger, unlike adblockers, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger may learn to block trackers your adblocker doesn’t know about.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is Privacy Badger compatible with Firefox's built-in privacy protections?&lt;/head&gt;
    &lt;p&gt;It’s fine to use Firefox’s built-in content blocking (Enhanced Tracking Protection or ETP) and Privacy Badger together. While there is overlap between Firefox’s tracker lists and Privacy Badger, Privacy Badger automatically learns to block trackers based on their behavior. This means that Privacy Badger’s automatically-generated and regularly updated blocklist contains trackers not found in Firefox’s human-generated lists. Additionally, Firefox does not fully block “tracking content” in regular (non-“private”) windows by default.&lt;/p&gt;
    &lt;p&gt;What about Firefox’s Total Cookie Protection (dynamic First Party Isolation or dFPI)? Total Cookie Protection works by keeping third-party cookies isolated to the site they were set on. However, if unblocked, trackers can still use techniques like first-party cookie syncing and browser fingerprinting. They can track your IP address, or they can use some combination of these techniques. Trackers harvest sensitive information, and serve as vectors for malware. Not to mention, unblocked trackers slow down websites and waste your bandwidth.&lt;/p&gt;
    &lt;p&gt;Keep in mind that Privacy Badger is not just a tracker blocker.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does my browser connect to fastly.com IP addresses on startup after installing Privacy Badger?&lt;/head&gt;
    &lt;p&gt;EFF uses Fastly to host EFF’s Web resources: Fastly is EFF’s CDN. Privacy Badger pings the CDN for the following resources to ensure that the information in them is fresh even if there hasn’t been a new Privacy Badger release in a while:&lt;/p&gt;
    &lt;p&gt;EFF does not set cookies or retain IP addresses for these queries.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why does Privacy Badger need access to my data for all websites?&lt;/head&gt;
    &lt;p&gt;When you install Privacy Badger, your browser warns that Privacy Badger can “access your data for all websites” (in Firefox), or “read and change all your data on the websites you visit” (in Chrome). You are right to be alarmed. You should only install extensions made by organizations you trust.&lt;/p&gt;
    &lt;p&gt;Privacy Badger requires these permissions to do its job of automatically detecting and blocking trackers on all websites you visit. We are not ironically (or unironically) spying on you. For more information, see our Privacy Badger extension permissions explainer.&lt;/p&gt;
    &lt;p&gt;Note that the extension permissions warnings only cover what the extension has access to, not what the extension actually does with what it has access to (such as whether the extension secretly uploads your browsing data to its servers). Privacy Badger will never share data about your browsing unless you choose to share it (by filing a broken site report). For more information, see EFF’s Privacy Policy for Software.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why aren't videos loading on YouTube? Why isn't Privacy Badger blocking ads on YouTube?&lt;/head&gt;
    &lt;p&gt;Is YouTube not working? Try disabling Privacy Badger on YouTube. If that resolves the issue, see if re-enabling Privacy Badger breaks YouTube again. If YouTube goes back to not working, please tell us so we can look into what’s going on.&lt;/p&gt;
    &lt;p&gt;Are you surprised that ads aren’t being blocked on YouTube? Privacy Badger is primarily a privacy tool, not an ad blocker. When you visit YouTube directly, Privacy Badger does not block ads on YouTube because YouTube does not use “third party” trackers. If you really dislike ads, you can also install a traditional ad blocker.&lt;/p&gt;
    &lt;head rend="h3"&gt;I need help! I found a bug! What do I do now?&lt;/head&gt;
    &lt;p&gt;If a website isn’t working like it should, you can disable Privacy Badger just for that site, leaving Privacy Badger enabled and protecting you everywhere else. To do so, navigate to the site with the problem, click on Privacy Badger’s icon in your browser toolbar, and click the “Disable for this site” button in Privacy Badger’s popup. You can also let us know about broken sites using the “Report broken site” button.&lt;/p&gt;
    &lt;p&gt;To get help or to report bugs, please email extension-devs@eff.org. If you have a GitHub account, you can use our GitHub issue tracker.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45404021</guid><pubDate>Sun, 28 Sep 2025 12:59:54 +0000</pubDate></item><item><title>When I say "alphabetical order", I mean "alphabetical order"</title><link>https://sebastiano.tronto.net/blog/2025-09-28-alphabetic-order/</link><description>&lt;doc fingerprint="144ce85c2ae519d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;When I say “alphabetical order”, I mean “alphabetical order”&lt;/head&gt;
    &lt;p&gt;Last month I have been on a multi-day hike with my dad. Each of us took many pictures, and when we came back we put them all in a shared folder. We both have Android phones, and the naming scheme used for our pictures was the same: &lt;code&gt;IMG_YYYYMMDD_HHmmss&lt;/code&gt; followed maybe by some other numbers
and then a &lt;code&gt;.jpg&lt;/code&gt;. Here &lt;code&gt;YYYY&lt;/code&gt; stands for the year, &lt;code&gt;MM&lt;/code&gt; for month and
so on, so that sorting the pictures in alphabetical order is the same as
sorting them by date.&lt;/p&gt;
    &lt;p&gt;Or so I thought. Strangely, when I looked at the files from my dad’s Windows PC, they were not sorted correctly: all the pictures took with my phone came first, followed by all the pictures took by him. I thought this was surely some weird Microsoft bug - after using Windows 11 at work for a while, I would not be surprised if you told me their file explorer can’t figure out how to sort strings.&lt;/p&gt;
    &lt;p&gt;But then I looked at the same files in a shared Google Drive folder, and again they were in the wrong order:&lt;/p&gt;
    &lt;p&gt;As you can see, the picture taken at 5:54 (with my dad’s phone) comes before the one taken at 9:20 (also with my dad’s phone), but after the one taken at 12:11 (with my phone).&lt;/p&gt;
    &lt;p&gt;Weird. Well, maybe Microsoft and Google got this wrong. But that seems unlikely.&lt;/p&gt;
    &lt;p&gt;Indeed, KDE’s Dolphin file manager does the same thing:&lt;/p&gt;
    &lt;p&gt;I’ll spare you the screenshots, but Gnome and both the file managers that I have on my phone also get the alphabetical order wrong.&lt;/p&gt;
    &lt;p&gt;At this point I thought that maybe one of the two phones is using some weird alternative unicode character instead of the underscore &lt;code&gt;_&lt;/code&gt;. Really,
I could not see any other explanation. But nope, this is not it, because
the good old &lt;code&gt;ls&lt;/code&gt; sorts my files correctly:&lt;/p&gt;
    &lt;code&gt;$ ls -l

total 218572
-rw-r--r-- 1 seba seba 1866185 Aug 28 18:51 IMG_20250820_055436307.jpg
-rw-r--r-- 1 seba seba 4749899 Aug 28 18:50 IMG_20250820_092016029_HDR.jpg
-rw-r--r-- 1 seba seba 6201609 Aug 28 18:52 IMG_20250820_092440966_HDR.jpg
-rw-r--r-- 1 seba seba 7694802 Aug 28 18:51 IMG_20250820_092832138_HDR.jpg
-rw-r--r-- 1 seba seba 1536520 Aug 20 09:57 IMG_20250820_095716_607.jpg
-rw-r--r-- 1 seba seba 1054553 Aug 20 10:38 IMG_20250820_103857_991.jpg
-rw-r--r-- 1 seba seba  965353 Aug 20 10:39 IMG_20250820_103903_811.jpg
(and so on)
&lt;/code&gt;
    &lt;p&gt;This was consistent among the couple of Linux distros I use, as well as my OpenBSD server. On the one hand this is good: not every single piece of software fucks up something as basic as string sorting. On the other hand, this makes it harder to debug what the fuck is going on with all the other file managers.&lt;/p&gt;
    &lt;p&gt;It took me more than a month to figure this one out. Tell me, which file do you think comes first in alphabetical order, &lt;code&gt;file-9.txt&lt;/code&gt; or
&lt;code&gt;file-10.txt&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Of course, the user who named those files probably wants &lt;code&gt;file-9.txt&lt;/code&gt; to
come before &lt;code&gt;file-10.txt&lt;/code&gt;. But &lt;code&gt;1&lt;/code&gt; is smaller than &lt;code&gt;9&lt;/code&gt;, so &lt;code&gt;file-10.txt&lt;/code&gt;
should be first in alphabetical order. Everyone understands that, and
soon people learn to put enough leading zeros if they want their files
to stay sorted the way they like.&lt;/p&gt;
    &lt;p&gt;Well, apparently all these operating systems have decided that no, users are too dumb and they cannot possibly understand what alphabetical order means. So when you ask them to sort your files alphabetically, they don’t. Instead, they decide that if some piece of the file name is a number, the real numerical value must be used.&lt;/p&gt;
    &lt;p&gt;I don’t know when this became the norm, to be honest I have not used a normal graphical file manager in a long time.&lt;/p&gt;
    &lt;p&gt;I know you asked for the files to be sorted in alphabetical order, but you don’t want &lt;code&gt;file-10.txt&lt;/code&gt; to come before &lt;code&gt;file-9.txt&lt;/code&gt;, do
you? No, I know you don’t. I am not even going to ask you, your
mushy human brain is too small to comprehend the intricacies of
such a question. I’ll spare you the thinking.&lt;/p&gt;
    &lt;p&gt;So it turns out that my dad’s phone wrote the milliseconds in the file name right after the seconds, while mine added an extra underscore to separate them from the seconds. Which in my mind it should not have mattered, because alphabetically they should still have been sorted correctly to the second. But with this “modern” interpretation of the alphabetical order, the files without the extra separator in the name had a much higher number, so they come last.&lt;/p&gt;
    &lt;p&gt;Now that I know what the issue is, I can solve it by renaming the files with a consistent scheme. I have also found a setting to fix Dolphin’s behavior, but it was very much buried into its many configuration options. And I would rather not have to change this setting in every application I use, assuming they even allow it.&lt;/p&gt;
    &lt;p&gt;I miss the time when computers did what you told them to, instead of trying to read your mind.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45404022</guid><pubDate>Sun, 28 Sep 2025 13:00:16 +0000</pubDate></item><item><title>Plan 9 from User Space</title><link>https://github.com/9fans/plan9port</link><description>&lt;doc fingerprint="a605bc5c90f1b0c2"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a port of many Plan 9 libraries and programs to Unix.&lt;/p&gt;
    &lt;p&gt;To install, run ./INSTALL. It builds mk and then uses mk to run the rest of the installation.&lt;/p&gt;
    &lt;p&gt;For more details, see install(1), at install.txt in this directory and at https://9fans.github.io/plan9port/man/man1/install.html.&lt;/p&gt;
    &lt;p&gt;See https://9fans.github.io/plan9port/man/ for more documentation. (Documentation is also in this tree, but you need to run a successful install first. After that, "9 man 1 intro".)&lt;/p&gt;
    &lt;p&gt;Intro(1) contains a list of man pages that describe new features or differences from Plan 9.&lt;/p&gt;
    &lt;p&gt;If you'd like to help out, great!&lt;/p&gt;
    &lt;p&gt;If you port this code to other architectures, please share your changes so others can benefit.&lt;/p&gt;
    &lt;p&gt;You can use Git to keep your local copy up-to-date as we make changes and fix bugs. See the git(1) man page here ("9 man git") for details on using Git.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Mailing list: https://groups.google.com/group/plan9port-dev&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Issue tracker: https://github.com/9fans/plan9port/issues&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Submitting changes: https://github.com/9fans/plan9port/pulls&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Russ Cox rsc@swtch.com&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45404573</guid><pubDate>Sun, 28 Sep 2025 14:27:44 +0000</pubDate></item><item><title>Show HN: Toolbrew – Free little tools without signups or ads</title><link>https://toolbrew.co/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45404667</guid><pubDate>Sun, 28 Sep 2025 14:40:46 +0000</pubDate></item><item><title>What's New in PostgreSQL 18 – A Devel</title><link>https://www.bytebase.com/blog/what-is-new-in-postgres-18-for-developer/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405055</guid><pubDate>Sun, 28 Sep 2025 15:27:18 +0000</pubDate></item><item><title>Scm2wasm: A Scheme to WASM compiler in 600 lines of C, making use of WASM GC</title><link>https://git.lain.faith/iitalics/scm2wasm</link><description>&lt;doc fingerprint="9fa21c334d8329a0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;milo 7cbcaf8ccd&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.gitignore&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Makefile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;README.md&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;input.scm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;scm2wasm.c&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt; README.md &lt;/head&gt;
    &lt;head rend="h1"&gt;scm2wasm&lt;/head&gt;
    &lt;p&gt;really bad minimal scheme compiler&lt;/p&gt;
    &lt;head rend="h2"&gt;building&lt;/head&gt;
    &lt;code&gt;$ make
&lt;/code&gt;
    &lt;head rend="h2"&gt;running&lt;/head&gt;
    &lt;code&gt;$ ./scm2wasm &amp;lt; input.scm &amp;gt; output.wasm
$ wasm-tools validate output.wasm
$ wasm-tools print output.wasm -o output.wat
$ wasmtime -Wgc --invoke start output.wasm
...
30
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405175</guid><pubDate>Sun, 28 Sep 2025 15:43:25 +0000</pubDate></item><item><title>The AI coding trap</title><link>https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap</link><description>&lt;doc fingerprint="a549d23b2ab12319"&gt;
  &lt;main&gt;
    &lt;p&gt;If you ever watch someone “coding”, you might see them spending far more time staring into space than typing on their keyboard. No, they (probably) aren’t slacking off. Software development is fundamentally a practice of problem-solving, and so, as with solving a tricky crossword, most of the work is done in your head.&lt;/p&gt;
    &lt;p&gt;In the software development lifecycle, coding is the letters filled into the crossword, only a small amount of effort compared to all the head scratching and scribbled notes. The real work usually happens alongside coding, as the developer learns the domain, narrows down requirements, maps out relevant abstractions, considers side effects, tests features incrementally, and finally squashes bugs that survived this rigorous process. It looks something like this:&lt;/p&gt;
    &lt;p&gt;But with AI-driven coding, things play out very differently.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Code first, ask questions later”&lt;/head&gt;
    &lt;p&gt;AI coding agents such as Claude Code are making it astonishingly fast to write code in isolation. But most software lives within complex systems, and since LLMs can't yet hold the full context of an application in memory at once, human review, testing, and integration needs will remain. And that is a lot harder when the code has been written without the human thinking about it. As a result, for complex software, much of the time will be spent on post hoc understanding of what code the AI has written.&lt;/p&gt;
    &lt;p&gt;This is the root of the difference between marketing copy that boasts of the paradigm shifting speed of writing code with AI (often framed as “10X faster”), and the marginal productivity gains in delivering working software seen in the wild (usually closer to 10%).&lt;/p&gt;
    &lt;p&gt;An even more dispiriting upshot of this is that, as developers, we spend an ever greater proportion of our time merely fixing up the output of these wondrous babbling machines. While the LLMs get to blast through all the fun, easy work at lightning speed, we are then left with all the thankless tasks: testing to ensure existing functionality isn’t broken, clearing out duplicated code, writing documentation, handling deployment and infrastructure, etc. Very little time is actually dedicated to the thing that developers actually love doing: coding.&lt;/p&gt;
    &lt;p&gt;Fortunately, help is at hand. While LLMs are shaking up how software development is performed, this issue in itself is not actually new. In fact, it is merely a stark example of an age-old problem, which I call:&lt;/p&gt;
    &lt;head rend="h2"&gt;The tech lead’s dilemma&lt;/head&gt;
    &lt;p&gt;As engineers progress in their careers, they will eventually step into the role of tech lead. They might be managing a team, or they could be a principal engineer, driving technical delivery without the people management. In either case, they are responsible for the team’s technical delivery. They are also usually the most experienced developer in the team: either in their career, in the specialised domain of the team, or in both.&lt;/p&gt;
    &lt;p&gt;Software delivery is a team effort, but one in which experience can have a highly imbalancing effect on individual contribution velocity. As such, when the tech lead’s primary job is to maximise delivery, they will often face an internal conflict between two ways to deliver software:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fair delegation across the team, maximising learning and ownership opportunities for junior team members, but allowing delivery to be bottlenecked by the speed of the least productive team members.&lt;/item&gt;
      &lt;item&gt;Mollycoddling the team, by delegating only the easy or non-critical work to juniors, and keeping the hardest work for themselves, as the person on the team most capable of delivering at speed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, while we shall see that mollycoddling is extremely harmful to long-term team health, it is also often a very effective way to accelerate delivery. The higher bandwidth of the tech lead is often most efficiently deployed by eating up all the hardest work:&lt;/p&gt;
    &lt;p&gt;As such, I have seen this pattern repeated time and again over the course of my career. And, of course, it comes at a cost. Siloing of experience in the tech lead makes the team brittle, it makes support harder, and it places ever greater pressure on the tech lead as a single point of failure. What follows next is predictable: burnout, departure, and ensuing crisis as the team struggles to survive without the one person who really knows how everything works.&lt;/p&gt;
    &lt;p&gt;As is usually the case, the solution lies in a third way that avoids these two extremes and balances delivery with team growth. We might frame it as something like:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Implement team practices that allow engineers to deliver working code within a framework that minimises rework, maximises effective collaboration, and promotes personal growth and learning.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When I was CTO of Datasine, we enshrined this attitude in a simple tech team motto:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Learn. Deliver. Have fun.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Good tech leads expose their engineers to work at the limit of their capabilities, using processes and practices that minimise delivery risk while also enabling each team member to grow their skills, knowledge, and domain expertise. This is, in fact, the essence of good technical leadership.&lt;/p&gt;
    &lt;p&gt;There are many ways to accomplish it, from strict codified frameworks such as the Extreme Programming rules, through to looser sets of principles which we might broadly refer to as “best practices”:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code reviews&lt;/item&gt;
      &lt;item&gt;Incremental delivery&lt;/item&gt;
      &lt;item&gt;Modular design&lt;/item&gt;
      &lt;item&gt;Test-driven development&lt;/item&gt;
      &lt;item&gt;Pair programming&lt;/item&gt;
      &lt;item&gt;Quality documentation&lt;/item&gt;
      &lt;item&gt;Continuous integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, for experienced engineers today, an urgent question is: how can we translate these practices into a world of AI-driven coding?&lt;/p&gt;
    &lt;head rend="h2"&gt;LLMs are lightning fast junior engineers&lt;/head&gt;
    &lt;p&gt;In 2025, many engineers are finding themselves for the first time in a position familiar to every tech lead: overseeing a brilliant but unpredictable junior engineer. Harnessing and controlling such talent, in a way that benefits effective team collaboration, is one of the primary challenges of engineering leadership. But AI coding agents need different management to junior engineers, because the nature of their productivity and growth is fundamentally different.&lt;/p&gt;
    &lt;p&gt;As software engineers gain experience, we tend to improve our productivity in multiple ways at the same time: writing more robust code, using better abstractions, spending less time writing and fixing bugs, understanding more complex architectures, covering edge cases more effectively, spotting repeated patterns earlier, etc. Engineering is a rich and complex discipline with many avenues for specialisation, but for simplicity we might group these dimensions into two broad themes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Quality: ability to deliver more complex, more performant, more maintainable code&lt;/item&gt;
      &lt;item&gt;Velocity: ability to develop working, bug-free code in a shorter space of time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Over time, good engineers will improve in both axes.&lt;/p&gt;
    &lt;p&gt;Early LLMs were fast to write code, but time spent fixing bugs and removing hallucinations meant they were slow to complete bug-free code. Over time, smarter LLMs and better use of context engineering and tools have meant that modern AI coding agents are much better at “one shot” writing of code. The current generation of commercially available agents can be incredibly fast at producing working code for problems that would challenge some mid-level engineers, though they cannot yet match the expertise of senior engineers:&lt;/p&gt;
    &lt;p&gt;So we can think of the current generation of AI coding agents as junior engineers, albeit with two fundamental differences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLMs deliver code much, much faster than junior engineers, constrained neither by thinking nor writing time;&lt;/item&gt;
      &lt;item&gt;LLMs have no true capacity to learn, and instead only improve through more effective context engineering or the arrival of new foundation models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As with junior engineering talent, there are broadly two ways that you can deploy them, depending on whether your focus is long-term or short-term:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI-driven engineering: employing best practices, foregrounding human understanding of the code, moving slowly to make development sustainable.&lt;/item&gt;
      &lt;item&gt;Vibe coding: throwing caution to the wind and implementing at speed, sacrificing understanding for delivery velocity, hitting an eventual wall of unsalvageable, messy code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As might be expected, the long-term trajectories of choosing between these two approaches follow much the same pattern as choosing between parallel delegation and mollycoddling of a junior team:&lt;/p&gt;
    &lt;p&gt;This is why the vibe coding approach is great for tiny projects or throwaway prototypes: applications of sufficient simplicity can be delivered without the need for any human thinking at all. By limiting the complexity of our projects and leaning into the capabilities of the tools, we can deliver end-to-end working software in no time at all.&lt;/p&gt;
    &lt;p&gt;But you will hit a wall of complexity that AI is incapable of scaling alone.&lt;/p&gt;
    &lt;p&gt;Building prototypes is now easier than ever. But if we want to effectively use LLMs to accelerate delivery of real, complex, secure, working software, and to realise more than marginal efficiency gains, we need to write a new playbook of engineering practices tailored to maximise collaboration between engineering teams that include both humans and LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to avoid the AI coding trap&lt;/head&gt;
    &lt;p&gt;AI coding agents are dazzlingly productive, but lack in-depth knowledge of your business, codebase, or roadmap. Left unchecked, they will happily churn out thousands of lines of code with no heed paid to design, consistency, or maintainability. The job of the engineer, then, is to act as a tech lead to these hotshots: to provide the structure, standards, and processes that convert raw speed into sustainable delivery.&lt;/p&gt;
    &lt;p&gt;We need a new playbook for how to deliver working software efficiently, and we can look to the past to learn how to do that. By treating LLMs as lightning-fast junior engineers, we can lean on best practices from the software development lifecycle to build systems that scale.&lt;/p&gt;
    &lt;p&gt;Just as tech leads don't just write code but set practices for the team, engineers now need to set practices for AI agents. That means bringing AI into every stage of the lifecycle:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Specification: exploring, analysing, and refining feature specifications to cover edge cases and narrow focus.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Documentation: generating and reviewing documentation up front to provide reusable guardrails and lasting evidence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Modular Design: scaffolding modular architectures to control context scope and maximise comprehension.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Test-Driven Development: generating extensive test cases prior to implementation to guide implementation and prevent regression.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Coding Standards: applying house styles and best practice when generating code, through context engineering.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Monitoring &amp;amp; Introspection: analysing logs and extracting insights faster than any human ever could.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;By understanding that delivering software is so much more than just writing code, we can avoid the AI coding trap and instead hugely amplify our ability to deliver working, scalable software.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405177</guid><pubDate>Sun, 28 Sep 2025 15:43:33 +0000</pubDate></item><item><title>Why I'm not rushing to take sides in the RubyGems fiasco</title><link>https://justin.searls.co/posts/why-im-not-rushing-to-take-sides-in-the-rubygems-fiasco/</link><description>&lt;doc fingerprint="cd771095c9b21fe4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I'm not rushing to take sides in the RubyGems fiasco&lt;/head&gt;
    &lt;p&gt;We are in the midst of a Ruby drama for the ages. I'm sure a bunch of people figured we were all too old for this shit, but apparently we are not.&lt;/p&gt;
    &lt;p&gt;This debate has been eating at me ever since the news first broke, but I've tried to keep the peace by staying out of it. Unlike most discourse about what's going on, my discomfort stems less from the issue at hand—what Ruby Central did, how they did it, and how poorly it was communicated—and more to do with how one-sided the public discussion has been. Beneath the surface of this story are the consequences of a decade-old conflict that was never fully resolved. Then and now, one side—Andre Arko and many people associated with him—has availed itself of public channels to voice their perspective, while the other—which includes a surprisingly wide swath of well-known Ruby and Rails contributors—has chosen to stay silent.&lt;/p&gt;
    &lt;p&gt;The losers in this dynamic are the vast majority normal everyday Ruby developers, most of whom are operating on very little information and who understandably feel confused and concerned. People whose livelihood depends on the health of the Ruby ecosystem deserve more information than they're getting, especially now that its operational stability has come under threat. The future of that ecosystem is once again uncertain, but—just like last time—the outcome is being shaped by a history that's been kept from the public, widening the rift between its key decision-makers and the communities they serve.&lt;/p&gt;
    &lt;p&gt;I don't have the answers to what's going on in 2025. A few details have been shared with me—details that would contradict fact-checks and timelines others have pieced together and published—but I can't pretend to have a clear picture of what actually happened, why no one is setting the record straight, or when we'll have clarity on what the future holds. All I can do is offer a little bit of context to explain why I'm dubious of the dominant narrative that has taken shape online. Namely, I don't believe this is a cut-and-dry case of altruistic open-source maintainers being persecuted by oppressive corporate interests.&lt;/p&gt;
    &lt;p&gt;After you read this, perhaps your perspective will shift as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;The relevant proper nouns to know&lt;/head&gt;
    &lt;p&gt;Before anything else can make sense, it's important to understand how weird the governance of the Ruby ecosystem is. There are three moving parts involved that are ostensibly managed by three different groups, but whose members have such broadly overlapping systems access that it has now led to disputes over who owns what:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ruby itself, created by Matz and maintained by a large group of (mostly Japanese) committers, who host ruby-lang.org, control the @ruby GitHub organization, and are supported by the Ruby Association&lt;/item&gt;
      &lt;item&gt;RubyGems, specifically the &lt;code&gt;gem&lt;/code&gt;and&lt;code&gt;bundler&lt;/code&gt;CLI tools distributed with the Ruby language, which is hosted under @rubygems on GitHub&lt;/item&gt;
      &lt;item&gt;RubyGems.org, the website, API, and host from which gem dependencies are installed and which has been run by Ruby Central for ages&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If Ruby were invented today, a single party would probably control all three of these things, but it took nearly fifteen years for today's status quo to take shape. Ruby was invented by someone in Japan in the 1990s. RubyGems was created at a conference in Texas by a few Americans in the early 2000s. RubyGems.org only became the de facto canonical host for gems six years later. My impression is that at no point was communication and coordination particularly fluent between the various parties.&lt;/p&gt;
    &lt;p&gt;Adding to this, Bundler—a meta tool for resolving the correct versions of all of a project's gem dependencies and which quickly became vital to nearly all Ruby application development—was created independently of the above players by Yehuda Katz and Carl Lerche. Andre Arko later became the lead maintainer of Bundler, and in 2015 he founded a 501(c)(6) nonprofit called Ruby Together. In 2019, Bundler was folded into RubyGems. In 2022, Ruby Together was absorbed by Ruby Central.&lt;/p&gt;
    &lt;p&gt;Those last two events—the merging of Bundler and the unwinding of Ruby Together—came about after years of bitter conflict and simmering discord that I hope to shed some light on below. My direct involvement with any of these events was extremely minimal, but I had contemporaneous discussions with dozens of the principals involved. I never donated to Ruby Together and have never materially contributed to Bundler or RubyGems. That said, simply being made aware of several incidents as they were playing out in private was enough to leave behind a scar that has never fully healed. I can only imagine how others are feeling right now. Based on how badly things are playing out this time, it seems they were deeply impacted, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;The things people told me&lt;/head&gt;
    &lt;p&gt;The earliest recollection I have of someone telling me about Andre Arko was in the summer of 2015, after getting dinner with a friend who happened to be a Ruby Together board member. The friend explained that Andre believed programmers working on open source tools deserve to earn an income that's commensurate with what salaried engineers earn at the companies who benefit from those tools. As such, Andre's goal with Ruby Together was characterized as an effort to fund development activities—initially his own, but eventually others—by paying themselves a market hourly rate. I remember being extremely sympathetic to this perspective, having also wasted countless hours of my life maintaining open source for free only for others to benefit from it. I also recall a figure like either $200 or $250 per hour being mentioned as the rate he was effectively paying himself. Whatever the rate actually was, I distinctly remember thinking, "holy shit, that's a lot higher than individual donors would probably assume."&lt;/p&gt;
    &lt;p&gt;The first time I remember meeting Andre in person was at Ruby on Ales 2016. I remember trying to make a good impression, because growing my network in the community was the primary reason I spoke at conferences. I was presenting with my beloved 12-inch MacBook, which meant I was traveling with the first iteration of this cursed dongle. Andre needed an adapter, so I ran up to lend him mine. As he was giving it back, I recall him making a half-joking, flippant remark about either his dongle or his computer, saying that "Ruby Together will just buy me another one." It really rubbed me the wrong way. Over the years to follow, more than one person told me stories of Andre paying for shared meals on behalf of Ruby Together without an apparent legitimate justification. They told those stories, I assume, because the attitude he exhibited made them uncomfortable. If I had donated money to Ruby Together and heard the same stories, I would have been upset.&lt;/p&gt;
    &lt;p&gt;For how little has been said about this publicly, a lot of different people told me a lot of concerning stories about Ruby Together over the years, often providing evidence to back it up. I'll do my best to stick to the highlights in this post. Hopefully they will explain why I have not joined the rush to defend the maintainers whose access was recently removed. What Ruby Central did was undoubtedly handled very poorly, but I don't think their bungling of its execution and communication alone is enough to answer the question of what should happen to the future custody and direction of Bundler, RubyGems, and RubyGems.org.&lt;/p&gt;
    &lt;head rend="h3"&gt;2015&lt;/head&gt;
    &lt;p&gt;When Ruby Together first launched in 2015, the website suggested donations went to pay "our team", which initially linked to a list of the board members without any explanation of how the money was being allocated. This resulted in a nonzero number of donors believing they were funding the work of people like Steve Klabnik, Aaron Patterson, and Sarah Mei, when in fact only Andre was being paid at the time. Shortly after the wording was raised as misleading, the team page was updated accordingly.&lt;/p&gt;
    &lt;p&gt;In May of 2015, Andre suggested making support for older versions of Bundler contingent on Heroku paying Ruby Together, which was interpreted as leveraging his control over Bundler as a pay-to-play scheme. Because Heroku serves other people's Ruby apps—many of which aren't updated for very long stretches—the security of their service depends on clear and predictable support windows for Ruby's core technologies, it seems reasonable they would interpret this sudden revocation of support as a pressure tactic, aimed to solicit corporate sponsorship for Ruby Together. (Years later, Andre responded to a feature request from a Heroku engineer, which was interpreted at the time as indicating the feature would be withheld from Bundler because Heroku had failed to pay Ruby Together.)&lt;/p&gt;
    &lt;head rend="h3"&gt;2016&lt;/head&gt;
    &lt;p&gt;The minutes of a December 2016 Ruby Together board meeting were leaked. The document acknowledged who was paying for the RubyGems.org service at the time: "Fastly is comping us something like $35k worth of CDN service per month. And that's on top of Ruby Central paying for $5k of servers and Ruby Together paying for about $15k of dev work every month." The use of "us" in that sentence suggests that Ruby Together was responsible for hosting RubyGems.org, which Ruby Central later went on to publicly dispute. Additionally, one presumes the number of hours and rate paid for development work was determined by Ruby Together itself, rather than being a hard operational cost. Later in the document was a discussion of potential strategies to increase revenue after "new memberships [had] flatlined." Several ideas were discussed, culminating in a proposal to rate-limit access to RubyGems.org as the apparent best option:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Rate limiting RubyGems.org seems like the option that scales most linearly with our costs. Companies that cost us money need to pay more (or stop costing us money), and companies that don't cost us money can continue to have a free ride. To be clear, this would not mean cutting off anyone's access to RubyGems.org. I'm imagining that it would work something like GitHub's model: anonymous access has a low limit, registered accounts have a higher limit, and even higher limits are available at each Ruby Together membership level. There are a lot of implementation details that would need to be worked out, but in general I feel like this is probably the most effective option to make companies feel like they are paying money for something they use and that covers our costs well.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The leaked minutes were widely circulated in private at the time, due largely to outrage over the document's presupposition that Ruby Together was paying to host RubyGems.org (citing "our costs" as scaling with usage), as opposed to paying for developer effort (the costs of which do not scale with usage). The leak left myself and others worried that Andre might leverage his systems access to effectively hold the Ruby ecosystem hostage for the financial benefit of Ruby Together and—since it was compensating his own development efforts—Andre himself.&lt;/p&gt;
    &lt;head rend="h3"&gt;2017&lt;/head&gt;
    &lt;p&gt;In January 2017, Andre added a "post-install message" imploring users to fund Ruby Together, which would be displayed every time anyone installed Bundler. Unlike the aforementioned board meeting, this happened in the open and triggered an immediate backlash before eventually being rolled back. In one comment, Andre defended the action and pointed to Shopify's failure to pay Ruby Together, publicly conflating Ruby Together's sponsorship of development effort with "~$40k/mo worth of servers." But, as Ruby Together's own board minutes from the month prior directly stated, money sent to Ruby Together wasn't going to pay server expenses—server costs were covered by Fastly and Ruby Central.&lt;/p&gt;
    &lt;p&gt;In February 2017, following protracted discussion of the post-install message and the threat of rate-limiting access to gem installs, I agreed to put my name on a letter alongside 18 others (including one of Bundler's creators). The letter requested Ruby Together stop misleading the community in this way. My understanding is that some private one-on-one communication followed, that none of it was particularly productive, and that no formal communication occurred between the two groups afterward.&lt;/p&gt;
    &lt;p&gt;In March 2017, Ruby Central went on the record, attempting to clear up confusion and reassure users that they alone were in control of RubyGems.org, stating that Ruby Together donations were immaterial to its continued operation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Unfortunately, this past year has also given rise to some misunderstandings about this relationship in some quarters: chiefly, that by donating to Ruby Together, companies were paying for the operations of RubyGems. And in turn, that if enough companies didn't donate to Ruby Together, RubyGems would subsequently be in a perilous situation. This isn't so.&lt;/p&gt;
      &lt;p&gt;No one in the Ruby community should worry about the availability or security of RubyGems being connected in any way to the fundraising of Ruby Together. Funds raised by Ruby Together go primarily towards paying developers to add features and fix bugs. Ruby Central, on the other hand, is wholly responsible for the operations and baseline stability of the system. While these two efforts go hand-in-hand, it's vitally important to understand that they are two different things. Ruby Together's requests for donations do not mean that there is any reason for concern about RubyGems' continued existence or operation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Later, in August 2017, Andre accused Google Cloud Platform of wholesale copying gemstash's codebase, going so far as to threaten legal action in his opening message. He juxtaposed the accusation with the complaint that Google had, "repeatedly declined to support Ruby Together." The incident appeared to fit a pattern of behavior to pair high-conflict messaging with an admonition of the target's failure to fund the organization that paid him. Ultimately, Andre's claim turned out to be factually baseless—Google hadn't copied gemstash's code, after all.&lt;/p&gt;
    &lt;head rend="h3"&gt;2018–2024&lt;/head&gt;
    &lt;p&gt;Things quieted down and I didn't hear much about any of this stuff anymore. Eventually, Bundler became part of RubyGems and many folks from Ruby Together migrated to analogous roles at Ruby Central.&lt;/p&gt;
    &lt;head rend="h3"&gt;2025&lt;/head&gt;
    &lt;p&gt;In August 2025, and seemingly out of nowhere, someone pointed me to the project spinel-coop/rv-ruby, an apparent fork of homebrew/homebrew-portable-ruby. I say "apparent", because rather than using GitHub's fork button—which would have maintained clear attribution of who created the upstream project—it looks like it was instead cloned and re-pushed by Andre. Specifically, I was sent this commit replacing references to Homebrew from late July. As evidence of Homebrew's authorship was being erased and obscured, no additional acknowledgement was added to credit Homebrew for having created and maintained Portable Ruby since 2016.&lt;/p&gt;
    &lt;p&gt;It immediately reminded me of Andre's baseless accusation against Google. "Not only did you not credit the Gemstash project in any way," Andre wrote, "from an ethical standpoint, this is also super gross." His blatant copying of Portable Ruby (a project significant enough that the lead maintainer gave a talk about how they did it) struck me as brazenly hypocritical, given Andre's previous litigious and mistaken accusation against Google.&lt;/p&gt;
    &lt;p&gt;In fairness to Andre, the rv-ruby repo continues to retain a copy of Homebrew's LICENSE.txt which names "Homebrew contributors" as the copyright holder. Andre also later added an explicit acknowledgement to the README, but that attribution came more than a month later, and (I'm told) only after he was directly asked to credit the original project.&lt;/p&gt;
    &lt;p&gt;Andre wrote this week that, "Ruby Together did not ever, at any point, demand any form of governance or control over the existing open source projects. Maintainers did their thing in the RubyGems and Bundler GitHub orgs, while Ruby Together staff and board members did their thing in the rubytogether GitHub org." However, while he was leading Ruby Together, he moved to restrict committer access to RubyGems in rubygems/rubygems, unilaterally erased the original authorship from Bundler's gemspec in bundler/bundler, and oversaw a number of contributors being removed from bundler/bundler-site in a redesign of Bundler's website.&lt;/p&gt;
    &lt;p&gt;As a result of this broader historical context and in spite of the serious claims and grave implications being thrown around this month, I'm trying my best not to rush to judgment about who's at fault in the current conflict and would urge others to do the same. The future of the Ruby ecosystem may depend on it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405221</guid><pubDate>Sun, 28 Sep 2025 15:48:10 +0000</pubDate></item><item><title>Show HN: Built an MCP server using Cloudflare's Code Mode pattern</title><link>https://github.com/jx-codes/codemode-mcp</link><description>&lt;doc fingerprint="9d08bed4f4026e9a"&gt;
  &lt;main&gt;
    &lt;p&gt;A local implementation of the "Code Mode" workflow for MCP servers. Instead of struggling with multiple tool calls, LLMs write TypeScript/JavaScript code that calls a simple HTTP proxy to access your MCP servers.&lt;/p&gt;
    &lt;p&gt;Note: It does not attempt to handle the MCP -&amp;gt; typescript API transpilation layer. Would be cool but I really wanted to test the workflow.&lt;/p&gt;
    &lt;p&gt;https://blog.cloudflare.com/code-mode/&lt;/p&gt;
    &lt;p&gt;This implements the core insight that LLMs are much better at writing code than at tool calling. Instead of exposing many tools directly to the LLM (which it struggles with), this server gives the LLM just one tool: &lt;code&gt;execute_code&lt;/code&gt;. The LLM writes code that makes HTTP requests to access your other MCP servers.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLM gets one tool: &lt;code&gt;execute_code&lt;/code&gt;- executes TypeScript/JavaScript&lt;/item&gt;
      &lt;item&gt;LLM writes code: Uses &lt;code&gt;fetch()&lt;/code&gt;to call&lt;code&gt;http://localhost:3001/mcp/*&lt;/code&gt;endpoints&lt;/item&gt;
      &lt;item&gt;HTTP proxy forwards: Transparently proxies requests to your actual MCP servers&lt;/item&gt;
      &lt;item&gt;Results flow back: Through the code execution to the LLM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This gives you all the benefits of complex tool orchestration, but leverages what LLMs are actually good at: writing code.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bun (latest version)&lt;/item&gt;
      &lt;item&gt;Deno (for code execution sandbox)&lt;/item&gt;
      &lt;item&gt;An MCP-compatible client (Claude Desktop, Cursor, VS Code with Copilot, etc.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repository&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/jx-codes/codemode-mcp.git
cd codemode-mcp&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;bun install&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Configure the server (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create a &lt;code&gt;codemode-config.json&lt;/code&gt; file to customize settings:&lt;/p&gt;
    &lt;code&gt;{
   "proxyPort": 3001,
   "configDirectories": [
      "~/.config/mcp/servers",
      "./mcp-servers",
      "./"
   ]
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up your MCP servers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create a &lt;code&gt;.mcp.json&lt;/code&gt; file with your MCP server configurations in any of the directories you specified above:&lt;/p&gt;
    &lt;code&gt;{
   "mcpServers": {
      "fs": {
         "command": "npx",
         "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"],
         "env": {}
      }
   }
}&lt;/code&gt;
    &lt;p&gt;Instead of direct tool calling, the LLM writes:&lt;/p&gt;
    &lt;code&gt;// List available servers
const servers = await fetch("http://localhost:3001/mcp/servers").then((r) =&amp;gt;
  r.json()
);
console.log("Available servers:", servers);

// Call a tool on the filesystem server
const result = await fetch("http://localhost:3001/mcp/call", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    server: "fs",
    tool: "read_file",
    args: { path: "/tmp/example.txt" },
  }),
}).then((r) =&amp;gt; r.json());

console.log("File contents:", result);&lt;/code&gt;
    &lt;p&gt;The real power shows when chaining operations:&lt;/p&gt;
    &lt;code&gt;// Get list of files
const files = await fetch("http://localhost:3001/mcp/call", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    server: "fs",
    tool: "list_directory",
    args: { path: "/tmp" },
  }),
}).then((r) =&amp;gt; r.json());

// Process each file
for (const file of files.content[0].text.split("\n")) {
  if (file.endsWith(".txt")) {
    const content = await fetch("http://localhost:3001/mcp/call", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({
        server: "fs",
        tool: "read_file",
        args: { path: `/tmp/${file}` },
      }),
    }).then((r) =&amp;gt; r.json());

    console.log(`${file}: ${content.content[0].text.length} characters`);
  }
}&lt;/code&gt;
    &lt;p&gt;Executes TypeScript/JavaScript code with network access to the MCP proxy.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;code&lt;/code&gt;(string): Code to execute&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;typescript&lt;/code&gt;(boolean): TypeScript mode (default: true)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Proxy Endpoints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GET /mcp/servers&lt;/code&gt;- List available MCP servers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GET /mcp/{server}/tools&lt;/code&gt;- List tools for server&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;POST /mcp/call&lt;/code&gt;- Call tool (body:&lt;code&gt;{server, tool, args}&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check Deno installation status.&lt;/p&gt;
    &lt;p&gt;Get a comprehensive overview of all available MCP servers and their tools. Returns structured JSON data optimized for LLM consumption, containing complete tool schemas and server status information.&lt;/p&gt;
    &lt;p&gt;JSON Output Structure:&lt;/p&gt;
    &lt;code&gt;{
  "summary": {
    "totalServers": 2,
    "successfulServers": 2,
    "totalTools": 4
  },
  "servers": [
    {
      "server": "filesystem",
      "status": "success",
      "toolCount": 3,
      "tools": [
        {
          "name": "read_file",
          "description": "Read contents of a file",
          "inputSchema": {
            "type": "object",
            "properties": {
              "path": {
                "type": "string",
                "description": "File path to read"
              }
            },
            "required": ["path"]
          }
        }
      ]
    },
    {
      "server": "database",
      "status": "success",
      "toolCount": 1,
      "tools": [
        {
          "name": "query",
          "description": "Execute a SQL query",
          "inputSchema": {
            "type": "object",
            "properties": {
              "query": {
                "type": "string",
                "description": "SQL query to execute"
              }
            },
            "required": ["query"]
          }
        }
      ]
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;This provides complete tool discovery information including parameter schemas, types, and requirements for programmatic access.&lt;/p&gt;
    &lt;p&gt;Create &lt;code&gt;codemode-config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "proxyPort": 3001,
  "configDirectories": ["~/.config/mcp/servers", "./mcp-servers", "./"]
}&lt;/code&gt;
    &lt;p&gt;Add your MCP servers to &lt;code&gt;.mcp.json&lt;/code&gt; files in those directories:&lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "fs": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/tmp"],
      "env": {}
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Traditional MCP: LLM → Tool Call → MCP Server → Result → LLM → Tool Call → ...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLMs struggle with tool syntax&lt;/item&gt;
      &lt;item&gt;Each call goes through the neural network&lt;/item&gt;
      &lt;item&gt;Hard to chain operations&lt;/item&gt;
      &lt;item&gt;Limited by training on synthetic tool examples&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Code Mode: LLM → Write Code → Code calls proxy → Proxy forwards to MCP → Results&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLMs excel at writing code (millions of real examples in training)&lt;/item&gt;
      &lt;item&gt;Code can chain operations naturally&lt;/item&gt;
      &lt;item&gt;Results flow through code logic, not neural network&lt;/item&gt;
      &lt;item&gt;Natural composition and data processing&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code runs in Deno sandbox with network access only&lt;/item&gt;
      &lt;item&gt;No filesystem, environment, or system access&lt;/item&gt;
      &lt;item&gt;30-second execution timeout&lt;/item&gt;
      &lt;item&gt;MCP servers accessed through controlled proxy&lt;/item&gt;
      &lt;item&gt;Temporary files auto-cleanup&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;"Deno not installed": Install Deno and restart "Permission denied": Code trying to access restricted resources "Module not found": Use &lt;code&gt;https://&lt;/code&gt; URLs for imports
"Execution timeout": Optimize code or break into smaller operations&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provide a simpler API layer for the MCP proxy something like mcp.tool('name', args); &lt;list rend="ul"&gt;&lt;item&gt;Could easily be done by injecting our own typescript file into the Deno scope before running user code&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;More config options&lt;/item&gt;
      &lt;item&gt;Filter out the tools somehow&lt;/item&gt;
      &lt;item&gt;Test it out more in my workflows and see the results&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405584</guid><pubDate>Sun, 28 Sep 2025 16:23:51 +0000</pubDate></item><item><title>Swiss voters back e-ID and abolish rental tax</title><link>https://www.swissinfo.ch/eng/swiss-politics/swiss-voters-have-decided-on-electronic-id-and-abolishing-rental-tax/90057432</link><description>&lt;doc fingerprint="ecdb2b4b4a2a39a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Swiss voters back e-ID and abolish rental tax&lt;/head&gt;
    &lt;head rend="h2"&gt;Swiss citizens have approved the introduction of an electronic proof of identity (e-ID) and the abolition of the rental tax for homeowners.&lt;/head&gt;
    &lt;p&gt;Final figures show 50.4% of voters backed e-ID and 57.7% approved a reform of home ownership taxes. Although it’s not a surprise that both votes passed, the rental tax outcome was a lot clearer than predicted by the polls and the e-ID result was a lot closer – it was on course to be rejected until votes from the last canton, Zurich, were added.&lt;/p&gt;
    &lt;p&gt;Turnout is projected to be 50%, which is higher than expected.&lt;/p&gt;
    &lt;p&gt;&amp;gt;&amp;gt; Check out the latest results from the cantons:&lt;/p&gt;
    &lt;p&gt; External Content &lt;/p&gt;
    &lt;p&gt;The e-ID bill had been supported by both chambers of parliament with a large majority. However, an opposing committee brought up concerns that privacy couldn’t be guaranteed. The committee collected over 50,000 signatures for a referendum against the law. For that reason, it went to the ballot box.&lt;/p&gt;
    &lt;p&gt;Interpreting the close result on Sunday, political scientist Lukas Golder told Swiss public television, SRF, that since the Covid pandemic he had observed a growing mistrust of state solutions, particularly in conservative regions.&lt;/p&gt;
    &lt;head rend="h2"&gt;More&lt;/head&gt;
    &lt;p&gt;Olga Baranova, secretary general of the CH association, which campaigned in favour of the e-ID, believed the subject remained difficult to grasp for some sections of the population. “It’s now essential that the government commits itself to better explaining the challenges of digital technology in our country,” she said. She warned that without this effort, Switzerland risked falling further behind in this area.&lt;/p&gt;
    &lt;p&gt;Surprise at the closeness of the result is also palpable in the camp of those opposed to e-ID. “Guarantees were missing from this law, and a lot of people have realised this,” said Jonas Sulzer, a member of the referendum committee.&lt;/p&gt;
    &lt;p&gt;He pointed to the discrepancy between the vote in parliament, which was largely in favour of the project, and that of the people, who were much more divided.&lt;/p&gt;
    &lt;p&gt;&amp;gt;&amp;gt; Here is our explainer on the new e-ID law:&lt;/p&gt;
    &lt;head rend="h2"&gt;More&lt;/head&gt;
    &lt;head rend="h3"&gt;Swiss voters to decide – again – on introducing electronic ID&lt;/head&gt;
    &lt;head rend="h2"&gt;Rental tax: linguistic divide&lt;/head&gt;
    &lt;p&gt;The reform of the home ownership taxes intends to abolish a tax homeowners have to pay on property they live in (imputed rental-value tax). To do this, the cantons will be compensated with an optional new tax on second homes. This move is in the sense of a compromise.&lt;/p&gt;
    &lt;p&gt;Both chambers of parliament and the government had supported the reform. Opponents, such as the left-wing Social Democratic Party and the Green Party, were concerned about the loss in tax revenue.&lt;/p&gt;
    &lt;p&gt;“There’s a very clear linguistic divide,” said political scientist Lukas Golder after the result was confirmed on Sunday. “In the German-speaking part of Switzerland there was a very clear mobilisation and appeal to people in rural areas.”&lt;/p&gt;
    &lt;p&gt;According to Golder, there was a conflict between owners and tenants in the vote. “But there are also some tenants who are hoping to inherit or buy their own property, and the majority of them also voted in favour.”&lt;/p&gt;
    &lt;p&gt;&amp;gt;&amp;gt; Read more on this tax change in our explainer:&lt;/p&gt;
    &lt;head rend="h2"&gt;More&lt;/head&gt;
    &lt;head rend="h3"&gt;Will Switzerland finally do away with imputed rental-value tax on homeowners?&lt;/head&gt;
    &lt;p&gt;Swiss citizens can decide on national issues up to four times a year. Around 5.5 million Swiss are eligible to go to vote. Over the past ten years, the average voter turnout has been 41%-57%, according to the Federal Statistical OfficeExternal link.&lt;/p&gt;
    &lt;p&gt;In compliance with the JTI standards&lt;/p&gt;
    &lt;p&gt;More: SWI swissinfo.ch certified by the Journalism Trust Initiative&lt;/p&gt;
    &lt;p&gt;You can find an overview of ongoing debates with our journalists here . Please join us!&lt;/p&gt;
    &lt;p&gt;If you want to start a conversation about a topic raised in this article or want to report factual errors, email us at english@swissinfo.ch.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405675</guid><pubDate>Sun, 28 Sep 2025 16:33:47 +0000</pubDate></item><item><title>The Weird Concept of Branchless Programming</title><link>https://sanixdk.xyz/blogs/the-weird-concept-of-branchless-programming</link><description>&lt;doc fingerprint="f7ba3738a05c1105"&gt;
  &lt;main&gt;&lt;p&gt;&lt;code&gt;2025-07-08 01:37&lt;/code&gt; â¢ 19+ min read â¢ #c #branchless&lt;/p&gt;&lt;code&gt;-O3 -ffast-math -march=native -fomit-this-entire-function&lt;/code&gt;) were taken.&lt;p&gt;Modern CPUs are predictive creatures. They guess what you're about to do, like a nosy algorithm trying to sell you sneakers after you Googled "foot pain." Branch predictors make CPUs fast by speculating on branches... until they guess wrong and everything grinds to a halt for 15,20 cycles.&lt;/p&gt;&lt;p&gt;Branchless programming is how we get around this: we rewrite our code to not branch at all. Instead of jumping to conclusions, we manipulate bits like 1980s assembly gremlins.&lt;/p&gt;&lt;p&gt;What is a branch first of all?In a program, you may observe parts like this:&lt;/p&gt;&lt;code&gt;if (condition), then
    ...
elif (condition), then
    ...
else
    ...
fi
&lt;/code&gt;&lt;p&gt;This block of instructions is a collection of three branches. Each &lt;code&gt;if&lt;/code&gt;, &lt;code&gt;elif&lt;/code&gt;, and &lt;code&gt;else&lt;/code&gt; represents a possible execution path the CPU can take depending on the evaluation of the conditions. At runtime, only one of these paths is taken, and the others are skipped. This choice ,  this deviation in the control flow ,  is what we call a branch.&lt;/p&gt;&lt;p&gt;In terms of machine code, a branch is often implemented as a jump (&lt;code&gt;jmp&lt;/code&gt;, &lt;code&gt;je&lt;/code&gt;, &lt;code&gt;jne&lt;/code&gt;, etc.). These are instructions that tell the CPU: âif this condition holds, skip to label X; otherwise, keep going.â That jump disrupts the nice linear stream of instruction execution, forcing the CPU to guess where youâre going next.&lt;/p&gt;&lt;p&gt;Hereâs a simple ASCII representation of how this decision tree looks:&lt;/p&gt;&lt;code&gt;          [Condition A]
              |
        +-----+------+
       Yes          No
       |             |
[Block A]     [Condition B]
                  |
            +-----+------+
           Yes          No
           |             |
      [Block B]      [Block C]
&lt;/code&gt;&lt;p&gt;From a CPU perspective, each conditional check and potential jump is a âfork in the road.â If your code has a predictable pattern (e.g., always taking the same branch), the CPU can guess well and maintain performance. But if it's unpredictable, say, random data or user input, then the CPU may guess wrong, flush its pipeline, and pay a heavy penalty.&lt;/p&gt;&lt;p&gt;This is why branches can be so dangerous in tight loops or performance-critical code: even one mispredicted branch can cost dozens of cycles, ruining your cache-fueled dreams.&lt;/p&gt;&lt;p&gt;Branches, when predictable, are cheap. But when unpredictable, they're evil. Imagine a tight loop that checks a condition based on data from user input, or real-world sensors, or shuffled arrays. The branch predictor stumbles, and every misstep means flushing the pipeline , a costly affair on modern superscalar out-of-order CPUs.&lt;/p&gt;&lt;p&gt;Branchless code avoids that entirely. By rewriting conditional logic into arithmetic and bit operations, or using CPU instructions like &lt;code&gt;cmov&lt;/code&gt;, we let the CPU chew through code without pausing to guess. Itâs smoother, faster, and often more deterministic, which is crucial in performance-critical or side-channel-resistant scenarios (looking at you, cryptography).&lt;/p&gt;&lt;p&gt;We're going to take you on a wild ride through three increasingly complex examples:&lt;/p&gt;&lt;code&gt;abs(x)&lt;/code&gt; ,  a gentle warm-up with unary fun&lt;code&gt;clamp(x, min, max)&lt;/code&gt; ,  a common pattern with two conditions&lt;code&gt;partition()&lt;/code&gt; ,  a full algorithm with data-dependent control flow&lt;p&gt;We'll compare these in C (our performance-hungry workhorse), we'll show you how these concepts look in both worlds, how they perform.&lt;/p&gt;&lt;p&gt;Absolute value is your first ticket to understanding how to cut down branches without cutting performance.&lt;/p&gt;&lt;p&gt;We want to calculate the absolute value of a signed integer without using a conditional branch. This is foundational â a single-bit operation can turn a branch into math.&lt;/p&gt;&lt;code&gt;int abs_branch(int x) {
    return x &amp;lt; 0 ? -x : x;
}

// Compiles to a `cmp` and a `jge` or `jl`, depending on compiler and optimization level.

int abs_branchless(int x) {
    int mask = x &amp;gt;&amp;gt; 31;
    return (x + mask) ^ mask;
}
&lt;/code&gt;&lt;code&gt;mask&lt;/code&gt; to &lt;code&gt;x&lt;/code&gt; either increments or doesn't.&lt;code&gt;mask&lt;/code&gt; flips bits only if &lt;code&gt;mask&lt;/code&gt; is -1.&lt;code&gt;int abs_alt(int x) {
    int mask = x &amp;gt;&amp;gt; 31;
    return (x ^ mask) - mask;
}
&lt;/code&gt;&lt;p&gt;Produces identical results; different taste of the same bit soup.&lt;/p&gt;&lt;code&gt;mov eax, edi       ; move x into eax
sar eax, 31        ; sign-extend right shift to produce mask
mov ecx, eax       ; duplicate mask
add edi, ecx       ; edi = x + mask
xor eax, edi       ; eax = result = (x + mask) ^ mask
&lt;/code&gt;&lt;p&gt;Fast. No jumps. Pure ALU (arithmetic logic unit).&lt;/p&gt;&lt;p&gt;The clamp is more complex. You want to bound a value between a &lt;code&gt;min&lt;/code&gt; and &lt;code&gt;max&lt;/code&gt;.We want to ensure a value stays within &lt;code&gt;[min, max]&lt;/code&gt; without branches. This is key in physics simulations, rendering, and safety constraints.&lt;/p&gt;&lt;code&gt;int clamp(int x, int min, int max) {
    if (x &amp;lt; min) return min;
    if (x &amp;gt; max) return max;
    return x;
}

int clamp_branchless(int x, int min, int max) {
    int r1 = x - ((x - min) &amp;amp; ((x - min) &amp;gt;&amp;gt; 31));
    return r1 - ((r1 - max) &amp;amp; ((r1 - max) &amp;gt;&amp;gt; 31));
}
&lt;/code&gt;&lt;code&gt;(x - min) &amp;gt;&amp;gt; 31&lt;/code&gt; creates a mask that's all 1s if &lt;code&gt;x &amp;lt; min&lt;/code&gt;.&lt;code&gt;min&lt;/code&gt; when necessary.&lt;code&gt;sub eax, min
sar eax, 31       ; create mask_low
and eax, (x - min)
sub x, eax        ; x = max(x, min)

sub x, max
sar ..., 31       ; create mask_high
and ..., (x - max)
sub x, ...        ; x = min(x, max)
&lt;/code&gt;&lt;p&gt;Every operation is ALU-based. No branching, just pure logic.This removes branches by computing masks and blending values accordingly. Elegant? No. Effective? Absolutely.&lt;/p&gt;&lt;p&gt;This is where branchless logic makes the biggest splash, in algorithms that iterate over data and make conditional swaps.Partition an array around a pivot such that all elements &amp;lt; pivot come before elements &amp;gt;= pivot, without any conditional branching in the inner loop.&lt;/p&gt;&lt;code&gt;void swap(int* a, int* b) {
    int temp = *a;
    *a = *b;
    *b = temp;
}

int partition(int* arr, int low, int high) {
    int pivot = arr[high];
    int i = low;
    for (int j = low; j &amp;lt; high; j++) {
        if (arr[j] &amp;lt; pivot) {
            swap(&amp;amp;arr[i], &amp;amp;arr[j]);
            i++;
        }
    }
    swap(&amp;amp;arr[i], &amp;amp;arr[high]);
    return i;
}

int partition_branchless(int* arr, int low, int high) {
    int pivot = arr[high];
    int i = low;
    for (int j = low; j &amp;lt; high; j++) {
        swap(&amp;amp;arr[i], &amp;amp;arr[j]);
        i += arr[i] &amp;lt; pivot;
    }
    swap(&amp;amp;arr[i], &amp;amp;arr[high]);
    return i;
}
&lt;/code&gt;&lt;code&gt;i += ((arr[i] - pivot) &amp;gt;&amp;gt; 31) &amp;amp; 1;
&lt;/code&gt;&lt;p&gt;Relies on arithmetic right shift and masking to conditionally increment.&lt;/p&gt;&lt;code&gt;mov eax, [arr+i*4]
cmp eax, pivot
setl bl
add i, ebx
&lt;/code&gt;&lt;p&gt;Or:&lt;/p&gt;&lt;code&gt;cmp eax, pivot
adc i, 0
&lt;/code&gt;&lt;p&gt;Clever use of &lt;code&gt;adc&lt;/code&gt; (add with carry) after compare to branchlessly increment.&lt;/p&gt;&lt;code&gt;| Operation      | Branchy | Branchless | Speedup |
| -------------- | ------- | ---------- | ------- |
| `abs(x)`       | \~5ms   | \~5ms      | 1.00x   |
| `clamp(x,m,M)` | \~6ms   | \~6ms      | 1.00x   |
| `partition()`  | \~6ms   | \~5ms      | 1.20x   |
&lt;/code&gt;&lt;code&gt;abs()&lt;/code&gt; and &lt;code&gt;clamp()&lt;/code&gt; show negligible gains; branch prediction handles them well.&lt;code&gt;partition()&lt;/code&gt; shows improvement due to high branch unpredictability.&lt;p&gt;See appendix below for full C benchmark code, compilation flags, and timing logic.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Warning: long code ahead. Skip if you value your retina and you have a life.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;time.h&amp;gt;

// ABS branchy and branchless
int abs_branch(int x) {
    return x &amp;lt; 0 ? -x : x;
}

int abs_branchless(int x) {
    int mask = x &amp;gt;&amp;gt; 31;
    return (x + mask) ^ mask;
}

// CLAMP branchy and branchless
int clamp(int x, int min, int max) {
    if (x &amp;lt; min) return min;
    if (x &amp;gt; max) return max;
    return x;
}

int clamp_branchless(int x, int min, int max) {
    int r1 = x - ((x - min) &amp;amp; ((x - min) &amp;gt;&amp;gt; 31));
    return r1 - ((r1 - max) &amp;amp; ((r1 - max) &amp;gt;&amp;gt; 31));
}

// PARTITION branchy and branchless
void swap(int* a, int* b) {
    int t = *a;
    *a = *b;
    *b = t;
}

int partition(int* arr, int low, int high) {
    int pivot = arr[high];
    int i = low;
    for (int j = low; j &amp;lt; high; j++) {
        if (arr[j] &amp;lt; pivot) {
            swap(&amp;amp;arr[i], &amp;amp;arr[j]);
            i++;
        }
    }
    swap(&amp;amp;arr[i], &amp;amp;arr[high]);
    return i;
}

int partition_branchless(int* arr, int low, int high) {
    int pivot = arr[high];
    int i = low;
    for (int j = low; j &amp;lt; high; j++) {
        swap(&amp;amp;arr[i], &amp;amp;arr[j]);
        i += arr[i] &amp;lt; pivot;
    }
    swap(&amp;amp;arr[i], &amp;amp;arr[high]);
    return i;
}

// Benchmarking helpers
void benchmark_abs(int* data, int count) {
    clock_t start = clock();
    volatile long long sum = 0;
    for (int i = 0; i &amp;lt; count; ++i) sum += abs_branch(data[i]);
    printf("ABS (branch):     %.3f sec\n", (double)(clock() - start)/CLOCKS_PER_SEC);

    start = clock(); sum = 0;
    for (int i = 0; i &amp;lt; count; ++i) sum += abs_branchless(data[i]);
    printf("ABS (branchless): %.3f sec\n", (double)(clock() - start)/CLOCKS_PER_SEC);
}

void benchmark_clamp(int* data, int count) {
    clock_t start = clock();
    volatile long long sum = 0;
    for (int i = 0; i &amp;lt; count; ++i) sum += clamp(data[i], -50, 50);
    printf("CLAMP (branch):     %.3f sec\n", (double)(clock() - start)/CLOCKS_PER_SEC);

    start = clock(); sum = 0;
    for (int i = 0; i &amp;lt; count; ++i) sum += clamp_branchless(data[i], -50, 50);
    printf("CLAMP (branchless): %.3f sec\n", (double)(clock() - start)/CLOCKS_PER_SEC);
}

void benchmark_partition(int* data, int count) {
    int* copy = malloc(sizeof(int) * count);
    memcpy(copy, data, sizeof(int) * count);

    clock_t start = clock();
    partition(data, 0, count - 1);
    printf("PARTITION (branch):     %.3f sec\n", (double)(clock() - start)/CLOCKS_PER_SEC);

    memcpy(data, copy, sizeof(int) * count);
    start = clock();
    partition_branchless(data, 0, count - 1);
    printf("PARTITION (branchless): %.3f sec\n", (double)(clock() - start)/CLOCKS_PER_SEC);

    free(copy);
}

int main() {
    const int N = 10000000;
    int* data = malloc(sizeof(int) * N);

    // Populate with mixed signed integers
    for (int i = 0; i &amp;lt; N; ++i)
        data[i] = rand() - (RAND_MAX / 2);

    puts("== Benchmarking ABS ==");
    benchmark_abs(data, N);

    puts("\n== Benchmarking CLAMP ==");
    benchmark_clamp(data, N);

    puts("\n== Benchmarking PARTITION ==");
    benchmark_partition(data, 1000000); // smaller size due to O(n log n) behavior

    free(data);
    return 0;
}
&lt;/code&gt;&lt;p&gt;(If the scroll wheel starts smoking, youâve found the end.)&lt;/p&gt;&lt;p&gt;Branchless programming is a scalpel, not a sledgehammer. Used wisely, it can make your code faster, safer, and cooler. Misused, it turns your logic into incomprehensible bit spaghetti.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"Premature optimization is the root of all evil â except when it's branchless, then it's performance art."&lt;/p&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405750</guid><pubDate>Sun, 28 Sep 2025 16:40:53 +0000</pubDate></item><item><title>Testing "Exotic" P2P VPN</title><link>https://blog.nommy.moe/blog/exotic-mesh-vpn/</link><description>&lt;doc fingerprint="b558ca29a9201472"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Testing "exotic" p2p VPN&lt;/head&gt;
    &lt;head rend="h1"&gt;How did the moose begin&lt;/head&gt;
    &lt;p&gt;My standard "everyday" solution when it comes to connecting computers into a single network is Wireguard. &lt;lb/&gt; Wireguard is good, supports p2p, and generally has no downsides.&lt;/p&gt;
    &lt;p&gt;The downsides come from having part of my home infrastructure located in territory controlled by a country that has blocked Wireguard by signatures. &lt;lb/&gt; This is, of course, utterly disgusting, and what's even more disgusting is that these blocks have long since stopped following any kind of legislation. &lt;lb/&gt; The result is an incomprehensible black box that can do anything, behave however it wants, and nobody knows how this shaitan-machine even works anymore.&lt;/p&gt;
    &lt;p&gt;So it's time for penetration.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why not obfuscation?&lt;/head&gt;
    &lt;p&gt;Actually, there are several projects that allow obfuscating Wireguard traffic and punching through firewalls. &lt;lb/&gt; udp2raw, wstunnel and others handle this excellently. &lt;lb/&gt; And Amnezia VPN has made their own fork of Wireguard, specifically for breaking through government censorship.&lt;/p&gt;
    &lt;p&gt;But the main problem with obfuscation is the reduction of effective packet MTU. Because we wrap one packet in another packet, and this overhead takes up space. &lt;lb/&gt; And that's not good.&lt;/p&gt;
    &lt;head rend="h1"&gt;What I want from a VPN&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;p2p mesh network&lt;/p&gt;&lt;lb/&gt;Wireguard is good, of course, but routing all traffic through one server has consequences.&lt;lb/&gt;The consequences usually include launching a Mars rover to switch the VPN to another server in case of IP blocking or just because the server started feeling unwell.&lt;lb/&gt;And routing traffic halfway around the planet just to get access to a machine that's within arm's reach — that's just wrong.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Open source and selfhosted&lt;/p&gt;&lt;lb/&gt;In matters like this, relying on a third-party provider is either dangerous or useless.&lt;lb/&gt;Tailscale, for example, is famous for its geographical blocks, so relying on it is pointless.&lt;lb/&gt;And since Tailscale doesn't do this on a whim (I hope), there's no guarantee that other services won't do the same.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ideologically correct VPN&lt;/p&gt;&lt;lb/&gt;This point exists here specifically for Headscale and ZeroTier.&lt;lb/&gt;Creating a crippled open-source product to advertise a commercial one is a vicious practice and I personally don't approve this.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Not Wireguard&lt;/p&gt;&lt;lb/&gt;For obvious reasons. Signature-based blocking.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Packaged in nixpkgs&lt;/p&gt;&lt;lb/&gt;This one's even more obvious. I'm not going to package a VPN into nix myself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Test subjects&lt;/head&gt;
    &lt;head rend="h2"&gt;EasyTier&lt;/head&gt;
    &lt;p&gt;This is probably the simplest way to create a p2p network. So simple that there isn't even a module in nixpkgs to run it.&lt;/p&gt;
    &lt;p&gt;For security, there's only a password in &lt;code&gt;--network-secret&lt;/code&gt;, which is used for traffic encryption.&lt;/p&gt;
    &lt;p&gt;To work, it immediately opens TCP, UDP, WG, WS, WSS and whatever Lucifer's IT department cooked up. If one gets blocked, it'll break through via another.&lt;/p&gt;
    &lt;p&gt;Essentially all nodes in the network are identical and you can specify multiple peers for initial connection establishment. &lt;lb/&gt; You can use either public ones, which can be viewed here, or specify one of your own nodes. &lt;lb/&gt; It doesn't require any additional configuration.&lt;/p&gt;
    &lt;p&gt;By the way, it has clients for Android, Windows and Mac OS, so it's a good time to dig out those old games you didn't finish in childhood and organize LAN party with friends who aren't very tech-savvy.&lt;/p&gt;
    &lt;p&gt;The main disadvantage is that you can't bind IP addresses to specific machines.&lt;/p&gt;
    &lt;p&gt;And yes, this is a project from China, which might not appeal to some for ideological reasons, but personally I hope it was created by enthusiasts specifically for breaking through the Great Firewall of China.&lt;head&gt;Configuration example&lt;/head&gt;&lt;code&gt;{
  networking.firewall = {
    allowedTCPPorts = [ 11010 11011 11012 ];
    allowedUDPPorts = [ 11010 11011 11012 ];
  };

  environment.systemPackages = [ pkgs.easytier ];
  systemd.services."easytier" = {
    enable = true;
    script = "easytier-core -d --network-name sumeragi --network-secret changeme -p tcp://public.easytier.cn:11010 --dev-name et0 --multi-thread";
    serviceConfig = {
      Restart = "always";
      RestartMaxDelaySec = "1m";
      RestartSec = "100ms";
      RestartSteps = 9;
      User = "root";
    };
    wantedBy = [ "multi-user.target" ];
    after = [ "network.target" ];
    path = with pkgs; [
      easytier
      iproute2
      bash
    ];
  };
}
&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h2"&gt;Nebula&lt;/head&gt;
    &lt;p&gt;This is a more pompous commercial solution from the creators of Slack.&lt;/p&gt;
    &lt;p&gt;It has elliptic curve encryption, suggests using its own PKI and looks generally reliable. &lt;lb/&gt; Though the prospect of manually distributing certificates to machines doesn't thrill me.&lt;/p&gt;
    &lt;p&gt;For its operation it requires "lighthouses" that will connect all other nodes. &lt;lb/&gt; Inside, everything works on Noise Protocol. &lt;lb/&gt; On the outside it exposes only a single UDP port.&lt;/p&gt;
    &lt;p&gt;Among the nice features there's a firewall and zoning, to build slightly more complex networks than "everyone with everyone."&lt;/p&gt;
    &lt;p&gt;And also Nebula's interface is absolutely shit. &lt;lb/&gt; Instead of a normal CLI, you need to configure an internal sshd and connect via SSH to localhost. &lt;lb/&gt; Maybe it's more secure, but it's utterly disgusting.&lt;head&gt;ConfigurationExample&lt;/head&gt;&lt;code&gt;let
  isLighthouse = if (config.networking.hostName == "lighthouse") then true else false;
in
{
  services.nebula.networks.sumeragi = {
    enable = true;
    ca = "/etc/nebula/ca.crt";
    cert = "/etc/nebula/node.crt";
    key = "/etc/nebula/node.key";

    isLighthouse = isLighthouse;
    lighthouses = if (isLighthouse) then [] else [ "10.1.0.1" ];

    listen = {
      host = "0.0.0.0";
      port = 4242;
    };

    staticHostMap = {
      "10.1.0.1" = [ "266.266.266.266:4242" ];
    };

    settings = if (isLighthouse) then {
      sshd = {
        enabled = true;
        listen = "127.0.0.1:2222";
        host_key = "/etc/nebula/id_ed25519";
        authorized_users = [
          {
            user = "nommy";
            keys = [
              "ssh-ed25519 AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
            ];
          }
        ];
      };
    } else {
    };

    firewall = {
      outbound = [
        { port = "any"; proto = "any"; host = "any"; }
      ];
      inbound = [
        { port = "any"; proto = "any"; host = "any"; }
      ];
    };
  };

  networking.firewall.allowedUDPPorts = [ 4242 ];
}

&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h2"&gt;Tinc&lt;/head&gt;
    &lt;p&gt;When I found this, my first thought was "The fuck is this?" &lt;lb/&gt; The project is over 10 years old and is still in an unstable state. &lt;lb/&gt; The current version is &lt;code&gt;1.1pre18&lt;/code&gt;, released way back in 2021. &lt;lb/&gt; The last commit to the &lt;code&gt;1.1&lt;/code&gt; branch was over a year ago. &lt;lb/&gt; It's packaged in Nix as Lucy knows what. &lt;lb/&gt; How is this even a thing?&lt;/p&gt;
    &lt;p&gt;But actually, Tinc can surprise you quite a bit.&lt;/p&gt;
    &lt;p&gt;Under the hood it uses its own protocol over UDP, elliptic curves and a ton of black magic (which, by the way, is properly documented) that makes it all work.&lt;/p&gt;
    &lt;p&gt;Of course, it still needs a node for initial connection bootstrapping, but there's no special setup required — any node can do it, and afterwards it's all direct node-to-node communication.&lt;/p&gt;
    &lt;p&gt;It has a relatively normal CLI, can show a graph of the entire network, has other tasty features, but really lacks some kind of TUI, or at least ASCII art for rendering that graph. For obvious reasons, the configuration was assembled in a dendrofecal manner, I strongly advise not copying it as-is, but rewriting it yourself. Yes, interface and route configuration is done through &lt;head&gt;Example of not very good configuration&lt;/head&gt;&lt;code&gt;tinc-up&lt;/code&gt; and &lt;code&gt;tinc-down&lt;/code&gt;. &lt;lb/&gt; This is the intended way&lt;code&gt;let
  hostName = config.networking.hostName;
in
{
  networking.firewall.allowedTCPPorts = [ 655 ];
  networking.firewall.allowedUDPPorts = [ 655 ];

  services.tinc = {
    networks = {
      sumeragi = {
        name = hostName;
        ed25519PrivateKeyFile = "/etc/tinc/sumeragi/ed25519_key.priv";
        interfaceType = "tun";
        debugLevel = 3;

        hostSettings = {
          lighthouse = {
            settings.Ed25519PublicKey = "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA";
            subnets = [
              { address = "10.2.0.1/32"; }
            ];
            addresses = [
              { address = "266.266.266.266"; port = 655; }
            ];
          };
          laptop = {
            settings.Ed25519PublicKey = "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA";
            subnets = [
              { address = "10.2.0.2/32"; }
            ];
          };
          rpi = {
            settings.Ed25519PublicKey = "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA";
            subnets = [
              { address = "10.2.0.3/32"; }
            ];
          };
        };
      };
    };
  };

  environment.etc = {
    "tinc/sumeragi/tinc-up".source = pkgs.writeScript "tinc-up-sumeragi" ''
        #!${pkgs.stdenv.shell}
        ${pkgs.nettools}/bin/ifconfig $INTERFACE ${(builtins.elemAt config.services.tinc.networks.sumeragi.hostSettings."${hostName}".subnets 0).address} netmask 255.255.255.0
        /run/current-system/sw/bin/ip r add 10.2.0.0/24 dev tinc.sumeragi
    '';
    "tinc/sumeragi/tinc-down".source = pkgs.writeScript "tinc-down-sumeragi" ''
        #!${pkgs.stdenv.shell}
        ${pkgs.nettools}/bin/ifconfig $INTERFACE down
        /run/current-system/sw/bin/ip r del 10.2.0.0/24 dev tinc.sumeragi
    '';
  };
}
&lt;/code&gt;&lt;/p&gt;
    &lt;head rend="h1"&gt;Methodology of measurment&lt;/head&gt;
    &lt;p&gt;This is actually a huge topic and you could write a whole book about it, but the most important thing is — IPerf lies. &lt;lb/&gt; Different versions of IPerf show different numbers, use different measurement methodologies by default, have many tuning options that affect results, and sometimes their readings differ significantly from reality.&lt;/p&gt;
    &lt;p&gt;So along with two versions of IPerf, it's worth adding some real-world network usage cases.&lt;/p&gt;
    &lt;p&gt;Internet speeds in both directions are roughly the same for all nodes, so I'll take numbers from the first direction that comes up, since the difference will be within the margin of error.&lt;/p&gt;
    &lt;head rend="h2"&gt;Infrastructure&lt;/head&gt;
    &lt;p&gt;For realistic measurements I'll use three machines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Home laptop (Laptop) in Spain&lt;/item&gt;
      &lt;item&gt;Intermediate server with public IP (Lighthouse) in Finland&lt;/item&gt;
      &lt;item&gt;Raspberry Pi (RPi) behind the Russian firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mesh network coordinators are hosted on Lighthouse, while speed is measured between Laptop and RPi.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ping&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;ping -c 300 10.1.0.3&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;We send ICMP packets, wait for the response to arrive, measure the time it took to get the response. &lt;lb/&gt; Here we can check latency, jittering and the number of lost packets.&lt;/p&gt;
    &lt;p&gt;Latency is the average ping response time. &lt;lb/&gt; Jittering is how much the response time "wanders" relative to the average. Measured in ms. &lt;lb/&gt; The number of lost packets is self-explanatory.&lt;/p&gt;
    &lt;p&gt;For more or less stable results, 300 packets should be enough.&lt;/p&gt;
    &lt;head rend="h2"&gt;/dev/zero through SSH&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;ssh 10.1.0.3 'dd if=/dev/zero bs=128M count=3 2&amp;gt;/dev/null' | dd of=/dev/null status=progress&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;We read three times 128 MB of zeros through SSH, then look at the reading speed. &lt;lb/&gt; Generally not a bad way to determine data transfer speed inside an SSH tunnel.&lt;/p&gt;
    &lt;p&gt;The main reason for using this test is that through some solutions SSH works so hellishly slow that more than a second can pass between pressing a key and the character appearing on screen, which is completely unacceptable. &lt;lb/&gt; And sometimes it doesn't work at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wget&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;wget 10.1.0.3:5201/testfile&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;As a test file — the same 384 MB of zeros from /dev/null.&lt;/p&gt;
    &lt;p&gt;As a server I use simple-http-server, setting the number of threads equal to the number of CPU cores (8). &lt;lb/&gt; Of course, with compression disabled, otherwise megabytes of zeros risk turning into kilobytes of headers.&lt;/p&gt;
    &lt;head rend="h1"&gt;iperf2 and iperf3&lt;/head&gt;
    &lt;p&gt;Yes, they show orange prices in Africa. Hell knows how to tune this. &lt;lb/&gt; So we just run them with standard configuration and then normalize the results from megabits to megabytes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reference values&lt;/head&gt;
    &lt;p&gt;Measuring exact values for speed, ping and all this stuff that we could use as a baseline is somewhat impossible, since both machines are behind NAT. &lt;lb/&gt; But since the infrastructure includes a Lighthouse with a public IP, we can run a few tests and fantasize about some results.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;ICMP packet loss&lt;/cell&gt;
        &lt;cell role="head"&gt;ICMP Latency&lt;/cell&gt;
        &lt;cell role="head"&gt;ICMP Jittering&lt;/cell&gt;
        &lt;cell role="head"&gt;/dev/zero through SSH&lt;/cell&gt;
        &lt;cell role="head"&gt;Wget&lt;/cell&gt;
        &lt;cell role="head"&gt;iperf2&lt;/cell&gt;
        &lt;cell role="head"&gt;iperf3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Laptop -&amp;gt; Lighthouse&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;71.879 ms&lt;/cell&gt;
        &lt;cell&gt;1.422 ms&lt;/cell&gt;
        &lt;cell&gt;25.5 MB/s&lt;/cell&gt;
        &lt;cell&gt;23.3 MB/s&lt;/cell&gt;
        &lt;cell&gt;18 MB/s&lt;/cell&gt;
        &lt;cell&gt;24.375 MB/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;RPi -&amp;gt; Lighthouse&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;51.872 ms&lt;/cell&gt;
        &lt;cell&gt;1.011 ms&lt;/cell&gt;
        &lt;cell&gt;9.0 MB/s&lt;/cell&gt;
        &lt;cell&gt;Timeout&lt;/cell&gt;
        &lt;cell&gt;9.963 MB/s&lt;/cell&gt;
        &lt;cell&gt;11.112 MB/s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Now we can start fantasizing.&lt;/p&gt;
    &lt;p&gt;Speed between nodes is limited by the slowest link, so we use the minimum values as our reference.&lt;lb/&gt; Latencies can simply be added together. But what to do with jittering isn't entirely clear. &lt;lb/&gt; Supposedly you can't add such values, &lt;lb/&gt; I don't want to recalculate every packet manually, so I'll just take the maximum value.&lt;/p&gt;
    &lt;p&gt;And it's time for the final results.&lt;/p&gt;
    &lt;head rend="h1"&gt;Results&lt;/head&gt;
    &lt;p&gt;All speeds are normalized in bytes. To convert to bits, multiply by 8.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;ICMP packet loss&lt;/cell&gt;
        &lt;cell role="head"&gt;ICMP Latency&lt;/cell&gt;
        &lt;cell role="head"&gt;ICMP Jittering&lt;/cell&gt;
        &lt;cell role="head"&gt;/dev/zero through SSH&lt;/cell&gt;
        &lt;cell role="head"&gt;Wget&lt;/cell&gt;
        &lt;cell role="head"&gt;iperf2&lt;/cell&gt;
        &lt;cell role="head"&gt;iperf3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Reference&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;123.751 ms&lt;/cell&gt;
        &lt;cell&gt;1.422 ms&lt;/cell&gt;
        &lt;cell&gt;9.0 MB/s&lt;/cell&gt;
        &lt;cell&gt;Timeout&lt;/cell&gt;
        &lt;cell&gt;9.963 MB/s&lt;/cell&gt;
        &lt;cell&gt;11.112 MB/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Wireguard + udp2raw&lt;/cell&gt;
        &lt;cell&gt;49.6%&lt;/cell&gt;
        &lt;cell&gt;108.806 ms&lt;/cell&gt;
        &lt;cell&gt;3.724 ms&lt;/cell&gt;
        &lt;cell&gt;Timeout&lt;/cell&gt;
        &lt;cell&gt;Timeout&lt;/cell&gt;
        &lt;cell&gt;3.175 KB/s&lt;/cell&gt;
        &lt;cell&gt;0.00 B/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;EasyTier&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;153.163 ms&lt;/cell&gt;
        &lt;cell&gt;36.290 ms&lt;/cell&gt;
        &lt;cell&gt;2.7 MB/s&lt;/cell&gt;
        &lt;cell&gt;8.09 MB/s&lt;/cell&gt;
        &lt;cell&gt;6.15 KB/s&lt;/cell&gt;
        &lt;cell&gt;0.00 B/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Nebula&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
        &lt;cell&gt;122.173 ms&lt;/cell&gt;
        &lt;cell&gt;15.054 ms&lt;/cell&gt;
        &lt;cell&gt;2.7 MB/s&lt;/cell&gt;
        &lt;cell&gt;3.40 MB/s&lt;/cell&gt;
        &lt;cell&gt;5.975 KB/s&lt;/cell&gt;
        &lt;cell&gt;0.00 B/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tinc&lt;/cell&gt;
        &lt;cell&gt;2.3%&lt;/cell&gt;
        &lt;cell&gt;115.065 ms&lt;/cell&gt;
        &lt;cell&gt;3.393 ms&lt;/cell&gt;
        &lt;cell&gt;14.7 MB/s&lt;/cell&gt;
        &lt;cell&gt;5.16 MB/s&lt;/cell&gt;
        &lt;cell&gt;6.488 MB/s&lt;/cell&gt;
        &lt;cell&gt;4.175 MB/s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Egyptian power of those iperfs...&lt;/p&gt;
    &lt;p&gt;Tinc, as I already said, is very capable of surprising.&lt;/p&gt;
    &lt;p&gt;EasyTier can be forgiven for such overheads, it's ad-hoc after all and generally "be thankful there's any connection at all."&lt;/p&gt;
    &lt;p&gt;But Nebula frankly disappointed me. Here I really want to crack a joke about the Slack client on Electron, but... I expected better, seriously.&lt;/p&gt;
    &lt;p&gt;So if you want to get something like this — Tinc is the best choice performance-wise. &lt;lb/&gt; I'll keep all of them at once for myself. &lt;lb/&gt; I don't like launching Mars rovers unnecessarily.&lt;/p&gt;
    &lt;p&gt;That's all, folks.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; And it all started when mom asked me to fix the robot vacuum...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405815</guid><pubDate>Sun, 28 Sep 2025 16:47:39 +0000</pubDate></item><item><title>Real-Time Radiation World Map</title><link>https://www.gmcmap.com/</link><description>&lt;doc fingerprint="623700cbe885594d"&gt;
  &lt;main&gt;
    &lt;p&gt;Toggle navigation GMCMAP Map Geiger Counter Map Radon Detector Map Submit Data Automatically Submit Data Tutorial (current) Store (current) Forum (current) Mobile apps Donate (current) Sign In/Register Real Time Radiation World Map Real-Time Radiation World Map Load Time range Any time Last 24 hours Last week Last month Last year 0 - 50 CPM 50 - 100 CPM 100 - 200 CPM Over 200 CPM&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405990</guid><pubDate>Sun, 28 Sep 2025 17:09:09 +0000</pubDate></item><item><title>Bayesian Data Analysis, Third edition [pdf]</title><link>https://sites.stat.columbia.edu/gelman/book/BDA3.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45406109</guid><pubDate>Sun, 28 Sep 2025 17:23:21 +0000</pubDate></item><item><title>Denmark bans civil drones after more sightings</title><link>https://www.dw.com/en/denmark-bans-civil-drones-after-more-sightings/a-74166973</link><description>&lt;doc fingerprint="a55649960adbb895"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Denmark bans civil drones after more sightings&lt;/head&gt;September 28, 2025&lt;p&gt;Denmark on Sunday ordered a ban on civil drone flights, after several unmanned aerial vehicles were witnessed at military facilities overnight, following a week in which drone sorties caused the temporary closures of several airports in the Nordic country.&lt;/p&gt;&lt;p&gt;The Danish military said it had deployed "several capacities" in response to the overnight drone sightings.&lt;/p&gt;&lt;p&gt;Earlier this week, drones forced Denmark to close its airports, including Copenhagen Airport, which was closed for nearly four hours on Monday.&lt;/p&gt;&lt;p&gt;Denmark has called the drones part of a "hybrid attack" but the government has stopped short of saying definitively who it believes is behind the missions.&lt;/p&gt;&lt;p&gt;Nevertheless, Danish Prime Minister Mette Frederiksen has said Russia is the main "country that poses a threat to European security."&lt;/p&gt;&lt;p&gt;"We are currently in a difficult security situation, and we must ensure the best possible working conditions for the armed forces and the police when they are responsible for security during the EU summit," Defense Minister Troels Lund Poulsen said in a statement on Sunday.&lt;/p&gt;&lt;head rend="h2"&gt;When will the drone ban be and why?&lt;/head&gt;&lt;p&gt;Civilian drones will be banned from Danish airspace from Monday through Friday of the coming week.&lt;/p&gt;&lt;p&gt;The Nordic country holds the rotating European Council presidency for the second half of 2025. Denmark will host EU leaders on Wednesday, followed by a summit on Thursday of the wider, 47-member European Political Community, set up to unite the bloc with other friendly European countries after Russia's 2022 full-scale invasion of Ukraine.&lt;/p&gt;&lt;p&gt;A German air defense frigate arrived in Copenhagen on Sunday to assist with airspace surveillance.&lt;/p&gt;&lt;head rend="h2"&gt;Is Russia behind the drone activity?&lt;/head&gt;&lt;p&gt;It is not clear who is behind the drone missions but Prime Minister Mette Frederiksen and NATO Secretary-General Mark Rutte have both said the notion of Russia being the perpetrator could not be ruled out.&lt;/p&gt;&lt;p&gt;Meanwhile, the Russian embassy in Denmark last week rejected claims of Moscow's involvement.&lt;/p&gt;&lt;p&gt;Edited by: Dmytro Hubenko&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45406256</guid><pubDate>Sun, 28 Sep 2025 17:40:05 +0000</pubDate></item><item><title>UK Petition: Do not introduce Digital ID cards</title><link>https://petition.parliament.uk/petitions/730194</link><description>&lt;doc fingerprint="89cb2d25e1ee6ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Petition Do not introduce Digital ID cards&lt;/head&gt;
    &lt;p&gt;We demand that the UK Government immediately commits to not introducing a digital ID cards. There are reports that this is being looked at.&lt;/p&gt;
    &lt;head&gt;More details&lt;/head&gt;
    &lt;p&gt;We think this would be a step towards mass surveillance and digital control, and that no one should be forced to register with a state-controlled ID system. We oppose the creation of any national ID system. &lt;lb/&gt;ID cards were scrapped in 2010, in our view for good reason.&lt;/p&gt;
    &lt;p&gt;2,333,651 signatures&lt;/p&gt;
    &lt;p&gt;Show on a map the geographical breakdown of signatures by constituency&lt;/p&gt;
    &lt;p&gt;100,000 signatures required to be considered for a debate in Parliament&lt;/p&gt;
    &lt;head rend="h2"&gt;Parliament will consider this for a debate&lt;/head&gt;
    &lt;p&gt;Parliament considers all petitions that get more than 100,000 signatures for a debate&lt;/p&gt;
    &lt;p&gt;Waiting for 6 days for a debate date&lt;/p&gt;
    &lt;head rend="h2"&gt;Government will respond&lt;/head&gt;
    &lt;p&gt;Government responds to all petitions that get more than 10,000 signatures&lt;/p&gt;
    &lt;p&gt;Waiting for 25 days for a government response&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45406442</guid><pubDate>Sun, 28 Sep 2025 18:01:39 +0000</pubDate></item><item><title>VMScape and why Xen dodged it</title><link>https://virtualize.sh/blog/vmscape-and-why-xen-dodged-it/</link><description>&lt;doc fingerprint="b4e311dca163e6c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VMScape and why Xen dodged it&lt;/head&gt;
    &lt;p&gt;It’s been less than two weeks since the security team at ETH Zürich published their research on a new microarchitectural attack they call VMScape:&lt;/p&gt;
    &lt;p&gt;It’s a neat piece of work, and it shows once again how CPUs, with all their clever tricks for performance, can sometimes open the door to data leaks across virtual machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is VMScape?&lt;/head&gt;
    &lt;p&gt;The short version: modern CPUs use a branch predictor to guess where code will go next. It makes things faster, but the predictor also “remembers” past patterns. If you can manipulate that memory, you can mislead the CPU and peek at things you shouldn’t. That’s the basic idea behind Spectre-style attacks.&lt;/p&gt;
    &lt;p&gt;According to the ETH team:&lt;/p&gt;
    &lt;quote&gt;“We find that branch predictor state is not fully flushed across VMs, enabling cross-VM Branch Target Injection (vBTI) primitives. We demonstrate the practical impact of vBTI with VMScape, a cross-VM attack capable of leaking QEMU userspace secrets from a malicious guest VM on AMD Zen 4 and Zen 5 CPUs.”&lt;/quote&gt;
    &lt;p&gt;In other words, a malicious VM can target the hypervisor’s userspace components and start leaking data. For KVM, that means QEMU, which is heavily exposed. VMware is in the same situation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Xen wasn’t affected&lt;/head&gt;
    &lt;p&gt;The researchers also note that Xen is not vulnerable. That’s not because Xen has no bugs (it does, like every hypervisor), but because of its architecture.&lt;/p&gt;
    &lt;p&gt;From day one, Xen was designed to keep the hypervisor core small and move everything else out. Device emulation, storage drivers, network stacks — they all live in Dom0, which is itself just another virtual machine. Dom0 has more privileges than a normal guest, but it’s still not the hypervisor.&lt;/p&gt;
    &lt;p&gt;That architectural choice makes Xen closer to a microkernel than a traditional monolithic hypervisor. The core stays minimal, with a narrow set of responsibilities, and anything that doesn’t absolutely need to run at the highest privilege level gets pushed out. That’s not just elegant — it’s a big deal for security.&lt;/p&gt;
    &lt;head rend="h2"&gt;Size matters (in a good way)&lt;/head&gt;
    &lt;p&gt;Because the hypervisor itself is small, it’s easier to audit, reason about, and even certify. That’s why you’ll find Xen at the heart of a lot of embedded and safety-critical projects, where formal verification and certification are required. Try doing that with a massive, monolithic kernel and you’ll quickly run into a wall. With Xen, it’s actually feasible (and being done as we speak).&lt;/p&gt;
    &lt;p&gt;VMScape highlights the benefits of that design: QEMU is simply not sitting next to the hypervisor. Even if you leak information from it, you’re still only talking about a process in Dom0, not the privileged heart of the system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why this matters&lt;/head&gt;
    &lt;p&gt;Architectural choices made twenty years ago are paying off today. By separating responsibilities, Xen reduced its attack surface and avoided a whole class of problems. That doesn’t make it invincible (Xen has had and will have its share of vulnerabilities) but it does mean that by design, certain attacks don’t land as hard.&lt;/p&gt;
    &lt;p&gt;As the ETH Zürich team points out, mitigations for KVM involve adding new predictor flushes, which Linux developers have already started to implement. VMware will need similar patches. Xen doesn’t need those same emergency measures, because the architecture already put a buffer in place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Defense in depth&lt;/head&gt;
    &lt;p&gt;It’s tempting to say “Xen wins” and stop there. But that’s not the whole story. Security is never just about one design decision. CPUs will keep evolving, new side channels will keep appearing, and no hypervisor can afford to be complacent.&lt;/p&gt;
    &lt;p&gt;Still, VMScape is a good reminder that defense in depth starts at the architecture level. A small, microkernel-like core, privilege separation, isolation of device emulation — all of that adds resilience. It won’t stop every possible attack, but it does add another layer of safety, and in security, layers are what make the difference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45406573</guid><pubDate>Sun, 28 Sep 2025 18:19:01 +0000</pubDate></item></channel></rss>