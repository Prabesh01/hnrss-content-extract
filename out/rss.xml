<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 19 Nov 2025 05:11:21 +0000</lastBuildDate><item><title>Short Little Difficult Books</title><link>https://countercraft.substack.com/p/short-little-difficult-books</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45966435</guid><pubDate>Tue, 18 Nov 2025 14:23:18 +0000</pubDate></item><item><title>Show HN: Browser-based interactive 3D Three-Body problem simulator</title><link>https://trisolarchaos.com/?pr=O_8(0.6)&amp;n=3&amp;s=5.0&amp;so=0.00&amp;im=rk4&amp;dt=1.00e-4&amp;rt=1.0e-6&amp;at=1.0e-8&amp;bs=0.15&amp;sf=0&amp;sv=0&amp;cm=free&amp;kt=1&amp;st=1&amp;tl=1500&amp;cp=2.5208,1.5125,2.5208&amp;ct=0.0000,0.0000,0.1670</link><description>&lt;doc fingerprint="1b8cc1fe919783ba"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;About the N-Body Simulator&lt;/head&gt;
    &lt;head rend="h3"&gt;What is the Three-Body Problem?&lt;/head&gt;
    &lt;p&gt;The three-body problem is one of the most famous challenges in classical physics and celestial mechanics. It asks: given the initial positions, masses, and velocities of three bodies in space, can we predict their future motion under mutual gravitational attraction?&lt;/p&gt;
    &lt;p&gt;Unlike the two-body problem (which has an exact analytical solution), the three-body problem has no general closed-form solution. This makes numerical simulation the primary tool for studying these complex gravitational systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;N-Body Gravitational Simulation&lt;/head&gt;
    &lt;p&gt;This simulator uses Newton's law of universal gravitation to model the gravitational forces between every pair of bodies:&lt;/p&gt;
    &lt;p&gt;F = G × m₁ × m₂ / (r² + ε²)&lt;/p&gt;
    &lt;p&gt;Each body experiences the sum of all pairwise gravitational forces from every other body. For N bodies, this requires calculating N(N-1)/2 force pairs each timestep. The ε² term is a softening parameter that prevents numerical singularities when bodies pass very close together.&lt;/p&gt;
    &lt;p&gt;The simulation supports multiple integration methods. By default, it uses the Velocity Verlet integration method, a symplectic integrator that provides superior energy conservation compared to simpler methods like Euler integration. This makes it ideal for long-term orbital mechanics simulations.&lt;/p&gt;
    &lt;p&gt;Users can switch to the 4th-order Runge-Kutta (RK4) method in the Advanced Settings, which offers higher accuracy per timestep and typically shows lower energy drift in short simulations. However, RK4 is not symplectic and accumulates systematic phase errors over long simulation times, causing orbits to gradually decay or expand. This makes RK4 better suited for short to medium duration simulations where minimizing instantaneous error is the priority, while Verlet excels at maintaining correct orbital shapes over extended periods.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preset Configurations&lt;/head&gt;
    &lt;p&gt;The simulator includes several famous periodic three-body orbits discovered through numerical searches:&lt;/p&gt;
    &lt;head rend="h4"&gt;2D Orbits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Figure-8 choreography: Discovered by Cris Moore in 1993, where three equal masses chase each other along a figure-eight shaped path&lt;/item&gt;
      &lt;item&gt;Lagrange triangular configuration: Equilateral triangle configuration with circular orbits.&lt;/item&gt;
      &lt;item&gt;Butterfly, Broucke, Hénon, and Yarn: Periodic orbits from the Šuvakov-Dmitrašinović database of three-body choreographies, discovered through systematic numerical exploration of initial conditions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;3D Orbits&lt;/head&gt;
    &lt;p&gt;Three-dimensional periodic orbits from Li and Liao (2025), which discovered 10,059 new periodic solutions including 21 choreographic orbits and 273 "piano-trio" orbits (where two equal-mass bodies share one orbit while a third body follows another). Paper | GitHub&lt;/p&gt;
    &lt;head rend="h3"&gt;Features &amp;amp; Applications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time Physics: Experience gravitational dynamics in 3D with interactive controls&lt;/item&gt;
      &lt;item&gt;Multiple Integration Methods: Choose between Velocity Verlet (energy-conserving) and RK4 (high accuracy).&lt;/item&gt;
      &lt;item&gt;Exploration Platform: Experiment with different initial conditions and masses&lt;/item&gt;
      &lt;item&gt;Timeline Playback: Scrub through simulation history to analyze orbital behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How to Use&lt;/head&gt;
    &lt;p&gt;Getting Started: Use the preset configurations (Figure-8 or Lagrange) to see stable three-body orbits, or generate random initial conditions to explore chaotic dynamics.&lt;/p&gt;
    &lt;p&gt;Controls: Adjust body masses, simulation speed, and physics parameters. Use the timeline to review and analyze orbital patterns. Drag bodies while paused to create custom configurations.&lt;/p&gt;
    &lt;p&gt;Sharing: Click "Share Configuration" to generate a URL that preserves your exact simulation initial state.&lt;/p&gt;
    &lt;head rend="h3"&gt;Energy Conservation &amp;amp; Simulation Accuracy&lt;/head&gt;
    &lt;p&gt;The simulator displays two important energy metrics in the Advanced Settings panel:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total Energy: The sum of kinetic energy (½mv²) and gravitational potential energy (-Gm₁m₂/r) of all bodies. In an ideal gravitational system, this value should remain constant over time.&lt;/item&gt;
      &lt;item&gt;Energy Drift: The percentage change in total energy from the initial state. This measures the numerical accuracy of the simulation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In real physics, energy is conserved in isolated systems. However, numerical integration methods introduce small errors at each timestep. The energy drift indicator helps you evaluate simulation quality:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Green (&amp;lt;1%): Excellent energy conservation - the simulation is highly accurate&lt;/item&gt;
      &lt;item&gt;Yellow (1-5%): Moderate drift - acceptable for most purposes but consider reducing timestep&lt;/item&gt;
      &lt;item&gt;Red (&amp;gt;5%): Significant drift - simulation may be unreliable, reduce timestep or try other integration methods&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Velocity Verlet integration method is "symplectic," meaning it preserves the phase-space structure of Hamiltonian systems. While RK4 typically shows lower energy drift in short-term simulations (better local accuracy), Verlet prevents systematic phase errors that accumulate over extended simulations. This makes Verlet ideal for long-term orbital mechanics where maintaining orbital stability over thousands of periods is more important than minimizing instantaneous error.&lt;/p&gt;
    &lt;p&gt;Why is Total Energy Negative? In gravitational systems, total energy is often negative, and this is perfectly normal! Gravitational potential energy is defined as zero at infinite separation and becomes increasingly negative as bodies move closer together (PE = -Gm₁m₂/r). When total energy is negative, it means the system is gravitationally bound - the bodies don't have enough kinetic energy to escape to infinity, so they remain in orbit. This is exactly what you see in stable orbital systems like planets around stars or the choreographed orbits in this simulator. A negative total energy that remains constant indicates a stable, bound orbital system.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical Details&lt;/head&gt;
    &lt;p&gt;Built with Three.js for WebGL-accelerated 3D graphics and modern JavaScript. The physics engine implements N-body gravitational calculations with a configurable softening parameter to prevent numerical singularities during close encounters.&lt;/p&gt;
    &lt;p&gt;The simulator tracks up to 10,000 frames of history, allowing you to review and analyze the evolution of complex orbital systems. All simulations are deterministic and reproducible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feedback&lt;/head&gt;
    &lt;p&gt;Have suggestions, found a bug, or want to share your thoughts? Give feedback and help improve this simulator!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45967079</guid><pubDate>Tue, 18 Nov 2025 15:00:36 +0000</pubDate></item><item><title>Gemini 3</title><link>https://blog.google/products/gemini/gemini-3/</link><description>&lt;doc fingerprint="f9d7a1cf9b3f9a95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A new era of intelligence with Gemini 3&lt;/head&gt;
    &lt;head rend="h3"&gt;A note from Google and Alphabet CEO Sundar Pichai:&lt;/head&gt;
    &lt;p&gt;Nearly two years ago we kicked off the Gemini era, one of our biggest scientific and product endeavors ever undertaken as a company. Since then, it’s been incredible to see how much people love it. AI Overviews now have 2 billion users every month. The Gemini app surpasses 650 million users per month, more than 70% of our Cloud customers use our AI, 13 million developers have built with our generative models, and that is just a snippet of the impact we’re seeing.&lt;/p&gt;
    &lt;p&gt;And we’re able to get advanced capabilities to the world faster than ever, thanks to our differentiated full stack approach to AI innovation — from our leading infrastructure to our world-class research and models and tooling, to products that reach billions of people around the world.&lt;/p&gt;
    &lt;p&gt;Every generation of Gemini has built on the last, enabling you to do more. Gemini 1’s breakthroughs in native multimodality and long context window expanded the kinds of information that could be processed — and how much of it. Gemini 2 laid the foundation for agentic capabilities and pushed the frontiers on reasoning and thinking, helping with more complex tasks and ideas, leading to Gemini 2.5 Pro topping LMArena for over six months.&lt;/p&gt;
    &lt;p&gt;And now we’re introducing Gemini 3, our most intelligent model, that combines all of Gemini’s capabilities together so you can bring any idea to life.&lt;/p&gt;
    &lt;p&gt;It’s state-of-the-art in reasoning, built to grasp depth and nuance — whether it’s perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. It’s amazing to think that in just two years, AI has evolved from simply reading text and images to reading the room.&lt;/p&gt;
    &lt;p&gt;And starting today, we’re shipping Gemini at the scale of Google. That includes Gemini 3 in AI Mode in Search with more complex reasoning and new dynamic experiences. This is the first time we are shipping Gemini in Search on day one. Gemini 3 is also coming today to the Gemini app, to developers in AI Studio and Vertex AI, and in our new agentic development platform, Google Antigravity — more below.&lt;/p&gt;
    &lt;p&gt;Like the generations before it, Gemini 3 is once again advancing the state of the art. In this new chapter, we’ll continue to push the frontiers of intelligence, agents, and personalization to make AI truly helpful for everyone.&lt;/p&gt;
    &lt;p&gt;We hope you like Gemini 3, we'll keep improving it, and look forward to seeing what you build with it. Much more to come!&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Gemini 3: our most intelligent model that helps you bring any idea to life&lt;/head&gt;
    &lt;p&gt;Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind and Chief AI Architect, Google, on behalf of the Gemini team&lt;/p&gt;
    &lt;p&gt;Today we’re taking another big step on the path toward AGI and releasing Gemini 3.&lt;/p&gt;
    &lt;p&gt;It’s the best model in the world for multimodal understanding and our most powerful agentic and vibe coding model yet, delivering richer visualizations and deeper interactivity — all built on a foundation of state-of-the-art reasoning.&lt;/p&gt;
    &lt;p&gt;We’re beginning the Gemini 3 era by releasing Gemini 3 Pro in preview and making it available today across a suite of Google products so you can use it in your daily life to learn, build and plan anything. We’re also introducing Gemini 3 Deep Think — our enhanced reasoning mode that pushes Gemini 3 performance even further — and giving access to safety testers before making it available to Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;State-of-the-art reasoning with unprecedented depth and nuance&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro can bring any idea to life with its state-of-the-art reasoning and multimodal capabilities. It significantly outperforms 2.5 Pro on every major AI benchmark.&lt;/p&gt;
    &lt;p&gt;It tops the LMArena Leaderboard with a breakthrough score of 1501 Elo. It demonstrates PhD-level reasoning with top scores on Humanity’s Last Exam (37.5% without the usage of any tools) and GPQA Diamond (91.9%). It also sets a new standard for frontier models in mathematics, achieving a new state-of-the-art of 23.4% on MathArena Apex.&lt;/p&gt;
    &lt;p&gt;Beyond text, Gemini 3 Pro redefines multimodal reasoning with 81% on MMMU-Pro and 87.6% on Video-MMMU. It also scores a state-of-the-art 72.1% on SimpleQA Verified, showing great progress on factual accuracy. This means Gemini 3 Pro is highly capable at solving complex problems across a vast array of topics like science and mathematics with a high degree of reliability.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is state-of-the-art across a range of key AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro also brings a new level of depth and nuance to every interaction. Its responses are smart, concise and direct, trading cliché and flattery for genuine insight — telling you what you need to hear, not just what you want to hear. It acts as a true thought partner that gives you new ways to understand information and express yourself, from translating dense scientific concepts by generating code for high-fidelity visualizations to creative brainstorming.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can code a visualization of plasma flow in a tokamak and write a poem capturing the physics of fusion.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gemini 3 Deep Think&lt;/head&gt;
    &lt;p&gt;Gemini 3 Deep Think mode pushes the boundaries of intelligence even further, delivering a step-change in Gemini 3’s reasoning and multimodal understanding capabilities to help you solve even more complex problems.&lt;/p&gt;
    &lt;p&gt;In testing, Gemini 3 Deep Think outperforms Gemini 3 Pro’s already impressive performance on Humanity’s Last Exam (41.0% without the use of tools) and GPQA Diamond (93.8%). It also achieves an unprecedented 45.1% on ARC-AGI-2 (with code execution, ARC Prize Verified), demonstrating its ability to solve novel challenges.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Deep Think mode excels on some of the most challenging AI benchmarks. See details on our evaluation methodology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gemini 3 helps you learn, build and plan anything&lt;/head&gt;
    &lt;head rend="h3"&gt;Learn anything&lt;/head&gt;
    &lt;p&gt;Gemini was built from the start to seamlessly synthesize information about any topic across multiple modalities, including text, images, video, audio and code. Gemini 3 pushes the frontier of multimodal reasoning to help you learn in ways that make sense for you by combining its state-of-the-art reasoning, vision and spatial understanding, leading multilingual performance, and 1 million-token context window.&lt;/p&gt;
    &lt;p&gt;For example, if you want to learn how to cook in your family tradition, Gemini 3 can decipher and translate handwritten recipes in different languages into a shareable family cookbook. Or if you want to learn about a new topic, you can give it academic papers, long video lectures or tutorials and it can generate code for interactive flashcards, visualizations or other formats that will help you master the material. It can even analyze videos of your pickleball match, identify areas where you can improve and generate a training plan for overall form improvements.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you learn and preserve family cooking traditions. Try it in Gemini Canvas.&lt;/p&gt;
    &lt;p&gt;Gemini 3 can help you analyze complex information like research papers and can generate code for an interactive guide.&lt;/p&gt;
    &lt;p&gt;Get expert-level sports analysis on your pickleball match to help improve your game.&lt;/p&gt;
    &lt;p&gt;To help you make better sense of information on the web, AI Mode in Search now uses Gemini 3 to enable new generative UI experiences like immersive visual layouts and interactive tools and simulations, all generated completely on the fly based on your query.&lt;/p&gt;
    &lt;p&gt;Learn a complex topic like how RNA polymerase works with generative UI in AI Mode in Search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build anything&lt;/head&gt;
    &lt;p&gt;Building on the success of 2.5 Pro, Gemini 3 delivers on the promise of bringing any idea to life for developers. It’s exceptional at zero-shot generation and handles complex prompts and instructions to render richer, more interactive web UI.&lt;/p&gt;
    &lt;p&gt;Gemini 3 is the best vibe coding and agentic coding model we’ve ever built – making our products more autonomous and boosting developer productivity. It tops the WebDev Arena leaderboard by scoring an impressive 1487 Elo. It also scores 54.2% on Terminal-Bench 2.0, which tests a model’s tool use ability to operate a computer via terminal and it greatly outperforms 2.5 Pro on SWE-bench Verified (76.2%), a benchmark that measures coding agents.&lt;/p&gt;
    &lt;p&gt;You can now build with Gemini 3 in Google AI Studio, Vertex AI, Gemini CLI and our new agentic development platform, Google Antigravity. It’s also available in third-party platforms like Cursor, GitHub, JetBrains, Manus, Replit and more.&lt;/p&gt;
    &lt;p&gt;Code a retro 3D spaceship game with richer visualizations and improved interactivity. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Bring your imagination to life by building, deconstructing and remixing detailed 3D voxel art using code. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;Build a playable sci-fi world with shaders using Gemini 3. Try it in AI Studio.&lt;/p&gt;
    &lt;p&gt;You can vibe code richer, more interactive web UI and apps with Gemini 3.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introducing a new agent-first development experience&lt;/head&gt;
    &lt;p&gt;As model intelligence accelerates with Gemini 3, we have the opportunity to reimagine the entire developer experience. Today we’re releasing Google Antigravity, our new agentic development platform that enables developers to operate at a higher, task-oriented level.&lt;/p&gt;
    &lt;p&gt;Using Gemini 3’s advanced reasoning, tool use and agentic coding capabilities, Google Antigravity transforms AI assistance from a tool in a developer’s toolkit into an active partner. While the core of Google Antigravity is a familiar AI IDE experience, its agents have been elevated to a dedicated surface and given direct access to the editor, terminal and browser. Now, agents can autonomously plan and execute complex, end-to-end software tasks simultaneously on your behalf while validating their own code.&lt;/p&gt;
    &lt;p&gt;In addition to Gemini 3 Pro, Google Antigravity also comes tightly coupled with our latest Gemini 2.5 Computer Use model for browser control and our top-rated image editing model Nano Banana (Gemini 2.5 Image).&lt;/p&gt;
    &lt;p&gt;Google Antigravity uses Gemini 3 to drive an end-to-end agentic workflow for a flight tracker app. The agent independently plans, codes the application and validates its execution through browser-based computer use.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan anything&lt;/head&gt;
    &lt;p&gt;Since introducing the agentic era with Gemini 2, we’ve made a lot of progress, not only advancing Gemini’s coding agent abilities, but also improving its ability to reliably plan ahead over longer horizons. Gemini 3 demonstrates this by topping the leaderboard on Vending-Bench 2, which tests longer horizon planning by managing a simulated vending machine business. Gemini 3 Pro maintains consistent tool usage and decision-making for a full simulated year of operation, driving higher returns without drifting off task.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Pro demonstrates better long-horizon planning to generate significantly higher returns compared to other frontier models.&lt;/p&gt;
    &lt;p&gt;This means Gemini 3 can better help you get things done in everyday life. By combining deeper reasoning with improved, more consistent tool use, Gemini 3 can take action on your behalf by navigating more complex, multi-step workflows from start to finish — like booking local services or organizing your inbox — all while under your control and guidance.&lt;/p&gt;
    &lt;p&gt;Google AI Ultra subscribers can try these agentic capabilities in the Gemini app with Gemini Agent today. We’ve learned a lot improving Gemini’s agentic capabilities, and we’re excited to see how you use it as we expand to more Google products soon.&lt;/p&gt;
    &lt;p&gt;Gemini Agent can help you organize your Gmail inbox. Try it now in the Gemini app for Google AI Ultra subscribers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building Gemini 3 responsibly&lt;/head&gt;
    &lt;p&gt;Gemini 3 is our most secure model yet, and has undergone the most comprehensive set of safety evaluations of any Google AI model to date. The model shows reduced sycophancy, increased resistance to prompt injections and improved protection against misuse via cyberattacks.&lt;/p&gt;
    &lt;p&gt;In addition to our in-house testing for the critical domains in our Frontier Safety Framework, we've also partnered on evaluations with world-leading subject matter experts, provided early access to bodies like the UK AISI, and obtained independent assessments from industry experts like Apollo, Vaultis, Dreadnode and more. For more information, see the Gemini 3 model card.&lt;/p&gt;
    &lt;head rend="h2"&gt;The next era of Gemini&lt;/head&gt;
    &lt;p&gt;This is just the start of the Gemini 3 era. As of today, Gemini 3 starts rolling out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For everyone in the Gemini app and for Google AI Pro and Ultra subscribers in AI Mode in Search&lt;/item&gt;
      &lt;item&gt;For developers in the Gemini API in AI Studio, our new agentic development platform, Google Antigravity; and Gemini CLI&lt;/item&gt;
      &lt;item&gt;For enterprises in Vertex AI and Gemini Enterprise&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Gemini 3 Deep Think mode, we’re taking extra time for safety evaluations and input from safety testers before making it available to Google AI Ultra subscribers in the coming weeks.&lt;/p&gt;
    &lt;p&gt;We plan to release additional models to the Gemini 3 series soon so you can do more with AI. We look forward to getting your feedback and seeing what you learn, build and plan with Gemini.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45967211</guid><pubDate>Tue, 18 Nov 2025 15:09:38 +0000</pubDate></item><item><title>Google Antigravity</title><link>https://antigravity.google/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45967814</guid><pubDate>Tue, 18 Nov 2025 15:47:38 +0000</pubDate></item><item><title>The code and open-source tools I used to produce a science fiction anthology</title><link>https://compellingsciencefiction.com/posts/the-code-and-open-source-tools-i-used-to-produce-a-science-fiction-anthology.html</link><description>&lt;doc fingerprint="2fc09a362ef7ad11"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;&lt;p&gt; Last month I published &lt;/p&gt;Think Weirder: The Year's Best Science Fiction Ideas&lt;p&gt;, a 16-story anthology featuring Greg Egan, Isabel J. Kim, Ray Nayler, Caroline M. Yoachim, and twelve other wonderful authors. The book ended up being the #1 New Release in the Short Stories Anthologies category for a short time on Amazon, outselling many other newly released short story anthologies published by the big NYC publishers with large marketing departments. &lt;/p&gt;&lt;/div&gt;
      &lt;p&gt; I'm not a professional publisher. I have a full-time job and two small kids, so all of this work happened after my kids went to sleep. I had to use my time judiciously, which meant creating an efficient process. Fortunately I'm a programmer, and it turns out that programming skills translate surprisingly well to book publishing. This post is about how I built a complete publishing pipeline using Python, YAML files, and LaTeX â and why you might want to do something similar if you're considering publishing a book. I know that by writing this I'll have my choices questioned by professional designers, but hopefully the software concepts will be helpful. &lt;/p&gt;
      &lt;p&gt; My initial thought: can I really do ALL of this? &lt;/p&gt;
      &lt;p&gt; When I started this project, I had some worries. Professional publishers have entire departments of specialists. How could I possibly handle all of that myself? &lt;/p&gt;
      &lt;p&gt; The answer turned out to be: build tools that automate the repetitive parts, and use simple file formats that make everything transparent and debuggable. &lt;/p&gt;
      &lt;p&gt; Step 1: Tracking stories with plain text files &lt;/p&gt;
      &lt;p&gt; The first challenge was tracking hundreds of candidate stories from different magazines. I read 391 stories published in 2024 before selecting the final 16. That's a lot of stories to keep organized. &lt;/p&gt;
      &lt;p&gt; I could have used a spreadsheet, but I went with plain YAML files instead. Here's why this worked well for me: &lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Git-friendly: Every decision I made was tracked in version control&lt;/item&gt;
          &lt;item&gt;Human-readable: I could open any file in a text editor and understand what I was looking at&lt;/item&gt;
          &lt;item&gt;Easy to build scripts around: I wrote several Python functions to do different kinds of metadata introspection that I'll go through&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The structure looks like this: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;data/
  story-progress.yaml       # Central tracking file
  markets.yaml              # Magazine metadata
  themes.yaml               # Theme occurrence tracking
  subgenres.yaml            # Subgenre tallies
stories/
  clarkesworld-magazine/
    nelson_11_24.yaml       # Individual story files
    pak_06_24.yaml
  reactor-magazine/
    larson_breathing.yaml
  ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Each story file is pure YAML containing the full story text plus metadata: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;title: "Twenty-Four Hours"
author: H.H. Pak
market: clarkesworld-magazine
url: https://clarkesworldmagazine.com/pak_06_24/
word_count: 4540
year: 2024
slug: pak_06_24
summary: ...&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Not all stories have public URLs available, but that's OK because all of the fields are optional. The central &lt;code&gt;story-progress.yaml&lt;/code&gt; tracks editorial state:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;clarkesworld-magazine-nelson_11_24:
  title: "LuvHomeâ¢"
  author: Resa Nelson
  market: clarkesworld-magazine
  status: accepted  # or: not_started/relevant/rejected
  date_added: '2024-09-08T08:22:47.033192'&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Step 2: A simple command-line tool &lt;/p&gt;
      &lt;p&gt; I built a small Python CLI tool (&lt;code&gt;se.py&lt;/code&gt;) to help me navigate all this data. Since I do all this work at night after my kids go to sleep, I wanted something fast that mirrored a lot of the other work I do on the command line. The tool is simple:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;python se.py âhelp
usage: se.py [-h] {markets,stories,relevant,decide,accepted,compile} ...

Story Evaluator CLI

positional arguments:
  {markets,stories,relevant,decide,accepted,compile}
                        Available commands
    markets             List markets
    stories             Manage stories
    relevant            List URLs for stories marked as relevant
    decide              Make accept/reject decisions on relevant stories
    accepted            Manage accepted stories
    compile             Show anthology compilation statistics

optional arguments:
  -h, âhelp            show this help message and exit&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;compile&lt;/code&gt; command ended up being really useful â it gave me instant feedback on anthology size and composition:
&lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;ANTHOLOGY COMPILATION STATISTICS
============================================================
Total Stories: 16
Total Word Count: 115,093 words
Average Word Count: 7,193 words
Unique Authors: 16
Markets Represented: 4

STORIES BY MARKET:
  analog-magazine: 2 stories (12.5%)
  asimovs-magazine: 2 stories (12.5%)
  clarkesworld-magazine: 10 stories (62.5%)
  reactor-magazine: 2 stories (12.5%)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This was really helpful during the selection process. I could quickly check how far along I was toward my ~120k word goal, and make sure I hadn't accidentally included multiple stories by the same author. &lt;/p&gt;
      &lt;p&gt; Step 3: Typesetting the print book &lt;/p&gt;
      &lt;p&gt; This part surprised me the most. I initially thought I'd have to learn Adobe InDesign or pay someone to do the typesetting. But I decided to use LaTeX instead, since I had some previous experience with it (another publishing friend sent me some of his example files, and I had some academic experience). The process worked out better than expected. &lt;/p&gt;
      &lt;p&gt; I used XeLaTeX with the &lt;code&gt;memoir&lt;/code&gt; document class. Here's what I liked about this approach:
&lt;/p&gt;
      &lt;div&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reproducible: I can rebuild the entire book from source in a few seconds, and I can use the same templates next year&lt;/item&gt;
          &lt;item&gt;Professional typography: LaTeX handles ligatures, kerning, and line breaking better than I could manually&lt;/item&gt;
          &lt;item&gt;Custom fonts: I used Crimson Pro for body text and Rajdhani for titles&lt;/item&gt;
          &lt;item&gt;Again, version control that I'm used to: The entire book is just text files in Git&lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
      &lt;p&gt; The main parts of the master file for the book are really simple: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\documentclass[final,11pt,twoside]{memoir}
\usepackage{compelling}

\begin{document}
\begin{frontmatter}
  \include{title}
  \tableofcontents
\end{frontmatter}

\begin{mainmatter}
  \include{introduction}
  \include{death-and-the-gorgon}
  \include{the-best-version-of-yourself}
  % ... 14 more stories
  \include{acknowledgements}
\end{mainmatter}
\end{document}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;div&gt;&lt;p&gt; All the formatting rules live in &lt;/p&gt;&lt;code&gt;compelling.sty&lt;/code&gt;&lt;p&gt;, a custom style package. &lt;/p&gt;Here's a link to the full, messy file&lt;p&gt;. Some highlights: &lt;/p&gt;&lt;/div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;% 6x9 inch trade paperback size
\setstocksize{9in}{6in}
\settrimmedsize{9in}{6in}{*}

% Margins
\setlrmarginsandblock{1.00in}{0.75in}{*}
\setulmarginsandblock{0.75in}{0.75in}{*}

% Typography nerding
\usepackage[final,protrusion=true,factor=1125,
            stretch=70,shrink=70]{microtype}

% Custom fonts loaded from local files
\setromanfont[
  Ligatures=TeX,
  Path=./Crimson_Pro/static/,
  UprightFont=CrimsonPro-Regular,
  BoldFont=CrimsonPro-Bold,
  ItalicFont=CrimsonPro-Italic,
  BoldItalicFont=CrimsonPro-BoldItalic
]{Crimson Pro}


\setsansfont[
  Path=./Rajdhani/,
  UprightFont=Rajdhani-Bold,
  BoldFont=Rajdhani-Bold,
  ItalicFont=Rajdhani-Bold,
  BoldItalicFont=Rajdhani-Bold
]{Rajdhani}

% Chinese font family for CJK characters
\newfontfamily\chinesefont{PingFang SC}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The &lt;code&gt;microtype&lt;/code&gt; package does a lot of subtle work with character spacing and line breaking that makes the text look professionally typeset.
&lt;/p&gt;
      &lt;p&gt; I wanted story titles in bold sans-serif with author names underneath in a lighter gray. Here's how I set that up: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\renewcommand{\chapter}[2]{
    \pagestyle{DefaultStyle}
    \stdchapter*{
        \sffamily
        \LARGE 
        \textbf{\MakeUppercase{#1}}
        \\ 
        \large 
        \color{dark-gray} 
        {\MakeUppercase{#2}}
    }
    \addcontentsline{toc}{chapter}{
        \protect\parbox[t]{\dimexpr\textwidth-3em}{
            \sffamily#1
            \\ 
            \protect\small
            \protect\color{gray}
            \protect\textit{#2}
        }
    }
    \def\leftmark{#1}
    \def\rightmark{#2}
}&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; This redefines the &lt;code&gt;chapter&lt;/code&gt; command to take two arguments, the title and byline, and sets up both the chapter formatting, TOC formatting, and makes sure that the title and byline are printed in the headers on alternating pages.
&lt;/p&gt;
      &lt;p&gt; Now every story file just says: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;\chapter{Death and the Gorgon}{by Greg Egan}
[story content]&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Most authors send me stories as HTML, PDF, or word, so I needed a way to convert them to LaTeX. I wrote a simple Python script to do this, which saved me a huge amount of manual formatting work. &lt;/p&gt;
      &lt;p&gt; Step 4: Creating the ebook &lt;/p&gt;
      &lt;p&gt; Print was one thing, but I also needed an ebook. This turned out to be easier than I expected because I could reuse all the LaTeX source I'd already created. &lt;/p&gt;
      &lt;p&gt; I used Pandoc to convert from LaTeX to EPUB: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Convert LaTeX to EPUB
pandoc 2025.tex -o Think_Weirder_2025.epub \
  âtoc \
  âepub-cover-image=cover_optimized.jpg \
  âcss=epub-style.css \
  âmetadata title="Think Weirder" \
  âmetadata author="Edited by Joe Stech"&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; Pandoc's default table of contents only showed story titles. But I wanted author names too, like you see in print anthologies. EPUBs are just zipped collections of XHTML files, so I wrote a small post-processing script: &lt;/p&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;def modify_toc(nav_content, authors):
    """Add author bylines to TOC entries."""
    pattern = r'&amp;lt;a href="([^"]+)"&amp;gt;([^&amp;lt;]+)&amp;lt;/a&amp;gt;'

    def add_author(match):
        href, title = match.group(1), match.group(2)
        chapter_id = extract_id_from_href(href)

        if chapter_id in authors:
            author = authors[chapter_id]
            return f'&amp;lt;a href="{href}"&amp;gt;{title}&amp;lt;br /&amp;gt;\n' \
                   f'&amp;lt;em&amp;gt;{author}&amp;lt;/em&amp;gt;&amp;lt;/a&amp;gt;'
        return match.group(0)

    return re.sub(pattern, add_author, nav_content)&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;p&gt; The script unzips the EPUB, finds the navigation file, adds author bylines, and rezips everything. Now the ebook table of contents matches the print version. &lt;/p&gt;
      &lt;p&gt; What I learned &lt;/p&gt;
      &lt;p&gt; The whole process took longer than I expected â many months of night work. The simple software I wrote really made it a feasible one-person project though, and motivates me to go through the whole process again next year. &lt;/p&gt;
      &lt;p&gt; Staying organized is crucial. When hundreds of stories are involved, it's easy to forget details, so using &lt;code&gt;se.py&lt;/code&gt; to save metadata in the moment that could be sliced and diced later was so important.
&lt;/p&gt;
      &lt;p&gt; Reproducible builds were a lifesaver. I made changes to the book layout right up until the week before publication. Because I could rebuild the entire book in seconds, and everything was backed up in git, I could experiment freely without worrying about breaking things. &lt;/p&gt;
      &lt;p&gt; Simple file formats made me comfortable. When something went wrong, I could always open a YAML file or look at the LaTeX source and understand what was happening. I never hit a point where the tools were a black box. &lt;/p&gt;
      &lt;p&gt; I didn't need to understand everything up front. I learned LaTeX details as I went (arguably I still don't really understand LaTeX). Same with Pandoc. I got something basic working first, then incrementally improved it. &lt;/p&gt;
      &lt;p&gt; Can you do this too? &lt;/p&gt;
      &lt;p&gt; If you're thinking about publishing a book â whether it's an anthology, a novel, or a collection of technical writing â I think this approach is worth considering. There's something motivating about having a detailed understanding of every step in the production process. If you have questions feel free to reach out, I love talking about this hobby! You can email me at joe@thinkweirder.com. &lt;/p&gt;
      &lt;div&gt;&lt;p&gt; And if you enjoy concept-driven science fiction that is heavy on novel ideas, check out &lt;/p&gt;Think Weirder! &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45968121</guid><pubDate>Tue, 18 Nov 2025 16:10:34 +0000</pubDate></item><item><title>Solving a million-step LLM task with zero errors</title><link>https://arxiv.org/abs/2511.09030</link><description>&lt;doc fingerprint="189cb409d4d7b6cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 12 Nov 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Solving a Million-Step LLM Task with Zero Errors&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:LLMs have achieved remarkable breakthroughs in reasoning, insights, and tool use, but chaining these abilities into extended processes at the scale of those routinely executed by humans, organizations, and societies has remained out of reach. The models have a persistent error rate that prevents scale-up: for instance, recent experiments in the Towers of Hanoi benchmark domain showed that the process inevitably becomes derailed after at most a few hundred steps. Thus, although LLM research is often still benchmarked on tasks with relatively few dependent logical steps, there is increasing attention on the ability (or inability) of LLMs to perform long range tasks. This paper describes MAKER, the first system that successfully solves a task with over one million LLM steps with zero errors, and, in principle, scales far beyond this level. The approach relies on an extreme decomposition of a task into subtasks, each of which can be tackled by focused microagents. The high level of modularity resulting from the decomposition allows error correction to be applied at each step through an efficient multi-agent voting scheme. This combination of extreme decomposition and error correction makes scaling possible. Thus, the results suggest that instead of relying on continual improvement of current LLMs, massively decomposed agentic processes (MDAPs) may provide a way to efficiently solve problems at the level of organizations and societies.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45968362</guid><pubDate>Tue, 18 Nov 2025 16:26:28 +0000</pubDate></item><item><title>Pebble, Rebble, and a path forward</title><link>https://ericmigi.com/blog/pebble-rebble-and-a-path-forward/</link><description>&lt;doc fingerprint="fc4b101b5b1408f0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I believe the Pebble community, Core Devices, Rebble and I all want the same thing. We love our Pebbles and want them to keep working long into the future. We love the community that has sprung up around Pebble, and how it’s persevered - next year will be the 14th anniversary of the original Kickstarter campaign!&lt;/p&gt;
      &lt;p&gt;But I have to respond to claims made by Rebble posted on their blog yesterday. I will link to their post so you can read their side of the story, and I’ve asked them to link back to this blog post from theirs.&lt;/p&gt;
      &lt;p&gt;Look - I’m the first person to call myself out when I fail. I wrote a detailed blog post about Success and Failure at Pebble and often write in detail about learning from my mistakes. But in this specific case, you’ll find that I’ve done my utmost to respect the Pebble legacy and community. Rebble is misleading the community with false accusations.&lt;/p&gt;
      &lt;p&gt;For those just passing through, here’s the TLDR: &lt;/p&gt;
      &lt;p&gt;Core Devices is a small company I started in 2025 to relaunch Pebble and build new Pebble smartwatches. Rebble is a non-profit organization that has supported the Pebble community since 2017. Rebble has done a ton of great work over the years and deserves recognition and support for that.&lt;/p&gt;
      &lt;p&gt;Core Devices and Rebble negotiated an agreement where Core would pay $0.20/user/month to support Rebble services. But the agreement broke down after over the following disagreement. &lt;/p&gt;
      &lt;p&gt;Rebble believes that they ‘100%’ own the data of the Pebble Appstore. They’re attempting to create a walled garden around 13,000 apps and faces that individual Pebble developers created and uploaded to the Pebble Appstore between 2012 and 2016. Rebble later scraped this data in 2017. &lt;/p&gt;
      &lt;p&gt;I disagree. I’m working hard to keep the Pebble ecosystem open source. I believe the contents of the Pebble Appstore should be freely available and not controlled by one organization. &lt;/p&gt;
      &lt;p&gt;Rebble posted a blog post yesterday with a bunch of false accusations, and in this post I speak to each of them.&lt;/p&gt;
      &lt;p&gt;Sections&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Dec 2016 - Pebble shut down. Some IP was sold to Fitbit. I blogged about why I think we failed. Fitbit continued to run the Pebble Appstore and web services for 1.5 years. I really appreciated that.&lt;list rend="ul"&gt;&lt;item&gt;Rebble organization grew out of the official Pebble Developers Discord.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;July 2018, Fitbit shut down the Pebble appstore.&lt;list rend="ul"&gt;&lt;item&gt;Before it shut down, Rebble (and others) scraped all 13,000 apps and metadata from the Pebble Appstore. Rebble began hosting a copy of the appstore. They created a new Dev Portal where developers could upload new apps, roughly 500 have been uploaded since July 2018.&lt;/item&gt;&lt;item&gt;Rebble also reverse engineered many Pebble web services (weather, timeline and voice transcription) and provided them as a paid service for the Pebble community.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Jan 2025 - Google open sourced PebbleOS, breathing new life into the community.&lt;/item&gt;
        &lt;item&gt;March 2025 - I announced a new company (Core Devices) and 2 new watches - store.rePebble.com&lt;/item&gt;
        &lt;item&gt;November 2025 - we finished shipping out 5,000 Pebble 2 Duos. We’re working hard on Pebble Time 2. We’re aiming to start shipping in January.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 1: ‘Rebble paid for the work that [Eric] took as a base for his commercial watches’&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I think they’re accusing me of ‘stealing’ open source contributions to PebbleOS that Rebble paid for. This is entirely false.&lt;/item&gt;
        &lt;item&gt;We did not take any PebbleOS work Rebble paid for ‘as a base for [our] commercial watches’. &lt;del rend="overstrike"&gt;To my best of my knowledge&lt;/del&gt;&lt;del rend="overstrike"&gt;,&lt;/del&gt;&lt;del rend="overstrike"&gt;Rebble never paid the&lt;/del&gt;&lt;del rend="overstrike"&gt;developer who ported NimBLE into PebbleOS.&lt;/del&gt;&lt;del rend="overstrike"&gt;My best guess is that they are referring to Rebble having paid CodeCoup, the company behind&lt;/del&gt;&lt;del rend="overstrike"&gt;NimBLE&lt;/del&gt;&lt;del rend="overstrike"&gt;, to fix some bugs that affected older non-Core Devices watches. Any Rebble-sponsored CodeCoup commits are not present in our repo. In fact, the opposite is true - we paid Codecoup $10,000 to fix multiple BLE stack issues, some of them on the host side that benefit all devices, including old Pebbles.&lt;/del&gt; Update: I’m told Rebble did pay him, months later. My point is valid - when we shifted development to our repo, Rebble had not paid anything. More broadly, I reject the premise that using open source software under the terms of the license, regardless of who funds development, is ‘stealing’.&lt;/item&gt;
        &lt;item&gt;We started using our own repo for PebbleOS development because PRs on the Rebble repo reviews were taking too long. We only had one firmware engineer at the time (now we have a whopping 2!) and he felt like he was being slowed down too much. All of our contributions to PebbleOS have been 100% open source.&lt;/item&gt;
        &lt;item&gt;Overall, the feedback that PebbleOS could benefit from open governance is well taken. Long term, PebbleOS would be a good fit for open source organization with experience in open governance, like Apache or Linux Foundation. I wrote about this last week.&lt;/item&gt;
        &lt;item&gt;With our small team and fairly quick development schedule, it's true that we haven't PRed our changes into Rebble’s repo. It’s tough to prioritize this while we are busy fixing bugs and getting ready for Pebble Time 2.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 2: ‘Core took Rebble’s work’ on &lt;code&gt;libpebblecommon&lt;/code&gt; to create &lt;code&gt;libpebble3&lt;/code&gt;&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;The majority (&amp;gt;90%) of our new open source&lt;code&gt;libpebble3&lt;/code&gt; library was written by Core Devices employees.  The remainder comes from &lt;code&gt;libpebblecommon&lt;/code&gt;, another open source library written by two people.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;In April 2025, Core purchased the copyright to the &lt;code&gt;libpebblecommon&lt;/code&gt; code from the two maintainers and incorporated it into &lt;code&gt;libpebble3&lt;/code&gt;**, which is also open source**.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;All our contributions to &lt;code&gt;libpebble3&lt;/code&gt; are GPL-3.0 licensed. Here’s the motivation behind that our licensing strategy for this repo. We use the same CLA agreement as Matrix, QT and MySQL. Our CLA explicitly includes a clause that requires to Core Devices to distribute all contributions under an OSI-compatible FOSS license (e.g. GPLv3).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Note that neither Rebble &lt;code&gt;libpebblecommon&lt;/code&gt; maintainer signed the Rebble blog post.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Side note regarding Cobble, I don’t think Rebble even knows this but in 2024, I personally spent over $30,000 to support its development, way before PebbleOS was open source. It was my own way to support the community.&lt;/p&gt;
      &lt;p&gt;Accusation 3: ‘Core promised that they would let Rebble maintain and own the developer site’&lt;/p&gt;
      &lt;p&gt;Facts:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Nothing of the sort was agreed upon. See the full written agreement that Core Devices has with Rebble towards the bottom. Rebble agreed that Core would host the developer site.&lt;/item&gt;
        &lt;item&gt;I have been maintaining and updating the developer site personally - all open source. Having two sources of truth would be confusing for the community.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Accusation 4: ‘[Eric] scraped our app store, in violation of the agreement that we reached with him previously’&lt;/p&gt;
      &lt;p&gt;Note: ‘scraping’ usually means to automated extraction of data from a website.&lt;/p&gt;
      &lt;p&gt;Facts: &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Here’s what happened. I wanted to highlight some of my favourite watchfaces on the Pebble Appstore. Last Monday Nov 10, after I put my kids to sleep and between long calls with factories in Asia, I started building a webapp to help me quickly go through Pebble Appstore and decide which were my top picks.&lt;/item&gt;
        &lt;item&gt;Let me be crystal clear - my little webapp did not download apps or ‘scrape’ anything from Rebble. The webapp displayed the name of each watchface and screenshots and let me click on my favs. I used it to manually look through 6000 watchfaces with my own eyes. I still have 7,000 to go. Post your server logs, they will match up identically to the app I (well…Claude) wrote (source code here)&lt;/item&gt;
        &lt;item&gt;I integrated these picks into the Pebble Appstore on Saturday and posted about it on Sunday.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of four of these accusations could have been clarified simply by asking me. Instead, Rebble decided to post them on their blog and threaten a lawsuit. &lt;/p&gt;
      &lt;p&gt;How did we get here?&lt;/p&gt;
      &lt;p&gt;Why are there dueling blog posts in the Pebbleverse? &lt;/p&gt;
      &lt;p&gt;I think most of the people are behind Rebble are great and the community overall is awesome. I know they truly mean well, but there are many aspects of the org that are severely troubling. I am very close with one of the Rebble board members, who I consider a personal friend. Over the years, I learned a lot about the organization and helped coach him through some major disputes between board members. &lt;/p&gt;
      &lt;p&gt;I exchanged literally thousands of messages with my friend on this topic over the span of 3 years. I refrained from getting too involved, despite being asked several times to join Rebble as a board member or lead the organization. I demurred - I saw how painful it was for him and I had no interest in being part of that. &lt;/p&gt;
      &lt;p&gt;Core Devices + Rebble: 2025&lt;/p&gt;
      &lt;p&gt;PebbleOS is now open source! Yay. This is thanks to the work of many Googlers, ex-Pebblers and others - I called out (hopefully) all of them in my blog post in March. I really wanted Rebble to be a part of the Pebble revival going forward. I hired 3 people from Rebble to join Core Devices. I regularly brought up Rebble’s efforts over the years.&lt;/p&gt;
      &lt;p&gt;I engaged with Rebble folks in discussions in the spring on how we could formally work together, and then made some concrete proposals in the summer. One difficulty was that Core Devices is a business with customers and schedules. This didn’t always sync up with the timeframes of a non-profit. Things became very drawn out. It was very hard to pin people down, even on simple stuff like what the goals of Rebble as an organization were. &lt;/p&gt;
      &lt;p&gt;Regardless, I continued pushing to make Rebble a key part of the Pebble relaunch.&lt;/p&gt;
      &lt;p&gt;By August, we finally got close to an agreement.&lt;/p&gt;
      &lt;p&gt;On September 30 2025, we agreed to the following document and published respective blog posts (ours, theres). Core Devices would pay Rebble $0.20/user/month. I considered it a donation to a group that has done so much to support the community. But I purposely pushed for openness - no single group (Core Devices or Rebble) should be in control. &lt;/p&gt;
      &lt;p&gt;Notice the final bullet in the App store section: &lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;All binary/metadata (including historical apps) will be published as archive file (no scraping Rebble services) &lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looking back, we should have had more clear wording in this agreement. But this was after months of chat discussions and hours of Zoom calls. I honestly thought that we had reached an agreement to make the archive open, like in this message I received from a Rebble board member.&lt;/p&gt;
      &lt;p&gt;By the end of October, Rebble has changed their mind about providing an archive file.&lt;/p&gt;
      &lt;p&gt;Not withstanding their false accusations of theft, the crux of our disagreement is the archive of 13,000 Pebble apps and watchfaces that were uploaded to the Pebble Appstore in July 2018 before it was shut down. &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;I believe that these apps and watchfaces should be archived publicly and freely accessible by anyone. They should not held behind a walled garden by one organization. I repeatedly advocated for hosting this data on a neutral 3rd party like Archive.org.&lt;/item&gt;
        &lt;item&gt;Rebble believes ‘the data behind the Pebble App Store is 100% Rebble’ (this is a direct quote from their blog post). They repeatedly refer to all watchfaces and watchapps as ‘our data’.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;This is just plainly false. The apps and watchfaces were originally uploaded by individual developers to an appstore run by a company that no longer exists. These folks created beautiful work and shared them freely with the Pebble community. I’ve spoken with numerous Pebble app developers about this. After the fall of Pebble Tech Corp, none of them envisioned one single organization claiming ownership of their work and restricting access, or charging money for access.&lt;/p&gt;
      &lt;p&gt;Let’s do the right thing - honour the original developers and create a free publicly available archive of their beautiful watchfaces and watchapps. &lt;/p&gt;
      &lt;p&gt;It's easy to assume the worst in situations like this. But our plan for the appstore is pretty straightforward. We’re working on rewriting the appstore frontend to be native in the mobile app rather than a web view. Rebble’s appstore backend API will be the data source. Rebble’s dev portal is where developers upload apps. No subscription or Rebble account will not be required to download apps. We intend to curate how the appstore is displayed Pebble app.&lt;/p&gt;
      &lt;p&gt;We’re excited to see other Pebble-supporting mobile apps pop up - like MicroPebble and GadgetBridge, offering different features and experiences. We’d love to support these efforts with open source code or financially.&lt;/p&gt;
      &lt;p&gt;Reading things like ‘We’re happy to let them build whatever they want as long as it doesn’t hurt Rebble’ in their blog post worries me. Take our voice-to-text and weather features. Rebble currently offers these as part of their paid subscription. Our new Pebble mobile app includes a on-device speech-to-text feature. We’re planning to include weather for free in our app and make the data available to all watchfaces so you don’t need to configure each one separately. These features are better for users but would they ‘hurt’ Rebble? Will I need to ask permission from Rebble before building these features? It’s clear that the goals of a non-profit and device manufacturer will not always be in alignment.&lt;/p&gt;
      &lt;p&gt;Now consider the appstore. It’s a fundamental part of the Pebble experience. Even before yesterday’s accusations, I felt wary about relying too heavily on a 3rd party like Rebble to provide such a critical service. When people buy a watch from Core Devices, they expect to be able to download apps and watchfaces. If Rebble leadership changes their mind, how can I be certain I can deliver a good experience for our customers? This is one of the primary reasons I think it’s important for an archive of the Pebble Appstore to be freely available.&lt;/p&gt;
      &lt;p&gt;Rebble - prove that you believe in an open, unrestricted Pebble community. Tear down the walled garden you are trying to create. Publish your copy of the Pebble Appstore archive. Stop saying that you ‘100%’ own other developers data. Let’s move on from this ridiculous sideshow and focus on making Pebble awesome!&lt;/p&gt;
      &lt;p&gt;I’ve worked hard to structure everything that we’re doing to be sustainable for the long term, and to do right by the Pebble community. I think Rebble should do the same. &lt;/p&gt;
      &lt;p&gt;I earned almost nothing from Pebble Tech Corp. I paid myself a $65,000 salary each year. I did not get any payout through the asset sale. I fought to make sure that all Pebble employees were taken care of as best as possible, and that the Pebble community would live on. I believe that at every turn, I’ve done right by the community.&lt;/p&gt;
      &lt;p&gt;I didn’t relaunch Pebble to make a lot of money. My goal this time round is to make it sustainable. I want to continue making more watches and cool gadgets. There are no investors. I am taking huge risks doing this. I relaunched it because I love Pebble and want it to live on long into the future. Generally, I am excited and positive for the future, despite everything.&lt;/p&gt;
      &lt;p&gt;For everyone else, again, I apologize for the extreme amounts of inside baseball and the better things you could be doing with your time. I’ll leave the comments open here. Please refrain from any personal attacks or vicious comments (at myself or other people) - follow the HN guidelines.&lt;/p&gt;
      &lt;p&gt;Eric Migicovsky&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45969250</guid><pubDate>Tue, 18 Nov 2025 17:24:27 +0000</pubDate></item><item><title>I am stepping down as the CEO of Mastodon</title><link>https://blog.joinmastodon.org/2025/11/my-next-chapter-with-mastodon/</link><description>&lt;doc fingerprint="6c92901d0c810af1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My next chapter with Mastodon&lt;/head&gt;
    &lt;p&gt;Eugen Rochko&lt;/p&gt;
    &lt;p&gt;Strategy &amp;amp; Product Advisor, Founder&lt;/p&gt;
    &lt;p&gt;After nearly 10 years, I am stepping down as the CEO of Mastodon and transferring my ownership of the trademark and other assets to the Mastodon non-profit. Over the course of my time at Mastodon, I have centered myself less and less in our outward communications, and to some degree, this is the culmination of that trend. Mastodon is bigger than me, and though the technology we develop on is itself decentralized—with heaps of alternative fediverse projects demonstrating that participation in this ecosystem is possible without our involvement—it benefits our community to ensure that the project itself which so many people have come to love and depend on remains true to its values. There are too many examples of founder egos sabotaging thriving communities, and while I’d like to think myself an exception, I understand why people would prefer better guardrails.&lt;/p&gt;
    &lt;p&gt;But it would be uncouth for me to pretend that there isn’t some self-interest involved. Being in charge of a social media project is, turns out, quite the stressful endeavour, and I don’t have the right personality for it. I think I need not elaborate that the passion so many feel for social media does not always manifest in healthy ways. You are to be compared with tech billionaires, with their immense wealth and layered support systems, but with none of the money or resources. It manifests in what people expect of you, and how people talk about you. I remember somebody jokingly suggesting that I challenge Elon Musk to a fight (this was during his and Mark Zuckerberg’s martial arts feud), and quietly thinking to myself, I am literally not paid enough for that. I remember also, some Spanish newspaper article that for some reason, concluded that I don’t dress as fashionably as Jeff Bezos, based on the extremely sparse number of pictures of myself I have shared on the web. Over an entire decade, these tiny things chip away at you slowly. Some things chip faster. I steer clear of showing vulnerability online, but there was a particularly bad interaction with a user last summer that made me realise that I need to take a step back and find a healthier relationship with the project, ultimately serving as the impetus to begin this restructuring process.&lt;/p&gt;
    &lt;p&gt;As for what the legacy of my run will be, I find hard to answer. For one, I think it is not up for me to judge. On the other hand, it is as much about what didn’t happen as it is about what did. I’ve always thought that one of the most important responsibilities I had was to say “no”. It is not a popular thing to do, nor is it a fun thing to do, but being pulled into too many different directions at once can spell disaster for any project. I’d like to think I avoided some trouble by being careful. But I’m also aware that my aversion to public appearances cost Mastodon some opportunities in publicity. Ultimately, while I cannot take sole credit for it, I am nevertheless most proud of how far we’ve made it over these last 10 years. From the most barebones project written out of my childhood bedroom, to one of the last remaining and thriving pieces of the original, community-centred internet.&lt;/p&gt;
    &lt;p&gt;I have so much passion for Mastodon and the fediverse. The fediverse is an island within an increasingly dystopian capitalist hellscape. And from my perspective, Mastodon is our best shot at bringing this vision of a better future to the masses. This is why I’m sticking around, albeit in a more advisory, and less public, role.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45969909</guid><pubDate>Tue, 18 Nov 2025 18:13:30 +0000</pubDate></item><item><title>Show HN: RowboatX – open-source Claude Code for everyday automations</title><link>https://github.com/rowboatlabs/rowboat</link><description>&lt;doc fingerprint="f5344fd3152ca8f9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✨ Create background agents with full shell access &lt;list rend="ul"&gt;&lt;item&gt;E.g. "Generate a NotebookLM-style podcast from my saved articles every morning"&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;🔧 Connect any MCP server to add capabilities &lt;list rend="ul"&gt;&lt;item&gt;Add MCP servers and RowboatX handles the integration&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;🎯 Let RowboatX control and monitor your background agents &lt;list rend="ul"&gt;&lt;item&gt;Easily inspect state on the filesystem&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inspired by Claude Code, RowboatX brings the same shell-native power to background automations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install RowboatX &lt;quote&gt;npx @rowboatlabs/rowboatx&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Configure LLM (defaults to OpenAI) &lt;code&gt;edit ~/.rowboat/config/models.json&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then set your API key in your environment. Supports OpenAI, Ollama, Anthropic, Gemini, LMStudio, OpenRouter, LiteLLM&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add MCP: 'Add this MCP server config: &amp;lt;config&amp;gt; '&lt;/item&gt;
      &lt;item&gt;Explore tools: 'What tools are there in &amp;lt;server-name&amp;gt; '&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;'Create agent to do X.'&lt;/item&gt;
      &lt;item&gt;'... Attach the correct tools from &amp;lt;mcp-server-name&amp;gt; to the agent'&lt;/item&gt;
      &lt;item&gt;'... Allow the agent to run shell commands including ffmpeg'&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;$ rowboatx&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;'Make agent &amp;lt;background-agent-name&amp;gt; run every day at 10 AM'&lt;/item&gt;
      &lt;item&gt;'What agents do I have scheduled to run and at what times'&lt;/item&gt;
      &lt;item&gt;'When was &amp;lt;background-agent-name&amp;gt; last run'&lt;/item&gt;
      &lt;item&gt;'Are any agents waiting for my input or confirmation'&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;rowboatx --agent=&amp;lt;agent-name&amp;gt; --input="xyz" --no-interactive=true&lt;/code&gt;
    &lt;code&gt;rowboatx --agent=&amp;lt;agent-name&amp;gt; --run_id=&amp;lt;run_id&amp;gt; # resume from a previous run&lt;/code&gt;
    &lt;p&gt;You can configure your models in &lt;code&gt;~/.rowboat/config/models.json&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;{
  "providers": {
    "openai": {
      "flavor": "openai"
    },
    "openai-compatible-host": {
      "flavor": "openai",
      "baseURL": "http://localhost:2000/...",
      "apiKey": "...",
      "headers": {
        "foo": "bar"
      }
    },
    "anthropic": {
      "flavor": "anthropic"
    },
    "google": {
      "flavor": "google"
    },
    "ollama": {
      "flavor": "ollama"
    }
  },
  "defaults": {
    "provider": "openai",
    "model": "gpt-5"
  }
}&lt;/code&gt;
    &lt;p&gt;To use Rowboat Classic UI (not RowboatX), refer to Classic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45970338</guid><pubDate>Tue, 18 Nov 2025 18:50:00 +0000</pubDate></item><item><title>OrthoRoute – GPU-accelerated autorouting for KiCad</title><link>https://bbenchoff.github.io/pages/OrthoRoute.html</link><description>&lt;doc fingerprint="f2d15d492c15bffa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;OrthoRoute — GPU-Accelerated Autorouting for KiCad&lt;/head&gt;
    &lt;p&gt;OrthoRoute is a GPU-accelerated PCB autorouter that uses a Manhattan lattice and the PathFinder algorithm to route high-density boards. Built as a KiCad plugin using the IPC API, it handles complex designs with thousands of nets that make traditional push-and-shove routers give up.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter, but at least this one is fast.&lt;/p&gt;
    &lt;head rend="h4"&gt;This document is a complement to the README in the Github repository. The README provides information about performance, capabilities, and tests. This document reflects more on the why and how OrthoRoute was developed.&lt;/head&gt;
    &lt;head rend="h1"&gt;Why I Built This&lt;/head&gt;
    &lt;p&gt;This is a project born out of necessity. Another thing I was working on needed an enormous backplane. A PCB with sixteen connectors, with 1,100 pins on each connector. That’s 17,600 individual pads, and 8,192 airwires that need to be routed. Here, just take a look:&lt;/p&gt;
    &lt;p&gt;Look at that shit. Hand routing this would take months. For a laugh, I tried FreeRouting, the KiCad autorouter plugin, and it routed 4% of the traces in seven hours. If that trend held, which it wouldn’t, that would be a month of autorouting. And it probably wouldn’t work in the end. I had a few options, all of which would take far too long&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I could route the board by hand. This would be painful and take months, but I would get a good-looking board at the end.&lt;/item&gt;
      &lt;item&gt;I could YOLO everything and just let the FreeRouting autorouter handle it. It would take weeks, because the first traces are easy, the last traces take the longest. This would result in an ugly board.&lt;/item&gt;
      &lt;item&gt;I could spend a month or two building my own autorouter plugin for KiCad. I have a fairly powerful GPU and I thought routing a PCB is a very parallel problem. I could also implement my own routing algorithms to make the finished product look good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When confronted with a task that will take months, always choose the more interesting path.&lt;/p&gt;
    &lt;head rend="h2"&gt;A New KiCad API, and a ‘Traditional’ Autorouter&lt;/head&gt;
    &lt;p&gt;KiCad, Pre-version 9.0, had a SWIG-based plugin system. There are serious deficits with this system compared to the new IPC plugin system released with KiCad 9. The SWIG-based system was locked to the Python environment bundled with KiCad. Process isolation, threading, and performance constraints were a problem. Doing GPU programming with CuPy or PyTorch, while not impossible, is difficult.&lt;/p&gt;
    &lt;p&gt;The new IPC plugin system for KiCad is a godsend. The basic structure of the OrthoRoute plugin looks something like this:&lt;/p&gt;
    &lt;p&gt;The OrthoRoute plugin communicates with KiCad via the IPC API over a UNIX-ey socket. This API is basically a bunch of C++ classes that gives me access to board data – nets, pads, copper pour geometry, airwires, and everything else. This allows me to build a second model of a PCB inside a Python script and model it however I want. With a second model of a board inside my plugin, all I have to do is draw the rest of the owl.&lt;/p&gt;
    &lt;head rend="h2"&gt;Development of the Manhattan Routing Engine&lt;/head&gt;
    &lt;p&gt;After wrapping my head around the the ability to read and write board information to and from KiCad, I had to figure out a way to route this stupidly complex backplane. A non-orthogonal autorouter is a good starting point, but I simply used that as an exercise to wrap my head around the KiCad IPC API. The real build is a ‘Manhattan Orthogonal Routing Engine’, the tool needed to route my mess of a backplane.&lt;/p&gt;
    &lt;head rend="h3"&gt;Project PathFinder&lt;/head&gt;
    &lt;p&gt;The algorithm used for this autorouter is PathFinder: a negotiation-based performance-driven router for FPGAs. My implementation of PathFinder treats the PCB as a graph: nodes are intersections on an x–y grid where vias can go, and edges are the segments between intersections where copper traces can run. Each edge and node is treated as a shared resource.&lt;/p&gt;
    &lt;p&gt;PathFinder is iterative. In the first iteration, all nets (airwires) are routed greedily, without accounting for overuse of nodes or edges. Subsequent iterations account for congestion, increasing the “cost” of overused edges and ripping up the worst offenders to re-route them. Over time, the algorithm converges to a PCB layout where no edge or node is over-subscribed by multiple nets.&lt;/p&gt;
    &lt;p&gt;With this architecture – the PathFinder algorithm on a very large graph, within the same order of magnitude of the largest FPGAs – it makes sense to run the algorithm with GPU acceleration. There are a few factors that went into this decision:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Everyone who’s routing giant backplanes probably has a gaming PC. Or you can rent a GPU from whatever company is advertising on MUNI bus stops this month.&lt;/item&gt;
      &lt;item&gt;The PathFinder algorithm requires hundreds of billions of calculations for every iteration, making single-core CPU computation glacially slow.&lt;/item&gt;
      &lt;item&gt;With CUDA, I can implement a SSSP (parallel Dijkstra) to find a path through a weighted graph very fast.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Adapting FPGA Algorithms to PCBs&lt;/head&gt;
    &lt;p&gt;The original PathFinder paper was, “A Negotiation-Based Performance-Driven Router for FPGAs” and from 1995, this meant early FPGAs like the Xilinx 3000 series and others manufactured by Tryptych. These devices were simple, and to get a good idea of how they worked, check out Ken Shirriff’s blog. Here’s what the inside of a Xilinx XC2064 looks like:&lt;/p&gt;
    &lt;p&gt;That looks complicated, but it’s really exceptionally simple. All the LUTs, or logic elements, are connected to each other with wires. Where the wires cross over, there are fuzes. Burn the fuzes and you’ve connected the wires together. It’s a simple graph and all the complexity of the actual paths inside the chip are abstracted away. For a circuit board, I don’t have this luxury. I have to figure out how to get the signal from the pads on the top layer of the PCB and ‘drill down’ with vias into the grid. I need to come up with some way to account for both the edges of the graph and nodes of the graph, something that’s untread territory with the PathFinder algorithm.&lt;/p&gt;
    &lt;p&gt;The first step of that is the pad escape planner that pre-computes the escape routing of all the pads. Because the entire Manhattan Routing Engine is designed for a backplane, we can make some assumptions: All of the components are going to be SMD, because THT parts would kill the efficiency of a routing lattice. The components are going to be arranged on a grid, and just to be nice I’d like some ‘randomization’ in where it puts the vias punching down into the grid. Here’s what the escape planning looks like:&lt;/p&gt;
    &lt;head rend="h3"&gt;How PathFinder Almost Killed Me, and How I made PathFinder not suck&lt;/head&gt;
    &lt;p&gt;I found every bug imaginable while developing OrthoRoute. For one, congestion of nets would grow each iterations. The router would start fine with 9,495 edges with congestion in iteration 1. Then iteration 2: 18,636 edges. Iteration 3: 36,998 edges. The overuse was growing by 3× per iteration instead of converging. Something was fundamentally broken. The culprit? History costs were decaying instead of accumulating. The algorithm needs to remember which edges were problematic in past iterations, but my implementation had &lt;code&gt;history_decay=0.995&lt;/code&gt;, so it was forgetting 0.5% of the problem every iteration. By iteration 10, it had forgotten everything. No memory = no learning = explosion.&lt;/p&gt;
    &lt;p&gt;With the history fixed, I ran another test. I got oscillation. The algorithm would improve for 12 iterations (9,495 → 5,527, a 42% improvement!), then spike back to 11,817, then drop to 7,252, then spike to 14,000. The pattern repeated forever. The problem was “adaptive hotset sizing”—when progress slowed, the algorithm would enlarge the set of nets being rerouted from 150 to 225, causing massive disruption. Fixing the hotset at 100 nets eliminated the oscillation.&lt;/p&gt;
    &lt;p&gt;Even with fixed hotsets, late-stage oscillation returned after iteration 15. Why? The present cost factor escalates exponentially: &lt;code&gt;pres_fac = 1.15^iteration&lt;/code&gt;. By iteration 19, present cost was 12.4× stronger than iteration 1, completely overwhelming history (which grows linearly). The solution: cap &lt;code&gt;pres_fac_max=8.0&lt;/code&gt; to keep history competitive throughout convergence.&lt;/p&gt;
    &lt;p&gt;PathFinder is designed for FPGAs, and each and every Xilinx XC3000 chip is the same as every other XC3000 chip. Configuring the parameters for an old Xilinx chip means every routing problem will probably converge on that particular chip. PCBs are different; every single PCB is different from every other PCB. There is no single set of history, pressure, and decay parameters that will work on every single PCB.&lt;/p&gt;
    &lt;p&gt;What I had to do was figure out these paramaters on the fly. So that’s what I did. Right now I’m using Board-adaptive parameters for the Manhattan router. Before beginning the PathFinder algorithm it analyzes the board in KiCad for the number of signal layers, how many nets will be routed, and how dense the set of nets are. It’s clunky, but it kinda works.&lt;/p&gt;
    &lt;p&gt;Where PathFinder was tuned once for each family of FPGAs, I’m auto-tuning it for the entire class of circuit boards. A huge backplane gets careful routing and an Arduino clone gets fast, aggressive routing. The hope is that both will converge – produce a valid routing solution – and maybe that works. Maybe it doesn’t. There’s still more work to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Routing The Monster Board&lt;/head&gt;
    &lt;p&gt;After significant testing with “small” boards (actually 500+ net subsets of my large backplane, with 18 layers), I started work on the entire purpose of this project, the 8000+ net, 17000 pad monster board. There was one significant problem: it wouldn’t fit on my GPU. Admittedly, I only have a 16GB Nvidia 5080, but even this was far too small for the big backplane.&lt;/p&gt;
    &lt;p&gt;This led me to develop a ‘cloud routing solution’. It boils down to extracting a “OrthoRoute PCB file” from the OrthoRoute plugin. From there, I rent a Linux box with a GPU and run the autorouting algorithm with a headless mode. This produces an “OrthoRoute Solution file”. I import this back into KiCad by running the OrthoRoute plugin on my local machine, and importing the solution file, then pushing that to KiCad.&lt;/p&gt;
    &lt;p&gt;Here’s the result:&lt;/p&gt;
    &lt;p&gt;That’s it, that’s the finished board. A few specs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;44,233 blind and buried vias. 68,975 track segments.&lt;/item&gt;
      &lt;item&gt;Routed on an 80GB A100 GPU, rented on vast.io. The total VRAM required to route this board was 33.5 GB, so close to being under 32GB and allowing me to rent a cheaper GPU&lt;/item&gt;
      &lt;item&gt;Total time to route this board to completion was 41 hours. This is far better than the months it would have taken FreeRouting to route this board, but it’s still not fast.&lt;/item&gt;
      &lt;item&gt;The routing result is good but not great. A big problem is the DRC-awareness of the escape pad planning. There are traces that don’t quite overlap, but because of the geometry generated by the escape route planner they don’t pass a strict DRC. This could be fixed in future versions. There are also some overlapping traces in what PathFinder generated. Not many, but a few.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While the output from my autorouter isn’t perfect, no one would expect an autorouter to produce a perfect result, ready for production. It’s an autorouter, something you shouldn’t trust. Turning the result for OrthoRoute into a DRC-compliant board took a few days, but it was far easier than the intractable problem of eight thousand airwires I had at the beginning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future of OrthoRoute&lt;/head&gt;
    &lt;p&gt;I built this for one reason: to route my pathologically large backplane. Mission accomplished. And along the way, I accidentally built something more useful than I expected.&lt;/p&gt;
    &lt;p&gt;OrthoRoute proves that GPU-accelerated routing isn’t just theoretical, and that algorithms designed for routing FPGAs can be adapted to the more general class of circuit boards. It’s fast, too. The Manhattan lattice approach handles high-density designs that make traditional autorouters choke. And the PathFinder implementation converges in minutes on boards that would take hours or days with CPU-based approaches.&lt;/p&gt;
    &lt;p&gt;More importantly, the architecture is modular. The hard parts—KiCad IPC integration, GPU acceleration framework, DRC-aware routing space generation are done. Adding new routing strategies on top of this foundation is straightforward. Someone could implement different algorithms, optimize for specific board types, or extend it to handle flex PCBs.&lt;/p&gt;
    &lt;p&gt;The code is up on GitHub. I’m genuinely curious what other people will do with it. Want to add different routing strategies? Optimize for RF boards? Extend it to flex PCBs? PRs welcome, contributors welcome.&lt;/p&gt;
    &lt;p&gt;And yes, you should still manually route critical signals. But for dense digital boards with hundreds of mundane power and data nets? Let the GPU handle it while you grab coffee. That’s what autorouters are for.&lt;/p&gt;
    &lt;p&gt;Never trust the autorouter. But at least this one is fast.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45970391</guid><pubDate>Tue, 18 Nov 2025 18:54:54 +0000</pubDate></item><item><title>Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark</title><link>https://simonwillison.net/2025/Nov/18/gemini-3/</link><description>&lt;doc fingerprint="7dd78fcf8f3f9a91"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Trying out Gemini 3 Pro with audio transcription and a new pelican benchmark&lt;/head&gt;
    &lt;p&gt;18th November 2025&lt;/p&gt;
    &lt;p&gt;Google released Gemini 3 Pro today. Here’s the announcement from Sundar Pichai, Demis Hassabis, and Koray Kavukcuoglu, their developer blog announcement from Logan Kilpatrick, the Gemini 3 Pro Model Card, and their collection of 11 more articles. It’s a big release!&lt;/p&gt;
    &lt;p&gt;I had a few days of preview access to this model via AI Studio. The best way to describe it is that it’s Gemini 2.5 upgraded to match the leading rival models.&lt;/p&gt;
    &lt;p&gt;Gemini 3 has the same underlying characteristics as Gemini 2.5. The knowledge cutoff is the same (January 2025). It accepts 1 million input tokens, can output up to 64,000 tokens, and has multimodal inputs across text, images, audio, and video.&lt;/p&gt;
    &lt;head rend="h4"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;Google’s own reported numbers (in the model card) show it scoring slightly higher against Claude 4.5 Sonnet and GPT-5.1 against most of the standard benchmarks. As always I’m waiting for independent confirmation, but I have no reason to believe those numbers are inaccurate.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pricing&lt;/head&gt;
    &lt;p&gt;It terms of pricing it’s a little more expensive than Gemini 2.5 but still cheaper than Claude Sonnet 4.5. Here’s how it fits in with those other leading models:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Input (per 1M tokens)&lt;/cell&gt;
        &lt;cell role="head"&gt;Output (per 1M tokens)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-5.1&lt;/cell&gt;
        &lt;cell&gt;$1.25&lt;/cell&gt;
        &lt;cell&gt;$10.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemini 2.5 Pro&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $1.25&lt;p&gt;&amp;gt; 200k tokens: $2.50&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $10.00&lt;p&gt;&amp;gt; 200k tokens: $15.00&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemini 3 Pro&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $2.00&lt;p&gt;&amp;gt; 200k tokens: $4.00&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $12.00&lt;p&gt;&amp;gt; 200k tokens: $18.00&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude Sonnet 4.5&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $3.00&lt;p&gt;&amp;gt; 200k tokens: $6.00&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt; ≤ 200k tokens: $15.00&lt;p&gt;&amp;gt; 200k tokens: $22.50&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude Opus 4.1&lt;/cell&gt;
        &lt;cell&gt;$15.00&lt;/cell&gt;
        &lt;cell&gt;$75.00&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Trying it out against a complex image&lt;/head&gt;
    &lt;p&gt;That screenshot of the benchmarks from above looked like a good test for Gemini 3’s multimodal support. I fed it that image URL and asked it to generate alt text for the image:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview -a https://static.simonwillison.net/static/2025/gemini-3-benchmarks.jpg 'Alt text for this image, include all figures and make them comprehensible to a screen reader user'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here’s what I got back:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A comprehensive table comparing four AI models across various performance benchmarks. The columns listed are: Benchmark Name, Description, Gemini 3 Pro (highlighted in blue), Gemini 2.5 Pro, Claude Sonnet 4.5, and GPT-5.1.&lt;/p&gt;
      &lt;p&gt;Humanity’s Last Exam (Academic reasoning)&lt;/p&gt;
      &lt;item&gt;No tools: Gemini 3 Pro 37.5%, Gemini 2.5 Pro 21.6%, Claude Sonnet 4.5 13.7%, GPT-5.1 26.5%.&lt;/item&gt;
      &lt;item&gt;With search and code execution: Gemini 3 Pro 45.8% (others have no data).&lt;/item&gt;
      &lt;p&gt;ARC-AGI-2 (Visual reasoning puzzles; ARC Prize Verified)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 31.1%, Gemini 2.5 Pro 4.9%, Claude Sonnet 4.5 13.6%, GPT-5.1 17.6%.&lt;/item&gt;
      &lt;p&gt;GPQA Diamond (Scientific knowledge; No tools)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 91.9%, Gemini 2.5 Pro 86.4%, Claude Sonnet 4.5 83.4%, GPT-5.1 88.1%.&lt;/item&gt;
      &lt;p&gt;AIME 2025 (Mathematics)&lt;/p&gt;
      &lt;item&gt;No tools: Gemini 3 Pro 95.0%, Gemini 2.5 Pro 88.0%, Claude Sonnet 4.5 87.0%, GPT-5.1 94.0%.&lt;/item&gt;
      &lt;item&gt;With code execution: Gemini 3 Pro 100%, Claude Sonnet 4.5 100%.&lt;/item&gt;
      &lt;p&gt;MathArena Apex (Challenging Math Contest problems)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 23.4%, Gemini 2.5 Pro 0.5%, Claude Sonnet 4.5 1.6%, GPT-5.1 1.0%.&lt;/item&gt;
      &lt;p&gt;MMMU-Pro (Multimodal understanding and reasoning)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 81.0%, Gemini 2.5 Pro 68.0%, Claude Sonnet 4.5 68.0%, GPT-5.1 76.0%.&lt;/item&gt;
      &lt;p&gt;ScreenSpot-Pro (Screen understanding)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 72.7%, Gemini 2.5 Pro 11.4%, Claude Sonnet 4.5 36.2%, GPT-5.1 3.5%.&lt;/item&gt;
      &lt;p&gt;CharXiv Reasoning (Information synthesis from complex charts)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 81.4%, Gemini 2.5 Pro 69.6%, Claude Sonnet 4.5 68.5%, GPT-5.1 69.5%.&lt;/item&gt;
      &lt;p&gt;OmniDocBench 1.5 (OCR; Overall Edit Distance, lower is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 0.115, Gemini 2.5 Pro 0.145, Claude Sonnet 4.5 0.145, GPT-5.1 0.147.&lt;/item&gt;
      &lt;p&gt;Video-MMMU (Knowledge acquisition from videos)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 87.6%, Gemini 2.5 Pro 83.6%, Claude Sonnet 4.5 77.8%, GPT-5.1 80.4%.&lt;/item&gt;
      &lt;p&gt;LiveCodeBench Pro (Competitive coding problems; Elo Rating, higher is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 2,439; Gemini 2.5 Pro 1,775; Claude Sonnet 4.5 1,418; GPT-5.1 2,243.&lt;/item&gt;
      &lt;p&gt;Terminal-Bench 2.0 (Agentic terminal coding; Terminus-2 agent)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 54.2%, Gemini 2.5 Pro 32.6%, Claude Sonnet 4.5 42.8%, GPT-5.1 47.6%.&lt;/item&gt;
      &lt;p&gt;SWE-Bench Verified (Agentic coding; Single attempt)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 76.2%, Gemini 2.5 Pro 59.6%, Claude Sonnet 4.5 77.2%, GPT-5.1 76.3%.&lt;/item&gt;
      &lt;p&gt;t2-bench (Agentic tool use)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 85.4%, Gemini 2.5 Pro 54.9%, Claude Sonnet 4.5 84.7%, GPT-5.1 80.2%.&lt;/item&gt;
      &lt;p&gt;Vending-Bench 2 (Long-horizon agentic tasks; Net worth (mean), higher is better)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro $5,478.16; Gemini 2.5 Pro $573.64; Claude Sonnet 4.5 $3,838.74; GPT-5.1 $1,473.43.&lt;/item&gt;
      &lt;p&gt;FACTS Benchmark Suite (Held out internal grounding, parametric, MM, and search retrieval benchmarks)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 70.5%, Gemini 2.5 Pro 63.4%, Claude Sonnet 4.5 50.4%, GPT-5.1 50.8%.&lt;/item&gt;
      &lt;p&gt;SimpleQA Verified (Parametric knowledge)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 72.1%, Gemini 2.5 Pro 54.5%, Claude Sonnet 4.5 29.3%, GPT-5.1 34.9%.&lt;/item&gt;
      &lt;p&gt;MMMLU (Multilingual Q&amp;amp;A)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 91.8%, Gemini 2.5 Pro 89.5%, Claude Sonnet 4.5 89.1%, GPT-5.1 91.0%.&lt;/item&gt;
      &lt;p&gt;Global PIQA (Commonsense reasoning across 100 Languages and Cultures)&lt;/p&gt;
      &lt;item&gt;Gemini 3 Pro 93.4%, Gemini 2.5 Pro 91.5%, Claude Sonnet 4.5 90.1%, GPT-5.1 90.9%.&lt;/item&gt;
      &lt;p&gt;MRCR v2 (8-needle) (Long context performance)&lt;/p&gt;
      &lt;item&gt;128k (average): Gemini 3 Pro 77.0%, Gemini 2.5 Pro 58.0%, Claude Sonnet 4.5 47.1%, GPT-5.1 61.6%.&lt;/item&gt;
      &lt;item&gt;1M (pointwise): Gemini 3 Pro 26.3%, Gemini 2.5 Pro 16.4%, Claude Sonnet 4.5 (not supported), GPT-5.1 (not supported).&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have not checked every line of this but a loose spot-check looks accurate to me.&lt;/p&gt;
    &lt;p&gt;That prompt took 1,105 input and 3,901 output tokens, at a cost of 5.6824 cents.&lt;/p&gt;
    &lt;p&gt;I ran this follow-up prompt:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -c 'Convert to JSON'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can see the full output here, which starts like this:&lt;/p&gt;
    &lt;code&gt;{
  "metadata": {
    "columns": [
      "Benchmark",
      "Description",
      "Gemini 3 Pro",
      "Gemini 2.5 Pro",
      "Claude Sonnet 4.5",
      "GPT-5.1"
    ]
  },
  "benchmarks": [
    {
      "name": "Humanity's Last Exam",
      "description": "Academic reasoning",
      "sub_results": [
        {
          "condition": "No tools",
          "gemini_3_pro": "37.5%",
          "gemini_2_5_pro": "21.6%",
          "claude_sonnet_4_5": "13.7%",
          "gpt_5_1": "26.5%"
        },
        {
          "condition": "With search and code execution",
          "gemini_3_pro": "45.8%",
          "gemini_2_5_pro": null,
          "claude_sonnet_4_5": null,
          "gpt_5_1": null
        }
      ]
    },&lt;/code&gt;
    &lt;head rend="h4"&gt;Analyzing a city council meeting&lt;/head&gt;
    &lt;p&gt;To try it out against an audio file I extracted the 3h33m of audio from the video Half Moon Bay City Council Meeting—November 4, 2025. I used &lt;code&gt;yt-dlp&lt;/code&gt; to get that audio:&lt;/p&gt;
    &lt;code&gt;yt-dlp -x --audio-format m4a 'https://www.youtube.com/watch?v=qgJ7x7R6gy0'&lt;/code&gt;
    &lt;p&gt;That gave me a 74M m4a file, which I ran through Gemini 3 Pro like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview -a /tmp/HMBCC\ 11⧸4⧸25\ -\ Half\ Moon\ Bay\ City\ Council\ Meeting\ -\ November\ 4,\ 2025\ \[qgJ7x7R6gy0\].m4a 'Output a Markdown transcript of this meeting. Include speaker names and timestamps. Start with an outline of the key meeting sections, each with a title and summary and timestamp and list of participating names. Note in bold if anyone raised their voices, interrupted each other or had disagreements. Then follow with the full transcript.'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;That failed with an “Internal error encountered” message, so I shrunk the file down to a more manageable 38MB using &lt;code&gt;ffmpeg&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;ffmpeg -i "/private/tmp/HMB.m4a" -ac 1 -ar 22050 -c:a aac -b:a 24k "/private/tmp/HMB_compressed.m4a"&lt;/code&gt;
    &lt;p&gt;Then ran it again like this (for some reason I had to use &lt;code&gt;--attachment-type&lt;/code&gt; this time):&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;llm -m gemini-3-pro-preview --attachment-type /tmp/HMB_compressed.m4a 'audio/aac' 'Output a Markdown transcript of this meeting. Include speaker names and timestamps. Start with an outline of the key meeting sections, each with a title and summary and timestamp and list of participating names. Note in bold if anyone raised their voices, interrupted each other or had disagreements. Then follow with the full transcript.'&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;This time it worked! The full output is here, but it starts like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the transcript of the Half Moon Bay City Council meeting.&lt;/p&gt;
      &lt;head&gt;Meeting Outline&lt;/head&gt;
      &lt;p&gt;1. Call to Order, Updates, and Public Forum&lt;/p&gt;
      &lt;item&gt;Summary: Mayor Brownstone calls the meeting to order. City Manager Chidester reports no reportable actions from the closed session. Announcements are made regarding food insecurity volunteers and the Diwali celebration. During the public forum, Councilmember Penrose (speaking as a citizen) warns against autocracy. Citizens speak regarding lease agreements, downtown maintenance, local music events, and homelessness outreach statistics.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:00:00—00:13:25&lt;/item&gt;
      &lt;item&gt;Participants: Mayor Brownstone, Matthew Chidester, Irma Acosta, Deborah Penrose, Jennifer Moore, Sandy Vella, Joaquin Jimenez, Anita Rees.&lt;/item&gt;
      &lt;p&gt;2. Consent Calendar&lt;/p&gt;
      &lt;item&gt;Summary: The Council approves minutes from previous meetings and a resolution authorizing a licensing agreement for Seahorse Ranch. Councilmember Johnson corrects a pull request regarding abstentions on minutes.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:13:25—00:15:15&lt;/item&gt;
      &lt;item&gt;Participants: Mayor Brownstone, Councilmember Johnson, Councilmember Penrose, Vice Mayor Ruddick, Councilmember Nagengast.&lt;/item&gt;
      &lt;p&gt;3. Ordinance Introduction: Commercial Vitality (Item 9A)&lt;/p&gt;
      &lt;item&gt;Summary: Staff presents a new ordinance to address neglected and empty commercial storefronts, establishing maintenance and display standards. Councilmembers discuss enforcement mechanisms, window cleanliness standards, and the need for objective guidance documents to avoid subjective enforcement.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:15:15—00:30:45&lt;/item&gt;
      &lt;item&gt;Participants: Karen Decker, Councilmember Johnson, Councilmember Nagengast, Vice Mayor Ruddick, Councilmember Penrose.&lt;/item&gt;
      &lt;p&gt;4. Ordinance Introduction: Building Standards &amp;amp; Electrification (Item 9B)&lt;/p&gt;
      &lt;item&gt;Summary: Staff introduces updates to the 2025 Building Code. A major change involves repealing the city’s all-electric building requirement due to the 9th Circuit Court ruling (California Restaurant Association v. City of Berkeley). Public speaker Mike Ferreira expresses strong frustration and disagreement with “unelected state agencies” forcing the City to change its ordinances.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:30:45—00:45:00&lt;/item&gt;
      &lt;item&gt;Participants: Ben Corrales, Keith Weiner, Joaquin Jimenez, Jeremy Levine, Mike Ferreira, Councilmember Penrose, Vice Mayor Ruddick.&lt;/item&gt;
      &lt;p&gt;5. Housing Element Update &amp;amp; Adoption (Item 9C)&lt;/p&gt;
      &lt;item&gt;Summary: Staff presents the 5th draft of the Housing Element, noting State HCD requirements to modify ADU allocations and place a measure on the ballot regarding the “Measure D” growth cap. There is significant disagreement from Councilmembers Ruddick and Penrose regarding the State’s requirement to hold a ballot measure. Public speakers debate the enforceability of Measure D. Mike Ferreira interrupts the vibe to voice strong distaste for HCD’s interference in local law. The Council votes to adopt the element but strikes the language committing to a ballot measure.&lt;/item&gt;
      &lt;item&gt;Timestamp: 00:45:00—01:05:00&lt;/item&gt;
      &lt;item&gt;Participants: Leslie (Staff), Joaquin Jimenez, Jeremy Levine, Mike Ferreira, Councilmember Penrose, Vice Mayor Ruddick, Councilmember Johnson.&lt;/item&gt;
      &lt;head&gt;Transcript&lt;/head&gt;
      &lt;p&gt;Mayor Brownstone [00:00:00] Good evening everybody and welcome to the November 4th Half Moon Bay City Council meeting. As a reminder, we have Spanish interpretation services available in person and on Zoom.&lt;/p&gt;
      &lt;p&gt;Victor Hernandez (Interpreter) [00:00:35] Thank you, Mr. Mayor, City Council, all city staff, members of the public. [Spanish instructions provided regarding accessing the interpretation channel on Zoom and in the room.] Thank you very much.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Those first two lines of the transcript already illustrate something interesting here: Gemini 3 Pro chose NOT to include the exact text of the Spanish instructions, instead summarizing them as “[Spanish instructions provided regarding accessing the interpretation channel on Zoom and in the room.]”.&lt;/p&gt;
    &lt;p&gt;I haven’t spot-checked the entire 3hr33m meeting, but I’ve confirmed that the timestamps do not line up. The transcript closes like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mayor Brownstone [01:04:00] Meeting adjourned. Have a good evening.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That actually happens at 3h31m5s and the mayor says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Okay. Well, thanks everybody, members of the public for participating. Thank you for staff. Thank you to fellow council members. This meeting is now adjourned. Have a good evening.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I’m disappointed about the timestamps, since mismatches there make it much harder to jump to the right point and confirm that the summarized transcript is an accurate representation of what was said.&lt;/p&gt;
    &lt;p&gt;This took 320,087 input tokens and 7,870 output tokens, for a total cost of $1.42.&lt;/p&gt;
    &lt;head rend="h4"&gt;And a new pelican benchmark&lt;/head&gt;
    &lt;p&gt;Gemini 3 Pro has a new concept of a “thinking level” which can be set to low or high (and defaults to high). I tried my classic Generate an SVG of a pelican riding a bicycle prompt at both levels.&lt;/p&gt;
    &lt;p&gt;Here’s low—Gemini decided to add a jaunty little hat (with a comment in the SVG that says &lt;code&gt;&amp;lt;!-- Hat (Optional Fun Detail) --&amp;gt;&lt;/code&gt;):&lt;/p&gt;
    &lt;p&gt;And here’s high. This is genuinely an excellent pelican, and the bicycle frame is at least the correct shape:&lt;/p&gt;
    &lt;p&gt;Honestly though, my pelican benchmark is beginning to feel a little bit too basic. I decided to upgrade it. Here’s v2 of the benchmark, which I plan to use going forward:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;Generate an SVG of a California brown pelican riding a bicycle. The bicycle must have spokes and a correctly shaped bicycle frame. The pelican must have its characteristic large pouch, and there should be a clear indication of feathers. The pelican must be clearly pedaling the bicycle. The image should show the full breeding plumage of the California brown pelican.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;For reference, here’s a photo I took of a California brown pelican recently (sadly without a bicycle):&lt;/p&gt;
    &lt;p&gt;Here’s Gemini 3 Pro’s attempt at high thinking level for that new prompt:&lt;/p&gt;
    &lt;p&gt;And for good measure, here’s that same prompt against GPT-5.1—which produced this dumpy little fellow:&lt;/p&gt;
    &lt;p&gt;And Claude Sonnet 4.5, which didn’t do quite as well:&lt;/p&gt;
    &lt;p&gt;None of the models seem to have caught on to the crucial detail that the California brown pelican is not, in fact, brown.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What happens if AI labs train for pelicans riding bicycles? - 13th November 2025&lt;/item&gt;
      &lt;item&gt;Reverse engineering Codex CLI to get GPT-5-Codex-Mini to draw me a pelican - 9th November 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45970519</guid><pubDate>Tue, 18 Nov 2025 19:05:44 +0000</pubDate></item><item><title>GitHub: Git operation failures</title><link>https://www.githubstatus.com/incidents/5q7nmlxz30sk</link><description>&lt;doc fingerprint="7096aa564e934ff0"&gt;
  &lt;main&gt;
    &lt;p&gt;This incident has been resolved. Thank you for your patience and understanding as we addressed this issue. A detailed root cause analysis will be shared as soon as it is available.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:59 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Git Operations is operating normally.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:56 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing full recovery after rolling out the fix and all services are operational.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:55 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Codespaces is operating normally.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:55 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have shipped a fix and are seeing recovery in some areas and will continue to provide updates.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:36 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We have identified the likely cause of the incident and are working on a fix. We will provide another update as we get closer to deploying the fix.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:27 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Codespaces is experiencing degraded availability. We are continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:25 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are currently investigating failures on all Git operations, including both SSH and HTTP.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 21:11 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;We are seeing failures for some git http operations and are investigating&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:52 UTC&lt;/p&gt;
    &lt;p&gt;Update&lt;/p&gt;
    &lt;p&gt;Git Operations is experiencing degraded availability. We are continuing to investigate.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:39 UTC&lt;/p&gt;
    &lt;p&gt;Investigating&lt;/p&gt;
    &lt;p&gt;We are currently investigating this issue.&lt;/p&gt;
    &lt;p&gt;Posted Nov 18, 2025 - 20:39 UTC&lt;/p&gt;
    &lt;p&gt;This incident affected: Git Operations and Codespaces.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45971726</guid><pubDate>Tue, 18 Nov 2025 20:40:46 +0000</pubDate></item><item><title>Bild AI (YC W25) is hiring – Make housing affordable</title><link>https://www.ycombinator.com/companies/bild-ai/jobs/m2ilR5L-founding-engineer-applied-ai</link><description>&lt;doc fingerprint="19b82e91a7888e36"&gt;
  &lt;main&gt;
    &lt;p&gt;AI that understands construction blueprints&lt;/p&gt;
    &lt;p&gt;Puneet and I (Roop) founded Bild AI to tackle the mess that is blueprint reading, cost estimation, and permit applications in construction. It's a tough technical problem that requires the newest CV and AI approaches, and we’re impact-driven to make it more efficient to build more houses, hospitals, and schools. Featured on Business Insider.&lt;/p&gt;
    &lt;p&gt;Bild AI is an early-stage startup with a ton of really difficult technical challenges to solve. We're building blueprint understanding with a model-garden approach, so there is a lots of ground to break. We raised from the top VCs in the world before demo day and have a customer-obsessed approach to product development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45972390</guid><pubDate>Tue, 18 Nov 2025 21:29:37 +0000</pubDate></item><item><title>Blender 5.0</title><link>https://www.blender.org/download/releases/5-0/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45972519</guid><pubDate>Tue, 18 Nov 2025 21:39:18 +0000</pubDate></item><item><title>Monotype font licencing shake-down</title><link>https://www.insanityworks.org/randomtangent/2025/11/14/monotype-font-licencing-shake-down</link><description>&lt;doc fingerprint="2c5093ddfa2347f0"&gt;
  &lt;main&gt;
    &lt;p&gt;tl;dr Don’t try to shake-down a typography nerd with your dubious, automated claims about his employer using unlicensed fonts.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it started&lt;/head&gt;
    &lt;p&gt;It started with a LinkedIn InMail message (sanitised to protect privacy):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Subject: [Urgent] Font Software Licensing Review&lt;/p&gt;
      &lt;p&gt;Hi Ameel&lt;/p&gt;
      &lt;p&gt;I hope you’re doing well.&lt;/p&gt;
      &lt;p&gt;I’m [NAME] from Monotype and have been trying to reach you at [WORK EMAIL ADDRESS], but I’m unsure if my emails have been received.&lt;/p&gt;
      &lt;p&gt;Our team has identified Monotype font software embedded in the websites/apps of [YOUR COMPANY], but we couldn’t locate the corresponding licenses in our database.&lt;/p&gt;
      &lt;p&gt;Would you be able to share the correct email address so I can provide more details and documentation? Alternatively, you’re welcome to reach out to me directly at [SENDER’S EMAIL ADDRESS]&lt;/p&gt;
      &lt;p&gt;I appreciate your time and look forward to resolving this with you at your earliest convenience.&lt;/p&gt;
      &lt;p&gt;Best regards,&lt;/p&gt;
      &lt;p&gt;[NAME]&lt;/p&gt;
      &lt;p&gt;Business Development Representative | Monotype, Australia&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was puzzled by this for two reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This person had my correct work email address, but at no point had they sent me an email about this issue or about anything else. I checked. (Starting with a lie? Not a good look.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As far as I knew, my employer didn’t use any Monotype fonts on its websites and apps.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Always good to check&lt;/head&gt;
    &lt;p&gt;Just in case there was any merit to this claim, I did a quick review and found that I was right. In Australia we have one corporate website, one retail website, and one retail app (with iOS and Android versions) and all of those use our official font which, for better or for worse, is Open Sans.&lt;/p&gt;
    &lt;p&gt;Now, not only is Open Sans not a Monotype typeface, it is available under the SIL Open Font License (OFL) meaning we can use this typeface for literally anything except selling the font itself. So that couldn’t be what this person was talking about.&lt;/p&gt;
    &lt;p&gt;Next I checked our international retail websites and our project and partnership websites, since all of those have their own branding.&lt;/p&gt;
    &lt;p&gt;I found that on the websites we directly manage, we use these typefaces:&lt;/p&gt;
    &lt;p&gt;So no issue there.&lt;/p&gt;
    &lt;p&gt;And on the websites our partners manage, we use these typefaces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Public Sans, available under the SIL OFL,&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network Sans, a custom font created for the government agency that built the website this is used on so they wouldn’t need a licence, and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proxima Nova, the only font that does require a licence, except that Monotype doesn’t sell a licence to it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So no issue there either.&lt;/p&gt;
    &lt;p&gt;With those initial checks done, I reached out to our digital team (who build and manage our websites and apps) with a screenshot of the LinkedIn message I’d received and a summary of my investigation. I asked them how they wanted me to reply.&lt;/p&gt;
    &lt;p&gt;I needed to check with these folks first because my team and I only look after the corporate website. My employer’s overall digital presence – including the back-end of the corporate website – is managed by the digital team.&lt;/p&gt;
    &lt;p&gt;Turns out a couple of people from the digital and design teams had received identical messages from this Monotype “Business Development Representative”.&lt;/p&gt;
    &lt;p&gt;A few internal back-and-forth emails later we decided that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Instead of all of us responding, only one nominated person from the digital team would respond.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;But before responding, the digital team would do their own investigation into the fonts we use and the licences we own so we could verify everything was in compliance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course that’s not how things actually went down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fishing (phishing?) around&lt;/head&gt;
    &lt;p&gt;What the Monotype rep did next is kind of what a malicious hacker does when they’re trying to get someone from your company to click on a link that’ll install malware on your computer. Over the next couple of weeks, the rep messaged a dozen or so more people from different parts of the business, hoping to hook just one person who would reply to the scary message they were sending.&lt;/p&gt;
    &lt;p&gt;Now I’d already emailed my design, brand, and digital team colleagues to tell them about this mass-messaging campaign and our plan of action for it, but the Monotype rep expanded their campaign to include people from our procurement team, who I hadn’t thought to forewarn.&lt;/p&gt;
    &lt;p&gt;So not long after, I received a message from one of my procurement team colleagues who’d been forwarded that LinkedIn message from their senior manager with an instruction to deal with this. I explained to my colleague that, as far as I could tell, this Monotype campaign was similar to the domain name scams the procurement team is already familiar with. So please sit tight till our digital team colleagues have completed their audit and then we’ll figure out which one person should start the conversation with Monotype.&lt;/p&gt;
    &lt;p&gt;But, like any successful phishing campaign, the Monotype rep’s LinkedIn messages eventually reached someone who did respond. This was another person in the procurement team and, just to be completely clear, I don’t blame them for responding. They were just doing their job of protecting our business from potential copyright liability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being forced to deal with the issue&lt;/head&gt;
    &lt;p&gt;Since I’d handed this over to the digital team, I hadn’t kept track of how things were progressing. I was brought back into the discussion when our brand manager included me in an email thread between her and the procurement person who’d responded to Monotype.&lt;/p&gt;
    &lt;p&gt;I quickly brought this second procurement person up to speed with our earlier plan of action and then I looped in the digital team again. Turns out the digital team had completed their audit, found that we were in compliance, but had gotten busy with other work so no one had responded to Monotype. *sigh*&lt;/p&gt;
    &lt;p&gt;Now, of course, everyone was on the back foot because our new procurement rep had shared the PDF that Monotype had sent, listing all the places where we were using Monotype fonts without a proper licence.&lt;/p&gt;
    &lt;p&gt;And, to quote from the procurement rep’s email:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Supplier has confirmed two options:&lt;/p&gt;
      &lt;p&gt;Past Use License Agreement is used (PULA) to cover the usage for the period without a license if the software is to be removed.&lt;/p&gt;
      &lt;p&gt;Process the PULA with a Go Forward license agreement to allow compliant continued use of the fonts in use.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Reading that, I got this procurement rep to quickly convene a meeting with everyone involved, though it turned out the person from the digital team who’d done the font audit had gone on annual leave.&lt;/p&gt;
    &lt;p&gt;The rest of the digital team didn’t know anything about font licencing and this was the first time this procurement rep was dealing with font licencing as well. So, partly spurred on by their senior manager’s instruction to deal with this, the procurement rep was seriously considering paying the licencing fee that Monotype had asked for, just to make this whole headache go away.&lt;/p&gt;
    &lt;p&gt;This is where I jumped in and told everyone to hold up. I said I would take the lead on this internally and I would take over the discussion we were having with Monotype as well. The procurement rep, I think somewhat relieved to have this taken out of their hands, agreed.&lt;/p&gt;
    &lt;p&gt;Why did I insist on taking this over? Two reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;I’m a bit of a typography nerd so I know what I’m talking about and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a quick look at the document Monotype had sent over with the list our alleged copyright infringements had told me that everything the Monotype rep was alleging was wrong.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Never send an AI to do a human’s job&lt;/head&gt;
    &lt;p&gt;An April 2025 blog post from Studio Twofold’s Jamie Walker titled ‘Unlicensed Fonts: The Hidden Risk in your Branding’ opens with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lately, we’ve seen a noticeable uptick in copyright issues related to both images and fonts. Several clients have reached out after receiving emails about fonts [an] agency used on their site years ago.&lt;/p&gt;
      &lt;p&gt;Naturally, they’re a bit rattled — and with good reason. These kinds of copyright claims can come with hefty fees if you’re found to be in breach.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Further down Jamie writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Smaller businesses can get caught off guard — and with AI now scanning the web for even the tiniest copyright infringements, it’s more important than ever to stay protected.&lt;/p&gt;
      &lt;p&gt;Think they’re only picking on the small guys? Think again. Even giants have been caught out – proving no one is too big to face the music (or in this case, the typography).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Basically, the reason so many organisations are getting out-of-the-blue copyright claims these days is because major copyright holders have started using automated, AI-powered copyright infringement detection software, of which there is a lot out there. *sigh*&lt;/p&gt;
    &lt;p&gt;Monotype seems to have used one of these products too, because the two fonts its report claimed we were using without a valid licence are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Credit Cards in our iOS and Android apps and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proxima Nova in one of our project websites.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s take those one at a time, shall we?&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s in a name?&lt;/head&gt;
    &lt;p&gt;The first thing I did as I read the report was look up the Credit Cards font on MyFonts.com, Monotype’s online store front.&lt;/p&gt;
    &lt;p&gt;Credit Cards is a pictogram font that contains these icons:&lt;/p&gt;
    &lt;p&gt;When I saw that I thought to myself, “Why would we want to use those icons in our apps?”&lt;/p&gt;
    &lt;p&gt;Next I read Monotype’s report in detail and saw this screenshot. This is from an analysis of our app’s payload and is the apparent proof that Credit Cards is being used in our smartphone apps:&lt;/p&gt;
    &lt;p&gt;Having worked with a great many font files over the years, when I saw that filename I thought to myself, “Are we sure that’s actually the Credit Cards font that Monotype claims it is?”&lt;/p&gt;
    &lt;p&gt;So I checked. I did a web search for fonts with “credit card” in their name and very quickly found one called ‘Credit Card’ – singular – from K-Type. This is a regular text font (as opposed to an icon font) that looks like the raised text that’s printed on credit cards.&lt;/p&gt;
    &lt;p&gt;Now that I was something I could see us using in our apps.&lt;/p&gt;
    &lt;p&gt;Happily, Credit Card is free for personal use so I downloaded it and looked at the zip file. And guess what the filename of the font is?&lt;/p&gt;
    &lt;p&gt;Yup, the filename is CREDC___.ttf – which is exactly the filename that was in the app payload analysis from Monotype.&lt;/p&gt;
    &lt;p&gt;Seeing this, I reached out to my design team contact who then got me in touch with the person who manages our smartphone apps. From them I found out that, sure enough, the only font over and above Open Sans that we use in our smartphone apps is Credit Card by K-Type.&lt;/p&gt;
    &lt;p&gt;SCORE: Ameel 1, Monotype 0&lt;/p&gt;
    &lt;head rend="h3"&gt;Proxima Nova, really?&lt;/head&gt;
    &lt;p&gt;Unlike the Credit Cards font, our use of Mark Simonson’s Proxima Nova was never in contention. We clearly use it in one of our project websites. My employer didn’t actually build that website – we bought this under-construction project from another entity – but this website is very much our responsibility now.&lt;/p&gt;
    &lt;p&gt;The problem for Monotype here was that it no longer sells licences to Proxima Nova. There was a time you could buy a licence to Proxima Nova from Fonts.com, which was Linotype’s online marketplace. But Monotype bought Linotype and eventually killed off Fonts.com and, some time after that (I don’t know when or why), the font’s designer stopped selling licences to Proxima Nova through MyFonts.com.&lt;/p&gt;
    &lt;p&gt;Of course learning this fact didn’t mean that I was just going to stop my investigation. I reached out to a person, who reached out to a person, who reached out to the design agency that designed (and still maintains) our project website. The design agency contact did their own investigation and quickly confirmed that, yes, they do indeed have a licence to use Proxima Nova on this site – one that they had purchased from Adobe several years ago.&lt;/p&gt;
    &lt;p&gt;SCORE: Ameel 2, Monotype 0&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing the book on this whole affair&lt;/head&gt;
    &lt;p&gt;Armed with this knowledge, I got the procurement person to introduce me to the Monotype rep. The rep and one of their colleagues were very eager to talk, replying to this introductory email within fifteen minutes. They wanted to organise a meeting so they could finally get the font licencing agreement signed. Instead what they got was a long email from me in which I explained the situation in detail, complete with annotated screenshots like the one above :)&lt;/p&gt;
    &lt;p&gt;The Monotype rep chewed on this for a few days and then made one final attempt at getting money out of us. They agreed that they were currently unable to sell a licence to Proxima Nova, but it turns out Monotype is one of K-Type’s authorised resellers and [they] “currently cannot see the license on our files for this use” – meaning there wasn’t a record of us purchasing a licence to Credit Card from Monotype. So could we please “confirm if there is one that we for some reason are unable to see in our systems?”.&lt;/p&gt;
    &lt;p&gt;*sigh*&lt;/p&gt;
    &lt;p&gt;I wrote back and told them the reason Monotype doesn’t have a record of this licence is because we purchased a one-off Enterprise Licence directly from K-Type several years ago.&lt;/p&gt;
    &lt;p&gt;This was several weeks ago and I haven’t heard a peep from them since.&lt;/p&gt;
    &lt;p&gt;¯\_(ツ)_/¯&lt;/p&gt;
    &lt;head rend="h2"&gt;Two sides to the story&lt;/head&gt;
    &lt;p&gt;Despite my making light of the situation, I don’t actually hate Monotype for doing this. Using fonts without purchasing a proper license (or purchasing the fonts outright) is stealing and you absolutely should not do it.&lt;/p&gt;
    &lt;p&gt;The fact that font licensing can be a complicated issue is not an excuse; lots of things are complicated and we figure them out.&lt;/p&gt;
    &lt;p&gt;More importantly, I think the type designers and type foundries that create fonts should be fairly compensated for their work. Paying for fonts, or an ongoing licence to those fonts, is how you do that.&lt;/p&gt;
    &lt;p&gt;In short, you should pay for fonts and you should call out people and organisations when they use fonts without a proper licence.&lt;/p&gt;
    &lt;p&gt;(For completeness’ sake, I should say that I also don’t mind that Monotype used automated systems to find copyright violations. The internet is so large that it’s impossible to manually find all the people who have stolen your stuff!)&lt;/p&gt;
    &lt;head rend="h3"&gt;It’s not what you do, it’s how you do it&lt;/head&gt;
    &lt;p&gt;That said, I hate how Monotype’s business development people went about doing this. Much like the blatantly overzealous content blockers on YouTube, the Monotype reps who reached out to us didn’t even bother to verify whether the report their AI spat out at them showed an actual copyright violation or not.&lt;/p&gt;
    &lt;p&gt;I mean, I know why they didn’t double-check. Just like with scammers and phishers, this is a volume game, not an accuracy or fairness game. You bombard people with messages, scaring them with your (potentially unverified) claims, and eventually some of the thousands of people you’ve messaged will reply. You then rush these folks into paying a licence fee because your targets don’t have enough information about font licencing and, frankly, they just want the problem to go away. This is a shitty way to do business and it reflects poorly on your organisation.&lt;/p&gt;
    &lt;head rend="h3"&gt;You’re not making any friends&lt;/head&gt;
    &lt;p&gt;Not that reputation seems to matter too much to popular digital marketplaces – Amazon being the poster child for this. They’re big, they’re arguably enshittified, and all they appear to care about is making as much money as possible.&lt;/p&gt;
    &lt;p&gt;I mean there’s a reason why so many type designers urge people not to licence fonts – even their own fonts – from MyFonts and instead buy or licence fonts directly from designer and type foundry websites. And, barring that, buying or licencing fonts from smaller, independent stores like Fontspring instead.&lt;/p&gt;
    &lt;p&gt;This is also why TypeType and Fontstand offer font subscriptions that are alternatives to those from Adobe Fonts and Monotype.&lt;/p&gt;
    &lt;p&gt;And this is why, for example, the independent video streaming site Nebula was forced to design their custom Nebula Sans font. Nebula’s website and streaming apps used to use the Screen Smart version of the Whitney font from Hoefler&amp;amp;Co. But when Monotype purchased Hoefler&amp;amp;Co, the new Monotype licencing/royalty structure meant that a licence to Whitney was suddenly unaffordable to Nebula. Since paying that (apparently much) higher amount to Monotype wasn’t going to be financially sustainable, Nebula instead paid Paul D. Hunt, the original designer of the excellent Source Sans font, to modify his font so it would be a drop-in replacement for Whitney in all of Nebula’s digital products. And because Source Sans was released under the SIL Open Font License, Nebula also released Nebula Sans under this OFL.&lt;/p&gt;
    &lt;p&gt;If that’s how much effort folks are willing to make to to avoid using your company, you probably already know that you’re not very well loved.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do do?&lt;/head&gt;
    &lt;p&gt;So what’s my take-away from all this?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Don’t use scammy tactics to scare people into purchasing your shit. And if you are going to use those tactics, at least don’t be wrong about it!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If anything like this happens to you or your employer, find the relevant nerd in your friend group or organisation and ask for their help.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you can, avoid licencing fonts from Monotype. Get your fonts directly from the original designers and type foundries, or maybe from smaller, independent marketplaces like Fontspring (which has its big annual “cyber sale” on till 4 December, by the way).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you’d rather avoid the hassle of font licencing altogether, then do what my employer did and pick an excellent, versatile OFL font and use that instead. Though, if you want to stand out from the crowd, please consider avoiding the most popular fonts on Google Fonts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh, and while I’m far from an expert on typography, if you need a hand with anything font-related, please reach out. I’d be happy to help in any way that I can :)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45973261</guid><pubDate>Tue, 18 Nov 2025 22:42:45 +0000</pubDate></item><item><title>Rebecca Heineman – from homelessness to porting Doom (2022)</title><link>https://corecursive.com/doomed-to-fail-with-burger-becky/</link><description>&lt;doc fingerprint="2f43d15529a6bff9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Adam: Hi. This is CoRecursive, and I’m Adam Gordon Bell. Each episode is the story of a piece of software being built. Why don’t you tell me what you do and who you are?&lt;/p&gt;
    &lt;p&gt;Becky: Okay. Yeah. I’m Rebecca Ann Heineman. I’ve been in the video game industry since the beginning of time. I started by winning the Atari 2600 Space Invaders tournament in November of 1980. Since then, went to work at companies like Avalon Hill, and then Boone Corporation until Boone imploded, and then we formed Interplay out of the ashes of Boone.&lt;/p&gt;
    &lt;p&gt;Adam: Becky’s a bit famous, she’s worked on hundreds of games. And her specialty was porting games from one platform to another. Today is about a game port that went sideways, porting a little game called Doom, and the struggles involved.&lt;/p&gt;
    &lt;p&gt;Becky: And I said, “Okay, see this golden master I’ve got right here? I just encrypted, ready to go for duplication. Where’s my other $20,000?” “Where’s the new weapons?” It’s like, “Didn’t have enough space, didn’t have enough time.” And he’s like, “Oh, but I promised people knew weapons.” Like, “Can’t do it. Don’t have time.” “But I gave you JPEGs.” “I put them in the trash. Sorry.” He was mad. Oh gosh, he was mad.&lt;/p&gt;
    &lt;head rend="h2"&gt;Childhood&lt;/head&gt;
    &lt;p&gt;Adam: So that’s today’s story. The story of Burger Becky porting Doom to the 3DO. It’s kind of a retro software story, but it’s also a story about project management and software development going a little bit sideways. There’s engines to tweak, there’s deadlines to hit, hardware acceleration to get working. And dramatic rock anthems to record.&lt;/p&gt;
    &lt;p&gt;The path that led Burger Becky into video game development, which he’s still doing today, it started back when she was a young teen. This was the late 70s, the Atari 2600 had recently come out, and so had the Apple II. And Becky’s childhood set her on a path that eventually led her right towards that port. But even so, she wouldn’t wish her early years on anyone.&lt;/p&gt;
    &lt;p&gt;Becky: If I was in my 20s, I would basically not even bring it up. In my 30s, I would probably burst into tears halfway through any of this. By the time I was in my 40s, my parents have already passed away. I’m like, okay, the people who caused this are gone. And now I could finally set myself free and no longer have to have nightmares of my dad bursting in the door to beat the crap out of me because he was drunk again. Which is one of the reasons why I don’t smoke or drink, because it reminds me of my shitty parents.&lt;/p&gt;
    &lt;p&gt;Adam: Becky normally doesn’t talk about this stuff because, well, the specifics are hard to hear.&lt;/p&gt;
    &lt;p&gt;Becky: My mom and dad married because my mom was already pregnant, and they couldn’t have a child out of wedlock. And my dad was not my dad, it was someone else. Then in time, they had two more children, two sons. I was the eldest. My dad would favor my two brothers, and he would treat them like princes. Whereas, I was being treated as the help. And so I was always having to clean the dishes, do this. And every time my dad got mad, which every other day, I would be the one he would beat the crap out of. And then when we had dinner with the family, I was the one serving the meals while everybody else was eating around there. And of course I didn’t get a plate. And then by the time it was all done, I would eat the leftovers. And of course if they saw me eating the leftovers, I would get punished.&lt;/p&gt;
    &lt;p&gt;Adam: As an adult, she’s never totally gotten past this food insecurity. Coworkers would spread rumors about her hiding burgers in her desks to nibble on when no one was looking.&lt;/p&gt;
    &lt;p&gt;Becky: Honestly, there’s truth to that. I was not picky about the condition of the food I was eating because I was conditioned from my earliest memory to the time I got out of that hell hole that food is life. Don’t be picky about it, because otherwise starving is your only other option.&lt;/p&gt;
    &lt;p&gt;Adam: Her nickname came from her food insecurities and from a rough childhood. And eventually it was all too much for her.&lt;/p&gt;
    &lt;head rend="h2"&gt;Homeless&lt;/head&gt;
    &lt;p&gt;Becky: Somewhere when I was 15-ish I just had it. And I went to school and I just didn’t go home. And to put it in perspective, my family never sent out a missing persons report, or anything like that. That’s how much they cared. I then lived behind a dumpster of a store, I think was called Market Basket at the time. But I just lived either in the dumpster when it was raining or behind it, because they had blankets and stuff like that.&lt;/p&gt;
    &lt;p&gt;Adam: Becky got a job at JC Penny.&lt;/p&gt;
    &lt;p&gt;Becky: I just went in there and just said, “Hey, I’m 18.” And they didn’t bother checking anything, because back then we weren’t so anal about immigration status and shit like that. Then Christmas was done, then they had me transfer to their warehouse, but I was just in there moving boxes around, moving shit around. And that was it until finally I ended up at a video game contest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Atari Space Invaders&lt;/head&gt;
    &lt;p&gt;Adam: The video game contest ends up being her entry into the industry, and it went like this. Space Invaders was pretty big at the time, and Atari decided as a publicity stunt to have a national tournament. It was the first eSport tournament in North America.&lt;/p&gt;
    &lt;p&gt;Becky: They had people come into five different cities. It was San Jose, Los Angeles, Dallas, Chicago, and New York. And then each of the winners were then flown to New York City, and then that contest was televised.&lt;/p&gt;
    &lt;p&gt;Adam: Becky’s friend convinced her to compete in the LA contest, and she won, and was flown off to New York for the championship.&lt;/p&gt;
    &lt;p&gt;Becky: It was more terrifying than exciting because I didn’t really understand what the heck was going on, I was just a kid. And I was there without my parents. They didn’t factor in that kids would be winning these contests, so they lucked out in which the parents of the other four contestants actually paid their own way and came to the nationals. I went by myself to which Atari, to their horror was like, “Where’s your parents?” “You only gave me one ticket.” There was a comedy of errors, but it was back to the fact that it was never been done before. And hence, when it was the first contest, yeah, you can imagine some growing pains.&lt;/p&gt;
    &lt;p&gt;Adam: All of a sudden they have a moment where they’re like, wait, are we just paying to fly minors across state lines without…&lt;/p&gt;
    &lt;p&gt;Becky: Oh, I’m certain their liability attorneys were just crapping their pants in the other room going like, oh, please don’t fall down the stairs. Please don’t stub her toe. Please don’t get food poisoning. Please don’t get in an accident, the liability, because we are taking care of a minor.&lt;/p&gt;
    &lt;p&gt;Adam: Becky won, and she became the first championship competitive gamer in the US.&lt;/p&gt;
    &lt;p&gt;Becky: They had people from the news all behind us, like 200 people from different newspapers. And CNN, which was just brand new. I think CNN was only three months old at the time. And then ABC, NBC, CBS, et cetera. And of course they recorded it. And then once the contest was over, then I had microphones shoved in my face and they interviewed me, and then sent it off.&lt;/p&gt;
    &lt;head rend="h2"&gt;Arcade Repair&lt;/head&gt;
    &lt;p&gt;Adam: Back at home, after the tournament, Becky’s a bit famous and she gets some opportunities thrown in her lap. She gets a job at an arcade where she’s making change and repairing machines because they thought it would be pretty cool to be able to say they had the national champ there.&lt;/p&gt;
    &lt;p&gt;Becky: But then I found I had a talent for fixing video games, and I met a friend there who taught me the fine art of video game logic board repair. Which then, now that’s why I learned all my TTL logic and stuff like that, which helped me design boards that would allow me to copy Atari 2600 cartridges. But yeah, I was homeless not that long. It was maybe three or four months, because every now and then I would go back to my old house. But then one day I went there and I saw a moving van. And, that’s weird. And then I saw my mom and she was like, “Oh, where have you been?” It’s like, “Been away.” And she’s like, “Well, we’re getting a divorce. I’m moving to this place.” And says, “Oh, so dad’s not going to be with you.” It’s like, “No, he’s staying here.” “Okay. Then, can I move in with you?” And at that point, moved back in. And that was my home base until I moved to Towson, Maryland.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Apple II and The 2600&lt;/head&gt;
    &lt;p&gt;Adam: At home with her mom, Becky had an Apple II she was tinkering around with, and she kept working at the arcade repairing video game boards. And this gave her some familiarity with game hardware. And meanwhile, the video game industry is exploding.&lt;/p&gt;
    &lt;p&gt;Becky: The Quaker Oats company formed a video game company. Milton Bradley had game company. 20th Century Fox made a game company. Of course, you know the big players, Imagic and Activision. And even Mattel making Atari cartridges. It’s like, this is also at a time when every video game console, the Atari, the Intellivision, and the Coleco, they were closely guarding how their machines worked because they did not license their technology to anyone because it had not been heard of. And you may remember when a bunch of Atari employees left to form Activision, Atari immediately sued them. And the grounds was, you can’t make cartridges for our player, the Atari 2600, because it’s covered under patent, covered under all this other copyright and other gobbledygook.&lt;/p&gt;
    &lt;p&gt;Adam: But the lawsuit failed, and the market for third party games opened up on the 2600. If you could hire an Atari game maker, you could boot up a game company and make a bunch of money. It was a bubble. And while all this was happening, Becky was using her arcade repair knowledge to pirate Atari cartridges. And then she starts figuring out how video game software works through an odd coincidence.&lt;/p&gt;
    &lt;head rend="h2"&gt;The 6502&lt;/head&gt;
    &lt;p&gt;Becky: The Apple II uses a 6502 processor. The Atari 2600 uses a 6502 processor. Now, in the Apple II, if you put data in your memory, go into what’s called the monitor, which allows you to disassemble 6502 code. I was constantly looking at the ROMs of the Apple II, and disassemble it and saw all this instructions, and figured out how the instructions worked. And so I can write my own little Apple II programs. Well, when I was piloting Atari 2600 cartridges, I put them in memory. I said, let’s take a look what’s in there. And I saw code, and it all made sense. It was 6502.&lt;/p&gt;
    &lt;p&gt;So I was disassembling I was like, what does this code do? What does this do? Then I would, let’s say, okay, it loaded value in this register. Let’s change that value, upload the game into an Atari 2600, see what happens. And eventually I found, oh, this register changes the background color. This changes the left Sprite, this changes the right Sprite. And then just through iteration, iteration, iteration, trial and error, I was able to figure out how to generate an Atari 2600 screen. And of course, yeah, there was a couple of registers I didn’t know what they did, but over time I figured them all out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Avalon Games&lt;/head&gt;
    &lt;p&gt;Adam: She mentions this, that she’s creating her own cartridges to someone who works in the industry.&lt;/p&gt;
    &lt;p&gt;Becky: And he’s like, “You reversed the Atari 2600?” Says, “Yeah, I did.” And he goes, “I’ve got a company to talk to.” And he introduced me to the owners of a company called the Avalon Hill Game Company, who called me up. And I told him, “Yeah, I know how to program the 2600. I got demo carts and everything.”&lt;/p&gt;
    &lt;p&gt;Adam: Avalon Games was a maker of board games, and they had produced some computer games as well. But they were small players, they couldn’t afford to get some Atari employee to work there and port their games to cartridges. But now they didn’t have to, they had their own cartridge expert in teenage Becky.&lt;/p&gt;
    &lt;p&gt;Becky: They hired me right there on the phone. And then a couple of days later I had a plane ticket and everything, and I relocated to Towson, Maryland from Whittier, California. And that was my first paid programming job. And I had never had any training whatsoever, it was just me learning on my own.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Port Master&lt;/head&gt;
    &lt;p&gt;Adam: And so Becky’s career took off from there, she became the go-to person for porting games. And eventually masters not just the Apple II, but the 2600 and the Commodore, and PCs, and much more. She was on the founding team of the company Interplay, and was their go-to person for ports and everything Apple. And then a new console came out.&lt;/p&gt;
    &lt;p&gt;Becky: This was around 1994, and I’d already gotten a reputation at Interplay that I can assimilate a new console or a new platform within weeks, and already start generating code on it. Everything from the Super Nintendo, the 8 bit Nintendo, the Game Boy. And of course, I already had an affection for Macintosh’s, because I had written so many games for Mac Play. But that aside, what happened was that a new console was coming out called the 3DO.&lt;/p&gt;
    &lt;p&gt;Adam: The 3DO was a gaming console proposed by EA founder Trip Hawkins, and produced by Panasonic among others. It was competing with the Atari Jaguar and the Sega Saturn. But it was the one with all the buzz. It had such a huge buzz that Time Magazine called it 1993’s Product of the Year.&lt;/p&gt;
    &lt;p&gt;Becky: Since it acquired a Mac for development, I was of course the logical choice. So I got a 3DO dev kit, got the software, and started just testing stuff to see what in the world I could do on this new console.&lt;/p&gt;
    &lt;p&gt;Adam: What does the dev kit look like? How does it work? What am I seeing?&lt;/p&gt;
    &lt;head rend="h2"&gt;3DO Dev Kit&lt;/head&gt;
    &lt;p&gt;Becky: The dev kit looks like a big PC. It literally is in a PC case, vertical orientation, popular among the 90s PCs. The only thing that you’ll notice is different is that in the front just below where the CD is, is a little panel which just says Reset, Debug, and has a 3DO logo on it, and a little switch that says Encrypted and Not. And the whole idea behind that is if you turn on encrypted, then you could put encrypted retail discs inside of it and run the game. But if you turn non-encrypted, then you don’t need to be encrypted. And then you could also use the debugging feature. There was a NuBus card made by 3DO that plugs into your 68K Mac on a NuBus card. And then you had a fiber optic cable, which goes from the 3DO into the Macintosh.&lt;/p&gt;
    &lt;p&gt;Adam: You did the development for the 3DO using your Mac over that cable using MPW, the Macintosh programmer’s workshop.&lt;/p&gt;
    &lt;p&gt;Becky: It essentially is their idea of an IDE/command line. In which you have text editors, you have a command line, and you could go ahead and issue commands and so forth, but it’s all hybrided. You could be in the middle of editing text, but if you hit command return or something like that, that line would execute it as a command line. It’s kind of weird. But I think VS code is now doing something like this these days. But what goes on is that you have a bunch of command lines, much like you do with a modern operating system, where you have a shell and a bunch of tools and so forth. Except of course the tools here you have is the Norcroft C compiler, which just compiles into ARM. Because it was like, as far as I know, the 3DO was the first console that used an ARM CPU.&lt;/p&gt;
    &lt;p&gt;Adam: So you wrote C code, compiled it on the 3DO operating system, and then you generate art using Electronic Arts 3D Studio.&lt;/p&gt;
    &lt;p&gt;Becky: And they had plugins specifically for 3DO. So you draw some art, when it’s all done, the plugin would convert it to 3DO cell format, which is the hardware blitters format.&lt;/p&gt;
    &lt;p&gt;Adam: A blitter is a graphics co-processor. It’s sort of an early version of a GPU.&lt;/p&gt;
    &lt;p&gt;Becky: And then you just take these files and put them in your game, and you’d run your game.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wolfenstien for the 3DO&lt;/head&gt;
    &lt;p&gt;Adam: The first game Becky ported to the 3DO was a personal favorite of mine, it was Wolfenstein.&lt;/p&gt;
    &lt;p&gt;Becky: And I just simply took the source code I had for Wolfenstein. At the time, it was the Mac. Put that here, built it for 3DO. Made a 3DO shell, added a console-like control panel. Because the Macintosh used a mouse and keyboard. The 3DO uses exclusively the console controller. Wrote that all together, put it out there, and the game shift. And then, yeah, it was very easy. And the 3DO really worked well with what Wolfenstein 3D wanted. The game ran at 30 frames per second, and it was perfect for the platform. And then I did games like Battle Chess, Siberia, Casper, Shadowland, Kingdom of Far Reaches. There’s games I’m even forgetting that I did on the 3DO. Out of This World, that’s another one. The 3DO version of Out of This World, we took all the backgrounds, sent them to an animation studio who does backgrounds for cartoons. And then they drew paintings of each background. We then scanned them, and then that is what was used in the video game.&lt;/p&gt;
    &lt;p&gt;Adam: In an upcoming Supporters episode, Becky’s going to go deep on this Out of the World port. It’s a wild story. But yeah, Becky becomes the 3DO expert, and so more of this work keeps coming her way.&lt;/p&gt;
    &lt;head rend="h2"&gt;The DOOM Project&lt;/head&gt;
    &lt;p&gt;Becky: The guys at 3DO were very impressed with the port, because it was something that was really showcasing the 3DO. We sold a lot of copies on that platform. Well, then they told me that, “Hey, you’re still doing in software conversions?” “Yeah, I still do that.” They said they had a company that has the rights to Doom for the 3DO. The problem was that they were having a hard time getting a copy of this game from 3DO, despite the fact that they’ve been telling people that it’s 90% complete. And that it’s going to have new weapons, new stuff, whatever. And the latest excuse they’re getting from them is that the game is 90% complete, but their developer flaked on them. So now they need someone to help them finish the game. They said, “Can you handle this?” And I go, “Yeah, if the game’s 90% complete, no problem.”&lt;/p&gt;
    &lt;p&gt;Adam: This seems like it would be a pretty sweet gig. The game had to be ready for Christmas, but it’s August and there’s only 10% left. Plus then Becky would be able to say she worked on the Doom port. It seems like a great deal. The thing was, Becky had been around in the industry and she knew a lot of the players, but she had never heard of Art Data. That actually wasn’t that surprising, considering what was going on in the industry at that time. The thing was, Becky knew a lot of the players in the industry, and she had never heard of Art Data. But that actually wasn’t that surprising considering what was going on at the industry at that time.&lt;/p&gt;
    &lt;head rend="h2"&gt;CD Multimedia Hype&lt;/head&gt;
    &lt;p&gt;Becky: 3DO was so hyped as they’re becoming the next wave of multimedia. Because in addition to being a new video game console, they were bragging about how the 3DO could play movie DVDs. It was actually not a DVD, it was a CD format, but it was called a CDV or something like that. So you could buy movies, put them on the 3DO and watch a movie on it. They had the idea that was going to be streaming, they were going to have some sort of add-on that allows you to connect to your cable TV box. There was going to be an add-on lets you have ethernet so you can go ahead and get on the web. So it was going to be like this all-in-one set-top box. That in addition to playing games, it was supposed to be the center of your entertainment center. And that the Trip Hawkins behind it, they were already putting in millions of dollars and Electronic Arts was devoting a large percentage of their game development into 3DO titles.&lt;/p&gt;
    &lt;p&gt;That’s why 3DO titles, there’s so many EA games. And then of course you had Samsung, Panasonic, et cetera, all making 3DO consoles. And there was plans that you’re going to buy a TV that had a 3DO built-in. And then eventually everything was going to have 3DO in it. So with that kind of hype, a lot of companies started coming out of the woodwork and say, “We got to get into CD multimedia.” And much like today where you say, “We’re based on the blockchain,” that’s all an investor has to know in order to start dumping money in your company. Despite the fact that you’re going to say, “Well, what we really do is we milk cows, but we’re based on the blockchain.” But it shows you exactly how people who just love buzzwords jump into the market. Well, that’s what happened with the CEO of Art Data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Art Data&lt;/head&gt;
    &lt;p&gt;Becky: He was a guy who lived up in Simi Valley, or something like that. He somehow convinced his friends and congregants at a church, and they gave him something like $250,000 or some God awful lot of money to form Art Data. And the first game they did was called Chess Wars, a Medieval fantasy which is essentially a ripoff of Battle Chess, but done very poorly. And then they licensed the game, which became Rise of the Robots, I believe. But then later on, the word got out that the rights to Doom for 3DO were up for grabs. But since ID Software wasn’t really that enthused about the 3DO platform, they just put an insane price. It was like $250,000.&lt;/p&gt;
    &lt;p&gt;Which in that year, $250,000 for the rights to Doom was a lot of money. And the 3DO wasn’t really selling that well. So to sell enough copies to recoup that amount of money, you’d have to either have the best version of Doom ever, or you just simply want Doom in your catalog as a loss leader and are trying to push for other titles. But Randy believed that if he got Doom, he would put his company on the map, and then he was going to have this game with new levels, new maps, new stuff like that.&lt;/p&gt;
    &lt;p&gt;Adam: So Randy raised the money, and Art Data bought the rights to Doom on the 3DO. And Doom at this time was huge. People were dragging their computers to each other’s houses to play Doom multiplayer. It was the birth of LAN parties. And Randy knew a good press opportunity when he saw one, so he started doing interviews.&lt;/p&gt;
    &lt;p&gt;Becky: There are videos and magazine articles of him, mostly in Japanese magazines, of him showing off mock-up screens with new weapons and new monsters and things like that. And then somewhere around April, there’s this video I saw him in April of ‘95 of him holding a CD that says Doom 3DO on it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Development Plan&lt;/head&gt;
    &lt;p&gt;Adam: That Doom that’s on that CD in the photo shoot is what Becky’s going to polish up for release. And she’s been doing this for a while. She’s worked with the Doom code base before, so she immediately knows how she would tackle a port. The trick is to get the game engine simplified so it can run on the lower memory constraints of a console, and then optimizing it to get a good frame rate because resources are more constrained.&lt;/p&gt;
    &lt;p&gt;Becky: I would’ve allocated at least three people and at least six months to maybe nine months to do the actual port. One person writing, there’s nothing but a brand new game engine designed to run a Doom-like game on the 3DO. And the walls would be the perfect 3D walls, because that’s what I did with Wolfenstein. It was perfect 3D. But the floor and ceilings would be sliding textures and stuff like that with some error correction to get it to work somewhat nicely on the 3DO. But then we would actually do some new weapons, new players. We would plan them out because we’d actually have to make an editor, taking the Jaguar levels, planting new objects in them, and probably even making a few more new levels specifically to highlight the new weapons and add some more storyline or something like that.&lt;/p&gt;
    &lt;p&gt;And lastly, I would’ve had to hire a musician to do a kicking soundtrack, because that was something that I really would’ve wanted to do on the 3DO no matter what happened. But it would’ve come out in which the game would’ve been running at 30 frames per second. And with that, we would’ve shipped a pretty decent game, and it probably would’ve sold 20,000 units.&lt;/p&gt;
    &lt;head rend="h2"&gt;90% Complete&lt;/head&gt;
    &lt;p&gt;Adam: Becky assumes the Art Data people are doing something similar to her plan, and she’s pretty excited to help them finish. And also help the 3DO people, because they’ve been pumping up this game as well. So she reaches out to Art Data.&lt;/p&gt;
    &lt;p&gt;Becky: They kept with the story. They said the game was 90% done. They needed me to add in a couple of new weapons and a couple of new maps. So I asked him, “Great, do you have the maps?” He says, “Yes, we do.” “Do you have the new weapons?” “Yes we do.” “Can I have the source code?” “Eventually. Once you sign.” I was like, “But I need to do a code review before I sign.” And they’re like, “Well, it’s 90% done. You don’t need to really do that.” I should have taken that red flag, but I didn’t, because I was excited to bring Doom to the 3DO. And if the game was already 90% complete, then yeah, I could put my credit on there even though I only worked on 10% of it. We negotiated a price for doing the project and then I said, “Okay, let’s get going.”&lt;/p&gt;
    &lt;p&gt;Adam: How much were they going to pay you?&lt;/p&gt;
    &lt;p&gt;Becky: They were going to pay me $10,000 to do the conversion at 90%, just to finish the game. Which is fine, $10,000 to finish a game, that’s reasonable. Well then I then waited, and waited, and waited. And said, “Where is the source code?” So I said, “Oh, okay, I thought you already had it.” It’s like, “No, you’re supposed to give me the game. You say it’s 90% complete.” So they gave me a copy of Doom two on a CD. It was a copy of the game for the PC.&lt;/p&gt;
    &lt;p&gt;Adam: They gave her a retail version of the game for the PC, a CD-ROM. The compiled final version of the game, compiled for a completely different platform with a different architecture, but it’s on a CD and the 3DO has a CD drive.&lt;/p&gt;
    &lt;p&gt;Becky: Said it runs on the PC, so we just say that’s 90% complete. And I’m like, “That’s not how this works.” So I then ask, “Who is the developer that you said flaked on you?” And they gave me radio silence. “Oh, the developer doesn’t want to talk to us. The developer can’t be reached because we owe them money.” “You owe them money? Okay. At least give me the name of the developer so I will speak to them directly.” Chirp, chirp, chirp.&lt;/p&gt;
    &lt;head rend="h2"&gt;Payment Issue&lt;/head&gt;
    &lt;p&gt;Adam: Okay, so maybe it’s just a payment issue. Not great, but maybe they didn’t pay the developer and so he didn’t hand over the new version of the game that has the new engine that works on the 3DO and the new levels and the new maps and all that. Maybe Becky can talk to them, get them paid, get the game, and start working on that final 10% of optimization and polish. Get the frame rate up and so on. But nope, that wasn’t the case.&lt;/p&gt;
    &lt;p&gt;Becky: An employee at Art Data came to me privately and said, “We don’t have anything. There is no developer, there’s nothing.” At this point I’m in a panic and I then call 3DO. It’s like, “Project canceled. We’re done. We’re not doing this because the tenants of the contract is that 90% complete. Get the source code, finish it. If you want me to do a full port, we’re going to talk something different.”&lt;/p&gt;
    &lt;p&gt;Adam: That doesn’t sound good to 3DO though. They want to pump up this release. They thought Doom could sell some consoles. And now the deadline to get something in stores for Christmas is getting closer. There’s only 10 weeks of development left if the game needs to be handed off to 3DO in time to get in shelves for Christmas. But 3DO and Art Data aren’t the only parties here. Remember, ID Software had licensed the game to Art Data for the 3DO, and they were probably pretty motivated to keep their big check.&lt;/p&gt;
    &lt;p&gt;Becky: So then I contacted ID Software, and after a small discussion with John, he then suggested, “Why don’t we just use the Jaguar version and base it off that?” Says, “Okay.” So he send me a CD with the complete assets of everything for the Jaguar version of Doom, which was perfect. The levels are already made for low memory, the code’s been simplified, networking’s been taken out, all this stuff going on. A lot of the work that I would’ve need to have done has already been done in the Jaguar version. So I then told 3DO, “Okay, let’s go get with Art Data and I’m just going to do a port, and it’s going to cost you $40,000 now.” So at that point, Art Data reluctantly said yes and says, “Great.” “I need to check for 20,000 right now. I’m not starting until I see $20,000.” So I got a check.&lt;/p&gt;
    &lt;p&gt;So okay, let’s go. Started the work. Unfortunately, the CEO of Art Data, who honestly believed that because the game was on a CD and the 3DO has a CD player, that the game will run on the 3DO because it’s a CD. That was his interpretation of 90% complete, because the game already runs. So when I realized that I was working with Einstein here, I decided, okay, just ignore him. I’m going to focus it. Because after working on this for a little bit and having him incessantly ask me for progress on stuff because he wanted to do demos and press tours, and I’m like, “I haven’t even gotten the graphics up for this game on the 3DO yet, and you’re asking me for this crap?”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Band&lt;/head&gt;
    &lt;p&gt;Becky: Well, the CEO guy at Art Data was bragging about all those things that he did, and he also bragged he had a band. Oh really? I had a cassette made of all the PC music, just had the PC music, started the game, let the music play into a cassette. Went to another level, did it again, did another level, did it again. I think it was, we gave him six or seven cassettes. Each one had three songs on it or something like that. Because after we record three songs, we give them the tape, another three songs, give them the tape, and so forth. I said, “Okay, take this cassette. It’s got an hour’s worth of music on it. Do covers.” Because once you gave him a tape he was off to the races. And then he went ahead and made these really kicking music tracks, which I then converted to 3DO format and put them in the game. And it gave him a feeling of he’s contributing to Doom, which technically he was, but it got him off my back.&lt;/p&gt;
    &lt;p&gt;Adam: What I picture is that Randy was in a church band at the same church he raised his money at, or maybe he had a old high school band. And he called them up and he put the band to work doing covers of these Doom midi tracks.&lt;/p&gt;
    &lt;p&gt;I’m not sure what actually happened, but one thing’s for sure, the music rocks. It’s probably going to be the best thing about this game. That music, it could be played because the 3DO with its CD format had more space for rich assets. It was easy to include and play high quality music and even videos, which means that if you wanted to make Doom the definitive game for the 3DO, you needed not just rocking music, but you needed live action.&lt;/p&gt;
    &lt;p&gt;Becky: He didn’t even notify me of it, he just did it. What had happened was that I told you about the chess game called Chess Wars. He hired an actual film crew with stunts, everything. So it was expensive. And they filmed all these scenes. Well, he then asked some of the people, says, “Hey, could you also do some scenes for a game based on Doom?” And they’re like, “Cool. Doom. We like Doom.” I understand that the director had an idea for a film in which the scenes would be shot in a way that it would actually tell a cohesive story.&lt;/p&gt;
    &lt;p&gt;Adam: But that didn’t happen, it was all a last minute scramble. But depending on how you feel about full motion video inside of video games, what they ended up with was pretty cool looking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Live Action Doom&lt;/head&gt;
    &lt;p&gt;Becky: So a Hollywood costume designer had an ogre outfit for some other event, some other film shoot or whatever it was. So from the neck down it was just literally an ogre, but from the head up, it’s more like an orc from Lord of the Rings. That wasn’t going to do, so they custom made this demon bull head, which is modeled after the Baron of Hell. And the guy says, “Yeah, it would’ve been a $10,000 job had I actually charged him for it.” Now of course this is 1994 dollars. But it was awesome. The head looked great.&lt;/p&gt;
    &lt;p&gt;And then they put that on. The guy who actually designed the costume actually put on the suit and everything. And then they had a couple of people dress up in Marine uniforms. They have a man and a woman dress up as Marines, they have prop guns, and they just did a bunch of poses. And then they took an ice cooler and put a red cross on it to mimic the health pack. They put it there and they just did a bunch of poses in front of her pictures. But then they did a bunch of scenes in which the woman and the guy would say some lines and then the monster would roar in. The monster would hold this guy… Like that.&lt;/p&gt;
    &lt;p&gt;Adam: There was originally a plan for how the scenes would fit together with the game.&lt;/p&gt;
    &lt;p&gt;Becky: The soldier’s going to run off away from his people and then he is going to open a door and then it’s going to be E1M1. And you start the game. And then after you get to the end of the level and you pull the switch, there’s going to be a cut scene of the soldier pulling the switch and then movie, movie, movie. Then it’ll introduce him going to the next level, which of course is reasonable. That’s actually thought out. But to do that you would have to first go through all the levels we had, which he had no knowledge because he didn’t know we switched from the PC levels to the Jaguar levels, which are different levels.&lt;/p&gt;
    &lt;p&gt;And secondly, is that’s a lot of footage. We’re talking about at least maybe 45 minutes to a full hour worth of footage. And on a CD, that’s pushing it on what we can fit with all the music and all the other stuff in there too. Because you got to put the game in there too. It’s doable. It is doable. But the amount of time to actually script, build sets, film, add visual effects, you’re talking at least for about an hour of footage of that type, it’d be about a million dollars. And our data was squawking about 20K.&lt;/p&gt;
    &lt;p&gt;Adam: Of course, Becky was oblivious to all this at the time because she was just heads down trying to get the game port working.&lt;/p&gt;
    &lt;p&gt;Becky: And of course there was a day, I do remember that Randy came by with me with a disc saying, “Here are all the cut scenes.” I’m like, “I can’t use any cut scene.” So I didn’t even take the disc from him. Now I look back and I regret it because those would’ve been great on YouTube right now. I have no idea where that disc is. And I’ve been asking around, it’s been almost 25 years since that. And to this date, I haven’t found anybody who would fess up to having the actual raw footage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Christmas Deadline&lt;/head&gt;
    &lt;p&gt;Adam: Also, there was a deadline, right? Remember this game was supposed to be ready for Christmas. There was PR events and interviews where the game was being shown on a CD. Expectations were high.&lt;/p&gt;
    &lt;p&gt;Becky: But there was only so much I could do without actually rewriting extensive parts of the Doom engine, which was beyond the amount of time. Because 3DO made it very, very clear they needed this game in the stores by Christmas because they wanted to do an ad campaign saying that Doom was on the 3DO. Saying the 3DO is so powerful, it runs Doom.&lt;/p&gt;
    &lt;p&gt;Adam: But here’s the thing, the 3DO has a 12 megahertz processor. It really can’t run the Jaguar version of Doom without extensive work. The Jaguar had three CPUs, two of which ran at 25 megahertz, twice the speed.&lt;/p&gt;
    &lt;p&gt;Becky: So after all this stuff, it turned out I just lived at the office, worked like a dog. Because halfway through the development process I’m like, how in the hell am I going to finish this? Because the game’s going to be buggy, the game’s going to have issues. I already knew the framer, it’s going to be probably laggy. And John’s going to hate my guts over at ID. Because he was still very touchy about the quality of Doom.&lt;/p&gt;
    &lt;p&gt;Adam: But Becky gets Doom up and actually running on the 3DO, and it’s a small miracle, but it only runs at three frames per second. For comparison, the Wolfenstein port she did earlier was running on the same 3DO software at 30 frames per second, 10 times as fast. Three frames per second is totally unplayable. So Becky phones John Carmack, and instead of being unhappy with her, he suggests just making the screen smaller. So now there’s less to draw. That helps. And then with some more optimizing, she gets the frame rate up to 10 frames per second, which is a bit choppy, but playable.&lt;/p&gt;
    &lt;p&gt;Becky: It shows that it was a 10 week port under extreme circumstances. Because while the game plays from beginning to end bug free, which is a miracle in itself, it only runs in a small window because I still had to render some of the walls, like the floor and ceiling still had to render in software. I couldn’t use the hardware blitter because I didn’t have time to make the code changes for that. But it ran, it was Doom. Doom on 3DO. But it had a kicking soundtrack.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Confrontation&lt;/head&gt;
    &lt;p&gt;Adam: The problem with working with someone who doesn’t understand the industry is that they don’t understand when you’ve pulled off a miracle. Without the Jaguar version, without Becky working day and night, they wouldn’t have had anything. But Randy didn’t see it that way. He thought building games was a simpler proposition.&lt;/p&gt;
    &lt;p&gt;Becky: “Well, where’s all the new weapons, and where’s the new maps as well? Look at the maps, they’re different.” It’s true, because the maps that were used in the Jaguar version are different from the PC version. “Where’s the new weapons?” Nevermind you have to place the new weapon in spawn points around the map and then also code in the weapon’s usage, what’s firing, and it’s effects. No, no, no, let’s not factor any of that in. All you had to do is draw a JPEG of a new weapon at the bottom of the screen and just put it into the directory of Doom and it automatically gives you the new weapon. Just put the JPEG in, it’s magic. It’s like, didn’t have enough space, didn’t have enough time.&lt;/p&gt;
    &lt;p&gt;Adam: By this point, Becky had enough. She hadn’t handed over the completed game, and so she had him at her mercy. She demanded the rest of the money.&lt;/p&gt;
    &lt;p&gt;Becky: He was mad. Oh gosh, he was mad. But I got the check. And after that gave the disc. And I think we did one revision after that. But then that was the revision that actually went out to the stores and was sold.&lt;/p&gt;
    &lt;p&gt;Adam: Now Becky wasn’t exactly happy with the end result of the game, but the game was bug free and could be played, which left her with one thought.&lt;/p&gt;
    &lt;p&gt;Becky: Mission accomplished. Not necessarily a game I’m going to be proud of as a game, but the very fact that I was able to deliver a title at that kind of insane deadline said I did it. I still got it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Business of Gaming&lt;/head&gt;
    &lt;p&gt;Adam: That’s awesome. You got it out. Did Art Data make their money back do you think?&lt;/p&gt;
    &lt;p&gt;Becky: Oh, hell no. He made the next mistake. After the game was golden mastered, he then went and had 50,000 copies pressed because he thought the game was going to just sell because it’s Doom. It’s the adage of, hey, if you put Harry Potter on a box and put cow poop in it, it’s going to sell. So he thought that. In which, hey, the game is Doom. It plays from start to finish. It is actually a decent version of the game for the 3DO, and it’s got a kicking soundtrack. But Doom was not a game that everybody was going to buy. So he had 50,000 units pressed, which meant that he had to give a check for about $300,000 to 3DO. So he was owing that much money. And then he sold maybe about, I don’t know, 3,000, 4,000 copies.&lt;/p&gt;
    &lt;p&gt;Adam: Oh, wow.&lt;/p&gt;
    &lt;p&gt;Becky: Yeah.&lt;/p&gt;
    &lt;p&gt;Adam: So there’s a warehouse somewhere Indiana Jones style?&lt;/p&gt;
    &lt;p&gt;Becky: Oh, I’m certain there’s a landfill somewhere. But let’s just say that he didn’t do any more 3DO games after that. Art Data folded not that much longer.&lt;/p&gt;
    &lt;p&gt;Adam: Maybe him and his band, they have a practice space, and the walls are just stacked up with all these boxes of the-&lt;/p&gt;
    &lt;p&gt;Becky: Yeah, he could use it to layer a wall with it and use that for the soundproof acoustics.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Learn Low Level Programming&lt;/head&gt;
    &lt;p&gt;Adam: That’s the story of porting Doom. Doom fans know it as one of the lowest frame rate ports, but it’s playable, and it has a rocking soundtrack. Not bad for a 10 week port. The thing Becky’s story brings to mind for me is that I could never do this. You couldn’t throw me at some new hardware and get me to port a game or anything to run on it. I feel like my development skills just don’t go down that close to the metal. But Becky says it’s a trainable skill.&lt;/p&gt;
    &lt;p&gt;Becky: Yeah. What I would advise a lot of people these days, if you really want to understand computers, first start off with one of the old classic machines. And you could do it with an emulator, but of course you could buy the real thing. Learn how to program an Apple II. Learn how to program an Atari 100 or a Commodore 64, because you have to learn a similar language. You learn the hardware, you learn how it all interacts. It takes away a lot of the magic that’s been given to modern computers. Because modern computers, they think that programming a webpage in JavaScript is programming a computer, that’s the furthest from the truth. Or writing code in Python is writing code directly to the metal. No, it’s an interpreted language. And even when you’re writing code in C++ and so forth, especially the most modern ones with their template bonanzas. When you look at the actual assembly code it generates, you see all the stuff it’s doing for the overhead and go, what the hell?&lt;/p&gt;
    &lt;p&gt;I miss the elegance and simplicity of writing 6502 code. The code, you enter it, it just does one task, leaves. Next function does the same thing. Whereas in C++, you can write code that actually still maintains that paradigm and it works just fine. But it’s so easy if you don’t really know what’s going on to write what you think is three lines of code, this’ll do everything. But when you look at the output of the compiler, it’s a novel. That’s my main takeaway. Is that when you truly understand the instruction sets, how the stack works, how memory management works, how the hardware works, how it all interacts, then you have a true understanding of the limitations of computers.&lt;/p&gt;
    &lt;p&gt;Adam: I think there’s a big divide between the programming that you have done and the people who never leave Java, JVM, or Python, or whatever. I wouldn’t know where to start exactly.&lt;/p&gt;
    &lt;p&gt;Becky: Oh, there are plenty of books out there. One I would recommend highly is a book called Assembly Lines, by Roger Wagner. It’s a great investment. Just look up Roger Wagner, Assembly Lines. It’s a book about maybe 200 pages long, and it is a beginner’s guide to assembly language, programming the Apple II, and all the fundamentals. In fact, in most cases, if you read the book cover to cover, you may not even have to start up an Apple II, you will get a very good fundamental grasp of how a computer operates. And then if what you’ve read intrigues you, then pull up an Apple II emulator or the real thing. You can buy one on eBay, it’s not that expensive, and go to town and see what’s going on.&lt;/p&gt;
    &lt;p&gt;But the end result is not for you to make a brand new Apple II video game. You could if you want, there’s still a market for them. Granted, you’ll sell a couple hundred copies instead of thousands. But what goes on is that you will truly understand how a computer works because there’s very little difference fundamentally from how an Apple II, a 1977 computer versus a modern computer. Yeah, there’s more stuff on it like memory protection, multi CPUs, and protected IO ports. But still fundamentally the same thing. A processor registers a stack, memory IO registers, and a memory map and firmware. Those exist in modern computers. They haven’t changed. So there you go.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outro&lt;/head&gt;
    &lt;p&gt;Adam: So there you go, that was the show. Learning the Apple II as a path towards bare metal programming sounds fun. If you attempt it, let me know on Twitter or on our Slack channel.&lt;/p&gt;
    &lt;p&gt;And thank you so much to Becky. You could find her on Twitter @BurgerBecky, and on Twitch as Burger Becky as well.&lt;/p&gt;
    &lt;p&gt;In an upcoming bonus episode for supporters, she’ll be sharing the porting of another game that I mentioned earlier, Out of this World for the Super Nintendo. And in this case, it’s a totally different story. It’s about a porting going very well under a different type of stress.&lt;/p&gt;
    &lt;p&gt;And you’ll get to hear some of the background behind this interview, and me chatting with Becky. I’m up to 13 bonus episodes now for supporters, 14 once that one’s out. So I recommend that you check it out. That’s at corecursive.com/supporters.&lt;/p&gt;
    &lt;p&gt;And until next time, enjoy this rocking Doom inspired rock anthems. I couldn’t actually include the real Doom music, because I don’t really know who owns them. But yeah, until next time, thank you so much for listening.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45973573</guid><pubDate>Tue, 18 Nov 2025 23:13:35 +0000</pubDate></item><item><title>Cloudflare outage on November 18, 2025 post mortem</title><link>https://blog.cloudflare.com/18-november-2025-outage/</link><description>&lt;doc fingerprint="bc1b3b5b0cb0a8cb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On 18 November 2025 at 11:20 UTC (all times in this blog are UTC), Cloudflare's network began experiencing significant failures to deliver core network traffic. This showed up to Internet users trying to access our customers' sites as an error page indicating a failure within Cloudflare's network. &lt;/p&gt;
      &lt;p&gt;The issue was not caused, directly or indirectly, by a cyber attack or malicious activity of any kind. Instead, it was triggered by a change to one of our database systems' permissions which caused the database to output multiple entries into a âfeature fileâ used by our Bot Management system. That feature file, in turn, doubled in size. The larger-than-expected feature file was then propagated to all the machines that make up our network.&lt;/p&gt;
      &lt;p&gt;The software running on these machines to route traffic across our network reads this feature file to keep our Bot Management system up to date with ever changing threats. The software had a limit on the size of the feature file that was below its doubled size. That caused the software to fail.&lt;/p&gt;
      &lt;p&gt;After we initially wrongly suspected the symptoms we were seeing were caused by a hyper-scale DDoS attack, we correctly identified the core issue and were able to stop the propagation of the larger-than-expected feature file and replace it with an earlier version of the file. Core traffic was largely flowing as normal by 14:30. We worked over the next few hours to mitigate increased load on various parts of our network as traffic rushed back online. As of 17:06 all systems at Cloudflare were functioning as normal.&lt;/p&gt;
      &lt;p&gt;We are sorry for the impact to our customers and to the Internet in general. Given Cloudflare's importance in the Internet ecosystem any outage of any of our systems is unacceptable. That there was a period of time where our network was not able to route traffic is deeply painful to every member of our team. We know we let you down today.&lt;/p&gt;
      &lt;p&gt;This post is an in-depth recount of exactly what happened and what systems and processes failed. It is also the beginning, though not the end, of what we plan to do in order to make sure an outage like this will not happen again.&lt;/p&gt;
      &lt;p&gt;The chart below shows the volume of 5xx error HTTP status codes served by the Cloudflare network. Normally this should be very low, and it was right up until the start of the outage. &lt;/p&gt;
      &lt;p&gt;The volume prior to 11:20 is the expected baseline of 5xx errors observed across our network. The spike, and subsequent fluctuations, show our system failing due to loading the incorrect feature file. Whatâs notable is that our system would then recover for a period. This was very unusual behavior for an internal error.&lt;/p&gt;
      &lt;p&gt;The explanation was that the file was being generated every five minutes by a query running on a ClickHouse database cluster, which was being gradually updated to improve permissions management. Bad data was only generated if the query ran on a part of the cluster which had been updated. As a result, every five minutes there was a chance of either a good or a bad set of configuration files being generated and rapidly propagated across the network.&lt;/p&gt;
      &lt;p&gt;This fluctuation made it unclear what was happening as the entire system would recover and then fail again as sometimes good, sometimes bad configuration files were distributed to our network. Initially, this led us to believe this might be caused by an attack. Eventually, every ClickHouse node was generating the bad configuration file and the fluctuation stabilized in the failing state.&lt;/p&gt;
      &lt;p&gt;Errors continued until the underlying issue was identified and resolved starting at 14:30. We solved the problem by stopping the generation and propagation of the bad feature file and manually inserting a known good file into the feature file distribution queue. And then forcing a restart of our core proxy.&lt;/p&gt;
      &lt;p&gt;The remaining long tail in the chart above is our team restarting remaining services that had entered a bad state, with 5xx error code volume returning to normal at 17:06.&lt;/p&gt;
      &lt;p&gt;The following services were impacted:&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Service / Product&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Impact description&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Core CDN and security services&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;HTTP 5xx status codes. The screenshot at the top of this post shows a typical error page delivered to end users.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Turnstile&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Turnstile failed to load.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Workers KV&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Workers KV returned a significantly elevated level of HTTP 5xx errors as requests to KVâs âfront endâ gateway failed due to the core proxy failing.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Dashboard&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;While the dashboard was mostly operational, most users were unable to log in due to Turnstile being unavailable on the login page.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Email Security&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;While email processing and delivery were unaffected, we observed a temporary loss of access to an IP reputation source which reduced spam-detection accuracy and prevented some new-domain-age detections from triggering, with no critical customer impact observed. We also saw failures in some Auto Move actions; all affected messages have been reviewed and remediated.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;Access&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Authentication failures were widespread for most users, beginning at the start of the incident and continuing until the rollback was initiated at 13:05. Any existing Access sessions were unaffected.&lt;/p&gt;
            &lt;p&gt;All failed authentication attempts resulted in an error page, meaning none of these users ever reached the target application while authentication was failing. Successful logins during this period were correctly logged during this incident.Â &lt;/p&gt;
            &lt;p&gt;Any Access configuration updates attempted at that time would have either failed outright or propagated very slowly. All configuration updates are now recovered.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;As well as returning HTTP 5xx errors, we observed significant increases in latency of responses from our CDN during the impact period. This was due to large amounts of CPU being consumed by our debugging and observability systems, which automatically enhance uncaught errors with additional debugging information.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How Cloudflare processes requests, and how this went wrong today&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Every request to Cloudflare takes a well-defined path through our network. It could be from a browser loading a webpage, a mobile app calling an API, or automated traffic from another service. These requests first terminate at our HTTP and TLS layer, then flow into our core proxy system (which we call FL for âFrontlineâ), and finally through Pingora, which performs cache lookups or fetches data from the origin if needed.&lt;/p&gt;
      &lt;p&gt;We previously shared more detail about how the core proxy works here.Â &lt;/p&gt;
      &lt;p&gt;As a request transits the core proxy, we run the various security and performance products available in our network. The proxy applies each customerâs unique configuration and settings, from enforcing WAF rules and DDoS protection to routing traffic to the Developer Platform and R2. It accomplishes this through a set of domain-specific modules that apply the configuration and policy rules to traffic transiting our proxy.&lt;/p&gt;
      &lt;p&gt;One of those modules, Bot Management, was the source of todayâs outage.Â &lt;/p&gt;
      &lt;p&gt;Cloudflareâs Bot Management includes, among other systems, a machine learning model that we use to generate bot scores for every request traversing our network. Our customers use bot scores to control which bots are allowed to access their sites â or not.&lt;/p&gt;
      &lt;p&gt;The model takes as input a âfeatureâ configuration file. A feature, in this context, is an individual trait used by the machine learning model to make a prediction about whether the request was automated or not. The feature configuration file is a collection of individual features.&lt;/p&gt;
      &lt;p&gt;This feature file is refreshed every few minutes and published to our entire network and allows us to react to variations in traffic flows across the Internet. It allows us to react to new types of bots and new bot attacks. So itâs critical that it is rolled out frequently and rapidly as bad actors change their tactics quickly.&lt;/p&gt;
      &lt;p&gt;A change in our underlying ClickHouse query behaviour (explained below) that generates this file caused it to have a large number of duplicate âfeatureâ rows. This changed the size of the previously fixed-size feature configuration file, causing the bots module to trigger an error.&lt;/p&gt;
      &lt;p&gt;As a result, HTTP 5xx error codes were returned by the core proxy system that handles traffic processing for our customers, for any traffic that depended on the bots module. This also affected Workers KV and Access, which rely on the core proxy.&lt;/p&gt;
      &lt;p&gt;Unrelated to this incident, we were and are currently migrating our customer traffic to a new version of our proxy service, internally known as FL2. Both versions were affected by the issue, although the impact observed was different.&lt;/p&gt;
      &lt;p&gt;Customers deployed on the new FL2 proxy engine, observed HTTP 5xx errors. Customers on our old proxy engine, known as FL, did not see errors, but bot scores were not generated correctly, resulting in all traffic receiving a bot score of zero. Customers that had rules deployed to block bots would have seen large numbers of false positives. Customers who were not using our bot score in their rules did not see any impact.&lt;/p&gt;
      &lt;p&gt;Throwing us off and making us believe this might have been an attack was another apparent symptom we observed: Cloudflareâs status page went down. The status page is hosted completely off Cloudflareâs infrastructure with no dependencies on Cloudflare. While it turned out to be a coincidence, it led some of the team diagnosing the issue to believe that an attacker may be targeting both our systems as well as our status page. Visitors to the status page at that time were greeted by an error message:&lt;/p&gt;
      &lt;p&gt;In the internal incident chat room, we were concerned that this might be the continuation of the recent spate of high volume Aisuru DDoS attacks:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;The query behaviour change&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;I mentioned above that a change in the underlying query behaviour resulted in the feature file containing a large number of duplicate rows. The database system in question uses ClickHouseâs software.&lt;/p&gt;
      &lt;p&gt;For context, itâs helpful to know how ClickHouse distributed queries work. A ClickHouse cluster consists of many shards. To query data from all shards, we have so-called distributed tables (powered by the table engine &lt;code&gt;Distributed&lt;/code&gt;) in a database called &lt;code&gt;default&lt;/code&gt;. The Distributed engine queries underlying tables in a database &lt;code&gt;r0&lt;/code&gt;. The underlying tables are where data is stored on each shard of a ClickHouse cluster.&lt;/p&gt;
      &lt;p&gt;Queries to the distributed tables run through a shared system account. As part of efforts to improve our distributed queries security and reliability, thereâs work being done to make them run under the initial user accounts instead.&lt;/p&gt;
      &lt;p&gt;Before today, ClickHouse users would only see the tables in the &lt;code&gt;default&lt;/code&gt; database when querying table metadata from ClickHouse system tables such as &lt;code&gt;system.tables&lt;/code&gt; or &lt;code&gt;system.columns&lt;/code&gt;.&lt;/p&gt;
      &lt;p&gt;Since users already have implicit access to underlying tables in &lt;code&gt;r0&lt;/code&gt;, we made a change at 11:05 to make this access explicit, so that users can see the metadata of these tables as well. By making sure that all distributed subqueries can run under the initial user, query limits and access grants can be evaluated in a more fine-grained manner, avoiding one bad subquery from a user affecting others.&lt;/p&gt;
      &lt;p&gt;The change explained above resulted in all users accessing accurate metadata about tables they have access to. Unfortunately, there were assumptions made in the past, that the list of columns returned by a query like this would only include the â&lt;code&gt;default&lt;/code&gt;â database:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;SELECT
  name,
  type
FROM system.columns
WHERE
  table = 'http_requests_features'
order by name;&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;Note how the query does not filter for the database name. With us gradually rolling out the explicit grants to users of a given ClickHouse cluster, after the change at 11:05 the query above started returning âduplicatesâ of columns because those were for underlying tables stored in the r0 database.&lt;/p&gt;
      &lt;p&gt;This, unfortunately, was the type of query that was performed by the Bot Management feature file generation logic to construct each input âfeatureâ for the file mentioned at the beginning of this section.Â &lt;/p&gt;
      &lt;p&gt;The query above would return a table of columns like the one displayed (simplified example):&lt;/p&gt;
      &lt;p&gt;However, as part of the additional permissions that were granted to the user, the response now contained all the metadata of the &lt;code&gt;r0&lt;/code&gt; schema effectively more than doubling the rows in the response ultimately affecting the number of rows (i.e. features) in the final file output.Â &lt;/p&gt;
      &lt;p&gt;Each module running on our proxy service has a number of limits in place to avoid unbounded memory consumption and to preallocate memory as a performance optimization. In this specific instance, the Bot Management system has a limit on the number of machine learning features that can be used at runtime. Currently that limit is set to 200, well above our current use of ~60 features. Again, the limit exists because for performance reasons we preallocate memory for the features.&lt;/p&gt;
      &lt;p&gt;When the bad file with more than 200 features was propagated to our servers, this limit was hit â resulting in the system panicking. The FL2 Rust code that makes the check and was the source of the unhandled error is shown below:&lt;/p&gt;
      &lt;p&gt;This resulted in the following panic which in turn resulted in a 5xx error:&lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;thread fl2_worker_thread panicked: called Result::unwrap() on an Err value&lt;/code&gt;
      &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Other impact during the incident&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Other systems that rely on our core proxy were impacted during the incident. This included Workers KV and Cloudflare Access. The team was able to reduce the impact to these systems at 13:04, when a patch was made to Workers KV to bypass the core proxy. Subsequently, all downstream systems that rely on Workers KV (such as Access itself) observed a reduced error rate.Â &lt;/p&gt;
      &lt;p&gt;The Cloudflare Dashboard was also impacted due to both Workers KV being used internally and Cloudflare Turnstile being deployed as part of our login flow.&lt;/p&gt;
      &lt;p&gt;Turnstile was impacted by this outage, resulting in customers who did not have an active dashboard session being unable to log in. This showed up as reduced availability during two time periods: from 11:30 to 13:10, and between 14:40 and 15:30, as seen in the graph below.&lt;/p&gt;
      &lt;p&gt;The first period, from 11:30 to 13:10, was due to the impact to Workers KV, which some control plane and dashboard functions rely upon. This was restored at 13:10, when Workers KV bypassed the core proxy system. The second period of impact to the dashboard occurred after restoring the feature configuration data. A backlog of login attempts began to overwhelm the dashboard. This backlog, in combination with retry attempts, resulted in elevated latency, reducing dashboard availability. Scaling control plane concurrency restored availability at approximately 15:30.&lt;/p&gt;
      &lt;p&gt;Now that our systems are back online and functioning normally, work has already begun on how we will harden them against failures like this in the future. In particular we are:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Hardening ingestion of Cloudflare-generated configuration files in the same way we would for user-generated input&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Enabling more global kill switches for features&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Eliminating the ability for core dumps or other error reports to overwhelm system resources&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Reviewing failure modes for error conditions across all core proxy modules&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Today was Cloudflare's worst outage since 2019. We've had outages that have made our dashboard unavailable. Some that have caused newer features to not be available for a period of time. But in the last 6+ years we've not had another outage that has caused the majority of core traffic to stop flowing through our network.&lt;/p&gt;
      &lt;p&gt;An outage like today is unacceptable. We've architected our systems to be highly resilient to failure to ensure traffic will always continue to flow. When we've had outages in the past it's always led to us building new, more resilient systems.&lt;/p&gt;
      &lt;p&gt;On behalf of the entire team at Cloudflare, I would like to apologize for the pain we caused the Internet today. &lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Time (UTC)&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Status&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell role="head"&gt;
            &lt;p&gt;Description&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;11:05&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Normal.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Database access control change deployed.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;11:28&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Impact starts.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Deployment reaches customer environments, first errors observed on customer HTTP traffic.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;11:32-13:05&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;The team investigated elevated traffic levels and errors to Workers KV service.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;The initial symptom appeared to be degraded Workers KV response rate causing downstream impact on other Cloudflare services.&lt;/p&gt;
            &lt;p&gt;Mitigations such as traffic manipulation and account limiting were attempted to bring the Workers KV service back to normal operating levels.&lt;/p&gt;
            &lt;p&gt;The first automated test detected the issue at 11:31 and manual investigation started at 11:32. The incident call was created at 11:35.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;13:05&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Workers KV and Cloudflare Access bypass implemented â impact reduced.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;During investigation, we used internal system bypasses for Workers KV and Cloudflare Access so they fell back to a prior version of our core proxy. Although the issue was also present in prior versions of our proxy, the impact was smaller as described below.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;13:37&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Work focused on rollback of the Bot Management configuration file to a last-known-good version.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;We were confident that the Bot Management configuration file was the trigger for the incident. Teams worked on ways to repair the service in multiple workstreams, with the fastest workstream a restore of a previous version of the file.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;14:24&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Stopped creation and propagation of new Bot Management configuration files.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;We identified that the Bot Management module was the source of the 500 errors and that this was caused by a bad configuration file. We stopped automatic deployment of new Bot Management configuration files.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;14:24&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Test of new file complete.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;We observed successful recovery using the old version of the configuration file and then focused on accelerating the fix globally.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;14:30&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;Main impact resolved. Downstream impacted services started observing reduced errors.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;A correct Bot Management configuration file was deployed globally and most services started operating correctly.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;p&gt;17:06&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;All services resolved. Impact ends.&lt;/p&gt;
          &lt;/cell&gt;
          &lt;cell&gt;
            &lt;p&gt;All downstream services restarted and all operations fully restored.&lt;/p&gt;
          &lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45973709</guid><pubDate>Tue, 18 Nov 2025 23:31:22 +0000</pubDate></item><item><title>Lucent 7 R/E 5ESS Telephone Switch Rescue (2024)</title><link>http://kev009.com/wp/2024/07/Lucent-5ESS-Rescue/</link><description>&lt;doc fingerprint="bf91da598eadf050"&gt;
  &lt;main&gt;
    &lt;p&gt;I am still recovering from the fairly challenging logistical project of saving a Lucent 5ESS. This is a whale of a project and I am still in a state of disbelief that I have gotten to this point. Thanks to my wife, brother, and a few friends for their help and the University of Arizona which has a very dedicated and professional Information Technology Services staff.&lt;/p&gt;
    &lt;p&gt;It started when I saw some telephone history enthusiasts post about a construction bid at the University of Arizona. It turns out, U of A installed the 5ESS in the late 1980s in a rather forward thinking move that netted a phone system that handled the growth of the University, medium speed data anywhere a phone may be located (ISDN BRI or PRI), and copper and fiber plant that will continue to be used indefinitely.&lt;/p&gt;
    &lt;p&gt;At peak, it served over 20,000 lines. They've done their own writeup, The End of An Era in Telecommunications, that is worth a read. In particular, the machine had an uptime of approximately 35 years including two significant retrofits to newer technology culminating in the current Lucent-dressed 7 R/E configuration that includes an optical packet-switched core called the Communications Module 3 (CM3) or Global Messaging Server 3 (GMS3).&lt;/p&gt;
    &lt;p&gt;Moving 40 frames of equipment, this required a ton of planning and muscle. The whole package took up two 26' straight-trucks, which is just 1' short of an entire standard US semi-trailer.&lt;/p&gt;
    &lt;p&gt;Coming from the computing and data networking world, the construction of the switch was quite bewildering at first. It is physically made up of standard frames which are interconnected into rows not unlike datacenter equipment, but the frames are integrated into an overhead system for cable management. Internally, they are wired up usually within the row and quite a few cables route horizontally beween frames, but some connections have to transit up and over to other rows.&lt;/p&gt;
    &lt;p&gt;Line Trunk Peripherals hook up to a Switching Module Controller (SMC) directly or an OXU (Optical Cross Connect Unit) which hooks up to an SMC and reduces the amount of copper cabling going between rows. Alarm cables run directly to an OAU (Office Alarm Unit) or form rings in between rows that eventually end at the OAU. Optical connections go from OXUs to SMCs and then to the CM, copper test circuits home run to a Metallic Test Service Unit shelf. Communications Cables come out the top and route toward the wire frame, usually in large 128 wire cables but occasionally in smaller quantity for direct or cross connect of special services. A pair of Power Distribution Frames distribute -48V throughout the entire footprint, taking into account redundancy at every level.&lt;/p&gt;
    &lt;p&gt;All of this was neatly cable laced with wax string. Moving a single frame required hundreds of distinct actions that vary from quick, like cutting cable lace, to time consuming removal of copper connections and bolts in all directions.&lt;/p&gt;
    &lt;p&gt;We were able to complete the removal in a single five day workweek, and I was able to unload it to my receiving area in two days over the weekend where it now safely resides.&lt;/p&gt;
    &lt;p&gt;The next step will be to acquire some AC and DC power distribution equipment, which will have to wait for my funds to recover.&lt;/p&gt;
    &lt;p&gt;I should be able to boot the Administrative Module (AM), a 3B21D computer, up relatively soon by acquiring a smaller DC rectifier and that alone will be very interesting as it is the only use I know of the DMERT or UNIX-RTR operating system, a fault tolerant micro-kernel realtime UNIX from Bell Labs.&lt;/p&gt;
    &lt;p&gt;The system came with a full set of manuals and schematics which will help greatly in rewiring and reconfiguring the machine. After the AM is up, I need to "de-grow" the disconnected equipment and I will eventually add back in an assortment of line, packet, and service units so that I can demonstrate POTS as well as ISDN voice and data. In particular, I am looking forward to interoperating with other communication and computing equipment I have.&lt;/p&gt;
    &lt;p&gt;I will have to reduce the size of the system quite a bit for power and space reasons so will have spare parts to sell or trade.&lt;/p&gt;
    &lt;p&gt;Additional Pictures are available here until I have a longer term project page established.&lt;/p&gt;
    &lt;p&gt;This is too much machine for one man, and it is part of a broader project I am working on to build a computing and telecommunications museum. If you are interested in working on the system with me, please feel free to reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45973955</guid><pubDate>Tue, 18 Nov 2025 23:59:14 +0000</pubDate></item><item><title>I just want working RCS messaging</title><link>https://wt.gd/i-just-want-my-rcs-messaging-to-work</link><description>&lt;doc fingerprint="a628d19d1b0711f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I just want working RCS messaging&lt;/head&gt;
    &lt;p&gt;I’m in over a month now with non-working RCS on my iPhone 15 Pro. Apple blames the carriers, the carriers tell me it’s not them (mostly T-Mobile since I have good contacts there). They tell me they can’t really do anything about iPhones not working on RCS, go back to Apple. This is what it looks like:&lt;/p&gt;
    &lt;p&gt;In short, it’s probably Apple or Google and there’s zero accountability from Apple. I have AppleCare+ and really hoped they’d actually try to troubleshoot and fix this rather than waste my time working around it (in a stupidly expensive way for me and Apple).&lt;/p&gt;
    &lt;head rend="h1"&gt;My background #&lt;/head&gt;
    &lt;p&gt;I’m OS agnostic as much as possible, I daily both Android and iOS devices and previously used BlackBerry 10 and Harmattan (Nokia N9’s OS). If Windows Phone was still around I’d probably still be running that as well. If it’s possible to gather information on how all this works under the hood, I can and do. The OnePlus Android devices I’m running are my own LineageOS builds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Previous history fixing MMS failures for Carriers/Vendors #&lt;/head&gt;
    &lt;p&gt;I’m also happy to blame carriers and vendors: I previously brought and helped resolve an issue with Verizon Wireless on LineageOS phones due to how MMS messaging works. Here’s my initial submission, their developer LuK found a better way to go about it, but it at least started the ball rolling: https://review.lineageos.org/c/LineageOS/android_device_oneplus_sm8250-common/+/333379&lt;/p&gt;
    &lt;p&gt;In short: When you received a picture message on Verizon in the past their network would notify your device that a new message arrived. When the device went to grab and download the image, it sends something similar to browser User Agent, called a UAProf. This is a link to a file that describes what the phone can handle, so a smartphone gets a high resolution image and a featurephone gets a lower resolution one. Verizon’s management sucks and decommissioned the domain that hosts all the UAProfs for their devices. Of note, Verizon is uniquely affected by this issue, T-Mobile doesn’t care what UAProf a device advertises, it’s not required on their network. I haven’t done enough testing with AT&amp;amp;T to answer whether it’s an issue for them.&lt;/p&gt;
    &lt;head rend="h2"&gt;MMS Failure Demonstrations #&lt;/head&gt;
    &lt;p&gt;This is a former link to a Verizon UAProf for a Samsung device: http://uaprof.vtext.com/sam/i515/i515.xml&lt;/p&gt;
    &lt;p&gt;Notice it doesn’t load? Apple/Blackberry and basically any non-Android manufacturers didn’t trust carriers to host these files. Some manager at Verizon decided to kill the vtext service and also fucked over any MMS users on their network not using an iPhone.&lt;/p&gt;
    &lt;p&gt;Here’s Apple’s: https://www.apple.com/mms/uaprof.rdf. &lt;lb/&gt; And here’s Blackberry’s: https://www.blackberry.com/go/mobile/profiles/uaprof/9700/5.0.0.rdf&lt;/p&gt;
    &lt;p&gt;I’m getting off-topic though, I just wanted to post some context that this is not my first rodeo with fixing these kinds of issues. Carriers are incompetent with this sort of interoperability and they gave up on running their own RCS servers to let Google do it through something called Google Jibe, I’ll talk about that soon.&lt;/p&gt;
    &lt;head rend="h1"&gt;Google breaking RCS on LineageOS #&lt;/head&gt;
    &lt;p&gt;Starting around the end of 2023, Google started to maliciously break RCS for custom Android OS’s. I say maliciously because it was a silent failure, RCS reported as working, but messages wouldn’t go through, and incoming messages would fail to receive. Google could have remained silent about it and rumors probably would have swirled: Perhaps it was a technical issue or the custom ROM developers’ faults?&lt;/p&gt;
    &lt;p&gt;No, Google intentionally broke it.&lt;/p&gt;
    &lt;p&gt;They straight up admitted to blocking it: https://www.androidauthority.com/google-silently-blocking-rcs-rooted-android-phones-custom-roms-3421652/ and it wasn’t until months later that they even showed a notification that it was disabled on affected devices. I really hope some lawyer or regulator reading this will get to extract their pound of blood because Google loves to boast about doing 911 over RCS: https://blog.google/products/messages/google-messages-rcs-911-emergency/&lt;/p&gt;
    &lt;p&gt;Eventually for my own devices I would spoof to the fingerprint of Google PIxel devices to be able to use RCS. It has mostly continued to work since then, but it begs the question: If I could reliably work around the blocking, then what excuse do you have about it being to prevent spam? Since those spammers will just use the same methods I’ve used, which are hardly secret. It just aims to hurt users that want some control of their device.&lt;/p&gt;
    &lt;head rend="h1"&gt;Apple launches RCS #&lt;/head&gt;
    &lt;p&gt;At some point Apple was dragged kicking and screaming into RCS interoperability. I actually have some sympathy here because MMS was really a terrible protocol that nobody should have adopted and Apple was dragged into supporting that years after the original iPhone launch in iOS 3. Regardless, with iOS 18, Apple brought in baseline RCS (version 2.4) support. It is hoped that they will update it sometime in the iOS 26 series to include E2E encryption.&lt;/p&gt;
    &lt;head rend="h1"&gt;My iPhone Background, Start of RCS Issues #&lt;/head&gt;
    &lt;p&gt;RCS always worked on my phone in iOS 18 until the past month when I upgraded to iOS 26. I should note that unlike Android, I do not modify iOS device in any way, basically I expect it should ‘just work’. The only unusual thing I run is Mullvad’s DNS to act as an adblocker, but so does my family and their iDevices don’t have RCS issues.&lt;/p&gt;
    &lt;p&gt;I am a dual-sim user on T-Mobile and US Mobile (usually on the AT&amp;amp;T network). With iOS 26 both lines have been stuck on “Waiting for activation…”. If I transfer the lines off to any other iPhone, the lines activate in seconds. I additionally took a Verizon Wireless line from my Mom’s 14 Pro Max and it also displayed the same issue. My girlfriend has a 14 Pro Max and a SE3, both can activate my RCS lines when I transfer them over.&lt;/p&gt;
    &lt;head rend="h1"&gt;Troubleshooting Steps I Did #&lt;/head&gt;
    &lt;p&gt;I’ve done an absolutely exhaustive level of testing to see if these lines would activate on my phone, there’s probably more than this but this is what I could think of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rebooted/Toggled Airplane Mode/Toggled RCS&lt;/item&gt;
      &lt;item&gt;Resetting Network Settings&lt;/item&gt;
      &lt;item&gt;Removed all my VPN profiles and apps. (Mullvad/Orbot/Mullvad’s DNS profile/my server’s wireguard profile)&lt;/item&gt;
      &lt;item&gt;Deactivated one of my lines and tried reactivating RCS.&lt;/item&gt;
      &lt;item&gt;Disabling 5G and trying to activate RCS.&lt;/item&gt;
      &lt;item&gt;Reissuing both eSIM’s from the carriers.&lt;/item&gt;
      &lt;item&gt;Toggling iMessage.&lt;/item&gt;
      &lt;item&gt;Resetting All settings 9 Resetting everything on device. &lt;list rend="ul"&gt;&lt;item&gt;Restoring from iTunes backup&lt;/item&gt;&lt;item&gt;Restoring from iCloud backup (literally activated a trial to be able to do this)&lt;/item&gt;&lt;item&gt;Tested resetting with and without eSIM.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Recovering device (recovery mode, setting up as new device) &lt;list rend="ul"&gt;&lt;item&gt;Both with and without eSIM’s on device.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Disabling RCS and waiting days before attempting to reactivate.&lt;/item&gt;
      &lt;item&gt;Updating my e911 addresses, disabling/renabling wifi calling. Testing on Wifi.&lt;/item&gt;
      &lt;item&gt;Reissuing just T-Mobile eSIM but to the other IMEI on the phone that it’s normally not on.&lt;/item&gt;
      &lt;item&gt;Deleting the numbers out numerous times in Carrier settings (I have no idea what this does but it does make the signal reconnect).&lt;/item&gt;
      &lt;item&gt;Testing sending messages from devices that work with RCS to this device in hopes it upgrades.&lt;/item&gt;
      &lt;item&gt;Testing the iOS beta releases.&lt;/item&gt;
      &lt;item&gt;I brought up the Gentoo Linux packages for libimobiledevice so I could run idevicesyslog and dump hundreds of megabytes of live logs in hopes of being able to see what the phone is failing on: (the packages) https://github.com/joecool1029/joecool-overlay/tree/master/app-pda &lt;list rend="ul"&gt;&lt;item&gt;This is a small T-Mobile related excerpt of what looks like the problem could be. Specifically, UserInteractionRequired.xml. I don’t know what interaction is needed and why Apple’s software isn’t presenting more information, but this is the best I could do from digging through a ton of redacted logs: &lt;code&gt;Nov 9 15:54:14.294398 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294406 CommCenter[101] &amp;lt;Notice&amp;gt;: #I --&amp;gt; switch: true, bundle_support: false, entitlement_support: true, enabled_by_default: true, disabled_by_profile: false, is_store_demo_device: false Nov 9 15:54:14.294415 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294424 CommCenter[101] &amp;lt;Notice&amp;gt;: #I --&amp;gt; encryption_supported: false, push_supported: false, push_enabled: false, private_relay_supported: false, msisdn_source: (empty) Nov 9 15:54:14.294432 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294440 CommCenter[101] &amp;lt;Notice&amp;gt;: #I --&amp;gt; Changed: (nothing) Nov 9 15:54:14.294448 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294455 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Ims registration interface: kUnknown --&amp;gt; kCellular Nov 9 15:54:14.294463 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294471 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Lazuli model not allowed: [provisioning style: kUsingToken, sms online: false, msisdn OK: true] Nov 9 15:54:14.294479 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294487 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Provisioning not possible Nov 9 15:54:14.294494 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294505 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Infinite validity of UserInteractionRequired.xml xml Nov 9 15:54:14.294514 CommCenter[101] &amp;lt;Notice&amp;gt;: #I [config.rcs.mnc260.mcc310.jibecloud.net] Declaring IMS not ready. Unexpired : UserInteractionRequired.xml Nov 9 15:54:14.294522 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294529 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Nudge not required: Allowed Nov 9 15:54:14.294537 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294546 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Evaluate recheckEntitlementForRCS. Ent:Allowed, Switch toggled:false, CB recheck:false Nov 9 15:54:14.294554 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294561 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Entitlement result: [RCS support: kSupported, user eligibile: kEligible, token-support: true] Nov 9 15:54:14.294569 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294577 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Evaluated provisioning style: kUsingToken Nov 9 15:54:14.294584 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294592 CommCenter[101] &amp;lt;Notice&amp;gt;: #I Retrieving feature switch state Nov 9 15:54:14.294600 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294608 CommCenter(CoreServices)[101] &amp;lt;Debug&amp;gt;: Starting database access (depth 0, options: 1) Nov 9 15:54:14.294616 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294624 CommCenter(CoreServices)[101] &amp;lt;Debug&amp;gt;: BindingEvaluator::CreateWithBundleInfo(ID=&amp;lt;private&amp;gt;, name=&amp;lt;private&amp;gt;, CC=????, vers=(null)) Nov 9 15:54:14.294633 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294641 CommCenter(CoreServices)[101] &amp;lt;Debug&amp;gt;: Truncating a list of bindings to max 1 known-good ones. Nov 9 15:54:14.294648 CommCenter[101] &amp;lt;Debug&amp;gt;: #D supportsHOVirtualInterfaces: ret = false Nov 9 15:54:14.294656 CommCenter(CoreServices)[101] &amp;lt;Debug&amp;gt;: Truncating a list of bindings to max 1 known-good ones.&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;This is a small T-Mobile related excerpt of what looks like the problem could be. Specifically, UserInteractionRequired.xml. I don’t know what interaction is needed and why Apple’s software isn’t presenting more information, but this is the best I could do from digging through a ton of redacted logs: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So this last entry probably tells us where to look. The carrier (T-Mobile here) is provisioned for RCS, it’s receiving this interaction required file with infinite validity. So long as that’s in place, it fails to activate. (This is a guess, but it’s certainly more information than the KB articles give on Apple’s sites).&lt;/p&gt;
    &lt;head rend="h2"&gt;Apple does not provide their employees with correct information on troubleshooting this issue. They do not empower them to properly troubleshoot the issue. #&lt;/head&gt;
    &lt;p&gt;The standard instruction given to them is: “Do not take accountability, blame the carrier.”&lt;/p&gt;
    &lt;p&gt;So then I come in and say I have failures with all 3 major carriers and categorically refuse to accept that explanation, when I know my lines work just fine on other iDevices.&lt;/p&gt;
    &lt;p&gt;The Apple Store initially blamed software, this would be reasonable except we’ve reloaded the state of my phone 3 times now (once from iTunes, and twice now from iCloud, tomorrow will be the 4th time). I gave them permission to wipe any setting and recover the phone, but I go a step further and request they transfer my T-Mobile eSIM to another store device preferably in the 15 Pro line. They cannot do this because of user privacy reasons. This is a dealbreaker from troubleshooting, I am not made of money and I do not have any additional 15 pro devices to test with, it’s already crazy enough I have multiple carriers at the ready to test, 2 14 Pro Max’s and a SE3.&lt;/p&gt;
    &lt;head rend="h1"&gt;Google Jibe #&lt;/head&gt;
    &lt;p&gt;I think this is where we need information. As I said before, the carriers in the US gave up running their own RCS infrastructure and Apple’s employees aren’t really trained about this situation. With the exception of my own knowledge and the logs I pulled from the phone, Jibe was not mentioned once in the 3 phone calls and the multiple hours onsite in Apple Store today.&lt;/p&gt;
    &lt;p&gt;I have no business relationship with Google Jibe, and there’s no way for me to interact with or contact them. Their documentation is probably here but I can’t read it, since I’m not a carrier partner: https://docs.jibemobile.com/ Apple knows there’s a ‘carrier’ issue, but in reality, RCS is run through Google Jibe in the US and this was never once disclosed to me. I never brought it up until this blog post, I cannot go into a store and say “I have been using opensource tools to analyze the logs from this phone and think it’s a failure with Jibe”. Do you get how crazy this sounds?&lt;/p&gt;
    &lt;head rend="h1"&gt;What Apple’s Going To Do Tomorrow #&lt;/head&gt;
    &lt;p&gt;Since they hit a wall and I refuse to continue to entertain the “go bug T-Mobile/US Mobile” direction, Apple is swapping the board in my phone. Of course they didn’t have the parts in the store to do it, so I have to wait to drive back tomorrow for them to do it. This will have new IMEI numbers and given the experience I’ve had with these lines activating on 3 other iDevices, it should probably work. The only way it wouldn’t is if this was a generational issue, but they have not given me a way to test this. They adamantly tell me: “We are doing you the favor as a courtesy, we don’t believe this is our problem.” I know they are trained to say this but it’s terrible customer service. I shelled out for Applecare+, if it might be the phone just swap it and analyze it back at Apple HQ, I’ve done enough testing now to know it’s something with just this specific device. I referred people to use iPhones because in general they do not often have these issues and the customer support was good. The board swap solution they are offering only wastes my time/fuel and punts the problem down the road. Since we never actually looked at the logs I might hit it again, other users might be affected.&lt;/p&gt;
    &lt;head rend="h1"&gt;I’d rather Apple actually fix the problem #&lt;/head&gt;
    &lt;p&gt;I use opensource software not because it’s inherently better, but rather because I can at least triage, understand, and fix problems. Give me a radar Apple. I’m a rare dual-SIM user in the US with a Google Jibe RCS failure. Where did it fail? Dig into my logs and tell me: Is it because I hop between primary data carriers (because the whole reason I have dual-carrier is better service coverage). I don’t spend a lot of time on WiFI, I run my house on mobile carriers. The only thing I know is I didn’t change my configuration from iOS 18 to iOS 26, but things stopped working and there’s no way for me to downgrade to 18 because you stopped signing it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45974869</guid><pubDate>Wed, 19 Nov 2025 01:41:33 +0000</pubDate></item><item><title>DOE gives Microsoft partner $1B loan to restart Three Mile Island reactor</title><link>https://techcrunch.com/2025/11/18/trump-doe-gives-microsoft-partner-1b-loan-to-restart-three-mile-island-reactor/</link><description>&lt;doc fingerprint="2790015c49b6a5f0"&gt;
  &lt;main&gt;
    &lt;p&gt;The Trump administration announced Tuesday it would provide Constellation Energy with a $1 billion loan to restart a nuclear reactor at Three Mile Island.&lt;/p&gt;
    &lt;p&gt;The energy company said last year it would reopen the reactor, which had been shuttered since 2019, after Microsoft committed to purchasing all the electricity from the 835 megawatt power plant for two decades. Constellation estimated the project would cost $1.6 billion, and it expects to complete the refurbishment in 2028.&lt;/p&gt;
    &lt;p&gt;Terms of Microsoft’s deal with Constellation weren’t disclosed. Analysts at Jefferies have estimated the tech company might be paying about $110 to $115 per megawatt-hour over 20 years of the deal.&lt;/p&gt;
    &lt;p&gt;That’s cheaper than a brand-new nuclear power plant would cost, but it’s a hefty premium over wind, solar, and geothermal, according to a comparison of energy costs from Lazard. Even wind and solar projects outfitted with utility-scale batteries to enable 24/7 power are cheaper.&lt;/p&gt;
    &lt;p&gt;Nonetheless, tech companies have recently fallen in love with nuclear as power demands for their data centers and AI efforts have skyrocketed. This summer, Microsoft competitor Meta signed its own deal with Constellation, buying the “clean energy attributes” of a 1.1 gigawatt nuclear power plant in Illinois.&lt;/p&gt;
    &lt;p&gt;The reactor at Three Mile Island that’s being restarted isn’t the infamous Unit 2, which melted down in 1979. Rather, it’s Unit 1, which was commissioned in 1974 and taken offline in 2019 as cheap natural gas eroded its profitability.&lt;/p&gt;
    &lt;p&gt;The debt facility is being made through the Department of Energy’s Loan Programs Office (LPO), which was formed under the Energy Policy Act of 2005 to foster the growth of clean energy technologies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;p&gt;The LPO is most famous for its loan to Solyndra, a U.S. solar startup that went belly-up during the Great Recession. Overall, though, experts consider the LPO a success, with a default rate of 3.3% after recoveries. Tesla, for instance, received a $465 million loan under the program in 2010 and paid it back by 2013.&lt;/p&gt;
    &lt;p&gt;Last month, the LPO finalized a $1.6 billion loan to American Electric Power, using federal dollars to support the upgrade of around 5,000 miles of transmission lines.&lt;/p&gt;
    &lt;p&gt;The Inflation Reduction Act, which passed during the Biden administration, created another pot of money under the LPO known as the Energy Infrastructure Reinvestment program. That program was created to restore existing power plants to operation provided they avoid or reduce pollutants or greenhouse gas emissions. The Trump administration kept it largely in tact, rebranding it the Energy Dominance Financing Program.&lt;/p&gt;
    &lt;p&gt;In its press release, the Department of Energy, perhaps erroneously, says the the EDF Program was created under the Working Families Tax Cut Act. It was instead authorized under the One Big Beautiful Bill Act.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45975006</guid><pubDate>Wed, 19 Nov 2025 02:02:10 +0000</pubDate></item></channel></rss>