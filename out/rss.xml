<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 20 Dec 2025 08:13:52 +0000</lastBuildDate><item><title>Brown/MIT shooting suspect found dead, officials say</title><link>https://www.washingtonpost.com/nation/2025/12/18/brown-university-shooting-person-of-interest/</link><description>&lt;doc fingerprint="ff5221384375c71"&gt;
  &lt;main&gt;
    &lt;p&gt;A man suspected in the fatal shooting at Brown University and killing days later of a professor from the Massachusetts Institute of Technology was found dead of a self-inflicted gunshot wound inside a New Hampshire storage facility, authorities announced Thursday evening.&lt;/p&gt;
    &lt;p&gt;Democracy Dies in Darkness&lt;/p&gt;
    &lt;head rend="h1"&gt;Brown U. and MIT professor shootings are linked; suspect found dead, officials say&lt;/head&gt;
    &lt;p&gt;The alleged shooter, Claudio Manuel Neves Valente, was a legal permanent U.S. resident and had been a graduate student at Brown two decades ago, authorities said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46321947</guid><pubDate>Fri, 19 Dec 2025 03:19:59 +0000</pubDate></item><item><title>Qwen-Image-Layered: transparency and layer aware open diffusion model</title><link>https://huggingface.co/papers/2512.15603</link><description>&lt;doc fingerprint="599fcb381976cb86"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition&lt;/head&gt;
    &lt;head rend="h2"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;Qwen-Image-Layered decomposes images into semantically disentangled RGBA layers using a diffusion model, enabling independent editing of each layer and improving decomposition quality and consistency.&lt;/p&gt;
    &lt;p&gt;Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose Qwen-Image-Layered, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling inherent editability, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on https://github.com/QwenLM/Qwen-Image-Layered{https://github.com/QwenLM/Qwen-Image-Layered}&lt;/p&gt;
    &lt;head rend="h3"&gt;Community&lt;/head&gt;
    &lt;p&gt;Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing.&lt;/p&gt;
    &lt;p&gt;arXiv lens breakdown of this paper üëâ https://arxivlens.com/PaperView/Details/qwen-image-layered-towards-inherent-editability-via-layer-decomposition-9194-7a40c6da&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Executive Summary&lt;/item&gt;
      &lt;item&gt;Detailed Breakdown&lt;/item&gt;
      &lt;item&gt;Practical Applications&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When will the code and model be released?&lt;/p&gt;
    &lt;p&gt;In the paper they point to a repo that does not exist üòî https://github.com/QwenLM/QwenImage-Layered&lt;/p&gt;
    &lt;p&gt;The repository exists and it's available at the following location https://github.com/QwenLM/Qwen-Image-Layered (it's just a typo in the paper).&lt;/p&gt;
    &lt;p&gt;how to use it in figma or photoshop&lt;/p&gt;
    &lt;head rend="h2"&gt;Models citing this paper 2&lt;/head&gt;
    &lt;head rend="h2"&gt;Datasets citing this paper 0&lt;/head&gt;
    &lt;p&gt;No dataset linking this paper&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46321972</guid><pubDate>Fri, 19 Dec 2025 03:24:26 +0000</pubDate></item><item><title>Rust's Block Pattern</title><link>https://notgull.net/block-pattern/</link><description>&lt;doc fingerprint="a620394777b53911"&gt;
  &lt;main&gt;
    &lt;p&gt;Here‚Äôs a little idiom that I haven‚Äôt really seen discussed anywhere, that I think makes Rust code much cleaner and more robust.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know if there‚Äôs an actual name for this idiom; I‚Äôm calling it the ‚Äúblock pattern‚Äù for lack of a better word. I find myself reaching for it frequently in code, and I think other Rust code could become cleaner if it followed this pattern. If there‚Äôs an existing name for this, please let me know!&lt;/p&gt;
    &lt;p&gt;The pattern comes from blocks in Rust being valid expressions. For example, this code:&lt;/p&gt;
    &lt;code&gt;let foo = { 1 + 2 };
&lt;/code&gt;
    &lt;p&gt;‚Ä¶is equal to this code:&lt;/p&gt;
    &lt;code&gt;let foo = 1 + 2;
&lt;/code&gt;
    &lt;p&gt;‚Ä¶which is, in turn, equal to this code:&lt;/p&gt;
    &lt;code&gt;let foo = {
    let x = 1;
    let y = 2;
    x + y
};
&lt;/code&gt;
    &lt;head rend="h2"&gt;So, why does this matter?&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say you have a function that loads a configuration file, then sends a few HTTP requests based on that config file. In order to load that config file, first you need to load the raw bytes of that file from the disk. Then you need to parse whatever the format of the configuration file is. For the sake of having a complex enough program to demonstrate the value of this pattern, let‚Äôs say it‚Äôs JSON with comments. You would need to remove the comments first using the &lt;code&gt;regex&lt;/code&gt; crate,
then parse the resulting JSON with something like &lt;code&gt;serde-json&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Such a function would look like this:&lt;/p&gt;
    &lt;code&gt;use regex::{Regex, RegexBuilder};
use std::{fs, sync::LazyLock};

/// Format of the configuration file.
#[derive(serde::Deserialize)]
struct Config { /* ... */ }

// Always make sure to cache your regexes!
static STRIP_COMMENTS: LazyLock&amp;lt;Regex&amp;gt; = LazyLock::new(|| {
    RegexBuilder::new(r"//.*").multi_line(true).build().expect("regex build failed")
});

/// Function to load the config and send some HTTP requests.
fn foo(cfg_file: &amp;amp;str) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
    // Load the raw bytes of the file.
    let config_data = fs::read(cfg_file)?;

    // Convert to a string to the regex can work on it.
    let config_string = String::from_utf8(&amp;amp;config_data)?;

    // Strip out all comments.
    let stripped_data = STRIP_COMMENTS.replace(&amp;amp;config_string, "");

    // Parse as JSON.
    let config = serde_json::from_str(&amp;amp;stripped_data)?;

    // Do some work based on this data.
    send_http_request(&amp;amp;config.url1)?;
    send_http_request(&amp;amp;config.url2)?;
    send_http_request(&amp;amp;config.url3)?;

    Ok(())
}
&lt;/code&gt;
    &lt;p&gt;This is fairly simple, and just leverages a few Rust crates and language features to parse JSON and then do something with it.&lt;/p&gt;
    &lt;p&gt;However, there are a few weaknesses here. In the &lt;code&gt;foo&lt;/code&gt; function, we declare four new variables (&lt;code&gt;config_data&lt;/code&gt;, &lt;code&gt;config_string&lt;/code&gt;,
&lt;code&gt;stripped_data&lt;/code&gt;, &lt;code&gt;config&lt;/code&gt;) only for only one of those variables to be used after the configuration parsing (&lt;code&gt;config&lt;/code&gt;). In addition,
let‚Äôs say you didn‚Äôt know what this code was for going in, and you didn‚Äôt have these comments (or you had bad comments). One might
ask why you‚Äôre declaring the regular expression &lt;code&gt;STRIP_COMMENTS&lt;/code&gt;, or why you‚Äôre loading data from a file.&lt;/p&gt;
    &lt;p&gt;When I write code, I try to make it immediately obvious what the purpose of the code is, and why it‚Äôs written that way. This is why I generally avoid C‚Äôs ‚Äúbottom-up‚Äù strategy for organizing code. It‚Äôs like being given a few screws and being expected to implicitly understand that it should be built into a chair. In Rust, I like that you are able to define your top-level functions first, and then go down and define all the bits and pieces after.&lt;/p&gt;
    &lt;p&gt;Although, we can do a little bit better. What if we organized the &lt;code&gt;foo&lt;/code&gt; function like this:&lt;/p&gt;
    &lt;code&gt;/// Function to load the config and send some HTTP requests.
fn foo(cfg_file: &amp;amp;str) -&amp;gt; anyhow::Result&amp;lt;()&amp;gt; {
    // Load the configuration from the file.
    let config = {
        // Cached regular expression for stripping comments.
        static STRIP_COMMENTS: LazyLock&amp;lt;Regex&amp;gt; = LazyLock::new(|| {
            RegexBuilder::new(r"//.*").multi_line(true).build().expect("regex build failed")
        });

        // Load the raw bytes of the file.
        let raw_data = fs::read(cfg_file)?;

        // Convert to a string to the regex can work on it.
        let data_string = String::from_utf8(&amp;amp;raw_data)?;

        // Strip out all comments.
        let stripped_data = STRIP_COMMENTS.replace(&amp;amp;config_string, "");

        // Parse as JSON.
        serde_json::from_str(&amp;amp;stripped_data)?
    };

    // Do some work based on this data.
    send_http_request(&amp;amp;config.url1)?;
    send_http_request(&amp;amp;config.url2)?;
    send_http_request(&amp;amp;config.url3)?;

    Ok(())
}
&lt;/code&gt;
    &lt;p&gt;In this function, we‚Äôve moved all of the configuration-related code (parsing, loading, even the static regex) into the block. This works because Rust lets you have items, statements and expressions inside of a block, hence why we were able to move everything inside. This pattern has three immediate advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The block starts with the intent of the code (&lt;code&gt;let config = ...&lt;/code&gt;). We can see that we‚Äôre working to resolve some kind of configuration object right off the bat. Only then do we move into the implementation details of the code.&lt;/item&gt;
      &lt;item&gt;It reduces pollution of the namespace of both the &lt;code&gt;foo&lt;/code&gt;function and the top-level module. Now in&lt;code&gt;foo&lt;/code&gt;, the variable names&lt;code&gt;config_data&lt;/code&gt;,&lt;code&gt;config_string&lt;/code&gt;et al are no longer used. In addition to allowing these variable names to be re-used, it makes this code a lot more ‚Äúidiot-proof‚Äù. If someone else were to edit the&lt;code&gt;foo&lt;/code&gt;function, they would only be able to use&lt;code&gt;config&lt;/code&gt;. They wouldn‚Äôt be able to use the&lt;code&gt;raw_data&lt;/code&gt;or&lt;code&gt;STRIP_COMMENTS&lt;/code&gt;items, which are only meant to be used by the&lt;code&gt;config&lt;/code&gt;parser.&lt;/item&gt;
      &lt;item&gt;The variables &lt;code&gt;raw_data&lt;/code&gt;and&lt;code&gt;data_string&lt;/code&gt;go out of scope at the end of the block, which means they are dropped, freeing up resources.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an aside, all three of these advantages also come if you were to refactor the block out into its own function. However, this pattern has two key advantages over that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code flow is still inline with the rest of the function. For shorter blocks, this improves reading comprehension, since it means you don‚Äôt have to go to a different part of the code to fully understand the function.&lt;/item&gt;
      &lt;item&gt;If there are a lot of variables that the block would use, it prevents needing to explicitly name those variables as parameters.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is one more benefit that‚Äôs not exposed in the above example: erasure of mutability. Let‚Äôs say you construct some object for use in a later part of the function:&lt;/p&gt;
    &lt;code&gt;let mut data = vec![];
data.push(1);
data.extend_from_slice(&amp;amp;[4, 5, 6, 7]);

data.iter().for_each(|x| println!("{x}"));
return data[2];
&lt;/code&gt;
    &lt;p&gt;The issue is that &lt;code&gt;data&lt;/code&gt; is declared as mutable, which means the rest of the function can mutate it. Since a lot of bugs come from
data being mutated when it isn‚Äôt supposed to be mutated, we‚Äôd like to restrict the mutability of the data to a certain area of the
function. This is also possible with the block pattern:&lt;/p&gt;
    &lt;code&gt;let data = {
    let mut data = vec![];
    data.push(1);
    data.extend_from_slice(&amp;amp;[4, 5, 6, 7]);
    data
};

data.iter().for_each(|x| println!("{x}"));
return data[2];
&lt;/code&gt;
    &lt;p&gt;This effectively ‚Äúcloses‚Äù the mutability to a certain section of the function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;I don‚Äôt know if this pattern is already well known to the Rust community. Even if it isn‚Äôt, I figure it‚Äôs still a good idea to bring it to people who may be inexperienced in Rust.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46322391</guid><pubDate>Fri, 19 Dec 2025 04:56:13 +0000</pubDate></item><item><title>Amazon will allow ePub and PDF downloads for DRM-free eBooks</title><link>https://www.kdpcommunity.com/s/article/New-eBook-Download-Options-for-Readers-Coming-in-2026?language=en_US</link><description>&lt;doc fingerprint="336f507caffaf299"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading √ó Sorry to interrupt CSS Error Refresh&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46324078</guid><pubDate>Fri, 19 Dec 2025 10:03:38 +0000</pubDate></item><item><title>GotaTun ‚Äì Mullvad's WireGuard Implementation in Rust</title><link>https://mullvad.net/en/blog/announcing-gotatun-the-future-of-wireguard-at-mullvad-vpn</link><description>&lt;doc fingerprint="77aac445c1b43610"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing GotaTun, the future of WireGuard at Mullvad VPN&lt;/head&gt;
    &lt;p&gt;GotaTun is a WireGuard¬Æ implementation written in Rust aimed at being fast, efficient and reliable.&lt;/p&gt;
    &lt;p&gt;GotaTun is a fork of the BoringTun project from Cloudflare. This is not a new protocol or connection method, just WireGuard¬Æ written in Rust. The name GotaTun is a combination of the original project, BoringTun, and G√∂tatunneln, a physical tunnel located in Gothenburg. We have integrated privacy enhancing features like DAITA &amp;amp; Multihop, added first-class support for Android and used Rust to achieve great performance by using safe multi-threading and zero-copy memory strategies.&lt;/p&gt;
    &lt;p&gt;Last month we rolled it out to all our Android users, and we aim to ship it to the remaining platforms next year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why GotaTun?&lt;/head&gt;
    &lt;p&gt;Our mobile apps have relied on wireguard-go for several years, a cross-platform userspace implementation of WireGuard¬Æ in Go. wireguard-go has been the de-facto userspace implementation of WireGuard¬Æ to this date, and many VPN providers besides Mullvad use it. Since mid-2024 we have been maintaining a fork of &lt;lb/&gt;wireguard-go to support features like DAITA &amp;amp; Multihop. While wireguard-go has served its purpose for many years it has not been without its challenges.&lt;/p&gt;
    &lt;p&gt;For Android apps distributed via the Google Play Store, Google collects crash reports and makes them available to developers. In the developer console we have seen that more than 85% of all crashes reported have stemmed from the wireguard-go. We have managed to solve some of the obscure issues over the years (#6727 and #7728 to name two examples), but many still remain. For these reasons we chose Android as the first platform to release GotaTun on, allowing us to see the impact right away.&lt;/p&gt;
    &lt;p&gt;Another challenge we have faced is interoperating Rust and Go. Currently, most of the service components of the Mullvad VPN app are written in Rust with the exception of wireguard-go. Crossing the boundary between Rust and Go is done using a foreign function interface (FFI), which is inherently unsafe and complex. Since Go is a managed language with its own separate runtime, how it executes is opaque to the Rust code. If wireguard-go were to hang or crash, recovering stacktraces is not always possible which makes debugging the code cumbersome. Limited visibility insight into crashes stemming from Go has made troubleshooting and long-term maintenance tedious.&lt;/p&gt;
    &lt;head rend="h3"&gt;Outcome&lt;/head&gt;
    &lt;p&gt;The impact has been immediate. So far not a single crash has stemmed from GotaTun, meaning that all our old crashes from wireguard-go are now gone. Since rolling out GotaTun on Android with version 2025.10 in the end of November we‚Äôve seen a big drop in the metric user-perceived crash rate, from 0.40% to 0.01%, when comparing to previous releases. The feedback from users' have also been positive, with reports of better speeds and lower battery usage.&lt;/p&gt;
    &lt;p&gt;User-perceived crash rate&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking ahead&lt;/head&gt;
    &lt;p&gt;We‚Äôve reached the first major milestone with the release of GotaTun on Android, but we have a lot more exciting things in store for 2026.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A third-party security audit will take place early next year.&lt;/item&gt;
      &lt;item&gt;We will replace wireguard-go with GotaTun across all platforms, including desktop and iOS.&lt;/item&gt;
      &lt;item&gt;More effort will be put into improving performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We hope you are as excited as we are for 2026!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46324543</guid><pubDate>Fri, 19 Dec 2025 11:16:23 +0000</pubDate></item><item><title>8-bit Bol√©ro</title><link>https://linusakesson.net/music/bolero/index.php</link><description>&lt;doc fingerprint="ad825aa56a18c6af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bol√©ro&lt;/head&gt;
    &lt;p&gt;I perform Maurice Ravel's Bol√©ro on a variety of homemade 8-bit instruments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linus Akesson - Maurice Ravel - Bol√©ro.mp3 (MP3, 26.2 MB)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Some stats and details&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;9 hours and 42 minutes of footage&lt;/item&gt;
      &lt;item&gt;52 mixer channels&lt;/item&gt;
      &lt;item&gt;13 neck- and bowties&lt;/item&gt;
      &lt;item&gt;9 different instruments&lt;/item&gt;
      &lt;item&gt;1 crazy automaton&lt;/item&gt;
      &lt;item&gt;0 regrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project took me a bit over half a year to finish.&lt;/p&gt;
    &lt;p&gt;I hope you'll enjoy the video as much as I enjoyed making it! There are many little details that I'll let you discover on your own.&lt;/p&gt;
    &lt;p&gt;It was fun to put my tools and methods to the test with this huge undertaking. When I started out I had no idea if my mixing and video-editing process would work at this scale, but it turned out to only need a few tweaks here and there.&lt;/p&gt;
    &lt;p&gt;The nine instruments are: The Qweremin (breadbin / regular C64C / dark C64C), Qwertuoso (breadbin), the Paulimba, the Tenor Commodordion, the Family Bass (albeit not as a bass this time), my still unnamed floppy-drive noise instrument (1541 / 1541-II), the C=TAR, the Chipophone, and a newcomer: NES timpani.&lt;/p&gt;
    &lt;p&gt;The timpani sound is based on the famous NES staircase triangle wave. But there is no register for controlling its volume, and yet we can hear an envelope with a release phase. To achieve this, I rely on a neat trick that was used already in Super Mario Bros.: It turns out that the triangle channel is mixed with the ADPCM sample-playback channel and then fed through a non-linear resistor network (the output is proportional to ((adpcm + triangle)‚Åª¬π + C)‚Åª¬π). Therefore, adding a constant DC offset via the sample channel makes the triangle more or less compressed.&lt;/p&gt;
    &lt;p&gt;In nearly every part of this video, what you see is what you hear; the audio and video were recorded at the same time. But the automaton is different: Thanks to its 100% repeatable performance I could capture the sound of each individual section of the hardware, with the microphone up close, and then mix the parts according to taste and combine them with the visuals from a separate take.&lt;/p&gt;
    &lt;p&gt;Fun fact: You can't see it in the video but the automaton is supported by original C64 boxes:&lt;/p&gt;
    &lt;p&gt;Posted Friday 19-Dec-2025 08:00&lt;/p&gt;
    &lt;head rend="h3"&gt;Discuss this page&lt;/head&gt;
    &lt;p&gt;Disclaimer: I am not responsible for what people (other than myself) write in the forums. Please report any abuse, such as insults, slander, spam and illegal material, and I will take appropriate actions. Don't feed the trolls.&lt;/p&gt;
    &lt;p&gt;Jag tar inget ansvar f√∂r det som skrivs i forumet, f√∂rutom mina egna inl√§gg. V√§nligen rapportera alla inl√§gg som bryter mot reglerna, s√• ska jag se vad jag kan g√∂ra. Som regelbrott r√§knas till exempel f√∂rol√§mpningar, f√∂rtal, spam och olagligt material. Mata inte tr√•larna.&lt;/p&gt;
    &lt;p&gt;Fri 19-Dec-2025 21:55&lt;/p&gt;
    &lt;p&gt;Emil Lindroth&lt;/p&gt;
    &lt;p&gt;Fri 19-Dec-2025 21:59&lt;/p&gt;
    &lt;p&gt;In this day and age of ai generated music you've surpassed excellence by not only bringing the computer as an electric music instrument to life but also delivering art at an unprecedented level.&lt;/p&gt;
    &lt;p&gt;You, sir Linus √Ökesson are a true artist. And I mean that with the deepest of respect as an artist myself. I hope your art will reach around the globe as many times as viraly possible for many years to come.&lt;/p&gt;
    &lt;p&gt;Thank you Linus for brightening the world with you brilliance and for raising the bar. I wish you all the best of fortunes in life.&lt;/p&gt;
    &lt;p&gt;Emil Lindroth&lt;/p&gt;
    &lt;p&gt;Blues keyboardist&lt;/p&gt;
    &lt;p&gt;Fri 19-Dec-2025 22:09&lt;/p&gt;
    &lt;p&gt;Fri 19-Dec-2025 22:12&lt;/p&gt;
    &lt;p&gt;Fri 19-Dec-2025 22:49&lt;/p&gt;
    &lt;p&gt;Congratulations on another amazing piece!&lt;/p&gt;
    &lt;p&gt;Sat 20-Dec-2025 01:13&lt;/p&gt;
    &lt;p&gt;Sat 20-Dec-2025 01:22&lt;/p&gt;
    &lt;p&gt;Sat 20-Dec-2025 03:54&lt;/p&gt;
    &lt;p&gt;Sat 20-Dec-2025 05:20&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46324702</guid><pubDate>Fri, 19 Dec 2025 11:38:54 +0000</pubDate></item><item><title>The FreeBSD Foundation's Laptop Support and Usability Project</title><link>https://github.com/FreeBSDFoundation/proj-laptop</link><description>&lt;doc fingerprint="f28eead3e9bb85fb"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Program Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Laptop Support and Usability&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Body&lt;/cell&gt;
        &lt;cell&gt;FreeBSD Foundation, and Quantum Leap Research&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Funding Status&lt;/cell&gt;
        &lt;cell&gt;Approved on September 27, 2024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Sponsor&lt;/cell&gt;
        &lt;cell&gt;Ed Maste&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Program Manager&lt;/cell&gt;
        &lt;cell&gt;Alice Sowerby&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Objectives&lt;/cell&gt;
        &lt;cell&gt;Deliver a package of improved or new FreeBSD functionality that, together, will ensure that it runs well ‚Äúout of the box‚Äù on a broad range of personal computing devices.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Organization goals&lt;/cell&gt;
        &lt;cell&gt;Laptop support and accessibility is a strategic priority for the FreeBSD Foundation to accelerate developer and corporate adoption, through: &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Output&lt;/cell&gt;
        &lt;cell&gt;Updates to FreeBSD 14.x and/or above that deliver contemporary WiFi, full audio, modern suspend and resume, improved graphics, Bluetooth, and other identified features. Documentation, and how-to guides for the new functionality.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Explore scope by area of functionality&lt;/p&gt;
    &lt;p&gt;Laptop and Desktop Working Group - (community owned)&lt;/p&gt;
    &lt;p&gt;Foundation blog about the Laptop Project&lt;/p&gt;
    &lt;p&gt;We have created discussion threads in the Desktop mailing list for key areas of the project:&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Power Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Hardware Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Audio Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Graphics Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] WiFi Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] System Management Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] Security Discussion Thread&lt;/p&gt;
    &lt;p&gt;[FF-laptop-LSU] User Testing Discussion Thread&lt;/p&gt;
    &lt;p&gt;Please come and join the discussion!&lt;/p&gt;
    &lt;p&gt;In total, $750,000 has been committed to a program of work to improve the experience of laptop users who run FreeBSD.&lt;/p&gt;
    &lt;p&gt;The program will start in Q4, 2024 and will likely run for 1-2 years.&lt;/p&gt;
    &lt;p&gt;The high-level scope was outlined by the FreeBSD Foundation with input from the community, including users such as program co-funder, Quantum Leap Research, and from laptop vendors including Dell, AMD and Framework.&lt;/p&gt;
    &lt;p&gt;The scope will be unpacked month by month as we make progress, focusing on where the most high-value functionality can be achieved with the resources and support that we have available. Our roadmap will contain work items that are candidates for future months.&lt;/p&gt;
    &lt;p&gt;No, these are high-level placeholders to help us visualise our intended order of work and to help share our plans with the community. The actual date of delivery on any item will be subject to change based on project progress and other factors.&lt;/p&gt;
    &lt;p&gt;The Foundation will be managing staff and a group of contracted FreeBSD developers to work on different functional areas to deliver regular updates to the laptop experience.&lt;/p&gt;
    &lt;p&gt;The FreeBSD community hosts a Laptop and Desktop Working Group where all interested parties can share their experiences, work in progress, and offer and receive help and support. You can also join the Desktop mailing list for more general updates. At present there is not a dedicated Laptop mailing list, this may change if there is community support for it.&lt;/p&gt;
    &lt;p&gt;Our target user is developers. However, we hope to be able to improve the experience for all users by reducing the need to "go under the hood" to set up, manage, and use FreeBSD on a laptop.&lt;/p&gt;
    &lt;p&gt;Broadly speaking this work is focused on laptop user experience. However, many of the areas that apply to laptops will also benefit the desktop user experience. We recommend engaging with the Laptop and Desktop Working Group to advocate for any desktop-specific work items.&lt;/p&gt;
    &lt;p&gt;We are mindful that UX is an important part of making FreeBSD functional and enjoyable for laptop users. We are framing the work as ‚Äúuser stories‚Äù that describe what a user wants to be able to accomplish and why. This is a user-focused approach to defining functional requirements.&lt;/p&gt;
    &lt;p&gt;There are several ways to keep yourself in the loop.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read the monthly updates that are posted into this repo.&lt;/item&gt;
      &lt;item&gt;Attend the Laptop and Desktop Working Group meetings. Work done as part of the program will be shared in these calls (these will also be recorded).&lt;/item&gt;
      &lt;item&gt;Check out the public roadmap on GitHub. We are developing a practice of keeping the program work up to date and available for anyone to see.&lt;/item&gt;
      &lt;item&gt;Sign up to the Desktop mailing list.&lt;/item&gt;
      &lt;item&gt;Sign up to the FreeBSD Foundation newsletter. All announcements about the program will be included in our updates.&lt;/item&gt;
      &lt;item&gt;Attend, or watch recordings of, the FreeBSD Foundation's Technology Team updates that are given at developer summits cohosted at conferences such as BSDCan, EuroBSDCon, and AsiaBSDCon.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using this repo and associated GH project board as tools for capturing the roadmap and progress on work items at a high-level. We are not using it for source code management. The repo and project are read-only for the public.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46326519</guid><pubDate>Fri, 19 Dec 2025 14:56:05 +0000</pubDate></item><item><title>Garage ‚Äì An S3 object store so reliable you can run it outside datacenters</title><link>https://garagehq.deuxfleurs.fr/</link><description>&lt;doc fingerprint="dd8215339a80dd6"&gt;
  &lt;main&gt;
    &lt;p&gt;An S3 object store so reliable you can run it outside datacenters&lt;/p&gt;
    &lt;p&gt;Made for redundancy&lt;/p&gt;
    &lt;p&gt;Each chunk of data is replicated in 3 zones&lt;/p&gt;
    &lt;p&gt;We made it lightweight and kept the efficiency in mind:&lt;/p&gt;
    &lt;p&gt;We ship a single dependency-free binary that runs on all Linux distributions&lt;/p&gt;
    &lt;p&gt;We are sysadmins, we know the value of operator-friendly software&lt;/p&gt;
    &lt;p&gt;We do not have a dedicated backbone, and neither do you,&lt;lb/&gt; so we made software that run over the Internet across multiple datacenters&lt;/p&gt;
    &lt;p&gt;We worked hard to keep requirements as low as possible:&lt;/p&gt;
    &lt;p&gt;We built Garage to suit your existing infrastructure:&lt;/p&gt;
    &lt;p&gt; Garage implements the Amazon S3 API&lt;lb/&gt;and thus is already compatible with many applications. &lt;/p&gt;
    &lt;p&gt;Garage leverages insights from recent research in distributed systems:&lt;/p&gt;
    &lt;p&gt;Garage has benefitted multiple times from public funding:&lt;/p&gt;
    &lt;p&gt;If you want to participate in funding Garage development, either through donation or support contract, please get in touch with us.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the European Union's Horizon 2021 research and innovation programme within the framework of the NGI-POINTER Project funded under grant agreement N√Ç¬∞ 871528.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the NGI Zero Entrust Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101069594.&lt;/p&gt;
    &lt;p&gt;This project has received funding from the NGI Zero Commons Fund, a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 101135429.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46326984</guid><pubDate>Fri, 19 Dec 2025 15:40:03 +0000</pubDate></item><item><title>Believe the Checkbook</title><link>https://robertgreiner.com/believe-the-checkbook/</link><description>&lt;doc fingerprint="b9a5135f4d93fc97"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Believe the Checkbook&lt;/head&gt;&lt;p&gt;AI companies talk as if engineering is over. Their acquisitions say the opposite.&lt;/p&gt;&lt;p&gt;Anthropic‚Äôs AI agent was the most prolific code contributor to Bun‚Äôs GitHub repository, submitting more merged pull requests than any human developer. Then Anthropic paid millions to acquire the human team anyway. The code was MIT-licensed; they could have forked it for free. Instead, they bought the people.&lt;/p&gt;&lt;p&gt;Everyone‚Äôs heard the line: ‚ÄúAI will write all the code; engineering as you know it is finished.‚Äù&lt;/p&gt;&lt;p&gt;Boards repeat it. CFOs love it. Some CTOs quietly use it to justify hiring freezes and stalled promotion paths.&lt;/p&gt;&lt;p&gt;The Bun acquisition blows a hole in that story.&lt;/p&gt;&lt;p&gt;Here‚Äôs a team whose project was open source, whose most active contributor was an AI agent, whose code Anthropic legally could have copied overnight. No negotiations. No equity. No retention packages.&lt;/p&gt;&lt;p&gt;Anthropic still fought competitors for the right to buy that group.&lt;/p&gt;&lt;p&gt;Publicly, AI companies talk like engineering is being automated away. Privately, they deploy millions of dollars to acquire engineers who already work with AI at full tilt. That contradiction is not a PR mistake. It is a signal.&lt;/p&gt;&lt;p&gt;The key constraint is obvious once you say it out loud. The bottleneck isn‚Äôt code production, it is judgment.&lt;/p&gt;&lt;p&gt;Anthropic‚Äôs own announcement barely talked about Bun‚Äôs existing codebase. It praised the team‚Äôs ability to rethink the JavaScript toolchain ‚Äúfrom first principles‚Äù.&lt;/p&gt;&lt;p&gt;That‚Äôs investor-speak for: we‚Äôre paying for how these people think, what they choose not to build, which tradeoffs they make under pressure. They didn‚Äôt buy a pile of code. They bought a track record of correct calls in a complex, fast-moving domain.&lt;/p&gt;&lt;p&gt;AI drastically increases the volume of code you can generate. It does almost nothing to increase your supply of people who know which ten lines matter, which pull request should never ship, and which ‚Äúclever‚Äù optimization will explode your latency or your reliability six months from now.&lt;/p&gt;&lt;p&gt;So when Anthropic‚Äôs own AI tops the contribution charts and they still decide the scarce asset is the human team, pay attention. That‚Äôs revealed preference.&lt;/p&gt;&lt;p&gt;Leaders don‚Äôt express their true beliefs in blog posts or conference quotes. They express them in hiring plans, acquisition targets, and compensation bands. If you want to understand what AI companies actually believe about engineering, follow the cap table, not the keynote.&lt;/p&gt;&lt;p&gt;So what do you do with this as a technical leader?&lt;/p&gt;&lt;p&gt;Stop using AI as an excuse to devalue your best knowledge workers. Use it to give them more leverage.&lt;/p&gt;&lt;p&gt;Treat AI as force multiplication for your highest-judgment people. The ones who can design systems, navigate ambiguity, shape strategy, and smell risk before it hits. They‚Äôll use AI to move faster, explore more options, and harden their decisions with better data.&lt;/p&gt;&lt;p&gt;Double down on developing judgment, not just syntax speed: architecture, performance modeling, incident response, security thinking, operational literacy. The skills Anthropic implicitly paid for when it bought a team famous for rethinking the stack, not just writing another bundler.&lt;/p&gt;&lt;p&gt;Be careful about starving your junior pipeline based on ‚Äúcoding is over‚Äù narratives. As AI pushes routine work down, the gap between senior and everyone else widens. Companies that maintain a healthy apprenticeship ladder will own the next generation of high-judgment engineers while everyone else hunts the same shrinking senior pool at auction.&lt;/p&gt;&lt;p&gt;Most important: calibrate your strategy to revealed preferences, not marketing copy. When someone‚Äôs AI writes more code than their engineers but they still pay millions for the engineers, believe the transaction, not the tweet.&lt;/p&gt;&lt;p&gt;How did you like this article?&lt;/p&gt;&lt;p&gt;Enjoyed this article? Subscribe to get weekly insights on AI, technology strategy, and leadership. Completely free.&lt;/p&gt;Subscribe for Free&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46327133</guid><pubDate>Fri, 19 Dec 2025 15:51:30 +0000</pubDate></item><item><title>Graphite is joining Cursor</title><link>https://cursor.com/blog/graphite</link><description>&lt;doc fingerprint="2586635ddc0e4d77"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Graphite is joining Cursor&lt;/head&gt;
    &lt;p&gt;The way developers write code looks different than it did a few years ago. But reviewing those changes, merging them safely, and collaborating on them has increasingly become the bottleneck for building production-grade software.&lt;/p&gt;
    &lt;p&gt;The team at Graphite has spent the past few years thinking deeply about these workflows and have built a code review platform used by hundreds of thousands of engineers at top engineering organizations. The boundary between where you write code and where you collaborate on it feels increasingly arbitrary, and there's a lot we think we can build by collapsing that distance.&lt;/p&gt;
    &lt;p&gt;We are excited to announce that Graphite has entered into a definitive agreement to be acquired by Cursor.&lt;/p&gt;
    &lt;p&gt;Graphite will continue to operate independently with the same team and product. Over the coming months, we'll explore connecting the two products in ways that we hope will feel natural: tighter integrations between local development and pull requests, smarter code review that learns from both systems, and some more radical ideas we can't share just yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46327206</guid><pubDate>Fri, 19 Dec 2025 15:57:01 +0000</pubDate></item><item><title>Performance Hints (2023)</title><link>https://abseil.io/fast/hints.html</link><description>&lt;doc fingerprint="ee11ba790e091688"&gt;
  &lt;main&gt;&lt;p&gt;Original version: 2023/07/27, last updated: 2025/12/16&lt;/p&gt;&lt;p&gt;Over the years, we (Jeff &amp;amp; Sanjay) have done a fair bit of diving into performance tuning of various pieces of code, and improving the performance of our software has been important from the very earliest days of Google, since it lets us do more for more users. We wrote this document as a way of identifying some general principles and specific techniques that we use when doing this sort of work, and tried to pick illustrative source code changes (change lists, or CLs) that provide examples of the various approaches and techniques. Most of the concrete suggestions below reference C++ types and CLs, but the general principles apply to other languages. The document focuses on general performance tuning in the context of a single binary, and does not cover distributed systems or machine learning (ML) hardware performance tuning (huge areas unto themselves). We hope others will find this useful.&lt;/p&gt;&lt;p&gt;Many of the examples in the document have code fragments that demonstrate the techniques (click the little triangles!). Note that some of these code fragments mention various internal Google codebase abstractions. We have included these anyway if we felt like the examples were self-contained enough to be understandable to those unfamiliar with the details of those abstractions.&lt;/p&gt;&lt;p&gt;Knuth is often quoted out of context as saying premature optimization is the root of all evil. The full quote reads: ‚ÄúWe should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.‚Äù This document is about that critical 3%, and a more compelling quote, again from Knuth, reads:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The improvement in speed from Example 2 to Example 2a is only about 12%, and many people would pronounce that insignificant. The conventional wisdom shared by many of today‚Äôs software engineers calls for ignoring efficiency in the small; but I believe this is simply an overreaction to the abuses they see being practiced by penny-wise-and-pound-foolish programmers, who can‚Äôt debug or maintain their ‚Äúoptimized‚Äù programs. In established engineering disciplines a 12% improvement, easily obtained, is never considered marginal; and I believe the same viewpoint should prevail in software engineering. Of course I wouldn‚Äôt bother making such optimizations on a one-shot job, but when it‚Äôs a question of preparing quality programs, I don‚Äôt want to restrict myself to tools that deny me such efficiencies.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Many people will say ‚Äúlet‚Äôs write down the code in as simple a way as possible and deal with performance later when we can profile‚Äù. However, this approach is often wrong:&lt;/p&gt;&lt;p&gt;Instead, we suggest that when writing code, try to choose the faster alternative if it does not impact readability/complexity of the code significantly.&lt;/p&gt;&lt;p&gt;If you can develop an intuition for how much performance might matter in the code you are writing, you can make a more informed decision (e.g., how much extra complexity is warranted in the name of performance). Some tips on estimating performance while you are writing code:&lt;/p&gt;&lt;p&gt;You can do a slightly deeper analysis when picking between options with potentially different performance characteristics by relying on back of the envelope calculations. Such calculations can quickly give a very rough estimate of the performance of different alternatives, and the results can be used to discard some of the alternatives without having to implement them.&lt;/p&gt;&lt;p&gt;Here is how such an estimation might work:&lt;/p&gt;&lt;p&gt;The following table, which is an updated version of a table from a 2007 talk at Stanford University (video of the 2007 talk no longer exists, but there is a video of a related 2011 Stanford talk that covers some of the same content) may be useful since it lists the types of operations to consider, and their rough cost:&lt;/p&gt;&lt;code&gt;L1 cache reference                             0.5 ns
L2 cache reference                             3 ns
Branch mispredict                              5 ns
Mutex lock/unlock (uncontended)               15 ns
Main memory reference                         50 ns
Compress 1K bytes with Snappy              1,000 ns
Read 4KB from SSD                         20,000 ns
Round trip within same datacenter         50,000 ns
Read 1MB sequentially from memory         64,000 ns
Read 1MB over 100 Gbps network           100,000 ns
Read 1MB from SSD                      1,000,000 ns
Disk seek                              5,000,000 ns
Read 1MB sequentially from disk       10,000,000 ns
Send packet CA-&amp;gt;Netherlands-&amp;gt;CA      150,000,000 ns
&lt;/code&gt;&lt;p&gt;The preceding table contains rough costs for some basic low-level operations. You may find it useful to also track estimated costs for higher-level operations relevant to your system. E.g., you might want to know the rough cost of a point read from your SQL database, the latency of interacting with a Cloud service, or the time to render a simple HTML page. If you don‚Äôt know the relevant cost of different operations, you can‚Äôt do decent back-of-the-envelope calculations!&lt;/p&gt;&lt;p&gt;As a rough approximation, a good quicksort algorithm makes log(N) passes over an array of size N. On each pass, the array contents will be streamed from memory into the processor cache, and the partition code will compare each element once to a pivot element. Let‚Äôs add up the dominant costs:&lt;/p&gt;&lt;p&gt;If necessary, we could refine our analysis to account for processor caches. This refinement is probably not needed since branch mispredictions are the dominant cost according to the analysis above, but we include it here anyway as another example. Let‚Äôs assume we have a 32MB L3 cache, and that the cost of transferring data from L3 cache to the processor is negligible. The L3 cache can hold 2^23 numbers, and therefore the last 22 passes can operate on the data resident in the L3 cache (the 23rd last pass brings data into the L3 cache and the remaining passes operate on that data.) That cuts down the memory transfer cost to 2.5 seconds (10 memory transfers of 4GB at 16GB/s) instead of 7.5 seconds (30 memory transfers).&lt;/p&gt;&lt;p&gt;Let‚Äôs compare two potential designs where the original images are stored on disk, and each image is approximately 1MB in size.&lt;/p&gt;&lt;p&gt;The preceding section gives some tips about how to think about performance when writing code without worrying too much about how to measure the performance impact of your choices. However, before you actually start making improvements, or run into a tradeoff involving various things like performance, simplicity, etc. you will want to measure or estimate potential performance benefits. Being able to measure things effectively is the number one tool you‚Äôll want to have in your arsenal when doing performance-related work.&lt;/p&gt;&lt;p&gt;As an aside, it‚Äôs worth pointing out that profiling code that you‚Äôre unfamiliar with can also be a good way of getting a general sense of the structure of the codebase and how it operates. Examining the source code of heavily involved routines in the dynamic call graph of a program can give you a high level sense of ‚Äúwhat happens‚Äù when running the code, which can then build your own confidence in making performance-improving changes in slightly unfamiliar code.&lt;/p&gt;&lt;p&gt;Many useful profiling tools are available. A useful tool to reach for first is pprof since it gives good high level performance information and is easy to use both locally and for code running in production. Also try perf if you want more detailed insight into performance.&lt;/p&gt;&lt;p&gt;Some tips for profiling:&lt;/p&gt;&lt;p&gt;Use a benchmark library to emit performance counter readings both for better precision, and to get more insight into program behavior.&lt;/p&gt;&lt;p&gt;You will often run into situations where your CPU profile is flat (there is no obvious big contributor to slowness). This can often happen when all low-hanging fruit has been picked. Here are some tips to consider if you find yourself in this situation:&lt;/p&gt;&lt;p&gt;Some of the techniques suggested below require changing data structures and function signatures, which may be disruptive to callers. Try to organize code so that the suggested performance improvements can be made inside an encapsulation boundary without affecting public interfaces. This will be easier if your modules are deep (significant functionality accessed via a narrow interface).&lt;/p&gt;&lt;p&gt;Widely used APIs come under heavy pressure to add features. Be careful when adding new features since these will constrain future implementations and increase cost unnecessarily for users who don‚Äôt need the new features. E.g., many C++ standard library containers promise iterator stability, which in typical implementations increases the number of allocations significantly, even though many users do not need pointer stability.&lt;/p&gt;&lt;p&gt;Some specific techniques are listed below. Consider carefully the performance benefits vs. any API usability issues introduced by such changes.&lt;/p&gt;&lt;p&gt;Provide bulk ops to reduce expensive API boundary crossings or to take advantage of algorithmic improvements.&lt;/p&gt;&lt;p&gt;Add bulk MemoryManager::LookupMany interface.&lt;/p&gt;&lt;p&gt;In addition to adding a bulk interface, this also simplified the signature for the new bulk variant: it turns out clients only needed to know if all the keys were found, so we can return a bool rather than a Status object.&lt;/p&gt;&lt;p&gt;memory_manager.h&lt;/p&gt;&lt;code&gt;class MemoryManager {
 public:
  ...
  util::StatusOr&amp;lt;LiveTensor&amp;gt; Lookup(const TensorIdProto&amp;amp; id);
&lt;/code&gt;&lt;code&gt;class MemoryManager {
 public:
  ...
  util::StatusOr&amp;lt;LiveTensor&amp;gt; Lookup(const TensorIdProto&amp;amp; id);

  // Lookup the identified tensors
  struct LookupKey {
    ClientHandle client;
    uint64 local_id;
  };
  bool LookupMany(absl::Span&amp;lt;const LookupKey&amp;gt; keys,
                  absl::Span&amp;lt;tensorflow::Tensor&amp;gt; tensors);
&lt;/code&gt;&lt;p&gt;Add bulk ObjectStore::DeleteRefs API to amortize locking overhead.&lt;/p&gt;&lt;p&gt;object_store.h&lt;/p&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
class ObjectStore {
 public:
  ...
  absl::Status DeleteRef(Ref);
&lt;/code&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
class ObjectStore {
 public:
  ...
  absl::Status DeleteRef(Ref);

  // Delete many references.  For each ref, if no other Refs point to the same
  // object, the object will be deleted.  Returns non-OK on any error.
  absl::Status DeleteRefs(absl::Span&amp;lt;const Ref&amp;gt; refs);
  ...
template &amp;lt;typename T&amp;gt;
absl::Status ObjectStore&amp;lt;T&amp;gt;::DeleteRefs(absl::Span&amp;lt;const Ref&amp;gt; refs) {
  util::Status result;
  absl::MutexLock l(&amp;amp;mu_);
  for (auto ref : refs) {
    result.Update(DeleteRefLocked(ref));
  }
  return result;
}
&lt;/code&gt;&lt;p&gt;memory_tracking.cc&lt;/p&gt;&lt;code&gt;void HandleBatch(int, const plaque::Batch&amp;amp; input) override {
  for (const auto&amp;amp; t : input) {
    auto in = In(t);
    PLAQUE_OP_ASSIGN_OR_RETURN(const auto&amp;amp; handles, in.handles());
    for (const auto handle : handles.value-&amp;gt;handles()) {
      PLAQUE_OP_RETURN_IF_ERROR(in_buffer_store_
                                    ? bstore_-&amp;gt;DeleteRef(handle)
                                    : tstore_-&amp;gt;DeleteRef(handle));
    }
  }
}
&lt;/code&gt;&lt;code&gt;void HandleBatch(int, const plaque::Batch&amp;amp; input) override {
  for (const auto&amp;amp; t : input) {
    auto in = In(t);
    PLAQUE_OP_ASSIGN_OR_RETURN(const auto&amp;amp; handles, in.handles());
    if (in_buffer_store_) {
      PLAQUE_OP_RETURN_IF_ERROR(
          bstore_-&amp;gt;DeleteRefs(handles.value-&amp;gt;handles()));
    } else {
      PLAQUE_OP_RETURN_IF_ERROR(
          tstore_-&amp;gt;DeleteRefs(handles.value-&amp;gt;handles()));
    }
  }
}
&lt;/code&gt;&lt;p&gt;Use Floyd's heap construction for efficient initialization.&lt;/p&gt;&lt;p&gt;Bulk initialization of a heap can be done in O(N) time, whereas adding one element at a time and updating the heap property after each addition requires O(N lg(N)) time.&lt;/p&gt;&lt;p&gt;Sometimes it is hard to change callers to use a new bulk API directly. In that case it might be beneficial to use a bulk API internally and cache the results for use in future non-bulk API calls:&lt;/p&gt;&lt;p&gt;Cache block decode results for use in future calls.&lt;/p&gt;&lt;p&gt;Each lookup needs to decode a whole block of K entries. Store the decoded entries in a cache and consult the cache on future lookups.&lt;/p&gt;&lt;p&gt;lexicon.cc&lt;/p&gt;&lt;code&gt;void GetTokenString(int pos, std::string* out) const {
  ...
  absl::FixedArray&amp;lt;LexiconEntry, 32&amp;gt; entries(pos + 1);

  // Decode all lexicon entries up to and including pos.
  for (int i = 0; i &amp;lt;= pos; ++i) {
    p = util::coding::TwoValuesVarint::Decode32(p, &amp;amp;entries[i].remaining,
                                                &amp;amp;entries[i].shared);
    entries[i].remaining_str = p;
    p += entries[i].remaining;  // remaining bytes trail each entry.
  }
&lt;/code&gt;&lt;code&gt;mutable std::vector&amp;lt;absl::InlinedVector&amp;lt;std::string, 16&amp;gt;&amp;gt; cache_;
...
void GetTokenString(int pos, std::string* out) const {
  ...
  DCHECK_LT(skentry, cache_.size());
  if (!cache_[skentry].empty()) {
    *out = cache_[skentry][pos];
    return;
  }
  ...
  // Init cache.
  ...
  const char* prev = p;
  for (int i = 0; i &amp;lt; block_sz; ++i) {
    uint32 shared, remaining;
    p = TwoValuesVarint::Decode32(p, &amp;amp;remaining, &amp;amp;shared);
    auto&amp;amp; cur = cache_[skentry].emplace_back();
    gtl::STLStringResizeUninitialized(&amp;amp;cur, remaining + shared);

    std::memcpy(cur.data(), prev, shared);
    std::memcpy(cur.data() + shared, p, remaining);
    prev = cur.data();
    p += remaining;
  }
  *out = cache_[skentry][pos];
&lt;/code&gt;&lt;p&gt;Prefer view types (e.g., &lt;code&gt;std::string_view&lt;/code&gt;, &lt;code&gt;std::Span&amp;lt;T&amp;gt;&lt;/code&gt;,
&lt;code&gt;absl::FunctionRef&amp;lt;R(Args...)&amp;gt;&lt;/code&gt;) for function arguments (unless ownership of the
data is being transferred). These types reduce copying, and allow callers to
pick their own container types (e.g., one caller might use &lt;code&gt;std::vector&lt;/code&gt; whereas
another one uses &lt;code&gt;absl::InlinedVector&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;For frequently called routines, sometimes it is useful to allow higher-level callers to pass in a data structure that they own or information that the called routine needs that the client already has. This can avoid the low-level routine being forced to allocate its own temporary data structure or recompute already-available information.&lt;/p&gt;&lt;p&gt;Add RPC_Stats::RecordRPC variant allowing client to pass in already available WallTime value.&lt;/p&gt;&lt;p&gt;rpc-stats.h&lt;/p&gt;&lt;code&gt;static void RecordRPC(const Name &amp;amp;name, const RPC_Stats_Measurement&amp;amp; m);
&lt;/code&gt;&lt;code&gt;static void RecordRPC(const Name &amp;amp;name, const RPC_Stats_Measurement&amp;amp; m,
                      WallTime now);
&lt;/code&gt;&lt;p&gt;clientchannel.cc&lt;/p&gt;&lt;code&gt;const WallTime now = WallTime_Now();
...
RPC_Stats::RecordRPC(stats_name, m);
&lt;/code&gt;&lt;code&gt;const WallTime now = WallTime_Now();
...
RPC_Stats::RecordRPC(stats_name, m, now);
&lt;/code&gt;&lt;p&gt;A type may be either thread-compatible (synchronized externally) or thread-safe (synchronized internally). Most generally used types should be thread-compatible. This way callers who do not need thread-safety don‚Äôt pay for it.&lt;/p&gt;&lt;p&gt;Make a class thread-compatible since callers are already synchronized.&lt;/p&gt;&lt;p&gt;hitless-transfer-phase.cc&lt;/p&gt;&lt;code&gt;TransferPhase HitlessTransferPhase::get() const {
  static CallsiteMetrics cm("HitlessTransferPhase::get");
  MonitoredMutexLock l(&amp;amp;cm, &amp;amp;mutex_);
  return phase_;
}
&lt;/code&gt;&lt;code&gt;TransferPhase HitlessTransferPhase::get() const { return phase_; }
&lt;/code&gt;&lt;p&gt;hitless-transfer-phase.cc&lt;/p&gt;&lt;code&gt;bool HitlessTransferPhase::AllowAllocate() const {
  static CallsiteMetrics cm("HitlessTransferPhase::AllowAllocate");
  MonitoredMutexLock l(&amp;amp;cm, &amp;amp;mutex_);
  return phase_ == TransferPhase::kNormal || phase_ == TransferPhase::kBrownout;
}
&lt;/code&gt;&lt;code&gt;bool HitlessTransferPhase::AllowAllocate() const {
  return phase_ == TransferPhase::kNormal || phase_ == TransferPhase::kBrownout;
}
&lt;/code&gt;&lt;p&gt;However if the typical use of a type needs synchronization, prefer to move the synchronization inside the type. This allows the synchronization mechanism to be tweaked as necessary to improve performance (e.g., sharding to reduce contention) without affecting callers.&lt;/p&gt;&lt;p&gt;The most critical opportunities for performance improvements come from algorithmic improvements, e.g., turning an O(N¬≤) algorithm to O(N lg(N)) or O(N), avoiding potentially exponential behavior, etc. These opportunities are rare in stable code, but are worth paying attention to when writing new code. A few examples that show such improvements to pre-existing code:&lt;/p&gt;&lt;p&gt;Add nodes to cycle detection structure in reverse post-order.&lt;/p&gt;&lt;p&gt;We were previously adding graph nodes and edges one at a time to a cycle-detection data structure, which required expensive work per edge. We now add the entire graph in reverse post-order, which makes cycle-detection trivial.&lt;/p&gt;&lt;p&gt;graphcycles.h&lt;/p&gt;&lt;code&gt;class GraphCycles : public util_graph::Graph {
 public:
  GraphCycles();
  ~GraphCycles() override;

  using Node = util_graph::Node;
&lt;/code&gt;&lt;code&gt;class GraphCycles : public util_graph::Graph {
 public:
  GraphCycles();
  ~GraphCycles() override;

  using Node = util_graph::Node;

  // InitFrom adds all the nodes and edges from src, returning true if
  // successful, false if a cycle is encountered.
  // REQUIRES: no nodes and edges have been added to GraphCycles yet.
  bool InitFrom(const util_graph::Graph&amp;amp; src);
&lt;/code&gt;&lt;p&gt;graphcycles.cc&lt;/p&gt;&lt;code&gt;bool GraphCycles::InitFrom(const util_graph::Graph&amp;amp; src) {
  ...
  // Assign ranks in topological order so we don't need any reordering during
  // initialization. For an acyclic graph, DFS leaves nodes in reverse
  // topological order, so we assign decreasing ranks to nodes as we leave them.
  Rank last_rank = n;
  auto leave = [&amp;amp;](util_graph::Node node) {
    DCHECK(r-&amp;gt;rank[node] == kMissingNodeRank);
    NodeInfo* nn = &amp;amp;r-&amp;gt;nodes[node];
    nn-&amp;gt;in = kNil;
    nn-&amp;gt;out = kNil;
    r-&amp;gt;rank[node] = --last_rank;
  };
  util_graph::DFSAll(src, std::nullopt, leave);

  // Add all the edges (detect cycles as we go).
  bool have_cycle = false;
  util_graph::PerEdge(src, [&amp;amp;](util_graph::Edge e) {
    DCHECK_NE(r-&amp;gt;rank[e.src], kMissingNodeRank);
    DCHECK_NE(r-&amp;gt;rank[e.dst], kMissingNodeRank);
    if (r-&amp;gt;rank[e.src] &amp;gt;= r-&amp;gt;rank[e.dst]) {
      have_cycle = true;
    } else if (!HasEdge(e.src, e.dst)) {
      EdgeListAddNode(r, &amp;amp;r-&amp;gt;nodes[e.src].out, e.dst);
      EdgeListAddNode(r, &amp;amp;r-&amp;gt;nodes[e.dst].in, e.src);
    }
  });
  if (have_cycle) {
    return false;
  } else {
    DCHECK(CheckInvariants());
    return true;
  }
}
&lt;/code&gt;&lt;p&gt;graph_partitioner.cc&lt;/p&gt;&lt;code&gt;absl::Status MergeGraph::Init() {
  const Graph&amp;amp; graph = *compiler_-&amp;gt;graph();
  clusters_.resize(graph.NodeLimit());
  graph.PerNode([&amp;amp;](Node node) {
    graph_-&amp;gt;AddNode(node);
    NodeList* n = new NodeList;
    n-&amp;gt;push_back(node);
    clusters_[node] = n;
  });
  absl::Status s;
  PerEdge(graph, [&amp;amp;](Edge e) {
    if (!s.ok()) return;
    if (graph_-&amp;gt;HasEdge(e.src, e.dst)) return;  // already added
    if (!graph_-&amp;gt;InsertEdge(e.src, e.dst)) {
      s = absl::InvalidArgumentError("cycle in the original graph");
    }
  });
  return s;
}
&lt;/code&gt;&lt;code&gt;absl::Status MergeGraph::Init() {
  const Graph&amp;amp; graph = *compiler_-&amp;gt;graph();
  if (!graph_-&amp;gt;InitFrom(graph)) {
    return absl::InvalidArgumentError("cycle in the original graph");
  }
  clusters_.resize(graph.NodeLimit());
  graph.PerNode([&amp;amp;](Node node) {
    NodeList* n = new NodeList;
    n-&amp;gt;push_back(node);
    clusters_[node] = n;
  });
  return absl::OkStatus();
}
&lt;/code&gt;&lt;p&gt;Replace the deadlock detection system built into a mutex implementation with a better algorithm.&lt;/p&gt;&lt;p&gt;Replaced deadlock detection algorithm by one that is ~50x as fast and scales to millions of mutexes without problem (the old algorithm relied on a 2K limit to avoid a performance cliff). The new code is based on the following paper: A dynamic topological sort algorithm for directed acyclic graphs David J. Pearce, Paul H. J. Kelly Journal of Experimental Algorithmics (JEA) JEA Homepage archive Volume 11, 2006, Article No. 1.7&lt;/p&gt;&lt;p&gt;The new algorithm takes O(|V|+|E|) space (instead of the O(|V|^2) bits needed by the older algorithm). Lock-acquisition order graphs are very sparse, so this is much less space. The algorithm is also quite simple: the core of it is ~100 lines of C++. Since the code now scales to much larger number of Mutexes, we were able to relax an artificial 2K limit, which uncovered a number of latent deadlocks in real programs.&lt;/p&gt;&lt;p&gt;Benchmark results: these were run in DEBUG mode since deadlock detection is mainly enabled in debug mode. The benchmark argument (/2k etc.) is the number of tracked nodes. At the default 2k limit of the old algorithm, the new algorithm takes only 0.5 microseconds per InsertEdge compared to 22 microseconds for the old algorithm. The new algorithm also easily scales to much larger graphs without problems whereas the old algorithm keels over quickly.&lt;/p&gt;&lt;code&gt;DEBUG: Benchmark            Time(ns)    CPU(ns) Iterations
----------------------------------------------------------
DEBUG: BM_StressTest/2k        23553      23566      29086
DEBUG: BM_StressTest/4k        45879      45909      15287
DEBUG: BM_StressTest/16k      776938     777472        817
&lt;/code&gt;&lt;code&gt;DEBUG: BM_StressTest/2k          392        393   10485760
DEBUG: BM_StressTest/4k          392        393   10485760
DEBUG: BM_StressTest/32k         407        407   10485760
DEBUG: BM_StressTest/256k        456        456   10485760
DEBUG: BM_StressTest/1M          534        534   10485760
&lt;/code&gt;&lt;p&gt;Replace an IntervalMap (with O(lg N) lookups) with a hash table (O(1) lookups).&lt;/p&gt;&lt;p&gt;The initial code was using IntervalMap because it seemed like the right data structure to support coalescing of adjacent blocks, but a hash table suffices since the adjacent block can be found by a hash table lookup. This (plus other changes in the CL) improve the performance of tpu::BestFitAllocator by ~4X.&lt;/p&gt;&lt;p&gt;best_fit_allocator.h&lt;/p&gt;&lt;code&gt;using Block = gtl::IntervalMap&amp;lt;int64, BlockState&amp;gt;::Entry;
...
// Map of pairs (address range, BlockState) with one entry for each allocation
// covering the range [0, allocatable_range_end_).  Adjacent kFree and
// kReserved blocks are coalesced. Adjacent kAllocated blocks are not
// coalesced.
gtl::IntervalMap&amp;lt;int64, BlockState&amp;gt; block_list_;

// Set of all free blocks sorted according to the allocation policy. Adjacent
// free blocks are coalesced.
std::set&amp;lt;Block, BlockSelector&amp;gt; free_list_;
&lt;/code&gt;&lt;code&gt;// A faster hash function for offsets in the BlockTable
struct OffsetHash {
  ABSL_ATTRIBUTE_ALWAYS_INLINE size_t operator()(int64 value) const {
    uint64 m = value;
    m *= uint64_t{0x9ddfea08eb382d69};
    return static_cast&amp;lt;uint64_t&amp;gt;(m ^ (m &amp;gt;&amp;gt; 32));
  }
};

// Hash table maps from block start address to block info.
// We include the length of the previous block in this info so we
// can find the preceding block to coalesce with.
struct HashTableEntry {
  BlockState state;
  int64 my_length;
  int64 prev_length;  // Zero if there is no previous block.
};
using BlockTable = absl::flat_hash_map&amp;lt;int64, HashTableEntry, OffsetHash&amp;gt;;
&lt;/code&gt;&lt;p&gt;Replace sorted-list intersection (O(N log N)) with hash table lookups (O(N)).&lt;/p&gt;&lt;p&gt;Old code to detect whether or not two nodes share a common source would get the sources for each node in sorted order and then do a sorted intersection. The new code places the sources for one node in a hash-table and then iterates over the other node's sources checking the hash-table.&lt;/p&gt;&lt;code&gt;name             old time/op  new time/op  delta
BM_CompileLarge   28.5s ¬± 2%   22.4s ¬± 2%  -21.61%  (p=0.008 n=5+5)
&lt;/code&gt;&lt;p&gt;Implement good hash function so that things are O(1) instead of O(N).&lt;/p&gt;&lt;p&gt;location.h&lt;/p&gt;&lt;code&gt;// Hasher for Location objects.
struct LocationHash {
  size_t operator()(const Location* key) const {
    return key != nullptr ? util_hash::Hash(key-&amp;gt;address()) : 0;
  }
};
&lt;/code&gt;&lt;code&gt;size_t HashLocation(const Location&amp;amp; loc);
...
struct LocationHash {
  size_t operator()(const Location* key) const {
    return key != nullptr ? HashLocation(*key) : 0;
  }
};
&lt;/code&gt;&lt;p&gt;location.cc&lt;/p&gt;&lt;code&gt;size_t HashLocation(const Location&amp;amp; loc) {
  util_hash::MurmurCat m;

  // Encode some simpler features into a single value.
  m.AppendAligned((loc.dynamic() ? 1 : 0)                    //
                  | (loc.append_shard_to_address() ? 2 : 0)  //
                  | (loc.is_any() ? 4 : 0)                   //
                  | (!loc.any_of().empty() ? 8 : 0)          //
                  | (loc.has_shardmap() ? 16 : 0)            //
                  | (loc.has_sharding() ? 32 : 0));

  if (loc.has_shardmap()) {
    m.AppendAligned(loc.shardmap().output() |
                    static_cast&amp;lt;uint64_t&amp;gt;(loc.shardmap().stmt()) &amp;lt;&amp;lt; 20);
  }
  if (loc.has_sharding()) {
    uint64_t num = 0;
    switch (loc.sharding().type_case()) {
      case Sharding::kModShard:
        num = loc.sharding().mod_shard();
        break;
      case Sharding::kRangeSplit:
        num = loc.sharding().range_split();
        break;
      case Sharding::kNumShards:
        num = loc.sharding().num_shards();
        break;
      default:
        num = 0;
        break;
    }
    m.AppendAligned(static_cast&amp;lt;uint64_t&amp;gt;(loc.sharding().type_case()) |
                    (num &amp;lt;&amp;lt; 3));
  }

  auto add_string = [&amp;amp;m](absl::string_view s) {
    if (!s.empty()) {
      m.Append(s.data(), s.size());
    }
  };

  add_string(loc.address());
  add_string(loc.lb_policy());

  // We do not include any_of since it is complicated to compute a hash
  // value that is not sensitive to order and duplication.
  return m.GetHash();
}
&lt;/code&gt;&lt;p&gt;Careful consideration of memory footprint and cache footprint of important data structures can often yield big savings. The data structures below focus on supporting common operations by touching fewer cache lines. Care taken here can (a) avoid expensive cache misses (b) reduce memory bus traffic, which speeds up both the program in question and anything else running on the same machine. They rely on some common techniques you may find useful when designing your own data structures.&lt;/p&gt;&lt;p&gt;Use compact representations for data that will be accessed often or that comprises a large portion of the application‚Äôs memory usage. A compact representation can significantly reduce memory usage and improve performance by touching fewer cache lines and reducing memory bus bandwidth usage. However, watch out for cache-line contention.&lt;/p&gt;&lt;p&gt;Carefully consider the memory layout of types that have a large memory or cache footprint.&lt;/p&gt;&lt;code&gt;enum class OpType : uint8_t { ...
}&lt;/code&gt; instead of &lt;code&gt;enum class OpType { ... }&lt;/code&gt;).&lt;p&gt;On modern 64-bit machines, pointers take up 64 bits. If you have a pointer-rich data structure, you can easily chew up lots of memory with indirections of T*. Instead, consider using integer indices into an array T[] or other data structure. Not only will the references be smaller (if the number of indices is small enough to fit in 32 or fewer bits), but the storage for all the T[] elements will be contiguous, often leading to better cache locality.&lt;/p&gt;&lt;p&gt;Avoid data structures that allocate a separate object per stored element (e.g., &lt;code&gt;std::map&lt;/code&gt;, &lt;code&gt;std::unordered_map&lt;/code&gt; in C++). Instead, consider types that use
chunked or flat representations to store multiple elements in close proximity in
memory (e.g., &lt;code&gt;std::vector&lt;/code&gt;, &lt;code&gt;absl::flat_hash_{map,set}&lt;/code&gt; in C++). Such types
tend to have much better cache behavior. Furthermore, they encounter less
allocator overhead.&lt;/p&gt;&lt;p&gt;One useful technique is to partition elements into chunks where each chunk can hold a fixed number of elements. This technique can reduce the cache footprint of a data structure significantly while preserving good asymptotic behavior.&lt;/p&gt;&lt;p&gt;For some data structures, a single chunk suffices to hold all elements (e.g., strings and vectors). Other types (e.g., &lt;code&gt;absl::flat_hash_map&lt;/code&gt;) also use this
technique.&lt;/p&gt;&lt;p&gt;Some container types are optimized for storing a small number of elements. These types provide space for a small number of elements at the top level and completely avoid allocations when the number of elements is small. This can be very helpful when instances of such types are constructed often (e.g., as stack variables in frequently executed code), or if many instances are live at the same time. If a container will typically contain a small number of elements consider using one of the inlined storage types, e.g., InlinedVector.&lt;/p&gt;&lt;p&gt;Caveat: if &lt;code&gt;sizeof(T)&lt;/code&gt; is large, inlined storage containers may not be the best
choice since the inlined backing store will be large.&lt;/p&gt;&lt;p&gt;Sometimes a nested map data structure can be replaced with a single-level map with a compound key. This can reduce the cost of lookups and insertions significantly.&lt;/p&gt;&lt;p&gt;Reduce allocations and improve cache footprint by converting btree&amp;lt;a,btree&amp;lt;b,c&amp;gt;&amp;gt; to btree&amp;lt;pair&amp;lt;a,b&amp;gt;,c&amp;gt;.&lt;/p&gt;&lt;p&gt;graph_splitter.cc&lt;/p&gt;&lt;code&gt;absl::btree_map&amp;lt;std::string, absl::btree_map&amp;lt;std::string, OpDef&amp;gt;&amp;gt; ops;
&lt;/code&gt;
&lt;code&gt;// The btree maps from {package_name, op_name} to its const Opdef*.
absl::btree_map&amp;lt;std::pair&amp;lt;absl::string_view, absl::string_view&amp;gt;,
                const OpDef*&amp;gt;
    ops;
&lt;/code&gt;
&lt;p&gt;Caveat: if the first map key is big, it might be better to stick with nested maps:&lt;/p&gt;&lt;p&gt;Switch to a nested map leads to 76% performance improvement in microbenchmark.&lt;/p&gt;&lt;p&gt;We previously had a single-level hash table where the key consisted of a (string) path and some other numeric sub-keys. Each path occurred in approximately 1000 keys on average. We split the hash table into two levels where the first level was keyed by the path and each second level hash table kept just the sub-key to data mapping for a particular path. This reduced the memory usage for storing paths by a factor of 1000, and also sped up accesses where many sub-keys for the same path were accessed together.&lt;/p&gt;&lt;p&gt;Arenas can help reduce memory allocation cost, but they also have the benefit of packing together independently allocated items next to each other, typically in fewer cache lines, and eliminating most destruction costs. They are likely most effective for complex data structures with many sub-objects. Consider providing an appropriate initial size for the arena since that can help reduce allocations.&lt;/p&gt;&lt;p&gt;Caveat: it is easy to misuse arenas by putting too many short-lived objects in a long-lived arena, which can unnecessarily bloat memory footprint.&lt;/p&gt;&lt;p&gt;If the domain of a map can be represented by a small integer or is an enum, or if the map will have very few elements, the map can sometimes be replaced by an array or a vector of some form.&lt;/p&gt;&lt;p&gt;Use an array instead of flat_map.&lt;/p&gt;&lt;p&gt;rtp_controller.h&lt;/p&gt;&lt;code&gt;const gtl::flat_map&amp;lt;int, int&amp;gt; payload_type_to_clock_frequency_;
&lt;/code&gt;
&lt;code&gt;// A map (implemented as a simple array) indexed by payload_type to clock freq
// for that paylaod type (or 0)
struct PayloadTypeToClockRateMap {
  int map[128];
};
...
const PayloadTypeToClockRateMap payload_type_to_clock_frequency_;
&lt;/code&gt;
&lt;p&gt;If the domain of a set can be represented by a small integer, the set can be replaced with a bit vector (InlinedBitVector is often a good choice). Set operations can also be nicely efficient on these representations using bitwise boolean operations (OR for union, AND for intersection, etc.).&lt;/p&gt;&lt;p&gt;Spanner placement system. Replace dense_hash_set&amp;lt;ZoneId&amp;gt; with a bit-vector with one bit per zone.&lt;/p&gt;&lt;p&gt;zone_set.h&lt;/p&gt;&lt;code&gt;class ZoneSet: public dense_hash_set&amp;lt;ZoneId&amp;gt; {
 public:
  ...
  bool Contains(ZoneId zone) const {
    return count(zone) &amp;gt; 0;
  }
&lt;/code&gt;
&lt;code&gt;class ZoneSet {
  ...
  // Returns true iff "zone" is contained in the set
  bool ContainsZone(ZoneId zone) const {
    return zone &amp;lt; b_.size() &amp;amp;&amp;amp; b_.get_bit(zone);
  }
  ...
 private:
  int size_;          // Number of zones inserted
  util::bitmap::InlinedBitVector&amp;lt;256&amp;gt; b_;
&lt;/code&gt;
&lt;p&gt;Benchmark results:&lt;/p&gt;&lt;code&gt;CPU: AMD Opteron (4 cores) dL1:64KB dL2:1024KB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_Evaluate/1                            960       676    +29.6%
BM_Evaluate/2                           1661      1138    +31.5%
BM_Evaluate/3                           2305      1640    +28.9%
BM_Evaluate/4                           3053      2135    +30.1%
BM_Evaluate/5                           3780      2665    +29.5%
BM_Evaluate/10                          7819      5739    +26.6%
BM_Evaluate/20                         17922     12338    +31.2%
BM_Evaluate/40                         36836     26430    +28.2%
&lt;/code&gt;

&lt;p&gt;Use bit matrix to keep track of reachability properties between operands instead of hash table.&lt;/p&gt;&lt;p&gt;hlo_computation.h&lt;/p&gt;&lt;code&gt;using TransitiveOperandMap =
    std::unordered_map&amp;lt;const HloInstruction*,
                       std::unordered_set&amp;lt;const HloInstruction*&amp;gt;&amp;gt;;
&lt;/code&gt;
&lt;code&gt;class HloComputation::ReachabilityMap {
  ...
  // dense id assignment from HloInstruction* to number
  tensorflow::gtl::FlatMap&amp;lt;const HloInstruction*, int&amp;gt; ids_;
  // matrix_(a,b) is true iff b is reachable from a
  tensorflow::core::Bitmap matrix_;
};
&lt;/code&gt;
&lt;p&gt;Memory allocation adds costs:&lt;/p&gt;&lt;p&gt;Garbage-collection runtimes sometimes obviate issue #3 by placing consecutive allocations sequentially in memory.&lt;/p&gt;&lt;p&gt;Reducing allocations increases benchmark throughput by 21%.&lt;/p&gt;&lt;p&gt;memory_manager.cc&lt;/p&gt;&lt;code&gt;LiveTensor::LiveTensor(tf::Tensor t, std::shared_ptr&amp;lt;const DeviceInfo&amp;gt; dinfo,
                       bool is_batched)
    : tensor(std::move(t)),
      device_info(dinfo ? std::move(dinfo) : std::make_shared&amp;lt;DeviceInfo&amp;gt;()),
      is_batched(is_batched) {
&lt;/code&gt;
&lt;code&gt;static const std::shared_ptr&amp;lt;DeviceInfo&amp;gt;&amp;amp; empty_device_info() {
  static std::shared_ptr&amp;lt;DeviceInfo&amp;gt;* result =
      new std::shared_ptr&amp;lt;DeviceInfo&amp;gt;(new DeviceInfo);
  return *result;
}

LiveTensor::LiveTensor(tf::Tensor t, std::shared_ptr&amp;lt;const DeviceInfo&amp;gt; dinfo,
                       bool is_batched)
    : tensor(std::move(t)), is_batched(is_batched) {
  if (dinfo) {
    device_info = std::move(dinfo);
  } else {
    device_info = empty_device_info();
  }
&lt;/code&gt;
&lt;p&gt;Use statically-allocated zero vector when possible rather than allocating a vector and filling it with zeroes.&lt;/p&gt;&lt;p&gt;embedding_executor_8bit.cc&lt;/p&gt;&lt;code&gt;// The actual implementation of the EmbeddingLookUpT using template parameters
// instead of object members to improve the performance.
template &amp;lt;bool Mean, bool SymmetricInputRange&amp;gt;
static tensorflow::Status EmbeddingLookUpT(...) {
    ...
  std::unique_ptr&amp;lt;tensorflow::quint8[]&amp;gt; zero_data(
      new tensorflow::quint8[max_embedding_width]);
  memset(zero_data.get(), 0, sizeof(tensorflow::quint8) * max_embedding_width);
&lt;/code&gt;
&lt;code&gt;// A size large enough to handle most embedding widths
static const int kTypicalMaxEmbedding = 256;
static tensorflow::quint8 static_zero_data[kTypicalMaxEmbedding];  // All zeroes
...
// The actual implementation of the EmbeddingLookUpT using template parameters
// instead of object members to improve the performance.
template &amp;lt;bool Mean, bool SymmetricInputRange&amp;gt;
static tensorflow::Status EmbeddingLookUpT(...) {
    ...
  std::unique_ptr&amp;lt;tensorflow::quint8[]&amp;gt; zero_data_backing(nullptr);

  // Get a pointer to a memory area with at least
  // "max_embedding_width" quint8 zero values.
  tensorflow::quint8* zero_data;
  if (max_embedding_width &amp;lt;= ARRAYSIZE(static_zero_data)) {
    // static_zero_data is big enough so we don't need to allocate zero data
    zero_data = &amp;amp;static_zero_data[0];
  } else {
    // static_zero_data is not big enough: we need to allocate zero data
    zero_data_backing =
        absl::make_unique&amp;lt;tensorflow::quint8[]&amp;gt;(max_embedding_width);
    memset(zero_data_backing.get(), 0,
           sizeof(tensorflow::quint8) * max_embedding_width);
    zero_data = zero_data_backing.get();
  }
&lt;/code&gt;

&lt;p&gt;Also, prefer stack allocation over heap allocation when object lifetime is bounded by the scope (although be careful with stack frame sizes for large objects).&lt;/p&gt;&lt;p&gt;When the maximum or expected maximum size of a vector (or some other container types) is known in advance, pre-size the container‚Äôs backing store (e.g., using &lt;code&gt;resize&lt;/code&gt; or &lt;code&gt;reserve&lt;/code&gt; in C++).&lt;/p&gt;&lt;p&gt;Pre-size a vector and fill it in, rather than N push_back operations.&lt;/p&gt;&lt;p&gt;indexblockdecoder.cc&lt;/p&gt;&lt;code&gt;for (int i = 0; i &amp;lt; ndocs-1; i++) {
  uint32 delta;
  ERRORCHECK(b-&amp;gt;GetRice(rice_base, &amp;amp;delta));
  docs_.push_back(DocId(my_shard_ + (base + delta) * num_shards_));
  base = base + delta + 1;
}
docs_.push_back(last_docid_);
&lt;/code&gt;
&lt;code&gt;docs_.resize(ndocs);
DocId* docptr = &amp;amp;docs_[0];
for (int i = 0; i &amp;lt; ndocs-1; i++) {
  uint32 delta;
  ERRORCHECK(b.GetRice(rice_base, &amp;amp;delta));
  *docptr = DocId(my_shard_ + (base + delta) * num_shards_);
  docptr++;
  base = base + delta + 1;
}
*docptr = last_docid_;
&lt;/code&gt;
&lt;p&gt;Caveat: Do not use &lt;code&gt;resize&lt;/code&gt; or &lt;code&gt;reserve&lt;/code&gt; to grow one element at a time since
that may lead to quadratic behavior. Also, if element construction is expensive,
prefer an initial &lt;code&gt;reserve&lt;/code&gt; call followed by several &lt;code&gt;push_back&lt;/code&gt; or
&lt;code&gt;emplace_back&lt;/code&gt; calls instead of an initial &lt;code&gt;resize&lt;/code&gt; since that will double the
number of constructor calls.&lt;/p&gt;&lt;p&gt;Avoid an extra copy when receiving a tensor via gRPC.&lt;/p&gt;&lt;p&gt;A benchmark that sends around 400KB tensors speeds up by ~10-15%:&lt;/p&gt;&lt;code&gt;Benchmark              Time(ns)    CPU(ns) Iterations
-----------------------------------------------------
BM_RPC/30/98k_mean    148764691 1369998944       1000
&lt;/code&gt;
&lt;code&gt;Benchmark              Time(ns)    CPU(ns) Iterations
-----------------------------------------------------
BM_RPC/30/98k_mean    131595940 1216998084       1000
&lt;/code&gt;

&lt;p&gt;Move large options structure rather than copying it.&lt;/p&gt;&lt;p&gt;index.cc&lt;/p&gt;&lt;code&gt;return search_iterators::DocPLIteratorFactory::Create(opts);
&lt;/code&gt;
&lt;code&gt;return search_iterators::DocPLIteratorFactory::Create(std::move(opts));
&lt;/code&gt;
&lt;p&gt;Use std::sort instead of std::stable_sort, which avoids an internal copy inside the stable sort implementation.&lt;/p&gt;&lt;p&gt;encoded-vector-hits.h&lt;/p&gt;&lt;code&gt;std::stable_sort(hits_.begin(), hits_.end(),
                 gtl::OrderByField(&amp;amp;HitWithPayloadOffset::docid));
&lt;/code&gt;
&lt;code&gt;struct HitWithPayloadOffset {
  search_iterators::LocalDocId64 docid;
  int first_payload_offset;  // offset into the payload vector.
  int num_payloads;

  bool operator&amp;lt;(const HitWithPayloadOffset&amp;amp; other) const {
    return (docid &amp;lt; other.docid) ||
           (docid == other.docid &amp;amp;&amp;amp;
            first_payload_offset &amp;lt; other.first_payload_offset);
  }
};
    ...
    std::sort(hits_.begin(), hits_.end());
&lt;/code&gt;
&lt;p&gt;A container or an object declared inside a loop will be recreated on every loop iteration. This can lead to expensive construction, destruction, and resizing. Hoisting the declaration outside the loop enables reuse and can provide a significant performance boost. (Compilers are often unable to do such hoisting on their own due to language semantics or their inability to ensure program equivalence.)&lt;/p&gt;&lt;p&gt;Hoist variable definition outside of loop iteration.&lt;/p&gt;&lt;p&gt;autofdo_profile_utils.h&lt;/p&gt;&lt;code&gt;auto iterator = absl::WrapUnique(sstable-&amp;gt;GetIterator());
while (!iterator-&amp;gt;done()) {
  T profile;
  if (!profile.ParseFromString(iterator-&amp;gt;value_view())) {
    return absl::InternalError(
        "Failed to parse mem_block to specified profile type.");
  }
  ...
  iterator-&amp;gt;Next();
}
&lt;/code&gt;
&lt;code&gt;auto iterator = absl::WrapUnique(sstable-&amp;gt;GetIterator());
T profile;
while (!iterator-&amp;gt;done()) {
  if (!profile.ParseFromString(iterator-&amp;gt;value_view())) {
    return absl::InternalError(
        "Failed to parse mem_block to specified profile type.");
  }
  ...
  iterator-&amp;gt;Next();
}
&lt;/code&gt;
&lt;p&gt;Define a protobuf variable outside a loop so that its allocated storage can be reused across loop iterations.&lt;/p&gt;&lt;p&gt;stats-router.cc&lt;/p&gt;&lt;code&gt;for (auto&amp;amp; r : routers_to_update) {
  ...
  ResourceRecord record;
  {
    MutexLock agg_lock(r.agg-&amp;gt;mutex());
    r.agg-&amp;gt;AddResourceRecordUsages(measure_indices, &amp;amp;record);
  }
  ...
}
&lt;/code&gt;
&lt;code&gt;ResourceRecord record;
for (auto&amp;amp; r : routers_to_update) {
  ...
  record.Clear();
  {
    MutexLock agg_lock(r.agg-&amp;gt;mutex());
    r.agg-&amp;gt;AddResourceRecordUsages(measure_indices, &amp;amp;record);
  }
  ...
}
&lt;/code&gt;
&lt;p&gt;Serialize to same std::string repeatedly.&lt;/p&gt;&lt;p&gt;program_rep.cc&lt;/p&gt;&lt;code&gt;std::string DeterministicSerialization(const proto2::Message&amp;amp; m) {
  std::string result;
  proto2::io::StringOutputStream sink(&amp;amp;result);
  proto2::io::CodedOutputStream out(&amp;amp;sink);
  out.SetSerializationDeterministic(true);
  m.SerializePartialToCodedStream(&amp;amp;out);
  return result;
}
&lt;/code&gt;
&lt;code&gt;absl::string_view DeterministicSerializationTo(const proto2::Message&amp;amp; m,
                                               std::string* scratch) {
  scratch-&amp;gt;clear();
  proto2::io::StringOutputStream sink(scratch);
  proto2::io::CodedOutputStream out(&amp;amp;sink);
  out.SetSerializationDeterministic(true);
  m.SerializePartialToCodedStream(&amp;amp;out);
  return absl::string_view(*scratch);
}
&lt;/code&gt;
&lt;p&gt;Caveat: protobuf, string, vector, containers etc. tend to grow to the size of the largest value ever stored in them. Therefore reconstructing them periodically (e.g., after every N uses) can help reduce memory requirements and reinitialization costs.&lt;/p&gt;&lt;p&gt;Perhaps one of the most effective categories of improving performance is avoiding work you don‚Äôt have to do. This can take many forms, including creating specialized paths through code for common cases that avoid more general expensive computation, precomputation, deferring work until it is really needed, hoisting work into less-frequently executed pieces of code, and other similar approaches. Below are many examples of this general approach, categorized into a few representative categories.&lt;/p&gt;&lt;p&gt;Often, code is written to cover all cases, but some subset of the cases are much simpler and more common than others. E.g., &lt;code&gt;vector::push_back&lt;/code&gt; usually has
enough space for the new element, but contains code to resize the underlying
storage when it does not. Some attention paid to the structure of code can help
make the common simple case faster without hurting uncommon case performance
significantly.&lt;/p&gt;&lt;p&gt;Make fast path cover more common cases.&lt;/p&gt;&lt;p&gt;Add handling of trailing single ASCII bytes, rather than only handling multiples of four bytes with this routine. This avoids calling the slower generic routine for all-ASCII strings that are, for example, 5 bytes.&lt;/p&gt;&lt;p&gt;utf8statetable.cc&lt;/p&gt;&lt;code&gt;// Scan a UTF-8 stringpiece based on state table.
// Always scan complete UTF-8 characters
// Set number of bytes scanned. Return reason for exiting
// OPTIMIZED for case of 7-bit ASCII 0000..007f all valid
int UTF8GenericScanFastAscii(const UTF8ScanObj* st, absl::string_view str,
                             int* bytes_consumed) {
                             ...
  int exit_reason;
  do {
    //  Skip 8 bytes of ASCII at a whack; no endianness issue
    while ((src_limit - src &amp;gt;= 8) &amp;amp;&amp;amp;
           (((UNALIGNED_LOAD32(src + 0) | UNALIGNED_LOAD32(src + 4)) &amp;amp;
             0x80808080) == 0)) {
      src += 8;
    }
    //  Run state table on the rest
    int rest_consumed;
    exit_reason = UTF8GenericScan(
        st, absl::ClippedSubstr(str, src - initial_src), &amp;amp;rest_consumed);
    src += rest_consumed;
  } while (exit_reason == kExitDoAgain);

  *bytes_consumed = src - initial_src;
  return exit_reason;
}
&lt;/code&gt;
&lt;code&gt;// Scan a UTF-8 stringpiece based on state table.
// Always scan complete UTF-8 characters
// Set number of bytes scanned. Return reason for exiting
// OPTIMIZED for case of 7-bit ASCII 0000..007f all valid
int UTF8GenericScanFastAscii(const UTF8ScanObj* st, absl::string_view str,
                             int* bytes_consumed) {
                             ...
  int exit_reason = kExitOK;
  do {
    //  Skip 8 bytes of ASCII at a whack; no endianness issue
    while ((src_limit - src &amp;gt;= 8) &amp;amp;&amp;amp;
           (((UNALIGNED_LOAD32(src + 0) | UNALIGNED_LOAD32(src + 4)) &amp;amp;
             0x80808080) == 0)) {
      src += 8;
    }
    while (src &amp;lt; src_limit &amp;amp;&amp;amp; Is7BitAscii(*src)) { // Skip ASCII bytes
      src++;
    }
    if (src &amp;lt; src_limit) {
      //  Run state table on the rest
      int rest_consumed;
      exit_reason = UTF8GenericScan(
          st, absl::ClippedSubstr(str, src - initial_src), &amp;amp;rest_consumed);
      src += rest_consumed;
    }
  } while (exit_reason == kExitDoAgain);

  *bytes_consumed = src - initial_src;
  return exit_reason;
}
&lt;/code&gt;
&lt;p&gt;Simpler fast paths for InlinedVector.&lt;/p&gt;&lt;p&gt;inlined_vector.h&lt;/p&gt;&lt;code&gt;auto Storage&amp;lt;T, N, A&amp;gt;::Resize(ValueAdapter values, size_type new_size) -&amp;gt; void {
  StorageView storage_view = MakeStorageView();

  IteratorValueAdapter&amp;lt;MoveIterator&amp;gt; move_values(
      MoveIterator(storage_view.data));

  AllocationTransaction allocation_tx(GetAllocPtr());
  ConstructionTransaction construction_tx(GetAllocPtr());

  absl::Span&amp;lt;value_type&amp;gt; construct_loop;
  absl::Span&amp;lt;value_type&amp;gt; move_construct_loop;
  absl::Span&amp;lt;value_type&amp;gt; destroy_loop;

  if (new_size &amp;gt; storage_view.capacity) {
  ...
  } else if (new_size &amp;gt; storage_view.size) {
    construct_loop = {storage_view.data + storage_view.size,
                      new_size - storage_view.size};
  } else {
    destroy_loop = {storage_view.data + new_size, storage_view.size - new_size};
  }
&lt;/code&gt;
&lt;code&gt;auto Storage&amp;lt;T, N, A&amp;gt;::Resize(ValueAdapter values, size_type new_size) -&amp;gt; void {
  StorageView storage_view = MakeStorageView();
  auto* const base = storage_view.data;
  const size_type size = storage_view.size;
  auto* alloc = GetAllocPtr();
  if (new_size &amp;lt;= size) {
    // Destroy extra old elements.
    inlined_vector_internal::DestroyElements(alloc, base + new_size,
                                             size - new_size);
  } else if (new_size &amp;lt;= storage_view.capacity) {
    // Construct new elements in place.
    inlined_vector_internal::ConstructElements(alloc, base + size, &amp;amp;values,
                                               new_size - size);
  } else {
  ...
  }
&lt;/code&gt;
&lt;p&gt;Fast path for common cases of initializing 1-D to 4-D tensors.&lt;/p&gt;&lt;p&gt;tensor_shape.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;class Shape&amp;gt;
TensorShapeBase&amp;lt;Shape&amp;gt;::TensorShapeBase(gtl::ArraySlice&amp;lt;int64&amp;gt; dim_sizes) {
  set_tag(REP16);
  set_data_type(DT_INVALID);
  set_ndims_byte(0);
  set_num_elements(1);
  for (int64 s : dim_sizes) {
    AddDim(internal::SubtleMustCopy(s));
  }
}
&lt;/code&gt;
&lt;code&gt;template &amp;lt;class Shape&amp;gt;
void TensorShapeBase&amp;lt;Shape&amp;gt;::InitDims(gtl::ArraySlice&amp;lt;int64&amp;gt; dim_sizes) {
  DCHECK_EQ(tag(), REP16);

  // Allow sizes that are under kint64max^0.25 so that 4-way multiplication
  // below cannot overflow.
  static const uint64 kMaxSmall = 0xd744;
  static_assert(kMaxSmall * kMaxSmall * kMaxSmall * kMaxSmall &amp;lt;= kint64max,
                "bad overflow check");
  bool large_size = false;
  for (auto s : dim_sizes) {
    if (s &amp;gt; kMaxSmall) {
      large_size = true;
      break;
    }
  }

  if (!large_size) {
    // Every size fits in 16 bits; use fast-paths for dims in {1,2,3,4}.
    uint16* dst = as16()-&amp;gt;dims_;
    switch (dim_sizes.size()) {
      case 1: {
        set_ndims_byte(1);
        const int64 size = dim_sizes[0];
        const bool neg = Set16(kIsPartial, dst, 0, size);
        set_num_elements(neg ? -1 : size);
        return;
      }
      case 2: {
        set_ndims_byte(2);
        const int64 size0 = dim_sizes[0];
        const int64 size1 = dim_sizes[1];
        bool neg = Set16(kIsPartial, dst, 0, size0);
        neg |= Set16(kIsPartial, dst, 1, size1);
        set_num_elements(neg ? -1 : (size0 * size1));
        return;
      }
      case 3: {
      ...
      }
      case 4: {
      ...
      }
    }
  }

  set_ndims_byte(0);
  set_num_elements(1);
  for (int64 s : dim_sizes) {
    AddDim(internal::SubtleMustCopy(s));
  }
}
&lt;/code&gt;
&lt;p&gt;Make varint parser fast path cover just the 1-byte case, instead of covering 1-byte and 2-byte cases.&lt;/p&gt;&lt;p&gt;Reducing the size of the (inlined) fast path reduces code size and icache pressure, which leads to improved performance.&lt;/p&gt;&lt;p&gt;parse_context.h&lt;/p&gt;&lt;code&gt;template &amp;lt;typename T&amp;gt;
PROTOBUF_NODISCARD const char* VarintParse(const char* p, T* out) {
  auto ptr = reinterpret_cast&amp;lt;const uint8_t*&amp;gt;(p);
  uint32_t res = ptr[0];
  if (!(res &amp;amp; 0x80)) {
    *out = res;
    return p + 1;
  }
  uint32_t byte = ptr[1];
  res += (byte - 1) &amp;lt;&amp;lt; 7;
  if (!(byte &amp;amp; 0x80)) {
    *out = res;
    return p + 2;
  }
  return VarintParseSlow(p, res, out);
}
&lt;/code&gt;
&lt;code&gt;template &amp;lt;typename T&amp;gt;
PROTOBUF_NODISCARD const char* VarintParse(const char* p, T* out) {
  auto ptr = reinterpret_cast&amp;lt;const uint8_t*&amp;gt;(p);
  uint32_t res = ptr[0];
  if (!(res &amp;amp; 0x80)) {
    *out = res;
    return p + 1;
  }
  return VarintParseSlow(p, res, out);
}
&lt;/code&gt;
&lt;p&gt;parse_context.cc&lt;/p&gt;&lt;code&gt;std::pair&amp;lt;const char*, uint32_t&amp;gt; VarintParseSlow32(const char* p,
                                                   uint32_t res) {
  for (std::uint32_t i = 2; i &amp;lt; 5; i++) {
  ...
}
...
std::pair&amp;lt;const char*, uint64_t&amp;gt; VarintParseSlow64(const char* p,
                                                   uint32_t res32) {
  uint64_t res = res32;
  for (std::uint32_t i = 2; i &amp;lt; 10; i++) {
  ...
}
&lt;/code&gt;
&lt;code&gt;std::pair&amp;lt;const char*, uint32_t&amp;gt; VarintParseSlow32(const char* p,
                                                   uint32_t res) {
  for (std::uint32_t i = 1; i &amp;lt; 5; i++) {
  ...
}
...
std::pair&amp;lt;const char*, uint64_t&amp;gt; VarintParseSlow64(const char* p,
                                                   uint32_t res32) {
  uint64_t res = res32;
  for (std::uint32_t i = 1; i &amp;lt; 10; i++) {
  ...
}
&lt;/code&gt;
&lt;p&gt;Skip significant work in RPC_Stats_Measurement addition if no errors have occurred.&lt;/p&gt;&lt;p&gt;rpc-stats.h&lt;/p&gt;&lt;code&gt;struct RPC_Stats_Measurement {
  ...
  double errors[RPC::NUM_ERRORS];
&lt;/code&gt;
&lt;code&gt;struct RPC_Stats_Measurement {
  ...
  double get_errors(int index) const { return errors[index]; }
  void set_errors(int index, double value) {
    errors[index] = value;
    any_errors_set = true;
  }
 private:
  ...
  // We make this private so that we can keep track of whether any of
  // these values have been set to non-zero values.
  double errors[RPC::NUM_ERRORS];
  bool any_errors_set;  // True iff any of the errors[i] values are non-zero
&lt;/code&gt;
&lt;p&gt;rpc-stats.cc&lt;/p&gt;&lt;code&gt;void RPC_Stats_Measurement::operator+=(const RPC_Stats_Measurement&amp;amp; x) {
  ...
  for (int i = 0; i &amp;lt; RPC::NUM_ERRORS; ++i) {
    errors[i] += x.errors[i];
  }
}
&lt;/code&gt;
&lt;code&gt;void RPC_Stats_Measurement::operator+=(const RPC_Stats_Measurement&amp;amp; x) {
  ...
  if (x.any_errors_set) {
    for (int i = 0; i &amp;lt; RPC::NUM_ERRORS; ++i) {
      errors[i] += x.errors[i];
    }
    any_errors_set = true;
  }
}
&lt;/code&gt;
&lt;p&gt;Do array lookup on first byte of string to often avoid fingerprinting full string.&lt;/p&gt;&lt;p&gt;soft-tokens-helper.cc&lt;/p&gt;&lt;code&gt;bool SoftTokensHelper::IsSoftToken(const StringPiece&amp;amp; token) const {
  return soft_tokens_.find(Fingerprint(token.data(), token.size())) !=
      soft_tokens_.end();
}
&lt;/code&gt;
&lt;p&gt;soft-tokens-helper.h&lt;/p&gt;&lt;code&gt;class SoftTokensHelper {
 ...
 private:
  ...
  // Since soft tokens are mostly punctuation-related, for performance
  // purposes, we keep an array filter_.  filter_[i] is true iff any
  // of the soft tokens start with the byte value 'i'.  This avoids
  // fingerprinting a term in the common case, since we can just do an array
  // lookup based on the first byte, and if filter_[b] is false, then
  // we can return false immediately.
  bool          filter_[256];
  ...
};

inline bool SoftTokensHelper::IsSoftToken(const StringPiece&amp;amp; token) const {
  if (token.size() &amp;gt;= 1) {
    char first_char = token.data()[0];
    if (!filter_[first_char]) {
      return false;
    }
  }
  return IsSoftTokenFallback(token);
}
&lt;/code&gt;
&lt;p&gt;soft-tokens-helper.cc&lt;/p&gt;&lt;code&gt;bool SoftTokensHelper::IsSoftTokenFallback(const StringPiece&amp;amp; token) const {
  return soft_tokens_.find(Fingerprint(token.data(), token.size())) !=
      soft_tokens_.end();
}
&lt;/code&gt;
&lt;p&gt;Precompute a TensorFlow graph execution node property that allows us to quickly rule out certain unusual cases.&lt;/p&gt;&lt;p&gt;executor.cc&lt;/p&gt;&lt;code&gt;struct NodeItem {
  ...
  bool kernel_is_expensive = false;  // True iff kernel-&amp;gt;IsExpensive()
  bool kernel_is_async = false;      // True iff kernel-&amp;gt;AsAsync() != nullptr
  bool is_merge = false;             // True iff IsMerge(node)
  ...
  if (IsEnter(node)) {
  ...
  } else if (IsExit(node)) {
  ...
  } else if (IsNextIteration(node)) {
  ...
  } else {
    // Normal path for most nodes
    ...
  }
&lt;/code&gt;
&lt;code&gt;struct NodeItem {
  ...
  bool kernel_is_expensive : 1;  // True iff kernel-&amp;gt;IsExpensive()
  bool kernel_is_async : 1;      // True iff kernel-&amp;gt;AsAsync() != nullptr
  bool is_merge : 1;             // True iff IsMerge(node)
  bool is_enter : 1;             // True iff IsEnter(node)
  bool is_exit : 1;              // True iff IsExit(node)
  bool is_control_trigger : 1;   // True iff IsControlTrigger(node)
  bool is_sink : 1;              // True iff IsSink(node)
  // True iff IsEnter(node) || IsExit(node) || IsNextIteration(node)
  bool is_enter_exit_or_next_iter : 1;
  ...
  if (!item-&amp;gt;is_enter_exit_or_next_iter) {
    // Fast path for nodes types that don't need special handling
    DCHECK_EQ(input_frame, output_frame);
    ...
  } else if (item-&amp;gt;is_enter) {
  ...
  } else if (item-&amp;gt;is_exit) {
  ...
  } else {
    DCHECK(IsNextIteration(node));
    ...
  }
&lt;/code&gt;
&lt;p&gt;Precompute 256 element array and use during trigram initialization.&lt;/p&gt;&lt;p&gt;byte_trigram_classifier.cc&lt;/p&gt;&lt;code&gt;void ByteTrigramClassifier::VerifyModel(void) const {
  ProbT class_sums[num_classes_];
  for (int cls = 0; cls &amp;lt; num_classes_; cls++) {
    class_sums[cls] = 0;
  }
  for (ByteNgramId id = 0; id &amp;lt; trigrams_.num_trigrams(); id++) {
    for (int cls = 0; cls &amp;lt; num_classes_; ++cls) {
      class_sums[cls] += Prob(trigram_probs_[id].log_probs[cls]);
    }
  }
  ...
}                         
&lt;/code&gt;
&lt;code&gt;void ByteTrigramClassifier::VerifyModel(void) const {
  CHECK_EQ(sizeof(ByteLogProbT), 1);
  ProbT fast_prob[256];
  for (int b = 0; b &amp;lt; 256; b++) {
    fast_prob[b] = Prob(static_cast&amp;lt;ByteLogProbT&amp;gt;(b));
  }

  ProbT class_sums[num_classes_];
  for (int cls = 0; cls &amp;lt; num_classes_; cls++) {
    class_sums[cls] = 0;
  }
  for (ByteNgramId id = 0; id &amp;lt; trigrams_.num_trigrams(); id++) {
    for (int cls = 0; cls &amp;lt; num_classes_; ++cls) {
      class_sums[cls] += fast_prob[trigram_probs_[id].log_probs[cls]];
    }
  }
  ...
}                         
&lt;/code&gt;
&lt;p&gt;General advice: check for malformed inputs at module boundaries instead of repeating checks internally.&lt;/p&gt;&lt;p&gt;Move bounds computation outside loop.&lt;/p&gt;&lt;p&gt;literal_linearizer.cc&lt;/p&gt;&lt;code&gt;for (int64 i = 0; i &amp;lt; src_shape.dimensions(dimension_numbers.front());
     ++i) {
&lt;/code&gt;
&lt;code&gt;int64 dim_front = src_shape.dimensions(dimension_numbers.front());
const uint8* src_buffer_data = src_buffer.data();
uint8* dst_buffer_data = dst_buffer.data();
for (int64 i = 0; i &amp;lt; dim_front; ++i) {
&lt;/code&gt;
&lt;p&gt;Defer GetSubSharding call until needed, which reduces 43 seconds of CPU time to 2 seconds.&lt;/p&gt;&lt;p&gt;sharding_propagation.cc&lt;/p&gt;&lt;code&gt;HloSharding alternative_sub_sharding =
    user.sharding().GetSubSharding(user.shape(), {i});
if (user.operand(i) == &amp;amp;instruction &amp;amp;&amp;amp;
    hlo_sharding_util::IsShardingMoreSpecific(alternative_sub_sharding,
                                              sub_sharding)) {
  sub_sharding = alternative_sub_sharding;
}
&lt;/code&gt;
&lt;code&gt;if (user.operand(i) == &amp;amp;instruction) {
  // Only evaluate GetSubSharding if this operand is of interest,
  // as it is relatively expensive.
  HloSharding alternative_sub_sharding =
      user.sharding().GetSubSharding(user.shape(), {i});
  if (hlo_sharding_util::IsShardingMoreSpecific(
          alternative_sub_sharding, sub_sharding)) {
    sub_sharding = alternative_sub_sharding;
  }
}
&lt;/code&gt;
&lt;p&gt;Don't update stats eagerly; compute them on demand.&lt;/p&gt;&lt;p&gt;Do not update stats on the very frequent allocation/deallocation calls. Instead, compute stats on demand when the much less frequently called Stats() method is invoked.&lt;/p&gt;&lt;p&gt;Preallocate 10 nodes not 200 for query handling in Google's web server.&lt;/p&gt;&lt;p&gt;A simple change that reduced web server's CPU usage by 7.5%.&lt;/p&gt;&lt;p&gt;querytree.h&lt;/p&gt;&lt;code&gt;static const int kInitParseTreeSize = 200;   // initial size of querynode pool
&lt;/code&gt;
&lt;code&gt;static const int kInitParseTreeSize = 10;   // initial size of querynode pool
&lt;/code&gt;
&lt;p&gt;Change search order for 19% throughput improvement.&lt;/p&gt;&lt;p&gt;An old search system (circa 2000) had two tiers: one contained a full-text index, and the other tier contained just the index for the title and anchor terms. We used to search the smaller title/anchor tier first. Counter-intuitively, we found that it is cheaper to search the larger full-text index tier first since if we reach the end of the full-text tier, we can entirely skip searching the title/anchor tier (a subset of the full-text tier). This happened reasonably often and allowed us to reduce the average number of disk seeks to process a query.&lt;/p&gt;&lt;p&gt;See discussion of title and anchor text handling in The Anatomy of a Large-Scale Hypertextual Web Search Engine for background information.&lt;/p&gt;&lt;p&gt;A particular performance-sensitive call-site may not need the full generality provided by a general-purpose library. Consider writing specialized code in such cases instead of calling the general-purpose code if it provides a performance improvement.&lt;/p&gt;&lt;p&gt;Custom printing code for Histogram class is 4x as fast as sprintf.&lt;/p&gt;&lt;p&gt;This code is performance sensitive because it is invoked when monitoring systems gather statistics from various servers.&lt;/p&gt;&lt;p&gt;histogram_export.cc&lt;/p&gt;&lt;code&gt;void Histogram::PopulateBuckets(const string &amp;amp;prefix,
                                expvar::MapProto *const var) const {
                                ...
  for (int i = min_bucket; i &amp;lt;= max_bucket; ++i) {
    const double count = BucketCount(i);
    if (!export_empty_buckets &amp;amp;&amp;amp; count == 0.0) continue;
    acc += count;
    // The label format of exported buckets for discrete histograms
    // specifies an inclusive upper bound, which is the same as in
    // the original Histogram implementation.  This format is not
    // applicable to non-discrete histograms, so a half-open interval
    // is used for them, with "_" instead of "-" as a separator to
    // make possible to distinguish the formats.
    string key =
        options_.export_cumulative_counts() ?
            StringPrintf("%.12g", boundaries_-&amp;gt;BucketLimit(i)) :
        options_.discrete() ?
            StringPrintf("%.0f-%.0f",
                         ceil(boundaries_-&amp;gt;BucketStart(i)),
                         ceil(boundaries_-&amp;gt;BucketLimit(i)) - 1.0) :
            StringPrintf("%.12g_%.12g",
                         boundaries_-&amp;gt;BucketStart(i),
                         boundaries_-&amp;gt;BucketLimit(i));
    EscapeMapKey(&amp;amp;key);
    const double value = options_.export_cumulative_counts() ? acc : count;
    expvar::AddMapFloat(StrCat(prefix,
                               options_.export_bucket_key_prefix(),
                               key),
                        value * count_mult,
                        var);
  }
&lt;/code&gt;
&lt;code&gt;// Format "val" according to format.  If "need_escape" is true, then the
// format can produce output with a '.' in it, and the result will be escaped.
// If "need_escape" is false, then the caller guarantees that format is
// such that the resulting number will not have any '.' characters and
// therefore we can avoid calling EscapeKey.
// The function is free to use "*scratch" for scratch space if necessary,
// and the resulting StringPiece may point into "*scratch".
static StringPiece FormatNumber(const char* format,
                                bool need_escape,
                                double val, string* scratch) {
  // This routine is specialized to work with only a limited number of formats
  DCHECK(StringPiece(format) == "%.0f" || StringPiece(format) == "%.12g");

  scratch-&amp;gt;clear();
  if (val == trunc(val) &amp;amp;&amp;amp; val &amp;gt;= kint32min &amp;amp;&amp;amp; val &amp;lt;= kint32max) {
    // An integer for which we can just use StrAppend
    StrAppend(scratch, static_cast&amp;lt;int32&amp;gt;(val));
    return StringPiece(*scratch);
  } else if (isinf(val)) {
    // Infinity, represent as just 'inf'.
    return StringPiece("inf", 3);
  } else {
    // Format according to "format", and possibly escape.
    StringAppendF(scratch, format, val);
    if (need_escape) {
      EscapeMapKey(scratch);
    } else {
      DCHECK(!StringPiece(*scratch).contains("."));
    }
    return StringPiece(*scratch);
  }
}
...
void Histogram::PopulateBuckets(const string &amp;amp;prefix,
                                expvar::MapProto *const var) const {
                                ...
  const string full_key_prefix = StrCat(prefix,
                                        options_.export_bucket_key_prefix());
  string key = full_key_prefix;  // Keys will start with "full_key_prefix".
  string start_scratch;
  string limit_scratch;
  const bool cumul_counts = options_.export_cumulative_counts();
  const bool discrete = options_.discrete();
  for (int i = min_bucket; i &amp;lt;= max_bucket; ++i) {
    const double count = BucketCount(i);
    if (!export_empty_buckets &amp;amp;&amp;amp; count == 0.0) continue;
    acc += count;
    // The label format of exported buckets for discrete histograms
    // specifies an inclusive upper bound, which is the same as in
    // the original Histogram implementation.  This format is not
    // applicable to non-discrete histograms, so a half-open interval
    // is used for them, with "_" instead of "-" as a separator to
    // make possible to distinguish the formats.
    key.resize(full_key_prefix.size());  // Start with full_key_prefix.
    DCHECK_EQ(key, full_key_prefix);

    const double limit = boundaries_-&amp;gt;BucketLimit(i);
    if (cumul_counts) {
      StrAppend(&amp;amp;key, FormatNumber("%.12g", true, limit, &amp;amp;limit_scratch));
    } else {
      const double start = boundaries_-&amp;gt;BucketStart(i);
      if (discrete) {
        StrAppend(&amp;amp;key,
                  FormatNumber("%.0f", false, ceil(start), &amp;amp;start_scratch),
                  "-",
                  FormatNumber("%.0f", false, ceil(limit) - 1.0,
                               &amp;amp;limit_scratch));
      } else {
        StrAppend(&amp;amp;key,
                  FormatNumber("%.12g", true, start, &amp;amp;start_scratch),
                  "_",
                  FormatNumber("%.12g", true, limit, &amp;amp;limit_scratch));
      }
    }
    const double value = cumul_counts ? acc : count;

    // Add to map var
    expvar::AddMapFloat(key, value * count_mult, var);
  }
}
&lt;/code&gt;
&lt;p&gt;Add specializations for VLOG(1), VLOG(2), ‚Ä¶ for speed and smaller code size.&lt;/p&gt;&lt;p&gt;&lt;code&gt;VLOG&lt;/code&gt; is a heavily used macro throughout the code base. This change avoids
passing an extra integer constant at nearly every call site (if the log level is
constant at the call site, as it almost always is, as in &lt;code&gt;VLOG(1) &amp;lt;&amp;lt; ...&lt;/code&gt;),
which saves code space.&lt;/p&gt;&lt;p&gt;vlog_is_on.h&lt;/p&gt;&lt;code&gt;class VLogSite final {
 public:
  ...
  bool IsEnabled(int level) {
    int stale_v = v_.load(std::memory_order_relaxed);
    if (ABSL_PREDICT_TRUE(level &amp;gt; stale_v)) {
      return false;
    }

    // We put everything other than the fast path, i.e. vlogging is initialized
    // but not on, behind an out-of-line function to reduce code size.
    return SlowIsEnabled(stale_v, level);
  }
  ...
 private:
  ...
  ABSL_ATTRIBUTE_NOINLINE
  bool SlowIsEnabled(int stale_v, int level);
  ...
};
&lt;/code&gt;
&lt;code&gt;class VLogSite final {
 public:
  ...
  bool IsEnabled(int level) {
    int stale_v = v_.load(std::memory_order_relaxed);
    if (ABSL_PREDICT_TRUE(level &amp;gt; stale_v)) {
      return false;
    }

    // We put everything other than the fast path, i.e. vlogging is initialized
    // but not on, behind an out-of-line function to reduce code size.
    // "level" is almost always a call-site constant, so we can save a bit
    // of code space by special-casing for levels 1, 2, and 3.
#if defined(__has_builtin) &amp;amp;&amp;amp; __has_builtin(__builtin_constant_p)
    if (__builtin_constant_p(level)) {
      if (level == 0) return SlowIsEnabled0(stale_v);
      if (level == 1) return SlowIsEnabled1(stale_v);
      if (level == 2) return SlowIsEnabled2(stale_v);
      if (level == 3) return SlowIsEnabled3(stale_v);
      if (level == 4) return SlowIsEnabled4(stale_v);
      if (level == 5) return SlowIsEnabled5(stale_v);
    }
#endif
    return SlowIsEnabled(stale_v, level);
    ...
 private:
  ...
  ABSL_ATTRIBUTE_NOINLINE
  bool SlowIsEnabled(int stale_v, int level);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled0(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled1(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled2(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled3(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled4(int stale_v);
  ABSL_ATTRIBUTE_NOINLINE bool SlowIsEnabled5(int stale_v);
  ...
};
&lt;/code&gt;
&lt;p&gt;vlog_is_on.cc&lt;/p&gt;&lt;code&gt;bool VLogSite::SlowIsEnabled0(int stale_v) { return SlowIsEnabled(stale_v, 0); }
bool VLogSite::SlowIsEnabled1(int stale_v) { return SlowIsEnabled(stale_v, 1); }
bool VLogSite::SlowIsEnabled2(int stale_v) { return SlowIsEnabled(stale_v, 2); }
bool VLogSite::SlowIsEnabled3(int stale_v) { return SlowIsEnabled(stale_v, 3); }
bool VLogSite::SlowIsEnabled4(int stale_v) { return SlowIsEnabled(stale_v, 4); }
bool VLogSite::SlowIsEnabled5(int stale_v) { return SlowIsEnabled(stale_v, 5); }
&lt;/code&gt;
&lt;p&gt;Replace RE2 call with a simple prefix match when possible.&lt;/p&gt;&lt;p&gt;read_matcher.cc&lt;/p&gt;&lt;code&gt;enum MatchItemType {
  MATCH_TYPE_INVALID,
  MATCH_TYPE_RANGE,
  MATCH_TYPE_EXACT,
  MATCH_TYPE_REGEXP,
};
&lt;/code&gt;
&lt;code&gt;enum MatchItemType {
  MATCH_TYPE_INVALID,
  MATCH_TYPE_RANGE,
  MATCH_TYPE_EXACT,
  MATCH_TYPE_REGEXP,
  MATCH_TYPE_PREFIX,   // Special type for regexp ".*"
};
&lt;/code&gt;
&lt;p&gt;read_matcher.cc&lt;/p&gt;&lt;code&gt;p-&amp;gt;type = MATCH_TYPE_REGEXP;
&lt;/code&gt;
&lt;code&gt;term.NonMetaPrefix().CopyToString(&amp;amp;p-&amp;gt;prefix);
if (term.RegexpSuffix() == ".*") {
  // Special case for a regexp that matches anything, so we can
  // bypass RE2::FullMatch
  p-&amp;gt;type = MATCH_TYPE_PREFIX;
} else {
  p-&amp;gt;type = MATCH_TYPE_REGEXP;
&lt;/code&gt;
&lt;p&gt;Use StrCat rather than StringPrintf to format IP addresses.&lt;/p&gt;&lt;p&gt;ipaddress.cc&lt;/p&gt;&lt;code&gt;string IPAddress::ToString() const {
  char buf[INET6_ADDRSTRLEN];

  switch (address_family_) {
    case AF_INET:
      CHECK(inet_ntop(AF_INET, &amp;amp;addr_.addr4, buf, INET6_ADDRSTRLEN) != NULL);
      return buf;
    case AF_INET6:
      CHECK(inet_ntop(AF_INET6, &amp;amp;addr_.addr6, buf, INET6_ADDRSTRLEN) != NULL);
      return buf;
    case AF_UNSPEC:
      LOG(DFATAL) &amp;lt;&amp;lt; "Calling ToString() on an empty IPAddress";
      return "";
    default:
      LOG(FATAL) &amp;lt;&amp;lt; "Unknown address family " &amp;lt;&amp;lt; address_family_;
  }
}
...
string IPAddressToURIString(const IPAddress&amp;amp; ip) {
  switch (ip.address_family()) {
    case AF_INET6:
      return StringPrintf("[%s]", ip.ToString().c_str());
    default:
      return ip.ToString();
  }
}
...
string SocketAddress::ToString() const {
  return IPAddressToURIString(host_) + StringPrintf(":%u", port_);
}
&lt;/code&gt;
&lt;code&gt;string IPAddress::ToString() const {
  char buf[INET6_ADDRSTRLEN];

  switch (address_family_) {
    case AF_INET: {
      uint32 addr = gntohl(addr_.addr4.s_addr);
      int a1 = static_cast&amp;lt;int&amp;gt;((addr &amp;gt;&amp;gt; 24) &amp;amp; 0xff);
      int a2 = static_cast&amp;lt;int&amp;gt;((addr &amp;gt;&amp;gt; 16) &amp;amp; 0xff);
      int a3 = static_cast&amp;lt;int&amp;gt;((addr &amp;gt;&amp;gt; 8) &amp;amp; 0xff);
      int a4 = static_cast&amp;lt;int&amp;gt;(addr &amp;amp; 0xff);
      return StrCat(a1, ".", a2, ".", a3, ".", a4);
    }
    case AF_INET6:
      CHECK(inet_ntop(AF_INET6, &amp;amp;addr_.addr6, buf, INET6_ADDRSTRLEN) != NULL);
      return buf;
    case AF_UNSPEC:
      LOG(DFATAL) &amp;lt;&amp;lt; "Calling ToString() on an empty IPAddress";
      return "";
    default:
      LOG(FATAL) &amp;lt;&amp;lt; "Unknown address family " &amp;lt;&amp;lt; address_family_;
  }
}
...
string IPAddressToURIString(const IPAddress&amp;amp; ip) {
  switch (ip.address_family()) {
    case AF_INET6:
      return StrCat("[", ip.ToString(), "]");
    default:
      return ip.ToString();
  }
}
...
string SocketAddress::ToString() const {
  return StrCat(IPAddressToURIString(host_), ":", port_);
}
&lt;/code&gt;
&lt;p&gt;Cache based on precomputed fingerprint of large serialized proto.&lt;/p&gt;&lt;p&gt;dp_ops.cc&lt;/p&gt;&lt;code&gt;InputOutputMappingProto mapping_proto;
PLAQUE_OP_REQUIRES(
    mapping_proto.ParseFromStringPiece(GetAttrMappingProto(state)),
    absl::InternalError("Failed to parse InputOutputMappingProto"));
ParseMapping(mapping_proto);
&lt;/code&gt;
&lt;code&gt;uint64 mapping_proto_fp = GetAttrMappingProtoFp(state);
{
  absl::MutexLock l(&amp;amp;fp_to_iometa_mu);
  if (fp_to_iometa == nullptr) {
    fp_to_iometa =
        new absl::flat_hash_map&amp;lt;uint64, std::unique_ptr&amp;lt;ProgramIOMetadata&amp;gt;&amp;gt;;
  }
  auto it = fp_to_iometa-&amp;gt;find(mapping_proto_fp);
  if (it != fp_to_iometa-&amp;gt;end()) {
    io_metadata_ = it-&amp;gt;second.get();
  } else {
    auto serial_proto = GetAttrMappingProto(state);
    DCHECK_EQ(mapping_proto_fp, Fingerprint(serial_proto));
    InputOutputMappingProto mapping_proto;
    PLAQUE_OP_REQUIRES(
        mapping_proto.ParseFromStringPiece(GetAttrMappingProto(state)),
        absl::InternalError("Failed to parse InputOutputMappingProto"));
    auto io_meta = ParseMapping(mapping_proto);
    io_metadata_ = io_meta.get();
    (*fp_to_iometa)[mapping_proto_fp] = std::move(io_meta);
  }
}
&lt;/code&gt;
&lt;p&gt;The compiler may have trouble optimizing through layers of abstractions because it must make conservative assumptions about the overall behavior of the code, or may not make the right speed vs. size tradeoffs. The application programmer will often know more about the behavior of the system and can aid the compiler by rewriting the code to operate at a lower level. However, only do this when profiles show an issue since compilers will often get things right on their own. Looking at the generated assembly code for performance critical routines can help you understand if the compiler is ‚Äúgetting it right‚Äù. Pprof provides a very helpful display of source code interleaved with disassembly and annotated with performance data.&lt;/p&gt;&lt;p&gt;Some techniques that may be useful:&lt;/p&gt;&lt;p&gt;Speed up ShapeUtil::ForEachState by replacing absl::Span with raw pointers to the underlying arrays.&lt;/p&gt;&lt;p&gt;shape_util.h&lt;/p&gt;&lt;code&gt;struct ForEachState {
  ForEachState(const Shape&amp;amp; s, absl::Span&amp;lt;const int64_t&amp;gt; b,
               absl::Span&amp;lt;const int64_t&amp;gt; c, absl::Span&amp;lt;const int64_t&amp;gt; i);
  ~ForEachState();

  const Shape&amp;amp; shape;
  const absl::Span&amp;lt;const int64_t&amp;gt; base;
  const absl::Span&amp;lt;const int64_t&amp;gt; count;
  const absl::Span&amp;lt;const int64_t&amp;gt; incr;
&lt;/code&gt;
&lt;code&gt;struct ForEachState {
  ForEachState(const Shape&amp;amp; s, absl::Span&amp;lt;const int64_t&amp;gt; b,
               absl::Span&amp;lt;const int64_t&amp;gt; c, absl::Span&amp;lt;const int64_t&amp;gt; i);
  inline ~ForEachState() = default;

  const Shape&amp;amp; shape;
  // Pointers to arrays of the passed-in spans
  const int64_t* const base;
  const int64_t* const count;
  const int64_t* const incr;
&lt;/code&gt;
&lt;p&gt;Hand unroll cyclic redundancy check (CRC) computation loop.&lt;/p&gt;&lt;p&gt;crc.cc&lt;/p&gt;&lt;code&gt;void CRC32::Extend(uint64 *lo, uint64 *hi, const void *bytes, size_t length)
                      const {
                      ...
  // Process bytes 4 at a time
  while ((p + 4) &amp;lt;= e) {
    uint32 c = l ^ WORD(p);
    p += 4;
    l = this-&amp;gt;table3_[c &amp;amp; 0xff] ^
        this-&amp;gt;table2_[(c &amp;gt;&amp;gt; 8) &amp;amp; 0xff] ^
        this-&amp;gt;table1_[(c &amp;gt;&amp;gt; 16) &amp;amp; 0xff] ^
        this-&amp;gt;table0_[c &amp;gt;&amp;gt; 24];
  }

  // Process the last few bytes
  while (p != e) {
    int c = (l &amp;amp; 0xff) ^ *p++;
    l = this-&amp;gt;table0_[c] ^ (l &amp;gt;&amp;gt; 8);
  }
  *lo = l;
}
&lt;/code&gt;
&lt;code&gt;void CRC32::Extend(uint64 *lo, uint64 *hi, const void *bytes, size_t length)
                      const {
                      ...
#define STEP {                                  \
    uint32 c = l ^ WORD(p);                     \
    p += 4;                                     \
    l = this-&amp;gt;table3_[c &amp;amp; 0xff] ^               \
        this-&amp;gt;table2_[(c &amp;gt;&amp;gt; 8) &amp;amp; 0xff] ^        \
        this-&amp;gt;table1_[(c &amp;gt;&amp;gt; 16) &amp;amp; 0xff] ^       \
        this-&amp;gt;table0_[c &amp;gt;&amp;gt; 24];                 \
}

  // Process bytes 16 at a time
  while ((e-p) &amp;gt;= 16) {
    STEP;
    STEP;
    STEP;
    STEP;
  }

  // Process bytes 4 at a time
  while ((p + 4) &amp;lt;= e) {
    STEP;
  }
#undef STEP

  // Process the last few bytes
  while (p != e) {
    int c = (l &amp;amp; 0xff) ^ *p++;
    l = this-&amp;gt;table0_[c] ^ (l &amp;gt;&amp;gt; 8);
  }
  *lo = l;
}

&lt;/code&gt;
&lt;p&gt;Handle four characters at a time when parsing Spanner keys.&lt;/p&gt;&lt;p&gt;Hand unroll loop to deal with four characters at a time rather than using memchr&lt;/p&gt;&lt;p&gt;Manually unroll loop for finding separated sections of name&lt;/p&gt;&lt;p&gt;Go backwards to find separated portions of a name with '#' separators (rather than forwards) since the first part is likely the longest in the name.&lt;/p&gt;&lt;p&gt;key.cc&lt;/p&gt;&lt;code&gt;void Key::InitSeps(const char* start) {
  const char* base = &amp;amp;rep_[0];
  const char* limit = base + rep_.size();
  const char* s = start;

  DCHECK_GE(s, base);
  DCHECK_LT(s, limit);

  for (int i = 0; i &amp;lt; 3; i++) {
    s = (const char*)memchr(s, '#', limit - s);
    DCHECK(s != NULL);
    seps_[i] = s - base;
    s++;
  }
}
&lt;/code&gt;
&lt;code&gt;inline const char* ScanBackwardsForSep(const char* base, const char* p) {
  while (p &amp;gt;= base + 4) {
    if (p[0] == '#') return p;
    if (p[-1] == '#') return p-1;
    if (p[-2] == '#') return p-2;
    if (p[-3] == '#') return p-3;
    p -= 4;
  }
  while (p &amp;gt;= base &amp;amp;&amp;amp; *p != '#') p--;
  return p;
}

void Key::InitSeps(const char* start) {
  const char* base = &amp;amp;rep_[0];
  const char* limit = base + rep_.size();
  const char* s = start;

  DCHECK_GE(s, base);
  DCHECK_LT(s, limit);

  // We go backwards from the end of the string, rather than forwards,
  // since the directory name might be long and definitely doesn't contain
  // any '#' characters.
  const char* p = ScanBackwardsForSep(s, limit - 1);
  DCHECK(*p == '#');
  seps_[2] = p - base;
  p--;

  p = ScanBackwardsForSep(s, p);
  DCHECK(*p == '#');
  seps_[1] = p - base;
  p--;

  p = ScanBackwardsForSep(s, p);
  DCHECK(*p == '#');
  seps_[0] = p - base;
}
&lt;/code&gt;
&lt;p&gt;Avoid frame setup costs by converting ABSL_LOG(FATAL) to ABSL_DCHECK(false).&lt;/p&gt;&lt;p&gt;arena_cleanup.h&lt;/p&gt;&lt;code&gt;inline ABSL_ATTRIBUTE_ALWAYS_INLINE size_t Size(Tag tag) {
  if (!EnableSpecializedTags()) return sizeof(DynamicNode);

  switch (tag) {
    case Tag::kDynamic:
      return sizeof(DynamicNode);
    case Tag::kString:
      return sizeof(TaggedNode);
    case Tag::kCord:
      return sizeof(TaggedNode);
    default:
      ABSL_LOG(FATAL) &amp;lt;&amp;lt; "Corrupted cleanup tag: " &amp;lt;&amp;lt; static_cast&amp;lt;int&amp;gt;(tag);
      return sizeof(DynamicNode);
  }
}
&lt;/code&gt;
&lt;code&gt;inline ABSL_ATTRIBUTE_ALWAYS_INLINE size_t Size(Tag tag) {
  if (!EnableSpecializedTags()) return sizeof(DynamicNode);

  switch (tag) {
    case Tag::kDynamic:
      return sizeof(DynamicNode);
    case Tag::kString:
      return sizeof(TaggedNode);
    case Tag::kCord:
      return sizeof(TaggedNode);
    default:
      ABSL_DCHECK(false) &amp;lt;&amp;lt; "Corrupted cleanup tag: " &amp;lt;&amp;lt; static_cast&amp;lt;int&amp;gt;(tag);
      return sizeof(DynamicNode);
  }
}
&lt;/code&gt;
&lt;p&gt;Balance the utility of stats and other behavioral information about a system against the cost of maintaining that information. The extra information can often help people to understand and improve high-level behavior, but can also be costly to maintain.&lt;/p&gt;&lt;p&gt;Stats that are not useful can be dropped altogether.&lt;/p&gt;&lt;p&gt;Stop maintaining expensive stats about number of alarms and closures in SelectServer.&lt;/p&gt;&lt;p&gt;Part of changes that reduce time for setting an alarm from 771 ns to 271 ns.&lt;/p&gt;&lt;p&gt;selectserver.h&lt;/p&gt;&lt;code&gt;class SelectServer {
 public:
 ...
 protected:
  ...
  scoped_ptr&amp;lt;MinuteTenMinuteHourStat&amp;gt; num_alarms_stat_;
  ...
  scoped_ptr&amp;lt;MinuteTenMinuteHourStat&amp;gt; num_closures_stat_;
  ...
};
&lt;/code&gt;
&lt;code&gt;// Selectserver class
class SelectServer {
 ...
 protected:
 ...
};
&lt;/code&gt;
&lt;p&gt;/selectserver.cc&lt;/p&gt;&lt;code&gt;void SelectServer::AddAlarmInternal(Alarmer* alarmer,
                                    int offset_in_ms,
                                    int id,
                                    bool is_periodic) {
                                    ...
  alarms_-&amp;gt;insert(alarm);
  num_alarms_stat_-&amp;gt;IncBy(1);
  ...
}
&lt;/code&gt;
&lt;code&gt;void SelectServer::AddAlarmInternal(Alarmer* alarmer,
                                    int offset_in_ms,
                                    int id,
                                    bool is_periodic) {
                                    ...
  alarms_-&amp;gt;Add(alarm);
  ...
}
&lt;/code&gt;
&lt;p&gt;/selectserver.cc&lt;/p&gt;&lt;code&gt;void SelectServer::RemoveAlarm(Alarmer* alarmer, int id) {
      ...
      alarms_-&amp;gt;erase(alarm);
      num_alarms_stat_-&amp;gt;IncBy(-1);
      ...
}
&lt;/code&gt;
&lt;code&gt;void SelectServer::RemoveAlarm(Alarmer* alarmer, int id) {
      ...
      alarms_-&amp;gt;Remove(alarm);
      ...
}
&lt;/code&gt;
&lt;p&gt;Often, stats or other properties can be maintained for a sample of the elements handled by the system (e.g., RPC requests, input records, users). Many subsystems use this approach (tcmalloc allocation tracking, /requestz status pages, Dapper samples).&lt;/p&gt;&lt;p&gt;When sampling, consider reducing the sampling rate when appropriate.&lt;/p&gt;&lt;p&gt;Maintain stats for just a sample of doc info requests.&lt;/p&gt;&lt;p&gt;Sampling allows us to avoid touching 39 histograms and MinuteTenMinuteHour stats for most requests.&lt;/p&gt;&lt;p&gt;generic-leaf-stats.cc&lt;/p&gt;&lt;code&gt;... code that touches 39 histograms to update various stats on every request ...
&lt;/code&gt;
&lt;code&gt;// Add to the histograms periodically
if (TryLockToUpdateHistogramsDocInfo(docinfo_stats, bucket)) {
  // Returns true and grabs bucket-&amp;gt;lock only if we should sample this
  // request for maintaining stats
  ... code that touches 39 histograms to update various stats ...
  bucket-&amp;gt;lock.Unlock();
}
&lt;/code&gt;

&lt;p&gt;Reduce sampling rate and make faster sampling decisions.&lt;/p&gt;&lt;p&gt;This change reduces the sampling rate from 1 in 10 to 1 in 32. Furthermore, we now keep execution time stats just for the sampled events and speed up sampling decisions by using a power of two modulus. This code is called on every packet in the Google Meet video conferencing system and needed performance work to keep up with capacity demands during the first part of the COVID outbreak as users rapidly migrated to doing more online meetings.&lt;/p&gt;&lt;p&gt;packet_executor.cc&lt;/p&gt;&lt;code&gt;class ScopedPerformanceMeasurement {
 public:
  explicit ScopedPerformanceMeasurement(PacketExecutor* packet_executor)
      : packet_executor_(packet_executor),
        tracer_(packet_executor-&amp;gt;packet_executor_trace_threshold_,
                kClosureTraceName) {
    // ThreadCPUUsage is an expensive call. At the time of writing,
    // it takes over 400ns, or roughly 30 times slower than absl::Now,
    // so we sample only 10% of closures to keep the cost down.
    if (packet_executor-&amp;gt;closures_executed_ % 10 == 0) {
      thread_cpu_usage_start_ = base::ThreadCPUUsage();
    }

    // Sample start time after potentially making the above expensive call,
    // so as not to pollute wall time measurements.
    run_start_time_ = absl::Now();
  }

  ~ScopedPerformanceMeasurement() {
&lt;/code&gt;
&lt;code&gt;ScopedPerformanceMeasurement::ScopedPerformanceMeasurement(
    PacketExecutor* packet_executor)
    : packet_executor_(packet_executor),
      tracer_(packet_executor-&amp;gt;packet_executor_trace_threshold_,
              kClosureTraceName) {
  // ThreadCPUUsage is an expensive call. At the time of writing,
  // it takes over 400ns, or roughly 30 times slower than absl::Now,
  // so we sample only 1 in 32 closures to keep the cost down.
  if (packet_executor-&amp;gt;closures_executed_ % 32 == 0) {
    thread_cpu_usage_start_ = base::ThreadCPUUsage();
  }

  // Sample start time after potentially making the above expensive call,
  // so as not to pollute wall time measurements.
  run_start_time_ = absl::Now();
}
&lt;/code&gt;
&lt;p&gt;packet_executor.cc&lt;/p&gt;&lt;code&gt;~ScopedPerformanceMeasurement() {
  auto run_end_time = absl::Now();
  auto run_duration = run_end_time - run_start_time_;

  if (thread_cpu_usage_start_.has_value()) {
  ...
  }

  closure_execution_time-&amp;gt;Record(absl::ToInt64Microseconds(run_duration));
&lt;/code&gt;
&lt;code&gt;ScopedPerformanceMeasurement::~ScopedPerformanceMeasurement() {
  auto run_end_time = absl::Now();
  auto run_duration = run_end_time - run_start_time_;

  if (thread_cpu_usage_start_.has_value()) {
    ...
    closure_execution_time-&amp;gt;Record(absl::ToInt64Microseconds(run_duration));
  }
&lt;/code&gt;
&lt;p&gt;Benchmark results:&lt;/p&gt;&lt;code&gt;Run on (40 X 2793 MHz CPUs); 2020-03-24T20:08:19.991412535-07:00
CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_PacketOverhead_mean                               224          85    +62.0%
&lt;/code&gt;

&lt;p&gt;Logging statements can be costly, even if the logging-level for the statement doesn‚Äôt actually log anything. E.g., &lt;code&gt;ABSL_VLOG&lt;/code&gt;‚Äôs implementation requires at
least a load and a comparison, which may be a problem in hot code paths. In
addition, the presence of the logging code may inhibit compiler optimizations.
Consider dropping logging entirely from hot code paths.&lt;/p&gt;&lt;p&gt;Remove logging from guts of memory allocator.&lt;/p&gt;&lt;p&gt;This was a small part of a larger change.&lt;/p&gt;&lt;p&gt;gpu_bfc_allocator.cc&lt;/p&gt;&lt;code&gt;void GPUBFCAllocator::SplitChunk(...) {
  ...
  VLOG(6) &amp;lt;&amp;lt; "Adding to chunk map: " &amp;lt;&amp;lt; new_chunk-&amp;gt;ptr;
  ...
}
...
void GPUBFCAllocator::DeallocateRawInternal(void* ptr) {
  ...
  VLOG(6) &amp;lt;&amp;lt; "Chunk at " &amp;lt;&amp;lt; c-&amp;gt;ptr &amp;lt;&amp;lt; " no longer in use";
  ...
}
&lt;/code&gt;
&lt;code&gt;void GPUBFCAllocator::SplitChunk(...) {
...
}
...
void GPUBFCAllocator::DeallocateRawInternal(void* ptr) {
...
}
&lt;/code&gt;

&lt;p&gt;Precompute whether or not logging is enabled outside a nested loop.&lt;/p&gt;&lt;p&gt;image_similarity.cc&lt;/p&gt;&lt;code&gt;for (int j = 0; j &amp;lt; output_subimage_size_y; j++) {
  int j1 = j - rad + output_to_integral_subimage_y;
  int j2 = j1 + 2 * rad + 1;
  // Create a pointer for this row's output, taking into account the offset
  // to the full image.
  double *image_diff_ptr = &amp;amp;(*image_diff)(j + min_j, min_i);

  for (int i = 0; i &amp;lt; output_subimage_size_x; i++) {
    ...
    if (VLOG_IS_ON(3)) {
    ...
    }
    ...
  }
}
&lt;/code&gt;
&lt;code&gt;const bool vlog_3 = DEBUG_MODE ? VLOG_IS_ON(3) : false;

for (int j = 0; j &amp;lt; output_subimage_size_y; j++) {
  int j1 = j - rad + output_to_integral_subimage_y;
  int j2 = j1 + 2 * rad + 1;
  // Create a pointer for this row's output, taking into account the offset
  // to the full image.
  double *image_diff_ptr = &amp;amp;(*image_diff)(j + min_j, min_i);

  for (int i = 0; i &amp;lt; output_subimage_size_x; i++) {
    ...
    if (vlog_3) {
    ...
    }
  }
}
&lt;/code&gt;
&lt;code&gt;Run on (40 X 2801 MHz CPUs); 2016-05-16T15:55:32.250633072-07:00
CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_NCCPerformance/16                   29104     26372     +9.4%
BM_NCCPerformance/64                  473235    425281    +10.1%
BM_NCCPerformance/512               30246238  27622009     +8.7%
BM_NCCPerformance/1k              125651445  113361991     +9.8%
BM_NCCLimitedBoundsPerformance/16       8314      7498     +9.8%
BM_NCCLimitedBoundsPerformance/64     143508    132202     +7.9%
BM_NCCLimitedBoundsPerformance/512   9335684   8477567     +9.2%
BM_NCCLimitedBoundsPerformance/1k   37223897  34201739     +8.1%
&lt;/code&gt;

&lt;p&gt;Precompute whether logging is enabled and use the result in helper routines.&lt;/p&gt;&lt;p&gt;periodic_call.cc&lt;/p&gt;&lt;code&gt;  VLOG(1) &amp;lt;&amp;lt; Logid()
          &amp;lt;&amp;lt; "MaybeScheduleAlarmAtNextTick. Time until next real time: "
          &amp;lt;&amp;lt; time_until_next_real_time;
          ...
  uint64 next_virtual_time_ms =
      next_virtual_time_ms_ - num_ticks * kResolutionMs;
  CHECK_GE(next_virtual_time_ms, 0);
  ScheduleAlarm(now, delay, next_virtual_time_ms);
}

void ScheduleNextAlarm(uint64 current_virtual_time_ms)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  if (calls_.empty()) {
    VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "No calls left, entering idle mode";
    next_real_time_ = absl::InfiniteFuture();
    return;
  }
  uint64 next_virtual_time_ms = FindNextVirtualTime(current_virtual_time_ms);
  auto delay =
      absl::Milliseconds(next_virtual_time_ms - current_virtual_time_ms);
  ScheduleAlarm(GetClock().TimeNow(), delay, next_virtual_time_ms);
}

// An alarm scheduled by this function supersedes all previously scheduled
// alarms. This is ensured through `scheduling_sequence_number_`.
void ScheduleAlarm(absl::Time now, absl::Duration delay,
                   uint64 virtual_time_ms)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  next_real_time_ = now + delay;
  next_virtual_time_ms_ = virtual_time_ms;
  ++ref_count_;  // The Alarm holds a reference.
  ++scheduling_sequence_number_;
  VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "ScheduleAlarm. Time : "
          &amp;lt;&amp;lt; absl::FormatTime("%M:%S.%E3f", now, absl::UTCTimeZone())
          &amp;lt;&amp;lt; ", delay: " &amp;lt;&amp;lt; delay &amp;lt;&amp;lt; ", virtual time: " &amp;lt;&amp;lt; virtual_time_ms
          &amp;lt;&amp;lt; ", refs: " &amp;lt;&amp;lt; ref_count_
          &amp;lt;&amp;lt; ", seq: " &amp;lt;&amp;lt; scheduling_sequence_number_
          &amp;lt;&amp;lt; ", executor: " &amp;lt;&amp;lt; executor_;

  executor_-&amp;gt;AddAfter(
      delay, new Alarm(this, virtual_time_ms, scheduling_sequence_number_));
}
&lt;/code&gt;
&lt;code&gt;  const bool vlog_1 = VLOG_IS_ON(1);

  if (vlog_1) {
    VLOG(1) &amp;lt;&amp;lt; Logid()
            &amp;lt;&amp;lt; "MaybeScheduleAlarmAtNextTick. Time until next real time: "
            &amp;lt;&amp;lt; time_until_next_real_time;
  }
  ...
  uint64 next_virtual_time_ms =
      next_virtual_time_ms_ - num_ticks * kResolutionMs;
  CHECK_GE(next_virtual_time_ms, 0);
  ScheduleAlarm(now, delay, next_virtual_time_ms, vlog_1);
}

void ScheduleNextAlarm(uint64 current_virtual_time_ms, bool vlog_1)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  if (calls_.empty()) {
    if (vlog_1) {
      VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "No calls left, entering idle mode";
    }
    next_real_time_ = absl::InfiniteFuture();
    return;
  }
  uint64 next_virtual_time_ms = FindNextVirtualTime(current_virtual_time_ms);
  auto delay =
      absl::Milliseconds(next_virtual_time_ms - current_virtual_time_ms);
  ScheduleAlarm(GetClock().TimeNow(), delay, next_virtual_time_ms, vlog_1);
}

// An alarm scheduled by this function supersedes all previously scheduled
// alarms. This is ensured through `scheduling_sequence_number_`.
void ScheduleAlarm(absl::Time now, absl::Duration delay,
                   uint64 virtual_time_ms,
                   bool vlog_1)
    ABSL_EXCLUSIVE_LOCKS_REQUIRED(mutex_) {
  next_real_time_ = now + delay;
  next_virtual_time_ms_ = virtual_time_ms;
  ++ref_count_;  // The Alarm holds a reference.
  ++scheduling_sequence_number_;
  if (vlog_1) {
    VLOG(1) &amp;lt;&amp;lt; Logid() &amp;lt;&amp;lt; "ScheduleAlarm. Time : "
            &amp;lt;&amp;lt; absl::FormatTime("%M:%S.%E3f", now, absl::UTCTimeZone())
            &amp;lt;&amp;lt; ", delay: " &amp;lt;&amp;lt; delay &amp;lt;&amp;lt; ", virtual time: " &amp;lt;&amp;lt; virtual_time_ms
            &amp;lt;&amp;lt; ", refs: " &amp;lt;&amp;lt; ref_count_
            &amp;lt;&amp;lt; ", seq: " &amp;lt;&amp;lt; scheduling_sequence_number_
            &amp;lt;&amp;lt; ", executor: " &amp;lt;&amp;lt; executor_;
  }

  executor_-&amp;gt;AddAfter(
      delay, new Alarm(this, virtual_time_ms, scheduling_sequence_number_));
}
&lt;/code&gt;
&lt;p&gt;Performance encompasses more than just runtime speed. Sometimes it is worth considering the effects of software choices on the size of generated code. Large code size means longer compile and link times, bloated binaries, more memory usage, more icache pressure, and other sometimes negative effects on microarchitectural structures like branch predictors, etc. Thinking about these issues is especially important when writing low-level library code that will be used in many places, or when writing templated code that you expect will be instantiated for many different types.&lt;/p&gt;&lt;p&gt;The techniques that are useful for reducing code size vary significantly across programming languages. Here are some techniques that we have found useful for C++ code (which can suffer from an over-use of templates and inlining).&lt;/p&gt;&lt;p&gt;Widely called functions combined with inlining can have a dramatic effect on code size.&lt;/p&gt;&lt;p&gt;Speed up TF_CHECK_OK.&lt;/p&gt;&lt;p&gt;Avoid creating Ok object, and save code space by doing complex formatting of fatal error message out of line instead of at every call site.&lt;/p&gt;&lt;p&gt;status.h&lt;/p&gt;&lt;code&gt;#define TF_CHECK_OK(val) CHECK_EQ(::tensorflow::Status::OK(), (val))
#define TF_QCHECK_OK(val) QCHECK_EQ(::tensorflow::Status::OK(), (val))
&lt;/code&gt;
&lt;code&gt;extern tensorflow::string* TfCheckOpHelperOutOfLine(
    const ::tensorflow::Status&amp;amp; v, const char* msg);
inline tensorflow::string* TfCheckOpHelper(::tensorflow::Status v,
                                           const char* msg) {
  if (v.ok()) return nullptr;
  return TfCheckOpHelperOutOfLine(v, msg);
}
#define TF_CHECK_OK(val)                                           \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(FATAL) &amp;lt;&amp;lt; *(_result)
#define TF_QCHECK_OK(val)                                          \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(QFATAL) &amp;lt;&amp;lt; *(_result)
&lt;/code&gt;
&lt;p&gt;status.cc&lt;/p&gt;&lt;code&gt;string* TfCheckOpHelperOutOfLine(const ::tensorflow::Status&amp;amp; v,
                                 const char* msg) {
  string r("Non-OK-status: ");
  r += msg;
  r += " status: ";
  r += v.ToString();
  // Leaks string but this is only to be used in a fatal error message
  return new string(r);
}
&lt;/code&gt;
&lt;p&gt;Shrink each RETURN_IF_ERROR call site by 79 bytes of code.&lt;/p&gt;&lt;p&gt;Improve performance of CHECK_GE by 4.5X and shrink code size from 125 bytes to 77 bytes.&lt;/p&gt;&lt;p&gt;logging.h&lt;/p&gt;&lt;code&gt;struct CheckOpString {
  CheckOpString(string* str) : str_(str) { }
  ~CheckOpString() { delete str_; }
  operator bool() const { return str_ == NULL; }
  string* str_;
};
...
#define DEFINE_CHECK_OP_IMPL(name, op) \
  template &amp;lt;class t1, class t2&amp;gt; \
  inline string* Check##name##Impl(const t1&amp;amp; v1, const t2&amp;amp; v2, \
                                   const char* names) { \
    if (v1 op v2) return NULL; \
    else return MakeCheckOpString(v1, v2, names); \
  } \
  string* Check##name##Impl(int v1, int v2, const char* names);
DEFINE_CHECK_OP_IMPL(EQ, ==)
DEFINE_CHECK_OP_IMPL(NE, !=)
DEFINE_CHECK_OP_IMPL(LE, &amp;lt;=)
DEFINE_CHECK_OP_IMPL(LT, &amp;lt; )
DEFINE_CHECK_OP_IMPL(GE, &amp;gt;=)
DEFINE_CHECK_OP_IMPL(GT, &amp;gt; )
#undef DEFINE_CHECK_OP_IMPL
&lt;/code&gt;
&lt;code&gt;struct CheckOpString {
  CheckOpString(string* str) : str_(str) { }
  // No destructor: if str_ is non-NULL, we're about to LOG(FATAL),
  // so there's no point in cleaning up str_.
  operator bool() const { return str_ == NULL; }
  string* str_;
};
...
extern string* MakeCheckOpStringIntInt(int v1, int v2, const char* names);

template&amp;lt;int, int&amp;gt;
string* MakeCheckOpString(const int&amp;amp; v1, const int&amp;amp; v2, const char* names) {
  return MakeCheckOpStringIntInt(v1, v2, names);
}
...
#define DEFINE_CHECK_OP_IMPL(name, op) \
  template &amp;lt;class t1, class t2&amp;gt; \
  inline string* Check##name##Impl(const t1&amp;amp; v1, const t2&amp;amp; v2, \
                                   const char* names) { \
    if (v1 op v2) return NULL; \
    else return MakeCheckOpString(v1, v2, names); \
  } \
  inline string* Check##name##Impl(int v1, int v2, const char* names) { \
    if (v1 op v2) return NULL; \
    else return MakeCheckOpString(v1, v2, names); \
  }
DEFINE_CHECK_OP_IMPL(EQ, ==)
DEFINE_CHECK_OP_IMPL(NE, !=)
DEFINE_CHECK_OP_IMPL(LE, &amp;lt;=)
DEFINE_CHECK_OP_IMPL(LT, &amp;lt; )
DEFINE_CHECK_OP_IMPL(GE, &amp;gt;=)
DEFINE_CHECK_OP_IMPL(GT, &amp;gt; )
#undef DEFINE_CHECK_OP_IMPL
&lt;/code&gt;
&lt;p&gt;logging.cc&lt;/p&gt;&lt;code&gt;string* MakeCheckOpStringIntInt(int v1, int v2, const char* names) {
  strstream ss;
  ss &amp;lt;&amp;lt; names &amp;lt;&amp;lt; " (" &amp;lt;&amp;lt; v1 &amp;lt;&amp;lt; " vs. " &amp;lt;&amp;lt; v2 &amp;lt;&amp;lt; ")";
  return new string(ss.str(), ss.pcount());
}
&lt;/code&gt;
&lt;p&gt;Inlining can often improve performance, but sometimes it can increase code size without a corresponding performance payoff (and in some case even a performance loss due to increased instruction cache pressure).&lt;/p&gt;&lt;p&gt;Reduce inlining in TensorFlow.&lt;/p&gt;&lt;p&gt;The change stops inlining many non-performance-sensitive functions (e.g., error paths and op registration code). Furthermore, slow paths of some performance-sensitive functions are moved into non-inlined functions.&lt;/p&gt;&lt;p&gt;These changes reduces the size of tensorflow symbols in a typical binary by 12.2% (8814545 bytes down to 7740233 bytes)&lt;/p&gt;&lt;p&gt;Protocol buffer library change. Avoid expensive inlined code space for encoding message length for messages ‚â• 128 bytes and instead do a procedure call to a shared out-of-line routine.&lt;/p&gt;&lt;p&gt;Not only makes important large binaries smaller but also faster.&lt;/p&gt;&lt;p&gt;Bytes of generated code per line of a heavily inlined routine in one large binary. First number represents the total bytes generated for a particular source line including all locations where that code has been inlined.&lt;/p&gt;&lt;p&gt;Before:&lt;/p&gt;&lt;code&gt;.           0   1825 template &amp;lt;typename MessageType&amp;gt;
.           0   1826 inline uint8* WireFormatLite::InternalWriteMessage(
.           0   1827     int field_number, const MessageType&amp;amp; value, uint8* target,
.           0   1828     io::EpsCopyOutputStream* stream) {
&amp;gt;&amp;gt;&amp;gt;    389246   1829   target = WriteTagToArray(field_number, WIRETYPE_LENGTH_DELIMITED, target);
&amp;gt;&amp;gt;&amp;gt;   5454640   1830   target = io::CodedOutputStream::WriteVarint32ToArray(
&amp;gt;&amp;gt;&amp;gt;    337837   1831       static_cast&amp;lt;uint32&amp;gt;(value.GetCachedSize()), target);
&amp;gt;&amp;gt;&amp;gt;   1285539   1832   return value._InternalSerialize(target, stream);
.           0   1833 }
&lt;/code&gt;
&lt;p&gt;The new codesize output with this change looks like:&lt;/p&gt;&lt;code&gt;.           0   1825 template &amp;lt;typename MessageType&amp;gt;
.           0   1826 inline uint8* WireFormatLite::InternalWriteMessage(
.           0   1827     int field_number, const MessageType&amp;amp; value, uint8* target,
.           0   1828     io::EpsCopyOutputStream* stream) {
&amp;gt;&amp;gt;&amp;gt;    450612   1829   target = WriteTagToArray(field_number, WIRETYPE_LENGTH_DELIMITED, target);
&amp;gt;&amp;gt;       9609   1830   target = io::CodedOutputStream::WriteVarint32ToArrayOutOfLine(
&amp;gt;&amp;gt;&amp;gt;    434668   1831       static_cast&amp;lt;uint32&amp;gt;(value.GetCachedSize()), target);
&amp;gt;&amp;gt;&amp;gt;   1597394   1832   return value._InternalSerialize(target, stream);
.           0   1833 }
&lt;/code&gt;
&lt;p&gt;coded_stream.h&lt;/p&gt;&lt;code&gt;class PROTOBUF_EXPORT CodedOutputStream {
  ...
  // Like WriteVarint32()  but writing directly to the target array, and with the
  // less common-case paths being out of line rather than inlined.
  static uint8* WriteVarint32ToArrayOutOfLine(uint32 value, uint8* target);
  ...
};
...
inline uint8* CodedOutputStream::WriteVarint32ToArrayOutOfLine(uint32 value,
                                                               uint8* target) {
  target[0] = static_cast&amp;lt;uint8&amp;gt;(value);
  if (value &amp;lt; 0x80) {
    return target + 1;
  } else {
    return WriteVarint32ToArrayOutOfLineHelper(value, target);
  }
}
&lt;/code&gt;
&lt;p&gt;coded_stream.cc&lt;/p&gt;&lt;code&gt;uint8* CodedOutputStream::WriteVarint32ToArrayOutOfLineHelper(uint32 value,
                                                              uint8* target) {
  DCHECK_GE(value, 0x80);
  target[0] |= static_cast&amp;lt;uint8&amp;gt;(0x80);
  value &amp;gt;&amp;gt;= 7;
  target[1] = static_cast&amp;lt;uint8&amp;gt;(value);
  if (value &amp;lt; 0x80) {
    return target + 2;
  }
  target += 2;
  do {
    // Turn on continuation bit in the byte we just wrote.
    target[-1] |= static_cast&amp;lt;uint8&amp;gt;(0x80);
    value &amp;gt;&amp;gt;= 7;
    *target = static_cast&amp;lt;uint8&amp;gt;(value);
    ++target;
  } while (value &amp;gt;= 0x80);
  return target;
}
&lt;/code&gt;
&lt;p&gt;Reduce absl::flat_hash_set and absl::flat_hash_map code size.&lt;/p&gt;&lt;p&gt;Reduces sizes of some large binaries by ~0.5%.&lt;/p&gt;&lt;p&gt;Do not inline string allocation and deallocation when not using protobuf arenas.&lt;/p&gt;&lt;p&gt;public/arenastring.h&lt;/p&gt;&lt;code&gt;  if (IsDefault(default_value)) {
    std::string* new_string = new std::string();
    tagged_ptr_.Set(new_string);
    return new_string;
  } else {
    return UnsafeMutablePointer();
  }
}
&lt;/code&gt;
&lt;code&gt;  if (IsDefault(default_value)) {
    return SetAndReturnNewString();
  } else {
    return UnsafeMutablePointer();
  }
}
&lt;/code&gt;
&lt;p&gt;internal/arenastring.cc&lt;/p&gt;&lt;code&gt;std::string* ArenaStringPtr::SetAndReturnNewString() {
  std::string* new_string = new std::string();
  tagged_ptr_.Set(new_string);
  return new_string;
}
&lt;/code&gt;
&lt;p&gt;Avoid inlining some routines. Create variants of routines that take 'const char*' rather than 'const std::string&amp;amp;' to avoid std::string construction code at every call site.&lt;/p&gt;&lt;p&gt;op.h&lt;/p&gt;&lt;code&gt;class OpDefBuilderWrapper {
 public:
  explicit OpDefBuilderWrapper(const char name[]) : builder_(name) {}
  OpDefBuilderWrapper&amp;amp; Attr(std::string spec) {
    builder_.Attr(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Input(std::string spec) {
    builder_.Input(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Output(std::string spec) {
    builder_.Output(std::move(spec));
    return *this;
  }
&lt;/code&gt;
&lt;code&gt;class OpDefBuilderWrapper {
 public:
  explicit OpDefBuilderWrapper(const char name[]) : builder_(name) {}
  OpDefBuilderWrapper&amp;amp; Attr(std::string spec) {
    builder_.Attr(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Attr(const char* spec) TF_ATTRIBUTE_NOINLINE {
    return Attr(std::string(spec));
  }
  OpDefBuilderWrapper&amp;amp; Input(std::string spec) {
    builder_.Input(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Input(const char* spec) TF_ATTRIBUTE_NOINLINE {
    return Input(std::string(spec));
  }
  OpDefBuilderWrapper&amp;amp; Output(std::string spec) {
    builder_.Output(std::move(spec));
    return *this;
  }
  OpDefBuilderWrapper&amp;amp; Output(const char* spec) TF_ATTRIBUTE_NOINLINE {
    return Output(std::string(spec));
  }
&lt;/code&gt;
&lt;p&gt;Templated code can be duplicated for every possible combination of template arguments when it is instantiated.&lt;/p&gt;&lt;p&gt;Replace template argument with a regular argument.&lt;/p&gt;&lt;p&gt;Changed a large routine templated on a bool to instead take the bool as an extra argument. (The bool was only being used once to select one of two string constants, so a run-time check was just fine.) This reduced the # of instantiations of the large routine from 287 to 143.&lt;/p&gt;&lt;p&gt;sharding_util_ops.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;bool Split&amp;gt;
Status GetAndValidateAttributes(OpKernelConstruction* ctx,
                                std::vector&amp;lt;int32&amp;gt;&amp;amp; num_partitions,
                                int&amp;amp; num_slices, std::vector&amp;lt;int32&amp;gt;&amp;amp; paddings,
                                bool&amp;amp; has_paddings) {
  absl::string_view num_partitions_attr_name =
      Split ? kNumSplitsAttrName : kNumConcatsAttrName;
      ...
  return OkStatus();
}
&lt;/code&gt;
&lt;code&gt;Status GetAndValidateAttributes(bool split, OpKernelConstruction* ctx,
                                std::vector&amp;lt;int32&amp;gt;&amp;amp; num_partitions,
                                int&amp;amp; num_slices, std::vector&amp;lt;int32&amp;gt;&amp;amp; paddings,
                                bool&amp;amp; has_paddings) {
  absl::string_view num_partitions_attr_name =
      split ? kNumSplitsAttrName : kNumConcatsAttrName;
      ...
  return OkStatus();
}
&lt;/code&gt;
&lt;p&gt;Move bulky code from templated constructor to a non-templated shared base class constructor.&lt;/p&gt;&lt;p&gt;Also reduce number of template instantiations from one for every combination of &lt;code&gt;&amp;lt;T, Device, Rank&amp;gt;&lt;/code&gt; to one for every &lt;code&gt;&amp;lt;T&amp;gt;&lt;/code&gt; and every &lt;code&gt;&amp;lt;Rank&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;sharding_util_ops.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;typename Device, typename T&amp;gt;
class XlaSplitNDBaseOp : public OpKernel {
 public:
  explicit XlaSplitNDBaseOp(OpKernelConstruction* ctx) : OpKernel(ctx) {
    OP_REQUIRES_OK(
        ctx, GetAndValidateAttributes(/*split=*/true, ctx, num_splits_,
                                      num_slices_, paddings_, has_paddings_));
  }
&lt;/code&gt;
&lt;code&gt;// Shared base class to save code space
class XlaSplitNDShared : public OpKernel {
 public:
  explicit XlaSplitNDShared(OpKernelConstruction* ctx) TF_ATTRIBUTE_NOINLINE
      : OpKernel(ctx),
        num_slices_(1),
        has_paddings_(false) {
    GetAndValidateAttributes(/*split=*/true, ctx, num_splits_, num_slices_,
                             paddings_, has_paddings_);
  }
&lt;/code&gt;
&lt;p&gt;Reduce generated code size for absl::flat_hash_set and absl::flat_hash_map.&lt;/p&gt;&lt;p&gt;Consider the impact of map and other container operations since each call to such and operation can produce large amounts of generated code.&lt;/p&gt;&lt;p&gt;Turn many map insertion calls in a row to initialize a hash table of emoji characters into a single bulk insert operation (188KB of text down to 360 bytes in library linked into many binaries). üòä&lt;/p&gt;&lt;p&gt;textfallback_init.h&lt;/p&gt;&lt;code&gt;inline void AddEmojiFallbacks(TextFallbackMap *map) {
  (*map)[0xFE000] = &amp;amp;kFE000;
  (*map)[0xFE001] = &amp;amp;kFE001;
  (*map)[0xFE002] = &amp;amp;kFE002;
  (*map)[0xFE003] = &amp;amp;kFE003;
  (*map)[0xFE004] = &amp;amp;kFE004;
  (*map)[0xFE005] = &amp;amp;kFE005;
  ...
  (*map)[0xFEE7D] = &amp;amp;kFEE7D;
  (*map)[0xFEEA0] = &amp;amp;kFEEA0;
  (*map)[0xFE331] = &amp;amp;kFE331;
};
&lt;/code&gt;
&lt;code&gt;inline void AddEmojiFallbacks(TextFallbackMap *map) {
#define PAIR(x) {0x##x, &amp;amp;k##x}
  // clang-format off
  map-&amp;gt;insert({
    PAIR(FE000),
    PAIR(FE001),
    PAIR(FE002),
    PAIR(FE003),
    PAIR(FE004),
    PAIR(FE005),
    ...
    PAIR(FEE7D),
    PAIR(FEEA0),
    PAIR(FE331)});
  // clang-format on
#undef PAIR
};
&lt;/code&gt;
&lt;p&gt;Stop inlining a heavy user of InlinedVector operations.&lt;/p&gt;&lt;p&gt;Moved very long routine that was being inlined from .h file to .cc (no real performance benefit from inlining this).&lt;/p&gt;&lt;p&gt;reduction_ops_common.h&lt;/p&gt;&lt;code&gt;Status Simplify(const Tensor&amp;amp; data, const Tensor&amp;amp; axis,
                const bool keep_dims) {
  ... Eighty line routine body ...
}
&lt;/code&gt;
&lt;code&gt;Status Simplify(const Tensor&amp;amp; data, const Tensor&amp;amp; axis, const bool keep_dims);
&lt;/code&gt;

&lt;p&gt;Modern machines have many cores, and they are often underutilized. Expensive work may therefore be completed faster by parallelizing it. The most common approach is to process different items in parallel and combine the results when done. Typically, the items are first partitioned into batches to avoid paying the cost of running something in parallel per item.&lt;/p&gt;&lt;p&gt;Four-way parallelization improves the rate of encoding tokens by ~3.6x.&lt;/p&gt;&lt;p&gt;blocked-token-coder.cc&lt;/p&gt;&lt;code&gt;MutexLock l(&amp;amp;encoder_threads_lock);
if (encoder_threads == NULL) {
  encoder_threads = new ThreadPool(NumCPUs());
  encoder_threads-&amp;gt;SetStackSize(262144);
  encoder_threads-&amp;gt;StartWorkers();
}
encoder_threads-&amp;gt;Add
    (NewCallback(this,
                 &amp;amp;BlockedTokenEncoder::EncodeRegionInThread,
                 region_tokens, N, region,
                 stats,
                 controller_-&amp;gt;GetClosureWithCost
                 (NewCallback(&amp;amp;DummyCallback), N)));
&lt;/code&gt;
&lt;p&gt;Parallelization improves decoding performance by 5x.&lt;/p&gt;&lt;p&gt;coding.cc&lt;/p&gt;&lt;code&gt;for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  RET_CHECK_OK(DecodeBulkForCluster(...);
}
&lt;/code&gt;
&lt;code&gt;struct SubTask {
  absl::Status result;
  absl::Notification done;
};

std::vector&amp;lt;SubTask&amp;gt; tasks(clusters-&amp;gt;size());
for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  options_.executor-&amp;gt;Schedule([&amp;amp;, c] {
    tasks[c].result = DecodeBulkForCluster(...);
    tasks[c].done.Notify();
  });
}
for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  tasks[c].done.WaitForNotification();
}
for (int c = 0; c &amp;lt; clusters-&amp;gt;size(); c++) {
  RETURN_IF_ERROR(tasks[c].result);
}
&lt;/code&gt;

&lt;p&gt;The effect on system performance should be measured carefully ‚Äì if spare CPU is not available, or if memory bandwidth is saturated, parallelization may not help, or may even hurt.&lt;/p&gt;&lt;p&gt;Avoid fine-grained locking to reduce the cost of Mutex operations in hot paths. Caveat: this should only be done if the change does not increase lock contention.&lt;/p&gt;&lt;p&gt;Acquire lock once to free entire tree of query nodes, rather than reacquiring lock for every node in tree.&lt;/p&gt;&lt;p&gt;mustang-query.cc&lt;/p&gt;&lt;code&gt;// Pool of query nodes
ThreadSafeFreeList&amp;lt;MustangQuery&amp;gt; pool_(256);
...
void MustangQuery::Release(MustangQuery* node) {
  if (node == NULL)
    return;
  for (int i=0; i &amp;lt; node-&amp;gt;children_-&amp;gt;size(); ++i)
    Release((*node-&amp;gt;children_)[i]);
  node-&amp;gt;children_-&amp;gt;clear();
  pool_.Delete(node);
}
&lt;/code&gt;
&lt;code&gt;// Pool of query nodes
Mutex pool_lock_;
FreeList&amp;lt;MustangQuery&amp;gt; pool_(256);
...
void MustangQuery::Release(MustangQuery* node) {
  if (node == NULL)
    return;
  MutexLock l(&amp;amp;pool_lock_);
  ReleaseLocked(node);
}

void MustangQuery::ReleaseLocked(MustangQuery* node) {
#ifndef NDEBUG
  pool_lock_.AssertHeld();
#endif
  if (node == NULL)
    return;
  for (int i=0; i &amp;lt; node-&amp;gt;children_-&amp;gt;size(); ++i)
    ReleaseLocked((*node-&amp;gt;children_)[i]);
  node-&amp;gt;children_-&amp;gt;clear();
  pool_.Delete(node);
}
&lt;/code&gt;
&lt;p&gt;Avoid expensive work inside critical sections. In particular, watch out for innocuous looking code that might be doing RPCs or accessing files.&lt;/p&gt;&lt;p&gt;Reduce number of cache lines touched in critical section.&lt;/p&gt;&lt;p&gt;Careful data structure adjustments reduce the number of cache lines accessed significantly and improve the performance of an ML training run by 3.3%.&lt;/p&gt;&lt;code&gt;~2 + O(num_outgoing edges)&lt;/code&gt;
(and for large graphs with many cores executing them there is also less TLB
pressure).&lt;p&gt;Avoid RPC while holding Mutex.&lt;/p&gt;&lt;p&gt;trainer.cc&lt;/p&gt;&lt;code&gt;{
  // Notify the parameter server that we are starting.
  MutexLock l(&amp;amp;lock_);
  model_ = model;
  MaybeRecordProgress(last_global_step_);
}
&lt;/code&gt;
&lt;code&gt;bool should_start_record_progress = false;
int64 step_for_progress = -1;
{
  // Notify the parameter server that we are starting.
  MutexLock l(&amp;amp;lock_);
  model_ = model;
  should_start_record_progress = ShouldStartRecordProgress();
  step_for_progress = last_global_step_;
}
if (should_start_record_progress) {
  StartRecordProgress(step_for_progress);
}
&lt;/code&gt;
&lt;p&gt;Also, be wary of expensive destructors that will run before a Mutex is unlocked (this can often happen when the Mutex unlock is triggered by a &lt;code&gt;~MutexUnlock&lt;/code&gt;.)
Declaring objects with expensive destructors before MutexLock may help (assuming
it is thread-safe).&lt;/p&gt;&lt;p&gt;Sometimes a data structure protected by a Mutex that is exhibiting high contention can be safely split into multiple shards, each shard with its own Mutex. (Note: this requires that there are no cross-shard invariants between the different shards.)&lt;/p&gt;&lt;p&gt;Shard a cache 16 ways which improves throughput under a multi-threaded load by ~2x.&lt;/p&gt;&lt;p&gt;cache.cc&lt;/p&gt;&lt;code&gt;class ShardedLRUCache : public Cache {
 private:
  LRUCache shard_[kNumShards];
  port::Mutex id_mutex_;
  uint64_t last_id_;

  static inline uint32_t HashSlice(const Slice&amp;amp; s) {
    return Hash(s.data(), s.size(), 0);
  }

  static uint32_t Shard(uint32_t hash) {
    return hash &amp;gt;&amp;gt; (32 - kNumShardBits);
  }
  ...
  virtual Handle* Lookup(const Slice&amp;amp; key) {
    const uint32_t hash = HashSlice(key);
    return shard_[Shard(hash)].Lookup(key, hash);
  }
&lt;/code&gt;
&lt;p&gt;Shard spanner data structure for tracking calls.&lt;/p&gt;&lt;p&gt;transaction_manager.cc&lt;/p&gt;&lt;code&gt;absl::MutexLock l(&amp;amp;active_calls_in_mu_);
ActiveCallMap::const_iterator iter = active_calls_in_.find(m-&amp;gt;tid());
if (iter != active_calls_in_.end()) {
  iter-&amp;gt;second.ExtractElements(&amp;amp;m-&amp;gt;tmp_calls_);
}
&lt;/code&gt;
&lt;code&gt;ActiveCalls::LockedShard shard(active_calls_in_, m-&amp;gt;tid());
const ActiveCallMap&amp;amp; active_calls_map = shard.active_calls_map();
ActiveCallMap::const_iterator iter = active_calls_map.find(m-&amp;gt;tid());
if (iter != active_calls_map.end()) {
  iter-&amp;gt;second.ExtractElements(&amp;amp;m-&amp;gt;tmp_calls_);
}
&lt;/code&gt;
&lt;p&gt;If the data structure in question is a map, consider using a concurrent hash map implementation instead.&lt;/p&gt;&lt;p&gt;Be careful with the information used for shard selection. If, for example, you use some bits of a hash value for shard selection and then those same bits end up being used again later, the latter use may perform poorly since it sees a skewed distribution of hash values.&lt;/p&gt;&lt;p&gt;Fix information used for shard selection to prevent hash table issues.&lt;/p&gt;&lt;p&gt;netmon_map_impl.h&lt;/p&gt;&lt;code&gt;ConnectionBucket* GetBucket(Index index) {
  // Rehash the hash to make sure we are not partitioning the buckets based on
  // the original hash. If num_buckets_ is a power of 2 that would drop the
  // entropy of the buckets.
  size_t original_hash = absl::Hash&amp;lt;Index&amp;gt;()(index);
  int hash = absl::Hash&amp;lt;size_t&amp;gt;()(original_hash) % num_buckets_;
  return &amp;amp;buckets_[hash];
}
&lt;/code&gt;
&lt;code&gt;ConnectionBucket* GetBucket(Index index) {
  absl::Hash&amp;lt;std::pair&amp;lt;Index, size_t&amp;gt;&amp;gt; hasher{};
  // Combine the hash with 42 to prevent shard selection using the same bits
  // as the underlying hashtable.
  return &amp;amp;buckets_[hasher({index, 42}) % num_buckets_];
}
&lt;/code&gt;
&lt;p&gt;Shard Spanner data structure used for tracking calls.&lt;/p&gt;&lt;p&gt;This CL partitions the ActiveCallMap into 64 shards. Each shard is protected by a separate mutex. A given transaction will be mapped to exactly one shard. A new interface LockedShard(tid) is added for accessing the ActiveCallMap for a transaction in a thread-safe manner. Example usage:&lt;/p&gt;&lt;p&gt;transaction_manager.cc&lt;/p&gt;&lt;code&gt;{
  absl::MutexLock l(&amp;amp;active_calls_in_mu_);
  delayed_locks_timer_ring_.Add(delayed_locks_flush_time_ms, tid);
}
&lt;/code&gt;
&lt;code&gt;{
  ActiveCalls::LockedShard shard(active_calls_in_, tid);
  shard.delayed_locks_timer_ring().Add(delayed_locks_flush_time_ms, tid);
}
&lt;/code&gt;
&lt;p&gt;The results show a 69% reduction in overall wall-clock time when running the benchmark with 8192 fibers&lt;/p&gt;&lt;code&gt;Benchmark                   Time(ns)        CPU(ns)     Iterations
------------------------------------------------------------------
BM_ActiveCalls/8k        11854633492     98766564676            10
BM_ActiveCalls/16k       26356203552    217325836709            10
&lt;/code&gt;
&lt;code&gt;Benchmark                   Time(ns)        CPU(ns)     Iterations
------------------------------------------------------------------
BM_ActiveCalls/8k         3696794642     39670670110            10
BM_ActiveCalls/16k        7366284437     79435705713            10
&lt;/code&gt;

&lt;p&gt;Explore whether handling multiple items at once using SIMD instructions available on modern CPUs can give speedups (e.g., see &lt;code&gt;absl::flat_hash_map&lt;/code&gt; discussion below in Bulk Operations
section).&lt;/p&gt;&lt;p&gt;If different threads access different mutable data, consider placing the different data items on different cache lines, e.g., in C++ using the &lt;code&gt;alignas&lt;/code&gt;
directive. However, these directives are easy to misuse and may increase object
sizes significantly, so make sure performance measurements justify their use.&lt;/p&gt;&lt;p&gt;Segregate commonly mutated fields in a different cache line than other fields.&lt;/p&gt;&lt;p&gt;histogram.h&lt;/p&gt;&lt;code&gt;HistogramOptions options_;
...
internal::HistogramBoundaries *boundaries_;
...
std::vector&amp;lt;double&amp;gt; buckets_;

double min_;             // Minimum.
double max_;             // Maximum.
double count_;           // Total count of occurrences.
double sum_;             // Sum of values.
double sum_of_squares_;  // Sum of squares of values.
...
RegisterVariableExporter *exporter_;
&lt;/code&gt;
&lt;code&gt;  HistogramOptions options_;
  ...
  internal::HistogramBoundaries *boundaries_;
  ...
  RegisterVariableExporter *exporter_;
  ...
  // Place the following fields in a dedicated cacheline as they are frequently
  // mutated, so we can avoid potential false sharing.
  ...
#ifndef SWIG
  alignas(ABSL_CACHELINE_SIZE)
#endif
  std::vector&amp;lt;double&amp;gt; buckets_;

  double min_;             // Minimum.
  double max_;             // Maximum.
  double count_;           // Total count of occurrences.
  double sum_;             // Sum of values.
  double sum_of_squares_;  // Sum of squares of values.
&lt;/code&gt;
&lt;p&gt;Process small work items inline instead of on device thread pool.&lt;/p&gt;&lt;p&gt;cast_op.cc&lt;/p&gt;&lt;code&gt;template &amp;lt;typename Device, typename Tout, typename Tin&amp;gt;
void CastMaybeInline(const Device&amp;amp; d, typename TTypes&amp;lt;Tout&amp;gt;::Flat o,
                     typename TTypes&amp;lt;Tin&amp;gt;::ConstFlat i) {
  if (o.size() * (sizeof(Tin) + sizeof(Tout)) &amp;lt; 16384) {
    // Small cast on a CPU: do inline
    o = i.template cast&amp;lt;Tout&amp;gt;();
  } else {
    o.device(d) = i.template cast&amp;lt;Tout&amp;gt;();
  }
}
&lt;/code&gt;
&lt;p&gt;Channels can be unbuffered which means that a writer blocks until a reader is ready to pick up an item. Unbuffered channels can be useful when the channel is being used for synchronization, but not when the channel is being used to increase parallelism.&lt;/p&gt;&lt;p&gt;Sometimes lock-free data structures can make a difference over more conventional mutex-protected data structures. However, direct atomic variable manipulation can be dangerous. Prefer higher-level abstractions.&lt;/p&gt;&lt;p&gt;Use lock-free map to manage a cache of RPC channels.&lt;/p&gt;&lt;p&gt;Entries in an RPC stub cache are read thousands of times a second and modified rarely. Switching to an appropriate lock-free map reduces search latency by 3%-5%.&lt;/p&gt;&lt;p&gt;Use a fixed lexicon+lock-free hash map to speed-up determining IsValidTokenId.&lt;/p&gt;&lt;p&gt;dynamic_token_class_manager.h&lt;/p&gt;&lt;code&gt;mutable Mutex mutex_;

// The density of this hash map is guaranteed by the fact that the
// dynamic lexicon reuses previously allocated TokenIds before trying
// to allocate new ones.
dense_hash_map&amp;lt;TokenId, common::LocalTokenClassId&amp;gt; tid_to_cid_
    GUARDED_BY(mutex_);
&lt;/code&gt;
&lt;code&gt;// Read accesses to this hash-map should be done using
// 'epoch_gc_'::(EnterFast / LeaveFast). The writers should periodically
// GC the deleted entries, by simply invoking LockFreeHashMap::CreateGC.
typedef util::gtl::LockFreeHashMap&amp;lt;TokenId, common::LocalTokenClassId&amp;gt;
    TokenIdTokenClassIdMap;
TokenIdTokenClassIdMap tid_to_cid_;
&lt;/code&gt;
&lt;p&gt;Protobufs are a convenient representation of data, especially if the data will be sent over the wire or stored persistently. However, they can have significant performance costs. For example, a piece of code that fills in a list of 1000 points and then sums up the Y coordinates, speeds up by a factor of 20 when converted from protobufs to a C++ std::vector of structs!&lt;/p&gt;&lt;p&gt;Benchmark code for both versions.&lt;/p&gt;&lt;code&gt;name                old time/op  new time/op  delta
BenchmarkIteration  17.4¬µs ¬± 5%   0.8¬µs ¬± 1%  -95.30%  (p=0.000 n=11+12)
&lt;/code&gt;
&lt;p&gt;Protobuf version:&lt;/p&gt;&lt;code&gt;message PointProto {
  int32 x = 1;
  int32 y = 2;
}
message PointListProto {
  repeated PointProto points = 1;
}
&lt;/code&gt;
&lt;code&gt;void SumProto(const PointListProto&amp;amp; vec) {
  int sum = 0;
  for (const PointProto&amp;amp; p : vec.points()) {
    sum += p.y();
  }
  ABSL_VLOG(1) &amp;lt;&amp;lt; sum;
}

void BenchmarkIteration() {
  PointListProto points;
  points.mutable_points()-&amp;gt;Reserve(1000);
  for (int i = 0; i &amp;lt; 1000; i++) {
    PointProto* p = points.add_points();
    p-&amp;gt;set_x(i);
    p-&amp;gt;set_y(i * 2);
  }
  SumProto(points);
}
&lt;/code&gt;
&lt;p&gt;Non-protobuf version:&lt;/p&gt;&lt;code&gt;struct PointStruct {
  int x;
  int y;
};

void SumVector(const std::vector&amp;lt;PointStruct&amp;gt;&amp;amp; vec) {
  int sum = 0;
  for (const PointStruct&amp;amp; p : vec) {
    sum += p.y;
  }
  ABSL_VLOG(1) &amp;lt;&amp;lt; sum;
}

void BenchmarkIteration() {
  std::vector&amp;lt;PointStruct&amp;gt; points;
  points.reserve(1000);
  for (int i = 0; i &amp;lt; 1000; i++) {
    points.push_back({i, i * 2});
  }
  SumVector(points);
}
&lt;/code&gt;

&lt;p&gt;In addition, the protobuf version adds a few kilobytes of code and data to the binary, which may not seem like much, but adds up quickly in systems with many protobuf types. This increased size creates performance problems by creating i-cache and d-cache pressure.&lt;/p&gt;&lt;p&gt;Here are some tips related to protobuf performance:&lt;/p&gt;&lt;p&gt;Do not use protobufs unnecessarily.&lt;/p&gt;&lt;p&gt;Given the factor of 20 performance difference described above, if some data is never serialized or parsed, you probably should not put it in a protocol buffer. The purpose of protocol buffers is to make it easy to serialize and deserialize data structures, but they can have significant code-size, memory, and CPU overheads. Do not use them if all you want are some of the other niceties like &lt;code&gt;DebugString&lt;/code&gt; and copyability.&lt;/p&gt;&lt;p&gt;Avoid unnecessary message hierarchies.&lt;/p&gt;&lt;p&gt;Message hierarchy can be useful to organize information in a more readable fashion. However, the extra level of message hierarchy incurs overheads like memory allocations, function calls, cache misses, larger serialized messages, etc.&lt;/p&gt;&lt;p&gt;E.g., instead of:&lt;/p&gt;&lt;code&gt;message Foo {
  optional Bar bar = 1;
}
message Bar {
  optional Baz baz = 1;
}
message Baz {
  optional int32 count = 1;
}
&lt;/code&gt;
&lt;p&gt;Prefer:&lt;/p&gt;&lt;code&gt;message Foo {
  optional int32 count = 1;
}
&lt;/code&gt;
&lt;p&gt;A protocol buffer message corresponds to a message class in C++ generated code and emits a tag and the length of the payload on the wire. To carry an integer, the old form requires more allocations (and deallocations) and emits a larger amount of generated code. As a result, all protocol buffer operations (parsing, serialization, size, etc.) become more expensive, having to traverse the message tree. The new form does not have such overhead and is more efficient.&lt;/p&gt;&lt;p&gt;Use small field numbers for frequently occurring fields.&lt;/p&gt;&lt;p&gt;Protobufs use a variable length integer representation for the combination of field number and wire format (see the protobuf encoding documentation). This representation is 1 byte for field numbers between 1 and 15, and two bytes for field numbers between 16 and 2047. (Field numbers 2048 or greater should typically be avoided.)&lt;/p&gt;&lt;p&gt;Consider pre-reserving some small field numbers for future extension of performance-sensitive protobufs.&lt;/p&gt;&lt;p&gt;Choose carefully between int32, sint32, fixed32, and uint32 (and similarly for the 64 bit variants).&lt;/p&gt;&lt;p&gt;Generally, use &lt;code&gt;int32&lt;/code&gt; or &lt;code&gt;int64&lt;/code&gt;, but use &lt;code&gt;fixed32&lt;/code&gt; or &lt;code&gt;fixed64&lt;/code&gt; for large
values like hash codes and &lt;code&gt;sint32&lt;/code&gt; or &lt;code&gt;sint64&lt;/code&gt; for values are that are often
negative.&lt;/p&gt;&lt;p&gt;A varint occupies fewer bytes to encode small integers and can save space at the cost of more expensive decoding. However, it can take up more space for negative or large values. In that case, using fixed32 or fixed64 (instead of uint32 or uint64) reduces size with much cheaper encoding and decoding. For small negative integers, use sint32 or sint64 instead of int32 or int64.d&lt;/p&gt;&lt;p&gt;For proto2, pack repeated numeric fields by annotating them with &lt;code&gt;[packed=true]&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In proto2, repeated values are serialized as a sequence of (tag, value) pairs by default. This is inefficient because tags have to be decoded for every element.&lt;/p&gt;&lt;p&gt;Packed repeated primitives are serialized with the length of the payload first followed by values without tags. When using fixed-width values, we can avoid reallocations by knowing the final size the moment we start parsing; i.e., no reallocation cost. We still don't know how many varints are in the payload and may have to pay the reallocation cost.&lt;/p&gt;&lt;p&gt;In proto3, repeated fields are packed by default.&lt;/p&gt;&lt;p&gt;Packed works best with fixed-width values like fixed32, fixed64, float, double, etc. since the entire encoded length can be predetermined by multiplying the number of elements by the fixed value size, instead of having to calculate the length of each individual element.&lt;/p&gt;&lt;p&gt;Use &lt;code&gt;bytes&lt;/code&gt; instead for &lt;code&gt;string&lt;/code&gt; for binary data
and large values.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;string&lt;/code&gt; type holds UTF8-encoded text, and can sometimes require validation.
The &lt;code&gt;bytes&lt;/code&gt; type can hold an arbitrary sequence of bytes (non-text data) and is
often more appropriate as well as more efficient than &lt;code&gt;string&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Consider &lt;code&gt;string_type = VIEW&lt;/code&gt; to avoid copying.&lt;/p&gt;&lt;p&gt;Copying a big string or bytes field during parsing is expensive. Such cost can often be avoided by marking the field with &lt;code&gt;string_type = VIEW&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;message Image {
  ...
  bytes jpeg_encoding = 4 [features.(pb.cpp).string_type=VIEW];
}
&lt;/code&gt;
&lt;p&gt;Without the &lt;code&gt;VIEW&lt;/code&gt; annotation, when the protocol buffer is parsed, the
potentially large field contents are copied from the serialized protocol buffer
to a string object in memory. Depending on the number of string or bytes fields
and the size of those fields, the overhead of copying can be significant.&lt;/p&gt;&lt;p&gt;Instead of copying the big binary blobs, routines like &lt;code&gt;ParseFromStringWithAliasing&lt;/code&gt; use &lt;code&gt;absl::string_view&lt;/code&gt; to reference the original
backing string. Note that the backing string (the serialized protocol buffer)
must outlive the protocol buffer instance that contains the alias.&lt;/p&gt;&lt;p&gt;Consider using &lt;code&gt;Cord&lt;/code&gt; for large fields to reduce copying
costs.&lt;/p&gt;&lt;p&gt;Annotating large &lt;code&gt;bytes&lt;/code&gt; and &lt;code&gt;string&lt;/code&gt; fields with &lt;code&gt;[ctype=CORD]&lt;/code&gt; may reduce
copying costs. This annotation changes the representation of the field from
&lt;code&gt;std::string&lt;/code&gt; to &lt;code&gt;absl::Cord&lt;/code&gt;. &lt;code&gt;absl::Cord&lt;/code&gt; uses reference counting and
tree-based storage to reduce copying and appending costs. If a protocol buffer
is serialized to a cord, parsing a string or bytes field with &lt;code&gt;[ctype=CORD]&lt;/code&gt; can
avoid copying the field contents.&lt;/p&gt;&lt;code&gt;message Document {
  ...
  bytes html = 4 [ctype = CORD];
}
&lt;/code&gt;
&lt;p&gt;Performance of a Cord field depends on length distribution and access patterns. Use benchmarks to validate such changes.&lt;/p&gt;&lt;p&gt;Use protobuf arenas in C++ code.&lt;/p&gt;&lt;p&gt;Consider using arenas to save allocation and deallocation costs, especially for protobufs containing repeated, string, or message fields.&lt;/p&gt;&lt;p&gt;Message and string fields are heap-allocated (even if the top-level protocol buffer object is stack-allocated). If a protocol buffer message has a lot of sub message fields and string fields, allocation and deallocation cost can be significant. Arenas amortize allocation costs and makes deallocation virtually free. It also improves memory locality by allocating from contiguous chunks of memory.&lt;/p&gt;&lt;p&gt;Keep .proto files small&lt;/p&gt;&lt;p&gt;Do not put too many messages in a single .proto file. Once you rely on anything at all from a .proto file, the entire file will get pulled in by the linker even if it's mostly unused. This increases build times and binary sizes. You can use extensions and &lt;code&gt;Any&lt;/code&gt; to avoid creating hard dependencies on big
.proto files with many message types.&lt;/p&gt;&lt;p&gt;Consider storing protocol buffers in serialized form, even in memory.&lt;/p&gt;&lt;p&gt;In-memory protobuf objects have a large memory footprint (often 5x the wire format size), potentially spread across many cache lines. So if your application is going to keep many protobuf objects live for long periods of time, consider storing them in serialized form.&lt;/p&gt;&lt;p&gt;Avoid protobuf map fields.&lt;/p&gt;&lt;p&gt;Protobuf map fields have performance problems that usually outweigh the small syntactic convenience they provide. Prefer using non-protobuf maps initialized from protobuf contents:&lt;/p&gt;&lt;p&gt;msg.proto&lt;/p&gt;&lt;code&gt;map&amp;lt;string, bytes&amp;gt; env_variables = 5;
&lt;/code&gt;
&lt;code&gt;message Var {
  string key = 1;
  bytes value = 2;
}
repeated Var env_variables = 5;
&lt;/code&gt;

&lt;p&gt;Use protobuf message definition with a subset of the fields.&lt;/p&gt;&lt;p&gt;If you want to access only a few fields of a large message type, consider defining your own protocol buffer message type that mimics the original type, but only defines the fields that you care about. Here's an example:&lt;/p&gt;&lt;code&gt;message FullMessage {
  optional int32 field1 = 1;
  optional BigMessage field2 = 2;
  optional int32 field3 = 3;
  repeater AnotherBigMessage field4 = 4;
  ...
  optional int32 field100 = 100;
}
&lt;/code&gt;
&lt;code&gt;message SubsetMessage {
  optional int32 field3 = 3;
  optional int32 field88 = 88;
}
&lt;/code&gt;
&lt;p&gt;By parsing a serialized &lt;code&gt;FullMessage&lt;/code&gt; into a &lt;code&gt;SubsetMessage&lt;/code&gt;, only two out of a
hundred fields are parsed and others are treated as unknown fields. Consider
using APIs that discard unknown fields to improve performance even more when
appropriate.&lt;/p&gt;&lt;p&gt;Reuse protobuf objects when possible.&lt;/p&gt;&lt;p&gt;Declare protobuf objects outside loops so that their allocated storage can be reused across loop iterations.&lt;/p&gt;&lt;p&gt;Absl hash tables usually out-perform C++ standard library containers such as &lt;code&gt;std::map&lt;/code&gt; and
&lt;code&gt;std::unordered_map&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Speed up LanguageFromCode (use absl::flat_hash_map instead of a __gnu_cxx::hash_map).&lt;/p&gt;&lt;p&gt;languages.cc&lt;/p&gt;&lt;code&gt;class CodeToLanguage
    ...
    : public __gnu_cxx::hash_map&amp;lt;absl::string_view, i18n::languages::Language,
                                 CodeHash, CodeCompare&amp;gt; {
&lt;/code&gt;
&lt;code&gt;class CodeToLanguage
    ...
    : public absl::flat_hash_map&amp;lt;absl::string_view, i18n::languages::Language,
                                 CodeHash, CodeCompare&amp;gt; {
&lt;/code&gt;
&lt;p&gt;Benchmark results:&lt;/p&gt;&lt;code&gt;name               old time/op  new time/op  delta
BM_CodeToLanguage  19.4ns ¬± 1%  10.2ns ¬± 3%  -47.47%  (p=0.000 n=8+10)
&lt;/code&gt;

&lt;p&gt;Speed up stats publish/unpublish (an older change, so uses dense_hash_map instead of absl::flat_hash_map, which did not exist at the time).&lt;/p&gt;&lt;p&gt;publish.cc&lt;/p&gt;&lt;code&gt;typedef hash_map&amp;lt;uint64, Publication*&amp;gt; PublicationMap;
static PublicationMap* publications = NULL;
&lt;/code&gt;
&lt;code&gt;typedef dense_hash_map&amp;lt;uint64, Publication*&amp;gt; PublicationMap;;
static PublicationMap* publications GUARDED_BY(mu) = NULL;
&lt;/code&gt;
&lt;p&gt;Use dense_hash_map instead of hash_map for keeping track of SelectServer alarms (would use absl::flat_hash_map today).&lt;/p&gt;&lt;p&gt;alarmer.h&lt;/p&gt;&lt;code&gt;typedef hash_map&amp;lt;int, Alarm*&amp;gt; AlarmList;
&lt;/code&gt;
&lt;code&gt;typedef dense_hash_map&amp;lt;int, Alarm*&amp;gt; AlarmList;
&lt;/code&gt;
&lt;p&gt;absl::btree_map and absl::btree_set store multiple entries per tree node. This has a number of advantages over ordered C++ standard library containers such as &lt;code&gt;std::map&lt;/code&gt;. First, the pointer overhead of pointing to child tree nodes is often
significantly reduced. Second, because the entries or key/values are stored
consecutively in memory for a given btree tree node, cache efficiency is often
significantly better.&lt;/p&gt;&lt;p&gt;Use btree_set instead of std::set to represent a very heavily used work-queue.&lt;/p&gt;&lt;p&gt;register_allocator.h&lt;/p&gt;&lt;code&gt;using container_type = std::set&amp;lt;WorklistItem&amp;gt;;
&lt;/code&gt;
&lt;code&gt;using container_type = absl::btree_set&amp;lt;WorklistItem&amp;gt;;
&lt;/code&gt;

&lt;p&gt;&lt;code&gt;util::bitmap::InlinedBitvector&lt;/code&gt; can store short bit-vectors inline, and
therefore can often be a better choice than &lt;code&gt;std::vector&amp;lt;bool&amp;gt;&lt;/code&gt; or other bitmap
types.&lt;/p&gt;&lt;p&gt;Use InlinedBitVector instead of std::vector&amp;lt;bool&amp;gt;, and then use FindNextBitSet to find the next item of interest.&lt;/p&gt;&lt;p&gt;block_encoder.cc&lt;/p&gt;&lt;code&gt;vector&amp;lt;bool&amp;gt; live_reads(nreads);
...
for (int offset = 0; offset &amp;lt; b_.block_width(); offset++) {
  ...
  for (int r = 0; r &amp;lt; nreads; r++) {
    if (live_reads[r]) {
&lt;/code&gt;
&lt;code&gt;util::bitmap::InlinedBitVector&amp;lt;4096&amp;gt; live_reads(nreads);
...
for (int offset = 0; offset &amp;lt; b_.block_width(); offset++) {
  ...
  for (size_t r = 0; live_reads.FindNextSetBit(&amp;amp;r); r++) {
    DCHECK(live_reads[r]);
&lt;/code&gt;
&lt;p&gt;absl::InlinedVector stores a small number of elements inline (configurable via the second template argument). This enables small vectors up to this number of elements to generally have better cache efficiency and also to avoid allocating a backing store array at all when the number of elements is small.&lt;/p&gt;&lt;p&gt;Use InlinedVector instead of std::vector in various places.&lt;/p&gt;&lt;p&gt;bundle.h&lt;/p&gt;&lt;code&gt;class Bundle {
 public:
 ...
 private:
  // Sequence of (slotted instruction, unslotted immediate operands).
  std::vector&amp;lt;InstructionRecord&amp;gt; instructions_;
  ...
};
&lt;/code&gt;
&lt;code&gt;class Bundle {
 public:
 ...
 private:
  // Sequence of (slotted instruction, unslotted immediate operands).
  absl::InlinedVector&amp;lt;InstructionRecord, 2&amp;gt; instructions_;
  ...
};
&lt;/code&gt;

&lt;p&gt;Saves space by using a customized vector type that only supports sizes that fit in 32 bits.&lt;/p&gt;&lt;p&gt;Simple type change saves ~8TiB of memory in Spanner.&lt;/p&gt;&lt;p&gt;table_ply.h&lt;/p&gt;&lt;code&gt;class TablePly {
    ...
    // Returns the set of data columns stored in this file for this table.
    const std::vector&amp;lt;FamilyId&amp;gt;&amp;amp; modified_data_columns() const {
      return modified_data_columns_;
    }
    ...
   private:
    ...
    std::vector&amp;lt;FamilyId&amp;gt; modified_data_columns_;  // Data columns in the table.
&lt;/code&gt;
&lt;code&gt;#include "util/gtl/vector32.h"
    ...
    // Returns the set of data columns stored in this file for this table.
    absl::Span&amp;lt;const FamilyId&amp;gt; modified_data_columns() const {
      return modified_data_columns_;
    }
    ...

    ...
    // Data columns in the table.
    gtl::vector32&amp;lt;FamilyId&amp;gt; modified_data_columns_;
&lt;/code&gt;
&lt;p&gt;gtl::small_map uses an inline array to store up to a certain number of unique key-value-pair elements, but upgrades itself automatically to be backed by a user-specified map type when it runs out of space.&lt;/p&gt;&lt;p&gt;Use gtl::small_map in tflite_model.&lt;/p&gt;&lt;p&gt;tflite_model.cc&lt;/p&gt;&lt;code&gt;using ChoiceIdToContextMap = gtl::flat_hash_map&amp;lt;int, TFLiteContext*&amp;gt;;
&lt;/code&gt;
&lt;code&gt;using ChoiceIdToContextMap =
    gtl::small_map&amp;lt;gtl::flat_hash_map&amp;lt;int, TFLiteContext*&amp;gt;&amp;gt;;
&lt;/code&gt;
&lt;p&gt;gtl::small_ordered_set is an optimization for associative containers (such as std::set or absl::btree_multiset). It uses a fixed array to store a certain number of elements, then reverts to using a set or multiset when it runs out of space. For sets that are typically small, this can be considerably faster than using something like set directly, as set is optimized for large data sets. This change shrinks cache footprint and reduces critical section length.&lt;/p&gt;&lt;p&gt;Use gtl::small_ordered_set to hold set of listeners.&lt;/p&gt;&lt;p&gt;broadcast_stream.h&lt;/p&gt;&lt;code&gt;class BroadcastStream : public ParsedRtpTransport {
 ...
 private:
  ...
  std::set&amp;lt;ParsedRtpTransport*&amp;gt; listeners_ ABSL_GUARDED_BY(listeners_mutex_);
};
&lt;/code&gt;
&lt;code&gt;class BroadcastStream : public ParsedRtpTransport {
 ...
 private:
  ...
  using ListenersSet =
      gtl::small_ordered_set&amp;lt;std::set&amp;lt;ParsedRtpTransport*&amp;gt;, 10&amp;gt;;
  ListenersSet listeners_ ABSL_GUARDED_BY(listeners_mutex_);
&lt;/code&gt;
&lt;p&gt;&lt;code&gt;gtl::intrusive_list&amp;lt;T&amp;gt;&lt;/code&gt; is a doubly-linked list where the link pointers are
embedded in the elements of type T. It saves one cache line+indirection per
element when compared to &lt;code&gt;std::list&amp;lt;T*&amp;gt;&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Use intrusive_list to keep track of inflight requests for each index row update.&lt;/p&gt;&lt;p&gt;row-update-sender-inflight-set.h&lt;/p&gt;&lt;code&gt;std::set&amp;lt;int64&amp;gt; inflight_requests_ GUARDED_BY(mu_);
&lt;/code&gt;
&lt;code&gt;class SeqNum : public gtl::intrusive_link&amp;lt;SeqNum&amp;gt; {
  ...
  int64 val_ = -1;
  ...
};
...
gtl::intrusive_list&amp;lt;SeqNum&amp;gt; inflight_requests_ GUARDED_BY(mu_);
&lt;/code&gt;
&lt;p&gt;Even though &lt;code&gt;absl::Status&lt;/code&gt; and &lt;code&gt;absl::StatusOr&lt;/code&gt; types are fairly efficient, they
have a non-zero overhead even in the success path and should therefore be
avoided for hot routines that don‚Äôt need to return any meaningful error details
(or perhaps never even fail!):&lt;/p&gt;&lt;p&gt;Avoid StatusOr&amp;lt;int64&amp;gt; return type for RoundUpToAlignment() function.&lt;/p&gt;&lt;p&gt;best_fit_allocator.cc&lt;/p&gt;&lt;code&gt;absl::StatusOr&amp;lt;int64&amp;gt; BestFitAllocator::RoundUpToAlignment(int64 bytes) const {
  TPU_RET_CHECK_GE(bytes, 0);

  const int64 max_aligned = MathUtil::RoundDownTo&amp;lt;int64&amp;gt;(
      std::numeric_limits&amp;lt;int64&amp;gt;::max(), alignment_in_bytes_);
  if (bytes &amp;gt; max_aligned) {
    return util::ResourceExhaustedErrorBuilder(ABSL_LOC)
           &amp;lt;&amp;lt; "Attempted to allocate "
           &amp;lt;&amp;lt; strings::HumanReadableNumBytes::ToString(bytes)
           &amp;lt;&amp;lt; " which after aligning to "
           &amp;lt;&amp;lt; strings::HumanReadableNumBytes::ToString(alignment_in_bytes_)
           &amp;lt;&amp;lt; " cannot be expressed as an int64.";
  }

  return MathUtil::RoundUpTo&amp;lt;int64&amp;gt;(bytes, alignment_in_bytes_);
}
&lt;/code&gt;
&lt;p&gt;best_fit_allocator.h&lt;/p&gt;&lt;code&gt;// Rounds bytes up to nearest multiple of alignment_.
// REQUIRES: bytes &amp;gt;= 0.
// REQUIRES: result does not overflow int64.
// REQUIRES: alignment_in_bytes_ is a power of 2 (checked in constructor).
int64 RoundUpToAlignment(int64 bytes) const {
  DCHECK_GE(bytes, 0);
  DCHECK_LE(bytes, max_aligned_bytes_);
  int64 result =
      ((bytes + (alignment_in_bytes_ - 1)) &amp;amp; ~(alignment_in_bytes_ - 1));
  DCHECK_EQ(result, MathUtil::RoundUpTo&amp;lt;int64&amp;gt;(bytes, alignment_in_bytes_));
  return result;
}
&lt;/code&gt;
&lt;p&gt;Add ShapeUtil::ForEachIndexNoStatus to avoid creating a Status return object for every element of a tensor.&lt;/p&gt;&lt;p&gt;shape_util.h&lt;/p&gt;&lt;code&gt;using ForEachVisitorFunction =
    absl::FunctionRef&amp;lt;StatusOr&amp;lt;bool&amp;gt;(absl::Span&amp;lt;const int64_t&amp;gt;)&amp;gt;;
    ...
static void ForEachIndex(const Shape&amp;amp; shape, absl::Span&amp;lt;const int64_t&amp;gt; base,
                         absl::Span&amp;lt;const int64_t&amp;gt; count,
                         absl::Span&amp;lt;const int64_t&amp;gt; incr,
                         const ForEachVisitorFunction&amp;amp; visitor_function);

&lt;/code&gt;
&lt;code&gt;using ForEachVisitorFunctionNoStatus =
    absl::FunctionRef&amp;lt;bool(absl::Span&amp;lt;const int64_t&amp;gt;)&amp;gt;;
    ...
static void ForEachIndexNoStatus(
    const Shape&amp;amp; shape, absl::Span&amp;lt;const int64_t&amp;gt; base,
    absl::Span&amp;lt;const int64_t&amp;gt; count, absl::Span&amp;lt;const int64_t&amp;gt; incr,
    const ForEachVisitorFunctionNoStatus&amp;amp; visitor_function);
&lt;/code&gt;
&lt;p&gt;literal.cc&lt;/p&gt;&lt;code&gt;ShapeUtil::ForEachIndex(
    result_shape, [&amp;amp;](absl::Span&amp;lt;const int64_t&amp;gt; output_index) {
      for (int64_t i = 0, end = dimensions.size(); i &amp;lt; end; ++i) {
        scratch_source_index[i] = output_index[dimensions[i]];
      }
      int64_t dest_index = IndexUtil::MultidimensionalIndexToLinearIndex(
          result_shape, output_index);
      int64_t source_index = IndexUtil::MultidimensionalIndexToLinearIndex(
          shape(), scratch_source_index);
      memcpy(dest_data + primitive_size * dest_index,
             source_data + primitive_size * source_index, primitive_size);
      return true;
    });
&lt;/code&gt;
&lt;code&gt;ShapeUtil::ForEachIndexNoStatus(
    result_shape, [&amp;amp;](absl::Span&amp;lt;const int64_t&amp;gt; output_index) {
      // Compute dest_index
      int64_t dest_index = IndexUtil::MultidimensionalIndexToLinearIndex(
          result_shape, result_minor_to_major, output_index);

      // Compute source_index
      int64_t source_index;
      for (int64_t i = 0, end = dimensions.size(); i &amp;lt; end; ++i) {
        scratch_source_array[i] = output_index[dimensions[i]];
      }
      if (src_shape_dims == 1) {
        // Fast path for this case
        source_index = scratch_source_array[0];
        DCHECK_EQ(source_index,
                  IndexUtil::MultidimensionalIndexToLinearIndex(
                      src_shape, src_minor_to_major, scratch_source_span));
      } else {
        source_index = IndexUtil::MultidimensionalIndexToLinearIndex(
            src_shape, src_minor_to_major, scratch_source_span);
      }
      // Move one element from source_index in source to dest_index in dest
      memcpy(dest_data + PRIMITIVE_SIZE * dest_index,
             source_data + PRIMITIVE_SIZE * source_index, PRIMITIVE_SIZE);
      return true;
    });
&lt;/code&gt;
&lt;p&gt;In TF_CHECK_OK, avoid creating Ok object in order to test for ok().&lt;/p&gt;&lt;p&gt;status.h&lt;/p&gt;&lt;code&gt;#define TF_CHECK_OK(val) CHECK_EQ(::tensorflow::Status::OK(), (val))
#define TF_QCHECK_OK(val) QCHECK_EQ(::tensorflow::Status::OK(), (val))
&lt;/code&gt;
&lt;code&gt;extern tensorflow::string* TfCheckOpHelperOutOfLine(
    const ::tensorflow::Status&amp;amp; v, const char* msg);
inline tensorflow::string* TfCheckOpHelper(::tensorflow::Status v,
                                           const char* msg) {
  if (v.ok()) return nullptr;
  return TfCheckOpHelperOutOfLine(v, msg);
}
#define TF_CHECK_OK(val)                                           \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(FATAL) &amp;lt;&amp;lt; *(_result)
#define TF_QCHECK_OK(val)                                          \
  while (tensorflow::string* _result = TfCheckOpHelper(val, #val)) \
  LOG(QFATAL) &amp;lt;&amp;lt; *(_result)
&lt;/code&gt;
&lt;p&gt;Remove StatusOr from the hot path of remote procedure calls (RPCs).&lt;/p&gt;&lt;p&gt;Removal of StatusOr from a hot path eliminated a 14% CPU regression in RPC benchmarks caused by an earlier change.&lt;/p&gt;&lt;p&gt;privacy_context.h&lt;/p&gt;&lt;code&gt;absl::StatusOr&amp;lt;privacy::context::PrivacyContext&amp;gt; GetRawPrivacyContext(
    const CensusHandle&amp;amp; h);
&lt;/code&gt;
&lt;p&gt;privacy_context_statusfree.h&lt;/p&gt;&lt;code&gt;enum class Result {
  kSuccess,
  kNoRootScopedData,
  kNoPrivacyContext,
  kNoDDTContext,
  kDeclassified,
  kNoPrequestContext
};
...
Result GetRawPrivacyContext(const CensusHandle&amp;amp; h,
                            PrivacyContext* privacy_context);
&lt;/code&gt;
&lt;p&gt;If possible, handle many items at once rather than just one at a time.&lt;/p&gt;&lt;p&gt;absl::flat_hash_map compares one hash byte per key from a group of keys using a single SIMD instruction.&lt;/p&gt;&lt;p&gt;See Swiss Table Design Notes and related CppCon 2017 and CppCon 2019 talks by Matt Kulukundis.&lt;/p&gt;&lt;p&gt;raw_hash_set.h&lt;/p&gt;&lt;code&gt;// Returns a bitmask representing the positions of slots that match hash.
BitMask&amp;lt;uint32_t&amp;gt; Match(h2_t hash) const {
  auto ctrl = _mm_loadu_si128(reinterpret_cast&amp;lt;const __m128i*&amp;gt;(pos));
  auto match = _mm_set1_epi8(hash);
  return BitMask&amp;lt;uint32_t&amp;gt;(_mm_movemask_epi8(_mm_cmpeq_epi8(match, ctrl)));
}
&lt;/code&gt;
&lt;p&gt;Do single operations to deal with many bytes and fix things up, rather than checking every byte what to do.&lt;/p&gt;&lt;p&gt;ordered-code.cc&lt;/p&gt;&lt;code&gt;int len = 0;
while (val &amp;gt; 0) {
  len++;
  buf[9 - len] = (val &amp;amp; 0xff);
  val &amp;gt;&amp;gt;= 8;
}
buf[9 - len - 1] = (unsigned char)len;
len++;
FastStringAppend(dest, reinterpret_cast&amp;lt;const char*&amp;gt;(buf + 9 - len), len);
&lt;/code&gt;
&lt;code&gt;BigEndian::Store(val, buf + 1);  // buf[0] may be needed for length
const unsigned int length = OrderedNumLength(val);
char* start = buf + 9 - length - 1;
*start = length;
AppendUpto9(dest, start, length + 1);
&lt;/code&gt;
&lt;p&gt;Improve Reed-Solomon processing speed by handling multiple interleaved input buffers more efficiently in chunks.&lt;/p&gt;&lt;code&gt;Run on (12 X 3501 MHz CPUs); 2016-09-27T16:04:55.065995192-04:00
CPU: Intel Haswell with HyperThreading (6 cores) dL1:32KB dL2:256KB dL3:15MB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_OneOutput/3/2                      466867    351818    +24.6%
BM_OneOutput/4/2                      563130    474756    +15.7%
BM_OneOutput/5/3                      815393    688820    +15.5%
BM_OneOutput/6/3                      897246    780539    +13.0%
BM_OneOutput/8/4                     1270489   1137149    +10.5%
BM_AllOutputs/3/2                     848772    642942    +24.3%
BM_AllOutputs/4/2                    1067647    638139    +40.2%
BM_AllOutputs/5/3                    1739135   1151369    +33.8%
BM_AllOutputs/6/3                    2045817   1456744    +28.8%
BM_AllOutputs/8/4                    3012958   2484937    +17.5%
BM_AllOutputsSetUpOnce/3/2            717310    493371    +31.2%
BM_AllOutputsSetUpOnce/4/2            833866    600060    +28.0%
BM_AllOutputsSetUpOnce/5/3           1537870   1137357    +26.0%
BM_AllOutputsSetUpOnce/6/3           1802353   1398600    +22.4%
BM_AllOutputsSetUpOnce/8/4           3166930   2455973    +22.4%
&lt;/code&gt;

&lt;p&gt;Decode four integers at a time (circa 2004).&lt;/p&gt;&lt;p&gt;Introduced a GroupVarInt format that encodes/decodes groups of 4 variable-length integers at a time in 5-17 bytes, rather than one integer at a time. Decoding one group of 4 integers in the new format takes ~1/3rd the time of decoding 4 individually varint-encoded integers.&lt;/p&gt;&lt;p&gt;groupvarint.cc&lt;/p&gt;&lt;code&gt;const char* DecodeGroupVar(const char* p, int N, uint32* dest) {
  assert(groupvar_initialized);
  assert(N % 4 == 0);
  while (N) {
    uint8 tag = *p;
    p++;

    uint8* lenptr = &amp;amp;groupvar_table[tag].length[0];

#define GET_NEXT                                        \
    do {                                                \
      uint8 len = *lenptr;                              \
      *dest = UNALIGNED_LOAD32(p) &amp;amp; groupvar_mask[len]; \
      dest++;                                           \
      p += len;                                         \
      lenptr++;                                         \
    } while (0)
    GET_NEXT;
    GET_NEXT;
    GET_NEXT;
    GET_NEXT;
#undef GET_NEXT

    N -= 4;
  }
  return p;
}
&lt;/code&gt;
&lt;p&gt;Encode groups of 4 k-bit numbers at a time.&lt;/p&gt;&lt;p&gt;Added KBitStreamEncoder and KBitStreamDecoder classes to encode/decode 4 k-bit numbers at a time into a bit stream. Since K is known at compile time, the encoding and decoding can be quite efficient. E.g., since four numbers are encoded at a time, the code can assume that the stream is always byte-aligned (for even k), or nibble-aligned (for odd k).&lt;/p&gt;&lt;p&gt;Sometimes a single CL contains a number of performance-improving changes that use many of the preceding techniques. Looking at the kinds of changes in these CLs is sometimes a good way to get in the mindset of making general changes to speed up the performance of some part of a system after that has been identified as a bottleneck.&lt;/p&gt;&lt;p&gt;Speed up GPU memory allocator by ~40%.&lt;/p&gt;&lt;p&gt;36-48% speedup in allocation/deallocation speed for GPUBFCAllocator:&lt;/p&gt;&lt;p&gt;Identify chunks by a handle number, rather than by a pointer to a Chunk. Chunk data structures are now allocated in a &lt;code&gt;vector&amp;lt;Chunk&amp;gt;&lt;/code&gt;, and a handle
is an index into this vector to refer to a particular chunk. This allows the
next and prev pointers in Chunk to be ChunkHandle (4 bytes), rather than
&lt;code&gt;Chunk*&lt;/code&gt; (8 bytes).&lt;/p&gt;&lt;p&gt;When a Chunk object is no longer in use, we maintain a free list of Chunk objects, whose head is designated by ChunkHandle &lt;code&gt;free_chunks_list_&lt;/code&gt;, and
with the &lt;code&gt;Chunk-&amp;gt;next&lt;/code&gt; pointing to the next free list entry. Together with
(1), this allows us to avoid heap allocation/deallocation of Chunk objects
in the allocator, except (rarely) when the &lt;code&gt;vector&amp;lt;Chunk&amp;gt;&lt;/code&gt; grows. It also
makes all the memory for Chunk objects contiguous.&lt;/p&gt;&lt;p&gt;Rather than having the bins_ data structure be a std::set and using lower_bound to locate the appropriate bin given a byte_size, we instead have an array of bins, indexed by a function that is log‚ÇÇ(byte_size/256). This allows the bin to be located with a few bit operations, rather than a binary search tree lookup. It also allows us to allocate the storage for all the Bin data structures in a contiguous array, rather than in many different cache lines. This reduces the number of cache lines that must be moved around between cores when multiple threads are doing allocations.&lt;/p&gt;&lt;p&gt;Added fast path to GPUBFCAllocator::AllocateRaw that first tries to allocate memory without involving the retry_helper_. If an initial attempt fails (returns nullptr), then we go through the retry_helper_, but normally we can avoid several levels of procedure calls as well as the allocation/deallocation of a std::function with several arguments.&lt;/p&gt;&lt;p&gt;Commented out most of the VLOG calls. These can be reenabled selectively when needed for debugging purposes by uncommenting and recompiling.&lt;/p&gt;&lt;p&gt;Added multi-threaded benchmark to test allocation under contention.&lt;/p&gt;&lt;p&gt;Speeds up ptb_word_lm on my desktop machine with a Titan X card from 8036 words per second to 8272 words per second (+2.9%).&lt;/p&gt;&lt;code&gt;Run on (40 X 2801 MHz CPUs); 2016/02/16-15:12:49
CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                          Base (ns)  New (ns) Improvement
------------------------------------------------------------------
BM_Allocation                            347       184    +47.0%
BM_AllocationThreaded/1                  351       181    +48.4%
BM_AllocationThreaded/4                 2470      1975    +20.0%
BM_AllocationThreaded/16               11846      9507    +19.7%
BM_AllocationDelayed/1                   392       199    +49.2%
BM_AllocationDelayed/10                  285       169    +40.7%
BM_AllocationDelayed/100                 245       149    +39.2%
BM_AllocationDelayed/1000                238       151    +36.6%
&lt;/code&gt;

&lt;p&gt;Speed up Pathways throughput by ~20% via a set of miscellaneous changes.&lt;/p&gt;&lt;p&gt;Unified a bunch of special fast descriptor parsing functions into a single ParsedDescriptor class and use this class in more places to avoid expensive full parse calls.&lt;/p&gt;&lt;p&gt;Change several protocol buffer fields from string to bytes (avoids unnecessary utf-8 checks and associated error handling code).&lt;/p&gt;&lt;p&gt;DescriptorProto.inlined_contents is now a string, not a Cord (it is expected to be used only for small-ish tensors). This necessitated the addition of a bunch of copying helpers in tensor_util.cc (need to now support both strings and Cords).&lt;/p&gt;&lt;p&gt;Use flat_hash_map instead of std::unordered_map in a few places.&lt;/p&gt;&lt;p&gt;Added MemoryManager::LookupMany for use by Stack op instead of calling Lookup per batch element. This change reduces setup overhead like locking.&lt;/p&gt;&lt;p&gt;Removed some unnecessary string creation in TransferDispatchOp.&lt;/p&gt;&lt;p&gt;Performance results for transferring a batch of 1000 1KB tensors from one component to another in the same process:&lt;/p&gt;&lt;code&gt;Before: 227.01 steps/sec
After:  272.52 steps/sec (+20% throughput)
&lt;/code&gt;

&lt;p&gt;~15% XLA compiler performance improvement through a series of changes.&lt;/p&gt;&lt;p&gt;Some changes to speed up XLA compilation:&lt;/p&gt;&lt;p&gt;In SortComputationsByContent, return false if a == b in comparison function, to avoid serializing and fingerprinting long computation strings.&lt;/p&gt;&lt;p&gt;Turn CHECK into DCHECK to avoid touching an extra cache line in HloComputation::ComputeInstructionPostOrder&lt;/p&gt;&lt;p&gt;Avoid making an expensive copy of the front instruction in CoreSequencer::IsVectorSyncHoldSatisfied().&lt;/p&gt;&lt;p&gt;Rework 2-argument HloComputation::ToString and HloComputation::ToCord routines to do the bulk of the work in terms of appending to std::string, rather than appending to a Cord.&lt;/p&gt;&lt;p&gt;Change PerformanceCounterSet::Increment to just do a single hash table lookup rather than two.&lt;/p&gt;&lt;p&gt;Streamline Scoreboard::Update code&lt;/p&gt;&lt;p&gt;Overall speedup of 14% in XLA compilation time for one important model.&lt;/p&gt;&lt;p&gt;Speed up low level logging in Google Meet application code.&lt;/p&gt;&lt;p&gt;Speed up ScopedLogId, which is on the critical path for each packet.&lt;/p&gt;&lt;code&gt;LOG_EVERY_N(ERROR, ...)&lt;/code&gt; messages that seemed to be there only
to see if invariants were violated.&lt;code&gt;LOG_EVERY_N_SECONDS(ERROR, ...)&lt;/code&gt; statements, they are now small enough to
inline.&lt;code&gt;InlinedVector&amp;lt;...&amp;gt;&lt;/code&gt; for maintaining the thread local state. Since we
never were growing beyond size 4 anyway, the InlinedVector's functionality
was more general than needed.&lt;code&gt;Base: Baseline plus the code in scoped_logid_test.cc to add the benchmark
New: This changelist

CPU: Intel Ivybridge with HyperThreading (20 cores) dL1:32KB dL2:256KB dL3:25MB
Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_ScopedLogId/threads:1                               8           4    +52.6%
BM_ScopedLogId/threads:2                               8           4    +51.9%
BM_ScopedLogId/threads:4                               8           4    +52.9%
BM_ScopedLogId/threads:8                               8           4    +52.1%
BM_ScopedLogId/threads:16                             11           6    +44.0%

&lt;/code&gt;

&lt;p&gt;Reduce XLA compilation time by ~31% by improving Shape handling.&lt;/p&gt;&lt;p&gt;Several changes to improve XLA compiler performance:&lt;/p&gt;&lt;p&gt;Improved performance of ShapeUtil::ForEachIndex... iteration in a few ways:&lt;/p&gt;&lt;p&gt;In ShapeUtil::ForEachState, save just pointers to the arrays represented by the spans, rather than the full span objects.&lt;/p&gt;&lt;p&gt;Pre-form a ShapeUtil::ForEachState::indexes_span pointing at the ShapeUtil::ForEachState::indexes vector, rather than constructing this span from the vector on every loop iteration.&lt;/p&gt;&lt;p&gt;Save a ShapeUtil::ForEachState::indexes_ptr pointer to the backing store of the ShapeUtil::ForEachState::indexes vector, allowing simple array operations in ShapeUtil::ForEachState::IncrementDim(), rather than more expensive vector::operator[] operations.&lt;/p&gt;&lt;p&gt;Save a ShapeUtil::ForEachState::minor_to_major array pointer initialized in the constructor by calling shape.layout().minor_to_major().data() rather than calling LayoutUtil::Minor(...) for each dimension for each iteration.&lt;/p&gt;&lt;p&gt;Inlined the ShapeUtil::ForEachState constructor and the ShapeUtil::ForEachState::IncrementDim() routines&lt;/p&gt;&lt;p&gt;Improved the performance of ShapeUtil::ForEachIndex iteration for call sites that don't need the functionality of returning a Status in the passed in function. Did this by introducing ShapeUtil::ForEachIndexNoStatus variants, which accept a ForEachVisitorFunctionNoStatus (which returns a plain bool). This is faster than the ShapeUtil::ForEachIndex routines, which accept a ForEachVisitorFunction (which returns a &lt;code&gt;StatusOr&amp;lt;bool&amp;gt;&lt;/code&gt;, which requires an
expensive &lt;code&gt;StatusOr&amp;lt;bool&amp;gt;&lt;/code&gt; destructor call per element that we iterate
over).&lt;/p&gt;&lt;p&gt;Improved performance of LiteralBase::Broadcast in several ways:&lt;/p&gt;&lt;p&gt;Introduced templated BroadcastHelper routine in literal.cc that is specialized for different primitive byte sizes (without this, primitive_size was a runtime variable and so the compiler couldn't do a very good job of optimizing the memcpy that occurred per element, and would invoke the general memcpy path that assumes the byte count is fairly large, even though in our case it is a tiny power of 2 (typically 1, 2, 4, or 8)).&lt;/p&gt;&lt;p&gt;Avoided all but one of ~(5 + num_dimensions + num_result_elements) virtual calls per Broadcast call by making a single call to 'shape()' at the beginning of the LiteralBase::Broadcast routine. The innocuous looking 'shape()' calls that were sprinkled throughout end up boiling down to "root_piece().subshape()", where subshape() is a virtual function.&lt;/p&gt;&lt;p&gt;In the BroadcastHelper routine, Special-cased the source dimensions being one and avoided a call to IndexUtil::MultiDimensionalIndexToLinearIndex for this case.&lt;/p&gt;&lt;p&gt;In BroadcastHelper, used a scratch_source_array pointer variable that points into the backing store of the scratch_source_index vector, and used that directly to avoid vector::operator[] operations inside the per-element code. Also pre-computed a scratch_source_span that points to the scratch_source_index vector outside the per-element loop in BroadcastHelper, to avoid constructing a span from the vector on each element.&lt;/p&gt;&lt;p&gt;Introduced new three-argument variant of IndexUtil::MultiDimensionalIndexToLinearIndex where the caller passes in the minor_to_major span associated with the shape argument. Used this in BroadcastHelper to compute this for the src and dst shapes once per Broadcast, rather than once per element copied.&lt;/p&gt;&lt;p&gt;In ShardingPropagation::GetShardingFromUser, for the HloOpcode::kTuple case, only call user.sharding().GetSubSharding(...) if we have found the operand to be of interest. Avoiding calling it eagerly reduces CPU time in this routine for one lengthy compilation from 43.7s to 2.0s.&lt;/p&gt;&lt;p&gt;Added benchmarks for ShapeUtil::ForEachIndex and Literal::Broadcast and for the new ShapeUtil::ForEachIndexNoStatus.&lt;/p&gt;&lt;code&gt;Base is with the benchmark additions of
BM_ForEachIndex and BM_BroadcastVectorToMatrix (and BUILD file change to add
benchmark dependency), but no other changes.

New is this cl

Run on (72 X 1357.56 MHz CPU s) CPU Caches: L1 Data 32 KiB (x36)
L1 Instruction 32 KiB (x36) L2 Unified 1024 KiB (x36) L3 Unified 25344 KiB (x2)

Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_MakeShape                                       18.40       18.90     -2.7%
BM_MakeValidatedShape                              35.80       35.60     +0.6%
BM_ForEachIndex/0                                  57.80       55.80     +3.5%
BM_ForEachIndex/1                                  90.90       85.50     +5.9%
BM_ForEachIndex/2                               1973606     1642197     +16.8%
&lt;/code&gt;
&lt;p&gt;The newly added ForEachIndexNoStatus is considerably faster than the ForEachIndex variant (it only exists in this new cl, but the benchmark work that is done by BM_ForEachIndexNoStatus/NUM is comparable to the BM_ForEachIndex/NUM results above).&lt;/p&gt;&lt;code&gt;Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_ForEachIndexNoStatus/0                             0        46.90    ----
BM_ForEachIndexNoStatus/1                             0        65.60    ----
BM_ForEachIndexNoStatus/2                             0     1001277     ----
&lt;/code&gt;
&lt;p&gt;Broadcast performance improves by ~58%.&lt;/p&gt;&lt;code&gt;Benchmark                                      Base (ns)    New (ns) Improvement
----------------------------------------------------------------------------
BM_BroadcastVectorToMatrix/16/16                   5556        2374     +57.3%
BM_BroadcastVectorToMatrix/16/1024               319510      131075     +59.0%
BM_BroadcastVectorToMatrix/1024/1024           20216949     8408188     +58.4%
&lt;/code&gt;
&lt;p&gt;Macro results from doing ahead-of-time compilation of a large language model (program does more than just the XLA compilation, but spends a bit less than half its time in XLA-related code):&lt;/p&gt;&lt;p&gt;Baseline program overall: 573 seconds With this cl program overall: 465 seconds (+19% improvement)&lt;/p&gt;&lt;p&gt;Time spent in compiling the two largest XLA programs in running this program:&lt;/p&gt;&lt;p&gt;Baseline: 141s + 143s = 284s With this CL: 99s + 95s = 194s (+31% improvement)&lt;/p&gt;&lt;p&gt;Reduce compilation time for large programs by ~22% in Plaque (a distributed execution framework).&lt;/p&gt;&lt;p&gt;Small tweaks to speed up compilation by ~22%.&lt;/p&gt;&lt;code&gt;pair&amp;lt;package, opname&amp;gt;&lt;/code&gt; instead of a btree of btrees.&lt;p&gt;Measurement of speed on large programs (~45K ops):&lt;/p&gt;&lt;code&gt;name             old time/op  new time/op  delta
BM_CompileLarge   28.5s ¬± 2%   22.4s ¬± 2%  -21.61%  (p=0.008 n=5+5)
&lt;/code&gt;

&lt;p&gt;MapReduce improvements (~2X speedup for wordcount benchmark).&lt;/p&gt;&lt;p&gt;Mapreduce speedups:&lt;/p&gt;&lt;p&gt;The combiner data structures for the SafeCombinerMapOutput class have been changed. Rather than using a &lt;code&gt;hash_multimap&amp;lt;SafeCombinerKey, StringPiece&amp;gt;&lt;/code&gt;,
which had a hash table entry for each unique key/value inserted in the
table, we instead use a &lt;code&gt;hash_map&amp;lt;SafeCombinerKey, ValuePtr*&amp;gt;&lt;/code&gt; (where
ValuePtr is a linked list of values and repetition counts). This helps in
three ways:&lt;/p&gt;&lt;p&gt;It significantly reduces memory usage, since we only use "sizeof(ValuePtr) + value_len" bytes for each value, rather than "sizeof(SafeCombinerKey) + sizeof(StringPiece) + value_len + new hash table entry overhead" for each value. This means that we flush the reducer buffer less often.&lt;/p&gt;&lt;p&gt;It's significantly faster, since we avoid extra hash table entries when we're inserting a new value for a key that already exists in the table (and instead we just hook the value into the linked list of values for that key).&lt;/p&gt;&lt;p&gt;Since we associate a repetition count with each value in the linked list, we can represent this sequence:&lt;/p&gt;&lt;code&gt;Output(key, "1");
Output(key, "1");
Output(key, "1");
Output(key, "1");
Output(key, "1");
&lt;/code&gt;
&lt;p&gt;as a single entry in the linked list for "key" with a repetition count of 5. Internally we yield "1" five times to the user-level combining function. (A similar trick could be applied on the reduce side, perhaps).&lt;/p&gt;&lt;p&gt;(Minor) Added a test for "nshards == 1" to the default MapReductionBase::KeyFingerprintSharding function that avoids fingerprinting the key entirely if we are just using 1 reduce shard (since we can just return 0 directly in that case without examining the key).&lt;/p&gt;&lt;p&gt;Turned some VLOG(3) statements into DVLOG(3) in the code path that is called for each key/value added to the combiner.&lt;/p&gt;&lt;p&gt;Reduces time for one wordcount benchmark from 12.56s to 6.55s.&lt;/p&gt;&lt;p&gt;Rework the alarm handling code in the SelectServer to significantly improve its performance (adding+removing an alarm from 771 ns to 271 ns).&lt;/p&gt;&lt;p&gt;Reworked the alarm handling code in the SelectServer to significantly improve its performance.&lt;/p&gt;&lt;p&gt;Changes:&lt;/p&gt;&lt;p&gt;Switched to using &lt;code&gt;AdjustablePriorityQueue&amp;lt;Alarm&amp;gt;&lt;/code&gt; instead of a a
&lt;code&gt;set&amp;lt;Alarm*&amp;gt;&lt;/code&gt; for the &lt;code&gt;AlarmQueue&lt;/code&gt;. This significantly speeds up alarm
handling, reducing the time taken to add and remove an alarm from 771
nanoseconds to 281 nanoseconds. This change avoids an
allocation/deallocation per alarm setup (for the red-black tree node in the
STL set object), and also gives much better cache locality (since the
AdjustablePriorityQueue is a heap implemented in a vector, rather than a
red-black tree), there are fewer cache lines touched when manipulating the
&lt;code&gt;AlarmQueue&lt;/code&gt; on every trip through the selectserver loop.&lt;/p&gt;&lt;p&gt;Converted AlarmList in Alarmer from a hash_map to a dense_hash_map to avoid another allocation/deallocation per alarm addition/deletion (this also improves cache locality when adding/removing alarms).&lt;/p&gt;&lt;p&gt;Removed the &lt;code&gt;num_alarms_stat_&lt;/code&gt; and &lt;code&gt;num_closures_stat_&lt;/code&gt;
MinuteTenMinuteHourStat objects, and the corresponding exported variables.
Although monitoring these seems nice, in practice they add significant
overhead to critical networking code. If I had left these variables in as
Atomic32 variables instead of MinuteTenMinuteHourStat, they would have still
increased the cost of adding and removing alarms from 281 nanoseconds to 340
nanoseconds.&lt;/p&gt;&lt;p&gt;Benchmark results&lt;/p&gt;&lt;code&gt;Benchmark                      Time(ns)  CPU(ns) Iterations
-----------------------------------------------------------
BM_AddAlarm/1                       902      771     777777
&lt;/code&gt;
&lt;p&gt;With this change&lt;/p&gt;&lt;code&gt;Benchmark                      Time(ns)  CPU(ns) Iterations
-----------------------------------------------------------
BM_AddAlarm/1                       324      281    2239999
&lt;/code&gt;

&lt;p&gt;3.3X performance in index serving speed!&lt;/p&gt;&lt;p&gt;We found a number of performance issues when planning a switch from on-disk to in-memory index serving in 2001. This change fixed many of these problems and took us from 150 to over 500 in-memory queries per second (for a 2 GB in-memory index on dual processor Pentium III machine).&lt;/p&gt;&lt;p&gt;In no particular order, a list of performance related books and articles that the authors have found helpful:&lt;/p&gt;&lt;p&gt;If you want to cite this document, we suggest:&lt;/p&gt;&lt;code&gt;Jeffrey Dean &amp;amp; Sanjay Ghemawat, Performance Hints, 2025, https://abseil.io/fast/hints.html
&lt;/code&gt;&lt;p&gt;Or in BibTeX:&lt;/p&gt;&lt;code&gt;@misc{DeanGhemawatPerformance2025,
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  title = {Performance Hints},
  year = {2025},
  howpublished = {\url{https://abseil.io/fast/hints.html}},
}
&lt;/code&gt;&lt;p&gt;Many colleagues have provided helpful feedback on this document, including:&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46328274</guid><pubDate>Fri, 19 Dec 2025 17:14:42 +0000</pubDate></item><item><title>TP-Link Tapo C200: Hardcoded Keys, Buffer Overflows and Privacy</title><link>https://www.evilsocket.net/2025/12/18/TP-Link-Tapo-C200-Hardcoded-Keys-Buffer-Overflows-and-Privacy-in-the-Era-of-AI-Assisted-Reverse-Engineering/</link><description>&lt;doc fingerprint="2540d911803d63cc"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi friends and welcome to the last post for this year! Whenever someone asks me how to get started with reverse engineering, I always give the same advice: buy the cheapest IP camera you can find. These devices are self-contained little ecosystems - they have firmware you can extract, network protocols you can sniff, and mobile apps you can decompile. Chances are, you‚Äôll find something interesting. At worst, you‚Äôll learn a lot about assembly and embedded systems. At best, you‚Äôll find some juicy vulnerability and maybe learn how to exploit it!&lt;/p&gt;
    &lt;p&gt;I own several TP-Link Tapo C200 cameras myself. They‚Äôre cheap (less than 20 EUR from Italy), surprisingly stable, and I genuinely like them - they just work. One weekend, I decided just for fun to take my own advice. The Tapo C200 has been around for a while and has had a few CVEs discovered and more or less patched over the years, so I honestly wasn‚Äôt expecting to find much in the latest firmware. However, I wanted to use this chance to perform some AI assisted reverse engineering and test whether I could still find anything at all.&lt;/p&gt;
    &lt;p&gt;I documented the entire process live on Arcadia - my thought process, the dead ends, the AI prompts that worked and the ones that didn‚Äôt. If you want the raw, unfiltered version with screenshots and videos of things crashing, go check that out.&lt;/p&gt;
    &lt;p&gt;This post is the cleaned-up version of that journey, where I wanted to show how I approach firmware analysis these days, now that we have AI. You will notice that in several instances I will be particularly lazy and delegate to AI things I could have done manually and/or inferred myself after some more work. Keep in mind that while I am generally lazy, this was also an experiment in integrating and documenting how effective AI can be for security research and reverse engineering, and especially in making them accessible to less experienced/sophisticated researchers/attackers.&lt;/p&gt;
    &lt;p&gt;What started as a lazy weekend project turned into finding a few security vulnerabilities that affect about 25,000 of these devices directly exposed on the internet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the Firmware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Old friend JD-GUI to reverse the Android app and get a sense of things&lt;/item&gt;
      &lt;item&gt;The AWS CLI to download the firmware image.&lt;/item&gt;
      &lt;item&gt;binwalk for firmware inspection.&lt;/item&gt;
      &lt;item&gt;Grok to give a quick AI assisted look into prior research.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first step is always obtaining the firmware binary file and this time it was super easy! After some basic reversing of the Tapo Android app, I found out that TP-Link have their entire firmware repository in an open S3 bucket. No authentication required. So, you can list and download every version of every firmware they‚Äôve ever released for any device they ever produced:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ aws s3 ls s3://download.tplinkcloud.com/ --no-sign-request --recursive&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The entire output is here, for the curious. This provides access to the firmware image of every TP-Link device - routers, cameras, smart plugs, you name it. A reverse engineer‚Äôs candy store.&lt;/p&gt;
    &lt;p&gt;I grabbed version 1.4.2 Build 250313 Rel.40499n for the C200 (Hardware Revision 3), named &lt;code&gt;Tapo_C200v3_en_1.4.2_Build_250313_Rel.40499n_up_boot-signed_1747894968535.bin&lt;/code&gt;, and started poking around. However, the first attempt at identifying its format via binwalk was not successful, indicating that some sort of encryption or obfuscation was in place.&lt;/p&gt;
    &lt;p&gt;And here is where I started using AI. I used Grok to do some deep research on how to decrypt the firmware for these cameras. Since I knew other hackers worked on this before, I delegated searching into hundreds of relevant web pages to the AI:&lt;/p&gt;
    &lt;head rend="h3"&gt;Decrypting the Firmware&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The tp-link-decrypt tool to decrypt the firmware image.&lt;/item&gt;
      &lt;item&gt;binwalk for firmware inspection.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Grok, the tp-link-decrypt tool and the fact that every firmware image for every device seems to be encrypted the same exact way, we can now decrypt the firmware. The tool extracts RSA keys from TP-Link‚Äôs own GPL code releases - they publish the decryption keys themselves as part of their open source obligations.&lt;/p&gt;
    &lt;p&gt;Credits to @watchfulip for the original extensive TP-Link firmware research and @tangrs for finding that the relevant binaries are published in TP-Link GPL code dumps and how to extract keys from them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ git clone https://github.com/robbins/tp-link-decrypt&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;After decryption, the firmware revealed a fairly standard structure: a bootloader, a kernel, and a SquashFS root filesystem.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;$ binwalk -e Tapo_C200_v3_1.4.2_decrypted.bin&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Hunting for Bugs&lt;/head&gt;
    &lt;head rend="h3"&gt;Tools&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ghidra to decompile and understand the MIPS binaries&lt;/item&gt;
      &lt;item&gt;GhidraMCP to let an AI connect to my running Ghidra instance and support me in the process.&lt;/item&gt;
      &lt;item&gt;Cline to ask AI to explore the filesystem and find interesting components.&lt;/item&gt;
      &lt;item&gt;A mix of Anthropic's Opus and Sonnet 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once extracted, I used AI and Cline to explore the filesystem in search of which components handle the discovery protocol, camera web API, video streaming, etc all discovered earlier while reversing the Android app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Claude Opus 4: "this is the firmware of an ipcam, i'm trying to find where the webapp that serves the API is managed" pic.twitter.com/NrgtKGUD8h&lt;/p&gt;‚Äî Simone Margaritelli (@evilsocket) July 18, 2025&lt;/quote&gt;
    &lt;p&gt;Loading Ghidra and giving a quick look at the &lt;code&gt;tp_manage&lt;/code&gt; binary, revealed the first interesting thing:&lt;/p&gt;
    &lt;p&gt;This private key is not generated at boot. Similarly to CVE-2025-1099 for the C500, the C200 embeds in its firmware the private key that serves the SSL for a few APIs. If you‚Äôre on the same network as a camera, you can MitM and decrypt their HTTPS traffic with keys you extracted from the firmware image - without ever touching the hardware. For a security camera streaming video of people‚Äôs homes, this is‚Ä¶ not ideal.&lt;/p&gt;
    &lt;p&gt;I kept loading the other interesting binaries and exploring them in Ghidra using AI to quickly get a sense of the main features and possible entry points for an attacker.&lt;/p&gt;
    &lt;p&gt;Asking AI to explain a function and its relation to the other functions proved to be very useful for instance to understand encryption / obfuscation routines and network protocol handlers. This allows you to go from here:&lt;/p&gt;
    &lt;p&gt;To a higher level understanding that the AI can provide:&lt;/p&gt;
    &lt;p&gt;Another technique I found particularly effective is asking the AI to analyze a given function of interest and rename its variables and parameters to something meaningful based on context. Then do the same for the functions it calls, recursively following the branches you‚Äôre interested in. After a few iterations, what started as &lt;code&gt;FUN_0042eb7c(undefined2 *param_1, undefined4 param_2, int param_3)&lt;/code&gt; becomes &lt;code&gt;handleConnectAp(connection *conn, int flags, json *params)&lt;/code&gt; - and suddenly the decompiled code reads almost like the original source. &lt;/p&gt;
    &lt;p&gt;This iterative refinement approach, which I find a great example of human-AI collaboration where neither alone would be as efficient, is how I mapped most of the HTTP handlers, discovery protocol, and so on. What follows is the bottom line of my findings. For more details on the process, refer to the original Discord thread.&lt;/p&gt;
    &lt;p&gt;As a side note, I did not investigate (much) the exploitability of the following bugs to achieve code execution, mostly because I‚Äôm not familiar with MIPS, and it was not my intent. You can however do it relatively easily once obtained a shell via physical access, due to the presence of the &lt;code&gt;/bin/gdbserver&lt;/code&gt; binary in the firmware.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug 1: Pre-Auth ONVIF SOAP XML Parser Memory Overflow&lt;/head&gt;
    &lt;p&gt;The Tapo C200 exposes an ONVIF service via the &lt;code&gt;/bin/main&lt;/code&gt; server listening on port 2020 for interoperability with standard video management systems. The problem is in how it parses SOAP XML requests.&lt;/p&gt;
    &lt;p&gt;When processing XML elements, the parser (&lt;code&gt;soap_parse_and_validate_request&lt;/code&gt; at &lt;code&gt;0x0045ae8c&lt;/code&gt;) calls &lt;code&gt;ds_parse&lt;/code&gt; without any bounds checking on the number of elements or total memory allocation. Send it enough XML elements, and you‚Äôll overflow allocated memory.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the PoC:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Send this, and the camera crashes, requiring a power cycle to recover.&lt;/p&gt;
    &lt;quote&gt;‚Äî Simone Margaritelli (@evilsocket) July 19, 2025&lt;/quote&gt;
    &lt;head rend="h2"&gt;Bug 2: Pre-Auth HTTPS Content-Length Integer Overflow&lt;/head&gt;
    &lt;p&gt;The HTTPS server routine running on port 443 has a classic integer overflow in its &lt;code&gt;Content-Length&lt;/code&gt; header parsing. The vulnerable function at &lt;code&gt;0x004bd054&lt;/code&gt; does this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;iVar1 = atoi(value);&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That‚Äôs it. No bounds checking. No validation. Just raw &lt;code&gt;atoi()&lt;/code&gt; on user input.&lt;/p&gt;
    &lt;p&gt;On a 32-bit system, &lt;code&gt;atoi("4294967295")&lt;/code&gt; causes integer overflow, resulting in undefined behavior. In this case, the camera crashes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;And two pic.twitter.com/tt7eL7MA27&lt;/p&gt;‚Äî Simone Margaritelli (@evilsocket) July 19, 2025&lt;/quote&gt;
    &lt;p&gt;Another crash üí™&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug 3: Pre-Auth WiFi Hijacking&lt;/head&gt;
    &lt;p&gt;The camera exposes an API endpoint called &lt;code&gt;connectAp&lt;/code&gt; that‚Äôs used during initial setup to configure WiFi. The problem? It‚Äôs accessible without any authentication. Even after the camera is fully set up and connected to your network.&lt;/p&gt;
    &lt;p&gt;The vulnerable handler at &lt;code&gt;0x0042eb7c&lt;/code&gt; processes the request without any auth checks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;void connectApHandler(undefined2 *param_1,undefined4 param_2,int json_params)&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;And three! üöÄ pic.twitter.com/2GZiG4bTm0&lt;/p&gt;‚Äî Simone Margaritelli (@evilsocket) July 22, 2025&lt;/quote&gt;
    &lt;p&gt;The exploit is trivial:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This allows a remote attacker to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disconnect the camera from its legitimate network (DoS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If in WiFi range proximity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Force it to connect to an attacker-controlled network (MitM)&lt;/item&gt;
      &lt;item&gt;Intercept all video traffic once on the malicious network (not that we really needed this since the HTTPS private key is shared by all devices, as mentioned earlier XD)&lt;/item&gt;
      &lt;item&gt;Maintain persistent access even if the owner changes their WiFi password&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Bug 4: Pre-Auth Nearby WiFi Network Scanning&lt;/head&gt;
    &lt;p&gt;Related to Bug 3, the &lt;code&gt;scanApList&lt;/code&gt; method is also accessible without authentication - even when the device is not in onboarding mode. This endpoint returns a list of all WiFi networks visible to the camera:&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;1&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;#!/usr/bin/env python3&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A test on one of the devices exposed on the internet:&lt;/p&gt;
    &lt;p&gt;This is particularly concerning given the number of these devices exposed on the internet. An attacker can remotely enumerate WiFi networks in the camera‚Äôs vicinity, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSIDs of nearby networks&lt;/item&gt;
      &lt;item&gt;BSSIDs (MAC addresses of access points)&lt;/item&gt;
      &lt;item&gt;Signal strength (useful for triangulation)&lt;/item&gt;
      &lt;item&gt;Security configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here‚Äôs where it gets worse: tools like apple_bssid_locator can query Apple‚Äôs location services API with a BSSID and return precise GPS coordinates.&lt;/p&gt;
    &lt;p&gt;This means an attacker can:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find an exposed Tapo camera via services like ZoomEye, Shodan or similar indexes&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;scanApList&lt;/code&gt;to retrieve nearby WiFi BSSIDs&lt;/item&gt;
      &lt;item&gt;Query Apple‚Äôs location database with those BSSIDs&lt;/item&gt;
      &lt;item&gt;Pinpoint the camera‚Äôs physical location to within a few meters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Remote attackers can not only see what WiFi networks exist around a camera - they can determine exactly where that camera (and by extension, the home or business it‚Äôs monitoring) is located on a map.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;I‚Äôve decided to follow the industry standard 90+30 days responsible disclosure process; here‚Äôs the timeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;July 22, 2025: Sent initial report to TP-Link‚Äôs security team (security@tp-link.com) with full technical details, PoC exploits and videos. All compiled according to their guidelines.&lt;/item&gt;
      &lt;item&gt;July 22, 2025: Acknowledgment received.&lt;/item&gt;
      &lt;item&gt;August 22, 2025: TP-Link confirms they‚Äôre still reviewing the report&lt;/item&gt;
      &lt;item&gt;September 27, 2025: TP-Link responds and sets the timeline for the remediation patch to the end of November 2025.&lt;/item&gt;
      &lt;item&gt;November 2025: Nothing happens.&lt;/item&gt;
      &lt;item&gt;December 1, 2025: Sent follow up email, no response.&lt;/item&gt;
      &lt;item&gt;December 4, 2025: Sent another follow up email, which TP-Link responds to, further postponing the patch to the following week.&lt;/item&gt;
      &lt;item&gt;The following week: Nothing happens.&lt;/item&gt;
      &lt;item&gt;December 19, 2025: Public disclosure after 150 days.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The 90+30 period has long passed, so I decided to publish this writeup.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflict Of Interest&lt;/head&gt;
    &lt;p&gt;As of April 25, TP-Link is a CVE Numbering Authority (CNA). This means they have the authority to assign CVE identifiers for vulnerabilities in their own products - at least for the ones reported directly to them. And they actively encourage responsible disclosure directly to their security team, which means they control a considerable pipeline of vulnerability reports.&lt;/p&gt;
    &lt;p&gt;On their Security Commitment page, TP-Link prominently displays charts comparing their CVE count to competitors. They explicitly market themselves as having fewer CVEs than Cisco, Netgear, and D-Link. They state they ‚Äúaim to patch vulnerabilities within 90 days.‚Äù&lt;/p&gt;
    &lt;p&gt;There‚Äôs an obvious and structural conflict of interest when a vendor is allowed to be their own CNA while simultaneously using their CVE count as a marketing metric.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46329038</guid><pubDate>Fri, 19 Dec 2025 18:19:32 +0000</pubDate></item><item><title>LLM Year in Review</title><link>https://karpathy.bearblog.dev/year-in-review-2025/</link><description>&lt;doc fingerprint="7e43c31528a49999"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;2025 LLM Year in Review&lt;/head&gt;
    &lt;p&gt;2025 has been a strong and eventful year of progress in LLMs. The following is a list of personally notable and mildly surprising "paradigm changes" - things that altered the landscape and stood out to me conceptually.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Reinforcement Learning from Verifiable Rewards (RLVR)&lt;/head&gt;
    &lt;p&gt;At the start of 2025, the LLM production stack in all labs looked something like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Pretraining (GPT-2/3 of ~2020)&lt;/item&gt;
      &lt;item&gt;Supervised Finetuning (InstructGPT ~2022) and&lt;/item&gt;
      &lt;item&gt;Reinforcement Learning from Human Feedback (RLHF ~2022)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This was the stable and proven recipe for training a production-grade LLM for a while. In 2025, Reinforcement Learning from Verifiable Rewards (RLVR) emerged as the de facto new major stage to add to this mix. By training LLMs against automatically verifiable rewards across a number of environments (e.g. think math/code puzzles), the LLMs spontaneously develop strategies that look like "reasoning" to humans - they learn to break down problem solving into intermediate calculations and they learn a number of problem solving strategies for going back and forth to figure things out (see DeepSeek R1 paper for examples). These strategies would have been very difficult to achieve in the previous paradigms because it's not clear what the optimal reasoning traces and recoveries look like for the LLM - it has to find what works for it, via the optimization against rewards.&lt;/p&gt;
    &lt;p&gt;Unlike the SFT and RLHF stage, which are both relatively thin/short stages (minor finetunes computationally), RLVR involves training against objective (non-gameable) reward functions which allows for a lot longer optimization. Running RLVR turned out to offer high capability/$, which gobbled up the compute that was originally intended for pretraining. Therefore, most of the capability progress of 2025 was defined by the LLM labs chewing through the overhang of this new stage and overall we saw ~similar sized LLMs but a lot longer RL runs. Also unique to this new stage, we got a whole new knob (and and associated scaling law) to control capability as a function of test time compute by generating longer reasoning traces and increasing "thinking time". OpenAI o1 (late 2024) was the very first demonstration of an RLVR model, but the o3 release (early 2025) was the obvious point of inflection where you could intuitively feel the difference.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Ghosts vs. Animals / Jagged Intelligence&lt;/head&gt;
    &lt;p&gt;2025 is where I (and I think the rest of the industry also) first started to internalize the "shape" of LLM intelligence in a more intuitive sense. We're not "evolving/growing animals", we are "summoning ghosts". Everything about the LLM stack is different (neural architecture, training data, training algorithms, and especially optimization pressure) so it should be no surprise that we are getting very different entities in the intelligence space, which are inappropriate to think about through an animal lens. Supervision bits-wise, human neural nets are optimized for survival of a tribe in the jungle but LLM neural nets are optimized for imitating humanity's text, collecting rewards in math puzzles, and getting that upvote from a human on the LM Arena. As verifiable domains allow for RLVR, LLMs "spike" in capability in the vicinity of these domains and overall display amusingly jagged performance characteristics - they are at the same time a genius polymath and a confused and cognitively challenged grade schooler, seconds away from getting tricked by a jailbreak to exfiltrate your data.&lt;/p&gt;
    &lt;p&gt;(human intelligence: blue, AI intelligence: red. I like this version of the meme (I'm sorry I lost the reference to its original post on X) for pointing out that human intelligence is also jagged in its own different way.)&lt;/p&gt;
    &lt;p&gt;Related to all this is my general apathy and loss of trust in benchmarks in 2025. The core issue is that benchmarks are almost by construction verifiable environments and are therefore immediately susceptible to RLVR and weaker forms of it via synthetic data generation. In the typical benchmaxxing process, teams in LLM labs inevitably construct environments adjacent to little pockets of the embedding space occupied by benchmarks and grow jaggies to cover them. Training on the test set is a new art form.&lt;/p&gt;
    &lt;p&gt;What does it look like to crush all the benchmarks but still not get AGI?&lt;/p&gt;
    &lt;p&gt;I have written a lot more on the topic of this section here:&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Cursor / new layer of LLM apps&lt;/head&gt;
    &lt;p&gt;What I find most notable about Cursor (other than its meteoric rise this year) is that it convincingly revealed a new layer of an "LLM app" - people started to talk about "Cursor for X". As I highlighted in my Y Combinator talk this year (transcript and video), LLM apps like Cursor bundle and orchestrate LLM calls for specific verticals:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;They do the "context engineering"&lt;/item&gt;
      &lt;item&gt;They orchestrate multiple LLM calls under the hood strung into increasingly more complex DAGs, carefully balancing performance and cost tradeoffs.&lt;/item&gt;
      &lt;item&gt;They provide an application-specific GUI for the human in the loop&lt;/item&gt;
      &lt;item&gt;They offer an "autonomy slider"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A lot of chatter has been spent in 2025 on how "thick" this new app layer is. Will the LLM labs capture all applications or are there green pastures for LLM apps? Personally I suspect that LLM labs will trend to graduate the generally capable college student, but LLM apps will organize, finetune and actually animate teams of them into deployed professionals in specific verticals by supplying private data, sensors and actuators and feedback loops.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Claude Code / AI that lives on your computer&lt;/head&gt;
    &lt;p&gt;Claude Code (CC) emerged as the first convincing demonstration of what an LLM Agent looks like - something that in a loopy way strings together tool use and reasoning for extended problem solving. In addition, CC is notable to me in that it runs on your computer and with your private environment, data and context. I think OpenAI got this wrong because they focused their early codex / agent efforts on cloud deployments in containers orchestrated from ChatGPT instead of simply &lt;code&gt;localhost&lt;/code&gt;. And while agent swarms running in the cloud feels like the "AGI endgame", we live in an intermediate and slow enough takeoff world of jagged capabilities that it makes more sense to run the agents directly on the developer's computer. Note that the primary distinction that matters is not about where the "AI ops" happen to run (in the cloud, locally or whatever), but about everything else - the already-existing and booted up computer, its installation, context, data, secrets, configuration, and the low-latency interaction. Anthropic got this order of precedence correct and packaged CC into a delightful, minimal CLI form factor that changed what AI looks like - it's not just a website you go to like Google, it's a little spirit/ghost that "lives" on your computer. This is a new, distinct paradigm of interaction with an AI.&lt;/p&gt;
    &lt;head rend="h3"&gt;5. Vibe coding&lt;/head&gt;
    &lt;p&gt;2025 is the year that AI crossed a capability threshold necessary to build all kinds of impressive programs simply via English, forgetting that the code even exists. Amusingly, I coined the term "vibe coding" in this shower of thoughts tweet totally oblivious to how far it would go :). With vibe coding, programming is not strictly reserved for highly trained professionals, it is something anyone can do. In this capacity, it is yet another example of what I wrote about in Power to the people: How LLMs flip the script on technology diffusion, on how (in sharp contrast to all other technology so far) regular people benefit a lot more from LLMs compared to professionals, corporations and governments. But not only does vibe coding empower regular people to approach programming, it empowers trained professionals to write a lot more (vibe coded) software that would otherwise never be written. In nanochat, I vibe coded my own custom highly efficient BPE tokenizer in Rust instead of having to adopt existing libraries or learn Rust at that level. I vibe coded many projects this year as quick app demos of something I wanted to exist (e.g. see menugen, llm-council, reader3, HN time capsule). And I've vibe coded entire ephemeral apps just to find a single bug because why not - code is suddenly free, ephemeral, malleable, discardable after single use. Vibe coding will terraform software and alter job descriptions.&lt;/p&gt;
    &lt;head rend="h3"&gt;6. Nano banana / LLM GUI&lt;/head&gt;
    &lt;p&gt;Google Gemini Nano banana is one of the most incredible, paradigm-shifting models of 2025. In my world view, LLMs are the next major computing paradigm similar to computers of the 1970s, 80s, etc. Therefore, we are going to see similar kinds of innovations for fundamentally similar kinds of reasons. We're going to see equivalents of personal computing, of microcontrollers (cognitive core), or internet (of agents), etc etc. In particular, in terms of the UIUX, "chatting" with LLMs is a bit like issuing commands to a computer console in the 1980s. Text is the raw/favored data representation for computers (and LLMs), but it is not the favored format for people, especially at the input. People actually dislike reading text - it is slow and effortful. Instead, people love to consume information visually and spatially and this is why the GUI has been invented in traditional computing. In the same way, LLMs should speak to us in our favored format - in images, infographics, slides, whiteboards, animations/videos, web apps, etc. The early and present version of this of course are things like emoji and Markdown, which are ways to "dress up" and lay out text visually for easier consumption with titles, bold, italics, lists, tables, etc. But who is actually going to build the LLM GUI? In this world view, nano banana is a first early hint of what that might look like. And importantly, one notable aspect of it is that it's not just about the image generation itself, it's about the joint capability coming from text generation, image generation and world knowledge, all tangled up in the model weights.&lt;/p&gt;
    &lt;p&gt;TLDR. 2025 was an exciting and mildly surprising year of LLMs. LLMs are emerging as a new kind of intelligence, simultaneously a lot smarter than I expected and a lot dumber than I expected. In any case they are extremely useful and I don't think the industry has realized anywhere near 10% of their potential even at present capability. Meanwhile, there are so many ideas to try and conceptually the field feels wide open. And as I mentioned on my Dwarkesh pod earlier this year, I simultaneously (and on the surface paradoxically) believe that we will both see rapid and continued progress and that yet there is a lot of work to be done. Strap in.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46330726</guid><pubDate>Fri, 19 Dec 2025 20:49:20 +0000</pubDate></item><item><title>A better zip bomb (2019)</title><link>https://www.bamsoftware.com/hacks/zipbomb/</link><description>&lt;doc fingerprint="2f9c981635a30e33"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;A better zip bomb&lt;/head&gt;&lt;p&gt; David Fifield&lt;lb/&gt; david@bamsoftware.com &lt;/p&gt;&lt;head rend="h2"&gt;Summary&lt;/head&gt;&lt;p&gt;This article shows how to construct a non-recursive zip bomb that achieves a high compression ratio by overlapping files inside the zip container. "Non-recursive" means that it does not rely on a decompressor's recursively unpacking zip files nested within zip files: it expands fully after a single round of decompression. The output size increases quadratically in the input size, reaching a compression ratio of over 28√Ç million (10√Ç MB √¢ 281√Ç TB) at the limits of the zip format. Even greater expansion is possible using 64-bit extensions. The construction uses only the most common compression algorithm, DEFLATE, and is compatible with most zip parsers.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Source code:&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;quote&gt;git clone https://www.bamsoftware.com/git/zipbomb.git&lt;/quote&gt;zipbomb-20210121.zip&lt;/item&gt;&lt;item rend="dt-2"&gt;Data and source for figures:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;quote&gt;git clone https://www.bamsoftware.com/git/zipbomb-paper.git&lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;√ê √ë√ë√ë√ê¬∫√ê¬∏√ê¬π √ê¬ø√ê¬µ√ë√ê¬µ√ê¬≤√ê¬æ√ê¬¥ √ê¬æ√ë @m1rko.&lt;/p&gt;&lt;p&gt;√§¬∏√¶√ß¬ø¬ª√®¬Ø: √•√•¬≤¬∏√•¬∑√®¬•√•¬∞√©.&lt;/p&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;non-recursive&lt;/cell&gt;&lt;cell role="head"&gt;recursive&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;zipped size&lt;/cell&gt;&lt;cell&gt;unzipped size&lt;/cell&gt;&lt;cell&gt;ratio&lt;/cell&gt;&lt;cell&gt;unzipped size&lt;/cell&gt;&lt;cell&gt;ratio&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Cox quine&lt;/cell&gt;&lt;cell&gt;440&lt;/cell&gt;&lt;cell&gt;440&lt;/cell&gt;&lt;cell&gt;1.0&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;Ellingsen quine&lt;/cell&gt;&lt;cell&gt;28√¢¬Ø809&lt;/cell&gt;&lt;cell&gt;42√¢¬Ø569&lt;/cell&gt;&lt;cell&gt;1.5&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;42.zip&lt;/cell&gt;&lt;cell&gt;*42√¢¬Ø374&lt;/cell&gt;&lt;cell&gt;558√¢¬Ø432&lt;/cell&gt;&lt;cell&gt;13.2&lt;/cell&gt;&lt;cell&gt;4√¢¬Ø507√¢¬Ø981√¢¬Ø343√¢¬Ø026√¢¬Ø016&lt;/cell&gt;&lt;cell&gt;106 billion&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;this technique&lt;/cell&gt;&lt;cell&gt;42√¢¬Ø374&lt;/cell&gt;&lt;cell&gt;5√¢¬Ø461√¢¬Ø307√¢¬Ø620&lt;/cell&gt;&lt;cell&gt;129 thousand&lt;/cell&gt;&lt;cell&gt;5√¢¬Ø461√¢¬Ø307√¢¬Ø620&lt;/cell&gt;&lt;cell&gt;129 thousand&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;this technique&lt;/cell&gt;&lt;cell&gt;9√¢¬Ø893√¢¬Ø525&lt;/cell&gt;&lt;cell&gt;281√¢¬Ø395√¢¬Ø456√¢¬Ø244√¢¬Ø934&lt;/cell&gt;&lt;cell&gt;28 million&lt;/cell&gt;&lt;cell&gt;281√¢¬Ø395√¢¬Ø456√¢¬Ø244√¢¬Ø934&lt;/cell&gt;&lt;cell&gt;28 million&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;this technique (Zip64)&lt;/cell&gt;&lt;cell&gt;45√¢¬Ø876√¢¬Ø952&lt;/cell&gt;&lt;cell&gt;4√¢¬Ø507√¢¬Ø981√¢¬Ø427√¢¬Ø706√¢¬Ø459&lt;/cell&gt;&lt;cell&gt;98 million&lt;/cell&gt;&lt;cell&gt;4√¢¬Ø507√¢¬Ø981√¢¬Ø427√¢¬Ø706√¢¬Ø459&lt;/cell&gt;&lt;cell&gt;98 million&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Compression bombs that use the zip format must cope with the fact that DEFLATE, the compression algorithm most commonly supported by zip parsers, cannot achieve a compression ratio greater than 1032. For this reason, zip bombs typically rely on recursive decompression, nesting zip files within zip files to get an extra factor of 1032 with each layer. But the trick only works on implementations that unzip recursively, and most do not. The best-known zip bomb, 42.zip, expands to a formidable 4.5√Ç PB if all six of its layers are recursively unzipped, but a trifling 0.6√Ç MB at the top layer. Zip quines, like those of Ellingsen and Cox, which contain a copy of themselves and thus expand infinitely if recursively unzipped, are likewise perfectly safe to unzip once.&lt;/p&gt;&lt;p&gt;This article shows how to construct a non-recursive zip bomb whose compression ratio surpasses the DEFLATE limit of 1032. It works by overlapping files inside the zip container, in order to reference a "kernel" of highly compressed data in multiple files, without making multiple copies of it. The zip bomb's output size grows quadratically in the input size; i.e., the compression ratio gets better as the bomb gets bigger. The construction depends on features of both zip and DEFLATE√¢it is not directly portable to other file formats or compression algorithms. It is compatible with most zip parsers, the exceptions being "streaming" parsers that parse in one pass without first consulting the zip file's central directory. We try to balance two conflicting goals:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Maximize the compression ratio. We define the compression ratio as the the sum of the sizes of all the files contained the in the zip file, divided by the size of the zip file itself. It does not count filenames or other filesystem metadata, only contents.&lt;/item&gt;&lt;item&gt;Be compatible. Zip is a tricky format and parsers differ, especially around edge cases and optional features. Avoid taking advantage of tricks that only work with certain parsers. We will remark on certain ways to increase the efficiency of the zip bomb that come with some loss of compatibility.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Structure of a zip file&lt;/head&gt;&lt;p&gt;A√Ç zip file consists of a central directory which references files.&lt;/p&gt;&lt;p&gt;The central directory is at the end of the zip file. It is a list of central directory headers. Each central directory header contains metadata for a single file, like its filename and CRC-32 checksum, and a backwards pointer to a local file header. A√Ç central directory header is 46√Ç bytes long, plus the length of the filename.&lt;/p&gt;&lt;p&gt;A√Ç file consists of a local file header followed by compressed file data. The local file header is 30√Ç bytes long, plus the length of the filename. It contains a redundant copy of the metadata from the central directory header, and the compressed and uncompressed sizes of the file data that follows. Zip is a container format, not a compression algorithm. Each file's data is compressed using an algorithm specified in the metadata√¢usually DEFLATE.&lt;/p&gt;&lt;p&gt;This description of the zip format omits many details that are not needed for understanding the zip bomb. For full information, refer to section√Ç 4.3 of APPNOTE.TXT or The structure of a PKZip file by Florian Buchholz, or see the source code.&lt;/p&gt;&lt;head rend="h2"&gt;The first insight: overlapping files&lt;/head&gt;&lt;p&gt;By compressing a long string of repeated bytes, we can produce a kernel of highly compressed data. By itself, the kernel's compression ratio cannot exceed the DEFLATE limit of 1032, so we want a way to reuse the kernel in many files, without making a separate copy of it in each file. We can do it by overlapping files: making many central directory headers point to a single file, whose data is the kernel.&lt;/p&gt;&lt;p&gt;Let's look at an example to see how this construction affects the compression ratio. Suppose the kernel is 1000√Ç bytes and decompresses to 1√Ç MB. Then the first MB of output "costs" 1078√Ç bytes of input:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;31√Ç bytes for a local file header (including a 1-byte filename)&lt;/item&gt;&lt;item&gt;47√Ç bytes for a central directory header (including a 1-byte filename)&lt;/item&gt;&lt;item&gt;1000√Ç bytes for the kernel itself&lt;/item&gt;&lt;/list&gt;&lt;p&gt;But every 1√Ç MB of output after the first costs only 47√Ç bytes√¢we don't need another local file header or another copy of the kernel, only an additional central directory header. So while the first reference of the kernel has a compression ratio of 1√¢¬Ø000√¢¬Ø000√Ç /√Ç 1078 √¢√Ç 928, each additional reference pulls the ratio closer to 1√¢¬Ø000√¢¬Ø000√Ç /√Ç 47 √¢√Ç 21√¢¬Ø277. A√Ç bigger kernel raises the ceiling.&lt;/p&gt;&lt;p&gt; The problem with this idea is a lack of compatibility. Because many central directory headers point to a single local file header, the metadata√¢specifically the filename√¢cannot match for every file. Some parsers balk at that. Info-ZIP UnZip (the standard Unix &lt;code&gt;unzip&lt;/code&gt; program)
extracts the files, but with warnings:
&lt;/p&gt;&lt;p&gt;And the Python zipfile module throws an exception:&lt;/p&gt;&lt;p&gt;Next we will see how to modify the construction for consistency of filenames, while still retaining most of the advantage of overlapping files.&lt;/p&gt;&lt;head rend="h2"&gt;The second insight: quoting local file headers&lt;/head&gt;&lt;p&gt;We need to separate the local file headers for each file, while still reusing a single kernel. Simply concatenating all the local file headers does not work, because the zip parser will find a local file header where it expects to find the beginning of a DEFLATE stream. But the idea will work, with a minor modification. We'll use a feature of DEFLATE, non-compressed blocks, to "quote" local file headers so that they appear to be part of the same DEFLATE stream that terminates in the kernel. Every local file header (except the first) will be interpreted in two ways: as code (part of the structure of the zip file) and as data (part of the contents of a file).&lt;/p&gt;&lt;p&gt;A DEFLATE stream is a sequence of blocks, where each block may be compressed or non-compressed. Compressed blocks are what we usually think of; for example the kernel is one big compressed block. But there are also non-compressed blocks, which start with a 5-byte header with a length field that means simply, "output the next n bytes verbatim." Decompressing a non-compressed block means only stripping the 5-byte header. Compressed and non-compressed blocks may be intermixed freely in a DEFLATE stream. The output is the concatenation of decompressing all the blocks in order. The "non-compressed" notion only has meaning at the DEFLATE layer; the file data still counts as "compressed" at the zip layer, no matter what kind of blocks are used.&lt;/p&gt;&lt;p&gt;It is easiest to understand this quoted-overlap construction from the inside out, beginning with the last file and working backwards to the first. Start by inserting the kernel, which will form the end of file data for every file. Prepend a local file header LFHN and add a central directory header CDHN that points to it. Set the "compressed size" metadata field in the LFHN and CDHN to the compressed size of the kernel. Now prepend a 5-byte non-compressed block header (colored green in the diagram) whose length field is equal to the size of LFHN. Prepend a second local file header LFHN√¢1 and add a central directory header CDHN√¢1 that points to it. Set the "compressed size" metadata field in both of the new headers to the compressed size of the kernel plus the size of the non-compressed block header (5√Ç bytes) plus the size of LFHN.&lt;/p&gt;&lt;p&gt;At this point the zip file contains two files, named "Y" and "Z". Let's walk through what a zip parser would see while parsing it. Suppose the compressed size of the kernel is 1000√Ç bytes and the size of LFHN is 31√Ç bytes. We start at CDHN√¢1 and follow the pointer to LFHN√¢1. The first file's filename is "Y" and the compressed size of its file data is 1036√Ç bytes. Interpreting the next 1036 bytes as a DEFLATE stream, we first encounter the 5-byte header of a non-compressed block that says to copy the next 31√Ç bytes. We write the next 31√Ç bytes, which are LFHN, which we decompress and append to file "Y". Moving on in the DEFLATE stream, we find a compressed block (the kernel), which we decompress to file "Y". Now we have reached the end of the compressed data and are done with file "Y". Proceeding to the next file, we follow the pointer from CDHN to LFHN and find a file named "Z" whose compressed size is 1000√Ç bytes. Interpreting those 1000√Ç bytes as a DEFLATE stream, we immediately encounter a compressed block (the kernel again) and decompress it to the file "Z". Now we have reached the end of the final file and are done. The output file "Z" contains the decompressed kernel; the output file "Y" is the same, but additionally prefixed by the 31√Ç bytes of LFHN.&lt;/p&gt;&lt;p&gt;We complete the construction by repeating the quoting procedure until the zip file contains the desired number of files. Each new file adds a central directory header, a local file header, and a non-compressed block to quote the immediately succeeding local file header. Compressed file data is generally a chain of DEFLATE non-compressed blocks (the quoted local file headers) followed by the compressed kernel. Each byte in the kernel contributes about 1032√¢¬ØN to the output size, because each byte is part of all N files. The output files are not all the same size: those that appear earlier in the zip file are larger than those that appear later, because they contain more quoted local file headers. The contents of the output files are not particularly meaningful, but no one said they had to make sense.&lt;/p&gt;&lt;p&gt;This quoted-overlap construction has better compatibility than the full-overlap construction of the previous section, but the compatibility comes at the expense of the compression ratio. There, each added file cost only a central directory header; here, it costs a central directory header, a local file header, and another 5√Ç bytes for the quoting header.&lt;/p&gt;&lt;head rend="h2"&gt;Optimization&lt;/head&gt;&lt;p&gt;Now that we have the basic zip bomb construction, we will try to make it as efficient as possible. We want to answer two questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;For a given zip file size, what is the maximum compression ratio?&lt;/item&gt;&lt;item&gt;What is the maximum compression ratio, given the limits of the zip format?&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Kernel compression&lt;/head&gt;&lt;p&gt;It pays to compress the kernel as densely as possible, because every decompressed byte gets magnified by a factor of N. To that end, we use a custom DEFLATE compressor called bulk_deflate, specialized for compressing a string of repeated bytes.&lt;/p&gt;&lt;p&gt;All decent DEFLATE compressors will approach a compression ratio of 1032 when given an infinite stream of repeating bytes, but we care more about specific finite sizes than asymptotics. bulk_deflate compresses more data into the same space than the general-purpose compressors: about 26√Ç kB more than zlib and Info-ZIP, and about 15√Ç kB more than Zopfli, a compressor that trades speed for density.&lt;/p&gt;&lt;p&gt;The price of bulk_deflate's high compression ratio is a lack of generality. bulk_deflate can only compress strings of a single repeated byte, and only those of specific lengths, namely 517√Ç +√Ç 258√¢¬Øk for integer k√Ç √¢¬•√Ç 0. Besides compressing densely, bulk_deflate is fast, doing essentially constant work regardless of the input size, aside from the work of actually writing out the compressed string.&lt;/p&gt;&lt;head rend="h3"&gt;Filenames&lt;/head&gt;&lt;p&gt;For our purposes, filenames are mostly dead weight. While filenames do contribute something to the output size by virtue of being part of quoted local file headers, a byte in a filename does not contribute nearly as much as a byte in the kernel. We want filenames to be as short as possible, while keeping them all distinct, and subject to compatibility considerations.&lt;/p&gt;&lt;p&gt;The first compatibility consideration is character encoding. The zip format specification states that filenames are to be interpreted as CP√Ç 437, or UTF-8 if a certain flag bit is set (APPNOTE.TXT Appendix√Ç D). But this is a major point of incompatibility across zip parsers, which may interpret filenames as being in some fixed or locale-specific encoding. So for compatibility, we must limit ourselves to characters that have the same encoding in both CP√Ç 437 and UTF-8; namely, the 95 printable characters of US-ASCII.&lt;/p&gt;&lt;p&gt;We are further restricted by filesystem naming limitations. Some filesystems are case-insensitive, so "a" and "A" do not count as distinct names. Common filesystems like FAT32 prohibit certain characters like '*' and '?'.&lt;/p&gt;&lt;p&gt;As a safe but not necessarily optimal compromise, our zip bomb will use filenames consisting of characters drawn from a 36-character alphabet that does not rely on case distinctions or use special characters:&lt;/p&gt;&lt;p&gt;Filenames are generated in the obvious way, cycling each position through the possible characters and adding a position on overflow:&lt;/p&gt;&lt;p&gt;There are 36 filenames of length√Ç 1, 362 filenames of length√Ç 2, and so on. The length of the nth filename is ‚åälog36((n√Ç +√Ç 1)√Ç /√Ç (36√¢¬Ø/√¢¬Ø35))‚åã√Ç +√Ç 1. Four bytes are enough to represent 1√¢¬Ø727√¢¬Ø604 distinct filenames.&lt;/p&gt;&lt;p&gt;Given that the N filenames in the zip file are generally not all of the same length, which way should we order them, shortest to longest or longest to shortest? A√Ç little reflection shows that it is better to put the longest names last, because those names are the most quoted. Ordering filenames longest last adds over 900√Ç MB of output to zblg.zip, compared to ordering them longest first. It is a minor optimization, though, as those 900√Ç MB comprise only 0.0003% of the total output size.&lt;/p&gt;&lt;head rend="h3"&gt;Kernel size&lt;/head&gt;&lt;p&gt;The quoted-overlap construction allows us to place a compressed kernel of data, and then cheaply copy it many times. For a given zip file size√Ç X, how much space should we devote to storing the kernel, and how much to making copies?&lt;/p&gt;&lt;p&gt;To find the optimum balance, we only have to optimize the single variable N, the number of files in the zip file. Every value of N requires a certain amount of overhead for central directory headers, local file headers, quoting block headers, and filenames. All the remaining space can be taken up by the kernel. Because N has to be an integer, and you can only fit so many files before the kernel size drops to zero, it suffices to test every possible value of N and select the one that yields the most output.&lt;/p&gt;&lt;p&gt;Applying the optimization procedure to X√Ç =√Ç 42√¢¬Ø374, the size of 42.zip, finds a maximum at N√Ç =√Ç 250. Those 250 files require 21√¢¬Ø195 bytes of overhead, leaving 21√¢¬Ø179 bytes for the kernel. A√Ç kernel of that size decompresses to 21√¢¬Ø841√¢¬Ø249 bytes (a√Ç ratio of 1031.3). The 250 copies of the decompressed kernel, plus the little bit extra that comes from the quoted local file headers, produces an overall unzipped output of 5√¢¬Ø461√¢¬Ø307√¢¬Ø620√Ç bytes and a√Ç compression ratio of 129√Ç thousand.&lt;/p&gt;&lt;quote&gt;zipbomb --mode=quoted_overlap --num-files=250 --compressed-size=21179 &amp;gt; zbsm.zip&lt;/quote&gt;&lt;p&gt;Optimization produced an almost even split between the space allocated to the kernel and the space allocated to file headers. It is not a coincidence. Let's look at a simplified model of the quoted-overlap construction. In the simplified model, we ignore filenames, as well as the slight increase in output file size due to quoting local file headers. Analysis of the simplified model will show that the optimum split between kernel and file headers is approximately even, and that the output size grows quadratically when allocation is optimal.&lt;/p&gt;&lt;p&gt;Define some constants and variables:&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell&gt;X&lt;/cell&gt;&lt;cell&gt;zip file size (take as fixed)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;N&lt;/cell&gt;&lt;cell&gt;number of files in the zip file (variable to optimize)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;CDH&lt;/cell&gt;&lt;cell&gt;√Ç =√Ç 46&lt;/cell&gt;&lt;cell&gt;size of a central directory header (without filename)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;LFH&lt;/cell&gt;&lt;cell&gt;√Ç =√Ç 30&lt;/cell&gt;&lt;cell&gt;size of a local file header (without filename)&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Q&lt;/cell&gt;&lt;cell&gt;√Ç =√Ç 5&lt;/cell&gt;&lt;cell&gt;the size of DEFLATE non-compressed block header&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;C&lt;/cell&gt;&lt;cell&gt;√Ç √¢√Ç 1032&lt;/cell&gt;&lt;cell&gt;compression ratio of the kernel&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Let H(N) be the amount of header overhead required by N files. Refer to the diagram to understand where this formula comes from.&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;H(N)&lt;/cell&gt;&lt;cell&gt;√Ç = N√Ç √¢ √Ç (CDH + LFH) + (N√Ç √¢√Ç 1)√Ç √¢ √Ç Q&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The space remaining for the kernel is X√Ç √¢√Ç H(N). The total unzipped size SX(N) is the size of N copies of the kernel, decompressed at ratio√Ç C. (In this simplified model we ignore the minor additional expansion from quoted local file headers.)&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;SX(N)&lt;/cell&gt;&lt;cell&gt;√Ç = (X √¢ H(N))√¢¬ØC√¢¬ØN&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;√Ç = (X √¢ (N √¢ (CDH + LFH) + (N √¢ 1) √¢ Q))√¢¬ØC√¢¬ØN&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;√Ç = √¢(CDH + LFH + Q)√¢¬ØC√¢¬ØN2 + (X + Q)√¢¬ØC√¢¬ØN&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;SX(N) is a polynomial in N, so its maximum must be at a place where the derivative S√¢¬≤X(N) is zero. Taking the derivative and finding the zero gives us NOPT, the optimal number of files.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;S√¢¬≤X(NOPT)&lt;/cell&gt;&lt;cell&gt;√Ç = √¢2√¢¬Ø(CDH + LFH + Q)√¢¬ØC√¢¬ØNOPT + (X + Q)√¢¬ØC&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;√Ç = √¢2√¢¬Ø(CDH + LFH + Q)√¢¬ØC√¢¬ØNOPT + (X + Q)√¢¬ØC&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;NOPT&lt;/cell&gt;&lt;cell&gt;√Ç = (X + Q) / (CDH + LFH + Q) / 2&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;H(NOPT) gives the optimal amount of space to allocate for file headers. It is independent of CDH, LFH, and C, and is close to X√¢¬Ø/√¢¬Ø2.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;H(NOPT)&lt;/cell&gt;&lt;cell&gt;√Ç = NOPT √¢ (CDH + LFH) + (NOPT √¢ 1) √¢ Q&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;√Ç = (X √¢ Q) / 2&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;SX(NOPT) is the total unzipped size when the allocation is optimal. From this we see that the output size grows quadratically in the input size.&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;SX(NOPT)&lt;/cell&gt;&lt;cell&gt;√Ç = (X + Q)2√¢¬ØC / (CDH + LFH + Q) / 4&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;As we make the zip file larger, eventually we run into the limits of the zip format. A√Ç zip file can contain at most 216√Ç √¢√Ç 1 files, and each file can have an uncompressed size of at most 232√Ç √¢√Ç 1 bytes. Worse than that, some implementations take the maximum possible values as an indicator of the presence of 64-bit extensions, so our limits are actually 216√Ç √¢√Ç 2 and 232√Ç √¢√Ç 2. It happens that the first limit we hit is the one on uncompressed file size. At a zip file size of 8√¢¬Ø319√¢¬Ø377√Ç bytes, naive optimization would give us a file count of 47√¢¬Ø837 and a largest file of 232√Ç +√Ç 311 bytes.&lt;/p&gt;&lt;p&gt;Accepting that we cannot increase N nor the size of the kernel without bound, we would like find the maximum compression ratio achievable while remaining within the limits of the zip format. The way to proceed is to make the kernel as large as possible, and have the maximum number of files. Even though we can no longer maintain the roughly even split between kernel and file headers, each added file does increase the compression ratio√¢just not as fast as it would if we were able to keep growing the kernel, too. In fact, as we add files we will need to decrease the size of the kernel to make room for the maximum file size that gets slightly larger with each added file.&lt;/p&gt;&lt;p&gt;The plan results in a zip file that contains 216√Ç √¢√Ç 2 files and a kernel that decompresses to 232√Ç √¢√Ç 2√¢¬Ø178√¢¬Ø825 bytes. Files get longer towards the beginning of the zip file√¢the first and largest file decompresses to 232√Ç √¢√Ç 56 bytes. That is as close as we can get using the coarse output sizes of bulk_deflate√¢encoding the final 54√Ç bytes would cost more bytes than they are worth. (The zip file as a whole has a compression ratio of 28√Ç million, and the final 54√Ç bytes would gain at most 54√Ç √¢ √Ç 1032√Ç √¢ √Ç (216√Ç √¢√Ç 2) √¢ 36.5√Ç million bytes, so it only helps if the 54√Ç bytes can be encoded in 1√Ç byte√¢I√Ç could not do it in less than√Ç 2.) The output size of this zip bomb, 281√¢¬Ø395√¢¬Ø456√¢¬Ø244√¢¬Ø934√Ç bytes, is 99.97% of the theoretical maximum (232√Ç √¢√Ç 1) √¢ (216√Ç √¢√Ç 1). Any major improvements to the compression ratio can only come from reducing the input size, not increasing the output size.&lt;/p&gt;&lt;quote&gt;zipbomb --mode=quoted_overlap --num-files=65534 --max-uncompressed-size=4292788525 &amp;gt; zblg.zip&lt;/quote&gt;&lt;head rend="h2"&gt;Efficient CRC-32 computation&lt;/head&gt;&lt;p&gt; Among the metadata in the central directory header and local file header is a CRC-32 checksum of the uncompressed file data. This poses a problem, because directly calculating the CRC-32 of each file requires doing work proportional to the total unzipped size, which is large by design. (It's a zip bomb, after all.) We would prefer to do work that in the worst case is proportional to the zipped size. Two factors work in our advantage: all files share a common suffix (the kernel), and the uncompressed kernel is a string of repeated bytes. We will represent CRC-32 as a matrix product√¢this will allow us not only to compute the checksum of the kernel quickly, but also to reuse computation across files. The technique described in this section is a slight extension of the &lt;code&gt;crc32_combine&lt;/code&gt;
function in zlib,
which Mark Adler explains
here.
&lt;/p&gt;&lt;p&gt;You can model CRC-32 as a state machine that updates a 32-bit state register for each incoming bit. The basic update operations for a 0√Ç bit and a 1√Ç bit are:&lt;/p&gt;&lt;p&gt; If you think of the state register as a 32-element binary vector, and use XOR for addition and AND for multiplication, then &lt;code&gt;crc32_update_0&lt;/code&gt; is a
linear transformation;
i.e., it can be represented as multiplication by a
32√É32 binary
transformation matrix.
To see why, observe that multiplying a matrix by a vector
is just summing the columns of the matrix,
after multiplying each column by the corresponding element of the vector.
The shift operation &lt;code&gt;state√Ç¬†&amp;gt;&amp;gt;√Ç¬†1&lt;/code&gt;
is just taking each bit√Ç¬†i of the state vector
and multiplying it by a vector that is 0 everywhere except at bit i√Ç¬†√¢√Ç¬†1
(numbering the bits from right to left).
The conditional final XOR &lt;code&gt;state√Ç¬†^√Ç¬†0xedb88320&lt;/code&gt;
that only happens when bit√Ç¬†&lt;code&gt;b&lt;/code&gt; is 1
can instead be represented as first multiplying
&lt;code&gt;b&lt;/code&gt; by 0xedb88320
and then XORing it into the state.
&lt;/p&gt;&lt;p&gt; Furthermore, &lt;code&gt;crc32_update_1&lt;/code&gt; is just
&lt;code&gt;crc32_update_0&lt;/code&gt; plus (XOR) a√Ç¬†constant.
That makes &lt;code&gt;crc32_update_1&lt;/code&gt; an
affine transformation:
a√Ç¬†matrix multiplication followed by a translation (i.e., vector addition).
We can represent both the matrix multiplication and the translation
in a single step
if we enlarge the dimensions of the transformation matrix to 33√É33
and append an extra element to the state vector that is always√Ç¬†1.
(This representation is called
homogeneous coordinates.)
&lt;/p&gt;&lt;table&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="33"&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;0&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;code&gt;state√Ç¬†&amp;gt;&amp;gt;√Ç¬†1&lt;/code&gt;.
&lt;p&gt; Both operations &lt;code&gt;crc32_update_0&lt;/code&gt; and &lt;code&gt;crc32_update_1&lt;/code&gt;
can be represented by a 33√É33 transformation matrix.
The matrices M0 and M1 are shown.
The benefit of a matrix representation is that matrices compose.
Suppose we want to represent the state change effected by processing
the ASCII character 'a', whose binary representation is
011000012.
We can represent the cumulative CRC-32 state change of those 8√Ç¬†bits
in a single transformation matrix:
&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Ma&lt;/cell&gt;&lt;cell&gt;√Ç = M0 M1√Ç M1√Ç M0√Ç M0√Ç M0√Ç M0√Ç M1&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;And we can represent the state change of a string of repeated 'a's by multiplying many copies of Ma together√¢matrix exponentiation. We can do matrix exponentiation quickly using a square-and-multiply algorithm, which allows us to compute Mn in only about log2√¢¬Øn steps. For example, the matrix representing the state change of a string of 9√Ç 'a's is&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell&gt;(M&lt;code&gt;a&lt;/code&gt;)9&lt;/cell&gt;&lt;cell&gt;√Ç = M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;√Ç = (M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;)2√Ç¬†M&lt;code&gt;a&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;√Ç = ((M&lt;code&gt;a&lt;/code&gt;√Ç¬†M&lt;code&gt;a&lt;/code&gt;)2)2√Ç¬†M&lt;code&gt;a&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;√Ç = (((M&lt;code&gt;a&lt;/code&gt;)2)2)2√Ç¬†M&lt;code&gt;a&lt;/code&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The square-and-multiply algorithm is useful for computing Mkernel, the matrix for the uncompressed kernel, because the kernel is a string of repeated bytes. To produce a CRC-32 checksum value from a matrix, multiply the matrix by the zero vector. (The zero vector in homogeneous coordinates, that is: 32√Ç 0's followed by a√Ç 1. Here we omit the minor complication of pre- and post-conditioning the checksum.) To compute the checksum for every file, we work backwards. Start by initializing M√Ç :=√Ç Mkernel. The checksum of the kernel is also the checksum of the final file, file√Ç N, so multiply M by the zero vector and store the resulting checksum in CDHN and LFHN. The file data of file√Ç N√Ç √¢√Ç 1 is the same as the file data of file√Ç N, but with an added prefix of LFHN. So compute MLFHN, the state change matrix for LFHN, and update M√Ç :=√Ç M√Ç MLFHN. Now M represents the cumulative state change from processing LFHN followed by the kernel. Compute the checksum for file N√Ç √¢√Ç 1 by again multiplying M by the zero vector. Continue the procedure, accumulating state change matrices into M, until all the files have been processed.&lt;/p&gt;&lt;head rend="h2"&gt;Extension: Zip64&lt;/head&gt;&lt;p&gt;Earlier we hit a wall on expansion due to limits of the zip format√¢it was impossible to produce more than about 281√Ç TB of output, no matter how cleverly packed the zip file. It is possible to surpass those limits using Zip64, an extension to the zip format that increases the size of certain header fields to 64√Ç bits. Support for Zip64 is by no means universal, but it is one of the more commonly implemented extensions. As regards the compression ratio, the effect of Zip64 is to increase the size of a central directory header from 46√Ç bytes to 58√Ç bytes, and the size of a local directory header from 30√Ç bytes to 50√Ç bytes. Referring to the formula for optimal expansion in the simplified model, we see that a zip bomb in Zip64 format still grows quadratically, but more slowly because of the larger denominator√¢this is visible in the figure below in the Zip64 line's slightly lower vertical placement. In exchange for the loss of compatibility and slower growth, we get the removal of all practical file size limits.&lt;/p&gt;&lt;p&gt;Suppose we want a zip bomb that expands to 4.5√Ç PB, the same size that 42.zip recursively expands to. How big must the zip file be? Using binary search, we find that the smallest zip file whose unzipped size exceeds the unzipped size of 42.zip has a zipped size of 46√Ç MB.&lt;/p&gt;&lt;quote&gt;zipbomb --mode=quoted_overlap --num-files=190023 --compressed-size=22982788 --zip64 &amp;gt; zbxl.zip&lt;/quote&gt;&lt;p&gt;4.5√Ç PB is roughly the size of the data captured by the Event Horizon Telescope to make the first image of a black hole, stacks and stacks of hard drives.&lt;/p&gt;&lt;p&gt;With Zip64, it's no longer practically interesting to consider the maximum compression ratio, because we can just keep increasing the zip file size, and the compression ratio along with it, until even the compressed zip file is prohibitively large. An interesting threshold, though, is 264√Ç bytes (18√Ç EB or 16√Ç EiB)√¢that much data will not fit on most filesystems. Binary search finds the smallest zip bomb that produces at least that much output: it contains 12√Ç million files and has a compressed kernel of 1.5√Ç GB. The total size of the zip file is 2.9√Ç GB and it unzips to 264√Ç +√Ç 11√¢¬Ø727√¢¬Ø895√¢¬Ø877 bytes, having a compression ratio of over 6.2√Ç billion. I√Ç didn't make this one downloadable, but you can generate it yourself using the source code. It contains files so large that it uncovers a bug in Info-ZIP UnZip√Ç 6.0.&lt;/p&gt;&lt;quote&gt;zipbomb --mode=quoted_overlap --num-files=12056313 --compressed-size=1482284040 --zip64 &amp;gt; zbxxl.zip&lt;/quote&gt;&lt;head rend="h2"&gt;Extension: bzip2&lt;/head&gt;&lt;p&gt;DEFLATE is the most common compression algorithm used in the zip format, but it is only one of many options. Probably the second most common algorithm is bzip2, while not as compatible as DEFLATE, is probably the second most commonly supported compression algorithm. Empirically, bzip2 has a maximum compression ratio of about 1.4√Ç million, which allows for denser packing of the kernel. Ignoring the loss of compatibility, does bzip2 enable a more efficient zip bomb?&lt;/p&gt;&lt;p&gt;Yes√¢but only for small files. The problem is that bzip2 does not have anything like the non-compressed blocks of DEFLATE that we used to quote local file headers. So it is not possible to overlap files and reuse the kernel√¢each file must have its own copy, and therefore the overall compression ratio is no better than the ratio of any single file. In the figure we see that no-overlap bzip2 outperforms quoted DEFLATE only for files under about a megabyte.&lt;/p&gt;&lt;p&gt;There is still hope for using bzip2√¢an alternative means of local file header quoting discussed in the next section. Additionally, if you happen to know that a certain zip parser supports bzip2 and tolerates mismatched filenames, then you can use the full-overlap construction, which has no need for quoting.&lt;/p&gt;&lt;head rend="h2"&gt;Extension: extra-field quoting&lt;/head&gt;&lt;p&gt;So far we have used a feature of DEFLATE to quote local file headers, and we have just seen that the same trick does not work with bzip2. There is an alternative means of quoting, somewhat more limited, that only uses features of the zip format and does not depend on the compression algorithm.&lt;/p&gt;&lt;p&gt;At the end of the local file header structure there is a variable-length extra field whose purpose is to store information that doesn't fit into the ordinary fields of the header (APPNOTE.TXT section 4.3.7). The extra information may include, for example, a high-resolution timestamp or a Unix uid/gid; Zip64 works by using the extra field. The extra field is a length√¢value structure: if we increase the length field without adding to the value, then it will grow to include whatever comes after it in the zip file√¢namely the next local file header. Each local file header "quotes" the local file headers that follow it by enclosing them within its own extra field. The benefits of extra-field quoting over DEFLATE quoting are threefold:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Extra-field quoting requires only 4√Ç bytes of overhead, not√Ç 5, leaving more room for the kernel.&lt;/item&gt;&lt;item&gt;Extra-field quoting does not increase the size of files, which leaves more headroom for a bigger kernel when operating at the limits of the zip format.&lt;/item&gt;&lt;item&gt;Extra-field quoting provides a way to combine quoting with bzip2.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Despite these benefits, extra-field quoting is less flexible than DEFLATE quoting. It does not chain: each local file header must enclose not only the immediately next header but all headers which follow. The extra fields increase in length as they get closer to the beginning of the zip file. Because the extra field has a maximum length of 216√Ç √¢√Ç 1 bytes, it can only contain up to 1808 local file headers, or 1170 with Zip64, assuming that filenames are allocated as described. (With DEFLATE, you can use extra-field quoting for the earliest local file headers, then switch to DEFLATE quoting for the remainder.) Another problem is that, in order to conform to the internal data structure of the extra field, you must select a 16-bit header ID (APPNOTE.TXT section 4.5.2). to precede the quoted data. We want a header ID that will make parsers ignore the quoted data, not try to interpret it as meaningful metadata. Zip parsers are supposed to ignore unknown header IDs, so we could choose one at random, but there is the risk that the ID may be allocated in the future, breaking compatibility.&lt;/p&gt;&lt;p&gt;The figure illustrates the possibility of combining extra-field quoting with bzip2, with and without Zip64. Both "extra-field-quoted bzip2" lines have a knee at which the growth transitions from quadratic to linear. In the non-Zip64 case, the knee occurs at the maximum uncompressed file size (232√Ç √¢√Ç 2 bytes); after this point, one can only increase the number of files, not their size. The line stops completely when the number of files reaches 1809, and we run out of room in the extra field. In the Zip64 case, the knee occurs at 1171 files, after which the size of files can be increased, but not their number. Extra-field quoting may also be used with DEFLATE, but the improvement is so slight that it has been omitted from the figure. It increases the compression ratio of zbsm.zip by 1.2%; zblg.zip by 0.019%; and zbxl.zip by 0.0025%.&lt;/p&gt;&lt;head rend="h2"&gt;Discussion&lt;/head&gt;&lt;p&gt;In related work, Pl√É¬∂tz et√Ç al. used overlapping files to create a near-self-replicating zip file. Gynvael Coldwind has previously suggested (slide√Ç 47) overlapping files. Pellegrino et√Ç al. found systems vulnerable to compression bombs and other resource exhaustion attacks and listed common pitfalls in specification, implementation, and configuration.&lt;/p&gt;&lt;p&gt;We have designed the quoted-overlap zip bomb construction for compatibility, taking into consideration a number of implementation differences, some of which are shown in the table below. The resulting construction is compatible with zip parsers that work in the usual back-to-front way, first consulting the central directory and using it as an index of files. Among these is the example zip parser included in Nail, which is automatically generated from a formal grammar. The construction is not compatible, however, with "streaming" parsers, those that parse the zip file from beginning to end in one pass without first reading the central directory. By their nature, streaming parsers do not permit any kind of file overlapping. The most likely outcome is that they will extract only the first file. They may even raise an error besides, as is the case with sunzip, which parses the central directory at the end and checks it for consistency with the local file headers it has already seen.&lt;/p&gt;&lt;p&gt; If you need the extracted files to start with a certain prefix (so that they will be identified as a certain file type, for example), you can insert a data-carrying DEFLATE block just before the block that quotes the next header. Not every file has to participate in the bomb construction: you can include ordinary files alongside the bomb files if you need the zip file to conform to some higher-level format. (The source code has a &lt;code&gt;--template&lt;/code&gt;
option to facilitate this use case.)
Many file formats use zip as a container;
examples are Java JAR, Android APK, and LibreOffice documents.
&lt;/p&gt;&lt;p&gt;PDF is in many ways similar to zip. It has a cross-reference table at the end of the file that points to objects earlier in the file, and it supports DEFLATE compression of objects through the FlateDecode filter. Didier Stevens writes about having contained a 1√Ç GB stream inside a 2.6√Ç kB PDF file by stacking FlateDecode filters. If a PDF parser limits the amount of stacking, then it is probably possible to use the DEFLATE quoting idea to overlap PDF objects.&lt;/p&gt;&lt;p&gt;Detecting the specific class of zip bomb we have developed in this article is easy: look for overlapping files. Mark Adler has written a patch for Info-ZIP UnZip that does just that. In general, though, rejecting overlapping files does not by itself make it safe to handle untrusted zip files. There are zip bombs that do not rely on overlapping files, and there are malicious zip files that are not bombs. Furthermore, any such detection logic must be implemented inside the parser itself, not as a separate prefilter. One of the details omitted from the description of the zip format is that there is no single well-defined algorithm for locating the central directory in a zip file: two parsers may find two different central directories and therefore may not even agree on what files a zip file contains (slides 67√¢80). Predicting the total uncompressed size by summing the sizes of all files does not work, in general, because the sizes stored in metadata may not match (√Ç¬ß4.2.2) the actual uncompressed sizes. (See the "permits too-short file size" row in the compatibility table.) Robust protection against zip bombs involves sandboxing the parser to limit its use of time, memory, and disk space√¢just as if you were processing image files, or any other complex file format prone to parser bugs.&lt;/p&gt;&lt;table&gt;&lt;row span="8"&gt;&lt;cell role="head"&gt;Info-ZIP&lt;p&gt;UnZip 6.0&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;Python 3.7&lt;p&gt;zipfile&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;Go 1.12&lt;p&gt;archive/zip&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;yauzl 2.10.0&lt;p&gt;(Node.js)&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;Nail&lt;p&gt;examples/zip&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;Android√Ç 9.0.0√Ç r1&lt;p&gt;libziparchive&lt;/p&gt;&lt;/cell&gt;&lt;cell role="head"&gt;sunzip 0.4&lt;p&gt;(streaming)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;DEFLATE&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;Zip64&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;bzip2&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;permits mismatched filenames&lt;/cell&gt;&lt;cell&gt;warns&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;permits incorrect CRC-32&lt;/cell&gt;&lt;cell&gt;warns&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;if zero&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;permits too-short file size&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;permits file size of 232√Ç √¢√Ç 1&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;permits file count of 216√Ç √¢√Ç 1&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;unzips overlap.zip&lt;/cell&gt;&lt;cell&gt;warns&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row span="8"&gt;&lt;cell&gt;unzips zbsm.zip and zblg.zip&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;unzips zbxl.zip&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;cell&gt;√¢&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Credits&lt;/head&gt;&lt;p&gt; I√Ç thank Mark Adler, Blake Burkhart, Gynvael Coldwind, Russ Cox, Brandon Enright, Joran Dirk Greef, Marek Majkowski, Josh Wolfe, and the USENIX WOOT 2019 reviewers for comments on this article or a draft. Caol√É¬°n McNamara evaluated the security impact of the zip bombs in LibreOffice. @m1rko wrote a Russian translation. √•√•¬≤¬∏√•¬∑√®¬•√•¬∞√© wrote a Chinese translation. Daniel Ketterer reported that the &lt;code&gt;--template&lt;/code&gt; option
was broken after the addition of &lt;code&gt;--giant-steps&lt;/code&gt;.
&lt;/p&gt;&lt;p&gt;A√Ç version of this article appeared at the USENIX WOOT 2019 workshop. The workshop talk video, slides, and transcript are available. The source code of the paper is available. The artifacts prepared for submission are zipbomb-woot19.zip.&lt;/p&gt;&lt;p&gt;Did you find a system that chokes on one of these zip bombs? Did they help you demonstrate a vulnerability or win a bug bounty? Let me know and I'll try to mention it here.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;LibreOffice 6.1.5.2&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;zblg.zip renamed to zblg.odt or zblg.docx will cause LibreOffice to create and delete a number of ~4√Ç GB temporary files as it attempts to determine the file format. It does eventually finish, and it deletes the temporary files as it goes, so it's only a temporary DoS that doesn't fill up the disk. Caol√É¬°n McNamara replied to my bug report.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-2"&gt;Mozilla addons-server 2019.06.06&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;I tried the zip bombs against a local installation of addons-server, which is part of the software behind addons.mozilla.org. The system handles it gracefully, imposing a time limit of on extraction. The zip bomb expands as fast as the disk will let it up to the time limit, but after that point the process is killed and the unzipped files are eventually automatically cleaned up.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;UnZip 6.0&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;Mark Adler wrote a patch for UnZip to detect this class of zip bomb.&lt;/p&gt;&lt;p&gt;: I noticed that CVE-2019-13232 was assigned for UnZip. Personally, I would dispute that UnZip's (or any zip parser's) ability to process a zip bomb of the kind discussed here necessarily represents a security vulnerability, or even a bug. It's a natural implementation and does not violate the specification in any way that I can tell. The type discussed in this article is only one type of zip bomb, and there are many ways in which zip parsing can go wrong that are not bombs. If you want to defend against resource exhaustion attacks, you should not try to enumerate, detect, and block every individual known attack; rather you should impose external limits on time and other resources so that the parser cannot misbehave too much, no matter what kind of attack it faces. There is nothing wrong with attempting to detect and reject certain constructions as a first-pass optimization, but you can't stop there. If you do not eventually isolate and limit operations on untrusted data, your system is likely still vulnerable. Consider an analogy with cross-site scripting in HTML: the right defense is not to try and filter out bytes that may be interpreted as code, it's to escape everything properly.&lt;/p&gt;&lt;p&gt;Mark Adler's patch made its way into Debian in bug√Ç #931433. There were some unanticipated consequences: problems parsing certain Java JARs (bug #931895) and problems with the mutant zip format of Firefox's omni.ja file (bug #932404). SUSE decided not to do anything about CVE-2019-13232. I think both Debian's and SUSE's choices are defensible.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;ronomon/zip&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;Shortly after the publication of this article, Joran Dirk Greef published a restrictive zip parser (JavaScript) that prohibits irregularities such as overlapping files or unused space between files. While it may thereby reject certain valid zip files, the idea is to ensure that any downstream parsers will receive only clean, easy-to-parse files.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;antivirus engines&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;Overall, it seems that malware scanners have slowly begun to recognize zip bombs of this kind (or at least the specific samples available for download) as malicious. It would be interesting to see whether the detection is robust or brittle. You could reverse the order of the entries in the central directory, for example, and see whether the zip files are still detected. In the source code, there's a recipe for generating zbsm.extra.zip, which is like zbsm.zip except that it uses extra-field quoting instead of DEFLATE quoting√¢if you are a customer of an AV service that detects zbsm.zip but not zbsm.extra.zip, you should ask for an explanation. Another simple variant is inserting spacer files between the bomb files, which may fool certain overlap-detection algorithms.&lt;/p&gt;&lt;p&gt;Twitter user @TVqQAAMAAAAEAAA reports "McAfee AV on my test machine just exploded." I haven't independently confirmed it, nor do I have details such as a version number.&lt;/p&gt;&lt;p&gt;Tavis Ormandy points out that there are a number of "Timeout" results in the VirusTotal for zblg.zip (screenshot ). AhnLab-V3, ClamAV, DrWeb, Endgame, F-Secure, GData, K7AntiVirus, K7GW, MaxSecure, McAfee, McAfee-GW-Edition, Panda, Qihoo-360, Sophos ML, VBA32. The results for zbsm.zip (screenshot ) are similar, though with a different set of timed-out engines: Baido, Bkav, ClamAV, CMC, DrWeb, Endgame, ESET-NOD32, F-Secure, GData, Kingsoft, McAfee-GW-Edition, NANO-Antivirus, Acronis. Interestingly, there are no timeouts in the results for zbxl.zip; (screenshot ) perhaps this means that some antivirus doesn't support Zip64?&lt;/p&gt;&lt;p&gt;Forum user 100 reported that a certain ESET product did not detect zbxl.zip, possibly because it uses Zip64. An update in the thread three days later showed the product being updated to detect it.&lt;/p&gt;&lt;p&gt;In ClamAV bug 12356, Hanno B√É¬∂ck reported that zblg.zip caused high CPU usage in clamscan. An initial patch to detect overlapping files turned out to be incomplete because it only checked adjacent pairs of files. (I personally mishandled this issue by posting details of a workaround on the bug tracker, instead of reporting it privately.) A later patch imposed a time limit on file analysis.&lt;/p&gt;&lt;p&gt;: FlyTech Videos presented a video testing various zip bombs, including zbxl.zip, against Windows Defender, Windows Explorer, and 7-zip.&lt;/p&gt;&lt;p&gt;In my web server logs, I noticed a number of referers that appear to point to bug trackers.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;http://jira.athr.ru/browse/WEB-12882&lt;/item&gt;&lt;item&gt;https://project.avira.org/browse/ENGINE-2307&lt;/item&gt;&lt;item&gt;https://project.avira.org/browse/ENGINE-2363&lt;/item&gt;&lt;item&gt;https://topdesk-imp.cicapp.nl/tas/secure/mango/window/4&lt;/item&gt;&lt;item&gt;https://jira-eng-rtp3.cisco.com/jira/browse/AMP4E-4849&lt;/item&gt;&lt;item&gt;https://jira-eng-sjc1.cisco.com/jira/browse/CLAM-965&lt;/item&gt;&lt;item&gt;https://flightdataservices.atlassian.net/secure/RapidBoard.jspa?selectedIssue=FDS-136&lt;/item&gt;&lt;item&gt;https://projects.ucd.gpn.gov.uk/browse/VULN-1483&lt;/item&gt;&lt;item&gt;https://testrail-int.qa1.immunet.com/index.php?/cases/view/923720&lt;/item&gt;&lt;item&gt;http://redmine-int-prod.intranet.cnim.net/issues/5596&lt;/item&gt;&lt;item&gt;https://bugs.drweb.com/view.php?id=159759&lt;/item&gt;&lt;item&gt;https://dev-jira.dynatrace.org/browse/APM-188227&lt;/item&gt;&lt;item&gt;https://webgate.ec.europa.eu/CITnet/jira/browse/EPREL-2150&lt;/item&gt;&lt;item&gt;https://jira.egnyte-it.com/browse/IN-8480&lt;/item&gt;&lt;item&gt;https://jira.hq.eset.com/browse/CCDBL-1492&lt;/item&gt;&lt;item&gt;https://bugzilla.olympus.f5net.com/show_bug.cgi?id=819053&lt;/item&gt;&lt;item&gt;https://mantis.fortinet.com/bug_view_page.php?bug_id=0570222&lt;/item&gt;&lt;item&gt;https://redmine.joesecurity.org:64998/issues/4705&lt;/item&gt;&lt;item&gt;http://dev.maildev.jp/mantis/view.php?id=5839&lt;/item&gt;&lt;item&gt;https://confluence.managed.lu/pages/viewpage.action?pageId=47974242&lt;/item&gt;&lt;item&gt;https://jira-lvs.prod.mcafee.com/browse/TSWS-653&lt;/item&gt;&lt;item&gt;https://jira.modulbank.ru/browse/PV-33012&lt;/item&gt;&lt;item&gt;http://jira.netzwerk.intern:8080/browse/SALES-81&lt;/item&gt;&lt;item&gt;https://jira-hq.paloaltonetworks.local/browse/CON-43391&lt;/item&gt;&lt;item&gt;https://jira-hq.paloaltonetworks.local/browse/GSRT-11680&lt;/item&gt;&lt;item&gt;https://jira-hq.paloaltonetworks.local/browse/PAN-124201&lt;/item&gt;&lt;item&gt;https://paynearme.atlassian.net/browse/PNM-4494&lt;/item&gt;&lt;item&gt;https://jira.proofpoint.com/jira/browse/PE-29410&lt;/item&gt;&lt;item&gt;https://dev.pulsesecure.net/jira/browse/PRS-379163&lt;/item&gt;&lt;item&gt;https://qualtrics.atlassian.net/browse/APP-326&lt;/item&gt;&lt;item&gt;https://jira.sastdev.net/browse/CIS-2819&lt;/item&gt;&lt;item&gt;https://jira.sastdev.net/secure/RapidBoard.jspa?selectedIssue=EC-709&lt;/item&gt;&lt;item&gt;https://bugzilla.seeburger.de/show_bug.cgi?id=89294&lt;/item&gt;&lt;item&gt;https://svm.cert.siemens.com/auseno/create_edit_vulnerability.php?vulnid=48573&lt;/item&gt;&lt;item&gt;https://jira.sophos.net/browse/CPISSUE-6560&lt;/item&gt;&lt;item&gt;https://jira.vrt.sourcefire.com/browse/TT-1070&lt;/item&gt;&lt;item&gt;https://task.jarvis.trendmicro.com/browse/JPSE-10432&lt;/item&gt;&lt;item&gt;https://segjira.trendmicro.com:8443/browse/SEG-55636&lt;/item&gt;&lt;item&gt;https://segjira.trendmicro.com:8443/browse/SEG-58824&lt;/item&gt;&lt;item&gt;https://ucsc-cgl.atlassian.net/secure/RapidBoard.jspa?selectedIssue=SEAB-327&lt;/item&gt;&lt;item&gt;https://jira.withbc.com/browse/BC-43950&lt;/item&gt;&lt;item&gt;https://zscaler.zendesk.com/agent/tickets/849971&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-6"&gt;web browsers&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;I didn't directly experience this myself, but reports online say that Chrome and Safari may automatically unzip files after downloading.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;ittakir: "√ê¬°√ê¬∫√ê¬∞√ë√ê¬∞√ê¬ª √ë√ê¬∞√ê¬º√ë√ê¬π √ê¬º√ê¬∞√ê¬ª√ê¬µ√ê¬Ω√ë√ê¬∫√ê¬∏√ê¬π √ë√ê¬∞√ê¬π√ê¬ª √ê¬Ω√ê¬∞ 5GB, Chrome √ë√ë√ë √ê¬∂√ê¬µ √ê¬Ω√ê¬∞√ë√ê¬∞√ê¬ª √ê¬µ√ê¬≥√ê¬æ √ë√ê¬∞√ë√ê¬ø√ê¬∞√ê¬∫√ê¬æ√ê¬≤√ë√ê¬≤√ê¬∞√ë√ë, √ë √ê¬æ√ë√ë √ê¬µ√ê¬≥√ê¬æ √ê¬æ√ê¬± √ë√ë√ê¬æ√ê¬º √ê¬Ω√ê¬µ √ê¬ø√ë√ê¬æ√ë√ê¬∏√ê¬ª√ê¬∏, √ê¬Ω√ë √ê¬∏ √ê¬∫√ë√ë√ê¬∞√ë√ë √ê¬ø√ë√ê¬æ√ë√ê¬µ√ë√ë√ê¬æ√ë √ê¬∏ √ê¬¥√ê¬∏√ë√ê¬∫." "I downloaded the smallest file on 5GB, Chrome immediately began to unpack it, although it was not asked for it, well, to eat the processor and disk."&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Rzah: "Yet another reason why 'Open Safe files after downloading' is a stupid default setting for a web browser."&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Chromium commit f04d9b15bd1cba1433ad5453bc3ebff933d0e3bb is perhaps related:&lt;/p&gt;&lt;p&gt;Add metrics detecting anomalously high ZIP compression ratios&lt;/p&gt;&lt;p&gt;It's possible for a single ZIP entry to be very large, even if we only scan small ZIP archives. These metrics will measure how often that occurs.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-7"&gt;filesystems&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;Something I didn't anticipate: unzipping one of the bombs on a compressed filesystem can be relatively safe.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;flying_gel: "If I unzip this onto a compressed zfs dataset, will the resulting file be small? Edit: Just did a small test with a 42KB-&amp;gt;5.5GB zip bomb. I ended up with 165MB worth of files so while just 3% of the full bomb, it's still a 4028 times inflation. ... I only have the standard LZ4 compression enabled, no dedup."&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dd-8"&gt;&lt;p&gt;Links to this article had been widely shared on Twitter since around , but around it began showing an "unsafe link" interstitial (screenshot, archive).&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-9"&gt;Safe Browsing&lt;/item&gt;&lt;item rend="dd-9"&gt;&lt;p&gt;Sometime around it seems that this page, and every page on a *.bamsoftware.com domain, got added to the Safe Browsing service used by web browsers to block malware and phishing sites. Site status check, block page screenshot. From a few quick checks, it looks like pages on bamsoftware.com have been demoted or delisted on the google.com search engine as well.&lt;/p&gt;&lt;p&gt;The Safe Browsing block is a bit annoying, because it disrupted Snowflake, a completely unrelated service that happened to use the domain snowflake-broker.bamsoftware.com, which did not even host any files but was strictly a web API server. See #31230 Firefox addon blocked from agent by Google Safe Browsing service.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-10"&gt;Xfinity xFi Protected Browsing&lt;/item&gt;&lt;item rend="dd-10"&gt;&lt;p&gt;On , I was informed by Hooman Mohajeri Moghaddam that the Comcast Xfinity xFi "Protected Browsing" feature blocks the bamsoftware.com domain, including this page (screenshot).&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-11"&gt;D std.zip&lt;/item&gt;&lt;item rend="dd-11"&gt;&lt;p&gt;The D programming language made a modification to the std.zip module to detect overlapping files.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-12"&gt;Apple iOS and iPadOS&lt;/item&gt;&lt;item rend="dd-12"&gt;&lt;p&gt;Dzmitry Plotnikau sent me a report saying that a zip bomb could use up all cache storage on iPhones running iOS 12 and 13, even if only opened using "Quick look." The exhaustion of storage could have various side effects, including misbehaving apps, deletion of local cloud files, and OS crashes, in some cases requiring a factory reset to remedy. The bug was mitigated in iOS 14.0 (and likely other, contemporaneous point release of iOS and iPadOS). See HT211850 under the "libarchive" heading.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;A final plea&lt;/head&gt;&lt;p&gt;It's time to put an end to Facebook. Working there is not ethically neutral: every day that you go into work, you are doing something wrong. If you have a Facebook account, delete it. If you work at Facebook, quit.&lt;/p&gt;&lt;p&gt;And let us not forget that the National Security Agency must be destroyed.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46331216</guid><pubDate>Fri, 19 Dec 2025 21:34:10 +0000</pubDate></item><item><title>CSS Grid Lanes</title><link>https://webkit.org/blog/17660/introducing-css-grid-lanes/</link><description>&lt;doc fingerprint="de98d3333fec29eb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing CSS Grid Lanes&lt;/head&gt;
    &lt;p&gt;It‚Äôs here, the future of masonry layouts on the web! After the groundwork laid by Mozilla, years of effort by Apple‚Äôs WebKit team, and many rounds debate at the CSS Working Group with all the browsers, it‚Äôs now clear how it works.&lt;/p&gt;
    &lt;p&gt;Introducing CSS Grid Lanes.&lt;/p&gt;
    &lt;code&gt;.container {
  display: grid-lanes;
  grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
  gap: 16px;
}
&lt;/code&gt;
    &lt;p&gt;Try it today in Safari Technology Preview 234.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Grid Lanes work&lt;/head&gt;
    &lt;p&gt;Let‚Äôs break down exactly how to create this classic layout.&lt;/p&gt;
    &lt;p&gt;First, the HTML.&lt;/p&gt;
    &lt;code&gt;&amp;lt;main class="container"&amp;gt;
  &amp;lt;figure&amp;gt;&amp;lt;img src="photo-1.jpg"&amp;gt;&amp;lt;/figure&amp;gt;
  &amp;lt;figure&amp;gt;&amp;lt;img src="photo-2.jpg"&amp;gt;&amp;lt;/figure&amp;gt;
  &amp;lt;figure&amp;gt;&amp;lt;img src="photo-3.jpg"&amp;gt;&amp;lt;/figure&amp;gt;
  &amp;lt;!-- etc --&amp;gt;
&amp;lt;/main&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Let‚Äôs start by applying &lt;code&gt;display: grid-lanes&lt;/code&gt; to the &lt;code&gt;main&lt;/code&gt; element to create a Grid container ready to make this kind of layout. Then we use &lt;code&gt;grid-template-columns&lt;/code&gt; to create the ‚Äúlanes‚Äù with the full power of CSS Grid.&lt;/p&gt;
    &lt;p&gt;In this case, we‚Äôll use &lt;code&gt;repeat(auto-fill, minmax(250px, 1fr))&lt;/code&gt; to create flexible columns at least 250 pixels wide. The browser will decide how many columns to make, filling all available space.&lt;/p&gt;
    &lt;p&gt;And then, &lt;code&gt;gap: 16px&lt;/code&gt; gives us 16 pixel gaps between the lanes, and 16 pixel gaps between items within the lanes.&lt;/p&gt;
    &lt;code&gt;.container {
  display: grid-lanes;
  grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
  gap: 16px;
}
&lt;/code&gt;
    &lt;p&gt;That‚Äôs it! In three lines of CSS, with zero media queries or container queries, we created a flexible layout that works on all screen sizes.&lt;/p&gt;
    &lt;p&gt;Think of it like a highway of cars in bumper-to-bumper traffic.&lt;/p&gt;
    &lt;p&gt;Just like the classic Masonry library, as the browser decides where to put each item, the next one is placed in whichever column gets it closest to the top of the window. Like traffic, each car ‚Äúchanges lanes‚Äù to end up in the lane that gets them ‚Äúthe furthest ahead‚Äù.&lt;/p&gt;
    &lt;p&gt;This layout makes it possible for users to tab across the lanes to all currently-visible content, (not down the first column below the fold to the very bottom, and then back to the top of the second column). It also makes it possible for you to build a site that keeps loading more content as the user scrolls, infinitely, without needing JavaScript to handle the layout.&lt;/p&gt;
    &lt;head rend="h2"&gt;The power of Grid&lt;/head&gt;
    &lt;head rend="h3"&gt;Varying lane sizes&lt;/head&gt;
    &lt;p&gt;Because Grid Lanes uses the full power of CSS Grid to define lanes using &lt;code&gt;grid-template-*&lt;/code&gt;, it‚Äôs easy to create creative design variations.&lt;/p&gt;
    &lt;p&gt;For example, we can create a flexible layout with alternating narrow and wide columns ‚Äî where both the first and last columns are always narrow, even as the number of columns changes with the viewport size. This is accomplished with &lt;code&gt;grid-template-columns: repeat(auto-fill, minmax(8rem, 1fr) minmax(16rem, 2fr)) minmax(8rem, 1fr)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a whole world of possibilities using &lt;code&gt;grid-template-*&lt;/code&gt; syntax.&lt;/p&gt;
    &lt;head rend="h4"&gt;Spanning items&lt;/head&gt;
    &lt;p&gt;Since we have the full power of Grid layout, we can also span lanes, of course.&lt;/p&gt;
    &lt;code&gt;main {
  display: grid-lanes;
  grid-template-columns: repeat(auto-fill, minmax(20ch, 1fr));
  gap: 2lh;
}
article { 
  grid-column: span 1; 
}
@media (1250px &amp;lt; width) {
  article:nth-child(1) { 
    grid-column: span 4;             
  }
  article:nth-child(2), article:nth-child(3), article:nth-child(4), article:nth-child(5), article:nth-child(6), article:nth-child(7), article:nth-child(8) { 
    grid-column: span 2; 
  }
}
&lt;/code&gt;
    &lt;p&gt;All the article teasers are first set to span 1 column. Then the 1st item is specifically told to span 4 columns, while the 2nd ‚Äì 8th to span 2 columns. This creates a far more dynamic graphic design than the typical symmetrical, everything the same-width, everything the same-height layout that‚Äôs dominated over the last decade.&lt;/p&gt;
    &lt;head rend="h3"&gt;Placing items&lt;/head&gt;
    &lt;p&gt;We can also explicitly place items while using Grid Lanes. Here, the header is always placed in the last column, no matter how many columns exist.&lt;/p&gt;
    &lt;code&gt;main {
  display: grid-lanes;
  grid-template-columns: repeat(auto-fill, minmax(24ch, 1fr));
}
header {
  grid-column: -3 / -1;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Changing directions&lt;/head&gt;
    &lt;p&gt;Yes, lanes can go either direction! All of the examples above happen to create a ‚Äúwaterfall‚Äù shape, where the content is laid out in columns. But Grid Lanes can be used to create a layout in the other direction, in a ‚Äúbrick‚Äù layout shape.&lt;/p&gt;
    &lt;p&gt;The browser automatically creates a waterfall layout when you define columns with &lt;code&gt;grid-template-columns&lt;/code&gt;, like this:&lt;/p&gt;
    &lt;code&gt;.container {
  display: grid-lanes;
  grid-template-columns: 1fr 1fr 1fr 1fr;
}
&lt;/code&gt;
    &lt;p&gt;If you want a brick layout in the other direction, instead define the rows with &lt;code&gt;grid-template-rows&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;.container {
  display: grid-lanes;
  grid-template-rows: 1fr 1fr 1fr;
}
&lt;/code&gt;
    &lt;p&gt;This works automatically thanks to a new default for&lt;code&gt;grid-auto-flow&lt;/code&gt;, the &lt;code&gt;normal&lt;/code&gt; value.  It figures out whether to create columns or rows based on whether you defined the lanes using &lt;code&gt;grid-template-columns&lt;/code&gt; or &lt;code&gt;grid-template-rows&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The CSS Working Group is still discussing which property will explicitly control the flow orientation, and what its syntax will be. The debate is over whether to reuse &lt;code&gt;grid-auto-flow&lt;/code&gt; or create new properties like &lt;code&gt;grid-lanes-direction&lt;/code&gt;. If you‚Äôre interested in reading about the options being considered or chime in with your thoughts, see this discussion.&lt;/p&gt;
    &lt;p&gt;However, since &lt;code&gt;normal&lt;/code&gt; will be the initial value either way, you don‚Äôt have to wait for this decision to learn Grid Lanes. When you define only one direction ‚Äî &lt;code&gt;grid-template-rows&lt;/code&gt; or &lt;code&gt;grid-template-columns&lt;/code&gt; ‚Äî it will Just Work‚Ñ¢. (If it doesn‚Äôt, check if &lt;code&gt;grid-auto-flow&lt;/code&gt; is set to a conflicting value. You can&lt;code&gt;unset&lt;/code&gt; it if needed.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Placement sensitivity&lt;/head&gt;
    &lt;p&gt;‚ÄúTolerance‚Äù is a new concept created for Grid Lanes. It lets you adjust just how picky the layout algorithm is when deciding where to place items.&lt;/p&gt;
    &lt;p&gt;Look at the next drawing. Notice that Car 4 is a tiny bit shorter than Car 1. When the ‚Äútolerance‚Äù is zero, Car 6 ends up in the right-most lane, while Car 7 is on the left. Car 6 ends up behind Car 4 on the right because that gets it a tiny bit closer ‚Äúdown the road‚Äù (closer to the top of the Grid container). Car 7 then takes the next-closest-to-the-top slot, and ends up behind Car 1 on the left. The end result? The first horizontal grouping of content is ordered 1, 2, 3, 4, and the next is 7, 5, 6.&lt;/p&gt;
    &lt;p&gt;But the difference in length between Car 1 and Car 4 is tiny. Car 6 isn‚Äôt meaningfully closer to the top of the page. And having item 6 on the right, with item 7 on the left is likely an unexpected experience ‚Äî especially for users who are tabbing through content, or when the content order is somehow labeled.&lt;/p&gt;
    &lt;p&gt;These tiny differences in size don‚Äôt matter in any practical sense. Instead, the browser should consider item sizes like Car 1 and Car 4 to be a tie. That‚Äôs why the default for &lt;code&gt;item-tolerance&lt;/code&gt; is &lt;code&gt;1em&lt;/code&gt; ‚Äî which means only differences in content length greater than 1 em will matter when figuring out where the next item goes.&lt;/p&gt;
    &lt;p&gt;If you‚Äôd like the layout of items to shuffle around less, you can set a higher value for &lt;code&gt;item-tolerance&lt;/code&gt;. In the next digram, the tolerance is set to half-a-car, causing the cars to lay out basically from left to right and only moving to another lane to avoid the extra-long limo. Now, the horizontal groupings of content are 1, 2, 3, 4, and 5, 6, 7.&lt;/p&gt;
    &lt;p&gt;Think of tolerance as how chill you want the car drivers to be. Will they change lanes to get just a few inches ahead? Or will they only move if there‚Äôs a lot of space in the other lane? The amount of space you want them to care about is the amount you set in &lt;code&gt;item-tolerance&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Remember that people tabbing through the page will see each item highlighted as it comes into focus, and may be experiencing the page through a screenreader. An item tolerance that‚Äôs set too high can create an awkward experience jumping up and down the layout. An item tolerance that‚Äôs too low can result in jumping back and forth across the layout more than necessary. Adjust &lt;code&gt;item-tolerance&lt;/code&gt; to something appropriate for the sizes and size variations of your content.&lt;/p&gt;
    &lt;p&gt;Currently, this property is named &lt;code&gt;item-tolerance&lt;/code&gt; in the specification and in Safari Technology Preview 234. However, there is still a chance this name will change, perhaps to something like &lt;code&gt;flow-tolerance&lt;/code&gt; or &lt;code&gt;pack-tolerance&lt;/code&gt;. If you have a preference, or ideas for a better name, you can chime in here. Keep an eye out for updates about the final name before using this property on production websites.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out&lt;/head&gt;
    &lt;p&gt;Try out Grid Lanes in Safari Technology Preview 234! All of the demos at webkit.org/demos/grid3 have been updated with the new syntax, including other use cases for Grid Lanes. It‚Äôs not just for images! For example, a mega menu footer full of links suddenly becomes easy to layout.&lt;/p&gt;
    &lt;code&gt;.container {
  display: grid-lanes;
  grid-template-columns: repeat(auto-fill, minmax(max-content, 24ch));
  column-gap: 4lh;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;What‚Äôs next?&lt;/head&gt;
    &lt;p&gt;There are a few last decisions for the CSS Working Group to make. But overall, the feature as described in this article is ready to go. It‚Äôs time to try it out. And it‚Äôs finally safe to commit the basic syntax to memory!&lt;/p&gt;
    &lt;p&gt;We‚Äôd love for you to make some demos! Demonstrate what new use cases you can imagine. And let us know about any bugs or possible improvements you discover. Ping Jen Simmons on Bluesky or Mastodon with links, comments and ideas.&lt;/p&gt;
    &lt;p&gt;Our team has been working on this since mid-2022, implementing in WebKit and writing the web standard. We can‚Äôt wait to see what you will do with it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46331586</guid><pubDate>Fri, 19 Dec 2025 22:13:06 +0000</pubDate></item><item><title>Build Your Own React</title><link>https://pomb.us/build-your-own-react/</link><description>&lt;doc fingerprint="1f00bd736d799b78"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Build your own React&lt;/head&gt;
    &lt;p&gt;We are going to rewrite React from scratch. Step by step. Following the architecture from the real React code but without all the optimizations and non-essential features.&lt;/p&gt;
    &lt;p&gt;If you‚Äôve read any of my previous ‚Äúbuild your own React‚Äù posts, the difference is that this post is based on React 16.8, so we can now use hooks and drop all the code related to classes.&lt;/p&gt;
    &lt;p&gt;You can find the history with the old blog posts and the code on the Didact repo. There‚Äôs also a talk covering the same content. But this is a self-contained post.&lt;/p&gt;
    &lt;p&gt;Starting from scratch, these are all the things we‚Äôll add to our version of React one by one:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Step I: The &lt;code&gt;createElement&lt;/code&gt;Function&lt;/item&gt;
      &lt;item&gt;Step II: The &lt;code&gt;render&lt;/code&gt;Function&lt;/item&gt;
      &lt;item&gt;Step III: Concurrent Mode&lt;/item&gt;
      &lt;item&gt;Step IV: Fibers&lt;/item&gt;
      &lt;item&gt;Step V: Render and Commit Phases&lt;/item&gt;
      &lt;item&gt;Step VI: Reconciliation&lt;/item&gt;
      &lt;item&gt;Step VII: Function Components&lt;/item&gt;
      &lt;item&gt;Step VIII: Hooks&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Step Zero: Review&lt;/head&gt;
    &lt;p&gt;But first let‚Äôs review some basic concepts. You can skip this step if you already have a good idea of how React, JSX and DOM elements work.&lt;/p&gt;
    &lt;p&gt;We‚Äôll use this React app, just three lines of code. The first one defines a React element. The next one gets a node from the DOM. The last one renders the React element into the container.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs remove all the React specific code and replace it with vanilla JavaScript.&lt;/p&gt;
    &lt;p&gt;On the first line we have the element, defined with JSX. It isn‚Äôt even valid JavaScript, so in order to replace it with vanilla JS, first we need to replace it with valid JS.&lt;/p&gt;
    &lt;p&gt;JSX is transformed to JS by build tools like Babel. The transformation is usually simple: replace the code inside the tags with a call to &lt;code&gt;createElement&lt;/code&gt;, passing the tag name, the props and the children as parameters.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;React.createElement&lt;/code&gt; creates an object from its arguments. Besides some validations, that‚Äôs all it does. So we can safely replace the function call with its output.&lt;/p&gt;
    &lt;p&gt;And this is what an element is, an object with two properties: &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;props&lt;/code&gt; (well, it has more, but we only care about these two).&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;type&lt;/code&gt; is a string that specifies the type of the DOM node we want to create, it‚Äôs the &lt;code&gt;tagName&lt;/code&gt; you pass to &lt;code&gt;document.createElement&lt;/code&gt; when you want to create an HTML element. It can also be a function, but we‚Äôll leave that for Step VII.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;props&lt;/code&gt; is another object, it has all the keys and values from the JSX attributes. It also has a special property: &lt;code&gt;children&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;children&lt;/code&gt; in this case is a string, but it‚Äôs usually an array with more elements. That‚Äôs why elements are also trees.&lt;/p&gt;
    &lt;p&gt;The other piece of React code we need to replace is the call to &lt;code&gt;ReactDOM.render&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;render&lt;/code&gt; is where React changes the DOM, so let‚Äôs do the updates ourselves.&lt;/p&gt;
    &lt;p&gt;First we create a node* using the element &lt;code&gt;type&lt;/code&gt;, in this case &lt;code&gt;h1&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Then we assign all the element &lt;code&gt;props&lt;/code&gt; to that node. Here it‚Äôs just the title.&lt;/p&gt;
    &lt;p&gt;* To avoid confusion, I‚Äôll use ‚Äúelement‚Äù to refer to React elements and ‚Äúnode‚Äù for DOM elements.&lt;/p&gt;
    &lt;p&gt;Then we create the nodes for the children. We only have a string as a child so we create a text node.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;textNode&lt;/code&gt; instead of setting &lt;code&gt;innerText&lt;/code&gt; will allow us to treat all elements in the same way later. Note also how we set the &lt;code&gt;nodeValue&lt;/code&gt; like we did it with the &lt;code&gt;h1&lt;/code&gt; title, it‚Äôs almost as if the string had &lt;code&gt;props: {nodeValue: "hello"}&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally, we append the &lt;code&gt;textNode&lt;/code&gt; to the &lt;code&gt;h1&lt;/code&gt; and the &lt;code&gt;h1&lt;/code&gt; to the &lt;code&gt;container&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And now we have the same app as before, but without using React.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step I: The &lt;code&gt;createElement&lt;/code&gt; Function&lt;/head&gt;
    &lt;p&gt;Let‚Äôs start again with another app. This time we‚Äôll replace React code with our own version of React.&lt;/p&gt;
    &lt;p&gt;We‚Äôll start by writing our own &lt;code&gt;createElement&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs transform the JSX to JS so we can see the &lt;code&gt;createElement&lt;/code&gt; calls.&lt;/p&gt;
    &lt;p&gt;As we saw in the previous step, an element is an object with &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;props&lt;/code&gt;. The only thing that our function needs to do is create that object.&lt;/p&gt;
    &lt;p&gt;We use the spread operator for the &lt;code&gt;props&lt;/code&gt; and the rest parameter syntax for the &lt;code&gt;children&lt;/code&gt;, this way the &lt;code&gt;children&lt;/code&gt; prop will always be an array.&lt;/p&gt;
    &lt;p&gt;For example, &lt;code&gt;createElement("div")&lt;/code&gt; returns:&lt;/p&gt;
    &lt;quote&gt;{"type": "div","props": { "children": [] }}&lt;/quote&gt;
    &lt;p&gt;&lt;code&gt;createElement("div", null, a)&lt;/code&gt; returns:&lt;/p&gt;
    &lt;quote&gt;{"type": "div","props": { "children": [a] }}&lt;/quote&gt;
    &lt;p&gt;and &lt;code&gt;createElement("div", null, a, b)&lt;/code&gt; returns:&lt;/p&gt;
    &lt;quote&gt;{"type": "div","props": { "children": [a, b] }}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;children&lt;/code&gt; array could also contain primitive values like strings or numbers. So we‚Äôll wrap everything that isn‚Äôt an object inside its own element and create a special type for them: &lt;code&gt;TEXT_ELEMENT&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;React doesn‚Äôt wrap primitive values or create empty arrays when there aren‚Äôt &lt;code&gt;children&lt;/code&gt;, but we do it because it will simplify our code, and for our library we prefer simple code than performant code.&lt;/p&gt;
    &lt;p&gt;We are still using React‚Äôs &lt;code&gt;createElement&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In order to replace it, let‚Äôs give a name to our library. We need a name that sounds like React but also hints its didactic purpose.&lt;/p&gt;
    &lt;p&gt;We‚Äôll call it Didact.&lt;/p&gt;
    &lt;p&gt;But we still want to use JSX here. How do we tell babel to use Didact‚Äôs &lt;code&gt;createElement&lt;/code&gt; instead of React‚Äôs?&lt;/p&gt;
    &lt;p&gt;If we have a comment like this one, when babel transpiles the JSX it will use the function we define.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step II: The &lt;code&gt;render&lt;/code&gt; Function&lt;/head&gt;
    &lt;p&gt;Next, we need to write our version of the &lt;code&gt;ReactDOM.render&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;For now, we only care about adding stuff to the DOM. We‚Äôll handle updating and deleting later.&lt;/p&gt;
    &lt;p&gt;We start by creating the DOM node using the element type, and then append the new node to the container.&lt;/p&gt;
    &lt;p&gt;We recursively do the same for each child.&lt;/p&gt;
    &lt;p&gt;We also need to handle text elements, if the element type is &lt;code&gt;TEXT_ELEMENT&lt;/code&gt; we create a text node instead of a regular node.&lt;/p&gt;
    &lt;p&gt;The last thing we need to do here is assign the element props to the node.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs it. We now have a library that can render JSX to the DOM.&lt;/p&gt;
    &lt;p&gt;Give it a try on codesandbox.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step III: Concurrent Mode&lt;/head&gt;
    &lt;p&gt;But‚Ä¶ before we start adding more code we need a refactor.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a problem with this recursive call.&lt;/p&gt;
    &lt;p&gt;Once we start rendering, we won‚Äôt stop until we have rendered the complete element tree. If the element tree is big, it may block the main thread for too long. And if the browser needs to do high priority stuff like handling user input or keeping an animation smooth, it will have to wait until the render finishes.&lt;/p&gt;
    &lt;p&gt;So we are going to break the work into small units, and after we finish each unit we‚Äôll let the browser interrupt the rendering if there‚Äôs anything else that needs to be done.&lt;/p&gt;
    &lt;p&gt;We use &lt;code&gt;requestIdleCallback&lt;/code&gt; to make a loop. You can think of &lt;code&gt;requestIdleCallback&lt;/code&gt; as a &lt;code&gt;setTimeout&lt;/code&gt;, but instead of us telling it when to run, the browser will run the callback when the main thread is idle.&lt;/p&gt;
    &lt;p&gt;React doesn‚Äôt use &lt;code&gt;requestIdleCallback&lt;/code&gt; anymore. Now it uses the scheduler package. But for this use case it‚Äôs conceptually the same.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;requestIdleCallback&lt;/code&gt; also gives us a deadline parameter. We can use it to check how much time we have until the browser needs to take control again.&lt;/p&gt;
    &lt;p&gt;As of November 2019, Concurrent Mode isn‚Äôt stable in React yet. The stable version of the loop looks more like this:&lt;/p&gt;
    &lt;quote&gt;while (nextUnitOfWork) {nextUnitOfWork = performUnitOfWork(nextUnitOfWork)}&lt;/quote&gt;
    &lt;p&gt;To start using the loop we‚Äôll need to set the first unit of work, and then write a &lt;code&gt;performUnitOfWork&lt;/code&gt; function that not only performs the work but also returns the next unit of work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step IV: Fibers&lt;/head&gt;
    &lt;p&gt;To organize the units of work we‚Äôll need a data structure: a fiber tree.&lt;/p&gt;
    &lt;p&gt;We‚Äôll have one fiber for each element and each fiber will be a unit of work.&lt;/p&gt;
    &lt;p&gt;Let me show you with an example.&lt;/p&gt;
    &lt;p&gt;Suppose we want to render an element tree like this one:&lt;/p&gt;
    &lt;quote&gt;Didact.render(&amp;lt;div&amp;gt;&amp;lt;h1&amp;gt;&amp;lt;p /&amp;gt;&amp;lt;a /&amp;gt;&amp;lt;/h1&amp;gt;&amp;lt;h2 /&amp;gt;&amp;lt;/div&amp;gt;,container)&lt;/quote&gt;
    &lt;p&gt;In the &lt;code&gt;render&lt;/code&gt; we‚Äôll create the root fiber and set it as the &lt;code&gt;nextUnitOfWork&lt;/code&gt;. The rest of the work will happen on the &lt;code&gt;performUnitOfWork&lt;/code&gt; function, there we will do three things for each fiber:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;add the element to the DOM&lt;/item&gt;
      &lt;item&gt;create the fibers for the element‚Äôs children&lt;/item&gt;
      &lt;item&gt;select the next unit of work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the goals of this data structure is to make it easy to find the next unit of work. That‚Äôs why each fiber has a link to its first child, its next sibling and its parent.&lt;/p&gt;
    &lt;p&gt;When we finish performing work on a fiber, if it has a &lt;code&gt;child&lt;/code&gt; that fiber will be the next unit of work.&lt;/p&gt;
    &lt;p&gt;From our example, when we finish working on the &lt;code&gt;div&lt;/code&gt; fiber the next unit of work will be the &lt;code&gt;h1&lt;/code&gt; fiber.&lt;/p&gt;
    &lt;p&gt;If the fiber doesn‚Äôt have a &lt;code&gt;child&lt;/code&gt;, we use the &lt;code&gt;sibling&lt;/code&gt; as the next unit of work.&lt;/p&gt;
    &lt;p&gt;For example, the &lt;code&gt;p&lt;/code&gt; fiber doesn‚Äôt have a &lt;code&gt;child&lt;/code&gt; so we move to the &lt;code&gt;a&lt;/code&gt; fiber after finishing it.&lt;/p&gt;
    &lt;p&gt;And if the fiber doesn‚Äôt have a &lt;code&gt;child&lt;/code&gt; nor a &lt;code&gt;sibling&lt;/code&gt; we go to the ‚Äúuncle‚Äù: the &lt;code&gt;sibling&lt;/code&gt; of the &lt;code&gt;parent&lt;/code&gt;. Like &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;h2&lt;/code&gt; fibers from the example.&lt;/p&gt;
    &lt;p&gt;Also, if the &lt;code&gt;parent&lt;/code&gt; doesn‚Äôt have a &lt;code&gt;sibling&lt;/code&gt;, we keep going up through the &lt;code&gt;parent&lt;/code&gt;s until we find one with a &lt;code&gt;sibling&lt;/code&gt; or until we reach the root. If we have reached the root, it means we have finished performing all the work for this &lt;code&gt;render&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs put it into code.&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs remove this code from the &lt;code&gt;render&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;We keep the part that creates a DOM node in its own function, we are going to use it later.&lt;/p&gt;
    &lt;p&gt;In the &lt;code&gt;render&lt;/code&gt; function we set &lt;code&gt;nextUnitOfWork&lt;/code&gt; to the root of the fiber tree.&lt;/p&gt;
    &lt;p&gt;Then, when the browser is ready,it will call our &lt;code&gt;workLoop&lt;/code&gt; and we‚Äôll start working on the root.&lt;/p&gt;
    &lt;p&gt;First, we create a new node and append it to the DOM.&lt;/p&gt;
    &lt;p&gt;We keep track of the DOM node in the &lt;code&gt;fiber.dom&lt;/code&gt; property.&lt;/p&gt;
    &lt;p&gt;Then for each child we create a new fiber.&lt;/p&gt;
    &lt;p&gt;And we add it to the fiber tree setting it either as a child or as a sibling, depending on whether it‚Äôs the first child or not.&lt;/p&gt;
    &lt;p&gt;Finally we search for the next unit of work. We first try with the child, then with the sibling, then with the uncle, and so on.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs our &lt;code&gt;performUnitOfWork&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step V: Render and Commit Phases&lt;/head&gt;
    &lt;p&gt;We have another problem here.&lt;/p&gt;
    &lt;p&gt;We are adding a new node to the DOM each time we work on an element. And, remember, the browser could interrupt our work before we finish rendering the whole tree. In that case, the user will see an incomplete UI. And we don‚Äôt want that.&lt;/p&gt;
    &lt;p&gt;So we need to remove the part that mutates the DOM from here.&lt;/p&gt;
    &lt;p&gt;Instead, we‚Äôll keep track of the root of the fiber tree. We call it the work in progress root or &lt;code&gt;wipRoot&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And once we finish all the work (we know it because there isn‚Äôt a next unit of work) we commit the whole fiber tree to the DOM.&lt;/p&gt;
    &lt;p&gt;We do it in the &lt;code&gt;commitRoot&lt;/code&gt; function. Here we recursively append all the nodes to the dom.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step VI: Reconciliation&lt;/head&gt;
    &lt;p&gt;So far we only added stuff to the DOM, but what about updating or deleting nodes?&lt;/p&gt;
    &lt;p&gt;That‚Äôs what we are going to do now, we need to compare the elements we receive on the &lt;code&gt;render&lt;/code&gt; function to the last fiber tree we committed to the DOM.&lt;/p&gt;
    &lt;p&gt;So we need to save a reference to that ‚Äúlast fiber tree we committed to the DOM‚Äù after we finish the commit. We call it &lt;code&gt;currentRoot&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We also add the &lt;code&gt;alternate&lt;/code&gt; property to every fiber. This property is a link to the old fiber, the fiber that we committed to the DOM in the previous commit phase.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs extract the code from &lt;code&gt;performUnitOfWork&lt;/code&gt; that creates the new fibers‚Ä¶&lt;/p&gt;
    &lt;p&gt;‚Ä¶to a new &lt;code&gt;reconcileChildren&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;Here we will reconcile the old fibers with the new elements.&lt;/p&gt;
    &lt;p&gt;We iterate at the same time over the children of the old fiber (&lt;code&gt;wipFiber.alternate&lt;/code&gt;) and the array of elements we want to reconcile.&lt;/p&gt;
    &lt;p&gt;If we ignore all the boilerplate needed to iterate over an array and a linked list at the same time, we are left with what matters most inside this while: &lt;code&gt;oldFiber&lt;/code&gt; and &lt;code&gt;element&lt;/code&gt;. The &lt;code&gt;element&lt;/code&gt; is the thing we want to render to the DOM and the &lt;code&gt;oldFiber&lt;/code&gt; is what we rendered the last time.&lt;/p&gt;
    &lt;p&gt;We need to compare them to see if there‚Äôs any change we need to apply to the DOM.&lt;/p&gt;
    &lt;p&gt;To compare them we use the type:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;if the old fiber and the new element have the same type, we can keep the DOM node and just update it with the new props&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;if the type is different and there is a new element, it means we need to create a new DOM node&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;and if the types are different and there is an old fiber, we need to remove the old node&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here React also uses keys, that makes a better reconciliation. For example, it detects when children change places in the element array.&lt;/p&gt;
    &lt;p&gt;When the old fiber and the element have the same type, we create a new fiber keeping the DOM node from the old fiber and the props from the element.&lt;/p&gt;
    &lt;p&gt;We also add a new property to the fiber: the &lt;code&gt;effectTag&lt;/code&gt;. We‚Äôll use this property later, during the commit phase.&lt;/p&gt;
    &lt;p&gt;Then for the case where the element needs a new DOM node we tag the new fiber with the &lt;code&gt;PLACEMENT&lt;/code&gt; effect tag.&lt;/p&gt;
    &lt;p&gt;And for the case where we need to delete the node, we don‚Äôt have a new fiber so we add the effect tag to the old fiber.&lt;/p&gt;
    &lt;p&gt;But when we commit the fiber tree to the DOM we do it from the work in progress root, which doesn‚Äôt have the old fibers.&lt;/p&gt;
    &lt;p&gt;So we need an array to keep track of the nodes we want to remove.&lt;/p&gt;
    &lt;p&gt;And then, when we are commiting the changes to the DOM, we also use the fibers from that array.&lt;/p&gt;
    &lt;p&gt;Now, let‚Äôs change the &lt;code&gt;commitWork&lt;/code&gt; function to handle the new &lt;code&gt;effectTags&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the fiber has a &lt;code&gt;PLACEMENT&lt;/code&gt; effect tag we do the same as before, append the DOM node to the node from the parent fiber.&lt;/p&gt;
    &lt;p&gt;If it‚Äôs a &lt;code&gt;DELETION&lt;/code&gt;, we do the opposite, remove the child.&lt;/p&gt;
    &lt;p&gt;And if it‚Äôs an &lt;code&gt;UPDATE&lt;/code&gt;, we need to update the existing DOM node with the props that changed.&lt;/p&gt;
    &lt;p&gt;We‚Äôll do it in this &lt;code&gt;updateDom&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;We compare the props from the old fiber to the props of the new fiber, remove the props that are gone, and set the props that are new or changed.&lt;/p&gt;
    &lt;p&gt;One special kind of prop that we need to update are event listeners, so if the prop name starts with the ‚Äúon‚Äù prefix we‚Äôll handle them differently.&lt;/p&gt;
    &lt;p&gt;If the event handler changed we remove it from the node.&lt;/p&gt;
    &lt;p&gt;And then we add the new handler.&lt;/p&gt;
    &lt;p&gt;Try the version with reconciliation on codesandbox.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step VII: Function Components&lt;/head&gt;
    &lt;p&gt;The next thing we need to add is support for function components.&lt;/p&gt;
    &lt;p&gt;First let‚Äôs change the example. We‚Äôll use this simple function component, that returns an &lt;code&gt;h1&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Note that if we transform the jsx to js, it will be:&lt;/p&gt;
    &lt;quote&gt;function App(props) {return Didact.createElement("h1",null,"Hi ",props.name)}const element = Didact.createElement(App, {name: "foo",})&lt;/quote&gt;
    &lt;p&gt;Function components are differents in two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the fiber from a function component doesn‚Äôt have a DOM node&lt;/item&gt;
      &lt;item&gt;and the children come from running the function instead of getting them directly from the &lt;code&gt;props&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We check if the fiber type is a function, and depending on that we go to a different update function.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;updateHostComponent&lt;/code&gt; we do the same as before.&lt;/p&gt;
    &lt;p&gt;And in &lt;code&gt;updateFunctionComponent&lt;/code&gt; we run the function to get the children.&lt;/p&gt;
    &lt;p&gt;For our example, here the &lt;code&gt;fiber.type&lt;/code&gt; is the &lt;code&gt;App&lt;/code&gt; function and when we run it, it returns the &lt;code&gt;h1&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Then, once we have the children, the reconciliation works in the same way, we don‚Äôt need to change anything there.&lt;/p&gt;
    &lt;p&gt;What we need to change is the &lt;code&gt;commitWork&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;Now that we have fibers without DOM nodes we need to change two things.&lt;/p&gt;
    &lt;p&gt;First, to find the parent of a DOM node we‚Äôll need to go up the fiber tree until we find a fiber with a DOM node.&lt;/p&gt;
    &lt;p&gt;And when removing a node we also need to keep going until we find a child with a DOM node.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step VIII: Hooks&lt;/head&gt;
    &lt;p&gt;Last step. Now that we have function components let‚Äôs also add state.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs change our example to the classic counter component. Each time we click it, it increments the state by one.&lt;/p&gt;
    &lt;p&gt;Note that we are using &lt;code&gt;Didact.useState&lt;/code&gt; to get and update the counter value.&lt;/p&gt;
    &lt;p&gt;Here is where we call the &lt;code&gt;Counter&lt;/code&gt; function from the example. And inside that function we call &lt;code&gt;useState&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We need to initialize some global variables before calling the function component so we can use them inside of the &lt;code&gt;useState&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;First we set the work in progress fiber.&lt;/p&gt;
    &lt;p&gt;We also add a &lt;code&gt;hooks&lt;/code&gt; array to the fiber to support calling &lt;code&gt;useState&lt;/code&gt; several times in the same component. And we keep track of the current hook index.&lt;/p&gt;
    &lt;p&gt;When the function component calls &lt;code&gt;useState&lt;/code&gt;, we check if we have an old hook. We check in the &lt;code&gt;alternate&lt;/code&gt; of the fiber using the hook index.&lt;/p&gt;
    &lt;p&gt;If we have an old hook, we copy the state from the old hook to the new hook, if we don‚Äôt we initialize the state.&lt;/p&gt;
    &lt;p&gt;Then we add the new hook to the fiber, increment the hook index by one, and return the state.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;useState&lt;/code&gt; should also return a function to update the state, so we define a &lt;code&gt;setState&lt;/code&gt; function that receives an action (for the &lt;code&gt;Counter&lt;/code&gt; example this action is the function that increments the state by one).&lt;/p&gt;
    &lt;p&gt;We push that action to a queue we added to the hook.&lt;/p&gt;
    &lt;p&gt;And then we do something similar to what we did in the &lt;code&gt;render&lt;/code&gt; function, set a new work in progress root as the next unit of work so the work loop can start a new render phase.&lt;/p&gt;
    &lt;p&gt;But we haven‚Äôt run the action yet.&lt;/p&gt;
    &lt;p&gt;We do it the next time we are rendering the component, we get all the actions from the old hook queue, and then apply them one by one to the new hook state, so when we return the state it‚Äôs updated.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs all. We‚Äôve built our own version of React.&lt;/p&gt;
    &lt;p&gt;You can play with it on codesandbox or github.&lt;/p&gt;
    &lt;head rend="h3"&gt;Epilogue&lt;/head&gt;
    &lt;p&gt;Besides helping you understand how React works, one of the goals of this post is to make it easier for you to dive deeper in the React codebase. That‚Äôs why we used the same variable and function names almost everywhere.&lt;/p&gt;
    &lt;p&gt;For example, if you add a breakpoint in one of your function components in a real React app, the call stack should show you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;workLoop&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;performUnitOfWork&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;updateFunctionComponent&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We didn‚Äôt include a lot of React features and optimizations. For example, these are a few things that React does differently:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Didact, we are walking the whole tree during the render phase. React instead follows some hints and heuristics to skip entire sub-trees where nothing changed.&lt;/item&gt;
      &lt;item&gt;We are also walking the whole tree in the commit phase. React keeps a linked list with just the fibers that have effects and only visit those fibers.&lt;/item&gt;
      &lt;item&gt;Every time we build a new work in progress tree, we create new objects for each fiber. React recycles the fibers from the previous trees.&lt;/item&gt;
      &lt;item&gt;When Didact receives a new update during the render phase, it throws away the work in progress tree and starts again from the root. React tags each update with an expiration timestamp and uses it to decide which update has a higher priority.&lt;/item&gt;
      &lt;item&gt;And many more‚Ä¶&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are also a few features that you can add easily:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;use an object for the style prop&lt;/item&gt;
      &lt;item&gt;flatten children arrays&lt;/item&gt;
      &lt;item&gt;useEffect hook&lt;/item&gt;
      &lt;item&gt;reconciliation by key&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you add any of these or other features to Didact send a pull request to the GitHub repo, so others can see it.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;And if you want to comment, like or share this post you can use this tweet:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Had to build a new blog and some tools to be able to publish this post with the format I wanted. It took some time but it's finally ready!&lt;/p&gt;‚Äî Rodrigo Pombo (@pomber) November 13, 2019&lt;lb/&gt;üì¢ the updated DIY guide to build React from scratch&lt;lb/&gt;‚ú® https://t.co/RfGrl8ARYz pic.twitter.com/3kih0xLHIu&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46332526</guid><pubDate>Sat, 20 Dec 2025 00:16:13 +0000</pubDate></item><item><title>Android introduces $2-4 install fee and 10‚Äì20% cut for US external content links</title><link>https://support.google.com/googleplay/android-developer/answer/16470497?hl=en</link><description>&lt;doc fingerprint="bc03b91c74b7dbe3"&gt;
  &lt;main&gt;
    &lt;p&gt;The external content links program allows developers of Google Play-distributed apps to link users in the United States to external content, including to purchase in-app digital items or to download an app whose install and updates are not managed by Google Play. Additionally, developers can offer external links to purchase in-app digital items in lieu of or alongside Google Play Billing.&lt;/p&gt;
    &lt;p&gt;Developers must meet the eligibility and requirements set out below, and successfully complete their enrollment in this program prior to using external content links.&lt;/p&gt;
    &lt;head rend="h2"&gt;Availability&lt;/head&gt;
    &lt;p&gt;This program is available in the US in connection with the US District Court's order. Google reserves the right to modify or terminate this program, including as a result of any changes to or termination of the US District Court's order.&lt;/p&gt;
    &lt;head rend="h2"&gt;Requirements&lt;/head&gt;
    &lt;p&gt;Developers participating in this program must comply with the following requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enroll and get approval for your app(s) and any linked external apps in the external content links program as explained below.&lt;/item&gt;
      &lt;item&gt;Limit external content links to users in the US and its territories.&lt;/item&gt;
      &lt;item&gt;Ensure all Play Apps enrolled in this program and any linked external apps comply with Play Developer Policies, with the exception of policies that are not applicable to linked apps, such as the Payments policy.&lt;/item&gt;
      &lt;item&gt;If using Google Play Billing with this program, all users must be able to access Google Play Billing in a consistent and reliable manner.&lt;/item&gt;
      &lt;item&gt;Integrate with the external content links APIs, which surface an information screen, enable parental controls, and facilitate transaction reporting once required.&lt;/item&gt;
      &lt;item&gt;Provide customer support for users completing transactions or downloading apps outside of Play and provide a process to dispute unauthorized transactions.&lt;/item&gt;
      &lt;item&gt;Offer refund methods for users completing transactions outside of Play, unless the user was clearly informed that the transaction is non-refundable before completing the purchase.&lt;/item&gt;
      &lt;item&gt;All external content links must meet the destination requirements as outlined below.&lt;/item&gt;
      &lt;item&gt;Once required, pay Google the applicable fees for qualifying transactions or app installs that are concluded outside the app following the external content link as outlined below.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Destination requirements&lt;/head&gt;
    &lt;p&gt;All external content destinations must meet the following requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;External content links to purchase in-app offers can be shown on a browser, in a webview or another app store already installed on the user‚Äôs device.&lt;/item&gt;
      &lt;item&gt;For user safety, external content links to download apps can link to the download of apps on a browser or another app store already installed on the user‚Äôs device. The destination page cannot contain any downloads that have not been approved for the ECL program.&lt;/item&gt;
      &lt;item&gt;External content links must inform the user about the destination page and its purpose in the app before linking out.&lt;/item&gt;
      &lt;item&gt;External content URLs must not contain a user‚Äôs personally identifiable information without sufficient security or encryption techniques in order to protect the user‚Äôs data.&lt;/item&gt;
      &lt;item&gt;External content links must not redirect or mislead users to a different destination page than presented in your external link, or present other false or deceptive information.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition, to ensure a good user experience and keep Play‚Äôs users safe, any destination for links to download need to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Have the right to distribute the apps available at the destination.&lt;/item&gt;
      &lt;item&gt;Clearly and visibly publish details for the app that is to be downloaded. The app details must include at least app name, app icon, app description, app version number, app download size, and information about the app developer.&lt;/item&gt;
      &lt;item&gt;Only install an app with the user‚Äôs knowledge and at the explicit direction of the user for each app, through a clearly labeled UI component visible to the user.&lt;/item&gt;
      &lt;item&gt;Provide direct, publicly accessible customer support to end users through readily accessible communication channels.&lt;/item&gt;
      &lt;item&gt;Have user data policies for their users.&lt;/item&gt;
      &lt;item&gt;Be responsible for adhering to all applicable local regulations, laws and ordinances.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Play Service Fees&lt;/head&gt;
    &lt;p&gt;Like our standard service fees, the fees associated with the external content links program reflect the value provided by Android and Play and support our continued investments across Android and Play. The following fees apply when a user completes any transactions or any app installs within 24 hours of following an external content link:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In-app item purchases: 10% for auto-renewing subscriptions and 20% for other offers of in-app digital features and services. Transactions for the first $1M (USD) of total developer earnings annually will be charged at 10%.&lt;/item&gt;
      &lt;item&gt;App download event: A fixed fee (subject to periodic adjustments) per install based on the app category of the linked external app being installed. The linked app category must be declared as part of transaction reporting. &lt;list rend="ul"&gt;&lt;item&gt;Games: $3.65&lt;/item&gt;&lt;item&gt;Apps: $2.85&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Eligibility&lt;/head&gt;
    &lt;p&gt;In order to be eligible for this program, your app(s) must be a mobile or tablet app or game serving users in the United States and its territories.&lt;/p&gt;
    &lt;p&gt;Please note that eligibility and requirements are subject to change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Enroll your app in the external content links program&lt;/head&gt;
    &lt;p&gt;To enroll in the external content links program, you must complete the following steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Review the requirements on this page to determine if your app(s) meets all the eligibility criteria.&lt;/item&gt;
      &lt;item&gt;Complete the external content links declaration form and complete any onboarding steps required to enroll in the program through Google's support team.&lt;/item&gt;
      &lt;item&gt;Integrate the external links APIs in your app for external links prior to linking users out to purchase in-app digital items or to download an external app.&lt;/item&gt;
      &lt;item&gt;Enroll your app(s) that will be using external content links through Play Console on the External content links page (Settings &amp;gt; External content links).&lt;/item&gt;
      &lt;item&gt;If you are using external content links to link users to download an app, complete the following steps in Play Console: &lt;head rend="h2"&gt;a. Register and submit all versions of the linked external app(s)&lt;/head&gt;&lt;p&gt;i. Open Play Console and go to the Register external apps page (Settings &amp;gt; Register external apps).&lt;/p&gt;&lt;p&gt;ii. External apps submitted for registration are processed as soon as possible, subject to our standard review processes, and may result in review times of up to 7 days or longer in exceptional cases.&lt;/p&gt;&lt;p&gt;b. Declare all external content links to external app(s) in your Play app(s)&lt;/p&gt;&lt;p&gt;i. Open Play Console and go to the External apps declaration (Monitor and improve &amp;gt; Policy and programs &amp;gt; App Content).&lt;/p&gt;&lt;p&gt;ii. Input the package name of the external app. Note: only external apps that have been successfully registered will show up in the drop-down list. See this FAQ for more information.&lt;/p&gt;&lt;p&gt;iii. Provide all user facing landing page URLs for the external app downloads.&lt;/p&gt;&lt;p&gt;iv. Provide all download links that are being used to distribute the external app APK to users.&lt;/p&gt;&lt;head rend="h2"&gt;c. When your external app(s) are updated, submit all updated versions and APKs of the linked external app(s)&lt;/head&gt;&lt;p&gt;i. Open Play Console and go to the Register external apps page (Settings &amp;gt; Register external apps).&lt;/p&gt;&lt;p&gt;ii. Find the external app that is being updated and click Manage in the table.&lt;/p&gt;&lt;p&gt;iii. Ensure the external app is approved before linking users to the updated versions or APKs.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Once required, keep track of all transactions, including $0 transactions resulting from free trial purchases, and app installs completed through external content links for reporting through the external links APIs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any additional questions, you can contact our support team here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;p&gt;Can I utilize external content links for users in other geographies beyond the US?&lt;/p&gt;
    &lt;p&gt;Are game developers eligible for this program?&lt;/p&gt;
    &lt;p&gt;Are all developers required to enroll in the external content links program?&lt;/p&gt;
    &lt;p&gt;Is there a limit to the number of external content links I am allowed to have in my app?&lt;/p&gt;
    &lt;p&gt;What types of transactions are we required to report when using external content links to link users to purchase in-app digital items?&lt;/p&gt;
    &lt;p&gt;Why can‚Äôt I select the external app when declaring external content links in the Play Console?&lt;/p&gt;
    &lt;p&gt;Do I have to register the external apps that I link to?&lt;/p&gt;
    &lt;p&gt;Can I make external content links available for only some of my apps?&lt;/p&gt;
    &lt;p&gt;How can I notify Google of any changes to my app package enrollment selections?&lt;/p&gt;
    &lt;head rend="h2"&gt;What are the steps to integrate with the external links APIs?&lt;/head&gt;
    &lt;p&gt;If I am already participating in an alternative billing program, can I also participate in the external content links program?&lt;/p&gt;
    &lt;p&gt;Can I use Google Play's billing system alongside external content links?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46333734</guid><pubDate>Sat, 20 Dec 2025 05:00:55 +0000</pubDate></item><item><title>Data Bank ‚Äì Nuforc ‚Äì Latest UFO Sightings</title><link>https://nuforc.org/databank/</link><description>&lt;doc fingerprint="e493e20f46024721"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The National UFO Reporting Center Online Database&lt;/head&gt;
    &lt;p&gt;The NUFORC Databank is the largest independently collected set of UFO / UAP sighting reports available on the internet. For 25 years, it has been the go-to resource for researchers and the general public interested in following the most recent first hand UFO witness accounts, or delving into a deep archive of historical reports. The reports are freely available to the public to browse through the indexes below. For any other use, including commercial use, please refer to our Terms of Service at the bottom of the page.&lt;/p&gt;
    &lt;p&gt;NUFORC staff review each report, and grade them as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TIER 1: The most dramatic sightings ‚Äì strange structured craft or highly anomalous phenomena seen at close distance. Indicated by a bright yellow Open! Link.&lt;/item&gt;
      &lt;item&gt;TIER 2: Objects exhibiting unusual characteristics such as incredible speed, non-inertial turns, unexplainable propulsion, etc. Indicated by a faded yellow Open link.&lt;/item&gt;
      &lt;item&gt;TIER 3: Other reports that cannot be easily explained.&lt;/item&gt;
      &lt;item&gt;TIER 4: Reports we feel are explainable by human or natural phenomenon. Explanations are rated as Possible (indicated by ?), Probable or Certain.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Reports received prior to March 2023 have not yet been graded.&lt;/p&gt;
    &lt;p&gt;Access reports from any of the lists below:&lt;/p&gt;
    &lt;p&gt;TIER 1 REPORTS&lt;lb/&gt;PILOT REPORTS&lt;lb/&gt;LATEST INVESTIGATIONS&lt;lb/&gt;ALL REPORTS&lt;/p&gt;
    &lt;p&gt;Index by EVENT DATE&lt;lb/&gt;Index by STATE/COUNTRY&lt;lb/&gt;Index by SHAPE OF UFO&lt;lb/&gt;Index by DATE POSTED&lt;/p&gt;
    &lt;p&gt;We hope that this information will prove to be useful to the general public and the UFO community at large. We post reports on a periodic basis as they are processed through our database.&lt;/p&gt;
    &lt;p&gt;The National UFO Reporting Center makes no claims as to the validity of the information in any of these reports. Obvious hoaxes have been omitted, however most reports have been posted exactly as received in the author‚Äôs own wordO?&lt;/p&gt;
    &lt;head rend="h1"&gt;Audio Archive&lt;/head&gt;
    &lt;p&gt;In the early years of NUFORC, reports were primarily made over the telephone hotline, which was set up by NUFORC‚Äôs founder, Robert Gribble. The Northern Ontario UFO Research and Study organization maintains an online archive of calls from 1974-1977, which can be accessed here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46333959</guid><pubDate>Sat, 20 Dec 2025 06:00:31 +0000</pubDate></item><item><title>Charles Proxy</title><link>https://www.charlesproxy.com/</link><description>&lt;doc fingerprint="bd3f5e502db4d594"&gt;
  &lt;main&gt;
    &lt;p&gt;Charles is an HTTP proxy / HTTP monitor / Reverse Proxy that enables a developer to view all of the HTTP and SSL / HTTPS traffic between their machine and the Internet. This includes requests, responses and the HTTP headers (which contain the cookies and caching information).&lt;/p&gt;
    &lt;head rend="h3"&gt;Recent Developments&lt;/head&gt;
    &lt;p&gt;For discussion on the latest changes to Charles, please see Karl‚Äôs blog.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;20 Sep 2025&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Charles 5.0.3 released fixing a performance issue on macOS and minor improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;9 Aug 2025&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;Charles 5.0.2 released including bug fixes and minor improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;12 Mar 2025&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;Charles 5 released! Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;24 Jan 2024&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;Charles 5 public beta 13 is now available for testing, featuring more UI improvements particularly on Windows including dark mode support. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;7 Jul 2023&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;Charles 5 public beta 11 is now available for testing, featuring more UI improvements, performance improvements, new features and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;11 Jun 2023&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;Charles 5 public beta 9 is now available for testing, featuring more UI improvements and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;11 Apr 2023&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;Charles 5 public beta is now available for testing, featuring major UI improvements and technology upgrades. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;4 Apr 2023&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;Charles 4.6.4 released with macOS crash fixed and Windows code signing updated. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;26 Sep 2022&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;Charles 4.6.3 released with minor bug fixes and Java 11 update Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;14 Dec 2021&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;In light of the current log4j2 vulnerabilities, we confirm that no version of Charles shipped or used any version of log4j and Charles is therefore thankfully unaffected by this issue. Our best wishes to the log4j developers and everyone affected by this.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-6"&gt;6 Jul 2021&lt;/item&gt;
      &lt;item rend="dd-6"&gt;
        &lt;p&gt;Charles 4.6.2 released including bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-7"&gt;15 Nov 2020&lt;/item&gt;
      &lt;item rend="dd-7"&gt;
        &lt;p&gt;Charles 4.6.1 released to fix Dark Mode support on macOS Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-8"&gt;7 Nov 2020&lt;/item&gt;
      &lt;item rend="dd-8"&gt;
        &lt;p&gt;Charles 4.6 released including new features and stability improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-9"&gt;15 Jan 2020&lt;/item&gt;
      &lt;item rend="dd-9"&gt;
        &lt;p&gt;Charles 4.5.6 released with minor bug fixes and patched security vulnerability. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-10"&gt;5 Dec 2019&lt;/item&gt;
      &lt;item rend="dd-10"&gt;
        &lt;p&gt;Charles 4.5.5 released including bug fixes for SSL certificate imports. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-11"&gt;3 Nov 2019&lt;/item&gt;
      &lt;item rend="dd-11"&gt;
        &lt;p&gt;Charles 4.5.2 released including new features, bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-12"&gt;28 Feb 2019&lt;/item&gt;
      &lt;item rend="dd-12"&gt;
        &lt;p&gt;Charles 4.2.8 released with minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-13"&gt;14 Sep 2018&lt;/item&gt;
      &lt;item rend="dd-13"&gt;
        &lt;p&gt;Charles 4.2.7 released with minor bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-14"&gt;5 May 2018&lt;/item&gt;
      &lt;item rend="dd-14"&gt;
        &lt;p&gt;Charles Security Bulletin for a local privilege escalation in Charles 4.2 and 3.12.1 and earlier. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-15"&gt;7 Apr 2018&lt;/item&gt;
      &lt;item rend="dd-15"&gt;
        &lt;p&gt;Charles 4.2.5 released with major bug fixes and minor improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-16"&gt;28 Mar 2018&lt;/item&gt;
      &lt;item rend="dd-16"&gt;
        &lt;p&gt;Charles for iOS released. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-17"&gt;22 Nov 2017&lt;/item&gt;
      &lt;item rend="dd-17"&gt;
        &lt;p&gt;Charles 4.2.1 released with important bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-18"&gt;30 Sep 2017&lt;/item&gt;
      &lt;item rend="dd-18"&gt;
        &lt;p&gt;Charles 4.2 released with major new TLS debugging capability, minor improvements and bug fixes including macOS High Sierra support. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-19"&gt;10 Jul 2017&lt;/item&gt;
      &lt;item rend="dd-19"&gt;
        &lt;p&gt;Charles 4.1.4 released with minor improvements and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-20"&gt;20 Jun 2017&lt;/item&gt;
      &lt;item rend="dd-20"&gt;
        &lt;p&gt;Charles 4.1.3 released including Brotli compression support and other minor bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-21"&gt;13 May 2017&lt;/item&gt;
      &lt;item rend="dd-21"&gt;
        &lt;p&gt;Charles 4.1.2 released with bug fixes and minor improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-22"&gt;21 Apr 2017&lt;/item&gt;
      &lt;item rend="dd-22"&gt;
        &lt;p&gt;Charles 4.1.1 released with bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-23"&gt;10 Apr 2017&lt;/item&gt;
      &lt;item rend="dd-23"&gt;
        &lt;p&gt;Charles 4.1 released including major new features and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-24"&gt;19 Nov 2016&lt;/item&gt;
      &lt;item rend="dd-24"&gt;
        &lt;p&gt;Charles 4.0.2 released including bug fixes and minor improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-25"&gt;20 Sep 2016&lt;/item&gt;
      &lt;item rend="dd-25"&gt;
        &lt;p&gt;Charles 4.0.1 released including bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-26"&gt;16 Sep 2016&lt;/item&gt;
      &lt;item rend="dd-26"&gt;
        &lt;p&gt;Charles 3.11.6 released with support for macOS Sierra and minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-27"&gt;1 Aug 2016&lt;/item&gt;
      &lt;item rend="dd-27"&gt;
        &lt;p&gt;Charles 4 released featuring HTTP 2, IPv6 and improved look and feel. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-28"&gt;29 May 2016&lt;/item&gt;
      &lt;item rend="dd-28"&gt;
        &lt;p&gt;Charles 3.11.5 released including minor bug fixes; especially fixes SSL certificate installation on Android. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-29"&gt;29 Feb 2016&lt;/item&gt;
      &lt;item rend="dd-29"&gt;
        &lt;p&gt;Charles 3.11.4 released with support for ATS on iOS 9 and crash fixes for older versions of Mac OS X. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-30"&gt;15 Feb 2016&lt;/item&gt;
      &lt;item rend="dd-30"&gt;
        &lt;p&gt;Charles v3.11.3 released including bug fixes and minor improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-31"&gt;9 Nov 2015&lt;/item&gt;
      &lt;item rend="dd-31"&gt;
        &lt;p&gt;Charles v3.11.2 released with SSL and Websockets improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-32"&gt;4 Oct 2015&lt;/item&gt;
      &lt;item rend="dd-32"&gt;
        &lt;p&gt;Charles 3.11 released including major new features. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-33"&gt;7 Jul 2015&lt;/item&gt;
      &lt;item rend="dd-33"&gt;
        &lt;p&gt;Charles 3.10.2 released with bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-34"&gt;31 Mar 2015&lt;/item&gt;
      &lt;item rend="dd-34"&gt;
        &lt;p&gt;Charles 3.10.1 released with minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-35"&gt;21 Mar 2015&lt;/item&gt;
      &lt;item rend="dd-35"&gt;
        &lt;p&gt;Charles 3.10 released with improved SSL (new SSL CA certificate install required), major new features and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-36"&gt;22 Oct 2014&lt;/item&gt;
      &lt;item rend="dd-36"&gt;
        &lt;p&gt;Charles v3.9.3 released with improvements to SSL support, Mac OS X Yosemite support and other minor bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-37"&gt;26 May 2014&lt;/item&gt;
      &lt;item rend="dd-37"&gt;
        &lt;p&gt;Charles v3.9.2 released with minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-38"&gt;5 May 2014&lt;/item&gt;
      &lt;item rend="dd-38"&gt;
        &lt;p&gt;Charles 3.9.1 released with minor bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-39"&gt;25 Apr 2014&lt;/item&gt;
      &lt;item rend="dd-39"&gt;
        &lt;p&gt;Charles 3.9 released with major new features and bug fixes, including the ability to "focus" on hosts so they are separated from the noise. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-40"&gt;23 Oct 2013&lt;/item&gt;
      &lt;item rend="dd-40"&gt;
        &lt;p&gt;Charles 3.8.3 released with support for Mac OS X Mavericks and minor bug fixes. Happy Mavericks Day. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-41"&gt;21 Oct 2013&lt;/item&gt;
      &lt;item rend="dd-41"&gt;
        &lt;p&gt;Charles 3.8.2 released with minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-42"&gt;9 Sep 2013&lt;/item&gt;
      &lt;item rend="dd-42"&gt;
        &lt;p&gt;Charles 3.8.1 released with minor bug fixes and improvements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-43"&gt;4 Sep 2013&lt;/item&gt;
      &lt;item rend="dd-43"&gt;
        &lt;p&gt;Charles 3.8 has been released with new features and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-44"&gt;12 Feb 2013&lt;/item&gt;
      &lt;item rend="dd-44"&gt;
        &lt;p&gt;Charles 3.7 has been released. Includes new features, bundled Java runtime (so you don‚Äôt need to install Java anymore), and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-45"&gt;27 Jun 2012&lt;/item&gt;
      &lt;item rend="dd-45"&gt;
        &lt;p&gt;Charles 3.7 beta 2 has been released. This changes the SSL signing for Charles on Mac OS X to use Apple's new Developer ID code-signing. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-46"&gt;8 Dec 2011&lt;/item&gt;
      &lt;item rend="dd-46"&gt;
        &lt;p&gt;Charles v3.6.5 released including bug fixes and minor changes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-47"&gt;15 Nov 2011&lt;/item&gt;
      &lt;item rend="dd-47"&gt;
        &lt;p&gt;Charles v3.6.4 released including major bug fixes and enhancements. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-48"&gt;5 Sep 2011&lt;/item&gt;
      &lt;item rend="dd-48"&gt;
        &lt;p&gt;Charles v3.6.3 released including minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-49"&gt;24 Aug 2011&lt;/item&gt;
      &lt;item rend="dd-49"&gt;
        &lt;p&gt;Charles v3.6.1 released including minor enhancements and bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-50"&gt;18 Aug 2011&lt;/item&gt;
      &lt;item rend="dd-50"&gt;
        &lt;p&gt;Charles v3.6 released including new features, enhancements and bug fixes. New features include HAR and SAZ file import. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-51"&gt;17 Aug 2010&lt;/item&gt;
      &lt;item rend="dd-51"&gt;
        &lt;p&gt;Charles v3.5.2 released including bug fixes and minor new features. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-52"&gt;1 Jan 2010&lt;/item&gt;
      &lt;item rend="dd-52"&gt;
        &lt;p&gt;Charles 3.5.1 released. Minor bug fixes. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-53"&gt;23 Dec 2009&lt;/item&gt;
      &lt;item rend="dd-53"&gt;
        &lt;p&gt;Charles 3.5 released. Major new features, bug fixes and enhancements.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-54"&gt;17 Oct 2009&lt;/item&gt;
      &lt;item rend="dd-54"&gt;
        &lt;p&gt;Charles 3.4.1 released. Minor features and bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-55"&gt;27 Sep 2009&lt;/item&gt;
      &lt;item rend="dd-55"&gt;
        &lt;p&gt;Charles 3.4 released. Major changes especially to SSL.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-56"&gt;11 May 2009&lt;/item&gt;
      &lt;item rend="dd-56"&gt;
        &lt;p&gt;New website launched. Follow @charlesproxy on Twitter. Say hi in San Francisco when I'm there for WWDC!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-57"&gt;7 Mar 2009&lt;/item&gt;
      &lt;item rend="dd-57"&gt;
        &lt;p&gt;Charles 3.3.1 released. Minor new features and bug fixes. Experimental 64 bit Windows support. Read more.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-58"&gt;15 Feb 2009&lt;/item&gt;
      &lt;item rend="dd-58"&gt;
        &lt;p&gt;Charles 3.3 released. Major new features. Download&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-59"&gt;24 Sep 2008&lt;/item&gt;
      &lt;item rend="dd-59"&gt;
        &lt;p&gt;Charles Autoconfiguration add-on for Mozilla Firefox adds support for Firefox 3.1&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-60"&gt;23 Sep 2008&lt;/item&gt;
      &lt;item rend="dd-60"&gt;
        &lt;p&gt;Charles 3.2.3 released. Minor new features and bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-61"&gt;6 Sep 2008&lt;/item&gt;
      &lt;item rend="dd-61"&gt;
        &lt;p&gt;Charles 3.2.2 released. Minor new features and bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-62"&gt;17 Apr 2008&lt;/item&gt;
      &lt;item rend="dd-62"&gt;
        &lt;p&gt;Charles 3.2.1 released. Minor new features and bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-63"&gt;24 Mar 2008&lt;/item&gt;
      &lt;item rend="dd-63"&gt;
        &lt;p&gt;Charles 3.2 released. Major new features. Release Notes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-64"&gt;28 Jan 2008&lt;/item&gt;
      &lt;item rend="dd-64"&gt;
        &lt;p&gt;Charles 3.2 public beta released. Download and more information on my blog.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-65"&gt;19 Dec 2007&lt;/item&gt;
      &lt;item rend="dd-65"&gt;
        &lt;p&gt;Charles 3.1.4 released. Bug fixes and minor new features.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-66"&gt;21 Nov 2007&lt;/item&gt;
      &lt;item rend="dd-66"&gt;
        &lt;p&gt;Charles Mozilla Firefox add-on updated for compatibility with Firefox 3.0.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-67"&gt;12 Nov 2007&lt;/item&gt;
      &lt;item rend="dd-67"&gt;
        &lt;p&gt;Charles 3.1.3 released. Minor bug fixes, minor new features.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Chart tab now includes charts for sizes, durations and types&lt;/item&gt;
          &lt;item&gt;Request &amp;amp; Response can now be displayed combined on one split-panel&lt;/item&gt;
          &lt;item&gt;SSL handshake and certificate errors are now displayed in the tree&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-68"&gt;29 Aug 2007&lt;/item&gt;
      &lt;item rend="dd-68"&gt;
        &lt;p&gt;Charles 3.1.2 released. Minor bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-69"&gt;27 Aug 2007&lt;/item&gt;
      &lt;item rend="dd-69"&gt;
        &lt;p&gt;Charles 3.1.1 released. Minor bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-70"&gt;13 Aug 2007&lt;/item&gt;
      &lt;item rend="dd-70"&gt;
        &lt;p&gt;Charles 3.1 released.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-71"&gt;22 May 2007&lt;/item&gt;
      &lt;item rend="dd-71"&gt;
        &lt;p&gt;Charles 3.0.4 released. Fixes SSL bug on Java 1.4.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-72"&gt;14 May 2007&lt;/item&gt;
      &lt;item rend="dd-72"&gt;
        &lt;p&gt;Charles 3.0.3 re-released. Fixes launch bug on computers that haven't used Charles before.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-73"&gt;12 May 2007&lt;/item&gt;
      &lt;item rend="dd-73"&gt;
        &lt;p&gt;Charles 3.0.3 released. Various improvements and minor bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-74"&gt;23 Apr 2007&lt;/item&gt;
      &lt;item rend="dd-74"&gt;
        &lt;p&gt;Charles 3.0.2 released. Minor bug fixes and improvements.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-75"&gt;28 Mar 2007&lt;/item&gt;
      &lt;item rend="dd-75"&gt;
        &lt;p&gt;Charles 3.0.1 released. Minor bug fixes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-76"&gt;24 Mar 2007&lt;/item&gt;
      &lt;item rend="dd-76"&gt;
        &lt;p&gt;Charles 3.0 released. Major new features and improvements&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-77"&gt;7 Mar 2007&lt;/item&gt;
      &lt;item rend="dd-77"&gt;
        &lt;p&gt;Charles 3.0 public beta released.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-78"&gt;27 Feb 2007&lt;/item&gt;
      &lt;item rend="dd-78"&gt;
        &lt;p&gt;Charles v2.6.4 release. Minor bug fixes:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;IBM JDK compatibility&lt;/item&gt;
          &lt;item&gt;Improved malformed Referer header support&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-79"&gt;17 Feb 2007&lt;/item&gt;
      &lt;item rend="dd-79"&gt;
        &lt;p&gt;Charles v2.6.3 release. Minor bug fixes:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Fixed Port Forwarding fault introduced in v2.6.2&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-80"&gt;1 Feb 2007&lt;/item&gt;
      &lt;item rend="dd-80"&gt;
        &lt;p&gt;Charles v2.6.2 release. Major improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;No more recording limits. Large responses are now saved to temporary files, reducing memory usage.&lt;/item&gt;
          &lt;item&gt;MTU support in the throttle settings&lt;/item&gt;
          &lt;item&gt;AMF3 / Flex 2 bug fixes&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-81"&gt;2 Dec 2006&lt;/item&gt;
      &lt;item rend="dd-81"&gt;
        &lt;p&gt;Charles v2.6.1 release. Minor bug fixes and improvements:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;SOAP information visible while response is still loading&lt;/item&gt;
          &lt;item&gt;AMF3 externalizable object parsing regression fixed&lt;/item&gt;
          &lt;item&gt;AMF view for AMF3/Flex messages simplified to hide Flex implementation details&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-82"&gt;27 Nov 2006&lt;/item&gt;
      &lt;item rend="dd-82"&gt;
        &lt;p&gt;Charles v2.6 release. Major improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Major UI overhaul&lt;/item&gt;
          &lt;item&gt;JSON and JSON-RPC support&lt;/item&gt;
          &lt;item&gt;SOAP support&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-83"&gt;20 Sep 2006&lt;/item&gt;
      &lt;item rend="dd-83"&gt;
        &lt;p&gt;Charles v2.5 release. Major improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Major UI improvements&lt;/item&gt;
          &lt;item&gt;Support for new filetypes including FLV&lt;/item&gt;
          &lt;item&gt;Major improvements to AMF / Flash remoting viewer&lt;/item&gt;
          &lt;item&gt;Thank you to everyone who made suggestions and participated in the long testing process.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-84"&gt;1 Jun 2006&lt;/item&gt;
      &lt;item rend="dd-84"&gt;
        &lt;p&gt;Charles v2.4.2 release. Minor improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Support for request body compression (used by web services)&lt;/item&gt;
          &lt;item&gt;Fix for parsing of AMFPHP responses&lt;/item&gt;
          &lt;item&gt;Improvements to AMF viewer&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-85"&gt;6 May 2006&lt;/item&gt;
      &lt;item rend="dd-85"&gt;
        &lt;p&gt;Charles v2.4.1 release. Minor improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Firefox extension improved&lt;/item&gt;
          &lt;item&gt;AMF 0 and AMF 3 parsing improved&lt;/item&gt;
          &lt;item&gt;Look and Feel changes to give a greater (and more consistent) range of font sizes in the Charles look and feel&lt;/item&gt;
          &lt;item&gt;SSL error reporting improved when a connection cannot be made to a remote host&lt;/item&gt;
          &lt;item&gt;Port Forwarding tool and Reverse Proxy tool re-bind exception fixed&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-86"&gt;26 Apr 2006&lt;/item&gt;
      &lt;item rend="dd-86"&gt;
        &lt;p&gt;Charles v2.4 release. Major new features, improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;AMF 3 support&lt;/item&gt;
          &lt;item&gt;SSL support for IBM JDK (thanks to Lance Bader for helping solve this)&lt;/item&gt;
          &lt;item&gt;Automatic Update Checking&lt;/item&gt;
          &lt;item&gt;Documentation wiki open to public&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-87"&gt;25 Mar 2006&lt;/item&gt;
      &lt;item rend="dd-87"&gt;
        &lt;p&gt;Charles v2.3 release. Major improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Proxy implementation improvements including better handling of keep-alive connections&lt;/item&gt;
          &lt;item&gt;SOCKS proxy added, so any SOCKSified application can now run through Charles&lt;/item&gt;
          &lt;item&gt;External proxies configuration improvements including authentication&lt;/item&gt;
          &lt;item&gt;Flash Remoting / AMF viewer improvements&lt;/item&gt;
          &lt;item&gt;Dynamic proxy port support, for multiuser systems&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-88"&gt;5 Nov 2005&lt;/item&gt;
      &lt;item rend="dd-88"&gt;
        &lt;p&gt;Charles v2.2.1 release. Minor improvements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Further improved Firefox proxy configuration&lt;/item&gt;
          &lt;item&gt;Port Forwarding enhancements including port ranges and UDP forwarding&lt;/item&gt;
          &lt;item&gt;Bug fixes for Reverse Proxy and AMF viewer&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-89"&gt;5 Oct 2005&lt;/item&gt;
      &lt;item rend="dd-89"&gt;
        &lt;p&gt;Charles v2.2 released. Major enhancements and bug fixes including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Improved Firefox proxy configuration&lt;/item&gt;
          &lt;item&gt;XML viewer improvements&lt;/item&gt;
          &lt;item&gt;Line numbers displayed in ASCII viewer&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-90"&gt;2 Sep 2005&lt;/item&gt;
      &lt;item rend="dd-90"&gt;
        &lt;p&gt;Charles v2.1 released. Major new features and enhancements including:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Automatic Firefox proxy configuration&lt;/item&gt;
          &lt;item&gt;Formatted form posts and query string information&lt;/item&gt;
          &lt;item&gt;Parsing of SWF and AMF (Flash Remoting) binary formats&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-91"&gt;18 Jun 2005&lt;/item&gt;
      &lt;item rend="dd-91"&gt;
        &lt;p&gt;Charles v2.0 released. Major enhancements and improvements.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Feedback &amp;amp; Reviews&lt;/head&gt;
    &lt;p&gt;Better Mobile Application Testing with Charles Proxy&lt;lb/&gt; by Andrew Bardallis A comprehensive walkthrough of using Charles to observe and modify traffic, including using it with mobile devices. &lt;/p&gt;
    &lt;p&gt;Monitor and Debug with Charles Proxy&lt;lb/&gt; by Tobias Sj√∂sten &lt;/p&gt;
    &lt;p&gt;iPhone App Store data mining&lt;lb/&gt; by Dan Grigsby Using Charles to explore the iPhone App Store XML. &lt;/p&gt;
    &lt;p&gt;iPhone HTTP Connection Debugging&lt;lb/&gt; by Gary Rogers Using Charles to debug the iPhone. &lt;/p&gt;
    &lt;p&gt;I Love Charles...&lt;lb/&gt; by MadeByPi &lt;/p&gt;
    &lt;p&gt;Basic use of Charles in Flex Design&lt;lb/&gt; by Frankie Loscavio &lt;/p&gt;
    &lt;p&gt;Charles review on flashgroup.net&lt;lb/&gt; by Darren Richardson A great review of Charles from the point of view of Flash developers. &lt;/p&gt;
    &lt;p&gt;Debugging Flash/Server Interaction with Charles&lt;lb/&gt; by uberGeek Using Charles to find those really annoying Flash bugs in record time. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46333983</guid><pubDate>Sat, 20 Dec 2025 06:09:17 +0000</pubDate></item><item><title>Privacy doesn't mean anything anymore, anonymity does</title><link>https://servury.com/blog/privacy-is-marketing-anonymity-is-architecture/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46334025</guid><pubDate>Sat, 20 Dec 2025 06:21:05 +0000</pubDate></item></channel></rss>