<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 24 Sep 2025 04:12:03 +0000</lastBuildDate><item><title>Shopify, pulling strings at Ruby Central, forces Bundler and RubyGems takeover</title><link>https://joel.drapper.me/p/rubygems-takeover/</link><description>&lt;doc fingerprint="af53309c118bf6fe"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Shopify, pulling strings at Ruby Central, forces Bundler and RubyGems takeover&lt;/head&gt;
    &lt;p&gt;Ruby Central recently took over a collection of open source projects from their maintainers without their consent. News of the takeover was first broken by Ellen on 19 September.&lt;/p&gt;
    &lt;p&gt;I have spoken to about a dozen people directly involved in the events, and seen a recording of a key meeting between Ruby Gems maintainers and Ruby Central, to uncover what went on.&lt;/p&gt;
    &lt;p&gt;Here’s a quick summary of what I know:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Ruby Central was struggling for money.&lt;/item&gt;
      &lt;item&gt;Sidekiq withdrew its $250,000/year sponsorship for Ruby Central because they platformed DHH at RailsConf 2025.&lt;/item&gt;
      &lt;item&gt;Shopify demanded that Ruby Central take full control of the RubyGems GitHub repositories and the &lt;code&gt;bundler&lt;/code&gt;and&lt;code&gt;rubygems-update&lt;/code&gt;gems, threatening to withdraw funding if Ruby Central did not comply.&lt;/item&gt;
      &lt;item&gt;HSBT jumped the gun and implemented the takeover plan adding Marty Haught as an owner and reducing maintainers permissions before Marty had discussed this with the maintainers.&lt;/item&gt;
      &lt;item&gt;Marty met with the maintainers after their access was temporarily restored.&lt;/item&gt;
      &lt;item&gt;Marty (and by extension, Ruby Central) understood that Ruby Central did not have the right to take over these GitHub repositories or gems from their long established community maintainers.&lt;/item&gt;
      &lt;item&gt;Marty presented alternatives such as making a fork of the relevant RubyGems projects and warned Ruby Central of the consequences of doing the takeover.&lt;/item&gt;
      &lt;item&gt;The board voted to execute the takeover anyway and Marty executed it immediately.&lt;/item&gt;
      &lt;item&gt;A number of board members subsequently misrepresented the takeover to the Ruby community on social media.&lt;/item&gt;
      &lt;item&gt;This was premeditated. Shopify had organised an on-call rotation to take over from the previous maintainers, some of which at the time were also operating the RubyGems Service.&lt;/item&gt;
      &lt;item&gt;Shopify specifically demanded that at least one of the RubyGems maintainers, André Arko, be excluded from returning to the project. André has been working on RubyGems for over a decade and was also one of the founders of Ruby Together, an organization that merged with Ruby Central.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Takeover&lt;/head&gt;
    &lt;p&gt;On 9 September, HSBT (Hiroshi Shibata) — a member of Ruby core and maintainer of RubyGems — renamed the RubyGems GitHub enterprise to “Ruby Central”, added a new owner, Marty Haught, and downgraded the permissions of several other maintainers.&lt;/p&gt;
    &lt;p&gt;According to one of the maintainers, when HSBT was challenged, he refused to revert these changes claiming he needed permission from Marty. On 15 September, Marty said the changes were a mistake and HSBT reverted some of the changes. However, Marty was not removed as an owner, even though the other maintainers never agreed to him being added.&lt;/p&gt;
    &lt;p&gt;On 17 September, RubyGems maintainers met with Marty on Zoom.&lt;/p&gt;
    &lt;p&gt;Marty explained he’s been working on “operational planning” for the RubyGems Service. He was putting together a new Operator Agreement that all the operators of the RubyGems Service would need to sign.&lt;/p&gt;
    &lt;p&gt;He also mentioned that it had been identified as a risk that there were external individuals with ownership permissions over repositories that are necessary for running the RubyGems Service. He said HSBT prematurely changed the ownership permissions before the operational plan was complete.&lt;/p&gt;
    &lt;p&gt;During the discussion, the maintainers clarified with Marty the distinction between the RubyGems source code and the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;RubyGems is a collection of community owned, community maintained repositories of code that are held in commons for everyone in the Ruby community to use.&lt;/p&gt;
    &lt;p&gt;The RubyGems Service is entirely separate from that. It’s a specific deployment: a domain name and servers that happen to be running RubyGems source code. It is operated by Ruby Central.&lt;/p&gt;
    &lt;p&gt;This distinction is important. Anyone else could run the RubyGems source code on their own servers with their own domains. And Ruby Central could decide to run different source code on its servers — whether that be a fork of the RubyGems source code or otherwise.&lt;/p&gt;
    &lt;p&gt;The RubyGems maintainers have been developing this software for decades, predating Ruby Central’s operation of the RubyGems Service. Their contributions represent countless hours of unpaid work, establishing a clear history of community ownership and stewardship.&lt;/p&gt;
    &lt;p&gt;Ruby Central did contribute financially towards RubyGems maintenance, but these contributions did not confer ownership. Ruby Central’s funding of RubyGems development is no different than if they had contributed to the development of Rails, RSpec, or any other open source project. In no case would such funding grant them ownership rights over the project itself.&lt;/p&gt;
    &lt;p&gt;Similarly, Ruby Central’s employment of some RubyGems maintainers to operate the RubyGems Service does not transfer ownership of the separate open source projects.&lt;/p&gt;
    &lt;p&gt;Having personally reviewed a recording of this meeting, I have no doubt that Marty understood this distinction. The RubyGems source code and GitHub organisation was not owned by Ruby Central, even though Ruby Central operated a service with the same name.&lt;/p&gt;
    &lt;p&gt;On 18 September, the team started losing access again. This time they were removed from the GitHub organisation, their &lt;code&gt;rubygems.org&lt;/code&gt; email accounts were disabled and they were removed as owners of the &lt;code&gt;bundler&lt;/code&gt; and &lt;code&gt;rubygems-update&lt;/code&gt; gems. One maintainer, André Arko, was on-call for the RubyGems Service at the time when his access to GitHub and Fastly was revoked.&lt;/p&gt;
    &lt;p&gt;The Ruby Central board had voted for Ruby Central to take control of the RubyGems GitHub repositories and gems. And since Marty was now an owner, he was able to execute this order.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ruby Central becomes mostly dependent on Shopify&lt;/head&gt;
    &lt;p&gt;When Ruby Central decided to platform DHH at the final RailsConf, they lost $250,000 USD of annual sponsorship from Sidekiq, and this I understand left them almost entirely dependent on Shopify.&lt;/p&gt;
    &lt;p&gt;An anonymous source told me that during Rails World, members of Ruby Central, Ruby Core, Rails Core and representatives from major companies (Shopify, GitHub) discussed possible funding options.&lt;/p&gt;
    &lt;p&gt;According to this source, Ruby Central was presented with a proposal for long-term support, but this would only happen if certain RubyGems maintainers were removed.&lt;/p&gt;
    &lt;p&gt;Another source has confirmed to me that a meeting between Rails Foundation and Ruby Central did take place at Rails World, however they were not able to verify the agenda or who was in attendance.&lt;/p&gt;
    &lt;p&gt;I do know that the Rails World conference was attended by HSBT, DHH, Shan Cureton, Marty Haught, Ufuk Kayserilioglu and Rafael França.&lt;/p&gt;
    &lt;p&gt;I also know that Shopify specifically put immense financial pressure on Ruby Central to take full control of the RubyGems GitHub organisation and Ruby gems.&lt;/p&gt;
    &lt;p&gt;Freedom Dumlao, a Ruby Central board member, described the board vote saying “if I had voted the other way, I felt I’d be voting to start the process of shutting down Ruby Central”.&lt;/p&gt;
    &lt;p&gt;A source familiar with the events told me that Shopify’s pressure was both carrot and stick. Essentially, do what we ask and we’ll reward you with more funding, long-term financial stability. Don’t do this and you’ll never see a dollar of enterprise money again.&lt;/p&gt;
    &lt;p&gt;This to me strongly suggests that other companies were involved, perhaps through the Rails Foundation. But I have not been able to confirm anything beyond Shopify’s involvement.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Vote&lt;/head&gt;
    &lt;p&gt;According to a source familiar with the events, the Ruby Central board was made aware by Marty of the risks and damage this takeover would likely do to the community. Apparently he also highlighted other options besides the takeover, such as forking some of the projects.&lt;/p&gt;
    &lt;p&gt;Despite this, the board voted in favour of carrying out the takeover and Marty executed it immediately with his new owner privileges.&lt;/p&gt;
    &lt;p&gt;Shopify had given Ruby Central a hard deadline and it seems that Ruby Central only capitulated at the last moment.&lt;/p&gt;
    &lt;p&gt;I don’t know if the timing was intentional, but this takeover happened on the second day of the EuRuKo conference in Europe, which meant many outspoken European Rubyists were distracted at the time.&lt;/p&gt;
    &lt;p&gt;Because this takeover meant locking out most of the RubyGems Service operators including André who was on-call at the time, Shopify had contributed engineers to a new on-call rotation ready to spring into action after the takeover.&lt;/p&gt;
    &lt;p&gt;Shopify developers had been warming up with their first commits in six years coming in at the same time as the takeover.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Response&lt;/head&gt;
    &lt;p&gt;About six hours after Ellen broke the news, Ruby Central published their response: Strengthening the Stewardship of RubyGems and Bundler.&lt;/p&gt;
    &lt;p&gt;A post that feels like AI-generated corporate speak and bears no signature from anyone at Ruby Central willing to take responsibility.&lt;/p&gt;
    &lt;p&gt;The response says, “To strengthen supply chain security, we are taking important steps to ensure that administrative access to the RubyGems.org, RubyGems, and Bundler is securely managed. This includes both our production systems and GitHub repositories. In the near term we will temporarily hold administrative access to these projects while we finalize new policies that limit commit and organization access rights. This decision was made and approved by the Ruby Central Board as part of our fiduciary responsibility.”&lt;/p&gt;
    &lt;p&gt;But while Ruby Central has the right to lock down the RubyGems Service infrastructure, it never owned the RubyGems GitHub repositories.&lt;/p&gt;
    &lt;p&gt;DHH ignored Ellen’s post but instead retweeted the Ruby Central announcement with the caption “Ruby Central is making the right moves to ensure the Ruby supply chain is beyond reproach both technically and organisationally.”&lt;/p&gt;
    &lt;p&gt;A position that seems to stand in stark contrast to his other opinions. For example, he criticised Apple’s control of the App Store and takes the ownership of his own open source projects seriously.&lt;/p&gt;
    &lt;p&gt;When the Advanced Custom Fields plugin was stolen by WordPress, DHH said “This is totally crazy. Like if the operators of rubygems dot org just decided to expropriate the official Rails gems, hand over control to a new team, and lock the core team out of it. We’re in uncharted and dangerous territory for open source now. What a sad sight.”&lt;/p&gt;
    &lt;p&gt;Ruby Central board member and Shopify employee Ufuk Kayserilioglu misrepresented what happened, responding to Bluesky threads. For example he said, “Ruby Central has been running the rubygems.org system for years now, so this can hardly be considered a supply chain attack. On the contrary, we have a legal obligation to all the users of the system to keep it safe and secure.”&lt;/p&gt;
    &lt;p&gt;But no one accused Ruby Central of taking over the RubyGems Service and the takeover of the RubyGems GitHub organization and gems was not required to meet Ruby Central’s legal obligations. Remember, Ruby Central was in full control of what source code it deployed to the RubyGems Service which it operated.&lt;/p&gt;
    &lt;p&gt;He also said “How is limiting access to critical and shared infra &amp;amp; code a supply chain attack?” once again conflating the RubyGems source code with the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;On 21 September, Freedom Dumlao published A board member’s perspective of the RubyGems controversy in which he claimed “Ruby Central has been responsible for RubyGems and Bundler for a long time. This isn’t a new development, and I’m honestly very confused about the confusion.”&lt;/p&gt;
    &lt;p&gt;This is a misrepresentation of the real situation where Ruby Central was responsible for operating the RubyGems Service but did not own the RubyGems source code, repositories or gems.&lt;/p&gt;
    &lt;p&gt;He goes on to talk about supply chain attacks, which I admit is a convenient cover, but I don’t believe is the genuine reason for the takeover.&lt;/p&gt;
    &lt;p&gt;He then confirms that a deadline loomed. “Either Ruby Central puts controls in place to ensure the safety and stability of the infrastructure we are responsible for, or lose the funding that we use to keep those things online and going. With less than 24 hours to go, we were still working on this. Conversations with some maintainers were still happening as far as I know but the cooperation we were hoping for was not emerging.”&lt;/p&gt;
    &lt;p&gt;He doesn’t mention Shopify, but based on my other sources, I know it was Shopify that applied this pressure.&lt;/p&gt;
    &lt;p&gt;“It was clear that we weren’t quite ready yet, but in the end we were out of time. A vote had to be cast so we could ensure we did not lose funding necessary to operate RubyGems. What I voted for, was to direct Marty, Ruby Central’s Director of Open Source, to temporarily remove access and lock down the systems, get operator agreements in place with maintainers, and then re-enable access to those folks who needed and wanted it. Marty did exactly what the board asked of him.”&lt;/p&gt;
    &lt;p&gt;This again highlights the pressure Shopify put on Ruby Central.&lt;/p&gt;
    &lt;p&gt;Two sources directly involved told me that access specifically would not be re-enabled for André who had been singled out. Sources have also suggested that Shopify had been pressuring Ruby Central to end their relationship with André and remove him from the RubyGems project for some period of time prior to this taking place.&lt;/p&gt;
    &lt;p&gt;On 23 September, Ruby Central shared a video address by Shan Cureton (Executive Director, Ruby Central) on behalf of Ruby Central’s board and team.&lt;/p&gt;
    &lt;p&gt;In it she claims that Bundler and RubyGems came under Ruby Central’s responsibility through the merger with Ruby Together. But Ruby Together never owned Bundler or RubyGems.&lt;/p&gt;
    &lt;p&gt;She mentioned the departure of a “lead maintainer” [André] and transition of security engineer [Samuel Giddins] as raising questions around access to RubyGems, Bundler and the RubyGems Service.&lt;/p&gt;
    &lt;p&gt;She says sponsors (plural) and companies who depend on Ruby tooling came to them with supply chain concerns. She explained that they couldn’t reach agreement with existing maintainers in the timelines they were facing.&lt;/p&gt;
    &lt;p&gt;I have seen the meeting with the maintainers and can tell you the conversation was primarily about ownership, not security. None of the maintainers had a problem with Ruby Central restricting access to the RubyGems Service that it operated.&lt;/p&gt;
    &lt;p&gt;They had a problem with Ruby Central taking control of the RubyGems open source code repositories and gems, which Ruby Central never owned.&lt;/p&gt;
    &lt;p&gt;She explains that the board voted to remove administrative and commit privileges until agreements could be put in place. She said it was never meant to be permanent.&lt;/p&gt;
    &lt;p&gt;She said “this is not a shutdown of community contribution and it’s not permanent”. However, my sources tell me this will be permanent for at least André and likely Samuel.&lt;/p&gt;
    &lt;p&gt;She said on-call coverage remains in place. We know that André was on-call when his access was revoked, so she must be talking about the new on-call rotation which Shopify contributed to.&lt;/p&gt;
    &lt;p&gt;She said “all of these changes are being made in good faith.” But we know that these changes were made at Shopify’s request to take control of the RubyGems projects and specifically to exclude André (and likely Samuel too).&lt;/p&gt;
    &lt;p&gt;She also talked about two new agreements: Operator Agreements cover access to production systems for on-call and maintenance responsibilities. Contributor Agreements cover access to Bundler and RubyGem code repositories, covering both paid and volunteer maintainers.&lt;/p&gt;
    &lt;p&gt;The Operator Agreements make sense, but it is not Ruby Central’s place to run the RubyGems projects including Bundler and the RubyGems.org source code, which are community owned as explained previously.&lt;/p&gt;
    &lt;p&gt;She said, “in most open source projects where the code is a library or framework, you usually don’t see formal operator agreements. People contribute under contributor license agreements, codes of conduct or decisions made by a steering committee. But RubyGems.org is different. It’s not just code, it’s a production service. It runs critical infrastructure for the Ruby ecosystem, processes billions of downloads, stores sensitive metadata and is relied on by companies that have compliance requirements. Because it’s a service, Ruby Central carries the legal liability, the financial exposure and the operational risk. This is why Operator Agreements are necessary. They ensure access is tied to responsibility and accountability.”&lt;/p&gt;
    &lt;p&gt;Here she conflates RubyGems.org (the source code) with the RubyGems Service operated by Ruby Central and running on the domain name &lt;code&gt;rubygems.org&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Claiming that Ruby Central owns the RubyGems.org repository because it operates a service that uses the source code is like claiming you own Rails because you have a Rails app and sponsored someone who contributed a PR to the project.&lt;/p&gt;
    &lt;p&gt;It’s confusing because of how the projects are named, and Ruby Central are taking advantage of that confusion.&lt;/p&gt;
    &lt;p&gt;The reality is Ruby Central never owned the Ruby Gems source code. They could only take it because Marty was added by HSBT without the consent of other maintainers.&lt;/p&gt;
    &lt;head rend="h2"&gt;RV&lt;/head&gt;
    &lt;p&gt;An important piece of context is that André and Samuel started a new cooperative with Kasper Timm Hansen and Sam Stephenson called Spinel.&lt;/p&gt;
    &lt;p&gt;Spinel is developing a new Ruby management tool called &lt;code&gt;rv&lt;/code&gt;. It was introduced on 25 August 2025, right before Rails World.&lt;/p&gt;
    &lt;p&gt;In his blog post, André says, “For the last ten years or so of working on Bundler, I’ve had a wish rattling around: I want a better dependency manager. It doesn’t just manage your gems, it manages your ruby versions, too. It doesn’t just manage your ruby versions, it installs pre-compiled rubies so you don’t have to wait for ruby to compile from source every time. And more than all of that, it makes it completely trivial to run any script or tool written in ruby, even if that script or tool needs a different ruby than your application does.”&lt;/p&gt;
    &lt;p&gt;Bluesky threads reveal that Rafael França (Shopify / Rails Core) saw this tool as a threat, saying “some of the “admins” even announced publicly many days ago they were launching a competitor tool [rv] and were funding raising for it. I’d not trust the system to such “admin”.”&lt;/p&gt;
    &lt;p&gt;He also quoted the &lt;code&gt;rv&lt;/code&gt; README which says, “Get rid of rvm, rbenv, chruby, asdf, mise, ruby-build, ruby-install, bundler, and rubygems, all at once”, adding the caption “I’m not so sure I trust them to not sabotage rubygems or bundler.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What I don’t know&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I don’t know how each member voted or exactly how the information was presented to the board. I was hoping that someone would leak it to me, but so far that has not happened.&lt;/item&gt;
      &lt;item&gt;I don’t know if other groups or companies were involved, though circumstantial evidence and hearsay seems to point to this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have any information you can provide, please contact me on Signal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;It is not clear that Ruby Central’s plans include returning control of the RubyGems codebases to their original owners.&lt;/p&gt;
    &lt;p&gt;I am concerned that Ruby Central seems to be vulnerable to coercion by Shopify.&lt;/p&gt;
    &lt;p&gt;I am concerned that Ruby Central’s board with full knowledge of the consequences and the alternatives voted to take over a collection of open source projects from their maintainers without consent. Especially when these maintainers were acting in good faith at the time. This is the organisation we are meant to trust to host our Ruby gems.&lt;/p&gt;
    &lt;p&gt;I am concerned that Rails Core seems to consider &lt;code&gt;rv&lt;/code&gt; a “threat” rather than an exciting development, and I wonder if the “threat” is more Spinel than &lt;code&gt;rv&lt;/code&gt;. It seems likely that Spinel would be less susceptible to enterprise coercion and could offer a genuine alternative to RubyCentral’s RubyGems Service.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclosure&lt;/head&gt;
    &lt;p&gt;I was employed by Shopify between 2017 and 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimer&lt;/head&gt;
    &lt;p&gt;I have put this story together to the best of my ability based on hours of conversations with many different people involved. But I am not a professional journalist and I may have missed something. If I have made a mistake, please let me know.&lt;/p&gt;
    &lt;p&gt;I am willing to talk to anyone involved to make sure the community has a fair and honest understanding of the events that took place.&lt;/p&gt;
    &lt;head rend="h3"&gt;Changelog&lt;/head&gt;
    &lt;p&gt;23 September 2025&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Removed a couple of names listed as being in attendance at Rails World. On reflection I did not feel they were relevant.&lt;/item&gt;
      &lt;item&gt;Added a quote from DHH about WordPress.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348390</guid><pubDate>Tue, 23 Sep 2025 15:25:29 +0000</pubDate></item><item><title>Always Invite Anna</title><link>https://sharif.io/anna-alexei</link><description>&lt;doc fingerprint="fedd98054dfc58ec"&gt;
  &lt;main&gt;
    &lt;p&gt;I was lucky enough to make a few friends my first semester of college. We ended up hanging out quite a bit during those early months.&lt;/p&gt;
    &lt;p&gt;We’d all get excited for the weekends because Friday nights meant going out to party. Everyone except for Anna, that is.&lt;/p&gt;
    &lt;p&gt;Anna was quiet, shy, and a definitely a goody-two-shoes. She was from Alabama and spoke with a pronounced southern drawl I’d rarely heard in Maryland. She was reserved but friendly once you got to know her. Anna cared about school a lot. She was almost always studying whenever I saw her.&lt;/p&gt;
    &lt;p&gt;Every Friday night we’d make plans to go out together and party. But Anna would always refuse to come. She’d say something along the lines of “I have to study” or “I just don’t feel like it tonight.”&lt;/p&gt;
    &lt;p&gt;Eventually, we stopped inviting Anna out. Everyone except Alexei.&lt;/p&gt;
    &lt;p&gt;I liked Alexei the most in our friend group. He was valedictorian of his high school, played tennis at a competitive level, and was remarkably smart. If anyone deserved to have an ego, it was Alexei. Yet somehow he managed to be the kindest person I’d ever known. But my absolute favorite thing about Alexei was that he always invited Anna to come party with us.&lt;/p&gt;
    &lt;p&gt;One Friday night as we were all about to leave the dorms for a house party, Alexei stopped us. “Hold on, let’s invite Anna.” We headed over to her dorm and invited her to come with us. She said “Sorry, I have to study for my Arabic exam next week, but you guys have fun.”&lt;/p&gt;
    &lt;p&gt;Alexei continued to invite Anna every time we went out for the rest of the semester. And Anna said no every single time.&lt;/p&gt;
    &lt;p&gt;Curious about his persistence, I asked him “Why do you keep inviting Anna out when she’ll just say no?”&lt;/p&gt;
    &lt;p&gt;I’ll never forget what he told me: “I know she’s always going to say no, but that’s not the point. I invite her out so she’ll always feel included in the group.”&lt;/p&gt;
    &lt;p&gt;After that first semester, the friend group disbanded and we all went our separate ways. Many years later I ran into Anna and we ended up catching up. She told me how difficult her first semester of college had been. She was very close with her mom and sister and missed them them terribly.&lt;/p&gt;
    &lt;p&gt;But then she said something that stayed with me: She was grateful. She was grateful to be part of that brief friend group because she felt like she had a family away from home. And that even though she never partied with us, she always felt included because we would stop by her room and invite her anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348495</guid><pubDate>Tue, 23 Sep 2025 15:33:23 +0000</pubDate></item><item><title>Find SF parking cops</title><link>https://walzr.com/sf-parking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45350690</guid><pubDate>Tue, 23 Sep 2025 18:06:07 +0000</pubDate></item><item><title>How to draw construction equipment for kids</title><link>https://alyssarosenberg.substack.com/p/how-to-draw-construction-equipment</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45351410</guid><pubDate>Tue, 23 Sep 2025 19:09:50 +0000</pubDate></item><item><title>Apple A19 SoC die shot</title><link>https://chipwise.tech/our-portfolio/apple-a19-dieshot/</link><description>&lt;doc fingerprint="229e687d1b3cd1b2"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Apple A19 SoC die shot&lt;/head&gt;
    &lt;p&gt;These images represent the first high-resolution microscopy of Apple’s A19 chip, extracted from the iPhone 17, revealing its full complexity under the hood. Built on TSMC’s third-generation 3 nm process node—dubbed N3P—the A19 marks a refinement over the earlier N3E technology used in the A18 series, offering higher transistor density, better energy efficiency, and modest performance gains. On the CPU side, the chip retains a hybrid core design (performance plus efficiency cores), while upgrades to the GPU include more cores on the Pro models. Key supporting blocks—image signal processor, display engine, Neural Engine—also see enhancements, enabling better on-device AI, imaging, and power management. Taken together, the die shots not only visualize the physical layout—logic blocks, cache banks, interconnects—but also reflect Apple’s continuous push in process technology and architectural refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;High Resolution Floorplan images available here&lt;/head&gt;
    &lt;p&gt;+31537113618&lt;/p&gt;
    &lt;p&gt;info@chipwise.tech&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45351437</guid><pubDate>Tue, 23 Sep 2025 19:12:08 +0000</pubDate></item><item><title>Is Fortran better than Python for teaching basics of numerical linear algebra?</title><link>https://loiseaujc.github.io/posts/blog-title/fortran_vs_python.html</link><description>&lt;doc fingerprint="4718593732e41c11"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Fortran better than Python for teaching the basics of numerical linear algebra?&lt;/head&gt;
    &lt;p&gt;Disclaimer – This is not a post about which language is the most elegant or which implementation is the fastest (we all know it’s &lt;code&gt;Fortran&lt;/code&gt;). It’s about teaching the basics of scientific computing to engineering students with a limited programming experience. Yes, the &lt;code&gt;Numpy&lt;/code&gt;/&lt;code&gt;Scipy&lt;/code&gt;/&lt;code&gt;matplotlib&lt;/code&gt; stack is awesome. Yes, you can use &lt;code&gt;numba&lt;/code&gt; or &lt;code&gt;jax&lt;/code&gt; to speed up your code, or &lt;code&gt;Cython&lt;/code&gt;, or even &lt;code&gt;Mojo&lt;/code&gt; the latest kid in the block. Or you know what? Use &lt;code&gt;Julia&lt;/code&gt; or &lt;code&gt;Rust&lt;/code&gt; instead. But that’s not the basics and it’s beyond the point.&lt;/p&gt;
    &lt;p&gt;I’ve been teaching an Intro to Scientific Computing class for nearly 10+ years. This class is intended for second year engineering students and, as such, places a large emphasis on numerical linear algebra. Like the rest of Academia, I’m using a combination of &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; arrays for this. Yet, after all these years, I start to believe it ain’t necessarily the right choice for a first encounter with numerical linear algebra. Obvisouly everything is not black and white and I’ll try to be nuanced. But, in my opinion, a strongly typed language such as &lt;code&gt;Fortran&lt;/code&gt; might lead to an overall better learning experience. And that’s what it’s all about when you start Uni: learning the principles of scientific programming, not the quirks of a particular language (unless you’re a CS student, which is a different crowd).&lt;/p&gt;
    &lt;p&gt;Don’t get me wrong though. Being proficient with &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; is an absolute necessity for STEM students today, and that’s a good thing. Even from an educational perspective, the scientific &lt;code&gt;Python&lt;/code&gt; ecosystem enables students to do really cool projects, putting the fun back in learning. It would be completely non-sensical to deny this. But using &lt;code&gt;x = np.linalg.solve(A, b)&lt;/code&gt; ain’t the same thing as having a basic understanding of how these algorithms work. And to be clear: the goal of these classes is not to transform a student into a numerical linear algebra expert who could write the next generation LAPACK. It is to teach them just enough of numerical computing so that, when they’ll transition to an engineering position, they’ll be able to make an informed decision regarding which solver or algorithm to use when writing a simulation or data analysis tool to tackle whatever business problem they’re working on.&lt;/p&gt;
    &lt;p&gt;If you liked and aced your numerical methods class, then what I’ll discuss might not necessary be relatable. You’re one of a kind. More often than not, students struggle with such courses. This could be due to genuine comprehension difficulties, or lazyness and lack of motivation simply because they don’t see the point. While both issues are equally important to address, I’ll focus on the first one: students who are willing to put the effort into learning the subject but have difficulties transforming the mathematical algorithm into an actionnable piece of code. Note however that initially motivated but struggling students might easily drift to the second type, hence my focus there first.&lt;/p&gt;
    &lt;p&gt;In the rest of this post, I’ll go through two examples. For each, I’ll show a typical &lt;code&gt;Python&lt;/code&gt; code such a student might write and discuss all of the classical problems they’ve encountered to get there. A large part of these are syntax issues or result from the permissiveness of an interpreted language like &lt;code&gt;Python&lt;/code&gt; which is a double edged sword. Then I’ll show an equivalent &lt;code&gt;Fortran&lt;/code&gt; implementation and explain why I believe it can solve part of these problems. But first, I need to address the two elephants in the room:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My research is on applied mathematics and numerical linear algebra for the physical sciences. I am not doing research on Education. Everything that follows comes from my reflection about my interactions with students I taught to or mentored. If you have scientific evidence (pertaining to scientific computing in particular) proving me wrong, please tell me.&lt;/item&gt;
      &lt;item&gt;When I write &lt;code&gt;Fortran&lt;/code&gt;, what I really mean is modern&lt;code&gt;Fortran&lt;/code&gt;, not&lt;code&gt;FORTRAN&lt;/code&gt;. Anything pre-dating the&lt;code&gt;Fortran 90&lt;/code&gt;standard (or even better, the&lt;code&gt;Fortran 2018&lt;/code&gt;one) is not even an option (yes, I’m looking at you&lt;code&gt;FORTRAN 77&lt;/code&gt;and your incomprehensible&lt;code&gt;goto&lt;/code&gt;, error-prone&lt;code&gt;common&lt;/code&gt;, artithmetic&lt;code&gt;if&lt;/code&gt;and what not).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that being said, let’s get started with a concrete, yet classical, example to illustrate my point.&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;Hello World&lt;/code&gt; of iterative solvers&lt;/head&gt;
    &lt;p&gt;You’ve started University a year ago and are taking your first class on scientific computing. Maybe you already went through the hassle of Gaussian elimination and the LU factorization. During the last class, Professor X discussed about iterative solvers for linear systems. It is now the hands-on session and today’s goal is to implement the Jacobi method. Why Jacobi? Because it is simple enough to implement in an hour or so.&lt;/p&gt;
    &lt;p&gt;The exact problem you’re given is the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the Poisson equation with homogeneous Dirichlet boundary conditions on the unit-square. Assume the Laplace operator has been discretized using a second-order accurate central finite-difference scheme. The discretized equation reads \[\dfrac{u_{i+1, j} - 2u_{i, j} + u_{i-1, j}}{\Delta x^2} + \dfrac{u_{i, j+1} - 2u_{i, j} + u_{i, j-1}}{\Delta y^2} = b_{i, j}.\] For the sake of simplicity, take \(\Delta x = \Delta y\). Write a function implementing the Jacobi method to solve the resulting linear system to a user-prescribed tolerance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We can all agree this is a simple enough yet somewhat realistic example. More importantly, it is sufficient to illustrate my point. Here is what the average student might write in &lt;code&gt;Python&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def jacobi(b , dx, tol, maxiter):
    # Initialize variables.
    nx, ny = b.shape
    residual = 1.0
    u = np.zeros((nx, ny))
    tmp = np.zeros((nx, ny))

    # Jacobi solver.
    for iteration in range(maxiter):
        # Jacobi iteration.
        for i in range(1, nx-1):
            for j in range(1, ny-1):
                tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])

        # Compute residual
        residual = np.linalg.norm(u-tmp)
        # Update solution.
        u = tmp
        # If converged, exit the loop.
        if residual &amp;lt;= tol:
            break

    return u&lt;/code&gt;
    &lt;p&gt;Yes, you shouldn’t do &lt;code&gt;for&lt;/code&gt; loops in &lt;code&gt;Python&lt;/code&gt;. But remember, you are not a seasoned programmer. You’re taking your first class on scientific computing and that’s how the Jacobi method is typically presented. Be forgiving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where do students struggle?&lt;/head&gt;
    &lt;p&gt;Admittidely, the code is quite readable and look very similar to the pseudocode you’d use to describe the Jacobi method. But if you’re reading this blog post, there probably are a handful of things you’ve internalized and don’t even think about anymore (true for both &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt;). And that’s precisely what the students (at least mine) struggle with, starting with the very first line.&lt;/p&gt;
    &lt;p&gt;What the hell is &lt;code&gt;numpy&lt;/code&gt; and why do I need it? Also, why import it as &lt;code&gt;np&lt;/code&gt;? – These questions come back every year. Yet, I don’t have satisfying answers. I always hesitate between&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Trust me kid, you don’t want to use nested lists in&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;to do any serious numerical computing.&lt;/quote&gt;
    &lt;p&gt;which naturally begs the question of why, or&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When I said we’ll use&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;for this scientific computing class, what I really meant is we’ll use&lt;code&gt;numpy&lt;/code&gt;which is a package written for numerical computing because&lt;code&gt;Python&lt;/code&gt;doesn’t naturally have good capabilities for number crunching. As for the import as&lt;code&gt;np&lt;/code&gt;, that’s just a convention.&lt;/quote&gt;
    &lt;p&gt;And this naturally leads to the question of “why Python in the first place then?” for which the only valid answer I have is&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well, because&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;is supposed to be easy to learn and everybody uses it.&lt;/quote&gt;
    &lt;p&gt;Clearly, &lt;code&gt;import numpy as np&lt;/code&gt; is an innocent-looking line of code. It has nothing to do with the subject being taught though, and everything with the choice of the language, only diverting the students from the learning process.&lt;/p&gt;
    &lt;p&gt;I coded everything correctly, 100% sure, but I get this weird error message about indentation – Oh boy! What a classic! The error message varies between&lt;/p&gt;
    &lt;code&gt;IndentationError: expected an indented block&lt;/code&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;code&gt;TabError: inconsistent use of tabs and spaces in indentation&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;&amp;lt;TAB&amp;gt;&lt;/code&gt; versus &lt;code&gt;SPACE&lt;/code&gt; is a surprisingly hot topic in programming which I don’t want to engage in. A seasoned programmer might say “simply configure your IDE properly” which is fair. But we’re talking about your average student (who’s not a CS one remember) and they might use IDLE or even just notepad. As for the &lt;code&gt;IndentationError&lt;/code&gt;, it is a relatively easy error to catch. Yet, the fact that &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;if&lt;/code&gt; or &lt;code&gt;while&lt;/code&gt; constructs are not clearly delineated in &lt;code&gt;Python&lt;/code&gt; other than visually is surprisingly hard for students. I find that it puts an additional cognitive burden on top of a subject which is already demanding enough.&lt;/p&gt;
    &lt;p&gt;It could also be more subtle. The code might run but the results are garbage because the student wrote something like&lt;/p&gt;
    &lt;code&gt;    for iteration in range(maxiter):
    # Jacobi iteration.
    for i in range(1, nx-1):
    for j in range(1, ny-1):
    tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])&lt;/code&gt;
    &lt;p&gt;You might argue that this perfectly understandable, though if you want to be picky, there is no dealineation of where the different loops end. Which the whole point of indentation in &lt;code&gt;Python&lt;/code&gt;. But students do not necessarily get that.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;range(1, nx-1)&lt;/code&gt; and not &lt;code&gt;range(2, nx-1)&lt;/code&gt;? The first column/row is my boundary. – Another classic related to 0-based vs 1-based indexing. And another very hot debate I don’t want to engage in. The fact however is that linear algebra (and a lot of scientific computing for that matter) use 1-based indexing. Think about vectors or matrices. Almost every single maths books write them as&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix}. \]&lt;/p&gt;
    &lt;p&gt;The upper left element has the (1, 1) index, not (0, 0). Why use a language with 0-based indexing for linear algebra other than putting an additional cognitive burden on the students learning the subject? This is a recipe for the nefarious off-by-one error. And these errors are sneaky. The code might run but produce incorrect results and it’s a nightmare for the students (or the poor TA helping them) to figure out why.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;np.linalg.norm&lt;/code&gt; and not just &lt;code&gt;norm&lt;/code&gt; or &lt;code&gt;np.norm&lt;/code&gt;? – This is one is related to my first point. When you’re used to it, you no longer question it. But you don’t know students then and, once more, I don’t have a really clear answer other than&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well,&lt;/p&gt;&lt;code&gt;linalg&lt;/code&gt;stand for linear algebra, and&lt;code&gt;np.linalg&lt;/code&gt;is a collection of linear algebra related function. It is a submodule of&lt;code&gt;numpy&lt;/code&gt;, the package I told you about before.&lt;/quote&gt;
    &lt;p&gt;Grouping like-minded functionalities into a dedicated submodule is definitely good practice, no question there. Discussing the architecture of &lt;code&gt;numpy&lt;/code&gt; makes a lot of sense when students have to do a big project involving numerical computing but not strictly speaking about numerical computing. On the other hand, when it is their first numerical computing class (and possibly first with &lt;code&gt;Python&lt;/code&gt;) I find it distracting. Again, it’s not a big thing really but still. And then you have to explain why &lt;code&gt;np.det&lt;/code&gt; and &lt;code&gt;np.trace&lt;/code&gt; are not part of &lt;code&gt;np.linalg&lt;/code&gt;…&lt;/p&gt;
    &lt;p&gt;Other common problems – There are other very common problems like using the wrong function or inconsistent use of lower- or upper-case for variables. Once you know &lt;code&gt;Python&lt;/code&gt; is case-sensitive, this is mainly a concentration problem. No big deal there. But there is one last thing that tends to cause problems to distracted students and that has to do with the dynamic nature of &lt;code&gt;Python&lt;/code&gt;. Nowhere in the code snippet is it clearly specified that &lt;code&gt;b&lt;/code&gt; needs to be a two-dimensional &lt;code&gt;np.array&lt;/code&gt; of real numbers nor that it shouldn’t be modified by the function. It is only implicit. And that can be a big problem for students when working with marginally more complicated algorithms. Sure enough, type annotation is a thing now in &lt;code&gt;Python&lt;/code&gt;, but it still is pretty new and comparatively few people actually use them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about &lt;code&gt;Fortran&lt;/code&gt;?&lt;/head&gt;
    &lt;p&gt;Alright, I’ve spent the last five minutes talking shit about &lt;code&gt;Python&lt;/code&gt; but how does &lt;code&gt;Fortran&lt;/code&gt; compare with it? Here is a typical implementation of the same function. I’ve actually digged it from my own set of archived homeworks I did 15+ years ago and hardly modified it.&lt;/p&gt;
    &lt;code&gt;function jacobi(b, dx, tol, maxiter) result(u)
    implicit none
    real, dimension(:, :), intent(in) :: b
    real, intent(in) :: dx, tol
    integer, intent(in) :: maxiter
    real, dimension(:, :), allocatable :: u
    ! Internal variables.
    real, dimension(:, :), allocatable :: tmp
    integer :: nx, ny, i, j, iteration

    ! Initialize variables.
    nx = size(b, 1) ; ny = size(b, 2)
    allocate(u(nx, ny), source = 0.0)
    residual = 1.0

    ! Jacobi solver.
    do iteration = 1, maxiter
        ! Jacobi iteration.
        do j = 2, ny-1
            do i = 2, nx-1
                tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                                - u(i, j+1) - u(i, j-1))
            enddo
        enddo

        ! Compute residual.
        residual = norm2(u - tmp)
        ! Update solution.
        u = tmp
        ! If convered, exit the loop.
        if (residual &amp;lt;= tol) exit
    enddo

end function&lt;/code&gt;
    &lt;p&gt;No surprise there. The task is sufficiently simple that both implementations are equally readable. If anything, the &lt;code&gt;Fortran&lt;/code&gt; one is a bit more verbose. But in view of what I’ve just said about the &lt;code&gt;Python&lt;/code&gt; code, I think it actually a good thing. Let me explain.&lt;/p&gt;
    &lt;p&gt;Definition of the variables – &lt;code&gt;Fortran&lt;/code&gt; is a strongly typed language. Lines 2 to 8 are nothing but the definitions of the different variables used in the routine. While you might argue it’s a pain in the a** to write these, I think it can actually be very beneficial for students. Before even implementing the method, they have to clearly think about which variables are input, which are ouput, what are their types and dimensions. And to do so, they have to have at least a minimal understanding of the algorithm itself. Once it’s done, there are no more surprises (hopefully), and the contract between the code and the user is crystal clear. And more importantly, the effort put in clearly identifying the input and output of numerical algorithm usually pays off and leads to less error-prone process.&lt;/p&gt;
    &lt;p&gt;Begining and end of the constructs – &lt;code&gt;Fortran&lt;/code&gt; uses the &lt;code&gt;do&lt;/code&gt;/&lt;code&gt;end do&lt;/code&gt; (or &lt;code&gt;enddo&lt;/code&gt;) construct, clearly specifying where the loop starts where it ends. The indentation used in the code snippet really is just a matter of style. In constrast to &lt;code&gt;Python&lt;/code&gt;, writing&lt;/p&gt;
    &lt;code&gt;    do j = 2, ny-1
    do i = 2, nx-1
    tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                    - u(i, j+1) - u(i, j-1))
    enddo
    enddo&lt;/code&gt;
    &lt;p&gt;does not make the code any less readable and wouldn’t change a dime in terms of computations. It’s a minor thing, fair enough. But it instantly get rid of the &lt;code&gt;IndentationError&lt;/code&gt; or &lt;code&gt;TabError&lt;/code&gt; which are puzzling students. I may be wrong, but I believe it actually reduces the cognitive load associated with the programming language and let the students focus on the actual numerical linear algebra task.&lt;/p&gt;
    &lt;p&gt;No off-by-one error – By default, &lt;code&gt;Fortran&lt;/code&gt; uses a 1-based indexing. No off-by-one errors, period.&lt;/p&gt;
    &lt;p&gt;Intrinsic functions for basic scientific computations – While you have to use &lt;code&gt;np.linalg.norm&lt;/code&gt; in &lt;code&gt;Python&lt;/code&gt; to compute the norm of a vector, &lt;code&gt;Fortran&lt;/code&gt; natively has the &lt;code&gt;norm2&lt;/code&gt; function for that. No external library required. If you want to be picky, you may say that &lt;code&gt;norm2&lt;/code&gt; is a weird name and that &lt;code&gt;norm&lt;/code&gt; might be just fine.&lt;/p&gt;
    &lt;p&gt;Some quirks of &lt;code&gt;Fortran&lt;/code&gt; – All is not perfect though, starting with Line 2 and the &lt;code&gt;implicit none&lt;/code&gt; statement. This is a historical remnant which is considered good practice by modern &lt;code&gt;Fortran&lt;/code&gt; standards but not actually needed. Students being students, they will more likely than not ask questions about it although it has nothing to do with the subject of the class itself. Admittidely, it can be a bit cumbersome to explicitely define all the integers you use even if it’s just for a one-time loop. Likewise, there is the question of &lt;code&gt;real&lt;/code&gt; vs &lt;code&gt;double precision&lt;/code&gt; vs &lt;code&gt;real(wp)&lt;/code&gt; (where &lt;code&gt;wp&lt;/code&gt; is yet another variable you’ve defined somewhere). I don’t think it matters too much though when learning the basics of numerical linear algebra algorithms, although it certainly does when you start discussing about precision and performances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linear least-squares, your first step into Machine Learning&lt;/head&gt;
    &lt;p&gt;Alright, let’s look at another example. Same class, later in the semester. Professor X now discusses over-determined linear systems and how it relates to least-squares, regression and basic machine learning applications. During the hands-on session, you’re given the following problem&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the following unconstrained quadratic program \[\mathrm{minimize} \quad \| Ax - b \|_2^2.\] Write a least-squares solver based on the QR factorization of the matrix \(A\). You can safely assume that \(A\) is a tall matrix (i.e. \(m &amp;gt; n\)).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here is what the typical &lt;code&gt;Python&lt;/code&gt; code written by the students might look like.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def qr(A):
    # Initialize variables.
    m, n = A.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    # QR factorization based on the Gram-Schmidt orthogonalization process.
    for i in range(n):
        q = A[:, i]
        # Orthogonalization w.r.t. to the previous basis vectors.
        for j in range(i):
            R[j, i] = np.vdot(q, Q[:, j])
            q = q - R[j, i]*Q[:, j]

        # Normalize and store the new vector.
        R[i, i] = np.linalg.norm(q)
        Q[:, i] = q / R[i, i]

    return Q, R

def upper_triangular_solve(R, b):
    # Initialize variables.
    n = R.shape[0]
    x = np.zeros((n))

    # Backsubstitution.
    for i in range(n-1, -1, -1):
        x[i] = b[i]
        for j in range(n-1, i, -1):
            x[i] = x[i] - R[i, j]*x[j]
        x[i] = x[i] / R[i, i]

    return x

def lstsq(A, b):
    # QR factorization.
    Q, R = qr(A)
    # Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, Q.T @ b)
    return x&lt;/code&gt;
    &lt;p&gt;This one was adapted from an exercise I gave last year. In reality, students lumped everything into one big function unless told otherwise, but nevermind. For comparison, here is the equivalent &lt;code&gt;Fortran&lt;/code&gt; code.&lt;/p&gt;
    &lt;code&gt;subroutine qr(A, Q, R)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:, :), allocatable, intent(out) :: Q, R
    ! Internal variables.
    integer :: i, j, m, n
    real, dimension(:), allocatable :: q_hat

    ! Initialize variables.
    m = size(A, 1); n = size(A, 2)
    allocate(Q(m, n), source=0.0)
    allocate(R(n, n), source=0.0)
    
    ! QR factorization based on the Gram-Schmidt orthogonalization process.
    do i = 1, n
        q_hat = A(:, i)
        ! Orthogonalize w.r.t. the previous basis vectors.
        do j = 1, i-1
            R(j, i) = dot_product(q_hat, Q(:, j))
            q_hat = q_hat - R(j, i)*Q(:, j)
        end do

        ! Normalize and store the new vector.
        R(i, i) = norm2(q_hat)
        Q(:, i) = q_hat / R(i, i)
    end do
end subroutine

function upper_triangular_solve(R, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: R
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    integer :: n, i, j

    ! Initialize variables.
    n = size(R, 1)
    allocate(x(n), source=0.0)

    ! Backsubstitution.
    do i = n, 1, -1
        x(i) = b(i)
        do j = n-1, i, -1
            x(i) = x(i) - R(i, j)*x(j)
        enddo
        x(i) = x(i) / R(i, i)
    end do
end function

function lstsq(A, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    real, dimension(:, :), allocatable :: Q, R

    ! QR factorization.
    call qr(A, Q, R)
    ! Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, matmul(transpose(Q), b))
end function&lt;/code&gt;
    &lt;p&gt;Just like the Jacobi example, both implementations are equally readable. At this point in the semester, the students got somewhat more comfortable with &lt;code&gt;Python&lt;/code&gt;. The classical indentation problems were not so much of a problem anymore. The off-by-one errors due to 0-based indexing for the Gram-Schmidt orthogonalization in &lt;code&gt;qr&lt;/code&gt; or in the backsubstitution algorithm on the other hand… That was painful. In a 90-minutes class, it took almost a whole hour simply for them to debug these errors.&lt;/p&gt;
    &lt;p&gt;But there was another thing that confused students. A lot. And that has to do with computing dot products in &lt;code&gt;numpy&lt;/code&gt;. There’s so many different ways: &lt;code&gt;np.vdot(x, y)&lt;/code&gt;, &lt;code&gt;np.dot(x.T, y)&lt;/code&gt;, &lt;code&gt;np.dot(np.transpose(x), y)&lt;/code&gt;, or &lt;code&gt;x.transpose().dot(y)&lt;/code&gt; to list just the ones I have seen in their codes. Again, this has nothing to do with linear algebra, but everything with the language. Not only do they need to learn the math, but they simultaneously need to learn the not-quite-necessarily-math-standard syntax used in the language (yes, I’m looking at you &lt;code&gt;@&lt;/code&gt;). It’s just a question of habits, sure enough, but again it can be impeding the learning process.&lt;/p&gt;
    &lt;p&gt;On the other hand, the &lt;code&gt;Fortran&lt;/code&gt; implementation is even closer to the standard mathematical description of the algorithm: 1-based indexing, intrinsic &lt;code&gt;dot_product&lt;/code&gt; function, etc. But beside the &lt;code&gt;implicit none&lt;/code&gt;, there is the need to use a &lt;code&gt;subroutine&lt;/code&gt; rather than a &lt;code&gt;function&lt;/code&gt; construct for the QR decomposition because it has two output variables. Not a big deal again, but to be fair, it does add another minor layer of abstraction due to the language semantics rather than that of the subject being studied.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Fortran&lt;/code&gt; may have a slight edge, but I swept some things under the rug…&lt;/head&gt;
    &lt;p&gt;In the end, when it comes to teaching the basics of numerical linear algebra, &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt; are not that different. And in that regard, neither is &lt;code&gt;Julia&lt;/code&gt; which I really like as well. The main advantages I see of using &lt;code&gt;Fortran&lt;/code&gt; for this task however are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1-based indexing : in my experience, the 0-based indexing in &lt;code&gt;Python&lt;/code&gt;leads to so many off-by-one erros driving the students crazy. Because linear algebra textbooks naturally use 1-based indexing, having to translate everything in your head to 0-based indices is a huge cognitive burden on top of a subject already demanding enough. You might get used to it eventually, but it’s a painful process impeding the learning outcomes.&lt;/item&gt;
      &lt;item&gt;Strong typing : combined with &lt;code&gt;implicit none&lt;/code&gt;, having to declare the type, dimension and input or output nature of every variable you use might seem cumbersome at first. But it forces students to pause and ponder to identify which is which. Sure this is an effort, but it is worth it. Learning is not effortless and this effort forces you to have a somewhat better understanding of a numerical algorithm before even starting to implement it. Which I think is a good thing.&lt;/item&gt;
      &lt;item&gt;Clear delineation of the constructs : at least during the first few weeks, having to rely only on visual clues to identify where does a loop ends in &lt;code&gt;Python&lt;/code&gt;seems to be quite complicated for a non-negligible fraction of the students I have. In that respect, the&lt;code&gt;do&lt;/code&gt;/&lt;code&gt;enddo&lt;/code&gt;construct in&lt;code&gt;Fortran&lt;/code&gt;is much more explicit and probably easier to grasp.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Obvisouly, I’m not expecting educators worldwide to switch back to &lt;code&gt;Fortran&lt;/code&gt; overnight, nor is it necessarily desirable. The advantages I see are non-negligible from my perspective but certainly not enough by themselves. There are many other things that need to be taken into account. &lt;code&gt;Python&lt;/code&gt; is a very generalist language. You can do so much more than just numerical computing so it makes complete sense to have it in the classroom. The ecosystem is incredibly vast and the interactive nature definitely has its pros. Notebooks such as &lt;code&gt;Jupyter&lt;/code&gt; can be incredible teaching tools (although they come with their own problems in term good coding practices). So are the &lt;code&gt;Pluto&lt;/code&gt; notebooks in &lt;code&gt;Julia&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Fortran&lt;/code&gt; is good at one thing: enabling computational scientists and engineers to write high-performing mathematical models without all the intricacies of equally peformant but more CS-oriented languages such as &lt;code&gt;C&lt;/code&gt; or &lt;code&gt;C++&lt;/code&gt;. Sure enough, the modern &lt;code&gt;Fortran&lt;/code&gt; ecosystem is orders of magnitude smaller than &lt;code&gt;Python&lt;/code&gt;, and targetted toward numerical computing almost exclusively. And the &lt;code&gt;Julia&lt;/code&gt; one is fairly impressive. But the community is working on it (see the fortran-lang website or the Fortran discourse if you don’t trust me). The bad rep of &lt;code&gt;Fortran&lt;/code&gt; is unjustified, particularly for teaching purposes. Many of its detractors have hardly been exposed to anything else than &lt;code&gt;FORTRAN 77&lt;/code&gt;. And it’s true that, by current standards, most of &lt;code&gt;FORTRAN 77&lt;/code&gt; codes are terrible sphagetti codes making extensive use of implicit typing and incomprehensible &lt;code&gt;goto&lt;/code&gt; statements. Even I, as a &lt;code&gt;Fortran&lt;/code&gt; programmer, acknowledge it. But that’s no longer what &lt;code&gt;Fortran&lt;/code&gt; is since the 1990’s, and certainly not today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45351624</guid><pubDate>Tue, 23 Sep 2025 19:29:26 +0000</pubDate></item><item><title>YouTube says it'll bring back creators banned for Covid and election content</title><link>https://www.businessinsider.com/youtube-reinstate-channels-banned-over-covid-content-policies-2025-9</link><description>&lt;doc fingerprint="bfc89971f0b03c43"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;YouTube is set to reinstate channels previously banned for flouting its COVID-19 policies.&lt;/item&gt;
      &lt;item&gt;That could mean the return of prominent conservative creators like Dan Bongino and Steve Bannon.&lt;/item&gt;
      &lt;item&gt;The change comes in the wake of a House Judiciary investigation into Big Tech.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A roster of high-profile conservative voices could soon return to YouTube.&lt;/p&gt;
    &lt;p&gt;YouTube's parent company, Alphabet, said in a letter published Tuesday that it intends to "provide an opportunity for all creators to rejoin the platform" whose accounts had been terminated over repeated violations of its COVID-19 and election integrity policies.&lt;/p&gt;
    &lt;p&gt;The letter, written by Alphabet lawyer Daniel Donovan to Jim Jordan, R-Ohio, chair of the House Judiciary Committee, said that YouTube "values conservative voices on its platform" and recognized their reach and role in civic discourse. (Read the letter in full below.)&lt;/p&gt;
    &lt;p&gt;The House Judiciary Committee published the letter on its website on Tuesday following its monthslong investigation into whether Biden White House officials pressured Big Tech platforms into censoring content. A Google spokesperson confirmed the authenticity of the letter.&lt;/p&gt;
    &lt;p&gt;YouTube's about-face on previously banned accounts marks the latest shift in Big Tech content moderation. Companies from Meta to X have overhauled their content policies and switched away from using third-party fact-checkers.&lt;/p&gt;
    &lt;p&gt;Prominent YouTube channels from conservative creators — including Dan Bongino, Steve Bannon, and Children's Health Defense, Robert F. Kennedy Jr.'s nonprofit activist group, as well as those from lesser-known creators — had been banned from YouTube for flouting its COVID-19 misinformation and election-related policies. Bongino has since become the deputy director of the Federal Bureau of Investigation, while Kennedy leads the Department of Health and Human Services.&lt;/p&gt;
    &lt;p&gt;Google's lawyer said in the letter published Tuesday that YouTube had ended all of its stand-alone, COVID-19-related policies by December 2024 and retired a separate policy regarding election integrity in 2023 to "allow for discussion of possible widespread fraud, errors, or glitches occurring in the 2020 and other past U.S. Presidential elections."&lt;/p&gt;
    &lt;p&gt;The letter didn't go into detail about how previously banned creators could resume their terminated channels or whether their content would be monetized and therefore eligible to get a cut of ad revenue.&lt;/p&gt;
    &lt;p&gt;Google, via its @UpdatesFromYT X account, later clarified that it is planning a pilot program that would be available to a "subset of creators" who have been suspended from YouTube — in addition to accounts that were kicked off for violating COVID-19 or election-misinformation policies that have since been deprecated.&lt;/p&gt;
    &lt;p&gt;The Google spokesperson had no further comment but added the company would have more to say in the coming weeks.&lt;/p&gt;
    &lt;p&gt;"This is another victory in the fight against censorship," Jordan wrote in a post on X.&lt;/p&gt;
    &lt;p&gt;Google's move to reinstate previously banned channels comes just over a year after Meta CEO Mark Zuckerberg said — also in a letter to Jordan — that the Biden administration had repeatedly pressured Meta in 2021 to remove content related to COVID-19.&lt;/p&gt;
    &lt;p&gt;"I believe the government pressure was wrong, and I regret that we were not more outspoken about it," Zuckerberg wrote in the August 2024 letter.&lt;/p&gt;
    &lt;p&gt;Google, too, said senior Biden administration officials, including White House officials, had pressed the company to remove content related to the COVID-19 pandemic that didn't violate YouTube's policies.&lt;/p&gt;
    &lt;p&gt;"It is unacceptable and wrong when any government, including the Biden Administration, attempts to dictate how" a company moderates content, Google wrote in the letter published Tuesday.&lt;/p&gt;
    &lt;p&gt;Google said it has been testing a feature similar to X's Community Notes to allow people to provide relevant context about its videos, but it "has not and will not empower fact-checkers to take action on or label content" on the platform.&lt;/p&gt;
    &lt;p&gt;Here's the letter from Google counsel to the House Judiciary Committee:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352213</guid><pubDate>Tue, 23 Sep 2025 20:16:47 +0000</pubDate></item><item><title>Podman Desktop celebrates 3M downloads</title><link>https://podman-desktop.io/blog/3-million</link><description>&lt;doc fingerprint="7cfef03e2d35f7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3,000,000 downloads. Thank you&lt;/head&gt;
    &lt;head rend="h2"&gt;Wooohooo!!&lt;/head&gt;
    &lt;p&gt;We are extremely excited to share that Podman Desktop just crossed 3,000,000 downloads! This is a huge step for the project and we are incredibly thankful for how each of you has helped! This milestone belongs to you. You file issues, suggest features, build extensions, teach teammates, and nudge us to make the day-to-day better. Thank you for helping turn an idea into a tool people rely on.&lt;/p&gt;
    &lt;p&gt;To celebrate this milestone, and thank you, we built a small surprise: https://3m.podman-desktop.io&lt;/p&gt;
    &lt;p&gt;We are grateful for all the feedback we have been receiving, here is just a short collection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Lovely to have all containers in one tool. Thanks!” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"Podman Desktop is a total win." - balancedchaos Reddit (r/podman)&lt;/item&gt;
      &lt;item&gt;“Great project! Small improvements each time make it strong long-term.” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"The experience has been nice, and the ability to run containers under user without going root is definitely nice." - ajyotirmay Hacker News&lt;/item&gt;
      &lt;item&gt;“You are doing a great job! Thanks to you I always recommend podman whenever 'docker' comes out in conversations” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;“OMG this tool is amazing. Tutorial was great. Much easier than minikube.” - anonymous user feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We read every comment. Yes, even the spicy ones. That feedback shapes our roadmap and helps us focus on the work that makes the biggest difference.&lt;/p&gt;
    &lt;p&gt;Here are other noteworthy milestones we’ve reached in our quest to help developers work with containers and Kubernetes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Podman Desktop is now an official CNCF Sandbox Project&lt;/head&gt;
    &lt;p&gt;Last year, we proudly contributed Podman Desktop to the Cloud Native Computing Foundation (CNCF), and we were accepted into the CNCF Sandbox on January 21, 2025. 🎉&lt;/p&gt;
    &lt;p&gt;This milestone highlights our commitment to building open, community-driven tools that empower developers to seamlessly work with containers and Kubernetes. Joining the CNCF Sandbox is just the beginning. Reaching this 3 million downloads milestone shows the need to build a vibrant cloud‑native ecosystem and collaborate with the community to take Podman Desktop even further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights from the past year&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smoother Kubernetes workflows: Easier context and namespace switching, a powerful dashboard for your cluster, and less jumping to the terminal when you want to apply YAML or peek at events and logs.&lt;/item&gt;
      &lt;item&gt;Better Docker compatibility: Clearer setup and diagnostics, improved socket handling, and fewer surprises when you bridge Docker and Podman workflows.&lt;/item&gt;
      &lt;item&gt;Everyday quality of life: Bulk actions for containers, better notifications, clearer status in the UI, and lots of fit and finish fixes that make everything feel calmer.&lt;/item&gt;
      &lt;item&gt;AI on your laptop, without drama: Podman AI Lab is easier to set up, with a curated model catalog, simple playgrounds, and an OpenAI-compatible API you can call from your apps.&lt;/item&gt;
      &lt;item&gt;Extensions, everywhere: More community-built extensions, plus tooling that makes it easier to develop and test your own. If you are extending Podman Desktop for your team, thank you. You are shaping where we take the platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Enterprise adoption of Podman Desktop&lt;/head&gt;
    &lt;p&gt;In recent months, we’ve seen more and more enterprises adopting Podman Desktop and making it part of critical developer workflows. To highlight this, we wanted to share a recent note we received:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2023, our company studied the possible solutions to run containers on our engineers’ laptop in the most efficient way. We judged that our best bet was to migrate our thousands of engineers to Podman Desktop. That was a brave move but we believed Podman Desktop was the most promising solution. We did not know how quickly it would become the best solution of all and how right that decision would be!&lt;/p&gt;
      &lt;p&gt;We migrated most engineers in 2023 and did the last mile at the beginning of 2024. Podman Desktop evolved at an insane pace. It improved release after release. And it still does. It quickly became a rock solid solution with more and more useful features to discover every month!&lt;/p&gt;
      &lt;p&gt;On top of that, Podman Desktop is a Community solution which allows us to have a very healthy relationship with the contributors of the project.&lt;/p&gt;
      &lt;p&gt;I am happy to hear that Podman Desktop reached 3M downloads. This means more and more people realise how good this software is. Thank you Podman Desktop. Special thanks to all the project’s contributors!&lt;/p&gt;
      &lt;p&gt;Fabrice Pipart, Amadeus&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;New here? Grab the latest build&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download Podman Desktop for Windows, macOS, and Linux: https://podman-desktop.io/downloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From all of us on the Podman Desktop team, thank you for trusting us with your workflow and for helping us get better with every release. If you haven't tried Podman Desktop in a while, grab the latest build and let us know what you think. If you are already a daily user, we would love to hear what is working and what is not, so we can make the next million downloads even more useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352460</guid><pubDate>Tue, 23 Sep 2025 20:40:21 +0000</pubDate></item><item><title>Is life a form of computation?</title><link>https://thereader.mitpress.mit.edu/is-life-a-form-of-computation/</link><description>&lt;doc fingerprint="b2025f96f139f3ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Life a Form of Computation?&lt;/head&gt;
    &lt;p&gt;In 1994, a strange, pixelated machine came to life on a computer screen. It read a string of instructions, copied them, and built a clone of itself — just as the Hungarian-American Polymath John von Neumann had predicted half a century earlier. It was a striking demonstration of a profound idea: that life, at its core, might be computational.&lt;/p&gt;
    &lt;p&gt;Although this is seldom fully appreciated, von Neumann was one of the first to establish a deep link between life and computation. Reproduction, like computation, he showed, could be carried out by machines following coded instructions. In his model, based on Alan Turing’s Universal Machine, self-replicating systems read and execute instructions much like DNA does: “if the next instruction is the codon CGA, then add an arginine to the protein under construction.” It’s not a metaphor to call DNA a “program” — that is literally the case.&lt;/p&gt;
    &lt;p&gt;Of course, there are meaningful differences between biological computing and the kind of digital computing done by a personal computer or your smartphone. DNA is subtle and multilayered, including phenomena like epigenetics and gene proximity effects. Cellular DNA is nowhere near the whole story, either. Our bodies contain (and continually swap) countless bacteria and viruses, each running their own code.&lt;/p&gt;
    &lt;p&gt;Biological computing is “massively parallel,” decentralized, and noisy. Your cells have somewhere in the neighborhood of 300 quintillion ribosomes, all working at the same time. Each of these exquisitely complex floating protein factories is, in effect, a tiny computer — albeit a stochastic one, meaning not entirely predictable. The movements of hinged components, the capture and release of smaller molecules, and the manipulation of chemical bonds are all individually random, reversible, and inexact, driven this way and that by constant thermal buffeting. Only a statistical asymmetry favors one direction over another, with clever origami moves tending to “lock in” certain steps such that a next step becomes likely to happen.&lt;/p&gt;
    &lt;p&gt;This differs greatly from the operation of “logic gates” in a computer, basic components that process binary inputs into outputs using fixed rules. They are irreversible and engineered to be 99.99 percent reliable and reproducible.&lt;/p&gt;
    &lt;p&gt;Biological computing is computing, nonetheless. And its use of randomness is a feature, not a bug. In fact, many classic algorithms in computer science also require randomness (albeit for different reasons), which may explain why Turing insisted that the Ferranti Mark I, an early computer he helped to design in 1951, include a random number instruction. Randomness is thus a small but important conceptual extension to the original Turing Machine, though any computer can simulate it by calculating deterministic but random-looking or “pseudorandom” numbers.&lt;/p&gt;
    &lt;p&gt;Parallelism, too, is increasingly fundamental to computing today. Modern AI, for instance, depends on both massive parallelism and randomness — as in the parallelized “stochastic gradient descent” (SGD) algorithm, used for training most of today’s neural nets, the “temperature” setting used in chatbots to introduce a degree of randomness into their output, and the parallelism of Graphics Processing Units (GPUs), which power most AI in data centers.&lt;/p&gt;
    &lt;p&gt;Traditional digital computing, which relies on the centralized, sequential execution of instructions, was a product of technological constraints. The first computers needed to carry out long calculations using as few parts as possible. Originally, those parts were flaky, expensive vacuum tubes, which had a tendency to burn out and needed frequent replacement by hand. The natural design, then, was a minimal “Central Processing Unit” (CPU) operating on sequences of bits ferried back and forth from an external memory. This has come to be known as the “von Neumann architecture.”&lt;/p&gt;
    &lt;p&gt;Turing and von Neumann were both aware that computing could be done by other means, though. Turing, near the end of his life, explored how biological patterns like leopard spots could arise from simple chemical rules, in a field he called morphogenesis. Turing’s model of morphogenesis was a biologically inspired form of massively parallel, distributed computation. So was his earlier concept of an “unorganized machine,” a randomly connected neural net modeled after an infant’s brain.&lt;/p&gt;
    &lt;p&gt;These were visions of what computing without a central processor could look like — and what it does look like, in living systems.&lt;/p&gt;
    &lt;p&gt;Von Neumann also began exploring massively parallel approaches to computation as far back as the 1940s. In discussions with Polish mathematician Stanisław Ulam at Los Alamos, he conceived the idea of “cellular automata,” pixel-like grids of simple computational units, all obeying the same rule, and all altering their states simultaneously by communicating only with their immediate neighbors. With characteristic bravura, von Neumann went so far as to design, on paper, the key components of a self-reproducing cellular automaton, including a horizontal “tape” of cells containing instructions and blocks of cellular “circuitry” for reading, copying, and executing them.&lt;/p&gt;
    &lt;p&gt;Designing a cellular automaton is far harder than ordinary programming, because every cell or “pixel” is simultaneously altering its own state and its environment. Add randomness and subtle feedback effects, as in biology, and it becomes even harder to reason about, “program,” or “debug.”&lt;/p&gt;
    &lt;p&gt;Nonetheless, Turing and von Neumann grasped something fundamental: Computation doesn’t require a central processor, logic gates, binary arithmetic, or sequential programs. There are infinite ways to compute, and, crucially, they are all equivalent. This insight is one of the greatest accomplishments of theoretical computer science.&lt;/p&gt;
    &lt;p&gt;This “platform independence” or “multiple realizability” means that any computer can emulate any other one. If the computers are of different designs, though, the emulation may be glacially slow. For that reason, von Neumann’s self-reproducing cellular automaton has never been physically built — though that would be fun to see!&lt;/p&gt;
    &lt;p&gt;That demonstration in 1994 — the first successful emulation of von Neumann’s self-reproducing automation — couldn’t have happened much earlier. A serial computer requires serious processing power to loop through the automaton’s 6,329 cells over the 63 billion time steps required for the automaton to complete its reproductive cycle. Onscreen, it worked as advertised: a pixelated two-dimensional Rube Goldberg machine, squatting astride a 145,315-cell–long instruction tape trailing off to the right, pumping information out of the tape and reaching out with a “writing arm” to slowly print a working clone of itself just above and to the right of the original.&lt;/p&gt;
    &lt;p&gt;It’s similarly inefficient for a serial computer to emulate a parallel neural network, heir to Turing’s “unorganized machine.” Consequently, running big neural nets like those in Transformer-based chatbots has only recently become practical, thanks to ongoing progress in the miniaturization, speed, and parallelism of digital computers.&lt;/p&gt;
    &lt;p&gt;In 2020, my colleague Alex Mordvintsev combined modern neural nets, Turing’s morphogenesis, and von Neumann’s cellular automata into the “neural cellular automaton” (NCA), replacing the simple per-pixel rule of a classic cellular automaton with a neural net. This net, capable of sensing and affecting a few values representing local morphogen concentrations, can be trained to “grow” any desired pattern or image, not just zebra stripes or leopard spots.&lt;/p&gt;
    &lt;p&gt;Real cells don’t literally have neural nets inside them, but they do run highly evolved, nonlinear, and purposive “programs” to decide on the actions they will take in the world, given external stimulus and an internal state. NCAs offer a general way to model the range of possible behaviors of cells whose actions don’t involve movement, but only changes of state (here, represented as color) and the absorption or release of chemicals.&lt;/p&gt;
    &lt;p&gt;The first NCA Alex showed me was of a lizard emoji, which could regenerate not only its tail, but also its limbs and head! It was a powerful demonstration of how complex multicellular life can “think locally” yet “act globally,” even when each cell (or pixel) is running the same program — just as each of your cells is running the same DNA. Simulations like these show how computation can produce lifelike behavior across scales. Building on von Neumann’s designs and extending into modern neural cellular automata, they offer a glimpse into the computational underpinnings of living systems.&lt;/p&gt;
    &lt;p&gt;Blaise Agüera y Arcas is a VP/Fellow at Google, where he is the CTO of Technology &amp;amp; Society, and the founder of Paradigms of Intelligence, an organization dedicated to fundamental AI research. He is the author of “What Is Intelligence?,” from which this article is adapted.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352533</guid><pubDate>Tue, 23 Sep 2025 20:46:24 +0000</pubDate></item><item><title>Qwen3-VL</title><link>https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352672</guid><pubDate>Tue, 23 Sep 2025 20:59:17 +0000</pubDate></item><item><title>Context Engineering for AI Agents: Lessons</title><link>https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus</link><description>&lt;doc fingerprint="a55290da61d9895c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Context Engineering for AI Agents: Lessons from Building Manus&lt;/head&gt;
    &lt;p&gt;Saturday, July 19&lt;/p&gt;
    &lt;p&gt;Tech&lt;/p&gt;
    &lt;p&gt;2025/7/18 --Yichao 'Peak' Ji&lt;/p&gt;
    &lt;p&gt;At the very beginning of the Manus project, my team and I faced a key decision: should we train an end-to-end agentic model using open-source foundations, or build an agent on top of the in-context learning abilities of frontier models?&lt;/p&gt;
    &lt;p&gt;Back in my first decade in NLP, we didn't have the luxury of that choice. In the distant days of BERT (yes, it's been seven years), models had to be fine-tuned—and evaluated—before they could transfer to a new task. That process often took weeks per iteration, even though the models were tiny compared to today's LLMs. For fast-moving applications, especially pre–PMF, such slow feedback loops are a deal-breaker. That was a bitter lesson from my last startup, where I trained models from scratch for open information extraction and semantic search. Then came GPT-3 and Flan-T5, and my in-house models became irrelevant overnight. Ironically, those same models marked the beginning of in-context learning—and a whole new path forward.&lt;/p&gt;
    &lt;p&gt;That hard-earned lesson made the choice clear: Manus would bet on context engineering. This allows us to ship improvements in hours instead of weeks, and kept our product orthogonal to the underlying models: If model progress is the rising tide, we want Manus to be the boat, not the pillar stuck to the seabed.&lt;/p&gt;
    &lt;p&gt;Still, context engineering turned out to be anything but straightforward. It's an experimental science—and we've rebuilt our agent framework four times, each time after discovering a better way to shape context. We affectionately refer to this manual process of architecture searching, prompt fiddling, and empirical guesswork as "Stochastic Graduate Descent". It's not elegant, but it works.&lt;/p&gt;
    &lt;p&gt;This post shares the local optima we arrived at through our own "SGD". If you're building your own AI agent, I hope these principles help you converge faster.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Around the KV-Cache&lt;/head&gt;
    &lt;p&gt;If I had to choose just one metric, I'd argue that the KV-cache hit rate is the single most important metric for a production-stage AI agent. It directly affects both latency and cost. To understand why, let's look at how a typical agent operates:&lt;/p&gt;
    &lt;p&gt;After receiving a user input, the agent proceeds through a chain of tool uses to complete the task. In each iteration, the model selects an action from a predefined action space based on the current context. That action is then executed in the environment (e.g., Manus's virtual machine sandbox) to produce an observation. The action and observation are appended to the context, forming the input for the next iteration. This loop continues until the task is complete.&lt;/p&gt;
    &lt;p&gt;As you can imagine, the context grows with every step, while the output—usually a structured function call—remains relatively short. This makes the ratio between prefilling and decoding highly skewed in agents compared to chatbots. In Manus, for example, the average input-to-output token ratio is around 100:1.&lt;/p&gt;
    &lt;p&gt;Fortunately, contexts with identical prefixes can take advantage of KV-cache, which drastically reduces time-to-first-token (TTFT) and inference cost—whether you're using a self-hosted model or calling an inference API. And we're not talking about small savings: with Claude Sonnet, for instance, cached input tokens cost 0.30 USD/MTok, while uncached ones cost 3 USD/MTok—a 10x difference.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;From a context engineering perspective, improving KV-cache hit rate involves a few key practices:&lt;/p&gt;
    &lt;p&gt;1.Keep your prompt prefix stable. Due to the autoregressive nature of LLMs, even a single-token difference can invalidate the cache from that token onward. A common mistake is including a timestamp—especially one precise to the second—at the beginning of the system prompt. Sure, it lets the model tell you the current time, but it also kills your cache hit rate.&lt;/p&gt;
    &lt;p&gt;2.Make your context append-only. Avoid modifying previous actions or observations. Ensure your serialization is deterministic. Many programming languages and libraries don't guarantee stable key ordering when serializing JSON objects, which can silently break the cache.&lt;/p&gt;
    &lt;p&gt;3.Mark cache breakpoints explicitly when needed. Some model providers or inference frameworks don't support automatic incremental prefix caching, and instead require manual insertion of cache breakpoints in the context. When assigning these, account for potential cache expiration and at minimum, ensure the breakpoint includes the end of the system prompt.&lt;/p&gt;
    &lt;p&gt;Additionally, if you're self-hosting models using frameworks like vLLM, make sure prefix/prompt caching is enabled, and that you're using techniques like session IDs to route requests consistently across distributed workers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mask, Don't Remove&lt;/head&gt;
    &lt;p&gt;As your agent takes on more capabilities, its action space naturally grows more complex—in plain terms, the number of tools explodes. The recent popularity of MCP only adds fuel to the fire. If you allow user-configurable tools, trust me: someone will inevitably plug hundreds of mysterious tools into your carefully curated action space. As a result, the model is more likely to select the wrong action or take an inefficient path. In short, your heavily armed agent gets dumber.&lt;/p&gt;
    &lt;p&gt;A natural reaction is to design a dynamic action space—perhaps loading tools on demand using something RAG-like. We tried that in Manus too. But our experiments suggest a clear rule: unless absolutely necessary, avoid dynamically adding or removing tools mid-iteration. There are two main reasons for this:&lt;/p&gt;
    &lt;p&gt;1.In most LLMs, tool definitions live near the front of the context after serialization, typically before or after the system prompt. So any change will invalidate the KV-cache for all subsequent actions and observations.&lt;/p&gt;
    &lt;p&gt;2.When previous actions and observations still refer to tools that are no longer defined in the current context, the model gets confused. Without constrained decoding, this often leads to schema violations or hallucinated actions.&lt;/p&gt;
    &lt;p&gt;To solve this while still improving action selection, Manus uses a context-aware state machine to manage tool availability. Rather than removing tools, it masks the token logits during decoding to prevent (or enforce) the selection of certain actions based on the current context.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In practice, most model providers and inference frameworks support some form of response prefill, which allows you to constrain the action space without modifying the tool definitions. There are generally three modes of function calling (we'll use the Hermes format from NousResearch as an example):&lt;/p&gt;
    &lt;p&gt;•Auto – The model may choose to call a function or not. Implemented by prefilling only the reply prefix: &amp;lt;|im_start|&amp;gt;assistant&lt;/p&gt;
    &lt;p&gt;•Required – The model must call a function, but the choice is unconstrained. Implemented by prefilling up to tool call token: &amp;lt;|im_start|&amp;gt;assistant&amp;lt;tool_call&amp;gt;&lt;/p&gt;
    &lt;p&gt;•Specified – The model must call a function from a specific subset. Implemented by prefilling up to the beginning of the function name: &amp;lt;|im_start|&amp;gt;assistant&amp;lt;tool_call&amp;gt;{"name": “browser_&lt;/p&gt;
    &lt;p&gt;Using this, we constrain action selection by masking token logits directly. For example, when the user provides a new input, Manus must reply immediately instead of taking an action. We've also deliberately designed action names with consistent prefixes—e.g., all browser-related tools start with browser_, and command-line tools with shell_. This allows us to easily enforce that the agent only chooses from a certain group of tools at a given state without using stateful logits processors.&lt;/p&gt;
    &lt;p&gt;These designs help ensure that the Manus agent loop remains stable—even under a model-driven architecture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Use the File System as Context&lt;/head&gt;
    &lt;p&gt;Modern frontier LLMs now offer context windows of 128K tokens or more. But in real-world agentic scenarios, that's often not enough, and sometimes even a liability. There are three common pain points:&lt;/p&gt;
    &lt;p&gt;1.Observations can be huge, especially when agents interact with unstructured data like web pages or PDFs. It's easy to blow past the context limit.&lt;/p&gt;
    &lt;p&gt;2.Model performance tends to degrade beyond a certain context length, even if the window technically supports it.&lt;/p&gt;
    &lt;p&gt;3.Long inputs are expensive, even with prefix caching. You're still paying to transmit and prefill every token.&lt;/p&gt;
    &lt;p&gt;To deal with this, many agent systems implement context truncation or compression strategies. But overly aggressive compression inevitably leads to information loss. The problem is fundamental: an agent, by nature, must predict the next action based on all prior state—and you can't reliably predict which observation might become critical ten steps later. From a logical standpoint, any irreversible compression carries risk.&lt;/p&gt;
    &lt;p&gt;That's why we treat the file system as the ultimate context in Manus: unlimited in size, persistent by nature, and directly operable by the agent itself. The model learns to write to and read from files on demand—using the file system not just as storage, but as structured, externalized memory.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Our compression strategies are always designed to be restorable. For instance, the content of a web page can be dropped from the context as long as the URL is preserved, and a document's contents can be omitted if its path remains available in the sandbox. This allows Manus to shrink context length without permanently losing information.&lt;/p&gt;
    &lt;p&gt;While developing this feature, I found myself imagining what it would take for a State Space Model (SSM) to work effectively in an agentic setting. Unlike Transformers, SSMs lack full attention and struggle with long-range backward dependencies. But if they could master file-based memory—externalizing long-term state instead of holding it in context—then their speed and efficiency might unlock a new class of agents. Agentic SSMs could be the real successors to Neural Turing Machines.&lt;/p&gt;
    &lt;head rend="h3"&gt;Manipulate Attention Through Recitation&lt;/head&gt;
    &lt;p&gt;If you've worked with Manus, you've probably noticed something curious: when handling complex tasks, it tends to create a todo.md file—and update it step-by-step as the task progresses, checking off completed items.&lt;/p&gt;
    &lt;p&gt;That's not just cute behavior—it's a deliberate mechanism to manipulate attention.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;A typical task in Manus requires around 50 tool calls on average. That's a long loop—and since Manus relies on LLMs for decision-making, it's vulnerable to drifting off-topic or forgetting earlier goals, especially in long contexts or complicated tasks.&lt;/p&gt;
    &lt;p&gt;By constantly rewriting the todo list, Manus is reciting its objectives into the end of the context. This pushes the global plan into the model's recent attention span, avoiding "lost-in-the-middle" issues and reducing goal misalignment. In effect, it's using natural language to bias its own focus toward the task objective—without needing special architectural changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keep the Wrong Stuff In&lt;/head&gt;
    &lt;p&gt;Agents make mistakes. That's not a bug—it's reality. Language models hallucinate, environments return errors, external tools misbehave, and unexpected edge cases show up all the time. In multi-step tasks, failure is not the exception; it's part of the loop.&lt;/p&gt;
    &lt;p&gt;And yet, a common impulse is to hide these errors: clean up the trace, retry the action, or reset the model's state and leave it to the magical "temperature". That feels safer, more controlled. But it comes at a cost: Erasing failure removes evidence. And without evidence, the model can't adapt.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;In our experience, one of the most effective ways to improve agent behavior is deceptively simple: leave the wrong turns in the context. When the model sees a failed action—and the resulting observation or stack trace—it implicitly updates its internal beliefs. This shifts its prior away from similar actions, reducing the chance of repeating the same mistake. In fact, we believe error recovery is one of the clearest indicators of true agentic behavior. Yet it's still underrepresented in most academic work and public benchmarks, which often focus on task success under ideal conditions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Don't Get Few-Shotted&lt;/head&gt;
    &lt;p&gt;Few-shot prompting is a common technique for improving LLM outputs. But in agent systems, it can backfire in subtle ways.&lt;/p&gt;
    &lt;p&gt;Language models are excellent mimics; they imitate the pattern of behavior in the context. If your context is full of similar past action-observation pairs, the model will tend to follow that pattern, even when it's no longer optimal.&lt;/p&gt;
    &lt;p&gt;This can be dangerous in tasks that involve repetitive decisions or actions. For example, when using Manus to help review a batch of 20 resumes, the agent often falls into a rhythm—repeating similar actions simply because that's what it sees in the context. This leads to drift, overgeneralization, or sometimes hallucination.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;The fix is to increase diversity. Manus introduces small amounts of structured variation in actions and observations—different serialization templates, alternate phrasing, minor noise in order or formatting. This controlled randomness helps break the pattern and tweaks the model's attention. In other words, don't few-shot yourself into a rut. The more uniform your context, the more brittle your agent becomes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Context engineering is still an emerging science—but for agent systems, it's already essential. Models may be getting stronger, faster, and cheaper, but no amount of raw capability replaces the need for memory, environment, and feedback. How you shape the context ultimately defines how your agent behaves: how fast it runs, how well it recovers, and how far it scales.&lt;/p&gt;
    &lt;p&gt;At Manus, we've learned these lessons through repeated rewrites, dead ends, and real-world testing across millions of users. None of what we've shared here is universal truth—but these are the patterns that worked for us. If they help you avoid even one painful iteration, then this post did its job.&lt;/p&gt;
    &lt;p&gt;The agentic future will be built one context at a time. Engineer them well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Less structure,&lt;lb/&gt; more intelligence.&lt;/head&gt;
    &lt;p&gt;© 2025 Manus AI · Singapore.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352901</guid><pubDate>Tue, 23 Sep 2025 21:20:15 +0000</pubDate></item><item><title>From Rust to reality: The hidden journey of fetch_max</title><link>https://questdb.com/blog/rust-fetch-max-compiler-journey/</link><description>&lt;doc fingerprint="1c018251a0ff3b2c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Rust to Reality: The Hidden Journey of fetch_max&lt;/head&gt;
    &lt;head rend="h2"&gt;How a Job Interview Sent Me Down a Compiler Rabbit Hole&lt;/head&gt;
    &lt;p&gt;I occasionally interview candidates for engineering roles. We need people who understand concurrent programming. One of our favorite questions involves keeping track of a maximum value across multiple producer threads - a classic pattern that appears in many real-world systems.&lt;/p&gt;
    &lt;p&gt;Candidates can use any language they want. In Java (the language I know best), you might write a CAS loop, or if you're feeling functional, use &lt;code&gt;updateAndGet()&lt;/code&gt; with a lambda:&lt;/p&gt;
    &lt;quote&gt;AtomicLong highScore = new AtomicLong(100);[...]highScore.updateAndGet(current -&amp;gt; Math.max(current, newScore));&lt;/quote&gt;
    &lt;p&gt;But that lambda is doing work - it's still looping under the hood, retrying if another thread interferes. You can see the loop right in AtomicLong's source code.&lt;/p&gt;
    &lt;p&gt;Then one candidate chose Rust.&lt;/p&gt;
    &lt;p&gt;I was following along as he started typing, expecting to see either an explicit CAS loop or some functional wrapper around one. But instead, he just wrote:&lt;/p&gt;
    &lt;quote&gt;high_score.fetch_max(new_score, Ordering::Relaxed);&lt;/quote&gt;
    &lt;p&gt;"Rust has fetch_max built in," he explained casually, moving on to the next part of the problem.&lt;/p&gt;
    &lt;p&gt;Hold on. This wasn't a wrapper around a loop pattern - this was a first-class atomic operation, sitting right there next to &lt;code&gt;fetch_add&lt;/code&gt; and &lt;code&gt;fetch_or&lt;/code&gt;. Java
doesn't have this. C++ doesn't have this. How could Rust just... have this?&lt;/p&gt;
    &lt;p&gt;After the interview, curiosity got the better of me. Why would Rust provide &lt;code&gt;fetch_max&lt;/code&gt; as a built-in intrinsic? Intrinsics usually exist to leverage
specific hardware instructions. But x86-64 doesn't have an &lt;code&gt;atomic max&lt;/code&gt;
instruction. So there had to be a CAS loop somewhere in the pipeline. Unless...
maybe some architectures do have this instruction natively? And if so, how
does the same Rust code work on both?&lt;/p&gt;
    &lt;p&gt;I had to find out. Was the loop in Rust's standard library? Was it in LLVM? Was it generated during code generation for x86-64?&lt;/p&gt;
    &lt;p&gt;So I started digging. What I found was a fascinating journey through five distinct layers of compiler transformations, each one peeling back another level of abstraction, until I found exactly where that loop materialized. Let me share what I discovered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 1: The Rust Code&lt;/head&gt;
    &lt;p&gt;Let's start with what that candidate wrote - a simple high score tracker that can be safely updated from multiple threads:&lt;/p&gt;
    &lt;quote&gt;use std::sync::atomic::{AtomicU64, Ordering};fn main() {let high_score = AtomicU64::new(100);// [...]// Another thread reports a new score of 200let _old_score = high_score.fetch_max(200, Ordering::Relaxed);// [...]}// Save this snippet as `main.rs` we are going to use it later.&lt;/quote&gt;
    &lt;p&gt;This single line does exactly what it promises: atomically fetches the current value, compares it with the new one, updates it if the new value is greater, and returns the old value. It's safe, concise, and impossible to mess up. No explicit loops, no retry logic visible anywhere. But how does it actually work under the hood?&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 2: The Macro Expansion&lt;/head&gt;
    &lt;p&gt;Before our &lt;code&gt;fetch_max&lt;/code&gt; call even reaches anywhere close to machine code generation,
there's another layer of abstraction at work. The &lt;code&gt;fetch_max&lt;/code&gt; method isn't hand-written
for each atomic type - it's generated by a Rust macro called &lt;code&gt;atomic_int!&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we peek into Rust's standard library source code, we find that &lt;code&gt;AtomicU64&lt;/code&gt;
and all its methods are actually created by
this macro:&lt;/p&gt;
    &lt;quote&gt;atomic_int! {cfg(target_has_atomic = "64"),// ... various configuration attributes ...atomic_umin, atomic_umax, // The intrinsics to use8, // Alignmentu64 AtomicU64 // The type to generate}&lt;/quote&gt;
    &lt;p&gt;Inside this macro, &lt;code&gt;fetch_max&lt;/code&gt; is defined as a
template
that works for any integer type:&lt;/p&gt;
    &lt;quote&gt;pub fn fetch_max(&amp;amp;self, val: $int_type, order: Ordering) -&amp;gt; $int_type {// SAFETY: data races are prevented by atomic intrinsics.unsafe { $max_fn(self.v.get(), val, order) }}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;$max_fn&lt;/code&gt; placeholder gets replaced with &lt;code&gt;atomic_umax&lt;/code&gt; for unsigned types
and &lt;code&gt;atomic_max&lt;/code&gt; for signed types. This single macro definition generates
&lt;code&gt;fetch_max&lt;/code&gt; methods for &lt;code&gt;AtomicI8&lt;/code&gt;, &lt;code&gt;AtomicU8&lt;/code&gt;, &lt;code&gt;AtomicI16&lt;/code&gt;, &lt;code&gt;AtomicU16&lt;/code&gt;, and so
on - all the way up to &lt;code&gt;AtomicU128&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So our simple &lt;code&gt;fetch_max&lt;/code&gt; call is actually invoking generated code. But what
does the &lt;code&gt;atomic_umax&lt;/code&gt; function actually do? To answer that, we need
to see what the Rust compiler produces next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 3: LLVM IR&lt;/head&gt;
    &lt;p&gt;Now that we know &lt;code&gt;fetch_max&lt;/code&gt; is macro-generated code calling &lt;code&gt;atomic_umax&lt;/code&gt;,
let's see what happens when the Rust compiler processes it. The compiler
doesn't go straight to assembly. First, it translates the code into an
intermediate representation. Rust uses the LLVM compiler project, so it
generates LLVM Intermediate Representation (IR).&lt;/p&gt;
    &lt;p&gt;If we peek at the LLVM IR for our &lt;code&gt;fetch_max&lt;/code&gt; call, we see something like this:&lt;/p&gt;
    &lt;quote&gt;; Before the transformationbb7:%0 = atomicrmw umax ptr %self, i64 %val monotonic, align 8...&lt;/quote&gt;
    &lt;p&gt;This is LLVM's language for saying: "I need an atomic read-modify-write operation. The modification I want to perform is an unsigned maximum."&lt;/p&gt;
    &lt;p&gt;This is a powerful, high-level instruction within the compiler itself. But it poses a critical question: does the CPU actually have a single instruction called &lt;code&gt;umax&lt;/code&gt;? For most architectures, the answer is no. So how does the
compiler bridge this gap?&lt;/p&gt;
    &lt;head rend="h3"&gt;How to See This Yourself&lt;/head&gt;
    &lt;p&gt;My goal is not to merely describe what is happening, but to give you the tools to see it for yourself. You can trace this transformation step-by-step on your own machine.&lt;/p&gt;
    &lt;p&gt;First, tell the Rust compiler to stop after generating the LLVM IR:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=llvm-ir main.rs&lt;/quote&gt;
    &lt;p&gt;This creates a &lt;code&gt;main.ll&lt;/code&gt; file. This file contains the LLVM IR
representation of your Rust code, including our &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.
Keep the file around; we'll use it in the next steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude: Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;We're missing something important. How does the Rust function &lt;code&gt;atomic_umax&lt;/code&gt;
actually become the LLVM instruction &lt;code&gt;atomicrmw umax&lt;/code&gt;? This is where compiler
intrinsics come into play.&lt;/p&gt;
    &lt;p&gt;If you dig into Rust's source code, you'll find that &lt;code&gt;atomic_umax&lt;/code&gt; is
defined like this:&lt;/p&gt;
    &lt;quote&gt;/// Updates `*dst` to the max value of `val` and the old value (unsigned comparison)#[inline]#[cfg(target_has_atomic)]#[cfg_attr(miri, track_caller)] // even without panics, this helps for Miri backtracesunsafe fn atomic_umax&amp;lt;T: Copy&amp;gt;(dst: *mut T, val: T, order: Ordering) -&amp;gt; T {// SAFETY: the caller must uphold the safety contract for `atomic_umax`unsafe {match order {Relaxed =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Relaxed }&amp;gt;(dst, val),Acquire =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Acquire }&amp;gt;(dst, val),Release =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Release }&amp;gt;(dst, val),AcqRel =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::AcqRel }&amp;gt;(dst, val),SeqCst =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::SeqCst }&amp;gt;(dst, val),}}}&lt;/quote&gt;
    &lt;p&gt;But what is this &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt; function? If you look at its
definition,
you find something slightly unusual:&lt;/p&gt;
    &lt;quote&gt;/// Maximum with the current value using an unsigned comparison./// `T` must be an unsigned integer type.////// The stabilized version of this intrinsic is available on the/// [`atomic`] unsigned integer types via the `fetch_max` method. For example, [`AtomicU32::fetch_max`].#[rustc_intrinsic]#[rustc_nounwind]pub unsafe fn atomic_umax&amp;lt;T: Copy, const ORD: AtomicOrdering&amp;gt;(dst: *mut T, src: T) -&amp;gt; T;&lt;/quote&gt;
    &lt;p&gt;There is no body. This is a declaration, not a definition. The &lt;code&gt;#[rustc_intrinsic]&lt;/code&gt; attribute tells the Rust compiler that this function
maps directly to a low-level operation understood by the compiler
itself. When the Rust compiler sees a call to &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt;, it
knows to
replace it
with the corresponding
LLVM intrinsic function.&lt;/p&gt;
    &lt;p&gt;So our journey actually looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;fetch_max&lt;/code&gt;method (user-facing API)&lt;/item&gt;
      &lt;item&gt;Macro expands to call &lt;code&gt;atomic_umax&lt;/code&gt;function&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;atomic_umax&lt;/code&gt;is a compiler intrinsic&lt;/item&gt;
      &lt;item&gt;Rustc replaces the intrinsic with LLVM's &lt;code&gt;atomicrmw umax&lt;/code&gt;← We are here&lt;/item&gt;
      &lt;item&gt;LLVM processes this instruction...&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Layer 4: The Transformation&lt;/head&gt;
    &lt;p&gt;LLVM runs a series of "passes" that analyze and transform the code. The one we're interested in is called the &lt;code&gt;AtomicExpandPass&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Its job is to look at high-level atomic operations like &lt;code&gt;atomicrmw umax&lt;/code&gt; and ask
the target architecture, "Can you do this natively?"&lt;/p&gt;
    &lt;p&gt;When the &lt;code&gt;x86-64&lt;/code&gt; backend says "No, I can't," this pass expands the single
instruction into a sequence of more fundamental ones that the CPU does
understand. The result is a
compare-and-swap (CAS) loop.&lt;/p&gt;
    &lt;p&gt;We can see this transformation in action by asking LLVM to emit the intermediate representation before and after this pass. To see the IR before the &lt;code&gt;AtomicExpandPass&lt;/code&gt;, run:&lt;/p&gt;
    &lt;quote&gt;llc -print-before=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Tip: If you do not have&lt;/p&gt;&lt;code&gt;llc&lt;/code&gt;installed, you can ask&lt;code&gt;rustc&lt;/code&gt;to run the pass for you directly.&lt;code&gt;rustc -C llvm-args="-print-before=atomic-expand -print-after=atomic-expand" main.rs&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;The code will be printed to your terminal. The function containing our atomic max looks like this:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump Before Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = atomicrmw umax ptr %self, i64 %val monotonic, align 8store i64 %2, ptr %_0, align 8br label %bb1bb5: ; preds = %start%3 = atomicrmw umax ptr %self, i64 %val release, align 8store i64 %3, ptr %_0, align 8br label %bb1bb6: ; preds = %start%4 = atomicrmw umax ptr %self, i64 %val acquire, align 8store i64 %4, ptr %_0, align 8br label %bb1bb4: ; preds = %start%5 = atomicrmw umax ptr %self, i64 %val acq_rel, align 8store i64 %5, ptr %_0, align 8br label %bb1bb3: ; preds = %start%6 = atomicrmw umax ptr %self, i64 %val seq_cst, align 8store i64 %6, ptr %_0, align 8br label %bb1bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;You can see the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction in multiple places, depending on
the memory ordering specified. This is the high-level atomic operation that the
compiler backend understands, but the CPU does not.&lt;/p&gt;
    &lt;quote&gt;llc -print-after=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;p&gt;This is the relevant part of the output:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump After Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = load i64, ptr %self, align 8 ; seed expected valuebr label %atomicrmw.start ; enter CAS loopatomicrmw.start: ; preds = %atomicrmw.start, %bb7%loaded = phi i64 [ %2, %bb7 ], [ %newloaded, %atomicrmw.start ] ; on first iteration: use %2, on retries: use value observed by last cmpxchg%3 = icmp ugt i64 %loaded, %val ; unsigned compare (umax semantics)%new = select i1 %3, i64 %loaded, i64 %val ; desired = max(loaded, val)%4 = cmpxchg ptr %self, i64 %loaded, i64 %new monotonic monotonic, align 8 ; CAS: if *self==loaded, store new%success = extractvalue { i64, i1 } %4, 1 ; boolean: whether the swap happened%newloaded = extractvalue { i64, i1 } %4, 0 ; value seen in memory before the CASbr i1 %success, label %atomicrmw.end, label %atomicrmw.start ; loop until CAS succeedsatomicrmw.end: ; preds = %atomicrmw.startstore i64 %newloaded, ptr %_0, align 8br label %bb1[... MORE OF THE SAME, JUST FOR DIFFERENT ORDERING..]bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;We can see the pass did not change the first part - it still has the code to dispatch based on the memory ordering. But in the &lt;code&gt;bb7&lt;/code&gt; block, where we originally had the
&lt;code&gt;atomicrmw umax&lt;/code&gt; LLVM instruction, we now see a full compare-and-swap loop.
A compiler engineer would say that the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction has been
"lowered" into a sequence of more primitive operations, that are closer to what
the hardware can actually execute.&lt;/p&gt;
    &lt;p&gt;Here's the simplified logic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read (seed): grab the current value (&lt;code&gt;expected&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Compute: &lt;code&gt;desired = umax(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Attempt: &lt;code&gt;observed, success = cmpxchg(ptr, expected, desired, [...])&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If success, return &lt;code&gt;observed&lt;/code&gt;(the old value). Otherwise&lt;code&gt;set expected = observed&lt;/code&gt;and loop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This CAS loop is a fundamental pattern in lock-free programming. The compiler just built it for us automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 5: The Final Product (x86-64 Assembly)&lt;/head&gt;
    &lt;p&gt;We're at the final step. To see the final machine code, you can tell &lt;code&gt;rustc&lt;/code&gt; to
emit the assembly directly:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=asm main.rs&lt;/quote&gt;
    &lt;p&gt;This will produce a &lt;code&gt;main.s&lt;/code&gt; file containing the final assembly code.
Inside, you'll find the result of the &lt;code&gt;cmpxchg&lt;/code&gt; loop:&lt;/p&gt;
    &lt;quote&gt;.LBB8_2:movq -32(%rsp), %rax # rax = &amp;amp;selfmovq (%rax), %rax # rax = *self (seed 'expected')movq %rax, -48(%rsp) # spill expected to stack.LBB8_3: # loop headmovq -48(%rsp), %rax # rax = expectedmovq -32(%rsp), %rcx # rcx = &amp;amp;selfmovq -40(%rsp), %rdx # rdx = valmovq %rax, %rsi # rsi = expected (scratch)subq %rdx, %rsi # set flags for unsigned compare: expected - valcmovaq %rax, %rdx # if (expected &amp;gt; val) rdx = expected; else rdx = val (compute max)lock cmpxchgq %rdx, (%rcx)# CAS: if *rcx==rax then *rcx=rdx; rax &amp;lt;- old *rcx; ZF=successsete %cl # cl = successmovq %rax, -56(%rsp) # spill observed to stacktestb $1, %cl # branch on successmovq %rax, -48(%rsp) # expected = observed (for retry)jne .LBB8_4 # success -&amp;gt; exitjmp .LBB8_3 # failure → retry&lt;/quote&gt;
    &lt;p&gt;The syntax might look a bit different from what you're used to, that's because it's in AT&amp;amp;T syntax, which is the default for &lt;code&gt;rustc&lt;/code&gt;. If you prefer Intel syntax, you can
use &lt;code&gt;rustc --emit=asm main.rs -C "llvm-args=-x86-asm-syntax=intel"&lt;/code&gt; to get that.&lt;/p&gt;
    &lt;p&gt;I'm not an assembly expert, but you can see the key parts of the CAS loop here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seed read (first iteration): Load &lt;code&gt;*self&lt;/code&gt;once to initialize the expected value.&lt;/item&gt;
      &lt;item&gt;Compute umax without branching: The pair &lt;code&gt;sub&lt;/code&gt;+&lt;code&gt;cmova&lt;/code&gt;implements&lt;code&gt;desired = max_u(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CAS operation: On x86-64, &lt;code&gt;cmpxchg&lt;/code&gt;uses&lt;code&gt;RAX&lt;/code&gt;as the expected value and returns the observed value in&lt;code&gt;RAX&lt;/code&gt;;&lt;code&gt;ZF&lt;/code&gt;encodes success.&lt;/item&gt;
      &lt;item&gt;Retry or finish: If &lt;code&gt;ZF&lt;/code&gt;is clear, we failed and need to retry. Otherwise, we are done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note we did not ask&lt;/p&gt;&lt;code&gt;rustc&lt;/code&gt;to optimize the code. If we did, the compiler would generate more efficient assembly: No spills to the stack, fewer jumps, no dispatch on memory ordering, etc. But I wanted to keep the output as close to the original IR as possible to make it easier to follow.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Beauty of Abstraction&lt;/head&gt;
    &lt;p&gt;And there we have it. Our journey is complete. We started with a safe, clear, single line of Rust and ended with a CAS loop written in assembly language.&lt;/p&gt;
    &lt;p&gt;Rust &lt;code&gt;fetch_max&lt;/code&gt; → Macro-generated &lt;code&gt;atomic_umax&lt;/code&gt; → LLVM
&lt;code&gt;atomicrmw umax&lt;/code&gt; → LLVM &lt;code&gt;cmpxchg&lt;/code&gt; loop → Assembly &lt;code&gt;lock cmpxchg&lt;/code&gt; loop&lt;/p&gt;
    &lt;p&gt;This journey is a perfect example of the power of modern compilers. We get to work at a high level of abstraction, focusing on safety and logic, while the compiler handles the messy, error-prone, and incredibly complex task of generating correct and efficient code for the hardware.&lt;/p&gt;
    &lt;p&gt;So, next time you use an atomic, take a moment to appreciate the incredible, hidden journey your code is about to take.&lt;/p&gt;
    &lt;p&gt;PS: After conducting this journey I learned that C++26 adds &lt;code&gt;fetch_max&lt;/code&gt;
too!&lt;/p&gt;
    &lt;p&gt;PPS: We are hiring!&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: Apple Silicon (AArch64)&lt;/head&gt;
    &lt;p&gt;Out of curiosity, I also checked how this looks on Apple Silicon (AArch64). This architecture does have a native &lt;code&gt;atomic max&lt;/code&gt; instruction, so the
&lt;code&gt;AtomicExpandPass&lt;/code&gt; does not need to lower it into a CAS loop. The LLVM code before and after
the pass is identical, still containing the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;The final assembly contains a variant of the &lt;code&gt;LDUMAX&lt;/code&gt; instruction. This is the relevant part of the assembly:&lt;/p&gt;
    &lt;quote&gt;ldr x8, [sp, #16] # x8 = value to compare withldr x9, [sp, #8] # x9 = pointer to the atomic variableldumax x8, x8, [x9] # atomic unsigned max (relaxed), [x9] = max(x8, [x9]), x8 = old valuestr x8, [sp, #40] # Store old valueb LBB8_11&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that AArch64 uses Unified Assembler Language, when reading the snippet above, it's important to remember that the destination register comes first.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's really it. We could continue to dig into the microarchitecture, to see how instructions are executed at the hardware level, what are the effects of the &lt;code&gt;LOCK&lt;/code&gt; prefix, dive into differences in memory ordering, etc.
But we'll leave that for another day.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alice: "Would you tell me, please, which way I ought to go from here?"&lt;/p&gt;&lt;lb/&gt;The Cat: "That depends a good deal on where you want to get to."&lt;lb/&gt;Alice: "I don't much care where."&lt;lb/&gt;The Cat: "Then it doesn't much matter which way you go."&lt;lb/&gt;Alice: "...So long as I get somewhere."&lt;lb/&gt;The Cat: "Oh, you're sure to do that, if only you walk long enough."&lt;p&gt;- Lewis Carroll, Alice's Adventures in Wonderland&lt;/p&gt;&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352944</guid><pubDate>Tue, 23 Sep 2025 21:24:45 +0000</pubDate></item><item><title>NYC Telecom Raid: What's Up with Those Weird SIM Banks?</title><link>https://tedium.co/2025/09/23/secret-service-raid-sim-bank-telecom-hardware/</link><description>&lt;doc fingerprint="26218114519a236e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SIMmetry&lt;/head&gt;
    &lt;head rend="h2"&gt;A recent Secret Service raid uncovers an insane network of SIM cards—along with perhaps the most unusual piece of hardware I’ve ever seen. Here’s the deal with the SIM bank.&lt;/head&gt;
    &lt;p&gt;When I learned that the Secret Service had taken down a giant “SIM farm” in the NYC area, I immediately had two thoughts: One, “Wow, that sounds like the reason we all get so many scam calls.” And two, “Holy crap, what is that weird-ass piece of hardware?!?!??!?!??!?!??!?!?”&lt;/p&gt;
    &lt;p&gt;You must understand, dear reader, the bizarre gear they were using. I’ve never seen anything like it before.&lt;/p&gt;
    &lt;p&gt;Much will be written about the threat to the telecom system, which is the angle the Secret Service is taking, as it was uncovered right around the time of a United Nations General Assembly meeting. I want to know the deal with the hardware itself.&lt;/p&gt;
    &lt;p&gt;You know the old board game Guess Who? You know, with the cards that stick up, and the other player has to guess what faces you have? Imagine that times 100, but with the cards a 20th of the size of the Guess Who cards, and add a whole freaking ton of antennas into the mix, and you have this crazy-ass device, the niche-iest of niche electronic devices. Each device holds numerous SIM cards, which means that someone had to pop out thousands of SIMs to put in these boxes, presumably one at a time.&lt;/p&gt;
    &lt;p&gt;Fortunately for us, the U.S. Secret Service gave us a picture of that insanity, too:&lt;/p&gt;
    &lt;p&gt;So basically, we have a device that is intended to hold literal hundreds of SIM cards, and apparently the people who ran this network had literal racks of these machines. They have this almost magical sense of symmetry to them, which makes them highly attractive to nerds like me. It reminds me of Aereo, the noble (but failed) attempt to use thousands of tiny antennas to capture broadcast television signals to resell online.&lt;/p&gt;
    &lt;head rend="h5"&gt;Sponsored By … You?&lt;/head&gt;
    &lt;p&gt;If you find weird or unusual topics like this super-fascinating, the best way to tell us is to give us a nod on Ko-Fi. It helps ensure that we can keep this machine moving, support outside writers, and bring on the tools to support our writing. (Also it’s heartening when someone chips in.)&lt;/p&gt;
    &lt;p&gt;We accept advertising, too! Check out this page to learn more.&lt;/p&gt;
    &lt;p&gt;So, what the heck is this thing, why did they have so many of them, and how come you’ve never seen them before?&lt;/p&gt;
    &lt;p&gt;The short answer: It’s a device called a “SIMbank” or “SIM gateway,” often attached to a “SIM pool,” which gives all those SIM cards access to a cellular network.&lt;/p&gt;
    &lt;p&gt;The longer answer: The devices in the Secret Service photo, apparently made by a Chinese company called Ejoin Technology, are used in VoIP settings to handle lots of SIM cards. Ejoin says they produce the devices for what it calls “SMS and voice gateway solutions.” In other words, these boxes made it possible to mass-text and mass-call people. They are not cheap devices, costing in the thousands of dollars. And that’s before you get in the business of purchasing all those SIM cards.&lt;/p&gt;
    &lt;p&gt;The exact devices that the Secret Service found are sold by Ejoin for an eye-watering $3,730. Here’s a press image of one:&lt;/p&gt;
    &lt;p&gt;With devices like these, you can text someone at one number and immediately switch to another using the same cellular line, as if you changed area codes on the fly. Which sounds great for marketing, but also great for spam, and even better for harassment.&lt;/p&gt;
    &lt;p&gt;(It should be noted that Ejoin is not alone in selling these. I also spotted them being sold by Etross Telecom, OpenVox, and China Skyline Telecom. These are defiantly obscure but presumably have a use case.)&lt;/p&gt;
    &lt;p&gt;If you think these devices seem sketchy, apparently Alibaba does as well. If you look up messages on Alibaba for Ejoin Technology’s products, you get a generic logo, and this message that appears:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Due to the website’s compliance with specific regulations or policies in China, product information is no longer publicly displayed, but purchasing or payment operations can still be carried out. If you require detailed product information or link, please contact the sales department OR move to Ejointech offical Website.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So, if you buy these objects via Alibaba, you are literally buying a $3,700 device from a black box. On the plus side, going to Ejoin’s website, you can actually see screenshots of the tech in action:&lt;/p&gt;
    &lt;p&gt;In this context, these are basically spam machines, and whoever ran this network—whether a state actor or a criminal scheme—had dozens of them, each costing the price of a high-end laptop. The SIM cards themselves probably cost like $5-$10 a piece, maybe more, which means that just filling them up with cards likely cost thousands more. Plus, there’s the manual labor of it all. 256 SIM cards don’t put themselves into a SIMbank.&lt;/p&gt;
    &lt;p&gt;(Side note: When I searched for information on how to buy bulk SIM cards, one of the first sites that came up was a black-hat hacking forum in which a user asked the very same question. Which, to my friends in the black-hat hacking world, hello.)&lt;/p&gt;
    &lt;p&gt;Now, to be clear, there are some legitimate reasons for users to have them, particularly for testing and quality assurance across networks. (Say, if you’re concerned that your app might work differently on Verizon than it might on AT&amp;amp;T or T-Mobile, or if you’re doing a lot of edge computing. Perhaps a legitimate VoIP company has a few for whatever reason.) And I did find a user on Medium who posted why they built a SIM bank solution for their marketing team. But illegitimate use cases appear to dwarf the legitimate ones, at least in terms of public attention.&lt;/p&gt;
    &lt;p&gt;The case in New York is far from unique. Earlier this year, Interpol broke up a SIM bank fraud scheme in South Africa that involved 40 people and more than 1,000 cards. The cards were used to reroute international traffic as local traffic to make the calls look legitimate. And a spate of cases both targeting and based in India have emerged in recent months.&lt;/p&gt;
    &lt;p&gt;(By the way, if you find this topic interesting, you might want to check out the Indian cybersecurity news outlet The 420, which appears to be on top of this.)&lt;/p&gt;
    &lt;p&gt;Beyond the sheer scope of SIM cards that the network had, the fact that the Secret Service uncovered the network around New York City is perhaps the most interesting part. It suggests that we might see more tricks like this in the future.&lt;/p&gt;
    &lt;p&gt;Anyway, if you see one of these boxes lying around somewhere, filled to the brim with SIMs, odds are you might be in the vicinity of something sketchy. (One has to wonder if the rise of eSIMs is designed to make these products obsolete.)&lt;/p&gt;
    &lt;p&gt;As criminal as they might be depending on the situation, they admittedly look cool.&lt;/p&gt;
    &lt;head rend="h5"&gt;SIMless Links&lt;/head&gt;
    &lt;p&gt;RIP Billy Hudson, a co-host of the popular YouTube channel The Game Chasers. He meant a lot to the retro gaming community, and went out amid some very serious health issues. A telling thing about Hudson is that the last video he posted before he died, created immediately after undergoing brain surgery, involved him advising his followers not to fall for crowdfunding scams. He didn’t have to do that; nobody would have blamed him. Yet he did.&lt;/p&gt;
    &lt;p&gt;If you’ve never seen this piece of found media, you’re in for a treat. It’s a video of Elliott Smith performing on Breakfast Time, a bizarre morning show hosted on the original iteration of the FX network. (As the video notes, it was a performance from well before Smith was famous.) After getting peppered with numerous demeaning questions by co-host Tom Bergeron (later of Dancing With The Stars fame), Smith pulls off a performance of “Clementine” that silences the room and presumably made Bergeron rethink his life choices. Oh, there’s a freaking puppet behind him as he’s playing.&lt;/p&gt;
    &lt;p&gt;I don’t know why the German gummy-makers Haribo are making some of the best power banks on the market, but apparently they are—and serious backpackers love them.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;Find this one an interesting read? Share it with a pal! And to anyone with one of these devices: Please don’t spam me, thanks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354262</guid><pubDate>Tue, 23 Sep 2025 23:36:16 +0000</pubDate></item><item><title>MLB approves robot umpires for 2026 as part of challenge system</title><link>https://www.espn.com/mlb/story/_/id/46357017/mlb-approves-robot-umpires-2026-part-challenge-system</link><description>&lt;doc fingerprint="a5a0613b892b831e"&gt;
  &lt;main&gt;
    &lt;p&gt;Major League Baseball will implement a challenge system for balls and strikes in the 2026 season after the league's competition committee voted Tuesday to usher in the era of robot umpiring.&lt;/p&gt;
    &lt;p&gt;Following years of testing in the minor leagues, as well as during spring training and at this year's All-Star Game, MLB forged ahead with a system that will give teams two challenges per game.&lt;/p&gt;
    &lt;p&gt;Hitters, pitchers and catchers will be the only ones allowed to trigger the system by tapping their head, and if a challenge is successful -- the pitch will be shown on in-stadium videoboards -- teams will retain it.&lt;/p&gt;
    &lt;p&gt;While the vote in favor of the automated ball-strike challenge system was not unanimous -- some of the four players on the 11-man committee voted no, according to sources -- the vote was a fait accompli, with MLB owners all in favor and in possession of a six-seat majority on the committee.&lt;/p&gt;
    &lt;p&gt;"I commend the Joint Competition Committee for striking the right balance of preserving the integral role of the umpire in the game with the ability to correct a missed call in a high-leverage situation, all while preserving the pace and rhythm of the game," commissioner Rob Manfred said Tuesday in a statement.&lt;/p&gt;
    &lt;p&gt;The ABS system uses similar technology to the line-calling system in tennis, with 12 cameras in each ballpark tracking the ball with a margin of error around one-sixth of an inch. The ABS zone will be a two-dimensional plane in the middle of the plate that spans its full width (17 inches). The zone's top will be 53.5% of a player's height and the bottom 27%.&lt;/p&gt;
    &lt;p&gt;Teams that run out of challenges over the first nine innings will be granted an extra challenge in the 10th inning, while those that still have unused challenges will simply carry them into extras. If a team runs out of challenges in the 10th, it will automatically receive another in the 11th -- a rule that extends for any extra inning.&lt;/p&gt;
    &lt;p&gt;During the league's spring training test this season, teams combined to average around four challenges per game and succeeded 52.2% of the time, according to the league. Catchers, whose value in framing pitches outside the zone to look like strikes could take a hit due to the new rule, were the most successful at a 56% overturn rate, while hitters were correct 50% of the time and pitchers 41%.&lt;/p&gt;
    &lt;p&gt;MLB's minor league testing, which started in 2021, led to Triple-A players in 2023 using ABS challenge three days a week and a full ABS system, with every pitch adjudicated by computer, the other three.&lt;/p&gt;
    &lt;p&gt;Support among league executives grew around the challenge system as the more palatable of the two options for fans, allowing for umpires still to play a role in balls and strikes but to have a backup system in case of blown calls in integral moments.&lt;/p&gt;
    &lt;p&gt;Adding the robot umps is likely to cut down on ejections. MLB said 61.5% of ejections among players, managers and coaches last year were related to balls and strikes, as were 60.3% this season through Sunday. The figures include ejections for derogatory comments, throwing equipment while protesting calls and inappropriate conduct.&lt;/p&gt;
    &lt;p&gt;Big league umpires call roughly 94% of pitches correctly, according to UmpScorecards.&lt;/p&gt;
    &lt;p&gt;Management officials on the competition committee include Seattle chairman John Stanton, St. Louis CEO Bill DeWitt Jr., San Francisco chairman Greg Johnson, Colorado CEO Dick Monfort, Toronto CEO Mark Shapiro and Boston chairman Tom Werner.&lt;/p&gt;
    &lt;p&gt;Players include Arizona's Corbin Burnes and Zac Gallen, Seattle's Cal Raleigh and the New York Yankees' Austin Slater, with the Chicago Cubs' Ian Happ and Detroit's Casey Mize as alternates. The union representatives make their decisions based on input from players on the 30 teams.&lt;/p&gt;
    &lt;p&gt;Bill Miller is the umpire representative.&lt;/p&gt;
    &lt;p&gt;The Associated Press contributed to this report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354304</guid><pubDate>Tue, 23 Sep 2025 23:41:54 +0000</pubDate></item><item><title>Top Programming Languages 2025</title><link>https://spectrum.ieee.org/top-programming-languages-2025</link><description>&lt;doc fingerprint="ff481adb873c88f9"&gt;
  &lt;main&gt;&lt;p&gt;Since 2013, we’ve been metaphorically peering over the shoulders of programmers to create our annual interactive rankings of the most popular programming languages. But fundamental shifts in how people are coding may not just make it harder to measure popularity, but could even make the concept itself irrelevant. And then things might get really weird. To see why, let’s start with this year’s rankings and a quick refresher of how we put this thing together.&lt;/p&gt;&lt;p&gt;In the “Spectrum” default ranking, which is weighted with the interests of IEEE members in mind, we see that once again Python has the top spot, with the biggest change in the top five being JavaScript’s drop from third place last year to sixth place this year. As JavaScript is often used to create web pages, and vibe coding is often used to create websites, this drop in the apparent popularity may be due to the effects of AI that we’ll dig into in a moment. But first to finish up with this year’s scores, in the “Jobs” ranking, which looks exclusively at what skills employers are looking for, we see that Python has also taken 1st place, up from second place last year, though SQL expertise remains an incredibly valuable skill to have on your resume.&lt;/p&gt;&lt;p&gt;Because we can’t literally look over the shoulders of everyone who codes, including kids hacking on Minecraft servers or academic researchers developing new architectures, we rely on proxies to measure popularity. We detail our methodology here, but the upshot is that we merge metrics from multiple sources to create our rankings. The metrics we choose publicly signal interest across a wide range of languages—Google search traffic, questions asked on Stack Exchange, mentions in research papers, activity on the GitHub open source code repository, and so on.&lt;/p&gt;&lt;p&gt;But programmers are turning away from many of these public expressions of interest. Rather than page through a book or search a website like Stack Exchange for answers to their questions, they’ll chat with an LLM like Claude or ChatGPT in a private conversation. And with an AI assistant like Cursor helping to write code, the need to pose questions in the first place is significantly decreased. For example, across the total set of languages evaluated in the TPL, the number of questions we saw posted per week on Stack Exchange in 2025 was just 22 percent of what it was in 2024.&lt;/p&gt;&lt;p&gt;With less signal in publicly available metrics, it becomes harder to track popularity across a broad range of languages. This existential problem for our rankings can be tackled by searching for new metrics, or trying to survey programmers—in all their variety—directly. However, an even more fundamental problem is looming in the wings.&lt;/p&gt;&lt;p&gt;Whether it’s a seasoned coder using an AI to handle the grunt work, or a neophyte vibe coding a complete web app, AI assistance means that programmers can concern themselves less and less with the particulars of any language. First details of syntax, then flow control and functions, and so on up the levels of how a program is put together—more and more is being left to the AI.&lt;/p&gt;&lt;p&gt;Although code-writing LLM’s are still very much a work in progress, as they take over an increasing share of the work, programmers inevitably shift from being the kind of people willing to fight religious wars over whether source code should be indented by typing tabs or spaces to people who care less and less about what language is used.&lt;/p&gt;&lt;p&gt;After all, the whole reason different computer languages exist is because given a particular challenge, it’s easier to express a solution in one language versus another. You wouldn’t control a washing machine using the R programming language, or conversely do a statistical analysis on large datasets using C.&lt;/p&gt;&lt;p&gt;But it is technically possible to do both. A human might tear their hair out doing it, but LLMs have about as much hair as they do sentience. As long as there’s enough training data, they’ll generate code for a given prompt in any language you want. In practical terms, this means using one—any one—of today’s most popular general purpose programming languages. In the same way most developers today don’t pay much attention to the instruction sets and other hardware idiosyncrasies of the CPUs that their code runs on, which language a program is vibe coded in ultimately becomes a minor detail.&lt;/p&gt;&lt;p&gt;Sure, there will always be some people who care, just as today there are nerds like me willing to debate the merits of writing for the Z80 versus the 6502 8-bit CPUs. But overall, the popularity of different computer languages could become as obscure a topic as the relative popularity of railway track gauges.&lt;/p&gt;&lt;p&gt;One obvious long-term consequence to this is that it will become harder for new languages to emerge. Previously, new languages could emerge from individuals or small teams evangelizing their approach to potential contributors and users. Presentations, papers, demos, sample code and tutorials seeded new developer ecosystems. A single well-written book, like Leo Brodie’s Starting Forth or Brian Kernighan and Dennis Ritchies’ The C Programming Language, could make an enormous difference to a language’s popularity.&lt;/p&gt;&lt;p&gt;But while a few samples and a tutorial can be enough material to jump-start adoption among programmers familiar with the ins and outs of hands-on coding, it’s not enough for today’s AIs. Humans build mental models that can extrapolate from relatively small amounts of data. LLMs rely on statistical probabilities, so the more data they can crunch, they better they are. Consequently programmers have noted that AIs give noticeably poorer results when trying to code in less-used languages.&lt;/p&gt;&lt;p&gt;There are research efforts to make LLMs more universal coders, but that doesn’t really help new languages get off the ground. Fundamentally new languages grow because they are scratching some itch a programmer has. That itch can be as small as being annoyed at semicolons having to be placed after every statement, or as large as a philosophical argument about the purpose of computation.&lt;/p&gt;&lt;p&gt;But if an AI is soothing our irritations with today’s languages, will any new ones ever reach the kind of critical mass needed to make an impact? Will the popularity of today’s languages remain frozen in time?&lt;/p&gt;&lt;head rend="h2"&gt;What’s the future of programming languages?&lt;/head&gt;&lt;p&gt;Before speculating further about the future, let’s touch base again where we are today. Modern high-level computer languages are really designed to do two things: create an abstraction layer that makes it easier to process data in a suitable fashion, and stop programmers from shooting themselves in the foot.&lt;/p&gt;&lt;p&gt;The first objective has been around since the days of Fortran and Cobol, aimed at processing scientific and business data respectively. The second objective emerged later, spurred in no small part by Edgar Dijkstra’s 1968 paper “Go To Statement Considered Harmful.” In this he argued for eliminating the ability for a programmer to make jumps to arbitrary points in their code. This restriction was to prevent so-called spaghetti code that makes it hard for a programmer to understand how a computer actually executes a given program. Instead, Dijkstra demanded that programmers bend to structural rules imposed by the language. Dijkstra’s argument ultimately won the day, and most modern languages do indeed minimize or eliminate Go Tos altogether in favor of structures like functions and other programmatic blocks.&lt;/p&gt;&lt;p&gt;These structures don’t exist at the level of the CPU. If you look at the instruction sets for Arm, x86, or RISC-V processors, the flow of a program is controlled by just three types of machine code instructions. These are conditional jumps, unconditional jumps, and jumps with a trace stored (so you can call a subroutine and return to where you started). In other words, it’s Go Tos all the way down. Similarly, strict data types designed to label and protect data from incorrect use dissolve into anonymous bits flowing in and out of memory.&lt;/p&gt;&lt;p&gt;So how much abstraction and anti-foot-shooting structure will a sufficiently-advanced coding AI really need? A hint comes from recent research in AI-assisted hardware design, such as Dall-EM, a generative AI developed at Princeton University used to create RF and electromagnetic filters. Designing these filters has always been something of a black art, involving the wrangling of complex electromagnetic fields as they swirl around little strips of metal. But Dall-EM can take in the desired inputs and outputs and spit out something that looks like a QR code. The results are something no human would ever design—but it works.&lt;/p&gt;&lt;p&gt;Similarly, could we get our AIs to go straight from prompt to an intermediate language that could be fed into the interpreter or compiler of our choice? Do we need high-level languages at all in that future? True, this would turn programs into inscrutable black boxes, but they could still be divided into modular testable units for sanity and quality checks. And instead of trying to read or maintain source code, programmers would just tweak their prompts and generate software afresh.&lt;/p&gt;&lt;p&gt;What’s the role of the programmer in a future without source code? Architecture design and algorithm selection would remain vital skills—for example, should a pathfinding program use a classic approach like the A* algorithm, or instead should it try to implement a new method? How should a piece of software be interfaced with a larger system? How should new hardware be exploited? In this scenario, computer science degrees, with their emphasis on fundamentals over the details of programming languages, rise in value over coding boot camps.&lt;/p&gt;Will there be a Top Programming Language in 2026? Right now, programming is going through the biggest transformation since compilers broke onto the scene in the early 1950s. Even if the predictions that much of AI is a bubble about to burst come true, the thing about tech bubbles is that there’s always some residual technology that survives. It’s likely that using LLMs to write and assist with code is something that’s going to stick. So we’re going to be spending the next 12 months figuring out what popularity means in this new age, and what metrics might be useful to measure. What do you think popularity should mean? What metrics do you think we should consider? Let us know in the comments below.&lt;list rend="ul"&gt;&lt;item&gt;AI Models Embrace Humanlike Reasoning ›&lt;/item&gt;&lt;item&gt;LLM Benchmarking Shows Capabilities Doubling Every 7 Months ›&lt;/item&gt;&lt;item&gt;Why Functional Programming Should Be the Future of Software Development ›&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354314</guid><pubDate>Tue, 23 Sep 2025 23:42:50 +0000</pubDate></item><item><title>Baldur's Gate 3 Steam Deck – Native Version</title><link>https://larian.com/support/faqs/steam-deck-native-version_121</link><description>&lt;doc fingerprint="bcd4a8bd8df387c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Steam Deck - Native Version&lt;/head&gt;
    &lt;p&gt;Upon release of Hotfix #34 on your Steam Deck, your device will install the Native version.&lt;/p&gt;
    &lt;p&gt;If you are unsure whether the build has been installed correctly, you can do the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any version that has Linux Runtime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update if an update appears.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s the difference between the Steam Deck Native and Proton version?&lt;/p&gt;
    &lt;p&gt;Our Proton version runs on the Steam Deck via the Proton compatibility layer, which requires extra CPU processing power. Running the game natively on the Steam Deck requires less CPU usage and memory consumption overall!&lt;/p&gt;
    &lt;p&gt;Can I still switch back to the Proton version?&lt;/p&gt;
    &lt;p&gt;Yes. If you’re having issues with the Steam Deck Native build, you can revert to the Proton version. Take the following steps to do so:&lt;/p&gt;
    &lt;p&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any Proton version 8 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that there is a Steam Deck Native build, is Baldur’s Gate 3 supported on Linux?&lt;/p&gt;
    &lt;p&gt;Larian does not provide support for the Linux platform. The Steam Deck Native build is only supported on Steam Deck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Savegames&lt;/head&gt;
    &lt;p&gt;Where are my saves located currently (before using the Steam Deck Native version)?&lt;/p&gt;
    &lt;p&gt;Before the Steam Deck Native version becomes the primary version, your saves will be in the compatdata folder: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Where are my saves located when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;After the Steam Deck Native version becomes the primary version, your saves will be in the following folder: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Why are my saves in different folders?&lt;/p&gt;
    &lt;p&gt;When Baldur’s Gate 3 runs on the Proton compatibility layer, the Proton version will store the saves in the compatdata folder, which is a mirrored version of the Windows file storage system. On the Steam Deck Native version, the saves are stored natively on the SteamOS file storage system.&lt;/p&gt;
    &lt;p&gt;Will my savegames be transferred over to the new version when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;If your Steam Cloud saves are turned on, your most recent saves will be synced to the Steam Deck Native savegame folder automatically.&lt;/p&gt;
    &lt;p&gt;What if I don’t have Cloud saves turned on, or I want my older saves?&lt;/p&gt;
    &lt;p&gt;Your saves are still stored on the Steam Deck, but they will be stored in the compatdata folder.&lt;lb/&gt; You can manually transfer these files via the Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a mouse and keyboard to hand, plug them in to make your life a little easier, and click on the folder icon on the bar at the bottom.&lt;/item&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Copy the Savegames folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; Will my old saves still take up storage space on my Steam Deck?&lt;/p&gt;
    &lt;p&gt;Yes, your old saves will still take up storage space. If you want to save some space and you don't plan on using the Proton version, you can delete the compatdata folder after you've copied over the folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;Will my mods be transferred over automatically?&lt;/p&gt;
    &lt;p&gt;If you are logged into your Larian Account and have it connected to mod.io, all mods you are subscribed to will be downloaded when the transition to Steam Deck Native occurs.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; What if I’m not logged into a Larian Account or connected to mod.io?&lt;/p&gt;
    &lt;p&gt;You can either manually download the mods from the Mod Manager or transfer them manually from the previous folder.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To do so,switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click on the folder icon on the bar at the bottom.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3&lt;/item&gt;
      &lt;item&gt;Copy the Mods folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354644</guid><pubDate>Wed, 24 Sep 2025 00:26:59 +0000</pubDate></item><item><title>Periodic Table of Cognition</title><link>https://kk.org/thetechnium/the-periodic-table-of-cognition/</link><description>&lt;doc fingerprint="ff34989f0b052f5d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Periodic Table of Cognition&lt;/head&gt;
    &lt;p&gt;Iâve been studying the early history of electricityâs discovery as a map for our current discovery of artificial intelligence. The smartest people alive back then, including Isaac Newton, who may have been the smartest person who ever lived, had confident theories about electricityâs nature that were profoundly wrong. In fact, despite the essential role of electrical charges in the universe, everyone who worked on this fundamental force was profoundly wrong for a long time. All the pioneers of electricity â such as Franklin, Wheatstone, Faraday, and Maxwell â had a few correct ideas of their own (not shared by all) mixed in with notions that mostly turned out to be flat out misguided. Most of the discoveries about what electricity could do happened without the knowledge of how they worked. That ignorance, of course, drastically slowed down the advances in electrical inventions.&lt;/p&gt;
    &lt;p&gt;In a similar way, the smartest people today, especially all the geniuses creating artificial intelligence, have theories about what intelligence is, and I believe all of them (me too) will be profoundly wrong. We donât know what artificial intelligence is in large part because we donât know what our own intelligence is. And this ignorance will later be seen as an impediment to the rate of progress in AI.&lt;/p&gt;
    &lt;p&gt;A major part of our ignorance stems from our confusion about the general category of either electricity or intelligence. We tend to view both electricity and intelligence as coherent elemental forces along a single dimension: you either have more of it or less. But in fact, electricity turned out to be so complicated, so complex, so full of counterintuitive effects that even today it is still hard to grasp how it works. It has particles and waves, and fields and flows, composed of things that are not really there. Our employment of electricity exceeds our understanding of it. Understanding electricity was essential to understanding matter. It wasnât until we learned to control electricity that we were able to split water â which had been considered an element â into its actual elements; that enlightened us that water was not a foundational element, but a derivative compound made up of sub elements.&lt;/p&gt;
    &lt;p&gt;It is very probable we will discover that intelligence is likewise not a foundational singular element, but a derivative compound composed of multiple cognitive elements, combined in a complex system unique to each species of mind. The result that we call intelligence emerges from many different cognitive primitives such as long-term memory, spatial awareness, logical deduction, advance planning, pattern perception, and so on. There may be dozens of them, or hundreds. We currently donât have any idea of what these elements are. We lack a periodic table of cognition.&lt;/p&gt;
    &lt;p&gt;The cognitive elements will more resemble the heavier elements in being unstable and dynamic. Or a better analogy would be to the elements in a biological cell. The primitives of cognition are flow states that appear in a thought cycle. They are like molecules in a cell which are in constant flux, shifting from one shape to another. Their molecular identity is related to their actions and interactions with other molecules. Thinking is a collective action that happens in time (like temperature in matter) and every mode can only be seen in relation to the other modes before and after it. It is a network phenomenon that makes it difficult to identify its borders. So each element of intelligence is embedded in a thought cycle, and requires the other elements as part of its identity. So each cognitive element is described in context of the other cognitive modes adjacent to it.&lt;/p&gt;
    &lt;p&gt;I asked ChatGPT5Pro to help me generate a periodic table of cognition given what we collectively know so far. It suggests 49 elements, arranged in a table so that related concepts are adjacent. The columns are families, or general categories of cognition such as âPerceptionâ, âReasoningâ, âLearningâ, so all the types of perception or reasoning are stacked in one column. The rows are sorted by stages in a cycle of thought. The earlier stages (such as âsensingâ) are at the top, while later stages in the cycle (such as âreflect &amp;amp; alignâ) are at the bottom. So for example, in the family or category of âSafetyâ the AIs will tend to do the estimation of uncertainty first, later do verification, and only get to a theory of mind at the end.&lt;/p&gt;
    &lt;p&gt;The chart is colored according to how much progress weâve made on each element. Red indicates we can synthesize that element in a robust way. Orange means we can kind of make it work with the right scaffolding. Yellow reflects promising research without operational generality yet.&lt;/p&gt;
    &lt;p&gt;I suspect many of these elements are not as distinct as shown here (taxonomically I am more of a lumper than a splitter), and I would expect this collection omits many types we are soon to discover, but as a start, this prototype chart serves its purpose: it reveals the complexity of intelligence. It is clear intelligence is compounded along multiple dimensions. We will engineer different AIs to have different combinations of different elements in different strengths. This will produce thousands of types of possible minds. We can see that even today different animals have their own combination of cognitive primitives, arranged in a pattern unique to their speciesâ needs. In some animals some of the elements â say long-term memory â may exceed our own in strength; of course they lack some elements we have.&lt;/p&gt;
    &lt;p&gt;With the help of AI, we are discovering what these elements of cognition are. Each advance illuminates a bit of how minds work and what is needed to achieve results. If the discovery of electricity and atoms has anything to teach us now, it is that we are probably very far from having discovered the complete set of cognitive elements. Instead we are at the stage of believing in ethers, instantaneous action, and phlogiston â a few of the incorrect theories of electricity the brightest scientists believed.&lt;/p&gt;
    &lt;p&gt;Almost no thinker, researcher, experimenter, or scientist at that time could see the true nature of electricity, electromagnetism, radiation and subatomic particles, because the whole picture was hugely unintuitive. Waves, force fields, particles of atoms did not make sense (and still does not make common sense). It required sophisticated mathematics to truly comprehend it, and even after Maxwell described it mathematically, he found it hard to visualize.&lt;/p&gt;
    &lt;p&gt;I expect the same from intelligence. Even after we identify its ingredients, the emergent properties they generate are likely to be obscure and hard to believe, hard to visualize. Intelligence is unlikely to make common sense.&lt;/p&gt;
    &lt;p&gt;A century ago, our use of electricity ran ahead of our understanding of it. We made motors from magnets and coiled wire without understanding why they worked. Theory lagged behind practice. As with electricity, our employment of intelligence exceeds our understanding of it. We are using LLMs to answer questions or to code software without having a theory of intelligence. A real theory of intelligence is so lacking that we donât know how our own minds work, let alone the synthetic ones we can now create.&lt;/p&gt;
    &lt;p&gt;The theory of the atomic world needed the knowledge of the periodic table of elements. You had to know all (or at least most) of the parts to make falsifiable predictions of what would happen. The theory of intelligence requires knowledge of all the elemental parts, which we have only slowly begun to identify, before we can predict what might happen next.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354689</guid><pubDate>Wed, 24 Sep 2025 00:33:24 +0000</pubDate></item><item><title>Zutty: Zero-cost Unicode Teletype, high-end terminal for low-end systems</title><link>https://git.hq.sig7.se/zutty.git</link><description>&lt;doc fingerprint="62bb15c620e32fef"&gt;
  &lt;main&gt;
    &lt;p&gt;A reference implementation of the sublinear-space ZKP prover/Verifier described in our whitepaper: "Zero-knowledge Proofs in Sublinear Space" (https://arxiv.org/abs/2509.05326). It realizes a streaming prover that uses only O(√T) memory over a trace of length T, while producing standard KZG commitments (BN254) for wires, the permutation accumulator &lt;code&gt;Z&lt;/code&gt;, and the quotient &lt;code&gt;Q&lt;/code&gt;. The design keeps aggregate-only Fiat–Shamir and never materializes whole polynomials.&lt;/p&gt;
    &lt;p&gt;Traditional zk proving pipelines routinely buffer whole polynomials, forcing O(T) memory and large intermediate states. This repository demonstrates a practical alternative:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sublinear space: the active working set stays O(√T) using blocked IFFTs and streaming accumulators.&lt;/item&gt;
      &lt;item&gt;Production-style commitments: standard KZG commitments/openings (pairing-checked) over BN254.&lt;/item&gt;
      &lt;item&gt;No full-poly buffers: wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;are built and opened without holding entire vectors.&lt;/item&gt;
      &lt;item&gt;Deterministic dev SRS: easy to run locally; switch to trusted SRS files for production.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building scalable zk systems, this repo shows how to restructure your pipeline around streaming and aggregate-only FS without giving up familiar cryptographic backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCS: KZG over BN254 with a linear interface and a streaming Aggregator.&lt;/item&gt;
      &lt;item&gt;Two commitment bases: commit from evaluation (domain-aligned) or coefficient slices.&lt;/item&gt;
      &lt;item&gt;Openings: real KZG openings for wires/&lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Q&lt;/code&gt;, with consistent witness construction.&lt;/item&gt;
      &lt;item&gt;Domain &amp;amp; transforms: radix-2 blocked IFFT/NTT, barycentric eval for streaming points.&lt;/item&gt;
      &lt;item&gt;AIR &amp;amp; residuals: small fixed-column AIR and permutation-coupled residual stream.&lt;/item&gt;
      &lt;item&gt;Scheduler: five-phase A→E pipeline, aggregate-only Fiat–Shamir, strictly increasing time order.&lt;/item&gt;
      &lt;item&gt;CLI tools: &lt;code&gt;prover&lt;/code&gt;and&lt;code&gt;verifier&lt;/code&gt;plus an end-to-end script.&lt;/item&gt;
      &lt;item&gt;Space profile: peak memory ≈ O(b_blk) with &lt;code&gt;b_blk ≈ √T&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Phase A: (Optional) commit selectors/fixed columns.&lt;/item&gt;
      &lt;item&gt;Phase B: Wires — stream a register’s evaluations block-by-block → blocked IFFT → feed coeff tiles (low→high) into PCS Aggregator.&lt;/item&gt;
      &lt;item&gt;Phase C: Permutation accumulator &lt;code&gt;Z&lt;/code&gt;— stream locals, update&lt;code&gt;Z&lt;/code&gt;on the fly and emit the&lt;code&gt;Z&lt;/code&gt;column in time order, then commit via the same blocked IFFT path.&lt;/item&gt;
      &lt;item&gt;Phase D: Quotient &lt;code&gt;Q&lt;/code&gt;— stream residual&lt;code&gt;R(ω^i)&lt;/code&gt;and convert to&lt;code&gt;Q&lt;/code&gt;coefficients online using&lt;code&gt;Z_H(X)=X^N−c&lt;/code&gt;(no full-poly buffers).&lt;/item&gt;
      &lt;item&gt;Phase E: Openings — produce real KZG openings for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;(witness&lt;code&gt;W = (f−f(ζ))/(X−ζ)&lt;/code&gt;) and verify via pairings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All Fiat–Shamir challenges are replayed by the verifier; pairing checks are always enforced. In dev builds, SRS is deterministic; in production, provide trusted SRS files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (stable toolchain)&lt;/item&gt;
      &lt;item&gt;No external SRS required for dev runs (deterministic in-crate SRS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone, then:
cargo build --quiet --bins --features dev-srs

# End-to-end script runs three scenarios + a tamper test
scripts/test_sszkp.sh&lt;/code&gt;
    &lt;p&gt;Expected output (abridged):&lt;/p&gt;
    &lt;code&gt;✔ build succeeded
✔ verification OK for eval-basis wires, b_blk=128, rows=1024
✔ tampered proof correctly rejected
✔ verification OK for coeff-basis wires, b_blk=64, rows=1536
✔ verification OK for eval-basis wires, b_blk=256, rows=2048
==&amp;gt; All tests passed 🎉
&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval
# writes proof.bin&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin verifier -- --rows 1024 --basis eval
# reads proof.bin and verifies&lt;/code&gt;
    &lt;p&gt;In non-dev builds you must provide both G1 and G2 SRS files.&lt;/p&gt;
    &lt;p&gt;Prover:&lt;/p&gt;
    &lt;code&gt;cargo run --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;p&gt;Verifier:&lt;/p&gt;
    &lt;code&gt;cargo run --bin verifier -- --rows 1024 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Format: the SRS files are Arkworks-serialized vectors of affine powers. G1:&lt;/p&gt;&lt;code&gt;[τ^0]G1 … [τ^d]G1&lt;/code&gt;(we use the degree bound you load). G2: a vector containing at least&lt;code&gt;[τ]G2&lt;/code&gt;(we read element 1 or 0).&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--rows &amp;lt;T&amp;gt;&lt;/code&gt;: total rows in the trace (domain size rounds up to power of two).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--b-blk &amp;lt;B&amp;gt;&lt;/code&gt;: block size; pick ≈ √T to achieve the sublinear memory bound.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--k &amp;lt;K&amp;gt;&lt;/code&gt;: number of registers (columns) in the AIR.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--basis &amp;lt;eval|coeff&amp;gt;&lt;/code&gt;: commitment basis for wires (Q is always coeff).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--selectors &amp;lt;FILE&amp;gt;&lt;/code&gt;: optional selectors/fixed columns CSV (rows × S).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--omega &amp;lt;u64&amp;gt;&lt;/code&gt;: override ω (power-of-two order must hold).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--coset &amp;lt;u64&amp;gt;&lt;/code&gt;: reserved; current domain uses subgroup (&lt;code&gt;Z_H(X)=X^N−1&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/pcs.rs&lt;/code&gt;— KZG PCS (BN254), streaming Aggregator, real openings, pairings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/domain.rs&lt;/code&gt;— domain&lt;code&gt;H&lt;/code&gt;, barycentric weights, blocked NTT/IFFT.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/air.rs&lt;/code&gt;— tiny fixed-column AIR + residual stream + permutation coupling.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/perm_lookup.rs&lt;/code&gt;— permutation accumulator&lt;code&gt;Z&lt;/code&gt;(lookups optional).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/quotient.rs&lt;/code&gt;— streaming quotient builder (R→Q tilewise).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/scheduler.rs&lt;/code&gt;— 5-phase orchestrator (aggregate-only FS, O(√T) space).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/opening.rs&lt;/code&gt;— streaming polynomial evaluation helpers (eval/coeff mode).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/transcript.rs&lt;/code&gt;— domain-separated FS transcript (BLAKE3→field).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/stream.rs&lt;/code&gt;— block partitioning + restreaming interfaces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bin/prover.rs&lt;/code&gt;,&lt;code&gt;bin/verifier.rs&lt;/code&gt;— CLIs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scripts/test_sszkp.sh&lt;/code&gt;— end-to-end tests + tamper test.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat the &lt;code&gt;Restreamer&lt;/code&gt;trait as the integration seam: implement it to feed rows from your own storage (disk, network, GPU), all while keeping O(b_blk) memory.&lt;/item&gt;
      &lt;item&gt;Keep your permutation/lookup logic time-ordered; the accumulator state must evolve monotonically in &lt;code&gt;t&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;When committing from evaluations, ensure your blocks align to the domain and use the provided blocked IFFT helpers to produce coeff tiles.&lt;/item&gt;
      &lt;item&gt;For openings, prefer the coeff-stream path; the code adapts eval-streams internally when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairing checks are always enforced; the verifier replays the FS transcript and checks KZG equalities for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tamper test flips one byte in &lt;code&gt;proof.bin&lt;/code&gt;—verification must fail.&lt;/item&gt;
      &lt;item&gt;Dev SRS exists only for convenience; do not use dev mode in production.&lt;/item&gt;
      &lt;item&gt;Algebraic identity at ζ (gate + perm coupling + boundary = &lt;code&gt;Z_H(ζ)·Q(ζ)&lt;/code&gt;) is implemented; by default, selectors are optional and gates are minimal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AIR is a compact demo; plug in your real selector/table wiring as needed.&lt;/item&gt;
      &lt;item&gt;Lookup accumulator is feature-gated and intentionally minimal (demo path).&lt;/item&gt;
      &lt;item&gt;Only BN254/KZG is shipped; adding Pallas/BLS12-381 is straightforward in this architecture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File issues for bugs or suggestions.&lt;/item&gt;
      &lt;item&gt;PRs welcome—especially alternative domains, SRS loaders, or integration examples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This codebase follows the aggregate-only Fiat–Shamir and streaming discipline described in the whitepaper and demonstrates that production-style commitments and sublinear space can coexist in a practical Rust implementation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45355462</guid><pubDate>Wed, 24 Sep 2025 02:07:23 +0000</pubDate></item><item><title>Quadratic memory reductions for Zero-knowledge Proofs</title><link>https://github.com/logannye/space-efficient-zero-knowledge-proofs</link><description>&lt;doc fingerprint="62bb15c620e32fef"&gt;
  &lt;main&gt;
    &lt;p&gt;A reference implementation of the sublinear-space ZKP prover/Verifier described in our whitepaper: "Zero-knowledge Proofs in Sublinear Space" (https://arxiv.org/abs/2509.05326). It realizes a streaming prover that uses only O(√T) memory over a trace of length T, while producing standard KZG commitments (BN254) for wires, the permutation accumulator &lt;code&gt;Z&lt;/code&gt;, and the quotient &lt;code&gt;Q&lt;/code&gt;. The design keeps aggregate-only Fiat–Shamir and never materializes whole polynomials.&lt;/p&gt;
    &lt;p&gt;Traditional zk proving pipelines routinely buffer whole polynomials, forcing O(T) memory and large intermediate states. This repository demonstrates a practical alternative:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sublinear space: the active working set stays O(√T) using blocked IFFTs and streaming accumulators.&lt;/item&gt;
      &lt;item&gt;Production-style commitments: standard KZG commitments/openings (pairing-checked) over BN254.&lt;/item&gt;
      &lt;item&gt;No full-poly buffers: wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;are built and opened without holding entire vectors.&lt;/item&gt;
      &lt;item&gt;Deterministic dev SRS: easy to run locally; switch to trusted SRS files for production.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re building scalable zk systems, this repo shows how to restructure your pipeline around streaming and aggregate-only FS without giving up familiar cryptographic backends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCS: KZG over BN254 with a linear interface and a streaming Aggregator.&lt;/item&gt;
      &lt;item&gt;Two commitment bases: commit from evaluation (domain-aligned) or coefficient slices.&lt;/item&gt;
      &lt;item&gt;Openings: real KZG openings for wires/&lt;code&gt;Z&lt;/code&gt;/&lt;code&gt;Q&lt;/code&gt;, with consistent witness construction.&lt;/item&gt;
      &lt;item&gt;Domain &amp;amp; transforms: radix-2 blocked IFFT/NTT, barycentric eval for streaming points.&lt;/item&gt;
      &lt;item&gt;AIR &amp;amp; residuals: small fixed-column AIR and permutation-coupled residual stream.&lt;/item&gt;
      &lt;item&gt;Scheduler: five-phase A→E pipeline, aggregate-only Fiat–Shamir, strictly increasing time order.&lt;/item&gt;
      &lt;item&gt;CLI tools: &lt;code&gt;prover&lt;/code&gt;and&lt;code&gt;verifier&lt;/code&gt;plus an end-to-end script.&lt;/item&gt;
      &lt;item&gt;Space profile: peak memory ≈ O(b_blk) with &lt;code&gt;b_blk ≈ √T&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Phase A: (Optional) commit selectors/fixed columns.&lt;/item&gt;
      &lt;item&gt;Phase B: Wires — stream a register’s evaluations block-by-block → blocked IFFT → feed coeff tiles (low→high) into PCS Aggregator.&lt;/item&gt;
      &lt;item&gt;Phase C: Permutation accumulator &lt;code&gt;Z&lt;/code&gt;— stream locals, update&lt;code&gt;Z&lt;/code&gt;on the fly and emit the&lt;code&gt;Z&lt;/code&gt;column in time order, then commit via the same blocked IFFT path.&lt;/item&gt;
      &lt;item&gt;Phase D: Quotient &lt;code&gt;Q&lt;/code&gt;— stream residual&lt;code&gt;R(ω^i)&lt;/code&gt;and convert to&lt;code&gt;Q&lt;/code&gt;coefficients online using&lt;code&gt;Z_H(X)=X^N−c&lt;/code&gt;(no full-poly buffers).&lt;/item&gt;
      &lt;item&gt;Phase E: Openings — produce real KZG openings for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;(witness&lt;code&gt;W = (f−f(ζ))/(X−ζ)&lt;/code&gt;) and verify via pairings.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All Fiat–Shamir challenges are replayed by the verifier; pairing checks are always enforced. In dev builds, SRS is deterministic; in production, provide trusted SRS files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rust (stable toolchain)&lt;/item&gt;
      &lt;item&gt;No external SRS required for dev runs (deterministic in-crate SRS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone, then:
cargo build --quiet --bins --features dev-srs

# End-to-end script runs three scenarios + a tamper test
scripts/test_sszkp.sh&lt;/code&gt;
    &lt;p&gt;Expected output (abridged):&lt;/p&gt;
    &lt;code&gt;✔ build succeeded
✔ verification OK for eval-basis wires, b_blk=128, rows=1024
✔ tampered proof correctly rejected
✔ verification OK for coeff-basis wires, b_blk=64, rows=1536
✔ verification OK for eval-basis wires, b_blk=256, rows=2048
==&amp;gt; All tests passed 🎉
&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval
# writes proof.bin&lt;/code&gt;
    &lt;code&gt;cargo run --features dev-srs --bin verifier -- --rows 1024 --basis eval
# reads proof.bin and verifies&lt;/code&gt;
    &lt;p&gt;In non-dev builds you must provide both G1 and G2 SRS files.&lt;/p&gt;
    &lt;p&gt;Prover:&lt;/p&gt;
    &lt;code&gt;cargo run --bin prover -- \
  --rows 1024 --b-blk 128 --k 3 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;p&gt;Verifier:&lt;/p&gt;
    &lt;code&gt;cargo run --bin verifier -- --rows 1024 --basis eval \
  --srs-g1 srs_g1.bin --srs-g2 srs_g2.bin&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Format: the SRS files are Arkworks-serialized vectors of affine powers. G1:&lt;/p&gt;&lt;code&gt;[τ^0]G1 … [τ^d]G1&lt;/code&gt;(we use the degree bound you load). G2: a vector containing at least&lt;code&gt;[τ]G2&lt;/code&gt;(we read element 1 or 0).&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--rows &amp;lt;T&amp;gt;&lt;/code&gt;: total rows in the trace (domain size rounds up to power of two).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--b-blk &amp;lt;B&amp;gt;&lt;/code&gt;: block size; pick ≈ √T to achieve the sublinear memory bound.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--k &amp;lt;K&amp;gt;&lt;/code&gt;: number of registers (columns) in the AIR.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--basis &amp;lt;eval|coeff&amp;gt;&lt;/code&gt;: commitment basis for wires (Q is always coeff).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--selectors &amp;lt;FILE&amp;gt;&lt;/code&gt;: optional selectors/fixed columns CSV (rows × S).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--omega &amp;lt;u64&amp;gt;&lt;/code&gt;: override ω (power-of-two order must hold).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--coset &amp;lt;u64&amp;gt;&lt;/code&gt;: reserved; current domain uses subgroup (&lt;code&gt;Z_H(X)=X^N−1&lt;/code&gt;).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/pcs.rs&lt;/code&gt;— KZG PCS (BN254), streaming Aggregator, real openings, pairings.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/domain.rs&lt;/code&gt;— domain&lt;code&gt;H&lt;/code&gt;, barycentric weights, blocked NTT/IFFT.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/air.rs&lt;/code&gt;— tiny fixed-column AIR + residual stream + permutation coupling.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/perm_lookup.rs&lt;/code&gt;— permutation accumulator&lt;code&gt;Z&lt;/code&gt;(lookups optional).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/quotient.rs&lt;/code&gt;— streaming quotient builder (R→Q tilewise).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/scheduler.rs&lt;/code&gt;— 5-phase orchestrator (aggregate-only FS, O(√T) space).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/opening.rs&lt;/code&gt;— streaming polynomial evaluation helpers (eval/coeff mode).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/transcript.rs&lt;/code&gt;— domain-separated FS transcript (BLAKE3→field).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/stream.rs&lt;/code&gt;— block partitioning + restreaming interfaces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;bin/prover.rs&lt;/code&gt;,&lt;code&gt;bin/verifier.rs&lt;/code&gt;— CLIs.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scripts/test_sszkp.sh&lt;/code&gt;— end-to-end tests + tamper test.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat the &lt;code&gt;Restreamer&lt;/code&gt;trait as the integration seam: implement it to feed rows from your own storage (disk, network, GPU), all while keeping O(b_blk) memory.&lt;/item&gt;
      &lt;item&gt;Keep your permutation/lookup logic time-ordered; the accumulator state must evolve monotonically in &lt;code&gt;t&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;When committing from evaluations, ensure your blocks align to the domain and use the provided blocked IFFT helpers to produce coeff tiles.&lt;/item&gt;
      &lt;item&gt;For openings, prefer the coeff-stream path; the code adapts eval-streams internally when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pairing checks are always enforced; the verifier replays the FS transcript and checks KZG equalities for wires, &lt;code&gt;Z&lt;/code&gt;, and&lt;code&gt;Q&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tamper test flips one byte in &lt;code&gt;proof.bin&lt;/code&gt;—verification must fail.&lt;/item&gt;
      &lt;item&gt;Dev SRS exists only for convenience; do not use dev mode in production.&lt;/item&gt;
      &lt;item&gt;Algebraic identity at ζ (gate + perm coupling + boundary = &lt;code&gt;Z_H(ζ)·Q(ζ)&lt;/code&gt;) is implemented; by default, selectors are optional and gates are minimal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AIR is a compact demo; plug in your real selector/table wiring as needed.&lt;/item&gt;
      &lt;item&gt;Lookup accumulator is feature-gated and intentionally minimal (demo path).&lt;/item&gt;
      &lt;item&gt;Only BN254/KZG is shipped; adding Pallas/BLS12-381 is straightforward in this architecture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File issues for bugs or suggestions.&lt;/item&gt;
      &lt;item&gt;PRs welcome—especially alternative domains, SRS loaders, or integration examples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This codebase follows the aggregate-only Fiat–Shamir and streaming discipline described in the whitepaper and demonstrates that production-style commitments and sublinear space can coexist in a practical Rust implementation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45355514</guid><pubDate>Wed, 24 Sep 2025 02:12:58 +0000</pubDate></item><item><title>America's top companies keep talking about AI – but can't explain the upsides</title><link>https://www.ft.com/content/e93e56df-dd9b-40c1-b77a-dba1ca01e473</link><description>&lt;doc fingerprint="9e2b007ddfca99e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;America’s top companies keep talking about AI — but can’t explain the upsides&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Save 40% on Standard Digital&lt;/head&gt;was $540 now $319 for your first year&lt;p&gt;Save now on essential digital access to quality FT journalism on any device. Saving based on monthly annualised price.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45355806</guid><pubDate>Wed, 24 Sep 2025 02:59:45 +0000</pubDate></item></channel></rss>