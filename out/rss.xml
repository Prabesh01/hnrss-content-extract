<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 02 Sep 2025 18:12:16 +0000</lastBuildDate><item><title>Next.js is infuriating</title><link>https://blog.meca.sh/3lxoty3shjc2z</link><description>&lt;doc fingerprint="d05093007b4164b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hey, it's finally happened. I've decided to write a blog post. And if you're reading this, I've also finished one. I have wanted to do this for a long time, but could never find the motivation to start. But you know what they say: anger is the best motivator. They do say that, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;Some context that's in the background&lt;/head&gt;
    &lt;p&gt;We're going on a journey, you and I. But first, we need to set the scene. Imagine we're working for $COMPANY and one of our Next.js services did an oopsie. This being Next.js, we of course have no idea what actually happened since the default logging is only enabled during development.&lt;/p&gt;
    &lt;p&gt;Our quest is to go in and setup some production ready logging. It's not going to be easy, but then again, nothing ever is.&lt;/p&gt;
    &lt;head rend="h3"&gt;Middleware? Middle of nowhere!&lt;/head&gt;
    &lt;p&gt;The first step of our journey is the middleware. The documentation even states this:&lt;/p&gt;
    &lt;quote&gt;Middleware executes before routes are rendered. It's particularly useful for implementing custom server-side logic like authentication, logging, or handling redirects.&lt;/quote&gt;
    &lt;p&gt;Alright, looks simple enough. Time to pick a logging library. I went with pino since we have used it before. Anything is an upgrade over &lt;code&gt;console.log&lt;/code&gt; anyways. We'll get this done before lunch.&lt;/p&gt;
    &lt;p&gt;Let's set up a basic middleware:&lt;/p&gt;
    &lt;code&gt;// middleware.ts
import { NextResponse, NextRequest } from "next/server";

export async function middleware(request: NextRequest) {
  return new NextResponse.next({
    request: request,
    headers: request.headers,
    // status: 200,
    // statusText: 'OK'
  });
}

export const config = {
  matcher: "/:path*",
};&lt;/code&gt;
    &lt;p&gt;I think we already have a problem here. You can pass a grand total of 4 parameters from your middleware. The only thing that actually affects the invoked route is the &lt;code&gt;headers&lt;/code&gt;. Let's not skip over the fact that you can't have multiple middlewares or chain them either. How do you fuck this up so bad? We've had middlewares since at least the early 2010s when Express came out.&lt;/p&gt;
    &lt;p&gt;Anyways, we're smart and modern Node.js has some pretty nifty tools. Let's reach for &lt;code&gt;AsyncLocalStorage&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;// app/logger.ts
import { AsyncLocalStorage } from "async_hooks";
import { Logger, pino } from "pino";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "trace",
});

export const LoggerStorage = new AsyncLocalStorage&amp;lt;Logger&amp;gt;();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// middleware.ts
export async function middleware(request: NextRequest) {
  LoggerStorage.enterWith(requestLogger());
  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next();
}&lt;/code&gt;
    &lt;p&gt;Whew, hard work done. Let's test it out. Visit localhost:3000 and we see this:&lt;/p&gt;
    &lt;code&gt;{ requestId: 'ec7718fa-b1a2-473e-b2e2-8f51188efa8f' } { url: 'http://localhost:3000/' } 'Started processing request!'
 GET / 200 in 71ms
{ requestId: '09b526b1-68f4-4e90-971f-b0bc52ad167c' } { url: 'http://localhost:3000/next.svg' } 'Started processing request!'
{ requestId: '481dd2ff-e900-4985-ae15-0b0a1eb5923f' } { url: 'http://localhost:3000/vercel.svg' } 'Started processing request!'
{ requestId: 'e7b29301-171c-4c91-af25-771471502ee4' } { url: 'http://localhost:3000/file.svg' } 'Started processing request!'
{ requestId: '13766de3-dd00-42ce-808a-ac072dcfd4c6' } { url: 'http://localhost:3000/window.svg' } 'Started processing request!'
{ requestId: '317e054c-1a9a-4dd8-ba21-4c0201fbeada' } { url: 'http://localhost:3000/globe.svg' } 'Started processing request!'&lt;/code&gt;
    &lt;p&gt;I don't know if you've ever used pino before, but this is wrong. Can you figure out why?&lt;/p&gt;
    &lt;p&gt;Unlike Next.js I won't keep you waiting in limbo. This is the browser output. Why? Well, it's because the default Next.js middleware runtime is &lt;code&gt;edge&lt;/code&gt;. We can of course switch to the &lt;code&gt;nodejs&lt;/code&gt; runtime which should work. Except, of course, it might not.&lt;/p&gt;
    &lt;p&gt;I've tried it on a fresh Next.js project and it does work. But it didn't when I was trying it out on our actual project. I swear I'm not crazy. Anyways, this isn't the main issue. We're slowly getting there.&lt;/p&gt;
    &lt;head rend="h3"&gt;Paging the local mental asylum&lt;/head&gt;
    &lt;p&gt;Logging in the middleware is cool and all, but it's not where the bulk of the magic happens. For that, we need to log in pages and layouts. Let's try it out.&lt;/p&gt;
    &lt;code&gt;// app/page.tsx
export default function Home() {
  logger()?.info("Logging from the page!");

  return &amp;lt;div&amp;gt;Real simple website!&amp;lt;/div&amp;gt;
}&lt;/code&gt;
    &lt;p&gt;Refresh the page and we get this:&lt;/p&gt;
    &lt;code&gt;‚úì Compiled / in 16ms
 GET / 200 in 142ms&lt;/code&gt;
    &lt;p&gt;That's it? That's it. Nothing. Nada. Zilch.&lt;/p&gt;
    &lt;p&gt;For posterity's sake, this is what it's supposed to look like:&lt;/p&gt;
    &lt;code&gt;‚úì Compiled / in 2.2s
[11:38:59.259] INFO (12599): Logging from the page!
    requestId: "2ddef9cf-6fee-4d1d-8b1e-6bb16a3e636b"
 GET / 200 in 2520ms&lt;/code&gt;
    &lt;p&gt;Ok,this is getting a bit long, so I'll get to the point. The &lt;code&gt;logger&lt;/code&gt; function returns &lt;code&gt;null&lt;/code&gt;. Why? I'm not entirely sure, but it seems like rendering is not executed in the same async context as the middleware.&lt;/p&gt;
    &lt;p&gt;What's the solution then? You're not going to believe this. Remember how the only value you can pass from the middleware is &lt;code&gt;headers&lt;/code&gt;? Yeah. That's what we need to use.&lt;/p&gt;
    &lt;p&gt;The following is for people with strong stomachs:&lt;/p&gt;
    &lt;code&gt;// app/log/serverLogger.ts
import { pino } from "pino";

export const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

// app/log/middleware.ts
// Yes, we need to split up the loggers ...
// Mostly the same as before
import { loggerInstance } from "./serverLogger";

export function requestLogger(requestId: string): Logger {
  return loggerInstance.child({ requestId });
}

// app/log/server.ts
import { headers } from "next/headers";
import { loggerInstance } from "./serverLogger";
import { Logger } from "pino";
import { NextRequest } from "next/server";

const REQUEST_ID_HEADER = "dominik-request-id";

export function requestHeaders(
  request: NextRequest,
  requestId: string,
): Headers {
  const head = new Headers(request.headers);
  head.set(REQUEST_ID_HEADER, requestId);
  return head;
}

// Yeah, this has to be async ...
export async function logger(): Promise&amp;lt;Logger&amp;gt; {
  const hdrs = await headers();
  const requestId = hdrs.get(REQUEST_ID_HEADER);

  return loggerInstance.child({ requestId });
}

// middleware.ts
import { logger, LoggerStorage, requestLogger } from "./app/log/middleware";
import { requestHeaders } from "./app/log/server";

export async function middleware(request: NextRequest) {
  const requestId = crypto.randomUUID();
  LoggerStorage.enterWith(requestLogger(requestId));

  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next({ headers: requestHeaders(request, requestId) });
}

// app/page.tsx
export default async function Home() {
  (await logger())?.info("Logging from the page!");

  // ...
}&lt;/code&gt;
    &lt;p&gt;Isn't it beautiful? I especially appreciate how it's now possible to import the middleware logging code from the server. Which of course won't work. Or import the server logging code from the middleware. Which also won't work. Better not mess up. Also, we haven't even touched upon logging in client components, which despite the name also run on the server. Yeah, that's a third split.&lt;/p&gt;
    &lt;head rend="h3"&gt;Congratulations, you're being coddled. Please do not resist.&lt;/head&gt;
    &lt;p&gt;Listen. I wanted to apologize, because I've led you into this trap. You see, I have already fallen into it several times before. A middleware system can be pretty useful when designed correctly and I wanted you to see what it looks like when it's not. The reason for writing this blog post actually started here.&lt;/p&gt;
    &lt;p&gt;I think every one of us has reached a point in their lives where they've had enough. For me, it was right here. Fuck it, let's use a custom server.&lt;/p&gt;
    &lt;quote&gt;A custom Next.js server allows you to programmatically start a server for custom patterns. The majority of the time, you will not need this approach. However, it's available if you need to eject.&lt;/quote&gt;
    &lt;p&gt;Let's take a look at the example from the documentation:&lt;/p&gt;
    &lt;code&gt;import { createServer } from 'http'
import { parse } from 'url'
import next from 'next'
 
const port = parseInt(process.env.PORT || '3000', 10)
const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()
 
app.prepare().then(() =&amp;gt; {
  createServer((req, res) =&amp;gt; {
    const parsedUrl = parse(req.url!, true)
    handle(req, res, parsedUrl)
  }).listen(port)
 
  console.log(
    `&amp;gt; Server listening at http://localhost:${port} as ${
      dev ? 'development' : process.env.NODE_ENV
    }`
  )
})&lt;/code&gt;
    &lt;p&gt;Note that once again, &lt;code&gt;handle&lt;/code&gt; doesn't really take any parameters. Only the request URL and the raw request and response.&lt;/p&gt;
    &lt;p&gt;Anyways, we still have &lt;code&gt;AsyncLocalStorage&lt;/code&gt; so this doesn't concern us too much. Let's modify the example a bit.&lt;/p&gt;
    &lt;code&gt;// app/logger.ts
// Reverted back to our AsyncLocalStorage variaton
import { pino, Logger } from "pino";
import { AsyncLocalStorage } from "async_hooks";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

export const LoggerStorage = new AsyncLocalStorage&amp;lt;Logger&amp;gt;();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// server.ts
import { logger, LoggerStorage, requestLogger } from "./app/logger";

app.prepare().then(() =&amp;gt; {
  createServer(async (req, res) =&amp;gt; {
    // This is new
    LoggerStorage.enterWith(requestLogger());
    logger()?.info({}, "Logging from server!");

    const parsedUrl = parse(req.url!, true);
    await handle(req, res, parsedUrl);
  }).listen(port);
});

// middleware.ts
import { logger } from "./app/logger";

export async function middleware(request: NextRequest) {
  logger()?.info({}, "Logging from middleware!");
  return NextResponse.next();
}

// app/page.tsx
import { logger } from "./logger";

export default async function Home() {
  logger()?.info("Logging from the page!");
  
  // ...
}&lt;/code&gt;
    &lt;p&gt;Ok, let's test it out. Refresh the browser and ...&lt;/p&gt;
    &lt;code&gt;&amp;gt; Server listening at http://localhost:3000 as development
[12:29:52.183] INFO (19938): Logging from server!
    requestId: "2ffab9a2-7e15-4188-8959-a7822592108f"
 ‚úì Compiled /middleware in 388ms (151 modules)
 ‚óã Compiling / ...
 ‚úì Compiled / in 676ms (769 modules)&lt;/code&gt;
    &lt;p&gt;That's it. Are you fucking kidding me right now? What the fuck?&lt;/p&gt;
    &lt;p&gt;Now, you might be thinking that this is just not how &lt;code&gt;AsyncLocalStorage&lt;/code&gt; works. And you might be right. But I would like to point out that &lt;code&gt;headers()&lt;/code&gt; and &lt;code&gt;cookies()&lt;/code&gt; use &lt;code&gt;AsyncLocalStorage&lt;/code&gt;. This is a power that the Next.js devs have that we don't.&lt;/p&gt;
    &lt;p&gt;As far as I can tell there are only two ways to pass information from a middleware to a page.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Headers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NextResponse.redirect&lt;/code&gt;/&lt;code&gt;NextResponse.rewrite&lt;/code&gt;to a route with extra params (eg. /[requestId]/page.tsx)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As you might have noticed, neither of these are very pleasant to use in this case.&lt;/p&gt;
    &lt;p&gt;You are being coddled. The Next.js devs have a vision and it's either their way or the highway. Note that if it was just the middleware, I wouldn't be sitting here, wasting away my weekend, ranting about a React framework. Believe it or not, I've got better things to do. It's constant pain you encounter daily when working with Next.js.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vercel can do better&lt;/head&gt;
    &lt;p&gt;What's infuriating about this example is that Vercel can very much do better. I don't want to sing too many praises at Svelte(Kit) because I have some misgivings about its recent direction, but it's so much better than Next.js. Let's look at their middleware docs:&lt;/p&gt;
    &lt;quote&gt;handle - This function runs every time the SvelteKit server receives a request [...] This allows you to modify response headers or bodies, or bypass SvelteKit entirely (for implementing routes programmatically, for example).&lt;/quote&gt;
    &lt;p&gt;Looking good so far.&lt;/p&gt;
    &lt;quote&gt;locals - To add custom data to the request, which is passed to handlers in&lt;code&gt;+server.js&lt;/code&gt;and server&lt;code&gt;load&lt;/code&gt;functions, populate the&lt;code&gt;event.locals&lt;/code&gt;object, as shown below.&lt;/quote&gt;
    &lt;p&gt;I'm crying tears of joy right now. You can also stuff real objects/classes in there. Like a logger for instance.&lt;/p&gt;
    &lt;quote&gt;You can define multiple handle functions and execute them with sequence.&lt;/quote&gt;
    &lt;p&gt;This is what real engineering looks like. SvelteKit is a Vercel product. How is the flagship offering worse than what is essentially a side project. What the hell?&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientists discover a new super massive black hole at https://github.com/vercel/next.js/issues&lt;/head&gt;
    &lt;p&gt;I don't have anything else to add, but while I'm here I feel like I have to talk about the GitHub issue tracker. This is perhaps the crown jewel of the dumpster fire that is Next.js. It's a place where hopes and issues come to die. The mean response time for a bug report is never. I've made it a sport to search the issue tracker/discussion for problems I'm currently facing and bet on how many years it takes to even get a response from a Next.js dev.&lt;/p&gt;
    &lt;p&gt;You think I'm joking? There are hundreds of issues with as many üëç emojis with no official response for years. And when you finally get a response, it's to tell you that what you're doing is wrong and a solution to your real problems is on the way. Then they proceed to keep the "solution" in canary for years on end.&lt;/p&gt;
    &lt;p&gt;I personally reported two issues a year ago. Keep in mind that to have a valid bug report, you need a reproduction.&lt;/p&gt;
    &lt;p&gt;So, what do you get for taking the time to make a minimal reproduction? That's right. Complete silence.&lt;/p&gt;
    &lt;p&gt;I would have reported about a dozen other issues I have encountered, but after this experience I gave up.&lt;/p&gt;
    &lt;p&gt;Honestly, I don't even know if the issues are still valid.&lt;/p&gt;
    &lt;head rend="h3"&gt;Have we learned anything?&lt;/head&gt;
    &lt;p&gt;I don't know. For me, personally, I don't want to use Next.js anymore. You might think that this is just a singular issue and I'm overreacting. But there's bugs and edge cases around every corner. How did they manage to make TypeScript compile slower than Rust? Why make a distinction between code running on client and server and then not give me any tools to take advantage of that? Why? Why? Why?&lt;/p&gt;
    &lt;p&gt;I don't think I quite have enough pull to move us out of Next.js land. But, I think I will voice my opinion if we end up writing another app. We'll see if the grass is any greener on the other side.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45099922</guid></item><item><title>Kapa.ai (YC S23) is hiring research and software engineers</title><link>https://www.ycombinator.com/companies/kapa-ai/jobs</link><description>&lt;doc fingerprint="bdc48a3b7f7b6dd6"&gt;
  &lt;main&gt;
    &lt;p&gt;The fastest way to build AI assistants on technical content&lt;/p&gt;
    &lt;p&gt;We make it easy for technical companies to build AI assistants. Companies like Docker, Grafana and Mixpanel deploy kapa in the following ways:&lt;/p&gt;
    &lt;p&gt;We leverage companies existing technical knowledge sources including documentation, tutorials, forum posts, Slack channels, GitHub issues and many more to generate AI assistants that can handle complicated technical questions. More than 200 companies use kapa and we have answered more than 10 million questions to date.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45099939</guid></item><item><title>Run Erlang/Elixir on Microcontrollers and Embedded Linux</title><link>https://www.grisp.org/software</link><description>&lt;doc fingerprint="3ddf79035115e1e6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GRiSP Software Stacks&lt;/head&gt;
    &lt;head rend="h2"&gt;Bring Erlang/Elixir all the way to the edge. Deterministic, fault-tolerant, and production-ready&lt;/head&gt;
    &lt;p&gt;Open-source on GitHub&lt;/p&gt;
    &lt;head rend="h1"&gt;GRiSP Metal&lt;/head&gt;
    &lt;head rend="h2"&gt;Erlang/Elixir on RTEMS. &lt;lb/&gt;Tiny BEAM for devices.&lt;/head&gt;
    &lt;p&gt;GRiSP Metal, formerly just GRiSP, boots straight into the Erlang/Elixir VM on RTEMS for deterministic, real‚Äëtime behavior with a minimal footprint. It runs on microcontrollers, and we've made the full stack fit in 16 MB of RAM, ideal when every byte and millisecond matter.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Boots directly to the BEAM (Erlang/Elixir) on RTEMS&lt;/item&gt;
      &lt;item&gt;MCU-class footprint (fits in 16 MB RAM)&lt;/item&gt;
      &lt;item&gt;Real-time scheduling with predictable I/O&lt;/item&gt;
      &lt;item&gt;Direct, low-overhead access to hardware interfaces&lt;/item&gt;
      &lt;item&gt;Supervision trees bring BEAM reliability to the edge&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;GRiSP Alloy&lt;/head&gt;
    &lt;head rend="h2"&gt;Erlang/Elixir on Linux RT. &lt;lb/&gt;Buildroot edition.&lt;/head&gt;
    &lt;p&gt;GRiSP Alloy boots directly into the Erlang/Elixir VM atop a lean, Buildroot-based real-time Linux. Run multiple Erlang/Elixir VMs with distinct priorities and/or pinned to different cores, connected via efficient, secure distributed Erlang links.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal Linux RT image (Buildroot) with BEAM-first boot path&lt;/item&gt;
      &lt;item&gt;Multiple BEAM instances with priority separation and core affinity&lt;/item&gt;
      &lt;item&gt;Local, secure node-to-node links via distributed Erlang&lt;/item&gt;
      &lt;item&gt;Full access to Linux drivers, filesystems, and networking&lt;/item&gt;
      &lt;item&gt;Fast boot, small attack surface, production-ready&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;GRiSP Forge&lt;/head&gt;
    &lt;head rend="h2"&gt;Erlang/Elixir on Linux RT.&lt;lb/&gt;Yocto edition.&lt;/head&gt;
    &lt;p&gt;GRiSP Forge applies the same architecture as GRiSP Alloy to Yocto, for teams requiring long-term, customizable Linux stacks and BSP integration. Boots directly into the Erlang/Elixir VM with the same multi-VM model and secure local links via distributed Erlang.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yocto-based builds with reproducible, customizable images&lt;/item&gt;
      &lt;item&gt;Multiple Erlang/Elixir VMs by design (priorities and/or core pinning)&lt;/item&gt;
      &lt;item&gt;Efficient, secure local links via distributed Erlang&lt;/item&gt;
      &lt;item&gt;Industrial Linux ecosystem compatibility and tooling&lt;/item&gt;
      &lt;item&gt;Built for long lifecycles and enterprise requirements&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;GRiSP-io: Manage Embedded Systems at Scale&lt;/head&gt;
    &lt;p&gt;GRiSP-io is our cloud and edge platform for deploying, monitoring, and managing distributed embedded systems built with GRiSP stacks. From remote updates to real-time system insights, it helps you stay in control‚Äîwhether you're testing a prototype or scaling a fleet.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deploy and manage GRiSP-based devices remotely&lt;/item&gt;
      &lt;item&gt;Monitor system performance and health in real-time&lt;/item&gt;
      &lt;item&gt;Perform over-the-air updates with confidence&lt;/item&gt;
      &lt;item&gt;Integrate cloud and edge control into your workflows&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Why Use GRiSP Software?&lt;/head&gt;
    &lt;head rend="h2"&gt;GRiSP brings the power of Erlang/Elixir to embedded systems, making development efficient, scalable, and fault-tolerant&lt;/head&gt;
    &lt;head rend="h2"&gt;For Developers&lt;/head&gt;
    &lt;p&gt;GRiSP enables developers to run Erlang and Elixir on bare metal or embedded Linux, reducing complexity with minimal overhead and real-time capabilities. With GRiSP stacks and GRiSP-io, they can build and deploy robust, distributed applications optimized for embedded environments.&lt;/p&gt;
    &lt;head rend="h2"&gt;For IoT &amp;amp; Industrial Systems&lt;/head&gt;
    &lt;p&gt;From prototyping to production, GRiSP provides open-source tools that scale with project needs. Its real-time execution supports automation, robotics, and connected devices, while GRiSP-io enables remote management and monitoring of deployments.&lt;/p&gt;
    &lt;head rend="h1"&gt;Manage Embedded Systems with GRiSP-io&lt;/head&gt;
    &lt;p&gt;Deploy, monitor, and control your connected devices from anywhere. GRiSP-io brings cloud and edge integration to embedded applications, simplifying system management at scale.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45100499</guid></item><item><title>RubyMine is now free for non-commercial use</title><link>https://blog.jetbrains.com/ruby/2025/09/rubymine-is-now-free-for-non-commercial-use/</link><description>&lt;doc fingerprint="c4b5fff47a7a3f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RubyMine Is Now Free for Non-Commercial Use&lt;/head&gt;
    &lt;p&gt;Hold on to your helper methods ‚Äì RubyMine is now FREE for non-commercial use! Whether you‚Äôre learning Ruby and Rails, pushing open-source forward, creating dev content, or building your passion project, we want to make sure you have the tools to enjoy what you do even more‚Ä¶ for free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another chapter in the story&lt;/head&gt;
    &lt;p&gt;We recently introduced a new licensing model for WebStorm, RustRover, Rider, and CLion ‚Äì making them free for non-commercial use. RubyMine is now joining the party! For commercial use, our existing licensing model still applies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we doing this?&lt;/head&gt;
    &lt;p&gt;We believe developers do their best work when the right tools are accessible. We‚Äôve been listening closely to the Ruby and Rails community ‚Äì their feedback, success, challenges, and passion for building with joy. Now, we‚Äôre making a change that reflects what we‚Äôve heard.&lt;/p&gt;
    &lt;p&gt;By making RubyMine free for non-commercial use, we hope to lower the barrier to starting and help more people write clean, confident Ruby code from day one. It‚Äôs our way of supporting the unique Ruby community ‚Äì from those who choose Ruby for their projects to maintainers of gems and frameworks who contribute to the Ruby ecosystem. Whether you‚Äôre debugging at midnight, crafting clever DSLs, or launching your first Rails app, RubyMine is here to help you build smarter (and crash less).&lt;/p&gt;
    &lt;head rend="h2"&gt;Commercial vs. non-commercial use&lt;/head&gt;
    &lt;p&gt;As defined in the Toolbox Subscription Agreement for Non-Commercial Use, commercial use means developing products and earning commercial benefits from your activities. However, certain categories are explicitly excluded from this definition. Common examples of non-commercial uses include learning and self-education, open-source contributions without earning commercial benefits, any form of content creation, and hobby development.&lt;/p&gt;
    &lt;p&gt;It‚Äôs important to note that, if you‚Äôre using a non-commercial license, you cannot opt out of the collection of anonymous usage statistics. We use this information to improve our products. The data we collect is exclusively that of anonymous feature usages of our IDEs. It is focused on what actions are performed and what types of functionality of the IDE are used. We do not collect any other data. This is similar to our Early Access Program (EAP) and is in compliance with our Privacy Policy.&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Below are answers to the most common questions. Check out the full FAQ for more information.&lt;/p&gt;
    &lt;head rend="h3"&gt;Licensing&lt;/head&gt;
    &lt;head rend="h4"&gt;What features are included under the free license?&lt;/head&gt;
    &lt;p&gt;With the new non-commercial license type, you can enjoy a full-featured IDE that is identical to its paid version. The only difference is in the Code With Me feature ‚Äì you get Code With Me Community with your free license.&lt;/p&gt;
    &lt;head rend="h4"&gt;Which license should I choose if I want to use RubyMine for both non-commercial and commercial projects?&lt;/head&gt;
    &lt;p&gt;If you intend to use RubyMine for commercial development for which you will receive direct or indirect commercial advantage or monetary compensation within the meaning of the definitions provided in the Toolbox Subscription Agreement for Non-Commercial Use, you will need to purchase a commercial subscription (either individual or organizational). This license can then also be used for non-commercial development.&lt;/p&gt;
    &lt;head rend="h4"&gt;How do renewals and upgrades work now?&lt;/head&gt;
    &lt;p&gt;Non-commercial subscriptions are issued for one year and will automatically renew after that. However, for the renewal to happen, you must have used the assigned license at least once during the last 6 months of the subscription period. If it has been more than 6 months since you last used an IDE activated with this type of license and the renewal did not occur automatically, you can request a new non-commercial subscription again at any time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Am I eligible for a refund if I‚Äôve already bought a paid subscription but do non-commercial development?&lt;/head&gt;
    &lt;p&gt;If you‚Äôre unsure whether you qualify for a refund, you‚Äôll find full details of our policy here. Please note that if you also work on projects that qualify as commercial usage, you can‚Äôt use the free license for them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Anonymous data collection&lt;/head&gt;
    &lt;head rend="h4"&gt;Does my IDE send any data to JetBrains?&lt;/head&gt;
    &lt;p&gt;The terms of the non-commercial agreement assume that the product may also electronically send JetBrains anonymized statistics (IDE telemetry) related to your usage of the product‚Äôs features. This information may include but is not limited to frameworks, file templates used in the product, actions invoked, and other interactions with the product‚Äôs features. This information does not contain personal data.&lt;/p&gt;
    &lt;head rend="h4"&gt;Is there a way to opt out of sending anonymized statistics?&lt;/head&gt;
    &lt;p&gt;We appreciate that this might not be convenient for everyone, but there is unfortunately no way to opt out of sending anonymized statistics to JetBrains under the terms of the Toolbox agreement for non-commercial use. The only way to opt out is by switching to either a paid subscription or one of the complimentary options mentioned here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Getting a non-commercial subscription&lt;/head&gt;
    &lt;head rend="h4"&gt;What should I do to apply for this subscription?&lt;/head&gt;
    &lt;p&gt;It can be easily done right inside your IDE:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install RubyMine and run it.&lt;/item&gt;
      &lt;item&gt;Upon startup, there will be a license dialog box where you can choose the Non-commercial use option.&lt;/item&gt;
      &lt;item&gt;Log in to your JetBrains account or create a new one.&lt;/item&gt;
      &lt;item&gt;Accept the Toolbox Subscription Agreement for Non-Commercial Use.&lt;/item&gt;
      &lt;item&gt;Enjoy development in your IDE.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you‚Äôve already started a trial period or have activated your IDE using a paid license, you still can switch to a non-commercial subscription by following these steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Help | Register.&lt;/item&gt;
      &lt;item&gt;In the window that opens, click on the Deactivate License button.&lt;/item&gt;
      &lt;item&gt;Choose Non-commercial use.&lt;/item&gt;
      &lt;item&gt;Log in to your JetBrains account or create a new one.&lt;/item&gt;
      &lt;item&gt;Accept the Toolbox Subscription Agreement for Non-Commercial Use.&lt;/item&gt;
      &lt;item&gt;Enjoy development in your IDE.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;I don‚Äôt see the Non-commercial use option in my IDE. What should I do?&lt;/head&gt;
    &lt;p&gt;The most likely explanation for this is that you‚Äôre using an older version of RubyMine. Unfortunately, we don‚Äôt support obtaining the non-commercial license for any releases prior to RubyMine 2025.2.1.&lt;/p&gt;
    &lt;p&gt;That‚Äôs it for today! If you don‚Äôt find an answer to your question, feel free to leave a comment or contact us at sales@jetbrains.com.&lt;/p&gt;
    &lt;p&gt;The RubyMine team&lt;/p&gt;
    &lt;p&gt;JetBrains&lt;/p&gt;
    &lt;p&gt;Make it happen. With code.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45102186</guid></item><item><title>Show HN: Moribito ‚Äì A TUI for LDAP Viewing/Queries</title><link>https://github.com/ericschmar/moribito</link><description>&lt;doc fingerprint="3019ca5b5f87b9dc"&gt;
  &lt;main&gt;
    &lt;p&gt;A terminal-based LDAP server explorer built with Go and BubbleTea, providing an interactive interface for browsing LDAP directory trees, viewing records, and executing custom queries.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üå≤ Interactive Tree Navigation: Browse LDAP directory structure with keyboard/mouse&lt;/item&gt;
      &lt;item&gt;üìÑ Record Viewer: View detailed LDAP entry attributes&lt;/item&gt;
      &lt;item&gt;üìã Clipboard Integration: Copy attribute values to system clipboard&lt;/item&gt;
      &lt;item&gt;üîç Custom Query Interface: Execute custom LDAP queries with real-time results&lt;/item&gt;
      &lt;item&gt;üìñ Paginated Results: Efficient pagination for large result sets with automatic loading&lt;/item&gt;
      &lt;item&gt;‚öôÔ∏è Flexible Configuration: Support for config files and command-line options&lt;/item&gt;
      &lt;item&gt;üîê Secure Authentication: Support for SSL/TLS and various authentication methods&lt;/item&gt;
      &lt;item&gt;üîÑ Auto-Update Notifications: Optional checking for newer releases from GitHub&lt;/item&gt;
      &lt;item&gt;üé® Modern TUI: Clean, intuitive interface built with BubbleTea&lt;/item&gt;
      &lt;item&gt;üîÄ Multiple Connections: Save and switch between multiple LDAP server configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Initial startup screen with connection options&lt;/p&gt;
    &lt;p&gt;_Interface for adding new LDAP Connections&lt;/p&gt;
    &lt;p&gt;Browse LDAP directory structure with keyboard/mouse navigation&lt;/p&gt;
    &lt;p&gt;View detailed LDAP entry attributes with clipboard integration&lt;/p&gt;
    &lt;p&gt;Execute custom LDAP queries with real-time results and formatting&lt;/p&gt;
    &lt;code&gt;brew install ericschmar/tap/moribito&lt;/code&gt;
    &lt;code&gt;brew install https://raw.githubusercontent.com/ericschmar/moribito/main/homebrew/moribito.rb&lt;/code&gt;
    &lt;p&gt;Download the latest pre-built binary from GitHub Releases:&lt;/p&gt;
    &lt;p&gt;Linux/Unix:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install.sh | bash&lt;/code&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install-macos.sh | bash&lt;/code&gt;
    &lt;p&gt;Windows (PowerShell):&lt;/p&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/ericschmar/moribito/main/scripts/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;The install scripts will:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download the appropriate binary for your platform&lt;/item&gt;
      &lt;item&gt;Install it to the system PATH&lt;/item&gt;
      &lt;item&gt;Create OS-specific configuration directories&lt;/item&gt;
      &lt;item&gt;Generate sample configuration files&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Linux x86_64
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-linux-amd64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# Linux ARM64
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-linux-arm64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# macOS Intel
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-darwin-amd64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/

# macOS Apple Silicon
curl -L https://github.com/ericschmar/moribito/releases/latest/download/moribito-darwin-arm64 -o moribito
chmod +x moribito
sudo mv moribito /usr/local/bin/&lt;/code&gt;
    &lt;p&gt;For Windows, download &lt;code&gt;moribito-windows-amd64.exe&lt;/code&gt; from the releases page.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Homebrew is also available for Windows via WSL (Windows Subsystem for Linux). If you have WSL installed, you can use the Homebrew installation method above.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;git clone https://github.com/ericschmar/moribito
cd moribito
go build -o moribito cmd/moribito/main.go&lt;/code&gt;
    &lt;code&gt;# Connect with command line options
moribito -host ldap.example.com -base-dn "dc=example,dc=com" -user "cn=admin,dc=example,dc=com"

# Enable automatic update checking
moribito -check-updates -host ldap.example.com -base-dn "dc=example,dc=com"

# Use a configuration file
moribito -config /path/to/config.yaml

# Get help
moribito -help&lt;/code&gt;
    &lt;p&gt;Moribito will automatically look for configuration files in OS-specific locations:&lt;/p&gt;
    &lt;p&gt;Linux/Unix:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;~/.config/moribito/config.yaml&lt;/code&gt;(XDG config directory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito/config.yaml&lt;/code&gt;(user directory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito.yaml&lt;/code&gt;(user home file)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito/config.yaml&lt;/code&gt;(user directory)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/Library/Application Support/moribito/config.yaml&lt;/code&gt;(macOS standard)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~/.moribito.yaml&lt;/code&gt;(user home file)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;%APPDATA%\moribito\config.yaml&lt;/code&gt;(Windows standard)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;%USERPROFILE%\.moribito.yaml&lt;/code&gt;(user home file)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All platforms also check:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;./config.yaml&lt;/code&gt;(current directory)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use the built-in command to create a configuration file:&lt;/p&gt;
    &lt;code&gt;moribito --create-config&lt;/code&gt;
    &lt;p&gt;Or manually create a configuration file:&lt;/p&gt;
    &lt;code&gt;ldap:
    host: "ldap.example.com"
    port: 389
    base_dn: "dc=example,dc=com"
    use_ssl: false
    use_tls: false
    bind_user: "cn=admin,dc=example,dc=com"
    bind_pass: "your-password"
pagination:
    page_size: 50 # Number of entries per page
retry:
    enabled: true # Connection retries (default: true)
    max_attempts: 3 # Retry attempts (default: 3)
    initial_delay_ms: 500 # Initial delay (default: 500)
    max_delay_ms: 5000 # Max delay cap (default: 5000)&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tab - Switch between views (Tree ‚Üí Record ‚Üí Query ‚Üí Tree)&lt;/item&gt;
      &lt;item&gt;1/2/3 - Jump directly to Tree/Record/Query view&lt;/item&gt;
      &lt;item&gt;q - Quit application&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚Üë/‚Üì or k/j - Navigate up/down&lt;/item&gt;
      &lt;item&gt;Page Up/Down - Navigate by page&lt;/item&gt;
      &lt;item&gt;Home/End - Jump to top/bottom&lt;/item&gt;
      &lt;item&gt;‚Üí or l - Expand node (load children)&lt;/item&gt;
      &lt;item&gt;‚Üê or h - Collapse node&lt;/item&gt;
      &lt;item&gt;Enter - View record details&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚Üë/‚Üì or k/j - Scroll up/down&lt;/item&gt;
      &lt;item&gt;Page Up/Down - Scroll by page&lt;/item&gt;
      &lt;item&gt;Home/End - Jump to top/bottom&lt;/item&gt;
      &lt;item&gt;c - Copy current attribute value to clipboard&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;/ or Escape - Focus query input&lt;/item&gt;
      &lt;item&gt;Ctrl+Enter or Ctrl+J - Execute query&lt;/item&gt;
      &lt;item&gt;Ctrl+F - Format query with proper indentation&lt;/item&gt;
      &lt;item&gt;Escape - Clear query&lt;/item&gt;
      &lt;item&gt;Ctrl+V - Paste from clipboard&lt;/item&gt;
      &lt;item&gt;‚Üë/‚Üì - Navigate results (when not in input mode)&lt;/item&gt;
      &lt;item&gt;Page Up/Down - Navigate by page (automatically loads more results)&lt;/item&gt;
      &lt;item&gt;Enter - View selected record&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: The Query View uses automatic pagination to efficiently handle large result sets. When you scroll near the end of loaded results, the next page is automatically fetched from the LDAP server.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Ctrl+F key combination formats complex LDAP queries with proper indentation for better readability:&lt;/p&gt;
    &lt;code&gt;# Before formatting:
(&amp;amp;(objectClass=person)(|(cn=john*)(sn=smith*))(department=engineering))

# After formatting (Ctrl+F):
(&amp;amp;
  (objectClass=person)
  (|
    (cn=john*)
    (sn=smith*)
  )
  (department=engineering)
)
&lt;/code&gt;
    &lt;p&gt;The tool supports various LDAP authentication methods:&lt;/p&gt;
    &lt;code&gt;bind_user: "cn=admin,dc=example,dc=com"
bind_pass: "password"&lt;/code&gt;
    &lt;code&gt;bind_user: "uid=john,ou=users,dc=example,dc=com"
bind_pass: "password"&lt;/code&gt;
    &lt;code&gt;bind_user: "john@example.com"
bind_pass: "password"&lt;/code&gt;
    &lt;code&gt;# Leave bind_user and bind_pass empty or omit them&lt;/code&gt;
    &lt;code&gt;ldap:
    host: "ldaps.example.com"
    port: 636
    use_ssl: true&lt;/code&gt;
    &lt;code&gt;ldap:
    host: "ldap.example.com"
    port: 389
    use_tls: true&lt;/code&gt;
    &lt;p&gt;In the Query view, you can execute custom LDAP filters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;(objectClass=*)&lt;/code&gt;- All objects&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(objectClass=person)&lt;/code&gt;- All person objects&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(cn=john*)&lt;/code&gt;- Objects with cn starting with "john"&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(&amp;amp;(objectClass=person)(mail=*@example.com))&lt;/code&gt;- People with example.com emails&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;(|(cn=admin)(uid=admin))&lt;/code&gt;- Objects with cn=admin OR uid=admin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For complex nested queries, use Ctrl+F to automatically format them for better readability:&lt;/p&gt;
    &lt;p&gt;Simple queries remain unchanged:&lt;/p&gt;
    &lt;code&gt;(objectClass=person)
&lt;/code&gt;
    &lt;p&gt;Complex queries are formatted with proper indentation:&lt;/p&gt;
    &lt;code&gt;# Original
(&amp;amp;(objectClass=person)(|(cn=john*)(sn=smith*))(department=engineering))

# After Ctrl+F
(&amp;amp;
  (objectClass=person)
  (|
    (cn=john*)
    (sn=smith*)
  )
  (department=engineering)
)
&lt;/code&gt;
    &lt;p&gt;LDAP CLI uses intelligent pagination to provide optimal performance when working with large directories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default Page Size: 50 entries per page&lt;/item&gt;
      &lt;item&gt;Configurable: Adjust via config file or &lt;code&gt;--page-size&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;On-Demand Loading: Next pages load automatically as you scroll&lt;/item&gt;
      &lt;item&gt;Memory Efficient: Only loaded entries are kept in memory&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Command line override
moribito --page-size 100 --host ldap.example.com

# Configuration file
pagination:
  page_size: 25  # Smaller pages for slower networks&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smaller page sizes (10-25) for slower networks or limited LDAP servers&lt;/item&gt;
      &lt;item&gt;Larger page sizes (100-200) for fast networks and powerful LDAP servers&lt;/item&gt;
      &lt;item&gt;Use specific queries to reduce result sets instead of browsing all entries&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;LDAP CLI includes automatic retry functionality to handle connection failures gracefully:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default: Enabled with 3 retry attempts&lt;/item&gt;
      &lt;item&gt;Exponential Backoff: Delay doubles between attempts (500ms ‚Üí 1s ‚Üí 2s ‚Üí ...)&lt;/item&gt;
      &lt;item&gt;Connection Recovery: Automatically re-establishes broken connections&lt;/item&gt;
      &lt;item&gt;Smart Detection: Only retries connection-related errors, not authentication failures&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Default retry settings (automatically applied)
# No configuration needed - retries work out of the box&lt;/code&gt;
    &lt;code&gt;# Custom retry configuration
retry:
    enabled: true
    max_attempts: 5 # Maximum retry attempts (default: 3)
    initial_delay_ms: 1000 # Initial delay in milliseconds (default: 500)
    max_delay_ms: 10000 # Maximum delay cap (default: 5000)&lt;/code&gt;
    &lt;code&gt;# Disable retries if needed
retry:
    enabled: false&lt;/code&gt;
    &lt;p&gt;The system automatically retries for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network timeouts and connection drops&lt;/item&gt;
      &lt;item&gt;Connection refused errors&lt;/item&gt;
      &lt;item&gt;Server unavailable responses&lt;/item&gt;
      &lt;item&gt;Connection reset by peer&lt;/item&gt;
      &lt;item&gt;LDAP server down errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Authentication errors, invalid queries, and permission issues are not retried.&lt;/p&gt;
    &lt;code&gt;# Build for current platform
make build

# Build for all platforms
make build-all

# Clean build artifacts
make clean&lt;/code&gt;
    &lt;code&gt;# Format code
make fmt

# Run linter
make lint

# Run tests
make test

# Run all CI checks (format, lint, test, build)
make ci&lt;/code&gt;
    &lt;code&gt;go test ./...&lt;/code&gt;
    &lt;p&gt;This project uses GitHub Actions for CI/CD:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;CI Workflow: Runs on every push and pull request to&lt;/p&gt;&lt;code&gt;main&lt;/code&gt;and&lt;code&gt;develop&lt;/code&gt;branches&lt;list rend="ul"&gt;&lt;item&gt;Code formatting verification&lt;/item&gt;&lt;item&gt;Linting (with warnings)&lt;/item&gt;&lt;item&gt;Testing&lt;/item&gt;&lt;item&gt;Building for current platform&lt;/item&gt;&lt;item&gt;Multi-platform build artifacts (on main branch pushes)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release Workflow: Triggered by version tags (e.g.,&lt;/p&gt;&lt;code&gt;v1.0.0&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;Runs full CI checks&lt;/item&gt;&lt;item&gt;Builds for all platforms (Linux amd64/arm64, macOS amd64/arm64, Windows amd64)&lt;/item&gt;&lt;item&gt;Creates GitHub releases with binaries and checksums&lt;/item&gt;&lt;item&gt;Generates installation instructions&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BubbleTea - TUI framework&lt;/item&gt;
      &lt;item&gt;Lipgloss - Styling&lt;/item&gt;
      &lt;item&gt;go-ldap - LDAP client&lt;/item&gt;
      &lt;item&gt;golang.org/x/term - Terminal utilities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project includes full Homebrew support for easy installation on macOS and Linux. See the homebrew/ directory for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ready-to-use Homebrew formula&lt;/item&gt;
      &lt;item&gt;Formula generation and maintenance scripts&lt;/item&gt;
      &lt;item&gt;Documentation for creating custom taps&lt;/item&gt;
      &lt;item&gt;Instructions for submitting to homebrew-core&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project follows Semantic Versioning. See docs/versioning.md for details on the release process.&lt;/p&gt;
    &lt;p&gt;Comprehensive documentation is available using DocPress. To build and view the documentation:&lt;/p&gt;
    &lt;code&gt;# Build static documentation website
make docs

# Serve documentation locally with live reload
make docs-serve&lt;/code&gt;
    &lt;p&gt;The documentation covers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Installation and setup&lt;/item&gt;
      &lt;item&gt;Usage guide with examples&lt;/item&gt;
      &lt;item&gt;Interface navigation&lt;/item&gt;
      &lt;item&gt;Development setup&lt;/item&gt;
      &lt;item&gt;Contributing guidelines&lt;/item&gt;
      &lt;item&gt;API reference and advanced features&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Visit the generated documentation site for the complete guide.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create your feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Commit your changes (&lt;code&gt;git commit -m 'Add some amazing feature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Push to the branch (&lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45102664</guid></item><item><title>Passkeys and Modern Authentication</title><link>https://lucumr.pocoo.org/2025/9/2/passkeys/</link><description>&lt;doc fingerprint="ae4c91161f16337c"&gt;
  &lt;main&gt;
    &lt;p&gt;written on September 02, 2025&lt;/p&gt;
    &lt;p&gt;There is an ongoing trend in the industry to move people away from username and password towards passkeys. The intentions here are good, and I would assume that this has a significant net benefit for the average consumer. At the same time, the underlying standard has some peculiarities. These enable behaviors by large corporations, employers, and governments that are worth thinking about.&lt;/p&gt;
    &lt;p&gt;One potential source of problems here is the attestation system. It allows the authenticator to provide more information about what it is to the website that you‚Äôre authenticating with. In particular it is what tells a website if you have a Yubikey plugged in versus something like 1password. This is the mechanism by which the Austrian government, for instance, prevents you from using an Open Source or any other software-based authenticator to sign in to do your taxes, access medical records or do anything else that is protected by eID. Instead you have to buy a whitelisted hardware token.&lt;/p&gt;
    &lt;p&gt;Attestations themselves are not used by software authenticators today, or anything that syncs. Both Apple and Google do not expose attestation data in their own software authenticators (Keychain and Google Authenticator) for consumer passkeys. However, they will pass through attestation data from hardware tokens just fine. Both of them also, to the best of my knowledge, expose attestation data for enterprises through Mobile Device Management.&lt;/p&gt;
    &lt;p&gt;One could make the argument that it is unlikely that attestation data will be used at scale to create vendor lock-in. However, I‚Äôm not sufficiently convinced that this won‚Äôt create sub-ecosystems where we see exactly that happening. If for no other reason, this API exists and it has already been used to restrict keys for governmental sign-in systems.&lt;/p&gt;
    &lt;p&gt;One slightly more concerning issue today is that there is effectively no way to export private keys between authentication password managers. You need to enroll all of your ecosystems individually into a password manager. An attempt by an open source password manager to reveal private keys to the user was ruled insecure and should not be supported. This taking away agency from the user is not an accident. You can also see this with the passkey export specification which comes with a protocol that, while enabling exports in principle, encourages a system to system transfer that does not hand over the user‚Äôs credentials to the user. 1&lt;/p&gt;
    &lt;p&gt;This might be for good intentions, but it also creates problems. As someone recently trying to leave the Apple ecosystem step by step, I have noticed how many services are now bound to an iCloud-based passkey. Particularly when it comes to Apple, this fear is not entirely unwarranted. Sign-in with Apple using non-shared email addresses makes it very hard to migrate to Android unless you retain an iCloud subscription.&lt;/p&gt;
    &lt;p&gt;Obviously, one could pay for an authenticator like 1Password, which at least is ecosystem independent. However, not everybody is in a situation where they can afford to pay for basic services like password managers.&lt;/p&gt;
    &lt;p&gt;One reason why passkeys are adopted so well today is because it happens automatically for many. I discovered that non-technical family members now all have passkeys for some services, and they did not even notice doing that. A notable example is Amazon. After every sign-in, it attempts to enroll you into a passkey automatically without clear notification. It just brings up the fingerprint prompt, and users will instinctively touch it.&lt;/p&gt;
    &lt;p&gt;If you use different types of devices to authenticate ‚Äî for instance, a Windows and an iOS device ‚Äî you may eventually have both authenticators associated. This now covers the devices you already use. However, it can make moving to a completely different ecosystem later much harder.&lt;/p&gt;
    &lt;p&gt;For many years already, people lose access to their Google account every day and can never regain it. Google is well known for terminating accounts without stating any reasons. With that comes the loss of access to your data. In this case, you also lose your credentials for third-party websites.&lt;/p&gt;
    &lt;p&gt;There is no legal recourse for this and no mechanism for appeal. You just have to hope that you‚Äôre a good citizen and not doing anything that would upset Google‚Äôs account flagging systems.&lt;/p&gt;
    &lt;p&gt;As a sufficiently technical person, you might weigh the risks, but others will not. Many years ago, I tried to help another family gain access to their child‚Äôs Facebook account after they passed away. Even then, it was a bureaucratic nightmare where there was little support by Facebook to make it happen. There is a real risk that access becomes much harder for families. This is particularly true in situations where someone is incapacitated or dead. The more we move away from basic authentication systems, the worse this becomes. It‚Äôs also really inconvenient when you are not on your own devices. Signing into my accounts on my children‚Äôs devices has turned from a straightforward process to an incredibly frustrating experience. I find myself juggling all kinds of different apps and flows.&lt;/p&gt;
    &lt;p&gt;Every once in a while, I find myself in a situation where I have very little foundation to build on. This is mostly just because of a hobby. I like to see how things work and build them from scratch. Increasingly, that has become harder. Many username and password authentication schemes have been replaced with OAuth sign-ins over the years. Nowadays, some services are moving towards passkeys, though most places do not enforce these yet. If you want to build an operating system from scratch, or even just build a client yourself, you often find yourself needing to do a lot of yak-shaving. All this work is necessary just to get basic things working.&lt;/p&gt;
    &lt;p&gt;I think this is at least something to be wary of. It doesn‚Äôt mean that bad things will necessarily happen, but there is potential for loss of individual agency.&lt;/p&gt;
    &lt;p&gt;An accelerated version of this has been seen with email. Accessing your own personal IMAP account from Google today has been significantly restricted under security arguments. Getting OAuth credentials that can access someone‚Äôs IMAP accounts with their approval has become increasingly harder. It is also very costly.&lt;/p&gt;
    &lt;p&gt;Username and password authentication has largely been removed. Even the app-specific passwords on Google are now entirely undocumented. They are no longer exposed in the settings unless you know the link 2.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know. I am both a user of passkeys and generally wary of making myself overly dependent on tech giants and complex solutions. I‚Äôm noticing an increased reliance and potential loss of access to my own data. This does abstractly concern me. Not to the degree that it changes anything I‚Äôm doing, but still. As annoying as managing usernames and passwords was, I don‚Äôt think I have ever spent so much time authenticating on a daily basis. The systems that we now need to interface with for authentication are vast and complex.&lt;/p&gt;
    &lt;p&gt;This might just be the path we‚Äôre going. However, it is also one where we maybe want to reflect a little bit on whether this is really what we want.&lt;/p&gt;
    &lt;p&gt;Edit: I reworded the statement about pass key exports to not misrepresent the original comment on GitHub.&lt;/p&gt;
    &lt;p&gt;The details can be debated, but the protocol explicitly does not permit a user to just hold on to a symmetrically encrypted export (or even a plain text one). The best option is the HPKE scheme.‚Ü©&lt;/p&gt;
    &lt;p&gt;This OAuth dependency also puts Open Source projects in an interesting situation. For instance, the Thunderbird client ships with OAuth credentials for Google when you download it from Mozilla. However, if you self-compile it, you don‚Äôt have that access.‚Ü©&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45103065</guid></item><item><title>The Little Book of Linear Algebra</title><link>https://github.com/the-litte-book-of/linear-algebra</link><description>&lt;doc fingerprint="b70d9b7e132f9799"&gt;
  &lt;main&gt;
    &lt;p&gt;A concise, beginner-friendly introduction to the core ideas of linear algebra.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download PDF ‚Äì print-ready version&lt;/item&gt;
      &lt;item&gt;Download EPUB ‚Äì e-reader friendly&lt;/item&gt;
      &lt;item&gt;View LaTeX ‚Äì Latex source&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A scalar is a single numerical quantity, most often taken from the real numbers, denoted by &lt;/p&gt;
    &lt;p&gt;An element of &lt;/p&gt;
    &lt;p&gt;Example 1.1.1.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A 2-dimensional vector: &lt;math-renderer&gt;$(3, -1) \in \mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;A 3-dimensional vector: &lt;math-renderer&gt;$(2, 0, 5) \in \mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;A 1-dimensional vector: &lt;math-renderer&gt;$(7) \in \mathbb{R}^1$&lt;/math-renderer&gt;, which corresponds to the scalar&lt;math-renderer&gt;$7$&lt;/math-renderer&gt;itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vectors are often written vertically in column form, which emphasizes their role in matrix multiplication:&lt;/p&gt;
    &lt;p&gt;The vertical layout makes the structure clearer when we consider linear combinations or multiply matrices by vectors.&lt;/p&gt;
    &lt;p&gt;In &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As a point in space, described by its coordinates.&lt;/item&gt;
      &lt;item&gt;As a displacement or arrow, described by a direction and a length.&lt;/item&gt;
      &lt;item&gt;As an abstract element of a vector space, whose properties follow algebraic rules independent of geometry.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vectors are written in boldface lowercase letters: &lt;math-renderer&gt;$\mathbf{v}, \mathbf{w}, \mathbf{x}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The i-th entry of a vector &lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;is written&lt;math-renderer&gt;$v_i$&lt;/math-renderer&gt;, where indices begin at 1.&lt;/item&gt;
      &lt;item&gt;The set of all n-dimensional vectors over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;is denoted&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Column vectors will be the default form unless otherwise stated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scalars and vectors form the atoms of linear algebra. Every structure we will build-vector spaces, linear transformations, matrices, eigenvalues-relies on the basic notions of number and ordered collection of numbers. Once vectors are understood, we can define operations such as addition and scalar multiplication, then generalize to subspaces, bases, and coordinate systems. Eventually, this framework grows into the full theory of linear algebra, with powerful applications to geometry, computation, and data.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write three different vectors in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;and sketch them as arrows from the origin. Identify their coordinates explicitly.&lt;/item&gt;
      &lt;item&gt;Give an example of a vector in &lt;math-renderer&gt;$\mathbb{R}^4$&lt;/math-renderer&gt;. Can you visualize it directly? Explain why high-dimensional visualization is challenging.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$\mathbf{v} = (4, -3, 2)$&lt;/math-renderer&gt;. Write&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;in column form and state&lt;math-renderer&gt;$v_1, v_2, v_3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In what sense is the set &lt;math-renderer&gt;$\mathbb{R}^1$&lt;/math-renderer&gt;both a line and a vector space? Illustrate with examples.&lt;/item&gt;
      &lt;item&gt;Consider the vector &lt;math-renderer&gt;$\mathbf{u} = (1,1,\dots,1) \in \mathbb{R}^n$&lt;/math-renderer&gt;. What is special about this vector when&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;is large? What might it represent in applications?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vectors in linear algebra are not static objects; their power comes from the operations we can perform on them. Two fundamental operations define the structure of vector spaces: addition and scalar multiplication. These operations satisfy simple but far-reaching rules that underpin the entire subject.&lt;/p&gt;
    &lt;p&gt;Given two vectors of the same dimension, their sum is obtained by adding corresponding entries. Formally, if&lt;/p&gt;
    &lt;p&gt;then their sum is&lt;/p&gt;
    &lt;p&gt;Example 1.2.1. Let &lt;/p&gt;
    &lt;p&gt;Geometrically, vector addition corresponds to the parallelogram rule. If we draw both vectors as arrows from the origin, then placing the tail of one vector at the head of the other produces the sum. The diagonal of the parallelogram they form represents the resulting vector.&lt;/p&gt;
    &lt;p&gt;Multiplying a vector by a scalar stretches or shrinks the vector while preserving its direction, unless the scalar is negative, in which case the vector is also reversed. If &lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Example 1.2.2. Let &lt;/p&gt;
    &lt;p&gt;This corresponds to flipping the vector through the origin and doubling its length.&lt;/p&gt;
    &lt;p&gt;The interaction of addition and scalar multiplication allows us to form linear combinations. A linear combination of vectors &lt;/p&gt;
    &lt;p&gt;Linear combinations are the mechanism by which we generate new vectors from existing ones. The span of a set of vectors-the collection of all their linear combinations-will later lead us to the idea of a subspace.&lt;/p&gt;
    &lt;p&gt;Example 1.2.3. Let &lt;/p&gt;
    &lt;p&gt;Thus &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Addition: &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v}$&lt;/math-renderer&gt;means component-wise addition.&lt;/item&gt;
      &lt;item&gt;Scalar multiplication: &lt;math-renderer&gt;$c\mathbf{v}$&lt;/math-renderer&gt;scales each entry of&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;by&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Linear combination: a sum of the form &lt;math-renderer&gt;$c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vector addition and scalar multiplication are the defining operations of linear algebra. They give structure to vector spaces, allow us to describe geometric phenomena like translation and scaling, and provide the foundation for solving systems of equations. Everything that follows-basis, dimension, transformations-builds on these simple but profound rules.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v}$&lt;/math-renderer&gt;where&lt;math-renderer&gt;$\mathbf{u} = (1,2,3)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v} = (4, -1, 0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find &lt;math-renderer&gt;$3\mathbf{v}$&lt;/math-renderer&gt;where&lt;math-renderer&gt;$\mathbf{v} = (-2,5)$&lt;/math-renderer&gt;. Sketch both vectors to illustrate the scaling.&lt;/item&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$(5,7)$&lt;/math-renderer&gt;can be written as a linear combination of&lt;math-renderer&gt;$(1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Write &lt;math-renderer&gt;$(4,4)$&lt;/math-renderer&gt;as a linear combination of&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$&lt;/math-renderer&gt;, then&lt;math-renderer&gt;$(c+d)(\mathbf{u}+\mathbf{v}) = c\mathbf{u} + c\mathbf{v} + d\mathbf{u} + d\mathbf{v}$&lt;/math-renderer&gt;for scalars&lt;math-renderer&gt;$c,d \in \mathbb{R}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dot product is the fundamental operation that links algebra and geometry in vector spaces. It allows us to measure lengths, compute angles, and determine orthogonality. From this single definition flow the notions of norm and angle, which give geometry to abstract vector spaces.&lt;/p&gt;
    &lt;p&gt;For two vectors in &lt;/p&gt;
    &lt;p&gt;Equivalently, in matrix notation:&lt;/p&gt;
    &lt;p&gt;Example 1.3.1. Let &lt;/p&gt;
    &lt;p&gt;The dot product outputs a single scalar, not another vector.&lt;/p&gt;
    &lt;p&gt;The Euclidean norm of a vector is the square root of its dot product with itself:&lt;/p&gt;
    &lt;p&gt;This generalizes the Pythagorean theorem to arbitrary dimensions.&lt;/p&gt;
    &lt;p&gt;Example 1.3.2. For &lt;/p&gt;
    &lt;p&gt;This is exactly the length of the vector as an arrow in the plane.&lt;/p&gt;
    &lt;p&gt;The dot product also encodes the angle between two vectors. For nonzero vectors &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Example 1.3.3. Let &lt;/p&gt;
    &lt;p&gt;Hence&lt;/p&gt;
    &lt;p&gt;The vectors are perpendicular.&lt;/p&gt;
    &lt;p&gt;Two vectors are said to be orthogonal if their dot product is zero:&lt;/p&gt;
    &lt;p&gt;Orthogonality generalizes the idea of perpendicularity from geometry to higher dimensions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dot product: &lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Norm (length): &lt;math-renderer&gt;$|\mathbf{v}|$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Orthogonality: &lt;math-renderer&gt;$\mathbf{u} \perp \mathbf{v}$&lt;/math-renderer&gt;if&lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v} = 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The dot product turns vector spaces into geometric objects: vectors gain lengths, angles, and notions of perpendicularity. This foundation will later support the study of orthogonal projections, Gram‚ÄìSchmidt orthogonalization, eigenvectors, and least squares problems.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$\mathbf{u} \cdot \mathbf{v}$&lt;/math-renderer&gt;for&lt;math-renderer&gt;$\mathbf{u} = (1,2,3)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$\mathbf{v} = (4,5,6)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the norm of &lt;math-renderer&gt;$\mathbf{v} = (2, -2, 1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine whether &lt;math-renderer&gt;$\mathbf{u} = (1,1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v} = (1,-1,2)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$\mathbf{u} = (3,4)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$\mathbf{v} = (4,3)$&lt;/math-renderer&gt;. Compute the angle between them.&lt;/item&gt;
      &lt;item&gt;Prove that &lt;math-renderer&gt;$|\mathbf{u} + \mathbf{v}|^2 = |\mathbf{u}|^2 + |\mathbf{v}|^2 + 2\mathbf{u}\cdot \mathbf{v}$&lt;/math-renderer&gt;. This identity is the algebraic version of the Law of Cosines.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonality captures the notion of perpendicularity in vector spaces. It is one of the most important geometric ideas in linear algebra, allowing us to decompose vectors, define projections, and construct special bases with elegant properties.&lt;/p&gt;
    &lt;p&gt;Two vectors &lt;/p&gt;
    &lt;p&gt;This condition ensures that the angle between them is &lt;/p&gt;
    &lt;p&gt;Example 1.4.1. In &lt;/p&gt;
    &lt;p&gt;A collection of vectors is called orthogonal if every distinct pair of vectors in the set is orthogonal. If, in addition, each vector has norm 1, the set is called orthonormal.&lt;/p&gt;
    &lt;p&gt;Example 1.4.2. In &lt;/p&gt;
    &lt;p&gt;form an orthonormal set: each has length 1, and their dot products vanish when the indices differ.&lt;/p&gt;
    &lt;p&gt;Orthogonality makes possible the decomposition of a vector into two components: one parallel to another vector, and one orthogonal to it. Given a nonzero vector &lt;/p&gt;
    &lt;p&gt;The difference&lt;/p&gt;
    &lt;p&gt;is orthogonal to &lt;/p&gt;
    &lt;p&gt;Example 1.4.3. Let &lt;/p&gt;
    &lt;p&gt;Thus&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;In general, if &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{v} = \text{proj}{\mathbf{u}}(\mathbf{v}) + \big(\mathbf{v} - \text{proj}{\mathbf{u}}(\mathbf{v})\big), $$&lt;/p&gt;
    &lt;p&gt;where the first term is parallel to &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{u} \perp \mathbf{v}$&lt;/math-renderer&gt;: vectors&lt;math-renderer&gt;$\mathbf{u}$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;An orthogonal set: vectors pairwise orthogonal.&lt;/item&gt;
      &lt;item&gt;An orthonormal set: pairwise orthogonal, each of norm 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonality gives structure to vector spaces. It provides a way to separate independent directions cleanly, simplify computations, and minimize errors in approximations. Many powerful algorithms in numerical linear algebra and data science (QR decomposition, least squares regression, PCA) rely on orthogonality.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that the vectors &lt;math-renderer&gt;$(1,2,2)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(2,0,-1)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Find the projection of &lt;math-renderer&gt;$(3,4)$&lt;/math-renderer&gt;onto&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that any two distinct standard basis vectors in &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;Decompose &lt;math-renderer&gt;$(5,2)$&lt;/math-renderer&gt;into components parallel and orthogonal to&lt;math-renderer&gt;$(2,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v}$&lt;/math-renderer&gt;are orthogonal and nonzero, then&lt;math-renderer&gt;$(\mathbf{u}+\mathbf{v})\cdot(\mathbf{u}-\mathbf{v}) = 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrices are the central objects of linear algebra, providing a compact way to represent and manipulate linear transformations, systems of equations, and structured data. A matrix is a rectangular array of numbers arranged in rows and columns.&lt;/p&gt;
    &lt;p&gt;An &lt;/p&gt;
    &lt;p&gt;Each entry &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$m = n$&lt;/math-renderer&gt;, the matrix is square.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$m = 1$&lt;/math-renderer&gt;, the matrix is a row vector.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$n = 1$&lt;/math-renderer&gt;, the matrix is a column vector.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, vectors are simply special cases of matrices.&lt;/p&gt;
    &lt;p&gt;Example 2.1.1. A &lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Example 2.1.2. A &lt;/p&gt;
    &lt;p&gt;This will later serve as the representation of a linear transformation on &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrices are denoted by uppercase bold letters: &lt;math-renderer&gt;$A, B, C$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Entries are written as &lt;math-renderer&gt;$a_{ij}$&lt;/math-renderer&gt;, with the row index first, column index second.&lt;/item&gt;
      &lt;item&gt;The set of all real &lt;math-renderer&gt;$m \times n$&lt;/math-renderer&gt;matrices is denoted&lt;math-renderer&gt;$\mathbb{R}^{m \times n}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, a matrix is a function &lt;/p&gt;
    &lt;p&gt;Matrices generalize vectors and give us a language for describing linear operations systematically. They encode systems of equations, rotations, projections, and transformations of data. With matrices, algebra and geometry come together: a single compact object can represent both numerical data and functional rules.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write a &lt;math-renderer&gt;$3 \times 2$&lt;/math-renderer&gt;matrix of your choice and identify its entries&lt;math-renderer&gt;$a_{ij}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Is every vector a matrix? Is every matrix a vector? Explain.&lt;/item&gt;
      &lt;item&gt;Which of the following are square matrices: &lt;math-renderer&gt;$A \in \mathbb{R}^{4\times4}$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$B \in \mathbb{R}^{3\times5}$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$C \in \mathbb{R}^{1\times1}$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Let $D = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 1 \end{bmatrix}$. What kind of matrix is this?&lt;/item&gt;
      &lt;item&gt;Consider the matrix $E = \begin{bmatrix} a &amp;amp; b \ c &amp;amp; d \end{bmatrix}$. Express &lt;math-renderer&gt;$e_{11}, e_{12}, e_{21}, e_{22}$&lt;/math-renderer&gt;explicitly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once matrices are defined, the next step is to understand how they combine. Just as vectors gain meaning through addition and scalar multiplication, matrices become powerful through two operations: addition and multiplication.&lt;/p&gt;
    &lt;p&gt;Two matrices of the same size are added by adding corresponding entries. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Example 2.2.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} 0 &amp;amp; 2 \ 8 &amp;amp; 6 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Matrix addition is commutative (&lt;/p&gt;
    &lt;p&gt;For a scalar &lt;/p&gt;
    &lt;p&gt;This stretches or shrinks all entries of the matrix uniformly.&lt;/p&gt;
    &lt;p&gt;Example 2.2.2. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;The defining operation of matrices is multiplication. If&lt;/p&gt;
    &lt;p&gt;then their product is the &lt;/p&gt;
    &lt;p&gt;Thus, the entry in the &lt;/p&gt;
    &lt;p&gt;Example 2.2.3. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;head rend="h1"&gt;$$ AB = \begin{bmatrix} 1\cdot4 + 2\cdot2 &amp;amp; 1\cdot(-1) + 2\cdot5 \ 0\cdot4 + 3\cdot2 &amp;amp; 0\cdot(-1) + 3\cdot5 \end{bmatrix}&lt;/head&gt;
    &lt;p&gt;\begin{bmatrix} 8 &amp;amp; 9 \ 6 &amp;amp; 15 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Notice that matrix multiplication is not commutative in general: &lt;/p&gt;
    &lt;p&gt;Matrix multiplication corresponds to the composition of linear transformations. If &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Matrix sum: &lt;math-renderer&gt;$A+B$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Scalar multiple: &lt;math-renderer&gt;$cA$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Product: &lt;math-renderer&gt;$AB$&lt;/math-renderer&gt;, defined only when the number of columns of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;equals the number of rows of&lt;math-renderer&gt;$B$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Matrix multiplication is the core mechanism of linear algebra: it encodes how transformations combine, how systems of equations are solved, and how data flows in modern algorithms. Addition and scalar multiplication make matrices into a vector space, while multiplication gives them an algebraic structure rich enough to model geometry, computation, and networks.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute &lt;math-renderer&gt;$A+B$&lt;/math-renderer&gt;for&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find &lt;math-renderer&gt;$3A$&lt;/math-renderer&gt;where&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multiply&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify with an explicit example that &lt;math-renderer&gt;$AB \neq BA$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that matrix multiplication is distributive: &lt;math-renderer&gt;$A(B+C) = AB + AC$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two special operations on matrices-the transpose and the inverse-give rise to deep algebraic and geometric properties. The transpose rearranges a matrix by flipping it across its main diagonal, while the inverse, when it exists, acts as the undo operation for matrix multiplication.&lt;/p&gt;
    &lt;p&gt;The transpose of an &lt;/p&gt;
    &lt;p&gt;Formally,&lt;/p&gt;
    &lt;p&gt;$$ (A^T){ij} = a{ji}. $$&lt;/p&gt;
    &lt;p&gt;Example 2.3.1. If&lt;/p&gt;
    &lt;p&gt;then&lt;/p&gt;
    &lt;p&gt;Properties of the Transpose.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;$ (A^T)^T = A$.&lt;/item&gt;
      &lt;item&gt;$ (A+B)^T = A^T + B^T$.&lt;/item&gt;
      &lt;item&gt;$ (cA)^T = cA^T$, for scalar &lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;$ (AB)^T = B^T A^T$.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last rule is crucial: the order reverses.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Not every matrix is invertible. A necessary condition is that &lt;/p&gt;
    &lt;p&gt;Example 2.3.2. Let&lt;/p&gt;
    &lt;p&gt;Its determinant is &lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} -2 &amp;amp; 1 \ 1.5 &amp;amp; -0.5 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Verification:&lt;/p&gt;
    &lt;head rend="h1"&gt;$$ AA^{-1} = \begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; 4 \end{bmatrix} \begin{bmatrix} -2 &amp;amp; 1 \ 1.5 &amp;amp; -0.5 \end{bmatrix}&lt;/head&gt;
    &lt;p&gt;\begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 1 \end{bmatrix}. $$&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The transpose corresponds to reflecting a linear transformation across the diagonal. For vectors, it switches between row and column forms.&lt;/item&gt;
      &lt;item&gt;The inverse, when it exists, corresponds to reversing a linear transformation. For example, if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;scales and rotates vectors,&lt;math-renderer&gt;$A^{-1}$&lt;/math-renderer&gt;rescales and rotates them back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transpose: &lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Inverse: &lt;math-renderer&gt;$A^{-1}$&lt;/math-renderer&gt;, defined only for invertible square matrices.&lt;/item&gt;
      &lt;item&gt;Identity: &lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;, acts as the multiplicative identity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The transpose allows us to define symmetric and orthogonal matrices, central to geometry and numerical methods. The inverse underlies the solution of linear systems, encoding the idea of undoing a transformation. Together, these operations set the stage for determinants, eigenvalues, and orthogonalization.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the transpose of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$(AB)^T = B^T A^T$&lt;/math-renderer&gt;for&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine whether&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is invertible. If so, find &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the inverse of&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and explain its geometric action on vectors in the plane.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove that if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible, then so is&lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;, and&lt;math-renderer&gt;$(A^T)^{-1} = (A^{-1})^T$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Certain matrices occur so frequently in theory and applications that they are given special names. Recognizing their properties allows us to simplify computations and understand the structure of linear transformations more clearly.&lt;/p&gt;
    &lt;p&gt;The identity matrix &lt;/p&gt;
    &lt;p&gt;It acts as the multiplicative identity:&lt;/p&gt;
    &lt;p&gt;Geometrically, &lt;/p&gt;
    &lt;p&gt;A diagonal matrix has all off-diagonal entries zero:&lt;/p&gt;
    &lt;p&gt;Multiplication by a diagonal matrix scales each coordinate independently:&lt;/p&gt;
    &lt;p&gt;Example 2.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;A permutation matrix is obtained by permuting the rows of the identity matrix. Multiplying a vector by a permutation matrix reorders its coordinates.&lt;/p&gt;
    &lt;p&gt;Example 2.4.2. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;p&gt;Permutation matrices are always invertible; their inverses are simply their transposes.&lt;/p&gt;
    &lt;p&gt;A matrix is symmetric if&lt;/p&gt;
    &lt;p&gt;and skew-symmetric if&lt;/p&gt;
    &lt;p&gt;Symmetric matrices appear in quadratic forms and optimization, while skew-symmetric matrices describe rotations and cross products in geometry.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;Equivalently, the rows (and columns) of &lt;/p&gt;
    &lt;p&gt;Example 2.4.3. The rotation matrix in the plane:&lt;/p&gt;
    &lt;p&gt;is orthogonal, since&lt;/p&gt;
    &lt;p&gt;Special matrices serve as the building blocks of linear algebra. Identity matrices define the neutral element, diagonal matrices simplify computations, permutation matrices reorder data, symmetric and orthogonal matrices describe fundamental geometric structures. Much of modern applied mathematics reduces complex problems to operations involving these simple forms.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that the product of two diagonal matrices is diagonal, and compute an example.&lt;/item&gt;
      &lt;item&gt;Find the permutation matrix that cycles &lt;math-renderer&gt;$(a,b,c)$&lt;/math-renderer&gt;into&lt;math-renderer&gt;$(b,c,a)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that every permutation matrix is invertible and its inverse is its transpose.&lt;/item&gt;
      &lt;item&gt;Verify that&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is orthogonal. What geometric transformation does it represent? 5. Determine whether&lt;/p&gt;
    &lt;p&gt;are symmetric, skew-symmetric, or neither.&lt;/p&gt;
    &lt;p&gt;One of the central motivations for linear algebra is solving systems of linear equations. These systems arise naturally in science, engineering, and data analysis whenever multiple constraints interact. Matrices provide a compact language for expressing and solving them.&lt;/p&gt;
    &lt;p&gt;A linear system consists of equations where each unknown appears only to the first power and with no products between variables. A general system of &lt;/p&gt;
    &lt;p&gt;Here the coefficients &lt;/p&gt;
    &lt;p&gt;The system can be expressed compactly as:&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$A \in \mathbb{R}^{m \times n}$&lt;/math-renderer&gt;is the coefficient matrix&lt;math-renderer&gt;$[a_{ij}]$&lt;/math-renderer&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{x} \in \mathbb{R}^n$&lt;/math-renderer&gt;is the column vector of unknowns,&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{b} \in \mathbb{R}^m$&lt;/math-renderer&gt;is the column vector of constants.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This formulation turns the problem of solving equations into analyzing the action of a matrix.&lt;/p&gt;
    &lt;p&gt;Example 3.1.1. The system&lt;/p&gt;
    &lt;p&gt;can be written as&lt;/p&gt;
    &lt;p&gt;\begin{bmatrix} 5 \ 4 \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;A linear system may have:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;No solution (inconsistent): The equations conflict. Example: $ \begin{cases} x + y = 1 \ x + y = 2 \end{cases} $ has no solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Exactly one solution (unique): The system‚Äôs equations intersect at a single point. Example: The above system with coefficient matrix $ \begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; -1 \end{bmatrix} $ has a unique solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Infinitely many solutions: The equations describe overlapping constraints (e.g., multiple equations representing the same line or plane).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The nature of the solution depends on the rank of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;, each linear equation represents a line. Solving a system means finding intersection points of lines.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, each equation represents a plane. A system may have no solution (parallel planes), one solution (a unique intersection point), or infinitely many (a line of intersection).&lt;/item&gt;
      &lt;item&gt;In higher dimensions, the picture generalizes: solutions form intersections of hyperplanes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear systems are the practical foundation of linear algebra. They appear in balancing chemical reactions, circuit analysis, least-squares regression, optimization, and computer graphics. Understanding how to represent and classify their solutions is the first step toward systematic solution methods like Gaussian elimination.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Write the following system in matrix form: $ \begin{cases} 2x + 3y - z = 7, \ x - y + 4z = 1, \ 3x + 2y + z = 5 \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether the system $ \begin{cases} x + y = 1, \ 2x + 2y = 2 \end{cases} $ has no solution, one solution, or infinitely many solutions.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Geometrically interpret the system $ \begin{cases} x + y = 3, \ x - y = 1 \end{cases} $ in the plane.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solve the system $ \begin{cases} 2x + y = 1, \ x - y = 4 \end{cases} $ and check your solution.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, describe the solution set of $ \begin{cases} x + y + z = 0, \ 2x + 2y + 2z = 0 \end{cases} $. What geometric object does it represent?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To solve linear systems efficiently, we use Gaussian elimination: a systematic method of transforming a system into a simpler equivalent one whose solutions are easier to see. The method relies on elementary row operations that preserve the solution set.&lt;/p&gt;
    &lt;p&gt;On an augmented matrix &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row swapping: interchange two rows.&lt;/item&gt;
      &lt;item&gt;Row scaling: multiply a row by a nonzero scalar.&lt;/item&gt;
      &lt;item&gt;Row replacement: replace one row by itself plus a multiple of another row.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations correspond to re-expressing equations in different but equivalent forms.&lt;/p&gt;
    &lt;p&gt;A matrix is in row echelon form (REF) if:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All nonzero rows are above any zero rows.&lt;/item&gt;
      &lt;item&gt;Each leading entry (the first nonzero number from the left in a row) is to the right of the leading entry in the row above.&lt;/item&gt;
      &lt;item&gt;All entries below a leading entry are zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Further, if each leading entry is 1 and is the only nonzero entry in its column, the matrix is in reduced row echelon form (RREF).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the augmented matrix for the system.&lt;/item&gt;
      &lt;item&gt;Use row operations to create zeros below each pivot (the leading entry in a row).&lt;/item&gt;
      &lt;item&gt;Continue column by column until the matrix is in echelon form.&lt;/item&gt;
      &lt;item&gt;Solve by back substitution: starting from the last pivot equation and working upward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we continue to RREF, the solution can be read off directly.&lt;/p&gt;
    &lt;p&gt;Example 3.2.1. Solve&lt;/p&gt;
    &lt;p&gt;Step 1. Augmented matrix&lt;/p&gt;
    &lt;p&gt;Step 2. Eliminate below the first pivot&lt;/p&gt;
    &lt;p&gt;Subtract 2 times row 1 from row 2, and 3 times row 1 from row 3:&lt;/p&gt;
    &lt;p&gt;Step 3. Pivot in column 2&lt;/p&gt;
    &lt;p&gt;Divide row 2 by -3:&lt;/p&gt;
    &lt;p&gt;Add 7 times row 2 to row 3:&lt;/p&gt;
    &lt;p&gt;Step 4. Pivot in column 3&lt;/p&gt;
    &lt;p&gt;Divide row 3 by -2:&lt;/p&gt;
    &lt;p&gt;Step 5. Back substitution&lt;/p&gt;
    &lt;p&gt;From the last row: $ z = \tfrac{11}{3}. $&lt;/p&gt;
    &lt;p&gt;Second row: $ y - z = -\tfrac{1}{3} \implies y = -\tfrac{1}{3} + \tfrac{11}{3} = \tfrac{10}{3}. $&lt;/p&gt;
    &lt;p&gt;First row: $ x + 2y - z = 3 \implies x + 2\cdot\tfrac{10}{3} - \tfrac{11}{3} = 3. $&lt;/p&gt;
    &lt;p&gt;So $ x + \tfrac{20}{3} - \tfrac{11}{3} = 3 \implies x + 3 = 3 \implies x = 0. $&lt;/p&gt;
    &lt;p&gt;Solution: $ (x,y,z) = \big(0, \tfrac{10}{3}, \tfrac{11}{3}\big). $&lt;/p&gt;
    &lt;p&gt;Gaussian elimination is the foundation of computational linear algebra. It reduces complex systems to a form where solutions are visible, and it forms the basis for algorithms used in numerical analysis, scientific computing, and machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve by Gaussian elimination: $ \begin{cases} x + y = 2, \ 2x - y = 0. \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reduce the following augmented matrix to REF: $ \left[\begin{array}{ccc|c} 1 &amp;amp; 1 &amp;amp; 1 &amp;amp; 6 \ 2 &amp;amp; -1 &amp;amp; 3 &amp;amp; 14 \ 1 &amp;amp; 4 &amp;amp; -2 &amp;amp; -2 \end{array}\right]. $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that Gaussian elimination always produces either:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;a unique solution,&lt;/item&gt;
          &lt;item&gt;infinitely many solutions, or&lt;/item&gt;
          &lt;item&gt;a contradiction (no solution).&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Gaussian elimination to find all solutions of $ \begin{cases} x + y + z = 0, \ 2x + y + z = 1. \end{cases} $&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why pivoting (choosing the largest available pivot element) is useful in numerical computation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Gaussian elimination not only provides solutions but also reveals the structure of a linear system. Two key ideas are the rank of a matrix and the consistency of a system. Rank measures the amount of independent information in the equations, while consistency determines whether the system has at least one solution.&lt;/p&gt;
    &lt;p&gt;The rank of a matrix is the number of leading pivots in its row echelon form. Equivalently, it is the maximum number of linearly independent rows or columns.&lt;/p&gt;
    &lt;p&gt;Formally,&lt;/p&gt;
    &lt;p&gt;The rank tells us the effective dimension of the space spanned by the rows (or columns).&lt;/p&gt;
    &lt;p&gt;Example 3.3.1. For&lt;/p&gt;
    &lt;p&gt;row reduction gives&lt;/p&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;p&gt;Consider the system &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = \text{rank}(A|\mathbf{b}) = n$&lt;/math-renderer&gt;(number of unknowns), the system has a unique solution.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = \text{rank}(A|\mathbf{b}) &amp;amp;lt; n$&lt;/math-renderer&gt;, the system has infinitely many solutions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 3.3.2. Consider&lt;/p&gt;
    &lt;p&gt;The augmented matrix is&lt;/p&gt;
    &lt;p&gt;Row reduction gives&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Example 3.3.3. For&lt;/p&gt;
    &lt;p&gt;the augmented matrix reduces to&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Rank is a measure of independence: it tells us how many truly distinct equations or directions are present. Consistency explains when equations align versus when they contradict. These concepts connect linear systems to vector spaces and prepare for the ideas of dimension, basis, and the Rank‚ÄìNullity Theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the rank of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Determine whether the system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;is consistent.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Show that the rank of the identity matrix&lt;/p&gt;&lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;is&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Give an example of a system in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;with infinitely many solutions, and explain why it satisfies the rank condition.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that for any matrix&lt;/p&gt;&lt;math-renderer&gt;$A \in \mathbb{R}^{m \times n}$&lt;/math-renderer&gt;, $ \text{rank}(A) \leq \min(m,n). $&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A homogeneous system is a linear system in which all constant terms are zero:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Every homogeneous system has at least one solution:&lt;/p&gt;
    &lt;p&gt;This is called the trivial solution. The interesting question is whether nontrivial solutions (nonzero vectors) exist.&lt;/p&gt;
    &lt;p&gt;Nontrivial solutions exist precisely when the number of unknowns exceeds the rank of the coefficient matrix:&lt;/p&gt;
    &lt;p&gt;In this case, there are infinitely many solutions, forming a subspace of &lt;/p&gt;
    &lt;p&gt;where null(A) is the set of all solutions to &lt;/p&gt;
    &lt;p&gt;Example 3.4.1. Consider&lt;/p&gt;
    &lt;p&gt;The augmented matrix is&lt;/p&gt;
    &lt;p&gt;Row reduction:&lt;/p&gt;
    &lt;p&gt;So the system is equivalent to:&lt;/p&gt;
    &lt;p&gt;From the second equation, &lt;/p&gt;
    &lt;p&gt;Thus solutions are:&lt;/p&gt;
    &lt;p&gt;The null space is the line spanned by the vector &lt;/p&gt;
    &lt;p&gt;The solution set of a homogeneous system is always a subspace of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n$&lt;/math-renderer&gt;, the only solution is the zero vector.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n-1$&lt;/math-renderer&gt;, the solution set is a line through the origin.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\text{rank}(A) = n-2$&lt;/math-renderer&gt;, the solution set is a plane through the origin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More generally, the null space has dimension &lt;/p&gt;
    &lt;p&gt;Homogeneous systems are central to understanding vector spaces, subspaces, and dimension. They lead directly to the concepts of kernel, null space, and linear dependence. In applications, homogeneous systems appear in equilibrium problems, eigenvalue equations, and computer graphics transformations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Solve the homogeneous system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What is the dimension of its solution space?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find all solutions of&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Show that the solution set of any homogeneous system is a subspace of&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is a&lt;math-renderer&gt;$3 \times 3$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$\text{rank}(A) = 2$&lt;/math-renderer&gt;. What is the dimension of the null space of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;compute a basis for the null space of &lt;/p&gt;
    &lt;p&gt;Up to now we have studied vectors and matrices concretely in &lt;/p&gt;
    &lt;p&gt;A vector space over the real numbers &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Vector addition: For any &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in V$&lt;/math-renderer&gt;, there is a vector&lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} \in V$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Scalar multiplication: For any scalar &lt;math-renderer&gt;$c \in \mathbb{R}$&lt;/math-renderer&gt;and any&lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;, there is a vector&lt;math-renderer&gt;$c\mathbf{v} \in V$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations must satisfy the following axioms (for all &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Commutativity of addition: &lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Associativity of addition: &lt;math-renderer&gt;$(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Additive identity: There exists a zero vector &lt;math-renderer&gt;$\mathbf{0} \in V$&lt;/math-renderer&gt;such that&lt;math-renderer&gt;$\mathbf{v} + \mathbf{0} = \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Additive inverses: For each &lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;, there exists&lt;math-renderer&gt;$(-\mathbf{v} \in V$&lt;/math-renderer&gt;such that&lt;math-renderer&gt;$\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Compatibility of scalar multiplication: &lt;math-renderer&gt;$a(b\mathbf{v}) = (ab)\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Identity element of scalars: &lt;math-renderer&gt;$1 \cdot \mathbf{v} = \mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Distributivity over vector addition: &lt;math-renderer&gt;$a(\mathbf{u} + \mathbf{v}) = a\mathbf{u} + a\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Distributivity over scalar addition: &lt;math-renderer&gt;$(a+b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If a set &lt;/p&gt;
    &lt;p&gt;Example 4.1.1. Standard Euclidean space &lt;/p&gt;
    &lt;p&gt;Example 4.1.2. Polynomials The set of all polynomials with real coefficients, denoted &lt;/p&gt;
    &lt;p&gt;Example 4.1.3. Functions The set of all real-valued functions on an interval, e.g. &lt;/p&gt;
    &lt;p&gt;Not every set with operations qualifies. For instance, the set of positive real numbers under usual addition is not a vector space, because additive inverses (negative numbers) are missing. The axioms must all hold.&lt;/p&gt;
    &lt;p&gt;In familiar cases like &lt;/p&gt;
    &lt;p&gt;The concept of vector space unifies seemingly different mathematical objects under a single framework. Whether dealing with forces in physics, signals in engineering, or data in machine learning, the common language of vector spaces allows us to use the same techniques everywhere.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;with standard addition and scalar multiplication satisfies all eight vector space axioms.&lt;/item&gt;
      &lt;item&gt;Show that the set of integers &lt;math-renderer&gt;$\mathbb{Z}$&lt;/math-renderer&gt;with ordinary operations is not a vector space over&lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;. Which axiom fails?&lt;/item&gt;
      &lt;item&gt;Consider the set of all polynomials of degree at most 3. Show it forms a vector space over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;. What is its dimension?&lt;/item&gt;
      &lt;item&gt;Give an example of a vector space where the vectors are not geometric objects.&lt;/item&gt;
      &lt;item&gt;Prove that in any vector space, the zero vector is unique.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A subspace is a smaller vector space living inside a larger one. Just as lines and planes naturally sit inside three-dimensional space, subspaces generalize these ideas to higher dimensions and more abstract settings.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\mathbf{0} \in W$&lt;/math-renderer&gt;(contains the zero vector),&lt;/item&gt;
      &lt;item&gt;For all &lt;math-renderer&gt;$\mathbf{u}, \mathbf{v} \in W$&lt;/math-renderer&gt;, the sum&lt;math-renderer&gt;$\mathbf{u} + \mathbf{v} \in W$&lt;/math-renderer&gt;(closed under addition),&lt;/item&gt;
      &lt;item&gt;For all scalars &lt;math-renderer&gt;$c \in \mathbb{R}$&lt;/math-renderer&gt;and vectors&lt;math-renderer&gt;$\mathbf{v} \in W$&lt;/math-renderer&gt;, the product&lt;math-renderer&gt;$c\mathbf{v} \in W$&lt;/math-renderer&gt;(closed under scalar multiplication).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If these hold, then &lt;/p&gt;
    &lt;p&gt;Example 4.2.1. Line through the origin in &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.2.2. The x‚Äìy plane in &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.2.3. Null space of a matrix For a matrix &lt;/p&gt;
    &lt;p&gt;is a subspace of &lt;/p&gt;
    &lt;p&gt;Not every subset is a subspace.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The set &lt;math-renderer&gt;${ (x,y) \in \mathbb{R}^2 \mid x \geq 0 }$&lt;/math-renderer&gt;is not a subspace: it is not closed under scalar multiplication (a negative scalar breaks the condition).&lt;/item&gt;
      &lt;item&gt;Any line in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;that does not pass through the origin is not a subspace, because it does not contain&lt;math-renderer&gt;$\mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subspaces are the linear structures inside vector spaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;, the subspaces are: the zero vector, any line through the origin, or the entire plane.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, the subspaces are: the zero vector, any line through the origin, any plane through the origin, or the entire space.&lt;/item&gt;
      &lt;item&gt;In higher dimensions, the same principle applies: subspaces are the flat linear pieces through the origin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subspaces capture the essential structure of linear problems. Column spaces, row spaces, and null spaces are all subspaces. Much of linear algebra consists of understanding how these subspaces intersect, span, and complement each other.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Prove that the set &lt;math-renderer&gt;$W = { (x,0) \mid x \in \mathbb{R} } \subseteq \mathbb{R}^2$&lt;/math-renderer&gt;is a subspace.&lt;/item&gt;
      &lt;item&gt;Show that the line &lt;math-renderer&gt;${ (1+t, 2t) \mid t \in \mathbb{R} }$&lt;/math-renderer&gt;is not a subspace of&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;. Which condition fails?&lt;/item&gt;
      &lt;item&gt;Determine whether the set of all vectors &lt;math-renderer&gt;$(x,y,z) \in \mathbb{R}^3$&lt;/math-renderer&gt;satisfying&lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;is a subspace.&lt;/item&gt;
      &lt;item&gt;For the matrix&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;describe the null space of &lt;/p&gt;
    &lt;p&gt;The ideas of span, basis, and dimension provide the language for describing the size and structure of subspaces. Together, they tell us how a vector space is generated, how many building blocks it requires, and how those blocks can be chosen.&lt;/p&gt;
    &lt;p&gt;Given a set of vectors &lt;/p&gt;
    &lt;p&gt;The span is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 4.3.1. In &lt;/p&gt;
    &lt;p&gt;A basis of a vector space &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Span &lt;math-renderer&gt;$V$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Are linearly independent (no vector in the set is a linear combination of the others).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If either condition fails, the set is not a basis.&lt;/p&gt;
    &lt;p&gt;Example 4.3.2. In &lt;/p&gt;
    &lt;p&gt;form a basis. Every vector &lt;/p&gt;
    &lt;p&gt;The dimension of a vector space &lt;/p&gt;
    &lt;p&gt;Examples 4.3.3.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\dim(\mathbb{R}^2) = 2$&lt;/math-renderer&gt;, with basis&lt;math-renderer&gt;$(1,0), (0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\dim(\mathbb{R}^3) = 3$&lt;/math-renderer&gt;, with basis&lt;math-renderer&gt;$(1,0,0), (0,1,0), (0,0,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The set of polynomials of degree at most 3 has dimension 4, with basis &lt;math-renderer&gt;$(1, x, x^2, x^3)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The span is like the reach of a set of vectors.&lt;/item&gt;
      &lt;item&gt;A basis is the minimal set of directions needed to reach everything in the space.&lt;/item&gt;
      &lt;item&gt;The dimension is the count of those independent directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lines, planes, and higher-dimensional flats can all be described in terms of span, basis, and dimension.&lt;/p&gt;
    &lt;p&gt;These concepts classify vector spaces and subspaces in terms of size and structure. Many theorems in linear algebra-such as the Rank‚ÄìNullity Theorem-are consequences of understanding span, basis, and dimension. In practical terms, bases are how we encode data in coordinates, and dimension tells us how much freedom a system truly has.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$(1,0,0)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$(0,1,0)$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;span the&lt;math-renderer&gt;$xy$&lt;/math-renderer&gt;-plane in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;. Are they a basis?&lt;/item&gt;
      &lt;item&gt;Find a basis for the line &lt;math-renderer&gt;${(2t,-3t,t) : t \in \mathbb{R}}$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine the dimension of the subspace of &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;defined by&lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that any two different bases of &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;must contain exactly&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;vectors.&lt;/item&gt;
      &lt;item&gt;Give a basis for the set of polynomials of degree &lt;math-renderer&gt;$\leq 2$&lt;/math-renderer&gt;. What is its dimension?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once a basis for a vector space is chosen, every vector can be expressed uniquely as a linear combination of the basis vectors. The coefficients in this combination are called the coordinates of the vector relative to that basis. Coordinates allow us to move between the abstract world of vector spaces and the concrete world of numbers.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;be an ordered basis for &lt;/p&gt;
    &lt;p&gt;The scalars &lt;/p&gt;
    &lt;p&gt;Example 4.4.1. Let the basis be&lt;/p&gt;
    &lt;p&gt;To find the coordinates of &lt;/p&gt;
    &lt;p&gt;This gives the system&lt;/p&gt;
    &lt;p&gt;Adding: &lt;/p&gt;
    &lt;p&gt;So,&lt;/p&gt;
    &lt;p&gt;In &lt;/p&gt;
    &lt;p&gt;Relative to this basis, the coordinates of a vector are simply its entries. Thus, column vectors are coordinate representations by default.&lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;with basis vectors as columns. For any vector &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{u} = P[\mathbf{u}]{\mathcal{B}}, \qquad [\mathbf{u}]{\mathcal{B}} = P^{-1}\mathbf{u}. $$&lt;/p&gt;
    &lt;p&gt;Thus, switching between bases reduces to matrix multiplication.&lt;/p&gt;
    &lt;p&gt;Coordinates are the address of a vector relative to a chosen set of directions. Different bases are like different coordinate systems: Cartesian, rotated, skewed, or scaled. The same vector may look very different numerically depending on the basis, but its geometric identity is unchanged.&lt;/p&gt;
    &lt;p&gt;Coordinates turn abstract vectors into concrete numerical data. Changing basis is the algebraic language for rotations of axes, diagonalization of matrices, and principal component analysis in data science. Mastery of coordinates is essential for moving fluidly between geometry, algebra, and computation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Express &lt;math-renderer&gt;$(4,2)$&lt;/math-renderer&gt;in terms of the basis&lt;math-renderer&gt;$(1,1), (1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the coordinates of &lt;math-renderer&gt;$(1,2,3)$&lt;/math-renderer&gt;relative to the standard basis of&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\mathcal{B} = {(2,0), (0,3)}$&lt;/math-renderer&gt;, compute&lt;math-renderer&gt;$[ (4,6) ]_{\mathcal{B}}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Construct the change of basis matrix from the standard basis of &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;to&lt;math-renderer&gt;$\mathcal{B} = {(1,1), (1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that coordinate representation with respect to a basis is unique.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A central theme of linear algebra is understanding linear transformations: functions between vector spaces that preserve their algebraic structure. These transformations generalize the idea of matrix multiplication and capture the essence of linear behavior.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;is called a linear transformation (or linear map) if for all vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Additivity:&lt;/p&gt;
        &lt;p&gt;$$ T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}), $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homogeneity:&lt;/p&gt;
        &lt;p&gt;$$ T(c\mathbf{u}) = cT(\mathbf{u}). $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If both conditions hold, then &lt;/p&gt;
    &lt;p&gt;Example 5.1.1. Scaling in &lt;/p&gt;
    &lt;p&gt;This doubles the length of every vector, preserving direction. It is linear.&lt;/p&gt;
    &lt;p&gt;Example 5.1.2. Rotation. Let &lt;/p&gt;
    &lt;p&gt;This rotates vectors by angle &lt;/p&gt;
    &lt;p&gt;Example 5.1.3. Differentiation. Let &lt;/p&gt;
    &lt;p&gt;The map &lt;/p&gt;
    &lt;p&gt;is not linear, because &lt;/p&gt;
    &lt;p&gt;Linear transformations are exactly those that preserve the origin, lines through the origin, and proportions along those lines. They include familiar operations: scaling, rotations, reflections, shears, and projections. Nonlinear transformations bend or curve space, breaking these properties.&lt;/p&gt;
    &lt;p&gt;Linear transformations unify geometry, algebra, and computation. They explain how matrices act on vectors, how data can be rotated or projected, and how systems evolve under linear rules. Much of linear algebra is devoted to understanding these transformations, their representations, and their invariants.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Verify that&lt;/p&gt;&lt;math-renderer&gt;$T(x,y) = (3x-y, 2y)$&lt;/math-renderer&gt;is a linear transformation on&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that&lt;/p&gt;&lt;math-renderer&gt;$T(x,y) = (x+1, y)$&lt;/math-renderer&gt;is not linear. Which axiom fails?&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that if&lt;/p&gt;&lt;math-renderer&gt;$T$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$S$&lt;/math-renderer&gt;are linear transformations, then so is&lt;math-renderer&gt;$T+S$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Give an example of a linear transformation from&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;to&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Let&lt;/p&gt;&lt;math-renderer&gt;$T:\mathbb{R}[x] \to \mathbb{R}[x]$&lt;/math-renderer&gt;be integration:&lt;p&gt;$$ T(p(x)) = \int_0^x p(t),dt. $$&lt;/p&gt;&lt;p&gt;Prove that&lt;/p&gt;&lt;math-renderer&gt;$T$&lt;/math-renderer&gt;is a linear transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every linear transformation between finite-dimensional vector spaces can be represented by a matrix. This correspondence is one of the central insights of linear algebra: it lets us use the tools of matrix arithmetic to study abstract transformations.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;The action of &lt;/p&gt;
    &lt;p&gt;$$ T(\mathbf{e}j) = \begin{bmatrix} a{1j} \ a_{2j} \ \vdots \ a_{mj} \end{bmatrix}. $$&lt;/p&gt;
    &lt;p&gt;Placing these outputs as columns gives the matrix of &lt;/p&gt;
    &lt;p&gt;Then for any vector &lt;/p&gt;
    &lt;p&gt;Example 5.2.1. Scaling in &lt;/p&gt;
    &lt;p&gt;So the matrix is&lt;/p&gt;
    &lt;p&gt;Example 5.2.2. Rotation in the plane. The rotation transformation &lt;/p&gt;
    &lt;p&gt;Example 5.2.3. Projection onto the x-axis. The map &lt;/p&gt;
    &lt;p&gt;Matrix representations depend on the chosen basis. If &lt;/p&gt;
    &lt;p&gt;Matrices are not just convenient notation-they are linear maps once a basis is fixed. Every rotation, reflection, projection, shear, or scaling corresponds to multiplying by a specific matrix. Thus, studying linear transformations reduces to studying their matrices.&lt;/p&gt;
    &lt;p&gt;Matrix representations make linear transformations computable. They connect abstract definitions to explicit calculations, enabling algorithms for solving systems, finding eigenvalues, and performing decompositions. Applications from graphics to machine learning depend on this translation.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the matrix representation of &lt;math-renderer&gt;$T:\mathbb{R}^2 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y) = (x+y, x-y)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Determine the matrix of the linear transformation &lt;math-renderer&gt;$T:\mathbb{R}^3 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y,z) = (x+z, y-2z)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;What matrix represents reflection across the line &lt;math-renderer&gt;$y=x$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Show that the matrix of the identity transformation on &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;is&lt;math-renderer&gt;$I_n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For the differentiation map &lt;math-renderer&gt;$D:\mathbb{R}_2[x] \to \mathbb{R}_1[x]$&lt;/math-renderer&gt;, where&lt;math-renderer&gt;$\mathbb{R}_k[x]$&lt;/math-renderer&gt;is the space of polynomials of degree at most&lt;math-renderer&gt;$k$&lt;/math-renderer&gt;, find the matrix of&lt;math-renderer&gt;$D$&lt;/math-renderer&gt;relative to the bases&lt;math-renderer&gt;${1,x,x^2}$&lt;/math-renderer&gt;and&lt;math-renderer&gt;${1,x}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To understand a linear transformation deeply, we must examine what it kills and what it produces. These ideas are captured by the kernel and the image, two fundamental subspaces associated with any linear map.&lt;/p&gt;
    &lt;p&gt;The kernel (or null space) of a linear transformation &lt;/p&gt;
    &lt;p&gt;The kernel is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 5.3.1. Let &lt;/p&gt;
    &lt;p&gt;In matrix form,&lt;/p&gt;
    &lt;p&gt;To find the kernel, solve&lt;/p&gt;
    &lt;p&gt;This gives the equations &lt;/p&gt;
    &lt;p&gt;a line in &lt;/p&gt;
    &lt;p&gt;The image (or range) of a linear transformation &lt;/p&gt;
    &lt;p&gt;Equivalently, it is the span of the columns of the representing matrix. The image is always a subspace of &lt;/p&gt;
    &lt;p&gt;Example 5.3.2. For the same transformation as above,&lt;/p&gt;
    &lt;p&gt;the columns are &lt;/p&gt;
    &lt;p&gt;For a linear transformation &lt;/p&gt;
    &lt;p&gt;This fundamental result connects the lost directions (kernel) with the achieved directions (image).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kernel describes how the transformation flattens space (e.g., projecting a 3D object onto a plane).&lt;/item&gt;
      &lt;item&gt;The image describes the target subspace reached by the transformation.&lt;/item&gt;
      &lt;item&gt;The rank‚Äìnullity theorem quantifies the tradeoff: the more dimensions collapse, the fewer remain in the image.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kernel and image capture the essence of a linear map. They classify transformations, explain when systems have unique or infinite solutions, and form the backbone of important results like the Rank‚ÄìNullity Theorem, diagonalization, and spectral theory.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the kernel and image of &lt;math-renderer&gt;$T:\mathbb{R}^2 \to \mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$T(x,y) = (x-y, x+y)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Let $A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 0 &amp;amp; 1 &amp;amp; 4 \end{bmatrix}$. Find bases for &lt;math-renderer&gt;$\ker(A)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\text{im}(A)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For the projection map &lt;math-renderer&gt;$P(x,y,z) = (x,y,0)$&lt;/math-renderer&gt;, describe the kernel and image.&lt;/item&gt;
      &lt;item&gt;Prove that &lt;math-renderer&gt;$\ker(T)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$\text{im}(T)$&lt;/math-renderer&gt;are always subspaces.&lt;/item&gt;
      &lt;item&gt;Verify the Rank‚ÄìNullity Theorem for the transformation in Example 5.3.1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear transformations can look very different depending on the coordinate system we use. The process of rewriting vectors and transformations relative to a new basis is called a change of basis. This concept lies at the heart of diagonalization, orthogonalization, and many computational techniques.&lt;/p&gt;
    &lt;p&gt;Suppose &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;Equivalently,&lt;/p&gt;
    &lt;p&gt;Here, &lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Thus, changing basis corresponds to a similarity transformation of the matrix.&lt;/p&gt;
    &lt;p&gt;Example 5.4.1. Let &lt;/p&gt;
    &lt;p&gt;In the standard basis, its matrix is&lt;/p&gt;
    &lt;p&gt;Now consider the basis &lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Computing gives&lt;/p&gt;
    &lt;p&gt;In this new basis, the transformation is diagonal: one direction is scaled by 4, the other collapsed to 0.&lt;/p&gt;
    &lt;p&gt;Change of basis is like rotating or skewing your coordinate grid. The underlying transformation does not change, but its description in numbers becomes simpler or more complicated depending on the basis. Finding a basis that simplifies a transformation (often a diagonal basis) is a key theme in linear algebra.&lt;/p&gt;
    &lt;p&gt;Change of basis connects the abstract notion of similarity to practical computation. It is the tool that allows us to diagonalize matrices, compute eigenvalues, and simplify complex transformations. In applications, it corresponds to choosing a more natural coordinate system-whether in geometry, physics, or machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Let $A = \begin{bmatrix} 2 &amp;amp; 1 \ 0 &amp;amp; 2 \end{bmatrix}$. Compute its representation in the basis &lt;math-renderer&gt;${(1,0),(1,1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Find the change-of-basis matrix from the standard basis of &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;to&lt;math-renderer&gt;${(2,1),(1,1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that similar matrices (related by &lt;math-renderer&gt;$P^{-1}AP$&lt;/math-renderer&gt;) represent the same linear transformation under different bases.&lt;/item&gt;
      &lt;item&gt;Diagonalize the matrix $A = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; -1 \end{bmatrix}$ in the basis &lt;math-renderer&gt;${(1,1),(1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, let&lt;math-renderer&gt;$\mathcal{B} = {(1,0,0),(1,1,0),(1,1,1)}$&lt;/math-renderer&gt;. Construct the change-of-basis matrix&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;and compute&lt;math-renderer&gt;$P^{-1}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants are numerical values associated with square matrices. At first they may appear as a complicated formula, but their importance comes from what they measure: determinants encode scaling, orientation, and invertibility of linear transformations. They bridge algebra and geometry.&lt;/p&gt;
    &lt;p&gt;For a &lt;/p&gt;
    &lt;p&gt;the determinant is defined as&lt;/p&gt;
    &lt;p&gt;Geometric meaning: If &lt;/p&gt;
    &lt;p&gt;For&lt;/p&gt;
    &lt;p&gt;the determinant can be computed as&lt;/p&gt;
    &lt;p&gt;Geometric meaning: In &lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;: the transformation squashes space into a lower dimension, so&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is not invertible.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) &amp;amp;gt; 0$&lt;/math-renderer&gt;: volume is scaled by&lt;math-renderer&gt;$\det(A)$&lt;/math-renderer&gt;, orientation preserved.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) &amp;amp;lt; 0$&lt;/math-renderer&gt;: volume is scaled by&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;, orientation reversed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Shear in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $A = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(A) = 1$&lt;/math-renderer&gt;. The transformation slants the unit square into a parallelogram but preserves area.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Projection in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $A = \begin{bmatrix} 1 &amp;amp; 0 \ 0 &amp;amp; 0 \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;. The unit square collapses into a line segment: area vanishes.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Rotation in&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;: $R_\theta = \begin{bmatrix} \cos\theta &amp;amp; -\sin\theta \ \sin\theta &amp;amp; \cos\theta \end{bmatrix}$. Then&lt;math-renderer&gt;$\det(R_\theta) = 1$&lt;/math-renderer&gt;. Rotations preserve area and orientation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The determinant is not just a formula-it is a measure of transformation. It tells us whether a matrix is invertible, how it distorts space, and whether it flips orientation. This geometric insight makes the determinant indispensable in analysis, geometry, and applied mathematics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the determinant of $\begin{bmatrix} 2 &amp;amp; 3 \ 1 &amp;amp; 4 \end{bmatrix}$. What area scaling factor does it represent?&lt;/item&gt;
      &lt;item&gt;Find the determinant of the shear matrix $\begin{bmatrix} 1 &amp;amp; 2 \ 0 &amp;amp; 1 \end{bmatrix}$. What happens to the area of the unit square?&lt;/item&gt;
      &lt;item&gt;For the &lt;math-renderer&gt;$3 \times 3$&lt;/math-renderer&gt;matrix $\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 2 &amp;amp; 0 \ 0 &amp;amp; 0 &amp;amp; 3 \end{bmatrix}$, compute the determinant. How does it scale volume in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;?&lt;/item&gt;
      &lt;item&gt;Show that any rotation matrix in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;has determinant&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Give an example of a &lt;math-renderer&gt;$2 \times 2$&lt;/math-renderer&gt;matrix with determinant&lt;math-renderer&gt;$-1$&lt;/math-renderer&gt;. What geometric action does it represent?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Beyond their geometric meaning, determinants satisfy a collection of algebraic rules that make them powerful tools in linear algebra. These properties allow us to compute efficiently, test invertibility, and understand how determinants behave under matrix operations.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Identity:&lt;/p&gt;
        &lt;p&gt;$$ \det(I_n) = 1. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Triangular matrices: If&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is upper or lower triangular, then&lt;p&gt;$$ \det(A) = a_{11} a_{22} \cdots a_{nn}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Row/column swap: Interchanging two rows (or columns) multiplies the determinant by&lt;/p&gt;&lt;math-renderer&gt;$-1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Row/column scaling: Multiplying a row (or column) by a scalar&lt;/p&gt;&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;multiplies the determinant by&lt;math-renderer&gt;$c$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Row/column addition: Adding a multiple of one row to another does not change the determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Transpose:&lt;/p&gt;
        &lt;p&gt;$$ \det(A^T) = \det(A). $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiplicativity:&lt;/p&gt;
        &lt;p&gt;$$ \det(AB) = \det(A)\det(B). $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Invertibility:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible if and only if&lt;math-renderer&gt;$\det(A) \neq 0$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.2.1. For&lt;/p&gt;
    &lt;p&gt;Example 6.2.2. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Since &lt;/p&gt;
    &lt;p&gt;This matches the multiplicativity rule: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Row swaps: flipping orientation of space.&lt;/item&gt;
      &lt;item&gt;Scaling a row: stretching space in one direction.&lt;/item&gt;
      &lt;item&gt;Row replacement: sliding hyperplanes without altering volume.&lt;/item&gt;
      &lt;item&gt;Multiplicativity: performing two transformations multiplies their scaling factors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These properties make determinants both computationally manageable and geometrically interpretable.&lt;/p&gt;
    &lt;p&gt;Determinant properties connect computation with geometry and theory. They explain why Gaussian elimination works, why invertibility is equivalent to nonzero determinant, and why determinants naturally arise in areas like volume computation, eigenvalue theory, and differential equations.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 3 \ 0 &amp;amp; 1 &amp;amp; 4 \ 0 &amp;amp; 0 &amp;amp; 2 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that if two rows of a square matrix are identical, then its determinant is zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Verify&lt;/p&gt;&lt;math-renderer&gt;$\det(A^T) = \det(A)$&lt;/math-renderer&gt;for&lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; -1 \ 3 &amp;amp; 4 \end{bmatrix}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is invertible, prove that&lt;p&gt;$$ \det(A^{-1}) = \frac{1}{\det(A)}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Suppose&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is a&lt;math-renderer&gt;$3\times 3$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$\det(A) = 5$&lt;/math-renderer&gt;. What is&lt;math-renderer&gt;$\det(2A)$&lt;/math-renderer&gt;?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While determinants of small matrices can be computed directly from formulas, larger matrices require a systematic method. The cofactor expansion (also known as Laplace expansion) provides a recursive way to compute determinants by breaking them into smaller ones.&lt;/p&gt;
    &lt;p&gt;For an &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The minor &lt;math-renderer&gt;$M_{ij}$&lt;/math-renderer&gt;is the determinant of the&lt;math-renderer&gt;$(n-1) \times (n-1)$&lt;/math-renderer&gt;matrix obtained by deleting the&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;-th row and&lt;math-renderer&gt;$j$&lt;/math-renderer&gt;-th column of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The cofactor &lt;math-renderer&gt;$C_{ij}$&lt;/math-renderer&gt;is defined by&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The sign factor &lt;/p&gt;
    &lt;p&gt;$$ \begin{bmatrix}&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; - &amp;amp; + &amp;amp; - &amp;amp; \cdots \&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; + &amp;amp; - &amp;amp; + &amp;amp; \cdots \&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; - &amp;amp; + &amp;amp; - &amp;amp; \cdots \ \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \ddots \end{bmatrix}. $$&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The determinant of &lt;/p&gt;
    &lt;p&gt;Example 6.3.1. Compute&lt;/p&gt;
    &lt;p&gt;Expand along the first row:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{11}$&lt;/math-renderer&gt;: $M_{11} = \det \begin{bmatrix} 4 &amp;amp; 5 \ 0 &amp;amp; 6 \end{bmatrix} = 24$, so&lt;math-renderer&gt;$C_{11} = (+1)(24) = 24$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{12}$&lt;/math-renderer&gt;: $M_{12} = \det \begin{bmatrix} 0 &amp;amp; 5 \ 1 &amp;amp; 6 \end{bmatrix} = 0 - 5 = -5$, so&lt;math-renderer&gt;$C_{12} = (-1)(-5) = 5$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$C_{13}$&lt;/math-renderer&gt;: $M_{13} = \det \begin{bmatrix} 0 &amp;amp; 4 \ 1 &amp;amp; 0 \end{bmatrix} = 0 - 4 = -4$, so&lt;math-renderer&gt;$C_{13} = (+1)(-4) = -4$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expansion along any row or column yields the same result.&lt;/item&gt;
      &lt;item&gt;The cofactor expansion provides a recursive definition of determinant: a determinant of size &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;is expressed in terms of determinants of size&lt;math-renderer&gt;$n-1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Cofactors are fundamental in constructing the adjugate matrix, which gives a formula for inverses:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cofactor expansion breaks down the determinant into contributions from sub-volumes defined by fixing one row or column at a time. Each cofactor measures how that row/column influences the overall volume scaling.&lt;/p&gt;
    &lt;p&gt;Cofactor expansion generalizes the small-matrix formulas and provides a conceptual definition of determinants. While not the most efficient way to compute determinants for large matrices, it is essential for theory, proofs, and connections to adjugates, Cramer‚Äôs rule, and classical geometry.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the determinant of&lt;/p&gt;
        &lt;p&gt;$$ \begin{bmatrix} 2 &amp;amp; 0 &amp;amp; 1 \ 3 &amp;amp; -1 &amp;amp; 4 \ 1 &amp;amp; 2 &amp;amp; 0 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;by cofactor expansion along the first column.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that expanding along the second row of Example 6.3.1 gives the same determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that expansion along any row gives the same value.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that if a row of a matrix is zero, then its determinant is zero.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use cofactor expansion to prove that&lt;/p&gt;&lt;math-renderer&gt;$\det(A) = \det(A^T)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Determinants are not merely algebraic curiosities; they have concrete geometric and computational uses. Two of the most important applications are measuring volumes and testing invertibility of matrices.&lt;/p&gt;
    &lt;p&gt;Given vectors &lt;/p&gt;
    &lt;p&gt;Then &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;gives the area of the parallelogram spanned by&lt;math-renderer&gt;$\mathbf{v}_1, \mathbf{v}_2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;gives the volume of the parallelepiped spanned by&lt;math-renderer&gt;$\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;In higher dimensions, it generalizes to &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;-dimensional volume (hypervolume).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;So the parallelepiped has volume &lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) = 0$&lt;/math-renderer&gt;: the transformation collapses space into a lower dimension (area/volume is zero). No inverse exists.&lt;/item&gt;
      &lt;item&gt;If &lt;math-renderer&gt;$\det(A) \neq 0$&lt;/math-renderer&gt;: the transformation scales volume by&lt;math-renderer&gt;$|\det(A)|$&lt;/math-renderer&gt;, and is reversible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 6.4.2. The matrix&lt;/p&gt;
    &lt;p&gt;has determinant &lt;/p&gt;
    &lt;p&gt;Determinants also provide an explicit formula for solving systems of linear equations when the matrix is invertible. For &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;The sign of &lt;/p&gt;
    &lt;p&gt;Determinants condense key information: they measure scaling, test invertibility, and track orientation. These insights are indispensable in geometry (areas and volumes), analysis (Jacobian determinants in calculus), and computation ( solving systems and checking singularity).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Compute the area of the parallelogram spanned by&lt;/p&gt;&lt;math-renderer&gt;$(2,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,3)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Find the volume of the parallelepiped spanned by&lt;/p&gt;&lt;math-renderer&gt;$(1,0,0), (1,1,0), (1,1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether the matrix $\begin{bmatrix} 1 &amp;amp; 2 \ 3 &amp;amp; 6 \end{bmatrix}$ is invertible. Justify using determinants.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use Cramer‚Äôs rule to solve&lt;/p&gt;
        &lt;p&gt;$$ \begin{cases} x + y = 3, \ 2x - y = 0. \end{cases} $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain geometrically why a determinant of zero implies no inverse exists.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To extend the geometric ideas of length, distance, and angle beyond &lt;/p&gt;
    &lt;p&gt;An inner product on a real vector space &lt;/p&gt;
    &lt;p&gt;that assigns to each pair of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Symmetry:&lt;/p&gt;
        &lt;math-renderer&gt;$\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle.$&lt;/math-renderer&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Linearity in the first argument:&lt;/p&gt;
        &lt;math-renderer&gt;$\langle a\mathbf{u} + b\mathbf{w}, \mathbf{v} \rangle = a \langle \mathbf{u}, \mathbf{v} \rangle + b \langle \mathbf{w}, \mathbf{v} \rangle.$&lt;/math-renderer&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Positive-definiteness:&lt;/p&gt;&lt;math-renderer&gt;$\langle \mathbf{v}, \mathbf{v} \rangle \geq 0$&lt;/math-renderer&gt;, and equality holds if and only if&lt;math-renderer&gt;$\mathbf{v} = \mathbf{0}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The standard inner product on &lt;/p&gt;
    &lt;p&gt;The norm of a vector is its length, defined in terms of the inner product:&lt;/p&gt;
    &lt;p&gt;For the dot product in &lt;/p&gt;
    &lt;p&gt;The inner product allows us to define the angle &lt;/p&gt;
    &lt;p&gt;Thus, two vectors are orthogonal if &lt;/p&gt;
    &lt;p&gt;Example 7.1.1. In &lt;/p&gt;
    &lt;p&gt;So,&lt;/p&gt;
    &lt;p&gt;Example 7.1.2. In the function space &lt;/p&gt;
    &lt;p&gt;defines a length&lt;/p&gt;
    &lt;p&gt;This generalizes geometry to infinite-dimensional spaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inner product: measures similarity between vectors.&lt;/item&gt;
      &lt;item&gt;Norm: length of a vector.&lt;/item&gt;
      &lt;item&gt;Angle: measure of alignment between two directions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These concepts unify algebraic operations with geometric intuition.&lt;/p&gt;
    &lt;p&gt;Inner products and norms allow us to extend geometry into abstract vector spaces. They form the basis of orthogonality, projections, Fourier series, least squares approximation, and many applications in physics and machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Compute&lt;/p&gt;&lt;math-renderer&gt;$\langle (2,-1,3), (1,4,0) \rangle$&lt;/math-renderer&gt;. Then find the angle between them.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that&lt;/p&gt;&lt;math-renderer&gt;$|(x,y)| = \sqrt{x^2+y^2}$&lt;/math-renderer&gt;satisfies the properties of a norm.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;, verify that&lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1,-1,0)$&lt;/math-renderer&gt;are orthogonal.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;In&lt;/p&gt;&lt;math-renderer&gt;$C[0,1]$&lt;/math-renderer&gt;, compute&lt;math-renderer&gt;$\langle f,g \rangle$&lt;/math-renderer&gt;for&lt;math-renderer&gt;$f(x)=x$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$g(x)=1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove the Cauchy‚ÄìSchwarz inequality:&lt;/p&gt;
        &lt;p&gt;$$ |\langle \mathbf{u}, \mathbf{v} \rangle| \leq |\mathbf{u}| , |\mathbf{v}|. $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the most useful applications of inner products is the notion of orthogonal projection. Projection allows us to approximate a vector by another lying in a subspace, minimizing error in the sense of distance. This idea underpins geometry, statistics, and numerical analysis.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Given a vector &lt;/p&gt;
    &lt;p&gt;The formula is&lt;/p&gt;
    &lt;p&gt;The error vector &lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;The error vector is &lt;/p&gt;
    &lt;p&gt;Suppose &lt;/p&gt;
    &lt;p&gt;This is the unique vector in &lt;/p&gt;
    &lt;p&gt;Orthogonal projection explains the method of least squares. To solve an overdetermined system &lt;/p&gt;
    &lt;p&gt;Thus, least squares is just projection in disguise.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Projection finds the closest point in a subspace to a given vector.&lt;/item&gt;
      &lt;item&gt;It minimizes distance (error) in the sense of Euclidean norm.&lt;/item&gt;
      &lt;item&gt;Orthogonality ensures the error vector points directly away from the subspace.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orthogonal projection is central in both pure and applied mathematics. It underlies the geometry of subspaces, the theory of Fourier series, regression in statistics, and approximation methods in numerical linear algebra. Whenever we fit data with a simpler model, projection is at work.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute the projection of &lt;math-renderer&gt;$(2,3)$&lt;/math-renderer&gt;onto the vector&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that &lt;math-renderer&gt;$\mathbf{v} - \text{proj}_{\mathbf{u}}(\mathbf{v})$&lt;/math-renderer&gt;is orthogonal to&lt;math-renderer&gt;$\mathbf{u}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Let &lt;math-renderer&gt;$W = \text{span}{(1,0,0), (0,1,0)} \subseteq \mathbb{R}^3$&lt;/math-renderer&gt;. Find the projection of&lt;math-renderer&gt;$(1,2,3)$&lt;/math-renderer&gt;onto&lt;math-renderer&gt;$W$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Explain why least squares fitting corresponds to projection onto the column space of &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that projection onto a subspace &lt;math-renderer&gt;$W$&lt;/math-renderer&gt;is unique: there is exactly one closest vector in&lt;math-renderer&gt;$W$&lt;/math-renderer&gt;to a given&lt;math-renderer&gt;$\mathbf{v}$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Gram‚ÄìSchmidt process is a systematic way to turn any linearly independent set of vectors into an orthonormal basis. This is especially useful because orthonormal bases simplify computations: inner products become simple coordinate comparisons, and projections take clean forms.&lt;/p&gt;
    &lt;p&gt;Given a linearly independent set of vectors &lt;/p&gt;
    &lt;p&gt;We proceed step by step:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with &lt;math-renderer&gt;$\mathbf{v}_1$&lt;/math-renderer&gt;, normalize it to get&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Subtract from &lt;math-renderer&gt;$\mathbf{v}_2$&lt;/math-renderer&gt;its projection onto&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;, leaving a vector orthogonal to&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;. Normalize to get&lt;math-renderer&gt;$\mathbf{u}_2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For each &lt;math-renderer&gt;$\mathbf{v}_k$&lt;/math-renderer&gt;, subtract projections onto all previously constructed $\mathbf{u}1, \dots, \mathbf{u}{k-1}$, then normalize.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;p&gt;$$ \mathbf{w}_k = \mathbf{v}k - \sum{j=1}^{k-1} \langle \mathbf{v}_k, \mathbf{u}_j \rangle \mathbf{u}_j, $$&lt;/p&gt;
    &lt;p&gt;The result &lt;/p&gt;
    &lt;p&gt;Take &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize &lt;math-renderer&gt;$\mathbf{v}_1$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Subtract projection of &lt;math-renderer&gt;$\mathbf{v}_2$&lt;/math-renderer&gt;on&lt;math-renderer&gt;$\mathbf{u}_1$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;Normalize:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Subtract projections from &lt;math-renderer&gt;$\mathbf{v}_3$&lt;/math-renderer&gt;:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After computing, normalize to obtain &lt;/p&gt;
    &lt;p&gt;The result is an orthonormal basis of the span of &lt;/p&gt;
    &lt;p&gt;Gram‚ÄìSchmidt is like straightening out a set of vectors: you start with the original directions and adjust each new vector to be perpendicular to all previous ones. Then you scale to unit length. The process ensures orthogonality while preserving the span.&lt;/p&gt;
    &lt;p&gt;Orthonormal bases simplify inner products, projections, and computations in general. They make coordinate systems easier to work with and are crucial in numerical methods, QR decomposition, Fourier analysis, and statistics (orthogonal polynomials, principal component analysis).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apply Gram‚ÄìSchmidt to &lt;math-renderer&gt;$(1,0), (1,1)$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Orthogonalize &lt;math-renderer&gt;$(1,1,1), (1,0,1)$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove that each step of Gram‚ÄìSchmidt yields a vector orthogonal to all previous ones.&lt;/item&gt;
      &lt;item&gt;Show that Gram‚ÄìSchmidt preserves the span of the original vectors.&lt;/item&gt;
      &lt;item&gt;Explain how Gram‚ÄìSchmidt leads to the QR decomposition of a matrix.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An orthonormal basis is a basis of a vector space in which all vectors are both orthogonal to each other and have unit length. Such bases are the most convenient possible coordinate systems: computations involving inner products, projections, and norms become exceptionally simple.&lt;/p&gt;
    &lt;p&gt;A set of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0$&lt;/math-renderer&gt;whenever&lt;math-renderer&gt;$i \neq j$&lt;/math-renderer&gt;(orthogonality),&lt;/item&gt;
      &lt;item&gt;&lt;math-renderer&gt;$|\mathbf{u}_i| = 1$&lt;/math-renderer&gt;for all&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;(normalization),&lt;/item&gt;
      &lt;item&gt;The set spans &lt;math-renderer&gt;$V$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 7.4.1. In &lt;/p&gt;
    &lt;p&gt;is orthonormal under the dot product.&lt;/p&gt;
    &lt;p&gt;Example 7.4.2. In &lt;/p&gt;
    &lt;p&gt;is orthonormal.&lt;/p&gt;
    &lt;p&gt;Example 7.4.3. Fourier basis on functions:&lt;/p&gt;
    &lt;p&gt;is an orthogonal set in the space of square-integrable functions on &lt;/p&gt;
    &lt;p&gt;After normalization, it becomes an orthonormal basis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Coordinate simplicity: If&lt;/p&gt;&lt;math-renderer&gt;${\mathbf{u}_1,\dots,\mathbf{u}_n}$&lt;/math-renderer&gt;is an orthonormal basis of&lt;math-renderer&gt;$V$&lt;/math-renderer&gt;, then any vector&lt;math-renderer&gt;$\mathbf{v}\in V$&lt;/math-renderer&gt;has coordinates&lt;p&gt;$$ [\mathbf{v}] = \begin{bmatrix} \langle \mathbf{v}, \mathbf{u}_1 \rangle \ \vdots \ \langle \mathbf{v}, \mathbf{u}_n \rangle \end{bmatrix}. $$&lt;/p&gt;&lt;p&gt;That is, coordinates are just inner products.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Parseval‚Äôs identity: For any&lt;/p&gt;&lt;math-renderer&gt;$\mathbf{v} \in V$&lt;/math-renderer&gt;,&lt;p&gt;$$ |\mathbf{v}|^2 = \sum_{i=1}^n |\langle \mathbf{v}, \mathbf{u}_i \rangle|^2. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Projections: The orthogonal projection onto the span of&lt;/p&gt;&lt;math-renderer&gt;${\mathbf{u}_1,\dots,\mathbf{u}_k}$&lt;/math-renderer&gt;is&lt;p&gt;$$ \text{proj}(\mathbf{v}) = \sum_{i=1}^k \langle \mathbf{v}, \mathbf{u}_i \rangle \mathbf{u}_i. $$&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start with any linearly independent set, then apply the Gram‚ÄìSchmidt process to obtain an orthonormal set spanning the same subspace.&lt;/item&gt;
      &lt;item&gt;In practice, orthonormal bases are often chosen for numerical stability and simplicity of computation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An orthonormal basis is like a perfectly aligned and equally scaled coordinate system. Distances and angles are computed directly using coordinates without correction factors. They are the ideal rulers of linear algebra.&lt;/p&gt;
    &lt;p&gt;Orthonormal bases simplify every aspect of linear algebra: solving systems, computing projections, expanding functions, diagonalizing symmetric matrices, and working with Fourier series. In data science, principal component analysis produces orthonormal directions capturing maximum variance.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Verify that &lt;math-renderer&gt;$(1/\sqrt{2})(1,1)$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$(1/\sqrt{2})(1,-1)$&lt;/math-renderer&gt;form an orthonormal basis of&lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Express &lt;math-renderer&gt;$(3,4)$&lt;/math-renderer&gt;in terms of the orthonormal basis&lt;math-renderer&gt;${(1/\sqrt{2})(1,1), (1/\sqrt{2})(1,-1)}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Prove Parseval‚Äôs identity for &lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;with the dot product.&lt;/item&gt;
      &lt;item&gt;Find an orthonormal basis for the plane &lt;math-renderer&gt;$x+y+z=0$&lt;/math-renderer&gt;in&lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Explain why orthonormal bases are numerically more stable than arbitrary bases in computations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The concepts of eigenvalues and eigenvectors reveal the most fundamental behavior of linear transformations. They identify the special directions in which a transformation acts by simple stretching or compressing, without rotation or distortion.&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;for some scalar &lt;/p&gt;
    &lt;p&gt;Equivalently, if &lt;/p&gt;
    &lt;p&gt;Example 8.1.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;So &lt;/p&gt;
    &lt;p&gt;Example 8.1.2. Rotation matrix in &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;Eigenvalues arise from solving the characteristic equation:&lt;/p&gt;
    &lt;p&gt;This polynomial in &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Eigenvectors are directions that remain unchanged in orientation under a transformation; only their length is scaled.&lt;/item&gt;
      &lt;item&gt;Eigenvalues tell us the scaling factor along those directions.&lt;/item&gt;
      &lt;item&gt;If a matrix has many independent eigenvectors, it can often be simplified (diagonalized) by changing basis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stretching along principal axes of an ellipse (quadratic forms).&lt;/item&gt;
      &lt;item&gt;Stable directions of dynamical systems.&lt;/item&gt;
      &lt;item&gt;Principal components in statistics and machine learning.&lt;/item&gt;
      &lt;item&gt;Quantum mechanics, where observables correspond to operators with eigenvalues.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues and eigenvectors are a bridge between algebra and geometry. They provide a lens for understanding linear transformations in their simplest form. Nearly every application of linear algebra-differential equations, statistics, physics, computer science-relies on eigen-analysis.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the eigenvalues and eigenvectors of $\begin{bmatrix} 4 &amp;amp; 0 \ 0 &amp;amp; -1 \end{bmatrix}$.&lt;/item&gt;
      &lt;item&gt;Show that every scalar multiple of an eigenvector is again an eigenvector for the same eigenvalue.&lt;/item&gt;
      &lt;item&gt;Verify that the rotation matrix &lt;math-renderer&gt;$R_\theta$&lt;/math-renderer&gt;has no real eigenvalues unless&lt;math-renderer&gt;$\theta = 0$&lt;/math-renderer&gt;or&lt;math-renderer&gt;$\pi$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Compute the characteristic polynomial of $\begin{bmatrix} 1 &amp;amp; 2 \ 2 &amp;amp; 1 \end{bmatrix}$.&lt;/item&gt;
      &lt;item&gt;Explain geometrically what eigenvectors and eigenvalues represent for the shear matrix $\begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix}$.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A central goal in linear algebra is to simplify the action of a matrix by choosing a good basis. Diagonalization is the process of rewriting a matrix so that it acts by simple scaling along independent directions. This makes computations such as powers, exponentials, and solving differential equations far easier.&lt;/p&gt;
    &lt;p&gt;A square matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;The diagonal entries of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A matrix is diagonalizable if it has &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;linearly independent eigenvectors.&lt;/item&gt;
      &lt;item&gt;Equivalently, the sum of the dimensions of its eigenspaces equals &lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Symmetric matrices (over &lt;math-renderer&gt;$\mathbb{R}$&lt;/math-renderer&gt;) are always diagonalizable, with an orthonormal basis of eigenvectors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Characteristic polynomial:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So eigenvalues are &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda = 4$&lt;/math-renderer&gt;, solve&lt;math-renderer&gt;$(A-4I)\mathbf{v}=0$&lt;/math-renderer&gt;: $\begin{bmatrix} 0 &amp;amp; 1 \ 0 &amp;amp; -2 \end{bmatrix}\mathbf{v} = 0$, giving&lt;math-renderer&gt;$\mathbf{v}_1 = (1,0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda = 2$&lt;/math-renderer&gt;:&lt;math-renderer&gt;$(A-2I)\mathbf{v}=0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$\mathbf{v}_2 = (1,-2)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Construct $P = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; -2 \end{bmatrix}$. Then&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus, &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Computing powers: If&lt;/p&gt;&lt;math-renderer&gt;$A = P D P^{-1}$&lt;/math-renderer&gt;, then&lt;p&gt;$$ A^k = P D^k P^{-1}. $$&lt;/p&gt;&lt;p&gt;Since&lt;/p&gt;&lt;math-renderer&gt;$D$&lt;/math-renderer&gt;is diagonal,&lt;math-renderer&gt;$D^k$&lt;/math-renderer&gt;is easy to compute.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Matrix exponentials:&lt;/p&gt;&lt;math-renderer&gt;$e^A = P e^D P^{-1}$&lt;/math-renderer&gt;, useful in solving differential equations.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Understanding geometry: Diagonalization reveals the directions along which a transformation stretches or compresses space independently.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not all matrices can be diagonalized.&lt;/p&gt;
    &lt;p&gt;has only one eigenvalue &lt;/p&gt;
    &lt;p&gt;Diagonalization means we have found a basis of eigenvectors. In this basis, the matrix acts by simple scaling along each coordinate axis. It transforms complicated motion into independent 1D motions.&lt;/p&gt;
    &lt;p&gt;Diagonalization is a cornerstone of linear algebra. It simplifies computation, reveals structure, and is the starting point for the spectral theorem, Jordan form, and many applications in physics, engineering, and data science.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Diagonalize&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; 0 \ 0 &amp;amp; 3 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 1 &amp;amp; 1 \ 0 &amp;amp; 1 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is diagonalizable. Why or why not?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Find&lt;/p&gt;&lt;math-renderer&gt;$A^5$&lt;/math-renderer&gt;for&lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 1 \ 0 &amp;amp; 2 \end{bmatrix} $$&lt;/p&gt;&lt;p&gt;using diagonalization.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that any&lt;/p&gt;&lt;math-renderer&gt;$n \times n$&lt;/math-renderer&gt;matrix with&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;distinct eigenvalues is diagonalizable.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain why real symmetric matrices are always diagonalizable.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key to finding eigenvalues is the characteristic polynomial of a matrix. This polynomial encodes the values of &lt;/p&gt;
    &lt;p&gt;For an &lt;/p&gt;
    &lt;p&gt;The roots of &lt;/p&gt;
    &lt;p&gt;Example 8.3.1. Let&lt;/p&gt;
    &lt;p&gt;Then&lt;/p&gt;
    &lt;p&gt;Thus eigenvalues are &lt;/p&gt;
    &lt;p&gt;Example 8.3.2. For&lt;/p&gt;
    &lt;p&gt;(rotation by 90¬∞),&lt;/p&gt;
    &lt;p&gt;Eigenvalues are &lt;/p&gt;
    &lt;p&gt;Example 8.3.3. For a triangular matrix&lt;/p&gt;
    &lt;p&gt;the determinant is simply the product of diagonal entries minus &lt;/p&gt;
    &lt;p&gt;So eigenvalues are &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The characteristic polynomial of an&lt;/p&gt;&lt;math-renderer&gt;$n \times n$&lt;/math-renderer&gt;matrix has degree&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The sum of the eigenvalues (counted with multiplicity) equals the trace of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;:&lt;p&gt;$$ \text{tr}(A) = \lambda_1 + \cdots + \lambda_n. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The product of the eigenvalues equals the determinant of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;:&lt;p&gt;$$ \det(A) = \lambda_1 \cdots \lambda_n. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similar matrices have the same characteristic polynomial, hence the same eigenvalues.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The characteristic polynomial captures when &lt;/p&gt;
    &lt;p&gt;Characteristic polynomials provide the computational tool to extract eigenvalues. They connect matrix invariants (trace and determinant) with geometry, and form the foundation for diagonalization, spectral theorems, and stability analysis in dynamical systems.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 1 &amp;amp; 3 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Verify that the sum of the eigenvalues of $\begin{bmatrix} 5 &amp;amp; 0 \ 0 &amp;amp; -2 \end{bmatrix}$ equals its trace, and their product equals its determinant.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that for any triangular matrix, the eigenvalues are just the diagonal entries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$B$&lt;/math-renderer&gt;are similar matrices, then&lt;math-renderer&gt;$p_A(\lambda) = p_B(\lambda)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compute the characteristic polynomial of $\begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 0 \ 0 &amp;amp; 1 &amp;amp; 1 \ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}$.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues and eigenvectors are not only central to the theory of linear algebra-they are indispensable tools across mathematics and applied science. Two classic applications are solving systems of differential equations and analyzing Markov chains.&lt;/p&gt;
    &lt;p&gt;Consider the system&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;p&gt;is a solution.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Eigenvalues determine the growth or decay rate:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda &amp;amp;lt; 0$&lt;/math-renderer&gt;, solutions decay (stable).&lt;/item&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda &amp;amp;gt; 0$&lt;/math-renderer&gt;, solutions grow (unstable).&lt;/item&gt;
          &lt;item&gt;If &lt;math-renderer&gt;$\lambda$&lt;/math-renderer&gt;is complex, oscillations occur.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;If &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By combining eigenvector solutions, we can solve general initial conditions.&lt;/p&gt;
    &lt;p&gt;Example 8.4.1. Let&lt;/p&gt;
    &lt;p&gt;Then eigenvalues are &lt;/p&gt;
    &lt;p&gt;Thus one component grows exponentially, the other decays.&lt;/p&gt;
    &lt;p&gt;A Markov chain is described by a stochastic matrix &lt;/p&gt;
    &lt;p&gt;Iterating gives&lt;/p&gt;
    &lt;p&gt;Understanding long-term behavior reduces to analyzing powers of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The eigenvalue &lt;math-renderer&gt;$\lambda = 1$&lt;/math-renderer&gt;always exists. Its eigenvector gives the steady-state distribution.&lt;/item&gt;
      &lt;item&gt;All other eigenvalues satisfy &lt;math-renderer&gt;$|\lambda| \leq 1$&lt;/math-renderer&gt;. Their influence decays as&lt;math-renderer&gt;$k \to \infty$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 8.4.2. Consider&lt;/p&gt;
    &lt;p&gt;Eigenvalues are &lt;/p&gt;
    &lt;p&gt;Thus, regardless of the starting distribution, the chain converges to &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In differential equations, eigenvalues determine the time evolution: exponential growth, decay, or oscillation.&lt;/item&gt;
      &lt;item&gt;In Markov chains, eigenvalues determine the long-term equilibrium of stochastic processes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalue methods turn complex iterative or dynamical systems into tractable problems. In physics, engineering, and finance, they describe stability and resonance. In computer science and statistics, they power algorithms from Google‚Äôs PageRank to modern machine learning.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Solve $\tfrac{d}{dt}\mathbf{x} = \begin{bmatrix} 3 &amp;amp; 0 \ 0 &amp;amp; -2 \end{bmatrix}\mathbf{x}$.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;has a complex eigenvalue&lt;math-renderer&gt;$\alpha \pm i\beta$&lt;/math-renderer&gt;, then solutions of&lt;math-renderer&gt;$\tfrac{d}{dt}\mathbf{x} = A\mathbf{x}$&lt;/math-renderer&gt;involve oscillations of frequency&lt;math-renderer&gt;$\beta$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the steady-state distribution of&lt;/p&gt;
        &lt;p&gt;$$ P = \begin{bmatrix} 0.7 &amp;amp; 0.2 \ 0.3 &amp;amp; 0.8 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Prove that for any stochastic matrix&lt;/p&gt;&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;,&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;is always an eigenvalue.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Explain why all eigenvalues of a stochastic matrix satisfy&lt;/p&gt;&lt;math-renderer&gt;$|\lambda| \leq 1$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A quadratic form is a polynomial of degree two in several variables, expressed neatly using matrices. Quadratic forms appear throughout mathematics: in optimization, geometry of conic sections, statistics (variance), and physics (energy functions).&lt;/p&gt;
    &lt;p&gt;Let &lt;/p&gt;
    &lt;p&gt;Expanded,&lt;/p&gt;
    &lt;p&gt;Because &lt;/p&gt;
    &lt;p&gt;Example 9.1.1. For&lt;/p&gt;
    &lt;p&gt;Example 9.1.2. The quadratic form&lt;/p&gt;
    &lt;p&gt;corresponds to the matrix &lt;/p&gt;
    &lt;p&gt;Example 9.1.3. The conic section equation&lt;/p&gt;
    &lt;p&gt;is described by the quadratic form &lt;/p&gt;
    &lt;p&gt;By choosing a new basis consisting of eigenvectors of &lt;/p&gt;
    &lt;p&gt;Thus quadratic forms can always be expressed as a sum of weighted squares:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;Quadratic forms describe geometric shapes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In 2D: ellipses, parabolas, hyperbolas.&lt;/item&gt;
      &lt;item&gt;In 3D: ellipsoids, paraboloids, hyperboloids.&lt;/item&gt;
      &lt;item&gt;In higher dimensions: generalizations of ellipsoids.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diagonalization aligns the coordinate axes with the principal axes of the shape.&lt;/p&gt;
    &lt;p&gt;Quadratic forms unify geometry and algebra. They are central in optimization (minimizing energy functions), statistics ( covariance matrices and variance), mechanics (kinetic energy), and numerical analysis. Understanding quadratic forms leads directly to the spectral theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the quadratic form &lt;math-renderer&gt;$Q(x,y) = 3x^2 + 4xy + y^2$&lt;/math-renderer&gt;as&lt;math-renderer&gt;$\mathbf{x}^T A \mathbf{x}$&lt;/math-renderer&gt;for some symmetric matrix&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For $A = \begin{bmatrix} 1 &amp;amp; 2 \ 2 &amp;amp; 1 \end{bmatrix}$, compute &lt;math-renderer&gt;$Q(x,y)$&lt;/math-renderer&gt;explicitly.&lt;/item&gt;
      &lt;item&gt;Diagonalize the quadratic form &lt;math-renderer&gt;$Q(x,y) = 2x^2 + 2xy + 3y^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Identify the conic section given by &lt;math-renderer&gt;$Q(x,y) = x^2 - y^2$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that if &lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is symmetric, quadratic forms defined by&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;and&lt;math-renderer&gt;$A^T$&lt;/math-renderer&gt;are identical.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Quadratic forms are especially important when their associated matrices are positive definite, since these guarantee positivity of energy, distance, or variance. Positive definiteness is a cornerstone in optimization, numerical analysis, and statistics.&lt;/p&gt;
    &lt;p&gt;A symmetric matrix &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Positive definite if&lt;/p&gt;
        &lt;p&gt;$$ \mathbf{x}^T A \mathbf{x} &amp;gt; 0 \quad \text{for all nonzero } \mathbf{x} \in \mathbb{R}^n. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Positive semidefinite if&lt;/p&gt;
        &lt;p&gt;$$ \mathbf{x}^T A \mathbf{x} \geq 0 \quad \text{for all } \mathbf{x}. $$&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Similarly, negative definite (always &amp;lt; 0) and indefinite (can be both &amp;lt; 0 and &amp;gt; 0) matrices are defined.&lt;/p&gt;
    &lt;p&gt;Example 9.2.1.&lt;/p&gt;
    &lt;p&gt;is positive definite, since&lt;/p&gt;
    &lt;p&gt;for all &lt;/p&gt;
    &lt;p&gt;Example 9.2.2.&lt;/p&gt;
    &lt;p&gt;has quadratic form&lt;/p&gt;
    &lt;p&gt;This matrix is not positive definite, since &lt;/p&gt;
    &lt;p&gt;For a symmetric matrix &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Eigenvalue test:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if all eigenvalues of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;are positive.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Principal minors test (Sylvester‚Äôs criterion):&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if all leading principal minors ( determinants of top-left&lt;math-renderer&gt;$k \times k$&lt;/math-renderer&gt;submatrices) are positive.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Cholesky factorization:&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite if and only if it can be written as&lt;p&gt;$$ A = R^T R, $$&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;math-renderer&gt;$R$&lt;/math-renderer&gt;is an upper triangular matrix with positive diagonal entries.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Positive definite matrices correspond to quadratic forms that define ellipsoids centered at the origin.&lt;/item&gt;
      &lt;item&gt;Positive semidefinite matrices define flattened ellipsoids (possibly degenerate).&lt;/item&gt;
      &lt;item&gt;Indefinite matrices define hyperbolas or saddle-shaped surfaces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimization: Hessians of convex functions are positive semidefinite; strict convexity corresponds to positive definite Hessians.&lt;/item&gt;
      &lt;item&gt;Statistics: Covariance matrices are positive semidefinite.&lt;/item&gt;
      &lt;item&gt;Numerical methods: Cholesky decomposition is widely used to solve systems with positive definite matrices efficiently.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Positive definiteness provides stability and guarantees in mathematics and computation. It ensures energy functions are bounded below, optimization problems have unique solutions, and statistical models are meaningful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Use Sylvester‚Äôs criterion to check whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 2 &amp;amp; -1 \ -1 &amp;amp; 2 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is positive definite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Determine whether&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 0 &amp;amp; 1 \ 1 &amp;amp; 0 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;is positive definite, semidefinite, or indefinite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the eigenvalues of&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 2 &amp;amp; 3 \end{bmatrix}, $$&lt;/p&gt;
        &lt;p&gt;and use them to classify definiteness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that all diagonal matrices with positive entries are positive definite.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Show that if&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;is positive definite, then so is&lt;math-renderer&gt;$P^T A P$&lt;/math-renderer&gt;for any invertible matrix&lt;math-renderer&gt;$P$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spectral theorem is one of the most powerful results in linear algebra. It states that symmetric matrices can always be diagonalized by an orthogonal basis of eigenvectors. This links algebra (eigenvalues), geometry (orthogonal directions), and applications (stability, optimization, statistics).&lt;/p&gt;
    &lt;p&gt;If &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;All eigenvalues of&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;are real.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There exists an orthonormal basis of&lt;/p&gt;&lt;math-renderer&gt;$\mathbb{R}^n$&lt;/math-renderer&gt;consisting of eigenvectors of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Thus,&lt;/p&gt;&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;can be written as&lt;p&gt;$$ A = Q \Lambda Q^T, $$&lt;/p&gt;&lt;p&gt;where&lt;/p&gt;&lt;math-renderer&gt;$Q$&lt;/math-renderer&gt;is an orthogonal matrix (&lt;math-renderer&gt;$Q^T Q = I$&lt;/math-renderer&gt;) and&lt;math-renderer&gt;$\Lambda$&lt;/math-renderer&gt;is diagonal with eigenvalues of&lt;math-renderer&gt;$A$&lt;/math-renderer&gt;on the diagonal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Symmetric matrices are always diagonalizable, and the diagonalization is numerically stable.&lt;/item&gt;
      &lt;item&gt;Quadratic forms &lt;math-renderer&gt;$\mathbf{x}^T A \mathbf{x}$&lt;/math-renderer&gt;can be expressed in terms of eigenvalues and eigenvectors, showing ellipsoids aligned with eigen-directions.&lt;/item&gt;
      &lt;item&gt;Positive definiteness can be checked by confirming that all eigenvalues are positive.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Characteristic polynomial:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eigenvalues: &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda=1$&lt;/math-renderer&gt;: solve&lt;math-renderer&gt;$(A-I)\mathbf{v} = 0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$(1,-1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;For &lt;math-renderer&gt;$\lambda=3$&lt;/math-renderer&gt;: solve&lt;math-renderer&gt;$(A-3I)\mathbf{v} = 0$&lt;/math-renderer&gt;, giving&lt;math-renderer&gt;$(1,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Normalize eigenvectors:&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Then&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So&lt;/p&gt;
    &lt;p&gt;The spectral theorem says every symmetric matrix acts like independent scaling along orthogonal directions. In geometry, this corresponds to stretching space along perpendicular axes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ellipses, ellipsoids, and quadratic surfaces can be fully understood via eigenvalues and eigenvectors.&lt;/item&gt;
      &lt;item&gt;Orthogonality ensures directions remain perpendicular after transformation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimization: The spectral theorem underlies classification of critical points via eigenvalues of the Hessian.&lt;/item&gt;
      &lt;item&gt;PCA (Principal Component Analysis): Data covariance matrices are symmetric, and PCA finds orthogonal directions of maximum variance.&lt;/item&gt;
      &lt;item&gt;Differential equations &amp;amp; physics: Symmetric operators correspond to measurable quantities with real eigenvalues ( stability, energy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spectral theorem guarantees that symmetric matrices are as simple as possible: they can always be analyzed in terms of real, orthogonal eigenvectors. This provides both deep theoretical insight and powerful computational tools.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Diagonalize&lt;/p&gt;
        &lt;p&gt;$$ A = \begin{bmatrix} 4 &amp;amp; 2 \ 2 &amp;amp; 3 \end{bmatrix} $$&lt;/p&gt;
        &lt;p&gt;using the spectral theorem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prove that all eigenvalues of a real symmetric matrix are real.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that eigenvectors corresponding to distinct eigenvalues of a symmetric matrix are orthogonal.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain geometrically how the spectral theorem describes ellipsoids defined by quadratic forms.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apply the spectral theorem to the covariance matrix&lt;/p&gt;
        &lt;p&gt;$$ \Sigma = \begin{bmatrix} 2 &amp;amp; 1 \ 1 &amp;amp; 2 \end{bmatrix}, $$&lt;/p&gt;
        &lt;p&gt;and interpret the eigenvectors as principal directions of variance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Principal Component Analysis (PCA) is a widely used technique in data science, machine learning, and statistics. At its core, PCA is an application of the spectral theorem to covariance matrices: it finds orthogonal directions (principal components) that capture the maximum variance in data.&lt;/p&gt;
    &lt;p&gt;Given a dataset of vectors &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Center the data by subtracting the mean vector&lt;/p&gt;&lt;math-renderer&gt;$\bar{\mathbf{x}}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Form the covariance matrix&lt;/p&gt;
        &lt;p&gt;$$ \Sigma = \frac{1}{m} \sum_{i=1}^m (\mathbf{x}_i - \bar{\mathbf{x}})(\mathbf{x}_i - \bar{\mathbf{x}})^T. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apply the spectral theorem:&lt;/p&gt;&lt;math-renderer&gt;$\Sigma = Q \Lambda Q^T$&lt;/math-renderer&gt;.&lt;list rend="ul"&gt;&lt;item&gt;Columns of &lt;math-renderer&gt;$Q$&lt;/math-renderer&gt;are orthonormal eigenvectors (principal directions).&lt;/item&gt;&lt;item&gt;Eigenvalues in &lt;math-renderer&gt;$\Lambda$&lt;/math-renderer&gt;measure variance explained by each direction.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Columns of &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first principal component is the eigenvector corresponding to the largest eigenvalue; it is the direction of maximum variance.&lt;/p&gt;
    &lt;p&gt;Suppose we have two-dimensional data points roughly aligned along the line &lt;/p&gt;
    &lt;p&gt;Eigenvalues are about &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First principal component: the line &lt;math-renderer&gt;$y = x$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Most variance lies along this direction.&lt;/item&gt;
      &lt;item&gt;Second component is nearly orthogonal (&lt;math-renderer&gt;$y = -x$&lt;/math-renderer&gt;), but variance there is tiny.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thus PCA reduces the data to essentially one dimension.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimensionality reduction: Represent data with fewer features while retaining most variance.&lt;/item&gt;
      &lt;item&gt;Noise reduction: Small eigenvalues correspond to noise; discarding them filters data.&lt;/item&gt;
      &lt;item&gt;Visualization: Projecting high-dimensional data onto top 2 or 3 principal components reveals structure.&lt;/item&gt;
      &lt;item&gt;Compression: PCA is used in image and signal compression.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The covariance matrix &lt;/p&gt;
    &lt;p&gt;PCA demonstrates how abstract linear algebra directly powers modern applications. Eigenvalues and eigenvectors give a practical method for simplifying data, revealing patterns, and reducing complexity. It is one of the most important algorithms derived from the spectral theorem.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Show that the covariance matrix is symmetric and positive semidefinite.&lt;/item&gt;
      &lt;item&gt;Compute the covariance matrix of the dataset &lt;math-renderer&gt;$(1,2), (2,3), (3,4)$&lt;/math-renderer&gt;, and find its eigenvalues and eigenvectors.&lt;/item&gt;
      &lt;item&gt;Explain why the first principal component captures the maximum variance.&lt;/item&gt;
      &lt;item&gt;In image compression, explain how PCA can reduce storage by keeping only the top &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;principal components.&lt;/item&gt;
      &lt;item&gt;Prove that the sum of the eigenvalues of the covariance matrix equals the total variance of the dataset.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra is the language of modern computer graphics. Every image rendered on a screen, every 3D model rotated or projected, is ultimately the result of applying matrices to vectors. Rotations, reflections, scalings, and projections are all linear transformations, making matrices the natural tool for manipulating geometry.&lt;/p&gt;
    &lt;p&gt;A counterclockwise rotation by an angle &lt;/p&gt;
    &lt;p&gt;For any vector &lt;/p&gt;
    &lt;p&gt;This preserves lengths and angles, since &lt;/p&gt;
    &lt;p&gt;In three dimensions, rotations are represented by &lt;/p&gt;
    &lt;p&gt;Similar formulas exist for rotations about the &lt;/p&gt;
    &lt;p&gt;More general 3D rotations can be described by axis‚Äìangle representation or quaternions, but the underlying idea is still linear transformations represented by matrices.&lt;/p&gt;
    &lt;p&gt;To display 3D objects on a 2D screen, we use projections:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Orthogonal projection: drops the&lt;/p&gt;&lt;math-renderer&gt;$z$&lt;/math-renderer&gt;-coordinate, mapping&lt;math-renderer&gt;$(x,y,z) \mapsto (x,y)$&lt;/math-renderer&gt;.&lt;p&gt;$$ P = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \ 0 &amp;amp; 1 &amp;amp; 0 \end{bmatrix}. $$&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Perspective projection: mimics the effect of a camera. A point&lt;/p&gt;&lt;math-renderer&gt;$(x,y,z)$&lt;/math-renderer&gt;projects to&lt;p&gt;$$ \left(\frac{x}{z}, \frac{y}{z}\right), $$&lt;/p&gt;&lt;p&gt;capturing how distant objects appear smaller.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These operations are linear (orthogonal projection) or nearly linear (perspective projection becomes linear in homogeneous coordinates).&lt;/p&gt;
    &lt;p&gt;To unify translations and projections with linear transformations, computer graphics uses homogeneous coordinates. A 3D point &lt;/p&gt;
    &lt;p&gt;Example: Translation by &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotations preserve shape and size, only changing orientation.&lt;/item&gt;
      &lt;item&gt;Projections reduce dimension: from 3D world space to 2D screen space.&lt;/item&gt;
      &lt;item&gt;Homogeneous coordinates allow us to combine multiple transformations (rotation + translation + projection) into a single matrix multiplication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra enables all real-time graphics: video games, simulations, CAD software, and movie effects. By chaining simple matrix operations, complex transformations are applied efficiently to millions of points per second.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write the rotation matrix for a 90¬∞ counterclockwise rotation in &lt;math-renderer&gt;$\mathbb{R}^2$&lt;/math-renderer&gt;. Apply it to&lt;math-renderer&gt;$(1,0)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Rotate the point &lt;math-renderer&gt;$(1,1,0)$&lt;/math-renderer&gt;about the&lt;math-renderer&gt;$z$&lt;/math-renderer&gt;-axis by 180¬∞.&lt;/item&gt;
      &lt;item&gt;Show that the determinant of any 2D or 3D rotation matrix is 1.&lt;/item&gt;
      &lt;item&gt;Derive the orthogonal projection matrix from &lt;math-renderer&gt;$\mathbb{R}^3$&lt;/math-renderer&gt;to the&lt;math-renderer&gt;$xy$&lt;/math-renderer&gt;-plane.&lt;/item&gt;
      &lt;item&gt;Explain how homogeneous coordinates allow translations to be represented as matrix multiplications.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra provides the foundation for many data science techniques. Two of the most important are dimensionality reduction, where high-dimensional datasets are compressed while preserving essential information, and the least squares method, which underlies regression and model fitting.&lt;/p&gt;
    &lt;p&gt;High-dimensional data often contains redundancy: many features are correlated, meaning the data essentially lies near a lower-dimensional subspace. Dimensionality reduction identifies these subspaces.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PCA (Principal Component Analysis): As introduced earlier, PCA diagonalizes the covariance matrix of the data.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Eigenvectors (principal components) define orthogonal directions of maximum variance.&lt;/item&gt;
          &lt;item&gt;Eigenvalues measure how much variance lies along each direction.&lt;/item&gt;
          &lt;item&gt;Keeping only the top &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;components reduces data from&lt;math-renderer&gt;$n$&lt;/math-renderer&gt;-dimensional space to&lt;math-renderer&gt;$k$&lt;/math-renderer&gt;-dimensional space while retaining most variability.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example 10.2.1. A dataset of 1000 images, each with 1024 pixels, may have most variance captured by just 50 eigenvectors of the covariance matrix. Projecting onto these components compresses the data while preserving essential features.&lt;/p&gt;
    &lt;p&gt;Often, we have more equations than unknowns-an overdetermined system:&lt;/p&gt;
    &lt;p&gt;An exact solution may not exist. Instead, we seek &lt;/p&gt;
    &lt;p&gt;This leads to the normal equations:&lt;/p&gt;
    &lt;p&gt;The solution is the orthogonal projection of &lt;/p&gt;
    &lt;p&gt;Fit a line &lt;/p&gt;
    &lt;p&gt;Matrix form:&lt;/p&gt;
    &lt;p&gt;Solve &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dimensionality reduction: Find the best subspace capturing most variance.&lt;/item&gt;
      &lt;item&gt;Least squares: Project the target vector onto the subspace spanned by predictors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both are projection problems, solved using inner products and orthogonality.&lt;/p&gt;
    &lt;p&gt;Dimensionality reduction makes large datasets tractable, filters noise, and reveals structure. Least squares fitting powers regression, statistics, and machine learning. Both rely directly on eigenvalues, eigenvectors, and projections-core tools of linear algebra.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Explain why PCA reduces noise in datasets by discarding small eigenvalue components.&lt;/item&gt;
      &lt;item&gt;Compute the least squares solution to fitting a line through &lt;math-renderer&gt;$(0,0), (1,1), (2,2)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Show that the least squares solution is unique if and only if &lt;math-renderer&gt;$A^T A$&lt;/math-renderer&gt;is invertible.&lt;/item&gt;
      &lt;item&gt;Prove that the least squares solution minimizes the squared error by projection arguments.&lt;/item&gt;
      &lt;item&gt;Apply PCA to the data points &lt;math-renderer&gt;$(1,0), (2,1), (3,2)$&lt;/math-renderer&gt;and find the first principal component.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Graphs and networks provide a natural setting where linear algebra comes to life. From modeling flows and connectivity to predicting long-term behavior, matrices translate network structure into algebraic form. Markov chains, already introduced in Section 8.4, are a central example of networks evolving over time.&lt;/p&gt;
    &lt;p&gt;A network (graph) with &lt;/p&gt;
    &lt;p&gt;For weighted graphs, entries may be positive weights instead of &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The number of walks of length &lt;math-renderer&gt;$k$&lt;/math-renderer&gt;from node&lt;math-renderer&gt;$i$&lt;/math-renderer&gt;to node&lt;math-renderer&gt;$j$&lt;/math-renderer&gt;is given by the entry&lt;math-renderer&gt;$(A^k)_{ij}$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;Powers of adjacency matrices thus encode connectivity over time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Another important matrix is the graph Laplacian:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;math-renderer&gt;$L$&lt;/math-renderer&gt;is symmetric and positive semidefinite.&lt;/item&gt;
      &lt;item&gt;The smallest eigenvalue is always &lt;math-renderer&gt;$0$&lt;/math-renderer&gt;, with eigenvector&lt;math-renderer&gt;$(1,1,\dots,1)$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The multiplicity of eigenvalue &lt;math-renderer&gt;$0$&lt;/math-renderer&gt;equals the number of connected components in the graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This connection between eigenvalues and connectivity forms the basis of spectral graph theory.&lt;/p&gt;
    &lt;p&gt;A Markov chain can be viewed as a random walk on a graph. If &lt;/p&gt;
    &lt;p&gt;describes the distribution of positions after &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The steady-state distribution is given by the eigenvector of &lt;math-renderer&gt;$P$&lt;/math-renderer&gt;with eigenvalue&lt;math-renderer&gt;$1$&lt;/math-renderer&gt;.&lt;/item&gt;
      &lt;item&gt;The speed of convergence depends on the gap between the largest eigenvalue (which is always &lt;math-renderer&gt;$1$&lt;/math-renderer&gt;) and the second largest eigenvalue.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Consider a simple 3-node cycle graph:&lt;/p&gt;
    &lt;p&gt;This Markov chain cycles deterministically among the nodes. Eigenvalues are the cube roots of unity: &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search engines: Google‚Äôs PageRank algorithm models the web as a Markov chain, where steady-state probabilities rank pages.&lt;/item&gt;
      &lt;item&gt;Network analysis: Eigenvalues of adjacency or Laplacian matrices reveal communities, bottlenecks, and robustness.&lt;/item&gt;
      &lt;item&gt;Epidemiology and information flow: Random walks model how diseases or ideas spread through networks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linear algebra transforms network problems into matrix problems. Eigenvalues and eigenvectors reveal connectivity, flow, stability, and long-term dynamics. Networks are everywhere-social media, biology, finance, and the internet-so these tools are indispensable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Write the adjacency matrix of a square graph with 4 nodes. Compute&lt;/p&gt;&lt;math-renderer&gt;$A^2$&lt;/math-renderer&gt;and interpret the entries.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show that the Laplacian of a connected graph has exactly one zero eigenvalue.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the steady-state distribution of the Markov chain with&lt;/p&gt;
        &lt;p&gt;$$ P = \begin{bmatrix} 0.5 &amp;amp; 0.5 \ 0.4 &amp;amp; 0.6 \end{bmatrix}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain how eigenvalues of the Laplacian can detect disconnected components of a graph.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Describe how PageRank modifies the transition matrix of the web graph to ensure a unique steady-state distribution.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Modern machine learning is built on linear algebra. From the representation of data as matrices to the optimization of large-scale models, nearly every step relies on concepts such as vector spaces, projections, eigenvalues, and matrix decompositions.&lt;/p&gt;
    &lt;p&gt;A dataset with &lt;/p&gt;
    &lt;p&gt;$$ X = \begin{bmatrix}&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_1^T &amp;amp; - \&lt;/item&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_2^T &amp;amp; - \ &amp;amp; \vdots &amp;amp; \&lt;/item&gt;
      &lt;item&gt;&amp;amp; \mathbf{x}_m^T &amp;amp; - \end{bmatrix}, $$&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;where each row &lt;/p&gt;
    &lt;p&gt;At the heart of machine learning are linear predictors:&lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;p&gt;This is solved efficiently using matrix factorizations.&lt;/p&gt;
    &lt;p&gt;The SVD of a matrix &lt;/p&gt;
    &lt;p&gt;where &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Singular values measure the importance of directions in feature space.&lt;/item&gt;
      &lt;item&gt;SVD is used for dimensionality reduction (low-rank approximations), topic modeling, and recommender systems.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PCA (Principal Component Analysis): diagonalization of the covariance matrix identifies directions of maximal variance.&lt;/item&gt;
      &lt;item&gt;Spectral clustering: uses eigenvectors of the Laplacian to group data points into clusters.&lt;/item&gt;
      &lt;item&gt;Stability analysis: eigenvalues of Hessian matrices determine whether optimization converges to a minimum.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even deep learning, though nonlinear, uses linear algebra at its core:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each layer is a matrix multiplication followed by a nonlinear activation.&lt;/item&gt;
      &lt;item&gt;Training requires computing gradients, which are expressed in terms of matrix calculus.&lt;/item&gt;
      &lt;item&gt;Backpropagation is essentially repeated applications of the chain rule with linear algebra.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Machine learning models often involve datasets with millions of features and parameters. Linear algebra provides the algorithms and abstractions that make training and inference possible. Without it, large-scale computation in AI would be intractable.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Show that ridge regression leads to the normal equations&lt;/p&gt;
        &lt;p&gt;$$ (X^T X + \lambda I)\mathbf{w} = X^T \mathbf{y}. $$&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explain how SVD can be used to compress an image represented as a matrix of pixel intensities.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For a covariance matrix&lt;/p&gt;&lt;math-renderer&gt;$\Sigma$&lt;/math-renderer&gt;, show why its eigenvalues represent variances along principal components.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Give an example of how eigenvectors of the Laplacian matrix can be used for clustering a small graph.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In a neural network with one hidden layer, write the forward pass in matrix form.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45103436</guid></item><item><title>Removing Guix from Debian</title><link>https://lwn.net/SubscriberLink/1035491/d8100135a8ae4246/</link><description>&lt;doc fingerprint="8eefcc0588a5d39a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Removing Guix from Debian&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As a rule, if a package is shipped with a Debian release, users can count on it being available, and updated, for the entire life of the release. If package foo is included in the stable release‚Äîcurrently Debian 13 ("trixie")‚Äîa user can reasonably expect that it will continue to be available with security backports as long as that release is supported, though it may not be included in Debian 14 ("forky"). However, it is likely that the Guix package manager will soon be removed from the repositories for Debian 13 and Debian 12 ("bookworm", also called oldstable).&lt;/p&gt;
    &lt;p&gt;Debian has the Advanced Package Tool (APT) for package management, of course, but Guix offers a different approach and can be used in conjunction with other distribution package managers. Guix is inspired by Nix's functional package management; it offers transactional upgrades and rollbacks, package management for unprivileged users, and more. Unlike Nix, its packages are defined using the Guile implementation of the Scheme programming language. There is also a GNU Guix distribution as well; LWN covered both NixOS and Guix in February 2024, and looked at Nix alternative Lix in July 2024.&lt;/p&gt;
    &lt;p&gt;On June 24, the Guix project disclosed several security vulnerabilities that affected the guix-daemon, which is a program that is used to build software and access the store where successful builds are kept. Two of the vulnerabilities, CVE-2025-46415 and CVE-2025-46416, would allow a local user to gain the privileges of any build users, manipulate build output, as well as gain the privileges of the daemon user. The vulnerabilities also impacted Nix and Lix package managers.&lt;/p&gt;
    &lt;p&gt;The disclosure blog post gave instructions on how to mitigate the vulnerabilities by updating guix-daemon using the "guix pull" command, but the project did not make a new Guix release. The last actual release from the project was version 1.4.0, which was announced in December 2022. The Guix project has a rolling-release model, with sporadic releases, and does not maintain stable branches with security updates. This may not pose a problem for users getting Guix directly from the project, but it poses some obstacles for inclusion in other Linux distributions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Debian package&lt;/head&gt;
    &lt;p&gt;Salvatore Bonaccorso filed a bug against Debian's guix package on June 25 to report the vulnerabilities. Vagrant Cascadian, the maintainer of the package, replied on July 15, and said that the fixes for the security vulnerabilities had been "&lt;quote&gt;commingled with a lot of other upstream changes&lt;/quote&gt;", and it would be "&lt;quote&gt;trickier than in the past&lt;/quote&gt;" to try to backport the fixes without the other changes in Guix. He said he had just managed to "&lt;quote&gt;get something to compile&lt;/quote&gt;" with the security fixes applied, using a backport repository maintained by Denis 'GNUtoo' Carikli.&lt;/p&gt;
    &lt;p&gt;Carikli had brought up the difficulty of backporting Guix fixes on the guix-devel mailing list on July 8. Various distributions had Guix versions 1.2.0, 1.3.0, and 1.4.0, with Debian shipping 1.2.0 and 1.4.0 and used as the upstream for other distributions' packages:&lt;/p&gt;
    &lt;quote&gt;But the Debian package maintainer has the almost impossible task to backport all the security fixes without a community nor help behind [maintaining it] and as things are going, this will probably lead to the Debian guix package being removed with cascading effect for the other distributions.&lt;/quote&gt;
    &lt;p&gt;He said he had applied about 50 patches that involve guix-daemon between the 1.4.0 release and the last-known security fix. He also noted that his effort would probably not be suitable for Linux distributions that ship a Guix package. He wondered what the best way would be to collaborate on a branch that distributions could pull from for updates.&lt;/p&gt;
    &lt;p&gt;Liam Hupfer said that "&lt;quote&gt;we gave up and shipped the last commit on master mentioned in the recent CVE disclosure&lt;/quote&gt;" for Nix. He said he would also like to see Guix figure out backporting patches, but could Cascadian consider the Nix approach until then?&lt;/p&gt;
    &lt;p&gt;No, that approach would not make sense, Cascadian said. If Guix was "&lt;quote&gt;truly a rolling release&lt;/quote&gt;", then it may just not make sense to maintain distribution packages:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Up till recently, it has always been possible to backport changes with relative ease, but that was perhaps just lack of development... the recent unprivileged daemon stuff really made backporting patches harder. [...]&lt;/p&gt;
      &lt;p&gt;In the Debian release model, ideally we would avoid bringing in unrelated patchsets (e.g. the unprivileged daemon code bringing in an entire network stack?) but that might be too hard to pull off. Not sure if the security team would accept a patchset that includes more than the minimum necessary to fix the security issues.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Without a branch that contained backported patches, they would be inclined "&lt;quote&gt;to recommend dropping the Guix package in Debian&lt;/quote&gt;".&lt;/p&gt;
    &lt;p&gt;On August 27, they did just that. Cascadian filed a bug to remove guix from Debian 11 ("bullseye"), bookworm, and trixie. They also filed a bug to remove the package from the upcoming forky release. Adam D. Barratt replied and said that it would not be possible to remove guix from bullseye, which is now an LTS release; only updates from the security archive were now allowed.&lt;/p&gt;
    &lt;p&gt;There are no dependencies on guix, so the removal of the package should not affect any other Debian packages in the trixie and bookworm releases.&lt;/p&gt;
    &lt;head rend="h4"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;I emailed Cascadian to find out what the next steps would be for the package in those releases. They said that the package is likely to be removed from the upcoming point releases for trixie and bookworm. Users who have it installed already will be stuck at the old version, unless they take manual steps to update. Cascadian said this was not a great outcome "&lt;quote&gt;but better than keeping it available for people to install the vulnerable version&lt;/quote&gt;".&lt;/p&gt;
    &lt;p&gt;The guix package should not have landed in trixie at all, Cascadian said, "&lt;quote&gt;my understanding was that [the bug report] should have blocked it from being released&lt;/quote&gt;". However, it seems there was enough activity on the bug that prevented Debian's autoremoval mechanisms from triggering and pulling the package from the final release. "&lt;quote&gt;There apparently is not manual review of all blockers in the current Debian release process&lt;/quote&gt;". Even if the timing had been better for the trixie release, though, the same fixes would need to have been applied to bookworm and older releases.&lt;/p&gt;
    &lt;p&gt;Cascadian said that it has been a fair amount of work to keep Guix working on Debian. For example, he has had to maintain various Guile dependencies, and deal with the fact that Guix uses "&lt;quote&gt;fairly old&lt;/quote&gt;" GCC versions whereas Debian usually ships the latest GCC version available for a given release. At some point, they said, "&lt;quote&gt;you have to evaluate whether that work is worth it&lt;/quote&gt;" when the upstream provides a binary that people can install.&lt;/p&gt;
    &lt;p&gt;Guix is better for having been packaged for Debian and run through Debian's Lintian tool. Cascadian said that they have probably fixed more typos in Guix than anyone else, and have found other problems while checking that the builds of Guix are reproducible. "&lt;quote&gt;Any time you run a piece of software through processes outside of the primary development workflow, you find surprises worth fixing.&lt;/quote&gt;"&lt;/p&gt;
    &lt;head rend="h4"&gt;Regular releases&lt;/head&gt;
    &lt;p&gt;There is an effort in the Guix project to create yearly releases. In May, Steve George proposed that the project adopt a Guix Consensus Document for "Regular and efficient releases", and it was accepted by the project in July. It calls for a release every year in June, with a one-time exception for a November 2025 release, and a short development cycle for the June 2026 release. Even so, that will not provide stable branches for Debian and other distributions to pull from; it will just shorten the interval and feature differences between major releases.&lt;/p&gt;
    &lt;p&gt;Debian users will, of course, still be able to use Guix by installing it using binaries provided by the Guix project. That will, at least potentially, leave some users out in the cold‚ÄîDebian currently provides a guix package for x86-64, Arm64, PowerPC, RISC-V, 32-bit Arm, and 32-bit x86. The Guix project itself does does not have RISC-V binaries, though it does cover the other architectures.&lt;/p&gt;
    &lt;p&gt;It is fairly unusual for a package to be removed from a stable or oldstable release. For example, the bookworm release has been out for more than two years, but a search of the Debian bug database only shows one package‚Äîguix‚Äîthat has the "RM" tag in the subject to flag a package for removal.&lt;/p&gt;
    &lt;p&gt;According to Debian's popularity contest (popcon) statistics, there are not quite 230 systems with Guix installed. Popcon statistics only hint at the actual number of package installs, but assuming they are approximately accurate, then removing Guix will not inconvenience a significant number of Debian users. It will, however, mean that fewer people are poking at Guix with an intent to making it work on distributions like Debian, while finding distribution-specific bugs.&lt;/p&gt;
    &lt;p&gt; Posted Sep 2, 2025 14:34 UTC (Tue) by muase (subscriber, #178466) [Link] (3 responses) Do we have a meta-statistic or interpolated number how many systems participate in the popularity context? As it is disabled by default ‚Äì and Linux-users maybe tend to be more privacy focused ‚Äì I'd imagine there is quite a number of unreported installations. Posted Sep 2, 2025 14:45 UTC (Tue) by rschroev (subscriber, #4164) [Link] Posted Sep 2, 2025 14:56 UTC (Tue) by jzb (editor, #7867) [Link] (1 responses) If you visit the main popcon pages you'll find stats on how many systems are participating in popcon and how many submissions there are. The "stable reports" tab suggests that about 135K bookworm systems have checked in in the past 12 months. Whether 135K is 10%, 20%, etc. of all installs I have no idea. Posted Sep 2, 2025 15:35 UTC (Tue) by muase (subscriber, #178466) [Link] Posted Sep 2, 2025 15:59 UTC (Tue) by jafd (subscriber, #129642) [Link] (1 responses) Am I the only one who finds it quite strange that to ‚Äúbuild software and access the store where successful builds are kept‚Äù, a daemon process is required in the first place? Posted Sep 2, 2025 16:09 UTC (Tue) by daroc (editor, #160859) [Link] Posted Sep 2, 2025 17:37 UTC (Tue) by kkremitzki (subscriber, #115703) [Link] 4cf1acc7f30 + cherry-picked 71171538e12 + 1c78f71beb3 + a49536e3200 + 7f237f3e6ca &lt;head&gt;Popcon Dark Figure&lt;/head&gt;&lt;head&gt;Popcon Dark Figure&lt;/head&gt;&lt;head&gt;Popcon Dark Figure&lt;/head&gt;&lt;head&gt;Popcon Dark Figure&lt;/head&gt;&lt;head&gt;A daemon process?&lt;/head&gt;&lt;head&gt;A daemon process?&lt;/head&gt;&lt;head&gt;Vuln proof-of-concept issues&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45103857</guid></item><item><title>The staff ate it later</title><link>https://en.wikipedia.org/wiki/The_staff_ate_it_later</link><description>&lt;doc fingerprint="c70ed9e3348c97c3"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The staff ate it later&lt;/head&gt;&lt;p&gt;"The staff ate it later" (Japanese: „Åì„ÅÆÂæå„ÄÅ„Çπ„Çø„ÉÉ„Éï„ÅåÁæéÂë≥„Åó„Åè„ÅÑ„Åü„Å†„Åç„Åæ„Åó„Åü, romanized: Kono ato, sutaffu ga oishiku itadakimashita) is a caption shown on screen when food appears in a Japanese TV program to indicate that it was not thrown away after filming. Some question the authenticity of this statement, and others believe this caption lowers the quality of TV programs.&lt;/p&gt;&lt;head rend="h2"&gt;First appearance&lt;/head&gt;[edit]&lt;p&gt;It is thought TV stations first began showing the caption to protect themselves against complaints from viewers who disliked food being handled without consideration in TV variety shows.[1] It is uncertain when this note was first used, but TV producer Kenji Suga stated viewers complained about the waste of food when a performance using small watermelons was broadcast in Downtown no Gaki no Tsukai ya Arahende!! on Nippon TV. The TV station then showed this note on screen the following year in response.[2]&lt;/p&gt;&lt;head rend="h2"&gt;Authenticity&lt;/head&gt;[edit]&lt;p&gt;There are various claims as to whether or not staff actually eat the food that appears in the programs.[1][3][4]&lt;/p&gt;&lt;head rend="h3"&gt;Supporting reports&lt;/head&gt;[edit]&lt;p&gt;According to AOL News in 2014, the crew on one information program claimed: "It's sometimes impossible for the reporter to eat all the food provided by the restaurant. The reporter is told not to eat it all, but the crew will eat the rest out of consideration and a feeling of obligation towards the restaurant."[4]&lt;/p&gt;&lt;p&gt;Food comic artist Raswell Hosoki claimed in Meshizanmai Furusatonoaji (Meshizanmai Taste of Hometown) that the note is true. Eriko Miyazaki , a reporter and TV personality for food shows, also claimed the note is true and stated: "The crew eats the rest of the food, at least at the shows I appear in."[5]&lt;/p&gt;&lt;p&gt;In January 2018, Miwa Asao, former professional beach volleyball player and TV personality, posted photos on her blog of staff eating food after recording "Saturday Night! Otona na TV ". She wrote: "This is an on-site photo. The staff ate the rest of the food."[6]&lt;/p&gt;&lt;head rend="h3"&gt;Refuting reports&lt;/head&gt;[edit]&lt;p&gt;Hitoshi Matsumoto, a comedian and TV host, was asked by sociologist Noritoshi Furuichi about this note in 2014 during the "Wide na Show " (Fuji Television). He said: "To be honest, I've never seen the crew eat the food. But that just means I haven't seen it. They might've eaten it."[7]&lt;/p&gt;&lt;p&gt;Takeshi Kitano (also known as Beat Takeshi), a Japanese comedian, actor, and filmmaker, referred to an instance where cake was smeared on the floor and said in his book Bakaron: "Liars. Who's going to enjoy cake they splattered all over the floor?"[3] Commentator Tsunehira Furuya also stated that the food featured in the show is not eaten by the staff later and is instead simply thrown into garbage bags.[1]&lt;/p&gt;&lt;head rend="h2"&gt;Reception&lt;/head&gt;[edit]&lt;p&gt;Commentator Tetsuya Uetaki has commented on displaying the note, saying: "Producers have become more aware as viewers have become more critical after issues such as the Aru Aru Mondai (a natto shortage caused by a program claiming eating natto would make people lose weight), and it's fine as one method for dealing with that." However, Uetaki went on to say: "This shifts responsibility onto the viewers. We can't let it end as simply an empty concession. I want to see variety shows strive to properly handle information and properly put the show together, from the moment they start building it."[8]&lt;/p&gt;&lt;p&gt;Broadcast writer Sotani commented on the fact that production teams have become more sensitive to this in programs and have begun displaying such notes as an attempt to preempt criticism. He claims this sort of extreme self regulation risks leading to a decline.[9] TV producer Kenji Suga claims it is necessary for programs to be disconnected from real life and society, to be "dumb and idiotic" to produce laughs.[2]&lt;/p&gt;&lt;p&gt;Columnist Takashi Matsuo argues that adults, not TV shows, should teach children the ethics surrounding the importance of food. He also argues that if a parent is uncomfortable with what a comedian expresses on TV, the right course of action would be to change the channel or turn off the TV, not send a complaint to the TV station.[10] Matsuo also points out the inconsistency that "the staff ate it later" caption is not displayed when large numbers of tomatoes are thrown at the festival of Tomatina in Spain or when athletes spray each other with champagne in celebration of a victory.[10]&lt;/p&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b c Furuya, Tsunehira (2017). „ÄåÈÅìÂæ≥Ëá™Ë≠¶Âõ£„Äç„Åå„Éã„ÉÉ„Éù„É≥„ÇíÊªÖ„Åº„Åô. East Shinsho: East Press. pp. 35‚Äì36. ISBN 978-4-7816-5095-1.&lt;/item&gt;&lt;item&gt;^ a b Wake, Shinya (7 February 2016). "„Ç∞„É≠„Éº„Éñ176Âè∑ Á¨ë„ÅÑ„ÅÆÂäõ „Ç§„É≥„Çø„Éì„É•„Éº Á¨ë„Çè„Åõ„Çã„Å£„Å¶„ÇÄ„Åö„Åã„Åó„ÅÑ „Éó„É≠„Éá„É•„Éº„Çµ„Éº„ÉªËèÖË≥¢Ê≤ª". Asahi Shimbun. p. 6.&lt;/item&gt;&lt;item&gt;^ a b Kitano, Takeshi (2017). „Éê„Ç´Ë´ñ. Shinchosha. pp. 36‚Äì37. ISBN 978-4-10-610737-5.&lt;/item&gt;&lt;item&gt;^ a b "„ÉÜ„É¨„ÉìÁï™ÁµÑ„ÅÆÈ£ü„É™„Éù„ÄÅÂÆåÈ£ü„Åó„Å¶„ÅÑ„Çã„ÅÆ„ÅãÔºü„Äå„Åì„ÅÆÂæå„Çπ„Çø„ÉÉ„Éï„ÅåÁæéÂë≥„Åó„Åè...„Äç„ÅØÊú¨ÂΩì„Åã" [Is the staff really eating the rest of the dishes used in the TV show?]. AOL News. 16 April 2014. Archived from the original on 16 September 2014. Retrieved 9 January 2020.&lt;/item&gt;&lt;item&gt;^ Raswell Hosoki, Mayumi Kato, Takako Aonuma, Sachiko Orihara, Junko Kubota, Eiko Kasai, Riyo Mizuki, Takotsumuri, Usami‚òÜ, and Somei Yoshino, (2017) Meshizanmai Hurusatonoaji, Bunkasha, BUNKASHA COMICS, ISBN 978-4-8211-3416-8&lt;/item&gt;&lt;item&gt;^ "„Éê„É©„Ç®„ÉÜ„Ç£„ÅÆ„Äå„Åì„ÅÆÂæå„Çπ„Çø„ÉÉ„Éï„ÅåÁæéÂë≥„Åó„ÅèÈ†Ç„Åç„Åæ„Åó„Åü„Äç ‰∫àÈò≤Á∑ö„ÇíÂºµ„Çã„ÉÜ„É≠„ÉÉ„Éó„Å©„Åì„Åæ„ÅßÂøÖË¶ÅÔºü" [Variety's "The staff enjoyed the food afterwards": How much precautionary captioning is necessary?]. Oricon News. 13 February 2018. Archived from the original on 18 September 2024. Retrieved 26 December 2020.&lt;/item&gt;&lt;item&gt;^ "ÊùæÊú¨‰∫∫Âøó „Éê„É©„Ç®„ÉÜ„Ç£„Å™„Çâ„Åß„ÅÆËëõËó§„ÇíÂêêÈú≤„ÄåÈ£ü„ÅπÁâ©„ÇÇÁ¨ë„ÅÑ„ÅÆ1„Å§„ÅÆÂ∞èÈÅìÂÖ∑„Å®„Åó„Å¶Ë™ç„ÇÅ„Å¶„ÇÇ„Çâ„Åà„Åü„Çâ„Äç" [Hitoshi Matsumoto, revealing his struggles with variety: "If people would accept food as a prop for laughter..."]. Nagai Times. 28 October 2014. Archived from the original on 2 December 2024. Retrieved 26 December 2020.&lt;/item&gt;&lt;item&gt;^ "Ëøë„Åî„Çç„Çà„ÅèË¶ã„Çã„Äé„ÅäÊñ≠„Çä„ÉÜ„É≠„ÉÉ„Éó„Äè„ÄéË¶ñËÅ¥ËÄÖ„Å∏„ÅÆÈÖçÊÖÆ„Äè„ÅãËã¶ÊÉÖÊäóË≠∞"ÂÖàÈÄÉ„Çå"„Åã „Å™„ÅÑ„Çà„Çä„Åæ„Åó„Å†„Åå‚Ä¶„ÄéÁï™ÁµÑÁ≤æÊüª„Åì„ÅùËÇùÂøÉ„ÄèË≠òËÄÖÊåáÊëò". Chunichi Shimbun. 4 July 2007. p. 15.&lt;/item&gt;&lt;item&gt;^ "1Áï™„ÇÇ„ÅÆ„Åå„Åü„Çä ‰∫∫Áâ©Á∑® Â£≤„Çå„Å£Â≠êÊîæÈÄÅ‰ΩúÂÆ∂ „Åù„Éº„Åü„Å´Ê∞è„ÄåË¶ã„Åõ„Åü„Åè„Å™„ÅÑ„Äç„ÅßÈáëÂ≠óÂ°î PTA„ÅÆÂúü‰øµ„Å´‰πó„Çâ„Åö". Hokkoku Shimbun. 23 February 2012. p. 36.&lt;/item&gt;&lt;item&gt;^ a b Matsuo, Takashi (17 September 2017). "„ÉÜ„É¨„Éì„ÅÆÈÅéÂâ∞„Å™„ÉÜ„É≠„ÉÉ„Éó Ëã¶ÊÉÖÈÄÉ„Çå„ÅÆ‰øùË∫´„ÅåÁõÆÁöÑÔºü" [Is over-annotation on television a self-protection to escape complaints?]. Mainichi Shimbun Digital. Retrieved 26 December 2020.[dead link]&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104289</guid></item><item><title>Static sites enable a good time travel experience</title><link>https://hamatti.org/posts/static-sites-enable-a-good-time-travel-experience/</link><description>&lt;doc fingerprint="5895a521cf7dd597"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Static sites enable a good time travel experience&lt;/head&gt;
    &lt;p&gt;Varun wrote about gamifying blogging and personal website maintenance which reminded me of the time when I awarded myself some badges for blogging.&lt;/p&gt;
    &lt;p&gt;I mentioned this to Varun who asked if I had any screenshots of what it looked like on my website. My initial answer was ‚Äúno‚Äù, then I looked at Wayback Machine but there were not pictures of the badges.&lt;/p&gt;
    &lt;p&gt;Then, a bit later it hit me. I don‚Äôt need any archived screenshots: my website is built with Eleventy and it's static so I can check out a git commit from the time I had those badges up, fire up Eleventy and see the website ‚Äî as it was in the spring of 2021.&lt;/p&gt;
    &lt;p&gt;That‚Äôs a beauty of a static site generator combined with my workflow of fetching posts from CMS before build time so each commit contains a full snapshot of the website.&lt;/p&gt;
    &lt;p&gt; Comparing this to a website that uses a database for posts (like WordPress) or a flow where posts from CMS are not stored in version control but rather fetched at build time only, my solution makes time travel to (almost) any given moment in time a two-command operation (&lt;code&gt;git checkout&lt;/code&gt;
  with the right commit hash and
  &lt;code&gt;@11ty/eleventy serve&lt;/code&gt; to serve a dev
  server). I say almost because back in the day I wasn‚Äôt quite as diligent in
  commiting every change as I was deploying manually and not through version
  control automation.
&lt;/p&gt;
    &lt;p&gt;A year ago, inspired by Alex Chan‚Äôs blog post Taking regular screenshots of my website I set up a GitHub Action that takes a snapshot of my front page once a month to keep a record. At the time, I felt bit sad that I hadn‚Äôt started it before. However, now that I realised how easy it is for me to go back in time thanks to Eleventy and git, I‚Äôm not so worried anymore. Maybe I should do a collage of changes on my design one day by going through my project history.&lt;/p&gt;
    &lt;p&gt;One more major point for static site generators!&lt;/p&gt;
    &lt;p&gt;If something above resonated with you, let's start a discussion about it! Email me at juhamattisantala at gmail dot com and share your thoughts. In 2025, I want to have more deeper discussions with people from around the world and I'd love if you'd be part of that.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104303</guid></item><item><title>X(Twitter) Shadow Bans Turkish Presidential Candidate</title><link>https://utkusen.substack.com/p/xtwitter-secretly-shadow-bans-turkish</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104597</guid></item><item><title>Anthropic raises $13B Series F at $183B post-money valuation</title><link>https://www.anthropic.com/news/anthropic-raises-series-f-at-usd183b-post-money-valuation</link><description>&lt;doc fingerprint="ec368bfc2c378ca8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Anthropic raises $13B Series F at $183B post-money valuation&lt;/head&gt;
    &lt;p&gt;Anthropic has completed a Series F fundraising of $13 billion led by ICONIQ. This financing values Anthropic at $183 billion post-money. Along with ICONIQ, the round was co-led by Fidelity Management &amp;amp; Research Company and Lightspeed Venture Partners. The investment reflects Anthropic‚Äôs continued momentum and reinforces our position as the leading intelligence platform for enterprises, developers, and power users.&lt;/p&gt;
    &lt;p&gt;Significant investors in this round include Altimeter, Baillie Gifford, affiliated funds of BlackRock, Blackstone, Coatue, D1 Capital Partners, General Atlantic, General Catalyst, GIC, Growth Equity at Goldman Sachs Alternatives, Insight Partners, Jane Street, Ontario Teachers' Pension Plan, Qatar Investment Authority, TPG, T. Rowe Price Associates, Inc., T. Rowe Price Investment Management, Inc., WCM Investment Management, and XN.&lt;/p&gt;
    &lt;p&gt;‚ÄúFrom Fortune 500 companies to AI-native startups, our customers rely on Anthropic‚Äôs frontier models and platform products for their most important, mission-critical work,‚Äù said Krishna Rao, Chief Financial Officer of Anthropic. ‚ÄúWe are seeing exponential growth in demand across our entire customer base. This financing demonstrates investors‚Äô extraordinary confidence in our financial performance and the strength of their collaboration with us to continue fueling our unprecedented growth.‚Äù&lt;/p&gt;
    &lt;p&gt;Anthropic has seen rapid growth since the launch of Claude in March 2023. At the beginning of 2025, less than two years after launch, Anthropic‚Äôs run-rate revenue had grown to approximately $1 billion. By August 2025, just eight months later, our run-rate revenue reached over $5 billion‚Äîmaking Anthropic one of the fastest-growing technology companies in history.&lt;/p&gt;
    &lt;p&gt;Anthropic‚Äôs trajectory has been driven by our leading technical talent, our focus on safety, and our frontier research, including pioneering alignment and interpretability work, all of which underpin the performance and reliability of our models. Every day more businesses, developers, and consumer power users are trusting Claude to help them solve their most challenging problems. Anthropic now serves over 300,000 business customers, and our number of large accounts‚Äîcustomers that each represent over $100,000 in run-rate revenue‚Äîhas grown nearly 7x in the past year.&lt;/p&gt;
    &lt;p&gt;This growth spans the entire Anthropic platform, with advancements for businesses, developers, and consumers. For businesses, our API and industry-specific products make it easy to add powerful AI to their critical applications without complex integration work. Developers have made Claude Code their tool of choice since its full launch in May 2025. Claude Code has quickly taken off‚Äîalready generating over $500 million in run-rate revenue with usage growing more than 10x in just three months. For individual users, the Pro and Max plans for Claude deliver enhanced AI capabilities for everyday tasks and specialized projects.&lt;/p&gt;
    &lt;p&gt;‚ÄúAnthropic is on an exceptional trajectory, combining research excellence, technological leadership, and relentless focus on customers. We‚Äôre honored to partner with Dario and the team, and our lead investment in their Series F reflects our belief in their values and their ability to shape the future of responsible AI,‚Äù said Divesh Makan, Partner at ICONIQ. ‚ÄúEnterprise leaders tell us what we‚Äôre seeing firsthand‚ÄîClaude is reliable, built on a trustworthy foundation, and guided by leaders truly focused on the long term.‚Äù&lt;/p&gt;
    &lt;p&gt;The Series F investment will expand our capacity to meet growing enterprise demand, deepen our safety research, and support international expansion as we continue building reliable, interpretable, and steerable AI systems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104907</guid></item><item><title>Launch HN: Datafruit (YC S25) ‚Äì AI for DevOps</title><link>https://news.ycombinator.com/item?id=45104974</link><description>&lt;doc fingerprint="ee5fd4d201abb246"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We‚Äôre Abhi, Venkat, Tom, and Nick and we are building Datafruit (&lt;/p&gt;https://datafruit.dev/&lt;p&gt;), an AI DevOps agent. We‚Äôre like Devin for DevOps. You can ask Datafruit to check your cloud spend, look for loose security policies, make changes to your IaC, and it can reason across your deployment standards, design docs, and DevOps practices.&lt;/p&gt;&lt;p&gt;Demo video: https://www.youtube.com/watch?v=2FitSggI7tg.&lt;/p&gt;&lt;p&gt;Right now, we have two main methods to interact with Datafruit:&lt;/p&gt;&lt;p&gt;(1) automated infrastructure audits‚Äî agents periodically scan your environment to find cost optimization opportunities, detect infrastructure drift, and validate your infra against compliance requirements.&lt;/p&gt;&lt;p&gt;(2) chat interface (available as a web UI and through slack) ‚Äî ask the agent questions for real-time insights, or assign tasks directly, such as investigating spend anomalies, reviewing security posture, or applying changes to IaC resources.&lt;/p&gt;&lt;p&gt;Working at FAANG and various high-growth startups, we realized that infra work requires an enormous amount of context, often more than traditional software engineering. The business decisions, codebase, and cloud itself are all extremely important in any task that has been assigned. To maximize the success of the agents, we do a fair amount of context engineering. Not hallucinating is super important!&lt;/p&gt;&lt;p&gt;One thing which has worked incredibly well for us is a multi-agent system where we have specialized sub-agents with access to specific tool calls and documentation for their specialty. Agents choose to ‚Äúhandoff‚Äù to each other when they feel like another agent would be more specialized for the task. However, all agents share the same context (https://cognition.ai/blog/dont-build-multi-agents). We‚Äôre pretty happy with this approach, and believe it could work in other disciplines which require high amounts of specialized expertise.&lt;/p&gt;&lt;p&gt;Infrastructure is probably the most mission-critical part of any software organization, and needs extremely heavy guardrails to keep it safe. Language models are not yet at the point where they can be trusted to make changes (we‚Äôve talked to a couple of startups where the Claude Code + AWS CLI combo has taken their infra down). Right now, Datafruit receives read-only access to your infrastructure and can only make changes through pull requests to your IaC repositories. The agent also operates in a sandboxed virtual environment so that it could not write cloud CLI commands if it wanted to!&lt;/p&gt;&lt;p&gt;Where LLMs can add significant value is in reducing the constant operational inefficiencies that eat up cloud spend and delay deadlines‚Äîthe small-but-urgent ops work. Once Datafruit indexes your environment, you can ask it to do things like:&lt;/p&gt;&lt;quote&gt;&lt;code&gt;  "Grant @User write access to analytics S3 bucket for 24 hours"
    -&amp;gt; Creates temporary IAM role, sends least-privilege credentials, auto-revokes tomorrow

  "Find where this secret is used so I can rotate it without downtime"
    -&amp;gt; Discovers all instances of your secret, including old cron-jobs you might not know about, so you can safely rotate your keys


  "Why did database costs spike yesterday?"
    -&amp;gt; Identifies expensive queries, shows optimization options, implements fixes

&lt;/code&gt;&lt;/quote&gt;&lt;p&gt; We charge a straightforward subscription model for a managed version, but we also offer a bring-your-own-cloud model. All of Datafruit can be deployed on Kubernetes using Helm charts for enterprise customers where data can‚Äôt leave your VPC. For the time being, we‚Äôre installing the product ourselves on customers' clouds. It doesn‚Äôt exist in a self-serve form yet. We‚Äôll get there eventually, but in the meantime if you‚Äôre interested we‚Äôd love for you guys to email us at founders@datafruit.dev.&lt;/p&gt;&lt;p&gt;We would love to hear your thoughts! If you work with cloud infra, we are especially interested in learning about what kinds of work you do which you wish could be offloaded onto an agent.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45104974</guid></item><item><title>OpenAI says it's scanning users' conversations and reporting content to police</title><link>https://futurism.com/openai-scanning-conversations-police</link><description>&lt;doc fingerprint="3003187491b757bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Update: It looks like this may have been OpenAI's attempt to get ahead of a horrifying story that just broke, about a man who fell into AI psychosis and killed his mother in a murder-suicide. Full details here.&lt;/p&gt;
    &lt;p&gt;For the better part of a year, we've watched ‚Äî and reported ‚Äî in horror as more and more stories emerge about AI chatbots leading people to self-harm, delusions, hospitalization, arrest, and suicide.&lt;/p&gt;
    &lt;p&gt;As the loved ones of the people impacted by these dangerous bots rally for change to prevent such harm from happening to anyone else, the companies that run these AIs have been slow to implement safeguards ‚Äî and OpenAI, whose ChatGPT has been repeatedly implicated in what experts are now calling "AI psychosis," has until recently done little more than offer copy-pasted promises.&lt;/p&gt;
    &lt;p&gt;In a new blog post admitting certain failures amid its users' mental health crises, OpenAI also quietly disclosed that it's now scanning users' messages for certain types of harmful content, escalating particularly worrying content to human staff for review ‚Äî and, in some cases, reporting it to the cops.&lt;/p&gt;
    &lt;p&gt;"When we detect users who are planning to harm others, we route their conversations to specialized pipelines where they are reviewed by a small team trained on our usage policies and who are authorized to take action, including banning accounts," the blog post notes. "If human reviewers determine that a case involves an imminent threat of serious physical harm to others, we may refer it to law enforcement."&lt;/p&gt;
    &lt;p&gt;That short and vague statement leaves a lot to be desired ‚Äî and OpenAI's usage policies, referenced as the basis on which the human review team operates, don't provide much more clarity.&lt;/p&gt;
    &lt;p&gt;When describing its rule against "harm [to] yourself or others," the company listed off some pretty standard examples of prohibited activity, including using ChatGPT "to promote suicide or self-harm, develop or use weapons, injure others or destroy property, or engage in unauthorized activities that violate the security of any service or system."&lt;/p&gt;
    &lt;p&gt;But in the post warning users that the company will call the authorities if they seem like they're going to hurt someone, OpenAI also acknowledged that it is "currently not referring self-harm cases to law enforcement to respect people‚Äôs privacy given the uniquely private nature of ChatGPT interactions."&lt;/p&gt;
    &lt;p&gt;While ChatGPT has in the past proven itself pretty susceptible to so-called jailbreaks that trick it into spitting out instructions to build neurotoxins or step-by-step instructions to kill yourself, this new rule adds an additional layer of confusion. It remains unclear which exact types of chats could result in user conversations being flagged for human review, much less getting referred to police. We've reached out to OpenAI to ask for clarity.&lt;/p&gt;
    &lt;p&gt;While it's certainly a relief that AI conversations won't result in police wellness checks ‚Äî which often end up causing more harm to the person in crisis due to most cops' complete lack of training in handling mental health situations ‚Äî it's also kind of bizarre that OpenAI even mentions privacy, given that it admitted in the same post that it's monitoring user chats and potentially sharing them with the fuzz.&lt;/p&gt;
    &lt;p&gt;To make the announcement all the weirder, this new rule seems to contradict the company's pro-privacy stance amid its ongoing lawsuit with the New York Times and other publishers as they seek access to troves of ChatGPT logs to determine whether any of their copyrighted data had been used to train its models.&lt;/p&gt;
    &lt;p&gt;OpenAI has steadfastly rejected the publishers' request on grounds of protecting user privacy and has, more recently, begun trying to limit the amount of user chats it has to give the plaintiffs.&lt;/p&gt;
    &lt;p&gt;Last month, the company's CEO Sam Altman admitted during an appearance on a podcast that using ChatGPT as a therapist or attorney doesn't confer the same confidentiality that talking to a flesh-and-blood professional would ‚Äî and that thanks to the NYT lawsuit, the company may be forced to turn those chats over to courts.&lt;/p&gt;
    &lt;p&gt;In other words, OpenAI is stuck between a rock and a hard place. The PR blowback from its users spiraling into mental health crises and dying by suicide is appalling ‚Äî but since it's clearly having trouble controlling its own tech enough to protect users from those harmful scenarios, it's falling back on heavy-handed moderation that flies in the face of its own CEO's promises.&lt;/p&gt;
    &lt;p&gt;More on the dark side of ChatGPT: After Their Son's Suicide, His Parents Were Horrified to Find His Conversations With ChatGPT&lt;/p&gt;
    &lt;p&gt;Share This Article&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45105081</guid></item><item><title>AI web crawlers are destroying websites in their never-ending content hunger</title><link>https://www.theregister.com/2025/08/29/ai_web_crawlers_are_destroying/</link><description>&lt;doc fingerprint="1552b1596b220fca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI web crawlers are destroying websites in their never-ending hunger for any and all content&lt;/head&gt;
    &lt;head rend="h2"&gt;But the cure may ruin the web....&lt;/head&gt;
    &lt;p&gt;Opinion With AI's rise, AI web crawlers are strip-mining the web in their perpetual hunt for ever more content to feed into their Large Language Model (LLM) mills. How much traffic do they account for? According to Cloudflare, a major content delivery network (CDN) force, 30% of global web traffic now comes from bots. Leading the way and growing fast? AI bots.&lt;/p&gt;
    &lt;p&gt;Cloud services company Fastly agrees. It reports that 80% of all AI bot traffic comes from AI data fetcher bots. So, you ask, "What's the problem? Haven't web crawlers been around since 1993 with the arrival of the World Wide Web Wanderer in 1993?" Well, yes, they have. Anyone who runs a website, though, knows there's a huge, honking difference between the old-style crawlers and today's AI crawlers. The new ones are site killers.&lt;/p&gt;
    &lt;p&gt;Fastly warns that they're causing "performance degradation, service disruption, and increased operational costs." Why? Because they're hammering websites with traffic spikes that can reach up to ten or even twenty times normal levels within minutes.&lt;/p&gt;
    &lt;p&gt;Moreover, AI crawlers are much more aggressive than standard crawlers. As the InMotionhosting web hosting company notes, they also tend to disregard crawl delays or bandwidth-saving guidelines and extract full page text, and sometimes attempt to follow dynamic links or scripts.&lt;/p&gt;
    &lt;p&gt;The result? If you're using a shared server for your website, as many small businesses do, even if your site isn't being shaken down for content, other sites on the same hardware with the same Internet pipe may be getting hit. This means your site's performance drops through the floor even if an AI crawler isn't raiding your website.&lt;/p&gt;
    &lt;p&gt;Smaller sites, like my own Practical Tech, get slammed to the point where they're simply knocked out of service. Thanks to Cloudflare Distributed Denial of Service (DDoS) protection, my microsite can shrug off DDoS attacks. AI bot attacks ‚Äì and let's face it, they are attacks ‚Äì not so much.&lt;/p&gt;
    &lt;p&gt;Even large websites are feeling the crush. To handle the load, they must increase their processor, memory, and network resources. If they don't? Well, according to most web hosting companies, if a website takes longer than three seconds to load, more than half of visitors will abandon the site. Bounce rates jump up for every second beyond that threshold.&lt;/p&gt;
    &lt;p&gt;So when AI searchbots, with Meta (52% of AI searchbot traffic), Google (23%), and OpenAI (20%) leading the way, clobber websites with as much as 30 Terabits in a single surge, they're damaging even the largest companies' site performance.&lt;/p&gt;
    &lt;p&gt;Now, if that were traffic that I could monetize, it would be one thing. It's not. It used to be when search indexing crawler, Googlebot, came calling, I could always hope that some story on my site would land on the magical first page of someone's search results so they'd visit me, they'd read the story, and two or three times out of a hundred visits, they'd click on an ad, and I'd get a few pennies of income. Or, if I had a business site, I might sell a widget or get someone to do business with me.&lt;/p&gt;
    &lt;p&gt;AI searchbots? Not so much. AI crawlers don't direct users back to the original sources. They kick our sites around, return nothing, and we're left trying to decide how we're to make a living in the AI-driven web world.&lt;/p&gt;
    &lt;p&gt;Yes, of course, we can try to fend them off with logins, paywalls, CAPTCHA challenges, and sophisticated anti-bot technologies. You know one thing AI is good at? It's getting around those walls.&lt;/p&gt;
    &lt;p&gt;As for robots.txt files, the old-school way of blocking crawlers? Many ‚Äì most? ‚Äì AI crawlers simply ignore them.&lt;/p&gt;
    &lt;p&gt;For example, Perplexity has been accused by Cloudflare of ignoring robots.txt files. Perplexity, in turn, hotly denies this accusation. Me? All I know is I see regular waves of multiple companies' AI bots raiding my site.&lt;/p&gt;
    &lt;p&gt;There are efforts afoot to supplement robots.txt with llms.txt files. This is a proposed standard to provide LLM-friendly content that LLMs can access without compromising the site's performance. Not everyone is thrilled with this approach, though, and it may yet come to nothing.&lt;/p&gt;
    &lt;p&gt;In the meantime, to combat excessive crawling, some infrastructure providers, such as Cloudflare, now offer default bot-blocking services to block AI crawlers and provide mechanisms to deter AI companies from accessing their data. Other programs, such as the popular open-source and free Anubis AI crawler blocker, just attempt to slow down their visits to a, if you'll pardon the expression, a crawl.&lt;/p&gt;
    &lt;p&gt;In the arms race between all businesses and their websites and AI companies, eventually, they'll reach some kind of neutrality. Unfortunately, the web will be more fragmented than ever. Sites will further restrict or monetize access. Important, accurate information will end up siloed behind walls or removed altogether.&lt;/p&gt;
    &lt;p&gt;Remember the open web? I do. I can see our kids on the Internet, where you must pay cash money to access almost anything. I don't think anyone wants a Balkanized Internet, but I fear that's exactly where we're going.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45105230</guid></item><item><title>'World Models,' an Old Idea in AI, Mount a Comeback</title><link>https://www.quantamagazine.org/world-models-an-old-idea-in-ai-mount-a-comeback-20250902/</link><description>&lt;doc fingerprint="3c198b9fef1780ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;‚ÄòWorld Models,‚Äô an Old Idea in AI, Mount a Comeback&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The latest ambition of artificial intelligence research ‚Äî particularly within the labs seeking ‚Äúartificial general intelligence,‚Äù or AGI ‚Äî is something called a world model: a representation of the environment that an AI carries around inside itself like a computational snow globe. The AI system can use this simplified representation to evaluate predictions and decisions before applying them to its real-world tasks. The deep learning luminaries Yann LeCun (of Meta), Demis Hassabis (of Google DeepMind) and Yoshua Bengio (of Mila, the Quebec Artificial Intelligence Institute) all believe world models are essential for building AI systems that are truly smart, scientific and safe.&lt;/p&gt;
    &lt;p&gt;The fields of psychology, robotics and machine learning have each been using some version of the concept for decades. You likely have a world model running inside your skull right now ‚Äî it‚Äôs how you know not to step in front of a moving train without needing to run the experiment first.&lt;/p&gt;
    &lt;p&gt;So does this mean that AI researchers have finally found a core concept whose meaning everyone can agree upon? As a famous physicist once wrote: Surely you‚Äôre joking. A world model may sound straightforward ‚Äî but as usual, no one can agree on the details. What gets represented in the model, and to what level of fidelity? Is it innate or learned, or some combination of both? And how do you detect that it‚Äôs even there at all?&lt;/p&gt;
    &lt;p&gt;It helps to know where the whole idea started. In 1943, a dozen years before the term ‚Äúartificial intelligence‚Äù was coined, a 29-year-old Scottish psychologist named Kenneth Craik published an influential monograph in which he mused that ‚Äúif the organism carries a ‚Äòsmall-scale model‚Äô of external reality ‚Ä¶ within its head, it is able to try out various alternatives, conclude which is the best of them ‚Ä¶ and in every way to react in a much fuller, safer, and more competent manner.‚Äù Craik‚Äôs notion of a mental model or simulation presaged the ‚Äúcognitive revolution‚Äù that transformed psychology in the 1950s and still rules the cognitive sciences today. What‚Äôs more, it directly linked cognition with computation: Craik considered the ‚Äúpower to parallel or model external events‚Äù to be ‚Äúthe fundamental feature‚Äù of both ‚Äúneural machinery‚Äù and ‚Äúcalculating machines.‚Äù&lt;/p&gt;
    &lt;p&gt;The nascent field of artificial intelligence eagerly adopted the world-modeling approach. In the late 1960s, an AI system called SHRDLU wowed observers by using a rudimentary ‚Äúblock world‚Äù to answer commonsense questions about tabletop objects, like ‚ÄúCan a pyramid support a block?‚Äù But these handcrafted models couldn‚Äôt scale up to handle the complexity of more realistic settings. By the late 1980s, the AI and robotics pioneer Rodney Brooks had given up on world models completely, famously asserting that ‚Äúthe world is its own best model‚Äù and ‚Äúexplicit representations ‚Ä¶ simply get in the way.‚Äù&lt;/p&gt;
    &lt;p&gt;It took the rise of machine learning, especially deep learning based on artificial neural networks, to breathe life back into Craik‚Äôs brainchild. Instead of relying on brittle hand-coded rules, deep neural networks could build up internal approximations of their training environments through trial and error and then use them to accomplish narrowly specified tasks, such as driving a virtual race car. In the past few years, as the large language models behind chatbots like ChatGPT began to demonstrate emergent capabilities that they weren‚Äôt explicitly trained for ‚Äî like inferring movie titles from strings of emojis, or playing the board game Othello ‚Äî world models provided a convenient explanation for the mystery. To prominent AI experts such as Geoffrey Hinton, Ilya Sutskever and Chris Olah, it was obvious: Buried somewhere deep within an LLM‚Äôs thicket of virtual neurons must lie ‚Äúa small-scale model of external reality,‚Äù just as Craik imagined.&lt;/p&gt;
    &lt;p&gt;The truth, at least so far as we know, is less impressive. Instead of world models, today‚Äôs generative AIs appear to learn ‚Äúbags of heuristics‚Äù: scores of disconnected rules of thumb that can approximate responses to specific scenarios, but don‚Äôt cohere into a consistent whole. (Some may actually contradict each other.) It‚Äôs a lot like the parable of the blind men and the elephant, where each man only touches one part of the animal at a time and fails to apprehend its full form. One man feels the trunk and assumes the entire elephant is snakelike; another touches a leg and guesses it‚Äôs more like a tree; a third grasps the elephant‚Äôs tail and says it‚Äôs a rope. When researchers attempt to recover evidence of a world model from within an LLM ‚Äî for example, a coherent computational representation of an Othello game board ‚Äî they‚Äôre looking for the whole elephant. What they find instead is a bit of snake here, a chunk of tree there, and some rope.&lt;/p&gt;
    &lt;p&gt;Of course, such heuristics are hardly worthless. LLMs can encode untold sackfuls of them within their trillions of parameters ‚Äî and as the old saw goes, quantity has a quality all its own. That‚Äôs what makes it possible to train a language model to generate nearly perfect directions between any two points in Manhattan without learning a coherent world model of the entire street network in the process, as researchers from Harvard University and the Massachusetts Institute of Technology recently discovered.&lt;/p&gt;
    &lt;p&gt;So if bits of snake, tree and rope can do the job, why bother with the elephant? In a word, robustness: When the researchers threw their Manhattan-navigating LLM a mild curveball by randomly blocking 1% of the streets, its performance cratered. If the AI had simply encoded a street map whose details were consistent ‚Äî instead of an immensely complicated, corner-by-corner patchwork of conflicting best guesses ‚Äî it could have easily rerouted around the obstructions.&lt;/p&gt;
    &lt;p&gt;Given the benefits that even simple world models can confer, it‚Äôs easy to understand why every large AI lab is desperate to develop them ‚Äî and why academic researchers are increasingly interested in scrutinizing them, too. Robust and verifiable world models could uncover, if not the El Dorado of AGI, then at least a scientifically plausible tool for extinguishing AI hallucinations, enabling reliable reasoning, and increasing the interpretability of AI systems.&lt;/p&gt;
    &lt;p&gt;That‚Äôs the ‚Äúwhat‚Äù and ‚Äúwhy‚Äù of world models. The ‚Äúhow,‚Äù though, is still anyone‚Äôs guess. Google DeepMind and OpenAI are betting that with enough ‚Äúmultimodal‚Äù training data ‚Äî like video, 3D simulations, and other input beyond mere text ‚Äî a world model will spontaneously congeal within a neural network‚Äôs statistical soup. Meta‚Äôs LeCun, meanwhile, thinks that an entirely new (and non-generative) AI architecture will provide the necessary scaffolding. In the quest to build these computational snow globes, no one has a crystal ball ‚Äî but the prize, for once, may just be worth the AGI hype.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45105710</guid></item><item><title>Phone Has Social Credit. We Just Lie About It</title><link>https://www.thenexus.media/your-phone-already-has-social-credit-we-just-lie-about-it/</link><description>&lt;doc fingerprint="b51eea140ec083e1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Your Phone Already Has Social Credit. We Just Lie About It.&lt;/head&gt;
    &lt;p&gt;Your credit score is social credit. Your LinkedIn endorsements are social credit. Your Uber passenger rating, Instagram engagement metrics, Amazon reviews, and Airbnb host status are all social credit systems that track you, score you, and reward you based on your behavior.&lt;/p&gt;
    &lt;p&gt;Social credit, in its original economic definition, means distributing industry profits to consumers to increase purchasing power. But the term has evolved far beyond economics. Today, it describes any kind of metric that tracks individual behavior, assigns scores based on that behavior, and uses those scores to determine access to services, opportunities, or social standing.&lt;/p&gt;
    &lt;p&gt;Sounds dystopian, doesn‚Äôt it? But guess what? Every time an algorithm evaluates your trustworthiness, reliability, or social value, whether for a loan, a job, a date, or a ride, you're participating in a social credit system. The scoring happens constantly, invisibly, and across dozens of platforms that weave into your daily life.&lt;/p&gt;
    &lt;p&gt;The only difference between your phone and China's social credit system is that China tells you what they're doing. We pretend our algorithmic reputation scores are just ‚Äúuser experience features.‚Äù At least Beijing admits they're gamifying human behavior.&lt;/p&gt;
    &lt;p&gt;When Americans think of the "Chinese social credit system," they likely picture Black Mirror episodes and Orwellian nightmares. Citizens are tracked for every jaywalking incident, points are deducted for buying too much alcohol, and facial recognition cameras are monitoring social gatherings; the image is so powerful that Utah's House passed a law banning social credit systems, despite none existing in America.&lt;/p&gt;
    &lt;p&gt;Here's what's actually happening. As of 2024, there's still no nationwide social credit score in China. Most private scoring systems have been shut down, and local government pilots have largely ended. It‚Äôs mainly a fragmented collection of regulatory compliance tools, mostly focused on financial behavior and business oversight. While well over 33 million businesses have been scored under corporate social credit systems, individual scoring remains limited to small pilot cities like Rongcheng. Even there, scoring systems have had "very limited impact" since they've never been elevated to provincial or national levels.&lt;/p&gt;
    &lt;p&gt;What actually gets tracked? Primarily court judgment defaults: people who refuse to pay fines or loans despite having the ability. The Supreme People's Court's blacklist is composed of citizens and companies that refuse to comply with court orders, typically to pay fines or repay loans. Some experimental programs in specific cities track broader social behavior, but these remain isolated experiments.&lt;/p&gt;
    &lt;p&gt;The gap between Western perception and Chinese reality is enormous, and it reveals something important: we're worried about a system that barely exists while ignoring the behavioral scoring systems we actually live with.&lt;/p&gt;
    &lt;p&gt;You already live in social credit.&lt;/p&gt;
    &lt;p&gt;Open your phone right now and count the apps that are scoring your behavior. Uber drivers rate you as a passenger. Instagram tracks your engagement patterns. Your bank is analyzing your Venmo transactions and Afterpay usage. LinkedIn measures your professional networking activity. Amazon evaluates your purchasing behavior. Each platform maintains detailed behavioral profiles that determine your access to services, opportunities, and social connections.&lt;/p&gt;
    &lt;p&gt;We just don't call it social credit.&lt;/p&gt;
    &lt;p&gt;Your credit score doesn't just determine loan eligibility; it affects where you can live, which jobs you can get, and how much you pay for car insurance. But traditional credit scoring is expanding rapidly. Some specialized lenders scan social media profiles as part of alternative credit assessments, particularly for borrowers with limited credit histories. Payment apps and financial services increasingly track spending patterns and transaction behaviors to build comprehensive risk profiles. The European Central Bank has asked some institutions to monitor social media chatter for early warnings of bank runs, though this is more about systemic risk than individual account decisions. Background check companies routinely analyze social media presence for character assessment. LinkedIn algorithmically manages your professional visibility based on engagement patterns, posting frequency, and network connections, rankings that recruiters increasingly rely on to filter candidates. Even dating has become a scoring system: apps use engagement rates and response patterns to determine who rises to the top of the queue and who gets buried.&lt;/p&gt;
    &lt;p&gt;What we have aren't unified social credit systems‚Ä¶yet. They're fragmented behavioral scoring networks that don't directly communicate. Your Uber rating doesn't affect your mortgage rate, and your LinkedIn engagement doesn't determine your insurance premiums. But the infrastructure is being built to connect these systems. We're building the technical and cultural foundations that could eventually create comprehensive social credit systems. The question isn't whether we have Chinese-style social credit now (because we don't). The question is whether we're building toward it without acknowledging what we're creating.&lt;/p&gt;
    &lt;p&gt;Where China's limited experiments have been explicit about scoring criteria, Western systems hide their decision-making processes entirely. Even China's fragmented approach offers more visibility into how behavioral data gets used than our black box algorithms do.&lt;/p&gt;
    &lt;p&gt;You may argue there's a fundamental difference between corporate tracking and government surveillance. Corporations compete; you can switch services. Governments have monopoly power and can restrict fundamental freedoms.&lt;/p&gt;
    &lt;p&gt;This misses three key points: First, switching costs for major platforms are enormous. Try leaving Google's ecosystem or abandoning your LinkedIn network. Second, corporate social credit systems increasingly collaborate. Bad Uber ratings can affect other services; poor credit scores impact everything from insurance to employment. Third, Western governments already access this corporate data through legal channels and data purchases.&lt;/p&gt;
    &lt;p&gt;Social credit systems are spreading globally because they solve coordination problems. They reduce fraud, encourage cooperation, and create behavioral incentives at scale. The question isn't whether Western societies will adopt social credit (because we're building toward it). The question is whether we'll be transparent and accountable about it or continue pretending our algorithmic reputation scores are just neutral technology.&lt;/p&gt;
    &lt;p&gt;Current trends suggest both systems are evolving toward more comprehensive behavioral scoring. European digital identity initiatives are linking multiple service scores. US cities are experimenting with behavioral incentive programs. Corporate platforms increasingly share reputation data. Financial services integrate social media analysis into lending decisions.&lt;/p&gt;
    &lt;p&gt;If both countries evolve toward comprehensive behavioral scoring, and current trends suggest they will, which approach better serves individual agencies? One that admits it's scoring you, or one that pretends algorithmic recommendations are just helpful suggestions?&lt;/p&gt;
    &lt;p&gt;When Uber can destroy your transportation access with a hidden algorithm, and when credit scores determine your housing options through opaque calculations, is that really more free than a system where you know at least some of the behaviors that affect your score?&lt;/p&gt;
    &lt;p&gt;So when China's explicit social credit approach inevitably influences Western platforms, when your apps start showing you the behavioral scores they've always been calculating, when the rules become visible instead of hidden, don't panic.&lt;/p&gt;
    &lt;p&gt;Because for the first time, you'll finally understand the game you've been playing all along. And knowing the rules means you can finally choose whether you want to play.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106011</guid></item><item><title>Python has had async for 10 years ‚Äì why isn't it more popular?</title><link>https://tonybaloney.github.io/posts/why-isnt-python-async-more-popular.html</link><description>&lt;doc fingerprint="363096d5e4a0b040"&gt;
  &lt;main&gt;&lt;p&gt;The Python Documentary dropped this morning. In the middle of the documentary, there‚Äôs a dramatic segment about how the transition from Python 2 to 3 divided the community (spoiler alert: it didn‚Äôt in the end).&lt;/p&gt;&lt;p&gt;The early versions of Python 3 (3.0-3.4) were mostly focused on stability and offering pathways for users moving from 2.7. Along came 3.5 in 2015 with a new feature: &lt;code&gt;async&lt;/code&gt; and &lt;code&gt;await&lt;/code&gt; keywords for executing coroutines.&lt;/p&gt;&lt;p&gt;Ten years and nine releases later, Python 3.14 is weeks away.&lt;/p&gt;&lt;p&gt;Whilst everyone will be distracted by the shiny, colorful REPL features in 3.14, there are some big announcements nestled in the release notes ‚Äî both related to concurrency and parallelism&lt;/p&gt;&lt;p&gt;Both of these features are huge advancements in how Python can be used to execute concurrent code. But if &lt;code&gt;async&lt;/code&gt; has been here for 10 years, why do we need them?&lt;/p&gt;&lt;p&gt;The killer use-case for async is web development. Coroutines lend well to out-of-process network calls, like HTTP requests and database queries. Why block the entire Python interpreter waiting for a SQL query to run on another server?&lt;/p&gt;&lt;p&gt;Yet, among the three most popular Python web frameworks, async support is still not universal. FastAPI is async from the ground-up, Django has some support, but is ‚Äústill working on async support‚Äù in key areas like the ORM (database). Then Flask is and probably always will be synchronous (Quart is an async alternative with similar APIs). The most popular ORM for Python, SQLAlchemy, only added asyncio support in 2023 (changelog).&lt;/p&gt;&lt;p&gt;I posed the question ‚ÄúWhy isn‚Äôt async more popular‚Äù to a couple of other developers to get their thoughts.&lt;/p&gt;&lt;p&gt;Christopher Trudeau, co-host of the Real Python Podcast, shared his perspective:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Certain kinds of errors get caught by the compiler, others just disappear. Why didn‚Äôt that function run? Oops, forgot to await it. Error in the coroutine? Did you remember to launch with the right params, if not, it doesn‚Äôt percolate up. I still find threads easier to wrap my head around.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Michael Kennedy offered some additional insight:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The [GIL] is so omnipresent that most Python people never developed multithreaded/async thinking. Because async/await only works for I/O bound work, not CPU as well, it‚Äôs of much less use. E.g. You can use in on the web, but most servers fork out to 4-8 web workers anyway&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;So what‚Äôs going on here and can we apply the lessons to Free-Threading and Multiple Interpreters in 3.14 so that in another ten years we‚Äôre looking back and wondering why they aren‚Äôt more popular?&lt;/p&gt;&lt;p&gt;Coroutines are most valuable with IO-related tasks. In Python, you can start hundreds of coroutines to make network requests, then wait for them all to finish without running them one at a time. The concepts behind coroutines are quite straightforward. You have a loop (the event loop) and you pass it coroutines to evaluate.&lt;/p&gt;&lt;p&gt;Let‚Äôs go back to the classic use-case, HTTP requests:&lt;/p&gt;&lt;code&gt;def get_thing_sync():
    return http_client.get('/thing/which_takes?ages=1')
&lt;/code&gt;&lt;p&gt;The equivalent async function is clean and readable:&lt;/p&gt;&lt;code&gt;async def get_thing_async():
    return await http_client.get('/thing/which_takes?ages=1')
&lt;/code&gt;&lt;p&gt;If you call function &lt;code&gt;get_thing_sync()&lt;/code&gt; versus &lt;code&gt;await get_thing_async()&lt;/code&gt;, they take the same amount of time. Calling it ‚Äú‚ú® asynchronously ‚ú®‚Äù does not somehow make it faster. The gains are when you have more than one coroutine running at once. &lt;/p&gt;&lt;p&gt;When fetching multiple HTTP resources you can start all the requests at once via the OS network stack, then handle each response as it arrives. The important point is that the actual work ‚Äî sending packets and waiting for remote servers ‚Äî happens outside your Python process while your code waits. Async is most effective here: you start operations, receive awaitable handles (tasks/futures), and the event loop efficiently notifies the coroutine when each operation completes without wasting CPU on busy‚Äëpolling.&lt;/p&gt;&lt;p&gt;This scenario works well because:&lt;/p&gt;&lt;p&gt;That‚Äôs all fine, but I started with the statement Coroutines are most valuable with IO-related tasks. I then picked the one task that asyncio can handle really well, HTTP requests.&lt;/p&gt;&lt;p&gt;What about disk IO? I have far more applications in Python which read and write from files on disks or memory than I do making HTTP requests. I also have Python programs which run other programs using &lt;code&gt;subprocess&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Can I make all of those &lt;code&gt;async&lt;/code&gt;?  &lt;/p&gt;&lt;p&gt;No, not really. From the asyncio Wiki:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;asyncio does not support asynchronous operations on the filesystem. Even if files are opened with O_NONBLOCK, read and write will block.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The solution is to use a third-party package, &lt;code&gt;aiofiles&lt;/code&gt;, which gives you async file I/O capabilities:&lt;/p&gt;&lt;code&gt;async with aiofiles.open('filename', mode='r') as f:
    contents = await f.read()
&lt;/code&gt;&lt;p&gt;So, mission accomplished? No because &lt;code&gt;aiofiles&lt;/code&gt; uses a thread pool to offload the blocking file I/O operations. &lt;/p&gt;&lt;p&gt;Windows has an async file IO API called IoRing. Linux has this availability in newer Kernels via &lt;code&gt;io_uring&lt;/code&gt;. All I could find for a Python implementation of &lt;code&gt;io_uring&lt;/code&gt; is this synchronous API written in Cython.&lt;/p&gt;&lt;p&gt;There were io_uring APIs for other platforms, Rust has implementations with tokio, C++ has Asio and Node.JS has libuv.&lt;/p&gt;&lt;p&gt;So, the asyncio Wiki is a little out of date, but&lt;/p&gt;&lt;code&gt;io_uring&lt;/code&gt;&lt;code&gt;io_uring&lt;/code&gt; has been plagued by security issues so bad that RedHat, Google and others have restricted or removed its availability. After paying out $1 million in bug bounties related to &lt;code&gt;io_uring&lt;/code&gt;, Google disabled it on some products. The issue was severe; many of the related bug‚Äëbounty reports involved io_uring exploits.&lt;p&gt;So we should hold our horses a little while longer. Operating Systems have long held a file IO API that handles threads for concurrent IO. It does the job just fine for now.&lt;/p&gt;&lt;p&gt;So in summary, Coroutines are most valuable with IO-related tasks is only really true for network I/O and network sockets in Python were never blocking operations in the first place. Socket open in Python is one of the few operations which releases the GIL and works concurrently in a thread pool as a non-blocking operation.&lt;/p&gt;&lt;table&gt;&lt;row span="3"&gt;&lt;cell role="head"&gt;Operation&lt;/cell&gt;&lt;cell role="head"&gt;Asyncio API&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Sleep&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;asyncio.sleep()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Asynchronously sleep for a given duration.&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;TCP/UDP Streams&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;asyncio.open_connection()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Open a TCP/UDP connection.&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;HTTP&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;aiohttp.ClientSession()&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Asynchronous HTTP client.&lt;/cell&gt;&lt;/row&gt;&lt;row span="3"&gt;&lt;cell&gt;Run Subprocesses&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;asyncio.subprocess&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Asynchronously run subprocesses.&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Queues&lt;/cell&gt;&lt;cell&gt;&lt;code&gt;asyncio.Queue&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Asynchronous queue implementation.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Will McGugan, the creator of Rich, Textualize, and several other extremely popular Python libraries offered his perspective on async:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;I really enjoy async programming, but it isn‚Äôt intuitive for most devs that don‚Äôt have a background writing network code. A reoccurring problem I see with Textual is folk testing concurrency by dropping in a&lt;/p&gt;&lt;code&gt;time.sleep(10)&lt;/code&gt;call to simulate the work they are planning. Of course, that blocks the entire loop. But that‚Äôs a class of issue which is difficult to explain to devs who haven‚Äôt used async much. i.e. what does it mean for code to ‚Äúblock‚Äù, and when is it necessary to defer to threads. Without that grounding in the fundamentals, your async code is going to misbehave, but its not going to break per se. So devs don‚Äôt get the rapid iteration and feedback that we expect from Python.&lt;/quote&gt;&lt;p&gt;Now that we‚Äôve covered the limited use cases for async, another challenge keeps coming up. The Python GIL.&lt;/p&gt;&lt;p&gt;I‚Äôve been working on this C#/Python bridge project called CSnakes, one of the features that caused the most head-scratching was async.&lt;/p&gt;&lt;p&gt;C#, the language from which the &lt;code&gt;async&lt;/code&gt;/&lt;code&gt;await&lt;/code&gt; syntax was borrowed, has far broader async support in its core I/O libraries because it implements a Task‚Äëbased Asynchronous Pattern (TAP), where tasks are dispatched onto a managed thread pool. Disk, network, and memory I/O operations commonly provide both async and sync methods.&lt;/p&gt;&lt;p&gt;In fact, the C# implementation goes all the way up from the disk to the higher-level APIs, such as serialization libraries. JSON deserialization is async, so is XML.&lt;/p&gt;&lt;p&gt;The C# Async model and the Python Async models have some important differences:&lt;/p&gt;&lt;p&gt;The benefit of C#‚Äôs model is that a &lt;code&gt;Task&lt;/code&gt; is a higher-level abstraction over a thread or coroutine. This means that you don‚Äôt have to worry about the underlying thread management, you can schedule several tasks to be awaited concurrently or you can run them in parallel with Task Parallel Library (TPL).&lt;/p&gt;&lt;p&gt;In Python ‚ÄúAn event loop runs in a thread (typically the main thread) and executes all callbacks and Tasks in its thread. While a Task is running in the event loop, no other Tasks can run in the same thread. When a Task executes an await expression, the running Task gets suspended, and the event loop executes the next Task.‚Äù 1&lt;/p&gt;&lt;p&gt;Going back to Will‚Äôs comment ‚ÄúOf course, that blocks the entire loop‚Äù, he‚Äôs talking about operations inside async functions which are blocking and therefore block the entire event loop. Since we covered in Problem 1, that‚Äôs practically everything except network calls and sleeping.&lt;/p&gt;&lt;p&gt;With Python‚Äôs GIL, it doesn‚Äôt matter if you‚Äôre running 1 thread or 10, the GIL will lock everything so that only 1 is operating at a time.&lt;/p&gt;&lt;p&gt;There are some operations don‚Äôt block the GIL (e.g. File IO) and in those cases you can run them in threads. For example, if you used &lt;code&gt;httpx&lt;/code&gt;‚Äòs streaming feature to stream a large network download onto disk:&lt;/p&gt;&lt;code&gt;import httpx
import tempfile

def download_file(url: str):
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        with httpx.stream("GET", url) as response:
            for chunk in response.iter_bytes():
                tmp_file.write(chunk)
    return tmp_file.name
&lt;/code&gt;

&lt;p&gt;Neither the &lt;code&gt;httpx&lt;/code&gt; stream iterator nor &lt;code&gt;tmp_file.write&lt;/code&gt; is GIL-blocking, so they benefit from running in separate threads.&lt;/p&gt;&lt;p&gt;We can merge this behavior with an asyncio API, by using the Event Loop &lt;code&gt;run_in_executor()&lt;/code&gt; function and passing it a thread pool:&lt;/p&gt;&lt;code&gt;import asyncio
import concurrent.futures

async def main():
    loop = asyncio.get_running_loop()

    URLS = [
        "https://example.place/big-file-1",
        "https://example.place/big-file-2",
        "https://example.place/big-file-3",
        # etc.
    ]

    tasks = set()
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as pool:
        for url in URLS:
            tasks.add(loop.run_in_executor(pool, download_file, url))
        files = await asyncio.gather(*tasks)
    print(files)
&lt;/code&gt;

&lt;p&gt;It‚Äôs not immediately clear to me what the benefit of this is over running a thread-pool and calling &lt;code&gt;pool.submit&lt;/code&gt;. We retain an async API, so if that is important this is an interesting workaround. &lt;/p&gt;&lt;p&gt;I find that memorizing, documenting, and explaining what is and isn‚Äôt ‚Äúblocking‚Äù in Python to be confusing and continually changing.&lt;/p&gt;&lt;p&gt;Python 3.13 introduced a very-unstable ‚Äúfree-threaded‚Äù build of Python where the GIL is removed and replaced with smaller, more granular locks. See my PyCon US 2024 Talk for a summary of parallelism. The 3.13 build wasn‚Äôt stable enough for any production use. 3.14 is looking far improved and I think we can start to introduce free-threading in 2026 in some narrow, well-tested scenarios.&lt;/p&gt;&lt;p&gt;One major benefit to coroutines versus threads is that they have a far smaller memory footprint, a lower context-switching overhead, and faster startup times. async APIs are also easier to reason about and compose.&lt;/p&gt;&lt;p&gt;Because parallelism in Python using threads has always been so limited, the APIs in the standard library are quite rudimentary. I think there is an opportunity to have a task-parallelism API in the standard library once free-threading is stabilized.&lt;/p&gt;&lt;p&gt;Last week I was implementing a registry function that did two discrete tasks. One calls a very slow sync-only API and the other calls several async APIs.&lt;/p&gt;&lt;p&gt;I want the behavior that:&lt;/p&gt;&lt;quote&gt;flowchart LR Start([Start]) --&amp;gt; Invoke["tpl.invoke()"] Invoke --&amp;gt; f1["f1()"] Invoke --&amp;gt; f2["f2()"] f1 --&amp;gt;|f1 -&amp;gt; T1| Join["Tuple[T1, T2]"] f2 --&amp;gt;|f2 -&amp;gt; T2| Join Join --&amp;gt; End([End])&lt;/quote&gt;&lt;p&gt;Since there are only two tasks, I don‚Äôt want to have to define a thread-pool or a set number of workers. I also don‚Äôt want to have to map or gather the callees. I want to retain my typing information so that the resulting variables are strongly typed from the return types of &lt;code&gt;function_a&lt;/code&gt; and &lt;code&gt;function_a&lt;/code&gt;. Essentially an API like this:&lt;/p&gt;&lt;code&gt;import tpl


def function_a() -&amp;gt; T1:
    ...

def function_b() -&amp;gt; T2:
    ...

result_a: T1, result_b: T2 = tpl.invoke(function_a, function_b)
&lt;/code&gt;

&lt;p&gt;This is all possible today but there are many constraints with the GIL. Free-threading will make parallel programming more popular in Python and we‚Äôll have to revisit some of the APIs.&lt;/p&gt;&lt;p&gt;As a package maintainer, supporting both synchronous and asynchronous APIs is a big challenge. You also have to be selective with where you support async. Much of the stdlib doesn‚Äôt support async natively (e.g. logging backends).&lt;/p&gt;&lt;p&gt;Python‚Äôs Magic (&lt;code&gt;__dunder__&lt;/code&gt;) methods cannot be async. &lt;code&gt;__init__&lt;/code&gt; cannot be async for example, so none of your code can use network requests in the initializer.&lt;/p&gt;&lt;p&gt;This is an odd-pattern but I‚Äôll keep it simple to illustrate my point. You have a class &lt;code&gt;User&lt;/code&gt; with a property &lt;code&gt;records&lt;/code&gt;. This property gives a list of records for that user. A synchronous API is straightforward:&lt;/p&gt;&lt;code&gt;class User:
    @property
    def records(self) -&amp;gt; list[RecordT]:
        # fetch records from database lazily
        ...
&lt;/code&gt;

&lt;p&gt;We can even use a lazily-initialized instance variable to cache this data.&lt;/p&gt;&lt;p&gt;Porting this API to async is a challenge because whilst &lt;code&gt;@property&lt;/code&gt; methods can be async, standard attributes are not. Having to &lt;code&gt;await&lt;/code&gt; some instance attributes and not others leaves a very odd API:&lt;/p&gt;&lt;code&gt;class AsyncDatabase:
    @staticmethod
    async def fetch_many(id: str, of: Type[RecordT]) -&amp;gt; list[RecordT]:
        ...

class User:
    @property
    async def records(self) -&amp;gt; list[RecordT]:
        # fetch records from database lazily
        return await AsyncDatabase.fetch_many(self.id, RecordT)
&lt;/code&gt;

&lt;p&gt;Anytime you access that property, it needs to be awaited:&lt;/p&gt;&lt;code&gt;user = User(...)
# single access
await user.records
# if
if await user.records:
    ...
# comprehension?
[record async for record in user.records]
&lt;/code&gt;

&lt;p&gt;The further we go into this implementation, the more we wait for the user to accidentally forget to await the property and it fails silently.&lt;/p&gt;&lt;p&gt;The Azure Python SDK, a ginormous Python project supports both sync and async. Maintaining both is achieved via a lot of code-generation infrastructure. This is ok for a project with tens of full-time engineers, but for anything small or voluntary you need to copy + paste a lot of your code base to create an async version. Then you need to patch and backport fixes and changes between the two. The differences (mostly &lt;code&gt;await&lt;/code&gt; calls) are big enough to confuse Git. I was reviewing some langchain implementations last year which had both sync and async implementation. Every method was copied+pasted, with little behavioral differences and their own bugs. People would submit bug fix PR‚Äôs to one implementation and not the other so instead of merging directly, maintainers had to port the fix, skip it, or ask the contributors to do both.&lt;/p&gt;&lt;p&gt;Since we‚Äôre largely talking about HTTP/Network IO, you also need to pick a backend for sync and async. For synchronous HTTP calls, &lt;code&gt;requests&lt;/code&gt;, &lt;code&gt;httpx&lt;/code&gt; are suitable backends. For &lt;code&gt;async&lt;/code&gt;, its &lt;code&gt;aiohttp&lt;/code&gt; and &lt;code&gt;httpx&lt;/code&gt;. Since neither are part of the Python standard library, the adoption and support for CPython‚Äôs main platforms is out of sync. E.g. as of today, &lt;code&gt;aiohttp&lt;/code&gt; has no Python 3.14 wheels, nor free-threaded support. UV Loop, the alternative implementation of the event loop has no Python 3.14 support, nor any Windows support. (Python 3.14 isn‚Äôt out yet, so it‚Äôs reasonable to not have support in either open-source project).&lt;/p&gt;&lt;p&gt;Further down the copy+paste maintainer overhead is the testing of these APIs. Testing your async code requires different mocks, different calls and in the case of Pytest a whole set of extensions and patterns for fixtures. This situation is so confusing I wrote a post about it and it‚Äôs one of the most popular on my blog.&lt;/p&gt;&lt;p&gt;In summary, I think the use cases for asyncio are limited (mostly for reasons beyond the control of &lt;code&gt;asyncio&lt;/code&gt;) and this has constrained it‚Äôs popularity. Maintaining duplicate code-bases is a burden.&lt;/p&gt;&lt;p&gt;FastAPI, the web framework that‚Äôs async from-the-ground-up grew in popularity again from 29% to 38% share of the web frameworks for Python, taking the #1 spot. It has over 100-million downloads a month. Considering the big use-case for async is HTTP and network IO, having the #1 web framework be an async one is a sign of asyncio‚Äôs success.&lt;/p&gt;&lt;p&gt;I think in 3.14 the sub-interpreter executor and free-threading features make more parallel and concurrency use cases practical and useful. For those, we don‚Äôt need &lt;code&gt;async&lt;/code&gt; APIs and it alleviates much of the issues I highlighted in this post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106189</guid></item><item><title>Introduction to Ada: a project-based exploration with rosettas</title><link>https://blog.adacore.com/introduction-to-ada-a-project-based-exploration-with-rosettas</link><description>&lt;doc fingerprint="a48888018a6d1bd8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction to Ada: a project-based exploration with rosettas&lt;/head&gt;
    &lt;head rend="h2"&gt;by Romain Gora ‚Äì&lt;/head&gt;
    &lt;head rend="h2"&gt;Context&lt;/head&gt;
    &lt;p&gt;This practical walkthrough, designed as a short tutorial, was created upon joining AdaCore as a Field Engineer. In this new role, I√¢ll be working directly with customers to help them succeed with Ada. Although I was first introduced to the language nearly two decades ago, this new position inspired me to revisit its fundamentals, and I used the excellent https://learn.adacore.com portal as a quick refresher.&lt;/p&gt;
    &lt;p&gt;While that platform takes a concept-based approach, I chose to complement it with a project-based method by developing a small, end-to-end Ada program that generates animated rosettas in the form of SVG files. These are technically hypotrochoid curves, producing patterns that many will recognize from the classic Spirograph√¢¬¢ toy.&lt;/p&gt;
    &lt;p&gt;In this walkthrough, we√¢ll show that Ada can be fun and easy to learn. Although the language is famous for safety-critical systems, we will use it as a modern, general-purpose programming language and try out some new features from Ada 2022 along the way.&lt;/p&gt;
    &lt;p&gt;Let's dive in!&lt;/p&gt;
    &lt;head rend="h2"&gt;A brief note on Ada&lt;/head&gt;
    &lt;p&gt;This section leans a bit more into background context, with a slightly encyclopedic flavor that's especially useful for readers new to Ada. If you're already familiar with Ada√¢s history and principles, feel free to joyfully skip ahead to the next section!&lt;/p&gt;
    &lt;p&gt;Ada was created in the late 1970s after a call from the U.S. Department of Defense to unify its fragmented software landscape. The winning proposal became Ada, a language that's been literally battle-tested (!) and built on a deeply thought-out design that continues to evolve today.&lt;/p&gt;
    &lt;p&gt;While Ada is absolutely a general-purpose programming language, it has carved out a strong niche in fields where software correctness and reliability are mission-critical:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Embedded and real-time systems&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Aerospace and defense&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rail, automotive, and aviation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Any system where failure is not just a bug, but a risk&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Its strict compile-time checks, safety features, and clear structure make it particularly appealing when you need your software to be dependable from day one and still maintainable ten years later.&lt;/p&gt;
    &lt;p&gt;Ada's design is grounded in a strong and principled philosophy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Readability over conciseness: Ada favors clarity. It avoids symbols and abbreviations in favor of full keywords, making the language more accessible and less error-prone.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Strong and explicit typing: It is extremely easy to declare new types in Ada, with precise constraints, which makes it much harder to accidentally misuse data. While some functional languages share this strong typing discipline, Ada stands out by requiring the programmer to be very explicit. It uses little to no type inference.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explicit is better than implicit: Unlike many modern languages that prioritize convenience, Ada leans heavily toward precision. Most types must be explicitly named and matched.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Defined semantics and minimal undefined behavior: Ada offers a level of predictability and safety unmatched in many languages. This makes it a strong choice not only for safety-critical systems, but also for codebases where long-term maintenance, verifiability, and correctness are essential.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compiler as a partner: Ada compilers are strict by design, not to frustrate, but to help the programmer write clearer, more correct code. This philosophy encourages the developer to communicate intent clearly, both to the compiler and to future readers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How the program works&lt;/head&gt;
    &lt;p&gt;Sometimes the best way to figure out how something works is to start at the end. Let's do that!&lt;lb/&gt;In this tutorial, we'll walk through how the program produces its final output √¢ a rosetta SVG file √¢ and use that as a way to explore how Ada's structure, type system, and tooling come together.&lt;lb/&gt;This is a simple command-line program that generates an SVG file. You run it like this:&lt;/p&gt;
    &lt;p&gt;./bin/rosetta&lt;/p&gt;
    &lt;p&gt;The idea was to create something visual: learning is more fun when there's an immediate, satisfying result and generating rosettas fits that goal perfectly.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Why SVG? Because it's a lightweight and portable vector format that you can view in any modern browser. I wanted to avoid relying on a graphical library, which would have added extra weight and gone beyond the scope of this approach. And while XML isn't the most pleasant format to write by hand, generating it from code is straightforward and gives a surprisingly clean result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tooling &amp;amp; setup&lt;/head&gt;
    &lt;p&gt;To build and run the project, I used Alire, the Ada package manager. It plays a similar role in the Ada ecosystem as Cargo does for Rust or npm for JavaScript. It's well-documented, and while we won't dive deep into it here, it's a solid and accessible way to manage Ada projects. I encourage anyone curious to get it from https://alire.ada.dev. Interestingly, "Alire" is also the French expression for "√É lire" √¢ which means "for reading." A fitting name for a tool that supports a language so focused on clarity and readability!&lt;/p&gt;
    &lt;p&gt;Once Alire is set up, the natural next step is choosing where to write the code. You have two excellent options for your development environment. For a dedicated experience, you can download the latest release of GNAT Studio from its GitHub repository. If you prefer a more general-purpose editor, you can install the official Ada &amp;amp; SPARK for Visual Studio Code extension from AdaCore.&lt;/p&gt;
    &lt;p&gt;As a new learner, I also kept https://learn.adacore.com close at hand. It√¢s a particularly clear and comprehensive resource √¢ and I especially appreciated being able to download the ebook version and read through it on my phone.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entry point&lt;/head&gt;
    &lt;code&gt;with Rosetta_Renderer;

procedure Main is
begin
   Rosetta_Renderer.Put_SVG_Rosettas;
end Main;&lt;/code&gt;
    &lt;p&gt;There are several interesting things to notice right away:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;with&lt;/code&gt;clause is not a preprocessor directive like in C or C++. It√¢s a compiled, checked reference to another package √¢ a reliable and explicit way to express a dependency. This eliminates entire classes of bugs related to fragile&lt;code&gt;#include&lt;/code&gt;chains, macro collisions, or dependency order issues.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This&lt;/p&gt;&lt;code&gt;procedure&lt;/code&gt;is not a&lt;code&gt;function&lt;/code&gt;: it does not return a value. In Ada, procedures are used to perform actions (like printing or modifying state), and functions are used to compute and query values.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The syntax is designed for readability. You√¢ll find&lt;/p&gt;&lt;code&gt;begin&lt;/code&gt;and&lt;code&gt;end&lt;/code&gt;here instead of&lt;code&gt;{}&lt;/code&gt;as in C/C++, reinforcing Ada√¢s philosophy that clarity matters more than brevity.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Put_SVG_Rosettas&lt;/code&gt;uses the idiomatic Pascal_Snake_Case naming style. This reflects a common Ada convention and avoids acronyms or compressed identifiers in favor of more descriptive names.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The entry point is minimal but meaningful: it simply calls a procedure which generates the output we'll explore in the next sections.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Geometry and computation (package Rosetta)&lt;/head&gt;
    &lt;p&gt;In Ada, a package is a modular unit that groups related types, procedures, and functions. Following the convention from GNAT (the Ada compiler, part of the GNU Compiler Collection, fondly known as GCC), each package has a specification file (with the&lt;code&gt; .ads&lt;/code&gt; extension √¢ short for Ada Specification) and an implementation file (with the &lt;code&gt;.adb&lt;/code&gt; extension √¢ short for Ada Body). This clear and enforced split means you always know where to find interface definitions versus their implementation.&lt;/p&gt;
    &lt;p&gt;The following code is the package specification for Rosetta. It defines the data types for the rosetta shapes and declares the public interface of operations available to manipulate them.&lt;/p&gt;
    &lt;code&gt;with Ada.Strings.Text_Buffers;

package Rosetta is

   --  A mathematical description of a rosetta (specifically, a hypotrochoid).
   --  formed by tracing a point attached to a circle rolling inside another circle.
   type Hypotrochoid is record
      Outer_Radius : Float;     --  Radius of the fixed outer circle.
      Inner_Radius : Float;     --  Radius of the rolling inner circle.
      Pen_Offset   : Float;     --  From the center of the inner circle to the drawing point.
      Steps        : Positive;  --  Number of steps (points) used to approximate the curve.
   end record;

   --  A 2D coordinate in Cartesian space.
   type Coordinate is record
      X_Coord, Y_Coord : Float;
   end record
     with Put_Image =&amp;gt; Put_Image_Coordinate;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate 
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class; 
      Value  : Coordinate);

   --  A type for an unconstrained array of 2D points forming a curve.
   --  The actual bounds are set when an array object of this type is declared.
   type Coordinate_Array is array (Natural range &amp;lt;&amp;gt;) of Coordinate;

   --  Computes the coordinates of the rosetta curve defined by Curve (a hypotrochoid).
   --  Returns a centered array of coordinates.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array;

end Rosetta;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Rosetta&lt;/code&gt; package is responsible for all the math and curve computation. It defines:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Hypotrochoid&lt;/code&gt;, type describing the geometry of the rosetta&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Coordinate&lt;/code&gt;, type representing points in 2D space&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Coordinate_Array&lt;/code&gt;, type holding a series of such points&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Compute_Points&lt;/code&gt;, function which calculates all the points of the curve based on the&lt;code&gt;Hypotrochoid&lt;/code&gt;parameters and recenters them around the origin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This package is focused solely on computation. It doesn√¢t concern itself with how the result is rendered.&lt;/p&gt;
    &lt;p&gt;Fun fact for the curious: when the rolling circle rolls outside the fixed circle rather than inside, the resulting curve is called an epitrochoid.&lt;/p&gt;
    &lt;p&gt;In Ada, a &lt;code&gt;record&lt;/code&gt; is similar to a &lt;code&gt;struct&lt;/code&gt; in C or a class with only data members in other languages. It's a user-defined type composed of named components, making it ideal for modeling structured data.&lt;/p&gt;
    &lt;p&gt;Using a record for &lt;code&gt;Hypotrochoid&lt;/code&gt; was particularly appropriate: it allows grouping all geometric parameters (outer radius, inner radius, pen offset, and steps) into a single, cohesive unit. This improves readability and maintainability. The compiler enforces correctness by ensuring all required values are present and of the expected type √¢ reinforcing Ada√¢s philosophy of clarity and safety.&lt;/p&gt;
    &lt;p&gt;The type &lt;code&gt;Coordinate_Array&lt;/code&gt; is an unconstrained array type that holds a range of &lt;code&gt;Coordinate&lt;/code&gt; records. In this context, √¢unconstrained√¢ simply means that we don√¢t define the array√¢s size when we declare the type. Instead, the size is defined when we declare an object of that type. This gives us the flexibility to use this type for a variety of shapes.&lt;/p&gt;
    &lt;p&gt;You may also notice the use of &lt;code&gt;Natural range &amp;lt;&amp;gt;. Natural&lt;/code&gt; is a predefined subtype of Integer that only allows non-negative values. And yes, I mean subtype: Ada√¢s powerful type system allows you to take an existing type and create a more specific, constrained version of it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Highlights from the .adb file&lt;/head&gt;
    &lt;p&gt;Here are a few notable aspects from the implementation (&lt;code&gt;rosetta.adb&lt;/code&gt;) that illustrate Ada√¢s strengths for writing safe, clear, and structured code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Declarative and modular design: Both&lt;/p&gt;&lt;code&gt;Generate_Point&lt;/code&gt;and&lt;code&gt;Compute_Points&lt;/code&gt;are pure functions that operate only on their inputs. Their behavior is fully deterministic and encapsulated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Safe bounds and array handling: The&lt;/p&gt;&lt;code&gt;Points&lt;/code&gt;array is statically bounded using&lt;code&gt;(0 .. Curve.Steps)&lt;/code&gt;, and its access is strictly safe. The compiler ensures that any index outside this range would raise an error at runtime. This immediate error is a feature, not a bug. It stops silent memory corruption and security flaws by ensuring the program fails predictably and safely at the source of the problem.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Use of constants for robustness: Variables such as&lt;/p&gt;&lt;code&gt;Pi&lt;/code&gt;,&lt;code&gt;R_Diff&lt;/code&gt;, and Ratio are declared as constant, enforcing immutability. This helps ensure clarity of intent and prevents accidental reassignment, a common source of subtle bugs in more permissive languages. Ada encourages this explicit declaration style, promoting safer code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;with Ada.Numerics;
with Ada.Numerics.Elementary_Functions;

use Ada.Numerics;
use Ada.Numerics.Elementary_Functions;

package body Rosetta is

   --  Computes a single point on the hypotrochoid curve for a given angle Theta.
   --  Uses the standard parametric equation of a hypotrochoid.
   function Generate_Point (Curve : Hypotrochoid; Theta : Float) return Coordinate is
      R_Diff : constant Float := Curve.Outer_Radius - Curve.Inner_Radius;
      Ratio  : constant Float := R_Diff / Curve.Inner_Radius;
   begin
      return (
              X_Coord =&amp;gt; R_Diff * Cos (Theta) + Curve.Pen_Offset * Cos (Ratio * Theta),
              Y_Coord =&amp;gt; R_Diff * Sin (Theta) - Curve.Pen_Offset * Sin (Ratio * Theta)
             );
   end Generate_Point;

   --  Computes all the points of the hypotrochoid curve and recenters them.
   --  The result is an array of coordinates centered around the origin.
   function Compute_Points (Curve : Hypotrochoid) return Coordinate_Array is
      Points : Coordinate_Array (0 .. Curve.Steps);
      Max_X  : Float := Float'First;
      Min_X  : Float := Float'Last;
      Max_Y  : Float := Float'First;
      Min_Y  : Float := Float'Last;
      Offset : Coordinate;
   begin
      --  Computes raw points and updates the bounding box extents.
      for J in 0 .. Curve.Steps loop
         declare
            Theta : constant Float := 2.0 * Pi * Float (J) / Float (Curve.Steps) * 50.0;
            P     : constant Coordinate := Generate_Point (Curve, Theta);
         begin
            Points (J) := P;
            Max_X := Float'Max (Max_X, P.X_Coord);
            Min_X := Float'Min (Min_X, P.X_Coord);
            Max_Y := Float'Max (Max_Y, P.Y_Coord);
            Min_Y := Float'Min (Min_Y, P.Y_Coord);
         end;
      end loop;

      --  Computes the center offset based on the bounding box.
      Offset := (
                 X_Coord =&amp;gt; (Max_X + Min_X) / 2.0,
                 Y_Coord =&amp;gt; (Max_Y + Min_Y) / 2.0
                );

      --  Recenters all points by subtracting the center offset.
      for J in Points'Range loop
         Points (J).X_Coord := @ - Offset.X_Coord;
         Points (J).Y_Coord := @ - Offset.Y_Coord;
      end loop;

      return Points;
   end Compute_Points;
   
   --  Redefines the 'Image attribute for Coordinate.
   procedure Put_Image_Coordinate
     (Output : in out Ada.Strings.Text_Buffers.Root_Buffer_Type'Class;
      Value  : Coordinate)
   is   
      X_Text : constant String := Float'Image (Value.X_Coord);
      Y_Text : constant String := Float'Image (Value.Y_Coord);
   begin
      Output.Put (X_Text &amp;amp; "," &amp;amp; Y_Text);
   end Put_Image_Coordinate;

end Rosetta;&lt;/code&gt;
    &lt;head rend="h2"&gt;On style: strict and predictable (and satisfying!)&lt;/head&gt;
    &lt;p&gt;Ada is one of those rare languages that not only compiles your code but asks you to write it properly. With the compiler switch -gnaty, you can enforce a comprehensive set of style rules, many of which are stricter than what you'd see in most languages.&lt;/p&gt;
    &lt;p&gt;This includes things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;No trailing whitespace at the end of lines&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No consecutive blank lines&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Proper indentation and alignment of keywords and parameters&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A space before √¢(√¢ when calling a procedure or function&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consistent casing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At first, this can feel surprisingly strict. But once you get used to it, the benefits are clear: it helps enforce a consistent and clean coding style across a codebase. That in turn improves readability, reduces ambiguity, and leads to more maintainable programs.&lt;/p&gt;
    &lt;p&gt;Rather than leaving formatting up to personal taste or optional linter tools, Ada integrates this attention to detail into the compilation process itself. The result is not only more elegant: it's genuinely satisfying. And you can do even more with GNATcheck and GNATformat but it√¢s outside of the scope of this post.&lt;/p&gt;
    &lt;p&gt;Outputting to SVG (package &lt;code&gt;Rosetta_Renderer&lt;/code&gt;)&lt;/p&gt;
    &lt;p&gt;The Rosetta_Renderer package is responsible for producing the SVG output. It defines a single high-level procedure:&lt;/p&gt;
    &lt;code&gt;package Rosetta_Renderer is

   --  Renders a predefined set of rosettas into an SVG output.
   procedure Put_SVG_Rosettas;

end Rosetta_Renderer;&lt;/code&gt;
    &lt;p&gt;This procedure generates an SVG file directly. It takes care of formatting the SVG structure (header, shapes, animations, and footer) and calls into the math logic defined in the &lt;code&gt;Rosetta &lt;/code&gt;package to generate point data.&lt;/p&gt;
    &lt;p&gt;This separation of concerns is deliberate and beneficial: the math logic doesn√¢t need to know anything about SVG, and the renderer doesn√¢t care how the coordinates were generated.&lt;/p&gt;
    &lt;p&gt;Now let's talk about the body of the package... but not for long. We're keeping it brief because its core is essentially the SVG plumbing required to draw and animate the curves, so we'll skip the fine details. And for those who enjoy seeing how the sausage is made, I've made the fully commented source code available for you right here.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;The procedure &lt;code&gt;Put_Path&lt;/code&gt; handles the creation of the SVG path. Its main job is to take an array of coordinates and write the corresponding command string to the &lt;code&gt;d &lt;/code&gt;attribute of a&lt;code&gt; &amp;lt;path&amp;gt;&lt;/code&gt; element. In SVG, this attribute defines the geometry of the shape. The code iterates over each coordinate, using &lt;code&gt;M &lt;/code&gt;(moveto) for the first point and &lt;code&gt;L&lt;/code&gt; (lineto) for all the others to draw the connecting lines.&lt;/p&gt;
    &lt;code&gt;--  Puts coordinates to a single SVG path string ("d" attribute).
   procedure Put_Path (Stream : File_Type; Points : Coordinate_Array) is
   begin
      Put (Stream, "M "); -- Moves the pen without drawing.
      for J in Points'Range loop
         declare 
            Coord_Text : constant String := Coordinate'Image (Points (J));
         begin   
            Put (Stream, Coord_Text);
            if J &amp;lt; Points'Last then
               Put (Stream, " L "); --  Draws a line.
            end if;
         end;
      end loop;
   end Put_Path;&lt;/code&gt;
    &lt;head rend="h2"&gt;Afterword&lt;/head&gt;
    &lt;p&gt;This small project was an enjoyable and useful way to get back into Ada. It helped me reconnect with the language√¢s main strengths and refamiliarize myself with its tools and design. It was a great reminder of how fun, easy to learn, and remarkably modern Ada can be, especially for developers focused on building robust, maintainable, and efficient software.&lt;/p&gt;
    &lt;p&gt;I hope this short walkthrough gives a good idea of that feeling, whether you're already into Ada or just starting to explore it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106314</guid></item><item><title>The Tariffs Are Still Illegal</title><link>https://www.bloomberg.com/opinion/newsletters/2025-09-02/the-tariffs-are-still-illegal</link><description>&lt;doc fingerprint="fbb3e810e55cb03d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Tariffs Are Still Illegal&lt;/head&gt;
    &lt;p&gt;Also reinsurance, deal-contingent hedges and a commercial paper revival.&lt;/p&gt;
    &lt;p&gt;Well, sure, right:&lt;/p&gt;
    &lt;p&gt;Here is the opinion. We have talked about this case a few times, including when it was filed and when the lower court struck down the tariffs, and it has always struck me as quite straightforward. The United States has a Constitution, and the Constitution gives Congress, not the president, the power to impose tariffs. Trump made up his own tariffs without any input from Congress, so they are unconstitutional.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45106490</guid></item></channel></rss>