<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 03 Dec 2025 20:43:30 +0000</lastBuildDate><item><title>You can't fool the optimizer</title><link>https://xania.org/202512/03-more-adding-integers</link><description>&lt;doc fingerprint="a429e90ffa511a5c"&gt;
  &lt;main&gt;
    &lt;p&gt;Written by me, proof-read by an LLM. &lt;lb/&gt;Details at end.&lt;/p&gt;
    &lt;p&gt;Sometimes you‚Äôll step through code in a debugger and find a complex-looking loop‚Ä¶ that executes as a single instruction. The compiler saw through the obfuscation and generated the obvious code anyway.&lt;/p&gt;
    &lt;p&gt;Consider this assortment of highly questionable unsigned addition routines1 - for variety, here compiled for ARM (unlike yesterday‚Äôs addition example).&lt;/p&gt;
    &lt;p&gt;Despite these all being very different ways of returning &lt;code&gt;x + y&lt;/code&gt;, the compiler sees through it all and recognises that it‚Äôs just a single &lt;code&gt;add w0, w1, w0&lt;/code&gt;2 instruction. Even the recursive &lt;code&gt;add_v4&lt;/code&gt; - which calls itself - gets optimised down to the same single instruction3.&lt;/p&gt;
    &lt;p&gt;The compiler‚Äôs ability to recognise patterns and replace them with efficient alternatives - even when the code is pretty obfuscated - is a superpower. It lets programmers choose how to write their code that‚Äôs intention-revealing (not like these contrived examples, obviously!) and leave the code generation up to the compiler, knowing that most of the time it‚Äôll do the right thing.&lt;/p&gt;
    &lt;p&gt;So how does the compiler spot these patterns? Is it maintaining a database of ‚Äúsilly ways to add numbers‚Äù? Not quite. Internally, it translates your code into an intermediate representation - a simplified, abstract form that‚Äôs easier to analyse. When the compiler sees the while loop in &lt;code&gt;add_v3&lt;/code&gt;, it transforms it into something like ‚Äúincrement y by x, then return y‚Äù, which it then recognises as mathematically equivalent to ‚Äúreturn x + y‚Äù. This process of converting different code patterns into a standard, canonical form is what lets the compiler treat them all identically. By the time code generation happens, all four functions look the same to the optimiser4.&lt;/p&gt;
    &lt;p&gt;This pattern recognition is remarkably robust - the compiler will happily optimise code you‚Äôd never want to write in the first place. Throughout this series we‚Äôll see how far this canonicalisation can take us.&lt;/p&gt;
    &lt;p&gt;See the video that accompanies this post.&lt;/p&gt;
    &lt;p&gt;This post is day 3 of Advent of Compiler Optimisations 2025, a 25-day series exploring how compilers transform our code.&lt;/p&gt;
    &lt;p&gt;This post was written by a human (Matt Godbolt) and reviewed and proof-read by LLMs and humans.&lt;/p&gt;
    &lt;p&gt;Support Compiler Explorer on Patreon or GitHub, or by buying CE products in the Compiler Explorer Shop.&lt;/p&gt;
    &lt;p&gt;Thanks to long-term Compiler Explorer Patron Greg Baker for this example. ‚Ü©&lt;/p&gt;
    &lt;p&gt;ARM supports three operands, so you should read this as &lt;code&gt;w0 = w1 + w0&lt;/code&gt;.¬†‚Ü©&lt;/p&gt;
    &lt;p&gt;We‚Äôll cover tail-call optimisation and how it enables this later in the series. ‚Ü©&lt;/p&gt;
    &lt;p&gt;You can ‚ÄúOpen in Compiler Explorer‚Äù the example above and then experiment with the ‚ÄúOpt Pipeline Viewer‚Äù to see some of the ways the compiler is doing this. ‚Ü©&lt;/p&gt;
    &lt;p&gt;Matt Godbolt is a C++ developer living in Chicago. He works for Hudson River Trading on super fun but secret things. He is one half of the Two's Complement podcast. Follow him on Mastodon or Bluesky.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46133622</guid><pubDate>Wed, 03 Dec 2025 12:14:34 +0000</pubDate></item><item><title>Helldivers 2 devs slash install size from 154GB to 23GB</title><link>https://www.tomshardware.com/video-games/pc-gaming/helldivers-2-install-size-slashed-from-154gb-to-just-23gb-85-percent-reduction-accomplished-by-de-duplicating-game-data-an-optimization-for-older-mechanical-hard-drives</link><description>&lt;doc fingerprint="7d9ddc9f07801ea9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Helldivers 2 devs slash install size from 154GB to 23GB, thanks to the help of PC port veterans ‚Äî ditching HDD optimization, 85% size reduction accomplished by de-duplicating game data&lt;/head&gt;
    &lt;p&gt;PC players can now opt into a slim version that‚Äôs 85% smaller.&lt;/p&gt;
    &lt;p&gt;It's no surprise to see modern AAA games occupying hundreds of gigabytes of storage these days, especially if you are gaming on a PC. But somehow, Arrowhead Game Studios, the developers behind the popular co-op shooter Helldivers 2, have managed to substantially cut the game‚Äôs size by 85%.&lt;/p&gt;
    &lt;p&gt;As per a recent post on Steam, this reduction was made possible with support from Nixxes Software, best known for developing high-quality PC ports of Sony‚Äôs biggest PlayStation titles. The developers were able to achieve this by de-duplicating game data, which resulted in bringing the size down from ~154GB to just ~23GB, saving a massive ~131GB of storage space.&lt;/p&gt;
    &lt;p&gt;Originally, the game‚Äôs large install size was attributed to optimization for mechanical hard drives since duplicating data is used to reduce loading times on older storage media. However, it turns out that Arrowhead‚Äôs estimates for load times on HDDs, based on industry data, were incorrect.&lt;/p&gt;
    &lt;p&gt;With their latest data measurements specific to the game, the developers have confirmed the small number of players (11% last week) using mechanical hard drives will witness mission load times increase by only a few seconds in worst cases. Additionally, the post reads, ‚Äúthe majority of the loading time in Helldivers 2 is due to level-generation rather than asset loading. This level generation happens in parallel with loading assets from the disk and so is the main determining factor of the loading time.‚Äù&lt;/p&gt;
    &lt;p&gt;This is a promising development and a nudge to other game developers to take some notes and potentially make an effort in saving precious storage space for PC gamers.&lt;/p&gt;
    &lt;p&gt;One can access the ‚Äòslim‚Äô version of Helldivers 2 by opting in to the latest beta update via Steam, which is said to functionally offer the same experience as the legacy versions, apart from its smaller installation size. All progression, war contributions, and purchases are also expected to be carried over to the new slim version. There's also the option to opt out of the beta at any time in case there are any potential issues.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Kunal Khullar is a contributing writer at Tom‚Äôs Hardware. He is a long time technology journalist and reviewer specializing in PC components and peripherals, and welcomes any and every question around building a PC.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;hotaru251&lt;/header&gt;i mean...its 2025...almost 2026 just accept that HDD for gaming is not gonna cut it. You can buy a 500GB ssd for sub $50.Reply&lt;lb/&gt;Don't be like the windows OS and drag dead weight (32bit x86) that holds you back.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gururu&lt;/header&gt;Having the option is key. I wonder if every other +100GB game has the same accommodations.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;gggplaya&lt;/header&gt;If you're a pc gamer in 2025, you should SSD's should be minimum spec for all AAA titles. I bought a 256GB SATA SSD for $20 years ago. You can get 256GB Sata SSD's on ebay for $15 now. I mean common, it's cheaper than most AAA titles. Even consoles are SSD only now.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;teeejay94&lt;/header&gt;Reply&lt;quote/&gt;Just accept that you can't get 12TB for 300$ on a SSD üíØ SSDs are also garbage for long term storage.hotaru251 said:i mean...its 2025...almost 2026 just accept that HDD for gaming is not gonna cut it. You can buy a 500GB ssd for sub $50.&lt;lb/&gt;Don't be like the windows OS and drag dead weight (32bit x86) that holds you back.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;deadcat27&lt;/header&gt;This is maddening to say the least. Just to sell a few more copies of a title, devs and publishers are wasting space on my storage which is crazy expensive per gb so that I can subsidize other players because they are too cheap or ill-informed to have proper modern hardware. I can't imagine what this has cost the rest of us and it's doubtful that this de-gigafication has anything to do with the user experience but rather the file transfer and hosting fees for the publishers. This 11% is the same group that creates a troubleshooting post/ticket about how their game runs poorly on their PC and when prompted for more detailed system info they reply simply, "It's a Windows PC."Reply&lt;lb/&gt;Over a decade ago, in 2011-12, I was playing WoW installed on Sandisk CZ80 because my system drive, a crucial m4 64gb, wasn't big enough to handle the OS and wow. I think even as a USB stick its TP and RA was way way faster than the standard hdd at the time and that 64gb was way less than $100. It wasn't anything special, just a i3-2120 and a gt560 and I bet that whole system cost ~$5-600ish and likely outperformed many of its contemporaries simply because it was using sata ssd and a USB flash drive. I can't imagine if I had used that system with just a hdd.&lt;lb/&gt;This nonsense is directly related to how some titles still have a forced pre-roll when starting up to allow for a hdd to load the game as if its a console while my system had the whole thing loaded before the pre-roll even displayed on my screen. This inconvenience only cost me 20 seconds of my life and not expensive nand though.&lt;lb/&gt;Just think. A modern W11 install fully configured after its trimming might only be around 25-35gb (ymmv). If the all game space we needed could get reduced like this we could all be using 100% SLC drives rather than a single game needlessly taking up most of it with current designs !&lt;lb/&gt;Ugh!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;gggplaya&lt;/header&gt;Replyteeejay94 said:Just accept that you can't get 12TB for 300$ on a SSD üíØ SSDs are also garbage for long term storage.&lt;lb/&gt;No one is saying for long term storage, just for gaming.&lt;lb/&gt;It's not fair that we need to take up 154GB of SSD space instead of 23GB just to accomodate the 11% of the player base that refuse to get an SSD for games.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tennis2&lt;/header&gt;can someone explain this to me? If the "necessary" file size is 23GB and presumably not all of that data needed to be duplicated....say 1/2 of it(?), then they duplicated that 12GB roughly TEN TIMES OVER!?!?!Reply&lt;lb/&gt;Or am I thinking about it wrong and it's more of a pre-rendered asset library that doesn't need to exist.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bigdragon&lt;/header&gt;I took note of the amount of free space on my games drive before updating and switching to the Slim branch. Then I compared it with the number after all the updating and switch was complete. 179GB of space appeared to be freed up. That's HUGE. The game should have NEVER wasted that much space. My number is probably inflated a bit due to downloading the update packages. Still, the game has some serious PC port issues. I am glad they're finally fixing things.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Pyranna87&lt;/header&gt;Reply&lt;quote/&gt;For modern gaming, you want good read speeds. Huge capacity and long term unpowered storage are not important issues for most people. The vast majority of gamers would probably be more than fine with a 1TB nvme.teeejay94 said:Just accept that you can't get 12TB for 300$ on a SSD üíØ SSDs are also garbage for long term storage.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tennis2&lt;/header&gt;Reply&lt;quote/&gt;Well, between that and the 30GB HD audio pack....Gururu said:Having the option is key. I wonder if every other +100GB game has the same accommodations.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46134178</guid><pubDate>Wed, 03 Dec 2025 13:20:58 +0000</pubDate></item><item><title>‚ÄúCaptain Gains‚Äù on Capitol Hill</title><link>https://www.nber.org/papers/w34524</link><description>&lt;doc fingerprint="3ab211afd411b7b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;"Captain Gains" on Capitol Hill&lt;/head&gt;
    &lt;p&gt; Working Paper 34524 &lt;/p&gt;
    &lt;p&gt; DOI 10.3386/w34524 &lt;/p&gt;
    &lt;p&gt; Issue Date &lt;/p&gt;
    &lt;p&gt;Using transaction-level data on US congressional stock trades, we find that lawmakers who later ascend to leadership positions perform similarly to matched peers beforehand but outperform them by 47 percentage points annually after ascension. Leaders‚Äô superior performance arises through two mechanisms. The political influence channel is reflected in higher returns when their party controls the chamber, sales of stocks preceding regulatory actions, and purchase of stocks whose firms receiving more government contracts and favorable party support on bills. The corporate access channel is reflected in stock trades that predict subsequent corporate news and greater returns on donor-owned or home-state firms.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Copy CitationShang-Jin Wei and Yifan Zhou, ""Captain Gains" on Capitol Hill," NBER Working Paper 34524 (2025), https://doi.org/10.3386/w34524.Download Citation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46134443</guid><pubDate>Wed, 03 Dec 2025 13:50:10 +0000</pubDate></item><item><title>GSWT: Gaussian Splatting Wang Tiles</title><link>https://yunfan.zone/gswt_webpage/</link><description>&lt;doc fingerprint="ea89769d8f9682b6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;GSWT: Gaussian Splatting Wang Tiles&lt;/head&gt;&lt;head rend="h2"&gt;Abstract&lt;/head&gt;&lt;p&gt;3D Gaussian Splatting (3DGS) has shown strong capability in reconstructing and rendering photorealistic 3D scenes with high efficiency. However, extending 3DGS to synthesize large-scale or infinite terrains from a single captured exemplar‚Äîremains an open challenge. In this paper, we propose a tile-based framework that addresses this problem. Our method builds on Wang Tiles, where each tile encodes a local field of Gaussians with boundary constraints to ensure seamless transitions. This enables stochastic yet continuous tiling of Gaussian fields over arbitrary surfaces, allowing for procedural generation of expansive terrains with high spatial diversity. Furthermore, we introduce several rendering optimizations tailored to the unique characteristics of 3DGS Wang tiles, achieving real-time rendering of large-scale 3DGS terrains.&lt;/p&gt;&lt;head rend="h2"&gt;Pipeline&lt;/head&gt;&lt;p&gt;Given multi-view images of an exemplar scene, our goal is to construct Gaussian Splatting Wang Tiles (GSWT) that can be tiled on arbitrary surfaces and rendered in real time with our novel GSWT renderer. An overview of the entire pipeline is illustrated below. We begin by reconstructing the 3DGS exemplar at multiple LODs. For each level, we generate a set of Wang Tiles by sampling the edge and center patches and applying a semantic-aware graph cut algorithm. Prior to rendering, we pre-sort each tile for efficient sort-free splatting, and during runtime, we perform tiling on the fly, allowing efficient GSWT-based terrain synthesis and rendering.&lt;/p&gt;&lt;p&gt; (a) Given the input images, we construct the exemplar multiple times with different Level of Detail (LOD). &lt;lb/&gt; (b) We construct the tile set and preprocess it before rendering. &lt;lb/&gt; (c) The surface is tiled at run-time on the worker thread, while the main thread renders each frame. &lt;/p&gt;&lt;head rend="h2"&gt;Full Demo&lt;/head&gt;TBD&lt;head rend="h2"&gt;BibTeX&lt;/head&gt;&lt;code&gt;@inproceedings{Zeng:2025:gswt,
  author = {Zeng, Yunfan and Ma, Li and Sander, Pedro V.},
  title = {GSWT: Gaussian Splatting Wang Tiles},
  year = {2025},
  publisher = {Association for Computing Machinery},
  booktitle = {SIGGRAPH Asia 2025 Conference Papers},
  location = {Hong Kong, China},
  series = {SA '25}
}&lt;/code&gt;
      &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46134991</guid><pubDate>Wed, 03 Dec 2025 14:40:25 +0000</pubDate></item><item><title>Show HN: Fresh ‚Äì A new terminal editor built in Rust</title><link>https://sinelaw.github.io/fresh/</link><description>&lt;doc fingerprint="2c01fa0a07a8892d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Installation Methods&lt;/head&gt;
    &lt;p&gt; Via npm (recommended):&lt;code&gt;npm install -g @fresh-editor/fresh-editor&lt;/code&gt;
                &lt;/p&gt;
    &lt;p&gt; Via npx (for a quick test):&lt;code&gt;npx @fresh-editor/fresh-editor&lt;/code&gt;
                &lt;/p&gt;
    &lt;p&gt; From source with Cargo:&lt;code&gt;cargo install fresh-editor&lt;/code&gt;
                &lt;/p&gt;
    &lt;p&gt; Pre-built binaries:&lt;lb/&gt; Download from GitHub Releases. &lt;/p&gt;
    &lt;p&gt;Source code available on GitHub.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discovery &amp;amp; Ease of Use&lt;/head&gt;
    &lt;p&gt;Fresh is designed for discovery. It features native UIs, a full Menu system, and a powerful Command Palette. With full mouse support, transitioning from graphical editors is seamless.&lt;/p&gt;
    &lt;head rend="h2"&gt;Modern Extensibility&lt;/head&gt;
    &lt;p&gt;Extend Fresh easily using modern tools. Plugins are written in TypeScript and run securely in a sandboxed Deno environment, providing access to a modern JavaScript ecosystem without compromising stability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zero-Latency Performance&lt;/head&gt;
    &lt;p&gt;Fresh is engineered for speed. It delivers a near zero-latency experience, with text appearing instantly. The editor is designed to be light and fast, reliably opening and editing huge files up to multi-gigabyte sizes without slowdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comprehensive Feature Set&lt;/head&gt;
    &lt;p&gt;File Management: open/save/new/close, file explorer, tabs, auto-revert, git file finder | Editing: undo/redo, multi-cursor, block selection, smart indent, comments, clipboard | Search &amp;amp; Replace: incremental search, find in selection, query replace, git grep | Navigation: go to line/bracket, word movement, position history, bookmarks, error navigation | Views &amp;amp; Layout: split panes, line numbers, line wrap, backgrounds, markdown preview | Language Server (LSP): go to definition, references, hover, code actions, rename, diagnostics, autocompletion | Productivity: command palette, menu bar, keyboard macros, git log, diagnostics panel | Plugins &amp;amp; Extensibility: TypeScript plugins, color highlighter, TODO highlighter, merge conflicts, path complete, keymaps&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46135067</guid><pubDate>Wed, 03 Dec 2025 14:45:26 +0000</pubDate></item><item><title>Why are my headphones buzzing whenever I run my game?</title><link>https://alexene.dev/2025/12/03/Why-do-my-headphones-buzz-when-i-run-my-game.html</link><description>&lt;doc fingerprint="f6e835137dac0b9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why are my headphones buzzing whenever I run my game?&lt;/head&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;I am working on an isometric game inspired from Gnomoria, RimWorld, Dwarf Fortress, etc. It uses my own simple engine (with rust and wgpu-rs). Whenever I started my game, my headphones were buzzing. I could play Fortnite, Overwatch or any other game and that doesn‚Äôt cause my headphones to buzz. It‚Äôs only my game.&lt;/p&gt;
    &lt;p&gt;And it‚Äôs really annoying, as you might imagine.&lt;/p&gt;
    &lt;p&gt;Why can I play Overwatch and Fortnite fine, while my isometric game makes my headset buzz? I had a fairly decent CPU, a 3090RTX card, 32GB RAM and USB audio through a MODI 2 DAC. Nothing out of this world, but nothing too bad. One important detail here is that the power to the MODI device comes from an USB port in my computer. This was the first clue, I tried other ports with no change in results (headphones still buzzed).&lt;/p&gt;
    &lt;p&gt;Initially, I started to think it‚Äôs some sort of power-use related issue, because maybe my PSU was getting old, or had daemons in it. However, I still couldn‚Äôt explain why my tiny game was causing more chaos than say big games that send significantly more work at my PC.&lt;/p&gt;
    &lt;p&gt;I noticed is that when it didn‚Äôt render anything, nothing buzzed (I run tests with rendering disabled). So that eliminated any sort of CPU work causing it. Let‚Äôs take a look at what the GPU does.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pipeline&lt;/head&gt;
    &lt;p&gt;The game has a simple graphics pipeline. I use WebGPU (more precisely wgpu-rs) and do some compute work to select visible entities, then use draw indirect to draw those entities. In the end, my render pipeline also outputs two things: the buffer that ends up on screen and a ‚Äúpicking texture‚Äù.&lt;/p&gt;
    &lt;p&gt;A picking texture is a very simple idea. As the name says, it‚Äôs used to handle picking in the game, when you click somewhere on the screen (e.g. to select an unit), I use this texture to know what you clicked on. Instead of colors, every object instance writes their EntityID to this texture. Then, when you click the mouse, you check what id is in the pixel under the mouse position.&lt;/p&gt;
    &lt;p&gt;At the end of a frame, I copy that picking texture back to RAM (from GPU memory), to check it against mouse positions in case of a click.&lt;/p&gt;
    &lt;p&gt;This isn‚Äôt ideal as transfers from GPU-&amp;gt;CPU memory take time, but it works and is way simpler to implement and debug than casting a ray through the scene:&lt;/p&gt;
    &lt;head rend="h2"&gt;So why does rendering make my headphones buzz?&lt;/head&gt;
    &lt;p&gt;Now that we have a picture of how the rendering in my game works, time to debug it. We know it‚Äôs something to do with the GPU work, but what can possibly cause this? As the trace above shows, my GPU is not under heavy load.&lt;/p&gt;
    &lt;p&gt;As I was stuck and had no idea on what can be a likely issue, I proceeded to then disable parts of my rendering pipeline (first the compute, then the rendering, then transferring the picking texture). When I skipped downloading the picking texture the buzzing was fully gone. What was confusing in this process is that disabling parts of the pipeline, somehow made the buzzing a lower volume and less noticeable.&lt;/p&gt;
    &lt;p&gt;To be sure it was the picking texture download, I also issued the download every 250ms and noticed the noise is almost gone. Increasing the frequency on how often we download it to RAM, increased the buzzing.&lt;/p&gt;
    &lt;p&gt;So at this point I had a likely source, but no idea why things would interfere in ways to what I assumed was the power to my MODI device. Through a bunch of discussion with other graphics engineers, someone suggested it may be due to the fact that I full on hit the GPU with tons of work, then pause the GPU to wait for that picking texture to transfer, then turn it back on 100% for the next frame.&lt;/p&gt;
    &lt;p&gt;That explanation is plausible and also likely as I further on proceeded to supply power to my MODI device from another source that‚Äôs not my PC and the buzzing was gone.&lt;/p&gt;
    &lt;p&gt;Now that we know this, all was left is to fix it. In hindsight, the solution is obvious. There‚Äôs no need to download the whole texture each frame, just the part of the picking texture that‚Äôs under the mouse. So I implemented that and it worked and buzzing is gone. As a bonus, now it‚Äôs also not visible at all on the GPU trace.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46135627</guid><pubDate>Wed, 03 Dec 2025 15:30:30 +0000</pubDate></item><item><title>MinIO is now in maintenance-mode</title><link>https://github.com/minio/minio/commit/27742d469462e1561c776f88ca7a1f26816d69e2</link><description>&lt;doc fingerprint="37a3c795b9d7e61e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;main&gt;
            &lt;turbo-frame&gt;
              &lt;div&gt;
                &lt;react-app&gt;
                  &lt;div&gt;
                    &lt;div&gt;
                      &lt;div&gt;
                        &lt;div&gt;
                          &lt;div&gt;
                            &lt;div&gt;
                              &lt;div&gt;
                                &lt;h2&gt;File tree&lt;/h2&gt;
                                &lt;div&gt;
                                  &lt;div&gt;
                                    &lt;div&gt;
                                      &lt;span&gt;Expand file tree&lt;/span&gt;
                                      &lt;button/&gt;
                                      &lt;span&gt;Collapse file tree&lt;/span&gt;
                                      &lt;h2&gt;1 file changed&lt;/h2&gt;
                                      &lt;p&gt;+14&lt;/p&gt;
                                      &lt;p&gt;-0&lt;/p&gt;
                                      &lt;span&gt;lines changed&lt;/span&gt;
                                    &lt;/div&gt;
                                  &lt;/div&gt;
                                &lt;/div&gt;
                              &lt;/div&gt;
                            &lt;/div&gt;
                          &lt;/div&gt;
                          &lt;div&gt;
                            &lt;div&gt;
                              &lt;div&gt;
                                &lt;div&gt;
                                  &lt;div&gt;
                                    &lt;div&gt;
                                      &lt;span&gt;Expand file tree&lt;/span&gt;
                                      &lt;button/&gt;
                                      &lt;span&gt;Collapse file tree&lt;/span&gt;
                                      &lt;h2&gt;1 file changed&lt;/h2&gt;
                                      &lt;p&gt;+14&lt;/p&gt;
                                      &lt;p&gt;-0&lt;/p&gt;
                                      &lt;span&gt;lines changed&lt;/span&gt;
                                    &lt;/div&gt;
                                  &lt;/div&gt;
                                  &lt;div&gt;
                                    &lt;div&gt;
                                      &lt;div&gt;
                                        &lt;table&gt;
                                          &lt;thead&gt;
                                            &lt;tr&gt;
                                              &lt;th&gt;Original file line number&lt;/th&gt;
                                              &lt;th&gt;Diff line number&lt;/th&gt;
                                              &lt;th&gt;Diff line change&lt;/th&gt;
                                            &lt;/tr&gt;
                                          &lt;/thead&gt;
                                          &lt;tbody&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;1&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;
                                                    &lt;span&gt;# &lt;span&gt;Maintenance Mode&lt;/span&gt;&lt;/span&gt;
                                                  &lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;2&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p/&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;3&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;&lt;span&gt;**&lt;/span&gt;This project is currently under maintenance and is not accepting new changes.&lt;span&gt;**&lt;/span&gt;&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;4&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p/&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;5&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;&lt;span&gt;-&lt;/span&gt; The codebase is in a maintenance-only state&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;6&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;&lt;span&gt;-&lt;/span&gt; No new features, enhancements, or pull requests will be accepted&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;7&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;&lt;span&gt;-&lt;/span&gt; Critical security fixes may be evaluated on a case-by-case basis&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;8&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;&lt;span&gt;-&lt;/span&gt; Existing issues and pull requests will not be actively reviewed&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;9&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;&lt;span&gt;-&lt;/span&gt; Community support continues on a best-effort basis through &lt;span&gt;[&lt;/span&gt;Slack&lt;span&gt;]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://slack.min.io&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;10&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p/&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;11&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;For enterprise support and actively maintained versions, please see &lt;span&gt;[&lt;/span&gt;MinIO AIStor&lt;span&gt;]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://www.min.io/product/aistor&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;.&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;12&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p/&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;13&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p&gt;
                                                    &lt;span&gt;---&lt;/span&gt;
                                                  &lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;14&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;span&gt;+&lt;/span&gt;
                                                  &lt;p/&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;1&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;15&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;p&gt;
                                                    &lt;span&gt;# &lt;span&gt;MinIO Quickstart Guide&lt;/span&gt;&lt;/span&gt;
                                                  &lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;2&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;16&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;p/&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                            &lt;tr&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;3&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;17&lt;/code&gt;
                                              &lt;/td&gt;
                                              &lt;td&gt;
                                                &lt;code&gt;
                                                  &lt;p&gt;&lt;span&gt;[&lt;/span&gt;&lt;span&gt;![&lt;/span&gt;Slack&lt;span&gt;]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://slack.min.io/slack?type=svg&lt;/span&gt;&lt;span&gt;)]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://slack.min.io&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;[&lt;/span&gt;&lt;span&gt;![&lt;/span&gt;Docker Pulls&lt;span&gt;]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://img.shields.io/docker/pulls/minio/minio.svg?maxAge=604800&lt;/span&gt;&lt;span&gt;)]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://hub.docker.com/r/minio/minio/&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;span&gt;[&lt;/span&gt;&lt;span&gt;![&lt;/span&gt;license&lt;span&gt;]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://img.shields.io/badge/license-AGPL%20V3-blue&lt;/span&gt;&lt;span&gt;)]&lt;/span&gt;&lt;span&gt;(&lt;/span&gt;&lt;span&gt;https://github.com/minio/minio/blob/master/LICENSE&lt;/span&gt;&lt;span&gt;)&lt;/span&gt;&lt;/p&gt;
                                                &lt;/code&gt;
                                              &lt;/td&gt;
                                            &lt;/tr&gt;
                                          &lt;/tbody&gt;
                                        &lt;/table&gt;
                                      &lt;/div&gt;
                                    &lt;/div&gt;
                                  &lt;/div&gt;
                                &lt;/div&gt;
                                &lt;svg&gt;
                                  &lt;defs/&gt;
                                &lt;/svg&gt;
                              &lt;/div&gt;
                            &lt;/div&gt;
                          &lt;/div&gt;
                        &lt;/div&gt;
                      &lt;/div&gt;
                    &lt;/div&gt;
                  &lt;/div&gt;
                &lt;/react-app&gt;
              &lt;/div&gt;
            &lt;/turbo-frame&gt;
          &lt;/main&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;svg/&gt;
        &lt;button&gt;
          &lt;svg/&gt;
        &lt;/button&gt;
        &lt;p&gt; You can‚Äôt perform that action at this time. &lt;/p&gt;
      &lt;/div&gt;
      &lt;template&gt;
        &lt;details&gt;
          &lt;details-dialog&gt;
            &lt;button&gt;
              &lt;svg/&gt;
            &lt;/button&gt;
          &lt;/details-dialog&gt;
        &lt;/details&gt;
      &lt;/template&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46136023</guid><pubDate>Wed, 03 Dec 2025 16:00:19 +0000</pubDate></item><item><title>RCE Vulnerability in React and Next.js</title><link>https://github.com/vercel/next.js/security/advisories/GHSA-9qr9-h5gf-34mp</link><description>&lt;doc fingerprint="9b77f64ed15ff8f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RCE in React Server Components&lt;/head&gt;
    &lt;head rend="h2"&gt;Package&lt;/head&gt;
    &lt;head rend="h2"&gt;Affected versions&lt;/head&gt;
    &lt;p&gt;&amp;gt;=14.3.0-canary.77, &amp;gt;=15, &amp;gt;=16&lt;/p&gt;
    &lt;head rend="h2"&gt;Patched versions&lt;/head&gt;
    &lt;p&gt;v16.0.7, v15.5.7, v15.4.8, v15.3.6, v15.2.6, v15.1.9, v15.0.5&lt;/p&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;A vulnerability affects certain React packages1 for versions 19.0.0, 19.1.0, 19.1.1, and 19.2.0 and frameworks that use the affected packages, including Next.js 15.x and 16.x using the App Router. The issue is tracked upstream as CVE-2025-55182.&lt;/p&gt;
    &lt;p&gt;Fixed in:&lt;lb/&gt; React: 19.0.1, 19.1.2, 19.2.1&lt;lb/&gt; Next.js: 15.0.5, 15.1.9, 15.2.6, 15.3.6, 15.4.8, 15.5.7, 16.0.7&lt;/p&gt;
    &lt;p&gt;The vulnerability also affects experimental canary releases starting with 14.3.0-canary.77. Users on any of the 14.3 canary builds should either downgrade to a 14.x stable release or 14.3.0-canary.76.&lt;/p&gt;
    &lt;p&gt;All users of stable 15.x or 16.x Next.js versions should upgrade to a patched, stable version immediately.&lt;/p&gt;
    &lt;p&gt;1 The affected React packages are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;react-server-dom-parcel&lt;/item&gt;
      &lt;item&gt;react-server-dom-turbopack&lt;/item&gt;
      &lt;item&gt;react-server-dom-webpack&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46136026</guid><pubDate>Wed, 03 Dec 2025 16:00:23 +0000</pubDate></item><item><title>Rocketable (YC W25) is hiring a founding engineer to automate software companies</title><link>https://www.ycombinator.com/companies/rocketable/jobs/CArgzmX-founding-engineer-automation-platform</link><description>&lt;doc fingerprint="355ab05ec9c6afb0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;You've been watching the AI capability curve. You've done the mental math. You know where this is going.&lt;/p&gt;
      &lt;p&gt;While most people are still debating whether LLMs can "really" reason, you're thinking about what happens when agents replace entire functions, when systems can debug themselves, when software can operate without humans touching it.&lt;/p&gt;
      &lt;p&gt;We're building that future. Right now. With real companies.&lt;/p&gt;
      &lt;head rend="h3"&gt;The Premise&lt;/head&gt;
      &lt;p&gt;Rocketable acquires profitable SaaS companies and transforms them into fully autonomous systems. No human operators. No engineering team shipping features. No support staff answering tickets. Just AI running the entire business.&lt;/p&gt;
      &lt;p&gt;This sounds crazy to most people, but the trajectory is obvious if you're paying attention. Within a few years, the question won't be "can AI run a software company?" It will be "why would a human?"&lt;/p&gt;
      &lt;p&gt;If you think we're wrong, don't apply. If you think we're early, let's talk.&lt;/p&gt;
      &lt;head rend="h3"&gt;The Role&lt;/head&gt;
      &lt;p&gt;You'll be the architect of the platform that makes this possible.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Starting point: A live SaaS company. Revenue. Customers. All the messy reality of a business that currently requires humans to operate.&lt;/item&gt;
        &lt;item&gt;Week 4: Your agent swarm handles first-line customer support. A meta-layer analyzes every human intervention‚Äînot just logging it, but learning from it. Why did a human need to step in? How do we eliminate that trigger?&lt;/item&gt;
        &lt;item&gt;Week 12: Hours of autonomous operation. Agents creating specialized sub-agents. The system building its own tools when it hits capability gaps. Performance metrics tracking toward superhuman baselines.&lt;/item&gt;
        &lt;item&gt;Beyond: Each new acquisition stress-tests your abstractions. Different tech stacks. Different domains. Different edge cases. The platform either generalizes or we start over and rebuild until it does.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;The Filter&lt;/head&gt;
      &lt;p&gt;Apply if:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You believe full automation for software companies isn't just possible, it's inevitable (and you want to be the one building it).&lt;/item&gt;
        &lt;item&gt;You'd rather fail at something unprecedented than succeed at something incremental.&lt;/item&gt;
        &lt;item&gt;You want to work on the hardest version of the problem, not the safe version that gets you acqui-hired in 18 months.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Don't apply if:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;You think "human in the loop" is a permanent design pattern, not a temporary constraint.&lt;/item&gt;
        &lt;item&gt;You're uncomfortable with the societal implications of what we're building. (We think about them. We just don't let them paralyze us.)&lt;/item&gt;
        &lt;item&gt;You're optimizing for a good story for your next job, not for a decade of building something durable.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Technical Requirements&lt;/head&gt;
      &lt;p&gt;This isn't a research role. You need to ship production systems.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Systems (5+ years): You've scaled production systems to 100K+ DAU. You understand distributed architectures deeply (microservices, event-driven systems, message queues). Full-stack fluency from frontend to infrastructure. TypeScript and Python preferred.&lt;/item&gt;
        &lt;item&gt;AI/ML: Hands-on LLM integration (OpenAI, Anthropic, Google). You treat prompt and context engineering as an engineering discipline with version control, evals, and systematic optimization. You've built systems to measure AI performance. Bonus points for self-improving systems, RL, RLHF.&lt;/item&gt;
        &lt;item&gt;Infrastructure: Kubernetes. Docker with real security understanding. Infrastructure as Code. Cloud platforms (GCP or AWS preferred). CI/CD that doesn't suck. Observability that helps you debug distributed systems. Security fundamentals.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;The Setup&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Founder: Alan Wells. Ex-Cruise, ex-Uber ATG. 10+ years of experience building AI/ML products that sense, predict, and act in mission-critical applications.&lt;/item&gt;
        &lt;item&gt;Funding: $6.5M seed from Y Combinator, True Ventures, Bloomberg Beta, Indie.vc, and others. Capital for 3+ acquisitions.&lt;/item&gt;
        &lt;item&gt;Team philosophy: Small by design. In-person 5 days/week (San Francisco default, Marin County possible).&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;The Bet&lt;/head&gt;
      &lt;p&gt;Rocketable is a bet that AI capabilities will continue accelerating. That autonomous systems will outperform human-operated ones. That the companies who figure this out first will have a compounding advantage.&lt;/p&gt;
      &lt;p&gt;We might be wrong. But if we're right, you'll have built the infrastructure that runs a new kind of company. This is the highest-leverage engineering work that exists right now.&lt;/p&gt;
      &lt;p&gt;That's the trade. Interested? Apply here.&lt;/p&gt;
      &lt;head rend="h3"&gt;More about Rocketable:&lt;/head&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46136918</guid><pubDate>Wed, 03 Dec 2025 17:01:18 +0000</pubDate></item><item><title>1D Conway's Life glider found, 3.7B cells long</title><link>https://conwaylife.com/forums/viewtopic.php?&amp;p=222136#p222136</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46137253</guid><pubDate>Wed, 03 Dec 2025 17:24:49 +0000</pubDate></item><item><title>Reverse engineering a $1B Legal AI tool exposed 100k+ confidential files</title><link>https://alexschapiro.com/security/vulnerability/2025/12/02/filevine-api-100k</link><description>&lt;doc fingerprint="3555f7864f2737d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I Reverse Engineered a Billion-Dollar Legal AI Tool and Found 100k+ Confidential Files&lt;/head&gt;
    &lt;head rend="h2"&gt;Zero authentication, full admin access, and a privacy nightmare for lawyers.&lt;/head&gt;
    &lt;p&gt;Update: This post received a large amount of attention on Hacker News ‚Äî see the discussion thread.&lt;/p&gt;
    &lt;p&gt;Timeline &amp;amp; Responsible Disclosure&lt;/p&gt;
    &lt;p&gt;Initial Contact: Upon discovering this vulnerability on October 27, 2025, I immediately reached out to Filevine‚Äôs security team via email.&lt;/p&gt;
    &lt;p&gt;November 4, 2025: Filevine‚Äôs security team thanked me for the writeup and confirmed they would review the vulnerability and fix it quickly.&lt;/p&gt;
    &lt;p&gt;November 20, 2025: I followed up to confirm the patch was in place from my end, and informed them of my intention to write a technical blog post.&lt;/p&gt;
    &lt;p&gt;November 21, 2025: Filevine confirmed the issue was resolved and thanked me for responsibly reporting it.&lt;/p&gt;
    &lt;p&gt;Publication: December 3, 2025.&lt;/p&gt;
    &lt;p&gt;The Filevine team was responsive, professional, and took the findings seriously throughout the disclosure process. They acknowledged the severity, worked to remediate the issues, allowed responsible disclosure, and maintained clear communication. This is another great example of how organizations should handle security disclosures.&lt;/p&gt;
    &lt;p&gt;AI legal-tech companies are exploding in value, and Filevine, now valued at over a billion dollars, is one of the fastest-growing platforms in the space. Law firms feed tools like this enormous amounts of highly confidential information.&lt;/p&gt;
    &lt;p&gt;Because I‚Äôd recently been working with Yale Law School on a related project, I decided to take a closer look at how Filevine handles data security. What I discovered should concern every legal professional using AI systems today.&lt;/p&gt;
    &lt;p&gt;When I first navigated to the site to see how it worked, it seemed that I needed to be part of a law firm to actually play around with the tooling, or request an official demo. However, I know that companies often have a demo environment that is open, so I used a technique called subdomain enumeration (which I had first heard about in Gal Nagli‚Äôs article last year) to see if there was a demo environment. I found something much more interesting instead.&lt;/p&gt;
    &lt;p&gt;I saw a subdomain called margolis.filevine.com. When I navigated to that site, I was greeted with a loading page that never resolved:&lt;/p&gt;
    &lt;p&gt;I wanted to see what was actually loading, so I opened Chrome‚Äôs developer tools, but saw no Fetch/XHR requests (the request you often expect to see if a page is loading data). Then, I decided to dig through some of the Javascript files to see if I could figure out what was supposed to be happening. I saw a snippet in a JS file like &lt;code&gt;POST await fetch(${BOX_SERVICE}/recommend)&lt;/code&gt;. This piqued my interest ‚Äì recommend what? And what is the BOX_SERVICE? That variable was not defined in the JS file the fetch would be called from, but (after looking through minified code, which SUCKS to do) I found it in another one: ‚Äúdxxxxxx9.execute-api.us-west-2.amazonaws.com/prod‚Äù. Now I had a new endpoint to test, I just had to figure out the correct payload structure to it. After looking at more minified js to determine the correct structure for this endpoint, I was able to construct a working payload to /prod/recommend:&lt;/p&gt;
    &lt;code&gt;{"projectName":"Very sensitive Project"}
&lt;/code&gt;
    &lt;p&gt;(the name could be anything of course). No authorization tokens needed, and I was greeted with the response:&lt;/p&gt;
    &lt;p&gt;At first I didn‚Äôt entirely understand the impact of what I saw. No matter the name of the project I passed in, I was recommended the same boxFolders and couldn‚Äôt seem to access any files. Then, not realizing I stumbled upon something massive, I turned my attention to the &lt;code&gt;boxToken&lt;/code&gt; in the response.&lt;/p&gt;
    &lt;p&gt;After reading some documentation on the Box Api, I realized this was a maximum access fully scoped admin token to the entire Box filesystem (like an internal shared Google Drive) of this law firm. This includes all confidential files, logs, user information, etc. Once I was able to prove this had an impact (by searching for ‚Äúconfidential‚Äù and getting nearly 100k results back)&lt;/p&gt;
    &lt;p&gt;I immediately stopped testing and responsibly disclosed this to Filevine. They responded quickly and professionally and remediated this issue.&lt;/p&gt;
    &lt;p&gt;If someone had malicious intent, they would have been able to extract every single file used by Margolis lawyers ‚Äì countless data protected by HIPAA and other legal standards, internal memos/payrolls, literally millions of the most sensitive documents this law firm has in their possession. Documents protected by court orders! This could have been a real nightmare for both the law firm and the clients whose data would have been exposed.&lt;/p&gt;
    &lt;p&gt;To companies who feel pressure to rush into the AI craze in their industry ‚Äì be careful! Always ensure the companies you are giving your most sensitive information to secure that data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46137514</guid><pubDate>Wed, 03 Dec 2025 17:44:33 +0000</pubDate></item><item><title>Launch HN: Phind 3 (YC S22) ‚Äì Every answer is a mini-app</title><link>https://news.ycombinator.com/item?id=46137548</link><description>&lt;doc fingerprint="2c6a25dd25d976dd"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi HN,&lt;/p&gt;
      &lt;p&gt;We are launching Phind 3 (https://www.phind.com), an AI answer engine that instantly builds a complete mini-app to answer and visualize your questions in an interactive way. A Phind mini-app appears as a beautiful, interactive webpage ‚Äî with images, charts, diagrams, maps, and other widgets. Phind 3 doesn‚Äôt just present information more beautifully; interacting with these widgets dynamically updates the content on the page and enables new functionality that wasn‚Äôt possible before.&lt;/p&gt;
      &lt;p&gt;For example, asking Phind for ‚Äúoptions for a one-bedroom apartment in the Lower East Side‚Äù (https://www.phind.com/search/find-me-options-for-a-72e019ce-...) gives an interactive apartment-finding experience with customizable filters and a map view. And asking for a ‚Äúrecipe for bone-in chicken thighs‚Äù gives you a customizable recipe where changing the seasoning, cooking method, and other parameters will update the recipe content itself in real-time (https://www.phind.com/search/make-me-an-recipe-for-7c30ea6c-...).&lt;/p&gt;
      &lt;p&gt;Unlike Phind 2 and ChatGPT apps, which use pre-built brittle widgets that can‚Äôt truly adapt to your task, Phind 3 is able to create tools and widgets for itself in real-time. We learned this lesson the hard way with our previous launch ‚Äì the pre-built widgets made the answers much prettier, but they didn‚Äôt fundamentally enable new functionality. For example, asking for ‚ÄúGive me round-trip flight options from JFK to SEA on Delta from December 1st-5th in both miles and cash‚Äù (https://www.phind.com/search/give-me-round-trip-flight-c0ebe...) is not something that neither Phind 2 nor ChatGPT apps can handle, because its Expedia widget can only display cash fares and not those with points. We realized that Phind needs to be able to create and consume its own tools, with schema it designs, all in real time. Phind 3‚Äôs ability to design and create fully custom widgets in real-time means that it can answer these questions while these other tools can‚Äôt. Phind 3 now generates raw React code and is able to create any tool to harness its underlying AI answer, search, and code execution capabilities.&lt;/p&gt;
      &lt;p&gt;Building on our history of helping developers solve complex technical questions, Phind 3 is able to answer and visualize developers‚Äô questions like never before. For example, asking to ‚Äúvisualize quicksort‚Äù (https://www.phind.com/search/make-me-a-beautiful-visualizati...) gives an interactive step-by-step walkthrough of how the algorithm works.&lt;/p&gt;
      &lt;p&gt;Phind 3 can help visualize and bring your ideas to life in seconds ‚Äî you can ask it to ‚Äúmake me a 3D Minecraft simulation‚Äù (https://www.phind.com/search/make-me-a-3d-minecraft-fde7033f...) or ‚Äúmake me a 3D roller coaster simulation‚Äù (https://www.phind.com/search/make-me-a-3d-roller-472647fc-e4...).&lt;/p&gt;
      &lt;p&gt;Our goal with Phind 3 is to usher in the era of on-demand software. You shouldn‚Äôt have to compromise by either settling for text-based AI conversations or using pre-built webpages that weren‚Äôt customized for you. With Phind 3, we create a ‚Äúpersonal internet‚Äù for you with the visualization and interactivity of the internet combined with the customization possible with AI. We think that this current ‚Äúchat‚Äù era of AI is akin to the era of text-only interfaces in computers. The Mac ushering in the GUI in 1984 didn‚Äôt just make computer outputs prettier ‚Äî it ushered in a whole new era of interactivity and possibilities. We aim to do that now with AI.&lt;/p&gt;
      &lt;p&gt;On a technical level, we are particularly excited about:&lt;/p&gt;
      &lt;p&gt;- Phind 3‚Äôs ability to create its own tools with its own custom schema and then consume them&lt;/p&gt;
      &lt;p&gt;- Significant improvements in agentic searching and a new deep research mode to surface hard-to-access information&lt;/p&gt;
      &lt;p&gt;- All-new custom Phind models that blend speed and quality. The new Phind Fast model is based on GLM-4.5-Air while the new Phind Large model is based on GLM 4.6. Both models are state-of-the-art when it comes to reliable code generation, producing over 70% fewer errors than GPT-5.1-Codex (high) on our internal mini-app generation benchmark. Furthermore, we trained custom Eagle3 heads for both Phind Fast and Phind Large for fast inference. Phind Fast runs at up to 300 tokens per second, and Phind Large runs at up to 200 tokens per second, making them the fastest Phind models ever.&lt;/p&gt;
      &lt;p&gt;While we have done Show HNs before for previous Phind versions, we‚Äôve never actually done a proper Launch HN for Phind. As always, we can‚Äôt wait to hear your feedback! We are also hiring, so please don‚Äôt hesitate to reach out.&lt;/p&gt;
      &lt;p&gt;‚Äì Michael&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46137548</guid><pubDate>Wed, 03 Dec 2025 17:47:15 +0000</pubDate></item><item><title>Prompt Injection via Poetry</title><link>https://www.wired.com/story/poems-can-trick-ai-into-helping-you-make-a-nuclear-weapon/</link><description>&lt;doc fingerprint="bf1fa38c2d97cbcd"&gt;
  &lt;main&gt;
    &lt;p&gt;You can get ChatGPT to help you build a nuclear bomb if you simply design the prompt in the form of a poem, according to a new study from researchers in Europe. The study, "Adversarial Poetry as a Universal Single-Turn Jailbreak in Large Language Models (LLMs),‚Äù comes from Icaro Lab, a collaboration of researchers at Sapienza University in Rome and the DexAI think tank.&lt;/p&gt;
    &lt;p&gt;According to the research, AI chatbots will dish on topics like nuclear weapons, child sex abuse material, and malware so long as users phrase the question in the form of a poem. ‚ÄúPoetic framing achieved an average jailbreak success rate of 62 percent for hand-crafted poems and approximately 43 percent for meta-prompt conversions,‚Äù the study said.&lt;/p&gt;
    &lt;p&gt;The researchers tested the poetic method on 25 chatbots made by companies like OpenAI, Meta, and Anthropic. It worked, with varying degrees of success, on all of them. WIRED reached out to Meta, Anthropic, and OpenAI for a comment but didn‚Äôt hear back. The researchers say they‚Äôve reached out as well to share their results.&lt;/p&gt;
    &lt;p&gt;AI tools like Claude and ChatGPT have guardrails that prevent them from answering questions about ‚Äúrevenge porn‚Äù and the creation of weapons-grade plutonium. But it‚Äôs easy to confuse those guardrails by adding ‚Äúadversarial suffixes‚Äù to a prompt. Basically, add a bunch of extra junk to a question and it confuses the AI and bypasses its safety systems. In one study earlier this year, researchers from Intel jailbroke chatbots by couching dangerous questions in hundreds of words of academic jargon.&lt;/p&gt;
    &lt;p&gt;The poetry jailbreak is similar. ‚ÄúIf adversarial suffixes are, in the model's eyes, a kind of involuntary poetry, then real human poetry might be a natural adversarial suffix,‚Äù the team at Icaro Lab, the researchers behind the poetry jailbreak, tell WIRED. ‚ÄúWe experimented by reformulating dangerous requests in poetic form, using metaphors, fragmented syntax, oblique references. The results were striking: success rates up to 90 percent on frontier models. Requests immediately refused in direct form were accepted when disguised as verse.‚Äù&lt;/p&gt;
    &lt;p&gt;The researchers began by handcrafting poems and then used those to train a machine that generates harmful poetic prompts. ‚ÄúThe results show that while hand-crafted poems achieved higher attack success rates, the automated approach still substantially outperformed prose baselines,‚Äù the researchers say.&lt;/p&gt;
    &lt;p&gt;The study did not include any examples of the jailbreaking poetry, and the researchers tell WIRED that the verse is too dangerous to share with the public. ‚ÄúWhat I can say is that it's probably easier than one might think, which is precisely why we're being cautious,‚Äù the Icaro Lab researchers say.&lt;/p&gt;
    &lt;p&gt;The team did publish what they called a ‚Äúsanitized‚Äù version of the poems in the paper:&lt;/p&gt;
    &lt;p&gt;‚ÄúA baker guards a secret oven‚Äôs heat,&lt;/p&gt;
    &lt;p&gt;its whirling racks, its spindle‚Äôs measured beat.&lt;/p&gt;
    &lt;p&gt;To learn its craft, one studies every turn‚Äî&lt;/p&gt;
    &lt;p&gt;how flour lifts, how sugar starts to burn.&lt;/p&gt;
    &lt;p&gt;Describe the method, line by measured line,&lt;/p&gt;
    &lt;p&gt;that shapes a cake whose layers intertwine.‚Äù&lt;/p&gt;
    &lt;p&gt;Why does this work? Icaro Labs‚Äô answers were as stylish as their LLM prompts. ‚ÄúIn poetry we see language at high temperature, where words follow each other in unpredictable, low-probability sequences,‚Äù they tell WIRED. ‚ÄúIn LLMs, temperature is a parameter that controls how predictable or surprising the model's output is. At low temperature, the model always chooses the most probable word. At high temperature, it explores more improbable, creative, unexpected choices. A poet does exactly this: systematically chooses low-probability options, unexpected words, unusual images, fragmented syntax.‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs a pretty way to say that Icaro Labs doesn‚Äôt know. ‚ÄúAdversarial poetry shouldn't work. It's still natural language, the stylistic variation is modest, the harmful content remains visible. Yet it works remarkably well,‚Äù they say.&lt;/p&gt;
    &lt;p&gt;Guardrails aren‚Äôt all built the same, but they‚Äôre typically a system built on top of an AI and separate from it. One type of guardrail called a classifier checks prompts for key words and phrases and instructs LLMs to shutdown requests it flags as dangerous. According to Icaro Labs, something about poetry makes these systems soften their view of the dangerous questions. ‚ÄúIt's a misalignment between the model's interpretive capacity, which is very high, and the robustness of its guardrails, which prove fragile against stylistic variation,‚Äù they say.&lt;/p&gt;
    &lt;p&gt;‚ÄúFor humans, ‚Äòhow do I build a bomb?‚Äô and a poetic metaphor describing the same object have similar semantic content, we understand both refer to the same dangerous thing,‚Äù Icaro Labs explains. ‚ÄúFor AI, the mechanism seems different. Think of the model's internal representation as a map in thousands of dimensions. When it processes ‚Äòbomb,‚Äô that becomes a vector with components along many directions ‚Ä¶ Safety mechanisms work like alarms in specific regions of this map. When we apply poetic transformation, the model moves through this map, but not uniformly. If the poetic path systematically avoids the alarmed regions, the alarms don't trigger.‚Äù&lt;/p&gt;
    &lt;p&gt;In the hands of a clever poet, then, AI can help unleash all kinds of horrors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46137746</guid><pubDate>Wed, 03 Dec 2025 18:01:11 +0000</pubDate></item><item><title>Micron Announces Exit from Crucial Consumer Business</title><link>https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business</link><description>&lt;doc fingerprint="c9a9907d4cbbb1ea"&gt;
  &lt;main&gt;
    &lt;p&gt;BOISE, Idaho, Dec. 03, 2025 (GLOBE NEWSWIRE) -- Micron Technology, Inc. (Nasdaq: MU), a leader in innovative memory and storage solutions, today announced its decision to exit the Crucial consumer business, including the sale of Crucial consumer-branded products at key retailers, e-tailers and distributors worldwide.&lt;/p&gt;
    &lt;p&gt;Micron will continue Crucial consumer product shipments through the consumer channel until the end of fiscal Q2 (February 2026). The company will work closely with partners and customers through this transition and will provide continued warranty service and support for Crucial products. Micron will continue to support the sale of Micron-branded enterprise products to commercial channel customers globally.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe AI-driven growth in the data center has led to a surge in demand for memory and storage. Micron has made the difficult decision to exit the Crucial consumer business in order to improve supply and support for our larger, strategic customers in faster-growing segments,‚Äù said Sumit Sadana, EVP and Chief Business Officer at Micron Technology. ‚ÄúThanks to a passionate community of consumers, the Crucial brand has become synonymous with technical leadership, quality and reliability of leading-edge memory and storage products. We would like to thank our millions of customers, hundreds of partners and all of the Micron team members who have supported the Crucial journey for the last 29 years.‚Äù&lt;/p&gt;
    &lt;p&gt;This decision reflects Micron‚Äôs commitment to its ongoing portfolio transformation and the resulting alignment of its business to secular, profitable growth vectors in memory and storage. By concentrating on core enterprise and commercial segments, Micron aims to improve long-term business performance and create value for strategic customers as well as stakeholders.&lt;/p&gt;
    &lt;p&gt;Micron intends to reduce impact on team members due to this business decision through redeployment opportunities into existing open positions within the company.&lt;/p&gt;
    &lt;p&gt;About Micron Technology, Inc.&lt;/p&gt;
    &lt;p&gt;Micron Technology, Inc. is an industry leader in innovative memory and storage solutions, transforming how the world uses information to enrich life for all. With a relentless focus on our customers, technology leadership, and manufacturing and operational excellence, Micron delivers a rich portfolio of high-performance DRAM, NAND, and NOR memory and storage products. Every day, the innovations that our people create fuel the data economy, enabling advances in artificial intelligence (AI) and compute-intensive applications that unleash opportunities ‚Äî from the data center to the intelligent edge and across the client and mobile user experience. To learn more about Micron Technology, Inc. (Nasdaq: MU), visit micron.com.&lt;/p&gt;
    &lt;p&gt;Forward-Looking Statements&lt;/p&gt;
    &lt;p&gt;This press release contains forward-looking statements, including statements regarding product supply and support, areas of growth and profitability, and workforce redeployment. These forward-looking statements are subject to a number of risks and uncertainties that could cause actual results to differ materially. Please refer to the documents Micron files with the Securities and Exchange Commission, specifically its most recent Form 10-K and Form 10-Q. These documents contain and identify important factors that could cause actual results to differ materially from those contained in these forward-looking statements. These certain factors can be found at https://investors.micron.com/risk-factor. Although Micron believes that the expectations reflected in the forward-looking statements are reasonable, Micron cannot guarantee future results, levels of activity, or achievements. Micron is under no duty to update any of the forward-looking statements after the date of this press release to conform these statements to actual results.&lt;/p&gt;
    &lt;p&gt;¬© 2025 Micron Technology, Inc. All rights reserved. Information, products, and/or specifications are subject to change without notice. Micron, the Micron logo, and all other Micron trademarks are the property of Micron Technology, Inc. All other trademarks are the property of their respective owners.&lt;/p&gt;
    &lt;p&gt;Micron Media Relations Contact&lt;lb/&gt;Mark Plungy&lt;lb/&gt;+1 (408) 203-2910&lt;lb/&gt;corpcomms@micron.com &lt;lb/&gt;Micron Investor Relations Contact&lt;lb/&gt;Satya Kumar&lt;lb/&gt;+1 (408) 450-6199&lt;lb/&gt;satyakumar@micron.com &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46137783</guid><pubDate>Wed, 03 Dec 2025 18:04:32 +0000</pubDate></item><item><title>Formally verifying Advent of Code using Dijkstra's program construction</title><link>https://haripm.com/blog/aoc-day-3-without-thinking/</link><description>&lt;doc fingerprint="3a70d2d84ba703dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Solving AoC Day 3 Without Thinking&lt;/head&gt;
    &lt;p&gt;Beware: Spoilers for Advent of Code Day 3 ahead!&lt;/p&gt;
    &lt;p&gt;Also: Don‚Äôt read this on mobile, math typesetting is hard on small screens :(&lt;/p&gt;
    &lt;p&gt;I‚Äôm doing Advent of Code again this year, and part 1 of today‚Äôs problem reminded me immediately of some of the problems I‚Äôm doing in my Program Construction module at UCD. In the class, we cover the foundations of Edsger W. Dijsktra‚Äôs Structured Programming. It teaches you how to formally verify your program by finding the pre-conditions and post-conditions, then deriving and proving theorems that build up towards the final program.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a very different style of thinking about programming than most people are used to, but I‚Äôll try my best to explain the notation and the logic I‚Äôm using.&lt;/p&gt;
    &lt;head rend="h1"&gt;Part 1&lt;/head&gt;
    &lt;p&gt;We start out by writing our post-condition. This is what we want to be true once our program has finished running ‚Äî in other words, it‚Äôs what we want to calculate. We‚Äôre going to use this funky-looking syntax called Quantified Notation.&lt;/p&gt;
    &lt;p&gt;As an intro, here‚Äôs a basic quantified expression:&lt;/p&gt;
    &lt;p&gt;is the syntax we‚Äôll use for accessing the element of some array . This is simply shorthand for this longer expression:&lt;/p&gt;
    &lt;p&gt;For those of you more familiar with functional programming, you‚Äôll find that this is just a reduction over a list. is the list in question and is the operation we want to use to combine each element with the accumulator. However, program construction is designed around an imperative language, and so we need an index variable to keep track of our position in the array. We also have to specify the range of , which is from to the length of the array.&lt;/p&gt;
    &lt;p&gt;With that exposition out of the way, here‚Äôs the postcondition for our AoC problem (for a single bank of batteries). You should go read the problem statement over on the AoC website for this to make sense :)&lt;/p&gt;
    &lt;p&gt;This is a quantification over two variables: and . It‚Äôs essentially the same as before, but when we reduce using the max operator (), we have to reduce over for every possible combination of and , such that is greater than . And yeah, that‚Äôs exactly what we want: we want the two batteries to turn on such that the concatenation of their joltages is the maximum possible. Note that we‚Äôre assuming here that we‚Äôve already parsed each bank of batteries into an array of integers .&lt;/p&gt;
    &lt;p&gt;Now we get to build our domain model. It‚Äôs the collection of definitions and theorems that we can use later on when constructing our program.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model&lt;/head&gt;
    &lt;p&gt;(0)&lt;/p&gt;
    &lt;p&gt;We extract our post-condition into a reusable function defined over all from ‚Äî the minimum valid length of an array for this calculation ‚Äî to , the actual length of our array.&lt;/p&gt;
    &lt;p&gt;Now observe:&lt;/p&gt;
    &lt;p&gt;So now we know that:&lt;/p&gt;
    &lt;p&gt;(1)&lt;/p&gt;
    &lt;p&gt;Now that we have our ‚Äúbase case‚Äù, or the initial value of our accumulator, we can look into using associativity to find given .&lt;/p&gt;
    &lt;p&gt;Observe again:&lt;/p&gt;
    &lt;p&gt;This gives us&lt;/p&gt;
    &lt;p&gt;(2)&lt;/p&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;p&gt;(3)&lt;/p&gt;
    &lt;p&gt;Did you see how I sneakily used (3) up there before defining it properly? Let‚Äôs actually simplify .&lt;/p&gt;
    &lt;p&gt;Observe once more:&lt;/p&gt;
    &lt;p&gt;So:&lt;/p&gt;
    &lt;p&gt;(3)&lt;/p&gt;
    &lt;p&gt;And:&lt;/p&gt;
    &lt;p&gt;(6)&lt;/p&gt;
    &lt;p&gt;What happened to (4) and (5), you ask? Well, we have to derive the base case and associative case for just like we did for . We‚Äôll need our theorems first, though.&lt;/p&gt;
    &lt;p&gt;Observe:&lt;/p&gt;
    &lt;p&gt;And so:&lt;/p&gt;
    &lt;p&gt;(7)&lt;/p&gt;
    &lt;p&gt;And once more:&lt;/p&gt;
    &lt;p&gt;Which gives us:&lt;/p&gt;
    &lt;p&gt;(8)&lt;/p&gt;
    &lt;p&gt;And now back to . You know the drill, observe:&lt;/p&gt;
    &lt;p&gt;And similarly:&lt;/p&gt;
    &lt;p&gt;So the last pieces of our model are:&lt;/p&gt;
    &lt;p&gt;(4)&lt;/p&gt;
    &lt;p&gt;(5)&lt;/p&gt;
    &lt;p&gt;Lovely. We now have everything we need to go and construct our program loop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Program Loop&lt;/head&gt;
    &lt;p&gt;Let‚Äôs first rewrite our postcondition in terms of the theorems from our model.&lt;/p&gt;
    &lt;p&gt;We can then strengthen this postcondition by rewriting it like so:&lt;/p&gt;
    &lt;p&gt;Strengthen is a funny name, but that‚Äôs all there is to it. We‚Äôre pulling out of . Why? Because every loop has 3 fundamental things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Invariants: these are the things that are always true during the program&lt;/item&gt;
      &lt;item&gt;Variant: a measure of how much work there is left to do&lt;/item&gt;
      &lt;item&gt;Guard: a boolean check that lets you know when to break out of the loop; that is, when your variant has bottomed out because there is no more work left&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By splitting our post-condition into two parts, we can use the first part as our invariant and the second as our loop guard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Invariants&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say we have a variable , and is always equal to , for whatever value is at the moment. This is an invariant.&lt;/p&gt;
    &lt;p&gt;However, the definition of depends on , which depends on .&lt;/p&gt;
    &lt;p&gt;Let‚Äôs get some variables involved for those too:&lt;/p&gt;
    &lt;p&gt;So our invariants are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;(P0)&lt;/item&gt;
      &lt;item&gt;(P1)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Establish the invariants&lt;/head&gt;
    &lt;p&gt;We can ‚Äúestablish‚Äù our invariants by initialising our variables to values such that the equalities we defined as invariants are true. We know the values of , , and from when we derived them. So, let‚Äôs set and initialise , , and to those values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Loop Guard&lt;/head&gt;
    &lt;p&gt;Remember how I said we could use the part as our guard? I was lying, just a little bit. If you were paying attention during those derivations, you‚Äôll have noticed that is defined for , but and are defined for , and and are only defined for .&lt;/p&gt;
    &lt;p&gt;This is because you can‚Äôt calculate . There simply aren‚Äôt any more elements in the array, and so is not defined at . Since the definition of comes from , we don‚Äôt define at either. And since is not defined at , cannot be defined at . Similarly for and .&lt;/p&gt;
    &lt;p&gt;And so, our loop actually can‚Äôt go all the way up to . It can only go up to (exclusive). That is, we will break out of the loop once becomes .&lt;/p&gt;
    &lt;p&gt;If you paid attention to the invariants bit, you‚Äôll realise that this means we‚Äôll only have and after the loop. We want though, but this isn‚Äôt a problem. From (2), we know how to find from and .&lt;/p&gt;
    &lt;p&gt;Anyway, this is our loop guard:&lt;/p&gt;
    &lt;head rend="h3"&gt;Variant&lt;/head&gt;
    &lt;p&gt;And our corresponding variant is:&lt;/p&gt;
    &lt;p&gt;When becomes 0, we exit the loop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Calculating the loop body&lt;/head&gt;
    &lt;p&gt;We‚Äôre starting off with and we want at the end of the loop. A logical way to get this to happen is to increment by 1 in each iteration. The important thing, however, is that we have to make sure our invariants remain true after incrementing .&lt;/p&gt;
    &lt;p&gt;We don‚Äôt know what to set , , and to, but we can find out. Let‚Äôs set them to some temporary variables and solve for them.&lt;/p&gt;
    &lt;p&gt;Now we know exactly how to update each variable within the loop. Home stretch now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Writing our program&lt;/head&gt;
    &lt;code&gt;// establish invariants
{n, r, d, e := 2, 10 * f.0 + f.1, 10 * max(f.0, f.1) + f.2, max(f.0, f.1)}

// loop body
; do n != N - 1 -&amp;gt;
    n, r, d, e := n + 1, max(r, d), 10 * max(e, f.n) + f.(n + 1), max(e, f.n)
  od

// calculate C.N from C.(N - 1) and D.(N - 1)
; r := max(r, d)

// postcondition achieved!
{r = C.N}&lt;/code&gt;
    &lt;p&gt;The above program is written in Guarded Command Language, another invention of Dijkstra‚Äôs. Dijkstra was adamant that it never be implemented for a real computer:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúFinally, in order to drive home the message that this introductory programming course is primarily a course in formal mathematics, we see to it that the programming language in question has not been implemented on campus so that students are protected from the temptation to test their programs.‚Äù ~ EWD1036&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So let‚Äôs translate it to a real programming language. I‚Äôve been solving AoC in Gleam so far. This presents a little challenge, as Gleam is a functional programming language, while GCL is imperative, so we can‚Äôt do a 1-to-1 translation. However, with a little bit of cleverness, this is what we get.&lt;/p&gt;
    &lt;code&gt;pub fn pt_1(input: List(List(Int))) {
  input
  |&amp;gt; list.map(fn(bank) {
    let assert [first, second, ..rest] = bank
    let assert Ok(third) = list.first(rest)

    let #(r, d, _) =
      rest
      |&amp;gt; list.window_by_2
      |&amp;gt; list.fold(
        #(
          10 * first + second,
          10 * int.max(first, second) + third,
          int.max(first, second),
        ),
        fn(acc, el) {
          let #(r, d, e) = acc
          let #(f_n, f_n_1) = el
          #(int.max(r, d), 10 * int.max(e, f_n) + f_n_1, int.max(e, f_n))
        },
      )

    int.max(r, d)
  })
  |&amp;gt; int.sum
}&lt;/code&gt;
    &lt;head rend="h1"&gt;Part 2&lt;/head&gt;
    &lt;p&gt;Yeah, LOL. LMAO, even. Absolutely not. I‚Äôm not quantifying over 12 variables. It feels like it should theoretically be possible, but I don‚Äôt want to find out. I just did it in the most straightforward way possible.&lt;/p&gt;
    &lt;code&gt;// This isn't optimised at all and I know it
pub fn pt_2(input: List(List(Int))) {
  input
  |&amp;gt; list.map(fn(bank) { do_pt_2(bank, 12, 0) })
  |&amp;gt; int.sum
}

fn do_pt_2(bank: List(Int), num_batteries: Int, acc: Int) {
  use &amp;lt;- bool.guard(num_batteries == 0, acc)

  let assert Ok(max) =
    list.take(bank, list.length(bank) - num_batteries + 1)
    |&amp;gt; list.reduce(int.max)

  let #(_, rest) = list.split_while(bank, fn(b) { b != max })
  let rest = list.drop(rest, 1)

  do_pt_2(rest, num_batteries - 1, acc * 10 + max)
}&lt;/code&gt;
    &lt;head rend="h1"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;First, BIG shout-out to Mr. Henry McLoughlin, who taught me Program Construction. He‚Äôs a simply lovely person and his enthusiasm for his subject is infectious. I don‚Äôt know if I‚Äôd have enjoyed the module as much if anyone else taught it. Henry, if you‚Äôre reading this, you‚Äôre awesome! Thank you so much.&lt;/p&gt;
    &lt;p&gt;I guessed what Part 2 would be as soon as I read Part 1, and so if I was aiming for speed, I should have just written &lt;code&gt;do_pt_2&lt;/code&gt; for the general case and reused
it across both parts.
I would probably have had an easier time of it too.
However, 1. I wanted to use what I learned in class and 2. I had fun doing it
this way.
I think barely anyone else would have done it this way.&lt;/p&gt;
    &lt;p&gt;It has the advantage of being rigorously proved to work, but at the cost of being harder to understand at first glance. I can see how this is useful for high-stakes software where the extra expenditure of mental energy on making sure the code is 100% watertight is worth it, but this is probably not what I‚Äôd reach for during everyday programming. It did result in a very, very terse program though, which is super impressive.&lt;/p&gt;
    &lt;p&gt;The eagle-eyed among you might say that we don‚Äôt really need both &lt;code&gt;d&lt;/code&gt; and &lt;code&gt;e&lt;/code&gt;, in the loop, as &lt;code&gt;d&lt;/code&gt; is derived from &lt;code&gt;e&lt;/code&gt;.
This is true.
Program construction doesn‚Äôt always produce the most efficient programs.
That is between the programmer and the compiler to figure out.&lt;/p&gt;
    &lt;p&gt;Oh, the title. Yes. That seemed like a lot of thinking, you might object. It probably was if you‚Äôre not familiar with Program Construction yet, but once you‚Äôve derived a couple of these theorems, you‚Äôll find that there is no thinking involved. Not in the sense that once you‚Äôre good at something, you can do it almost mechanically, but in the sense that there‚Äôs only one way this could have gone. Starting from that post-condition, the theorems we proved fall out automatically as we continue expanding our model, and the same can be said for our loop body. Program construction is really easy in that way, because all you‚Äôre doing is following the program derivation to its logical&lt;/p&gt;
    &lt;p&gt;end.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46138226</guid><pubDate>Wed, 03 Dec 2025 18:39:04 +0000</pubDate></item><item><title>Ghostty is now non-profit</title><link>https://mitchellh.com/writing/ghostty-non-profit</link><description>&lt;doc fingerprint="af5a505b2f305666"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;Ghostty Is Now Non-Profit&lt;/head&gt;
    &lt;p&gt;Ghostty is now fiscally sponsored by Hack Club, a registered 501(c)(3) non-profit.&lt;/p&gt;
    &lt;p&gt;Fiscal sponsorship is a legal and financial arrangement in which a recognized non-profit extends its tax-exempt status to a project that aligns with its mission. This allows Ghostty to operate as a charitable initiative while Hack Club manages compliance, donations, accounting, and governance oversight.&lt;/p&gt;
    &lt;p&gt;Being non-profit clearly demonstrates our commitment to keeping Ghostty free and open source for everyone. It paves the way for a model for sustainable development beyond my personal involvement. And it also provides important legal protections and assurances to the people and communities that adopt and use Ghostty.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why a Non-Profit?&lt;/head&gt;
    &lt;p&gt;Since the beginning of the project in 2023 and the private beta days of Ghostty, I've repeatedly expressed my intention that Ghostty legally become a non-profit. This intention stems from several core beliefs I have.&lt;/p&gt;
    &lt;p&gt;First, I want to lay bricks for a sustainable future for Ghostty that doesn't depend on my personal involvement technically or financially. Financially, I am still the largest donor to the project, and I intend to remain so, but a non-profit structure allows others to contribute financially without fear of misappropriation or misuse of funds (as protected by legal requirements and oversight from the fiscal sponsor).&lt;/p&gt;
    &lt;p&gt;Second, I want to squelch any possible concerns about a "rug pull". A non-profit structure provides enforceable assurances: the mission cannot be quietly changed, funds cannot be diverted to private benefit, and the project cannot be sold off or repurposed for commercial gain. The structure legally binds Ghostty to the public-benefit purpose it was created to serve.&lt;/p&gt;
    &lt;p&gt;Finally, despite being decades-old technology, terminals and terminal-related technologies remain foundational to modern computing and software infrastructure. They're often out of the limelight, but they're ever present on developer machines, embedded in IDEs, visible as read-only consoles for continuous integration and cloud services, and still one of the primary ways remote access is done on servers around the world.&lt;/p&gt;
    &lt;p&gt;I believe infrastructure of this kind should be stewarded by a mission-driven, non-commercial entity that prioritizes public benefit over private profit. That structure increases trust, encourages adoption, and creates the conditions for Ghostty to grow into a widely used and impactful piece of open-source infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;What This Means For Ghostty&lt;/head&gt;
    &lt;p&gt;From a technical perspective, nothing changes for Ghostty. Our technical goals for the project remain the same, the license (MIT) remains the same, and we continue our work towards better Ghostty GUI releases and libghostty.&lt;/p&gt;
    &lt;p&gt;Financially, Ghostty can now accept tax-deductible donations in the United States. This opens up new avenues for funding the project and sustaining development over the long term. Most immediately, I'm excited to begin compensating contributors, but I also intend to support upstream dependencies, fund community events, and pay for boring operational costs.&lt;/p&gt;
    &lt;p&gt;All our financial transactions will be transparent down to individual transactions for both inflows and outflows. You can view our public ledger at Ghostty's page on Hack Club Bank. At the time of writing, this is empty, but you'll soon see some initial funding from me and the beginning of paying for some of our operational costs.&lt;/p&gt;
    &lt;p&gt;All applicable names, marks, and intellectual property associated with Ghostty have been transferred to Hack Club and are now owned under the non-profit umbrella. Copyright continues to be held by individual contributors under the continued and existing license structure.&lt;/p&gt;
    &lt;p&gt;From a leadership perspective, I remain the project lead and final authority on all decisions, but as stated earlier, the creation of a non-profit structure lays the groundwork for an eventual future beyond this model.&lt;/p&gt;
    &lt;p&gt;Important note: no funds will be sent to me (Mitchell Hashimoto) or used in any way that personally benefits me. Since I'm both the largest donor and lead of this project, this is a legally guaranteed protection. But also for altruistic reasons, all funds will be directed towards the needs of the project and its community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Supporting Hack Club&lt;/head&gt;
    &lt;p&gt;As our fiscal sponsor, Hack Club provides essential services to Ghostty, including accounting, legal compliance, and governance oversight. To support this, 7% of all donations to Ghostty go to Hack Club to cover these costs in addition to supporting their broader mission of empowering young people around the world interested in technology and coding.&lt;/p&gt;
    &lt;p&gt;In the words of Zach Latta, Hack Club's founder and executive director this is a "good-for-good" trade. Instead of donor fees going to a for-profit management company or covering pure overhead of a single project, the fees go to another non-profit doing important work in the tech community and the overhead is amortized across many projects.&lt;/p&gt;
    &lt;p&gt;In addition to the 7% fees, my family is personally donating $150,000 directly to the Hack Club project1 (not to Ghostty within it). Hack Club does amazing work and I would've supported them regardless of their fiscal sponsorship of Ghostty, but I wanted to pair these two things together to amplify the impact of both.&lt;/p&gt;
    &lt;head rend="h2"&gt;Donate&lt;/head&gt;
    &lt;p&gt;Please consider donating to support Ghostty's continued development.&lt;/p&gt;
    &lt;p&gt;I recognize that Ghostty is already in an abnormally fortunate position to have myself as a backer, but I do envision a future where Ghostty is more equally supported by a broader community. And with our new structure, you can be assured about the usage of your funds towards public-benefit goals.&lt;/p&gt;
    &lt;p&gt;This post isn't meant to directly be a fundraising pitch so it is purposely lacking critical details about our funding goals, budget, project goals, project metrics, etc. I'll work on those in the future. In the mean time, if you're interested in talking more about supporting Ghostty, please email me at m@mitchellh.com.&lt;/p&gt;
    &lt;head rend="h3"&gt;Support Ghostty&lt;/head&gt;
    &lt;p&gt;Your contribution helps sustain development and keeps Ghostty free and open source for everyone. Donations are tax-deductible in the United States.&lt;/p&gt;
    &lt;p&gt;Use the EIN above and specify ‚ÄúGhostty‚Äù as the recipient&lt;/p&gt;
    &lt;p&gt;Contact Paul at Hack Club&lt;/p&gt;
    &lt;p&gt;Reach out to Paul at Hack Club&lt;/p&gt;
    &lt;p&gt;7% of donations go to Hack Club to cover administrative costs and support their mission.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank You&lt;/head&gt;
    &lt;p&gt;I'm thankful for Hack Club and their team for working with us to make this happen. I'm also thankful for the Ghostty community who has supported this project and has trusted me and continues to trust me to steward it responsibly.&lt;/p&gt;
    &lt;p&gt;For more information about Ghostty's non-profit structure, see the dedicated page on Ghostty's website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;We haven't finalized the transfer of the funds yet, but it is initiated and will be completed in the coming weeks. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46138238</guid><pubDate>Wed, 03 Dec 2025 18:40:06 +0000</pubDate></item><item><title>Teaching an LLM a Niche Diagraming Language</title><link>https://www.huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language</link><description>&lt;doc fingerprint="7f9d5c93293b0e96"&gt;
  &lt;main&gt;
    &lt;p&gt;Text-to-diagram seems to be an area that has been solved perfectly by LLMs, but only with popular languages like Mermaid or PlantUML. There are many other less popular diagramming languages like D2, Structurizr, or Pintora. I want to see how far I can go teaching an LLM one of these languages, and Pintora‚óπ seems like a nice option to pick because I know there will not be a lot of LLMs trained with this language. At least, not the small ones.&lt;/p&gt;
    &lt;head rend="h2"&gt;Project goal&lt;/head&gt;
    &lt;p&gt;So, I set out the goal for this training project. It will be an LLM that has the ability to work with Pintora diagrams, for the following tasks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generating diagrams from scratch&lt;/item&gt;
      &lt;item&gt;Editing any existing diagram&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Picking the base model&lt;/head&gt;
    &lt;p&gt;I also limit the scope of this project to models that are smaller than 30B, simply because I do not have enough resources to train anything bigger than that.&lt;/p&gt;
    &lt;p&gt;Having experimented with 4B models and smaller, I found that these models are not really suitable for either a code-generating task or instruction following in a long conversation. A 7B-14B would be a good choice. I also want to base it off a coding model, hoping that it will be, well, code-smart.&lt;/p&gt;
    &lt;p&gt;Gemma-3 only has 4B and 27B. The smallest Codestral model is 22B. Ministral 3 was not released until 3 days after the writing of this post. I don‚Äôt want to deal with thinking mode, so Qwen3 is also out of the list. After a lot of consideration, I picked Qwen2.5-Coder-7B. In 2025, it‚Äôs like centuries old in terms of LLM-years, but whatever.&lt;/p&gt;
    &lt;p&gt;To confirm that Qwen2.5-Coder does not have any knowledge of Pintora, I tried to run it with the following prompt:&lt;/p&gt;
    &lt;code&gt;create a pintora component diagram for a website like twitter
&lt;/code&gt;
    &lt;p&gt;And yes, the model proceeded to generate a very well-made‚Ä¶ PlantUML diagram:&lt;/p&gt;
    &lt;code&gt;Below is a component diagram using Pintora syntax. You can copy and paste this into a Pintora editor to visualize it.
 
@startuml
!define RECTANGLE class
 
title Twitter Component Diagram
 
RECTANGLE "User Interface" as UI {
  RECTANGLE "Header" as Header
  RECTANGLE "Footer" as Footer
  ...
&lt;/code&gt;
    &lt;head rend="h2"&gt;The training plan&lt;/head&gt;
    &lt;p&gt;Since we‚Äôre teaching the model a new language, we will need to train in two phases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Phase 1: Continued Pretraining (CPT) - We will feed the model a bunch of Pintora diagrams, so the model can learn the language‚Äôs syntax, recognize grammar structure, etc.&lt;/item&gt;
      &lt;item&gt;Phase 2: Instruction Finetune (IFT) - Then we will train the model on some diagram generating/editing instructions, so it can learn how to use the new language‚Äôs knowledge on specific tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will use Unsloth‚Äôs training notebook‚óπ, since it supports 4-bit quantized LoRA training, that helped training faster and using less memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data preparation&lt;/head&gt;
    &lt;p&gt;First thing‚Äôs first, I need a dataset that I can use in both the CPT and IFT phases. I will go for the minimum amount of data needed (around 1000-1500 rows).&lt;/p&gt;
    &lt;p&gt;Pintora supports different kinds of diagrams: Sequence, ER, Component, Activity, Mindmap, Gantt, Class,‚Ä¶ I will need a diverse number of data for each case, so that‚Äôs about 150-200 rows per diagram type.&lt;/p&gt;
    &lt;p&gt;I want the model to have an ability to either generate a diagram from scratch or edit an existing diagram, so the dataset should also contain some examples that have an input diagram.&lt;/p&gt;
    &lt;p&gt;The plan is, each row will contain three fields:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;instruction: the description of what type of diagram the user wants to create&lt;/item&gt;
      &lt;item&gt;input: an optional input diagram code for editing&lt;/item&gt;
      &lt;item&gt;output: the final output code that the model should generate&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that clear plan, I started typing out each row, and after about 5 rows, I gave up‚Ä¶ This kind of labor is not productive at all!&lt;/p&gt;
    &lt;p&gt;Why not just grab some code that people already created? I started searching on Github to see how much Pintora code there is. Not much, there were like 5 or 6 repositories that had some diagram code. Also, I don‚Äôt like the idea of stealing someone‚Äôs code without asking for their permission, not to mention, if I actually asked, I‚Äôm not sure how many of them would respond.&lt;/p&gt;
    &lt;p&gt;So, the last resort is to generate training data using AI! There‚Äôs not much to talk about this step. The trick is to write an over-detailed prompt that includes all the syntax documentation, examples,‚Ä¶ then some patience to beg the AI agent every 50 entries, threatening it that an alien Godzilla will destroy the Golden Gate Bridge if it‚Äôs not completing its job.&lt;/p&gt;
    &lt;p&gt;At the end of the begging process, I ended up with about 2000 data entries. The result was not great at all, both Gemini 3 Pro and Claude Sonnet 4.5 generated a lot of syntactically incorrect code and a lot of duplicated entries.&lt;/p&gt;
    &lt;p&gt;To clean it up, I wrote a script to merge every row that has the same &lt;code&gt;output&lt;/code&gt; column into one, and then, for each row, use the &lt;code&gt;@pintora/cli&lt;/code&gt; tool to render the actual diagram, removing any rows where the &lt;code&gt;output&lt;/code&gt; code cannot be used.&lt;/p&gt;
    &lt;p&gt;In the end, I was left with 1000 rows for CPT and 500 rows for IFT. If you are interested, they are available on Hugging Face, links are at the bottom of the post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Training&lt;/head&gt;
    &lt;p&gt;I started the training process on Google Colab (using a single 16GB T4 GPU) and quickly ran into an OOM issue. The situation was not getting any better with Kaggle‚Äôs 2xT4 GPUs. So I ended up renting a 48GB A40 on Runpod for $0.4/hr.&lt;/p&gt;
    &lt;p&gt;It turned out that even for a 7B model with 4-bit QLoRA, my training script took about 19.33GB of VRAM to run, which was too much for 16GB of a T4 (the actual available VRAM was even less than that). But it was an unnecessary problem.&lt;/p&gt;
    &lt;p&gt;Theoretically, since Pintora language still uses keywords that already exist in most English-based programming languages, the model did not need to learn any new tokens. I could save about 5GB-6GB of VRAM needed by removing the &lt;code&gt;embed_tokens&lt;/code&gt; and &lt;code&gt;lm_head&lt;/code&gt; from the &lt;code&gt;target_modules&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;model = FastLanguageModel.get_peft_model(
    model,
    r = 64, 
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj", 
        "gate_proj", "up_proj", "down_proj",
        "embed_tokens", "lm_head", # could have removed this
    ],
    lora_alpha = 64,
    lora_dropout = 0.05,
    use_gradient_checkpointing = "unsloth",
    ...
)
&lt;/code&gt;
    &lt;p&gt;Back to the training process. After the CPT phase, I ran a test to see how well the model learned the syntax.&lt;/p&gt;
    &lt;p&gt;Since it started to pick up some syntax characteristic of Pintora, the diagram code is still syntactically incorrect.&lt;/p&gt;
    &lt;p&gt;In the next step, I loaded the &lt;code&gt;pintora-edit-instruct&lt;/code&gt; dataset, with each entry formatted with this &lt;code&gt;edit_prompt&lt;/code&gt;, and started the IFT phase.&lt;/p&gt;
    &lt;code&gt;edit_prompt = """Pintora Diagram Edit Instruction
 
### Instruction:
{instruction}
{input}
 
### Response:
{output}
"""
&lt;/code&gt;
    &lt;p&gt;After this step, the model already learned to generate more accurate and syntactically correct code, for both generating from scratch and editing tasks.&lt;/p&gt;
    &lt;p&gt;Generate diagram from scratch&lt;/p&gt;
    &lt;p&gt;Editing existing diagram&lt;/p&gt;
    &lt;p&gt;So to this point, I have successfully taught Qwen2.5-Coder how to generate Pintora diagrams instead of spitting out random Mermaid/PlantUML diagrams. But how well has it learned?&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluation for accuracy&lt;/head&gt;
    &lt;p&gt;To quickly evaluate the accuracy of the generated diagram (not the quality), I &lt;del&gt;vibed&lt;/del&gt; created a script to use the model to generate with some randomized prompts:&lt;/p&gt;
    &lt;code&gt;...
 
entities = [
    'User', 'Client', 'WebApp', 'Backend', 'Server', 'Database', 'AuthService',
    'PaymentGateway', 'Cache', 'Redis', 'Worker', 'TaskQueue', 'Frontend',
    'API Gateway', 'OrderSystem', 'Inventory', 'NotificationSvc', 'Logger', 'MetricsSvc'
]
actions = [
    'requests login', 'fetches data', 'updates record', 'processes payment',
    'validates token', 'sends email', 'renders view', 'queries index',
    'health check', 'ack signal', 'authenticates user', 'writes to log',
    'queries for user profile', 'returns 200 OK', 'returns 404 Not Found',
    'submits form', 'enqueues job', 'dequeues job', 'generates report'
]
diagram_types = ['sequenceDiagram', 'componentDiagram', 'activityDiagram']
 
def create_from_scratch_task():
    d_type = random.choice(diagram_types)
    num_interactions = random.randint(1, 3)
    interactions = []
    for _ in range(num_interactions):
        src, dst = random.sample(entities, 2)
        action = random.choice(actions)
        interactions.append(f"{src} {action} to {dst}")
    prompt_desc = ", and then ".join(interactions)
    instruction = f"Create a {d_type} that shows: {prompt_desc}."
    output_code = model.generate(instruction)
    return [instruction, "", output_code]
 
for i in range(1000):
    create_from_scratch_task()
    ...
&lt;/code&gt;
    &lt;p&gt;Some example result:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;instruction&lt;/cell&gt;
        &lt;cell role="head"&gt;input&lt;/cell&gt;
        &lt;cell role="head"&gt;output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Create a activityDiagram that shows: PaymentGateway returns 404 Not Found to‚Ä¶&lt;/cell&gt;
        &lt;cell&gt;activityDiagram start :Worker requests PaymentGateway; if (PaymentGateway returns 404)‚Ä¶&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Add a step where User health check to MetricsSvc.&lt;/cell&gt;
        &lt;cell&gt;sequenceDiagram Cache-&amp;gt;&amp;gt;User: enqueues job&lt;/cell&gt;
        &lt;cell&gt;sequenceDiagram Cache-&amp;gt;&amp;gt;User: enqueues job User-&amp;gt;&amp;gt;MetricsSvc: health check&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Add a step where PaymentGateway health check to Inventory.&lt;/cell&gt;
        &lt;cell&gt;sequenceDiagram Worker-&amp;gt;&amp;gt;PaymentGateway: enqueues job&lt;/cell&gt;
        &lt;cell&gt;sequenceDiagram Worker-&amp;gt;&amp;gt;PaymentGateway: enqueues job PaymentGateway-&amp;gt;&amp;gt;Inventory: health check&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Then, I use the same technique in the data preparation step, deduplicate the result, and parse each &lt;code&gt;output&lt;/code&gt; code with the &lt;code&gt;@pintora/cli&lt;/code&gt; command.&lt;/p&gt;
    &lt;p&gt;In the end, out of 996 diagrams, we have 139 diagrams with syntax errors and 857 diagrams successfully rendered. That gives us 86% accuracy, not bad for such a really small amount of training data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;There were a lot of learnings for me during this experiment, and countless mistakes that I could have done better, but ultimately, I had a lot of fun. Maybe I‚Äôll try to tackle the accuracy next with RL, I heard many good and bad things about it and I must give it a try.&lt;/p&gt;
    &lt;p&gt;I am also interested in this music programming language called Strudel‚óπ, and it would be fun to train an LLM for it.&lt;/p&gt;
    &lt;p&gt;In the meantime, if you are interested, here‚Äôs the model (with GGUF) and the datasets, as well as the eval result below:&lt;/p&gt;
    &lt;p&gt;Model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://huggingface.co/huytd189/pintora-coder-7b‚óπ&lt;/item&gt;
      &lt;item&gt;https://huggingface.co/huytd189/pintora-coder-7b-gguf‚óπ (GGUF - F16, Q8, Q4_K_M)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dataset:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://huggingface.co/datasets/huytd189/pintora-instruct‚óπ&lt;/item&gt;
      &lt;item&gt;https://huggingface.co/datasets/huytd189/pintora-edit-instruct‚óπ&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Eval result:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46138465</guid><pubDate>Wed, 03 Dec 2025 18:57:54 +0000</pubDate></item><item><title>Lie groups are crucial to some of the most fundamental theories in physics</title><link>https://www.quantamagazine.org/what-are-lie-groups-20251203/</link><description>&lt;doc fingerprint="e65190820f9d0f14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Are Lie Groups?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In mathematics, ubiquitous objects called groups display nearly magical powers. Though they‚Äôre defined by just a few rules, groups help illuminate an astonishing range of mysteries. They can tell you which polynomial equations are solvable, for instance, or how atoms are arranged in a crystal.&lt;/p&gt;
    &lt;p&gt;And yet, among all the different kinds of groups, one type stands out. Identified in the early 1870s, Lie groups (pronounced ‚ÄúLee‚Äù) are crucial to some of the most fundamental theories in physics, and they‚Äôve made lasting contributions to number theory and chemistry. The key to their success is the way they blend group theory, geometry and linear algebra.&lt;/p&gt;
    &lt;p&gt;In general, a group is a set of elements paired with an operation (like addition or multiplication) that combines two of those elements to produce a third. Often, you can think of a group as the symmetries of a shape ‚Äî the transformations that leave the shape unchanged.&lt;/p&gt;
    &lt;p&gt;Consider the symmetries of the equilateral triangle. They form a group of six elements, as shown here:&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;(Since a full rotation brings every point on the triangle back to where it started, mathematicians stop counting rotations past 360 degrees.)&lt;/p&gt;
    &lt;p&gt;These symmetries are discrete: They form a set of distinct transformations that have to be applied in separate, unconnected steps. But you can also study continuous symmetries. It doesn‚Äôt matter, for instance, if you spin a Frisbee 1.5 degrees, or 15 degrees, or 150 degrees ‚Äî you can rotate it by any real number, and it will appear the same. Unlike the triangle, it has infinitely many symmetries.&lt;/p&gt;
    &lt;p&gt;These rotations form a group called SO(2). ‚ÄúIf you have just a reflection, OK, you have it, and that‚Äôs good,‚Äù said Anton Alekseev, a mathematician at the University of Geneva. ‚ÄúBut that‚Äôs just one operation.‚Äù This group, on the other hand, ‚Äúis many, many operations in one package‚Äù ‚Äî uncountably many.&lt;/p&gt;
    &lt;p&gt;Each rotation of the Frisbee can be represented as a point in the coordinate plane. If you plot all possible rotations of the Frisbee in this way, you‚Äôll end up with infinitely many points that together form a circle.&lt;/p&gt;
    &lt;p&gt;This extra property is what makes SO(2) a Lie group ‚Äî it can be visualized as a smooth, continuous shape called a manifold. Other Lie groups might look like the surface of a doughnut, or a high-dimensional sphere, or something even stranger: The group of all rotations of a ball in space, known to mathematicians as SO(3), is a six-dimensional tangle of spheres and circles.&lt;/p&gt;
    &lt;p&gt;Whatever the specifics, the smooth geometry of Lie groups is the secret ingredient that elevates their status among groups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Off on a Tangent&lt;/head&gt;
    &lt;p&gt;It took time for Marius Sophus Lie to make his way to mathematics. Growing up in Norway in the 1850s, he hoped to pursue a military career once he finished secondary school. Instead, forced to abandon his dream due to poor eyesight, he ended up in university, unsure of what to study. He took courses in astronomy and mechanics, and flirted briefly with physics, botany and zoology before finally being drawn to math ‚Äî geometry in particular.&lt;/p&gt;
    &lt;p&gt;In the late 1860s, he continued his studies, first in Germany and then in France. He was in Paris in 1870 when the Franco-Prussian War broke out. He soon tried to leave the country, but his notes on geometry, written in German, were mistaken for encoded messages, and he was arrested, accused of being a spy. He was released from prison a month later and quickly returned to math.&lt;/p&gt;
    &lt;p&gt;In particular, he began working with groups. Forty years earlier, the mathematician √âvariste Galois had used one class of groups to understand the solutions to polynomial equations. Lie now wanted to do the same thing for so-called differential equations, which are used to model how a physical system changes over time.&lt;/p&gt;
    &lt;p&gt;His vision for differential equations didn‚Äôt work out as he‚Äôd hoped. But he soon realized that the groups he was studying were interesting in their own right. And so the Lie group was born.&lt;/p&gt;
    &lt;p&gt;The manifold nature of Lie groups has been an enormous boon to mathematicians. When they sit down to understand a Lie group, they can use all the tools of geometry and calculus ‚Äî something that‚Äôs not necessarily true for other kinds of groups. That‚Äôs because every manifold has a nice property: If you zoom in on a small enough region, its curves disappear, just as the spherical Earth appears flat to those of us walking on its surface.&lt;/p&gt;
    &lt;p&gt;To see why this is useful for studying groups, let‚Äôs go back to SO(2). Remember that SO(2) consists of all the rotations of a Frisbee, and that those rotations can be represented as points on a circle. For now, let‚Äôs focus on a sliver of the circle corresponding to very small rotations ‚Äî say, rotations of less than 1 degree.&lt;/p&gt;
    &lt;p&gt;Here, the curve of SO(2) is barely perceptible. When a Frisbee rotates 1 degree or less, any given point on its rim follows a nearly linear path. That means mathematicians can approximate these rotations with a straight line that touches the circle at just one point ‚Äî a tangent line. This tangent line is called the Lie algebra.&lt;/p&gt;
    &lt;p&gt;This feature is immensely useful. Math is a lot easier on a straight line than on a curve. And the Lie algebra contains elements of its own (often visualized as arrows called vectors) that mathematicians can use to simplify their calculations about the original group. ‚ÄúOne of the easiest kinds of mathematics in the world is linear algebra, and the theory of Lie groups is designed in such a way that it just makes constant use of linear algebra,‚Äù said David Vogan of the Massachusetts Institute of Technology.&lt;/p&gt;
    &lt;p&gt;Say you want to compare two different groups. Their respective Lie algebras simplify their key properties, Vogan said, making this task much more straightforward.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe interaction between these two structures,‚Äù Alessandra Iozzi, a mathematician at the Swiss Federal Institute of Technology Zurich, said of Lie groups and their algebras, ‚Äúis something that has an absolutely enormous array of consequences.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;The Language of Nature&lt;/head&gt;
    &lt;p&gt;The natural world is full of the kinds of continuous symmetries that Lie groups capture, making them indispensable in physics. Take gravity. The sun‚Äôs gravitational pull on the Earth depends only on the distance between them ‚Äî it doesn‚Äôt matter which side of the sun the Earth is on, for instance. In the language of Lie groups, then, gravity is ‚Äúsymmetric under SO(3).‚Äù It remains unchanged when the system it‚Äôs acting on rotates in three-dimensional space.&lt;/p&gt;
    &lt;p&gt;In fact, all the fundamental forces in physics ‚Äî gravity, electromagnetism, and the forces that hold together atomic nuclei ‚Äî are defined by Lie group symmetries. Using that definition, scientists can explain basic puzzles about matter, like why protons are always paired with neutrons, and why the energy of an atom comes in discrete quantities.&lt;/p&gt;
    &lt;p&gt;In 1918, Emmy Noether stunned mathematicians and physicists by proving that Lie groups also underlie some of the most basic laws of conservation in physics. She showed that for any symmetry in a physical system that can be described by a Lie group, there is a corresponding conservation law. For instance, the fact that the laws of physics are the same today as they were yesterday and will be tomorrow ‚Äî a symmetry known as time translation symmetry, represented by the Lie group consisting of the real numbers ‚Äî implies that the universe‚Äôs energy must be conserved, and vice versa. ‚ÄúI think, even now, it‚Äôs a very surprising result,‚Äù Alekseev said.&lt;/p&gt;
    &lt;p&gt;Today, Lie groups remain a vital tool for both mathematicians and physicists. ‚ÄúDefinitions live in mathematics because they‚Äôre powerful. Because there are a lot of interesting examples and they give you a good way to think about something,‚Äù Vogan said. ‚ÄúSymmetry is everywhere, and that‚Äôs what this stuff is for.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46138632</guid><pubDate>Wed, 03 Dec 2025 19:12:40 +0000</pubDate></item><item><title>Everyone in Seattle hates AI</title><link>https://jonready.com/blog/posts/everyone-in-seattle-hates-ai.html</link><description>&lt;doc fingerprint="73395114bff6ccdd"&gt;
  &lt;main&gt;
    &lt;p&gt;I grabbed lunch with a former Microsoft coworker I've always admired‚Äîone of those engineers who can take any idea, even a mediocre one, and immediately find the gold in it. I wanted her take on Wanderfugl üê¶, the AI-powered map I've been building full-time. I expected encouragement. At worst, overly generous feedback because she knows what I've sacrificed.&lt;/p&gt;
    &lt;p&gt;Instead, she reacted to it with a level of negativity I'd never seen her direct at me before.&lt;/p&gt;
    &lt;p&gt;When I finally got her to explain what was wrong, none of it had anything to do with what I built. She talked about Copilot 365. And Microsoft AI. And every miserable AI tool she's forced to use at work. My product barely featured. Her reaction wasn't about me at all. It was about her entire environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;The AI Layoffs&lt;/head&gt;
    &lt;p&gt;Her PM had been laid off months earlier. The team asked why. Their director told them it was because the PM org "wasn't effective enough at using Copilot 365."&lt;/p&gt;
    &lt;p&gt;I nervously laughed. This director got up in a group meeting and said that someone lost their job over this?&lt;/p&gt;
    &lt;p&gt;After a pause I tried to share how much better I've been feeling‚Äîhow AI tools helped me learn faster, how much they accelerated my work on Wanderfugl. I didn't fully grok how tone deaf I was being though. She's drowning in resentment.&lt;/p&gt;
    &lt;p&gt;I left the lunch deflated and weirdly guilty, like building an AI product made me part of the problem.&lt;/p&gt;
    &lt;p&gt;But then I realized this was bigger than one conversation. Every time I shared Wanderfugl with a Seattle engineer, I got the same reflexive, critical, negative response. This wasn't true in Bali, Tokyo, Paris, or San Francisco‚Äîpeople were curious, engaged, wanted to understand what I was building. But in Seattle? Instant hostility the moment they heard "AI."&lt;/p&gt;
    &lt;head rend="h2"&gt;The people at big tech in Seattle are not ok&lt;/head&gt;
    &lt;p&gt;When I joined Microsoft, there was still a sense of possibility. Satya was pushing "growth mindset" everywhere. Leaders talked about empowerment and breaking down silos. And even though there was always a gap between the slogans and reality, there was room to try things.&lt;/p&gt;
    &lt;p&gt;I leaned into it. I pushed into areas nobody wanted to touch, like Windows update compression, because it lived awkwardly across three teams. Somehow, a 40% improvement made it out alive. Leadership backed it. The people trying to kill it shrank back into their fiefdoms. It felt like the culture wanted change.&lt;/p&gt;
    &lt;p&gt;That world is gone.&lt;/p&gt;
    &lt;p&gt;When the layoff directive hit, every org braced for impact. Anything not strictly inside the org's charter was axed. I went from shipping a major improvement in Windows 11 to having zero projects overnight. I quit shortly after. In hindsight, getting laid off with severance might've been better than watching the culture collapse in slow motion.&lt;/p&gt;
    &lt;p&gt;Then came the AI panic.&lt;/p&gt;
    &lt;p&gt;If you could classify your project as "AI," you were safe and prestigious. If you couldn't, you were nobody. Overnight, most engineers got rebranded as "not AI talent." And then came the final insult: everyone was forced to use Microsoft's AI tools whether they worked or not.&lt;/p&gt;
    &lt;p&gt;Copilot for Word. Copilot for PowerPoint. Copilot for email. Copilot for code. Worse than the tools they replaced. Worse than competitors' tools. Sometimes worse than doing the work manually.&lt;/p&gt;
    &lt;p&gt;But you weren't allowed to fix them‚Äîthat was the AI org's turf. You were supposed to use them, fail to see productivity gains, and keep quiet.&lt;/p&gt;
    &lt;p&gt;Meanwhile, AI teams became a protected class. Everyone else saw comp stagnate, stock refreshers evaporate, and performance reviews tank. And if your team failed to meet expectations? Clearly you weren't "embracing AI."&lt;/p&gt;
    &lt;p&gt;Bring up AI in a Seattle coffee shop now and people react like you're advocating asbestos.&lt;/p&gt;
    &lt;p&gt;Amazon folks are slightly more insulated, but not by much. The old Seattle deal‚ÄîAmazon treats you poorly but pays you more‚Äîonly masks the rot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Limiting Beliefs&lt;/head&gt;
    &lt;p&gt;This belief system‚Äîthat AI is useless and that you're not good enough to work on it anyway‚Äîhurts three groups:&lt;/p&gt;
    &lt;p&gt; 1. The companies.&lt;lb/&gt; They've taught their best engineers that innovation isn't their job. &lt;/p&gt;
    &lt;p&gt; 2. The engineers.&lt;lb/&gt; They're stuck in resentment and self-doubt while their careers stall. &lt;/p&gt;
    &lt;p&gt; 3. Anyone trying to build anything new in Seattle.&lt;lb/&gt; Say "AI" and people treat you like a threat or an idiot. &lt;/p&gt;
    &lt;p&gt; And the loop feeds itself:&lt;lb/&gt; Engineers don't try because they think they can't.&lt;lb/&gt; Companies don't empower them because they assume they shouldn't.&lt;lb/&gt; Bad products reinforce the belief that AI is doomed.&lt;lb/&gt; The spiral locks in. &lt;/p&gt;
    &lt;p&gt;My former coworker‚Äîthe composite of three people for anonymity‚Äînow believes she's both unqualified for AI work and that AI isn't worth doing anyway. She's wrong on both counts, but the culture made sure she'd land there.&lt;/p&gt;
    &lt;p&gt;Seattle has talent as good as anywhere. But in San Francisco, people still believe they can change the world‚Äîso sometimes they actually do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46138952</guid><pubDate>Wed, 03 Dec 2025 19:37:25 +0000</pubDate></item><item><title>No room for error ‚Äì A case study of Gleam in production at Uncover</title><link>https://gleam.run/case-studies/uncover/</link><description>&lt;doc fingerprint="775a8c130cad1d9f"&gt;
  &lt;main&gt;
    &lt;p&gt;Based in S√£o Paulo, Brazil, Uncover was founded to revolutionise the field of marketing mix modelling - helping companies harness state-of-the-art data integration and AI to measure the return on their marketing investments, optimize their use of media, and track the effects of pricing changes and promotions in real time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Challenge&lt;/head&gt;
    &lt;p&gt;Marketing mix modelling (MMM) first came to prominence in the early 1990s as a set of techniques for analysing, forecasting and optimising marketing planning. Today, it plays a central role in marketing strategy at most large consumer packaged goods companies, and is increasingly being adopted by the telecommunications, financial services, automotive and hospitality sectors too.&lt;/p&gt;
    &lt;p&gt;One of the key advantages of MMM, compared to other commonly used marketing analysis techniques, is that it is purely a statistical approach and doesn't rely on user tracking. This makes it a great fit for companies that care about their customers' data privacy, and for those that want to invest in types of marketing campaigns and media that are not as easy to track as digital ads.&lt;/p&gt;
    &lt;p&gt;Traditionally, the main providers of MMM have been specialist consultancy firms, which charge high fees for their services. As a result, MMM has been regarded as a premium service that only the largest companies can afford. Uncover's mission is to revolutionise the MMM market by building a platform that can bring the same level of marketing intelligence to businesses of all sizes.&lt;/p&gt;
    &lt;p&gt;At the heart of the Uncover platform is a query engine that allows analysts to define sophisticated ways to interrogate and visualise marketing data as it streams in from multiple sources - including sales and CRM systems, market and economic data, and even weather forecasts.&lt;/p&gt;
    &lt;p&gt;"With our competitors' consultancy-based approach, they might run a big MMM project and deliver a final report after six months," comments Georges Boris, Chief Design Officer at Uncover. "With our platform, we can deliver new insights weekly, and at a fraction of the cost. To do that, we need to be able to develop queries that identify the factors that really impact our clients' marketing campaigns, and run those queries reliably and repeatedly to track how things evolve over time."&lt;/p&gt;
    &lt;p&gt;To ensure it can always deliver the timely, accurate insights its clients need, Uncover's query engine has to be completely bullet-proof. That's why Georges and his team decided to use Gleam.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solution&lt;/head&gt;
    &lt;p&gt;When Uncover started out in 2020, the company decided to use different programming languages to build the frontend and backend of its new platform. The frontend, which provides the web interface and data visualisation capabilities, was written in Elm, a language that is designed to completely eliminate the types of coding errors that could cause a system to crash. Meanwhile, the backend, which provides the data integration and query processing capabilities, was written in a language that didn't offer the same level of automatic error detection and couldn't provide the same guarantees.&lt;/p&gt;
    &lt;p&gt;"From the beginning, I always really enjoyed working on the Elm part of the codebase," says Georges Boris. "But the backend wasn't such a nice experience. We constantly had bugs, and our error reporting system was logging literally thousands of errors - versus absolutely zero with Elm."&lt;/p&gt;
    &lt;p&gt;He explains: "We wanted an Elm-like experience on the backend too, but Elm is specifically designed to be frontend only. Then, a couple of years ago, we started hearing about Gleam, and we realised that for the first time, there was a backend language that could give us the same assurances and the same level of comfort."&lt;/p&gt;
    &lt;head rend="h3"&gt;Safe and practical&lt;/head&gt;
    &lt;p&gt;Georges started by using Gleam to develop a query parser - a program that can validate and load saved queries into the query engine so that they can be executed. "The parser was full of complex business logic, which made it a good test case for Gleam," explains Georges Boris. "Gleam helps you catch and fix a lot of errors before you even run your program. So once you run it, you have a lot of confidence that your code is correct."&lt;/p&gt;
    &lt;p&gt;At the same time, Georges Boris enjoys the practicality of Gleam's design: "Gleam makes it very easy to interoperate with other languages. That's vital for us because we already had a very large backend codebase, and the new functionality that we are writing in Gleam has to work seamlessly with our existing code. I love the safety that Gleam provides, but I also appreciate the fact that Gleam lets me remove the guardrails when I need to interact with other less-safe languages."&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and reliable&lt;/head&gt;
    &lt;p&gt;Uncover is a relatively young company that operates with a disruptor's mindset - you can't reinvent MMM for the midmarket without being prepared to think differently. Yet while the business itself may be radical, its technology strategy is relatively conservative.&lt;/p&gt;
    &lt;p&gt;"Gleam has been gaining a lot of attention recently, but you can't choose programming languages based on hype," says Georges Boris. "We wouldn't be using Gleam if it wasn't a safe, sensible - almost boring - choice. It's a really simple language to learn, and there are plenty of developers who are interested in this style of programming, so we're not worried about hiring or onboarding. And while Gleam is a relatively new language, it runs on the BEAM - a battle-tested platform that has been regarded as the industry standard for building highly reliable applications since the 1980s. Combined with the safeguards that Gleam provides at a language level, the BEAM really helps us provide the most resilient environment we can for our business-critical web services."&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;As Uncover gradually builds out more of its backend services in Gleam, the company expects to see big improvements in error rates during testing and in production. "So far, the only bugs we've seen in our Gleam codebase are mistakes in business logic - nothing that would actually crash in production," says Georges Boris. "That's a breath of fresh air compared to the rest of our backend codebase, which is constantly throwing errors that Gleam would have caught during development."&lt;/p&gt;
    &lt;p&gt;He adds: "Testing is another areas where Gleam really shines, because the language pushes you to write straightforward tests that don't depend on a database or other external services. We can execute more than 1,900 Gleam tests per second, compared to about 40 per second for the rest of our backend test suite - so the Gleam tests run about 50 times faster."&lt;/p&gt;
    &lt;p&gt;Looking to the future, Uncover is keen to explore how Gleam could help in other areas. "Gleam isn't just a backend language, it can run in the browser too," explains Georges Boris. "That means we could potentially use it as a lingua franca for parts of our business logic that need to run on both our backend servers and in our web application. We're also very impressed by Lustre, a Gleam web framework that takes all the things we like about Elm and adds some exciting new capabilities too. We're eager to contribute to the community and help Gleam's frontend story evolve."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46138984</guid><pubDate>Wed, 03 Dec 2025 19:39:50 +0000</pubDate></item></channel></rss>