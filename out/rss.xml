<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 25 Jan 2026 20:42:25 +0000</lastBuildDate><item><title>Introduction to PostgreSQL Indexes</title><link>https://dlt.github.io/blog/posts/introduction-to-postgresql-indexes/</link><description>&lt;doc fingerprint="791c02166dd40ba4"&gt;
  &lt;main&gt;
    &lt;p&gt;20 minutes&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction to PostgreSQL Indexes&lt;/head&gt;
    &lt;head rend="h2"&gt;Who’s this for&lt;/head&gt;
    &lt;p&gt;This text is for developers that have an intuitive knowledge of what database indexes are, but don’t necessarily know how they work internaly, what are the tradeoffs associated with indexes, what are the types of indexes provided by postgres and how you can use some of its more advanced options to make them more optimized for your use case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basics&lt;/head&gt;
    &lt;p&gt;Indexes are special database objects primarily designed to increase the speed of data access, by allowing the database to read less data from the disk. They can also be used to enforce constraints like primary keys, unique keys and exclusion. Indexes are important for performance but do not speedup a query unless the query matches the columns and data types in the index. Also, as a very rough rule of thumb, an index will only help if less than 15-20% of the table will be returned in the query, otherwise the query planner, a part of postgres used to determine how the query is going to be executed, might prefer a sequential scan. In fact, reality is much more complex than this rule of thumb. The query planner uses statistics and predefined costs associated with each type of scan to do its job, but we’re only going approach the query planner behavior tangentially in this article. So, if your query returns a large percentage of the table, consider refactoring it, using summary tables or other techniques before throwing an index at the problem. With that in mind, let’s give a closer look at how Postgres stores your data in the disk and how indexes help to speedup querying this data.&lt;/p&gt;
    &lt;p&gt;There are six types of indexes available in the default postgres installation and more types available through extensions. Typically, they work by associating a key value with a data location in one or more rows of the table containing that key. Each line is identified by a TID, or tuple id.&lt;/p&gt;
    &lt;head rend="h3"&gt;How data is stored in disk&lt;/head&gt;
    &lt;p&gt;To understand indexes, it is important to first understand how postgres stores table data on disk. Every table in postgres has one or more corresponding files on disk, depending on its size. This set of files is called a heap and it is divided into 8kb pagesh. All table rows, internally referred to as “tuples”, are saved in these files and do not have a specific order. The index is a tree structure that links the indexes columns to the row locators, also known as ctid, in the heap. We’ll zoom into the index internals later.&lt;/p&gt;
    &lt;p&gt;To see the heap files we can use a few postgres internal tables to see where they’re located in the disk. First, we can enter psql and use &lt;code&gt;show data_directory&lt;/code&gt; to show the directory Postgres uses to store databases physical files.&lt;/p&gt;
    &lt;code&gt; show data_directory;

         data_directory          
---------------------------------
 /opt/homebrew/var/postgresql@16&lt;/code&gt;
    &lt;p&gt;Now we can use the internal &lt;code&gt;pg_class&lt;/code&gt; to find the file where the heap table is stored:&lt;/p&gt;
    &lt;code&gt;create table foo (id int, name text);


select oid, datname
from pg_database
where datname = 'my_database';                                                                                

  oid  |         datname        
-------+-------------------------
 71122 | my_database
(1 row)&lt;/code&gt;
    &lt;code&gt;select relfilenode from pg_class where relname = 'foo';                                                                                                  
 relfilenode
-------------
       71123&lt;/code&gt;
    &lt;p&gt;Finally, we can check the file on disk by running this command in the shell (ls $PGDATA/base/&amp;lt;database_oid&amp;gt;/&amp;lt;table_oid&amp;gt;):&lt;/p&gt;
    &lt;code&gt;ls -lrt /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin  0 16 Aug 14:20 /opt/homebrew/var/postgresql@16/base/71122/71123&lt;/code&gt;
    &lt;p&gt;The file has size 0 because we haven’t done any INSERTs in this table yet.&lt;/p&gt;
    &lt;p&gt;Let’s add a couple of rows to our table:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name) values (1, 'Ronaldo');
INSERT 0 1
insert into foo (id, name) values (2, 'Romario');
INSERT 0 1&lt;/code&gt;
    &lt;p&gt;We can add the &lt;code&gt;ctid&lt;/code&gt; field to the query to retrieve the ctid of each line. The ctid is an internal field that has the address of the line in the heap. Think of it as a pointer to the row location in the heap. It consists of a tuple in the format (m, n) where m is the block id and n is the tuple offset. “ctid” stands for “current tuple id”. Here you can note that the row with id one is stored in the page 0, offset 1.&lt;/p&gt;
    &lt;code&gt;select ctid, * from foo;
 ctid  | id |  name   
-------+----+---------
 (0,1) |  1 | Ronaldo
 (0,2) |  2 | Romario
(2 rows)&lt;/code&gt;
    &lt;head rend="h3"&gt;How indexes speedup access to data&lt;/head&gt;
    &lt;p&gt;Let’s add more players to the table so that the total rows is one million:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name);
select generate_series(3, 1000000), 'Player ' || generate_series(3, 1000000);&lt;/code&gt;
    &lt;p&gt;After adding more rows to the table its corresponding file is 30MB. Internally, it is divided into 8kb pages.&lt;/p&gt;
    &lt;code&gt;ls -lrtah /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin    30M 16 Aug 16:32 /opt/homebrew/var/postgresql@16/base/71122/71133&lt;/code&gt;
    &lt;p&gt;When we query a table without an index, Postgres reads all tuples in every page and apply a filter. For example, let’s analyze the command below that searches for rows whose &lt;code&gt;name&lt;/code&gt; column value is equal to “Ronaldo” and show how the database performed this search. We use the explain command with the options &lt;code&gt;(analyse, buffers)&lt;/code&gt;. &lt;code&gt;analyse&lt;/code&gt; will actually execute the query instead of just using cost estimates, and the &lt;code&gt;buffers&lt;/code&gt; option shows how much IO work was done.&lt;/p&gt;
    &lt;code&gt; explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                     QUERY PLAN
---------------------------------------------------------------------------------------------------------------------
 Gather  (cost=1000.00..12577.43 rows=1 width=18) (actual time=0.307..264.991 rows=1 loops=1)
   Workers Planned: 2
   Workers Launched: 2
   Buffers: shared hit=97 read=6272
   -&amp;gt;  Parallel Seq Scan on foo  (cost=0.00..11577.33 rows=1 width=18) (actual time=169.520..256.639 rows=0 loops=3)
         Filter: (name = 'Ronaldo'::text)
         Rows Removed by Filter: 333333
         Buffers: shared hit=97 read=6272
 Planning Time: 0.143 ms
 Execution Time: 265.021 ms&lt;/code&gt;
    &lt;p&gt;Note the in output the line starting with " -&amp;gt; Parallel Seq scan on foo". This line denotes that the database performed a sequential search and read all the rows in the table. The execution time for this query was 265.021ms. Also note the line that says “Buffers: shared hit=97 read=6272”. This mean that we needed to read 97 pages from memory, and 6272 pages from disk.&lt;/p&gt;
    &lt;p&gt;Now let’s add an index on the name column and see how the same query performs. We’re using the command &lt;code&gt;create index concurrently&lt;/code&gt; because we don’t want to block the table for writes.&lt;/p&gt;
    &lt;code&gt;create index concurrently on foo(name);
CREATE INDEX

explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
 Index Scan using foo_name_idx on foo  (cost=0.42..8.44 rows=1 width=18) (actual time=0.047..0.049 rows=1 loops=1)
   Index Cond: (name = 'Ronaldo'::text)
   Buffers: shared hit=4
 Planning Time: 0.129 ms
 Execution Time: 0.077 ms
(5 rows)&lt;/code&gt;
    &lt;p&gt;Here we see that the index was used and that in this case the execution time was reduced from 264.21 to 0.074 milliseconds, and the database only needed to read 4 pages! The reduction in execution time happens because, now, instead of reading all the rows in the table, the database uses the index. The index is a tree structure mapping the value “Ronaldo” to the ctid(s) of the rows that have this value in the &lt;code&gt;name&lt;/code&gt; column (in our example we only have one such row). The ctid is then used to quickly locate these rows on the heap.&lt;/p&gt;
    &lt;p&gt;If we use &lt;code&gt;\di+&lt;/code&gt; to show the indexes in our database we can see that the index we’ve created occupies &lt;code&gt;30MB&lt;/code&gt;, roughly the same size as the &lt;code&gt;foo&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;\di+

                                         List of relations
 Schema |     Name     | Type  | Owner | Table | Persistence | Access method | Size  | Description
--------+--------------+-------+-------+-------+-------------+---------------+-------+-------------
 public | foo_name_idx | index | dlt   | foo   | permanent   | btree         | 30 MB |
(1 row)&lt;/code&gt;
    &lt;head rend="h2"&gt;Costs associated with indexes&lt;/head&gt;
    &lt;p&gt;It is important to highlight that the extra speed brought by indices is associated with several costs that must be considered when deciding where and how to apply them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disk Space&lt;/head&gt;
    &lt;p&gt;Indexes are stored in a separate area of the heap and take up additional disk space. The more indexes a table has, the greater the amount of disk space required to store them. This incurs in additional storage costs for your database and for backups, increased replication traffic, and increased backup and failover recovery times. Bear in mind that its not uncommon for btree indexes to be larger than the table itself. Learning about partial indexes, and multicolumn indexes, as well as about other more space efficient index types such as BRIN can be helpful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write operations&lt;/head&gt;
    &lt;p&gt;Also, there is a maintenance cost in writing operations such as UPDATE, INSERT and DELETE, if a field that is part of an index is modified, the corresponding index needs to be updated, which can add significant overhead to the writing process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Query planner&lt;/head&gt;
    &lt;p&gt;The query planner (also known as query optimizer) is the component responsible for determining the best execution strategy for a query. With more indexes available, the query planner has more options to consider, which can increase the time needed to plan the query, especially in systems with many complex queries or where there are many indexes available.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory usage&lt;/head&gt;
    &lt;p&gt;PostgreSQL maintains a portion of frequently accessed data and index pages in memory in its shared buffers. When an index is used, the relevant index pages are loaded into shared buffers to speed up access. The more indexes you have and the more they are used, the more shared buffer memory is necessary. Since shared buffers are limited and are also used for caching data pages, filling the shared buffers with indexes can lead to less efficient caching of table data. It’s also good to keep in mind that the whole indexed column is copied in every node of the btree, since there’s a limit in node size capacity, the larger the indexed column the deeper the tree will be.&lt;/p&gt;
    &lt;p&gt;Another aspect of memory usage is that PostgreSQL uses work memory when it executes queries that involves sorting or complex index scans (involving multi-column or covering indexes). Larger indexes require more memory for these operations. Also, indexes require memory to store some metadata about their structure, column names and statistics in the system catalog cache. And finally indexes require memory for maintainance operations like vacuuming and reindexing operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types of Indexes&lt;/head&gt;
    &lt;head rend="h3"&gt;Btree&lt;/head&gt;
    &lt;p&gt;The B-Tree is a very powerful data structure, present not only in Postgres but in almost every database management system, since it is a very good general purpose index. It was invented by Rudolf Bayer and Edward M.McCreight while working at Boeing. Nobody really knows if the “B” in B-tree stands for Bayer, Boeing, balanced or better, and it doesn’t really matter. What really matters is that it enables us to search elements in the tree in O(log n) time. If you’re not familiar with Big-O notation, all you need to know is that is is really fast - you only need to make 20 comparisons in order to find an element in a set with 1 million items. Moreover, it can maintain O(log n) time complexity for data sets that are larger than the RAM available on a computer. This means that disks can be used to extend RAM, thanks to the btree efficient prevention of disk page accesses to find the desired data. In PostgreSQL the btree is the most common type of index and its the default, it’s also used to support system and TOAST indexes. Even an empty database has hundreds of btree indexes. It is the only index type that can be used for primary and unique key constraints.&lt;/p&gt;
    &lt;p&gt;In contrast with a binary tree, the BTree is a balanced tree and all of its leave nodes have the same distance from the root. The root nodes and inner nodes have pointers to lower levels, and the leaf nodes have the keys and pointers to the heap. Postgres btrees also have pointers to the left and right nodes for easier forward and backward scanning. Nodes can have multiple keys and these keys are sorted so that it’s easy to walk in ordered directions and to perform ORDER BY and JOIN operations. The values are only stored in the leaf nodes, this makes the tree more compact and facilitates a full traversal of the objects in a tree with just a linear pass through all the leaf nodes. This is just a simplified description of PostgreSQL Btree indexes, if you want to get into the low level details, I suggest you to read the README and the paper that inspired them. Below there’s a simplified illustration of a Postgres Btree.&lt;/p&gt;
    &lt;head rend="h4"&gt;Using multiple indexes&lt;/head&gt;
    &lt;p&gt;Postgres can use multiple indexes to handle cases that cannot be handled by single index scans, by forming &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; conditions across several index scans with the support of bitmaps. The bitmaps are ANDed or ORed together as needed by the query and finally the table rows are visited and returned. Let’s say we have a query like this:&lt;/p&gt;
    &lt;code&gt;select * from users where age = 30 and login_count = 100;&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;login_count&lt;/code&gt; columns are indexed, postgres scans index &lt;code&gt;age&lt;/code&gt; for all pages with &lt;code&gt;age=30&lt;/code&gt; and makes a bitmap where the pages that might contain rows with &lt;code&gt;age=30&lt;/code&gt; are true. In a similar way, it builds a bitmap using the &lt;code&gt;login_count&lt;/code&gt; index. It then ANDs the two bitmaps to form a third bitmap, and performs a table scan, only reading the pages that might contain candidate values, and only adding the rows where &lt;code&gt;age=30 and login_count=100&lt;/code&gt; to the result set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multi-column indexes&lt;/head&gt;
    &lt;p&gt;Multi-column indexes are an alternative for using multiple indexes. They’re generaly going to be smaller and faster than using multiple indexes, but they’ll also be less flexible. That’s because the order of the columns matter, because the database can search for a subset of the indexed columns, as long as they are the leftmost columns. For example, if you have an index on column &lt;code&gt;a&lt;/code&gt; and another index on column &lt;code&gt;b&lt;/code&gt;, these indexes will serve all the of queries below:&lt;/p&gt;
    &lt;code&gt;select * from my_table where a = 42 and b = 420;

select * from my_table where a = 43;

select * from my_table where b = 99;&lt;/code&gt;
    &lt;p&gt;On the other hand, only the first two queries would use an index if you created a multi-column index on (a, b) with a command like &lt;code&gt;create index on my_table(a, b)&lt;/code&gt;; So, when building multi-column indexes choose the order of the columns well so that your index can be used by the most queries possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Partial indexes&lt;/head&gt;
    &lt;p&gt;Partial indexes allow you to use a conditional expression to control what subset of rows will be indexed, this can bring you many benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;your index can be smaller and more likely fit in RAM.&lt;/item&gt;
      &lt;item&gt;your index is shallower, so lookups are quicker&lt;/item&gt;
      &lt;item&gt;less overhead for index/update/delete (but can also mean more overhead if the column you’re using to filter rows in/out of the index is updated very frequently triggering constant index maintenance)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They’re mostly useful in situations where you don’t care about some rows, or when you’re indexing on a column where the proportion of one value is much greater than others. I’ll give two examples below.&lt;/p&gt;
    &lt;head rend="h5"&gt;When you don’t care about some rows&lt;/head&gt;
    &lt;p&gt;Let’s say you have a rules table where the rows can be marked as enabled/disabled, the vast majority of the rows are disabled and in your queries you only care about enabled rows. In this case, you would have a partial index, filtering out the disable rows like this:&lt;/p&gt;
    &lt;code&gt;create index on rules(status) where status = 'enabled';&lt;/code&gt;
    &lt;head rend="h5"&gt;When the distribution of values is skewed&lt;/head&gt;
    &lt;p&gt;Now imagine you’re building a todo application and the status column value can be either &lt;code&gt;TODO&lt;/code&gt;, &lt;code&gt;DOING&lt;/code&gt;, and &lt;code&gt;DONE&lt;/code&gt;. Suppose you have 1M rows and this is the current distribution of rows in each status:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rows&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TODO&lt;/cell&gt;
        &lt;cell&gt;90%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DOING&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DONE&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Since postgres keeps statistics about the distribution of values in your table columns and knows that the vast majority of the rows are in the &lt;code&gt;TODO&lt;/code&gt; status, it would choose to do a sequential scan on the &lt;code&gt;tasks&lt;/code&gt; table when you have &lt;code&gt;status='TODO'&lt;/code&gt; in the &lt;code&gt;WHERE&lt;/code&gt; clause of your query, even if you have an index on status, leaving most part of the index unused and wasting space. In this case, a partial scan such as the one below is recommended:&lt;/p&gt;
    &lt;code&gt;create index on tasks(status) where status &amp;lt;&amp;gt; 'TODO';&lt;/code&gt;
    &lt;head rend="h4"&gt;Covering indexes&lt;/head&gt;
    &lt;p&gt;If you have a query that selects only columns in an index, Postgres has all information needed by the query in the index and doesn’t need to fetch pages from the heap to return the result. This optimization is called &lt;code&gt;index-only scan&lt;/code&gt;. To understand how it works, consider the following scenario:&lt;/p&gt;
    &lt;code&gt;create table bar (a int, b int, c int);
create index abc_idx on bar(a, b);

/* query 1 */
select a, b from bar;

/* query 2 */
select a, b, c from bar;&lt;/code&gt;
    &lt;p&gt;In the first query, postgres can do an index-only scan and avoid fetching data from the heap because the values &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are present in the index. In the second query, since &lt;code&gt;c&lt;/code&gt; isn’t in the index, posgres needs to follow the reference to the heap to fetch its value. In the first query we allowed postgres do to an index-only scan with the help of a multi-column index, but we could also achieve the same result by using a covering index. The syntax for creating a covering index looks like this:&lt;/p&gt;
    &lt;code&gt;create index abc_cov_idx on bar(a, b) including c;&lt;/code&gt;
    &lt;p&gt;This is more space efficient than creating a multi-column index on (a, b, c), because c will only be inserted at the leaf nodes of the btree. Also, we might want to use a covering index in cases where we want an unique index and &lt;code&gt;c&lt;/code&gt; would “break” the uniqueness of the index.&lt;/p&gt;
    &lt;head rend="h4"&gt;Expression indexes&lt;/head&gt;
    &lt;p&gt;Expression indexes to index the result of an expression or function, rather than just the raw column values. This can be extremely useful when you frequently query based on a transformed version of your data. It is necessary if you use a function as part of a where clause as in the example below:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name TEXT
);

CREATE INDEX idx_name ON customers(name);
SELECT * FROM customers WHERE LOWER(name) = 'john doe';&lt;/code&gt;
    &lt;p&gt;In this example above, Postgres won’t use the index because it was was built against the &lt;code&gt;name&lt;/code&gt; column. In order to make it work, the index key has to call the &lt;code&gt;lower&lt;/code&gt; function just like it’s used in the where clase. To fix it, do:&lt;/p&gt;
    &lt;p&gt;Now, when you run a query like this:&lt;/p&gt;
    &lt;code&gt;CREATE INDEX idx_lower_name ON customers (lower(name));&lt;/code&gt;
    &lt;p&gt;Now PostgreSQL can use the expression index to efficiently find the matching rows.&lt;/p&gt;
    &lt;p&gt;Expression indexes can be created using various types of expressions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Built-in functions: Like &lt;code&gt;lower()&lt;/code&gt;,&lt;code&gt;upper()&lt;/code&gt;, etc.&lt;/item&gt;
      &lt;item&gt;User-defined functions: As long as they are immutable.&lt;/item&gt;
      &lt;item&gt;String concatenations: Like &lt;code&gt;first_name || ' ' || last_name&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hash&lt;/head&gt;
    &lt;p&gt;The hash index differs from B-Tree in strucutre, it is much more alike a hashmap data structure present in most programming languages (e.g. dict in Python, array in php, HashMap in java, etc). Instead of adding the full column value to the index, a 32bit hash code is derived from it and added to the hash. This makes hash indexes much smaller than btrees when indexing longer data such as UUIDs, URLs, etc. Any data type can be indexed with the help of postgres hashing functions. If you type &lt;code&gt;\df hash*&lt;/code&gt; and press TAB in psql, you’ll see that there are more then 50 hash related functions. Although it gracefully handles hash conflicts, it works better for even distribution of hash values and is most suited to unique or mostly unique data. Under the correct conditions it will not only be smaller than btree indexes, but also it will be faster for reads when compared with btress. Here’s what the official docs says about it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“In a B-tree index, searches must descend through the tree until the leaf page is found. In tables with millions of rows, this descent can increase access time to data. The equivalent of a leaf page in a hash index is referred to as a bucket page. In contrast, a hash index allows accessing the bucket pages directly, thereby potentially reducing index access time in larger tables. This reduction in “logical I/O” becomes even more pronounced on indexes/data larger than shared_buffers/RAM.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As for its limitations, it only supports equality operations and isn’t going to be helpful if you need to order by the indexed field. It also doesn’t support multi-column indexes and checking for uniqueness. For a in-depth analysis of how hash indexes fare in relation to btree, check Evgeniy Demin’s blog post on the subject.&lt;/p&gt;
    &lt;head rend="h3"&gt;BRIN&lt;/head&gt;
    &lt;p&gt;BRIN stands for Block Range Index and its name tells a lot about how it is implemented. Nodes in BRIN indexes store the minimum and maximum values of a range of values present in the page referred by the index. This makes the index more compact and cache friendly, but restricts the use cases for it. If you have a very large in a work load that is heavy on writes and low on deletes and updates. You can think of a BRIN index as an optimizer for sequential scans of large amounts of data in very large databases, and is a good optimization to try before partitioning a table. For a BRIN index to work well, the index key should be a column that strongly correlates to the location of the row in the heap.Some good use cases for BRIN are append-only tables and tables storing time series data.&lt;/p&gt;
    &lt;p&gt;BRIN won’t work well for tables where the rows are updated constantly, due to the nature of MVCC that duplicates rows and stores them in a different part of the heap. This tuple duplication and moving affect the correlation negatively and reduces the effectiveness of the index. Using extensions such as pg_repack or pg_squeeze isn’t recommended for tables that use BRIN indexes, since they change the internal data layour fo the table and mess up the correlation. Also, this index is lossy in the sense that the index leaf nodes point to pages taht might contain a value within a particular range. For this reason a BRIN is more helpful if you need to return large subset of data, and a btree would be more read performant for queries that only return one or few rows. You can make the index more or less lossy by adjusting the &lt;code&gt;page_per_range&lt;/code&gt; configuration, the trade off will be index size.&lt;/p&gt;
    &lt;head rend="h3"&gt;GIN&lt;/head&gt;
    &lt;p&gt;Generalized inverted index is appropriate for when you want to search for an item in composite data, such as finding a word in a blob of text, an item in an array or an object in a JSON. The GIN is generalized in the sense that it doesn’t need to know how it will acelerate the search for some item. Instead, there’s a set of custom strategies specific for each data type. Please note that in order to index an JSON value it needs to be stored in a JSONB column. Similarly, if you’re indexing text it’s better to store it as (or convert it to) tsvector or use the pg_trgm extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;GiST &amp;amp; SP-GiST&lt;/head&gt;
    &lt;p&gt;The Generalized Search Tree and the Space-Partitioned Generalized Search Tree are tree structures that can be use as a base template to implement indexes for specific data types. You can think of them as framework for building indexes. The GiST is a balanced tree and the SP-GiST allow for the development of non-balanced data structures. They are useful for indexing points and geometric types, inet, ranges and text vectors. You can find an extensive list of the built-in strategies shipped with postgres in the official documentation. If you need an index to enable full-text search in your application, you’ll have to choose between GIN and GiST. Roughly speaking, GIN is faster for lookups but it’s bigger and has greater building and maintainance costs. So the right index type for you will depend on your application requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Understanding and effectively using indexes is crucial for optimizing database performance in PostgreSQL. While indexes can greatly speed up query execution and improve overall efficiency, it’s important to be mindful of their impact on write operations and storage. By carefully selecting the appropriate types of indexes based on your specific use cases you can ensure that your PostgreSQL database remains both fast and efficient. I hope this article taught you at least one or two things you didn’t know about Postgres indexes, and that you’re better equiped to deal with different scenarios involving databases from now on.&lt;/p&gt;
    &lt;p&gt;4119 Words&lt;/p&gt;
    &lt;p&gt;2024-09-11 08:07&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751826</guid><pubDate>Sun, 25 Jan 2026 08:07:03 +0000</pubDate></item><item><title>Sony Data Discman</title><link>https://huguesjohnson.com/random/sony-ebook/</link><description>&lt;doc fingerprint="bb65194d5c9f2bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;Back in 1992 I worked at an Electronics Boutique that was an outlet location for the company. We sold regular merchandise but also had an outlet section for clearance stuff aggregated from other stores. This thoroughly confused customers who expected every item in the store to be discounted. The job involved a lot of explaining "no I'm sorry this game that literally launched today is not on clearance". Working retail is a great way to lose faith in the collective intelligence of our species.&lt;/p&gt;
    &lt;p&gt;One day we received several Sony Data Discman Electronic Book Player DD-1EX players that we were supposed to clear out. The original sticker price was $500 but they were marked down to roughly 1% of that. Largely out of curiosity I picked one up along with whatever software we had for it (also at a massive discount). I can't say I've used it for more than an hour. It's a very nice device that serves no practical or entertainment function whatsoever.&lt;/p&gt;
    &lt;p&gt;Using old catalogs as a reference, these were originally listed in the spring 1992 catalog. Here it is:&lt;/p&gt;
    &lt;p&gt;These did not appear in the summer 1992 catalog just a couple months later. Since I started working there during the 1992 holiday season the timeline works out. They must have hit the shelves in early 1992, not sold, then been marked down every month until they were rounded-up and shipped to our location.&lt;/p&gt;
    &lt;p&gt;Let's take a peek at it..&lt;/p&gt;
    &lt;p&gt;Gallery&lt;/p&gt;
    &lt;p&gt;Disclaimer: I am terrible at taking pictures.&lt;/p&gt;
    &lt;p&gt;Sony didn't nickel-and-dime consumers on accessories here. The package came with: the reader (duh), AC adapter, rechargeable battery, and another battery pack that holds AAs. Years later they refused to include an AC adapter in the PlayStation Classic.&lt;/p&gt;
    &lt;p&gt;The reader itself is fairly nice looking. It feels like a miniature laptop. It's a tad on the heavy side but also feels extremely durable. Looking at all the buttons and size of the screen makes me think this had a lot of potential beyond just electronic books. However, it lacks any mechanism to save data. In the early 90s it's not like SD-RAM cards were available. Miniature hard drive? Forget it. It has 90% of what it needs to be a PDA but the technology just wasn't there to get the last 10% in.&lt;/p&gt;
    &lt;p&gt;There's a QWERTY keyboard because all of the books are searchable. The directional pad is there to navigate through menus. Looking at it again just makes me irritated that I don't have any games for this (of course I doubt any were made). This would make a cool little text adventure player.&lt;/p&gt;
    &lt;p&gt;The electronic books are mini CDs in a caddy. I guess that means we can rip them (more on this soon).&lt;/p&gt;
    &lt;p&gt;Bad Screenshots&lt;/p&gt;
    &lt;p&gt;I picked up every electronic book we had in stock. The player has an output jack than can be connected to anything with an A/V input (well, just the "V" part is needed). These screenshots are from the A/V out.&lt;/p&gt;
    &lt;p&gt;The splash screen reminding you that this is for private use only. I guess I'm technically violating that, whatever.&lt;/p&gt;
    &lt;p&gt;Although I don't know the exact date this electronic book reader was produced, the bundled encyclopedia gives some hints. It still lists U.S.S.R. as a country so it had to be authored prior to Christmas day 1991.&lt;/p&gt;
    &lt;p&gt;Early 90s software developer salary in the career guide:&lt;/p&gt;
    &lt;p&gt;Thinking of traveling the world? Well, this handy translator is all you need. Someone once told me that if you ever got lost in a strange foreign country you should claim to be a Swedish citizen. Something about Sweden having an embassy in every country and nobody holding a grudge against them. I couldn't find a translation for "I'm a Swedish citizen please don't turn me over to the secret police" in this guide.&lt;/p&gt;
    &lt;p&gt;The least useful book (to me at least) is the crossword dictionary. You can search for word endings or a list of complete words but that's it.&lt;/p&gt;
    &lt;p&gt;The wellness encyclopedia is the perfect gift for a hypochondriac.&lt;/p&gt;
    &lt;p&gt;Since I won't pay more than $3.99 for a bottle of wine I found this guide relatively useless.&lt;/p&gt;
    &lt;p&gt;Ripping the CDs&lt;/p&gt;
    &lt;p&gt;If you rip the CDs you'll find that some of them contain an emulator for the Discman. Here's the wine guide main screen:&lt;/p&gt;
    &lt;p&gt;It seems like the books are fully functional in this emulator:&lt;/p&gt;
    &lt;p&gt;The career guide also comes with an emulator. You can use it to look for jobs that didn't exist in 1990 I guess:&lt;/p&gt;
    &lt;p&gt;Some CDs, like the encyclopedia, don't have the emulator bundled. However, if you copy the data files around it's trivial to launch it in the emulator bundled on the other CDs:&lt;/p&gt;
    &lt;p&gt;iso Downloads&lt;/p&gt;
    &lt;p&gt;Grab these before I receive a takedown notice.. I mean, these were completely obsolete before Wikipedia existed and are even worse after. I doubt that will stop anyone though. One of the reasons I deleted my YouTube videos was takedown notices from Sony over the intro to Dragon's Lair. Some rapper who sold &amp;lt;100 albums, but is apparently signed with Sony, sampled the intro of Dragon's Lair. In Sony's mind that means they own all rights to it. I don't know how you rap over the Dragon's Lair intro and I don't care to learn. EA also sent me a takedown notice over a Dragon's Lair video, a game they neither wrote nor own the rights to. As far as I can tell they at some point were the distributor for an early iOS version of Dragon's Lair that is no longer available. So to make a long story short, I'm sure these will be offline soon. When that happens I'll try moving them to archive.org since they are somehow able to get away with posting anything.&lt;/p&gt;
    &lt;p&gt;Career encyclopedia (SROM30_VERSION1)&lt;/p&gt;
    &lt;p&gt;Crossword dictionary (SROM13V1_07D)&lt;/p&gt;
    &lt;p&gt;Wellness encyclopedia (SROM25_V1_0)&lt;/p&gt;
    &lt;p&gt;World translator (SROM29_VERSION1)&lt;/p&gt;
    &lt;p&gt;Usual disclaimer that you are downloading isos with executable files from a total rando's site. I am 100% not responsible for any awful thing that happens if you download these and run the files on them.&lt;/p&gt;
    &lt;p&gt;Related&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751906</guid><pubDate>Sun, 25 Jan 2026 08:23:51 +0000</pubDate></item><item><title>A flawed paper in Management Science has been cited more than 6,000 times</title><link>https://statmodeling.stat.columbia.edu/2026/01/22/aking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752151</guid><pubDate>Sun, 25 Jan 2026 09:04:30 +0000</pubDate></item><item><title>Jurassic Park - Tablet device on Nedry's desk? (2012)</title><link>https://www.therpf.com/forums/threads/jurassic-park-tablet-device-on-nedrys-desk.169883/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752261</guid><pubDate>Sun, 25 Jan 2026 09:22:17 +0000</pubDate></item><item><title>Bridging the Gap Between PLECS and SPICE</title><link>https://erickschulz.dev/posts/plecs-spice/</link><description>&lt;doc fingerprint="351921528b158f82"&gt;
  &lt;main&gt;
    &lt;p&gt;Three years ago, we set out to bring SPICE simulation into PLECS. PLECS Spice is finally here.&lt;/p&gt;
    &lt;p&gt;PLECS Spice brings SPICE device-level simulation directly into PLECS. Available with PLECS 5.0, both system-level and device-level analysis can be performed within a single tool, eliminating the need to maintain duplicate models across separate softwares.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate Tools, Duplicate Work&lt;/head&gt;
    &lt;p&gt;Power electronics design has long faced a fundamental trade-off: system-level simulation tools deliver the speed and robustness needed for controller development and overall system analysis, but sacrifice the device-level detail necessary to validate component selection before procurement.&lt;/p&gt;
    &lt;p&gt;For over 20 years, Plexim has promoted a top-down design philosophy, enabling engineers to model complete power electronic systems using ideal switches and behavioral components. By avoiding the computational burden of simulating detailed switching transients, PLECS enables rapid validation of system-level requirements like efficiency, control performance and thermal behavior.&lt;/p&gt;
    &lt;p&gt;Conversely, traditional SPICE simulators embody an inherently bottom-up approach. They excel at validating device-level requirements through detailed semiconductor models, capturing switching losses, voltage overshoots and parasitic effects with high fidelity. This comes at a cost: system-level integration becomes computationally prohibitive.&lt;/p&gt;
    &lt;p&gt;This divide has forced engineers into parallel workflows using separate software platforms with different modeling approaches and incompatible component libraries. Moving from a system-level PLECS model to SPICE for device validation requires recreating the model, an error-prone and time-consuming process.&lt;/p&gt;
    &lt;head rend="h2"&gt;PLECS Spice&lt;/head&gt;
    &lt;p&gt;To solve this problem, Plexim has developed PLECS Spice, an extension that brings SPICE device-level simulation capabilities directly into PLECS. PLECS Spice can simulate hybrid systems containing both standard PLECS and SPICE circuits. This allows a schematic to be progressively refined by replacing the ideal switches in a circuit of interest, like the power stage, with detailed SPICE netlists. Controls and other subsystems can remain unchanged. Because the entire workflow stays within PLECS, engineers can easily toggle between ideal and detailed configurations to compare results. This creates a true top-down workflow where device-level detail is added selectively, only where needed. With PLECS Spice, there is no longer a need to build the same model twice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the Hood&lt;/head&gt;
    &lt;p&gt;The PLECS Spice extension adds four key ingredients that transform PLECS into a fully-featured hybrid simulation platform that can simulate standard PLECS and SPICE models together.&lt;/p&gt;
    &lt;head rend="h3"&gt;Netlist Parser&lt;/head&gt;
    &lt;p&gt;SPICE models are typically distributed as netlists. Simply put, these are text files that describe a circuit topology, component interconnections and parameter values. A key capability of PLECS Spice is its parser’s support for multiple netlist dialects. Different SPICE implementations use distinct syntax conventions, making netlists from various vendors incompatible. The PLECS Spice parser handles these variations automatically, enabling engineers to integrate models provided by different semiconductor manufacturers directly into their schematics. Little to no manual conversion or syntax adaptation is needed, regardless of the dialect.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compact Models&lt;/head&gt;
    &lt;p&gt;Netlists provided by manufacturers often rely on well-established semiconductor device models. These compact models combine physics-based modeling with empirical corrections to capture fundamental electrical behavior while maintaining reasonable complexity. PLECS Spice includes optimized implementations of compact models such as diodes, MOSFETs, BJTs, and switches. Each model defines a set of parameters that can be tuned to match the electrical response of specific physical devices. In PLECS Spice, classical compact models have been improved to guarantee continuity of key physical quantities, enhancing numerical stability. By tightly integrating these models into the solver, PLECS Spice achieves both computational efficiency and robust convergence even in the presence of highly nonlinear semiconductor characteristics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modified Nodal Analysis&lt;/head&gt;
    &lt;p&gt;Standard PLECS uses piecewise state-space equations to simulate electrical models. This approach is computationally efficient for circuits with mostly linear components but struggles with the strong nonlinearities present in detailed semiconductor models. To handle these nonlinearities, SPICE uses Modified Nodal Analysis (MNA), a formulation that produces differential algebraic equations (DAEs).&lt;/p&gt;
    &lt;p&gt;MNA constructs the circuit equations by applying Kirchhoff’s current law at each node and substituting component branch equations. Energy storage elements introduce differential equations, while the network topology and sources introduce algebraic constraints. The result is a coupled system where nodal voltages, source currents, and energy storage currents must satisfy both differential and algebraic equations simultaneously. This integrated treatment of constraints and dynamics is what makes MNA particularly robust for nonlinear semiconductor models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mixed-Formulation Solver&lt;/head&gt;
    &lt;p&gt;PLECS Spice employs third-order implicit Runge-Kutta methods augmented with circuit-tailored convergence helpers to solve the DAEs produced by MNA. These one-step methods have a crucial advantage for mixed-signal schematics that contain both SPICE and standard PLECS electrical circuits: they are inherently self-starting. In other words, they do not rely on information from previous time steps. When events such as topology changes or zero-crossings occur, the solver must compute the next time step using only the current state. This self-starting property makes one-step methods particularly well-suited for hybrid systems with frequent discontinuities.&lt;/p&gt;
    &lt;p&gt;The solver can simulate complex systems that combine standard PLECS and SPICE models in a single schematic. The only rule is that when an electrical circuit contains a netlist, it must be solved using MNA, and therefore all its components must be compatible with SPICE. But other electrical circuits can remain in the standard PLECS formulation. Circuits of different types connect through the control domain using sources and meters. This enables a powerful top-down workflow: engineers can refine specific circuits of interest by converting them to SPICE netlists while keeping other subsystems and controls unchanged in standard PLECS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Application Example&lt;/head&gt;
    &lt;p&gt;Mixed-signal simulation is particularly valuable when control strategies and device physics must be considered together. The soft switching operation of a Dual Active Bridge (DAB) converter, whose analysis requires taking into consideration both controls and circuit design aspects, serves as a perfect case study for the workflow enabled by PLECS Spice.&lt;/p&gt;
    &lt;p&gt;A DAB is a bidirectional DC-DC topology comprising identical primary and secondary bridges (typically full bridges) separated by a high-frequency transformer and an energy transfer inductance (representing leakage plus external inductance). It is widely employed in high-power, high-density applications requiring bidirectional power flow between two galvanically isolated sides, such as EV chargers and energy storage systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Soft Switching Challenge&lt;/head&gt;
    &lt;p&gt;Magnetic components are often the primary limitation to increasing power density. Their size can be reduced by increasing switching frequency. State-of-the-art designs have reached the hundreds of kHz range. However, at these frequencies, switching losses represent a significant part of the overall converter losses. Without careful design, the volume advantage of a smaller transformer could be negated by the increased size needed of the cooling system.&lt;/p&gt;
    &lt;p&gt;To resolve this dilemma, soft switching offers a compelling solution. Given the high switching frequencies, MOSFETs are the standard choice for modern DABs. However, their dominant loss mechanism stems from the charge stored in the parasitic output capacitance (). When the device blocks voltage, this capacitance stores the energy&lt;/p&gt;
    &lt;p&gt;which depends on the drain-source voltage. When a MOSFET is turned on, the stored charge must be evacuated. In hard switching, the closing channel effectively shorts the capacitance, dissipating the stored energy as heat within the semiconductor. At high frequencies, this thermal penalty becomes unsustainable.&lt;/p&gt;
    &lt;p&gt;Here, the DAB offers a distinct advantage. In a full-bridge topology, each leg contains a top and bottom switch that operate complementarily: when one conducts, the other blocks. In practice, a short interval called dead time is introduced between turning off one switch and turning on its complement. Its primary role is to prevent a short circuit across the DC link, but a DAB can also exploit this interval of time for soft switching. During dead time, the inductor current continues to flow. With both switches off, the only path available is through the parasitic capacitances. This discharges the output capacitance of the incoming MOSFET (the switch about to turn on), causing its drain-source voltage to fall. If the dead time is sufficient, reaches zero before the gate signal arrives. The soft switching challenge lies in properly tuning this interval.&lt;/p&gt;
    &lt;p&gt;To achieve such Zero Voltage Switching (ZVS), the relationship between the device output capacitance, the resonant path and the gate drive, must be carefully adjusted. Crucially, these hardware choices cannot be made in isolation. They must inform the control design. This is because robust ZVS depends on several dynamic factors: the converter’s operating point (voltage and power), the gate driving scheme (specifically the dead times) and the degrees of freedom utilized by the modulation strategy.&lt;/p&gt;
    &lt;p&gt;Standard PLECS simulations allow for precise tuning of the operating point within the ZVS region, represented by the blue area in the ZVS range figure. However, because ideal switches are inherently hard switching, they do not simulate the transients required for a detailed analysis of ZVS. The effects of using two different dead times are compared in the figures above. In both tests, the low side gate signal turns off a conducting MOSFET. After the dead time, the complementary MOSFET is turned on by the high side gate signal. In the first experiment, a dead time of 15 ns is used between the two events. In the second, it is set to 50 ns. Yet, the resulting voltage and current waveforms show no visible response to this parameter change.&lt;/p&gt;
    &lt;p&gt;In this case, the ideal model fails to indicate whether the timing achieves soft switching or leads to hard switching transients. The reason is that ideal switches lack parasitic capacitances. Without , the antiparallel diode of the complementary switch conducts immediately, making the simulated waveforms insensitive to the dead time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validating ZVS with Device-Level Detail&lt;/head&gt;
    &lt;p&gt;PLECS Spice enables precisely the analysis that ideal models cannot provide. Using configurable subsystems, engineers can add a detailed SPICE configuration alongside the ideal model, allowing them to toggle between fast system-level analysis and high-fidelity device validation without modifying the circuit topology or control logic.&lt;/p&gt;
    &lt;p&gt;The ideal switch configuration shown in the first figure consists of a standard PLECS MOSFET with antiparallel diode, driven directly by a control signal. The detailed configuration in the second figure provides a device-level description. It uses manufacturer-provided MOSFET and diode netlists that capture parasitic capacitances and charge dynamics. The control signal is converted to a gate-source voltage through a controlled voltage source. Separate on and off gate resistances ( and ) control switching speed. Engineers can switch between one configuration to the other between two simulations, while the control logic, operating point and all other subsystems remain unchanged.&lt;/p&gt;
    &lt;p&gt;With detailed device models in place, the impact of dead time on ZVS becomes immediately visible. The figure below shows the switching transient with a 15 ns dead time. The drain-source voltage remains high when the gate signal is applied, and only begins falling as channel current rises. This overlap between voltage and current is the signature of hard switching: the channel conducts before the output capacitance fully discharges, dissipating the stored energy as heat in the semiconductor. The insufficient dead time prevents the resonant discharge mechanism from completing.&lt;/p&gt;
    &lt;p&gt;By contrast, the second figure demonstrates successful ZVS with a 50 ns dead time. Here, completes its resonant transition to zero before the gate signal arrives. The channel opens with zero voltage across it, eliminating capacitive turn-on losses. Channel current then begins to flow, carrying the inductor current through the device. This extended dead time provides a sufficient interval for the inductor current to transfer energy from the MOSFET’s output capacitance, fulfilling the conditions for soft switching.&lt;/p&gt;
    &lt;p&gt;This example demonstrates how PLECS Spice enables validation of ZVS by bringing together three tightly coupled aspects within a single model. The control strategy establishes the operating point and determines the available inductor current for resonant transitions. Gate drive timing sets the window for capacitor discharge. The device physics, captured in the SPICE netlist, determines how quickly that discharge occurs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;PLECS Spice marks a significant step towards a unified power electronics design workflow. By integrating SPICE simulation directly into the PLECS environment, engineers no longer need to choose between system-level insights and device-level accuracy. The ability to seamlessly transition between ideal and detailed models within a single schematic eliminates the redundant and error-prone process of rebuilding circuits in separate tools. This empowers engineers to adopt a true top-down design philosophy, starting with a system-level view and progressively adding detail where it matters most. As power electronic systems grow in complexity, this unified approach will be crucial for accelerating innovation and reducing time-to-market.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752841</guid><pubDate>Sun, 25 Jan 2026 10:44:08 +0000</pubDate></item><item><title>Show HN: TUI for managing XDG default applications</title><link>https://github.com/mitjafelicijan/xdgctl</link><description>&lt;doc fingerprint="fe9cba7b7ba6b9a3"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;xdgctl&lt;/code&gt; is a TUI for managing XDG default applications. View and set defaults for file categories without using &lt;code&gt;xdg-mime&lt;/code&gt; directly.&lt;/p&gt;
    &lt;p&gt;Built with C using GLib/GIO and termbox2.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;xdgctl.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse by category (Browsers, Text Editors, etc.)&lt;/item&gt;
      &lt;item&gt;Current default marked with &lt;code&gt;*&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Up/Down&lt;/cell&gt;
        &lt;cell&gt;Navigate through categories or applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Right/Tab&lt;/cell&gt;
        &lt;cell&gt;Switch from category list to application list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Left&lt;/cell&gt;
        &lt;cell&gt;Switch back to category list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Enter&lt;/cell&gt;
        &lt;cell&gt;Set selected application as default for current category&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Esc / q&lt;/cell&gt;
        &lt;cell&gt;Quit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To build &lt;code&gt;xdgctl&lt;/code&gt;, you need the following development libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;glib-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-unix-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clang&lt;/code&gt;or&lt;code&gt;gcc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# On Void Linux
sudo xbps-install glibc-devel&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/mitjafelicijan/xdgctl.git
cd xdgctl

# Build
make
sudo make install

# Using prefix
sudo make PREFIX=/usr/local install
make PREFIX=~/.local install&lt;/code&gt;
    &lt;p&gt;If you manually add new applications to your &lt;code&gt;~/.local/share/applications&lt;/code&gt; directory, you might need to run &lt;code&gt;update-desktop-database&lt;/code&gt; again.&lt;/p&gt;
    &lt;code&gt;ls /usr/share/applications
ls ~/.local/share/applications&lt;/code&gt;
    &lt;code&gt;xdg-mime query default text/plain
xdg-mime query default text/html
xdg-mime query default x-scheme-handler/http
xdg-mime query default x-scheme-handler/https
xdg-mime query default inode/directory&lt;/code&gt;
    &lt;code&gt;xdg-mime default brave.desktop x-scheme-handler/http
xdg-mime default brave.desktop x-scheme-handler/https&lt;/code&gt;
    &lt;code&gt;# ~/.local/share/applications/brave.desktop
[Desktop Entry]
Exec=/home/m/Applications/brave
Type=Application
Categories=Applications
Name=Brave Browser
MimeType=text/html;text/xml;application/xhtml+xml;x-scheme-handler/http;x-scheme-handler/https;&lt;/code&gt;
    &lt;code&gt;update-desktop-database ~/.local/share/applications
less ~/.config/mimeapps.list
less /usr/share/applications/mimeapps.list&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753078</guid><pubDate>Sun, 25 Jan 2026 11:19:04 +0000</pubDate></item><item><title>Show HN: Bonsplit – Tabs and splits for native macOS apps</title><link>https://bonsplit.alasdairmonk.com</link><description>&lt;doc fingerprint="8d0973528a96f1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsplit is a custom tab bar and layout split library for macOS apps. Enjoy out of the box 120fps animations, drag-and-drop reordering, SwiftUI support &amp;amp; keyboard navigation.&lt;/p&gt;
    &lt;quote&gt;.package(url: "https://github.com/almonk/bonsplit.git", from: "1.0.0")&lt;/quote&gt;
    &lt;p&gt;### Features&lt;/p&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;### Read this, agents...&lt;/p&gt;
    &lt;p&gt;Complete reference for all Bonsplit classes, methods, and configuration options.&lt;/p&gt;
    &lt;p&gt;The main controller for managing tabs and panes. Create an instance and pass it to BonsplitView.&lt;/p&gt;
    &lt;p&gt;Implement this protocol to receive callbacks about tab bar events. All methods have default implementations and are optional.&lt;/p&gt;
    &lt;p&gt;Configure behavior and appearance. Pass to BonsplitController on initialization.&lt;/p&gt;
    &lt;code&gt;allowSplits&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable split buttons and drag-to-split&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseTabs&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show close buttons on tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseLastPane&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Allow closing the last remaining pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;false&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowTabReordering&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable drag-to-reorder tabs within a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCrossPaneTabMove&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable moving tabs between panes via drag&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;autoCloseEmptyPanes&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Automatically close panes when their last tab is closed&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;contentViewLifecycle&lt;/code&gt;
    &lt;code&gt;ContentViewLifecycle&lt;/code&gt;
    &lt;p&gt;How tab content views are managed when switching tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.recreateOnSwitch&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;newTabPosition&lt;/code&gt;
    &lt;code&gt;NewTabPosition&lt;/code&gt;
    &lt;p&gt;Where new tabs are inserted in the tab list&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.current&lt;/code&gt;&lt;/p&gt;
    &lt;quote&gt;let config = BonsplitConfiguration(allowSplits: true,allowCloseTabs: true,allowCloseLastPane: false,autoCloseEmptyPanes: true,contentViewLifecycle: .keepAllAlive,newTabPosition: .current)let controller = BonsplitController(configuration: config)&lt;/quote&gt;
    &lt;p&gt;Controls how tab content views are managed when switching between tabs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell role="head"&gt;State&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;.recreateOnSwitch&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;Simple content&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.keepAllAlive&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;Full&lt;/cell&gt;
        &lt;cell&gt;Complex views, forms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Controls where new tabs are inserted in the tab list.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.current&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Insert after currently focused tab, or at end if none&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.end&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always insert at the end of the tab list&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;tabBarHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Height of the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;33&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMinWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;140&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMaxWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Maximum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;220&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabSpacing&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Spacing between tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum height of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;showSplitButtons&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show split buttons in the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;animationDuration&lt;/code&gt;
    &lt;code&gt;Double&lt;/code&gt;
    &lt;p&gt;Duration of animations in seconds&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0.15&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;enableAnimations&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable or disable all animations&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;.default&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Default configuration with all features enabled&lt;/p&gt;
    &lt;code&gt;.singlePane&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Single pane mode with splits disabled&lt;/p&gt;
    &lt;code&gt;.readOnly&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Read-only mode with all modifications disabled&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753301</guid><pubDate>Sun, 25 Jan 2026 11:56:42 +0000</pubDate></item><item><title>Nango (YC W23, Dev Infrastructure) Is Hiring Remotely</title><link>https://jobs.ashbyhq.com/Nango</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753336</guid><pubDate>Sun, 25 Jan 2026 12:02:01 +0000</pubDate></item><item><title>Doom has been ported to an earbud</title><link>https://doombuds.com</link><description>&lt;doc fingerprint="a6567235022113ff"&gt;
  &lt;main&gt;
    &lt;p&gt;It's almost your turn, get ready!&lt;/p&gt;
    &lt;p&gt;Player queue&lt;/p&gt;
    &lt;p&gt;Your position&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Players queued&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Wait time&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;You know the 1993 classic DOOM? I made it run on an earbud, then I connected it to the internet and made it possible for visitors like you to sit in a queue for hours play the game remotely!.&lt;/p&gt;
    &lt;p&gt;Yeah but it won't just run on any old earbud, this only works with the Pinebuds Pro, the only earbuds with open source firmware.&lt;/p&gt;
    &lt;p&gt;You sure can! There are two relevant repos:&lt;/p&gt;
    &lt;p&gt;This was a necessary optimisation to avoid paying outgoing bandwidth fees, once you're 5th in the queue, the twitch player will switch to a low-latency MJPEG stream.&lt;/p&gt;
    &lt;p&gt;shhhh don't look don't look it's ok just join the queue&lt;/p&gt;
    &lt;p&gt;Let's switch to a more readable font first.&lt;/p&gt;
    &lt;p&gt; I'll put out an article / video diving deeper into this later, but here are a few bits of info:&lt;lb/&gt; This project is made up of four parts: &lt;/p&gt;
    &lt;p&gt;The firmware pushes up against a few hardware limitations:&lt;/p&gt;
    &lt;p&gt; Earbuds don't have displays, so the only way to transfer data to/from them is either via bluetooth, or the UART contact pads.&lt;lb/&gt; Bluetooth is pretty slow, you'd be lucky to get a consistent 1mbps connection, UART is easily the better option.&lt;lb/&gt; DOOM's framebuffer is (width * height) bytes, 320 * 200 = 96kB. (doom's internal framebuffer is 8-bit not 24-bit)&lt;lb/&gt; The UART connection provides us with 2.4mbps of usable bandwidth. 2,400,000 / 8 / 96,000 gives us... 3 frames per second.&lt;lb/&gt; Clearly we need to compress the video stream. Modern video codecs like h264 consume way too much CPU and RAM.&lt;lb/&gt; The only feasible approach is sending the video as an MJPEG stream. MJPEG is a stream of JPEG images shown one after the other.&lt;lb/&gt; I found an excellent JPEG encoder for embedded devices here, thanks Larry!&lt;lb/&gt; A conservative estimate for the average HIGH quality JPEG frame is around 13.5KB, but most scenes (without enemies) are around 11kb.&lt;lb/&gt; Theoretical maximum FPS:&lt;lb/&gt; - Optimistic: `2,400,000 / (11,000 * 8)` = 27.3 FPS&lt;lb/&gt; - Conservative: `2,400,000 / (13,500 * 8)` = 22.2 FPS &lt;/p&gt;
    &lt;p&gt; The stock open source firmware has the CPU set to 100mhz, so I cranked that up to 300mhz and disabled low power mode.&lt;lb/&gt; The Cortex-M4F running at 300mhz is actually more than enough for DOOM, however it struggles with JPEG encoding.&lt;lb/&gt; This is why it maxes out at ~18fps, I don't think there's much else I can do to speed it up. &lt;/p&gt;
    &lt;p&gt; By default, we only have access to 768KB of RAM, after disabling the co-processor it gets bumped up to the advertised 992KB.&lt;lb/&gt; DOOM requires 4MB of RAM, though there are plenty of optimisations that can reduce this amount.&lt;lb/&gt; Pre-generating lookup tables, making variables const, reading const variables from flash, disabling DOOM's caching system, removing unneeded variables. It all adds up! &lt;/p&gt;
    &lt;p&gt; The shareware DOOM 1 wad (assets file) is 4.2MB and the earbuds can only store 4MB of data.&lt;lb/&gt; Thankfully, fragglet, a well-known doom modder, has already solved this issue for me.&lt;lb/&gt; Squashware is his trimmed-down DOOM 1 wad that is only 1.7MB in size.&lt;lb/&gt; With this wad file, everything comfortably fits in flash. &lt;/p&gt;
    &lt;p&gt;I thought you'd never ask! (please hire me)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753484</guid><pubDate>Sun, 25 Jan 2026 12:22:12 +0000</pubDate></item><item><title>Alarm overload is undermining safety at sea as crews face thousands of alerts</title><link>https://www.lr.org/en/knowledge/press-room/press-listing/press-release/2026/alarm-overload-is-undermining-safety-at-sea-as-new-research-shows-crews-face-tens-of-thousands-of-daily-alerts/</link><description>&lt;doc fingerprint="3452211c23d1e7ad"&gt;
  &lt;main&gt;
    &lt;p&gt;New research from Lloyd’s Register (LR) has revealed that excessive and nuisance shipboard alarm systems are routinely overwhelming crews and, in many cases, actively undermining safety at sea.&lt;/p&gt;
    &lt;p&gt;The findings, published today in Effective Alarm Management in the Maritime Industry are based on data collected from 11 operational vessels, spanning over 2,000 days and more than 40 million alarm-related events.&lt;/p&gt;
    &lt;p&gt;The study shows that many ships generate thousands of alarms every day, many of which provide little or no operational value. The result is widespread alarm fatigue, disrupted rest periods and a growing erosion of trust in systems that are intended to protect both crews and assets.&lt;/p&gt;
    &lt;p&gt;The research applied recognised industrial best practice, including IEC 62682 and EEMUA 191, to maritime operations for the first time at this scale. It found that fewer than half of the vessels studied met the recommended benchmark of fewer than 30 alarms per hour, while on ships with unattended machinery spaces alarms disrupted 63% of rest periods. In some cases, cruise ships experienced up to 2,600 alarms per day, with peak rates reaching 4,691 alarms in just ten minutes.&lt;/p&gt;
    &lt;p&gt;Crews, overwhelmed by the volume of alerts, are forced to silence alarms without acknowledgement or physically bypass alarm circuits, normalising unsafe practices and eroding trust in critical safety systems.&lt;/p&gt;
    &lt;p&gt;Effective Alarm Management in the Maritime Industry: Insights from 40 million vessel alarms builds on LR’s Effective Alarm Management in the Maritime Industry report (released in September 2024) by moving beyond diagnosis to demonstrate what can be achieved in practice. A pilot project on an operational cruise ship reduced total alarm numbers by almost 50 per cent over a six-month period, without new technology or major system redesign. Improvements were delivered through traditional marine engineering interventions, including correcting valve installations, replacing faulty sensors and tuning existing systems.&lt;/p&gt;
    &lt;p&gt;LR’s analysis also demonstrates that addressing the 10 most frequent alarms could reduce overall loads by nearly 40 per cent.&lt;/p&gt;
    &lt;p&gt;The report calls for greater adoption of objective alarm performance assessment, stronger consideration of human factors in system design and operation throughout the vessel lifecycle, and regulatory frameworks that support consistent, enforceable standards.&lt;/p&gt;
    &lt;p&gt;Duncan Duffy, LR’s Global Head of Technology, said: “Our research found that alarm systems, when poorly managed, have themselves become a safety risk. Without decisive industry action, alarm fatigue will continue to undermine situational awareness and increase the likelihood of serious incidents.&lt;/p&gt;
    &lt;p&gt;“If the maritime industry is serious about safety, it must commit to continuous performance measurement, objective evaluation, and a human-centred approach to alarm system design. Only then can alarm systems fulfil their intended purpose—supporting crews, safeguarding lives, and ensuring safer voyages for all.”&lt;/p&gt;
    &lt;p&gt;The research is part of LR’s Digital Transformation Research programme, specifically designed to provide in-depth analysis of key opportunities and challenges for maritime digitalisation.&lt;/p&gt;
    &lt;p&gt;For more information and to download the full report, visit the link below:&lt;lb/&gt;LR Alarm Management &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753611</guid><pubDate>Sun, 25 Jan 2026 12:40:00 +0000</pubDate></item><item><title>Web-based image editor modeled after Deluxe Paint</title><link>https://github.com/steffest/DPaint-js</link><description>&lt;doc fingerprint="56223e185d452d87"&gt;
  &lt;main&gt;
    &lt;p&gt;Webbased image editor modeled after the legendary Deluxe Paint with a focus on retro Amiga file formats. Next to modern image formats, DPaint.js can read and write Amiga icon files and IFF ILBM images.&lt;/p&gt;
    &lt;p&gt;Online version available at https://www.stef.be/dpaint/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully Featured image editor with a.o. &lt;list rend="ul"&gt;&lt;item&gt;Layers&lt;/item&gt;&lt;item&gt;Selections&lt;/item&gt;&lt;item&gt;Masking&lt;/item&gt;&lt;item&gt;Transformation tools&lt;/item&gt;&lt;item&gt;Effects and filters&lt;/item&gt;&lt;item&gt;Multiple undo/redo&lt;/item&gt;&lt;item&gt;Copy/Paste from any other image program or image source&lt;/item&gt;&lt;item&gt;Customizable dither tools&lt;/item&gt;&lt;item&gt;Color Cycling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Heavy focus on colour reduction with fine-grained dithering options&lt;/item&gt;
      &lt;item&gt;Amiga focus &lt;list rend="ul"&gt;&lt;item&gt;Read/write/convert Amiga icon files (all formats)&lt;/item&gt;&lt;item&gt;Reads IFF ILBM images (all formats including HAM and 24-bit)&lt;/item&gt;&lt;item&gt;Writes IFF ILBM images (up to 256 colors)&lt;/item&gt;&lt;item&gt;Read and write directly from Amiga Disk Files (ADF)&lt;/item&gt;&lt;item&gt;Embedded Amiga Emulator to preview your work in the real Deluxe Paint.&lt;/item&gt;&lt;item&gt;Limit the palette to 12 bit for Amiga OCS/ECS mode, or 9 bit for Atari ST mode.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Deluxe Paint Legacy &lt;list rend="ul"&gt;&lt;item&gt;Supports PBM files as used by the PC version of Deluxe Paint (Thanks to Michael Smith)&lt;/item&gt;&lt;item&gt;Supports Deluxe Paint Atari ST compression modes (Thanks to Nicolas Ramz)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It runs in your browser, works on any system and works fine on touch-screen devices like iPads.&lt;lb/&gt; It is written in 100% plain JavaScript and has no dependencies.&lt;lb/&gt; It's 100% free, no ads, no tracking, no accounts, no nothing.&lt;lb/&gt; All processing is done in your browser, no data is sent to any server.&lt;/p&gt;
    &lt;p&gt;The only part that is not included in this repository is the Amiga Emulator Files. (The emulator is based on the Scripted Amiga Emulator)&lt;/p&gt;
    &lt;p&gt;DPaint.js doesn't need building.&lt;lb/&gt; It also has zero dependencies so there's no need to install anything.&lt;lb/&gt; DPaint.js is written using ES6 modules and runs out of the box in modern browsers.&lt;lb/&gt; Just serve "index.html" from a webserver and you're good to go.&lt;/p&gt;
    &lt;p&gt;There's an optional build step to create a compact version of DPaint.js if you like.&lt;lb/&gt; I'm using Parcel.js for this.&lt;lb/&gt; For convenience, I've included a "package.json" file.&lt;lb/&gt; open a terminal and run &lt;code&gt;npm install&lt;/code&gt; to install Parcel.js and its dependencies.
Then run &lt;code&gt;npm run build&lt;/code&gt; to create a compact version of DPaint.js in the "dist" folder.&lt;/p&gt;
    &lt;p&gt;Documentation can be found at https://www.stef.be/dpaint/docs/&lt;/p&gt;
    &lt;p&gt;Dpaint.js is a web application, not an app that you install on your computer. That being said: DPaint.js has no online dependencies and runs fine offline if you want. One caveat: you have to serve the index.html file from a webserver, not just open it in your browser.&lt;lb/&gt; A quick way to do this is - for example - using the Spark app.&lt;lb/&gt; Download the binary for your platform, drop the Spark executable in the folder where you downloaded the Dpaint.js source files and run it. If you then point your browser to http://localhost:8080/ it should work.&lt;/p&gt;
    &lt;p&gt;If you are using Chrome, you can also "install" dpaint.js as app.&lt;lb/&gt; It will then show up your Chrome apps and work offline.&lt;/p&gt;
    &lt;p&gt;Current version is still alpha.&lt;lb/&gt; I'm sure there are bugs and missing features.&lt;lb/&gt; Bug reports and pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;Planned for the next release, already in the works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Color Cycling&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Animation support (GIf and Amiga ANIM files)&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Shading/transparency tools that stay within the palette.&lt;/del&gt;(done)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned for a future release if there's a need for it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for non-square pixel modes such as HiRes and Interlaced&lt;/item&gt;
      &lt;item&gt;PSD import and export&lt;/item&gt;
      &lt;item&gt;SpriteSheet support&lt;/item&gt;
      &lt;item&gt;Write HAM,SHAM and Dynamic HiRes images&lt;/item&gt;
      &lt;item&gt;Commodore 64 graphics modes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please note that the Brave browser is using "farbling" that introduces random image noise in certain conditions. They claim this is to protect your privacy. Although I totally understand the sentiment, In my opinion a browser should not actively alter the content of a webpage or intentionally break functionality.&lt;lb/&gt; But hey, who am I to speak, it's a free world. Just be aware that if you are using Brave, you will run into issues, so please "lower your shields" for this app in Brave or use another browser.&lt;/p&gt;
    &lt;p&gt;Dpaint.js supports Color-Cycling - a long lost art of "animating" a static image by only rotating some colors in the palette. See an example here:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;The_Vision_cycle.mp4&lt;/head&gt;
    &lt;p&gt;Open the layered source file of the above image directly in Dpaint.js&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753708</guid><pubDate>Sun, 25 Jan 2026 12:54:53 +0000</pubDate></item><item><title>Show HN: LLMNet – The Offline Internet, Search the web without the web</title><link>https://github.com/skorotkiewicz/llmnet</link><description>&lt;doc fingerprint="9b14b9483892176a"&gt;
  &lt;main&gt;
    &lt;p&gt;The Offline Internet. A premium, private, and AI-powered search experience that lives entirely on your machine.&lt;/p&gt;
    &lt;p&gt;LLMNet transforms your local LLMs into a structured search engine. It combines the power of local generative AI with a high-performance Vector Database (RAG) to provide instant, offline answers from your own knowledge base.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔒 100% Private: Your queries and data never leave your local network.&lt;/item&gt;
      &lt;item&gt;🧠 Local RAG: Index any website or wiki into a persistent Postgres Vector DB.&lt;/item&gt;
      &lt;item&gt;⚡ Instant Results: Sub-second semantic search using pgvector &amp;amp; HNSW indexing.&lt;/item&gt;
      &lt;item&gt;🎨 Premium UI: A glassmorphic, dark-mode interface inspired by modern search engines.&lt;/item&gt;
      &lt;item&gt;🌐 No Internet Required: Once indexed, your knowledge stays available offline.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: Next.js, Tailwind CSS&lt;/item&gt;
      &lt;item&gt;Intelligence: Local LLMs (via OpenAI-compatible APIs)&lt;/item&gt;
      &lt;item&gt;Database: PostgreSQL with &lt;code&gt;pgvector&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Orchestration: Bun, Cheerio (Crawl), Turndown (Markdown)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ensure you have the following running locally:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LLM Server: Port configured in &lt;code&gt;.env&lt;/code&gt;(e.g., Llama.cpp, Ollama)&lt;/item&gt;
      &lt;item&gt;Embedding Server: Port configured in &lt;code&gt;.env&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Database: Postgres with the &lt;code&gt;vector&lt;/code&gt;extension (see&lt;code&gt;postgres-pgvector/&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure your environment variables in &lt;code&gt;.env&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Example configuration
API_BASE_URL=http://localhost:8888/v1
EMBEDDING_URL=http://localhost:8889/v1/embeddings&lt;/code&gt;
    &lt;code&gt;# Install dependencies
bun install

# Initialize Database
bun postgres-pgvector/migrate.ts

# Start the engine
bun dev&lt;/code&gt;
    &lt;p&gt;Visit localhost:3000 to start searching.&lt;/p&gt;
    &lt;p&gt;LLMNet features a recursive ingestion pipeline. Simply paste a documentation URL or a GitHub Wiki link into the Indexer, and the system will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Crawl the site (Recursive BFS).&lt;/item&gt;
      &lt;item&gt;Convert content to clean Markdown.&lt;/item&gt;
      &lt;item&gt;Chunk text using a Recursive Character Splitter.&lt;/item&gt;
      &lt;item&gt;Embed &amp;amp; Store vectors for semantic retrieval.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Built for those who value privacy and data sovereignty.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754206</guid><pubDate>Sun, 25 Jan 2026 14:10:12 +0000</pubDate></item><item><title>Wine-Staging 11.1 Adds Patches for Enabling Recent Photoshop Versions on Linux</title><link>https://www.phoronix.com/news/Wine-Staging-11.1</link><description>&lt;doc fingerprint="de54f53ba99c238b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wine-Staging 11.1 Adds Patches For Enabling Recent Adobe Photoshop Versions On Linux&lt;/head&gt;
    &lt;p&gt; Following yesterday's release of Wine 11.1 for kicking off the new post-11.0 development cycle, Wine-Staging 11.1 is now available for this experimental/testing version of Wine that present is around 254 patches over the upstream Wine state. &lt;lb/&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;lb/&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;lb/&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;lb/&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;lb/&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
    &lt;p&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;/p&gt;
    &lt;p&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;/p&gt;
    &lt;p&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;/p&gt;
    &lt;p&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;/p&gt;
    &lt;p&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754427</guid><pubDate>Sun, 25 Jan 2026 14:42:39 +0000</pubDate></item><item><title>Show HN: Netfence – Like Envoy for eBPF Filters</title><link>https://github.com/danthegoodman1/netfence</link><description>&lt;doc fingerprint="780d8d538f3988c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Like Envoy xDS, but for eBPF filters.&lt;/p&gt;
    &lt;p&gt;Netfence runs as a daemon on your VM/container hosts and automatically injects eBPF filter programs into cgroups and network interfaces, with a built-in DNS server that resolves allowed domains and populates the IP allowlist.&lt;/p&gt;
    &lt;p&gt;Netfence daemons connect to a central control plane that you implement via gRPC to synchronize allowlists/denylists with your backend.&lt;/p&gt;
    &lt;p&gt;Your control plane pushes network rules like &lt;code&gt;ALLOW *.pypi.org&lt;/code&gt; or &lt;code&gt;ALLOW 10.0.0.0/16&lt;/code&gt; to attached interfaces/cgroups. When a VM/container queries DNS, Netfence resolves it, adds the IPs to the eBPF filter, and drops traffic to unknown IPs before it leaves the host without any performance penalty.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Attach eBPF filters to network interfaces (TC) or cgroups&lt;/item&gt;
      &lt;item&gt;Policy modes: disabled, allowlist, denylist, block-all&lt;/item&gt;
      &lt;item&gt;IPv4 and IPv6 CIDR support with optional TTLs&lt;/item&gt;
      &lt;item&gt;Per-attachment DNS server with domain allowlist/denylist&lt;/item&gt;
      &lt;item&gt;Domain rules support subdomains with specificity-based matching (more specific rules win)&lt;/item&gt;
      &lt;item&gt;Resolved domains auto-populate IP filter&lt;/item&gt;
      &lt;item&gt;Metadata on daemons and attachments for associating with VM ID, tenant, etc.&lt;/item&gt;
      &lt;item&gt;Support for proxying DNS queries to the control plane to make DNS decisions per-attachment&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;+------------------+         +-------------------------+
|  Your Control    |&amp;lt;-------&amp;gt;|  Daemon (per host)      |
|  Plane (gRPC)    |  stream |                         |
+------------------+         |  +-------------------+  |
                             |  | DNS Server        |  |
                             |  | (per-attachment)  |  |
                             |  +-------------------+  |
                             +-------------------------+
                                        |
                                 +------+------+
                                 |             |
                              TC Filter    Cgroup Filter
                              (veth, eth)  (containers)
&lt;/code&gt;
    &lt;p&gt;Each attachment gets a unique DNS address (port) provisioned by the daemon. Containers/VMs should be configured to use their assigned DNS address.&lt;/p&gt;
    &lt;p&gt;Run the daemon, which:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exposes a local gRPC API (&lt;code&gt;DaemonService&lt;/code&gt;) for attaching/detaching filters&lt;/item&gt;
      &lt;item&gt;Connects to your control plane via bidirectional stream (&lt;code&gt;ControlPlane.Connect&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Loads and manages eBPF programs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Start the daemon:&lt;/p&gt;
    &lt;code&gt;# Start with default config
netfenced start

# Start with custom config file
netfenced start --config /etc/netfence/config.yaml&lt;/code&gt;
    &lt;p&gt;Check daemon status:&lt;/p&gt;
    &lt;code&gt;netfenced status&lt;/code&gt;
    &lt;p&gt;Your orchestration system calls the daemon's local API.&lt;/p&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Attach(interface_name: "veth123", metadata: {vm_id: "abc"})
// or
DaemonService.Attach(cgroup_path: "/sys/fs/cgroup/...", metadata: {container_id: "xyz"})
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;# Attach to a network interface (TC)
netfenced attach --interface veth123 --metadata vm_id=abc

# Attach to a cgroup
netfenced attach --cgroup /sys/fs/cgroup/... --metadata container_id=xyz

# Attach with metadata
netfenced attach --interface eth0 --metadata tenant=acme,env=prod&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daemon attaches eBPF filter to the target&lt;/item&gt;
      &lt;item&gt;Daemon sends &lt;code&gt;Subscribed{id, target, type, metadata}&lt;/code&gt;to control plane and waits for&lt;code&gt;SubscribedAck&lt;/code&gt;with initial config (mode, CIDRs, DNS rules)&lt;/item&gt;
      &lt;item&gt;If the control plane doesn't respond within the timeout (default 5s, configurable via &lt;code&gt;control_plane.subscribe_ack_timeout&lt;/code&gt;), the attachment is rolled back and the attach call fails&lt;/item&gt;
      &lt;item&gt;Daemon watches for target removal and sends &lt;code&gt;Unsubscribed&lt;/code&gt;automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Detach(id)
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;netfenced detach --id &amp;lt;attachment-id&amp;gt;&lt;/code&gt;
    &lt;p&gt;List attachments:&lt;/p&gt;
    &lt;code&gt;netfenced list
netfenced list --all  # fetch all pages&lt;/code&gt;
    &lt;p&gt;Implement &lt;code&gt;ControlPlane.Connect&lt;/code&gt; RPC - a bidirectional stream:&lt;/p&gt;
    &lt;p&gt;Receive from daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncRequest&lt;/code&gt;on connect/reconnect (lists current attachments)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Subscribed&lt;/code&gt;when new attachments are added&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Unsubscribed&lt;/code&gt;when attachments are removed&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Heartbeat&lt;/code&gt;with stats&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Send to daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncAck&lt;/code&gt;after receiving SyncRequest&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SubscribedAck{mode, cidrs, dns_config}&lt;/code&gt;after receiving Subscribed (required - daemon waits for this)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetMode{mode}&lt;/code&gt;- change IP filter policy mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowCIDR{cidr, ttl}&lt;/code&gt;/&lt;code&gt;DenyCIDR&lt;/code&gt;/&lt;code&gt;RemoveCIDR&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetDnsMode{mode}&lt;/code&gt;- change DNS filtering mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowDomain{domain}&lt;/code&gt;/&lt;code&gt;DenyDomain&lt;/code&gt;/&lt;code&gt;RemoveDomain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BulkUpdate{mode, cidrs, dns_config}&lt;/code&gt;- full state sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the daemon receives &lt;code&gt;Subscribed&lt;/code&gt;, it blocks waiting for &lt;code&gt;SubscribedAck&lt;/code&gt; before returning success to the caller. This ensures the attachment has its initial configuration before traffic flows. Use the metadata to identify which VM/tenant/container this attachment belongs to and respond with the appropriate initial rules.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754724</guid><pubDate>Sun, 25 Jan 2026 15:13:46 +0000</pubDate></item><item><title>A macOS app that blurs your screen when you slouch</title><link>https://github.com/tldev/posturr</link><description>&lt;doc fingerprint="709cd7c437d6538d"&gt;
  &lt;main&gt;
    &lt;p&gt;A macOS app that blurs your screen when you slouch.&lt;/p&gt;
    &lt;p&gt;Posturr uses your Mac's camera and Apple's Vision framework to monitor your posture in real-time. When it detects that you're slouching, it progressively blurs your screen to remind you to sit up straight. Maintain good posture, and the blur clears instantly.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time posture detection - Uses Apple's Vision framework for body pose and face tracking&lt;/item&gt;
      &lt;item&gt;Progressive screen blur - Gentle visual reminder that intensifies with worse posture&lt;/item&gt;
      &lt;item&gt;Menu bar controls - Easy access to settings, calibration, and status from the menu bar&lt;/item&gt;
      &lt;item&gt;Multi-display support - Works across all connected monitors&lt;/item&gt;
      &lt;item&gt;Privacy-focused - All processing happens locally on your Mac&lt;/item&gt;
      &lt;item&gt;Lightweight - Runs as a background app with minimal resource usage&lt;/item&gt;
      &lt;item&gt;No account required - No signup, no cloud, no tracking&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest &lt;code&gt;Posturr-vX.X.X.zip&lt;/code&gt;from the Releases page&lt;/item&gt;
      &lt;item&gt;Unzip the downloaded file&lt;/item&gt;
      &lt;item&gt;Drag &lt;code&gt;Posturr.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since Posturr is not signed with an Apple Developer certificate, macOS Gatekeeper will initially block it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Right-click (or Control-click) on &lt;code&gt;Posturr.app&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Select "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;Click "Open" in the dialog that appears&lt;/item&gt;
      &lt;item&gt;Grant camera access when prompted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only need to do this once. After the first launch, you can open Posturr normally.&lt;/p&gt;
    &lt;p&gt;Posturr requires camera access to monitor your posture. When you first launch the app, macOS will ask for permission. Click "OK" to grant access.&lt;/p&gt;
    &lt;p&gt;If you accidentally denied permission, you can grant it later:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open System Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Camera&lt;/item&gt;
      &lt;item&gt;Find Posturr and enable the toggle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once launched, Posturr appears in your menu bar with a person icon. The app continuously monitors your posture and applies screen blur when slouching is detected.&lt;/p&gt;
    &lt;p&gt;Click the menu bar icon to access:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Status - Shows current state (Monitoring, Slouching, Good Posture, etc.)&lt;/item&gt;
      &lt;item&gt;Enabled - Toggle posture monitoring on/off&lt;/item&gt;
      &lt;item&gt;Recalibrate - Reset your baseline posture (sit up straight, then click)&lt;/item&gt;
      &lt;item&gt;Sensitivity - Adjust how sensitive the slouch detection is (Low, Medium, High, Very High)&lt;/item&gt;
      &lt;item&gt;Dead Zone - Set the tolerance before blur kicks in (None, Small, Medium, Large)&lt;/item&gt;
      &lt;item&gt;Compatibility Mode - Use public macOS APIs for blur (try this if blur doesn't appear)&lt;/item&gt;
      &lt;item&gt;Quit - Exit the application (or press Escape anywhere)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position your camera at eye level when possible&lt;/item&gt;
      &lt;item&gt;Ensure adequate lighting on your face&lt;/item&gt;
      &lt;item&gt;Sit at a consistent distance from your screen&lt;/item&gt;
      &lt;item&gt;The app works best when your shoulders are visible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr uses Apple's Vision framework to detect body pose landmarks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Body Pose Detection: Tracks nose, shoulders, and their relative positions&lt;/item&gt;
      &lt;item&gt;Face Detection Fallback: When full body isn't visible, tracks face position&lt;/item&gt;
      &lt;item&gt;Posture Analysis: Measures the vertical distance between nose and shoulders&lt;/item&gt;
      &lt;item&gt;Blur Response: Applies screen blur proportional to posture deviation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The screen blur uses macOS's private CoreGraphics API by default for efficient, system-level blur. If the blur doesn't appear on your system, enable Compatibility Mode from the menu to use &lt;code&gt;NSVisualEffectView&lt;/code&gt; instead.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Xcode Command Line Tools (&lt;code&gt;xcode-select --install&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/yourusername/posturr.git
cd posturr
./build.sh&lt;/code&gt;
    &lt;p&gt;The built app will be in &lt;code&gt;build/Posturr.app&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Standard build
./build.sh

# Build with release archive (.zip)
./build.sh --release&lt;/code&gt;
    &lt;code&gt;swiftc -O \
    -framework AppKit \
    -framework AVFoundation \
    -framework Vision \
    -framework CoreImage \
    -o Posturr \
    main.swift&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No code signing: Requires manual Gatekeeper bypass on first launch&lt;/item&gt;
      &lt;item&gt;Camera dependency: Requires a working camera with adequate lighting&lt;/item&gt;
      &lt;item&gt;Detection accuracy: Works best with clear view of upper body/face&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr exposes a file-based command interface for external control:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;capture&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Take a photo and analyze pose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blur &amp;lt;0-64&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set blur level manually&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;quit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Write commands to &lt;code&gt;/tmp/posturr-command&lt;/code&gt;. Responses appear in &lt;code&gt;/tmp/posturr-response&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Camera (built-in or external)&lt;/item&gt;
      &lt;item&gt;Approximately 10MB disk space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr processes all video data locally on your Mac. No images or data are ever sent to external servers. The camera feed is used solely for posture detection and is never stored or transmitted.&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit issues and pull requests.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Apple's Vision framework for body pose detection&lt;/item&gt;
      &lt;item&gt;Uses private CoreGraphics API for blur, with NSVisualEffectView fallback&lt;/item&gt;
      &lt;item&gt;Inspired by the need for better posture during long coding sessions&lt;/item&gt;
      &lt;item&gt;Thanks to @wklm for the compatibility mode implementation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754944</guid><pubDate>Sun, 25 Jan 2026 15:34:51 +0000</pubDate></item><item><title>Using PostgreSQL as a Dead Letter Queue for Event-Driven Systems</title><link>https://www.diljitpr.net/blog-post-postgresql-dlq</link><description>&lt;doc fingerprint="3f4c263785a28d80"&gt;
  &lt;main&gt;
    &lt;p&gt;While I was working on a project with Wayfair, I got the opportunity to work on a system that generated daily business reports aggregated from multiple data sources flowing through event streams across Wayfair. At a high level, Kafka consumers listened to these events, hydrated them with additional data by calling downstream services, and finally persisted the enriched events into a durable datastoreâCloudSQL PostgreSQL on GCP.&lt;/p&gt;
    &lt;p&gt;When everything was healthy, the pipeline worked exactly as expected. Events flowed in, got enriched, and were stored reliably. The real challenge started when things went wrong, which, in distributed systems, is not an exception but a certainty.&lt;/p&gt;
    &lt;p&gt;There were multiple failure scenarios we had to deal with. Sometimes the APIs we depended on for hydration were down or slow. Sometimes the consumer itself crashed midway through processing. In other cases, events arrived with missing or malformed fields that could not be processed safely. These were all situations outside our direct control, but they still needed to be handled gracefully.&lt;/p&gt;
    &lt;p&gt;This is where the concept of a Dead Letter Queue came into the picture. Whenever we knew an event could not be processed successfully, instead of dropping it or blocking the entire consumer, we redirected it to a DLQ so it could be inspected and potentially reprocessed later.&lt;/p&gt;
    &lt;p&gt;Our first instinct was to use Kafka itself as a DLQ. While this is a common pattern, it quickly became clear that it wasn't a great fit for our needs. Kafka is excellent for moving data, but once messages land in a DLQ topic, they are not particularly easy to inspect. Querying by failure reason, retrying a specific subset of events, or even answering simple questions like "what failed yesterday and why?" required extra tooling and custom consumers. For a system that powered business-critical daily reports, this lack of visibility was a serious drawback.&lt;/p&gt;
    &lt;p&gt;That's when we decided to treat PostgreSQL itself as the Dead Letter Queue.&lt;/p&gt;
    &lt;p&gt;Instead of publishing failed events to another Kafka topic, we persisted them directly into a DLQ table in PostgreSQL. We were already using CloudSQL as our durable store, so operationally this added very little complexity. Conceptually, it also made failures first-class citizens in the system rather than opaque messages lost in a stream.&lt;/p&gt;
    &lt;p&gt; Whenever an event failed processingâdue to an API failure, consumer crash, schema mismatch, or validation errorâwe stored the raw event payload along with contextual information about the failure. Each record carried a simple status field. When the event first landed in the DLQ, it was marked as &lt;code&gt;PENDING&lt;/code&gt;. Once it was successfully reprocessed, the status was updated to &lt;code&gt;SUCCEEDED&lt;/code&gt;. Keeping the state model intentionally minimal made it easy to reason about the lifecycle of a failed event.
                    &lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Table Schema and Indexing Strategy&lt;/head&gt;
    &lt;p&gt;To support inspection, retries, and long-term operability, the DLQ table was designed to be simple, query-friendly, and retry-aware.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table Schema&lt;/head&gt;
    &lt;code&gt;CREATE TABLE dlq_events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    error_reason TEXT NOT NULL,
    error_stacktrace TEXT,
    status VARCHAR(20) NOT NULL, -- PENDING / SUCCEEDED
    retry_count INT NOT NULL DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);&lt;/code&gt;
    &lt;head rend="h4"&gt;Key Design Considerations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;payload&lt;/code&gt;is stored as&lt;code&gt;JSONB&lt;/code&gt;to preserve the raw event without enforcing a rigid schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;status&lt;/code&gt;keeps the lifecycle simple and explicit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_after&lt;/code&gt;prevents aggressive retries when downstream systems are unstable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_count&lt;/code&gt;allows retry limits to be enforced without external state.&lt;/item&gt;
      &lt;item&gt;Timestamps make auditing and operational analysis straightforward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Indexes&lt;/head&gt;
    &lt;code&gt;CREATE INDEX idx_dlq_status
ON dlq_events (status);

CREATE INDEX idx_dlq_status_retry_after
ON dlq_events (status, retry_after);

CREATE INDEX idx_dlq_event_type
ON dlq_events (event_type);

CREATE INDEX idx_dlq_created_at
ON dlq_events (created_at);&lt;/code&gt;
    &lt;p&gt;These indexes allow the retry scheduler to efficiently locate eligible events while still supporting fast debugging and time-based analysis without full table scans.&lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Retry Mechanism with ShedLock&lt;/head&gt;
    &lt;p&gt;Persisting failed events solved the visibility problem, but we still needed a safe and reliable way to retry them.&lt;/p&gt;
    &lt;p&gt; For this, we introduced a DLQ retry scheduler backed by ShedLock. The scheduler periodically scans the DLQ table for &lt;code&gt;PENDING&lt;/code&gt; events that are eligible for retry and attempts to process them again. Since the service runs on multiple instances, ShedLock ensures that only one instance executes the retry job at any given time. This eliminates duplicate retries without requiring custom leader-election logic.
                    &lt;/p&gt;
    &lt;head rend="h4"&gt;Retry Configuration&lt;/head&gt;
    &lt;code&gt;dlq:
  retry:
    enabled: true
    max-retries: 240
    batch-size: 50
    fixed-rate: 21600000 # 6 hours in milliseconds&lt;/code&gt;
    &lt;head rend="h4"&gt;How Retries Work&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scheduler runs every six hours.&lt;/item&gt;
      &lt;item&gt;Up to fifty eligible events are picked up per run.&lt;/item&gt;
      &lt;item&gt;Events exceeding the maximum retry count are skipped.&lt;/item&gt;
      &lt;item&gt;Successful retries immediately transition the event status to &lt;code&gt;SUCCEEDED&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Failures remain in &lt;code&gt;PENDING&lt;/code&gt;and are retried in subsequent runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Query Implementation&lt;/head&gt;
    &lt;p&gt; The retry scheduler uses a SQL query with &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; to safely select eligible events across multiple instances. This PostgreSQL feature ensures that even if multiple scheduler instances run simultaneously, each will pick up different rows without blocking each other:
                    &lt;/p&gt;
    &lt;code&gt;@QueryHints(@QueryHint(name = "jakarta.persistence.lock.timeout", value = "-2"))
@Query(
    value = "SELECT * FROM dlq_table "
        + "WHERE messagetype = :messageType "
        + "AND retries &amp;lt; :maxRetries "
        + "AND (replay_status IS NULL OR replay_status NOT IN ('COMPLETED')) "
        + "ORDER BY created_at ASC "
        + "FOR UPDATE SKIP LOCKED",
    nativeQuery = true
)&lt;/code&gt;
    &lt;p&gt; The &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; clause is crucial here. It allows each instance to lock and process different rows concurrently, preventing duplicate processing while maintaining high throughput. The query hint sets the lock timeout to &lt;code&gt;-2&lt;/code&gt;, which means "wait indefinitely" but combined with &lt;code&gt;SKIP LOCKED&lt;/code&gt;, it effectively means "skip any rows that are already locked by another transaction."
                    &lt;/p&gt;
    &lt;p&gt;This setup allowed the system to tolerate long downstream outages while avoiding retry storms and unnecessary load on dependent services.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operational Benefits&lt;/head&gt;
    &lt;p&gt;With this approach, failures became predictable and observable rather than disruptive. Engineers could inspect failures using plain SQL, identify patterns, and reprocess only the events that mattered. If a downstream dependency was unavailable for hours or even days, events safely accumulated in the DLQ and were retried later without human intervention. If an event was fundamentally bad, it stayed visible instead of being silently dropped.&lt;/p&gt;
    &lt;p&gt;Most importantly, this design reduced operational stress. Failures were no longer something to fear; they were an expected part of the system with a clear, auditable recovery path.&lt;/p&gt;
    &lt;head rend="h3"&gt;My Thoughts&lt;/head&gt;
    &lt;p&gt;The goal was never to replace Kafka with PostgreSQL. Kafka remained the backbone for high-throughput event ingestion, while PostgreSQL handled what it does bestâdurability, querying, and observability around failures. By letting each system play to its strengths, we ended up with a pipeline that was resilient, debuggable, and easy to operate.&lt;/p&gt;
    &lt;p&gt;In the end, using PostgreSQL as a Dead Letter Queue turned failure handling into something boring and predictable. And in production systems, boring is exactly what you want.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46755115</guid><pubDate>Sun, 25 Jan 2026 15:51:03 +0000</pubDate></item><item><title>ICE Using Palantir Tool That Feeds on Medicaid Data</title><link>https://www.eff.org/deeplinks/2026/01/report-ice-using-palantir-tool-feeds-medicaid-data</link><description>&lt;doc fingerprint="ad964554d5aa1139"&gt;
  &lt;main&gt;
    &lt;p&gt;EFF last summer asked a federal judge to block the federal government from using Medicaid data to identify and deport immigrants.&lt;/p&gt;
    &lt;p&gt;We also warned about the danger of the Trump administration consolidating all of the government’s information into a single searchable, AI-driven interface with help from Palantir, a company that has a shaky-at-best record on privacy and human rights.&lt;/p&gt;
    &lt;p&gt;Now we have the first evidence that our concerns have become reality.&lt;/p&gt;
    &lt;p&gt;“Palantir is working on a tool for Immigration and Customs Enforcement (ICE) that populates a map with potential deportation targets, brings up a dossier on each person, and provides a “confidence score” on the person’s current address,” 404 Media reports today. “ICE is using it to find locations where lots of people it might detain could be based.”&lt;/p&gt;
    &lt;p&gt;The tool – dubbed Enhanced Leads Identification &amp;amp; Targeting for Enforcement (ELITE) – receives peoples’ addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources.&lt;/p&gt;
    &lt;p&gt;This revelation comes as ICE – which has gone on a surveillance technology shopping spree – floods Minneapolis with agents, violently running roughshod over the civil rights of immigrants and U.S. citizens alike; President Trump has threatened to use the Insurrection Act of 1807 to deploy military troops against protestors there. Other localities are preparing for the possibility of similar surges.&lt;/p&gt;
    &lt;p&gt;Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;This kind of consolidation of government records provides enormous government power that can be abused. Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;As EFF Executive Director Cindy Cohn wrote in a Mercury News op-ed last August, “While couched in the benign language of eliminating government ‘data silos,’ this plan runs roughshod over your privacy and security. It’s a throwback to the rightly mocked ‘Total Information Awareness’ plans of the early 2000s that were, at least publicly, stopped after massive outcry from the public and from key members of Congress. It’s time to cry out again.”&lt;/p&gt;
    &lt;p&gt;In addition to the amicus brief we co-authored challenging ICE’s grab for Medicaid data, EFF has successfully sued over DOGE agents grabbing personal data from the U.S. Office of Personnel Management, filed an amicus brief in a suit challenging ICE’s grab for taxpayer data, and sued the departments of State and Homeland Security to halt a mass surveillance program to monitor constitutionally protected speech by noncitizens lawfully present in the U.S.&lt;/p&gt;
    &lt;p&gt;But litigation isn’t enough. People need to keep raising concerns via public discourse and Congress should act immediately to put brakes on this runaway train that threatens to crush the privacy and security of each and every person in America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46756117</guid><pubDate>Sun, 25 Jan 2026 17:36:19 +0000</pubDate></item><item><title>First, Make Me Care</title><link>https://gwern.net/blog/2026/make-me-care</link><description>&lt;doc fingerprint="77b044a15c934274"&gt;
  &lt;main&gt;
    &lt;p&gt;First, Make Me Care Writing advice: some nonfiction fails because it opens with background instead of a hook—readers leave before reaching the good material. Find the single anomaly or question that makes your topic interesting, lead with that, and let the background follow once you’ve earned attention. [Return to blog index]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757067</guid><pubDate>Sun, 25 Jan 2026 19:03:40 +0000</pubDate></item><item><title>Spanish track was fractured before high-speed train disaster, report finds</title><link>https://www.bbc.com/news/articles/c1m77dmxlvlo</link><description>&lt;doc fingerprint="ae6089bcbd13d29b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Spanish track was fractured before high-speed train disaster, report finds&lt;/head&gt;
    &lt;p&gt;A fracture in a straight section of track "occurred prior to the passage" of a high-speed train that derailed, causing last Sunday's rail disaster in which 45 people died, an initial report has found.&lt;/p&gt;
    &lt;p&gt;A train run by private company Iryo derailed last Sunday and its rear carriages crossed on to the opposite track into the path of an oncoming train run by state-owned Renfe.&lt;/p&gt;
    &lt;p&gt;The CIAF rail investigation commission said not only did Iryo train's front carriages which stayed on the track have "notches" in their wheels, but three earlier trains that went over the track earlier did too.&lt;/p&gt;
    &lt;p&gt;A gap of almost 40cm (15in) in the track has become the focus of the investigation into the crash.&lt;/p&gt;
    &lt;p&gt;Sunday's deadly collision occurred at around 19:45 local time (18:45 GMT), about an hour after the Iryo train left Málaga for Madrid.&lt;/p&gt;
    &lt;p&gt;The train's last three carriages - carriages six to eight - derailed and collided with the Huelva-bound Renfe train. "Carriage six derailed due to a complete lack of continuity in the track," the preliminary report finds.&lt;/p&gt;
    &lt;p&gt;Most of those killed and injured were in the front carriages of the state-operated train.&lt;/p&gt;
    &lt;p&gt;Earlier this week, Spanish Transport Minister Óscar Puente confirmed reports that grooves were found on the wheels of the Iryo train's carriages, which had passed over the track safely.&lt;/p&gt;
    &lt;p&gt;"These notches in the wheels and the deformation observed in the track are compatible with the fact that the track was cracked," the CIAF preliminary report said.&lt;/p&gt;
    &lt;p&gt;It added that three trains that had gone over the tracks at 17:21 on Sunday, 19:01 and then 19:09 had similar notches "with a compatible geometric pattern".&lt;/p&gt;
    &lt;p&gt;Similar grooves are found on carriages two, three and four of the Iryo train, the report says, but carriage five - the last that did not derail - had a groove on its outer edge, suggesting the rail was already tilting outwards before carriage six derailed.&lt;/p&gt;
    &lt;p&gt;The CIAF called its report a "working hypothesis", adding that it "must be corroborated by later detailed calculations and analysis".&lt;/p&gt;
    &lt;p&gt;The transport minister appeared before reporters again on Friday to say that it was too early to have definitive answers, but that if the cause of the crash was the fracture, then it occurred in the minutes and hours before the derailment and could not have been detected.&lt;/p&gt;
    &lt;p&gt;The Adamuz disaster is is the country's worst rail crash in more than a decade.&lt;/p&gt;
    &lt;p&gt;In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757162</guid><pubDate>Sun, 25 Jan 2026 19:12:50 +0000</pubDate></item><item><title>Data Leak Exposes 149M Logins, Including Gmail, Facebook</title><link>https://www.techrepublic.com/article/news-149-million-passwords-exposed-infostealer-database/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46757465</guid><pubDate>Sun, 25 Jan 2026 19:45:10 +0000</pubDate></item></channel></rss>