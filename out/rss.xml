<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 17 Nov 2025 17:10:39 +0000</lastBuildDate><item><title>FPGA Based IBM-PC-XT</title><link>https://bit-hack.net/2025/11/10/fpga-based-ibm-pc-xt/</link><description>&lt;doc fingerprint="a75e788d5dab06ad"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently I undertook a hobby project to recreate an IBM XT Personal Computer from the 1980s using a mix of authentic parts and modern technology. I had a clear goal in mind: I wanted to be able to play the EGA version of Monkey Island 1 on it, with no features missing. This means I need mouse support, hard drive with write access for saving the game, and Adlib audio, my preferred version of the game‚Äôs musical score.&lt;/p&gt;
    &lt;p&gt;The catalyst for this project was the discovery that there are low-power versions of the NEC V20 CPU available (UPD70108H), which is compatible with the Intel 8088 used in the XT. Being a low-power version significantly simplifies its connection to an FPGA, which typically operate with 3.3-volt IO voltages. Coupled with a low-power 1MB SRAM chip (CY62158EV30) to provide the XT with its 640KB of memory, and I started to have the bones of a complete system worked out.&lt;/p&gt;
    &lt;p&gt;I started off by designing the hardware of the system, which would then serve as my development board while I worked on the software/gateware. The following features were added:&lt;lb/&gt;‚Äì DIP-40 socket for an low power NEC V20 CPU&lt;lb/&gt;‚Äì 1MB SRAM chip for the system memory&lt;lb/&gt;‚Äì An icesugar-pro FPGA board with a Lattice LFE5U-25F&lt;lb/&gt;‚Äì Dual PS/2 connectors for keyboard and mouse&lt;lb/&gt;‚Äì Micro SD card socket to act as a Fixed Disk&lt;lb/&gt;‚Äì An authentic YM3014B digital-to-analogue converter for audio&lt;lb/&gt;‚Äì A Piezo speaker that can be driven by the programmable-interval-timer for system bleeps&lt;lb/&gt;‚Äì Lastly, a reset switch and some status LEDs&lt;/p&gt;
    &lt;p&gt;I drew up my design using the EasyEDA CAD software as I‚Äôm already familiar with it, and it has really good integration with the JLCPCB PCB assembly service. Some of the components in the design are too tricky for me to hand solder by myself. I did however have to solder the SRAM chips once the boards arrived since they were not stocked by LCSC so I had to source them elsewhere.&lt;/p&gt;
    &lt;p&gt;The first step was to write a bus controller for the processor. The V20 CPU clock is more forgiving than an original i8088 since its can be run right down to 0hz and its uses a regular 50% duty cycle. The external interface for an 8088 CPU operates in terms of bus cycles. At that start of a bus cycle the CPU asserts some pins to let everyone know what it wants to try and do‚Ä¶ read memory, write to IO, etc. Each type of bus cycle follows a specific sequence of events that happen over a number of clock cycles. It was straight forward to make a state machine that could detect the start of a bus-cycle, figure out what kind it was, and then produce or consume the data as needed by the CPU. Key here, was to make sure that all of the timing requirements were met, so that signals the CPU generates are sampled at the correct time, and signals the CPU requires have been driven correctly before the CPU reads them.&lt;/p&gt;
    &lt;p&gt;My first test for the bus controller was to write a simple program using NASM to be executed on the V20, with a simple goal‚Ä¶ it will flash an LED mapped to an IO port address. Simple, but a blinking LED seems to be the hardware equivalent of the software hello-world. For the initial version, the program was simply loaded into a FPGA block ram and used directly as the system memory.&lt;/p&gt;
    &lt;p&gt;Later, I used a more complex approach for memory accesses. The bios, for example, is loaded into an FPGA block ram, so that CPU memory reads will come from that rather than the system SRAM chip. Video memory is implemented a differently still. CPU memory writes are passed to both the video memory block ram and system SRAM, but CPU reads alway come from only the system SRAM. This then means that I have a spare read port on the video block ram that can then be used by the VGA signal generator to display the video memory contents.&lt;/p&gt;
    &lt;p&gt;After my success with a blinky program, I installed a virtual copy of Supersoft/Landmark Diagnostic ROM in place of the BIOS and wrote a basic CGA adapter for video output. I was then able to use the diagnostic ROM to test the SRAM memory interface as well as some of the peripherals required by the XT such as the programmable-interval-timer (i8253) and programmable-interrupt-controller (i8259).&lt;/p&gt;
    &lt;p&gt;Once I was confident the basic system was stable I then swaped in a generic XT bios from https://www.phatcode.net in place of the diagnostic ROM. It was amazing to see the bios start to boot up, and complain when it couldnt find a boot disk.&lt;/p&gt;
    &lt;p&gt;Fixed Disk access is achieved by making a small Verilog SPI controller accessible to the CPU via some unused IO ports. I then wrote an option ROM to handle BIOS INT13H (disk service) calls, which had routines that could issue commands to the SD-Card over SPI. The tricky part for me was learning the SD card protocol and then writing 8088 assembly to perform the correct operations. The mapping itself is very straightforward as both SD card and DOS assume 512byte sectors.&lt;/p&gt;
    &lt;p&gt;I saved a lot of time when writing the option ROM by developing and debugging the code using a software emulator of the board that I cobbled together. Some historic sources for it can be found here: https://github.com/bit-hack/iceXt/tree/master/misc/emulator&lt;/p&gt;
    &lt;p&gt;Perhaps the hardest part of the project was, surprisingly, getting the mouse to work. Mice of the XT era would typically be connected to a UART serial port. I had however placed a PS/2 connector on the hardware board, and those mice use a very different protocol. In my efforts to support a mouse I startedto learn about PS/2 devices, however I would need to implement a much more complex keyboard controller, and the BIOS I was also lacked support for such modern peripherals, and I just plain didn‚Äôt feel like I understood everything required to get that working.&lt;/p&gt;
    &lt;p&gt;What makes it tricky is that PS/2 is a bidirectional protocol, and the mouse has to be asked by the PC to broadcast updates, otherwise we will not receive any. That added a lot more complexity than I was wanting. The keyboard on the otherhand is relatively easy to work with and send out keypresses without having to be asked.&lt;/p&gt;
    &lt;p&gt;I chose an alternative. I wrote some Verilog code to talk directly to the PS/2 mouse, which would early in the boot process tell it to start sending over mouse events, as they have to be requested. When the bridge then receives mouse events, it translates and presents them to the computer via a pseudo UART peripheral. I had implemented a basic PS/2 mouse to Serial mouse bridge. A little convoluted but it works really well.&lt;/p&gt;
    &lt;p&gt;During this process, I lobotomised a spare mouse by attaching a logic analyser the clk and dat pads inside the mouse. I was then able to capture the communications between a real PC and the mouse and observe it during use. This gave me invaluable insight into exactly how the protocol worked, and what a real mouse expected.&lt;/p&gt;
    &lt;p&gt;I also found that having real waveforms to look at made it much easier to test components of my design in verilator, a Verilog simulator, as I could closely model the stimulus it should see when running in the FPGA.&lt;/p&gt;
    &lt;p&gt;Just like the XT, one of the channels of the PIT timer is used to drive the internal speaker to produce bleep and bloop sounds. I extended this by having disk accesses trigger short pulses out of the peizo speaker as a crude emulation of a hard disk seeking. I think it really adds to the experience when you can hear your computer thinking away while doing its tasks. When it comes to music, the internal PC speaker quickly looses its charm however. Writing an YM3812 implementation (the FM chip used in the Adlib card) is beyond my skill level but thankfully Jose Tejada has written an amazing open source version that I was able to pull into my project; https://github.com/jotego/jtopl.&lt;/p&gt;
    &lt;p&gt;I wrote a small Verilog module to take the PCM sample data generated by this soft YM3812 and convert it to the unusual 3:10 floating point format required by the YM3014 DAC on my board. This is very similar to the operation of the real Adlib hardware, where the YM3812 generates and sends serial audio data to a YM3014 DAC chip. A modern I2S DAC may have been cleaner, but having a chance to play with the authentic DAC seemed a little more fun to me. All of this combined results in the same lovely crisp FM tones I was so fond of when I played games on my PC growing up.&lt;/p&gt;
    &lt;p&gt;A lot of other elements of this project have been glossed over or omitted, such as support for CGA and EGA graphics. There is even a USB to UART bridge for sending files from a host PC directly to the SD card. I also made some nice clear acrylic panels on a CNC machine to round off the design and protect the bare PCB.&lt;/p&gt;
    &lt;p&gt;A video demo is shown below.&lt;lb/&gt;Unfortunately there is a ton of screen tearing due to the phase between the monitor and my camera. It isn‚Äôt visible in person.&lt;/p&gt;
    &lt;p&gt;Source code, schematics and gerber files are available on github here: https://github.com/bit-hack/iceXt&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45945784</guid><pubDate>Sun, 16 Nov 2025 15:26:24 +0000</pubDate></item><item><title>I finally understand Cloudflare Zero Trust tunnels</title><link>https://david.coffee/cloudflare-zero-trust-tunnels</link><description>&lt;doc fingerprint="a5b8a71e23286293"&gt;
  &lt;main&gt;
    &lt;p&gt;A while ago, after frustration with Tailscale in environments where it couldn‚Äôt properly penetrate NAT/firewall and get a p2p connection, I decided to invest some time into learning something new: Cloudflare Zero Trust + Warp.&lt;/p&gt;
    &lt;p&gt;There are so many new concepts, but after way too long, I can finally say that I understand Cloudflare Zero Trust Warp now. I am a full-on Cloudflare Zero Trust with Warp convert, and while I still have Tailscale running in parallel, almost everything I do now is going through Zero Trust tunnels.&lt;/p&gt;
    &lt;p&gt;This post is an explanation of the basic concepts, because I‚Äôm sure others will have similar issues wrapping their head around it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why tho?&lt;/head&gt;
    &lt;p&gt;Why would you even sink so much time into learning this? What does it give you?&lt;/p&gt;
    &lt;p&gt;Argo tunnels through Zero Trust allow you to do a bunch of really cool things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Connect private networks together - can be home networks, can be kubernetes clusters, you can create tunnels to and from every infra&lt;/item&gt;
      &lt;item&gt;Expose private services to the public, on public hostnames, no matter where they are running. You could even put your router running at 192.168.1.1 on the internet, accessible to everyone, no Warp client required&lt;/item&gt;
      &lt;item&gt;Create fully private networks with private IPs (10.x.x.x) that only resolve when Warp is connected, to services you specify&lt;/item&gt;
      &lt;item&gt;Quickly expose a public route to any service running locally or on any server, for quick development, testing webhooks or giving coworkers a quick preview&lt;/item&gt;
      &lt;item&gt;Create a fully private network running at home that‚Äôs only available when you‚Äôre connected to the Warp VPN client, or only to you, reachable anywhere&lt;/item&gt;
      &lt;item&gt;No worries about NAT, everything goes through the Cloudflare network, no direct p2p connection required&lt;/item&gt;
      &lt;item&gt;Add very granular access policies on who can access what - what login method does the user need, which email addresses are allowed. Allow bots and server-to-server exceptions with service access tokens.&lt;list rend="ul"&gt;&lt;item&gt;Does the user need to have Warp running? Does he need to be enrolled in Zero Trust? Does he need some special permission flag?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Authenticate to SSH servers through Zero Trust access policies without the need of SSH keys. Just connect Warp, type &lt;code&gt;ssh host&lt;/code&gt;and you‚Äôre logged in&lt;list rend="ul"&gt;&lt;item&gt;Close public SSH ports completely to only allow login through Warp&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Get the benefits of Cloudflare VPN edge routing on top (similar to 1.1.1.1 Warp+)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Quickie: Cloudflare Zero Trust vs Tailscale&lt;/head&gt;
    &lt;p&gt;To get this out of the way:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tailscale: peer-to-peer, uses NAT and firewall penetration methods to establish p2p connections. If not possible, it goes through central relay servers. Absolute best speed and latency if a connection is established.&lt;/item&gt;
      &lt;item&gt;Cloudflare: All traffic (with the exception of warp-to-warp routing, which is p2p) goes through Cloudflare‚Äôs edge network. So even SSH-ing into your local router will hop through Cloudflare servers. This adds latency, but no issues with NAT at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Cloudflared != Warp&lt;/head&gt;
    &lt;p&gt;Cloudflare has 2 tools available: Warp Client and Cloudflared. They interact with each other and have similarities in some areas but are not the same.&lt;/p&gt;
    &lt;p&gt;Warp Client&lt;/p&gt;
    &lt;p&gt;The tool that connects you to the Cloudflare network. This is the thing that you configure to add clients into your Zero Trust network and enforces policies.&lt;/p&gt;
    &lt;p&gt;Usually this runs on clients, but can also run on servers.&lt;/p&gt;
    &lt;p&gt;Warp client also supports warp-to-warp routing which is a true p2p connection similar to Tailscale.&lt;/p&gt;
    &lt;p&gt;Cloudflared&lt;/p&gt;
    &lt;p&gt;The thing that creates a tunnel and adds it to the Zero Trust network.&lt;/p&gt;
    &lt;p&gt;Most commonly you run this on servers to expose tunnels into your network, but you can also run it on clients.&lt;/p&gt;
    &lt;p&gt;On the client side you can use &lt;code&gt;cloudflared access&lt;/code&gt; to establish a connection with other things in your Zero Trust network.&lt;/p&gt;
    &lt;p&gt;Can also create one-time-use tunnels that aren‚Äôt connected to the Zero Trust network. Good for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tunnels, Routes, Targets&lt;/head&gt;
    &lt;p&gt;This took me the longest to understand. Zero Trust allows you to configure Tunnels, Routes and Targets; here‚Äôs how they interplay.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tunnels&lt;/head&gt;
    &lt;p&gt;The most important part of your setup. Tunnels are deployed through &lt;code&gt;cloudflared&lt;/code&gt; and are simply an exit for traffic. Think of it as a literal tunnel that has its end somewhere.&lt;/p&gt;
    &lt;p&gt;Tunnels are deployed to infrastructure in the target network. So if you have a home network with 192.168.1.1/24, you want to deploy &lt;code&gt;cloudflared&lt;/code&gt; on any machine that‚Äôs always on and within that network. It can be your router, or your Raspi, it doesn‚Äôt matter.&lt;/p&gt;
    &lt;p&gt;For server-hosted services, you can have a tunnel on your main dev server, on a server, or on a pod in your Kubernetes cluster.&lt;/p&gt;
    &lt;p&gt;Now you have an opening into these networks through Warp/Argo tunnels.&lt;/p&gt;
    &lt;head rend="h4"&gt;Configuring tunnels&lt;/head&gt;
    &lt;p&gt;You can either configure tunnels through the Zero Trust UI by ‚Äúadopting‚Äù them, or configure them in the &lt;code&gt;/etc/cloudflared/config.yml&lt;/code&gt; config on the machine itself. Personal preference, I usually configure them on the machine itself.&lt;/p&gt;
    &lt;p&gt;The config specifies where a request should get routed to when it arrives at the tunnel. So the tunnel knows what to do with it.&lt;/p&gt;
    &lt;p&gt;In this config we tell cloudflared to route traffic arriving at this tunnel for hostname &lt;code&gt;gitlab.widgetcorp.tech&lt;/code&gt; to localhost:80, and &lt;code&gt;gitlab-ssh&lt;/code&gt; to the local SSH server.&lt;/p&gt;
    &lt;code&gt;‚ùØ cat /etc/cloudflared/config.yml
tunnel: a2f17e27-cd4d-4fcd-b02a-63839f57a96f
credentials-file: /etc/cloudflared/a2f17e27-cd4d-4fcd-b02a-63839f57a96f.json
ingress:
  - hostname: gitlab.widgetcorp.tech
    service: http://localhost:80
  - hostname: gitlab-ssh.widgetcorp.tech
    service: ssh://localhost:22
  - service: http_status:404

  # Catch-all for WARP routing
  - service: http_status:404

warp-routing:
  enabled: true
&lt;/code&gt;
    &lt;p&gt;The config alone doesn‚Äôt do anything. It just exposes a tunnel, and that‚Äôs it. What we need now are routes and targets.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exposing a private network to the public with tunnels quickly&lt;/head&gt;
    &lt;p&gt;Quick addition, as this is a super common use case. If you want to just expose something in your home network to the internet, you can add a config like this:&lt;/p&gt;
    &lt;code&gt;tunnel: a2f17e27-cd4d-4fcd-b02a-63839f57a96f
credentials-file: /etc/cloudflared/a2f17e27-cd4d-4fcd-b02a-63839f57a96f.json
ingress:
  - hostname: homeassistant.mydomain.com
    service: http://192.168.1.3:80
&lt;/code&gt;
    &lt;p&gt;Then go into Cloudflare DNS settings and map the domain &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to the tunnel:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;CNAME homeassistant.mydomain.com a2f17e27-cd4d-4fcd-b02a-63839f57a96f.cfargotunnel.com&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Now all traffic going to this domain will go through the cloudflared tunnel, which is configured to route &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to &lt;code&gt;192.168.1.3&lt;/code&gt;. No Warp client needed, Argo tunnel does everything for us.&lt;/p&gt;
    &lt;p&gt;Note: If you adopted the tunnels and don‚Äôt use &lt;code&gt;config.yaml&lt;/code&gt;, you can automatically create matching DNS records in the Cloudflare UI and don‚Äôt need to do this manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;Routes&lt;/head&gt;
    &lt;p&gt;A route defines where to direct traffic to.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say your homeassistant runs on 192.168.1.3 at home and you want to reach it from outside. Just above we deployed a &lt;code&gt;cloudflared&lt;/code&gt; tunnel on our router at 192.168.1.3, and added a config pointing the domain to the Argo tunnel, so &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; is already available to the public. However, &lt;code&gt;192.168.1.3&lt;/code&gt; isn‚Äôt, as it‚Äôs a private network IP.&lt;/p&gt;
    &lt;p&gt;You can define:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A route like &lt;code&gt;192.168.1.1/24&lt;/code&gt;pointing at your tunnel, to route ALL traffic to the full IP range through that tunnel (so even 192.168.1.245 will go through your tunnel)&lt;/item&gt;
      &lt;item&gt;Or a more specific route like &lt;code&gt;192.168.1.3/32&lt;/code&gt;pointing at your tunnel, to ONLY route traffic to 192.168.1.3 through that tunnel.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When configured, once your user connects their Warp client that‚Äôs set up with your Zero Trust network, the Warp client will see requests to 192.168.1.3 and route it through the Cloudflare network to reach your specific tunnel. Like a little police helper directing cars where to go.&lt;/p&gt;
    &lt;p&gt;If the Warp client is not connected, 192.168.1.3 will just resolve in your current local network. If connected, it will resolve to the tunnel.&lt;/p&gt;
    &lt;p&gt;The routed IP doesn‚Äôt need to exist! So you could, for example, route a random IP you like (e.g., 10.128.1.1) to your tunnel, the tunnel then forwards it based on your routes, for example to 192.168.1.1. This is extremely powerful because it allows you to build your own fully virtual network.&lt;/p&gt;
    &lt;p&gt;That‚Äôs all it does, what happens afterwards is up to the tunnel config that we created above. The tunnel decides where to point the incoming request to, whether that‚Äôs localhost or somewhere else.&lt;/p&gt;
    &lt;p&gt;To summarize, the &lt;code&gt;route&lt;/code&gt; tells the Warp client where to route traffic to.&lt;/p&gt;
    &lt;p&gt;Now we have 2 things working:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;- goes through a Cloudflare DNS record pointing at an Argo tunnel, which then forwards to 192.168.1.3. This works without Warp connected as it‚Äôs on the DNS level, public to everyone.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;192.168.1.3&lt;/code&gt;- The Warp client sees the request and routes it through the Argo tunnel, which then forwards it to&lt;code&gt;192.168.1.3&lt;/code&gt;within that network. This needs Warp connected to work, and is only visible to people in your Zero Trust org.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Targets&lt;/head&gt;
    &lt;p&gt;This one took me a while.&lt;/p&gt;
    &lt;p&gt;Targets are needed to define a piece of infrastructure that you want to protect through Zero Trust. They are like a pointer pointing to something in your network. This goes hand-in-hand with routes, but isn‚Äôt always needed.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs say you have 192.168.1.3 (homeassistant) exposed through a Cloudflare tunnel. By default, anyone in your network that is part of your Zero Trust org and has Warp client installed can now access your homeassistant at 192.168.1.3.&lt;/p&gt;
    &lt;p&gt;We can change that with targets. For example, defining a target with hostname = &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; to the route &lt;code&gt;192.168.1.3/32&lt;/code&gt; allows us to add access policies to it. We can also put an entire network into the target by specifying &lt;code&gt;192.168.1.3/24&lt;/code&gt; to control access. This also works with virtual IPs like 10.128.1.1!&lt;/p&gt;
    &lt;p&gt;Targets alone won‚Äôt do anything, they just point to the service or network. ‚ÄúHey, here is homeassistant‚Äù, or ‚Äúhey, here is my home network‚Äù.&lt;/p&gt;
    &lt;head rend="h2"&gt;Access Policies: Protecting Who Can Access What&lt;/head&gt;
    &lt;p&gt;Continuing the example from above:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we have a tunnel running on our home network that routes &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to&lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;we set up public DNS records to point &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to the Argo tunnel in Cloudflare&lt;/item&gt;
      &lt;item&gt;we created a route &lt;code&gt;192.168.1.3&lt;/code&gt;to go through the same tunnel&lt;/item&gt;
      &lt;item&gt;we also created a target pointing to &lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When users access either &lt;code&gt;192.168.1.3&lt;/code&gt; or &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;, the Warp client will route the request through the tunnel, which then forwards the request to 192.168.1.3. Homeassistant loads and everything is fine.&lt;/p&gt;
    &lt;p&gt;But do we want that?&lt;/p&gt;
    &lt;p&gt;Probably not.&lt;/p&gt;
    &lt;p&gt;Access policies to the rescue!&lt;/p&gt;
    &lt;p&gt;With access policies, we can leave things in the public but protect them with Cloudflare Zero Trust access. So while 192.168.1.3 is only available if Warp is connected (so routing to it works), we can add security to our public &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Go to Access -&amp;gt; Applications -&amp;gt; Add an Application -&amp;gt; Self-hosted.&lt;/p&gt;
    &lt;p&gt;Here we can define what should be protected, and how.&lt;/p&gt;
    &lt;p&gt;Going with our previous example, we can add a public hostname &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; or an IP like &lt;code&gt;192.168.1.3&lt;/code&gt; (or both), then attach policies of who should be able to access it.&lt;/p&gt;
    &lt;p&gt;You can specify Include (‚ÄúOR‚Äù) and Require (‚ÄúAND‚Äù) selectors.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Require rules must always be met, on top of include rules, to grant access&lt;/item&gt;
      &lt;item&gt;Any of the Include rules must match to grant access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then there are Actions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow - when the policy matches, allow access&lt;/item&gt;
      &lt;item&gt;Deny - when the policy matches, deny access. aka blocking something.&lt;/item&gt;
      &lt;item&gt;Bypass - when the policy matches, bypass Zero Trust completely. No more checking.&lt;/item&gt;
      &lt;item&gt;Service Auth - when the policy matches, allow authentication to the service with a service token header (good for server-to-server, or bots). Check Access -&amp;gt; Service Auth to create these tokens.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Allow public access to everyone logging into your network&lt;/head&gt;
    &lt;p&gt;The most common use case: &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; is public. We want to keep it public, but add an extra layer of security.&lt;/p&gt;
    &lt;p&gt;Add an include policy, pick any of the &lt;code&gt;email&lt;/code&gt; selectors, add the email of the user you want to allow access to. Now only people authenticated with your Zero Trust org with the specified emails can access your homeassistant, without needing to have Warp running.&lt;/p&gt;
    &lt;p&gt;We can harden this by adding require rules: Add a Login Method selector rule, pick a specific login method like GitHub. Now only people with specific emails that have authenticated through GitHub can access your homeassistant, without needing to have Warp running.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bypass login completely when connected through WARP&lt;/head&gt;
    &lt;p&gt;Another policy I like having is to skip the login screen entirely when connected through Warp. If a user is already enrolled into my Zero Trust org and has the Warp client provisioned, then there‚Äôs no need to ask them to authenticate again.&lt;/p&gt;
    &lt;p&gt;We can add a separate policy (don‚Äôt edit the one we just created above), pick the Gateway selector and set it to Allow or Bypass.&lt;/p&gt;
    &lt;p&gt;Don‚Äôt use ‚ÄòWarp‚Äô - the Warp selector will match anyone that has Warp running, including the consumer 1.1.1.1 app. Gateway, on the other hand, matches only if someone is connecting through your Gateway, be that DNS or a provisioned Warp client.&lt;/p&gt;
    &lt;p&gt;(The ‚ÄòGateway‚Äô selector is only available if the Warp client is set to allow WARP authentication identity)&lt;/p&gt;
    &lt;p&gt;Now when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Warp through Zero Trust is running on a machine: No login screen&lt;/item&gt;
      &lt;item&gt;No Warp running (public access): Prompt for login screen, but only allow specific emails that authenticated through GitHub&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This setup makes it very convenient to reach homeassistant, no matter if connected through Warp or not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Deploying the Warp client and enrolling into Zero Trust&lt;/head&gt;
    &lt;p&gt;Are you still with me?&lt;/p&gt;
    &lt;p&gt;Our network is basically done. We have a login-protected &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt; that routes through our tunnel into our private network and terminates at &lt;code&gt;192.168.1.3&lt;/code&gt;, and we have a direct route to &lt;code&gt;192.168.1.3&lt;/code&gt; that only works when connected with Warp.&lt;/p&gt;
    &lt;p&gt;We also have login policies to make sure only specific users (logged in with GitHub and certain email addresses) can access homeassistant.&lt;/p&gt;
    &lt;p&gt;So how do we deploy the dang Warp client?&lt;/p&gt;
    &lt;p&gt;Actually the same: We create some policies.&lt;/p&gt;
    &lt;p&gt;Head to Settings -&amp;gt; Warp Client&lt;/p&gt;
    &lt;p&gt;In Enrollment Permissions, we specify the same policies for who can enroll. For example, ‚Äú[email protected]‚Äù when authenticated through GitHub is allowed to enroll. In the Login Methods we can specify what login methods are available when someone tries to enroll into our Zero Trust org.&lt;/p&gt;
    &lt;p&gt;Toggle WARP authentication identity settings to make the Gateway selector available in policies, effectively allowing the configured WARP client to be used as a login method.&lt;/p&gt;
    &lt;p&gt;Careful here, once someone is enrolled, they are basically in your Zero Trust network through Warp. Make sure you harden this.&lt;/p&gt;
    &lt;p&gt;Then, in Profile settings, we define how the WARP client behaves. These are things like protocol: MASQUE or WireGuard, service mode, what IPs and domains to exclude from WARP routing (e.g., the local network should never go through WARP), setting it to exclude or include mode and so on.&lt;/p&gt;
    &lt;p&gt;Other settings I recommend setting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install CA to system certificate store - installs the Cloudflare CA certificate automatically when enrolled.&lt;/item&gt;
      &lt;item&gt;Override local interface IP - assigns a unique CGNAT private IP to the client. This is needed for warp-to-warp routing.&lt;/item&gt;
      &lt;item&gt;Device Posture - what checks the WARP client should perform for the org. E.g., check the OS version, some OS files on disk, etc. I have this set to WARP and Gateway because I want the client to provide information on whether the user is connected through WARP and Gateway, for skipping certain login pages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once done, just open the Warp client (https://developers.cloudflare.com/warp-client/), and log in to your network. This should open the login pages you specified in the Device Enrollment screen, and check all the enrollment policies you specified.&lt;/p&gt;
    &lt;p&gt;Once passed, congratulations, your WARP client is now connected to your Zero Trust network. The client will then go ahead and start routing &lt;code&gt;192.168.1.3&lt;/code&gt; through your tunnels, as specified in your tunnel and route settings.&lt;/p&gt;
    &lt;p&gt;üéâ&lt;/p&gt;
    &lt;head rend="h2"&gt;What we built&lt;/head&gt;
    &lt;p&gt;If you followed this guide, here is what we built:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Login methods to connect the Warp client to your Zero Trust org through GitHub and specific email addresses&lt;/item&gt;
      &lt;item&gt;A tunnel within your private network that&lt;list rend="ul"&gt;&lt;item&gt;Forwards any request coming in with host &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;to&lt;code&gt;192.168.1.3&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Forwards any request coming in with host &lt;/item&gt;
      &lt;item&gt;A route that forwards all traffic for &lt;code&gt;192.168.1.3&lt;/code&gt;to the tunnel in your private network, which will terminate it at 192.168.1.3, which will only work when connected through Warp to route the request&lt;/item&gt;
      &lt;item&gt;A DNS name &lt;code&gt;homeassistant.mydomain.com&lt;/code&gt;that points to the Argo tunnel, and will allow everyone (even if not connected through Warp) to access homeassistant which runs at 192.168.1.3&lt;/item&gt;
      &lt;item&gt;Access policies that will&lt;list rend="ul"&gt;&lt;item&gt;Ask users that are not connected to Zero Trust through Warp to log in with GitHub and specific email, so everyone can access it if they can log in&lt;/item&gt;&lt;item&gt;A policy that skips the login screen completely and just shows homeassistant if the user connects through Zero Trust Warp client (enrolled into our org)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You don‚Äôt need the public domain and you don‚Äôt need the route to 192.168.1.3. These are 2 different options that you can use to expose homeassistant when you‚Äôre not at home. One is using a public domain name everyone can see, one is explicitly requiring connecting through enrolled Warp.&lt;/p&gt;
    &lt;p&gt;What I didn‚Äôt cover in this post:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Warp-to-warp routing&lt;/item&gt;
      &lt;item&gt;Creating and assigning fully private IPs that only exist within your Zero Trust network&lt;/item&gt;
      &lt;item&gt;SSH authentication through Zero Trust access policies (that‚Äôs what we need Targets for)&lt;/item&gt;
      &lt;item&gt;The other application types besides Self-Hosted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm happy to expand on it if there‚Äôs interest. Let me know on X or Bluesky.&lt;/p&gt;
    &lt;p&gt;Happy tunneling! ‚õÖ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45946865</guid><pubDate>Sun, 16 Nov 2025 17:39:43 +0000</pubDate></item><item><title>The fate of "small" open source</title><link>https://nolanlawson.com/2025/11/16/the-fate-of-small-open-source/</link><description>&lt;doc fingerprint="2360927dbacf3e3b"&gt;
  &lt;main&gt;
    &lt;p&gt;By far the most popular npm package I‚Äôve ever written is &lt;code&gt;blob-util&lt;/code&gt;, which is ~10 years old and still gets 5+ million weekly downloads.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a small collection of utilities for working with &lt;code&gt;Blob&lt;/code&gt;s in JavaScript. I wrote it because I found that PouchDB users were endlessly confused about how to work with &lt;code&gt;Blob&lt;/code&gt;s and how to convert them to strings, &lt;code&gt;ArrayBuffer&lt;/code&gt;s, etc.&lt;/p&gt;
    &lt;p&gt;Given that some 80% of developers are now using AI in their regular work, &lt;code&gt;blob-util&lt;/code&gt; is almost certainly the kind of thing that most developers would just happily have an LLM generate for them. Sure, you could use &lt;code&gt;blob-util&lt;/code&gt;, but then you‚Äôd be taking on an extra dependency, with unknown performance, maintenance, and supply-chain risks.&lt;/p&gt;
    &lt;p&gt;And sure enough, Claude will happily spit out whatever &lt;code&gt;Blob&lt;/code&gt; utilities you need when prompted:&lt;/p&gt;
    &lt;p&gt;&amp;gt; Write me a utility function in TypeScript to convert a Blob to an ArrayBuffer. It should return a Promise.&lt;/p&gt;
    &lt;quote&gt;function blobToArrayBuffer(blob: Blob): Promise&amp;lt;ArrayBuffer&amp;gt; { return new Promise((resolve, reject) =&amp;gt; { const reader = new FileReader(); reader.onload = () =&amp;gt; { if (reader.result instanceof ArrayBuffer) { resolve(reader.result); } else { reject(new Error('Failed to read Blob as ArrayBuffer')); } }; reader.onerror = () =&amp;gt; { reject(reader.error); }; reader.readAsArrayBuffer(blob); }); }&lt;/quote&gt;
    &lt;p&gt;Claude‚Äôs version is pretty close to the &lt;code&gt;blob-util&lt;/code&gt; version (unsurprising, since it was probably trained on it!). Although it‚Äôs much more verbose, unnecessarily checking if &lt;code&gt;readAsArrayBuffer&lt;/code&gt; actually gives you an &lt;code&gt;ArrayBuffer&lt;/code&gt; (although this does make TypeScript happy). To be fair, it also improves on my implementation by directly &lt;code&gt;reject&lt;/code&gt;ing with an error rather than the more awkward &lt;code&gt;onerror&lt;/code&gt; event.&lt;/p&gt;
    &lt;p&gt;Note: for anyone wondering, yes Claude did suggest the new &lt;code&gt;Blob.arrayBuffer()&lt;/code&gt; method, but it also generated the above for ‚Äúolder environments.‚Äù&lt;/p&gt;
    &lt;p&gt;I suppose some people would see this as progress: fewer dependencies, more robust code (even if it‚Äôs a bit more verbose), quicker turnaround time than the old ‚Äúsearch npm, find a package, read the docs, install it‚Äù approach.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have any excessive pride in this library, and I don‚Äôt particularly care if the download numbers go up or down. But I do think something is lost with the AI approach. When I wrote &lt;code&gt;blob-util&lt;/code&gt;, I took a teacher‚Äôs mentality: the README has a cutesy and whimsical tutorial featuring Kirby, in all his blobby glory. (I had a thing for putting Nintendo characters in all my stuff at the time.)&lt;/p&gt;
    &lt;p&gt;The goal wasn‚Äôt just to give you a utility to solve your problem (although it does that) ‚Äì the goal was also to teach people how to use JavaScript effectively, so that you‚Äôd have an understanding of how to solve other problems in the future.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt know which direction we‚Äôre going in with AI (well, ~80% of us; to the remaining holdouts, I salute you and wish you godspeed!), but I do think it‚Äôs a future where we prize instant answers over teaching and understanding. There‚Äôs less reason to use something like &lt;code&gt;blob-util&lt;/code&gt;, which means there‚Äôs less reason to write it in the first place, and therefore less reason to educate people about the problem space.&lt;/p&gt;
    &lt;p&gt;Even now there‚Äôs a movement toward putting documentation in an &lt;code&gt;llms.txt&lt;/code&gt; file, so you can just point an agent at it and save your brain cells the effort of deciphering English prose. (Is this even documentation anymore? What is documentation?)&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I still believe in open source, and I‚Äôm still doing it (in fits and starts). But one thing has become clear to me: the era of small, low-value libraries like &lt;code&gt;blob-util&lt;/code&gt; is over. They were already on their way out thanks to Node.js and the browser taking on more and more of their functionality (see &lt;code&gt;node:glob&lt;/code&gt;, &lt;code&gt;structuredClone&lt;/code&gt;, etc.), but LLMs are the final nail in the coffin.&lt;/p&gt;
    &lt;p&gt;This does mean that there‚Äôs less opportunity to use these libraries as a springboard for user education (Underscore.js also had this philosophy), but maybe that‚Äôs okay. If there‚Äôs no need to find a library to, say, group the items in an array, then maybe learning about the mechanics of such libraries is unnecessary. Many software developers will argue that asking a candidate to reverse a binary tree is pointless, since it never comes up in the day-to-day job, so maybe the same can be said for utility libraries.&lt;/p&gt;
    &lt;p&gt;I‚Äôm still trying to figure out what kinds of open source are worth writing in this new era (hint: ones that an LLM can‚Äôt just spit out on command), and where education is the most lacking. My current thinking is that the most value is in bigger projects, more inventive projects, or in more niche topics not covered in an LLM‚Äôs training data. For example, I look back on my work on &lt;code&gt;fuite&lt;/code&gt; and various memory-leak-hunting blog posts, and I‚Äôm pretty satisfied that an LLM couldn‚Äôt reproduce this, because it requires novel research and creative techniques. (Although who knows: maybe someday an agent will be able to just bang its head against Chrome heap snapshots until it finds the leak. I‚Äôll believe it when I see it.)&lt;/p&gt;
    &lt;p&gt;There‚Äôs been a lot of hand-wringing lately about where open source fits in in a world of LLMs, but I still see people pushing the boundaries. For example, a lot of naysayers think there‚Äôs no point in writing a new JavaScript framework, since LLMs are so heavily trained on React, but then there goes the indefatigable Dominic Gannaway writing Ripple.js, yet another JavaScript framework (and with some new ideas, to boot!). This is the kind of thing I like to see: humans laughing in the face of the machine, going on with their human thing.&lt;/p&gt;
    &lt;p&gt;So if there‚Äôs a conclusion to this meandering blog post (excuse my squishy human brain; I didn‚Äôt use an LLM to write this), it‚Äôs just that: yes, LLMs have made some kinds of open source obsolete, but there‚Äôs still plenty of open source left to write. I‚Äôm excited to see what kinds of novel and unexpected things you all come up with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45947639</guid><pubDate>Sun, 16 Nov 2025 19:21:13 +0000</pubDate></item><item><title>The Pragmatic Programmer: 20th Anniversary Edition (2023)</title><link>https://www.ahalbert.com/technology/2023/12/19/the_pragmatic_programmer.html</link><description>&lt;doc fingerprint="f74bd85969a12f9c"&gt;
  &lt;main&gt;
    &lt;p&gt;The Pragmatic Programmer: From Journeyman to Master by Dave Thomas and Andrew Hunt was given to me as a gift after an internship. The book gave me invaluable advice as I started out in my career as a professional software engineer. Re-reading it a decade later, I thought the general advice still held up well, but it made references to technologies such as CORBA that are no longer used and felt dated as a result. The authors agreed and wrote a 20th anniversary edition that was updated for modern developers. A third of the book is brand-new material, covering subjects such as security and concurrency. The rest of the book has been extensively rewritten based on the authors√¢ experience putting these principles into practice. We discussed the 20th anniversary edition in my book club at work.√Ç&lt;/p&gt;
    &lt;p&gt;The book is meant for those just starting out in the world of professional software engineering. Many of the tips, such as Tip 28: Always Use Version Control will seem obvious to experienced hands. However, it can also be a guide for senior developers mentoring junior developers, putting actionable advice into words. The book is also valuable to those who lack a formal CS education; it explains things like big-O notation and where to learn more about these subjects. I think that any software engineer will get one or two things out of this book, though it√¢s most valuable for beginners.&lt;/p&gt;
    &lt;p&gt;One of the things I appreciate about the book is that they talk about applying the principles not only to software engineering but to writing the book as well. The book was originally written in troff and later converted to LaTeX. For example, to illustrate Tip 29: Write Code That Writes Code they wrote a program to convert troff markup to LaTeX. In the 20th anniversary edition, they talk about their efforts to use parallelism to speed up the book build process and how it led to surprising bugs.&lt;/p&gt;
    &lt;p&gt;Perhaps the best thing about the book is that the authors summarize their points into short tips highlighted throughout the book. The authors helpfully attach these tips to a card attached to the physical book. This makes it easy to remember the principles espoused in the book and to refer to them later. I think this is a feature that more books should include, especially managerial or technical books.&lt;/p&gt;
    &lt;p&gt;The first chapter is less about coding and more about the general principles a pragmatic programmer follows. Most of all, it√¢s about taking responsibility for your work. The first tip of the chapter is Tip 3: You Have Agency: if you don√¢t like something, you can be a catalyst for change. Or you can change organizations if change isn√¢t happening. The most important tip of the chapter to me is Tip 4: Provide Options, Don√¢t Make Lame Excuses. In this section, they discuss taking responsibility for the commitments you make and having a contingency plan for things outside your control. If you don√¢t meet the commitment, provide solutions to fix the problems. Don√¢t tell your boss, √¢The cat ate my source code.√¢&lt;/p&gt;
    &lt;p&gt;Software rots over time without efforts to fix it. The authors talk about broken windows policing, the theory that minor problems such as a single broken window give people the psychological safety to commit larger crimes. Regardless of whether broken windows policing is actually true, the metaphor applies to software. This leads to Tip 5: Don√¢t Live with Broken Windows: If you see a broken window in your software, make an effort to fix it, even if it√¢s only a minor effort to board it up. This may seem impractical if your project already has a lot of broken windows, but this tip helps you avoid creating such an environment in the first place. In my experience, it works: when we set up a new project at work, we made a commitment to use git commit hooks to enforce coding standards. This made each of us more reluctant to compromise on software to begin with, and all of the code was a good example to copy from.&lt;/p&gt;
    &lt;p&gt;A pragmatic programmer is always learning, and learns things outside their specialty; they are a jack of all trades. Even if they are a specialist in their current role, they invest regularly in a broad knowledge portfolio. In addition to software skills, people skills are important as well. The section √¢Communicate!√¢ shows how to effectively communicate your ideas, such as how to present, what to say, and how pick the right time. In the words of Tip 11: English is Just Another Programming Language. If you don√¢t have an answer to an email immediately, respond with an acknowledgment and that you√¢ll get back to them later - nobody wants to be talking to a void. Don√¢t be afraid to reach out for help if you need it; that√¢s what your colleagues are there for, after all. And don√¢t neglect documentation! Make it an integral part of the development process, not an afterthought.&lt;/p&gt;
    &lt;p&gt;Finally, the principles in this book are not iron-clad: you must consider the tradeoffs between different values and make the right decision for your project. Your software does not need to be perfect. When working on software, involve your users in deciding what quality issues are acceptable in return for getting it out faster. After all, if you wait a year to ship the perfect version, their requirements will change anyways. As Tip 8 says: Make Quality a Requirements Issue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Why is decoupling good? Because by isolating concerns we make each easier to change. ETC.&lt;/p&gt;&lt;lb/&gt;Why is the single responsibility principle useful? Because a change in requirements is mirrored by a change in just one module. ETC.&lt;lb/&gt;Why is naming important? Because good names make code easier to read, and you have to read it to change it. ETC!&lt;/quote&gt;
    &lt;p&gt;However, the authors also stress that ETC is a value, not a rule. For example, ETC may not be appropriate for writing code that has high performance requirements; making the code complex to achieve the performance requirements is an acceptable tradeoff.√Ç&lt;/p&gt;
    &lt;p&gt;They then turn to another important acronym for implementing ETC in Tip 15: DRY√¢Don√¢t Repeat Yourself. DRY makes things easier to change by having one place to change anything. Worse, if you forget to make a change, you√¢ll have contradictory information in your program that could crash it or silently corrupt data.&lt;/p&gt;
    &lt;p&gt;A closely related principle to DRY is Orthogonality. Two components of a software system are orthogonal if changes in one do not effect the other. Systems should be designed as a set of cooperating independent modules, each of which has a single, well-defined purpose. Modules communicate between themselves using well defined interfaces and don√¢t rely on shared global data or the implementation details of another module. Unless you change a component√¢s external interfaces, it should not cause changes in the rest of the system. Orthogonal systems are easier to test, because more testing can be done at the module level in unit tests rather than end-to-end integration tests that test the whole system.&lt;/p&gt;
    &lt;p&gt;Often, when starting a software project, there are a lot of unknowns. The user has an idea of what they want, but there√¢s some ambiguity in the requirements. You don√¢t know if the library and frameworks you pick will work nicely together. The solution here is Tip 20: Use Tracer Bullets to Find the Target. In a machine gun, tracer bullets are bullets that glow in the air, enabling the user to see if they√¢re hitting the target at night. Tracer Bullet Development provides that kind of immediate feedback. Look for a single feature that can be built quickly using the architectural approach you√¢ve chosen, and put that in front of the users. You may miss; users may say that√¢s not quite what they wanted. But that√¢s the point of tracer code: it allows you to adjust your aim with a skeleton project that√¢s easier to change than a final application. Users will be delighted to see something working early, and you√¢ll have an integration platform to build the rest of the application on.&lt;/p&gt;
    &lt;p&gt;Tracer code is different from prototypes. To the authors, prototypes are disposable code used to learn about a problem domain, never meant to be used in production. Prototypes don√¢t even have to be code. A UI can be mocked up in an interface builder, or an architecture mapped out with post-it notes. In terms of Tip 21: Prototype to Learn. In contrast, tracer bullet code is meant to be part of the final application.&lt;/p&gt;
    &lt;p&gt;The final tip of this chapter I bring up is Tip 18: There Are No Final Decisions. Decisions should be reversible; if you rely on MySQL today, you may find yourself needing to switch to Postgres six months from now. If you√¢ve properly abstracted the database logic, making this change should be easy. Marketing may decide that your web app should be a mobile app in the future; if your architecture is built well, this extra demand should not be a burden. This is one tip I disagree with: I think it can easily be taken too far. If you provide too much reversibility, you√¢ll end up with over-abstracted code with configuration options that are never used. I think it√¢s more reasonable to think about what decisions can reasonably change and make them flexible; if you spend all your time trying to cover for every possibility, you√¢ll never get around to actually coding the required functionality.&lt;/p&gt;
    &lt;p&gt;This chapter focuses on how to make the most out of your tools, what tools to invest in, and how to approach debugging. The first bit of advice: Tip 25: Keep Knowledge in Plain Text. By plain text, they mean keep knowledge such as configuration or data in a simultaneously human-readable and computer readable format. Plain text insures you against obsolesce; you can always write something to parse it later, while reverse-engineering a binary format is significantly harder. In addition, almost any other tool in existence can process plain text in some way, so you√¢ll have an extensive suite of other tools to use. As an extension of the power of plain text, they also suggest you master a command shell such as &lt;code&gt;bash&lt;/code&gt;. Shells provide a family of tools that are composable with each other, and can be combined as much as your imagination allows. A GUI in contrast, limits you to the actions the programmers of the GUI thought of in advance. Finally, you should learn a text processing language such as &lt;code&gt;awk&lt;/code&gt; or &lt;code&gt;perl&lt;/code&gt; to get the most out of text - the authors used perl (first edition) and ruby (20th anniversary edition) to automatically highlight the source code in the book, for example.&lt;/p&gt;
    &lt;p&gt;The next topic the authors turn to is debugging. Debugging is the main task a software engineer does throughout their day, so it√¢s essential you get good at it. Defects show up in a variety of ways, from misunderstood requirements to coding errors. Some cultures try to find someone to blame for a defect; the authors think you should avoid that with Tip 29: Fix the Problem, Not the Blame.&lt;/p&gt;
    &lt;p&gt;They give the following tips on debugging your code:&lt;/p&gt;
    &lt;p&gt;Once you√¢ve solved the bug, however there√¢s still one more step: you should write a test to catch that bug in the future.&lt;/p&gt;
    &lt;p&gt;Tip 36: You Can√¢t Write Perfect Software starts off the chapter. While we√¢d like to write perfect software, there will always be bugs, poor design decisions, and missing documentation. The theme of this chapter is how to design this fact in mind.&lt;/p&gt;
    &lt;p&gt;The first idea they propose is Design By Contract. Similar to legal contracts, it explains a function or module√¢s rights and responsibilities. A contract has three parts: It has Preconditions: things that must be true when it is called, such as what qualifies as valid inputs. Postconditions are what will be true when it is done, such as a sort routine returning a sorted array. Finally, Invariants are things that are always true from the caller√¢s perspective - they may change while the routine is running, but will hold at the beginning and the end of the call. For example, in a sort routine, the invariant is that the list to be sorted will contain the same number of items when it started as when it finished. If the contract is violated, the contract will specify what to do, such as crash or throw an exception.&lt;/p&gt;
    &lt;p&gt;Some languages, such as Clojure have built-in semantics for design by contract, with explicit pre- and post- conditions. However, if your language doesn√¢t support contracts, you can implement them with Tip 39: Use Assertions to Prevent the Impossible. You can assert that the conditions of your contract are true, and handle the cases where the contract is violated. If you don√¢t know what to do when a contract is violated, the authors recommend Tip 38: Crash Early. It√¢s better that you crash rather than write incorrect data to the database. After all, dead programs tell no lies. Of course, crashing immediately may not be appropriate - if you have resources open make sure to close them before exiting.&lt;/p&gt;
    &lt;p&gt;The final paranoid tip is Tip 43: Avoid Fortune-Telling. Pragmatic programmers only make decisions that they can get immediate feedback on. The more predictions you make about the future, the more likely you√¢ll get some of the predictions wrong and make the wrong decision based on them.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You might find yourself slipping into fortune telling when you have to:&lt;/p&gt;
      &lt;item&gt;Estimate completion dates months in the future&lt;/item&gt;
      &lt;item&gt;Plan a design for future maintenance or extendability&lt;/item&gt;
      &lt;item&gt;Guess user√¢s future needs&lt;/item&gt;
      &lt;item&gt;Guess future tech availability&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;In a previous chapter, the authors wrote about making decisions reversible and easier to change. This chapter tells you how to implement it in your code. The key here is to make your code flexible rather than rigid - good code bends to circumstances rather than breaks. Part of this is decoupling code. Code is consider coupled when they share something in common. This may be something as simple as a shared global variable, or something more complex like an inheritance chain.&lt;/p&gt;
    &lt;p&gt;The authors argue against what they term Train Wrecks - long chains of method calls, such as this example they give:&lt;/p&gt;
    &lt;code&gt;public void applyDiscount(customer, order_id, discount) { 
	totals = customer
			  .orders 
			  .find(order_id) 
			  .getTotals();

	totals.grandTotal = totals.grandTotal - discount;
	totals.discount = discount; 
}

&lt;/code&gt;
    &lt;p&gt;This code is traversing many different levels of abstraction - you have to know that a customer object exposes orders, that orders have a &lt;code&gt;find&lt;/code&gt; method, and that the order &lt;code&gt;find&lt;/code&gt; returns has a &lt;code&gt;getTotal&lt;/code&gt; method. If any of these levels of abstraction are changed, your code might break. And requirements may change; What if the business decides to implement a maximum discount amount of 40%? Certainly, this could be applied in the &lt;code&gt;applyDiscount&lt;/code&gt; routine, but anything could modify the &lt;code&gt;grandTotal&lt;/code&gt; and &lt;code&gt;discount&lt;/code&gt; fields - this rule could be violated if other modules modifying the totals object don√¢t get the memo.&lt;/p&gt;
    &lt;p&gt;The authors suggest refactoring the code so that there is no orders object, just a &lt;code&gt;find&lt;/code&gt; method and an applyDiscount method for the order object that implements the 40% rule:&lt;/p&gt;
    &lt;code&gt;public void applyDiscount(customer, order_id, discount) { 
	customer
	.findOrder(order_id)
	.applyDiscount(discount); 
}
&lt;/code&gt;
    &lt;p&gt;The authors suggest having only one . when you access something if that something is likely to change, such as anything in your application, or a fast moving external API. This includes using intermediate variables between accesses, such as this code:&lt;/p&gt;
    &lt;code&gt;# This is cheating!
orders = customer.orders
order = orders.find(order_id)
totals = order.getTotals
&lt;/code&gt;
    &lt;p&gt;However, the rule does not apply to things that are unlikely to change, such as core language APIs. So this code is ok:&lt;/p&gt;
    &lt;code&gt;people
.sort_by {|person| person.age } 
.first(10)
.map {| person | person.name }
&lt;/code&gt;
    &lt;p&gt;Another source of coupling is globally accessible data. Global data makes it hard to reason about the state of a program, since any other module might be able to change it. Global data includes design patterns such as singletons, and external resources such as databases. Given how extensive global resources are, how can one avoid them? If global data is unavoidable, the key is to manage them through a well-defined API that you control, rather than allowing anything to read and write global data. In the words of Tip 48: If It√¢s Important Enough to Be Global, Wrap It in an API.&lt;/p&gt;
    &lt;p&gt;Poor use of inheritance is a third source of coupling. Inheritance is used for two reasons: code reuse and type modeling. Inheritance doesn√¢t work for code reuse; Not only is the code of a child class coupled to any ancestor of the class, so is any code that uses the class. Things may unexpectedly break when an ancestor changes an API, even if you are using a subclass.&lt;/p&gt;
    &lt;p&gt;Nor does inheritance work for modeling types. Class hierarchies quickly become tangled, wall covering monstrosities. Another problem is multiple inheritance. A &lt;code&gt;Car&lt;/code&gt; may be a type of &lt;code&gt;Vehicle&lt;/code&gt;, but it may be an &lt;code&gt;Asset&lt;/code&gt; or &lt;code&gt;InsuredItem&lt;/code&gt;. Multiple inheritance is required to model this, and many OO languages don√¢t support multiple inheritance. Instead of paying the inheritance tax, the authors suggest using:&lt;/p&gt;
    &lt;p&gt;Interfaces or Protocols are classes that contain no code but instead contains behaviors. A class that implements an interface promises to define the behaviors. For example, a &lt;code&gt;Car&lt;/code&gt; might implement &lt;code&gt;Drivable&lt;/code&gt; which has methods such as &lt;code&gt;accelerate&lt;/code&gt; and &lt;code&gt;brake&lt;/code&gt;. Interfaces can be used as types, and any class that implements the interface will be compatible with that type. This is a much easier way to provide polymorphism than inheritance.&lt;/p&gt;
    &lt;p&gt;Another alternative to inheritance is delegation. If you want to include behavior from class &lt;code&gt;Foo&lt;/code&gt; add a member of type &lt;code&gt;Foo&lt;/code&gt; to your class rather than inherit from &lt;code&gt;Foo&lt;/code&gt;. You can then use Foo√¢s API wrapped in code you control. Delegation is a has-a relationship rather than a is-a relationship.&lt;/p&gt;
    &lt;p&gt;The problem with interfaces and delegation is that they require writing lots of boilerplate code. For example, it√¢s likely that most of your classes that implement &lt;code&gt;Drivable&lt;/code&gt; will have the same logic for &lt;code&gt;brake&lt;/code&gt;, but each class will have to write it√¢s own implementation of &lt;code&gt;brake&lt;/code&gt;. This leads to repeated code across your codebase, violating the DRY principle. To resolve this, the authors turn to Mixins - sets of functions that can be √¢mixed into√¢ a class. This allows you to add common functionality without using inheritance. I wonder how mixins are implemented in a language like Java, which doesn√¢t have an obvious version of that feature. It√¢s also not clear to me how mixins are different from inheritance; aren√¢t they just a form of multiple inheritance?&lt;/p&gt;
    &lt;p&gt;Tip 55: Parameterize Your App Using External Configuration: Code may have values that change while the application is running, such as credentials for for third-party services. Rather than directly including the values in your code, you should externalize them and put them in a configuration bucket. Keeping credentials in source code is a security risk - hackers scan public git repositories for common security credentials, such as AWS keys. It√¢s common to store them in a flat file or database tables, and read them when the application initializes. However, in our world of highly-available applications that√¢s not as appropriate. Instead the authors propose configuration-as-a-service, where configuration is stored behind a service API. This allows multiple applications to share configuration information, use access control to control who can see and edit configuration, and provide a UI to easily edit config information. Using the configuration service, applications can subscribe to a configuration item and get notifications when they change. This allows applications to update config data on their side without restarting.&lt;/p&gt;
    &lt;p&gt;This chapter deals with Parallelism, where two pieces of code run at the same time, and Concurrency, where things act as if they run at the same time. In the real world, things are asynchronous - the user is supplying input, network resources are called, and the screen is being redrawn all at the same time. Applications that run everything serially feel sluggish.&lt;/p&gt;
    &lt;p&gt;In Tip 56: Analyze Workflow to Improve Concurrency the authors advocate that you break temporal coupling where possible. Temporal Coupling is when your code depends on event A happening before event B. You should look at your workflow to see what can be executed concurrently. Look for activities that take a lot of time that would allow for something else to be done in the meantime. If your application makes multiple independent API calls to a remote service, execute them on separate threads rather than serially, then gather up the results of each call. If your workflow allows a way to split the work into multiple independent units, take advantage of those multiple cores and execute them in parallel.&lt;/p&gt;
    &lt;p&gt;Of course, parallelism has its pitfalls as well. For example, imagine reading an integer, incrementing it, and writing it back. If two processes read that integer at the same time, they will each increment the value to n+1, when you want it to be n+2. The update needs to be atomic; each process needs to do this sequentially without the other process interfering. This can be done through synchronized methods, semaphores, or other forms of resource locking. However, they have their own dangers as well, such as deadlocking, where two processes each get a lock on one of two needed resources, but not the other. Each waits forever for the other to release its lock. The authors think you should avoid shared state rather than try to handle yourself wherever possible; Tip 57: Shared State Is Incorrect State.&lt;/p&gt;
    &lt;p&gt;The authors ran into this issue when writing the 20th anniversary edition: they updated the build process for the book to utilize parallelism. However, the build would randomly fail. The authors tracked this down to changing the directory temporarily. In the original, a subtask would change directory, then go back to the original directory. However, this no longer worked when new threads started, expecting to be in the root directory. Depending on the timing, this could break the build. This prompted them to write Tip 58: Random Failures Are Often Concurrency Issues.&lt;/p&gt;
    &lt;p&gt;This chapter is more of a grab-bag. It covers subjects such as psychology, big-O notation, refactoring, security, and testing.&lt;/p&gt;
    &lt;p&gt;In Tip 61: Listen to Your Inner Lizard the authors talk about listening to your instincts (your lizard-brain). If you find yourself having a hard time writing code, your brain is trying to tell you something. Perhaps the structure or design is wrong, or you don√¢t fully understand the requirements. If you find yourself in this situation, take a step back and think about what you are doing. Maybe go for a walk, or sleep on it. You might find that the solution is staring you in the face when you come back.&lt;/p&gt;
    &lt;p&gt;Perhaps you need to refactor the code instead of writing more. Refactoring is a continuous process, espoused in Tip 65: Refactor Early, Refactor Often. If anything strikes you as wrong in your code, such as DRY violations, outdated knowledge or non-orthogonal design, don√¢t hesitate to fix it. When you are refactoring, make sure you have a good suite of unit tests beforehand to test if your changes break anything. Run the tests frequently to check if you√¢ve broken anything.&lt;/p&gt;
    &lt;p&gt;Speaking of tests, the authors start with a bold assertion: Tip 67: Testing Is Not About Finding Bugs. Instead, tests function as the First User of Your Code - a source of immediate feedback, and immediately forces you to think about what counts as a correct solution. In addition, tightly coupled code tends to be hard to test, so it helps you make good design decisions. The authors emphatically do not think you should adopt full-on Test Driven Development - it√¢s too easy to become a slave to writing tests. They note an example of a TDD advocate starting a sudoku solver using TDD and spent so much time writing the tests they failed to write the solver itself!&lt;/p&gt;
    &lt;p&gt;In a sidebar, Dave Thomas explains that he stopped writing tests for a few months, and said √¢not a lot√¢ happened. The quality didn√¢t drop, nor did he introduce bugs into the code. His code was still testable, it just wasn√¢t tested.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Andy says I shouldn√¢t include this sidebar. He worries it will tempt inexperienced developers not to test. Here√¢s my compromise: Should you write tests? Yes. But after you√¢ve been doing it for 30 years, feel free to experiment a little to see where the benefit lies for you.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This chapter focuses on how to start your project on the right foot. The first subject the authors tackle is requirements gathering: The Requirements Pit. While we talk about gathering requirements as if they are on the ground, waiting to be picked up, requirements are non-obvious because of Tip 75: No One Knows Exactly What They Want. They think of requirements gathering as a kind of therapy, where you take an initial requirement and ask questions about the details to nail down exactly what they need. The authors show an example of a simple requirement: √¢Shipping should be free on all orders costing $50 or more√¢. Does that include the shipping cost itself? Tax? If you√¢re selling ebooks as well, should they be included? The job of the programmer is Tip 76: Programmers Help People Understand What They Want. You should find any edge cases the client may not have considered and make sure they√¢re documented. This doesn√¢t mean creating long specifications the client won√¢t read. Instead, the authors think requirements should be able to fit on an index card. This helps prevent feature creep; if the client understands how adding one more index card will impact the schedule, they√¢ll consider the tradeoffs and prioritize the requirements they need the most.&lt;/p&gt;
    &lt;p&gt;You are given constraints in your requirements as well. Your job as a software engineer is to evaluate if those constraints are things you actually have to live with or if you can relax them. In the words of Tip 81: Don√¢t Think Outside the Box√¢Find the Box, the constraints are the edges of the box. What you initially thought of as a constraint may actually be an assumption you held.&lt;/p&gt;
    &lt;p&gt;Another tip the authors advocate for is Tip 78: Work with a User to Think Like a User. If you√¢re building an inventory system, work in the warehouse for a few days to get an idea of their processes and how your system will be used. If you don√¢t understand how it will be used, you could create something that meets all of the requirements but is totally useless. They cite an example of a digital sound mixing board that could do anything to sound that was possible, yet nobody wanted to use it. Rather than take advantage of recording engineers√¢ experience with tactile sliders and knobs, they built an interface that was unfamiliar to them. Each feature was buried behind menus and given unintuitive names. √Ç It did what was required, but didn√¢t do it how it was required.√Ç&lt;/p&gt;
    &lt;p&gt;The authors also consider in this chapter what it means to be Agile. Many teams and companies are eager for an off-the-shelf solution: call it Agile-in-a-Box. But no process can make you Agile; √¢Use this process and you√¢ll be agile√¢ ignores a key part of the Agile manifesto: Individuals and interactions over processes and tools. To the authors Agile can be boiled down to the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Work out where you are.&lt;/item&gt;
      &lt;item&gt;Make the smallest meaningful step towards where you want to be.&lt;/item&gt;
      &lt;item&gt;Evaluate where you end up, and fix anything you broke.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;Do this for every level of what you do, from process to code, and you√¢ll have adopted the Agile spirit.&lt;/p&gt;
    &lt;p&gt;Can the lessons of The Pragmatic Programmer be applied to teams too? The authors say yes. This chapter focuses on how to apply the lessons of the previous chapters to the team level. Many of the lessons are the same as those mentioned previously, so I won√¢t go into them again.&lt;/p&gt;
    &lt;p&gt;The authors advise Tip 87: Do What Works, Not What√¢s Fashionable. Just because Google or Facebook adopts process $ x $ doesn√¢t mean it√¢s right for your team. How do you know if something works? Try it. Pilot an idea with a small team, and see what works about it and what doesn√¢t. The goal isn√¢t to √¢do Scrum√¢ or √¢be Agile√¢, but to deliver working software continuously. When you adopt a new idea, you should do it with improving continuous deployment of software in mind. If you√¢re measuring your deployments in months, try to get it down to weeks instead. Once you get it down to weeks, try to deliver in one-week iterations.&lt;/p&gt;
    &lt;p&gt;Related to continuously delivering software is Tip 96: Delight Users, Don√¢t Just Deliver Code. Delivering working software in a timely matter is not enough to delight your users; that is merely meeting expectations. The authors suggest you ask your users a question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How will you know that we√¢ve all been successful a month (or a year, or whatever) after this project is done?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The answer may not be related to the requirements, and may surprise you. For example, a recommendations engine might be valued on driving customer retention. But once you know what the secret to success is, you should aim not just to hit the goal but to exceed it.&lt;/p&gt;
    &lt;p&gt;Finally, take pride in your work. The final tip of the book is Tip 97: Sign Your Work.&lt;/p&gt;
    &lt;p&gt;I was only able to cover a portion of this remarkable book in this review. I highly recommend this book to any software engineer, especially to those just starting out in the field. It makes a great graduation gift to someone just finishing their CS degree.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45948254</guid><pubDate>Sun, 16 Nov 2025 20:46:30 +0000</pubDate></item><item><title>Why Castrol Honda Superbike crashes on (most) modern systems</title><link>https://seri.tools/blog/castrol-honda-superbike/</link><description>&lt;doc fingerprint="b601d2142fe5fb84"&gt;
  &lt;main&gt;
    &lt;p&gt;Posted: 2025-11-16&lt;/p&gt;
    &lt;head rend="h1"&gt;Why Castrol Honda Superbike crashes on (most) modern systems&lt;/head&gt;
    &lt;p&gt;A friend cleaned up and gave me a copy of a game I've not heard about before: Castrol Honda Superbike World Champions, a motorbike racing game for PC, released 1998 by Interactive Entertainment Ltd. and Midas Interactive Entertainment.&lt;/p&gt;
    &lt;p&gt;Given the age of the game (and looking at the system requirements) it's clear that the game comes from the tricky era of early 3D-accelerated PC gaming. For context, my copy of the game helpfully asks to install DirectX 5.&lt;/p&gt;
    &lt;p&gt;Before Windows was known for cramming AI and account requirements into every single corner of the system, no matter how unnecessary, it was known for its excellent backwards compatibility with older software. Generally, unless there are genuine bugs (and sometimes even despite them), Windows tries its hardest to run old applications correctly.&lt;/p&gt;
    &lt;p&gt;Pushing my luck and trying to run it on my Windows 7 machine, however, resulted in either a getting stuck on a black screen, or a crash, seemingly at random:&lt;/p&gt;
    &lt;p&gt;Let's go back in time and see how far we need to go to get it running: Installing and running it on my Windows 98 and Windows XP machines was as uneventful, and the game works just fine1, including with 3D acceleration. Glorious 1024x768x16:&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging the issue #&lt;/head&gt;
    &lt;p&gt;Debugging is more fun than playing, so let's get started! :^)&lt;/p&gt;
    &lt;p&gt;I pulled over the installation directory to my main machine and ran Detect It Easy to see what we can learn about the executable:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Linker: Microsoft Linker(5.10)&lt;/code&gt; and &lt;code&gt;Compiler: Microsoft Visual C/C++(...)[libcmtd]&lt;/code&gt; are the interesting bits here. Notice how it's &lt;code&gt;libcmtd&lt;/code&gt;, not &lt;code&gt;libcmt&lt;/code&gt;? The binary is linked against the static debug version of VC5's runtime. The debug runtime has heaps of extra checks and logging, which might help later.&lt;/p&gt;
    &lt;p&gt;Let's attach a debugger and see what's going on. Given that the game crashes very early on (before the credits intro screen), I hoped to see something right away. The cases where the game got stuck in a loop seemed to actually get stuck in some Windows API call stack.&lt;/p&gt;
    &lt;p&gt;Anyway, the cases where it crashed gave a clearer starting point:&lt;/p&gt;
    &lt;p&gt;The game seems to be stuck after a call to DirectInput's &lt;code&gt;DirectInputCreateEx&lt;/code&gt; function. At this point I started to do some static analysis of the functions leading to this call. While doing that I noticed that the game seems to have quite extensive logging, anything from game initialization to memory allocations.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;If you're interested in all the logs, here are the config settings to enable them all:&lt;/p&gt;&lt;item&gt;In&lt;/item&gt;&lt;code&gt;Config.dat&lt;/code&gt;, switch&lt;code&gt;ErrorLog&lt;/code&gt;,&lt;code&gt;FileLog&lt;/code&gt;,&lt;code&gt;MallocLog&lt;/code&gt;from&lt;code&gt;off&lt;/code&gt;to&lt;code&gt;on&lt;/code&gt;.&lt;item&gt;These are the "normal" log files the game produces.&lt;/item&gt;&lt;code&gt;ErrorLog&lt;/code&gt;produces&lt;code&gt;error.log&lt;/code&gt;, which is the general log file.&lt;code&gt;FileLog&lt;/code&gt;produces&lt;code&gt;files.log&lt;/code&gt;, tracking all opened files and their access modes.&lt;code&gt;MallocLog&lt;/code&gt;produces&lt;code&gt;malloc.log&lt;/code&gt;, tracking all memory allocations and frees. The devs even kept descriptions for every allocation site!&lt;item&gt;Set an environment variable named&lt;/item&gt;&lt;code&gt;errorfile&lt;/code&gt;to any file name (not path). The game will write logs to that file in the game directory.&lt;item&gt;You might also need to create an empty&lt;/item&gt;&lt;code&gt;*.c&lt;/code&gt;file in the game directory.&lt;item&gt;Just gives a bit of extra logging.&lt;/item&gt;&lt;p&gt;Bonus: add a setting named&lt;/p&gt;&lt;code&gt;windowed=true&lt;/code&gt;to the config file to force windowed mode; only works correctly in 16-bit mode (garbled graphics in True Color).&lt;/quote&gt;
    &lt;p&gt;After enabling all the logging, I ran the game a few more times, and noticed that the last log messages in &lt;code&gt;error.log&lt;/code&gt; before the crash were these:&lt;/p&gt;
    &lt;code&gt;0&amp;gt; Instance : Mouse
0&amp;gt; Product : Mouse
1&amp;gt; Instance : Keyboard
1&amp;gt; Product : Keyboard
2&amp;gt; Instance : Gaming Mouse G502
2&amp;gt; Product : Gaming Mouse G502
3&amp;gt; Instance : Gaming Mouse G502
3&amp;gt; Product : Gaming Mouse G502
4&amp;gt; Instance : Gaming Mouse G502
4&amp;gt; Product : Gaming Mouse G502
5&amp;gt; Instance : Gaming Mouse G502
5&amp;gt; Product : Gaming Mouse G502
6&amp;gt; Instance : USB Keyboard
6&amp;gt; Product : USB Keyboard
7&amp;gt; Instance : USB Keyboard
7&amp;gt; Product : USB Keyboard
8&amp;gt; Instance : LED Controller
8&amp;gt; Product : LED Controller
&lt;/code&gt;
    &lt;p&gt;Great, the game is enumerating input devices‚Äî uh, why is there an "LED Controller" device? The motherboard in my Windows 7 machine has a built-in LED controller, so that checks out. Maybe the detection isn't working properly, and the game is trying to use it as an input device?&lt;/p&gt;
    &lt;p&gt;After disabling the LED controller in Device Manager, the game started up just fine, consistently! So far, so good. Of course I wanted to know what was actually going wrong, though, so let's see where these messages are printed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Side quest: CD check #&lt;/head&gt;
    &lt;p&gt;The game seemed to close without any notice if I forgot to insert the game disc. A quick trace showed that the &lt;code&gt;GibbonPosture&lt;/code&gt; setting in &lt;code&gt;f1.cfg&lt;/code&gt; is used to point to the disc drive from which the game was installed. The only check seems to be that the path &lt;code&gt;redist\dsetup.dll&lt;/code&gt; exists on the disc. Copying the redist folder to the installation directory and changing the setting to &lt;code&gt;GibbonPosture=.\&lt;/code&gt; seems to work just fine. :)&lt;/p&gt;
    &lt;head rend="h2"&gt;The bug #&lt;/head&gt;
    &lt;p&gt;Finding the &lt;code&gt;Instance :&lt;/code&gt; and &lt;code&gt;Product :&lt;/code&gt; log messages in the binary was easy enough. They are referenced in only one function, which is a &lt;code&gt;DIEnumDevicesCallback&lt;/code&gt; callback function that is provided to &lt;code&gt;IDirectInput::EnumDevices&lt;/code&gt; (Microsoft has only kept the documentation for the DX8 version of EnumDevices left online, but it's close enough).&lt;/p&gt;
    &lt;p&gt;This is the pseudocode of the call and the callback, and the relevant data structure:&lt;/p&gt;
    &lt;code&gt;struct DinputDeviceData
{
  char instance_name[128];
  char product_name[128];
  DWORD dwDevType;
  GUID guid;
};

// ...

BOOL __stdcall dinput_enumdevices_callback(LPCDIDEVICEINSTANCEA lpDevice, LPVOID pvRef)
{
    int index = g_dinput_device_index;
    g_direct_input_devices[index].guid = lpDevice-&amp;gt;guidInstance;
    strcpy(g_direct_input_devices[index].instance_name, lpDevice-&amp;gt;tszInstanceName);
    strcpy(g_direct_input_devices[index].product_name, lpDevice-&amp;gt;tszProductName);
    g_direct_input_devices[index].dwDevType = lpDevice-&amp;gt;dwDevType;

    log_line("%d&amp;gt; Instance : %s\n", index, lpDevice-&amp;gt;tszInstanceName);
    log_line("%d&amp;gt; Product : %s\n", index, lpDevice-&amp;gt;tszProductName);

    if ( LOBYTE(g_direct_input_devices[index].dwDevType) == DIDEVTYPE_JOYSTICK )
    {
        int joystick_index = g_joystick_index;
        g_joystick_info[joystick_index].dinput_device_index = index;
        g_joystick_info[joystick_index].field_4 = 0;
        g_joystick_info[joystick_index].field_8 = 0;
        g_joystick_info[joystick_index].field_38 = 0;
        g_joystick_info[joystick_index].field_1 = 0;
        g_joystick_index = joystick_index + 1;
    }
    g_dinput_device_index = index + 1;

    return DIENUM_CONTINUE;
}

// ...

g_dinput_create_hresult = DirectInputCreateA(hInstance, 0x500u, &amp;amp;g_dinput_instance, 0);
g_dinput_device_index = 0;
g_joystick_index = 0;
g_dinput_instance-&amp;gt;lpVtbl-&amp;gt;EnumDevices(
    g_dinput_instance, 0, dinput_enumdevices_callback, 0, DIEDFL_ATTACHEDONLY);
&lt;/code&gt;
    &lt;p&gt;So, for each enumerated device, the game stores some general information about it in the global array &lt;code&gt;g_direct_input_devices&lt;/code&gt;. Then, if the device is a joystick (generally, a game controller), it also adds it to &lt;code&gt;g_joystick_info&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Can you guess the bug yet? :) If not, here's the declaration of the global arrays:&lt;/p&gt;
    &lt;code&gt;DinputDeviceData g_direct_input_devices[8];
// ...
JoystickInfo g_joystick_info[8];
&lt;/code&gt;
    &lt;p&gt;There's only space for eight DirectInput devices in the array! &lt;code&gt;8&amp;gt; Instance : LED Controller&lt;/code&gt; was the ninth one, overwriting lots of important other data in the process, including timer handles and the actual DirectInput instance pointer.&lt;/p&gt;
    &lt;p&gt;But it gets worse: The game uses DirectInput for game controllers only. Copying the device info out of &lt;code&gt;lpDevice&lt;/code&gt; is entirely pointless for other types of devices. Just moving the &lt;code&gt;DIDEVTYPE_JOYSTICK&lt;/code&gt; check up would have hidden the bug for basically all setups, since you'd have to have more than 8 game controllers connected for the game to write out of bounds.&lt;/p&gt;
    &lt;p&gt;Actually, there would've been an even simpler workaround: &lt;code&gt;EnumDevices&lt;/code&gt; allows passing a &lt;code&gt;DIDEVTYPE&lt;/code&gt; as a filter:&lt;/p&gt;
    &lt;code&gt;g_dinput_instance-&amp;gt;lpVtbl-&amp;gt;EnumDevices(
    g_dinput_instance, DIDEVTYPE_JOYSTICK, dinput_enumdevices_callback, 0, DIEDFL_ATTACHEDONLY);
                    // ^^^^^^^^^^^^^^^^^^
&lt;/code&gt;
    &lt;p&gt;This would make DirectInput call the callback for game controllers only. Without it, all devices, whether they are keyboards, mice, or actually any HID devices, are enumerated. (I've checked the DirectX 5 SDK docs, and even there it mentions the HID device support.) This includes the vendor-defined devices of my mouse and its emulated keyboard (for macros), and of course the motherboard's LED controller.&lt;/p&gt;
    &lt;p&gt;The moral of the story? Always check your bounds, kids! You'll never know if some weirdo comes along and plugs in a dozen game controllers to their PC. :^)&lt;/p&gt;
    &lt;head rend="h2"&gt;The fix #&lt;/head&gt;
    &lt;p&gt;Over on GitHub I've pushed a minimal patch as a classic DLL shim. With the provided &lt;code&gt;dinput.dll&lt;/code&gt; in the game directory, the game will load that instead of the system one. DirectInput has only one relevant exported function that we need to shim: &lt;code&gt;DirectInputCreateA&lt;/code&gt;. The rest of the API is implemented via COM interfaces, for which we can modify the respective vtables as needed.&lt;/p&gt;
    &lt;p&gt;I've implemented two fixes in the shim:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Inject the &lt;code&gt;DIDEVTYPE_JOYSTICK&lt;/code&gt;filter in the call to&lt;code&gt;EnumDevices&lt;/code&gt;to only return joysticks/game controllers.&lt;/item&gt;
      &lt;item&gt;Cancel enumeration once 8 joysticks have been found.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For fun, I've also tried to minimize the size of the shim DLL -- the final binary weighs in at 2 KiB.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;These are the reasonable settings I changed:&lt;/p&gt;&lt;item&gt;Compile with&lt;/item&gt;&lt;code&gt;opt-level = "z"&lt;/code&gt;to optimize for minimum size. (though the code is so low-level that it's effectively the same as&lt;code&gt;opt-level = 3&lt;/code&gt;)&lt;code&gt;#[no_std]&lt;/code&gt;to avoid linking the Rust standard library.&lt;code&gt;codegen-units = 1&lt;/code&gt;and&lt;code&gt;lto = true&lt;/code&gt;to enable whole-program optimization.&lt;code&gt;panic = "immediate-abort"&lt;/code&gt;to remove all unnecessary panic handling code; an unwrap will immediately abort the process.&lt;p&gt;And these are the cursed ones:&lt;/p&gt;&lt;code&gt;/NODEFAULTLIB&lt;/code&gt;to not link against any MSVC runtime library; added my own minimal&lt;code&gt;DllMain&lt;/code&gt;.&lt;code&gt;/FORCE:UNRESOLVED&lt;/code&gt;to ignore the missing symbols for&lt;code&gt;_aullrem&lt;/code&gt;,&lt;code&gt;_aulldiv&lt;/code&gt;, and&lt;code&gt;_fltused&lt;/code&gt;. We aren't using any of these, but the LLVM target still seems to insist on linking them in.&lt;code&gt;/FILEALIGN:512&lt;/code&gt;to force the linker to use the minimum supported PE section alignment in the file.&lt;code&gt;/MERGE:.rdata=.text&lt;/code&gt;merges the read-only data section into the code section.&lt;item&gt;Prevent zero-initialization of the system directory path by using&lt;/item&gt;&lt;code&gt;MaybeUninit&lt;/code&gt;.&lt;item&gt;Storing globals in&lt;/item&gt;&lt;code&gt;static mut&lt;/code&gt;just like the original game does. Since the fix is specific to this game I can do these "global" assumptions here :^)&lt;item&gt;Ensure all globals are zero-initialized so the&lt;/item&gt;&lt;code&gt;.data&lt;/code&gt;section is 0 bytes in the binary.&lt;code&gt;.unwrap_unchecked()&lt;/code&gt;to avoid any extra branches where they aren't needed.&lt;code&gt;/DEBUG:NONE&lt;/code&gt;to not generate debug information, and not store a&lt;code&gt;.pdb&lt;/code&gt;path in the binary.&lt;/quote&gt;
    &lt;p&gt;Furthermore, I switched to &lt;code&gt;rust-lld.exe&lt;/code&gt; as the linker, as it's fine with setting &lt;code&gt;/SUBSYSTEM:WINDOWS,4.0"&lt;/code&gt; and &lt;code&gt;/OSVERSION:4.0&lt;/code&gt; without complaining. :) Since there is no linked runtime code at all, the resulting binary should work on any 32-bit Windows version, even without Rust9x. I've tested it on Windows 7 and Windows 98 SE.&lt;/p&gt;
    &lt;p&gt;Feel free to grab the compiled DLL from the releases page. Except for True Color rendering, which seems to be broken, no matter the system, graphics card, or game version? Investigating that is left as a mystery for another day. ‚Ü©&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45948311</guid><pubDate>Sun, 16 Nov 2025 20:54:14 +0000</pubDate></item><item><title>Neuroscientists track the neural activity underlying an ‚Äúaha‚Äù</title><link>https://www.quantamagazine.org/how-your-brain-creates-aha-moments-and-why-they-stick-20251105/</link><description>&lt;doc fingerprint="fa73133599e7d742"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How Your Brain Creates ‚ÄòAha‚Äô Moments and Why They Stick&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Here are three words: pine, crab, sauce. There‚Äôs a fourth word that combines with each of the others to create another common word. What is it?&lt;/p&gt;
    &lt;p&gt;When the answer finally comes to you, it‚Äôll likely feel instantaneous. You might even say ‚ÄúAha!‚Äù This kind of sudden realization is known as insight, and a research team recently uncovered how the brain produces it, which suggests why insightful ideas tend to stick in our memory.&lt;/p&gt;
    &lt;p&gt;Maxi Becker, a cognitive neuroscientist at Duke University, first got interested in insight after reading the landmark 1962 book The Structure of Scientific Revolutions by the historian and philosopher of science Thomas Kuhn. ‚ÄúHe describes how some ideas are so powerful that they can completely shift the way an entire field thinks,‚Äù she said. ‚ÄúThat got me wondering: How does the brain come up with those kinds of ideas? How can a single thought change how we see the world?‚Äù&lt;/p&gt;
    &lt;p&gt;Such moments of insight are written across history. According to the Roman architect and engineer Vitruvius, in the third century BCE the Greek mathematician Archimedes suddenly exclaimed ‚ÄúEureka!‚Äù after he slid into a bathtub and saw the water level rise by an amount equal to his submerged volume (although this tale may be apocryphal). In the 17th century, according to lore, Sir Isaac Newton had a breakthrough in understanding gravity after an apple fell on his head. In the early 1900s, Einstein came to a sudden realization that ‚Äúif a man falls freely, he would not feel his weight,‚Äù which led him to his theory of relativity, as he later described in a lecture.&lt;/p&gt;
    &lt;p&gt;Insights are not limited to geniuses: We have these cognitive experiences all the time when solving riddles or dealing with social or intellectual problems. They are distinct from analytical problem-solving, such as the process of doing formulaic algebra, in which you arrive at a solution slowly and gradually as if you‚Äôre getting warmer. Instead, insights often follow periods of confusion. You never feel as if you‚Äôre getting warmer; rather, you go from cold to hot, seemingly in an instant. Or, as the neuropsychologist Donald Hebb, known for his work building neurobiological models of learning, wrote in the 1940s, sometimes ‚Äúlearning occurs as a single jump, an all-or-none affair.‚Äù&lt;/p&gt;
    &lt;p&gt;Ann Rosan Picture Library&lt;/p&gt;
    &lt;p&gt;An abrupt cognitive shift in how the mind understands information is known as a representational change. Although researchers have inferred sudden shifts in understanding from the behavior of subjects, they have not pinned down how the brain supports representational change.&lt;/p&gt;
    &lt;p&gt;During moments of insight, representational change typically occurs, said John Kounios, a cognitive neuroscientist at Drexel University and co-author of the book The Eureka Factor: Aha Moments, Creative Insight, and the Brain. ‚ÄúThe question is: How is it occurring?‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Insightful Activity&lt;/head&gt;
    &lt;p&gt;While at Humboldt University of Berlin, Becker set out to uncover this neural signature of insight. Given that it‚Äôs nearly impossible to fabricate life-changing, field-altering insights in the lab, her team needed to identify a simple task that could produce a sudden feeling of understanding rather than a slowly unfolding solution.&lt;/p&gt;
    &lt;p&gt;They turned to abstracted black-and-white pictures called Mooney images, which are made by cranking up the contrast on a photograph all the way so that the subjects ‚Äî a dog or a coffee mug, for example ‚Äî are unrecognizable at first. The pictures pose a challenge for human brains, which typically identify objects by piecing together their different parts. But if given enough time with a Mooney image, even a few seconds, the brain can rearrange the contours to recognize the pictured object ‚Äî and trigger the insightful ‚Äúaha‚Äù feeling, a representational change.&lt;/p&gt;
    &lt;p&gt;Over the course of two days, Becker had study participants lie in a functional magnetic resonance imaging (fMRI) scanner, which detects blood flow in the brain as a proxy for neural activity, and view a series of 120 Mooney images. After 10 seconds of viewing a single image, the participant would indicate whether they recognized the pictured object. If they did, they would then answer a series of questions about the suddenness, positive emotion and certainty associated with their experience ‚Äî three measures that have been linked to moments of insight.&lt;/p&gt;
    &lt;p&gt;Becker and her team then used neural networks to parse the fMRI data, looking to identify consistent changes in brain activity shared by participants when they correctly recognized Mooney images. They observed that when a participant noticed a hidden object, brain activity increased in the ventral occipitotemporal cortex (VOTC), a region responsible for recognizing visual patterns in the environment; the amygdala, which processes both positive and negative emotions; and the hippocampus, a deep-brain structure involved in handling memories. This activity was greater for experiences rated more certain and emotionally positive ‚Äî in other words, more insightful ones.&lt;/p&gt;
    &lt;p&gt;The hippocampus is sometimes known as the brain‚Äôs ‚Äúmismatch detector,‚Äù Becker said, because it reacts when an input doesn‚Äôt align with expectations. In this case, insight leads a once-meaningless image to gain meaning, going against the brain‚Äôs predictions.&lt;/p&gt;
    &lt;p&gt;Courtesy of Maxi Becker&lt;/p&gt;
    &lt;p&gt;These regions ‚Äî the hippocampus, amygdala and VOTC ‚Äî create ‚Äúa plausible network of brain areas‚Äù behind representational change, said Kounios, who was not involved in the study. These findings finally ‚Äúconnect the psychological theory with the neural mechanism,‚Äù said Yuhua Yu, a postdoctoral researcher in neuroscience at the University of Arizona, who was also not involved with the study.&lt;/p&gt;
    &lt;p&gt;Becker and her team likely found representational change in the VOTC because of the visual nature of their stimuli. If they had chosen another type of stimulus, like words, the change would probably have appeared in language-processing areas of the brain.&lt;/p&gt;
    &lt;p&gt;Once the team had figured out which brain areas support insight, they wanted to probe whether these regions might be working together to create a lasting memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Memory Boost&lt;/head&gt;
    &lt;p&gt;Since they began investigating insight, researchers have suspected that such experiences might boost memory. In his 1949 book The Organization of Behavior, Hebb wrote that ‚Äúwhatever insight is, we now know that it continually affects the learning of the adult mammal.‚Äù Insight not only feels notable or salient in the moment but also helps us retain new information as memory.&lt;/p&gt;
    &lt;p&gt;This memory boost, which became known as the insight-memory advantage, has since been studied in many types of problem-solving, including the unraveling of magic tricks and puzzles. ‚ÄúWhen you have an insight, you tend to be better able to remember the solution,‚Äù Becker said, compared to when you resolve a problem more gradually. She wanted to understand why.&lt;/p&gt;
    &lt;p&gt;A few days after the initial experiment, the team tested participants‚Äô memory by having them look at more Mooney images online, including some they had seen before. Participants were better able to remember prior images that they had rated highly on the three aspects of insight. This suggested that the insight-memory advantage was real, but the team wanted to see what was going on under the hood. Did brain activity during insight predict better memory five days later?&lt;/p&gt;
    &lt;p&gt;The researchers found that the larger the activity boost in both the VOTC and the hippocampus during the initial insight, the better participants remembered the Mooney images. The big change in brain activity likely makes the experience more salient, Becker said, and salient experiences are known to better encode long-term memories.&lt;/p&gt;
    &lt;p&gt;While insight creates stronger memories of an idea, it doesn‚Äôt mean the idea is correct. Previous work has shown that the quicker, more certain and more pleasurable a solution feels, the more likely it is to be correct ‚Äî but false insights can and do exist. In Becker‚Äôs study, participants wrongly identified the subjects of more than half the Mooney images they saw. Of those incorrect trials (which the researchers excluded from the analysis), the participants reported experiencing insight 40% of the time. In comparison, correct trials were accompanied by feelings of insight 65% of the time.&lt;/p&gt;
    &lt;p&gt;These kinds of studies of insight in the lab will set researchers up to look at how it functions in the real world. Once we decompose insight into ‚Äúvery simple tasks that we already understand well,‚Äù Becker said, we can ‚Äúmove on to more complex, truly creative tasks.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Insight Into the Future&lt;/head&gt;
    &lt;p&gt;As a self-described uncreative person, Yu has been particularly fascinated by insight‚Äôs role in the creative process. Creativity is ‚Äúlike a magic power,‚Äù she said. ‚ÄúA really big creative idea is [often] associated with insight because a creative idea is in some way a leap in your cognitive world, and a leap will often elicit an insight or ‚Äòaha‚Äô feeling.‚Äù&lt;/p&gt;
    &lt;p&gt;However, Yu is finding that insight‚Äôs role in creativity might depend on the kind of problem a person is solving. In a recent study, she asked participants to come up with metaphors for scientific concepts and asked whether they used insight as they did so. The insight-driven metaphors weren‚Äôt more or less creative than those created through analytic thinking, she found ‚Äî and the participants were more likely to remember the science concepts behind the latter.&lt;/p&gt;
    &lt;p&gt;This may be because, unlike the task of seeing a hidden object in a Mooney image, creating a metaphor tends to rely on slower cognitive problem-solving rather than sudden moments of insight, Becker suggested. The effects of insight therefore likely depend on the context.&lt;/p&gt;
    &lt;p&gt;Next, Yu wants to investigate insight in more contexts. ‚ÄúMost of the insight research is looking at insight in the problem-solving context and in the lab setting,‚Äù Yu said. She hopes that researchers will begin investigating ‚Äúinsight within many other domains, like in psychotherapy, in meditation, even in psychedelic experiences.‚Äù&lt;/p&gt;
    &lt;p&gt;Beyond offering a better understanding of how the human brain learns, these findings could have applications in classrooms. Kounios believes that applying insight-boosting strategies to teaching could lead to better learning outcomes for students. Insight seems to be a powerful and positive experience that generates accurate solutions, confidence in our answers and strong memories.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs very intensive for a teacher to do this, but a lot of really good teachers try to get the students to have the insights themselves about how something works, and that will burn it into their memories,‚Äù Kounios said. ‚ÄúAnother aspect of that [is], it‚Äôs very motivating, too.‚Äù&lt;/p&gt;
    &lt;p&gt;It‚Äôs a nice feeling when your brain suddenly comes up with an answer. Perhaps you‚Äôve even experienced that feeling since reading this piece‚Äôs first sentence. Maybe it even hit you like an apple on the head.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45948792</guid><pubDate>Sun, 16 Nov 2025 21:57:13 +0000</pubDate></item><item><title>PicoIDE ‚Äì An open IDE/ATAPI drive emulator</title><link>https://picoide.com/</link><description>&lt;doc fingerprint="5d0cba1441c185fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Currently PicoIDE only emulates one drive, but the hardware is capable of supporting two devices simultaneously. Enabling this is a priority for development after the PicoIDE is released.&lt;/p&gt;
    &lt;p&gt;Can you add [hardware feature]?&lt;/p&gt;
    &lt;p&gt;The current hardware design is locked in, but who knows what the future may bring.&lt;/p&gt;
    &lt;p&gt;Will PicoIDE work in my device?&lt;/p&gt;
    &lt;p&gt;The primary development/testing platform for PicoIDE is 90s-era PCs, but PicoIDE has a high level of compatibility with the ATA and ATAPI standards, so there's a good chance it will work in your device. If your device is particularly finicky, that sounds like a fun development challenge!&lt;/p&gt;
    &lt;p&gt;What about UDMA support?&lt;/p&gt;
    &lt;p&gt;In the interest of providing a cost-effective design, PicoIDE's hardware doesn't support UDMA, but faster systems are very well served in the area of fast HDD replacements by CF, SD, and M.2 to IDE adapters, and for optical, PicoIDE's MWDMA mode 2 and PIO mode 4 support is as fast as a 52X CD-ROM drive, plenty for fast systems.&lt;/p&gt;
    &lt;p&gt;Will there be a 5.25" or 2.5" version?&lt;/p&gt;
    &lt;p&gt;PicoIDE will launch with a 3.5" enclosure only, but depending how things go, other form factors may be made available.&lt;/p&gt;
    &lt;p&gt;When will PicoIDE be available?&lt;/p&gt;
    &lt;p&gt;PicoIDE will be launching soon‚Ñ¢. Sign up below to be notified when it becomes available for purchase.&lt;/p&gt;
    &lt;p&gt;Where will I be able to purchase a PicoIDE? How much will it cost?&lt;/p&gt;
    &lt;p&gt;Availability and pricing will be announced when PicoIDE is launched. If you're familiar with PicoGUS, you know I value making attainable hardware.&lt;/p&gt;
    &lt;p&gt;PicoIDE will be launching soon. Sign up to be notified when that happens!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45949352</guid><pubDate>Sun, 16 Nov 2025 23:19:24 +0000</pubDate></item><item><title>A 1961 Relay Computer Running in the Browser</title><link>https://minivac.greg.technology/</link><description>&lt;doc fingerprint="514993e1c0d08679"&gt;
  &lt;main&gt;
    &lt;p&gt;Best viewed on desktop for now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45950396</guid><pubDate>Mon, 17 Nov 2025 02:36:18 +0000</pubDate></item><item><title>Building a Simple Search Engine That Works</title><link>https://karboosx.net/post/4eZxhBon/building-a-simple-search-engine-that-actually-works</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45950720</guid><pubDate>Mon, 17 Nov 2025 03:52:50 +0000</pubDate></item><item><title>Giving C a Superpower</title><link>https://hwisnu.bearblog.dev/giving-c-a-superpower-custom-header-file-safe_ch/</link><description>&lt;doc fingerprint="2c11dabd8e0b7208"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Giving C a Superpower: custom header file (safe_c.h)&lt;/head&gt;
    &lt;p&gt;The story of how I wrote a leak-free, thread-safe grep in C23 without shooting yourself in the foot, and how you can too!&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Let's be honest: most people have a love-hate relationship with C. We love its raw speed, its direct connection to the metal, and its elegant simplicity. But we hate its footguns, its dragons, the untamed beasts. The segfaults that appear from nowhere, the memory leaks that slowly drain the life from our applications, and the endless goto cleanup; chains that make our code look like a plate of spaghetti pasta.&lt;/p&gt;
    &lt;p&gt;This is the classic C curse: power without guardrails...at least that's the fear mongering mantra being said again and again. But is that still relevant in today's world with all the tools available for C devs like static analyzer and dynamic sanitizers? I've written about this here and here.&lt;/p&gt;
    &lt;p&gt;What if, with the help of the modern tools and a custom header file (600 loc), you could tame those footguns beasts? What if you could keep C's power but wrap it in a suit of modern armor? That's what the custom header file safe_c.h is for. It's designed to give C some safety and convenience features from C++ and Rust, and I'm using it to build a high-performance grep clone called cgrep as my test case.&lt;/p&gt;
    &lt;p&gt;By the end this article I hope it could provide the audience with the idea of C is super flexible and extensible, sort of "do whatever you want with it" kind of thing. And this is why C (and its close cousin: Zig) remain to be my favorite language to write programs in; it's the language of freedom!&lt;/p&gt;
    &lt;head rend="h1"&gt;safe_c.h&lt;/head&gt;
    &lt;p&gt;Is a custom C header file that takes features mainly from C++ and Rust and implements them into our C code ~ [write C code, get C++ and Rust features!]&lt;/p&gt;
    &lt;p&gt;It starts by bridging the gap between old and new C. C23 gave us &lt;code&gt;[[cleanup]]&lt;/code&gt; attributes, but in the real world, you need code that compiles on GCC 11 or Clang 18. safe_c.h detects your compiler and gives you the same RAII semantics everywhere. No more &lt;code&gt;#ifdef&lt;/code&gt; soup.&lt;/p&gt;
    &lt;code&gt;// The magic behind CLEANUP: zero overhead, maximum safety
#if defined(__STDC_VERSION__) &amp;amp;&amp;amp; __STDC_VERSION__ &amp;gt;= 202311L
#define CLEANUP(func) [[cleanup(func)]]
#else
#define CLEANUP(func) __attribute__((cleanup(func)))
#endif

// Branch prediction that actually matters in hot paths
#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;Your cleanup code runs even if you return early, goto out, or panic. It's &lt;code&gt;finally&lt;/code&gt;, but for C.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Memory Management Beast: Slain with Smart Pointers (C++ feature)&lt;/head&gt;
    &lt;p&gt;The oldest, fiercest and most feared by devs: manual memory management.&lt;/p&gt;
    &lt;p&gt;Before: the highway path to leaks.&lt;lb/&gt; Forgetting a single &lt;code&gt;free()&lt;/code&gt; is a disaster. In cgrep, parsing command-line options the old way is a breeding ground for CVEs and its bestiary. You have to remember to free the memory on every single exit path, difficult for the undisciplined.&lt;/p&gt;
    &lt;code&gt;// The Old Way (don't do this)
char* include_pattern = NULL;
if (optarg) {
    include_pattern = strdup(optarg);
}
// ...200 lines later...
if (some_error) {
    if (include_pattern) free(include_pattern); // Did I free it? Did I??
    return 1;
}
// And remember to free it at *every* return path...
&lt;/code&gt;
    &lt;p&gt;After: memory that automatically cleans itself up.&lt;lb/&gt; UniquePtr is a "smart pointer" that owns a resource. When the UniquePtr variable goes out of scope, its resource is automatically freed. It's impossible to forget.&lt;/p&gt;
    &lt;p&gt;Here's the machinery inside &lt;code&gt;safe_c.h&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// The UniquePtr machinery: a struct + automatic cleanup
typedef struct {
    void* ptr;
    void (*deleter)(void*);
} UniquePtr;

#define AUTO_UNIQUE_PTR(name, ptr, deleter) \
    UniquePtr name CLEANUP(unique_ptr_cleanup) = UNIQUE_PTR_INIT(ptr, deleter)

static inline void unique_ptr_cleanup(UniquePtr* uptr) {
    if (uptr &amp;amp;&amp;amp; uptr-&amp;gt;ptr &amp;amp;&amp;amp; uptr-&amp;gt;deleter) {
        uptr-&amp;gt;deleter(uptr-&amp;gt;ptr);
        uptr-&amp;gt;ptr = NULL;
    }
}
&lt;/code&gt;
    &lt;p&gt;And here's how cgrep uses it. The cleanup is automatic, even if errors happen:&lt;/p&gt;
    &lt;code&gt;// In cgrep, we use this for command-line arguments
AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);

// When we get a new pattern, the old one is automatically freed!
unique_ptr_delete(&amp;amp;include_pattern_ptr);
include_pattern_ptr.ptr = strdup(optarg);
// No leaks, even if an error happens later!
&lt;/code&gt;
    &lt;p&gt;Sharing Safely with SharedPtr&lt;/p&gt;
    &lt;p&gt;Before: manual, bug-prone reference counting.&lt;lb/&gt; You'd have to implement reference counting by hand, creating a complex and fragile system where a single mistake leads to a leak or a use-after-free bug.&lt;/p&gt;
    &lt;code&gt;// The old way of manual reference counting
typedef struct {
    MatchStore* store;
    int ref_count;
    pthread_mutex_t mutex;
} SharedStore;

void release_store(SharedStore* s) {
    pthread_mutex_lock(&amp;amp;s-&amp;gt;mutex);
    s-&amp;gt;ref_count--;
    bool is_last = (s-&amp;gt;ref_count == 0);
    pthread_mutex_unlock(&amp;amp;s-&amp;gt;mutex);

    if (is_last) {
        match_store_deleter(s-&amp;gt;store);
        free(s);
    }
}
&lt;/code&gt;
    &lt;p&gt;After: automated reference counting.&lt;lb/&gt; SharedPtr automates this entire process. The last thread to finish using the object automatically triggers its destruction. The machinery:&lt;/p&gt;
    &lt;code&gt;// The SharedPtr machinery: reference counting without the boilerplate
typedef struct {
    void* ptr;
    void (*deleter)(void*);
    size_t* ref_count;
} SharedPtr;

#define AUTO_SHARED_PTR(name) \
    SharedPtr name CLEANUP(shared_ptr_cleanup) = {.ptr = NULL, .deleter = NULL, .ref_count = NULL}

static inline void shared_ptr_cleanup(SharedPtr* sptr) {
    shared_ptr_delete(sptr); // Decrement and free if last reference
}
&lt;/code&gt;
    &lt;p&gt;The usage is clean and safe. No more manual counting.&lt;/p&gt;
    &lt;code&gt;// In our thread worker context, multiple threads access the same results store
typedef struct {
    // ...
    SharedPtr store;  // No more worrying about who frees this!
    SharedPtr file_counts;
    // ...
} FileWorkerContext;

// In main(), we create it once and share it safely
// SharedPtr: Reference-counted stores for thread-safe sharing
SharedPtr store_shared = {0};
shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
// Pass to threads: ctx-&amp;gt;store = shared_ptr_copy(&amp;amp;store_shared);
// ref-count increments automatically; last thread out frees it.
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Buffer Overflow Beast: Contained with Vectors and Views (C++ feature)&lt;/head&gt;
    &lt;p&gt;Dynamically growing arrays in C is a horror show.&lt;/p&gt;
    &lt;p&gt;Before: the realloc dance routine.&lt;lb/&gt; You have to manually track capacity and size, and every realloc risks fragmenting memory or failing, requiring careful error handling for every single element you add.&lt;/p&gt;
    &lt;code&gt;// The old way: manual realloc is inefficient and complex
MatchEntry** matches = NULL;
size_t matches_count = 0;
size_t matches_capacity = 0;

for (/*...each match...*/) {
    if (matches_count &amp;gt;= matches_capacity) {
        matches_capacity = (matches_capacity == 0) ? 8 : matches_capacity * 2;
        MatchEntry** new_matches = realloc(matches, matches_capacity * sizeof(MatchEntry*));
        if (!new_matches) {
            free(matches); // Don't leak!
            /* handle error */
        }
        matches = new_matches;
    }
    matches[matches_count++] = current_match;
}
&lt;/code&gt;
    &lt;p&gt;After: a type-safe, auto-growing vector.&lt;lb/&gt; safe_c.h generates an entire type-safe vector for you. It handles allocation, growth, and cleanup automatically. The magic that generates the vector:&lt;/p&gt;
    &lt;code&gt;// The magic that generates a complete vector type from a single line
#define DEFINE_VECTOR_TYPE(name, type) \
    typedef struct { \
        Vector base; \
        type* data; \
    } name##Vector; \
    \
    static inline bool name##_vector_push_back(name##Vector* vec, type value) { \
        bool result = vector_push_back(&amp;amp;vec-&amp;gt;base, &amp;amp;value); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \
    \
    static inline bool name##_vector_reserve(name##Vector* vec, size_t new_capacity) { \
        bool result = vector_reserve(&amp;amp;vec-&amp;gt;base, new_capacity); \
        vec-&amp;gt;data = (type*)vec-&amp;gt;base.data; /* Sync pointer after potential realloc */ \
        return result; \
    } \


    /* more helper functions not outlined here */

// And the underlying generic Vector implementation
typedef struct {
    size_t size;
    size_t capacity;
    void* data;
    size_t element_size;
} Vector;
&lt;/code&gt;
    &lt;p&gt;Using it in cgrep is simple and safe. The vector cleans itself up when it goes out of scope.&lt;/p&gt;
    &lt;code&gt;// Type-safe vector for collecting matches
DEFINE_VECTOR_TYPE(MatchEntryPtr, MatchEntry*)

AUTO_TYPED_VECTOR(MatchEntryPtr, all_matches_vec);
MatchEntryPtr_vector_reserve(&amp;amp;all_matches_vec, store-&amp;gt;total_matches);

// Pushing elements is safe and simple
for (MatchEntry* entry = store-&amp;gt;buckets[i]; entry; entry = entry-&amp;gt;next) {
    MatchEntryPtr_vector_push_back(&amp;amp;all_matches_vec, entry);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Views: Look, Don't Touch (or malloc) - C++ feature&lt;/head&gt;
    &lt;p&gt;Before: needless allocations.&lt;lb/&gt; To handle a substring or a slice of an array, you'd often malloc a new buffer and copy the data into it, which is incredibly slow in a tight loop.&lt;/p&gt;
    &lt;code&gt;// The old way: allocating a new string just to get a substring
const char* line = "this is a long line of text";
char* pattern = "long line";
// To pass just the pattern to a function, you might do this:
char* sub = malloc(strlen(pattern) + 1);
strncpy(sub, pattern, strlen(pattern) + 1);
// ... use sub ...
free(sub); // And hope you remember this free call
&lt;/code&gt;
    &lt;p&gt;After: zero-cost, non-owning views.&lt;lb/&gt; A StringView or a Span is just a pointer and a length. It's a non-owning reference that lets you work with slices of data without any allocation. The definitions are pure and simple:&lt;/p&gt;
    &lt;code&gt;// The StringView and Span definitions: pure, simple, zero-cost
typedef struct {
    const char* data;
    size_t size;
} StringView;

typedef struct {
    void* data;
    size_t size;
    size_t element_size;
} Span;
&lt;/code&gt;
    &lt;p&gt;In cgrep, the search pattern becomes a StringView, avoiding allocation entirely.&lt;/p&gt;
    &lt;code&gt;// Our options struct holds a StringView, not a char*
typedef struct {
    StringView pattern; // Clean, simple, and safe
    // ...
} GrepOptions;

// Initializing it is a piece of cake
options.pattern = string_view_init(argv[optind]);
&lt;/code&gt;
    &lt;p&gt;For safe array access, Span provides a bounds-checked window into existing data.&lt;/p&gt;
    &lt;code&gt;// safe_c.h
#define DEFINE_SPAN_TYPE(name, type) \
    typedef struct { \
        type* data; \
        size_t size; \
    } name##Span; \
    \
    static inline name##Span name##_span_init(type* data, size_t size) { \
        return (name##Span){.data = data, .size = size}; \
    } \
    \

    /* other helper functions not outlined here */
&lt;/code&gt;
    &lt;code&gt;// Span: Type-safe array slices for chunk processing
DEFINE_SPAN_TYPE(LineBuffer, char)
LineBufferSpan input_span = LineBuffer_span_init((char*)start, len);

for (size_t i = 0; i &amp;lt; LineBuffer_span_size(&amp;amp;input_span); i++) {
    char* line = LineBuffer_span_at(&amp;amp;input_span, i); // asserts i &amp;lt; span.size
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Error-Handling &lt;code&gt;goto&lt;/code&gt; Beast: Replaced with Results (Rust feature) and RAII (C++ feature)&lt;/head&gt;
    &lt;p&gt;C's error handling is notoriously messy.&lt;/p&gt;
    &lt;p&gt;Before: goto cleanup spaghetti carbonara.&lt;lb/&gt; Functions return special values like -1 or NULL, and you have to check errno. This leads to deeply nested if statements and a single goto cleanup; label that has to handle every possible failure case.&lt;/p&gt;
    &lt;code&gt;// The old way: goto cleanup
int do_something(const char* path) {
    int fd = open(path, O_RDONLY);
    if (fd &amp;lt; 0) {
        return -1; // Error
    }

    void* mem = malloc(1024);
    if (!mem) {
        close(fd); // Manual cleanup
        return -1;
    }
    
    // ... do more work ...

    free(mem);
    close(fd);
    return 0; // Success
}
&lt;/code&gt;
    &lt;p&gt;After: explicit, type-safe result.&lt;lb/&gt; Inspired by Rust, Result&lt;/p&gt;
    &lt;code&gt;// The Result type machinery: tagged unions for success/failure
typedef enum { RESULT_OK, RESULT_ERROR } ResultStatus;

#define DEFINE_RESULT_TYPE(name, value_type, error_type) \
    typedef struct { \
        ResultStatus status; \
        union { \
            value_type value; \
            error_type error; \
        }; \
    } Result##name;
&lt;/code&gt;
    &lt;p&gt;Handling errors becomes easy. You can't accidentally use an error as a valid value.&lt;/p&gt;
    &lt;code&gt;// Define a Result for file operations
DEFINE_RESULT_TYPE(FileOp, i32, const char*)

// Our function now returns a clear Result
static ResultFileOp submit_stat_request_safe(...) {
    // ...
    if (!sqe) {
        return RESULT_ERROR(FileOp, "Could not get SQE for stat");
    }
    return RESULT_OK(FileOp, 0);
}

// And handling it is clean
ResultFileOp result = submit_stat_request_safe(path, &amp;amp;ring, &amp;amp;pending_ops);
if (!RESULT_IS_OK(result)) {
    fprintf(stderr, "Error: %s\n", RESULT_UNWRAP_ERROR(result));
}
&lt;/code&gt;
    &lt;p&gt;This is powered by RAII. The &lt;code&gt;CLEANUP&lt;/code&gt; attribute ensures resources are freed no matter how a function exits.&lt;/p&gt;
    &lt;code&gt;#define AUTO_MEMORY(name, size) \
    void* name CLEANUP(memory_cleanup) = malloc(size)

// DIR pointers are automatically closed, even on an early return.
DIR* dir CLEANUP(dir_cleanup) = opendir(req-&amp;gt;path);
if (!dir) {
    return RESULT_ERROR(FileOp, "Failed to open dir"); // dir_cleanup is NOT called
}
if (some_condition) {
    return RESULT_OK(FileOp, 0); // closedir() is called automatically HERE!
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Assumption Beast: Challenged with Contracts and Safe Strings&lt;/head&gt;
    &lt;p&gt;Before: &lt;code&gt;assert()&lt;/code&gt; and pray.&lt;lb/&gt; A standard &lt;code&gt;assert(ptr != NULL)&lt;/code&gt; is good, but when it fails, the message is generic. You know the condition failed, but not the context or why it was important.&lt;/p&gt;
    &lt;p&gt;After: self-documenting contracts.&lt;code&gt;requires()&lt;/code&gt; and &lt;code&gt;ensures()&lt;/code&gt; make function contracts explicit. The failure messages tell you exactly what went wrong.
The contract macros:&lt;/p&gt;
    &lt;code&gt;#define requires(cond) assert_msg(cond, "Precondition failed")
#define ensures(cond) assert_msg(cond, "Postcondition failed")

#define assert_msg(cond, msg) /* ... full implementation ... */
&lt;/code&gt;
    &lt;p&gt;This turns assertions into executable documentation:&lt;/p&gt;
    &lt;code&gt;// Preconditions that document and enforce contracts
static inline bool arena_create(Arena* arena, size_t size)
{
    requires(arena != NULL);  // Precondition: arena must not be null
    requires(size &amp;gt; 0);       // Precondition: size must be positive
    
    // ... implementation ...
    
    ensures(arena-&amp;gt;buffer != NULL);  // Postcondition: buffer is allocated
    ensures(arena-&amp;gt;size == size);    // Postcondition: size is set correctly
    
    return true;
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;strcpy()&lt;/code&gt; is a Security Vulnerability&lt;/head&gt;
    &lt;p&gt;Before: buffer overflows.&lt;lb/&gt; strcpy has no bounds checking. It's the source of countless security holes. &lt;code&gt;strncpy&lt;/code&gt; is little better, as it might not null-terminate the destination string.&lt;/p&gt;
    &lt;code&gt;// The old, dangerous way
char dest[20];
const char* src = "This is a very long string that will overflow the buffer";
strcpy(dest, src); // Undefined behavior! Stack corruption!
&lt;/code&gt;
    &lt;p&gt;After: safe, bounds-checked operations.&lt;lb/&gt; safe_c.h provides alternatives that check bounds and return a success/failure status. No surprises. The safe implementation:&lt;/p&gt;
    &lt;code&gt;// The safe string operations: bounds checking that can't be ignored
static inline bool safe_strcpy(char* dest, size_t dest_size, const char* src) {
    if (!dest || dest_size == 0 || !src) return false;
    size_t src_len = strlen(src);
    if (src_len &amp;gt;= dest_size) return false;
    memcpy(dest, src, src_len + 1);
    return true;
}
&lt;/code&gt;
    &lt;p&gt;In cgrep, this prevents path buffer overflows cleanly:&lt;/p&gt;
    &lt;code&gt;// Returns bool, not silent truncation
if (!safe_strcpy(req-&amp;gt;path, PATH_MAX, path)) {
    free(req);
    return RESULT_ERROR(FileOp, "Path is too long");
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Concurrency: Mutexes That Unlock Themselves (Rust feature)&lt;/head&gt;
    &lt;p&gt;Before: leaked locks and deadlocks.&lt;lb/&gt; Forgetting to unlock a mutex, especially on an error path, is a catastrophic bug that causes your program to deadlock.&lt;/p&gt;
    &lt;code&gt;// The Buggy Way
pthread_mutex_lock(&amp;amp;mutex);
if (some_error) {
    return; // Oops, mutex is still locked! Program will deadlock.
}
pthread_mutex_unlock(&amp;amp;mutex);
&lt;/code&gt;
    &lt;p&gt;After: RAII-based locks.&lt;lb/&gt; Using the same CLEANUP attribute, we can ensure a mutex is always unlocked when the scope is exited. This bug becomes impossible to write.&lt;/p&gt;
    &lt;code&gt;// With a cleanup function, unlocking is automatic.
void mutex_unlock_cleanup(pthread_mutex_t** lock) {
    if (lock &amp;amp;&amp;amp; *lock) pthread_mutex_unlock(*lock);
}

// RAII lock guard via cleanup attribute
pthread_mutex_t my_lock;
pthread_mutex_t* lock_ptr CLEANUP(mutex_unlock_cleanup) = &amp;amp;my_lock;
pthread_mutex_lock(lock_ptr);

if (some_error) {
    return; // Mutex is automatically unlocked here!
}
&lt;/code&gt;
    &lt;p&gt;Simple wrappers also clean up the boilerplate of managing threads:&lt;/p&gt;
    &lt;code&gt;// The concurrency macros: spawn and join without boilerplate
#define SPAWN_THREAD(name, func, arg) \
    thrd_t name; \
    thrd_create(&amp;amp;name, (func), (arg))

#define JOIN_THREAD(name) \
    thrd_join(name, NULL)
&lt;/code&gt;
    &lt;p&gt;And in cgrep:&lt;/p&gt;
    &lt;code&gt;// Thread pool spawn without boilerplate
SPAWN_THREAD(workers[i], file_processing_worker, &amp;amp;contexts[i]);
JOIN_THREAD(workers[i]); // No manual pthread_join() error handling
&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance: Safety at -O2, Not -O0&lt;/head&gt;
    &lt;p&gt;Safety doesn't mean slow. The UNLIKELY() macro tells the compiler which branches are cold, adding zero overhead in hot paths.&lt;/p&gt;
    &lt;code&gt;#ifdef __GNUC__
#define LIKELY(x)   __builtin_expect(!!(x), 1)
#define UNLIKELY(x) __builtin_expect(!!(x), 0)
#else
#define LIKELY(x)   (x)
#define UNLIKELY(x) (x)
#endif
&lt;/code&gt;
    &lt;p&gt;The real win is in the fast paths:&lt;/p&gt;
    &lt;code&gt;// In hot allocation path: branch prediction
if (UNLIKELY(store-&amp;gt;local_buffer_sizes[thread_id] &amp;gt;= LOCAL_BUFFER_CAPACITY)) {
    match_store_flush_buffer(store, thread_id); // Rarely taken
}

// In match checking: likely path first
if (!options-&amp;gt;case_insensitive &amp;amp;&amp;amp; options-&amp;gt;fixed_string) {
    // Most common case: fast path with no branches
    const char* result = strstr(line, options-&amp;gt;pattern.data);
    return result != NULL;
}
&lt;/code&gt;
    &lt;p&gt;The above is similar to what a PGO (Profile Guided Optimization) would have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Word: C That Doesn't Blow Your Own Foot!&lt;/head&gt;
    &lt;p&gt;This is what main() looks like when you stop fighting the language:&lt;/p&gt;
    &lt;code&gt;int main(int argc, char* argv[]) {
    initialize_simd();
    output_buffer_init(); // Auto-cleanup on exit
    
    GrepOptions options = {0};
    AUTO_UNIQUE_PTR(include_pattern_ptr, NULL, options_string_deleter);
    
    // ... parse args with getopt_long ...
    
    AUTO_UNIQUE_PTR(store_ptr, NULL, match_store_deleter);
    SharedPtr store_shared = {0};
    if (need_match_store) {
        store_ptr.ptr = malloc(sizeof(ConcurrentMatchStore));
        if (!store_ptr.ptr || !match_store_create(store_ptr.ptr, hash_capacity, 1000)) {
            return 1; // All allocations cleaned up automatically
        }
        shared_ptr_init(&amp;amp;store_shared, store_ptr.ptr, match_store_deleter);
    }
    
    // Process files with thread pool...
    
cleanup: // Single cleanup label needed -- RAII handles the rest
    output_buffer_destroy(); // Flushes and destroys
    return 0;
}
&lt;/code&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In the end, cgrep is 2,300 lines of C. Without safe_c.h, it would have required over 50 manual free() calls ~ a recipe for leaks and segfaults. With the custom header file, it's 2,300 lines that compile to the same assembly, run just as fast, and are fundamentally safer.&lt;/p&gt;
    &lt;p&gt;This proves that the best abstraction is the one you don't pay for and can't forget to use. It enables a clear and powerful development pattern: validate inputs at the boundary, then unleash C's raw speed on the core logic. You get all the power of C without the infamous self-inflicted footgun wounds.&lt;/p&gt;
    &lt;p&gt;C simplicity makes writing programs with it becomes fun, however there are ways to make it both fun and safe..just like using condoms, you know?&lt;/p&gt;
    &lt;p&gt;This post has gotten too long for comfort, but I have one final food for thought for you the readers: after all these guard rails, what do you think of cgrep's performance? Check the screenshots below:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on recursive directories&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;grep bench on single large file NOTE: make sure you check the memory usage comparison between cgrep and ripgrep&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the next article, I will discuss how I built cgrep, the design I chose for it, why and how cgrep managed to be a couple of times faster than ripgrep (more than 2x faster in the recursive directory bench) while being super efficient with resource usage (20x smaller memory footprint in the single large file bench).&lt;/p&gt;
    &lt;p&gt;It's gonna be a lot of fun! Cheers!&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments section here&lt;/head&gt;
    &lt;p&gt;If you enjoyed this post, click the little up arrow chevron on the bottom left of the page to help it rank in Bear's Discovery feed and if you got any questions or anything, please use the comments section.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952428</guid><pubDate>Mon, 17 Nov 2025 10:40:55 +0000</pubDate></item><item><title>Ned: ImGui Text Editor with GL Shaders</title><link>https://github.com/nealmick/ned</link><description>&lt;doc fingerprint="7ec6cf545336b8c1"&gt;
  &lt;main&gt;
    &lt;p&gt;A retro-style text editor with GL shader effects. NED offers Tree Sitter syntax highlighting, LSP integration, and a terminal emulator.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;ned.demo.github.mp4&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Amber&lt;/cell&gt;
        &lt;cell role="head"&gt;Solarized&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;amber.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;solarized.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Ned&lt;/cell&gt;
        &lt;cell role="head"&gt;Custom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;ned.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;custom.github.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenGL Shaders with retro style for the best coding vibes&lt;/item&gt;
      &lt;item&gt;Text Bookmarks make editing multiple files with saved cursors a breeze&lt;/item&gt;
      &lt;item&gt;Rainbow mode cursor so you never lose your cursor and stand out&lt;/item&gt;
      &lt;item&gt;LSP Adapters for easy navigation and advanced language support&lt;/item&gt;
      &lt;item&gt;Terminal Emulator based on suckless st.c ported to C++ with multiplexer support&lt;/item&gt;
      &lt;item&gt;Optional Custom lexers and tokenizers for custom languages and obscure syntax patterns&lt;/item&gt;
      &lt;item&gt;Copilot-like auto complete using OpenRouter, choose the latest and best LLM models&lt;/item&gt;
      &lt;item&gt;Multi-cursor support, easily find and replace strings with multi selection&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CMake (version 3.10 or higher) C++20 compatible compiler OpenGL GLFW3 Glew Curl&lt;/p&gt;
    &lt;p&gt;Clone the repository with its submodules:&lt;/p&gt;
    &lt;code&gt;#Make sure you clone with recursive flag
git clone --recursive https://github.com/nealmick/ned
cd ned
git submodule init
git submodule update

# macOS Intel/ARM)
brew install clang-format cmake llvm glfw glew pkg-config curl

# Ubuntu/Debian
sudo apt install cmake libglfw3-dev libglew-dev libgtk-3-dev pkg-config clang libcurl4-openssl-dev clang-format mesa-utils

# For Windows, the dependencies are installed using the build script&lt;/code&gt;
    &lt;code&gt;./build.sh&lt;/code&gt;
    &lt;code&gt;./build-win.bat
# On Windows, the build script will attempt to install Visual Studio with Build Tools. 10-20 minutes.
# After VS has been installed, you must close and re-open PowerShell and run ./build-win.bat again.
# Subsequent rebuilds are much faster after the initial dependencies have been installed.&lt;/code&gt;
    &lt;p&gt;Create app package&lt;/p&gt;
    &lt;code&gt;./pack-mac.sh
./pack-deb.sh

# Bypass quarantine/translocation or you can sign it with your own apple dev acc
xattr -dr com.apple.quarantine Ned.app
&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;embed.demo.github.mp4&lt;/head&gt;
    &lt;p&gt;Ned can be embedded in other ImGui applications, taking advantage of its text editor, file explorer, and terminal emulator. The embedded version also includes emoji support, themes, and much more. We have a demo repository that shows how to get started embedding the neditor into your projects.&lt;/p&gt;
    &lt;p&gt;Ned is a feature-rich text editor built with Dear ImGui that combines the power of modern development tools with a lightweight, embeddable architecture. At its core, Ned provides a sophisticated text editing experience with Tree Sitter syntax highlighting supporting over 15 programming languages including C++, Python, JavaScript, Rust, Go, and more. The editor features custom lexer modes for specialized file types and includes advanced features like multi-cursor editing, line jumping, and a built-in file tree explorer.&lt;/p&gt;
    &lt;p&gt;The editor includes LSP integration with support for clangd, gopls, pyright, and TypeScript language servers, providing goto definition, find references, and symbol information. Ned also includes a terminal emulator and AI integration with OpenRouter support. The editor features emoji support with proper font rendering, custom shader effects, and a theming system. The project is designed to be embeddable in other ImGui applications through the ned_embed library, making it easy to integrate into your own projects.&lt;/p&gt;
    &lt;p&gt;Currently Ned is tested on macOS ARM and Intel, Windows x64, and has a Debian build available. Windows support includes automated dependency management through the build script.&lt;/p&gt;
    &lt;p&gt;If you have questions or issues, feel free to reach out.&lt;/p&gt;
    &lt;p&gt;Ned has an AI agent that uses OpenRouter to connect to the latest models. The agent can use MCP to call tools such as read file, run command, or edit file. The edit file tool uses a specialized model called Morph to apply code edits on large files at high speed with high accuracy, similar to Cursor. Check it out at morph.so. The whole system is tied into the settings where the key for the agent and completion model is stored. Below is a demo of the agent:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;agent_compressed.mov&lt;/head&gt;
    &lt;p&gt;Ned has the ability to track multiple cursors at once, which can make editing in certain scenarios much easier. The multi cursor system is used for file content searches to spawn cursors at each instance of a text search string. The app also supports multi selection for selecting text with multiple cursors. The cursor also supports keybinds such as jump to line end or jump one word forward. Below is a demo:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;multi-cursor_compressed.mov&lt;/head&gt;
    &lt;p&gt;Windows support is still being tested, but there is a windows build available in releases as well as a build script for both the standalone and embedded versions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952824</guid><pubDate>Mon, 17 Nov 2025 11:53:00 +0000</pubDate></item><item><title>Are you stuck in movie logic?</title><link>https://usefulfictions.substack.com/p/are-you-stuck-in-movie-logic</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45952890</guid><pubDate>Mon, 17 Nov 2025 12:06:37 +0000</pubDate></item><item><title>FreeMDU: Open-source Miele appliance diagnostic tools</title><link>https://github.com/medusalix/FreeMDU</link><description>&lt;doc fingerprint="af8dedd4595f4be1"&gt;
  &lt;main&gt;
    &lt;p&gt;The FreeMDU project provides open hardware and software tools for communicating with Miele appliances via their optical diagnostic interface. It serves as a free and open alternative to the proprietary Miele Diagnostic Utility (MDU) software, which is only available to registered service technicians.&lt;/p&gt;
    &lt;p&gt;Most Miele devices manufactured after 1996 include an optical infrared-based diagnostic interface, hidden behind one of the indicator lights on the front panel. On older appliances, this interface is marked by a Program Correction (PC) label.&lt;/p&gt;
    &lt;p&gt;Until now, communication with this interface required an expensive infrared adapter sold exclusively by Miele, along with their closed-source software. The goal of FreeMDU is to make this interface accessible to everyone for diagnostic and home automation purposes.&lt;/p&gt;
    &lt;p&gt;The project is split into three main components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protocol: core protocol library and device implementations&lt;/item&gt;
      &lt;item&gt;TUI: terminal-based device diagnostic and testing tool&lt;/item&gt;
      &lt;item&gt;Home: communication adapter firmware with MQTT integration for Home Assistant&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More details about the proprietary diagnostic interface and the reverse-engineering process behind this project can be found in this blog post.&lt;/p&gt;
    &lt;p&gt;Caution&lt;/p&gt;
    &lt;p&gt;This project is highly experimental and can cause permanent damage to your Miele devices if not used responsibly. Proceed at your own risk.&lt;/p&gt;
    &lt;p&gt;When a connection is established via the diagnostic interface, the appliance responds with its software ID, a 16-bit number that uniquely identifies the firmware version running on the device's microcontroller. However, this ID does not directly correspond to a specific model or board type, so it's impossible to provide a comprehensive list of supported models.&lt;/p&gt;
    &lt;p&gt;The following table lists the software IDs and device/board combinations that have been confirmed to work with FreeMDU:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Software ID&lt;/cell&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Board&lt;/cell&gt;
        &lt;cell role="head"&gt;Microcontroller&lt;/cell&gt;
        &lt;cell role="head"&gt;Optical interface location&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;360&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 223-A&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38078MC-065FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;419&lt;/cell&gt;
        &lt;cell&gt;Bare board&lt;/cell&gt;
        &lt;cell&gt;EDPW 206&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M37451MC-804FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;605&lt;/cell&gt;
        &lt;cell&gt;G 651 I PLUS-3&lt;/cell&gt;
        &lt;cell&gt;EGPL 542-C&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38027M8&lt;/cell&gt;
        &lt;cell&gt;Salt (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;629&lt;/cell&gt;
        &lt;cell&gt;W 2446&lt;/cell&gt;
        &lt;cell&gt;EDPL 126-B&lt;/cell&gt;
        &lt;cell&gt;Mitsubishi M38079MF-308FP&lt;/cell&gt;
        &lt;cell&gt;Check inlet (PC) indicator&lt;/cell&gt;
        &lt;cell&gt;üü¢ Fully supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If your appliance is not listed here but has a model number similar to one of the above, it might already be compatible. In all other cases, determining the software ID is the first step toward adding support for new devices.&lt;/p&gt;
    &lt;p&gt;Details for adding support for new devices will be provided soon.&lt;/p&gt;
    &lt;p&gt;Before using any FreeMDU components, make sure you have the Rust toolchain installed on your system.&lt;/p&gt;
    &lt;p&gt;Next, you'll need to build a communication adapter to interface with your Miele device. Once the adapter is ready, choose the appropriate use case from the options below:&lt;/p&gt;
    &lt;p&gt;If you want to repair or test your appliance:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the TUI application on your desktop computer.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to integrate your appliance into Home Assistant or another home automation system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Flash the home firmware in standalone mode onto your communication adapter and attach it to your device.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you want to develop your own software to communicate with Miele devices:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Flash the home firmware in bridge mode onto your communication adapter and attach it to your device.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Use the protocol crate to implement your custom software.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is an independent, open-source project and is not affiliated with, endorsed by, or sponsored by Miele &amp;amp; Cie. KG or its affiliates. All product names and trademarks are the property of their respective owners. References to Miele appliances are for descriptive purposes only and do not imply any association with Miele.&lt;/p&gt;
    &lt;p&gt;Licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953452</guid><pubDate>Mon, 17 Nov 2025 13:40:40 +0000</pubDate></item><item><title>Replicate is joining Cloudflare</title><link>https://replicate.com/blog/replicate-cloudflare</link><description>&lt;doc fingerprint="3552db724b9eaa89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Replicate is joining Cloudflare&lt;/head&gt;
    &lt;p&gt;Big news: We‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;Replicate‚Äôs going to carry on as a distinct brand, and all that‚Äôll happen is that it‚Äôs going to get way better. It‚Äôll be faster, we‚Äôll have more resources, and it‚Äôll integrate with the rest of Cloudflare‚Äôs Developer Platform.&lt;/p&gt;
    &lt;p&gt;The API isn‚Äôt changing. The models you‚Äôre using today will keep working. If you‚Äôve built something on Replicate, it‚Äôll keep running just like it does now.&lt;/p&gt;
    &lt;p&gt;So, why are we doing this?&lt;/p&gt;
    &lt;p&gt;At Replicate, we‚Äôre building the primitives for AI: the tools and abstractions that let software developers use AI without having to understand all the complex stuff underneath.&lt;/p&gt;
    &lt;p&gt;We started with Cog, an open-source tool which defines a standard format for what a model is. Then, we created Replicate, a platform where people can share models and run them with an API. We‚Äôve defined what a model is, how you publish it, how you run it, how you get data in and out.&lt;/p&gt;
    &lt;p&gt;These abstractions are like the low-level primitives of an operating system. But what‚Äôs interesting is that these primitives are running in the cloud. They have to ‚Äî they need specialized GPUs and clusters to scale up in production. It‚Äôs like a distributed operating system for AI, running in the cloud. In other words, the network is the computer.&lt;/p&gt;
    &lt;p&gt;Who has the best network? Cloudflare.&lt;/p&gt;
    &lt;p&gt;Cloudflare has built so many other parts of this operating system. Workers is the perfect platform for running agents and glue code. Durable Objects for managing state, R2 for storing files, WebRTC for streaming media.&lt;/p&gt;
    &lt;p&gt;Now that we‚Äôve got these low-level abstractions, we can build higher-level abstractions. Ways to orchestrate models and build agents. Ways to run real-time models, or run models on the edge.&lt;/p&gt;
    &lt;p&gt;This is why we‚Äôre joining Cloudflare.&lt;/p&gt;
    &lt;p&gt;For my whole career, I‚Äôve looked up to Cloudflare. How they built a product for developers, and turned that into a huge enterprise business. It‚Äôs the only public company that actually gets developers and knows how to build good products for them.&lt;/p&gt;
    &lt;p&gt;Cloudflare is the default for building web apps. From day one of Replicate, when we were building a prototype to apply to Y Combinator, we put Cloudflare in front of it.&lt;/p&gt;
    &lt;p&gt;Together, we‚Äôre going to become the default for building AI apps.&lt;/p&gt;
    &lt;p&gt;Check out the official announcement on Cloudflare‚Äôs blog for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45953702</guid><pubDate>Mon, 17 Nov 2025 14:11:57 +0000</pubDate></item><item><title>WeatherNext 2: Our most advanced weather forecasting model</title><link>https://blog.google/technology/google-deepmind/weathernext-2/</link><description>&lt;doc fingerprint="3ff8f453711c2ba8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WeatherNext 2: Our most advanced weather forecasting model&lt;/head&gt;
    &lt;p&gt;The weather affects important decisions we make everyday ‚Äî from global supply chains and flight paths to your daily commute. In recent years, artificial intelligence (AI) has dramatically enhanced what‚Äôs possible in weather forecasting and the ways in which we can use it.&lt;/p&gt;
    &lt;p&gt;Today, Google DeepMind and Google Research are introducing WeatherNext 2, our most advanced and efficient forecasting model. WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour. This breakthrough is enabled by a new model that can provide hundreds of possible scenarios. Using this technology, we‚Äôve supported weather agencies in making decisions based on a range of scenarios through our experimental cyclone predictions.&lt;/p&gt;
    &lt;p&gt;We're now taking our research out of the lab and putting it into the hands of users. WeatherNext 2's forecast data is now available in Earth Engine and BigQuery. We‚Äôre also launching an early access program on Google Cloud‚Äôs Vertex AI platform for custom model inference.&lt;/p&gt;
    &lt;p&gt;By incorporating WeatherNext technology, we‚Äôve now upgraded weather forecasts in Search, Gemini, Pixel Weather and Google Maps Platform‚Äôs Weather API. In the coming weeks, it will also help power weather information in Google Maps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting more possible scenarios&lt;/head&gt;
    &lt;p&gt;From a single input, we use independently trained neural networks and inject noise in function space to create coherent variability in weather forecast predictions.&lt;/p&gt;
    &lt;p&gt;Weather predictions need to capture the full range of possibilities ‚Äî including worst case scenarios, which are the most important to plan for.&lt;/p&gt;
    &lt;p&gt;WeatherNext 2 can predict hundreds of possible weather outcomes from a single starting point. Each prediction takes less than a minute on a single TPU; it would take hours on a supercomputer using physics-based models.&lt;/p&gt;
    &lt;p&gt;Our model is also highly skillful and capable of higher-resolution predictions, down to the hour. Overall, WeatherNext 2 surpasses our previous state-of-the-art WeatherNext model on 99.9% of variables (e.g. temperature, wind, humidity) and lead times (0-15 days), enabling more useful and accurate forecasts.&lt;/p&gt;
    &lt;p&gt;This improved performance is enabled by a new AI modelling approach called a Functional Generative Network (FGN), which injects ‚Äònoise‚Äô directly into the model architecture so the forecasts it generates remain physically realistic and interconnected.&lt;/p&gt;
    &lt;p&gt;This approach is particularly useful for predicting what meteorologists refer to as ‚Äúmarginals‚Äù and ‚Äújoints.‚Äù Marginals are individual, standalone weather elements: the precise temperature at a specific location, the wind speed at a certain altitude or the humidity. What's novel about our approach is that the model is only trained on these marginals. Yet, from that training, it learns to skillfully forecast 'joints' ‚Äî large, complex, interconnected systems that depend on how all those individual pieces fit together. This 'joint' forecasting is required for our most useful predictions, such as identifying entire regions affected by high heat, or expected power output across a wind farm.&lt;/p&gt;
    &lt;p&gt;Continuous Ranked Probability Score (CRPS) comparing WeatherNext 2 to WeatherNext Gen&lt;/p&gt;
    &lt;head rend="h2"&gt;From research to reality&lt;/head&gt;
    &lt;p&gt;With WeatherNext 2, we're translating cutting edge research into high-impact applications. We‚Äôre committed to advancing the state of the art of this technology and making our latest tools available to the global community.&lt;/p&gt;
    &lt;p&gt;Looking ahead, we‚Äôre actively researching capabilities to improve our models, including integrating new data sources, and expanding access even further. By providing powerful tools and open data, we hope to accelerate scientific discovery and empower a global ecosystem of researchers, developers and businesses to make decisions on today‚Äôs most complex problems and build for the future.&lt;/p&gt;
    &lt;p&gt;To learn more about geospatial platforms and AI work at Google, check out Google Earth, Earth Engine, AlphaEarth Foundations, and Earth AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learn more about WeatherNext 2&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read our paper&lt;/item&gt;
      &lt;item&gt;WeatherNext developer documentation&lt;/item&gt;
      &lt;item&gt;Explore the Earth Engine Data Catalog&lt;/item&gt;
      &lt;item&gt;Query forecast data in BigQuery&lt;/item&gt;
      &lt;item&gt;Sign up to the early access program for Cloud Vertex AI&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954210</guid><pubDate>Mon, 17 Nov 2025 15:04:26 +0000</pubDate></item><item><title>Google is killing the open web, part 2</title><link>https://wok.oblomov.eu/tecnologia/google-killing-open-web-2/</link><description>&lt;doc fingerprint="3741815498261dcc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google is killing the open web, part 2&lt;/head&gt;
    &lt;p&gt;Do not comply in advance.&lt;/p&gt;
    &lt;p&gt;I wrote a few months ago about the proxy war by Google against the open web by means of XSLT. Unsurprisingly, Google has been moving forward on the deprecation, still without providing a solid justification on the reasons why other than √¢we've been leeching off a FLOSS library for which we've finally found enough security bugs to use as an excuse√¢. They do not explain why they haven't decided to fix the security issues in the library instead, or adopt a more modern library written in a safe language, taking the opportunity to upgrade XSLT support to a more recent, powerful and easier-to-use revision of the standard.&lt;/p&gt;
    &lt;p&gt;Instead, what they do is to provide a √¢polyfill√¢, a piece of JavaScript that can allegedly used to supplant the functionality. Curiously, however, they do not plan to ship such alternative in-browser, which would allow a transparent transition without even a need to talk about XSLT at all. No, they specifically refuse to do it, and instead are requesting anyone still relying on XSLT to replace the invocation of the XSLT with a non-standard invocation of the JavaScript polyfill that should replace it.&lt;/p&gt;
    &lt;p&gt;This means that at least one of these two things are true:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;the polyfill is not, in fact, sufficient to cover all the use cases previously covered by the built-in support for XSLT, and insofar as it's not, they (Google) do not intend to invest resources in maintaining it, meaning that the task is being dumped on web developers (IOW, Google is removing a feature that is going to create more work for web developers just to provide the same functionality that they used to have from the browsers);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;insofar as the polyfill is sufficient to replace the XSLT support in the browser, the policy to not ship it as a replacement confirms that the security issues in the XSLT library used in Chrome were nothing more than excuses to give the final blow to RSS and any other XML format that is still the backbone of an independent web.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I have mentioned in the Fediverse thread I wrote before this long-form article, there's an obvious parallel here with the events that I already mentioned in my previous article: when Mozilla bent over to Google's pressure to kill off RSS by removing the √¢Live Bookmarks√¢ features from the browser, they did this on presumed technical grounds (citing as usual security and maintenance costs, but despite paid lip service to their importance for an open and interoperable web, they didn't provide any official replacement for the functionality, directing users instead to a number of add-ons that provided similar functionality, none of which are written or supported by Mozilla. Compare and contrast with their Pocket integration that they force-installed everywhere before ultimately killing the service&lt;/p&gt;
    &lt;p&gt;Actions, as they say, speak louder than words. When a company claims that a service or feature they are removing can be still accessed by other means, but do not streamline such access said alternative, and instead require their users to do the work necessary to access it, you can rest assured that beyond any word of support they may coat their actions with there is a plain and direct intent at sabotaging said feature, and you can rest assured that any of the excuses brought forward to defend the choice are nothing but lies to cover a vested interest in sabotaging the adoption of the service or feature: the intent is for you to not use that feature at all, because they have a financial interest in you not using it.&lt;/p&gt;
    &lt;p&gt;And the best defense against that is to attack, and push the use of that feature even harder.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do. Not. Comply.&lt;/head&gt;
    &lt;p&gt;This is the gist of my Fediverse thread.&lt;/p&gt;
    &lt;p&gt;Do not install the polyfill. Do not change your XML files to load it. Instead, flood their issue tracker with requests to bring back in-browser XSLT support. Report failed support for XSLT as a broken in browsers, because this is not a website issue.&lt;/p&gt;
    &lt;p&gt;I will not comply. As I have for years continued using MathML, SVG and SMIL (sometimes even all together) despite Google's intent on their deprecation, I will keep using XSLT, and in fact will look for new opportunities to rely on it. At most, I'll set up an infobox warning users reading my site about their browser's potential brokenness and inability to follow standards, just like I've done for MathML and SMIL (you can see such infoboxes in the page I linked above). And just like ultimately I was proven right (after several years, Google ended up fixing both their SMIL and their MathML support in Chrome), my expectation is that, particularly with more of us pushing through, the standards will once again prevail.&lt;/p&gt;
    &lt;p&gt;Remember: there is not technical justification for Google's choice. This is not about a lone free software developer donating their free time to the community and finding they do not have the mental or financial resources to provide a particular feature. This is a trillion-dollar ad company who has been actively destroying the open web for over a decade and finally admitting to it as a consequence of the LLM push and intentional [enshittification of web search]404mediaSearch.&lt;/p&gt;
    &lt;p&gt;The deprecation of XSLT is entirely political, fitting within the same grand scheme of the parasitic corporation killing the foundations of its own success in an effort to grasp more and more control of it. And the fact that the WebKit team at Apple and the Firefox team at Mozilla are intentioned to follow along on the same destructive path is not a counterpoint, but rather an endorsement of the analysis, as neither of those companies is interested in providing a User Agent as much as a surveillance capitalism tool that you happen to use.&lt;/p&gt;
    &lt;p&gt;(Hence why Mozilla, a company allegedly starved for resources, is wasting them implementing LLM features nobody wants instead of fixing much-voted decade-old bugs with several duplicates. Notice how the bug pertains the (mis)treatment of XML-based formats √¢like RSS.)&lt;/p&gt;
    &lt;p&gt;If you have to spend any time at all to confront the Chrome push to deprecate XSLT, your time is much better spent inventing better uses of XSLT and reporting broken rendering if/when they start disabling it, than caving to their destructive requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;The WHATWG is not a good steward of the open web&lt;/head&gt;
    &lt;p&gt;I've mentioned it before, but the WHATWG, even assuming the best of intentions at the time it was founded, is not a good steward of the open web. It is more akin to the corrupt takeover you see in regulatory capture, except that instead of taking over the W3C they just decided to get the ball and run with it, taking advantage of the fact that, as implementors, they had the final say on what counted as √¢standard√¢ (de facto if not de jure): exactly the same attitude with which Microsoft tried taking over the web through Internet Explorer at the time of the First browser war, an attitude that was rightly condemned at the time √¢even as many of those who did, have so far failed to acknowledge the problem with Google's no less detrimental approach.&lt;/p&gt;
    &lt;p&gt;The key point here is that, whatever the WHATWG was (or was intended to be) when it was founded by Opera and Mozilla developers, it is now manifestly a corporate monster. Their corporate stakeholder have a very different vision of what the Web should be compared to the vision on which the Web was founded, the vision promoted by the W3C, and the vision that underlies a truly open and independent web.&lt;/p&gt;
    &lt;p&gt;The WHATWG aim is to turn the Web into an application delivery platform, a profit-making machine for corporations where the computer (and the browser through it) are a means for them to make money off you rather than for you to gain access to services you may be interested in. Because of this, the browser in their vision is not a User Agent anymore, but a tool that sacrifices privacy, actual security and user control at the behest of the corporations √¢on the other side of the wire√¢ √¢and of their political interests (refs. for Apple, Google, and a more recent list with all of them together).&lt;/p&gt;
    &lt;p&gt;Such vision is in direct contrast with that of the Web as a repository of knowledge, a vast vault of interconnected documents whose value emerges from organic connections, personalization, variety, curation and user control. But who in the WHATWG today would defend such vision?&lt;/p&gt;
    &lt;head rend="h2"&gt;A new browser war?&lt;/head&gt;
    &lt;p&gt;Maybe what we need is a new browser war. Not one of corporation versus corporation √¢doubly more so when all currently involved parties are allied in their efforts to enclose the Web than in fostering an open and independent one√¢ but one of users versus corporations, a war to take back control of the Web and its tools.&lt;/p&gt;
    &lt;p&gt;It's kind of ironic that in a time when hosting has become almost trivial, the fight we're going to have to fight is going to be on the client side. But the biggest question is: who do we have as champions on our side?&lt;/p&gt;
    &lt;p&gt;I would have liked to see browsers like Vivaldi, the spiritual successor to my beloved classic Opera browser, amongst our ranks, but with their dependency on the Blink rendering engine, controlled by Google, they won't be able to do anything but cave, as will all other FLOSS browsers relying on Google's or Apple's engines, none of which I foresee spending any significant efforts rolling back the extensive changes that these deprecations will involve. (We see this already when it comes to JPEG√Ç XL support, but it's also true that e.g. Vivaldi has made RSS feeds first-class documents, so who knows, maybe they'll find a way for XSLT through the polyfill that was mentioned above, or something like that?)&lt;/p&gt;
    &lt;p&gt;Who else is there? There is Servo, the rendering engine that was being developed at Mozilla to replace Gecko, and that turned into an independent project when its team was fired en masse in 2020; but they don't support XSLT yet, and I don't see why they would prioritize its implementation over, say, stuff like MathML or SVG animations with SMIL (just to name two of my pet peeves), or optimizing browsing speed (seriously, try opening the home page of this site and scrolling through).&lt;/p&gt;
    &lt;p&gt;What we're left with at the moment is basically just Firefox forks, and two of these (LibreWolf and WaterFox) are basically just √¢Firefox without the most egregious privacy-invasive misfeatures√¢, which leaves the question open about what they will be willing to do when Mozilla helps Google kill XSLT, and only the other one, Pale√Ç Moon, has grown into its own independent fork (since such an old version of Firefox, in fact, that it doesn't support WebExtensions-based plugins, such as the most recent versions of crucial plugins like uBlock Origin or Privacy Badger, although it's possible to install community-supported forks of these plugins designed for legacy versions of Firefox and forks like Pale√Ç Moon).&lt;/p&gt;
    &lt;p&gt;(Yes, I am aware that there are other minor independent browser projects, like Dillo and Ladybird, but the former is in no shape of being a serious contender for general use on more sophisticated pages √¢just see it in action on this site, as always√¢ and the latter is not even in alpha phase, just in case the questionable √¢no politics√¢ policies √¢which consistently prove to be weasel words for √¢we're right-wingers but too chicken to come out as such√¢√¢ weren't enough to stay away from it.)&lt;/p&gt;
    &lt;p&gt;Periodically, I go through them (the Firefox forks, that is) to check if they are good enough for me to become my daily drivers. Just for you (not really: just for me, actually), I just tested them again. They're not ready yet, at least not for me, although I must say that I'm seeing clear improvements since my last foray into the matter, that wasn't even that long ago. In some cases, I can attest that they are even better than Firefox: for example, Pale√Ç Moon and WaterFox have good JPEG√Ç XL support (including transparency and animation support, which break in LibreWolf as they do in the latest nightly version of Firefox I tried), and Pale√Ç Moon still has first-class support for RSS, from address bar indicator to rendering even in the absence of a stylesheet (be it CSS or XSLT).&lt;/p&gt;
    &lt;p&gt;(A suggestion? Look into more microformats support. An auxiliary bar with previous/&lt;/p&gt;
    &lt;p&gt;An interesting difference is that the user interface of these browsers is perceivably less refined than Firefox'. It's a bit surprising, given the common roots, but it emerges in several more and less apparent details, from the spacing between menu items to overlapping text and icons in context menus, passing through incomplete support for dark themes and other little details that all add up, giving these otherwise quite valid browsers and amateurish feeling.&lt;/p&gt;
    &lt;p&gt;And I get it: UI design is hard, and I myself suck at it, so I'm the last person that should be giving recommendations, but I'm still able to differentiate between more curated interfaces and ones that need some work; and if even someone like me who distinctly prefers function over form finds these little details annoying, I can imagine how much worse this may feel to users who care less about the former and more about the latter. Sadly, if a new browser war is to be fought to wrestle control from the corporate-controlled WHATWG, this matters.&lt;/p&gt;
    &lt;p&gt;In the end, I find myself in a √¢waiting√¢ position. How long will it take for Firefox to kill their XSLT support? What will its closest forks (WaterFox in particular is the one I'm eyeing) be able to do about it? Or will Pale√Ç Moon remain the only modern broser with support for it, as a hard fork that has since long gone its own way? Will they have matured enough to become my primary browsers? We'll see in time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another web?&lt;/head&gt;
    &lt;p&gt;There's more to the Internet than the World Wide Web built around the HTTP protocol and the HTML file format. There used to be a lot of the Internet beyond the Web, and while much of it still remains as little more than a shadow of the past, largely eclipsed by the Web and what has been built on top of it (not all of it good) outside of some modest revivals, there's also new parts of it that have tried to learn from the past, and build towards something different.&lt;/p&gt;
    &lt;p&gt;This is the case for example of the so-called √¢Gemini Space√¢, a small corner of the Internet that has nothing to do with the LLM Google is trying to shove down everyone's throat, and in fact not only predates it, as I've mentioned already, but is intentionally built around dfferent technology to stay away from the influence of Google and the like.&lt;/p&gt;
    &lt;p&gt;The Gemini protocol is designed to be conceptually simpler than HTTP, while providing modern features like built-in transport-level security and certificate-based client-side authentication, and its own √¢native√¢ document format, the so-called gemtext.&lt;/p&gt;
    &lt;p&gt;As I said in my aforementioned Fediverse thread:&lt;/p&gt;
    &lt;p&gt;There's something to be said about not wanting to share your environment with the poison that a large part of the web has become, but at the same time, there's also something to be said about throwing away the baby with the bathwater. The problem with the web isn't technical, it's social. The tech itself is fine.&lt;/p&gt;
    &lt;p&gt;I'm not going to write up an extensive criticism of the Gemini Space: you can find here an older opinion by the author of &lt;code&gt;curl&lt;/code&gt;,
(although it should be kept in mind that things have changed quite a bit since:
for example, the specification of the different components has been separated,
as suggested by Daniel),
and some criticism about how gemtext is used.&lt;/p&gt;
    &lt;p&gt;I'm not going to sing the praises of the Gemini protocol or gemtext either, even though I do like the idea of a web built on lightweight markup formats: I would love it if browsers had native support for formats like Markdown or AsciiDoc (and gemtext, for the matter): it's why I keep the AsciiDoctor Browser Extension installed.&lt;/p&gt;
    &lt;p&gt;But more in general, the Web (or at least its user agents) should not differentiate. It should not differentiate by protocol, and it should not differentiate by format. We've seen it with image formats like MNG being unfairly excluded, with [motivations based on alleged code bloat][nomng] that today are manifest in their idiocy (and yes, it hasn't escaped my that even Pale√Ç Moon doesn't support the format), and we're seeing it today with JPEG√Ç XL threatened with a similar fate, without even gracing us with a ridiculous excuse. On the upside, we have browsers shipping with a full-fledged PDF reader, which is a good step towards the integration of this format with the greater Web.&lt;/p&gt;
    &lt;p&gt;In an ideal world, browsers would have not deprecated older protocols like Gopher or FTP, and would just add support for new ones like Gemini, as they would have introduced support for new (open) document formats as they came along.&lt;/p&gt;
    &lt;p&gt;(Why insist on the open part? In another Fediverse thread about the XSLT deprecation I had an interesting discussion with the OP about SWF, the interactive multimedia format for the Web at the turn of the century. The Adobe Flash Player ultimately fell out of favour, arguably due to the advent of mobile Internet: it has been argued that the iPhone killed Flash, and while there's some well-deserved criticism of hypocrisy levelled against Steve Jobs infamous Thoughts on Flash letter, it is true that what ultimatelly truly killed the format was it being proprietary and not fully documented. And while we might not want to cry about the death of a proprietary format, it remains true even today that the loss of even just legacy suport for it has been a significant loss to culture and art, as argued by @whiteshark√¢@mastodon.social.)&lt;/p&gt;
    &lt;head rend="h2"&gt;A Web of interconnected software?&lt;/head&gt;
    &lt;p&gt;It shouldn't be up to the User Agent to determine which formats the user is able to access, and through which protocol. (If I had any artistic prowess (and willpower), I'd hack the √¢myth of consensual X√¢ meme representing the user and the server saying √¢I consent√¢, and the browser saying √¢I don't√¢.) I do appreciate that there is a non-trivial maintenance cost that grows with the number of formats and protocols, but we know from classic Opera that it is indeed quite possible to ship a full Internet suite in a browser packaging.&lt;/p&gt;
    &lt;p&gt;In the old days, browser developers were well-aware that a single vendor couldn't √¢cover all bases√¢, which is how interfaces like the once ubiquituous NPAPI were born. The plug-in interface has been since removed from most browsers, an initiative again promoted by Google, announced in 2013 and completed in 2015 (I should really add this to my previous post on Google killing the open web, but I also really don't feel like touching that anymore; here will have to suffice), with the other major browsers quickly following suit, and its support is now relegated only to independent browsers like Pale√Ç Moon.&lt;/p&gt;
    &lt;p&gt;And even if it can be argued that the NPAPI specifically was indeed mired with unfixable security and portability issues and it had to go, its removal without a clear cross-browser upgrade path has been a significant loss for the evolution of the web, destroying the primary √¢escape hatch√¢ to solve the chicken-and-egg problem of client-side format support versus server-side format adoption. By the way, it was also responsible for the biggest W3C blunder, the standardization of DRM for the web through the so-called Encrypted Media Extensions, a betrayal of the W3C own mission statement.&lt;/p&gt;
    &lt;head rend="h3"&gt;The role of multimedia streaming in the death of the User Agent&lt;/head&gt;
    &lt;p&gt;The timeline here is quite interesting, and correlates with the already briefly mentioned history of Flash, and its short-lived Microsoft Silverlight competitor, that were largely responsible for the early expansive growth of multimedia streaming services in the early years of the XXI century: with the tension between Apple's effort to kill Flash and the need of emerging streaming services like Netflix' and Hulu's to support in-browser multimedia streaming, there was a need to improve support for multimedia formats in the nascent HTML5 specification, but also a requirement from the MAFIAA partners that such a support would allow enforcing the necessary restrictions that would, among other things, prevent users from saving a local copy of the stream, something that could be more easily enforced within the Flash players the industries had control over than in a User Agent controlled by the user.&lt;/p&gt;
    &lt;p&gt;This is where the development of EME came in in 2013: this finally allowed a relatively quick phasing out of the Flash plugin, and a posteriori of the plugin interface that allowed its integration with the browsers: by that time, the Flash plugin was by and large the plugin the API existed for, and the plugin itself was indeed still supported by the browsers for some time after support for the API was otherwise discontinued (sometimes through alternative interfaces such as the PPAPI, other times by keeping the NPAPI support around, but only enabled for the Flash plugin).&lt;/p&gt;
    &lt;p&gt;There are several interesting consideration that emerge from this little glimpse at the history of Flash and the EME.&lt;/p&gt;
    &lt;p&gt;First of all, this is one more piece of history that goes to show how pivotal the year 2013 was for the enshittification of the World Wide Web, as discussed already.&lt;/p&gt;
    &lt;p&gt;Secondly, it shows how the developers of major browsers are more than willing to provide a smooth transition path with no user intervention, at least when catering to the interests of major industries. This indicates that when they don't, it's not because they can't: it's because they have a vested interest in not doing it. Major browser development is now (and has been for over a decade at least) beholden not to the needs and wants of their own users, but to those of other industries. But I repeat myself.&lt;/p&gt;
    &lt;p&gt;And thirdly, it's an excellent example, for the good and the bad, of how the plugin interface has helped drive the evolution of the web, as I was saying.&lt;/p&gt;
    &lt;head rend="h3"&gt;Controlled evolution&lt;/head&gt;
    &lt;p&gt;The removal of NPAPI support, followed a few years later by the removal of the (largely Chrome-specific) PPAPI interface (that was supposed to be the √¢safer, more portable√¢ evolution of NPAPI), without providing any alternative, is a very strong indication of the path that browser development has taken in the last √¢decade plus√¢: a path where the Web is entirely controlled by what Google, Apple and Microsoft (hey look, it's GAFAM all over again!) decide about what is allowed on it, and what is not allowed to not be on it (to wit, ads and other user tracking implements).&lt;/p&gt;
    &lt;p&gt;In this perspective, the transition from plugins to browser extensions cannot be viewed (just) as a matter of security and portability, but √¢more importantly, in fact√¢ as a matter of crippled functionality: indeed, extensions maintain enough capabilities to be a vector of malware and adware, but not enough to circumvent unwanted browser behavior, doubly more so with the so-called Extension Manifest V3 specifically designed to thwart ad blocking as I've already mentioned in the previous post of the series.&lt;/p&gt;
    &lt;p&gt;With plugins, anything could be integrated in the World Wide Web, and such integration would be close to as efficient as could be. Without plugins, such integration, when possible at all, becomes clumsier and more expensive.&lt;/p&gt;
    &lt;p&gt;As an example, there are browser extensions that can introduce support for JPEG√Ç XL to browsers that don't have native support. This provides a workaround to display such images in said browsers, but when a picture with multiple formats is offered (which is what I do e.g. to provide a PNG fallback for the JXL images I provide), this results in both the PNG and JXL formats being downloaded, increasing the amount of data transferred instead of decreasing it (one of the many benefits of JXL over PNG). By contrast, a plugin could register itself a handler for the JPEG√Ç XL format, and the browser would then be able to delegate rendering of the image to the plugin, only falling back to the PNG in case of failure, thus maximizing the usefulness of the format pending a built-in implementation.&lt;/p&gt;
    &lt;p&gt;The poster child of this lack of efficiency is arguably MathJax, that has been carrying for nearly two decades the burden of bringing math to the web while browser implementors slacked off on their MathML support. And while MathJax does offer more than just MathML support for browers without native implementations, there is little doubt that it would be more effective in delivering the services it delivers if it could be a plugin rather than a multi-megabyte (any efforts to minimize its size notwithstanding) JavaScript library each math-oriented website needs to load.&lt;/p&gt;
    &lt;p&gt;(In fact, it is somewhat surprising that there isn't a browser extesion version of MathJax that I can find other than a GreaseMonkey user script with convoluted usage requirements, but I guess this is the cost we have to pay for the library flexibility, and the sandboxing requirements enforced on JavaScript in modern browsers.)&lt;/p&gt;
    &lt;p&gt;Since apparently √¢defensive writing√¢ is a thing we need when jotting down an article such as this (as if it even matters, given how little attention people give to what they read √¢if they read it at all√¢ before commenting), I should clarify that I'm not necessarily for a return to NPAPI advocating. We have decades of experience about what could be considered the actual technical issues with that interface, and how they can be improved upon (which is for example what PPAPI allegedly did, before Google decided it would be better off to kill plugins entirely and thus gain full control of the Web as a platform), as we do about sandboxing external code running in browsers (largely through the efforts to sandbox JavaScript). A better plugin API could be designed.&lt;/p&gt;
    &lt;p&gt;It's not going to happen. It is now apparent that the major browsers explicitly and intentionally do not want to allow the kind of flexibility that plugins would allow, hiding their controlling efforts behind security excuses. It would thus be up to the minority browsers to come up with such an interface (or actually multiple ones, at least one for protocols and one for document types), but with most of them beholden to the rendering engines controlled by Google (for the most part), Apple (some, still using WebKit over Blink), and Mozilla (the few Firefox forks), they are left with very little leeway, if any at all, in terms of what they can support.&lt;/p&gt;
    &lt;p&gt;But even if, by some miraculous convergence, they did manage to agree on and implement support for such an API, would there actually be an interest by third party to develop plugins for it? I can envision this as a way for browsers to share coding efforts in supporting new protocols and formats before integrating them as first-class (for example, the already mentioned Gemini protocol and gemtext format could be implemented first as a plugin to the benefit of any browsers supporting such hypothetical interfaces) but there be any interest in developing for it, rather tha just trying to get the feature implemented in the browsers themselves?&lt;/p&gt;
    &lt;head rend="h3"&gt;A mesh of building blocks&lt;/head&gt;
    &lt;p&gt;Still, let me dream a bit of something like this, a browser made up of composable components, protocol handlers separate from primary document renderers separate from attachment handlers.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;A new protocol comes out?&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Implement a plugin to handle that, and you can test it by delivering the same content over it, and see it rendered just the same from the other components in the chain.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;A new document format comes out?&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Implement a plugin to handle that, and it will be used to render documents in the new format.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;A new image format comes out?&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Implement a plugin to handle that, and any image in the new format will be visible.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;A new scripting language comes out?&lt;/item&gt;
      &lt;item rend="dd-4"&gt;You guessed it: implement a plugin to handle that√Ç √¢¬¶&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How much tech would have had a real chance at proving itself in the field if this had been the case, or would have survived being ousted not by technical limitations, but by unfriendly corporate control? Who knows, maybe RSS and Atom integration would still be trivially at everybody's hand; nobody would have had to fight with the long-standing bugs in PNG rendering from Internet Explorer, MNG would have flourished, JPEG√Ç XL would have become ubiquituous six months after the specification had been finalized; we would have seen HTML+SMIL provide declarative interactive documents without JavaScript as far back as 2008; XSLT 2 and 3 would have long superseded XSLT 1 as the templating languages for the web, or XSLT would have been supplanted by the considerably more accessible XQuery; XHTML2 would have lived and grown alongside HTML5, offering more sensible markup for many common features, and much-wanted capabilities such as client-side includes.&lt;/p&gt;
    &lt;p&gt;The web would have been very different from what it is today, and most importantly we would never would have had to worry about a single corporation getting to dictate what is and what isn't allowed on the Web.&lt;/p&gt;
    &lt;p&gt;But the reality is much harsher and darker. Google has control, and we do need to wrestle it out of their hands.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resist&lt;/head&gt;
    &lt;p&gt;So, do not comply.&lt;lb/&gt; Resist.&lt;lb/&gt; Force the unwanted tech through.&lt;lb/&gt; Use RSS.&lt;lb/&gt; Use XSLT.&lt;lb/&gt; Adopt JPEG√Ç XL as your primary image format.&lt;lb/&gt; And report newly broken sites for what they are:&lt;lb/&gt; a browser fault, not a content issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Post scriptum&lt;/head&gt;
    &lt;p&gt;I would like to add here any pi√É¬®ces de r√É¬©sistance for XSLT.&lt;/p&gt;
    &lt;p&gt;I'm going to inaugurate with a link I've just discovered thanks to JWZ:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;xslt.rip (best viewed with a browser that supports XSLT; viewing the source is highly recommended);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;and last but not least (yeah I know, doesn't make much sense with the current short list, but still), a shameless plug of my own website, of course, because of the idea to use XSLT not to produce HTML, but to produce SVG.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954560</guid><pubDate>Mon, 17 Nov 2025 15:41:08 +0000</pubDate></item><item><title>WBlock: A New Ad-Blocker for Safari</title><link>https://github.com/0xCUB3/wBlock</link><description>&lt;doc fingerprint="f9dbd9187495bbb1"&gt;
  &lt;main&gt;
    &lt;p&gt; A Safari content blocker for macOS, iOS, and iPadOS utilizing declarative content blocking rules.&lt;lb/&gt; Supports 750,000 rules across 5 extensions with Protocol Buffer storage and LZ4 compression. &lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Looking for a detailed comparison? Check out my comparison guide to see how wBlock stacks up against other Safari content blockers.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Core Architecture&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Dependencies &amp;amp; Standards&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;wBlock is free and open-source software. Financial contributions support ongoing development and maintenance:&lt;/p&gt;
    &lt;head&gt;How does wBlock compare to other ad blockers?&lt;/head&gt;
    &lt;p&gt;Check out our comparison guide vs uBlock Origin Lite, AdGuard, and Wipr.&lt;/p&gt;
    &lt;head&gt;Can I use my own filter lists?&lt;/head&gt;
    &lt;p&gt;Yes! wBlock supports any AdGuard-compatible filter list. Add the URL in Custom Filter Lists.&lt;/p&gt;
    &lt;head&gt;Does wBlock slow down Safari?&lt;/head&gt;
    &lt;p&gt;No. wBlock uses Safari's native declarative content blocking API, which processes rules in a separate process. Memory overhead is ~40 MB at idle with no measurable impact on page load times.&lt;/p&gt;
    &lt;head&gt;Do userscripts work on iOS?&lt;/head&gt;
    &lt;p&gt;Yes. The userscript engine implements the Greasemonkey API (GM_getValue, GM_setValue, GM_xmlhttpRequest, GM_addStyle) on both iOS and macOS via Safari Web Extensions.&lt;/p&gt;
    &lt;head&gt;How often do filters update?&lt;/head&gt;
    &lt;p&gt;Auto-update intervals are configurable from 1 hour to 7 days, or manually triggered. Updates use HTTP conditional requests (If-Modified-Since/ETag headers) to minimize bandwidth usage.&lt;/p&gt;
    &lt;head&gt;Is the element zapper available on iOS?&lt;/head&gt;
    &lt;p&gt;Not yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954626</guid><pubDate>Mon, 17 Nov 2025 15:48:18 +0000</pubDate></item><item><title>Project Gemini</title><link>https://geminiprotocol.net/</link><description>&lt;doc fingerprint="7b91a15fbb3f9295"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Gemini&lt;/head&gt;
    &lt;head rend="h2"&gt;Gemini in 100 words&lt;/head&gt;
    &lt;p&gt;Gemini is a new internet technology supporting an electronic library of interconnected text documents. That's not a new idea, but it's not old fashioned either. It's timeless, and deserves tools which treat it as a first class concept, not a vestigial corner case. Gemini isn't about innovation or disruption, it's about providing some respite for those who feel the internet has been disrupted enough already. We're not out to change the world or destroy other technologies. We are out to build a lightweight online space where documents are just documents, in the interests of every reader's privacy, attention and bandwidth. &lt;/p&gt;
    &lt;p&gt; If you'd like to know more, read our FAQ&lt;/p&gt;
    &lt;p&gt; Or, if you'd prefer, here's a video overview &lt;/p&gt;
    &lt;head rend="h2"&gt;Official resources&lt;/head&gt;
    &lt;p&gt; Project Gemini news&lt;/p&gt;
    &lt;p&gt; Project Gemini documentation&lt;/p&gt;
    &lt;p&gt; Project Gemini history&lt;/p&gt;
    &lt;p&gt; Known Gemini software &lt;/p&gt;
    &lt;p&gt;All content at geminiprotocol.net is CC BY-NC-ND 4.0 licensed unless stated otherwise:&lt;/p&gt;
    &lt;p&gt; CC Attribution-NonCommercial-NoDerivs 4.0 International &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45954640</guid><pubDate>Mon, 17 Nov 2025 15:50:04 +0000</pubDate></item><item><title>Living my best Sun Microsystems ecosystem life in 2025</title><link>https://www.osnews.com/story/143570/living-my-best-sun-microsystems-ecosystem-life-in-2025/</link><description>&lt;doc fingerprint="a647d91a1982adec"&gt;
  &lt;main&gt;
    &lt;p&gt;In my lifetime, there‚Äôs been one ecosystem I deeply regret having missed out on: the Sun Microsystems ecosystem of the late 2000s. At that time, the company offered a variety of products that, when used together, formed a comprehensive ecosystem that was a fascinating, albeit expensive alternative to Microsoft and Apple. While not really intended for home use, I‚Äôve always believed that Sun‚Äôs approach to computing would‚Äôve made for an excellent computing environment in the home.&lt;/p&gt;
    &lt;p&gt;Since I was but a wee university student in the late 2000s living in a small apartment, I did not have the financial means nor the space to really test this hypothesis. Now, though, Sun‚Äôs products from that era are decidedly retro, and a lot more approachable ‚Äì especially if you have incredibly generous readers. So sit down and buckle up, because we‚Äôve got a long one today.&lt;/p&gt;
    &lt;p&gt;If you wish to support OSNews and longform content like this, consider becoming a Patreon or donating to our Ko-Fi. Note that absolutely zero generative ‚ÄúAI‚Äù was used in the writing of this article. No ‚ÄúAI‚Äù writing aids, no ‚ÄúAI‚Äù summaries, no ChatGPT, no Gemini search nonsense, nothing. I take pride in doing research and writing properly, without the ‚Äúaid‚Äù of digital parrots with brain damage, and if there‚Äôs any errors, they‚Äôre mine and mine alone. Take pride in your work and reject ‚ÄúAI‚Äù.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Ultra 45: the central hub&lt;/head&gt;
    &lt;p&gt;In the early 2000s, it had already become obvious that the future of workstations lied not with custom architectures, bespoke processors, and commercial UNIX variants, but with standard x86, off-the-shelf Intel and AMD processors, and Windows and Linux. The writing was on the wall, everyone knew it, and the ensuing consolidation on x86 turned into a veritable bloodbath. In the ‚Äô80s and ‚Äô90s, many of these ISAs were touted as vastly superior x86 killers, but fast-forward a decade or two, and x86 had bested them all in both price and performance, leaving behind a trail of dead ISAs.&lt;/p&gt;
    &lt;p&gt;Never bet against x86.&lt;/p&gt;
    &lt;p&gt;Virtually none of the commercial UNIX variants survived the one-two punch of losing the ISA they were married to and the rising popularity of Linux in the workstation space. HP-UX was tied to HP‚Äôs PA-RISC, and both died. SGI‚Äôs IRIX was tied to MIPS, and both died. Tru64 was tied to Alpha, and both died. The two exceptions are IBM‚Äôs AIX and Sun‚Äôs Solaris. AIX workstations were phased out, but AIX is still nominally in development for POWER servers, but wholly inaccessible to anyone who doesn‚Äôt wear a suit and has a massive corporate spending budget. Solaris, meanwhile, which had long been available on x86, saw its ‚Äúown‚Äù ISA SPARC live on in the server space until roughly 2017 or so, and was even briefly available as open source until Oracle did its thing. As a result, Solaris and its derivative Illumos are still nominally in active development, but in the grand scheme of things they‚Äôre barely even a blip on the radar in 2025.&lt;/p&gt;
    &lt;p&gt;Never bet against Linux.&lt;/p&gt;
    &lt;p&gt;During these tumultuous times, the various commercial UNIX vendors all pushed out systems that would become the final hurrahs of their respective UNIX workstation lines. DEC, then owned by HP, released its AlphaStation ES47 in 2003, marking the end of the road for Alpha and Tru64 UNIX. HP‚Äôs own PA-RISC architecture and HP-UX met their end with the HP c8000 (which I own), an all-out PA-RISC monster with two dual-core processors running at 1.1GHz. SGI gave its MIPS line of machines running IRIX a massive send-off with the enigmatic and rare Tezro in 2003. In 2005, IBM tried one last time with the IntelliStation POWER 285, followed a few months later by the heavily cut-down 185, the final AIX workstation.&lt;/p&gt;
    &lt;p&gt;And Sun unveiled the Ultra 45, its final SPARC workstation, in 2006. Sun was already in the middle of its transition to x86 with machines like the Sun Java Desktop System and its successors, the Ultra 20 and 40, and then surprised everyone by reviving their UltraSPARC workstation line with the Ultra 25 and 45, which shared most ‚Äì all? ‚Äì of their enclosures with their x86 brethren. They were beautiful, all-aluminium machines with gorgeous interior layouts, and a striking full-grill front, somewhat inspired by the PowerMac G5 of that era.&lt;/p&gt;
    &lt;p&gt;And ever since the Ultra 45 was rumoured in late 2005 and then became available in early 2006, I‚Äôve been utterly obsessed with it. It‚Äôs taken almost two decades, but thanks to an unfathomably generous donation from KDE e.V. board member and FreeBSD contributor Adriaan de Groot, a very unique and storied Sun Ultra 45 and a whole slew of accessories showed up at my doorstep only a few weeks ago. Let‚Äôs look back upon this piece of history that is but a footnote to most, but a whole book to me ‚Äì and experience Sun‚Äôs ecosystem from around 2006, today.&lt;/p&gt;
    &lt;p&gt;First and foremost, I want to express my deep gratitude to Adriaan de Groot. Without him, none of this would have been possible, and I can‚Äôt put into words how grateful I am. He donated this Ultra 45 to me at no cost ‚Äì not even the cost of shipping ‚Äì and he also shipped another box to me containing a few Sun Ray thin clients, completing the late 2000s Sun ecosystem I now own. Since the Ultra 45 was technically owned by KDE e.V. ‚Äì more on that below ‚Äì I‚Äôd also like to thank the KDE e.V. Board for giving Adriaan permission for the donation. I‚Äôd also like to thank Volker A. Brandt, who sent me a Sun Ray 3, a few Ultra 45 hard drive brackets, and some other Sun goodies.&lt;/p&gt;
    &lt;p&gt;The Sun Ultra 45 De Groot sent me was a base model with an upgraded GPU. It had a single UltraSPARC IIIi 1.6Ghz processor, 1GB of RAM, and the most powerful GPU Sun ever released for its SPARC workstation line, the Sun XVR-2500, a rebadged 3Dlabs Wildcat Realizm with 256MB of GDDR3 memory. Everything else you might need ‚Äì sound, networking, and so on ‚Äì are integrated into the motherboard. It also comes with a slot-loading, slimline DVD drive, a 250GB 7200 RPM SATA hard drive, and its massive 1000W power supply.&lt;/p&gt;
    &lt;p&gt;First order of business was upgrading the machine to match the specifications I wanted, with the most important upgrade being doubling the processor count. Finding a second 1.6Ghz UltraSPARC IIIi processor was easy, as they‚Äôre all over eBay and won‚Äôt cost you more than a few dozen euro excl. any shipping; they were also used in various Sun SPARC servers and are thus readily available. The bigger issue is finding a second CPU cooler, as they are entirely custom for Sun hardware and quite difficult to find. I found a seller on eBay who had them in stock, but be prepared to pay out the nose ‚Äì I paid about ‚Ç¨40 for the CPU, but around ‚Ç¨160 for the cooler, both excl. shipping.&lt;/p&gt;
    &lt;p&gt;Installing the second CPU and cooler was a breeze, as it‚Äôs no different than installing a CPU or cooler on any other, regular PC. The processor was detected properly by the machine, and the cooler whirred to life without any issues, but of course, if you‚Äôre buying used you may always run into issues with parts. If you want to save some money, there is a way to use a specific cooler from a Dell workstation instead (and possibly others?), but I wanted the real deal and was willing to pay for it.&lt;/p&gt;
    &lt;p&gt;The second upgrade was the RAM. A mere 1GB wasn‚Äôt going to cut it for me, so alongside the processor and cooler I also ordered a set of four 1GB RAM sticks, the exact right kind, and ECC registered, too, as the machine demands it. This turned out to be a major issue, as I discovered the machine simply would not boot in any way, shape, or form with this new RAM installed. It didn‚Äôt even throw up any error message over serial, and as such, it took me a while to pinpoint the issue. Thankfully, I remembered I had a broken, non-repairable Sun server from the same era as the Ultra 45 lying around, and it just so happened to have 8‚úï1GB Sun-branded RAM sticks in it. I pilfered the sticks out of the server, stuck them in the Ultra 45, and the machine booted up without any problems.&lt;/p&gt;
    &lt;p&gt;I later learned from people on Fedi who used to work with Sun gear from this era that RAM compatibility was always a major headache. It seems the wisest thing to do is to just buy Sun-branded memory kits, because there‚Äôs very little guarantee any generic RAM will work, even if it is entirely identical to whatever sticks Sun slapped its brand stickers on. For now, 8GB is enough for me, but in a future moment of weakness, I may order 8‚úï2GB Sun-branded memory to max the Ultra 45 out. The main reason you may want to invest in a decent amount of RAM is to make ZFS on Solaris 10 happy, so take that into account.&lt;/p&gt;
    &lt;p&gt;Aside from these upgrades to the base system itself, I also planned two specialty upgrades in the form of two unique expansion cards. First, a Sun Flash Accelerator card to speed up ZFS‚Äôs operations on the spinning hard drive, and second, a SunPCi IIIpro, which is an entire traditional x86 PC on a PCI-X card, the final iteration of a series of cards designed specifically for allowing Solaris SPARC users to run Windows inside their workstation. I‚Äôll detail these two expansion in more detail later in the article.&lt;/p&gt;
    &lt;head rend="h5"&gt;What‚Äôs in an Ultra 45?&lt;/head&gt;
    &lt;p&gt;The Sun Ultra 45 was launched as one of four brand new Sun workstations, with an entirely new design shared between all four of them. Two were successors to Sun‚Äôs first (okay, technically second) foray into x86 workstations, the Java Workstation: the Ultra 20 (single-socket Opteron) and Ultra 40 (dual-socket Opteron). These were mirrored by the Ultra 25 (single-socket UltraSPARC IIIi) and Ultra 45 (dual-socket UltraSPARC IIIi). However, where the Ultra 20/40 were genuine improvements over their Java Workstation predecessors, the story gets a bit more muddled when it comes their SPARC brethren.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take a look at the most powerful direct predecessor of the Ultra 45, the Sun Blade 2500 Silver. The table below lists the core specifications of the Blade 2500 Silver compared to the Ultra 45. Notice anything?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2500 Silver (2005)&lt;/cell&gt;
        &lt;cell&gt;Ultra 45 (2006)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CPU&lt;/cell&gt;
        &lt;cell&gt;2√óUltraSPARC IIIi 1.6Ghz&lt;/cell&gt;
        &lt;cell&gt;2√óUltraSPARC IIIi 1.6Ghz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CPU cache&lt;/cell&gt;
        &lt;cell&gt;64KB data&lt;p&gt;32KB instruction&lt;/p&gt;&lt;p&gt;1MB L2 cache&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;64KB data&lt;p&gt;32KB instruction&lt;/p&gt;&lt;p&gt;1MB L2 cache&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RAM&lt;/cell&gt;
        &lt;cell&gt;DDR SDRAM (PC2100)&lt;p&gt;8 slots&lt;/p&gt;&lt;p&gt;16GB max.&lt;/p&gt;&lt;p&gt;ECC registered&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;DDR SDRAM (PC2100)&lt;p&gt;8 slots&lt;/p&gt;&lt;p&gt;16GB max.&lt;/p&gt;&lt;p&gt;ECC registered&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPUs&lt;/cell&gt;
        &lt;cell&gt;XVR-100&lt;p&gt;XVR-600&lt;/p&gt;&lt;p&gt;XVR-1200&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;XVR-100&lt;p&gt;XVR-300&lt;/p&gt;&lt;p&gt;XVR-2500&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PCI&lt;/cell&gt;
        &lt;cell&gt;6 PCI slots 64bit&lt;p&gt;3√ó33/66MHz&lt;/p&gt;&lt;p&gt;3√ó33MHz&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;2√óPCIe size √ó16/lanes √ó8&lt;p&gt;1√óPCIe size √ó8/lanes √ó4&lt;/p&gt;&lt;p&gt;2√óPCI-X 100MHz/64bit&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Storage interface&lt;/cell&gt;
        &lt;cell&gt;1√óUltra160 SCSI&lt;/cell&gt;
        &lt;cell&gt;SAS/SATA controller&lt;p&gt;4 disks&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;As you can see, the Ultra 45 was only a very modest upgrade to the Blade 2500 Silver. While the upgrades the Ultra 45 brings over its predecessor are very welcome, it‚Äôs not like upgraded expansion slots and the move to SAS/SATA would make Blade 2500 owners rush out to upgrade in droves. For heavy graphics users, the new XVR-2500 graphics card may have been tempting as it is inherently incompatible with the Blade 2500 (it uses PCIe), but I have a feeling many customers at the time would‚Äôve probably just opted to move to x86 instead. For all intents and purposes, the Ultra 45 was a slim upgrade to its predecessor.&lt;/p&gt;
    &lt;p&gt;The story gets even more problematic for the SPARC side of Sun‚Äôs workstation business when you consider the age of the 2500 line. While the 2500 Silver was released in early 2005, its only upgrade compared to the 2500 Red was a clock speed bump (from 1280Mhz to 1600MHz), and the 2500 Red was released in 2003. This means that the Ultra 45 is effectively a computer from 2003 with improved expansion slots and a fancy new case. As a final knock on the Ultra 45, its processor had already been supplanted by Sun‚Äôs first multicore SPARC processors, the UltraSPARC IV in 2004 and the UltraSPARC IV+ in 2005, and Sun‚Äôs first multicore, multithreaded processor, the UltraSPARC T1, in 2005. These chips would never make it to any workstations, being used in servers exclusively.&lt;/p&gt;
    &lt;p&gt;Sun clearly knew further investments in SPARC workstations were simply not worth it at the time, and thus opted to squeeze as much as it could out of a 2000-2003ish platform, instead of investing in the development of a brand new workstation platform built around the UltraSPARC IV/IV+/T1. In other words, while the Ultra 45 is the last and most powerful SPARC workstation Sun ever made, it wasn‚Äôt really the balls-to-the-wall SPARC workstation sendoff it could‚Äôve been, and that‚Äôs a shame.&lt;/p&gt;
    &lt;head rend="h5"&gt;But this one is mine&lt;/head&gt;
    &lt;p&gt;Now that we have a good idea of where the Ultra 45 stood in the market, let‚Äôs take a closer look at my specific machine. My Ultra 45 is not just any machine, but actually a pre-production model, or, in Sun parlance, an ‚ÄúNSG EARLY ACCESS EVALUATION UNIT‚Äù. The bright orange sticker on the side and the big yellow sticker at the top make it very clear this isn‚Äôt your ordinary Ultra 45.&lt;/p&gt;
    &lt;p&gt;I‚Äôve removed and cleaned some other sticker residue, but these will remain exactly where they are, as I consider them crucial parts of its unique history. The fact it‚Äôs a pre-production unit means there are some very small differences between this particular machine and the final version sold to consumers. The biggest difference is found on the inside, where my model misses the two RAM air ducts found on the final version; my wild guess is that during late-stage testing they discovered the RAM could use some extra fresh air, and added the ducts.&lt;/p&gt;
    &lt;p&gt;Another difference inside is that the original CPU cooler, which came with the machine, is purple, while the second CPU cooler I bought off eBay is silver. As far as I can tell based on checking countless photos online, all CPU coolers on final models were silver, making my single purple cooler an oddity. I‚Äôd love to know the story behind the purple cooler ‚Äì Sun used purple a lot in its branding and hardware design at this point, and it could be that this is a cooler from one of their many server models. Other than the colour, it‚Äôs entirely identical to its silver counterpart.&lt;/p&gt;
    &lt;p&gt;On the outside, the only sign this is a pre-production model ‚Äì other than the stickers ‚Äì is the fact that the ports and LEDs on the front of the device are unlabeled, while the final models had nice and clear labels. My machine also lacked the ‚ÄúUltra 45‚Äù badge at the bottom of the front panel, so I did a silly and spent around ‚Ç¨50 (incl. shipping and EU import duties) on a genuine replacement. It clicks into place in a dedicated hole in the metal meshwork.&lt;/p&gt;
    &lt;p&gt;It‚Äôs the little details that matter.&lt;/p&gt;
    &lt;p&gt;On the front of my Ultra 45, there‚Äôs a strong hint as to it history: a yellow label maker sticker that says ‚ÄúKDE project‚Äù. Sun‚Äôs branch in Amersfoort, The Netherlands, donated (or loaned out?) this particular Ultra 45 to KDE e.V. back in 2008 or 2009, so that the KDE project could work on KDE for Solaris and SPARC. You can even find blog posts by Adriaan de Groot about this very machine from that time period. It served that function for a few years, I would guess up until around 2010, when Oracle acquired Sun and subsequently took Solaris closed-source again.&lt;/p&gt;
    &lt;p&gt;Since then, it‚Äôs mostly been sitting unused in Adriaan‚Äôs office, until he offered to send it to me (after confirming with KDE e.V. it could be donated to me). Considering KDE is an important part of the machine‚Äôs history, I‚Äôm leaving the little KDE label right where it is. Perhaps Sun sent out its preproduction machines to people and projects that could make use of it, which was a nice ‚Äì and a little self-serving, of course ‚Äì gesture. Now it‚Äôs getting yet another lease on life as by far my favourite (retro)computer in my collection, which is pretty neat.&lt;/p&gt;
    &lt;head rend="h5"&gt;The operating system&lt;/head&gt;
    &lt;p&gt;Once I had the machine set up and booting into the OpenBoot prompt, it was time to settle on the software I‚Äôd be running on it. Since I tend to prefer setting up machines like this as historically accurate as is reasonable, Solaris 10 was the obvious choice. Luckily, Oracle still makes the SPARC version of Solaris 10 available in the form of Solaris 10 1/13 as a free download. This article won‚Äôt go too deep into operating system installation and configuration ‚Äì it‚Äôs straightforward and well-documented ‚Äì but I do have a few notes I‚Äôd like to share.&lt;/p&gt;
    &lt;p&gt;First and foremost, if you intend to use ZFS as your file system ‚Äì and you should ‚Äì make sure you have enough RAM, as mentioned earlier, but also to start the installer in text mode. You can‚Äôt install on ZFS when using the graphical installer, in which you‚Äôll be restricted to UFS. Both variants of the installer are easy to use, straightforward, and a breeze to get through for anyone reading OSNews (or anyone crazy enough to buy SPARC hardware in 2025). If you‚Äôve never worked with SPARC hardware and Sun‚Äôs OpenBoot before, have a list of &lt;code&gt;ok&lt;/code&gt;&amp;gt; prompt commands at hand to boot the correct devices and change any low-level hardware settings.&lt;/p&gt;
    &lt;p&gt;The 1/13 in Solaris 10 1/13 means the DVD ISO is up-to-date as of January 2013, and sadly, Oracle hides post-1/13 patchsets behind support contract paywalls, so you won‚Äôt be getting them from any official sources. There‚Äôs a few 2018 and 2020 patchsets floating around, as well as collections of individual patch files, but I‚Äôve some issues with those. One of the major issues I ran into with a more recent patchset is that it broke the Solaris Management Console, a Java-based graphical tool to manage some settings. There is a fix, but it‚Äôs hidden behind Oracle‚Äôs dreaded support contract paywalls, so I couldn‚Äôt do anything about it.&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure a later version of the Solaris 10 patchset ‚Äì they‚Äôre still being made twice a year, it seems ‚Äì addressed this issue, but none of those patchsets ‚Äòleaked‚Äô online. I did try to install the individual patches in the massive patchset one-by-one to avoid potentially problematic ones identified by their description, but it was a hell of a lot of work that felt never-ending, since you also have the dependency graph to work through and track. After a few hours of this nonsense, I gave up. I would love for Oracle to stop being needlessly protective over a bunch of patchsets for a dead operating system running on a dead architecture, but I don‚Äôt own a massive Hawaiian island so I guess I‚Äôm the idiot.&lt;/p&gt;
    &lt;p&gt;One of the things you‚Äôll definitely want to do after installing Solaris 10 is set up OpenCSW. OpenCSW is a package manager and associated repository of Solaris 10-native SVR4 packages for a whole bunch of popular and useful open source programs and tools, with dependency tracking, update support, and so on. It‚Äôs incredibly easy to set up, just as easy to use, and installs its packages in &lt;code&gt;/opt/csw&lt;/code&gt; by default, for neat separation. As useful as OpenCSW is, though, it‚Äôs important to note that most packages have not been updated in years, so it‚Äôs not exactly a production-ready environment. Still, it contains a ton of tools that make using Solaris 10 on SPARC in 2025 a hell of a lot easier, all installable and manageable through a few simple commands.&lt;/p&gt;
    &lt;p&gt;I have a few other random notes about using Solaris 10 on a workstation like this. First, and this one is obvious, be sure to create a user for your day-to-day use so you don‚Äôt have be logged in as the root user all the time. If you intend to use the Solaris Management Console, which offers a graphical way to manage certain aspects of your machine, you‚Äôll want to create the Primary Administrator role and assign it to your user account. This way, you can use the SMC even through your regular user account since it‚Äôll ask you to log into the primary administrator role.&lt;/p&gt;
    &lt;p&gt;Second, assuming you want to do some basic browsing and emailing, you‚Äôll also want to install the latest possible versions of Firefox and Thunderbird, namely version 52.0 of both. You can either opt for basic tarball installation, or use the SVR4 packages available from UNIX Packages to make installation a little bit easier. Version 52.0 of Firefox is severely outdated, of course, so be advised; tons of websites won‚Äôt work properly or at all, and security is obviously out the window. A newer version will most likely not be released since that would require an up-to-date Rust port and toolchain for Solaris 10/SPARC as well, which isn‚Äôt going to happen.&lt;/p&gt;
    &lt;p&gt;In addition, if you‚Äôve set up OpenCSW, you should consider adding &lt;code&gt;/opt/csw/bin&lt;/code&gt; to your PATH, so that anything installed through OpenCSW is more easily accessible. Furthermore, Solaris 10 installs both CDE and the Java Desktop System ‚Äì GNOME 2.6 with a fancy Sun theme ‚Äì and I highly suggest using the JDS since it was properly maintained at the time, while CDE had already stagnated for years at that point. It‚Äôll give you niceties like automatic mounting of USB sticks and DVDs/CDs, and make it much easier to access any possible network locations. Speaking of which ‚Äì you‚Äôll want to set up a SAMBA or NFS share so you can easily download files on a more modern machine, and subsequently make them accessible on your Solaris 10 machine. Both of these protocols are installed by default.&lt;/p&gt;
    &lt;p&gt;As a final note, there are three sources I use to find ancient software for these older UNIX systems (I use both Solaris 10 and HP-UX): fsck.technology, whatever this is, and the Internet Archive. You can find an absolutely massive pile of programs, software, operating system patches, and everything else in these three sources, including various ways to circumvent any copy protection schemes. I don‚Äôt care about the legality, and neither should you.&lt;/p&gt;
    &lt;p&gt;If you want to go for something more modern than Solaris 10, SPARC is still supported by a variety of operating systems, like NetBSD, OpenBSD, and a number of Linux distributions. Your best bet is to buy one of the lower-end GPUs, like the XVR-300 or XVR-600, as the XVR-2500 is not supported by the BSDs, but may work on Linux. I haven‚Äôt tried any of them yet ‚Äì this article is long enough as it is ‚Äì but I will definitely try them out in the future.&lt;/p&gt;
    &lt;p&gt;The future island owners among you may also be wondering about Illumos and its various derivatives and distributions, like OpenIndiana and personal OSNews darling Tribblix. While they all do support SPARC, it‚Äôs spotty at best, especially on workstations like the Ultra 45. SPARC servers have a better success rate, but the Ultra 45 specifically is unsupported at this point due to bugs preventing Illumos and friends from even booting. The good news, though, is that the people working on the SPARC variants have access to Ultra 45 machines, and work is being done to fix these issues.&lt;/p&gt;
    &lt;p&gt;Now, let‚Äôs move on to the two specialty upgrades I bought for this machine.&lt;/p&gt;
    &lt;head rend="h5"&gt;Accelerating ZFS&lt;/head&gt;
    &lt;p&gt;The transition from spinning hard disk drives to solid-state drives was an awkward time. Early on, SSDs were still prohibitively expensive, even at small sizes, but the performance benefits were obviously significant, and everyone knew which way the wind was blowing. During this awkward time, though, people had to choose between a mix of solid state and spinning drives, leading to products like hybrids drives, which combined a small SSDs with a large hard drive to get the best of both worlds. As prices kept coming down, people could opt for a small SSD for their operating system and most-used applications, storing everything else on spinning drives.&lt;/p&gt;
    &lt;p&gt;A hybrid drive doesn‚Äôt necessarily have to exist as a single, integrated product, though; depending on factors like operating system, controller, and file system, you could also assign SSDs as dedicated accelerators. This is where Oracle‚Äôs line of Flash Accelerator cards ‚Äì the F20, F40, and F80 ‚Äì come into play. These were released starting in roughly 2010, and consisted of several replaceable flash memory modules on a PCIe card. They were rebranded LSI Nytro Warpdrives with some custom firmware, which can actually be flashed back to their generic LSI firmware to turn them into their white label LSI counterparts.&lt;/p&gt;
    &lt;p&gt;Oracle‚Äôs Flash Accelerator cards are remarkably flexible, because their firmware presents the individual flash modules as individual block devices to the Solaris 10 operating system. This way, you can assign each individual module to perform specific tasks, which, combined with the power of Solaris 10‚Äôs ZFS, gives people who know what they are doing quite a few options to speed up specific workloads. In addition ‚Äì and this is pretty cool ‚Äì these accelerator cards can also serve as a boot device, meaning you can install and run Solaris 10 straight from the accelerator card itself.&lt;/p&gt;
    &lt;p&gt;These cards come in a variety of sizes, and they‚Äôre incredibly cheap these days. They‚Äôre not particularly useful or economical for modern applications, but they‚Äôre still fun relics from an older time. And because they‚Äôre so cheap and plentiful on the used market, they‚Äôre a great addition to a retro project like my Ultra 45 ‚Äì even if they‚Äôre technically intended for server use. I ordered a Flash Accelerator F20 on eBay for like ‚Ç¨20 including shipping, giving me 96GB, spread out over four 24GB flash modules, to play with.&lt;/p&gt;
    &lt;p&gt;The card has two stacks of two flash modules, which can be removed and replaced in case of failure, as well as a replaceable battery. Sadly, the one I ordered didn‚Äôt come with the full-height PCI bracket, but even without any bracket, the card sits incredibly firmly in its slot. The card also functions as a host bus adapter, giving you two additional SAS HBA ports for further storage expansion. Do note that you‚Äôll need to perform a reconfiguration boot of your SPARC system after installing the card, which is done by first dropping to the ok&amp;gt; prompt, and then executing &lt;code&gt;boot -r&lt;/code&gt;. Once rebooted, the &lt;code&gt;format&lt;/code&gt; command should display the four flash modules.&lt;/p&gt;
    &lt;code&gt;# format
Searching for disks‚Ä¶done

AVAILABLE DISK SELECTIONS:
    0. c1t0d0 &amp;amp;lt;ATA-HITACHIHDS7225S-A94A cyl 65533 alt 2 hd 16 sec 465&amp;gt;
       /pci@1e,600000/pci@0/pci@9/pci@0/scsi@1/sd@0,0
    1. c3t0d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@0,0
    2. c3t1d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@1,0
    3. c3t2d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@2,0
    4. c3t3d0 &amp;amp;lt;ATA-MARVELLSD88SA02-D21Y cyl 23435 alt 2 hd 16 sec 128&amp;gt;
       /pci@1e,600000/pci@0/pci@8/LSILogic,sas@0/sd@3,0
Specify disk (enter its number):&lt;/code&gt;
    &lt;p&gt;Now it‚Äôs time to decide what you want to use them for. I‚Äôm not a system administrator and I have very little experience with ZFS, so I went for the crudest of options: I assigned each module as a ZFS cache device for the ZFS pool I have Solaris 10 installed onto, which is stupidly simple (the exact disk names can be identified using &lt;code&gt;format&lt;/code&gt;):&lt;/p&gt;
    &lt;quote&gt;# zpool add -f &amp;lt;pool name&amp;gt; cache &amp;lt;disk1&amp;gt; &amp;lt;disk2&amp;gt; &amp;lt;disk3&amp;gt; &amp;lt;disk4&amp;gt;&lt;/quote&gt;
    &lt;p&gt;To check the status of your pool and make sure the modules are now acting as cache devices:&lt;/p&gt;
    &lt;code&gt;# zpool status
pool: ultra45
state: ONLINE
scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    ultra45     ONLINE       0     0     0
      c1t0d0s0  ONLINE       0     0     0
    cache
      c3t0d0s0  ONLINE       0     0     0
      c3t1d0s0  ONLINE       0     0     0
      c3t2d0s0  ONLINE       0     0     0
      c3t3d0s0  ONLINE       0     0     0

errors: No known data errors&lt;/code&gt;
    &lt;p&gt;The theory here is that this should give the 7200 RPM SAS drive the ZFS pool in question is running on a nice performance boost. Now, this is mostly theory in my particular case, since I‚Äôm not using this machine for any heavy workloads in 2025, but perhaps if you were doing some heavy lifting back in 2010 on your Solaris 10 workstation, you might‚Äôve actually seen some benefit.&lt;/p&gt;
    &lt;p&gt;Of course, this is anything but an optimal setup to get the most out of this hardware, but I already had a fully configured Solaris 10 install on the spinning hard drive and didn‚Äôt feel like starting over. Like I said, I‚Äôm no system administrator or ZFS specialist, but even I can imagine several better setups than this. For instance, you could install Solaris 10 in a ZFS pool spanning two of the flash modules, while assigning the remaining two flash modules as log and cache devices for the spinning hard drives you keep your data files on. In fact, Oracle still has a ton of documentation online about creating exactly such setups, and it‚Äôs not particularly hard to do so.&lt;/p&gt;
    &lt;p&gt;This F20 card wasn‚Äôt part of my original planning, and the only reason I bought it is because it was so cheap. It‚Äôs a fun toy you could buy and use on a whole variety of older systems, as long as they have PCIe slots and compatibility with PCIe storage. The entire card is just a glorified HBA, after all, and many operating systems from the past 20 years or so can handle such cards and its flash storage just fine.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs move on to something more interesting ‚Äì something I‚Äôve been dying to use ever since I learned of their existence decades ago.&lt;/p&gt;
    &lt;head rend="h5"&gt;I put a computer in your computer so you can computer with other computers&lt;/head&gt;
    &lt;p&gt;Even in the ‚Äô90s, much of the computing world ‚Äì especially when it came to generic office and home use ‚Äì had already moved firmly to x86 and Windows. Sun knew full well that in order to entice more customers to even consider using SPARC-based workstations, they needed to be interoperable with the x86 Windows world, since those were the kinds of machines their SPARC workstations would have to interoperate with. So, from quite early on in the 1990s, they were working on solutions to this very problem.&lt;/p&gt;
    &lt;p&gt;Sun‚Äôs first solution was Wabi, a reimplementation of the Win16 API to allow a specific set of popular Win16 applications to run on non-x86 UNIX workstations. This product was licensed by other companies as well, with IBM, HP, and SCO all releasing their own versions, and eventually it was even ported to Linux by Caldera in 1996. Another solution Sun offered at the same time as Wabi was SunPC, a PC emulator based on technology used in SoftPC. SunPC was limited to at most 286 software, however, so if you wanted to emulate software that required a 386 or 486 ‚Äì like, say Windows 3.x or 95 ‚Äì you needed something more.&lt;/p&gt;
    &lt;p&gt;And it just so happens Sun offered something more: the SunPC Accelerator Card. This line of accelerator cards, for SBus-based SPARC workstations, contained a 486 processor (and one later model an AMD 5√ó86 processor) on an expansion card that the SunPC emulator could use to run x86 software that required a 386 or 486. With this card installed, SPARC users could run full Windows 3.x or Windows 95 on their workstations, albeit with a performance penalty as the SunPC Accelerator Card did not contain any memory; SunPC had to emulate the RAM.&lt;/p&gt;
    &lt;p&gt;With Sun‚Äôs SPARC workstations moving to more standard PCI-based expansion busses in the second half of the 1990s, Sun would evolve their SunPC line into the SunPCi (clever), and that‚Äôs when this product line really hit its stride. Instead of containing just an x86 processor, SunPCi cards also contained memory, a graphics chip, sound chip, networking, VGA ports, serial ports, parallel ports, and later USB and FireWire as well. A SunPCi card is genuinely an entire x86 PC on a PCI expansion card, and the operating system running on that x86 PC can be used either in a window inside Solaris, or by connecting a dedicated monitor, keyboard, mouse, speakers, and so on. Or both at the same time!&lt;/p&gt;
    &lt;p&gt;In the late 1990s and early 2000s, Sun would release a succession of ever faster models of SunPCi cards, culminating in the last and most powerful variant: the SunPCi IIIpro, released in 2005. This very card is one of the reasons I was so excited to get my hands on a machine that could do it justice, so I splurged on a new-in-box model offered on eBay. This absolute behemoth of a PCI-X card contains the following PC hardware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mobile AMD Athlon XP 2100+ at 1.60Ghz&lt;/item&gt;
      &lt;item&gt;Two DDR SODIMM slots for a maximum of 1GB of RAM&lt;/item&gt;
      &lt;item&gt;S3 Graphics ProSavage DDR&lt;/item&gt;
      &lt;item&gt;A sound chip of indeterminate origin&lt;/item&gt;
      &lt;item&gt;VIA Rhine II Fast Ethernet Adapter&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The base card contains a VGA port, a USB port, Ethernet port, and audio in and out. The number of ports can be expanded with two optional daughter cards, one of which adds two more USB ports as well as a FireWire port, while the other one adds a serial and parallel port. These two daughter boards each require an additional PCI slot, but only the USB/FireWire one actually makes use of a PCI-X connector. In other words, if you install the main card and its two daughter boards, you‚Äôll be using up three PCI slots, which is kind of insane.&lt;/p&gt;
    &lt;p&gt;By default, the card only comes with a single 256MB DDR SODIMM, which is a bit anemic for many of the operating systems it supports ‚Äì as such, I added an additional 512MB DDR SODIMM for a total of 768MB of RAM. Unlike the Ultra 45 itself, it seems the SunPCi IIIPro is not particularly picky about RAM, so you can most likely dig something up from your parts pile and have it work properly. The card has a few other expansion options too, like an IDE header so you can use a real hard disk instead of an emulated one, but that would require some hacking inside the Ultra 45 due to a lack of power options for IDE hard drives.&lt;/p&gt;
    &lt;p&gt;Once you have installed the card ‚Äì a fiddly process with the two daughterboards attached ‚Äì it‚Äôs time to boot the host machine back up and install the accompanying SunPCi software. The last version Sun shipped is SunPCi Software 3.2.2 in 2004, and in order to make it work on my Ultra 45 I had to perform some workarounds. Searching the web seems to indicate the problems I experienced are common, so I figured I‚Äôd collect the problems and workarounds here for posterity, so I can spare others the trouble.&lt;/p&gt;
    &lt;p&gt;What you‚Äôll need is the SunPCi Software 3.2.2, the latest version; you can find this in a variety of locations around the web, including in the software repositories I mentioned earlier in the article. The installation is fairly straightforward, but the post-install script might throw up an error about being unable to find a driver called &lt;code&gt;sunpcidrv.2100&lt;/code&gt;. The fix is simple, but the odds of finding this out on your own are slim. Once the installation is completed, run the following commands as root to symlink to the correct drivers:&lt;/p&gt;
    &lt;code&gt;cd /opt/SUNWspci3/drivers/solaris/
ln -s sunpcidrv.280 sunpcidrv.2100
ln -s sunpcidrv.280.64 sunpcidrv.2100.64&lt;/code&gt;
    &lt;p&gt;The second problem you‚Äôll most likely run into is absolutely hilarious. If you try and start the SunPCi software with &lt;code&gt;/opt/SUNWspci3/bin/sunpci&lt;/code&gt;, you‚Äôll be treated to this gem:&lt;/p&gt;
    &lt;code&gt;Your System Time appears to be set in the future
I can't believe it's really Tue Nov 11 18:01:23 2025
Please set the system time correctly&lt;/code&gt;
    &lt;p&gt;This error message comes from a bug in the 3.2.2 release that was fixed in patch 118591-04, so you‚Äôll have to download that patch (118591-04.zip) from any of the countless repositories that hold it (here, here, here, etc.) and install it according to the instructions to remove this time bomb. I‚Äôm glad people have been willing to share this patch in a variety of places, because if this one remained locked behind donations to the Larry Ellison Needs More Island Fund I‚Äôd be pretty upset.&lt;/p&gt;
    &lt;p&gt;Once installed, the SunPCi software should start just fine, greeting you with a dialog where you can configure your emulated hard drive and select the operating system you wish to install. Provided you have the correct operating system installation disc, the operating system you select will automatically be installed onto the emulated hard drive. You‚Äôll also see proof that yes, this card is really just a regular, run-of-the-mill PC: it boots up like one, it has a BIOS like any other PC, you can enter this BIOS, and you can mess around with it. It‚Äôs really just a PC.&lt;/p&gt;
    &lt;p&gt;Sun put a lot of effort into making the operating system installation process as seamless and straightforward as possible; in the case of Windows XP, for instance, the SunPCi software will copy the contents of your Windows XP disc to a temporary location, and slipstream all the necessary drivers and some other software (specifically Java Web Start, of course) into the Windows XP installation process. In other words, once the installation is completed and you end up at the Windows XP desktop, all proper device drivers have been installed and you‚Äôre ready to start using it.&lt;/p&gt;
    &lt;p&gt;The amount of effort and thought Sun put into this product shines through in other really nice touches as well. For instance, inserting a CD or DVD into the Ultra 45‚Äôs drive will not only automatically mount it in Solaris, but also inside Windows XP ‚Äì autorun and all. Making folders on the host‚Äôs file system available inside Windows XP is also an absolute breeze, as you can mount any folder on the host system inside Windows XP using Explorer‚Äôs Map Network Drive feature: &lt;code&gt;\\localhost\home\thomholwerda&lt;/code&gt;, for instance, will make &lt;code&gt;/home/thomholwerda&lt;/code&gt; available in Windows. You can also copy and paste text between host and client, and SunPCi offers the option to grow the virtual hard drive you‚Äôre using in case you need more space.&lt;/p&gt;
    &lt;p&gt;The installation procedure installs two different video drivers in Windows XP: one for the S3 Graphics ProSavageDDR, and one for the SunPCi Video. The former drives any external display connected to the SunPCi card, while the latter outputs to a window inside Solaris. If you need to do any graphics or video-related work, Sun strongly suggests you use the S3 chip by using an external display, and it‚Äôs obvious why. The performance of the SunPCi Video is so-so, and definitely feels like it‚Äôs rendering in software (which it is), so you can expect some UI stutters here and there. A nice touch is that there‚Äôs no need for the SunPCi window to ‚Äúcapture‚Äù the mouse pointer manually, as you can freely move your Solaris cursor in and out of the SunPCi window.&lt;/p&gt;
    &lt;p&gt;As for the performance of Windows XP ‚Äì it will align more or less with what you can expect from a mobile Athlon from 2002, so don‚Äôt expect miracles. It‚Äôs entirely usable for office and related tasks, but you won‚Äôt be doing any hardcore gaming or complex, demanding professional work. The goal of this card is not to replace a dedicated x86 workstation, but to give Solaris/SPARC users access to the various office-related applications most organisations were using at the time, like Microsoft Office, IBM‚Äôs Domino, and so on, and it achieves that goal admirably.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a ton of other things you can do with this card that I simply haven‚Äôt had the time yet to dive into (this article is already way too long), but that I‚Äôd like to come back to in the future. For instance, the list of officially supported operating systems includes not just Windows XP, but also Windows 2000, Server 2003, and a variety of versions of Red Hat (Enterprise) Linux (think Linux version ~2.4.20). The SunPCi software also contains an entire copy of DR-DOS 7.01, which is neat, I guess. Lastly, the user manual for the SunPCi software lists a whole lot of advanced features and tweaks you can play with, too.&lt;/p&gt;
    &lt;p&gt;I would also be remiss to note that you can actually use multiple SunPCi cards in a single machine, as that‚Äôs a fully supported configuration. You can totally get a big SPARC server, put multiple SunPCi cards in it, and let users log in remotely to use them, perhaps using Sun‚Äôs true thin client offering, Sun Rays. This is foreshadowing.&lt;/p&gt;
    &lt;p&gt;As for other operating systems ‚Äì I‚Äôve seen rumblings online that versions of NetBSD and Debian from the early 2000s were made to work on the SunPCi II (the previous model to what I have), but I can‚Äôt find any information on anything else that might work. The issue is that any operating system running on the card needs drivers for the emulated hard disk, which are obviously not available as those were made by Sun. Since the SunPCi IIIpro has an actual IDE connector, though, I‚Äôve been wondering if it would at least be possible to boot and run an ‚Äúunsupported‚Äù operating system using the external method (dedicated display, mouse, keyboard, etc.). If there‚Äôs interest, I can dive into this in the future and report back.&lt;/p&gt;
    &lt;p&gt;All in all, though, the SunPCi IIIPro is a much more thoughtful and pleasant product to use than I originally anticipated. Sun clearly put a lot of thought into making this card and its features as easy to use as possible, and I can totally see how it would make it palatable to use a SPARC workstation in an otherwise Windows-based corporate environment. Just load up Outlook or whatever Windows-based groupware thing your company used using the SunPCi software, and use it like any other application in your Solaris 10 SPARC environment. Since you can set the SunPCi software to load at boot, you‚Äôd probably mostly forget it was running on a dedicated PC on an expansion card, and your colleagues would be none the wiser.&lt;/p&gt;
    &lt;p&gt;To round out the Sun Microsystems ecosystem of the late 2000s, we really can‚Äôt get around explaining why the network is the computer. It‚Äôs time to talk Sun Ray.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sun Rays: the spokes&lt;/head&gt;
    &lt;p&gt;During most of its existence, Sun‚Äôs slogan was the iconic ‚ÄúThe network is the computer‚Äú, coined in the early 1980s by John Gage, one of the earliest employees at Sun. Today, the idea behind this slogan ‚Äì namely, that a computer without a network isn‚Äôt really a computer ‚Äì is so universally true it‚Äôs difficult to register just how forward-thinking this slogan was back in 1984. These days, everything with even a gram of computer power is networked, for better or worse, and the vast majority of people will consider any PC, laptop, smartphone, or tablet without a network connection to be effectively useless.&lt;/p&gt;
    &lt;p&gt;Gage was right, decades before the world realised it.&lt;/p&gt;
    &lt;p&gt;The product category that embodies Sun‚Äôs iconic slogan more than anything is the thin client, and Sun played a big role in this market segment with their line of Sun Ray products. The Sun Ray product line consisted of a variety of products, but the main two components were the Sun Ray Server Software and the various Sun Ray thin clients Sun (and Oracle) produced between 1999 and 2014. The server component would run on a server (or workstation, as we‚Äôll see in a moment), and the Sun Ray client devices would connect to said server over the network.&lt;/p&gt;
    &lt;p&gt;The idea was that you had a giant server somewhere in your building, running the Sun Ray Server Software, accompanied by whatever number of Sun Ray thin clients you needed on employees‚Äô desks. Each of your employees would have a user account on the server, and could log into that user account using any of the Sun Ray thin clients in the building. The special ingredient was the fact that Sun Rays were stateless, which meant that the thin clients themselves stored zero information about the user‚Äôs session; everything was running on the server.&lt;/p&gt;
    &lt;p&gt;This special ingredient made some real magic possible, most notably hotdesking, which, admittedly, sounds like something LinkedIn professionals do on OnlyFans, but is actually way cooler. You could roam from one Sun Ray to the next, and your desktop, including all the applications you were running and documents you had opened, would travel with you ‚Äì because they were running on the server. Sun also bet big on smartcards, so instead of logging in with a traditional username and password, you could also log in simply by sliding your smartcard into the card reader integrated into every single Sun Ray. Take your smartcard out, and your session would disappear from the display, ready to continue where you left off on any other Sun Ray.&lt;/p&gt;
    &lt;p&gt;And yes, you could make this work across the internet as well.&lt;/p&gt;
    &lt;p&gt;Reading all of this, you may assume Sun Rays and hotdesking involved a considerable amount of jank, but nothing could be further from the truth. I‚Äôve been fascinated by thin clients in general, and Sun Rays in particular, for decades, but I never had the hardware to properly set up a Sun Ray environment at home ‚Äì until now, of course. With the Ultra 45 all set up and running, and the generous Sun Ray-related donations from Adriaan and Volker, I had everything I needed to set up some Sun Rays. If the Ultra 45 is the central hub, Sun Rays are its spokes.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs hotdesk like it‚Äôs 2007.&lt;/p&gt;
    &lt;p&gt;I expected setting up a working Sun Ray environment would be a difficult endeavour, but nothing could be farther from the truth. It turns out that installing, setting up, and configuring the Sun Ray Server Software is incredibly easy, and Nico Maas made it even easier back in 2009 by condensing the instructions down to the bare essentials (Archive.org link just in case). After following Maas‚Äô list of steps (you can skip the personal notes section at the end if you‚Äôre not using a dedicated network card for the Sun Ray Server Software), any Sun Ray you connect to your network and turn on will automatically find the Sun Ray Server, perform any possible firmware updates, and show a login screen.&lt;/p&gt;
    &lt;p&gt;From here, you can log into any user account on the Sun Ray Server (the Ultra 45, in my case) as if you‚Äôre sitting right behind it. Depending on which generation of Sun Ray you‚Äôre using, loading your desktop will either be fast, faster, or near instantaneous. Thanks to Sun‚Äôs network display protocol, the Application Link Protocol, performance is stunningly good. Even on the very oldest Sun Ray 1 device I have, it feels genuinely like you‚Äôre using the machine you‚Äôre remotely logged into locally.&lt;/p&gt;
    &lt;p&gt;As part of Maas‚Äô instructions, you also installed Apache Tomcat, included in the Sun Ray Server Software‚Äôs zip file, which is a necessary component for the graphical configuration and administration utility. Since we‚Äôre talking 2000s Sun, this administration GUI is, of course, a web application written in Java, accessible through your browser (I suggest using the copy of Netscape included in Solaris 10) by browsing to your server‚Äôs IP address at port 1660. After logging in with your credentials, you‚Äôll discover a surprisingly nice, capable, and detailed set of configuration and administration panels, which mirror many of the CLI configuration tools. Depending on your preference, you may opt to use the CLI tools instead, but persoally, I‚Äôm an absolute sucker for 2000s enterprise GUIs.&lt;/p&gt;
    &lt;p&gt;While there‚Äôs a ton of configuration options to play around with here, the ones we‚Äôre looking for have to do with setting up smartcards so we no longer have to use bourgeois banalities like usernames and passwords. To enable logging in with a smartcard, you‚Äôll obviously need a smartcard ‚Äì I have a Sun-branded one, which are objectively the coolest ‚Äì but there are other options. You‚Äôll then need to read the token on said smartcard, and associate that token with your user account. As a final step, you need to utterly wreck the security of your setup by enabling passwordless smartcard login.&lt;/p&gt;
    &lt;p&gt;This process is fairly straightforward, but there a few arbitrary details you need to be aware of. First, you need to designate a Sun Ray as a card reader by going to Desktop Units, and selecting the Sun Ray unit you‚Äôd like to use as a card reader by clicking Edit and under Advanced, select ‚ÄúDesktop unit is used as token reader‚Äù ‚Äì click OK and perform the cold restart of the Sun Ray Server Software as instructed. Once the restart is completed, make sure the smartcard you wish to use is inserted, then go to the Tokens tab, click on New‚Ä¶, and you‚Äôll see that the token is already read and selected. Enter your username next to ‚ÄúOwner:‚Äù, and save. Make sure to undo the ‚ÄúDesktop unit is used as token reader‚Äù setting, and you‚Äôre good to go.&lt;/p&gt;
    &lt;p&gt;If you wish to log in entirely passwordless ‚Äì as in, you just need to insert the smartcard, no typed credentials required ‚Äì you need to go to Advanced &amp;gt; System Policy, scroll down to ‚ÄúSession Access when Hotdesking‚Äù, and tick the box next to ‚ÄúDirect Session Access Allowed‚Äù. It should go without warning that this is quite insecure, as someone would just need to yoink your smartcard to break into your account. For whatever that‚Äôs worth, on a retro environment in your own home.&lt;/p&gt;
    &lt;p&gt;Now you can properly hotdesk. Insert your smartcard into any connected Sun Ray, and your desktop will automatically appear, running applications and all. Take the card out, and the Sun Ray login screen will reappear. Wherever you insert your smartcard, your desktop will show up. This way, your session will travel with you no matter where you are ‚Äì as long as there‚Äôs a Sun Ray to log into, you can continue working, even across the internet if that functionality has been enabled. Nothing about this is particularly complex technology-wise, but it absolutely feels like magic.&lt;/p&gt;
    &lt;head rend="h5"&gt;The clients&lt;/head&gt;
    &lt;p&gt;Let‚Äôs dive a little deeper into the Sun Ray clients. Sun (and later Oracle) produced a wide variety of them over the years, but roughly they can be divided up unto three generations. Disregarding the extremely rare early prototypes, the Sun Ray 1 is probably the most iconic model, as far as its design goes. It also happens to be the only model powered by a SPARC chip, the 100MHz microSPARC IIep accompanied by ATI Radeon 7000 graphics. The second generation switched from SPARC to MIPS, a 500MHz RMI Alchemy Au1550 (built by AMD) accompanied by ATI ES1000 graphics. The third generation of Sun Rays moved to either a 667MHz RMI Alchemy Au1380 for the base model, or the MIPS 750MHz RMI XLS104 for the Plus model, both with graphics integrated into the processor.&lt;/p&gt;
    &lt;p&gt;None of these core specifications really matter though, as the performance will mostly be identical. What really matters is the port selection, and the display resolution the Sun Ray unit is capable of outputting. I would strongly suggest opting for models with DVI output capable of handling at least 1920√ó1080, since full HD panels are easy to come by and you probably have a few lying around anyway. All models of Sun Ray have USB, audio, and Ethernet ports, so you‚Äôre good there, but the Sun Ray 2FS and Sun Ray 3 Plus also have fibre optic Ethernet options. You know, just in case you really want to go nuts.&lt;/p&gt;
    &lt;p&gt;Your eyes are not deceiving you. That‚Äôs a KDE-branded Sun Ray 2FS, and according to Adriaan, there‚Äôs only two of these in existence.&lt;/p&gt;
    &lt;p&gt;Sun also produced various Sun Ray models with integrated displays for that all-in-one experience, including one with a CRT. Beyond Sun, various third parties also made Sun Ray-compatible devices, offering form factors Sun didn‚Äôt explore, like laptops and even tablets. Sadly, these third party models seem to be exceedingly rare, and I‚Äôve never seen one come up for sale anywhere. I would personally haul 16 tons and owe my soul to the King of Lanai‚Äôs company store to get my hands on a laptop and tablet model.&lt;/p&gt;
    &lt;p&gt;But what if you don‚Äôt want to deal with the hassle of real hardware? Thin clients or no, these things still take up space and require a ton of cabling and peripherals, which can be a hassle (I have three Sun Rays and their peripherals hooked up‚Ä¶ On the floor of my office). Fret not, as Sun and later Oracle also released virtual Sun Ray client software, allowing you to log into the Sun Ray Server form any regular PC. Called the Sun Desktop Access Client or the Oracle Virtual Desktop Client, it‚Äôs a simple Java-based application for Linux, Solaris, Windows, and Mac OS, available in 32 and 64 bit variants. Alongside the entire Sun Ray lineup, this piece of software was retired in 2017, but it‚Äôs still freely available on Oracle‚Äôs website, given you manage to navigate Oracle‚Äôs byzantine account signup, login, and download process.&lt;/p&gt;
    &lt;p&gt;I only tested the version available for Linux, and to my utter surprise, it still works just fine! Since I run Fedora, I downloaded the 64bit RPM, and installed it with &lt;code&gt;rpm -ivh --nodigest ovdc-3.2.0-1.x86_64.rpm&lt;/code&gt;. You‚Äôre going to need two dependencies, available from Fedora‚Äôs own repositories through the following packages: &lt;code&gt;libgnome-keyring&lt;/code&gt; and &lt;code&gt;libsnl&lt;/code&gt;. Once installed, the Oracle Virtual Desktop Client will appear in your desktop environment‚Äôs application menu, or you can start it with &lt;code&gt;ovdc&lt;/code&gt; from the terminal. Before you start the application, though, you‚Äôre going to need to enable the option ‚ÄúSun Desktop Access Client‚Äù in the Sun Ray Server Software web admin under Advanced &amp;gt; System Policy, and perform a warm restart.&lt;/p&gt;
    &lt;p&gt;You‚Äôll be greeted by an outdated Java application, so on my 4K panel the user interface looks positively tiny, but otherwise, it works entirely as expected. Enter the IP address of the machine you‚Äôre running the Sun Ray Server Software on, and the login screen will appear as if you‚Äôre using a hardware Sun Ray client. I find it quite neat that this ancient piece of software ‚Äì last updated in 2014, its RPM last updated in 2017 ‚Äì still works just fine on my Fedora 43 machines. There were also Android and iOS variants of the Oracle Virtual Desktop Client, but they‚Äôre no longer available in their respective application stores, and the Android APK I downloaded refused to install on my modern Android device.&lt;/p&gt;
    &lt;p&gt;The Sun Ray ecosystem is, even in 2025, a versatile and almost magical experience. It‚Äôs incredibly easy to set up and use, but of course, effectively useless in that Solaris 10 and its GNOME 2.6-based desktop Sun Rays provide remote access to are outdated and lack the modern applications we need. Still, this entire exercise has given me an immense appreciated for what Sun‚Äôs engineers built back in the late ‚Äô90s and early 2000s, and I wish the Sun Ray ecosystem didn‚Äôt die out at Oracle alongside everything else island boy got his hands on.&lt;/p&gt;
    &lt;p&gt;But did it die out, though? Recently, I reported on the latest OpenIndiana snapshot, and wrote this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A particularly interesting bullet point is maintenance work and improvements for Sun Ray support, and the changelog notes that these little thin clients are still popular among their users. I‚Äôm very deep into the world of Sun Rays at the moment, so reading that you can still use them through OpenIndiana is amazingly cool. There‚Äôs a Sun Ray metapackage that installs the necessary base components, allowing you to install Sun‚Äôs/Oracle‚Äôs original Sun Ray Server software on OpenIndiana. Even though MATE is the default desktop for OpenIndiana, the Sun Ray Server software does depend on a few GNOME components, so those will be pulled in.&lt;/p&gt;‚Ü´ Thom Holwerda at OSNews&lt;/quote&gt;
    &lt;p&gt;Now that you‚Äôre reading this article, it means the hold this project has had over my life has lessened somewhat, hopefully giving me some time to dive into OpenIndiana further. I‚Äôve had issues getting it to boot and work properly on any of my devices, but knowing it‚Äôs still entirely compatible with Sun Ray means I might build a machine specifically for it. The sun must shine.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Could Sun‚Äôs ecosystem have made for an excellent computing environment at home? I‚Äôm realistic and realise full well that nobody was going to buy an expensive Ultra 45 or otherwise set up a SPARC server with a SunPCi card and Sun Rays just for home use. These were enterprise products with enterprise prices, after all. Still, I think the basic idea of a powerful central computer in the home ‚Äì perhaps a server in the utility closet ‚Äì accompanied by a number of cheap thin clients is sound. Most of our computers are sitting idle most of their lifetime, and there‚Äôs really no need for every member of a household having access to their personal overpowered desktop and/or laptop.&lt;/p&gt;
    &lt;p&gt;Twenty years ago, Sun‚Äôs ecosystem showed us that such a setup need not be complex, janky, or cumbersome, and with a bit more end-user focused polish it would‚Äôve made for an amazing home computing environment. Of course, there‚Äôs far more profit to be made in selling multiple overpowered computing devices to each consumer, especially if you can also manage to force them into subscription software and ‚Äúcloud‚Äù services, while showing them ads every step of the way.&lt;/p&gt;
    &lt;p&gt;I‚Äôve only scratched the surface of everything you could possibly do with the hardware and software covered in this article, as I didn‚Äôt want to get too bogged down in the weeds. There‚Äôs other operating systems to try on the Ultra 45, there‚Äôs compatibility to explore with the SunPCi, there‚Äôs OpenIndiana to install to see just how hard it is to get the Sun Rays working with a modern operating system, and, of course, there‚Äôs a ton of finer details to tweak, fiddle with, and discover. I haven‚Äôt had this much fun with a bunch of computing devices in ages.&lt;/p&gt;
    &lt;p&gt;This deep dive into Sun‚Äôs ecosystem has consumed most of my life over the past few months. I know full well writing 10,000 word articles on dead Sun tech from the 2000s is not a particularly profitable use of my time. This isn‚Äôt going to draw scores of new readers to the OSNews Patreon and Ko-Fi and make me rich. For profit, I should be making YouTube videos with fast transitions and annoying sound effects to stir up drama based on GitHub discussions or LKML posts with my O face in the thumbnails.&lt;/p&gt;
    &lt;p&gt;But someone needs to show the world what we lost when Sun died and the industry enshittified. Tech has hit rock bottom, and I want to show everyone that yes, we can build something better.&lt;/p&gt;
    &lt;p&gt;We only have to look back at what we lost, instead of forward at what‚Äôs left to destroy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45955077</guid><pubDate>Mon, 17 Nov 2025 16:24:34 +0000</pubDate></item><item><title>Israeli-founded app preloaded on Samsung phones is attracting controversy</title><link>https://www.sammobile.com/news/israeli-app-app-cloud-samsung-phones-controversy/</link><description>&lt;doc fingerprint="1d0acc58f9b554c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Early Black Friday deals, Galaxy S25 FE, Fold 7, S25 Ultra. Follow us on YouTube, TikTok, or Instagram&lt;/p&gt;
    &lt;p&gt;Last updated: November 17th, 2025 at 07:11 UTC+01:00&lt;/p&gt;
    &lt;p&gt;SamMobile has affiliate and sponsored partnerships, we may earn a commission.&lt;/p&gt;
    &lt;p&gt;App's ties to Israel are the main cause of concern, but not the only one.&lt;/p&gt;
    &lt;p&gt;Reading time: 3 minutes&lt;/p&gt;
    &lt;p&gt;For years, Samsung has shipped its Galaxy M, F, and A series smartphones in India with a little-known app called AppCloud. Despite its name, AppCloud isn‚Äôt a cloud storage service. It‚Äôs essentially an app-installer that surfaces third-party app recommendations during device setup.&lt;/p&gt;
    &lt;p&gt;On new Galaxy devices in these lineups, AppCloud appears as part of the initial onboarding and forces users to choose whether they want to install certain apps before setup can be completed. You can postpone this by choosing the ‚Äúlater‚Äù option, but the app continues to push a persistent notification until you finish the selection process or disable it entirely.&lt;/p&gt;
    &lt;p&gt;For most users, AppCloud has long been regarded as little more than nuisance bloatware, a side effect of Samsung‚Äôs need to generate revenue beyond hardware margins while competing with aggressive Chinese smartphone brands in India.&lt;/p&gt;
    &lt;p&gt;But findings by the non-profit SMEX from earlier this year suggest AppCloud may not be as harmless as once assumed.&lt;/p&gt;
    &lt;p&gt;Since 2022, Samsung has also been preloading AppCloud on its A and M series phones in several West Asian and North African (WANA) markets. This rollout has triggered privacy concerns due to AppCloud‚Äôs ties to ironSource, a company founded in Israel and now owned by US-based Unity.&lt;/p&gt;
    &lt;p&gt;While AppCloud can be disabled, it is difficult to remove without root access. Furthermore, its privacy policy is not easily available online, raising questions about transparency, user consent, and what kind of data the app may collect.&lt;/p&gt;
    &lt;p&gt;ironSource itself has a controversial track record. The company previously operated an ‚ÄúInstallCore‚Äù program that became infamous for installing software without clear user permission and for bypassing security warnings, behavior that resulted in widespread criticism and blacklisting by several anti-malware tools.&lt;/p&gt;
    &lt;p&gt;The presence of an Israeli-origin technology component on Samsung phones in WANA countries poses additional problems. Several nations in this region legally bar Israeli companies from operating, and in light of the ongoing Israel‚ÄìPalestine conflict, the preload of an app tied to such a company becomes even more contentious.&lt;/p&gt;
    &lt;p&gt;ironSource‚Äôs Aura technology, which ‚Äúoptimizes device experiences‚Äù by surfacing apps, content, and services directly on smartphones, has been used on Samsung devices in Europe, Russia, and Southeast Asia, and by telecom operators in the US; it also appears to do something similar to AppCloud. However, AppCloud itself is not listed anywhere on ironSource‚Äôs website, which appears to be the major cause for concern, even though the app is now owned by a US company.&lt;/p&gt;
    &lt;p&gt;While there‚Äôs no concrete evidence that AppCloud engages in questionable data practices today, the lack of an accessible privacy policy and ironSource's past reputation are causing anxiety among users.&lt;/p&gt;
    &lt;p&gt;Consumer advocates and privacy-focused users are urging Samsung to take immediate steps, like providing a clear opt-out for AppCloud during setup, making its privacy policy public and accessible, and to stop preloading the app entirely in sensitive regions.&lt;/p&gt;
    &lt;p&gt;With concerns rising across multiple markets, Samsung will likely need to issue a statement to reassure customers. We have reached out to the company for comment and will update this story once we hear back.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45955424</guid><pubDate>Mon, 17 Nov 2025 16:54:53 +0000</pubDate></item></channel></rss>