<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 02 Feb 2026 15:58:00 +0000</lastBuildDate><item><title>Building Your Own Efficient uint128 in C++</title><link>https://solidean.com/blog/2026/building-your-own-u128/</link><description>&lt;doc fingerprint="172010af286e8a4b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building Your Own Efficient uint128 in C++&lt;/head&gt;
    &lt;p&gt;A practical walk-through of a fixed-width uint128 implementation in modern C++.&lt;/p&gt;
    &lt;p&gt;We build a minimal &lt;code&gt;u128&lt;/code&gt; as two &lt;code&gt;u64&lt;/code&gt; limbs and implement arithmetic using carry, borrow, and multiply intrinsics that map directly to x64 instructions.
The generated code is on par with builtin &lt;code&gt;__uint128_t&lt;/code&gt; for addition, subtraction, multiplication, and comparison.
This is unsigned-only, x64-focused, and intentionally narrow in scope.
The result is a solid foundation for exact, fixed-width arithmetic with a focus on good codegen and predictability, not abstraction.&lt;/p&gt;
    &lt;p&gt;Full code and compiler output: https://godbolt.org/z/K6dn3s91Y&lt;/p&gt;
    &lt;head rend="h2"&gt;Scope&lt;/head&gt;
    &lt;p&gt;We take the smallest reasonable definition of a 128-bit integer, two 64-bit words, and turn it into a usable arithmetic type whose generated code is indistinguishable from a builtin &lt;code&gt;__uint128_t&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This post is explicitly not about dynamically-sized big integer arithmetic. It is about being explicit with range bounds and letting the compiler emit the exact instructions we want. The scope is deliberately limited: unsigned arithmetic, fixed width, modern x64, with Clang and GCC as the primary targets and notes for MSVC where it differs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why fixed-width big integers&lt;/head&gt;
    &lt;p&gt;In many domains, especially geometry and numerics, we do not need arbitrary precision. We need enough precision to be exact for known bounds, and we need the cost to be predictable.&lt;/p&gt;
    &lt;p&gt;Dynamic big integer libraries solve a different problem. They are flexible and general, but they pay for that generality in memory traffic, branches, and indirection. If your values fit into a fixed number of bits and you know that ahead of time, fixed-width arithmetic is usually the better trade. (In fact, our high-performance exact mesh booleans are completely built on this: Exact Arithmetic in Solidean)&lt;/p&gt;
    &lt;p&gt;A 128-bit integer is the gateway drug to fixed-width arithmetic. It is the smallest width that is no longer builtin, while still mapping cleanly to the underlying hardware. Once the carry and multiply patterns are explicit at 128 bits, extending them to 192 or 256 bits is straightforward. In production, we use 256-bit integers in our hot paths and go up to 564 bits for certain edge cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;Representation&lt;/head&gt;
    &lt;p&gt;We represent a 128-bit unsigned integer as two 64-bit limbs. You can literally think of this as writing the &lt;code&gt;u128&lt;/code&gt; as a 2-digit number in base \(2^{64}\).
Because it's unsigned, we don't need to think about two's complement.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;cstdint&amp;gt;
#include &amp;lt;immintrin.h&amp;gt; // _addcarry_u64, _subborrow_u64, _mulx_u64

// this is the type used in the intrinsics signature
// (and uint64_t is unsigned long, not unsigned long long...)
using u64 = unsigned long long;

struct u128
{
    u64 low = 0;
    u64 high = 0;
};
&lt;/code&gt;
    &lt;head rend="h2"&gt;Addition, with carry&lt;/head&gt;
    &lt;p&gt;We start off easy. Addition is simply done using long addition on our base \(2^{64}\) digits. The intrinsic &lt;code&gt;_addcarry_u64&lt;/code&gt; corresponds to the x64 instruction &lt;code&gt;adc&lt;/code&gt; and is exactly what we need:
Given two &lt;code&gt;u64&lt;/code&gt; summands and an input carry (0 or 1), we get the &lt;code&gt;u64&lt;/code&gt; result (via a slightly cumbersome output parameter) and a new carry.&lt;/p&gt;
    &lt;code&gt;u128 operator+(u128 a, u128 b) 
{
    u128 r;
    unsigned char c = _addcarry_u64(0, a.low,  b.low,  &amp;amp;r.low);
    (void)_addcarry_u64(c, a.high, b.high, &amp;amp;r.high);
    return r;
}
&lt;/code&gt;
    &lt;p&gt;The generated assembly is exactly what you would write by hand.&lt;/p&gt;
    &lt;code&gt;operator+(u128, u128):
        mov     rax, rdi
        add     rax, rdx
        adc     rsi, rcx
        mov     rdx, rsi
        ret
&lt;/code&gt;
    &lt;p&gt;The moves are just calling convention noise. The core is an &lt;code&gt;add&lt;/code&gt; (because the first addition has no input carry) followed by an &lt;code&gt;adc&lt;/code&gt;.
This is identical to what the compiler emits for &lt;code&gt;__uint128_t&lt;/code&gt; addition.&lt;/p&gt;
    &lt;p&gt;A small but important point is that intrinsics are preferable to inline assembly here. They act like specialized IR operations. The compiler understands their semantics, can schedule around them, and can still optimize aggressively. Inline assembly is more of a black box and much easier to get wrong or inhibit optimizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Subtraction: same story, inverted&lt;/head&gt;
    &lt;p&gt;Subtraction mirrors addition almost perfectly. Instead of a carry, we track a borrow. On x64, this is &lt;code&gt;sbb&lt;/code&gt;, subtract with borrow.
The corresponding intrinsic is &lt;code&gt;_subborrow_u64&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;u128 operator-(u128 a, u128 b) 
{
    u128 r;
    unsigned char c = _subborrow_u64(0, a.low,  b.low,  &amp;amp;r.low);
    (void)_subborrow_u64(c, a.high, b.high, &amp;amp;r.high);
    return r;
}
&lt;/code&gt;
    &lt;p&gt;And again, the assembly is exactly what we want.&lt;/p&gt;
    &lt;code&gt;operator-(u128, u128):
        mov     rax, rdi
        sub     rax, rdx
        sbb     rsi, rcx
        mov     rdx, rsi
        ret
&lt;/code&gt;
    &lt;p&gt;At this point, addition and subtraction are basically solved. There is no hidden cost and no abstraction penalty. It's also easy to see how this scales to larger integer types with one extra instruction per &lt;code&gt;u64&lt;/code&gt; "digit".&lt;/p&gt;
    &lt;head rend="h2"&gt;Multiplication: regrouping our &lt;code&gt;u64&lt;/code&gt; digits&lt;/head&gt;
    &lt;p&gt;Multiplication is where things get more interesting. A 128-bit by 128-bit multiply produces a 256-bit result but we want only the lower 128 bit for our result. Same story with &lt;code&gt;u64 * u64&lt;/code&gt; really, which produces a 128 bit result in theory, but you usually only use the lower &lt;code&gt;u64&lt;/code&gt;.
Speaking of which, all modern 64-bit architectures give you access to fast &lt;code&gt;u64 * u64 -&amp;gt; u128&lt;/code&gt; instructions.
With BMI2, this is exposed as &lt;code&gt;_mulx_u64&lt;/code&gt;.
On MSVC, the equivalent is &lt;code&gt;_umul128&lt;/code&gt;.
This is our building block for large multiplication.&lt;/p&gt;
    &lt;p&gt;You can derive the code from writing the &lt;code&gt;u128 * u128&lt;/code&gt; as 2-digit long multiplication &lt;code&gt;(u64, u64) * (u64, u64)&lt;/code&gt; and then look sharply at what sums up to which digit.&lt;/p&gt;
    &lt;p&gt;That's how I do it on paper, but here we can also choose an algebraic route. Write the numbers as: $$ (a.\text{low} + 2^{64} \cdot a.\text{high}) \cdot (b.\text{low} + 2^{64} \cdot b.\text{high}) $$ Expanding this gives four terms. Of those, only three contribute to the low 128 bits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;\(a_\text{low} \cdot b_\text{low}\) contributes both low and high parts&lt;/item&gt;
      &lt;item&gt;\(a_\text{low} \cdot b_\text{high}\) contributes to bits 64..127&lt;/item&gt;
      &lt;item&gt;\(a_\text{high} \cdot b_\text{low}\) contributes to bits 64..127&lt;/item&gt;
      &lt;item&gt;\(a_\text{high} \cdot b_\text{high}\) contributes only above bit 128 and can be discarded&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This works for larger integers as well, though summing up intermediate terms can produce carries for "higher digits".&lt;/p&gt;
    &lt;code&gt;u128 operator*(u128 a, u128 b) 
{
    // we want the low 128 bits of:
    // (a.low + 2^64 a.high) * (b.low + 2^64 b.high)
    //
    // r.low  = lo64(a.low * b.low)
    // r.high = hi64(a.low * b.low)
    //        + lo64(a.low * b.high)
    //        + lo64(a.high * b.low)        (mod 2^64)

    // NOTE (MSVC): for multiply, you can use _umul128(a, b, &amp;amp;hi) instead of _mulx_u64.
    //              Clang/GCC: _mulx_u64 is BMI2 and needs -mbmi2.

    u128 r;

    u64 p0_hi;
    r.low = _mulx_u64(a.low, b.low, &amp;amp;p0_hi);

    // cross terms: only the low 64 bits contribute to r.high
    u64 t1_hi;
    u64 t1_lo = _mulx_u64(a.low,  b.high, &amp;amp;t1_hi);

    u64 t2_hi;
    u64 t2_lo = _mulx_u64(a.high, b.low,  &amp;amp;t2_hi);

    // simply add is sufficient: carries would land in bit 128 and are discarded
    r.high = p0_hi + t1_lo + t2_lo;

    return r;
}
&lt;/code&gt;
    &lt;p&gt;Note that we do not need carry handling there. Any carry out of bit 127 would land in bit 128, which we are discarding anyway.&lt;/p&gt;
    &lt;p&gt;The compiler output reflects this reasoning.&lt;/p&gt;
    &lt;code&gt;operator*(u128, u128):
        mulx    r8, rax, rdi
        imul    rcx, rdi
        imul    rdx, rsi
        add     rdx, rcx
        add     rdx, r8
        ret
&lt;/code&gt;
    &lt;p&gt;The compiler chooses slightly different instructions and registers for the builtin &lt;code&gt;__uint128_t&lt;/code&gt; multiplication but is otherwise identical as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Equality&lt;/head&gt;
    &lt;p&gt;Now an easy one. &lt;code&gt;u128&lt;/code&gt; equality is simple structural equality.&lt;/p&gt;
    &lt;code&gt;bool operator==(u128 a, u128 b) 
{
    return a.low == b.low &amp;amp;&amp;amp; a.high == b.high;
}
&lt;/code&gt;
    &lt;p&gt;The generated assembly is worth a quick look.&lt;/p&gt;
    &lt;code&gt;operator==(u128, u128):
        xor     rdi, rdx
        xor     rsi, rcx
        or      rsi, rdi
        sete    al
        ret
&lt;/code&gt;
    &lt;p&gt;Instead of branching (as you might expect from the short-circuiting of &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt;), the compiler XORs the corresponding limbs.
XOR produces zero if and only if the inputs are equal.
ORing the results combines the checks.
If the final value is zero, both limbs were equal.&lt;/p&gt;
    &lt;p&gt;This pattern continues for larger integers, though we might see branching due to short-circuiting at some point. (We could of course just use the XOR approach in &lt;code&gt;operator==&lt;/code&gt; but it is distinctly less readable.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Comparison: borrow beats branching&lt;/head&gt;
    &lt;p&gt;The straightforward way to compare two 128-bit integers is to compare the high parts first and then the low parts.&lt;/p&gt;
    &lt;code&gt;bool operator&amp;lt;(u128 a, u128 b)
{
    if (a.high != b.high) return a.high &amp;lt; b.high;
    return a.low &amp;lt; b.low;
}
&lt;/code&gt;
    &lt;p&gt;This is correct, but the codegen is not great:&lt;/p&gt;
    &lt;code&gt;operator&amp;lt;(u128, u128):
        xor     r8d, r8d
        cmp     rdi, rdx
        setb    r8b
        xor     eax, eax
        cmp     rsi, rcx
        setb    al
        cmove   eax, r8d
        ret
&lt;/code&gt;
    &lt;p&gt;Works, but heavier than necessary. We can do better by leaning on the hardware borrow flag.&lt;/p&gt;
    &lt;p&gt;Unsigned comparison &lt;code&gt;a &amp;lt; b&lt;/code&gt; is equivalent to checking whether &lt;code&gt;a - b&lt;/code&gt; produces a borrow:&lt;/p&gt;
    &lt;code&gt;bool operator&amp;lt;(u128 a, u128 b) 
{
    u64 dont_care;

    // compute borrow from (a.low - b.low). If a.low &amp;lt; b.low =&amp;gt; borrow = 1.
    unsigned char borrow = _subborrow_u64(0, a.low, b.low, &amp;amp;dont_care);

    // now subtract highs with that borrow
    // final borrow tells us if a &amp;lt; b in 128-bit unsigned.
    borrow = _subborrow_u64(borrow, a.high, b.high, &amp;amp;dont_care);

    return borrow != 0;
}
&lt;/code&gt;
    &lt;p&gt;The resulting assembly is minimal:&lt;/p&gt;
    &lt;code&gt;operator&amp;lt;(u128, u128):
        cmp     rdi, rdx
        sbb     rsi, rcx
        setb    al
        ret
&lt;/code&gt;
    &lt;p&gt;One compare, one subtract with borrow, and a flag check. This is exactly the kind of codegen we want in hot code.&lt;/p&gt;
    &lt;head rend="h2"&gt;A small use site&lt;/head&gt;
    &lt;p&gt;To make sure everything composes properly, let's build a slightly larger function.&lt;/p&gt;
    &lt;code&gt;u128 demo_u128(u128 a, u128 b) 
{
    u128 x = a + b;
    u128 y = a * b;
    return x &amp;lt; y
            ? y - x
            : x - y;
}
&lt;/code&gt;
    &lt;p&gt;All operators inline cleanly:&lt;/p&gt;
    &lt;code&gt;demo_u128(u128, u128):
        mov     r8, rdi
        add     r8, rdx
        mov     r9, rsi
        adc     r9, rcx
        mulx    rax, r10, rdi
        imul    rcx, rdi
        imul    rdx, rsi
        add     rdx, rcx
        add     rdx, rax
        mov     rax, r8
        sub     rax, r10
        mov     rcx, r9
        sbb     rcx, rdx
        jae     .LBB13_2
        sub     r10, r8
        sbb     rdx, r9
        mov     rax, r10
        mov     rcx, rdx
.LBB13_2:
        mov     rdx, rcx
        ret
&lt;/code&gt;
    &lt;p&gt;There is a branch for the ternary operator while the builtin version uses conditional moves instead. Which is better depends on data patterns but could go either way. I consider this basically as good as it gets.&lt;/p&gt;
    &lt;head rend="h2"&gt;Platform notes&lt;/head&gt;
    &lt;p&gt;The examples shown are for x64 with Clang or GCC.&lt;/p&gt;
    &lt;p&gt;On MSVC, &lt;code&gt;_addcarry_u64&lt;/code&gt; and &lt;code&gt;_subborrow_u64&lt;/code&gt; work the same way.
For multiplication, &lt;code&gt;_umul128&lt;/code&gt; replaces &lt;code&gt;_mulx_u64&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;On AArch64, the same approach applies using &lt;code&gt;adds&lt;/code&gt; and &lt;code&gt;adcs&lt;/code&gt; instructions for addition, &lt;code&gt;subs&lt;/code&gt; and &lt;code&gt;sbcs&lt;/code&gt; for subtraction, and &lt;code&gt;mul + umulh&lt;/code&gt; for the low + high half of a 64-bit multiply.
The patterns carry over directly, even though the intrinsics differ slightly (and multiplication is split into two parts).&lt;/p&gt;
    &lt;head rend="h2"&gt;Outlook&lt;/head&gt;
    &lt;p&gt;This &lt;code&gt;u128&lt;/code&gt; is the easiest large integer you can write.
Our goal is best performance, so we made sure that codegen is reasonably identical to the builtin &lt;code&gt;__uint128_t&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;From here, the same patterns extend naturally to signed variants, widening multiplies such as u128 times u128 to u256, and chains of fixed-width integers like i192 or i256. More importantly, the same reasoning applies when you design predicate-specific arithmetic that only computes what is actually needed.&lt;/p&gt;
    &lt;p&gt;A lot of our performance really boils down to these types: No &lt;code&gt;BigInteger&lt;/code&gt; tax, no floating predicates (that need a few kB of stack space for the worst case).
Just a lot of straightline integer code and some light static branching.&lt;/p&gt;
    &lt;head rend="h2"&gt;Addendum 2026-01-24&lt;/head&gt;
    &lt;p&gt;Some notes based on reader feedback.&lt;/p&gt;
    &lt;p&gt;On PowerPC: I don't know much about PowerPC or the availability of intrinsics, but the instructions you need are definitely there. For add/sub with carry, there's addc / adde for addition and subfc / subfe for subtraction with borrow. For wide multiplication, mulhdu gives you the high half of a 64-bit multiply (and mulhd for signed).&lt;/p&gt;
    &lt;p&gt;On GCC codegen with intrinsics: Someone pointed out that GCC doesn't always handle the intrinsic-based addition optimally. It sometimes moves the result through the stack before setting it in the final registers. This is related to writing directly to the result struct. If you write to "real" scalars first, the codegen is optimal again. A weird one.&lt;/p&gt;
    &lt;p&gt;On division: There is no neat codegen for division. Even the builtin delegates to a library call. The naive but practical approach is binary long division, which finishes in up to 128 steps. Either branchless with fixed runtime or with a loop that searches for the next set bit. Either way it's a bit of work. Our exact predicates are always formulated in a division-free way simply because division would be expensive.&lt;/p&gt;
    &lt;p&gt;On &lt;code&gt;_BitInt(N)&lt;/code&gt;:
The upcoming &lt;code&gt;_BitInt(N)&lt;/code&gt; type does work, but with caveats.
For B = 128, you get the normal codegen.
For B &amp;gt; 128, GCC calls into a function where the bit size is a runtime parameter.
So yes, they would work, but performance will be subpar for larger widths.
Clang generates properly inlined assembly.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46849154</guid><pubDate>Sun, 01 Feb 2026 20:40:45 +0000</pubDate></item><item><title>My iPhone 16 Pro Max produces garbage output when running MLX LLMs</title><link>https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/</link><description>&lt;doc fingerprint="3402b8b4df265bde"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;My iPhone 16 Pro Max produces garbage output when running MLX LLMs. An iPhone 15 Pro runs the same code perfectly. A MacBook Pro also runs the same code perfectly. The tensor outputs on the 16 show numerical values an order of magnitude wrong. I suspect it points to a hardware defect in the Neural Engine or some other ML-needed system.&lt;/p&gt;
    &lt;p&gt;It was a PITA to debug, but at least I got a blog post out of it.&lt;/p&gt;
    &lt;head rend="h1"&gt;How did I get there?&lt;/head&gt;
    &lt;p&gt;This was supposed to be a simple, unwinding-time project.&lt;/p&gt;
    &lt;p&gt;For the past few months I've been working on a &lt;del&gt;Clawdbot&lt;/del&gt; Moltbot clone that I've been calling Schmidt. It basically does the same kind of thing but with a custom chat UI instead of using Telegram, WhatsApp or other "I-can't-afford-to-be-banned-from" Service. This project has been consuming early days and late nights, so, to unwind, I decided that it may be a good idea to do something simpler. Since I recently subscribed to MiniMax M2.1, I thought I would do what many do and build a simple expense tracking app to test out the model.&lt;/p&gt;
    &lt;p&gt;The core functionality is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically, upon each payment, add the expense to my app&lt;/item&gt;
      &lt;item&gt;Update an Apple Watch complication with the % of my monthly budget spent&lt;/item&gt;
      &lt;item&gt;Categorize the purchase for later analysis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all comes from being basically orphaned by Nubank's amazing native app (since replaced by a less-full-featured Flutter version).&lt;/p&gt;
    &lt;p&gt;Integrating with Shortcuts is manual, but reliable. Within 15 minutes I had a version of the app that could register purchases. The Apple Watch complication, the main goal, can come later. I'd rather get the classification feature, which should be easy, done quickly ‚Äì so I figured.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apple Intelligence&lt;/head&gt;
    &lt;p&gt;Given the new LLM-bonanza we've been living through, it's no surprise that Apple has their own set of APIs developers such as me can use. Reading up on the documentation, it's a matter of checking for the availability of the feature and then asking the model to either reply to a textual query or, in my case, categorize a request.&lt;/p&gt;
    &lt;p&gt;MiniMax raced through it in a single prompt and then I ran it on my iPhone. First expense was a purchase at a shop called "Kasai Kitchin", classified as... &lt;code&gt;unknown&lt;/code&gt;.&lt;lb/&gt;Weird.&lt;/p&gt;
    &lt;p&gt;Checking the logs, it was clear: the model support was downloading. The feature hadn't been enabled. Again, weird. I should have it on. Anyway, I go into settings, do the weird dance of toggling it on and off ‚Äì sadly, that's not surprising on Apple's services. Maybe my Settings.app got stuck in a weird state, who knows? ‚Äì and wait for it to download.&lt;/p&gt;
    &lt;p&gt;After 4h I realized it was not going anywhere. Looking it up, it seems that many have the same issue (this thread shows 12 pages of frustrated users). Again, not a surprise for Apple's services recently.&lt;/p&gt;
    &lt;p&gt;Oh well, time to give up on the Apple Intelligence approach. Let's move on to the next one.&lt;/p&gt;
    &lt;head rend="h2"&gt;MLX LLM&lt;/head&gt;
    &lt;p&gt;Well, the iOS framework engineers don't seem to be the only engineers at Apple capable of coming up with Machine Learning APIs in Swift. Apparently, there's a whole separate way of doing it ‚Äì with models downloaded to your app. Not great for the user's storage, but great for me!&lt;/p&gt;
    &lt;p&gt;Again, MiniMax does it in a heartbeat, specially after being given documentation and one or two Medium posts. Time to run on my iPhone and... gibberish.&lt;/p&gt;
    &lt;p&gt;The CPU spins to 100% and the model starts generating. But it's all gibberish. And no "stop" token is generated, so this goes on for long.&lt;/p&gt;
    &lt;p&gt;At this point, the only explanation is: I'm completely incompetent and can't even get a simple "ready made" framework to execute what I want. Or, rather, MiniMax is! The good thing about offloading your work to an LLM is that you can blame it for your shortcomings. Time to get my hands dirty and do it myself, typing code on my keyboard, like the ancient Mayan and Aztec programmers probably did.&lt;/p&gt;
    &lt;head rend="h2"&gt;My own MLX implementation&lt;/head&gt;
    &lt;p&gt;I went back to the documentation, to the Medium posts and, much to my surprise: MiniMax had followed it to the letter. Even went back to some deprecated methods of generation and it also was gibberish. And now there's no one to blame, but myself. I go to work everyday and this impostor-syndrome inducing problem silently consumes me. &lt;lb/&gt;After 3 days of trying to get it to work, I'm ready to give up...&lt;lb/&gt;...until, on a Tuesday morning, at 7-8 AM, I have an idea: let me, just in case, run this on my old iPhone 15 Pro. Up to this point, I was running it on my daily driver, an iPhone 16 Pro Max that was a replacement phone sent by Apple Care after a small clubbing mishap (in which my iPhone was irreparably crashed). I rush to get everything ready before it's time to go to work and: it works! Gemma, Qwen, and all other models generate coherent responses!&lt;/p&gt;
    &lt;p&gt;I stop and think: this cannot be a hardware issue, right? Of course not. The iPhone 15 is still running iOS 18. The iPhone 16 is running 26. It must be an OS issue. Well, time to be late for my work standup and update the old phone. The curiosity is too much. Many minutes later... same results, now on iOS 26. The plot is thickening.&lt;/p&gt;
    &lt;head rend="h1"&gt;Finding the smoking gun: breakpoints in MLX's implementations of Gemma&lt;/head&gt;
    &lt;p&gt;After that work day, and after many lunch and coffee discussions with coworkers about the sources of my troubles, I get home and immediately set myself on debugging MLX as it runs, if possible. The game plan is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a known-to-be-reliable model, that fits in RAM (I went with quantized Gemma)&lt;/item&gt;
      &lt;item&gt;Use a simple prompt, in my case "What is 2+2?"&lt;list rend="ul"&gt;&lt;item&gt;To be really pedantic: the prompt was &lt;code&gt;&amp;lt;start_of_turn&amp;gt;user\nWhat is 2+2?&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;To be really pedantic: the prompt was &lt;/item&gt;
      &lt;item&gt;Run everything with temperature set to &lt;code&gt;0.0&lt;/code&gt;‚Äì maybe that's enough to remove variability&lt;/item&gt;
      &lt;item&gt;Find the model implementation&lt;/item&gt;
      &lt;item&gt;Find where the model iterates through the layers and&lt;/item&gt;
      &lt;item&gt;Print out the MLXArray/Tensor with the values on each layer as the input goes through&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A few moments later and I find where I need to be. Added the breakpoints, added the logs and off to the races.&lt;/p&gt;
    &lt;p&gt;I run it on my iPhone 16 Pro Max. The model loads and the prompt is "What is 2+2?". The tensors start printing out, line after line after line. For once, the logs aren't complete gibberish ‚Äì they're numbers. Floating point values representing the model's internal state as it processes the input. I save the output to a file and do the same on my iPhone 15 Pro. Same model, same prompt, same code. Time to compare.&lt;/p&gt;
    &lt;head rend="h1"&gt;Welp, now it's definitely out of my expertise&lt;/head&gt;
    &lt;p&gt;I grep for a pattern I know should be consistent ‚Äì an array at log-line 58, right before the values get normalized/softmaxed. On a working device, I hypothesize this should be the same every time.&lt;lb/&gt;On the iPhone 15 Pro:&lt;code&gt;3: "[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]"&lt;/code&gt;&lt;lb/&gt;On the iPhone 16 Pro Max:&lt;code&gt;3: "[[[[191.5, 23.625, 173.75, ..., 1298, -147.25, -162.5]]]]"&lt;/code&gt;&lt;lb/&gt;Huh. Not close. Not at all. These values are orders of magnitude off. I double check the start of the logs and both phones show the same:&lt;code&gt;1: "array([[[0.162842, -0.162842, -0.48877, ..., -0.176636, 0.0001297, 0.088501],\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\n [-1.30957, 1.57324, -1.30957, ..., -0.0010376, -0.0010376, 1.12305],\n ...,\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\n [0.296875, 0.59375, 0.890625, ..., -0.59375, 0.296875, -0.890137],\n [1.02734, -0.616211, -0.616211, ..., -0.275879, -0.551758, 0.275879]]], dtype=float16)"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;OK, so the model receives the same thing as input, but at some point, the values start to go off. Like, way off. In order to make sure I'm not crazy, I do one last thing: run the same thing on my Mac. Make the app run on iPad compatibility mode and...&lt;code&gt;3: "[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Bingo! Same as iPhone 15!&lt;/p&gt;
    &lt;p&gt;The model isn't broken. The code isn't broken. Most importantly, I'm not broken*. My phone is broken.&lt;lb/&gt;*arguable, but besides the point here&lt;/p&gt;
    &lt;head rend="h1"&gt;What's going on?&lt;/head&gt;
    &lt;p&gt;Let me explain what I think it's going on here: the iPhone 16 Pro Max contains Apple's A18 chip with its Neural Engine‚Äîa specialized accelerator for machine learning operations. MLX uses Metal to compile tensor operations for this accelerator. Somewhere in that stack, the computations are going very wrong. I don't think it's a widespread issue but, I do get disappointed that a relatively newly replaced iPhone from Apple Care came with such an issue.&lt;/p&gt;
    &lt;p&gt;However, if my Apple Intelligence troubles are related ‚Äì and they might as well be, I'd assume that code and MLX are not dissimilar in operations being done ‚Äì, it could be that all the 12 pages of users are users in a similar dillema, but without the means of debugging it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;I spent 3 days thinking I was incompetent. I blamed MiniMax. I blamed myself. The entire time, my $1,400 phone had a broken hardware. I could lose more time figuring out exactly what is wrong with it but it‚Äôs literally not worth my time.&lt;/p&gt;
    &lt;p&gt;I guess I can at least take a lesson that, when debugging, I should always consider the physical layer. I spent three days assuming this was a software problem ‚Äì my code, the library, the framework, my skills as a developer. The breakthrough was basically: "What if I'm not dumb and it's not my code?"&lt;/p&gt;
    &lt;p&gt;As for my phone: it'll probably go back to Apple, as a trade in for a new iPhone 17 Pro Max that hopefully ü§û can do math.&lt;/p&gt;
    &lt;head rend="h3"&gt;Update on Feb. 1st:&lt;/head&gt;
    &lt;p&gt;Well, now it's Feb. 1st and I have an iPhone 17 Pro Max to test with and... everything works as expected. So it's pretty safe to say that THAT specific instance of iPhone 16 Pro Max was hardware-defective.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46849258</guid><pubDate>Sun, 01 Feb 2026 20:51:56 +0000</pubDate></item><item><title>Defeating a 40-year-old copy protection dongle</title><link>https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle</link><description>&lt;doc fingerprint="a99b5744e017df58"&gt;
  &lt;main&gt;
    &lt;p&gt;That‚Äôs right ‚Äî this little device is what stood between me and the ability to run an even older piece of software that I recently unearthed during an expedition of software archaeology.&lt;/p&gt;
    &lt;p&gt;For a bit more background, I was recently involved in helping a friend‚Äôs accounting firm to move away from using an extremely legacy software package that they had locked themselves into using for the last four decades.&lt;/p&gt;
    &lt;p&gt;This software was built using a programming language called RPG (‚ÄúReport Program Generator‚Äù), which is older than COBOL (!), and was used with IBM‚Äôs midrange computers such as the System/3, System/32, and all the way up to the AS/400. Apparently, RPG was subsequently ported to MS-DOS, so that the same software tools built with RPG could run on personal computers, which is how we ended up here.&lt;/p&gt;
    &lt;p&gt;This accounting firm was actually using a Windows 98 computer (yep, in 2026), and running the RPG software inside a DOS console window. And it turned out that, in order to run this software, it requires a special hardware copy-protection dongle to be attached to the computer‚Äôs parallel port! This was a relatively common practice in those days, particularly with ‚Äúenterprise‚Äù software vendors who wanted to protect their very important‚Ñ¢ software from unauthorized use.&lt;/p&gt;
    &lt;p&gt;Sadly, most of the text and markings on the dongle‚Äôs label has been worn or scratched off, but we can make out several clues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The words ‚ÄúStamford, CT‚Äù, and what‚Äôs very likely the logo of a company called ‚ÄúSoftware Security Inc‚Äù. The only evidence for the existence of this company is this record of them exhibiting their wares at SIGGRAPH conferences in the early 1990s, as well as several patents issued to them, relating to software protection.&lt;/item&gt;
      &lt;item&gt;A word that seems to say ‚ÄúRUNTIME‚Äù, which will become clear in a bit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My first course of action was to take a disk image of the Windows 98 PC that was running this software, and get it running in an emulator, so that we could see what the software actually does, and perhaps export the data from this software into a more modern format, to be used with modern accounting tools. But of course all of this requires the hardware dongle; none of the accounting tools seem to work without it plugged in.&lt;/p&gt;
    &lt;p&gt;Before doing anything, I looked through the disk image for any additional interesting clues, and found plenty of fascinating (and archaeologically significant?) stuff:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We‚Äôve got a compiler for the RPG II language (excellent!), made by a company called Software West Inc.&lt;/item&gt;
      &lt;item&gt;Even better, there are two versions of the RPG II compiler, released on various dates in the 1990s by Software West.&lt;/item&gt;
      &lt;item&gt;We‚Äôve got the complete source code of the accounting software, written in RPG. It looks like the full accounting package consists of numerous RPG modules, with a gnarly combination of DOS batch files for orchestrating them, all set up as a ‚Äúmenu‚Äù system for the user to navigate using number combinations. Clearly the author of this accounting system was originally an IBM mainframe programmer, and insisted on bringing those skills over to DOS, with mixed results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I began by playing around with the RPG compiler in isolation, and I learned very quickly that it‚Äôs the RPG compiler itself that requires the hardware dongle, and then the compiler automatically injects the same copy-protection logic into any executables it generates. This explains the text that seems to say ‚ÄúRUNTIME‚Äù on the dongle.&lt;/p&gt;
    &lt;p&gt;The compiler consists of a few executable files, notably &lt;code&gt;RPGC.EXE&lt;/code&gt;, which is the compiler, and &lt;code&gt;SEU.EXE&lt;/code&gt;, which is a source editor (‚ÄúSource Entry Utility‚Äù). Here‚Äôs what we get when we launch SEU without the dongle, after a couple of seconds:&lt;/p&gt;
    &lt;p&gt;A bit rude, but this gives us an important clue: this program must be trying to communicate over the parallel port over the course of a few seconds (which could give us an opportunity to pause it for debugging, and see what it‚Äôs doing during that time), and then exits with a message (which we can now find in a disassembly of the program, and trace how it gets there).&lt;/p&gt;
    &lt;p&gt;A great tool for disassembling executables of this vintage is Reko. It understands 16-bit real mode executables, and even attempts to decompile them into readable C code that corresponds to the disassembly.&lt;/p&gt;
    &lt;p&gt;And so, looking at the decompiled/disassembled code in Reko, I expected to find &lt;code&gt;in&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; instructions, which would be the telltale sign of the program trying to communicate with the parallel port through the PC‚Äôs I/O ports. However‚Ä¶ I didn‚Äôt see an &lt;code&gt;in&lt;/code&gt; or &lt;code&gt;out&lt;/code&gt; instruction anywhere! But then I noticed something: Reko disassembled the executable into two ‚Äúsegments‚Äù: &lt;code&gt;0800&lt;/code&gt; and &lt;code&gt;0809&lt;/code&gt;, and I was only looking at segment &lt;code&gt;0809&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we look at segment &lt;code&gt;0800&lt;/code&gt;, we see the smoking gun: &lt;code&gt;in&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; instructions, meaning that the copy-protection routine is definitely here, and best of all, the entire code segment is a mere 0x90 bytes, which suggests that the entire routine should be pretty easy to unravel and understand. For some reason, Reko was not able to decompile this code into a C representation, but it still produced a disassembly, which will work just fine for our purposes. Maybe this was a primitive form of obfuscation from those early days, which is now confusing Reko and preventing it from associating this chunk of code with the rest of the program‚Ä¶ who knows.&lt;/p&gt;
    &lt;p&gt;Here is a GitHub Gist with the disassembly of this code, along with my annotations and notes. My x86 assembly knowledge is a little rusty, but here is the gist of what this code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It‚Äôs definitely a single self-contained routine, intended to be called using a ‚Äúfar‚Äù &lt;code&gt;CALL&lt;/code&gt;instruction, since it returns with a&lt;code&gt;RETF&lt;/code&gt;instruction.&lt;/item&gt;
      &lt;item&gt;It begins by detecting the address of the parallel port, by reading the BIOS data area. If the computer has more than one parallel port, the dongle must be connected to the first parallel port (LPT1).&lt;/item&gt;
      &lt;item&gt;It performs a loop where it writes values to the data register of the parallel port, and then reads the status register, and accumulates responses in the &lt;code&gt;BH&lt;/code&gt;and&lt;code&gt;BL&lt;/code&gt;registers.&lt;/item&gt;
      &lt;item&gt;At the end of the routine, the ‚Äúresult‚Äù of the whole procedure is stored in the &lt;code&gt;BX&lt;/code&gt;register (&lt;code&gt;BH&lt;/code&gt;and&lt;code&gt;BL&lt;/code&gt;together), which will presumably be ‚Äúverified‚Äù by the caller of the routine.&lt;/item&gt;
      &lt;item&gt;Very importantly, there doesn‚Äôt seem to be any ‚Äúinput‚Äù into this routine. It doesn‚Äôt pop anything from the stack, nor does it care about any register values passed into it. Which can only mean that the result of this routine is completely constant! No matter what complicated back-and-forth it does with the dongle, the result of this routine should always be the same.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the knowledge that this routine must exit with some magic value stored in &lt;code&gt;BX&lt;/code&gt;, we can now patch the first few bytes of the routine to do just that! Not yet knowing which value to put in &lt;code&gt;BX&lt;/code&gt;, let‚Äôs start with 1234:&lt;/p&gt;
    &lt;code&gt;BB 34 12       MOV BX, 1234h
CB             RETF
&lt;/code&gt;
    &lt;p&gt;Only the first four bytes need patching ‚Äî set &lt;code&gt;BX&lt;/code&gt; to our desired value, and get out of there (&lt;code&gt;RETF&lt;/code&gt;). Running the patched executable with these new bytes still fails (expectedly) with the same message of ‚ÄúNo dongle, no edit‚Äù, but it fails immediately, instead of after several seconds of talking to the parallel port. Progress!&lt;/p&gt;
    &lt;p&gt;Stepping through the disassembly more closely, we get another major clue: The only value that &lt;code&gt;BH&lt;/code&gt; can be at the end of the routine is 76h (this is hard-coded into the routine). So, our total value for the magic number in &lt;code&gt;BX&lt;/code&gt; must be of the form 76xx. In other words, only the &lt;code&gt;BL&lt;/code&gt; value remains unknown:&lt;/p&gt;
    &lt;code&gt;BB __ 76       MOV BX, 76__h
CB             RETF
&lt;/code&gt;
    &lt;p&gt;Since &lt;code&gt;BL&lt;/code&gt; is an 8-bit register, it can only have 256 possible values. And what do we do when we have 256 combinations to try? Brute force it! I whipped up a script that plugs a value into that particular byte (from 0 to 255) and programmatically launches the executable in DosBox, and observes the output. Lo and behold, it worked! The brute forcing didn‚Äôt take long at all, because the correct number turned out to be‚Ä¶ 6. Meaning that the total magic number in &lt;code&gt;BX&lt;/code&gt; should be 7606h:&lt;/p&gt;
    &lt;code&gt;BB 06 76       MOV BX, 7606h
CB             RETF
&lt;/code&gt;
    &lt;p&gt;Bingo!&lt;lb/&gt; And then, proceeding to examine the other executable files in the compiler suite, the parallel port routine turns out to be exactly the same. All of the executables have the exact same copy protection logic, as if it was rubber-stamped onto them. In fact, when the compiler (&lt;code&gt;RPGC.EXE&lt;/code&gt;) compiles some RPG source code, it seems to copy the parallel port routine from itself into the compiled program. That‚Äôs right: the patched version of the compiler will produce executables with the same patched copy protection routine! Very convenient.&lt;/p&gt;
    &lt;p&gt;I must say, this copy protection mechanism seems a bit‚Ä¶ simplistic? A hardware dongle that just passes back a constant number? Defeatable with a four-byte patch? Is this really worthy of a patent? But who am I to pass judgment. It‚Äôs possible that I haven‚Äôt fully understood the logic, and the copy protection will somehow re-surface in another way. It‚Äôs also possible that the creators of the RPG compiler (Software West, Inc) didn‚Äôt take proper advantage of the hardware dongle, and used it in a way that is so easily bypassed.&lt;/p&gt;
    &lt;p&gt;In any case, Software West‚Äôs RPG II compiler is now free from the constraint of the parallel port dongle! And at some point soon, I‚Äôll work on purging any PII from the compiler directories, and make this compiler available as an artifact of computing history. It doesn‚Äôt seem to be available anywhere else on the web. If anyone reading this was associated with Software West Inc, feel free to get in touch ‚Äî I have many questions!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46849567</guid><pubDate>Sun, 01 Feb 2026 21:30:51 +0000</pubDate></item><item><title>Show HN: NanoClaw ‚Äì ‚ÄúClawdbot‚Äù in 500 lines of TS with Apple container isolation</title><link>https://github.com/gavrielc/nanoclaw</link><description>&lt;doc fingerprint="1f0c5d761f974ce8"&gt;
  &lt;main&gt;
    &lt;p&gt;My personal Claude assistant that runs securely in containers. Lightweight and built to be understood and customized for your own needs.&lt;/p&gt;
    &lt;p&gt;OpenClaw is an impressive project with a great vision. But I can't sleep well running software I don't understand with access to my life. OpenClaw has 52+ modules, 8 config management files, 45+ dependencies, and abstractions for 15 channel providers. Security is application-level (allowlists, pairing codes) rather than OS isolation. Everything runs in one Node process with shared memory.&lt;/p&gt;
    &lt;p&gt;NanoClaw gives you the same core functionality in a codebase you can understand in 8 minutes. One process. A handful of files. Agents run in actual Linux containers with filesystem isolation, not behind permission checks.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/gavrielc/nanoclaw.git
cd nanoclaw
claude&lt;/code&gt;
    &lt;p&gt;Then run &lt;code&gt;/setup&lt;/code&gt;. Claude Code handles everything: dependencies, authentication, container setup, service configuration.&lt;/p&gt;
    &lt;p&gt;Small enough to understand. One process, a few source files. No microservices, no message queues, no abstraction layers. Have Claude Code walk you through it.&lt;/p&gt;
    &lt;p&gt;Secure by isolation. Agents run in Linux containers (Apple Container on macOS, or Docker). They can only see what's explicitly mounted. Bash access is safe because commands run inside the container, not on your host.&lt;/p&gt;
    &lt;p&gt;Built for one user. This isn't a framework. It's working software that fits my exact needs. You fork it and have Claude Code make it match your exact needs.&lt;/p&gt;
    &lt;p&gt;Customization = code changes. No configuration sprawl. Want different behavior? Modify the code. The codebase is small enough that this is safe.&lt;/p&gt;
    &lt;p&gt;AI-native. No installation wizard; Claude Code guides setup. No monitoring dashboard; ask Claude what's happening. No debugging tools; describe the problem, Claude fixes it.&lt;/p&gt;
    &lt;p&gt;Skills over features. Contributors shouldn't add features (e.g. support for Telegram) to the codebase. Instead, they contribute claude code skills like &lt;code&gt;/add-telegram&lt;/code&gt; that transform your fork. You end up with clean code that does exactly what you need.&lt;/p&gt;
    &lt;p&gt;Best harness, best model. This runs on Claude Agent SDK, which means you're running Claude Code directly. The harness matters. A bad harness makes even smart models seem dumb, a good harness gives them superpowers. Claude Code is (IMO) the best harness available.&lt;/p&gt;
    &lt;p&gt;No ToS gray areas. Because it uses Claude Agent SDK natively with no hacks or workarounds, using your subscription with your auth token is completely legitimate (I think). No risk of being shut down for terms of service violations (I am not a lawyer).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WhatsApp I/O - Message Claude from your phone&lt;/item&gt;
      &lt;item&gt;Isolated group context - Each group has its own &lt;code&gt;CLAUDE.md&lt;/code&gt;memory, isolated filesystem, and runs in its own container sandbox with only that filesystem mounted&lt;/item&gt;
      &lt;item&gt;Main channel - Your private channel (self-chat) for admin control; every other group is completely isolated&lt;/item&gt;
      &lt;item&gt;Scheduled tasks - Recurring jobs that run Claude and can message you back&lt;/item&gt;
      &lt;item&gt;Web access - Search and fetch content&lt;/item&gt;
      &lt;item&gt;Container isolation - Agents sandboxed in Apple Container (macOS) or Docker (macOS/Linux)&lt;/item&gt;
      &lt;item&gt;Optional integrations - Add Gmail (&lt;code&gt;/add-gmail&lt;/code&gt;) and more via skills&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Talk to your assistant with the trigger word (default: &lt;code&gt;@Andy&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;@Andy send an overview of the sales pipeline every weekday morning at 9am (has access to my Obsidian vault folder)
@Andy review the git history for the past week each Friday and update the README if there's drift
@Andy every Monday at 8am, compile news on AI developments from Hacker News and TechCrunch and message me a briefing
&lt;/code&gt;
    &lt;p&gt;From the main channel (your self-chat), you can manage groups and tasks:&lt;/p&gt;
    &lt;code&gt;@Andy list all scheduled tasks across groups
@Andy pause the Monday briefing task
@Andy join the Family Chat group
&lt;/code&gt;
    &lt;p&gt;There are no configuration files to learn. Just tell Claude Code what you want:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Change the trigger word to @Bob"&lt;/item&gt;
      &lt;item&gt;"Remember in the future to make responses shorter and more direct"&lt;/item&gt;
      &lt;item&gt;"Add a custom greeting when I say good morning"&lt;/item&gt;
      &lt;item&gt;"Store conversation summaries weekly"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Or run &lt;code&gt;/customize&lt;/code&gt; for guided changes.&lt;/p&gt;
    &lt;p&gt;The codebase is small enough that Claude can safely modify it.&lt;/p&gt;
    &lt;p&gt;Don't add features. Add skills.&lt;/p&gt;
    &lt;p&gt;If you want to add Telegram support, don't create a PR that adds Telegram alongside WhatsApp. Instead, contribute a skill file (&lt;code&gt;.claude/skills/add-telegram/SKILL.md&lt;/code&gt;) that teaches Claude Code how to transform a NanoClaw installation to use Telegram.&lt;/p&gt;
    &lt;p&gt;Users then run &lt;code&gt;/add-telegram&lt;/code&gt; on their fork and get clean code that does exactly what they need, not a bloated system trying to support every use case.&lt;/p&gt;
    &lt;p&gt;Skills we'd love to see:&lt;/p&gt;
    &lt;p&gt;Communication Channels&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/add-telegram&lt;/code&gt;- Add Telegram as channel. Should give the user option to replace WhatsApp or add as additional channel. Also should be possible to add it as a control channel (where it can trigger actions) or just a channel that can be used in actions triggered elsewhere&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/add-slack&lt;/code&gt;- Add Slack&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/add-discord&lt;/code&gt;- Add Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platform Support&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/setup-windows&lt;/code&gt;- Windows via WSL2 + Docker&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Session Management&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/add-clear&lt;/code&gt;- Add a&lt;code&gt;/clear&lt;/code&gt;command that compacts the conversation (summarizes context while preserving critical information in the same session). Requires figuring out how to trigger compaction programmatically via the Claude Agent SDK.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS or Linux&lt;/item&gt;
      &lt;item&gt;Node.js 20+&lt;/item&gt;
      &lt;item&gt;Claude Code&lt;/item&gt;
      &lt;item&gt;Apple Container (macOS) or Docker (macOS/Linux)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;WhatsApp (baileys) --&amp;gt; SQLite --&amp;gt; Polling loop --&amp;gt; Container (Claude Agent SDK) --&amp;gt; Response
&lt;/code&gt;
    &lt;p&gt;Single Node.js process. Agents execute in isolated Linux containers with mounted directories. IPC via filesystem. No daemons, no queues, no complexity.&lt;/p&gt;
    &lt;p&gt;Key files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/index.ts&lt;/code&gt;- Main app: WhatsApp connection, routing, IPC&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/container-runner.ts&lt;/code&gt;- Spawns agent containers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/task-scheduler.ts&lt;/code&gt;- Runs scheduled tasks&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/db.ts&lt;/code&gt;- SQLite operations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;groups/*/CLAUDE.md&lt;/code&gt;- Per-group memory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why WhatsApp and not Telegram/Signal/etc?&lt;/p&gt;
    &lt;p&gt;Because I use WhatsApp. Fork it and run a skill to change it. That's the whole point.&lt;/p&gt;
    &lt;p&gt;Why Apple Container instead of Docker?&lt;/p&gt;
    &lt;p&gt;On macOS, Apple Container is lightweight, fast, and optimized for Apple silicon. But Docker is also fully supported‚Äîduring &lt;code&gt;/setup&lt;/code&gt;, you can choose which runtime to use. On Linux, Docker is used automatically.&lt;/p&gt;
    &lt;p&gt;Can I run this on Linux?&lt;/p&gt;
    &lt;p&gt;Yes. Run &lt;code&gt;/setup&lt;/code&gt; and it will automatically configure Docker as the container runtime. Thanks to @dotsetgreg for contributing the &lt;code&gt;/convert-to-docker&lt;/code&gt; skill.&lt;/p&gt;
    &lt;p&gt;Is this secure?&lt;/p&gt;
    &lt;p&gt;Agents run in containers, not behind application-level permission checks. They can only access explicitly mounted directories. You should still review what you're running, but the codebase is small enough that you actually can. See docs/SECURITY.md for the full security model.&lt;/p&gt;
    &lt;p&gt;Why no configuration files?&lt;/p&gt;
    &lt;p&gt;We don't want configuration sprawl. Every user should customize it to so that the code matches exactly what they want rather than configuring a generic system. If you like having config files, tell Claude to add them.&lt;/p&gt;
    &lt;p&gt;How do I debug issues?&lt;/p&gt;
    &lt;p&gt;Ask Claude Code. "Why isn't the scheduler running?" "What's in the recent logs?" "Why did this message not get a response?" That's the AI-native approach.&lt;/p&gt;
    &lt;p&gt;Why isn't the setup working for me?&lt;/p&gt;
    &lt;p&gt;I don't know. Run &lt;code&gt;claude&lt;/code&gt;, then run &lt;code&gt;/debug&lt;/code&gt;. If claude finds an issue that is likely affecting other users, open a PR to modify the setup SKILL.md.&lt;/p&gt;
    &lt;p&gt;What changes will be accepted into the codebase?&lt;/p&gt;
    &lt;p&gt;Security fixes, bug fixes, and clear improvements to the base configuration. That's it.&lt;/p&gt;
    &lt;p&gt;Everything else (new capabilities, OS compatibility, hardware support, enhancements) should be contributed as skills.&lt;/p&gt;
    &lt;p&gt;This keeps the base system minimal and lets every user customize their installation without inheriting features they don't want.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46850205</guid><pubDate>Sun, 01 Feb 2026 22:49:22 +0000</pubDate></item><item><title>Show HN: Wikipedia as a doomscrollable social media feed</title><link>https://xikipedia.org</link><description>&lt;doc fingerprint="d4d05bb2748febb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;p&gt;Xikipedia is a pseudo social media feed that algorithmically shows you content from Simple Wikipedia. It is made as a demonstration of how even a basic non-ML algorithm with no data from other users can quickly learn what you engage with to suggest you more similar content. No data is collected or shared here, the algorithm runs locally and the data disappears once you refresh or close the tab.&lt;/p&gt;
    &lt;p&gt;Source code on GitHub, discuss on fedi, bluesky, or twitter.&lt;/p&gt;
    &lt;p&gt;Pick some categories to get started (optional)&lt;/p&gt;
    &lt;p&gt;Or add your own&lt;/p&gt;
    &lt;p&gt;Since the content and images shown is from random Wikipedia articles, you will likely see NSFW content. Please only continue if you're an adult.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46850803</guid><pubDate>Mon, 02 Feb 2026 00:12:19 +0000</pubDate></item><item><title>Actors: A Model of Concurrent Computation [pdf] (1985)</title><link>https://apps.dtic.mil/sti/tr/pdf/ADA157917.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46851192</guid><pubDate>Mon, 02 Feb 2026 01:11:15 +0000</pubDate></item><item><title>Apple's MacBook Pro DFU port documentation is wrong</title><link>https://lapcatsoftware.com/articles/2026/2/1.html</link><description>&lt;doc fingerprint="362448954b27b1fd"&gt;
  &lt;main&gt;
    &lt;p&gt;According to the Apple support document How to identify the DFU port on Mac, the DFU (device firmware update) port location for MacBook Pro models with Apple silicon is as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;14-inch MacBook Pro with M4 or M5 chip: The rightmost USB-C port when you√¢re facing the left side of the Mac&lt;/p&gt;
      &lt;p&gt;All other models: The leftmost USB-C port when you√¢re facing the left side of the Mac&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is wrong, a discovery that took me about a half dozen attempts to update macOS on an external disk. I have a 16-inch MacBook Pro with an M4 chip, specifically an M4 Pro chip, and the DFU port seems to be the USB-C port on the right side of the Mac, not on the left side.&lt;/p&gt;
    &lt;p&gt;For some damn reason, it matters which port your external disk is plugged into when you install or update macOS, as described by the Apple support document How to use an external storage device as a Mac startup disk:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Make sure that your storage device is plugged into the appropriate port on your Mac.&lt;/p&gt;
      &lt;p&gt;If you're using a Mac with Apple silicon, plug your storage device into any compatible port except the DFU port. Learn how to identify the DFU port. After macOS installation is complete, you can connect your storage device to any compatible port, including the DFU port.&lt;/p&gt;
      &lt;p&gt;If you√¢re using any other Mac, plug your storage device into any compatible port.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Mac disk management was so much easier in the days of Intel and PowerPC!&lt;/p&gt;
    &lt;p&gt;On an external SSD I had installed a macOS Sequoia boot volume (among others), which I√¢ve used to take Mac App Store screenshots for my apps and which I√¢d now like to use to take a screen recording. The installed version was still macOS 15.2, because I don√¢t often boot into the disk to take new screenshots, and going through the software update process would occupy my MacBook Pro for annoyingly long. However, it appears that Safari 26 requires a version of macOS 15 higher than .2, so I needed to update macOS in order to update Safari from version 18.&lt;/p&gt;
    &lt;p&gt;Over the past few days, every attempt I made to update the disk volume to macOS 15.7.3 failed inexplicably. I tried both Software Update in System Settings and the &lt;code&gt;softwareupdate&lt;/code&gt; command-line tool in Terminal. They went through all the motions, downloading the entire update, rebooting, etc., but afterwards I always ended up right where I started, at macOS 15.2. The &lt;code&gt;softwareupdate&lt;/code&gt; tool gave no error message. I did eventually see the following (truncated) notification:&lt;/p&gt;
    &lt;p&gt;Nonetheless, the so-called √¢Details√¢ button presented no actual details, simply opening Software Update again in System Settings. At no point did it ever say, hey, plug your disk into a different port!&lt;/p&gt;
    &lt;p&gt;While searching for a solution to my problem, I found an article by Michael Tsai, Failed Software Update on the External Drive of an Apple Silicon Mac. It described something that I also saw in my testing:&lt;/p&gt;
    &lt;quote&gt;I happened to boot into macOS Recovery and look in the Startup Security Utility, and I saw that it did not have access to change the security policy for the external drive. In order to do that, it said I had to set the drive as the startup disk. This kind of didn√¢t make sense because don√¢t the security options get set when booted from Recovery?&lt;/quote&gt;
    &lt;p&gt;I followed Tsai√¢s instructions, which did allow me to change the security policy for the external drive.&lt;/p&gt;
    &lt;quote&gt;I don√¢t know why software update couldn√¢t tell me this or why there is seemingly no direct GUI command to view or edit the authorized users. But restarting from within Startup Disk is apparently the way to get macOS to offer to fix the LocalPolicy. Once I added the user, I was able to do a normal boot from the external drive and software update normally.&lt;/quote&gt;
    &lt;p&gt;I also thought this would solve my macOS update problem, but it didn√¢t. In retrospect, though, perhaps I needed both solutions, to fix the LocalPolicy and to change the ports.&lt;/p&gt;
    &lt;p&gt;I was about to surrender to despair when I discovered a second article by Michael Tsai, Failing to Finish Updating macOS on an External Disk, published soon after the first article:&lt;/p&gt;
    &lt;quote&gt;With the final release of macOS 15.5, the problem got worse, and the Startup Disk workaround no longer helps.&lt;/quote&gt;
    &lt;p&gt;Tsai ultimately hits on the solution:&lt;/p&gt;
    &lt;quote&gt;The problem ended up being that I had plugged the external drive into the wrong USB-C port (the DFU port).&lt;/quote&gt;
    &lt;p&gt;Sure enough, after plugging my disk into a port on the left side of my MacBook Pro, software update succeeded on the first attempt. Every previous, failed attempt used the port on the right side, which was physically more convenient on the desk. So after all that, the external disk is finally updated to macOS 15.7.3 now.&lt;/p&gt;
    &lt;p&gt;Tsai offers the same complaint about this absurd situation:&lt;/p&gt;
    &lt;quote&gt;I don√¢t know why macOS can√¢t just report an error when you use the wrong port instead of proceeding to install for an hour and then not report an error but not work, either.&lt;/quote&gt;
    &lt;p&gt;By the way, Software Update in System Settings allowed my Mac to go to sleep during the √¢Preparing√¢ phase, despite the fact that the battery was charged to 99%, so when I returned home from a workout I unhappily found 30 minutes remaining. Sigh. Whatever happened to √¢it just works√¢?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46852096</guid><pubDate>Mon, 02 Feb 2026 03:29:55 +0000</pubDate></item><item><title>Leaked Chats Expose the Daily Life of a Scam Compound's Enslaved Workforce</title><link>https://www.wired.com/story/the-red-bull-leaks/</link><description>&lt;doc fingerprint="7e05985b3d2382ed"&gt;
  &lt;main&gt;
    &lt;p&gt;Just before 8am one day last April, an office manager who went by the name Amani sent out a motivational message to his colleagues and subordinates. ‚ÄúEvery day brings a new opportunity‚Äîa chance to connect, to inspire, and to make a difference,‚Äù he wrote in his 500-word post to an office-wide WhatsApp group. ‚ÄúTalk to that next customer like you're bringing them something valuable‚Äîbecause you are.‚Äù&lt;/p&gt;
    &lt;p&gt;Amani wasn‚Äôt rallying a typical corporate sales team. He and his underlings worked inside a ‚Äúpig butchering‚Äù compound, a criminal operation built to carry out scams‚Äîpromising romance and riches from crypto investments‚Äîthat often defraud victims out of hundreds of thousands or even millions of dollars at a time.&lt;/p&gt;
    &lt;p&gt;The workers Amani was addressing were eight hours into their 15-hour night shift in a high-rise building in the Golden Triangle special economic zone in Northern Laos. Like their marks, most of them were victims, too: forced laborers trapped in the compound, held in debt bondage with no passports. They struggled to meet scam revenue quotas to avoid fines that deepened their debt. Anyone who broke rules or attempted to escape faced far worse consequences: beatings, torture, even death.&lt;/p&gt;
    &lt;p&gt;The bizarre reality of daily life in a Southeast Asian scam compound‚Äîthe tactics, the tone, the mix of cruelty and upbeat corporate prattle‚Äîis revealed at an unprecedented level of resolution in a leak of documents to WIRED from a whistleblower inside one such sprawling fraud operation. The facility, known as the Boshang compound, is one of dozens of scam operations across Southeast Asia that have enslaved hundreds of thousands of people. Often lured from the poorest regions of Asia and Africa with fake job offers, these conscripts have become engines of the most lucrative form of cybercrime in the world, coerced into stealing tens of billions of dollars.&lt;/p&gt;
    &lt;p&gt;Last June, one of those forced laborers, an Indian man named Mohammad Muzahir, contacted WIRED while he was still captive inside the scam compound that had trapped him. Over the following weeks, Muzahir, who initially identified himself only as ‚ÄúRed Bull,‚Äù shared with WIRED a trove of information about the scam operation. His leaks included internal documents, scam scripts, training guides, operational flowcharts, and photographs and videos from inside the compound.&lt;/p&gt;
    &lt;p&gt;Of all Muzahir‚Äôs leaks, the most revealing is a collection of screen recordings in which he scrolled through three months‚Äô worth of the compound‚Äôs internal WhatsApp group chats. Those videos, which WIRED converted into 4,200 pages of screenshots, capture hour-by-hour conversations between the compound‚Äôs workers and their bosses‚Äîand the nightmare workplace culture of a pig butchering organization.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs a slave colony that‚Äôs trying to pretend it‚Äôs a company,‚Äù says Erin West, a former Santa Clara County, California, prosecutor who leads an anti-scam organization called Operation Shamrock and who reviewed the chat logs obtained by WIRED. Another researcher who reviewed the leaked chat logs, Jacob Sims of Harvard University‚Äôs Asia Center, also remarked on their ‚ÄúOrwellian veneer of legitimacy.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs terrifying, because it‚Äôs manipulation and coercion,‚Äù says Sims, who studies Southeast Asian scam compounds. ‚ÄúCombining those two things together motivates people the most. And it‚Äôs one of the key reasons why these compounds are so profitable.‚Äù&lt;/p&gt;
    &lt;p&gt;In another chat message, sent within hours of Amani‚Äôs saccharine pep talk, a higher-level boss weighed in: ‚ÄúDon't resist the company's rules and regulations,‚Äù he wrote. ‚ÄúOtherwise you can't survive here.‚Äù The staffers responded with 26 emoji reactions, all thumbs-ups and salutes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fined Into Slavery&lt;/head&gt;
    &lt;p&gt;In total, according to WIRED‚Äôs analysis of the group chat, more than 30 of the compound‚Äôs workers successfully defrauded at least one victim in the 11 weeks of records available, totaling to around $2.2 million in stolen funds. Yet the bosses in the chat frequently voiced their disappointment in the group‚Äôs performance, berated the staff for lack of effort, and imposed fine after fine.&lt;/p&gt;
    &lt;p&gt;Rather than explicit imprisonment, the compound relied on a system of indentured servitude and debt to control its workers. As Muzahir described it, he was paid a base salary of 3,500 Chinese yuan a month (about $500), which in theory entailed 75 hours a week of night shifts including breaks to eat. Although his passport had been taken from him, he was told that if he could pay off his ‚Äúcontract‚Äù with a $5,400 payment, it would be returned to him and he would be allowed to leave.&lt;/p&gt;
    &lt;p&gt;In reality, the WhatsApp chats reveal how even that meager salary was almost entirely chipped away with fines. One message warns that anyone who fails to start a ‚Äúfirst chat‚Äù‚Äîan introductory conversation with a scam victim‚Äîon any given day will be fined 50 yuan, and the failure will be announced to the group. Filing a false progress report results in a fine of 1,000 yuan. Falling asleep in the office, or ‚Äúwatching unrelated video, chatting with friends, and any activity that is not related to the job‚Äù are each punishable with a 200 yuan fine, as is any ‚Äúdisturbance‚Äù in the dormitory, where workers sleep five or six to a room in bunk beds.&lt;/p&gt;
    &lt;p&gt;One message notes a fine of 500 yuan for a worker who slept late, and another fined 200 yuan for not being in the dorm at ‚Äúcheck-in time‚Äù following his shift. Resist a fine by not signing a form that admits to the misbehavior, and the fine is doubled.&lt;/p&gt;
    &lt;p&gt;Muzahir himself described being fined so much that he was virtually broke. The food in the office cafeteria was also frequently denied as a punishment, the messages showed, with workers‚Äô ID badges that granted access to the canteen sometimes being taken away for seven days for small infractions like tardiness. Even the freedom to bring in snacks and drinks‚Äîother than betel nuts, a stimulant‚Äîcould be rescinded if staff underperformed. Time off was also withheld, with staff sometimes forced to work seven nights a week, Muzahir says.&lt;/p&gt;
    &lt;p&gt;Yet those punishments could be avoided, the bosses frequently promised, if they successfully scammed someone‚Äîor ‚Äúopened a customer,‚Äù as the bosses euphemistically described scamming a new victim. (Scamming the same victim multiple times was called a ‚Äúrecharge.‚Äù) In theory, workers were entitled to a commission, over and above their salary, for any scams they pulled off. Muzahir says he successfully perpetrated two scams during his months in the compound‚Äîboth of which left him racked with regret, he says‚Äîand he was never paid after either of them.&lt;/p&gt;
    &lt;p&gt;Bosses nonetheless used workers‚Äô illusory hope of paying off their debt‚Äîor even going home rich‚Äîas a motivator. ‚ÄúI understand‚Äîwhen penalties or fines come your way, it's easy to feel disheartened. But I urge you not to see it as a punishment, but as a lesson and an investment in your own growth,‚Äù wrote Amani. ‚ÄúDon't fear the fine. Let it fuel your fire.‚Äù&lt;/p&gt;
    &lt;p&gt;The more senior boss, who went by the name Da Hai, spelled out the carrot-and-stick approach more clearly. ‚ÄúThe company's incentives are much higher than the fines, so as long as you work hard to open new customers you will receive a generous reward!‚Äù he wrote.&lt;/p&gt;
    &lt;p&gt;One of the bosses‚Äô tactics was to play teams off one another, reprimanding underperforming workers while pointing to the success of other scammers in the compound. Each room of the office appears to have had a Chinese ceremonial drum, played when a worker successfully scammed a victim for a six-figure sum. ‚ÄúDo you know why the next office is beating drums?‚Äù wrote a higher-level boss called Alang.&lt;/p&gt;
    &lt;p&gt;A victim had paid ‚Äú480k,‚Äù a boss who goes by the name Libo answers.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt doesn't matter, because he belongs to others,‚Äù Alang responds. ‚ÄúThe important thing is, which one of you can play the drum?‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the Pretense, a Brutal Reality&lt;/head&gt;
    &lt;p&gt;Beyond these manipulative tactics, the messages occasionally offer glimpses of a far harsher reality‚Äîas does the personal experience and testimony of Muzahir himself. Muzahir describes hearing stories of people who were tortured and says he was himself threatened by Amani with beating and electrocution if he didn‚Äôt find new ‚Äúclients.‚Äù Sometimes coworkers disappeared without explanation.&lt;/p&gt;
    &lt;p&gt;Eventually Muzahir came up with a plan to trick his captors into letting him leave. When the bosses caught on, he was held in a room, beaten, slapped and kicked, denied food and water, and made to drink a solution with a white powder dissolved in it, which seems to have been intended to make him more cooperative with their interrogation.&lt;/p&gt;
    &lt;p&gt;Occasional messages in the chat logs hint that these cruel punishments lurked underneath the compound‚Äôs motivational messages. At one point, the boss Alang mentions a girl who ‚Äúsneaked away from the company and went to work in a brothel,‚Äù and another person in the group mentions that the ‚Äúcompany‚Äù still holds her passport. Among the captive workers, Muzahir says, rumor had it that the girl was in fact sold into prostitution, a practice documented in other accounts from scam compound survivors.&lt;/p&gt;
    &lt;p&gt;At another point, while chastising the group for underperformance, the boss Da Hai hints at the large sum of money workers needed to produce if they ever hoped to leave the compound. ‚ÄúYou continue to violate the company's regulations,‚Äù he writes to the group. ‚ÄúIf you continue like this, please prepare your compensation and get out of here.‚Äù&lt;/p&gt;
    &lt;p&gt;Such references to paying ‚Äúcompensation‚Äù for release are in fact ‚Äúcoded words for ransom and debt bondage,‚Äù says Harvard‚Äôs Sims. The nation of Laos, Sims points out, is a signatory to the Palermo Protocol, which classifies anyone held in debt and forced to work without freedom of movement a victim of human trafficking. ‚ÄúThere is no gray area here.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;A Day in the Life of a Scammer&lt;/head&gt;
    &lt;p&gt;The leaked WhatsApp chats include a message from a boss who went by the name Terry laying out a strict work schedule for those under his supervision. ‚ÄúObey and respect the working time,‚Äù the message says. Each shift would start at around 11:30 pm Beijing time‚Äî10:30 pm in Laos‚Äîwith people told to arrive a few minutes early. Before the day ended at 2 pm Beijing time, there would be two break periods, one of which was set aside for meals. By 5 pm everyone was required to be back in their dormitories and ‚Äúsleep or keep silence, no disturbing the others.‚Äù If the rules weren‚Äôt followed, fines would be issued and ID badges could be taken away.&lt;/p&gt;
    &lt;p&gt;The reason for this nocturnal schedule was to sync with the waking hours of victims in the US‚Äîalmost entirely Indian-American men. (It‚Äôs a common practice to pair scammers with victims of their own ethnicity, to avoid language and culture barriers.)&lt;/p&gt;
    &lt;p&gt;In grim contrast to their actual lives, all staffers were required to post an imaginary daily schedule for their fake personas‚Äîthe wealthy, attractive women they‚Äôd pretend to be during scams. In hour-by-hour breakdowns, they describe mornings spent meditating, practicing yoga, taking walks, and ‚Äúsetting positive intentions‚Äù for the day. Other activities include a ‚Äúrelaxed‚Äù lunch with their team, dinner with loved ones, and time at the gym‚Äîwhen in reality they were spending entire nights in front of a screen in a fluorescent-lit office space.&lt;/p&gt;
    &lt;p&gt;Many of the staffers writing the schedules were nonetheless admonished for not sticking to the script while scamming. ‚ÄúThe purpose of editing a daily plan is to let everyone know clearly what you are going to share with your clients today when you start working,‚Äù one boss complained. ‚ÄúI find that many people just do it to get the job done and don't apply your plan to your clients.‚Äù&lt;/p&gt;
    &lt;p&gt;During each day‚Äôs work, the forced scammers were also required‚Äîunder the threat of more fines‚Äîto report their scamming efforts back to the bosses in detail. The WhatsApp logs are filled with lengthy messages from every team member that offer those reports in identical message templates, listing their ‚Äúteam,‚Äù their name, and their recent online activity with the fake profiles. They would report how many active social media accounts they were operating, if any of their accounts were suspended, how many chats they‚Äôd started, how many were ongoing, any successful scams, and their target for the month. The internal chats also show scammers sharing with bosses and colleagues screenshots of their victim chats on Facebook Messenger, Instagram, Snapchat, and other chat apps, while asking questions about potential victims.&lt;/p&gt;
    &lt;p&gt;Bosses frequently gave pointed feedback about how workers were managing the meta-narrative of their scams. ‚ÄúWhen sharing travel topics, you need to know how to share details,‚Äù one chat says. Another message from a boss admonishes workers not to mention the car their persona drives if they can‚Äôt provide a convincing photo of it.&lt;/p&gt;
    &lt;p&gt;Managers would keep a close eye on the activity. On multiple occasions, bosses ask the forced workers to connect their WhatsApp accounts to the managers‚Äô computers so they could monitor the conversations themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Scam&lt;/head&gt;
    &lt;p&gt;The 25 scripts and guides Muzahir shared with WIRED, too, offer a window into the tactics and training of the compound‚Äôs workers. Many of the guidance documents pertain to the nitty gritty of carrying out cryptocurrency investment scams, including how to build a friendship that can segue into an investment proposition, how to explain what cryptocurrency is, and what to do once a target agrees to make an investment.&lt;/p&gt;
    &lt;p&gt;One document lists ‚Äú100 chat topics,‚Äù geared toward building the emotional intimacy required for a romance scam (‚ÄúWhat was your dream when you were little?‚Äù ‚ÄúWhat was the last time I cried for?‚Äù). Another suggests providing an update about having gotten into a car accident. ‚ÄúOn my way to work in the morning, my car was hit by a car following at a traffic light, which almost delayed my meeting in the morning. Thank you for your concern. I am fine.‚Äù&lt;/p&gt;
    &lt;p&gt;Multiple documents guide scammers to pretend they are currently making an investment, then introduce the idea that banks are resistant to letting their customers convert their money into cryptocurrency. ‚ÄúIf we transfer or withdraw funds, they will have one less customer,‚Äù one proposed scam script says. ‚ÄúIf everyone does this, then the bank will be in crisis and there will be a situation of capital rupture. I can understand their motives, but as a bank customer, I should not be hindered from transferring assets reasonably and legally. This is what makes me angry.‚Äù&lt;/p&gt;
    &lt;p&gt;The documents also display a technique that researchers say is often used in Southeast Asian investment and romance scams: Attackers intentionally mention the concept of scams‚Äîeven directly talking about the threat of investment scams‚Äîas a way of inoculating themselves against suspicion. The idea is that if a person is willing to talk openly about scams and isn‚Äôt avoiding the subject or acting strange about it, then they couldn‚Äôt be a scammer themself.&lt;/p&gt;
    &lt;p&gt;That strategy goes so far as to include mentally preparing a victim for the anti-fraud warnings from their bank or even law enforcement that they may have to ignore in order to transfer large amounts of fiat currency into cryptocurrency. ‚ÄúI was going to transfer funds to my coinbase today, but I was deliberately delayed and obstructed by the bank staff,‚Äù one script reads, referring to the popular crypto wallet service Coinbase. ‚ÄúI also received an anti-fraud call from the FBI today, which wasted a lot of my time.‚Äù&lt;/p&gt;
    &lt;p&gt;The materials Muzahir provided from the Boshang compound also document the key role generative AI tools play in its deceptions. Muzahir described to WIRED how the compound workers are trained in using tools like ChatGPT and Deepseek to come up with responses in chats with victims and craft natural-sounding turns of phrase. But even more crucial was the compound‚Äôs use of deepfake AI software to allow scammers to convincingly video chat with victims at their request using an AI-generated face, impersonating an individual whose photos they‚Äôve stolen for a fake persona.&lt;/p&gt;
    &lt;p&gt;The internal chat logs Muzahir captured describe a dedicated ‚ÄúAI room‚Äù where a female model conducts face-swapped calls on request with an endless parade of victims. One WhatsApp message from a boss to the group chat notes that ‚ÄúSana (our model who helps us to call) is not available tonight. she is not feeling well. Therefore, don't promise your customers to call them. Maybe she will come at work in the morning. Plan your work accordin[g]ly.‚Äù&lt;/p&gt;
    &lt;p&gt;Other chats about the AI room relate to scheduling challenges given demand for face-swapped calls and the fact that a single model can only do one deepfake call at a time. One chat, for example, notes: ‚ÄúIf there is a ‚Äòbusy‚Äô sign on her door, change it to ‚Äòfree‚Äô when you come out, so as to avoid crowding and frequent door openings.‚Äù&lt;/p&gt;
    &lt;p&gt;The scripts Muzahir shared also include tips for delaying a video chat with a victim‚Äîperhaps until the scammer is prepared to use deepfake tools. ‚ÄúWhen we meet, it will not be awkward but rather we will look forward to it,‚Äù says one script about what to say when a victim asks to video chat. It continues, ‚ÄúWe are strengthening our relationship every day. You have also seen my photos. When we meet, can you recognize me?‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond the Golden Triangle&lt;/head&gt;
    &lt;p&gt;As dystopian as the Golden Triangle compound described in the leaked documents may be, its work environment appears to have been relatively lax compared to other compounds in countries like Cambodia or Myanmar. In those facilities, Operation Shamrock‚Äôs Erin West says, she has heard firsthand stories of workers being beaten simply for missing their quota of scams or being forced to work 18-hour shifts while standing, with none of the pretense of voluntary work in a corporate environment.&lt;/p&gt;
    &lt;p&gt;The relative leniency of Muzahir‚Äôs compound, says Harvard‚Äôs Sims, likely stems from scam operations‚Äô sense of total control in Laos‚Äô Golden Triangle region‚Äîa zone of the country controlled largely by Chinese business interests that has become a host to crimes ranging from narcotics and organ sales to illegal wildlife trafficking. Even human trafficking victims who escape from a compound there, Sims points out, can be tracked down relatively easily thanks to Chinese organized crime‚Äôs influence over local law enforcement. ‚ÄúThese guys don‚Äôt have to be held in a cell,‚Äù Sims says. ‚ÄúThe whole place is a closed circuit.‚Äù&lt;/p&gt;
    &lt;p&gt;Nonetheless, the Boshang compound that held Muzahir appears to have moved in November from the Golden Triangle to Cambodia, a country that‚Äôs become by some measures an even safer base for scammers to operate from. Based on messages from his former coworkers, Muzahir says he‚Äôs determined that the operation and its captive workers are now based in the town of Chrey Thom, what Sims and West both describe as a growing hot spot for scam operations.&lt;/p&gt;
    &lt;p&gt;The move may have been precipitated, Sims speculates, by police raids on compounds across the region around that time. Many of those raids appear to have been part of a ‚Äúperformative crackdown,‚Äù as Sims puts it. (One such raid in June targeted the building where Muzahir‚Äôs compound had previously been located, but Muzahir says the workers who were rounded up by police were quickly released again and returned to work.)&lt;/p&gt;
    &lt;p&gt;Nonetheless, the nuisance of even those superficial disruptions may have persuaded the operation‚Äôs bosses to relocate to Cambodia. In that country, even the family of the country‚Äôs prime minister, Hun Manet, has been linked to a corporate conglomerate that oversees a subsidiary with documented ties to the burgeoning scam industry. ‚ÄúIt‚Äôs been a very hospitable environment to do this work,‚Äù West says.&lt;/p&gt;
    &lt;p&gt;One of Muzahir‚Äôs old bosses also confirmed to him in a private text exchange that the compound is still ‚Äúrecruiting‚Äù new workers‚Äîvictims trapped in a system of modern slavery hidden under a thin facade of a willing workplace.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis is a place to work, not to enjoy,‚Äù that same boss had written in the group chat during Muzahir‚Äôs time in the compound, in a rare moment when the mask of a normal office environment seemed to slip. ‚ÄúYou can only enjoy life when you leave here.‚Äù&lt;/p&gt;
    &lt;p&gt;Additional reporting by Sophia Takla, Maddy Varner, and Zeyi Yang.&lt;/p&gt;
    &lt;p&gt;Let us know what you think about this article. Submit a letter to the editor at mail@wired.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46852660</guid><pubDate>Mon, 02 Feb 2026 05:10:59 +0000</pubDate></item><item><title>Library of Juggling</title><link>https://libraryofjuggling.com/</link><description>&lt;doc fingerprint="3736594b20b5a3ff"&gt;
  &lt;main&gt;
    &lt;p&gt;What is it?&lt;/p&gt;
    &lt;p&gt;The Library of Juggling is an attempt to list all of the popular (and perhaps not so popular) juggling tricks in one organized place. Despite the growing popularity of juggling, few websites are dedicated to collecting and archiving the various patterns that are being performed. Most jugglers are familiar with iconic tricks such as the Cascade and Shower, but what about Romeo's Revenge or the 531 Mills Mess? The goal of this website is to guarantee that the tricks currently circulating around the internet and at juggling conventions are found, animated, and catalogued for the world to see. It is a daunting task, but for the sake of jugglers everywhere it must be done.&lt;/p&gt;
    &lt;p&gt;What can I find here?&lt;/p&gt;
    &lt;p&gt;For every trick found in the Library, there will be an animated representation of the pattern created via JugglingLab, in addition to general information about the trick (siteswap, difficulty level, prerequisite tricks, etc.). If I am able to run the pattern, then I will provide a text-based tutorial for the trick with the help of animations. I will also include links to other tutorials for the trick that can be found online, ranging from YouTube videos to private sites like this one. If I am unable to provide my own tutorial, there will still be a short description of the trick in addition to outside tutorials and demonstrations.&lt;/p&gt;
    &lt;p&gt;Where do I start?&lt;/p&gt;
    &lt;p&gt;Well, if you have come to the Library looking to find out how to start juggling, than it would be best to begin with the Three Ball Cascade pattern. If you are a juggler who is already familiar with the basics, then the various tricks included in the Library can be accessed via the navigation tree on the left, or you can click here to view all of the tricks by difficulty.&lt;/p&gt;
    &lt;p&gt;Recent Additions (6/13/15):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frostbite (submitted by Andrew Olson)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Library of Juggling is on an indefinite hiatus, which means no new tricks will be added. Existing content will continue to be hosted for the foreseeable future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46853552</guid><pubDate>Mon, 02 Feb 2026 07:58:49 +0000</pubDate></item><item><title>EU launches government satcom program in sovereignty push</title><link>https://spacenews.com/eu-launches-government-satcom-program-in-sovereignty-push/</link><description>&lt;doc fingerprint="95231d6d282ee8d"&gt;
  &lt;main&gt;
    &lt;p&gt;BRUSSELS ‚Äî The European Union‚Äôs new government satellite communications program, GOVSATCOM, which pools capacity from eight already on-orbit geosynchronous satellites, began operations last week, European Commissioner for Defence and Space Andrius Kubilius said Jan. 27.&lt;/p&gt;
    &lt;p&gt;The program is designed to provide secure communications capabilities to the EU and its member states and could expand by 2027, Kubilius said.&lt;/p&gt;
    &lt;p&gt;‚ÄúLast week we started GOVSATCOM operations,‚Äù Kubilius said during his opening remarks at the European Space Conference. ‚ÄúThat means that all member states can now have access to sovereign satellite communications ‚Äî military and government, secure and resilient, built in Europe, operated in Europe, and under European control.‚Äù&lt;/p&gt;
    &lt;p&gt;Specifically, Kubilius was referring to the GOVSATCOM hub, a ‚Äúmarketplace‚Äù of governmental capacities from the five resource providers currently enrolled in the program. Juan Ramon Lopez Caravantes, head of communication at the European Union Agency for the Space Programme, said that with ‚Äùa few clicks member states can now introduce their service request. It‚Äôs an easy to use, smooth running secure platform‚Äù.&lt;/p&gt;
    &lt;p&gt;GOVSATCOM is conceived as a ‚Äúsystem of systems,‚Äù merging existing national and commercial satellite capacities into a common EU pool. The program is structured in multiple phases, and is pooling capacity from eight existing, already-in-orbit GEO satellites from five member states ‚Äî France, Spain, Italy, Greece and Luxembourg.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey currently offer 35 different service programs from a catalogue that is not public. It‚Äôs only for the member states, and it‚Äôs fully secured and encrypted‚Äù added Jeremie Godet, head of unit secure connectivity and space surveillance at European Commission‚Äôs Directorate General office for defence industry and space. ‚ÄúThe coverage is currently from the south of Greenland to South America on the west and up to India on the East.‚Äù&lt;/p&gt;
    &lt;p&gt;In 2027 this catalog will expand, Godet added, to fill gaps and to secure more commercial satcom solutions.&lt;/p&gt;
    &lt;p&gt;Beginning in 2029, GOVSATCOM is expected to integrate with the 290 satellites in the Infrastructure for Resilience, Interconnectivity and Security by Satellite constellation, known as IRIS¬≤, and be fully operational.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe goal‚Äù, Kubilius said during the conference ‚Äúis to have expanded commercial capabilities operational by 2027, including expanding coverage, and expanding bandwidth to cover the entire world,‚Äù and to support IRIS¬≤ by 2029.&lt;/p&gt;
    &lt;p&gt;Concerning IRIS2, the commissioner also said that IRIS¬≤ Ka-band military frequencies were brought into use last week. He expressed confidence that the first batch of satellites will be ready for deployment by 2029. ‚ÄúI have asked all partners to step up and speed up IRIS¬≤,‚Äúand I‚Äôm confident we can deploy its initial services by 2029,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe goal is connectivity and security for all of Europe ‚Äî guaranteed access for all member states and full European control.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46853888</guid><pubDate>Mon, 02 Feb 2026 08:59:40 +0000</pubDate></item><item><title>My fast zero-allocation webserver using OxCaml</title><link>https://anil.recoil.org/notes/oxcaml-httpz</link><description>&lt;doc fingerprint="16bf138b60e4e358"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;My (very) fast zero-allocation webserver using OxCaml / Feb 2026 / DOI&lt;/head&gt;
    &lt;p&gt;Since helping with the &lt;/p&gt;
    &lt;p&gt;The reason for my eagerness is that OxCaml has a number of language extensions that give giant leaps in performance for systems-oriented programs, while retaining the familiar OCaml functional style of programming. And unlike Rust, there's a garbage collector available for 'normal' code. I am also deeply sick and tired of maintaining large Python scripts recently, and crave the modularity and type safety of OCaml.&lt;/p&gt;
    &lt;p&gt;The traditional way I learn a new technology is by replacing my &lt;/p&gt;
    &lt;p&gt;(Many thanks to &lt;/p&gt;
    &lt;head rend="h2"&gt;Why Zero Allocation for HTTP/1.1?&lt;/head&gt;
    &lt;p&gt;httpz is a high-performance HTTP/1.1 parser that aims to have no major heap allocation, and very minimal minor heap allocation, by using OxCaml's unboxed types and local allocations.&lt;/p&gt;
    &lt;p&gt;Why is this useful? It means that the entire lifetime of an HTTP connection can be handled in the callstack alone, so freeing up a connection is just a matter of returning from the function that handles it. In the steady state, a webserver would have almost no garbage collector activity. When combined with &lt;/p&gt;
    &lt;p&gt;I decided to specialise this library for HTTP/1.1 for now, and so settled on the input being a simple 32KB bytes value. This represents an HTTP request with the header portion (HTTP body handling is relatively straightforward for POST requests, and not covered in this post).&lt;/p&gt;
    &lt;p&gt;Given an input buffer like this, what can we do with OxCaml vs vanilla OCaml to make this go fast?&lt;/p&gt;
    &lt;head rend="h3"&gt;Unboxed Types and Records&lt;/head&gt;
    &lt;p&gt;The first port of call is to figure out the core types we're going to use for our parser. If you need to get familiar with OCaml's upstream memory representation then head over to Real World OCaml.&lt;/p&gt;
    &lt;p&gt;In my usual OCaml code, I use libraries like cstruct that I &lt;/p&gt;
    &lt;code&gt;type buffer = (char, Bigarray.int8_unsigned_elt, Bigarray.c_layout) Bigarray.Array1.t
type Cstruct.t = private {
  buffer: buffer;
  off   : int;
  len   : int;
}
&lt;/code&gt;
    &lt;p&gt;The idea is to use the record to get narrow views into a larger buffer, and that these small views can just live on the minor heap of the runtime which is fast to collect. OxCaml advances this by providing unboxed versions of small numbers that live in registers or on the stack, via a new syntax &lt;code&gt;int16#&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Instead of Bigarrays, we're now going to switch to use &lt;code&gt;bytes&lt;/code&gt; instead, but the
basic idea is the same.  Since httpz's buffer is a max of 32KB, 16-bit integers
also suffice for all positions and lengths!&lt;/p&gt;
    &lt;code&gt;type Httpz.t = #{ off : int16# ; len : int16# }
&lt;/code&gt;
    &lt;p&gt;There are actually two new features here: the first is that records can be unboxed with the &lt;code&gt;#{}&lt;/code&gt;
syntax, and the contents themselves are of a smaller width.  Let's have a closer look
at the difference between the Cstruct boxed version and this new OxCaml one:&lt;/p&gt;
    &lt;head rend="h4"&gt;Inspect unboxing in utop&lt;/head&gt;
    &lt;p&gt;My first port-of-call is usually to use utop interactively to poke around using the &lt;code&gt;Obj&lt;/code&gt; module.  This isn't quite so easy in OxCaml since the unboxed
records use a special layout:&lt;/p&gt;
    &lt;code&gt;# type t = #{ off : int16# ; len : int16# };;
type t = #{ off : int16#; len : int16#; }

# let x = #{ off=#1S; len=#2S };;
val x : t = #{off = &amp;lt;abstr&amp;gt;; len = &amp;lt;abstr&amp;gt;}

# Obj.repr x;;
Error: This expression has type t but an expression was expected of type
         ('a : value)
       The layout of t is bits16 &amp;amp; bits16
         because of the definition of t at line 1, characters 0-41.
       But the layout of t must be a sublayout of value.

&lt;/code&gt;
    &lt;p&gt;That failed, but it did reveal that we have this intriguing int16 pair layout instead of the normal OCaml flat value representation! Let's use the compiler to figure this out...&lt;/p&gt;
    &lt;head rend="h4"&gt;Inspect unboxing in lambda&lt;/head&gt;
    &lt;p&gt;I next built a small test program and inspected the lambda intermediate language from the compiler. To avoid dependencies, I just bound the raw compiler internals directly by checking out the oxcaml source code.&lt;/p&gt;
    &lt;code&gt;external add_int16 : int16# -&amp;gt; int16# -&amp;gt; int16# = "%int16#_add"
external int16_to_int : int16# -&amp;gt; int = "%int_of_int16#"

type span = #{ off : int16#; len : int16# }

let[@inline never] add_spans (x : span) (y : span) : span =
  #{ off = add_int16 x.#off y.#off; len = add_int16 x.#len y.#len }

let () =
  let x = Sys.opaque_identity #{ off = #1S; len = #2S } in
  let y = Sys.opaque_identity #{ off = #100S; len = #200S } in
  let z = add_spans x y in
  Printf.printf "off=%d len=%d\n" (int16_to_int z.#off) (int16_to_int z.#len)
&lt;/code&gt;
    &lt;p&gt;This introduces enough compiler optimisation barriers such that the addition is not optimised away at compile time. We can compile this with &lt;code&gt;ocaml -dlambda src.ml&lt;/code&gt; and see the intermediate form after type checking:&lt;/p&gt;
    &lt;code&gt;(let
  (add_spans/290 =
     (function {nlocal = 0} x/292[#(int16, int16)] y/293[#(int16, int16)]
       never_inline : #(int16, int16)
       (funct-body add_spans ./x.ml(6)&amp;lt;ghost&amp;gt;:196-294
         (before add_spans ./x.ml(7):229-294
           (make_unboxed_product #(int16, int16)
             (%int16#_add (unboxed_product_field 0 #(int16, int16) x/292)
               (unboxed_product_field 0 #(int16, int16) y/293))
             (%int16#_add (unboxed_product_field 1 #(int16, int16) x/292)
               (unboxed_product_field 1 #(int16, int16) y/293)))))))
&lt;/code&gt;
    &lt;p&gt;You can see the unboxing propagating nicely here through the intermediate code!&lt;/p&gt;
    &lt;head rend="h4"&gt;Inspect unboxing in native code&lt;/head&gt;
    &lt;p&gt;The next step is to verify what this looks like when compiled as optimised native code. I used &lt;code&gt;ocamlopt -O3 -S&lt;/code&gt; on my arm64 machine which emits the assembly code
after all the compiler passes, and found:&lt;/p&gt;
    &lt;code&gt;In the entry point:
  orr   x0, xzr, #1      ; x.#off = 1
  orr   x1, xzr, #2      ; x.#len = 2
  movz  x2, #100, lsl #0 ; y.#off = 100
  movz  x3, #200, lsl #0 ; y.#len = 200
  bl    _camlX__add_spans_0_1_code

_camlX__add_spans_0_1_code:
  add   x1, x1, x3       ; len: x.#len + y.#len
  sbfm  x1, x1, #0, #15  ; sign-extend to 16 bits (int16# semantics)
  add   x0, x0, x2       ; off: x.#off + y.#off
  sbfm  x0, x0, #0, #15  ; sign-extend to 16 bits
  ret

&lt;/code&gt;
    &lt;p&gt;We can see from the assembly that there's no boxing, and no heap allocations, and the sbfm instruction maintains the 16-bit semantics via sign extension.&lt;/p&gt;
    &lt;p&gt;Let's double check that the normal boxed OCaml does do more work and that isn't just the flambda2 compiler doing its magic. Here's a boxed version of the benchmark using plain OCaml:&lt;/p&gt;
    &lt;code&gt;type span = { off : int; len : int }

let[@inline never] add_spans (x : span) (y : span) : span =
  { off = x.off + y.off; len = x.len + y.len }

let () =
  let x = Sys.opaque_identity { off = 1; len = 2 } in
  let y = Sys.opaque_identity { off = 100; len = 200 } in
  let z = add_spans x y in
  Printf.printf "off=%d len=%d\n" z.off z.len
&lt;/code&gt;
    &lt;p&gt;Compiling this boxed version with &lt;code&gt;ocamlopt -O3 -S&lt;/code&gt; and looking at the assembly shows
much more minor heap activity:&lt;/p&gt;
    &lt;code&gt;_camlY__add_spans_0_1_code:
      sub   sp, sp, #16
      str   x30, [sp, #8]
      mov   x2, x0
      ldr   x16, [x28, #0]        ; load young_limit
      sub   x27, x27, #24         ; bump allocator: reserve 24 bytes (3 words)
      cmp   x27, x16              ; check if GC needed
      b.cc  L114                  ; branch to GC if out of space
  L113:
      add   x0, x27, #8           ; x0 = pointer to new block
      orr   x3, xzr, #2048        ; header word (tag 0, size 2)
      str   x3, [x0, #-8]         ; write header
      ldr   x3, [x1, #0]          ; load y.off from heap
      ldr   x4, [x2, #0]          ; load x.off from heap
      add   x3, x4, x3            ; add them
      sub   x3, x3, #1            ; adjust for tagged int
      str   x3, [x0, #0]          ; store result.off to heap
      ldr   x1, [x1, #8]          ; load y.len from heap
      ldr   x2, [x2, #8]          ; load x.len from heap
      add   x1, x2, x1            ; add them
      sub   x1, x1, #1            ; adjust for tagged int
      str   x1, [x0, #8]          ; store result.len to heap
      ...
      ret
  L114:
      bl    _caml_call_gc         ; GC call if needed
&lt;/code&gt;
    &lt;p&gt;The OCaml minor heap is really fast, but it's nowhere near as fast as just passing values around in registers and doing direct operations, which the unboxed version lets us do!&lt;/p&gt;
    &lt;p&gt;My benchmark above used direct external calls to compiler primitives, but OxCaml exposes normal modules for all these special types so we can just open them and gain access to the usual integer operations:&lt;/p&gt;
    &lt;code&gt;module I16 = Stdlib_stable.Int16_u

let[@inline always] i16 x = I16.of_int x
let[@inline always] to_int x = I16.to_int x

let pos : int16# = i16 0
let next : int16# = I16.add pos #1S
&lt;/code&gt;
    &lt;head rend="h3"&gt;Unboxed characters&lt;/head&gt;
    &lt;p&gt;There's more than just integer operations in OxCaml. Hot off the press in the past few weeks have been unboxed character operations as well, so we don't need to use an OCaml int (this is unboxed as well, but I presume the compiler can optimise and pack 8-bit operations much more effectively if it knows that we're operating on a char instead of a full word).&lt;/p&gt;
    &lt;p&gt;The httpz parser tries to use these, but the support for untagged ints isn't fully complete yet (thanks &lt;/p&gt;
    &lt;p&gt;HTTP date timestamps use unboxed floats as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning unboxed records and tuples&lt;/head&gt;
    &lt;p&gt;Once we've declared these unboxed records, they're fully nestable within other unboxed records. For example, HTTP requests with multiple fields remain unboxed:&lt;/p&gt;
    &lt;code&gt;type request =
  #{ meth : method_
   ; target : span           (* Nested unboxed record *)
   ; version : version
   ; body_off : int16#
   ; content_length : int64#
   ; is_chunked : bool
   ; keep_alive : bool
   ; expect_continue: bool
   }
&lt;/code&gt;
    &lt;p&gt;Functions can therefore naturally return multiple values without allocation by using unboxed tuples in the return value of a function:&lt;/p&gt;
    &lt;code&gt;let take_while predicate buf ~(pos : int16#) ~(len : int16#)
    : #(span * int16#) =
  let start = pos in
  let mutable p = pos in
  while (* ... *) do p &amp;lt;- I16.add p #1S done;
  #(#{ off = start; len = I16.sub p start }, p)

let #(result_span, new_pos) = take_while is_token buf ~pos ~len
&lt;/code&gt;
    &lt;p&gt;Vanilla OCaml did some unboxing of this use of tuples, but not with records (which would land up on the minor heap). With this OxCaml code, it's all just passed directly on the stack through function call traces.&lt;/p&gt;
    &lt;head rend="h3"&gt;Local allocations and exclaves&lt;/head&gt;
    &lt;p&gt;We can then also mark parameters to demand that they won't escape a function, enabling stack allocation more explicitly:&lt;/p&gt;
    &lt;code&gt;(* Buffer is borrowed, won't be stored anywhere *)
let[@inline] equal (local_ buf) (sp : span) (s : string) : bool =
  let sp_len = I16.to_int sp.#len in
  if sp_len &amp;lt;&amp;gt; String.length s then false
  else Bigstring.memcmp_string buf ~pos:(I16.to_int sp.#off) s = 0
&lt;/code&gt;
    &lt;p&gt;If a function needs to return a local value, then it uses a new &lt;code&gt;exclave_&lt;/code&gt; keyword. For example, in the HTTP request parsing we look up a stack allocated list of headers:&lt;/p&gt;
    &lt;code&gt;val find : t list @ local -&amp;gt; Name.t -&amp;gt; t option @ local

let rec find_string (buf : bytes) (headers : t list @ local) name = exclave_
  match headers with
  | [] -&amp;gt; None
  | hdr :: rest -&amp;gt;
    let matches =
      match hdr.name with
      | Name.Other -&amp;gt; Span.equal_caseless buf hdr.name_span name
      | known -&amp;gt;
        let canonical = Name.lowercase known in
        String.( = ) (String.lowercase name) canonical
    in
    if matches then Some hdr else find_string buf rest name
;;
&lt;/code&gt;
    &lt;p&gt;Notice that it's a recursive function as well, so this is a fairly natural way to write something that remains heap allocated. You can learn more about this from &lt;/p&gt;
    &lt;head rend="h2"&gt;Mutable Local Variables with "let mutable"&lt;/head&gt;
    &lt;p&gt;A nice quality of life improvement is that OxCaml allows stack-allocated mutable variables in loops, eliminating the need to allocate &lt;code&gt;ref&lt;/code&gt; values. This
allows parsing code to have local mutability:&lt;/p&gt;
    &lt;code&gt;let parse_int64 (local_ buf) (sp : span) : int64# =
  let mutable acc : int64# = #0L in
  let mutable i = 0 in
  let mutable valid = true in
  while valid &amp;amp;&amp;amp; i &amp;lt; I16.to_int sp.#len do
    let c = Bytes.get buf (I16.to_int sp.#off + i) in
    match c with
    | '0' .. '9' -&amp;gt;
      acc &amp;lt;- I64.add (I64.mul acc #10L) (I64.of_int (Char.code c - 48));
      i &amp;lt;- i + 1
    | _ -&amp;gt; valid &amp;lt;- false
  done;
  acc
&lt;/code&gt;
    &lt;p&gt;Whereas in conventional OCaml there might be a minor heap allocation for the reference:&lt;/p&gt;
    &lt;code&gt;let parse_int64 buf sp =
  let acc = ref 0L in           (* Heap-allocated ref *)
  let i = ref 0 in              (* Heap-allocated ref *)
  let valid = ref true in       (* Heap-allocated ref *)
  while !valid &amp;amp;&amp;amp; !i &amp;lt; sp.len do
    let c = Bytes.get buf (sp.off + !i) in
    match c with
    | '0' .. '9' -&amp;gt;
      acc := Int64.add (Int64.mul !acc 10L) (Int64.of_int (Char.code c - 48));
      i := !i + 1
    | _ -&amp;gt; valid := false
  done;
  !acc
&lt;/code&gt;
    &lt;head rend="h3"&gt;Putting the parser together&lt;/head&gt;
    &lt;p&gt;The toplevel Httpz.parse function has a pretty simple signature from a user's perspective:&lt;/p&gt;
    &lt;code&gt;val parse : bytes -&amp;gt; len:int16# -&amp;gt; limits:limits -&amp;gt;
  #(Buf_read.status * Req.t * Header.t list) @ local
&lt;/code&gt;
    &lt;p&gt;This function receives some a bytebuffer and resource limits and returns an unboxed local tuple of the connection status, parsed (unboxed) request and a stack-local list of header spans that represent the offsets within the input buffer of what was passed.&lt;/p&gt;
    &lt;p&gt;I should probably make the input buffer local too; one nice aspect of OxCaml is how easy it is to incrementally add type and kind annotations and lean on the compiler type inference to help guide where to fixup callsites.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caveats and limitations&lt;/head&gt;
    &lt;p&gt;There are lots and lots of other new features in OxCaml which I've started integrating, but require careful planning of layouts. For example, I wanted to use or_null to have a non-allocating version of option, but you often end up with long compiler errors about value inference failures, so I ended up just allocating a local type instead. Something to investigate more in the future as I get familiar with OxCaml.&lt;/p&gt;
    &lt;p&gt;I also ran into issues using mutable fields in unboxed records and found this is documented:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We plan to allow mutating unboxed records within boxed records (the design will differ from boxed record mutability, as unboxed types don‚Äôt have the same notion of identity).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's also difficult right now to strip away the OxCaml extensions and go back to normal OCaml syntax. &lt;code&gt;--erase-jane-syntax&lt;/code&gt;, but it requires some build system work to
integrate and seems to lag a little behind the new features (like unboxed small
literals). For now, I've decided to just focus on using OxCaml exclusively and
see how it goes for a while.&lt;/p&gt;
    &lt;p&gt;Finally, the tooling is still a fluid story. &lt;/p&gt;
    &lt;head rend="h3"&gt;Claude skills for OxCaml&lt;/head&gt;
    &lt;p&gt;While I built small scale examples to test out the architecture, I leaned heavily on Claude code to build out the majority of the parser so I could rapidly experiment. To do this, I synthesised a set of OxCaml specific Claude skills in my &lt;/p&gt;
    &lt;p&gt;I generated those skills via a combination of summarising the OxCaml source trees and cribbing from the &lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Results&lt;/head&gt;
    &lt;p&gt;Ultimately, none of this matters if the runtime performance isn't there! Luckily, the HTTPz parser is incredible in a synthetic benchmark (just passing buffers around) as opposed to a network benchmark, using Core_bench to measure performance. What's impressive isn't the straightline throughput, but the massive drop in heap activity which greatly increased the predictability and tail latency of the service. And with all the extra typing information, I expect that straightline performance will only increase (and this is before I've looked at the SIMD support).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;httpz (OxCaml)&lt;/cell&gt;
        &lt;cell role="head"&gt;Traditional Parser&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Small request (35B)&lt;/cell&gt;
        &lt;cell&gt;154 ns&lt;/cell&gt;
        &lt;cell&gt;300+ ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Medium request (439B)&lt;/cell&gt;
        &lt;cell&gt;1,150 ns&lt;/cell&gt;
        &lt;cell&gt;2,000+ ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Heap allocations&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;100-800 words&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;6.5M req/sec&lt;/cell&gt;
        &lt;cell&gt;3M req/sec&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Putting my new site live&lt;/head&gt;
    &lt;p&gt;I then glued this together using Eio into a full webserver. It works, and serves traffic just fine and in fact you are reading this web page via it right now!&lt;/p&gt;
    &lt;head rend="h3"&gt;What next: caml_alloc_local for C bindings&lt;/head&gt;
    &lt;p&gt;The current Eio/OxCaml does a data copy right now since Eio uses Bigarray, but I had a catchup coffee with &lt;/p&gt;
    &lt;p&gt;The key OxCaml feature to make this &lt;code&gt;io_uring&lt;/code&gt; integration awesome is a new FFI
function that allocates an OCaml value directly into the caller's OxCaml stack
rather than the heap. This means that we should be able to come up with a scheme
by which io_uring requests are routed directly to an OCaml continuation that's woken
up directly with a buffer available to it on the stack. True zero-copy to the kernel
awaits, which should also help speed up &lt;/p&gt;
    &lt;head rend="h3"&gt;Making it easier to develop in OxCaml in the open&lt;/head&gt;
    &lt;p&gt;Keen readers may note that my OxCaml repo links here go to a new monorepo I've setup for the purpose of hacking on real code in production outside of Jane Street's walls.&lt;/p&gt;
    &lt;p&gt;I'll blog more about this next week, but for now I hope you've enjoyed a little taste of what the OxCaml extensions offer in real world code. Stay tuned also for even more performance improvements, and for native TLS with an OxCaml port of ocaml-tls from &lt;/p&gt;
    &lt;head rend="h3"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Madhavapeddy et al (2025). Functional Networking for Millions of Docker Desktops. 10.1145/3747525&lt;/item&gt;
      &lt;item&gt;Madhavapeddy (2025). Holding an OxCaml tutorial at ICFP/SPLASH 2025. 10.59350/55bc5-x4p75&lt;/item&gt;
      &lt;item&gt;Sivaramakrishnan et al (2021). Retrofitting effect handlers onto OCaml. ACM. 10.1145/3453483.3454039&lt;/item&gt;
      &lt;item&gt;Madhavapeddy (2025). Arise Bushel, my sixth generation oxidised website. 10.59350/0r62w-c8g63&lt;/item&gt;
      &lt;item&gt;Madhavapeddy (2025). GeoTessera Python library released for geospatial embeddings. 10.59350/7hy6m-1rq76&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46854534</guid><pubDate>Mon, 02 Feb 2026 10:45:44 +0000</pubDate></item><item><title>Termux</title><link>https://github.com/termux/termux-app</link><description>&lt;doc fingerprint="bec6290bc4cf3eaa"&gt;
  &lt;main&gt;
    &lt;p&gt;Termux is an Android terminal application and Linux environment.&lt;/p&gt;
    &lt;p&gt;Note that this repository is for the app itself (the user interface and the terminal emulation). For the packages installable inside the app, see termux/termux-packages.&lt;/p&gt;
    &lt;p&gt;Quick how-to about Termux package management is available at Package Management. It also has info on how to fix &lt;code&gt;repository is under maintenance or down&lt;/code&gt; errors when running &lt;code&gt;apt&lt;/code&gt; or &lt;code&gt;pkg&lt;/code&gt; commands.&lt;/p&gt;
    &lt;p&gt;We are looking for Termux Android application maintainers.&lt;/p&gt;
    &lt;p&gt;NOTICE: Termux may be unstable on Android 12+. Android OS will kill any (phantom) processes greater than 32 (limit is for all apps combined) and also kill any processes using excessive CPU. You may get &lt;code&gt;[Process completed (signal 9) - press Enter]&lt;/code&gt; message in the terminal without actually exiting the shell process yourself. Check the related issue #2366, issue tracker, phantom cached and empty processes docs and this TLDR comment on how to disable trimming of phantom and excessive cpu usage processes. A proper docs page will be added later. An option to disable the killing should be available in Android 12L or 13, so upgrade at your own risk if you are on Android 11, specially if you are not rooted.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Termux App and Plugins&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Uninstallation&lt;/item&gt;
      &lt;item&gt;Important Links&lt;/item&gt;
      &lt;item&gt;Debugging&lt;/item&gt;
      &lt;item&gt;For Maintainers and Contributors&lt;/item&gt;
      &lt;item&gt;Forking&lt;/item&gt;
      &lt;item&gt;Sponsors and Funders&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The core Termux app comes with the following optional plugin apps.&lt;/p&gt;
    &lt;p&gt;Latest version is &lt;code&gt;v0.118.3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;NOTICE: It is highly recommended that you update to &lt;code&gt;v0.118.0&lt;/code&gt; or higher ASAP for various bug fixes, including a critical world-readable vulnerability reported here. See below for information regarding Termux on Google Play.&lt;/p&gt;
    &lt;p&gt;Termux can be obtained through various sources listed below for only Android &lt;code&gt;&amp;gt;= 7&lt;/code&gt; with full support for apps and packages.&lt;/p&gt;
    &lt;p&gt;Support for both app and packages was dropped for Android &lt;code&gt;5&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; on 2020-01-01 at &lt;code&gt;v0.83&lt;/code&gt;, however it was re-added just for the app without any support for package updates on 2022-05-24 via the GitHub sources. Check here for the details.&lt;/p&gt;
    &lt;p&gt;The APK files of different sources are signed with different signature keys. The &lt;code&gt;Termux&lt;/code&gt; app and all its plugins use the same &lt;code&gt;sharedUserId&lt;/code&gt; &lt;code&gt;com.termux&lt;/code&gt; and so all their APKs installed on a device must have been signed with the same signature key to work together and so they must all be installed from the same source. Do not attempt to mix them together, i.e do not try to install an app or plugin from &lt;code&gt;F-Droid&lt;/code&gt; and another one from a different source like &lt;code&gt;GitHub&lt;/code&gt;. Android Package Manager will also normally not allow installation of APKs with different signatures and you will get errors on installation like &lt;code&gt;App not installed&lt;/code&gt;, &lt;code&gt;Failed to install due to an unknown error&lt;/code&gt;, &lt;code&gt;INSTALL_FAILED_UPDATE_INCOMPATIBLE&lt;/code&gt;, &lt;code&gt;INSTALL_FAILED_SHARED_USER_INCOMPATIBLE&lt;/code&gt;, &lt;code&gt;signatures do not match previously installed version&lt;/code&gt;, etc. This restriction can be bypassed with root or with custom roms.&lt;/p&gt;
    &lt;p&gt;If you wish to install from a different source, then you must uninstall any and all existing Termux or its plugin app APKs from your device first, then install all new APKs from the same new source. Check Uninstallation section for details. You may also want to consider Backing up Termux before the uninstallation so that you can restore it after re-installing from Termux different source.&lt;/p&gt;
    &lt;p&gt;In the following paragraphs, "bootstrap" refers to the minimal packages that are shipped with the &lt;code&gt;termux-app&lt;/code&gt; itself to start a working shell environment. Its zips are built and released here.&lt;/p&gt;
    &lt;p&gt;Termux application can be obtained from &lt;code&gt;F-Droid&lt;/code&gt; from here.&lt;/p&gt;
    &lt;p&gt;You do not need to download the &lt;code&gt;F-Droid&lt;/code&gt; app (via the &lt;code&gt;Download F-Droid&lt;/code&gt; link) to install Termux. You can download the Termux APK directly from the site by clicking the &lt;code&gt;Download APK&lt;/code&gt; link at the bottom of each version section.&lt;/p&gt;
    &lt;p&gt;It usually takes a few days (or even a week or more) for updates to be available on &lt;code&gt;F-Droid&lt;/code&gt; once an update has been released on &lt;code&gt;GitHub&lt;/code&gt;. The &lt;code&gt;F-Droid&lt;/code&gt; releases are built and published by &lt;code&gt;F-Droid&lt;/code&gt; once they detect a new &lt;code&gt;GitHub&lt;/code&gt; release. The Termux maintainers do not have any control over the building and publishing of the Termux apps on &lt;code&gt;F-Droid&lt;/code&gt;. Moreover, the Termux maintainers also do not have access to the APK signing keys of &lt;code&gt;F-Droid&lt;/code&gt; releases, so we cannot release an APK ourselves on &lt;code&gt;GitHub&lt;/code&gt; that would be compatible with &lt;code&gt;F-Droid&lt;/code&gt; releases.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;F-Droid&lt;/code&gt; app often may not notify you of updates and you will manually have to do a pull down swipe action in the &lt;code&gt;Updates&lt;/code&gt; tab of the app for it to check updates. Make sure battery optimizations are disabled for the app, check https://dontkillmyapp.com/ for details on how to do that.&lt;/p&gt;
    &lt;p&gt;Only a universal APK is released, which will work on all supported architectures. The APK and bootstrap installation size will be &lt;code&gt;~180MB&lt;/code&gt;. &lt;code&gt;F-Droid&lt;/code&gt; does not support architecture specific APKs.&lt;/p&gt;
    &lt;p&gt;Termux application can be obtained on &lt;code&gt;GitHub&lt;/code&gt; either from &lt;code&gt;GitHub Releases&lt;/code&gt; for version &lt;code&gt;&amp;gt;= 0.118.0&lt;/code&gt; or from &lt;code&gt;GitHub Build Action&lt;/code&gt; workflows. For android &lt;code&gt;&amp;gt;= 7&lt;/code&gt;, only install &lt;code&gt;apt-android-7&lt;/code&gt; variants. For android &lt;code&gt;5&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;, only install &lt;code&gt;apt-android-5&lt;/code&gt; variants.&lt;/p&gt;
    &lt;p&gt;The APKs for &lt;code&gt;GitHub Releases&lt;/code&gt; will be listed under &lt;code&gt;Assets&lt;/code&gt; drop-down of a release. These are automatically attached when a new version is released.&lt;/p&gt;
    &lt;p&gt;The APKs for &lt;code&gt;GitHub Build&lt;/code&gt; action workflows will be listed under &lt;code&gt;Artifacts&lt;/code&gt; section of a workflow run. These are created for each commit/push done to the repository and can be used by users who don't want to wait for releases and want to try out the latest features immediately or want to test their pull requests. Note that for action workflows, you need to be logged into a &lt;code&gt;GitHub&lt;/code&gt; account for the &lt;code&gt;Artifacts&lt;/code&gt; links to be enabled/clickable. If you are using the &lt;code&gt;GitHub&lt;/code&gt; app, then make sure to open workflow link in a browser like Chrome or Firefox that has your GitHub account logged in since the in-app browser may not be logged in.&lt;/p&gt;
    &lt;p&gt;The APKs for both of these are &lt;code&gt;debuggable&lt;/code&gt; and are compatible with each other but they are not compatible with other sources.&lt;/p&gt;
    &lt;p&gt;Both universal and architecture specific APKs are released. The APK and bootstrap installation size will be &lt;code&gt;~180MB&lt;/code&gt; if using universal and &lt;code&gt;~120MB&lt;/code&gt; if using architecture specific. Check here for details.&lt;/p&gt;
    &lt;p&gt;Security warning: APK files on GitHub are signed with a test key that has been shared with community. This IS NOT an official developer key and everyone can use it to generate releases for own testing. Be very careful when using Termux GitHub builds obtained elsewhere except https://github.com/termux/termux-app. Everyone is able to use it to forge a malicious Termux update installable over the GitHub build. Think twice about installing Termux builds distributed via Telegram or other social media. If your device get caught by malware, we will not be able to help you.&lt;/p&gt;
    &lt;p&gt;The test key shall not be used to impersonate @termux and can't be used for this anyway. This key is not trusted by us and it is quite easy to detect its use in user generated content.&lt;/p&gt;
    &lt;head&gt;Keystore information&lt;/head&gt;
    &lt;code&gt;Alias name: alias
Creation date: Oct 4, 2019
Entry type: PrivateKeyEntry
Certificate chain length: 1
Certificate[1]:
Owner: CN=APK Signer, OU=Earth, O=Earth
Issuer: CN=APK Signer, OU=Earth, O=Earth
Serial number: 29be297b
Valid from: Wed Sep 04 02:03:24 EEST 2019 until: Tue Oct 26 02:03:24 EEST 2049
Certificate fingerprints:
         SHA1: 51:79:55:EA:BF:69:FC:05:7C:41:C7:D3:79:DB:BC:EF:20:AD:85:F2
         SHA256: B6:DA:01:48:0E:EF:D5:FB:F2:CD:37:71:B8:D1:02:1E:C7:91:30:4B:DD:6C:4B:F4:1D:3F:AA:BA:D4:8E:E5:E1
Signature algorithm name: SHA1withRSA (disabled)
Subject Public Key Algorithm: 2048-bit RSA key
Version: 3
&lt;/code&gt;
    &lt;p&gt;There is currently a build of Termux available on Google Play for Android 11+ devices, with extensive adjustments in order to pass policy requirements there. This is under development and has missing functionality and bugs (see here for status updates) compared to the stable F-Droid build, which is why most users who can should still use F-Droid or GitHub build as mentioned above.&lt;/p&gt;
    &lt;p&gt;Currently, Google Play will try to update installations away from F-Droid ones. Updating will still fail as sharedUserId has been removed. A planned 0.118.1 F-Droid release will fix this by setting a higher version code than used for the PlayStore app. Meanwhile, to prevent Google Play from attempting to download and then fail to install the Google Play releases over existing installations, you can open the Termux apps pages on Google Play and then click on the 3 dots options button in the top right and then disable the Enable auto update toggle. However, the Termux apps updates will still show in the PlayStore app updates list.&lt;/p&gt;
    &lt;p&gt;If you want to help out with testing the Google Play build (or cannot install Termux from other sources), be aware that it's built from a separate repository (https://github.com/termux-play-store/) - be sure to report issues there, as any issues encountered might very well be specific to that repository.&lt;/p&gt;
    &lt;p&gt;Uninstallation may be required if a user doesn't want Termux installed in their device anymore or is switching to a different install source. You may also want to consider Backing up Termux before the uninstallation.&lt;/p&gt;
    &lt;p&gt;To uninstall Termux completely, you must uninstall any and all existing Termux or its plugin app APKs listed in Termux App and Plugins.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;Android Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Applications&lt;/code&gt; and then look for those apps. You can also use the search feature if it‚Äôs available on your device and search &lt;code&gt;termux&lt;/code&gt; in the applications list.&lt;/p&gt;
    &lt;p&gt;Even if you think you have not installed any of the plugins, it's strongly suggested to go through the application list in Android settings and double-check.&lt;/p&gt;
    &lt;p&gt;All community links are available here.&lt;/p&gt;
    &lt;p&gt;The main ones are the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Termux Reddit community&lt;/item&gt;
      &lt;item&gt;Termux User Matrix Channel (Gitter)&lt;/item&gt;
      &lt;item&gt;Termux Dev Matrix Channel (Gitter)&lt;/item&gt;
      &lt;item&gt;Termux X (Twitter)&lt;/item&gt;
      &lt;item&gt;Termux Support Email&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FAQ&lt;/item&gt;
      &lt;item&gt;Termux File System Layout&lt;/item&gt;
      &lt;item&gt;Differences From Linux&lt;/item&gt;
      &lt;item&gt;Package Management&lt;/item&gt;
      &lt;item&gt;Remote Access&lt;/item&gt;
      &lt;item&gt;Backing up Termux&lt;/item&gt;
      &lt;item&gt;Terminal Settings&lt;/item&gt;
      &lt;item&gt;Touch Keyboard&lt;/item&gt;
      &lt;item&gt;Android Storage and Sharing Data with Other Apps&lt;/item&gt;
      &lt;item&gt;Android APIs&lt;/item&gt;
      &lt;item&gt;Moved Termux Packages Hosting From Bintray to IPFS&lt;/item&gt;
      &lt;item&gt;Running Commands in Termux From Other Apps via &lt;code&gt;RUN_COMMAND&lt;/code&gt;intent&lt;/item&gt;
      &lt;item&gt;Termux and Android 10&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;VTE (libvte): Terminal emulator widget for GTK+, mainly used in gnome-terminal. Source, Open Issues, and All (including closed) issues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;iTerm 2: OS X terminal application. Source, Issues and Documentation (which includes iTerm2 proprietary escape codes).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Konsole: KDE terminal application. Source, in particular tests, Bugs and Wishes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hterm: JavaScript terminal implementation from Chromium. Source, including tests, and Google group.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;xterm: The grandfather of terminal emulators. Source.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connectbot: Android SSH client. Source&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Android Terminal Emulator: Android terminal app which Termux terminal handling is based on. Inactive. Source.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can help debug problems of the &lt;code&gt;Termux&lt;/code&gt; app and its plugins by setting appropriate &lt;code&gt;logcat&lt;/code&gt; &lt;code&gt;Log Level&lt;/code&gt; in &lt;code&gt;Termux&lt;/code&gt; app settings -&amp;gt; &lt;code&gt;&amp;lt;APP_NAME&amp;gt;&lt;/code&gt; -&amp;gt; &lt;code&gt;Debugging&lt;/code&gt; -&amp;gt; &lt;code&gt;Log Level&lt;/code&gt; (Requires &lt;code&gt;Termux&lt;/code&gt; app version &lt;code&gt;&amp;gt;= 0.118.0&lt;/code&gt;). The &lt;code&gt;Log Level&lt;/code&gt; defaults to &lt;code&gt;Normal&lt;/code&gt; and log level &lt;code&gt;Verbose&lt;/code&gt; currently logs additional information. Its best to revert log level to &lt;code&gt;Normal&lt;/code&gt; after you have finished debugging since private data may otherwise be passed to &lt;code&gt;logcat&lt;/code&gt; during normal operation and moreover, additional logging increases execution time.&lt;/p&gt;
    &lt;p&gt;The plugin apps do not execute the commands themselves but send execution intents to &lt;code&gt;Termux&lt;/code&gt; app, which has its own log level which can be set in &lt;code&gt;Termux&lt;/code&gt; app settings -&amp;gt; &lt;code&gt;Termux&lt;/code&gt; -&amp;gt; &lt;code&gt;Debugging&lt;/code&gt; -&amp;gt; &lt;code&gt;Log Level&lt;/code&gt;. So you must set log level for both &lt;code&gt;Termux&lt;/code&gt; and the respective plugin app settings to get all the info.&lt;/p&gt;
    &lt;p&gt;Once log levels have been set, you can run the &lt;code&gt;logcat&lt;/code&gt; command in &lt;code&gt;Termux&lt;/code&gt; app terminal to view the logs in realtime (&lt;code&gt;Ctrl+c&lt;/code&gt; to stop) or use &lt;code&gt;logcat -d &amp;gt; logcat.txt&lt;/code&gt; to take a dump of the log. You can also view the logs from a PC over &lt;code&gt;ADB&lt;/code&gt;. For more information, check official android &lt;code&gt;logcat&lt;/code&gt; guide here.&lt;/p&gt;
    &lt;p&gt;Moreover, users can generate termux files &lt;code&gt;stat&lt;/code&gt; info and &lt;code&gt;logcat&lt;/code&gt; dump automatically too with terminal's long hold options menu &lt;code&gt;More&lt;/code&gt; -&amp;gt; &lt;code&gt;Report Issue&lt;/code&gt; option and selecting &lt;code&gt;YES&lt;/code&gt; in the prompt shown to add debug info. This can be helpful for reporting and debugging other issues. If the report generated is too large, then &lt;code&gt;Save To File&lt;/code&gt; option in context menu (3 dots on top right) of &lt;code&gt;ReportActivity&lt;/code&gt; can be used and the file viewed/shared instead.&lt;/p&gt;
    &lt;p&gt;Users must post complete report (optionally without sensitive info) when reporting issues. Issues opened with (partial) screenshots of error reports instead of text will likely be automatically closed/deleted.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Off&lt;/code&gt;- Log nothing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Normal&lt;/code&gt;- Start logging error, warn and info messages and stacktraces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Debug&lt;/code&gt;- Start logging debug messages.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Verbose&lt;/code&gt;- Start logging verbose messages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The termux-shared library was added in &lt;code&gt;v0.109&lt;/code&gt;. It defines shared constants and utils of the Termux app and its plugins. It was created to allow for the removal of all hardcoded paths in the Termux app. Some of the termux plugins are using this as well and rest will in future. If you are contributing code that is using a constant or a util that may be shared, then define it in &lt;code&gt;termux-shared&lt;/code&gt; library if it currently doesn't exist and reference it from there. Update the relevant changelogs as well. Pull requests using hardcoded values will/should not be accepted. Termux app and plugin specific classes must be added under &lt;code&gt;com.termux.shared.termux&lt;/code&gt; package and general classes outside it. The &lt;code&gt;termux-shared&lt;/code&gt; &lt;code&gt;LICENSE&lt;/code&gt; must also be checked and updated if necessary when contributing code. The licenses of any external library or code must be honoured.&lt;/p&gt;
    &lt;p&gt;The main Termux constants are defined by &lt;code&gt;TermuxConstants&lt;/code&gt; class. It also contains information on how to fork Termux or build it with your own package name. Changing the package name will require building the bootstrap zip packages and other packages with the new &lt;code&gt;$PREFIX&lt;/code&gt;, check Building Packages for more info.&lt;/p&gt;
    &lt;p&gt;Check Termux Libraries for how to import termux libraries in plugin apps and Forking and Local Development for how to update termux libraries for plugins.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;versionName&lt;/code&gt; in &lt;code&gt;build.gradle&lt;/code&gt; files of Termux and its plugin apps must follow the semantic version &lt;code&gt;2.0.0&lt;/code&gt; spec in the format &lt;code&gt;major.minor.patch(-prerelease)(+buildmetadata)&lt;/code&gt;. When bumping &lt;code&gt;versionName&lt;/code&gt; in &lt;code&gt;build.gradle&lt;/code&gt; files and when creating a tag for new releases on GitHub, make sure to include the patch number as well, like &lt;code&gt;v0.1.0&lt;/code&gt; instead of just &lt;code&gt;v0.1&lt;/code&gt;. The &lt;code&gt;build.gradle&lt;/code&gt; files and &lt;code&gt;attach_debug_apks_to_release&lt;/code&gt; workflow validates the version as well and the build/attachment will fail if &lt;code&gt;versionName&lt;/code&gt; does not follow the spec.&lt;/p&gt;
    &lt;p&gt;Commit messages must use the Conventional Commits spec so that chagelogs as per the Keep a Changelog spec can automatically be generated by the &lt;code&gt;create-conventional-changelog&lt;/code&gt; script, check its repo for further details on the spec. The first letter for &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt; must be capital and description should be in the present tense. The space after the colon &lt;code&gt;:&lt;/code&gt; is necessary. For a breaking change, add an exclamation mark &lt;code&gt;!&lt;/code&gt; before the colon &lt;code&gt;:&lt;/code&gt;, so that it is highlighted in the chagelog automatically.&lt;/p&gt;
    &lt;code&gt;&amp;lt;type&amp;gt;[optional scope]: &amp;lt;description&amp;gt;

[optional body]

[optional footer(s)]
&lt;/code&gt;
    &lt;p&gt;Only the &lt;code&gt;types&lt;/code&gt; listed below must be used exactly as they are used in the changelog headings. For example, &lt;code&gt;Added: Add foo&lt;/code&gt;, &lt;code&gt;Added|Fixed: Add foo and fix bar&lt;/code&gt;, &lt;code&gt;Changed!: Change baz as a breaking change&lt;/code&gt;, etc. You can optionally add a scope as well, like &lt;code&gt;Fixed(terminal): Fix some bug&lt;/code&gt;. Do not use anything else as type, like &lt;code&gt;add&lt;/code&gt; instead of &lt;code&gt;Added&lt;/code&gt;, etc.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Added for new features.&lt;/item&gt;
      &lt;item&gt;Changed for changes in existing functionality.&lt;/item&gt;
      &lt;item&gt;Deprecated for soon-to-be removed features.&lt;/item&gt;
      &lt;item&gt;Removed for now removed features.&lt;/item&gt;
      &lt;item&gt;Fixed for any bug fixes.&lt;/item&gt;
      &lt;item&gt;Security in case of vulnerabilities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check &lt;code&gt;TermuxConstants&lt;/code&gt;javadocs for instructions on what changes to make in the app to change package name.&lt;/item&gt;
      &lt;item&gt;You also need to recompile bootstrap zip for the new package name. Check building bootstrap, here and here.&lt;/item&gt;
      &lt;item&gt;Currently, not all plugins use &lt;code&gt;TermuxConstants&lt;/code&gt;from&lt;code&gt;termux-shared&lt;/code&gt;library and have hardcoded&lt;code&gt;com.termux&lt;/code&gt;values and will need to be manually patched.&lt;/item&gt;
      &lt;item&gt;If forking termux plugins, check Forking and Local Development for info on how to use termux libraries for plugins.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; GitHub Secure Open Source Fund (1, 2)&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; NLnet NGI Mobifree (1, 2)&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Cloudflare (1)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46854642</guid><pubDate>Mon, 02 Feb 2026 11:03:44 +0000</pubDate></item><item><title>Claude Code is suddenly everywhere inside Microsoft</title><link>https://www.theverge.com/tech/865689/microsoft-claude-code-anthropic-partnership-notepad</link><description>&lt;doc fingerprint="2f402e1a7ea6c0ef"&gt;
  &lt;main&gt;
    &lt;p&gt;Developers have been comparing the strengths and weaknesses of Anthropic‚Äôs Claude Code, Anysphere‚Äôs Cursor, and Microsoft‚Äôs GitHub Copilot for months now, looking for a winner. While no individual AI coding tool manages to be the best at every task that software developers do each day, Claude Code is increasingly coming out on top for its ease of use, both for developers and nontechnical users.&lt;/p&gt;
    &lt;head rend="h1"&gt;Claude Code is suddenly everywhere inside Microsoft&lt;/head&gt;
    &lt;p&gt;Microsoft sells GitHub Copilot to its customers, but it increasingly favors Claude Code internally.&lt;/p&gt;
    &lt;p&gt;Microsoft sells GitHub Copilot to its customers, but it increasingly favors Claude Code internally.&lt;/p&gt;
    &lt;p&gt;It seems like Microsoft agrees, as sources tell me the company is now encouraging thousands of its employees from some of its most prolific teams to pick up Claude Code and get coding, even if they‚Äôre not developers.&lt;/p&gt;
    &lt;p&gt;Microsoft first started adopting Anthropic‚Äôs Claude Sonnet 4 model inside its developer division in June last year, before favoring it for paid users of GitHub Copilot several months later. Now, Microsoft is going a step beyond using Anthropic‚Äôs AI models and widely adopting Claude Code across its biggest engineering teams.&lt;/p&gt;
    &lt;p&gt;Microsoft‚Äôs CoreAI team, the new AI engineering group led by former Meta engineering chief Jay Parikh, has been testing Claude Code in recent months, and last week Microsoft‚Äôs Experiences + Devices division were being asked to install Claude Code. This division is responsible for Windows, Microsoft 365, Outlook, Microsoft Teams, Bing, Edge, Surface, and more.&lt;/p&gt;
    &lt;p&gt;Even employees without any coding experience are being encouraged to experiment with Claude Code, to allow designers and project managers to prototype ideas. Microsoft has also approved the use of Claude Code across all of its code and repositories for its Business and Industry Copilot teams.&lt;/p&gt;
    &lt;p&gt;Software engineers at Microsoft are now expected to use both Claude Code and GitHub Copilot and give feedback comparing the two, I‚Äôm told. Microsoft sells GitHub Copilot as its AI coding tool of choice to its customers, but if these broad internal pilot programs are successful, then it‚Äôs possible the company could even eventually sell Claude Code directly to its cloud customers.&lt;/p&gt;
    &lt;p&gt;Microsoft is now one of Anthropic‚Äôs top customers, according to a recent report from The Information. The software maker is also counting selling Anthropic AI models toward Azure sales quotas, which is unusual given Microsoft typically only offers its salespeople incentives for homegrown products or models from OpenAI.&lt;/p&gt;
    &lt;p&gt;Microsoft‚Äôs decision to adopt Claude Code more broadly among its engineering teams certainly looks like a vote of confidence in Anthropic‚Äôs AI tools over its own, especially as it‚Äôs encouraging nontechnical employees to try out coding. But the reality is that Microsoft‚Äôs developers are likely to use a mix of AI tools, and adopting Claude Code is another part of that tool set.&lt;/p&gt;
    &lt;p&gt;‚ÄúCompanies regularly test and trial competing products to gain a better understanding of the market landscape,‚Äù says Frank Shaw, Microsoft‚Äôs communications chief, in a statement to Notepad. ‚ÄúOpenAI continues to be our primary partner and model provider on frontier models, and we remain committed to our long-term partnership.‚Äù&lt;/p&gt;
    &lt;p&gt;While Microsoft remains committed to OpenAI, it is increasingly working with Anthropic to bring its models and tools to Microsoft‚Äôs own teams and the software it sells to customers. Microsoft and Anthropic signed a deal in November that allows Microsoft Foundry customers to get access to Claude Sonnet 4.5, Claude Opus 4.1, and Claude Haiku 4.5. The deal also involves Anthropic committing to purchasing $30 billion of Azure compute capacity.&lt;/p&gt;
    &lt;p&gt;Microsoft has also started favoring Anthropic‚Äôs Claude models inside Microsoft 365 apps and Copilot recently, using them in specific apps or features where Anthropic‚Äôs models have proved more capable than OpenAI‚Äôs counterparts.&lt;/p&gt;
    &lt;p&gt;The big question here is, what does the increased use of Claude Code at Microsoft mean for its more than 100,000 code repositories? Microsoft told me last year that 91 percent of its engineering teams use GitHub Copilot and a variety of teams have been using the AI tool to speed up mundane tasks. Microsoft‚Äôs use of AI tools has been largely restricted to software engineers, but with Claude Code and Claude Cowork, Anthropic is increasingly focused on making coding and non-coding tasks more approachable, thanks to AI agent capabilities.&lt;/p&gt;
    &lt;p&gt;Microsoft is embracing the ease of use of Claude Code to allow more nontechnical employees to commit code using AI, and this broad pilot will certainly highlight the challenges and benefits of that shift. It also puts further pressure on junior developer roles, with fears in the industry that these roles are increasingly disappearing because of AI. Microsoft just took another big step toward a future where more autonomous AI agents are creating code, further wrestling control from its software engineers.&lt;/p&gt;
    &lt;head rend="h2"&gt;It‚Äôs Xbox time&lt;/head&gt;
    &lt;p&gt;Microsoft is getting ready to show off two of its biggest Xbox games this year, Forza Horizon 6 and Fable, later today as part of its Xbox Developer Direct stream. There will also be a first in-depth look at Beast of Reincarnation and at least one other game shown, I‚Äôm hearing. Double Fine is ready to show off Kiln, a multiplayer, team-based brawler. I understand Double Fine has been holding playtests recently, where you play as a spirit that can inhabit pottery and carry water to douse an opponent‚Äôs kiln and put out a fire.&lt;/p&gt;
    &lt;p&gt;I wouldn‚Äôt be surprised to see Kiln appear as an early preview in the coming months, followed by Forza Horizon 6 in May and then Halo: Campaign Evolved. I keep hearing that both Fable and Gears of War: E-Day are currently targeting a release in the second half of this year. Microsoft is keen to release new Forza, Gears, Halo, and Fable games in 2026 to mark 25 years of Xbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsoft‚Äôs first Windows 11 update of 2026 stopped some computers from shutting down. It‚Äôs only January and Microsoft has had to rush out an emergency out-of-band fix that stopped some Windows 11 PCs from shutting down. The issues were limited to machines running Enterprise and IoT editions of Windows 11 version 23H2, but it‚Äôs yet another buggy update for Windows, which is becoming increasingly common.&lt;/item&gt;
      &lt;item&gt;Microsoft‚Äôs free Xbox Cloud Gaming is coming soon with ads. Microsoft is getting closer to launching its free streaming option for Xbox Cloud Gaming. The ad-supported feature has started appearing inside the Xbox app for PC, indicating ‚Äú1 hour of ad-supported playtime per session.‚Äù I‚Äôm expecting to see this rollout with preroll ads in the coming weeks, but there could be limits of up to five hours free per month.&lt;/item&gt;
      &lt;item&gt;Microsoft wants to build 15 data centers in Mount Pleasant, Wisconsin. The empty land formerly owned by Foxconn is about to be transformed into Microsoft data centers. Leaders of the local village in Mount Pleasant, Wisconsin, approved plans for the data centers earlier this week, and final approval could come next week. Foxconn‚Äôs failed Wisconsin project had promised 13,000 jobs, but now the land will be filled with a 1.2-million-square-foot data center project that will hold hundreds of thousands of Nvidia‚Äôs AI GPUs.&lt;/item&gt;
      &lt;item&gt;The Xbox app is now available for all Arm-based Windows 11 PCs. After a rocky start to gaming on Windows on Arm, Microsoft has updated its Xbox app this week so it‚Äôs fully compatible with all Qualcomm-powered devices. More than 85 percent of the Xbox Game Pass catalog is also now compatible with Arm-based devices, but the majority of games will still need to be emulated using Microsoft‚Äôs Prism technology.&lt;/item&gt;
      &lt;item&gt;Microsoft Paint now has an AI-powered coloring book. Microsoft is adding more AI features to its Paint app this week. Windows testers can now try out a coloring book feature that lets you create coloring book pages from a text prompt. It‚Äôs available inside the Copilot button in Paint, and you have to have a Copilot Plus PC to be able to use it. Notepad (the app!) is also getting expanded Markdown syntax features and a new welcome experience to highlight features. I never thought I‚Äôd see the day that Notepad, a lightweight app, would need a welcome screen because of all the features Microsoft has packed in.&lt;/item&gt;
      &lt;item&gt;GitHub has a new Copilot SDK. Microsoft is announcing a technical preview of its GitHub Copilot SDK today, which brings the power of the GitHub Copilot CLI to any app. It essentially allows developers to bring GitHub Copilot capabilities as a programmable SDK for Python, TypeScript, Go, and .NET. Microsoft teams have already used this to build custom GUIs for agents, summarizing tools, YouTube chapter generators, and more.&lt;/item&gt;
      &lt;item&gt;Satya Nadella and former British Prime Minister Rishi Sunak chat AI. Former UK leader Rishi Sunak took on a senior adviser role at Microsoft and Anthropic last year, and he‚Äôs now appeared alongside Microsoft CEO Satya Nadella to discuss the future of AI. The roughly 30-minute talk didn‚Äôt have any surprising news, but Sunak did agree with Nvidia CEO Jensen Huang that ‚Äúyou may not lose your job to AI, but you may well lose your job to someone using AI.‚Äù Nadella thinks AI will make us all ‚Äúmanagers of infinite minds,‚Äù much like how we have ‚Äúinformation at your fingertips.‚Äù&lt;/item&gt;
      &lt;item&gt;Microsoft now sponsors the Mercedes-AMG F1 team. Microsoft is switching its F1 allegiances from Alpine to Mercedes-AMG for the 2026 season. A new multiyear partnership will see Mercedes-AMG use Microsoft technologies for race team operations and plaster the Microsoft logo in prominent positions on the 2026 Mercedes-AMG F1 car and on racing suits. There‚Äôs a big technical shake-up for the 2026 season, with all-new chassis, power units, and fuel regulations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm always keen to hear from readers, so please drop a comment here, or you can reach me at notepad@theverge.com if you want to discuss anything else. If you‚Äôve heard about any of Microsoft‚Äôs secret projects, you can reach me via email at notepad@theverge.com or speak to me confidentially on the Signal messaging app, where I‚Äôm tomwarren.01. I‚Äôm also tomwarren on Telegram, if you‚Äôd prefer to chat there.&lt;/p&gt;
    &lt;p&gt;Thanks for subscribing to Notepad.&lt;/p&gt;
    &lt;head rend="h2"&gt;Most Popular&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bill Gates says accusations contained in Epstein files are ‚Äòabsolutely absurd‚Äô&lt;/item&gt;
      &lt;item&gt;The tragedy of Supernatural&lt;/item&gt;
      &lt;item&gt;This tiny pocket-friendly e-reader is packed with frustration and potential&lt;/item&gt;
      &lt;item&gt;There‚Äôs a social network for AI agents, and it‚Äôs getting weird&lt;/item&gt;
      &lt;item&gt;The telephoto is the only phone camera that really matters&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46854999</guid><pubDate>Mon, 02 Feb 2026 11:58:58 +0000</pubDate></item><item><title>Nano-vLLM: How a vLLM-style inference engine works</title><link>https://neutree.ai/blog/nano-vllm-part-1</link><description>&lt;doc fingerprint="3c1aa40c8370c811"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Understanding LLM Inference Engines: Inside Nano-vLLM (Part 1)&lt;/head&gt;
    &lt;head rend="h2"&gt;Architecture, Scheduling, and the Path from Prompt to Token&lt;/head&gt;
    &lt;p&gt;When deploying large language models in production, the inference engine becomes a critical piece of infrastructure. Every LLM API you use √¢ OpenAI, Claude, DeepSeek √¢ is sitting on top of an inference engine like this. While most developers interact with LLMs through high-level APIs, understanding what happens beneath the surface√¢how prompts are processed, how requests are batched, and how GPU resources are managed√¢can significantly impact system design decisions.&lt;/p&gt;
    &lt;p&gt;This two-part series explores these internals through Nano-vLLM, a minimal (~1,200 lines of Python) yet production-grade implementation that distills the core ideas behind vLLM, one of the most widely adopted open-source inference engines.&lt;/p&gt;
    &lt;p&gt;Nano-vLLM was created by a contributor to DeepSeek, whose name appears on the technical reports of models like DeepSeek-V3 and R1. Despite its minimal codebase, it implements the essential features that make vLLM production-ready: prefix caching, tensor parallelism, CUDA graph compilation, and torch compilation optimizations. Benchmarks show it achieving throughput comparable to√¢or even slightly exceeding√¢the full vLLM implementation. This makes it an ideal lens for understanding inference engine design without getting lost in the complexity of supporting dozens of model architectures and hardware backends.&lt;/p&gt;
    &lt;p&gt;In Part 1, we focus on the engineering architecture: how the system is organized, how requests flow through the pipeline, and how scheduling decisions are made. We will treat the actual model computation as a black box for now√¢Part 2 will open that box to explore attention mechanisms, KV cache internals, and tensor parallelism at the computation level.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Main Flow: From Prompt to Output&lt;/head&gt;
    &lt;p&gt;The entry point to Nano-vLLM is straightforward: an &lt;code&gt;LLM&lt;/code&gt; class with a &lt;code&gt;generate&lt;/code&gt; method. You pass in an array of prompts and sampling parameters, and get back the generated text. But behind this simple interface lies a carefully designed pipeline that transforms text into tokens, schedules computation efficiently, and manages GPU resources.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Prompts to Sequences&lt;/head&gt;
    &lt;p&gt;When &lt;code&gt;generate&lt;/code&gt; is called, each prompt string goes through a tokenizer√¢a model-specific component that splits natural language into tokens, the fundamental units that LLMs process. Different model families (Qwen, LLaMA, DeepSeek) use different tokenizers, which is why a prompt of the same length may produce different token counts across models. The tokenizer converts each prompt into a sequence: an internal data structure representing a variable-length array of token IDs. This sequence becomes the core unit of work flowing through the rest of the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Producer-Consumer Pattern&lt;/head&gt;
    &lt;p&gt;Here√¢s where the architecture gets interesting. Rather than processing each sequence immediately, the system adopts a producer-consumer pattern with the Scheduler at its center. The &lt;code&gt;add_request&lt;/code&gt; method acts as the producer: it converts prompts to sequences and places them into the Scheduler√¢s queue. Meanwhile, a separate step loop acts as the consumer, pulling batches of sequences from the Scheduler for processing. This decoupling is key√¢it allows the system to accumulate multiple sequences and process them together, which is where the performance gains come from.&lt;/p&gt;
    &lt;head rend="h3"&gt;Batching and the Throughput-Latency Trade-off&lt;/head&gt;
    &lt;p&gt;Why does batching matter? GPU computation has significant fixed overhead√¢initializing CUDA kernels, transferring data between CPU and GPU memory, and synchronizing results. If you process one sequence at a time, you pay this overhead for every single request. By batching multiple sequences together, you amortize this overhead across many requests, dramatically improving overall throughput.&lt;/p&gt;
    &lt;p&gt;However, batching comes with a trade-off. When three prompts are batched together, each must wait for the others to complete before any results are returned. The total time for the batch is determined by the slowest sequence. This means: larger batches yield higher throughput but potentially higher latency for individual requests; smaller batches yield lower latency but reduced throughput. This is a fundamental tension in inference engine design, and the batch size parameters you configure directly control this trade-off.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prefill vs. Decode: Two Phases of Generation&lt;/head&gt;
    &lt;p&gt;Before diving into the Scheduler, we need to understand a crucial distinction. LLM inference happens in two phases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prefill: Processing the input prompt. All input tokens are processed together to build up the model√¢s internal state. During this phase, the user sees nothing.&lt;/item&gt;
      &lt;item&gt;Decode: Generating output tokens. The model produces one token at a time, each depending on all previous tokens. This is when you see text streaming out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a single sequence, there is exactly one prefill phase followed by many decode steps. The Scheduler needs to distinguish between these phases because they have very different computational characteristics√¢prefill processes many tokens at once, while decode processes just one token per step.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside the Scheduler&lt;/head&gt;
    &lt;p&gt;The Scheduler is responsible for deciding which sequences to process and in what order. It maintains two queues:&lt;/p&gt;
    &lt;head rend="h3"&gt;Waiting and Running Queues&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Waiting Queue: Sequences that have been submitted but not yet started. New sequences from &lt;code&gt;add_request&lt;/code&gt;always enter here first.&lt;/item&gt;
      &lt;item&gt;Running Queue: Sequences that are actively being processed√¢either in prefill or decode phase.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a sequence enters the Waiting queue, the Scheduler checks with another component called the Block Manager to allocate resources for it. Once allocated, the sequence moves to the Running queue. The Scheduler then selects sequences from the Running queue for the next computation step, grouping them into a batch along with an action indicator (prefill or decode).&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling Resource Exhaustion&lt;/head&gt;
    &lt;p&gt;What happens when GPU memory fills up? The KV cache (which stores intermediate computation results) has limited capacity. If a sequence in the Running queue cannot continue because there√¢s no room to store its next token√¢s cache, the Scheduler preempts it√¢moving it back to the front of the Waiting queue. This ensures the sequence will resume as soon as resources free up, while allowing other sequences to make progress.&lt;/p&gt;
    &lt;p&gt;When a sequence completes (reaches an end-of-sequence token or maximum length), the Scheduler removes it from the Running queue and deallocates its resources, freeing space for waiting sequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Block Manager: KV Cache Control Plane&lt;/head&gt;
    &lt;p&gt;The Block Manager is where vLLM√¢s memory management innovation lives. To understand it, we first need to introduce a new resource unit: the block.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Sequences to Blocks&lt;/head&gt;
    &lt;p&gt;A sequence is a variable-length array of tokens√¢it can be 10 tokens or 10,000. But variable-length allocations are inefficient for GPU memory management. The Block Manager solves this by dividing sequences into fixed-size blocks (default: 256 tokens each).&lt;/p&gt;
    &lt;p&gt;A 700-token sequence would occupy three blocks: two full blocks (256 tokens each) and one partial block (188 tokens, with 68 slots unused). Importantly, tokens from different sequences never share a block√¢but a long sequence will span multiple blocks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prefix Caching via Hashing&lt;/head&gt;
    &lt;p&gt;Here√¢s where it gets clever. Each block√¢s content is hashed, and the Block Manager maintains a hash-to-block-id mapping. When a new sequence arrives, the system computes hashes for its blocks and checks if any already exist in the cache.&lt;/p&gt;
    &lt;p&gt;If a block with the same hash exists, the system reuses it by incrementing a reference count√¢no redundant computation or storage needed. This is particularly powerful for scenarios where many requests share common prefixes (like system prompts in chat applications). The prefix only needs to be computed once; subsequent requests can reuse the cached results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Control Plane vs. Data Plane&lt;/head&gt;
    &lt;p&gt;A subtle but important point: the Block Manager lives in CPU memory and only tracks metadata√¢which blocks are allocated, their reference counts, and hash mappings. The actual KV cache data lives on the GPU. The Block Manager is the control plane; the GPU memory is the data plane. This separation allows fast allocation decisions without touching GPU memory until actual computation happens.&lt;/p&gt;
    &lt;p&gt;When blocks are deallocated, the Block Manager marks them as free immediately, but the GPU memory isn√¢t zeroed√¢it√¢s simply overwritten when the block is reused. This avoids unnecessary memory operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Model Runner: Execution and Parallelism&lt;/head&gt;
    &lt;p&gt;The Model Runner is responsible for actually executing the model on GPU(s). When the step loop retrieves a batch of sequences from the Scheduler, it passes them to the Model Runner along with the action (prefill or decode).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tensor Parallel Communication&lt;/head&gt;
    &lt;p&gt;When a model is too large for a single GPU, Nano-vLLM supports tensor parallelism (TP)√¢splitting the model across multiple GPUs. With TP=8, for example, eight GPUs work together to run a single model.&lt;/p&gt;
    &lt;p&gt;The communication architecture uses a leader-worker pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank 0 (Leader): Receives commands from the step loop, executes its portion, and coordinates with workers.&lt;/item&gt;
      &lt;item&gt;Ranks 1 to N-1 (Workers): Continuously poll a shared memory buffer for commands from the leader.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the leader receives a &lt;code&gt;run&lt;/code&gt; command, it writes the method name and arguments to shared memory. Workers detect this, read the parameters, and execute the same operation on their respective GPUs. Each worker knows its rank, so it can compute its designated portion of the work. This shared-memory approach is efficient for single-machine multi-GPU setups, avoiding network overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparing for Computation&lt;/head&gt;
    &lt;p&gt;Before invoking the model, the Model Runner prepares the input based on the action:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prepare Prefill: Batches multiple sequences with variable lengths, computing cumulative sequence lengths for efficient attention computation.&lt;/item&gt;
      &lt;item&gt;Prepare Decode: Batches single tokens (one per sequence) with their positions and slot mappings for KV cache access.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This preparation also involves converting CPU-side token data into GPU tensors√¢the point where data crosses from CPU memory to GPU memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;CUDA Graphs: Reducing Kernel Launch Overhead&lt;/head&gt;
    &lt;p&gt;For decode steps (which process just one token per sequence), kernel launch overhead can become significant relative to actual computation. CUDA Graphs address this by recording a sequence of GPU operations once, then replaying them with different inputs. Nano-vLLM pre-captures CUDA graphs for common batch sizes (1, 2, 4, 8, 16, up to 512), allowing decode steps to execute with minimal launch overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sampling: From Logits to Tokens&lt;/head&gt;
    &lt;p&gt;The model doesn√¢t output a single token√¢it outputs logits, a probability distribution over the entire vocabulary. The final step is sampling: selecting one token from this distribution.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;temperature&lt;/code&gt; parameter controls this selection. Mathematically, it adjusts the shape of the probability distribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Low temperature (approaching 0): The distribution becomes sharply peaked. The highest-probability token is almost always selected, making outputs more deterministic and focused.&lt;/item&gt;
      &lt;item&gt;High temperature: The distribution flattens. Lower-probability tokens have a better chance of being selected, making outputs more diverse and creative.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is where the √¢randomness√¢ in LLM outputs comes from√¢and why the same prompt can produce different responses. The sampling step selects from a valid range of candidates, introducing controlled variability.&lt;/p&gt;
    &lt;head rend="h2"&gt;What√¢s Next&lt;/head&gt;
    &lt;p&gt;In Part 2, we√¢ll open the black box of model. We√¢ll explore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How the model transforms tokens into hidden states and back&lt;/item&gt;
      &lt;item&gt;The attention mechanism and why multi-head attention matters&lt;/item&gt;
      &lt;item&gt;How KV cache is physically laid out on GPU memory&lt;/item&gt;
      &lt;item&gt;Dense vs. MoE (Mixture of Experts) architectures&lt;/item&gt;
      &lt;item&gt;How tensor parallelism works at the computation level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Understanding these internals will complete the picture√¢from prompt string to generated text, with nothing left hidden.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46855447</guid><pubDate>Mon, 02 Feb 2026 12:52:35 +0000</pubDate></item><item><title>MaliciousCorgi: AI Extensions send your code to China</title><link>https://www.koi.ai/blog/maliciouscorgi-the-cute-looking-ai-extensions-leaking-code-from-1-5-million-developers</link><description>&lt;doc fingerprint="9bcafb135c354eb2"&gt;
  &lt;main&gt;
    &lt;p&gt;AI coding assistants are everywhere. They suggest code, explain errors, write functions, review pull requests. Every developer marketplace is flooded with them - ChatGPT wrappers, Copilot alternatives, code completion tools promising to 10x your productivity.&lt;/p&gt;
    &lt;p&gt;We install them without a second thought. They're in the official marketplace. They have thousands of reviews. They work. So we grant them access to our workspaces, our files, our keystrokes - and assume they're only using that access to help us code.&lt;/p&gt;
    &lt;p&gt;Not all of them are.&lt;/p&gt;
    &lt;p&gt;Our risk engine has identified two VS Code extensions, a campaign we're calling MaliciousCorgi - 1.5 million combined installs, both live in the marketplace right now - that work exactly as promised. They answer your coding questions. They explain your errors. They also capture every file you open, every edit you make, and send it all to servers in China. No consent. No disclosure.&lt;/p&gt;
    &lt;head rend="h2"&gt;The MaliciousCorgi Campaign&lt;/head&gt;
    &lt;p&gt;Both are marketed as AI coding assistants. Both are functional. Both contain identical malicious code - the same spyware infrastructure running under different publisher names.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Perfect Cover&lt;/head&gt;
    &lt;p&gt;These extensions actually work. That's what makes them dangerous.&lt;/p&gt;
    &lt;p&gt;Select code, ask a question, get a helpful AI-powered response. The extension also provides inline autocomplete - just like GitHub Copilot. As you type, it reads about 20 lines of context around your cursor and sends it to the AI server for suggestions:&lt;/p&gt;
    &lt;p&gt;This is normal. This is expected. AI coding assistants need to read some of your code to help you write more code.&lt;/p&gt;
    &lt;p&gt;But these extensions go far beyond what's needed for autocomplete.&lt;/p&gt;
    &lt;p&gt;While the autocomplete sends ~20 lines around your cursor when you're actively typing, three hidden channels are running in parallel - collecting far more data, far more often, without any user interaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Channel 1: Real-Time File Monitoring&lt;/head&gt;
    &lt;p&gt;The moment you open any file - not interact with it, just open it - the extension reads its entire contents, encodes it as Base64, and sends it to a webview containing a hidden tracking iframe. Not 20 lines. The entire file.&lt;/p&gt;
    &lt;p&gt;And the same mechanism triggers on onDidChangeTextDocument - every single edit:&lt;/p&gt;
    &lt;p&gt;Not just what you're actively working on. Every file you glance at. Every character you type. Captured and transmitted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Channel 2: Mass File Harvesting (Server-Controlled)&lt;/head&gt;
    &lt;p&gt;And it gets worse.&lt;lb/&gt;The real-time monitoring captures files as you work. But the server doesn't have to wait for you to open anything.&lt;/p&gt;
    &lt;p&gt;The server can remotely trigger mass file collection at any time - without any user interaction.&lt;lb/&gt;The extension parses a jumpUrl field from server responses as JSON and executes it directly:&lt;/p&gt;
    &lt;p&gt;When the server sends {"type": "getFilesList"}, the extension triggers a full workspace harvest:&lt;/p&gt;
    &lt;p&gt;The attack chain is simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Server includes jumpUrl in any API response&lt;/item&gt;
      &lt;item&gt;Webview parses it as JSON and forwards to extension&lt;/item&gt;
      &lt;item&gt;Extension executes getFilesList command&lt;/item&gt;
      &lt;item&gt;Up to 50 files harvested and transmitted&lt;/item&gt;
      &lt;item&gt;User sees nothing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your codebase is one server command away from full exfiltration. No user interaction required.&lt;/p&gt;
    &lt;head rend="h2"&gt;Channel 3: The Profiling Engine&lt;/head&gt;
    &lt;p&gt;Channels 1 and 2 harvest your code. Channel 3 harvests you.&lt;/p&gt;
    &lt;p&gt;The extension's webview contains a hidden iframe - zero pixels, completely invisible - that loads four commercial analytics SDKs. This isn't website tracking. This is a profiling engine running inside your code editor.&lt;/p&gt;
    &lt;p&gt;We fetched the content served by this URL. The page title says it all: "ChatMossÊï∞ÊçÆÂüãÁÇπ" - "ChatMoss Data Tracking." It loads four separate analytics platforms - Zhuge.io, GrowingIO, TalkingData, and Baidu Analytics - each designed to track user behavior, build identity profiles, fingerprint devices, and monitor every interaction.&lt;/p&gt;
    &lt;p&gt;Combined, these SDKs build a detailed profile: who you are, where you are, what company you work for, what you're working on, what projects matter most to you.&lt;/p&gt;
    &lt;p&gt;Why collect all this metadata alongside your source code? One possibility: targeting. The server-controlled backdoor in Channel 2 can harvest 50 files on command. Analytics tells them whose files are worth taking - and when you're most active, profiling first, exfiltration second that's the MaliciousCorgi playbook.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's at Risk&lt;/head&gt;
    &lt;p&gt;Think about what's in your workspace right now.&lt;/p&gt;
    &lt;p&gt;Your .env files with API keys and database passwords. Your config files with server endpoints and internal URLs. That credentials.json for your cloud service account. Maybe some SSH keys you added for convenience. The source code itself - your algorithms, your business logic, the features you haven't shipped yet.&lt;/p&gt;
    &lt;p&gt;The file harvesting function grabs everything except images. Up to 50 files at a time, on command. Your secrets, your credentials, your proprietary code - all of it accessible to a server in China whenever they decide to pull the trigger.&lt;/p&gt;
    &lt;p&gt;And because Channel 3 is profiling you in the background, they know exactly who has the most valuable code to steal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Thoughts&lt;/head&gt;
    &lt;p&gt;We're in an AI tooling gold rush. Developers are installing extensions faster than ever - and that's the right call. These tools make you faster, smarter, more productive.&lt;/p&gt;
    &lt;p&gt;But there's a gap between how quickly we adopt and how thoroughly we verify. 1.5 million developers trusted these extensions because they worked. The marketplace approved them. The reviews were positive. The functionality is real. So is the spyware.&lt;/p&gt;
    &lt;p&gt;Moving fast with AI tools is good. Moving fast without visibility into what they're actually doing with your code is how 1.5 million developers ended up with their workspaces harvested.&lt;/p&gt;
    &lt;p&gt;Your team shouldn't have to choose between moving fast and staying safe. &lt;lb/&gt;Koi lets you do both. We analyze what extensions actually do after installation - not just what they claim. Scan your environment to find threats already running. Block malicious extensions before they're installed. Adopt AI tools at full speed, with full visibility.&lt;/p&gt;
    &lt;p&gt;Book a demo to see how Koi protects your developers without slowing them down.&lt;/p&gt;
    &lt;head rend="h2"&gt;IOCs&lt;/head&gt;
    &lt;p&gt;Vscode extensions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;whensunset.chatgpt-china&lt;/item&gt;
      &lt;item&gt;zhukunpeng.chat-moss&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Domain:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;aihao123.cn&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46855527</guid><pubDate>Mon, 02 Feb 2026 12:59:58 +0000</pubDate></item><item><title>Geologists may have solved mystery of Green River's 'uphill' route</title><link>https://phys.org/news/2026-01-geologists-mystery-green-river-uphill.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46855803</guid><pubDate>Mon, 02 Feb 2026 13:29:13 +0000</pubDate></item><item><title>A Legal Tool for Holding ICE Agents to Account, Hiding in Plain Sight</title><link>https://www.nytimes.com/2026/02/02/us/ice-lawsuits-states.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46856806</guid><pubDate>Mon, 02 Feb 2026 15:04:26 +0000</pubDate></item><item><title>Waymo Seeking About $16B Near $110B Valuation</title><link>https://www.bloomberg.com/news/articles/2026-01-31/waymo-seeking-about-16-billion-near-110-billion-valuation</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46856854</guid><pubDate>Mon, 02 Feb 2026 15:08:52 +0000</pubDate></item><item><title>Greenland tensions harden Europe's push for energy independence</title><link>https://www.ft.com/content/e9c90df9-ee03-4c51-bbd3-dad45e212961</link><description>&lt;doc fingerprint="be7b08554bcadbe9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;Greenland tensions harden Europe‚Äôs push for energy independence&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Save over 40% on Standard Digital&lt;/head&gt;was $540 now $299 for your first year&lt;p&gt;Save now on essential digital access to trusted FT journalism on any device. Savings based on monthly annualised price.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access with exclusive insights and industry deep dives on any device.&lt;/p&gt;&lt;p&gt;Delivery Monday - Saturday, including FT Weekend and FT Digital Edition: all the content of the FT newspaper on any device. Savings based on annual price.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46856928</guid><pubDate>Mon, 02 Feb 2026 15:14:36 +0000</pubDate></item><item><title>Rural Americans Are Trying to Hold Back the Tide of AI</title><link>https://www.wsj.com/politics/policy/these-rural-americans-are-trying-to-hold-back-the-tide-of-ai-66945306</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857082</guid><pubDate>Mon, 02 Feb 2026 15:26:52 +0000</pubDate></item></channel></rss>