<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 15 Oct 2025 11:09:12 +0000</lastBuildDate><item><title>DOJ seizes $15B in Bitcoin from 'pig butchering' scam based in Cambodia</title><link>https://www.cnbc.com/2025/10/14/bitcoin-doj-chen-zhi-pig-butchering-scam.html</link><description>&lt;doc fingerprint="b8b1f15a3f03d3d9"&gt;
  &lt;main&gt;
    &lt;p&gt;The Department of Justice has seized about $15 billion worth of bitcoin held in cryptocurrency wallets owned by a man who oversaw a massive "pig butchering" fraud operation based in Cambodia, prosecutors said Tuesday.&lt;/p&gt;
    &lt;p&gt;The seizure is the largest forfeiture action sought by the DOJ in history.&lt;/p&gt;
    &lt;p&gt;An indictment charging the alleged pig butcher, Chen Zhi, with wire fraud conspiracy and money laundering conspiracy was unsealed Tuesday in federal court in Brooklyn, New York.&lt;/p&gt;
    &lt;p&gt;Zhi, a 38-year-old Chinese-born emigre who is also known as "Vincent," remains at large, according to the U.S. Attorney's Office for the Eastern District of New York. Zhi faces up to 40 years in prison if convicted of the charges.&lt;/p&gt;
    &lt;p&gt;He was identified in court filings as the founder and chairman of Prince Holding Group, a multinational business conglomerate based in Cambodia, which prosecutors said grew "in secret .... into one of Asia's largest transnational criminal organizations." The Prince Group allegedly operates 10 scam compounds in Cambodia.&lt;/p&gt;
    &lt;p&gt;The Treasury Department, in parallel action on Tuesday, designated Prince Group as a transnational criminal organization and announced sanctions against the Zhi and more than 100 associated individuals and entities, for their roles in alleged illicit activity.&lt;/p&gt;
    &lt;p&gt;Brooklyn U.S. Attorney Joseph Nocella said that Zhi "directed one of the largest investment fraud operations in history, fueling an illicit industry that is reaching epidemic proportions."&lt;/p&gt;
    &lt;p&gt;"Prince Group's investment scams have caused billions of dollars in losses and untold misery to victims around the world, including here in New York, on the backs of individuals who have been trafficked and forced to work against their will," Nocella said.&lt;/p&gt;
    &lt;p&gt;The Prince Group, which operates businesses in more than 30 countries, ran "forced-labor scam compounds across Cambodia," the U.S. Attorney's Office said in a press release.&lt;/p&gt;
    &lt;p&gt;"Individuals held against their will in the compounds engaged in cryptocurrency investment fraud schemes, known as 'pig butchering' scams, that stole billions of dollars from victims in the United States and around the world," the release said.&lt;/p&gt;
    &lt;p&gt;The scams duped people contacted via social media and messaging applications online into transferring cryptocurrency into accounts controlled by the scheme with false promises that the crypto would be invested and produce profits, according to the office.&lt;/p&gt;
    &lt;p&gt;"In reality, the funds were stolen from the victims and laundered for the benefit of the perpetrators," the release said. "The scam perpetrators often built relationships with their victims over time, earning their trust before stealing their funds."&lt;/p&gt;
    &lt;p&gt;Prosecutors said that hundreds of people were trafficked and forced to work in the scam compounds, "often under the threat of violence."&lt;/p&gt;
    &lt;p&gt;Zhi and a network of top executives in the Prince Group are accused of using political influence in multiple countries to protect their criminal enterprise and paid bribes to public officials to avoid actions by law enforcement authorities targeting the scheme, according to prosecutors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45580981</guid><pubDate>Tue, 14 Oct 2025 15:08:38 +0000</pubDate></item><item><title>A modern approach to preventing CSRF in Go</title><link>https://www.alexedwards.net/blog/preventing-csrf-in-go</link><description>&lt;doc fingerprint="2f1e88507de31ea1"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Not sure how to structure your Go web application?&lt;/head&gt;&lt;p&gt;My new book guides you through the start-to-finish build of a real world web application in Go — covering topics like how to structure your code, manage dependencies, create dynamic database-driven pages, and how to authenticate and authorize users securely.&lt;/p&gt;Take a look!&lt;p&gt;Go 1.25 introduced a new &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; middleware to the standard library — and it got me wondering:&lt;/p&gt;&lt;p&gt;Have we finally reached the point where CSRF attacks can be prevented without relying on a token-based check (like double-submit cookies)? Can we build secure web applications without bringing in third-party packages like &lt;code&gt;justinas/nosurf&lt;/code&gt; or &lt;code&gt;gorilla/csrf&lt;/code&gt;?&lt;/p&gt;&lt;p&gt;And I think the answer now may be a cautious “yes” — so long as a few important conditions are met.&lt;/p&gt;&lt;p&gt;If you want to skip the explanations and just want to see what those conditions are, you can click here.&lt;/p&gt;&lt;head rend="h2"&gt;The http.CrossOriginProtection middleware&lt;/head&gt;&lt;p&gt;The new &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; middleware works by checking the values in a request's &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; and
&lt;code&gt;Origin&lt;/code&gt; headers to determine where the request is coming from.
It will automatically reject any non-safe requests that are not from the same origin, and will send the client a
&lt;code&gt;403 Forbidden&lt;/code&gt; response.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; middleware has some limitations, which we'll discuss in a moment, but it is robust and simple to use, and a great addition to the standard library.&lt;/p&gt;&lt;p&gt;At its simplest, you can use it like this:&lt;/p&gt;&lt;p&gt;If you want, it's also possible to configure the behavior of &lt;code&gt;http.CrossOriginProtection&lt;/code&gt;. Configuration options include being able to add trusted origins (from which cross-origin requests are allowed), and the ability to use a custom handler for rejected requests instead of the default &lt;code&gt;403 Forbidden&lt;/code&gt; response.&lt;/p&gt;&lt;p&gt;When I've wanted to customize the behavior, I've been using a pattern like this:&lt;/p&gt;&lt;head rend="h2"&gt;Limitations&lt;/head&gt;&lt;p&gt;The big limitation of &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; is that it is only effective at blocking requests from modern browsers. Your application will still be vulnerable to CSRF attacks coming from older (generally pre-2020) browsers which do not include at least one of the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; or &lt;code&gt;Origin&lt;/code&gt; headers in requests.&lt;/p&gt;&lt;p&gt;Right now, browser support for the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; header is at 92%, and for &lt;code&gt;Origin&lt;/code&gt; it is 95%. So — in general — relying on &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; is not sufficient as your only protection against CSRF.&lt;/p&gt;&lt;p&gt;It's also important to note that the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; header is only sent when your application has a "trustworthy origin" — which basically means that your application needs to be using HTTPS in production for &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; to work to its full potential.&lt;/p&gt;&lt;p&gt;And you should also be aware that when no &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; header is present in a request, and it falls back to comparing the &lt;code&gt;Origin&lt;/code&gt; and &lt;code&gt;Host&lt;/code&gt; headers, the &lt;code&gt;Host&lt;/code&gt; header does not include the scheme. This limitation means that &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; will wrongly allow cross-origin requests from &lt;code&gt;http://{host}&lt;/code&gt; to &lt;code&gt;https://{host}&lt;/code&gt; when there is no &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; header present but there is an &lt;code&gt;Origin&lt;/code&gt; header. To mitigate this risk, you should ideally configure your application to use HTTP Strict Transport Security (HSTS).&lt;/p&gt;&lt;head rend="h2"&gt;Enforcing TLS 1.3&lt;/head&gt;&lt;p&gt;Looking into this got me wondering... What if you're already planning to use HTTPS and enforce TLS 1.3 as the minimum supported TLS version? Could you be confident that all web browsers which support TLS 1.3 also support either the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; or &lt;code&gt;Origin&lt;/code&gt; headers?&lt;/p&gt;&lt;p&gt;As far as I can tell from the MDN compatibility data and tables from Can I Use, the answer is "yes" for (almost) all major browsers.&lt;/p&gt;If you enforce TLS 1.3 as the minimum version:&lt;list rend="ul"&gt;&lt;item&gt;Older browsers which don't support TLS 1.3 simply won't be able to connect to your application.&lt;/item&gt;&lt;item&gt;For the modern major browsers that do support TLS 1.3 and can connect, you can be confident that at least one of the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt;or&lt;code&gt;Origin&lt;/code&gt;headers are supported — and therefore&lt;code&gt;http.CrossOriginProtection&lt;/code&gt;will work effectively.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The only exception to this I can see is Firefox v60-69 (2018-2019), which did not support the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt; header and did not send the &lt;code&gt;Origin&lt;/code&gt; header for &lt;code&gt;POST&lt;/code&gt; requests. This means that &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; will not work effectively to block requests originating from that browser. Can I Use puts usage of Firefox v60-69 at 0%, so the risk here appears very low — but there are probably some computers somewhere in the world still running it.&lt;/p&gt;&lt;p&gt;Also, we only have this information for the major browsers — Chrome/Chromium, Firefox, Edge, Safari, Opera and Internet Explorer. But of course, other browsers exist. Most of them are forks of Chromium or Firefox and therefore will likely be OK, but there's no guarantee here and it is hard to quantify the risk.&lt;/p&gt;&lt;p&gt;So if you use HTTPS and enforce TLS 1.3, it's a huge step forward in making sure that &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; can work effectively. However, there remains a non-zero risk that comes from Firefox v60-69 and non-major browsers, so you may want to add some defense-in-depth and utilize &lt;code&gt;SameSite&lt;/code&gt; cookies too.&lt;/p&gt;&lt;p&gt;We'll talk more about &lt;code&gt;SameSite&lt;/code&gt; cookies in a moment, but first we need to take a quick detour and discuss the difference between the terms origin and site.&lt;/p&gt;&lt;head rend="h2"&gt;Cross-site vs cross-origin&lt;/head&gt;&lt;p&gt;In the world of web specifications and web browsers, cross-site and cross-origin are subtly different things, and in a security context like this it's important to understand the difference and be exact about what we mean.&lt;/p&gt;&lt;p&gt;I'll quickly explain.&lt;/p&gt;&lt;p&gt;Two websites have the same origin if they share the exact same scheme, hostname, and port (if present). So &lt;code&gt;https://example.com&lt;/code&gt; and &lt;code&gt;https://www.example.com&lt;/code&gt; are not the same origin because the hostnames (&lt;code&gt;example.com&lt;/code&gt; and &lt;code&gt;www.example.com&lt;/code&gt;) are different. A request between them would be cross-origin.&lt;/p&gt;&lt;p&gt;Two websites are 'same site' if they share the same scheme and registerable domain.&lt;/p&gt;&lt;p&gt;So &lt;code&gt;https://example.com&lt;/code&gt;, &lt;code&gt;https://www.example.com&lt;/code&gt; and &lt;code&gt;https://login.admin.example.com&lt;/code&gt; are all considered to be the same site because the scheme (&lt;code&gt;https&lt;/code&gt;) and registerable domain (&lt;code&gt;example.com&lt;/code&gt;) are the same. A request between these would not be considered to be cross-site, but it would be cross-origin.&lt;/p&gt;&lt;p&gt;So what are the points that I'm building up to here?&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Go's&lt;/p&gt;&lt;code&gt;http.CrossOriginProtection&lt;/code&gt;middleware is accurately and appropriately named. It blocks cross-origin requests. It's more strict than it would be if it only blocked cross-site requests, because it also blocks requests from other origins under the same site (i.e. registrable domain).&lt;p&gt;This is useful because it helps to prevent a situation where your janky-not-been-updated-in-the-last-decade WordPress blog at&lt;/p&gt;&lt;code&gt;https://blog.example.com&lt;/code&gt;is compromised and used to launch a request forgery attack at your important&lt;code&gt;https://admin.example.com&lt;/code&gt;website.&lt;/item&gt;&lt;item&gt;&lt;p&gt;When most people — myself included — casually talk about "CSRF attacks", what we are referring to most of the time is actually cross-origin request forgery, not just cross-site request forgery. It's a shame that CSRF is the commonly used and known acronym to describe this family of attacks, because most of the time CORF would be more accurate and appropriate. But hey! That's the messy world we live in.&lt;/p&gt;&lt;p&gt;For the rest of this post though, I'll use the term CORF instead of CSRF when that is exactly what I mean.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;SameSite cookies&lt;/head&gt;&lt;p&gt;The &lt;code&gt;SameSite&lt;/code&gt; cookie attribute has generally been supported by web browsers since 2017, and by Go since v1.11. If you set the &lt;code&gt;SameSite=Lax&lt;/code&gt; or &lt;code&gt;SameSite=Strict&lt;/code&gt; attributes on a cookie, that cookie will only be included in requests to the same site that set it. In turn, that prevents cross-site request forgery attacks (but not cross-origin attacks from within the same site).

&lt;/p&gt;&lt;p&gt;There is some good news here — all major browsers that support TLS 1.3 also fully support &lt;code&gt;SameSite&lt;/code&gt; cookies, with no exceptions that I can see. So if you enforce TLS 1.3, you can be confident that all the major browsers using your application will respect the &lt;code&gt;SameSite&lt;/code&gt; attribute.&lt;/p&gt;&lt;p&gt;This means that by using &lt;code&gt;SameSite=Lax&lt;/code&gt; or &lt;code&gt;SameSite=Strict&lt;/code&gt; on your cookies, you cover off the risk of cross-site request forgeries from Firefox v60-69 that we talked about earlier.&lt;/p&gt;&lt;head rend="h2"&gt;Putting it all together&lt;/head&gt;&lt;p&gt;If you combine using HTTPS, enforcing TLS 1.3 as the minimum version, using &lt;code&gt;SameSite=Lax&lt;/code&gt; or &lt;code&gt;SameSite=Strict&lt;/code&gt; cookies appropriately, and using the &lt;code&gt;http.CrossOriginProtection&lt;/code&gt; middleware in your application, as far as I can see there are only two unmitigated CSRF/CORF risks from major browsers:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;CORF attacks from within the same site (i.e. from another subdomain under your registrable domain) in Firefox v60-69.&lt;/item&gt;&lt;item&gt;CORF attacks from a HTTP version of your origin, from browsers that do not support the &lt;code&gt;Sec-Fetch-Site&lt;/code&gt;header.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;For the first of these risks, if you don't have any other websites under your registrable domain, or you're confident that the websites are secure and uncompromised, then this might be a risk that you're willing to accept given the extremely low usage of Firefox v60-69.&lt;/p&gt;&lt;p&gt;For the second, if you don't support HTTP on your origin at all (including redirects) then this isn't something you need to worry about. Otherwise, you can mitigate the risk by including a HSTS header on your HTTPS responses.&lt;/p&gt;&lt;p&gt;At the start of this article, I said that not using a token-based CSRF check might be OK under certain conditions. So let's run through what those are:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Your application uses HTTPS and enforces TLS 1.3 as the minimum version. You accept that users with older browsers will not be able to connect to your application at all.&lt;/item&gt;&lt;item&gt;You follow good-practice and never change important application state in response to requests with the safe methods &lt;code&gt;GET&lt;/code&gt;,&lt;code&gt;HEAD&lt;/code&gt;,&lt;code&gt;OPTIONS&lt;/code&gt;or&lt;code&gt;TRACE&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;You use both the &lt;code&gt;http.CrossOriginProtection&lt;/code&gt;middleware and&lt;code&gt;SameSite=Lax&lt;/code&gt;or&lt;code&gt;SameSite=Strict&lt;/code&gt;cookies. It's important to still use&lt;code&gt;SameSite&lt;/code&gt;cookies for general defense in depth, but more specifically to mitigate CSRF attacks from Firefox v60-69.&lt;/item&gt;&lt;item&gt;Because of the unprotected risk of a same-site CORF attack from Firefox v60-69, you either don't have any other websites under your registrable domain, or you're confident that they're secure and uncompromised.&lt;/item&gt;&lt;item&gt;There is either no HTTP version of your application origin at all, or you include a HSTS header on your HTTPS responses.&lt;/item&gt;&lt;item&gt;Finally, you are willing to accept the difficult-to-quantify risk of CSRF/CORF attacks from non-major browsers that support TLS 1.3 but don't support the &lt;code&gt;Origin&lt;/code&gt;header,&lt;code&gt;Sec-Fetch-Site&lt;/code&gt;header or&lt;code&gt;SameSite&lt;/code&gt;cookies. Does any such browser exist? I don't know, and I'm not sure there's a way to answer that question with 100% confidence. So you'll need to do your own risk assessment here, and it's a risk that you probably only want to accept if your application is a low-value target and the impact of a successful CSRF/CORF attack is both isolated and minor.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;If you enjoyed this post...&lt;/head&gt;&lt;p&gt;You might like to check out my other Go tutorials on this site, or if you're after something more structured, my books Let's Go and Let's Go Further cover how to build complete, production-ready, web apps and APIS with Go.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45581288</guid><pubDate>Tue, 14 Oct 2025 15:34:51 +0000</pubDate></item><item><title>How AI hears accents: An audible visualization of accent clusters</title><link>https://accent-explorer.boldvoice.com/</link><description>&lt;doc fingerprint="35bba6d935b3db90"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we’re going to go on a tour of the world's accents in English. Users of BoldVoice, the American accent training app, speak more than 200 different languages, and it is our mission to help them speak English clearly and confidently. While building the accent strength metric we covered in the previous blog post, we needed to understand how our models clustered accents, dialects, native languages, and language families. Today, we will share some of our findings using a 3D latent visualization.&lt;/p&gt;
    &lt;p&gt;To begin, we finetuned HuBERT, a pretrained audio-only foundation model for the task of accent identification using our in-house dataset of non-native English speech and self-reported accents. BoldVoice’s own dataset of accented speech is one of the largest of its kind in the world.&lt;/p&gt;
    &lt;p&gt;This model receives only the raw input audio and associated accent label; it gets neither a text prompt nor a transcript. For this "finetuning", we sampled 30 million speech recordings comprising 25,000 hours of English speech - a small fraction of our total accent dataset. Unlike a traditional finetune, we unfroze all layers of the pretrained base model due to the large size of our dataset. We trained the model for roughly a week on a cluster of A100 GPUs.&lt;/p&gt;
    &lt;p&gt;While the accent identifier performs quite well across the top hundred or so accents (play with it yourself at accentoracle.com), for today, we are less interested in its raw performance, and more interested in the clustering of accents in its latent space.&lt;/p&gt;
    &lt;p&gt;To observe how accents cluster, we've provided an audible latent space visualization for a small subset of recordings. Hover on the points on the graph to see the language labels.&lt;/p&gt;
    &lt;p&gt;The visualization is created by applying the UMAP dimensionality reduction technique to reduce the 768-dimensional latent space to just 3 dimensions.&lt;/p&gt;
    &lt;p&gt;Note that UMAP destroys much of the information in the full-dimensional latent space, but roughly preserves the global structure, including the relative distances between clusters. Each point represents a single recording inferenced by the model after it was fine tuned and the color corresponds to the true accent label.&lt;/p&gt;
    &lt;p&gt;Finally, in order to denoise the clusters, we cherry-pick only those points for which the predicted and target accents match. Remember, the purpose of this visualization is not to help us assess the performance of the model, but to understand where it has placed accents relative to one another.&lt;/p&gt;
    &lt;p&gt;By clicking or tapping on a point, you will hear a standardized version of the corresponding recording. The reason for voice standardization is two-fold: first, it anonymizes the speaker in the original recordings in order to protect their privacy. Second, it allows us to hear each accent projected onto a neutral voice, making it easier to hear the accent differences and ignore extraneous differences like gender, recording quality, and background noise. However, there is no free lunch: it does not perfectly preserve the source accent and introduces some audible phonetic artifacts.&lt;/p&gt;
    &lt;p&gt;This voice standardization model is an in-house accent-preserving voice conversion model.&lt;/p&gt;
    &lt;p&gt;Please explore the latent space visualization. You can click, drag, zoom, and scroll to navigate. You can also isolate accents by double clicking them in the legend to the right (desktop only) – double-clicking again will undo the filter.&lt;/p&gt;
    &lt;p&gt;Meanwhile, think about the following questions: which accents would you expect to be clustered together? Do you expect them to follow the taxonomy of language families or to cluster in other ways?&lt;/p&gt;
    &lt;p&gt;Our team was most surprised to see that geographic proximity, immigration, and colonialism seem to affect this model's learned accent groupings more than language taxonomy. Click the button below to explore our first grouping.&lt;/p&gt;
    &lt;p&gt;For example, the Australian cluster is right next to the Vietnamese cluster despite the fact that English and Vietnamese are not related taxonomically. If you listen to the 10 points that make up a bridge between the two clusters, you hear what sounds like native Vietnamese speakers who speak English with an Australian accent. Perhaps these hybrid accents could explain the overall proximity of these clusters.&lt;/p&gt;
    &lt;p&gt;We see something similar for the French/Nigerian/Ghanaian grouping.&lt;/p&gt;
    &lt;p&gt;It's important to remember that the distances on this map are not an objective measure of the phonetic similarity between accents. They are a byproduct of a model which has successfully learned to distinguish a variety of accents in L2 English speech from audio alone with no knowledge of language or linguistics.&lt;/p&gt;
    &lt;p&gt;Next, take a look at the Indian subcontinent accent cluster. Note that the Telugu, Tamil, and Malayalam accents are grouped together at one end of the cluster, and the Nepali and Bengali accents are at the other. This roughly mirrors geography, where Telugu, Tamil, and Malayalam are widely spoken languages in southern India, and Bengali and Nepali are widely spoken in northwest India and Nepal.&lt;/p&gt;
    &lt;p&gt;Finally, let's scroll to the Mongolian cluster, where the nearest cluster is actually Korean.&lt;/p&gt;
    &lt;p&gt;Experts and non-experts have observed phonetic similarities between Mongolian and Korean. A now-refuted hypothesis called the "Altaic language family" once grouped them together.&lt;/p&gt;
    &lt;p&gt;It is interesting that this model, with no concept of language families, has also picked up on the phonetic similarities even as filtered through a second language (English).&lt;/p&gt;
    &lt;p&gt;What do you think? Is this a meaningless artifact of latent space visualization or evidence of real phonetic features diffusing between Korean and Mongolian?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45581735</guid><pubDate>Tue, 14 Oct 2025 16:07:37 +0000</pubDate></item><item><title>How bad can a $2.97 ADC be?</title><link>https://excamera.substack.com/p/how-bad-can-a-297-adc-be</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45582462</guid><pubDate>Tue, 14 Oct 2025 17:12:10 +0000</pubDate></item><item><title>SmolBSD – build your own minimal BSD system</title><link>https://smolbsd.org</link><description>&lt;doc fingerprint="19dcff71d549ada5"&gt;
  &lt;main&gt;
    &lt;p&gt;build your own minimal BSD UNIX system&lt;/p&gt;
    &lt;p&gt;smolBSD is a meta-operating system built on top of NetBSD. It lets you compose your own UNIX environment â from a single-purpose microservice system to a fully-custom OS image â in just a few minutes.&lt;/p&gt;
    &lt;p&gt;The smolBSD environment uses the netbsd-MICROVM kernel as its foundation, leveraging the same portable, reliable codebase that powers NetBSD itself. You decide what to include â sshd, httpd, or your own service â and smolBSD builds a coherent, minimal, bootable image ready to run anywhere.&lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=bozohttpd build â¡ï¸ starting the builder microvm â¡ï¸ host filesystem mounted on /mnt â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: bozohttpd-amd64.img&lt;/quote&gt;
    &lt;p&gt;Build BSD systems like you build software â fast, reproducible, and minimal.&lt;/p&gt;
    &lt;p&gt;Pick only the components you need â from kernel to services.&lt;/p&gt;
    &lt;p&gt;Every build is deterministic, portable, and easy to version-control.&lt;/p&gt;
    &lt;p&gt;Powered by netbsd-MICROVM â boot to service in milliseconds.&lt;/p&gt;
    &lt;p&gt;Runs anywhere QEMU or Firecracker runs â cloud, CI, edge, or laptop.&lt;/p&gt;
    &lt;p&gt;Build and boot your own BSD system in seconds:&lt;/p&gt;
    &lt;quote&gt;$ git clone https://github.com/NetBSDfr/smolBSD $ cd smolBSD $ bmake SERVICE=sshd build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: sshd-amd64.img â¡ï¸ killing the builder microvm $ ./startnb.sh -f etc/sshd.conf [ 1.0092096] kernel boot time: 14ms Starting sshd. Server listening on :: port 22. Server listening on 0.0.0.0 port 22. $ ssh -p 2022 ssh@localhostDownload&lt;/quote&gt;
    &lt;p&gt; A complete static web server in a few megabytes. smolBSD builds a minimal system with &lt;code&gt;bozohttpd&lt;/code&gt;
preconfigured and ready to serve content immediately on boot.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=bozohttpd build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: bozohttpd-amd64.img $ ./startnb.sh -f etc/bozohttpd.conf [ 1.001231] kernel boot time: 10ms Starting bozohttpd on :80 listening on 0.0.0.0:80&lt;/quote&gt;
    &lt;p&gt; A lightweight build and image creation service based entirely on NetBSD tools. The &lt;code&gt;nbakery&lt;/code&gt; image gives you a taste of a preconfigured NetBSD environment with all the well known tools.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=nbakery build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: nbakery-amd64.img $ ./startnb.sh -f etc/nbakery.conf [ 1.008374] kernel boot time: 11ms Welcome to the (n)bakery! ð§ ðª doas&lt;command&gt;to run command as root ð¦ pkgin to manage packages ðª exit to cleanly shutdown, ^a-x to exit qemu ðª you are inside a tmux with prefix ^q&lt;/command&gt;&lt;/quote&gt;
    &lt;p&gt; A minimal secure shell server started with &lt;code&gt;nitro&lt;/code&gt;,
designed to launch instantly and provide remote access with zero unnecessary
services.  
          Ideal for an SSH bouncer.
        &lt;/p&gt;
    &lt;quote&gt;$ bmake SERVICE=nitrosshd build â¡ï¸ starting the builder microvm â¡ï¸ fetching sets â¡ï¸ creating root filesystem (512M) done â image ready: nitrosshd-amd64.img $ ./startnb.sh -f etc/sshd.conf [ 1.011598] kernel boot time: 12ms Created tmpfs /dev (1835008 byte, 3552 inodes) Starting sshd. Server listening on :: port 22. Server listening on 0.0.0.0 port 22.&lt;/quote&gt;
    &lt;p&gt;smolBSD is an independent project built on top of NetBSD. Join us, share your micro-systems, or contribute new services and build recipes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45582758</guid><pubDate>Tue, 14 Oct 2025 17:43:33 +0000</pubDate></item><item><title>Beliefs that are true for regular software but false when applied to AI</title><link>https://boydkane.com/essays/boss</link><description>&lt;doc fingerprint="ac3c1b1a6d77819d"&gt;
  &lt;main&gt;
    &lt;p&gt;(a note for technical folk)1 | read as pdf | Substack | LessWrong&lt;/p&gt;
    &lt;p&gt;When it comes to understanding the dangers of AI systems, the general public has the worst kind of knowledge: that what you know for sure that just ainât so.&lt;/p&gt;
    &lt;p&gt;After 40 years of persistent badgering, the software industry has convinced the public that bugs can have disastrous consequences. This is great! It is good that people understand that software can result in real-world harm. Not only does the general public mostly understand the dangers, but they mostly understand that bugs can be fixed. It might be expensive, it might be difficult, but it can be done.&lt;/p&gt;
    &lt;p&gt;The problem is that this understanding, when applied to AIs like ChatGPT, is completely wrong. The software that runs AI acts very differently to the software that runs most of your computer or your phone. Good, sensible assumptions about bugs in regular software actually end up being harmful and misleading when you try to apply them to AI.&lt;/p&gt;
    &lt;p&gt;Attempting to apply regular-software assumptions to AI systems leads to confusion, and remarks such as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âIf something goes wrong with ChatGPT, canât some boffin just think hard for a bit, find the missing semi-colon or whatever, and then fix the bug?â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âEven if itâs hard for one person to understand everything the AI does, surely still smart people who individually understand small parts of what the AI does?â.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âJust because current systems donât work perfectly, thatâs not a problem right? Because eventually weâll iron out all the bugs so the AIs will get more reliable over time, like old software is more reliable than new software.â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you understand how modern AI systems work, these statements are all painfully incorrect. But if youâre used to regular software, theyâre completely reasonable. I believe there is a gap between the experts and the novices in the field:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the experts donât see the gap because itâs so obvious, so they donât bother explaining the gap&lt;/item&gt;
      &lt;item&gt;the novices donât see the gap because they donât know to look, so they donât realise where their confusion comes from.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This leads to frustration on both sides, because the experts feel like their arguments arenât hitting home, and the novices feel like all arguments have obvious flaws. In reality, the experts and the novices have different, unspoken, assumptions about how AI systems work.&lt;/p&gt;
    &lt;head rend="h1"&gt;Some example false beliefs&lt;/head&gt;
    &lt;p&gt;To make this more concrete, here are some example ideas that are perfectly true when applied to regular software but become harmfully false when applied to modern AIs:&lt;/p&gt;
    &lt;head rend="h2"&gt;Software vulnerabilities are caused by mistakes in the code&lt;/head&gt;
    &lt;p&gt;In regular software, vulnerabilities are caused by mistakes in the lines of code that make up the software. There might be hundreds of thousands of lines of code, but code doesnât take up much space so this is only around 50MB of data, about the size of a small album of photos.&lt;/p&gt;
    &lt;p&gt;But in modern AI systems, vulnerabilities or bugs are usually caused by problems in the data used to train an AI2. It takes thousands of gigabytes of data to train modern AI systems, and bad behaviour isnât caused by any single bad piece of data, but by the combined effects of significant fractions of the dataset. Because these datasets are so large, nobody knows everything that an AI is actually trained on. One popular dataset, FineWeb, is about 11.25 trillion words long3, which, if you were reading at about 250 words per minute, would take you over 85 thousand years to read. Itâs just not possible for any single human (or even a team of humans) to have read everything that an LLM has read during training.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bugs in the code can be found by carefully analysing the code&lt;/head&gt;
    &lt;p&gt;With regular software, if thereâs a bug, itâs possible for smart people to carefully read through the code and logically figure out what must be causing the bug.&lt;/p&gt;
    &lt;p&gt;With AI systems, almost all bad behaviour originates from the data thatâs used to train them2, but itâs basically impossible to look at misbehaving AI and figure out parts of the training data caused that bad behaviour. In practice, itâs rare to even attempt this, researchers will retrain the AI with more data to try and counteract the bad behaviour, or theyâll start over and try to curate the data to not include the bad data.&lt;/p&gt;
    &lt;p&gt;You cannot logically deduce what pieces of data caused the bad behaviour, you can only make good guesses. For example, modern AIs are trained on lots of mathematics proofs and programming tasks, because that seems to make them do better at reasoning and logical thinking tasks. If an AI system makes a logical reasoning mistake, itâs impossible to attribute that mistake to any portion of the training data, the only answer weâve got is to use more data next time.&lt;/p&gt;
    &lt;p&gt;I think I need to emphasise this: With regular software, we can pinpoint mistakes precisely, walk step-by-step through the events leading up to the mistake, and logically understand why that mistake happened. When AIs make mistakes, we donât understand the steps that caused those mistakes. Even the people who made the AIs donât understand why they make mistakes4. Nobody understands where these bugs come from. We sometimes kinda have a rough idea about why they maybe did something unusual. But weâre far, far away from anything that guarantees the AI wonât have any catastrophic failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once a bug is fixed, it wonât come back again&lt;/head&gt;
    &lt;p&gt;With regular software, once youâve found the bug, you can fix the bug. And once youâve fixed the bug, it wonât re-appear5. There might be a bug that causes similar problems, but itâs not the same bug as the one you fixed. This means you can, if youâre patient, reduce the number of bugs over time and rest assured that removing new bugs wonât cause old bugs to re-appear.&lt;/p&gt;
    &lt;p&gt;This is not the case with AI. Itâs not really possible to âfixâ a bug in an AI, because even if the AI was behaving weirdly, and you retrained it, and now itâs not behaving weirdly anymore, you canât know for sure that the weird behaviour is gone, just that it doesnât happen for the prompts you tested. Itâs entirely possible that someone can find a prompt you forgot to test, and then the buggy behaviour is back again!&lt;/p&gt;
    &lt;head rend="h2"&gt;Every time you run the code, the same thing happens&lt;/head&gt;
    &lt;p&gt;With regular software, you can run the same piece of code multiple times and itâll behave in the same way. If you give it the same input, itâll give you the same output.&lt;/p&gt;
    &lt;p&gt;Now technically this is still true for AIs, if you give them exactly the prompt theyâll respond in exactly the same way. But practically, itâs very far from the truth6. Even tiny changes to the input of an AI can have dramatic changes in the output. Even innocent changes like adding a question mark at the end of your sentence or forgetting to start your sentence with a capital letter can cause the AI to return something different.&lt;/p&gt;
    &lt;p&gt;Additionally, most AI companies will slightly change the way their AIs respond, so that they say slightly different things to the same prompt. This helps their AIs seem less robotic and more natural.&lt;/p&gt;
    &lt;head rend="h2"&gt;If you give specifications beforehand, you can get software that meets those specifications&lt;/head&gt;
    &lt;p&gt;With regular software, this is true. You can sit with stakeholders to discuss the requirements for some piece of software, and then write code to meet those requirements. The requirements might change, but fundamentally you can write code to serve some specific purpose and have confidence that it will serve that specific purpose.&lt;/p&gt;
    &lt;p&gt;With AI systems, this is more or less false. Or at the very least, the creators of modern AI systems have far far less control about the behaviour the AIs will exhibit. We understand how to get an AI to meet narrow, testable specifications like speaking English and writing code, but we donât know how to get a brand new AI to achieve a certain score on some particular test or to guarantee global behaviour like ânever tells the user to commit a crimeâ. The best AI companies in the world have basically one lever which is âbetterâ, and they can pull that lever to make the AI better, but nobody knows precisely what to do to ensure an AI writes formal emails correctly or summarises text accurately.&lt;/p&gt;
    &lt;p&gt;This means that we donât know what an AI will be capable of before weâve trained it. Itâs very common for AIs to be released to the public for months before a random person on Twitter discovers some ability that the AI has which even its creators didnât know about. So far, these abilities have been mostly just fun, like being good at Geoguessr:&lt;/p&gt;
    &lt;p&gt;Or making photos look like they were from a Studio Ghibli film:&lt;/p&gt;
    &lt;p&gt;But thereâs no reason for these hidden abilities to always be positive. Itâs entirely possible that some dangerous capability is hidden in ChatGPT, but nobodyâs figured out the right prompt just yet.&lt;/p&gt;
    &lt;p&gt;While itâs possible to demonstrate the safety of an AI for a specific test suite or a known threat, itâs impossible for AI creators to definitively say their AI will never act maliciously or dangerously for any prompt it could be given.&lt;/p&gt;
    &lt;head rend="h1"&gt;Where to go from here&lt;/head&gt;
    &lt;p&gt;It is good that most people know the dangers of poorly written or buggy software. But this hard-won knowledge about regular software is misleading the public when it gets applied to AI. Despite the cries of âinscrutable arrays of floating point numbersâ, Iâd be surprised if a majority of people know that modern AI is architecturally different from regular software.&lt;/p&gt;
    &lt;p&gt;AI safety is a complicated and subtle argument. The best we can do is to make sure weâre starting from the same baseline, and that means conveying to our contemporaries that if it all starts to go wrong, we cannot just âpatch the bugâ7.&lt;/p&gt;
    &lt;p&gt;If this essay was the first time you realised AI was fundamentally different from regular software, let me know, and share this with a friend who might also not realise the difference.&lt;/p&gt;
    &lt;p&gt;If you always knew that regular software and AIs are fundamentally different, talk to your family and non-technical friends, or with a stranger at a coffee shop. I think youâll be surprised at how few people know that these two are different.&lt;/p&gt;
    &lt;p&gt;If youâre interested the dynamics between experts and novices, and how gaps between them arise, Iâve written more about the systemic biases encountered by experts (and the difficulties endured by novices) in this essay: Experts have it easy.&lt;/p&gt;
    &lt;p&gt;Thanks to Sam Cross and Caleb for reviewing drafts of this essay.&lt;/p&gt;
    &lt;p&gt;Discuss this essay:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HackerNews (284 points, 218 comments)&lt;/item&gt;
      &lt;item&gt;r/programming (55 points, 7 comments)&lt;/item&gt;
      &lt;item&gt;/slatestarcodex (17 points, 19 comments)&lt;/item&gt;
      &lt;item&gt;Lobse.rs (12 points, 4 comments)&lt;/item&gt;
      &lt;item&gt;LessWrong (9 points, 2 comments)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This article is attempting to bridge a gap between the technical and the non-technical, so Iâm going to be quite lax with the jargon here and there. By âAIâ Iâm referring to 2025 frontier LLMs. Iâm also going to be making some sweeping statements about âhow software worksâ, these claims mostly hold, but they break down when applied to distributed systems, parallel code, or complex interactions between software systems and human processes. Feel free to debate me in the comments if you think this piece discussing how experts struggle to empathise with novices should have had more jargon (: â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It can also come from the reward model used during RLHF, but the reward model is still trained from data at the end of the day. It can also come from prompt injections, but those also only work because of the data. â© â©2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;FineWeb is 15 trillion tokens, each token is about 0.75 words, 11.25 trillion words. â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Anthropic is doing very good work to try and figure out why AIs think the way they do, but even the state of the art does not have a full understanding of these AIs, and what understanding we do have, is often partial and with significant gaps. â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You are writing tests to prevent regressions, right? right?! â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;see for example, this blog post from Mira Muratiâs Thinking Machines Lab â©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Personally, I think some more empathy is needed when having good faith discussions with non-technical folk. Communication is empirically hard, in that it often goes wrong in practice, even if it feels easy to do. â©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45583180</guid><pubDate>Tue, 14 Oct 2025 18:26:00 +0000</pubDate></item><item><title>Intel Announces Inference-Optimized Xe3P Graphics Card with 160GB VRAM</title><link>https://www.phoronix.com/review/intel-crescent-island</link><description>&lt;doc fingerprint="a4591a3d988e20a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intel Announces "Crescent Island" Inference-Optimized Xe3P Graphics Card With 160GB vRAM&lt;/head&gt;
    &lt;p&gt;Back during the Intel Tech Tour in Arizona, Intel teased a new inference-optimized enterprise GPU would be announced soon. This new product would feature enhanced memory, bandwidth, and enterprise-level AI inference capabilities. Today the embargo expires on talking about this new GPU offering.&lt;/p&gt;
    &lt;p&gt;When Intel was teasing this new inference-optimized GPU a few weeks back in Arizona it sounded like Intel may have had an unexpected trick up its sleeves. What's being announced today is indeed a new enterprise GPU for AI that is interesting from a technology perspective, but it's not shipping until at least H2'2026. So while there was hope that perhaps Intel had managed to innovate some interesting Battlemage / BMG-G31 part for AI or the like with lots of vRAM, what's being announced is a next-gen part but one that is at least one year away still.&lt;/p&gt;
    &lt;p&gt;This new graphics card is codenamed Crescent Island and is built on their next-gen Xe3P Celestial micro-architecture. Xe3P will be optimized around performance-per-Watt and Crescent Island will feature 160GB of LPDDR5x memory to allow for plenty of space for large language models (LLMs).&lt;/p&gt;
    &lt;p&gt;Intel's embargoed announcement also notes that Crescent Island will feature support for a variety of different data types and be an "ideal" solution for tokens-as-a-service providers and inference use cases.&lt;/p&gt;
    &lt;p&gt;In addition to being optimized around performance-per-Watt, Crescent Island will also be air cooled and cost-optimized. Intel is currently working on refining their open-source software stack for Crescent Island via using current-generation Arc Pro B-Series GPUs.&lt;/p&gt;
    &lt;p&gt;Intel's announcement notes that customer sampling of this new data center GPU will begin in the second half of 2026. No official release timeframe was provided if they also hope to squeeze it out next year or if (more than likely) it will actually ship more broadly in 2027 but just noting their customer sampling for H2'2026 in the embargoed news release. No slides or prototype images or anything else to share today on Intel's Crescent Island.&lt;/p&gt;
    &lt;p&gt;Long story short, Intel is announcing Crescent Island today as a Xe3LP + 160GB LPDDR5X offering for H2'2026 or later that will be AI inference optimized around power efficiency and cost. It sounds interesting but technical details beyond those basics were light and it's going to be a long while before we see Crescent Island. Given the timing this will be going up against the AMD Instinct MI450 series and NVIDIA Vera Rubin. It seems like Intel wanted to have something to announce now given the ongoing AI rush albeit not many details today and no short term AI solution.&lt;/p&gt;
    &lt;p&gt;At least this does lead to more weight for the ongoing Project Battlematrix Linux driver improvements and other ongoing Intel Compute Runtime and Intel Xe Linux driver enhancements that are currently ongoing for the Arc Pro B-Series. With confirming Crescent Island now it also opens the door to them beginning to push open-source hardware enablement patches without otherwise spilling the beans on this forthcoming enterprise AI product.&lt;/p&gt;
    &lt;p&gt;Intel is using the OCP Global Summit to announce some additional Gaudi 3 rack-scale reference designs. These new Gaudi 3 rack-scale reference designs will allow up to 64 accelerators per rack with liquid cooling and 8.2TB of high bandwidth memory. Intel hasn't aggressively promoted Gaudi 3 in recent quarters after its launch last year. Gaudi 3 has enjoyed some reprieve since Intel canceled their Falcon Shores AI accelerator chip but still appears to be the end of the road for Gaudi especially with Jaguar Shores still expected and now Crescent Island too. The Gaudi 3 software support has been neglected over the past year with losing multiple rounds of the Habana Labs Linux driver maintainers and only recently seeing new activity to return to working on this AI accelerator Linux driver albeit as of writing for Linux 6.18 there still is no mainline kernel driver support for Gaudi 3.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this article consider joining Phoronix Premium to view this site ad-free, multi-page articles on a single page, and other benefits. PayPal or Stripe tips are also graciously accepted. Thanks for your support.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45583243</guid><pubDate>Tue, 14 Oct 2025 18:30:57 +0000</pubDate></item><item><title>What Americans die from vs. what the news reports on</title><link>https://ourworldindata.org/does-the-news-reflect-what-we-die-from</link><description>&lt;doc fingerprint="7655812702e0870d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Does the news reflect what we die from?&lt;/head&gt;
    &lt;head rend="h2"&gt;What do Americans die from, and what do the New York Times, Washington Post, and Fox News report on?&lt;/head&gt;
    &lt;head rend="h4"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;For this work, we relied on Media Cloud, an open-access platform for media analysis. We would like to thank their team, particularly Emily Boardman Ndulue and Fernando Bermejo, for making this invaluable resource available and for their help with this project.&lt;/p&gt;
    &lt;p&gt;More than 80% of people — including surveyed Americans, Brits, Germans, and Italians — say they follow the news because they “want to know what is going on in the world around them.”1 It’s not just that people expect the news to inform them about what’s going on in the world. Most think that it does. When asked what emotions the news generates, “informed” was the most common response.2&lt;/p&gt;
    &lt;p&gt;This is what media outlets themselves promise to do. Here are several quotes from the New York Times’s mission statement:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“We seek the truth and help people understand the world. [...]&lt;/p&gt;
      &lt;p&gt;We help a global audience understand a vast and diverse world.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;However, as we’ll see in this article, the media focuses on a particular sliver of our world, leaving much of the “vast and diverse world” largely out of their reporting. We’ll investigate this through the lens of health, looking at causes of death and reporting in the United States.&lt;/p&gt;
    &lt;p&gt;As we’ll discuss, our point is not that we should want or expect the media’s coverage to perfectly match the real distribution of deaths, although we’d argue that it would be better if it were less skewed. We wrote this article so that you, the reader, are aware of a significant disconnect between what we often hear and what actually happens.&lt;/p&gt;
    &lt;p&gt;It’s easy to conflate what we see in the news with the reality of our world, and keeping this mismatch in mind can help you avoid falling into this trap.&lt;/p&gt;
    &lt;head rend="h1"&gt;Counting deaths and mentions in popular media&lt;/head&gt;
    &lt;p&gt;We focused on causes of death and media coverage in the United States in 2023.&lt;/p&gt;
    &lt;p&gt;The full list of all causes of death is very long, and since many causes are very rare, we didn’t investigate all of them. But our analysis accounts for 76% of all deaths in the US in 2023.3 It includes the 12 leading causes of death in the US, plus homicide, drug overdoses, and terrorism, since they receive a lot of attention in the media.&lt;/p&gt;
    &lt;p&gt;We used data from the US Centers for Disease Control and Prevention (CDC) to calculate each cause’s share of the total.4 We then compared this to the relative share of articles that mentioned these causes of death in three media outlets: the New York Times, the Washington Post, and the news website of Fox News. We selected these three because they are among the biggest national news organizations, are extremely popular, and are seen as being on different parts of the political spectrum.&lt;/p&gt;
    &lt;p&gt;To count the number of mentions, we relied on Media Cloud, an open-source platform regularly used for media analysis. In an extended methodology document, we provide many more details on how we constructed the data. Two things are important to mention here.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each cause of death, we included synonyms in our search. So, when searching for mentions of “homicide”, we also included mentions of related terms such as “murder”, “killer”, and other terms. For “heart disease”, we included terms like “heart attack”, “cardiac arrest”, “heart failure”, and many others.&lt;/item&gt;
      &lt;item&gt;We only counted articles where a cause of death — or its related terms — was mentioned more than once. This ensures that our analysis is focused on reporting on causes of death rather than just articles that mention a cause of death in passing. Additionally, this approach reduces the number of false positives and noise in our results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;What do Americans die from, and what do they read about in the news?&lt;/head&gt;
    &lt;p&gt;You can see the results of our analysis in the chart below.&lt;/p&gt;
    &lt;p&gt;There are two big takeaways from this analysis. The first one is that the actual distribution of deaths shown on the left is very different from the causes of death that the media talks about.&lt;/p&gt;
    &lt;p&gt;The second insight is how similar the distribution of coverage is between the three media outlets. While there are some differences (Fox News was a bit more likely to mention homicides, for example, while the NYT did the same for terrorism), these are much smaller than we might expect. While right- and left-wing media might differ in how they cover particular topics, what they choose to write or talk about is similar.&lt;/p&gt;
    &lt;p&gt;The insight in this comparison, then, is not about differences between partisan media. It’s about the difference between actual causes of death and what the news tells Americans about. Those differences — as we can see in the chart — are huge.&lt;/p&gt;
    &lt;p&gt;Heart disease and cancer accounted for 56% of deaths among these 15 causes, but together they received just 7% of the media coverage. Other chronic issues, such as strokes, respiratory problems, diabetes, and kidney and liver disease, were also very underrepresented in the news.&lt;/p&gt;
    &lt;p&gt;Rare — but dramatic — events such as homicides and terrorism received more than half of all media coverage, despite being much smaller causes of death in the US. Terrorism, in particular, is a very rare cause of death, with 16 deaths in 2023.5&lt;/p&gt;
    &lt;head rend="h1"&gt;How over- and underrepresented are different causes of death in the media?&lt;/head&gt;
    &lt;p&gt;Another way to visualize this data is to measure how over- or underrepresented each cause is.&lt;/p&gt;
    &lt;quote&gt;Heart disease and cancer accounted for 56% of deaths but received just 7% of the coverage&lt;/quote&gt;
    &lt;p&gt;To do this, we calculate the ratio between a cause’s share of deaths and its share of news articles. In the chart below, we’ve done this for coverage in the New York Times (the results are similar for the other two outlets).&lt;/p&gt;
    &lt;p&gt;It highlights that homicides and terrorism are extremely overrepresented. Homicides received 43 times more coverage than their share of deaths; terrorism received over 18,000 times more.&lt;/p&gt;
    &lt;p&gt;At the other end, we see that conditions like heart disease, stroke, and liver disease are very underrepresented.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why is the media so biased towards dramatic risks?&lt;/head&gt;
    &lt;p&gt;The fact that the media focuses on dramatic, emotive events — and much less on “everyday”, more common mortality risks — has been found in several studies.6 These studies have shown that this mismatch has existed for a long time, and that genuine changes in death rates between causes of death account for a tiny fraction of the changes in media coverage.7&lt;/p&gt;
    &lt;p&gt;Our analysis adds to the evidence, with updated data for 2023.&lt;/p&gt;
    &lt;p&gt;Why does this mismatch exist?&lt;/p&gt;
    &lt;quote&gt;Homicides received 43 times more coverage than their share of deaths; terrorism received over 18,000 times more&lt;/quote&gt;
    &lt;p&gt;One explanation is that the news, true to its name, tells us what’s new. The fact that nearly 2000 Americans die from heart disease every single day means it is not novel or new. The headline tomorrow would be the same as it was today, which was the same as yesterday. Rarer events like terrorist attacks, plane crashes, homicides, or disasters each have their unique headline.&lt;/p&gt;
    &lt;p&gt;People who die from common health risks quickly become mere numbers. On the other hand, those who die in rarer events have a face, a name, and a story that can be told. As humans, this makes us much more likely to click and read, making these stories ideal for the media to write about.&lt;/p&gt;
    &lt;p&gt;While we would like news organizations to focus much more on the larger problems that societies face — that is what we try to do at Our World in Data — it would be wrong to put all of the blame on them. They respond to what readers want, and many want emotive and engaging stories (even if their circumstances are terrible and upsetting).&lt;/p&gt;
    &lt;p&gt;Even outside the news, some of the most successful television series are intense, crime-filled dramas. Disaster movies pull in record numbers at the box office. One of the most popular podcast genres is “true crime,” where we spend hours listening to the gripping, scary tales of serial killers or con artists.&lt;/p&gt;
    &lt;p&gt;It's not surprising, then, that we’re much more likely to click on a news story about the latest murder or disaster than one about heart or kidney disease. And because media organizations need traffic and attention to survive, they and the public are stuck in a reinforcing feedback loop where rare events are always in the headlines and chronic problems get drowned out.&lt;/p&gt;
    &lt;p&gt;This is not just a problem with the modern media environment. The audience for this type of media has always been there. What’s changed is the reporting frequency: rather than getting one newspaper in the morning, we are bombarded with updates almost in real-time. We also now receive news from a much larger geographical area; it’s not just about what’s happened in our own town, but also about what’s happened on the other side of the country, or even the world.&lt;/p&gt;
    &lt;head rend="h1"&gt;Does this bias really matter?&lt;/head&gt;
    &lt;p&gt;Our point is not that we think the New York Times, Washington Post, or Fox News’ coverage should exactly match the distribution of causes of death. A newspaper that constantly covers heart disease and kidney failure would be a boring one that soon goes out of business. Even though our mission at Our World in Data is to cover the world’s largest problems, our own writing and data publications also don’t precisely match the scale of those problems. We expect we will be closer to the real distribution than the mainstream media, but there will still be some mismatch.&lt;/p&gt;
    &lt;p&gt;The reason we’re doing this analysis is to make you or other readers more aware of this selection bias. The frequency of news coverage doesn’t reflect what’s happening across millions or billions of people, but it’s easy to fall into the trap of thinking it does.&lt;/p&gt;
    &lt;p&gt;Why, then, do we think that this bias matters? Does it actually affect people’s perceptions of problems?&lt;/p&gt;
    &lt;p&gt;In a large survey among US adults, people who consumed local crime news “often” were more than three times more likely to say they were “extremely concerned” about crime affecting them or their family than those who rarely or never read local crime news.8&lt;/p&gt;
    &lt;quote&gt;The frequency of news coverage doesn’t reflect what’s happening across millions or billions of people, but it’s easy to fall into the trap of thinking it does&lt;/quote&gt;
    &lt;p&gt;Nearly six-in-ten Americans still see international terrorism as a critical threat to the United States, despite the domestic impact on the US being relatively low for two decades. People are often far more anxious about flying than driving, even though commercial airline crashes are incredibly rare.&lt;/p&gt;
    &lt;p&gt;The information we’re exposed to profoundly impacts how we perceive the world, even if our perspective is less skewed than the media's.&lt;/p&gt;
    &lt;p&gt;But there’s one final reason why this bias matters. It makes it hard for us to understand how causes of death are changing over time. If we’re constantly bombarded with stories of the latest murders and crimes, we might easily think that these are happening more and more. That is a widespread sentiment. In 23 of the 27 Gallup surveys conducted since 1993, most Americans said there was more crime than the year before. In reality, rates of crime — including homicides and other violent crime — have fallen a lot.&lt;/p&gt;
    &lt;p&gt;And if we don’t hear about what’s happening to heart disease rates, treatments, or the odds of surviving cancer, we might wrongly imagine that no progress has been made. Yet childhood cancer deaths have plummeted over the last 50 years. Even among adults, death rates from cancer have fallen dramatically since the 1990s. So too have death rates from heart disease.&lt;/p&gt;
    &lt;p&gt;This perception gap about the world matters, and the media is not doing a good job of trying to close it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;If you’re interested in digging deeper, we provide a more detailed methodological document about how this data was generated, and a few additional analyses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Correction note&lt;/head&gt;
    &lt;p&gt;This article was initially published on October 6, 2025, and was updated on October 9. This update corrected an error, whereby “Drug and overdose” deaths were also included within the US CDC category of “Accidents”. This meant that they were double-counted. We have corrected this, and the change made only a small difference to the final numbers. The relative share of deaths from accidents changed from 9.5% to 7.8%, and the share of other causes increased slightly. We thank Karl Pettersson for flagging this.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;The limits of our personal experience and the value of statistics&lt;/head&gt;
        &lt;p&gt;The world is huge; to get a clear idea of what our world is like, we have to rely on carefully collected, well-documented statistics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;Causes of death globally: what do people die from?&lt;/head&gt;
        &lt;p&gt;To make progress towards a healthier world we need to have a good understanding of what health problems we face today.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h4"&gt;How are causes of death registered around the world?&lt;/head&gt;
        &lt;p&gt;In many countries, when people die, the cause of their death is officially registered in their country’s national system. How is this determined?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Endnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Respondents could choose to “agree” with multiple answers. This survey was from 2015, but as we’ll see, more recent data suggests that even in 2025, most Americans think the news keeps them informed about what’s happening worldwide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the Pew Research survey, 46% said it made them feel informed “extremely often or often” with a further 43% “sometimes”. That was more common than any other emotion. The other high-ranking ones were negative emotions such as anger or sadness.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In 2023, there were approximately 3 million (3,090,964) deaths in the United States. 2.3 million (2,350,117) died from the twelve leading causes plus drug overdoses, homicides and terrorism. You can find these results in our intermediate and final data files, which are available in our methodology document. That means the combined share was around 76% of the total [2,305,117 / 3,090,964 * 100 = 76%].&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We used mortality data from CDC Wonder for all causes except terrorism (which isn’t reported there). For this, we relied on data from the Global Terrorism Index.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This figure is sourced from the Institute for Economics and Peace (IEP)’s Global Terrorism Index 2024 Report. It states on page 38: “The impact of terrorism improved in North America over the past year, owing to an improvement in score in Canada. There was one attack and death from terrorism in Canada in 2023, down from the peak of 12 deaths and eight attacks in 2018. By contrast, the impact of terrorism increased in the US, with 16 deaths from seven incidents.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Isch, C. (2025). Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates. Social Science &amp;amp; Medicine, 364, 117542.&lt;/p&gt;
        &lt;p&gt;Pilar, M. R., Eyler, A. A., Moreland-Russell, S., &amp;amp; Brownson, R. C. (2020). Actual causes of death in relation to media, policy, and funding attention: Examining public health priorities. Frontiers in Public Health, 8, 279.&lt;/p&gt;
        &lt;p&gt;Bomlitz, L. J., &amp;amp; Brezis, M. (2008). Misrepresentation of health risks by mass media. Journal of Public Health, 30(2), 202-204.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Isch, C. (2025). Media bias in portrayals of mortality risks: Comparison of newspaper coverage to death rates. Social Science &amp;amp; Medicine, 364, 117542.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This survey was conducted by Pew Research in 2024. It asked US adults whether they were extremely/very concerned, somewhat concerned, or not at all concerned about crime in their local community affecting them or their family.&lt;/p&gt;
        &lt;p&gt;33% of those who “often” get local crime news were “extremely concerned”. The share among those who “sometimes” get this type of news was 19%. It was just 10% among those who rarely consume it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Cite this work&lt;/head&gt;
    &lt;p&gt;Our articles and data visualizations rely on work from many different people and organizations. When citing this article, please also cite the underlying data sources. This article can be cited as:&lt;/p&gt;
    &lt;code&gt;Hannah Ritchie, Tuna Acisu, and Edouard Mathieu (2025) - “Does the news reflect what we die from?” Published online at OurWorldinData.org. Retrieved from: 'https://ourworldindata.org/does-the-news-reflect-what-we-die-from' [Online Resource]&lt;/code&gt;
    &lt;p&gt;BibTeX citation&lt;/p&gt;
    &lt;code&gt;@article{owid-does-the-news-reflect-what-we-die-from,
    author = {Hannah Ritchie and Tuna Acisu and Edouard Mathieu},
    title = {Does the news reflect what we die from?},
    journal = {Our World in Data},
    year = {2025},
    note = {https://ourworldindata.org/does-the-news-reflect-what-we-die-from}
}&lt;/code&gt;
    &lt;head rend="h3"&gt;Reuse this work freely&lt;/head&gt;
    &lt;p&gt;All visualizations, data, and code produced by Our World in Data are completely open access under the Creative Commons BY license. You have the permission to use, distribute, and reproduce these in any medium, provided the source and authors are credited.&lt;/p&gt;
    &lt;p&gt;The data produced by third parties and made available by Our World in Data is subject to the license terms from the original third-party authors. We will always indicate the original source of the data in our documentation, so you should always check the license of any such third-party data before use and redistribution.&lt;/p&gt;
    &lt;p&gt;All of our charts can be embedded in any site.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45583336</guid><pubDate>Tue, 14 Oct 2025 18:40:23 +0000</pubDate></item><item><title>How to turn liquid glass into a solid interface</title><link>https://tidbits.com/2025/10/09/how-to-turn-liquid-glass-into-a-solid-interface/</link><description>&lt;doc fingerprint="36a4d5d74966e3dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How to Turn Liquid Glass into a Solid Interface&lt;/head&gt;
    &lt;p&gt;Apple’s new Liquid Glass interface design brings transparency and blur effects to all Apple operating systems, but many users find it distracting or difficult to read. Here’s how to control its effects and make your interface more usable. Although the relevant Accessibility settings are quite similar across macOS, iOS, watchOS, and tvOS, I separate them because they offer different levels of utility in each. I have no experience with (or interest in) a Vision Pro, so I can’t comment on Liquid Glass in visionOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;macOS 26&lt;/head&gt;
    &lt;p&gt;To help you get a feel for how the various settings affect the Mac interface, I’ve taken all the screenshots below with the same screen content. For a quick comparison, download the images and then use Quick Look to flip back and forth between any two to view the differences. Select one image in the Finder, press the Space bar, and then use the arrow keys to change the Finder selection to the other image.&lt;/p&gt;
    &lt;head rend="h3"&gt;Default Settings&lt;/head&gt;
    &lt;p&gt;Here’s our starting point with macOS. Note the transparency in the menu bar, widgets, Control Center, and Dock. In the System Settings window, you can see that the sidebar is also translucent, allowing the wallpaper thumbnails to bleed through as they scroll underneath and letting the General item blur with Search at the top.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency&lt;/head&gt;
    &lt;p&gt;The setting that makes the most difference in toning down the excesses of Liquid Glass is System Settings &amp;gt; Accessibility &amp;gt; Display &amp;gt; Reduce Transparency. It turns the menu bar opaque, prevents the wallpaper from being visible through the widgets, Control Center, and Dock, and eliminates the awkward bleeds under the System Settings sidebar. For those who take a lot of screenshots, like I do, Reduce Transparency is essential because it ensures that all screenshots have a consistent background. It would be highly distracting if screenshots had noticeably different colors due to being taken over different wallpapers or windows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Increase Contrast (and Reduce Transparency)&lt;/head&gt;
    &lt;p&gt;To further clarify the interface, you can turn on System Settings &amp;gt; Accessibility &amp;gt; Display &amp;gt; Increase Contrast, which also automatically enables Reduce Transparency. It outlines most interface elements so they stand out much more strongly—no more light gray on lighter gray. Note that it also changes some colors, so if you ever see a system that has odd blues and greens in Messages, for instance, it’s usually this setting. I find the Increase Contrast setting jarring, but it might be a significant help for those with low vision.&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquid Glass Off&lt;/head&gt;
    &lt;p&gt;Recently, Bob Pony revealed on Bluesky that there’s a hidden setting in macOS that lets you turn off Liquid Glass entirely. Enter the command below into Terminal to turn off Liquid Glass; turn it back on by running the command again after changing &lt;code&gt;YES&lt;/code&gt; to &lt;code&gt;NO&lt;/code&gt;. You’ll need to log out and log back in to see the full effect.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;defaults write -g com.apple.SwiftUI.DisableSolarium -bool YES&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;If you compare this screenshot to the first one, you can see that it eliminates the glass-like look of items in Control Center, drops the background around the Dock entirely, and reverts to the less-rounded corners familiar from macOS 15 Sequoia. The sidebar in System Settings is no longer on a layer above the rest of the window, and the Search field appears only at the top of the sidebar list, not as a hovering object that remains in view as you scroll the sidebar contents. You can’t see it in this screenshot, but the icon names that appear when you hover over them in the Dock also lose their background, becoming almost unreadable. Also not apparent from this screenshot is that all menus on the right side of the menu bar become almost entirely transparent and difficult to use—presumably they’re managed by Control Center.&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquid Glass Off, Reduce Transparency On&lt;/head&gt;
    &lt;p&gt;What disabling Liquid Glass doesn’t do—reasonably enough—is turn off transparency. When I add Reduce Transparency to the screenshot above, here’s what I get. The menu bar becomes opaque, which is good, and widgets become black. However, Control Center is completely borked, and the change makes no difference for the Dock.&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquid Glass Off, Reduce Transparency and Increase Contrast On&lt;/head&gt;
    &lt;p&gt;Adding Increase Contrast to the mix does what you expect but doesn’t improve any of the problematic spots.&lt;/p&gt;
    &lt;p&gt;I can’t recommend turning off Liquid Glass entirely in this way. Although it does make macOS 26 look more like macOS 15, it suffers from several glaring mistakes that Apple has no incentive to fix. Stick to Reduce Transparency and add Increase Contrast if your eyes would appreciate it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Liquid Glass Off, Per App&lt;/head&gt;
    &lt;p&gt;However, as Matt Sephton noted in TidBITS Talk, you can also turn off Liquid Glass on a per-app basis, at least for some Apple apps. He gives these commands as examples, and you can experiment with other apps by replacing their bundle identifier—the &lt;code&gt;com.apple.finder&lt;/code&gt; piece—in the command below. To find it, Control-click any app, choose Show Package Contents, and search &lt;code&gt;Contents/Info.plist&lt;/code&gt; for the CFBundleIdentifier key. It’s usually sensible, like &lt;code&gt;com.apple.Home&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;defaults write com.apple.finder com.apple.SwiftUI.DisableSolarium -bool YES&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;defaults write com.apple.Preview com.apple.SwiftUI.DisableSolarium -bool YES&lt;/code&gt;
    &lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Motion&lt;/head&gt;
    &lt;p&gt;There is one other setting that’s often recommended for solidifying the interface: System Settings &amp;gt; Accessibility &amp;gt; Motion &amp;gt; Reduce Motion. On the Mac, the main place I see this changing things is in the zoom animation when entering full-screen mode—it becomes a cross-fade transition instead. Turn it on if you like, but don’t expect it to make a notable difference unless you have specific vision issues that call for it.&lt;/p&gt;
    &lt;head rend="h2"&gt;iOS 26&lt;/head&gt;
    &lt;p&gt;iOS has all the Accessibility settings macOS does and then some, plus Increase Contrast can be turned on separately from Reduce Transparency. For each of the screenshots below, the original image is on the left and a screenshot showing the effect of the setting is on the right. iPadOS is similar, as you’d expect.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency&lt;/head&gt;
    &lt;p&gt;I’ll start by turning on Settings &amp;gt; Accessibility &amp;gt; Display &amp;amp; Text Size &amp;gt; Reduce Transparency, which is again the most effective way to bring Liquid Glass to heel. As you can see, the big win is the notifications, which gain a solid background that makes them far more readable against the background. The Search button at the bottom also becomes opaque, as does the Dock’s border. The downside is that you’ll encounter some apps, such as Photos, where top or bottom toolbars have relatively few buttons and look strange because Apple expects the background to show through.&lt;/p&gt;
    &lt;head rend="h3"&gt;Increase Contrast&lt;/head&gt;
    &lt;p&gt;Turning Reduce Transparency off but enabling Settings &amp;gt; Accessibility &amp;gt; Display &amp;amp; Text Size &amp;gt; Increase Contrast offers an intermediate level of improved readability. The notifications are still see-through, but the extra contrast causes their text to stand out more. Increase Contrast applies solid borders to elements such as the notifications, flashlight and camera buttons, the Search button, and the Dock, helping to differentiate them from the background as well. Be warned that the blues and greens in Messages—and those used for switches through the interface—will look different from everyone else’s if you have Increase Contrast turned on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency and Increase Contrast&lt;/head&gt;
    &lt;p&gt;What about combining the two? It does exactly what you’d expect, eliminating the bleedthrough of the background nearly everywhere, boosting the contrast, and adding borders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bold Text&lt;/head&gt;
    &lt;p&gt;Perhaps the transparency doesn’t bother you, but you’re still having trouble reading text. You can try turning on Settings &amp;gt; Accessibility &amp;gt; Display &amp;amp; Text Size &amp;gt; Bold Text, which adds some weight to all text in the interface. (Happily, it doesn’t require restarting the iPhone anymore.) I think it helps, but not as much as either Reduce Transparency or Increase Contrast.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency, Increase Contrast, and Bold Text&lt;/head&gt;
    &lt;p&gt;Since all these settings are independent, you can combine them for the ultimate in a non-transparent, highly readable interface. It’s more than I need, but there’s no shame if you like it best.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Motion&lt;/head&gt;
    &lt;p&gt;You may have noticed that Notification Center itself is a “pane of glass” with a curved edge that distorts the text above the bottom row of icons. That’s more annoying in static screenshots than in actual use, but you can eliminate refractions as well by turning on Settings &amp;gt; Accessibility &amp;gt; Motion &amp;gt; Reduce Motion. (While you’re there, test with Prefer Cross-Fade Transitions, too—I had trouble discerning what it did.) The right-hand screenshot below has only Reduce Motion turned on, but you can see that the edge of Notification Center’s pane no longer distorts the text under it, and it also makes the background noticeably blurrier. You can combine Reduce Motion with all the rest of the settings if you want. The downside of Reduce Motion is that some aspects of the iPhone and iPad experience may feel abrupt without their normal transitions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other Settings&lt;/head&gt;
    &lt;p&gt;For reference, here are the Settings &amp;gt; Accessibility screens I’ve been discussing. A few additional settings here may enhance your experience with Liquid Glass and the iOS 26 interface in general. You can increase the text size with Larger Text, although at some point, certain parts of the interface may look weird. It can be tricky to find examples of what turning on Button Shapes will do, but more buttons that are normally just text will get outlines and possibly be underlined. Enabling On/Off Labels adds a vertical line (for on) or circle (for off) symbol to all switches if the color change doesn’t work for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;watchOS 26&lt;/head&gt;
    &lt;p&gt;watchOS has the same Accessibility settings as iOS, and for the most part, they serve the same purpose. That said, the Apple Watch display is small enough that Apple was more restrained in how extensively it implemented Liquid Glass, so reducing the impact of Liquid Glass is less necessary.&lt;/p&gt;
    &lt;p&gt;The easiest way to test these settings is through the Accessibility screen in the iPhone’s Watch app—changes take effect immediately on your Apple Watch. They’re also available in Settings &amp;gt; Accessibility on the watch itself, but it’s much clumsier to work with.&lt;/p&gt;
    &lt;head rend="h3"&gt;Default Settings&lt;/head&gt;
    &lt;p&gt;To set the stage, here’s what four screens of watchOS 26 look like with the Photos face. Obviously, what bleeds through the Liquid Glass will depend on your face; some faces work better than others. Nevertheless, you can see that the photo is fairly visible under the notification in the second image, and somewhat visible behind the Smart Stack in the fourth, but hardly noticeable behind Control Center in the third. (Ignore some of the numbers changing; it’s devilishly difficult to capture these screenshots precisely and in the desired sequence, so I had to retake a few.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency&lt;/head&gt;
    &lt;p&gt;Enabling the Reduce Transparency switch results in notable changes. The background disappears from all three screens with overlays. My take is that the notification text is much easier to read; the other screens aren’t markedly different in terms of readability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Increase Contrast&lt;/head&gt;
    &lt;p&gt;Swapping Reduce Transparency for Increase Contrast keeps the black background under Notification Center, but restores it to Control Center and the Smart Stack. Nonetheless, the icons in Control Center stand out noticeably better thanks to darker backgrounds and brighter whites. For my eyes, Increase Contrast offers the best combination of making aspects of the interface—notably the notifications—easier to read while still retaining some of the background for aesthetic reasons.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bold Text&lt;/head&gt;
    &lt;p&gt;Perhaps the problem is simply the text being too thin? This test, with the Bold Text setting enabled, shows that making the text bold helps slightly, but the notification text remains harder to read against the image background. Bold Text appears to affect only the text in Control Center, not the icons, but it does bold the orange Timer icon in the Smart Stack. Notice that it also bolded the date and temperature on the initial screen. To my mind, Bold Text is not much of a win on its own.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency, Increase Contrast, and Bold Text&lt;/head&gt;
    &lt;p&gt;As expected, combining all three settings yields the most readable interface in watchOS. Reduce Transparency eliminates the blurry backgrounds, Increase Contrast makes the buttons in Control Center pop, and Bold Text makes all the text a bit thicker and easier to parse.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other Settings&lt;/head&gt;
    &lt;p&gt;As with iOS, a few other settings in the Accessibility screen may also be helpful. You can increase the text size throughout the interface using the Text Size slider, although setting it too large will significantly reduce the amount of information that fits on the screen and result in some awkward wrapping. There’s also a Reduce Motion switch, but I couldn’t figure out what effect it has.&lt;/p&gt;
    &lt;head rend="h2"&gt;tvOS&lt;/head&gt;
    &lt;p&gt;I’ll admit a distinct level of apathy when it comes to Liquid Glass in tvOS 26. Because tvOS is primarily about consuming content, and Apple has already put a lot of work into making it visually striking, the Liquid Glass changes don’t make much difference. That’s even more true in apps other than Apple’s TV app since they all do their own thing anyway.&lt;/p&gt;
    &lt;p&gt;To illustrate what you can control, I combined three screens that change based on the available Accessibility settings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Left: The TV app with its Liquid Glass sidebar&lt;/item&gt;
      &lt;item&gt;Middle: The tvOS Home Screen showing an Apple TV+ movie under the Liquid Glass-enabled Top Shelf of icons and Control Center&lt;/item&gt;
      &lt;item&gt;Right: The screen at Settings &amp;gt; Accessibility &amp;gt; Display, where these settings live.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Default Settings&lt;/head&gt;
    &lt;p&gt;In the default state, note that the Liquid Glass sidebar on the left side displays blurry images of the icons underneath it, and the Liquid Glass-driven Top Shelf features a movie poster background surrounding the icons. Similarly, the Control Center icons show the poster background. The Settings text is provided for reference—no Liquid Glass is involved.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency&lt;/head&gt;
    &lt;p&gt;Turning on Reduce Transparency eliminates the bleedthrough of the background in the sidebar, Top Shelf, and Control Center, and it slightly lightens the color of the Settings screen. The readability of the sidebar does improve, but the icons in the Top Shelf and Control Center are equally readable in either state.&lt;/p&gt;
    &lt;head rend="h3"&gt;Increase Contrast&lt;/head&gt;
    &lt;p&gt;Increase Contrast has a minimal effect in tvOS. Most notably, it makes the headings above the controls in the Settings screen brighter. The time and the Channels &amp;amp; Apps headings also become a bit brighter. The outlines around the Top Shelf and Control Center icons also become more prominent. Ironically, making the headings look more like the controls and sidebar items reduces contrast.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bold Text&lt;/head&gt;
    &lt;p&gt;I actively dislike Bold Text in tvOS. For my eyes, the text in tvOS is easy enough to read to start, and making it bold muddies it and makes it slightly harder to read in general. Nor do I think it helps readability in the Liquid Glass sidebar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Transparency, Increase Contrast, and Bold Text&lt;/head&gt;
    &lt;p&gt;Putting all the settings together does what you’d expect, and if your eyes prefer it, that’s fine. Since I don’t like the Bold Text changes and Increase Contrast has such a minimal effect, I wouldn’t use these settings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Reduce Motion&lt;/head&gt;
    &lt;p&gt;tvOS has an option at Settings &amp;gt; Accessibility &amp;gt; Motion &amp;gt; Reduce Motion, but as with watchOS, I couldn’t figure out what impact it has. However, I’m thrilled I found it because that screen also contains the option to turn off Auto-Play Video Previews, which I hate with the heat of a burning sun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommendations&lt;/head&gt;
    &lt;p&gt;As I hope I’ve been clear throughout, everyone’s vision is different, and you should choose the settings that make these various interfaces the most usable for you. If you’re looking for a starting point, here’s what I’ve done.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 26: Use Reduce Transparency to bring back the opaque menu bar and eliminate odd bleedthroughs in various parts of the interface. This setting is particularly important if you take screenshots for documentation.&lt;/item&gt;
      &lt;item&gt;iOS 26 and iPadOS 26: If you can read notifications and don’t have trouble with readability in other apps, I recommend leaving all the settings off to start and only turning on Reduce Transparency if you decide you need it. It significantly improves readability, but at the cost of awkward interfaces in some apps that heavily rely on Liquid Glass elements.&lt;/item&gt;
      &lt;item&gt;watchOS 26: My favorite setting for watchOS is Increase Contrast, which makes notifications more readable by eliminating the bleedthrough background and also causes some buttons in the interface to stand out more. Reduce Transparency isn’t much of a win and makes the interface less attractive.&lt;/item&gt;
      &lt;item&gt;tvOS: I don’t see enough benefit in any of the settings that reduce Liquid Glass to bother with them. Reduce Transparency works as expected, but there’s relatively little transparency used, so it makes little real-world difference. But turning off Auto-Play Video Previews is a huge relief!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let me leave with one final reminder. Liquid Glass is here to stay. Apple will continue to refine it in small ways throughout the rest of the OS 26 cycle, but there’s no avoiding it. Nor is it the worst thing ever—it won’t radically affect the Apple experience for most people, and with the judicious use of the Accessibility settings I’ve described here, you can reduce the impact of any changes that do make it harder for you to interact with your devices.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45583787</guid><pubDate>Tue, 14 Oct 2025 19:27:41 +0000</pubDate></item><item><title>Unpacking Cloudflare Workers CPU Performance Benchmarks</title><link>https://blog.cloudflare.com/unpacking-cloudflare-workers-cpu-performance-benchmarks/</link><description>&lt;doc fingerprint="ef3a174958a2cc9b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;On October 4, independent developer Theo Browne published a series of benchmarks designed to compare server-side JavaScript execution speed between Cloudflare Workers and Vercel, a competing compute platform built on AWS Lambda. The initial results showed Cloudflare Workers performing worse than Node.js on Vercel at a variety of CPU-intensive tasks, by a factor of as much as 3.5x.&lt;/p&gt;
      &lt;p&gt;We were surprised by the results. The benchmarks were designed to compare JavaScript execution speed in a CPU-intensive workload that never waits on external services. But, Cloudflare Workers and Node.js both use the same underlying JavaScript engine: V8, the open source engine from Google Chrome. Hence, one would expect the benchmarks to be executing essentially identical code in each environment. Physical CPUs can vary in performance, but modern server CPUs do not vary by anywhere near 3.5x.&lt;/p&gt;
      &lt;p&gt;On investigation, we discovered a wide range of small problems that contributed to the disparity, ranging from some bad tuning in our infrastructure, to differences between the JavaScript libraries used on each platform, to some issues with the test itself. We spent the week working on many of these problems, which means over the past week Workers got better and faster for all of our customers. We even fixed some problems that affect other compute providers but not us, such as an issue that made trigonometry functions much slower on Vercel. This post will dig into all the gory details. &lt;/p&gt;
      &lt;p&gt;It's important to note that the original benchmark was not representative of billable CPU usage on Cloudflare, nor did the issues involved impact most typical workloads. Most of the disparity was an artifact of the specific benchmark methodology. Read on to understand why.&lt;/p&gt;
      &lt;p&gt;With our fixes, the results now look much more like we'd expect:&lt;/p&gt;
      &lt;p&gt;There is still work to do, but we're happy to say that after these changes, Cloudflare now performs on par with Vercel in every benchmark case except the one based on Next.js. On that benchmark, the gap has closed considerably, and we expect to be able to eliminate it with further improvements detailed later in this post.&lt;/p&gt;
      &lt;p&gt;We are grateful to Theo for highlighting areas where we could make improvements, which will now benefit all our customers, and even many who aren't our customers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Our benchmark methodology&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We wanted to run Theo's test with no major design changes, in order to keep numbers comparable. Benchmark cases are nearly identical to Theo's original test but we made a couple changes in how we ran the test, in the hopes of making the results more accurate:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Theo ran the test client on a laptop connected by a Webpass internet connection in San Francisco, against Vercel instances running in its sfo1 region. In order to make our results easier to reproduce, we chose instead to run our test client directly in AWS's us-east-1 datacenter, invoking Vercel instances running in its iad1 region (which we understand to be in the same building). We felt this would minimize any impact from network latency. Because of this, Vercel's numbers are slightly better in our results than they were in Theo's.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We chose to use Vercel instances with 1 vCPU instead of 2. All of the benchmarks are single-threaded workloads, meaning they cannot take advantage of a second CPU anyway. Vercel's CTO, Malte Ubl, had stated publicly on X that using single-CPU instances would make no difference in this test, and indeed, we found this to be correct. Using 1 vCPU makes it easier to reason about pricing, since both Vercel and Cloudflare charge for CPU time (&lt;code&gt;$&lt;/code&gt;0.128/hr for Vercel in iad1, and &lt;code&gt;$&lt;/code&gt;0.072/hr for Cloudflare globally).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;We made some changes to fix bugs in the test, for which we submitted a pull request. More on this below.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Theo's benchmarks covered a variety of frameworks, making it clear that no single JavaScript library could be at fault for the general problem. Clearly, we needed to look first at the Workers Runtime itself. And so we did, and we found two problems â not bugs, but tuning and heuristic choices which interacted poorly with the benchmarks as written.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Sharding and warm isolate routing: A problem of scheduling, not CPU speed&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Over the last year we shipped smarter routing that sends traffic to warm isolates more often. That cuts cold starts for large apps, which matters for frameworks with heavy initialization requirements like Next.js. The original policy optimized for latency and throughput across billions of requests, but was less optimal for heavily CPU-bound workloads for the same reason that such workloads cause performance issues in other platforms like Node.js: When the CPU is busy computing an expensive operation for one request, other requests sent to the same isolate must wait for it to finish before they can proceed.&lt;/p&gt;
      &lt;p&gt;The system uses heuristics to detect when requests are getting blocked behind each other, and automatically spin up more isolates to compensate. However, these heuristics are not precise, and the particular workload generated by Theo's tests â in which a burst of expensive traffic would come from a single client â played poorly with our existing algorithm. As a result, the benchmarks showed much higher latency (and variability in latency) than would normally be expected.&lt;/p&gt;
      &lt;p&gt;It's important to understand that, as a result of this problem, the benchmark was not really measuring CPU time. Pricing on the Workers platform is based on CPU time â that is, time spent actually executing JavaScript code, as opposed to time waiting for things. Time spent waiting for the isolate to become available makes the request take longer, but is not billed as CPU time against the waiting request. So, this problem would not have affected your bill.&lt;/p&gt;
      &lt;p&gt;After analyzing the benchmarks, we updated the algorithm to detect sustained CPU-heavy work earlier, then bias traffic so that new isolates spin up faster. The result is that Workers can more effectively and efficiently autoscale when different workloads are applied. I/O-bound workloads coalesce into individual already warm isolates while CPU-bound are directed so that they do not block each other. This change has already been rolled out globally and is enabled automatically for everyone. It should be pretty clear from the graph when the change was rolled out:&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;V8 garbage collector tuning&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;While this scheduling issue accounted for the majority of the disparity in the benchmark, we did find a minor issue affecting code execution performance during our testing.&lt;/p&gt;
      &lt;p&gt;The range of issues that we uncovered in the framework code in these benchmarks repeatedly pointed at garbage collection and memory management issues as being key contributors to the results. But, we would expect these to be an issue with the same frameworks running in Node.js as well. To see exactly what was going on differently with Workers and why it was causing such a significant degradation in performance, we had to look inwards at our own memory management configuration.&lt;/p&gt;
      &lt;p&gt;The V8 garbage collector has a huge number of knobs that can be tuned that directly impact performance. One of these is the size of the "young generation". This is where newly created objects go initially. It's a memory area that's less compact, but optimized for short-lived objects. When objects have bounced around the "young space" for a few generations they get moved to the old space, which is more compact, but requires more CPU to reclaim.&lt;/p&gt;
      &lt;p&gt;V8 allows the embedding runtime to tune the size of the young generation. And it turns out, we had done so. Way back in June of 2017, just two months after the Workers project kicked off, we â or specifically, I, Kenton, as I was the only engineer on the project at the time â had configured this value according to V8's recommendations at the time for environments with 512MB of memory or less. Since Workers defaults to a limit of 128MB per isolate, this seemed appropriate.&lt;/p&gt;
      &lt;p&gt;V8's entire garbage collector has changed dramatically since 2017. When analyzing the benchmarks, it became apparent that the setting which made sense in 2017 no longer made sense in 2025, and we were now limiting V8's young space too rigidly. Our configuration was causing V8's garbage collection to work harder and more frequently than it otherwise needed to. As a result, we have backed off on the manual tuning and now allow V8 to pick its young space size more freely, based on its internal heuristics. This is already live on Cloudflare Workers, and it has given an approximately 25% boost to the benchmarks with only a small increase in memory usage. Of course, the benchmarks are not the only Workers that benefit: all Workers should now be faster. That said, for most Workers the difference has been much smaller.&lt;/p&gt;
      &lt;p&gt;The platform changes solved most of the problem. Following the changes, our testing showed we were now even on all of the benchmarks save one: Next.js.&lt;/p&gt;
      &lt;p&gt;Next.js is a popular web application framework which, historically, has not had built-in support for hosting on a wide range of platforms. Recently, a project called OpenNext has arisen to fill the gap, making Next.js work well on many platforms, including Cloudflare. On investigation, we found several missing optimizations and other opportunities to improve performance, explaining much of why the benchmark performed poorly on Workers.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Unnecessary allocations and copies&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;When profiling the benchmark code, we noticed that garbage collection was dominating the timeline. From 10-25% of the request processing time was being spent reclaiming memory.&lt;/p&gt;
      &lt;p&gt;So we dug in and discovered that OpenNext, and in some cases Next.js and React itself, will often create unnecessary copies of internal data buffers at some of the worst times during the handling of the process. For instance, there's one &lt;code&gt;pipeThrough()&lt;/code&gt; operation in the rendering pipeline that we saw creating no less than 50 2048-byte &lt;code&gt;Buffer&lt;/code&gt; instances, whether they are actually used or not.&lt;/p&gt;
      &lt;p&gt;We further discovered that on every request, the Cloudflare OpenNext adapter has been needlessly copying every chunk of streamed output data as itâs passed out of the renderer and into the Workers runtime to return to users. Given this benchmark returns a 5 MB result on every request, that's a lot of data being copied!&lt;/p&gt;
      &lt;p&gt;In other places, we found that arrays of internal Buffer instances were being copied and concatenated using &lt;code&gt;Buffer.concat&lt;/code&gt; for no other reason than to get the total number of bytes in the collection. That is, we spotted code of the form &lt;code&gt;getBody().length&lt;/code&gt;. The function &lt;code&gt;getBody()&lt;/code&gt; would concatenate a large number of buffers into a single buffer and return it, without storing the buffer anywhere. So, all that work was being done just to read the overall length. Obviously this was not intended, and fixing it was an easy win.&lt;/p&gt;
      &lt;p&gt;We've started opening a series of pull requests in OpenNext to fix these issues, and others in hot paths, removing some unnecessary allocations and copies:&lt;/p&gt;
      &lt;p&gt;We're not done. We intend to keep iterating through OpenNext code, making improvements wherever theyâre needed â not only in the parts that run on Workers. Many of these improvements apply to other OpenNext platforms. The shared goal of OpenNext is to make NextJS as fast as possible regardless of where you choose to run your code.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Inefficient Streams Adapters&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Much of the Next.js code was written to use Node.js's APIs for byte streams. Workers, however, prefers the web-standard Streams API, and uses it to represent HTTP request and response bodies. This necessitates using adapters to convert between the two APIs. When investigating the performance bottlenecks, we found a number of examples where inefficient streams adapters are being needlessly applied. For example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const stream = Readable.toWeb(Readable.from(res.getBody()))&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;res.getBody()&lt;/code&gt; was performing a &lt;code&gt;Buffer.concat(chunks)&lt;/code&gt; to copy accumulated chunks of data into a new Buffer, which was then passed as an iterable into a Node.js &lt;code&gt;stream.Readable&lt;/code&gt; that was then wrapped by an adapter that returns a &lt;code&gt;ReadableStream&lt;/code&gt;. While these utilities do serve a useful purpose, this becomes a data buffering nightmare since both Node.js streams and Web streams each apply their own internal buffers! Instead we can simply do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const stream = ReadableStream.from(chunks);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This returns a &lt;code&gt;ReadableStream&lt;/code&gt; directly from the accumulated chunks without additional copies, extraneous buffering, or passing everything through inefficient adaptation layers.&lt;/p&gt;
      &lt;p&gt;In other places we see that Next.js and React make extensive use of &lt;code&gt;ReadableStream&lt;/code&gt; to pass bytes through, but the streams being created are value-oriented rather than byte-oriented! For example,&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const readable = new ReadableStream({
  pull(controller) {
    controller.enqueue(chunks.shift());
    if (chunks.length === 0) {
      controller.close();
    }
});  // Default highWaterMark is 1!
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Seems perfectly reasonable. However, there's an issue here. If the chunks are &lt;code&gt;Buffer&lt;/code&gt; or &lt;code&gt;Uint8Array&lt;/code&gt; instances, every instance ends up being a separate read by default. So if the &lt;code&gt;chunk&lt;/code&gt; is only a single byte, or 1000 bytes, that's still always two reads. By converting this to a byte stream with a reasonable high water mark, we can make it possible to read this stream much more efficiently:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;const readable = new ReadableStream({
  type: 'bytes',
  pull(controller) {
    controller.enqueue(chunks.shift());
    if (chunks.length === 0) {
      controller.close();
    }
}, { highWaterMark: 4096 });
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now, the stream can be read as a stream of bytes rather than a stream of distinct JavaScript values, and the individual chunks can be coalesced internally into 4096 byte chunks, making it possible to optimize the reads much more efficiently. Rather than reading each individual enqueued chunk one at a time, the ReadableStream will proactively call &lt;code&gt;pull()&lt;/code&gt; repeatedly until the highWaterMark is reached. Reads then do not have to ask the stream for one chunk of data at a time.&lt;/p&gt;
      &lt;p&gt;While it would be best for the rendering pipeline to be using byte streams and paying attention to back pressure signals more, our implementation can still be tuned to better handle cases like this.&lt;/p&gt;
      &lt;p&gt;The bottom line? We've got some work to do! There are a number of improvements to make in the implementation of OpenNext and the adapters that allow it to work on Cloudflare that we will continue to investigate and iterate on. We've made a handful of these fixes already and we're already seeing improvements. Soon we also plan to start submitting patches to Next.js and React to make further improvements upstream that will ideally benefit the entire ecosystem.&lt;/p&gt;
      &lt;p&gt;Aside from buffer allocations and streams, one additional item stood out like a sore thumb in the profiles: &lt;code&gt;JSON.parse()&lt;/code&gt; with a reviver function. This is used in both React and Next.js and in our profiling this was significantly slower than it should be. We built a microbenchmark and found that JSON.parse with a reviver argument recently got even slower when the standard added a third argument to the reviver callback to provide access to the JSON source context.&lt;/p&gt;
      &lt;p&gt;For those unfamiliar with the reviver function, it allows an application to effectively customize how JSON is parsed. But it has drawbacks. The function gets called on every key-value pair included in the JSON structure, including every individual element of an Array that gets serialized. In Theo's NextJS benchmark, in any single request, it ends up being called well over 100,000 times!&lt;/p&gt;
      &lt;p&gt;Even though this problem affects all platforms, not just ours, we decided that we weren't just going to accept it. After all, we have contributors to V8 on the Workers runtime team! We've upstreamed a V8 patch that can speed up &lt;code&gt;JSON.parse()&lt;/code&gt; with revivers by roughly 33 percent. That should be in V8 starting with version 14.3 (Chrome 143) and can help everyone using V8, not just Cloudflare: Node.js, Chrome, Deno, the entire ecosystem.Â  If you are not using Cloudflare Workers or didn't change the syntax of your reviver you are currently suffering under the red performance bar.&lt;/p&gt;
      &lt;p&gt;We will continue to work with framework authors to reduce overhead in hot paths. Some changes belong in the frameworks, some belong in the engine, some in our platform.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Node.js's trigonometry problem&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We are engineers, and we like to solve engineering problems â whether our own, or for the broader community.&lt;/p&gt;
      &lt;p&gt;Theo's benchmarks were actually posted in response to a different benchmark by another author which compared Cloudflare Workers against Vercel. The original benchmark focused on calling trigonometry functions (e.g. sine and cosine) in a tight loop. In this benchmark, Cloudflare Workers performed 3x faster than Node.js running on Vercel.&lt;/p&gt;
      &lt;p&gt;The author of the original benchmark offered this as evidence that Cloudflare Workers are just faster. Theo disagreed, and so did we. We expect to be faster, but not by 3x! We don't implement math functions ourselves; these come with V8. We weren't happy to just accept the win, so we dug in.&lt;/p&gt;
      &lt;p&gt;It turns out that Node.js is not using the latest, fastest path for these functions. Node.js can be built with either the clang or gcc compilers, and is written to support a broader range of operating systems and architectures than Workers. This means that Node.js' compilation often ends up using a lowest-common denominator for some things in order to provide support for the broadest range of platforms. V8 includes a compile-time flag that, in some configurations, allows it to use a faster implementation of the trig functions. In Workers, mostly by coincidence, that flag is enabled by default. In Node.js, it is not. We've opened a pull request to enable the flag in Node.js so that everyone benefits, at least on platforms where it can be supported.&lt;/p&gt;
      &lt;p&gt;Assuming that lands, and once AWS Lambda and Vercel are able to pick it up, we expect this specific gap to go away, making these operations faster for everyone. This change won't benefit our customers, since Cloudflare Workers already uses the faster trig functions, but a bug is a bug and we like making everything faster.&lt;/p&gt;
      &lt;p&gt;Even the best benchmarks have bias and tradeoffs. It's difficult to create a benchmark that is truly representative of real-world performance, and all too easy to misinterpret the results of benchmarks that are not. We particularly liked Planetscale's take on this subject.&lt;/p&gt;
      &lt;p&gt;These specific CPU-bound tests are not an ideal choice to represent web applications. Theo even notes this in his video. Most real-world applications on Workers and Vercel are bound by databases, downstream services, network, and page size. End user experience is what matters. CPU is one piece of that picture. That said, if a benchmark shows us slower, we take it seriously.&lt;/p&gt;
      &lt;p&gt;While the benchmarks helped us find and fix many real problems, we also found a few problems with the benchmarks themselves, which contributed to the apparent disparity in speed:&lt;/p&gt;
      &lt;p&gt;The benchmark is designed to be run on your laptop, from which it hits Cloudflare's and Vercel's servers over the Internet. It makes the assumption that latency observed from the client is a close enough approximation of server-side CPU time. The reasons are fair: As Theo notes, Cloudflare does not permit an application to measure its own CPU time, in order to prevent timing side channel attacks. Actual CPU time can be seen in logs after the fact, but gathering those may be a lot of work. It's just easier to measure time from the client.&lt;/p&gt;
      &lt;p&gt;However, as Cloudflare and Vercel are hosted from different data centers, the network latency to each can be a factor in the benchmark, and this can skew the results. Typically, this effect will favor Cloudflare, because Cloudflare can run your Worker in locations spread across 330+ cities worldwide, and will tend to choose the closest one to you. Vercel, on the other hand, usually places compute in a central location, so latency will vary depending on your distance from that location.&lt;/p&gt;
      &lt;p&gt;For our own testing, to minimize this effect, we ran the benchmark client from a VM on AWS located in the same data center as our Vercel instances. Since Cloudflare is well-connected to every AWS location, we think this should have eliminated network latency from the picture. We chose AWS's us-east-1 / Vercel's iad1 for our test as it is widely seen as the default choice; any other choice could draw questions about cherry-picking.&lt;/p&gt;
      &lt;p&gt;Cloudflare's servers aren't all identical. Although we refresh them aggressively, there will always be multiple generations of hardware in production at any particular time. Currently, this includes generations 10, 11, and 12 of our server hardware.&lt;/p&gt;
      &lt;p&gt;Other cloud providers are no different. No cloud provider simply throws away all their old servers every time a new version becomes available.&lt;/p&gt;
      &lt;p&gt;Of course, newer CPUs run faster, even for single-threaded workloads. The differences are not as large as they used to be 20-30 years ago, but they are not nothing. As such, an application may get (a little bit) lucky or unlucky depending on what machine it is assigned to.&lt;/p&gt;
      &lt;p&gt;In cloud environments, even identical CPUs can yield different performance depending on circumstances, due to multitenancy. The server your application is assigned to is running many others as well. In AWS Lambda, a server may be running hundreds of applications; in Cloudflare, with our ultra-efficient runtime, a server may be running thousands. These "noisy neighbors" won't share the same CPU core as your app, but they may share other resources, such as memory bandwidth. As a result, performance can vary.&lt;/p&gt;
      &lt;p&gt;It's important to note that these problems create correlated noise. That is, if you run the test again, the application is likely to remain assigned to the same machines as before â this is true of both Cloudflare and Vercel. So, this noise cannot be corrected by simply running more iterations. To correct for this type of noise on Cloudflare, one would need to initiate requests from a variety of geographic locations, in order to hit different Cloudflare data centers and therefore different machines. But, that is admittedly a lot of work. (We are not familiar with how best to get an application to switch machines on Vercel.)&lt;/p&gt;
      &lt;p&gt;The Cloudflare version of the NextJS benchmark was not configured to use force-dynamic while the Vercel version was. This triggered curious behavior. Our understanding is that pages which are not "dynamic" should normally be rendered statically at build time. With OpenNext, however, it appears the pages are still rendered dynamically, but if multiple requests for the same page are received at the same time, OpenNext will only invoke the rendering once. Before we made the changes to fix our scheduling algorithm to avoid sending too many requests to the same isolate, this behavior may have somewhat counteracted that problem. Theo reports that he had disabled force-dynamic in the Cloudflare version specifically for this reason: with it on, our results were so bad as to appear outright broken, so he intentionally turned it off.&lt;/p&gt;
      &lt;p&gt;Ironically, though, once we fixed the scheduling issue, using "static" rendering (i.e. not enabling force-dynamic) hurt Cloudflare's performance for other reasons. It seems that when OpenNext renders a "cacheable" page, streaming of the response body is inhibited. This interacted poorly with a property of the benchmark client: it measured time-to-first-byte (TTFB), rather than total request/response time. When running in dynamic mode â as the test did on Vercel â the first byte would be returned to the client before the full page had been rendered. The rest of the rendering would happen as bytes streamed out. But with OpenNext in non-dynamic mode, the entire payload was rendered into a giant buffer upfront, before any bytes were returned to the client.&lt;/p&gt;
      &lt;p&gt;Due to the TTFB behavior of the benchmark client, in dynamic mode, the benchmark actually does not measure the time needed to fully render the page. We became suspicious when we noticed that Vercel's observability tools indicated more CPU time had been spent than the benchmark itself had reported.&lt;/p&gt;
      &lt;p&gt;One option would have been to change the benchmarks to use TTLB instead â that is, wait until the last byte is received before stopping the timer. However, this would make the benchmark even more affected by network differences: The responses are quite large, ranging from 2MB to 15MB, and so the results could vary depending on the bandwidth to the provider. Indeed, this would tend to favor Cloudflare, but as the point of the test is to measure CPU speed, not bandwidth, it would be an unfair advantage.&lt;/p&gt;
      &lt;p&gt;Once we changed the Cloudflare version of the test to use force-dynamic as well, matching the Vercel version, the streaming behavior then matched, making the request fair. This means that neither version is actually measuring the cost of rendering the full page to HTML, but at least they are now measuring the same thing.&lt;/p&gt;
      &lt;p&gt;As a side note, the original behavior allowed us to spot that OpenNext has a couple of performance bottlenecks in its implementation of the composable cache it uses to deduplicate rendering requests. While fixes to these aren't going to impact the numbers for this particular set of benchmarks, we're working on improving those pieces also.&lt;/p&gt;
      &lt;p&gt;The React SSR benchmark contained a more basic configuration error. React inspects the environment variable &lt;code&gt;NODE_ENV&lt;/code&gt; to decide whether the environment is "production" or a development environment. Many Node.js-based environments, including Vercel, set this variable automatically in production. Many frameworks, such as OpenNext, automatically set this variable for Workers in production as well. However, the React SSR benchmark was written against lower-level React APIs, not using any framework. In this case, the &lt;code&gt;NODE_ENV&lt;/code&gt; variable wasn't being set at all.&lt;/p&gt;
      &lt;p&gt;And, unfortunately, when &lt;code&gt;NODE_ENV&lt;/code&gt; is not set, React defaults to "dev mode", a mode that contains extra debugging checks and is therefore much slower than production mode. As a result, the numbers for Workers were much worse than they should have been.&lt;/p&gt;
      &lt;p&gt;Arguably, it may make sense for Workers to set this variable automatically for all deployed workers, particularly when Node.js compatibility is enabled. We are looking into doing this in the future, but for now we've updated the test to set it directly.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;What weâre going to do next&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Our improvements to the Workers Runtime are already live for all workers, so you do not need to change anything. Many apps will already see faster, steadier tail latency on compute heavy routes with less jitter during bursts. In places where garbage collection improved, some workloads will also use fewer billed CPU seconds.&lt;/p&gt;
      &lt;p&gt;We also sent Theo a pull request to update OpenNext with our improvements there, and with other test fixes.&lt;/p&gt;
      &lt;p&gt;But we're far from done. We still have work to do to close the gap between OpenNext and Next.js on Vercel â but given the other benchmark results, it's clear we can get there. We also have plans for further improvements to our scheduling algorithm, so that requests almost never block each other. We will continue to improve V8, and even Node.js â the Workers team employs multiple core contributors to each project. Our approach is simple: improve open source infrastructure so that everyone gets faster, then make sure our platform makes the most of those improvements.&lt;/p&gt;
      &lt;p&gt;And, obviously, we'll be writing more benchmarks, to make sure we're catching these kinds of issues ourselves in the future. If you have a benchmark that shows Workers being slower, send it to us with a repro. We will profile it, fix what we can upstream, and share back what we learn!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45584281</guid><pubDate>Tue, 14 Oct 2025 20:17:44 +0000</pubDate></item><item><title>Why Is SQLite Coded In C</title><link>https://www.sqlite.org/whyc.html</link><description>&lt;doc fingerprint="c34a58f9ca2a1b78"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;td&gt;Note: Sections 2.0 and 3.0 of this article were added in response to comments on Hacker News and Reddit.&lt;/td&gt;
    &lt;/quote&gt;
    &lt;p&gt;Since its inception on 2000-05-29, SQLite has been implemented in generic C. C was and continues to be the best language for implementing a software library like SQLite. There are no plans to recode SQLite in any other programming language at this time.&lt;/p&gt;
    &lt;p&gt;The reasons why C is the best language to implement SQLite include:&lt;/p&gt;
    &lt;p&gt;An intensively used low-level library like SQLite needs to be fast. (And SQLite is fast, see Internal Versus External BLOBs and 35% Faster Than The Filesystem for examples.)&lt;/p&gt;
    &lt;p&gt;C is a great language for writing fast code. C is sometimes described as "portable assembly language". It enables developers to code as close to the underlying hardware as possible while still remaining portable across platforms.&lt;/p&gt;
    &lt;p&gt;Other programming languages sometimes claim to be "as fast as C". But no other language claims to be faster than C for general-purpose programming, because none are.&lt;/p&gt;
    &lt;p&gt;Nearly all systems have the ability to call libraries written in C. This is not true of other implementation languages.&lt;/p&gt;
    &lt;p&gt;So, for example, Android applications written in Java are able to invoke SQLite (through an adaptor). Maybe it would have been more convenient for Android if SQLite had been coded in Java as that would make the interface simpler. However, on iPhone applications are coded in Objective-C or Swift, neither of which have the ability to call libraries written in Java. Thus, SQLite would be unusable on iPhones had it been written in Java.&lt;/p&gt;
    &lt;p&gt;Libraries written in C do not have a huge run-time dependency. In its minimum configuration, SQLite requires only the following routines from the standard C library:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;In a more complete build, SQLite also uses library routines like malloc() and free() and operating system interfaces for opening, reading, writing, and closing files. But even then, the number of dependencies is very small. Other "modern" languages, in contrast, often require multi-megabyte runtimes loaded with thousands and thousands of interfaces.&lt;/p&gt;
    &lt;p&gt;The C language is old and boring. It is a well-known and well-understood language. This is exactly what one wants when developing a module like SQLite. Writing a small, fast, and reliable database engine is hard enough as it is without the implementation language changing out from under you with each update to the implementation language specification.&lt;/p&gt;
    &lt;p&gt;Some programmers cannot imagine developing a complex system like SQLite in a language that is not "object oriented". So why is SQLite not coded in C++ or Java?&lt;/p&gt;
    &lt;p&gt;Libraries written in C++ or Java can generally only be used by applications written in the same language. It is difficult to get an application written in Haskell or Java to invoke a library written in C++. On the other hand, libraries written in C are callable from any programming language.&lt;/p&gt;
    &lt;p&gt;Object-Oriented is a design pattern, not a programming language. You can do object-oriented programming in any language you want, including assembly language. Some languages (ex: C++ or Java) make object-oriented easier. But you can still do object-oriented programming in languages like C.&lt;/p&gt;
    &lt;p&gt;Object-oriented is not the only valid design pattern. Many programmers have been taught to think purely in terms of objects. And, to be fair, objects are often a good way to decompose a problem. But objects are not the only way, and are not always the best way to decompose a problem. Sometimes good old procedural code is easier to write, easier to maintain and understand, and faster than object-oriented code.&lt;/p&gt;
    &lt;p&gt;When SQLite was first being developed, Java was a young and immature language. C++ was older, but was undergoing such growing pains that it was difficult to find any two C++ compilers that worked the same way. So C was definitely a better choice back when SQLite was first being developed. The situation is less stark now, but there is little to no benefit in recoding SQLite at this point.&lt;/p&gt;
    &lt;p&gt;There has lately been a lot of interest in "safe" programming languages like Rust or Go in which it is impossible, or is at least difficult, to make common programming errors like memory leaks or array overruns. So the question often arises as to why SQLite is not coded in a "safe" language.&lt;/p&gt;
    &lt;p&gt;None of the safe programming languages existed for the first 10 years of SQLite's existence. SQLite could be recoded in Go or Rust, but doing so would probably introduce far more bugs than would be fixed, and it may also result in slower code.&lt;/p&gt;
    &lt;p&gt;Safe languages insert additional machine branches to do things like verify that array accesses are in-bounds. In correct code, those branches are never taken. That means that the machine code cannot be 100% branch tested, which is an important component of SQLite's quality strategy.&lt;/p&gt;
    &lt;p&gt;Safe languages usually want to abort if they encounter an out-of-memory (OOM) situation. SQLite is designed to recover gracefully from an OOM. It is unclear how this could be accomplished in the current crop of safe languages.&lt;/p&gt;
    &lt;p&gt;All of the existing safe languages are new. The developers of SQLite applaud the efforts of computer language researchers in trying to develop languages that are easier to program safely. We encourage these efforts to continue. But we ourselves are more interested in old and boring languages when it comes to implementing SQLite.&lt;/p&gt;
    &lt;p&gt;All that said, it is possible that SQLite might one day be recoded in Rust. Recoding SQLite in Go is unlikely since Go hates assert(). But Rust is a possibility. Some preconditions that must occur before SQLite is recoded in Rust include:&lt;/p&gt;
    &lt;p&gt;If you are a "rustacean" and feel that Rust already meets the preconditions listed above, and that SQLite should be recoded in Rust, then you are welcomed and encouraged to contact the SQLite developers privately and argue your case.&lt;/p&gt;
    &lt;p&gt;This page last modified on 2025-05-09 15:56:17 UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45584464</guid><pubDate>Tue, 14 Oct 2025 20:32:32 +0000</pubDate></item><item><title>Surveillance data challenges what we thought we knew about location tracking</title><link>https://www.lighthousereports.com/investigation/surveillance-secrets/</link><description>&lt;doc fingerprint="ae9cf1128aa191e5"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;October 14, 2025&lt;/head&gt;
    &lt;head rend="h4"&gt;Trove of surveillance data challenges what we thought we knew about location tracking tools, who they target and how far they have spread&lt;/head&gt;
    &lt;p&gt;In June, a sharp-suited Austrian executive of one of the world’s most significant yet little-known surveillance companies told a prospective client that he could “go to prison” for organising the deal they were discussing. But the conversation did not end there.&lt;/p&gt;
    &lt;p&gt;The executive, Günther Rudolph, was seated at a booth at ISS World in Prague, a secretive trade fair for advanced surveillance technology companies. He went on to explain how his firm, First Wap, could provide sophisticated phone-tracking software called Altamides, capable of pinpointing any person in the world. The buyer? A private mining company, owned by an individual under sanction, who intended to use it to surveil environmental protestors. “I think we’re the only ones who can deliver,” Rudolph said.&lt;/p&gt;
    &lt;p&gt;What Rudolph did not know: he was talking to an undercover reporter from Lighthouse.&lt;/p&gt;
    &lt;p&gt;The road to that conference room in Prague began with a vast archive of data, found by a Lighthouse reporter on the deep web, containing more than a million tracking operations: efforts to grab real-time locations of thousands of people worldwide. Investigating that archive — and First Wap’s activities — drew together more than 70 journalists from 14 media outlets.&lt;/p&gt;
    &lt;p&gt;What emerged is one of the most complete pictures to date of the modern surveillance industry. The tracking archive is unprecedented in scope, and reveals how the company and its clients surveilled all types of people from all over the world. Reporters interviewed more than a hundred victims, as well as former employees and industry insiders. A trove of confidential emails and documents provide a detailed inside account of how First Wap’s tech was marketed to authoritarian governments and accessed by corporate actors. Behind closed doors, First Wap’s executives touted their ability to hack WhatsApp accounts, and laughed about evading sanctions.&lt;/p&gt;
    &lt;p&gt;The surveillance industry has long maintained that its tools are deployed exclusively by government agencies to fight serious crime, portraying instances of misuse as rare exceptions. This investigation definitively dismantles that narrative.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making sense of a secret data trove&lt;/head&gt;
    &lt;p&gt;This investigation began with an archive of data. This is not the first archive related to a surveillance company’s activities, but it is certainly the most granular. It contains 1.5 million records, more than 14,000 unique phone numbers, and people surveilled in over 160 countries. It represents an extraordinarily detailed account of when and where people were tracked, and what users of the tracking tool saw.&lt;/p&gt;
    &lt;p&gt;The only clue to a target’s identity was a phone number. A team of reporters at Lighthouse and paper trail media spent months painstakingly identifying the owners of those phone numbers. To drill down into the data and better understand it, we divided it into “clusters” of targets — networks of people connected in time or space. As we investigated clusters and put names to phone numbers, stories began to emerge.&lt;/p&gt;
    &lt;p&gt;For a more in-depth explanation of how we analysed the dataset, see our technical explainer.&lt;/p&gt;
    &lt;p&gt;The Altamides archive is global in scope. We found high-profile individuals, including powerful political figures such as the former Prime Minister of Qatar and the wife of ousted Syrian dictator Bashar al-Assad. We found Netflix producer Adam Ciralsky, Blackwater founder Erik Prince, Nobel Peace Prize nominee Benny Wenda, Austropop star Wolfgang Ambros, Tel Aviv district prosecutor Liat Ben Ari and Ali Nur Yasin, a senior editor at our Indonesian partner Tempo.&lt;/p&gt;
    &lt;p&gt;In Italy, investigative journalist Gianluigi Nuzzi was tracked days after publishing a dramatic exposé of corruption in the Vatican, as police closed in on his source. In California, Anne Wojcicki, founder of DNA startup 23andMe and then married to Google’s Sergey Brin, was tracked more than a thousand times as she moved across Silicon Valley. And in South Africa, associates of Rwandan opposition leader Patrick Karegeya were tracked before his assassination in a Johannesburg hotel room.&lt;/p&gt;
    &lt;p&gt;As our reporting partners dug into the archive, they found other traces of surveillance activity on their doorsteps. In Austria, home of First Wap’s founder Josef Fuchs, Der Standard uncovered a mystery surrounding a tracking spate of high-ranking employees at energy drink giant Red Bull. In Norway, NRK examined how Altamides zeroed in on a top telecom executive. In Indonesia, interviewees told our partner Tempo that they believed they had been targeted because they had taken part in political activities or spoken out against the government. In Serbia, KRIK identified targets in the energy industry, while in Israel, Haaretz located high profile lawyers and businessmen with interests in Africa and the Gulf.&lt;/p&gt;
    &lt;p&gt;First Wap said in a response to this investigation that it denies “any illegal activities” or “human rights violations.” The company said it could not comment on specific allegations that could “enable client identification.” It further elaborated that the company does not perform any tracking itself and that “after installation” of Altamides it has no further knowledge of how the product is used. First Wap emphasized that its technology is used by law enforcement to “fight against organized crime, terrorism and corruption.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Surveillance without borders&lt;/head&gt;
    &lt;p&gt;In 2012, Sophia (not her real name) was walking near the coast of Goa on vacation, unaware that her movements were being monitored from halfway around the world with government-grade surveillance tech. But she was not being tracked by an intelligence or law enforcement agency. She was being stalked by a man who had been pursuing her, following her over the course of ten months.&lt;/p&gt;
    &lt;p&gt;Sophia’s case illustrates how Altamides proliferated far beyond the hands of governments to non-government actors, who used it to surveil victims for commercial and personal purposes. In addition to business leaders and politically-exposed individuals, the Altamides archive contains hundreds of regular people: a teacher, a therapist, a tattoo artist.&lt;/p&gt;
    &lt;p&gt;First Wap’s surveillance software was marketed through a shadowy network of middlemen who resold the system to clients worldwide. Confidential documents obtained by this investigation detail the operations of one such company, the British corporate investigations firm KCS Group. As the Arab Spring unfolded across the Middle East and North Africa, documents show that KCS attempted to capitalise on the unrest throughout the region, making concerted efforts to sell the tracking system to governments in Morocco and Algeria, as well as other countries in Africa and Asia. But at the same time it was using Altamides for corporate investigation work, digging for dirt on clients’ opponents. The company told us that it “has not been involved in selling or using inappropriate surveillance materials” and is “committed to maintaining ethical standards in all our operations.”&lt;/p&gt;
    &lt;head rend="h2"&gt;A ruthless pioneer&lt;/head&gt;
    &lt;p&gt;Unlike other industry heavyweights, which have seen years of adverse coverage because their customers targeted journalists, activists, businesspeople and diplomats, First Wap has thrived for two decades without falling under the spotlight. The story of Altamides dates back to the early 2000s, when former Siemens engineer Josef Fuchs recognised a critical vulnerability in the global telecom network. By exploiting an outdated – but still essential – communication protocol known as SS7, he could trick phone networks into revealing the locations of their users. Seeing a new business opportunity, Fuchs quickly pivoted his Jakarta-based company away from its focus on marketing messages, turning it into one of the world’s first phone-tracking firms. Its arrival on the market was seismic. At a time when Blackberrys ruled and Nokias were everywhere, a user could enter a phone number and the software would pinpoint its location anywhere in the world, within seconds.&lt;/p&gt;
    &lt;p&gt;Since then, the company has quietly built a globe-spanning phone tracking empire, operating in the shadows, without any apparent red lines. It has also expanded its surveillance arsenal, adding features to Altamides that allow it to intercept SMS messages, listen in on phone calls, and even breach encrypted messaging apps like WhatsApp.&lt;/p&gt;
    &lt;head rend="h2"&gt;“We can find a way”&lt;/head&gt;
    &lt;p&gt;Our initial reporting surfaced dozens of non-criminal people surveilled without their knowledge by the company. Data, sources we spoke to and documents we examined indicated that Altamides had been used by authoritarian governments and, without lawful basis, by non-governmental clients. We decided it would be in the public interest to carry out an undercover operation to better understand what red lines the company placed around use of its products.&lt;/p&gt;
    &lt;p&gt;In a statement, First Wap insisted to us that it “vets and verifies any government client/final user for sanctions compliance prior to the signature of any agreement” and that “there has never been any exception to this.”&lt;/p&gt;
    &lt;p&gt;Testing the red lines required a fake character, complete with a fake company name and LinkedIn. One of Lighthouse’s reporters became Albert, a South Africa-based businessman who runs a boutique “research consultancy” registered in the British Virgin Islands. Accompanying him was Abdou, a colleague, who would be playing a mover and shaker with political connections throughout West Africa. They signed up for ISS World in the Czech Republic, the largest annual surveillance technology fair, to pitch some projects and see how the company responded.&lt;/p&gt;
    &lt;p&gt;So this June, our reporter found himself in a Prague hotel room, straightening a suit jacket outfitted with a hidden camera.&lt;/p&gt;
    &lt;p&gt;Albert and Abdou met First Wap’s sales director Günther Rudolph at the company’s booth, to discuss a series of business propositions. Could First Wap help a government monitor opponents abroad? Could the company crack encrypted WhatsApp chats? Could it help the owner of a mining company disrupt protests by environmental activists? “He knows already who are the leaders, or he wants to find out?” asked Rudolph.&lt;/p&gt;
    &lt;p&gt;Rudolph drew the undercover reporters’ attention to a potential snag: some of the people they propose selling to might be under sanction from the EU or US, meaning that European nationals like First Wap’s executives risked imprisonment if it were known they organised the sale. “That’s why when we make such a deal we make it through Jakarta,” Rudolph said, referring to First Wap’s corporate base in Indonesia. It was a “grey area”, he said. But “we can find a way”. What this way might look like became clear the following day: using a newly invented shell company to mask the connection in the papertrail between First Wap and the sanctioned client.&lt;/p&gt;
    &lt;p&gt;When confronted with our undercover operation in Prague, the company said that “misunderstandings evidently arose” and that the statements by its executives referred merely to technical feasibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;CO-PUBLICATIONS&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mother Jones: This Secret Tech Tracked World Leaders, a Vatican Enemy, and Maybe You&lt;/item&gt;
      &lt;item&gt;Mother Jones: The Surveillance Empire That Tracked World Leaders, a Vatican Enemy, and Maybe You&lt;/item&gt;
      &lt;item&gt;ZDF: Secretly monitored? Your phone the spy&lt;/item&gt;
      &lt;item&gt;Der Spiegel: A surveillance company and its shady dealings&lt;/item&gt;
      &lt;item&gt;Der Standard: Inside “First Wap”: How a dubious company can monitor us all&lt;/item&gt;
      &lt;item&gt;Der Standard: Big Brother from Austria: The company that tracks phones worldwide&lt;/item&gt;
      &lt;item&gt;Der Standard: Behind "Surveillance Secrets": How the research into massive tracking was carried out&lt;/item&gt;
      &lt;item&gt;Tamedia: 10,000 mobile phones illegally monitored worldwide – including in Switzerland&lt;/item&gt;
      &lt;item&gt;Le Monde: First Wap, the discreet surveillance company responsible for tracking journalists, celebrities and executives&lt;/item&gt;
      &lt;item&gt;Haaretz: 300 Israelis targeted: who spied on Israel’s arms dealers, spyware makers and Netanyahu’s prosecutor&lt;/item&gt;
      &lt;item&gt;NRK: Former Telenor executive among thousands exposed to tracking&lt;/item&gt;
      &lt;item&gt;IrpiMedia: The pioneering surveillance company you never heard of&lt;/item&gt;
      &lt;item&gt;IrpiMedia: Journalist investigating Ratzinger's pontificate was spied on&lt;/item&gt;
      &lt;item&gt;KRIK: Serbian businessmen monitored with "Altamides" spyware&lt;/item&gt;
      &lt;item&gt;Tempo: Tracking Phone Numbers of Activists and Public Figures&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45584498</guid><pubDate>Tue, 14 Oct 2025 20:36:01 +0000</pubDate></item><item><title>FSF announces Librephone project</title><link>https://www.fsf.org/news/librephone-project</link><description>&lt;doc fingerprint="e30f1910ee07a1b4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FSF announces Librephone project&lt;/head&gt;
    &lt;p&gt;Librephone is a new initiative by the FSF with the goal of bringing full freedom to the mobile computing environment. The vast majority of software users around the world use a mobile phone as their primary computing device. After forty years of advocacy for computing freedom, the FSF will now work to bring the right to study, change, share, and modify the programs users depend on in their daily lives to mobile phones.&lt;/p&gt;
    &lt;p&gt;"Forty years ago, when the FSF was founded, our focus was on providing an operating system people could use on desktop and server computers in freedom. Times have changed, technology has progressed, but our commitment to freedom hasn't," said Zoë Kooyman, executive director of the FSF. "A lot of work has been done in mobile phone freedom over the years that we'll be building on. The FSF is now ready to do what is necessary to bring freedom to cell phone users. Given the complexity of the devices, this work will take time, but we're used to playing the long game."&lt;/p&gt;
    &lt;p&gt;Practically, Librephone aims to close the last gaps between existing distributions of the Android operating system and software freedom. The FSF has hired experienced developer Rob Savoye (DejaGNU, Gnash, OpenStreetMap, and more) to lead the technical project. He is currently investigating the state of device firmware and binary blobs in other mobile phone freedom projects, prioritizing the free software work done by the not entirely free software mobile phone operating system LineageOS.&lt;/p&gt;
    &lt;p&gt;The initial work is funded by a donation from FSF board member John Gilmore, who explained, "I have enjoyed using a mobile phone running LineageOS with MicroG and F-Droid for years, which eliminates the spyware and control that Google embeds in standard Android phones. I later discovered that the LineageOS distribution links in significant proprietary binary modules copied from the firmware of particular phones. Rather than accept this sad situation, I looked for collaborators to reverse-engineer and replace those proprietary modules with fully free software, for at least one modern phone."&lt;/p&gt;
    &lt;p&gt;Triaging existing packages and device compatibility to find a phone with the fewest, most fixable freedom problems is the first step. From there, the FSF and Savoye aim to reverse-engineer and replace the remaining nonfree software. Librephone will serve existing developers and projects who aim to build a fully functioning and free (as in freedom) Android-compatible OS.&lt;/p&gt;
    &lt;p&gt;The FSF has been supporting earlier free software mobile phone projects such as Replicant, and is excited to launch this new effort. Gilmore added: "We were lucky to find Rob Savoye, a great engineer with decades of experience in free software, embedded systems, and project management."&lt;/p&gt;
    &lt;p&gt;When asked to comment on the project, Savoye said: "As a long-time embedded systems engineer who has worked on mobile devices for decades, I'm looking forward to this opportunity to work towards a freedom-supporting phone and help users gain control over their phone hardware."&lt;/p&gt;
    &lt;p&gt;He added: "Making fully free software for a modern commercial phone will not be quick, easy, or cheap, but our project benefits from standing on the shoulders of giants who have done most of the work. Please join us, with your efforts and/or with your donations."&lt;/p&gt;
    &lt;p&gt;Besides the campaign information at https://fsf.org/campaigns/librephone, the project will have its own website at https://librephone.fsf.org and anyone can connect using #librephone irc on irc.libera.chat.&lt;/p&gt;
    &lt;head rend="h4"&gt;About the Free Software Foundation&lt;/head&gt;
    &lt;p&gt;The FSF, founded in 1985, is dedicated to promoting computer users' right to use, study, copy, modify, and redistribute computer programs. The FSF promotes the development and use of free (as in freedom) software -- particularly the GNU operating system and its GNU/Linux variants -- and free documentation for free software. The FSF also helps to spread awareness of the ethical and political issues of freedom in the use of software, and its websites, located at https://www.fsf.org and https://www.gnu.org, are an important source of information about GNU/Linux. Donations to support the FSF's work can be made at https://donate.fsf.org. The FSF is a remote organization, incorporated in Massachusetts, US.&lt;/p&gt;
    &lt;head rend="h4"&gt;MEDIA CONTACT&lt;/head&gt;
    &lt;p&gt;Greg Farough&lt;lb/&gt; Campaigns Manager &lt;lb/&gt; Free Software Foundation &lt;lb/&gt; +1 (617) 542 5942 &lt;lb/&gt; campaigns@fsf.org&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45586339</guid><pubDate>Tue, 14 Oct 2025 23:47:08 +0000</pubDate></item><item><title>Nvidia DGX Spark: great hardware, early days for the ecosystem</title><link>https://simonwillison.net/2025/Oct/14/nvidia-dgx-spark/</link><description>&lt;doc fingerprint="3f0d1d9f0ea627c9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;NVIDIA DGX Spark: great hardware, early days for the ecosystem&lt;/head&gt;
    &lt;p&gt;14th October 2025&lt;/p&gt;
    &lt;p&gt;NVIDIA sent me a preview unit of their new DGX Spark desktop “AI supercomputer”. I’ve never had hardware to review before! You can consider this my first ever sponsored post if you like, but they did not pay me any cash and aside from an embargo date they did not request (nor would I grant) any editorial input into what I write about the device.&lt;/p&gt;
    &lt;p&gt;The device retails for around $4,000. They officially go on sale tomorrow.&lt;/p&gt;
    &lt;p&gt;First impressions are that this is a snazzy little computer. It’s similar in size to a Mac mini, but with an exciting textured surface that feels refreshingly different and a little bit science fiction.&lt;/p&gt;
    &lt;p&gt;There is a very powerful machine tucked into that little box. Here are the specs, which I had Claude Code figure out for me by poking around on the device itself:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Hardware Specifications&lt;/p&gt;
      &lt;item&gt;Architecture: aarch64 (ARM64)&lt;/item&gt;
      &lt;item&gt;CPU: 20 cores&lt;/item&gt;
      &lt;item&gt;10x Cortex-X925 (performance cores)&lt;/item&gt;
      &lt;item&gt;10x Cortex-A725 (efficiency cores)&lt;/item&gt;
      &lt;item&gt;RAM: 119 GB total (112 GB available)—I’m not sure why Claude reported it differently here, the machine is listed as 128GB—it looks like a 128GB == 119GiB thing because Claude used free -h&lt;/item&gt;
      &lt;item&gt;Storage: 3.7 TB (6% used, 3.3 TB available)&lt;/item&gt;
      &lt;p&gt;GPU Specifications&lt;/p&gt;
      &lt;item&gt;Model: NVIDIA GB10 (Blackwell architecture)&lt;/item&gt;
      &lt;item&gt;Compute Capability: sm_121 (12.1)&lt;/item&gt;
      &lt;item&gt;Memory: 119.68 GB&lt;/item&gt;
      &lt;item&gt;Multi-processor Count: 48 streaming multiprocessors&lt;/item&gt;
      &lt;item&gt;Architecture: Blackwell&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;Short version: this is an ARM64 device with 128GB of memory that’s available to both the GPU and the 20 CPU cores at the same time, strapped onto a 4TB NVMe SSD.&lt;/p&gt;
    &lt;p&gt;The Spark is firmly targeted at “AI researchers”. It’s designed for both training and running models.&lt;/p&gt;
    &lt;head rend="h4"&gt;The tricky bit: CUDA on ARM64&lt;/head&gt;
    &lt;p&gt;Until now almost all of my own model running experiments have taken place on a Mac. This has gotten far less painful over the past year and a half thanks to the amazing work of the MLX team and community, but it’s still left me deeply frustrated at my lack of access to the NVIDIA CUDA ecosystem. I’ve lost count of the number of libraries and tutorials which expect you to be able to use Hugging Face Transformers or PyTorch with CUDA, and leave you high and dry if you don’t have an NVIDIA GPU to run things on.&lt;/p&gt;
    &lt;p&gt;Armed (ha) with my new NVIDIA GPU I was excited to dive into this world that had long eluded me... only to find that there was another assumption baked in to much of this software: x86 architecture for the rest of the machine.&lt;/p&gt;
    &lt;p&gt;This resulted in all kinds of unexpected new traps for me to navigate. I eventually managed to get a PyTorch 2.7 wheel for CUDA on ARM, but failed to do so for 2.8. I’m not confident there because the wheel itself is unavailable but I’m finding navigating the PyTorch ARM ecosystem pretty confusing.&lt;/p&gt;
    &lt;p&gt;NVIDIA are trying to make this easier, with mixed success. A lot of my initial challenges got easier when I found their official Docker container, so now I’m figuring out how best to use Docker with GPUs. Here’s the current incantation that’s been working for me:&lt;/p&gt;
    &lt;code&gt;docker run -it --gpus=all \
  -v /usr/local/cuda:/usr/local/cuda:ro \
  nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04 \
  bash&lt;/code&gt;
    &lt;p&gt;I have not yet got my head around the difference between CUDA 12 and 13. 13 appears to be very new, and a lot of the existing tutorials and libraries appear to expect 12.&lt;/p&gt;
    &lt;head rend="h4"&gt;The missing documentation isn’t missing any more&lt;/head&gt;
    &lt;p&gt;When I first received this machine around a month ago there was very little in the way of documentation to help get me started. This meant climbing the steep NVIDIA+CUDA learning curve mostly on my own.&lt;/p&gt;
    &lt;p&gt;This has changed substantially in just the last week. NVIDIA now have extensive guides for getting things working on the Spark and they are a huge breath of fresh air—exactly the information I needed when I started exploring this hardware.&lt;/p&gt;
    &lt;p&gt;Here’s the getting started guide and the essential collection of playbooks. There’s still a lot I haven’t tried yet just in this official set of guides.&lt;/p&gt;
    &lt;head rend="h4"&gt;Claude Code for everything&lt;/head&gt;
    &lt;p&gt;Claude Code was an absolute lifesaver for me while I was trying to figure out how best to use this device. My Ubuntu skills were a little rusty, and I also needed to figure out CUDA drivers and Docker incantations and how to install the right versions of PyTorch. Claude 4.5 Sonnet is much better than me at all of these things.&lt;/p&gt;
    &lt;p&gt;Since many of my experiments took place in disposable Docker containers I had no qualms at all about running it in YOLO mode:&lt;/p&gt;
    &lt;code&gt;IS_SANDBOX=1 claude --dangerously-skip-permissions&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;IS_SANDBOX=1&lt;/code&gt; environment variable stops Claude from complaining about running as root.&lt;/p&gt;
    &lt;head style="font-style: italic"&gt;Before I found out about IS_SANDBOX&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;I was tipped off about IS_SANDBOX after I published this article. Here’s my original workaround:&lt;/p&gt;
    &lt;p&gt;Claude understandably won’t let you do this as root, even in a Docker container, so I found myself using the following incantation in a fresh &lt;code&gt;nvcr.io/nvidia/cuda:13.0.1-devel-ubuntu24.04&lt;/code&gt; instance pretty often:&lt;/p&gt;
    &lt;code&gt;apt-get update &amp;amp;&amp;amp; apt-get install -y sudo
# pick the first free UID &amp;gt;=1000
U=$(for i in $(seq 1000 65000); do if ! getent passwd $i &amp;gt;/dev/null; then echo $i; break; fi; done)
echo "Chosen UID: $U"
# same for a GID
G=$(for i in $(seq 1000 65000); do if ! getent group $i &amp;gt;/dev/null; then echo $i; break; fi; done)
echo "Chosen GID: $G"
# create user+group
groupadd -g "$G" devgrp
useradd -m -u "$U" -g "$G" -s /bin/bash dev
# enable password-less sudo:
printf 'dev ALL=(ALL) NOPASSWD:ALL\n' &amp;gt; /etc/sudoers.d/90-dev-nopasswd
chmod 0440 /etc/sudoers.d/90-dev-nopasswd
# Install npm
DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y npm
# Install Claude
npm install -g @anthropic-ai/claude-code&lt;/code&gt;
    &lt;p&gt;Then switch to the &lt;code&gt;dev&lt;/code&gt; user and run Claude for the first time:&lt;/p&gt;
    &lt;code&gt;su - dev
claude --dangerously-skip-permissions&lt;/code&gt;
    &lt;p&gt;This will provide a URL which you can visit to authenticate with your Anthropic account, confirming by copying back a token and pasting it into the terminal.&lt;/p&gt;
    &lt;p&gt;Docker tip: you can create a snapshot of the current image (with Claude installed) by running &lt;code&gt;docker ps&lt;/code&gt; to get the container ID and then:&lt;/p&gt;
    &lt;code&gt;docker commit --pause=false &amp;lt;container_id&amp;gt; cc:snapshot&lt;/code&gt;
    &lt;p&gt;Then later you can start a similar container using:&lt;/p&gt;
    &lt;code&gt;docker run -it \
  --gpus=all \
  -v /usr/local/cuda:/usr/local/cuda:ro \
  cc:snapshot bash&lt;/code&gt;
    &lt;p&gt;Here’s an example of the kinds of prompts I’ve been running in Claude Code inside the container:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;I want to run https://huggingface.co/unsloth/Qwen3-4B-GGUF using llama.cpp - figure out how to get llama cpp working on this machine such that it runs with the GPU, then install it in this directory and get that model to work to serve a prompt. Goal is to get this command to run: llama-cli -hf unsloth/Qwen3-4B-GGUF -p "I believe the meaning of life is" -n 128 -no-cnv&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;That one worked flawlessly—Claude checked out the &lt;code&gt;llama.cpp&lt;/code&gt; repo, compiled it for me and iterated on it until it could run that model on the GPU. Here’s a full transcript, converted from Claude’s &lt;code&gt;.jsonl&lt;/code&gt; log format to Markdown using a script I vibe coded just now.&lt;/p&gt;
    &lt;p&gt;I later told it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;Write out a markdown file with detailed notes on what you did. Start with the shortest form of notes on how to get a successful build, then add a full account of everything you tried, what went wrong and how you fixed it.&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which produced this handy set of notes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tailscale was made for this&lt;/head&gt;
    &lt;p&gt;Having a machine like this on my local network is neat, but what’s even neater is being able to access it from anywhere else in the world, from both my phone and my laptop.&lt;/p&gt;
    &lt;p&gt;Tailscale is perfect for this. I installed it on the Spark (using the Ubuntu instructions here), signed in with my SSO account (via Google)... and the Spark showed up in the “Network Devices” panel on my laptop and phone instantly.&lt;/p&gt;
    &lt;p&gt;I can SSH in from my laptop or using the Termius iPhone app on my phone. I’ve also been running tools like Open WebUI which give me a mobile-friendly web interface for interacting with LLMs on the Spark.&lt;/p&gt;
    &lt;head rend="h4"&gt;Here comes the ecosystem&lt;/head&gt;
    &lt;p&gt;The embargo on these devices dropped yesterday afternoon, and it turns out a whole bunch of relevant projects have had similar preview access to myself. This is fantastic news as many of the things I’ve been trying to figure out myself suddenly got a whole lot easier.&lt;/p&gt;
    &lt;p&gt;Four particularly notable examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ollama works out of the box. They actually had a build that worked a few weeks ago, and were the first success I had running an LLM on the machine.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;llama.cpp&lt;/code&gt;creator Georgi Gerganov just published extensive benchmark results from running&lt;code&gt;llama.cpp&lt;/code&gt;on a Spark. He’s getting ~3,600 tokens/second to read the prompt and ~59 tokens/second to generate a response with the MXFP4 version of GPT-OSS 20B and ~817 tokens/second to read and ~18 tokens/second to generate for GLM-4.5-Air-GGUF.&lt;/item&gt;
      &lt;item&gt;LM Studio now have a build for the Spark. I haven’t tried this one yet as I’m currently using my machine exclusively via SSH.&lt;/item&gt;
      &lt;item&gt;vLLM—one of the most popular engines for serving production LLMs—had early access and there’s now an official NVIDIA vLLM NGC Container for running their stack.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Should you get one?&lt;/head&gt;
    &lt;p&gt;It’s a bit too early for me to provide a confident recommendation concerning this machine. As indicated above, I’ve had a tough time figuring out how best to put it to use, largely through my own inexperience with CUDA, ARM64 and Ubuntu GPU machines in general.&lt;/p&gt;
    &lt;p&gt;The ecosystem improvements in just the past 24 hours have been very reassuring though. I expect it will be clear within a few weeks how well supported this machine is going to be.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude can write complete Datasette plugins now - 8th October 2025&lt;/item&gt;
      &lt;item&gt;Vibe engineering - 7th October 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45586776</guid><pubDate>Wed, 15 Oct 2025 00:49:25 +0000</pubDate></item><item><title>Can we know whether a profiler is accurate?</title><link>https://stefan-marr.de/2025/10/can-we-know-whether-a-profiler-is-accurate/</link><description>&lt;doc fingerprint="ae4e1bb93c224701"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can We Know Whether a Profiler is Accurate?&lt;/head&gt;
    &lt;p&gt;If you have been following the adventures of our hero over the last couple of years, you might remember that we canât really trust sampling profilers for Java, and itâs even worse for Javaâs instrumentation-based profilers.&lt;/p&gt;
    &lt;p&gt;For sampling profilers, the so-called observer effect gets in the way: when we profile a program, the profiling itself can change the programâs performance behavior. This means we canât simply increase the sampling frequency to get a more accurate profile, because the sampling causes inaccuracies. So, how could we possibly know whether a profile correctly reflects an execution?&lt;/p&gt;
    &lt;p&gt;We could try to look at the code and estimate how long each bit takes, and then painstakingly compute what an accurate profile would be. Unfortunately, with the complexity of todayâs processors and language runtimes, this would require a cycle-accurate simulator that needs to model everything, from the processorâs pipeline, over the cache hierarchy, to memory and storage. While there are simulators that do this kind of thing, they are generally too slow to simulate a full JVM with JIT compilation for any interesting program within a practical amount of time. This means that simulation is currently impractical, and it is impractical to determine what a ground truth would be.&lt;/p&gt;
    &lt;p&gt;So, what other approaches might there be to determine whether a profile is accurate?&lt;/p&gt;
    &lt;p&gt;In 2010, Mytkowicz et al. already checked whether Java profilers were actionable by inserting computations at the Java bytecode level. On todayâs VMs, thatâs unfortunately an approach that changes performance in fairly unpredictable ways, because it interacts with the compiler optimizations. However, the idea to check whether a profiler accurately reflects the slowdown of a program is sound. For example, an inaccurate profiler is less likely to correctly identify a change in the distribution of where a program spends its time. Similarly, if we change the overall amount of time a program takes, without changing the distribution of where time is spent, it may attribute run time to the wrong parts of a program.&lt;/p&gt;
    &lt;p&gt;We can detect both of these issues by accurately slowing down a program. And, as you might know from the previous post, we are able to slow down programs fairly accurately. Figure 1 illustrates the idea with a stacked bar chart for a hypothetical distribution of run-time over three methods. This distribution should remain identical, independent of a slowdown observed by the program. So, thereâs a linear relation between the absolute time measured and a constant relation between the percentage of time per method, depending on the slowdown.&lt;/p&gt;
    &lt;p&gt;With this slowdown approach, we can detect whether the profiler is accurate with respect to the predicted time increase. Iâll leave all the technical details to the paper. We can also slow down individual basic blocks accurately to make a particular method take more time. As it turns out, this is a good litmus test for the accuracy of profilers, and we find a number of examples where they fail to attribute the run time correctly. Figure 2 shows an example for the Havlak benchmark. The bar charts show how much change the four profilers detect after we slowed down &lt;code&gt;Vector.hasSome&lt;/code&gt; to the level indicated by the red dashed line.
In this particular example, async-profiler detects the change accurately.
JFR is probably within the margin of error.
However, JProfiler and YourKit are completely off. JProfiler likely canât deal with inlining and attributes the change to the &lt;code&gt;forEach&lt;/code&gt; method that calls &lt;code&gt;hasSome&lt;/code&gt;.
YourKit does not seem to see the change at all.&lt;/p&gt;
    &lt;p&gt;With this slowdown-based approach, we finally have a way to see how accurate sampling profilers are by approximating the ground truth profile. Since we canât measure the ground truth directly, we found a way to sidestep a fundamental problem and found a reasonably practical solution.&lt;/p&gt;
    &lt;p&gt;The paper details how we implement our divining approach, i.e., how we slow down programs accurately. It also has all the methodological details, research questions, benchmarking setup, and lots more numbers, especially in the appendix. So, please give it a read, and let us know what you think.&lt;/p&gt;
    &lt;p&gt;If you happen to attend the SPLASH conference, Humphrey is presenting our work today and on Saturday.&lt;/p&gt;
    &lt;p&gt;Questions, pointers, and suggestions are always welcome, for instance, on Mastodon, BlueSky, or Twitter.&lt;/p&gt;
    &lt;p&gt;Thanks to Octave for feedback on this post.&lt;/p&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Optimizing performance on top of modern runtime systems with just-in-time (JIT) compilation is a challenge for a wide range of applications from browser-based applications on mobile devices to large-scale server applications. Developers often rely on sampling-based profilers to understand where their code spends its time. Unfortunately, sampling of JIT-compiled programs can give inaccurate and sometimes unreliable results.&lt;/p&gt;
      &lt;p&gt;To assess accuracy of such profilers, we would ideally want to compare their results to a known ground truth. With the complexity of todayâs software and hardware stacks, such ground truth is unfortunately not available. Instead, we propose a novel technique to approximate a ground truth by accurately slowing down a Java program at the machine-code level, preserving its optimization and compilation decisions as well as its execution behavior on modern CPUs.&lt;/p&gt;
      &lt;p&gt;Our experiments demonstrate that we can slow down benchmarks by a specific amount, which is a challenge because of the optimizations in modern CPUs, and we verified with hardware profiling that on a basic-block level, the slowdown is accurate for blocks that dominate the execution. With the benchmarks slowed down to specific speeds, we confirmed that async-profiler, JFR, JProfiler, and YourKit maintain original performance behavior and assign the same percentage of run time to methods. Additionally, we identify cases of inaccuracy caused by missing debug information, which prevents the correct identification of the relevant source code. Finally, we tested the accuracy of sampling profilers by approximating the ground truth by the slowing down of specific basic blocks and found large differences in accuracy between the profilers.&lt;/p&gt;
      &lt;p&gt;We believe, our slowdown-based approach is the first practical methodology to assess the accuracy of sampling profilers for JIT-compiling systems and will enable further work to improve the accuracy of profilers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Divining Profiler Accuracy: An Approach to Approximate Profiler Accuracy Through Machine Code-Level Slowdown&lt;lb/&gt;H. Burchell, S. Marr; Proceedings of the ACM on Programming Languages, OOPSLA'25, ACM, 2025.&lt;/item&gt;
      &lt;item&gt;Paper: PDF&lt;/item&gt;
      &lt;item&gt;DOI: 10.1145/3763180&lt;/item&gt;
      &lt;item&gt;Appendix: online appendix&lt;/item&gt;
      &lt;item&gt; BibTex: bibtex &lt;quote&gt;@article{Burchell:2025:Divining, abstract = {Optimizing performance on top of modern runtime systems with just-in-time (JIT) compilation is a challenge for a wide range of applications from browser-based applications on mobile devices to large-scale server applications. Developers often rely on sampling-based profilers to understand where their code spends its time. Unfortunately, sampling of JIT-compiled programs can give inaccurate and sometimes unreliable results. To assess accuracy of such profilers, we would ideally want to compare their results to a known ground truth. With the complexity of today's software and hardware stacks, such ground truth is unfortunately not available. Instead, we propose a novel technique to approximate a ground truth by accurately slowing down a Java program at the machine-code level, preserving its optimization and compilation decisions as well as its execution behavior on modern CPUs. Our experiments demonstrate that we can slow down benchmarks by a specific amount, which is a challenge because of the optimizations in modern CPUs, and we verified with hardware profiling that on a basic-block level, the slowdown is accurate for blocks that dominate the execution. With the benchmarks slowed down to specific speeds, we confirmed that async-profiler, JFR, JProfiler, and YourKit maintain original performance behavior and assign the same percentage of run time to methods. Additionally, we identify cases of inaccuracy caused by missing debug information, which prevents the correct identification of the relevant source code. Finally, we tested the accuracy of sampling profilers by approximating the ground truth by the slowing down of specific basic blocks and found large differences in accuracy between the profilers. We believe, our slowdown-based approach is the first practical methodology to assess the accuracy of sampling profilers for JIT-compiling systems and will enable further work to improve the accuracy of profilers.}, acceptancerate = {0.356}, appendix = {https://doi.org/10.5281/zenodo.16911348}, articleno = {402}, author = {Burchell, Humphrey and Marr, Stefan}, blog = {https://stefan-marr.de/2025/10/can-we-know-whether-a-profiler-is-accurate/}, doi = {10.1145/3763180}, issn = {2475-1421}, journal = {Proceedings of the ACM on Programming Languages}, keywords = {Accuracy GroundTruth Java MeMyPublication Profiling Sampling myown}, month = oct, number = {OOPSLAB25}, numpages = {32}, pdf = {https://stefan-marr.de/downloads/oopsla25-burchell-marr-divining-profiler-accuracy.pdf}, publisher = {{ACM}}, series = {OOPSLA'25}, title = {{Divining Profiler Accuracy: An Approach to Approximate Profiler Accuracy Through Machine Code-Level Slowdown}}, year = {2025}, month_numeric = {10} }&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45587289</guid><pubDate>Wed, 15 Oct 2025 02:04:26 +0000</pubDate></item><item><title>Pixnapping Attack</title><link>https://www.pixnapping.com/</link><description>&lt;doc fingerprint="bd16f981309e636b"&gt;
  &lt;main&gt;
    &lt;p&gt;Pixnapping is a new class of attacks that allows a malicious Android app to stealthily leak information displayed by other Android apps or arbitrary websites. Pixnapping exploits Android APIs and a hardware side channel that affects nearly all modern Android devices. We have demonstrated Pixnapping attacks on Google and Samsung phones and end-to-end recovery of sensitive data from websites including Gmail and Google Accounts and apps including Signal, Google Authenticator, Venmo, and Google Maps. Notably, our attack against Google Authenticator allows any malicious app to steal 2FA codes in under 30 seconds while hiding the attack from the user.&lt;/p&gt;
    &lt;p&gt;The Pixnapping paper will appear in the 32nd ACM Conference on Computer and Communications Security (Taipei, Taiwan; October 13-17, 2025) with the following title:&lt;/p&gt;
    &lt;p&gt;You can download a preprint of the paper and cite it via this BibTeX citation.&lt;/p&gt;
    &lt;p&gt;The paper is the result of a collaboration between the following researchers:&lt;/p&gt;
    &lt;p&gt;We instantiated Pixnapping on five devices running Android versions 13 to 16 (up until build id BP3A.250905.014): Google Pixel 6, Google Pixel 7, Google Pixel 8, Google Pixel 9, and Samsung Galaxy S25.&lt;/p&gt;
    &lt;p&gt;We have not confirmed if Android devices from other vendors are affected by Pixnapping. However, the core mechanisms enabling the attack are typically available in all Android devices.&lt;/p&gt;
    &lt;p&gt;Any running Android app can mount this attack, even if it does not have any Android permissions (i.e., no permissions are specified in its manifest file).&lt;/p&gt;
    &lt;p&gt;Anything that is visible when the target app is opened can be stolen by the malicious app using Pixnapping. Chat messages, 2FA codes, email messages, etc. are all vulnerable since they are visible.&lt;/p&gt;
    &lt;p&gt;If an app has secret information that is not visible (e.g., it has a secret key that is stored but never shown on the screen), that information cannot be stolen by Pixnapping.&lt;/p&gt;
    &lt;p&gt;We do not know.&lt;/p&gt;
    &lt;p&gt;Make sure to install Android patches as soon as they become available.&lt;/p&gt;
    &lt;p&gt;We are not aware of mitigation strategies to protect apps against Pixnapping. If you have any insights into mitigations, please let us know and we will update this section.&lt;/p&gt;
    &lt;p&gt;The three steps a malicious app can use to mount a Pixnapping attack are:&lt;/p&gt;
    &lt;p&gt;Invoking a target app (e.g., Google Authenticator) to cause sensitive information to be submitted for rendering. This step is described in Section 3.1 of the paper.&lt;/p&gt;
    &lt;p&gt;Inducing graphical operations on individual sensitive pixels rendered by the target app (e.g., the pixels that are part of the screen region where a 2FA character is known to be rendered by Google Authenticator). This step is described in Section 3.2 of the paper.&lt;/p&gt;
    &lt;p&gt;Using a side channel (e.g., GPU.zip) to steal the pixels operated on during Step 2, one pixel at a time. This step is described in Section 3.3 of the paper.&lt;/p&gt;
    &lt;p&gt;Steps 2 and 3 are repeated for as many pixels as needed to run OCR over the recovered pixels and recover the original content. Conceptually, it is as if the malicious app was taking a screenshot of screen contents it should not have access to.&lt;/p&gt;
    &lt;p&gt;Pixnapping forces sensitive pixels into the rendering pipeline and overlays semi-transparent activities on top of those pixels via Android intents. To induce graphical operations on these pixels, our instantiations use Android’s window blur API. To measure rendering time, our instantiations use VSync callbacks. For a more detailed explanation, we refer to the paper.&lt;/p&gt;
    &lt;p&gt;Google has attempted to patch Pixnapping by limiting the number of activities an app can invoke blur on. However, we discovered a workaround to make Pixnapping work despite this patch. The workaround is still under embargo.&lt;/p&gt;
    &lt;p&gt;Pixnapping relies on the GPU.zip side channel to leak pixels.&lt;/p&gt;
    &lt;p&gt;As of October 2025, no GPU vendor has committed to patching GPU.zip.&lt;/p&gt;
    &lt;p&gt;Yes. Pixnapping is tracked under CVE-2025-48561 in the Common Vulnerabilities and Exposures (CVE) system.&lt;/p&gt;
    &lt;p&gt;Android is vulnerable to Pixnapping because it allows an app to:&lt;/p&gt;
    &lt;p&gt;We have not investigated the applicability of these properties on other platforms yet.&lt;/p&gt;
    &lt;p&gt;It is another vulnerability we discovered that an app can use to determine if any other app is installed on the phone. This information can be used to profile users. Note that unlike prior app list bypass tricks (e.g., [1] and [2]), nothing needs to be specified in the malicious app’s manifest file to exploit our app list bypass vulnerability. For a more detailed explanation, we refer to Section 3.1 of the paper.&lt;/p&gt;
    &lt;p&gt;As of October 2025, Google has not committed to patching our app list bypass vulnerability. They resolved our report as “Won’t fix (Infeasible)”.&lt;/p&gt;
    &lt;p&gt;Yes. The Pixnapping logo is free to use under a CC0 license.&lt;/p&gt;
    &lt;p&gt;We will release the source code at this link once patches become available: https://github.com/TAC-UCB/pixnapping&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45588594</guid><pubDate>Wed, 15 Oct 2025 06:05:51 +0000</pubDate></item><item><title>Just Talk to It – The No-Bs Way of Agentic Engineering</title><link>https://steipete.me/posts/just-talk-to-it</link><description>&lt;doc fingerprint="3f41187729a60ada"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve been more quiet here lately as I’m knee-deep working on my latest project. Agentic engineering has become so good that it now writes pretty much 100% of my code. And yet I see so many folks trying to solve issues and generating these elaborated charades instead of getting sh*t done.&lt;/p&gt;
    &lt;p&gt;This post partly is inspired by the conversations I had at last night’s Claude Code Anonymous in London and partly since it’s been an AI year since my last workflow update. Time for a check-in.&lt;/p&gt;
    &lt;p&gt;All of the basic ideas still apply, so I won’t mention simple things like context management again. Read my Optimal AI Workflow post for a primer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context &amp;amp; Tech-Stack&lt;/head&gt;
    &lt;p&gt;I work by myself, current project is a ~300k LOC TypeScript React app, a Chrome extension, a cli, a client app in Tauri and a mobile app in Expo. I host on vercel, a PR delivers a new version of my website in ~2 minutes to test. Everything else (apps etc) is not automated.&lt;/p&gt;
    &lt;head rend="h2"&gt;Harness &amp;amp; General Approach&lt;/head&gt;
    &lt;p&gt;I’ve completely moved to &lt;code&gt;codex&lt;/code&gt; cli as daily driver. I run between 3-8 in parallel in a 3x3 terminal grid, most of them in the same folder, some experiments go in separate folders. I experimented with worktrees, PRs but always revert back to this setup as it gets stuff done the fastest.&lt;/p&gt;
    &lt;p&gt;My agents do git atomic commits themselves. In order to maintain a mostly clean commit history, I iterated a lot on my agent file. This makes git ops sharper so each agent commits exactly the files it edited.&lt;/p&gt;
    &lt;p&gt;Yes, with claude you could do hooks and codex doesn’t support them yet, but models are incredibly clever and no hook will stop them if they are determined.&lt;/p&gt;
    &lt;p&gt;I was being ridiculed in the past and called a slop-generator, good to see that running parallel agents slowly gets mainstream.&lt;/p&gt;
    &lt;head rend="h2"&gt;Model Picker&lt;/head&gt;
    &lt;p&gt;I build pretty much everything with gpt-5-codex on mid settings. It’s a great compromise of smart &amp;amp; speed, and dials thinking up/down automatically. I found over-thinking these settings to not yield meaningful results, and it’s nice not having to think about ultrathink.&lt;/p&gt;
    &lt;head rend="h3"&gt;Blast Radius 💥&lt;/head&gt;
    &lt;p&gt;Whenever I work, I think about the “blast radius”. I didn’t come up with that term, I do love it tho. When I think of a change I have a pretty good feeling about how long it’ll take and how many files it will touch. I can throw many small bombs at my codebase or a one “Fat Man” and a few small ones. If you throw multiple large bombs, it’ll be impossible to do isolated commits, much harder to reset if sth goes wrong.&lt;/p&gt;
    &lt;p&gt;This is also a good indicator while I watch my agents. If something takes longer than I anticipated, I just hit escape and ask “what’s the status” to get a status update and then either help the model to find the right direction, abort or continue. Don’t be afraid of stopping models mid-way, file changes are atomic and they are really good at picking up where they stopped.&lt;/p&gt;
    &lt;p&gt;When I am unsure about the impact, I use “give me a few options before making changes” to gauge it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not worktrees?&lt;/head&gt;
    &lt;p&gt;I run one dev server, as I evolve my project I click through it and test multiple changes at once. Having a tree/branch per change would make this significantly slower, spawning multiple dev servers would quickly get annoying. I also have limitations for Twitter OAuth, so I can only register some domains for callbacks.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about Claude Code?&lt;/head&gt;
    &lt;p&gt;I used to love Claude Code, these days I can’t stand it anymore (even tho codex is a fan). It’s language, the absolutely right’s, the 100% production ready messages while tests fail - I just can’t anymore. Codex is more like the introverted engineer that chugs along and just gets stuff done. It reads much more files before starting work so even small prompts usually do exactly what I want.&lt;/p&gt;
    &lt;p&gt;There’s broad consensus in my timeline that codex is the way to go.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other benefits of codex&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;~230k usable context vs claude’s 156k. Yes, there’s Sonnet 1Mio if you get lucky or pay API pricing, but realistically Claude gets very silly long before it depletes that context so it’s not realistically something you can use.&lt;/item&gt;
      &lt;item&gt;More efficient token use. Idk what OpenAI does different, but my context fills up far slower than with Claude Code. I used to see Compacting… all the time when using claude, I very rarely manage to exceed the context in codex.&lt;/item&gt;
      &lt;item&gt;Message Queuing. Codex allows to queue messages. Claude had this feature, but a few months ago they changed it so your messages “steer” the model. If I want to steer codex, I just press escape and enter to send the new message. Having the option for both is just far better. I often queue related feature tasks and it just reliably works them off.&lt;/item&gt;
      &lt;item&gt;Speed OpenAI rewrote codex in Rust, and it shows. It’s incredibly fast. With Claude Code I often have multi-second freezes and it’s process blows up to gigabytes of memory. And then there’s the terminal flickering, especially when using Ghostty. Codex has none of that. It feels incredibly lightweight and fast.&lt;/item&gt;
      &lt;item&gt;Language. This really makes a difference to my mental health. I’ve been screaming at claude so many times. I rarely get angry with codex. Even if codex would be a worse model I’d use it for that fact alone. If you use both for a few weeks you will understand.&lt;/item&gt;
      &lt;item&gt;No random markdown files everywhere. IYKYK.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Why not $harness&lt;/head&gt;
    &lt;p&gt;IMO there’s simply not much space between the end user and the model company. I get by far the best deal using a subscription. I currently have 4 OpenAI subs and 1 Anthropic sub, so my overall costs are around 1k/month for basically unlimited tokens. If I’d use API calls, that’d cost my around 10x more. Don’t nail me on this math, I used some token counting tools like ccusage and it’s all somewhat imprecise, but even if it’s just 5x it’s a damn good deal.&lt;/p&gt;
    &lt;p&gt;I like that we have tools like amp or Factory, I just don’t see them surviving long-term. Both codex and claude code are getting better with every release, and they all converge to the same ideas and feature set. Some might have a temporary edge with better todo lists, steering or slight dx features, but I don’t see them significantly out-competing the big AI companies.&lt;/p&gt;
    &lt;p&gt;amp moved away from GPT-5 as driver and now calls it their “oracle”. Meanwhile I use codex and basically constantly work with the smarter model, the oracle. Yes, there are benchmarks, but given the skewed usage numbers, I don’t trust them. codex gets me far better results than amp. I have to give them kudos tho for session sharing, they push some interesting ideas ahead.&lt;/p&gt;
    &lt;p&gt;Factory, unconvinced. Their videos are a bit cringe, I do hear good things in my timeline about it tho, even if images aren’t supported (yet) and they have the signature flicker.&lt;/p&gt;
    &lt;p&gt;Cursor… it’s tab completion model is industry leading, if you still write code yourself. I use VS Code mostly, I do like them pushing things like browser automation and plan mode tho. I did experiment with GPT-5-Pro but Cursor still has the same bugs that annoyed me back in May. I hear that’s being worked on tho, so it stays in my dock.&lt;/p&gt;
    &lt;p&gt;Others like Auggie were a blip on my timeline and nobody ever mentioned them again. In the end they all wrap either GPT-5 and/or Sonnet and are replaceable. RAG might been helpful for Sonnet, but GPT-5 is so good at searching at you don’t need a separate vector index for your code.&lt;/p&gt;
    &lt;p&gt;The most promising candidates are opencode and crush, esp. in combination with open models. You can totally use your OpenAI or Anthropic sub with them as well (thanks to clever hax), but it’s questionable if that is allowed, and what’s the point of using a less capable harness for the model optimized for codex or Claude Code.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about $openmodel&lt;/head&gt;
    &lt;p&gt;I keep an eye on China’s open models, and it’s impressive how quickly they catch up. GLM 4.6 and Kimi K2.1 are strong contenders that slowly reach Sonnet 3.7 quality, I don’t recommend them as daily driver tho.&lt;/p&gt;
    &lt;p&gt;The benchmarks only tell half the story. IMO agentic engineering moved from “this is crap” to “this is good” around May with the release of Sonnet 4.0, and we hit an even bigger leap from good to “this is amazing” with gpt-5-codex.&lt;/p&gt;
    &lt;head rend="h3"&gt;Plan Mode &amp;amp; Approach&lt;/head&gt;
    &lt;p&gt;What benchmarks miss is the strategy that the model+harness pursue when they get a prompt. codex is far FAR more careful and reads much more files in your repo before deciding what to do. It pushes back harder when you make a silly request. Claude/other agents are much more eager and just try something. This can be mitigated with plan mode and rigorous structure docs, to me that feels like working around a broken system.&lt;/p&gt;
    &lt;p&gt;I rarely use big plan files now with codex. codex doesn’t even have a dedicated plan mode - however it’s so much better at adhering to the prompt that I can just write “let’s discuss” or “give me options” and it will diligently wait until I approve it. No harness charade needed. Just talk to it.&lt;/p&gt;
    &lt;head rend="h3"&gt;But Claude Code now has Plugins&lt;/head&gt;
    &lt;p&gt;Do you hear that noise in the distance? It’s me sigh-ing. What a big pile of bs. This one really left me disappointed in Anthropic’s focus. They try to patch over inefficiencies in the model. Yes, maintaining good documents for specific tasks is a good idea. I keep a big list of useful docs in a docs folder as markdown.&lt;/p&gt;
    &lt;head rend="h3"&gt;But but Subagents !!!1!&lt;/head&gt;
    &lt;p&gt;But something has to be said about this whole dance with subagents. Back in May this was called subtasks, and mostly a way to spin out tasks into a separate context when the model doesn’t need the full text - mainly a way to parallelize or to reduce context waste for e.g. noisy build scripts. Later they rebranded and improved this to subagents, so you spin of a task with some instructions, nicely packaged.&lt;/p&gt;
    &lt;p&gt;The use case is the same. What others do with subagents, I usually do with separate windows. If I wanna research sth I might do that in a separate terminal pane and paste it to another one. This gives me complete control and visibility over the context I engineer, unlike subagents who make it harder to view and steer or control what is sent back.&lt;/p&gt;
    &lt;p&gt;And we have to talk about the subagent Anthropic recommends on their blog. Just look at this “AI Engineer” agent. It’s an amalgamation of slop, mentioning GPT-4o and o1 for integration, and overall just seems like an autogenerated soup of words that tries to make sense. There’s no meat in there that would make your agent a better “AI engineer”.&lt;/p&gt;
    &lt;p&gt;What does that even mean? If you want to get better output, telling your model “You are an AI engineer specializing in production-grade LLM applications” will not change that. Giving it documentation, examples and do/don’t helps. I bet that you’d get better result if you ask your agent to “google AI agent building best practices” and let it load some websites than this crap. You could even make the argument that this slop is context poison.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I write prompts&lt;/head&gt;
    &lt;p&gt;Back when using claude, I used to write (ofc not, I speak) very extensive prompts, since this model “gets me” the more context I supply. While this is true with any model, I noticed that my prompts became significantly shorter with codex. Often it’s just 1-2 sentences + an image. The model is incredibly good at reading the codebase and just gets me. I even sometimes go back to typing since codex requires so much less context to understand.&lt;/p&gt;
    &lt;p&gt;Adding images is an amazing trick to provide more context, the model is really good at finding exactly what you show, it finds strings and matches it and directly arrives at the place you mention. I’d say at least 50% of my prompts contain a screenshot. I rarely annotate that, that works even better but is slower. A screenshot takes 2 seconds to drag into the terminal.&lt;/p&gt;
    &lt;p&gt;Wispr Flow with semantic correction is still king.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web-Based Agents&lt;/head&gt;
    &lt;p&gt;Lately I experimented again with web agents: Devin, Cursor and Codex. Google’s Jules looks nice but was really annoying to set up and Gemini 2.5 just isn’t a good model anymore. Things might change soon once we get Gemini 3 Pro. The only one that stuck is codex web. It also is annoying to setup and broken, the terminal currently doesn’t load correctly, but I had an older version of my environment and made it work, with the price of slower wramp-up times.&lt;/p&gt;
    &lt;p&gt;I use codex web as my short-term issue tracker. Whenever I’m on the go and have an idea, I do a one-liner via the iOS app and later review this on my Mac. Sure, I could do way more with my phone and even review/merge this, but I choose not to. My work is already addictive enough as-is, so when I’m out or seeing friends, I don’t wanna be pulled in even more. Heck, I say this as someone who spent almost two months building a tool to make it easier to code on your phone.&lt;/p&gt;
    &lt;p&gt;Codex web didn’t even count towards your usage limits, but these days sadly are numbered.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Agentic Journey&lt;/head&gt;
    &lt;p&gt;Let’s talk about tools. Conductor, Terragon, Sculptor and the 1000 other ones. Some are hobby projects, some are drowning in VC money. I tried so many of them. None stick. IMO they work around current inefficiencies and promote a workflow that just isn’t optimal. Plus, most of them hide the terminal and don’t show everything the model shows.&lt;/p&gt;
    &lt;p&gt;Most are thin wrappers around Anthropic’s SDK + work tree management. There’s no moat. And I question if you even want easier access to coding agents on your phone. The little use case these did for me, codex web fully covers.&lt;/p&gt;
    &lt;p&gt;I do see this pattern tho that almost every engineer goes through a phase of building their own tools, mostly because it’s fun and because it’s so much easier now. And what else to build than tools that (we think) will make it simpler to build more tools?&lt;/p&gt;
    &lt;head rend="h2"&gt;But Claude Code can Background Tasks!&lt;/head&gt;
    &lt;p&gt;True. codex currently lacks a few bells and whistles that claude has. The most painful omission is background task management. While it should have a timeout, I did see it get stuck quite a few times with cli tasks that don’t end, like spinning up a dev server or tests that deadlock.&lt;/p&gt;
    &lt;p&gt;This was one of the reasons I reverted back to claude, but since that model is just so silly in other ways, I now use &lt;code&gt;tmux&lt;/code&gt;. It’s an old tool to run CLIs in persistent sessions in the background and there’s plenty world knowledge in the model, so all you need to do is “run via tmux”. No custom agent md charade needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about MCPs&lt;/head&gt;
    &lt;p&gt;Other people wrote plenty about MCPs. IMO most are something for the marketing department to make a checkbox and be proud. Almost all MCPs really should be clis. I say that as someone who wrote 5 MCPs myself.&lt;/p&gt;
    &lt;p&gt;I can just refer to a cli by name. I don’t need any explanation in my agents file. The agent will try $randomcrap on the first call, the cli will present the help menu, context now has full info how this works and from now on we good. I don’t have to pay a price for any tools, unlike MCPs which are a constant cost and garbage in my context. Use GitHub’s MCP and see 23k tokens gone. Heck, they did make it better because it was almost 50.000 tokens when it first launched. Or use the &lt;code&gt;gh&lt;/code&gt; cli which has basically the same feature set, models already know how to use it, and pay zero context tax.&lt;/p&gt;
    &lt;p&gt;I did open source some of my cli tools, like bslog and inngest.&lt;/p&gt;
    &lt;p&gt;I do use &lt;code&gt;chrome-devtools-mcp&lt;/code&gt; these days to close the loop. it replaced Playwright as my to-go MCP for web debugging. I don’t need it lots but when I do, it’s quite useful to close the loop. I designed my website so that I can create api keys that allow my model to query any endpoint via curl, which is faster and more token-efficient in almost all use cases, so even that MCP isn’t something I need daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;But the code is slop!&lt;/head&gt;
    &lt;p&gt;I spend about 20% of my time on refactoring. Ofc all of that is done by agents, I don’t waste my time doing that manually. Refactor days are great when I need less focus or I’m tired, since I can make great progress without the need of too much focus or clear thinking.&lt;/p&gt;
    &lt;p&gt;Typical refactor work is using &lt;code&gt;jscpd&lt;/code&gt; for code duplication, &lt;code&gt;knip&lt;/code&gt; for dead code, running &lt;code&gt;eslint&lt;/code&gt;’s &lt;code&gt;react-compiler&lt;/code&gt; and deprecation plugins, checking if we introduced api routes that can be consolidated, maintaining my docs, breaking apart files that grew too large, adding tests and code comments for tricky parts, updating dependencies, tool upgrades, file restructuring, finding and rewriting slow tests, mentioning modern react patterns and rewriting code (e.g. you might not need &lt;code&gt;useEffect&lt;/code&gt;). There’s always something to do.&lt;/p&gt;
    &lt;p&gt;You could make the argument that this could be done on each commit, I do find these phases of iterating fast and then maintaining and improving the codebase - basically paying back some technical debt, to be far more productive, and overall far more fun.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do you do spec-driven development?&lt;/head&gt;
    &lt;p&gt;I used to back in June. Designing a big spec, then let the model build it, ideally for hours. IMO that’s the old way of thinking about building software.&lt;/p&gt;
    &lt;p&gt;My current approach is usually that I start a discussion with codex, I paste in some websites, some ideas, ask it to read code, and we flesh out a new feature together. If it’s something tricky, I ask it to write everything into a spec, give that to GPT-5-Pro for review (via chatgpt.com) to see if it has better ideas (surprisingly often, this greatly improves my plan!) and then paste back what I think is useful into the main context to update the file.&lt;/p&gt;
    &lt;p&gt;By now I have a good feeling which tasks take how much context, and codex’s context space is quite good, so often I’ll just start building. Some people are religious and always use a new context with the plan - IMO that was useful for Sonnet, but GPT-5 is far better at dealing with larger contexts, and doing that would easily add 10 minutes to everything as the model has to slowly fetch all files needed to build the feature again.&lt;/p&gt;
    &lt;p&gt;The far more fun approach is when I do UI-based work. I often start with sth simple and woefully under-spec my requests, and watch the model build and see the browser update in real time. Then I queue in additional changes and iterate on the feature. Often I don’t fully know how something should look like, and that way I can play with the idea and iterate and see it slowly come to life. I often saw codex build something interesting I didn’t even think of. I don’t reset, I simply iterate and morph the chaos into the shape that feels right.&lt;/p&gt;
    &lt;p&gt;Often I get ideas for related interactions and iterate on other parts as well while I build it, that work I do in a different agent. Usually I work on one main feature and some smaller, tangentially related tasks.&lt;/p&gt;
    &lt;p&gt;As I’m writing this, I build a new Twitter data importer in my Chrome extension, and for that I reshape the graphql importer. Since I’m a bit unsure if that is the right approach, that one is in a separate folder so I can look at the PR and see if that approach makes sense. The main repo does refactoring, so I can focus on writing this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Show me your slash commands!&lt;/head&gt;
    &lt;p&gt;I only have a few, and I use them rarely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/commit&lt;/code&gt;(custom text to explain that multiple agents work in the same folder and to only commit your changes, so I get clean comments and gpt doesn’t freak out about other changes and tries to revert things if linter fails)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/automerge&lt;/code&gt;(process one PR at a time, react to bot comments, reply, get CI green and squash when green)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/massageprs&lt;/code&gt;(same as automerge but without the squashing so I can parallelize the process if I have a lot of PRs)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/review&lt;/code&gt;(built-in, only sometimes since I have review bots on GH, but can be useful)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And even with these, usually I just type “commit”, unless I know that there’s far too many dirty files and the agent might mess up without some guidance. No need for charade/context waste when I’m confident that this is enough. Again, you develop intuition for these. I have yet to see other commands that really are useful.&lt;/p&gt;
    &lt;head rend="h2"&gt;What other tricks do you have?&lt;/head&gt;
    &lt;p&gt;Instead of trying to formulate the perfect prompt to motivate the agent to continue on a long-running task, there’s lazy workarounds. If you do a bigger refactor, codex often stops with a mid-work reply. Queue up continue messages if you wanna go away and just see it done. If codex is done and gets more messages, it happily ignores them.&lt;/p&gt;
    &lt;p&gt;Ask the model to write tests after each feature/fix is done. Use the same context. This will lead to far better tests, and likely uncover a bug in your implementation. If it’s purely a UI tweak, tests likely make less sense, but for anything else, do it. AI generally is bad at writing good tests, it’s still helpful tho, and let’s be honest - are you writing tests for every fix you make?&lt;/p&gt;
    &lt;p&gt;Ask the model to preserve your intent and “add code comments on tricky parts” helps both you and future model runs.&lt;/p&gt;
    &lt;p&gt;When things get hard, prompting and adding some trigger words like “take your time” “comprehensive” “read all code that could be related” “create possible hypothesis” makes codex solve even the trickiest problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does your Agents/Claude file look like?&lt;/head&gt;
    &lt;p&gt;I have an Agents.md file with a symlink to claude.md, since Anthropic decided not to standardize. I recognize that this is difficult and sub-optimal, since GPT-5 prefers quite different prompting than Claude. Stop here and read their prompting guide if you haven’t yet.&lt;/p&gt;
    &lt;p&gt;While Claude reacts well to 🚨 SCREAMING ALL-CAPS 🚨 commands that threaten it that it will imply ultimate failure and 100 kittens will die if it runs command X, that freaks out GPT-5. (Rightfully so). So drop all of that and just use words like a human. That also means that these files can’t optimally be shared. Which isn’t a problem to me since I mostly use codex, and accept that the instructions might be too weak for the rare instances where claude gets to play.&lt;/p&gt;
    &lt;p&gt;My Agent file is currently ~800 lines long and feels like a collection of organizational scar tissue. I didn’t write it, codex did, and anytime sth happens I ask it to make a concise note in there. I should clean this up at some point, but despite it being large it works incredibly well, and gpt really mostly honors entries there. At least it does far far more often than Claude ever did. (Sonnet 4.5 got better there, to give them some credit)&lt;/p&gt;
    &lt;p&gt;Next to git instruction it contains an explanation about my product, common naming and API patterns I prefer, notes about React Compiler - often it’s things that are newer than world knowledge because my tech stack is quite bleeding edge. I expect that I can again reduce things in there with model updates. For example, Sonnet 4.0 really needed guidance to understand Tailwind 4, Sonnet 4.5 and GPT-5 are newer and know about that version so I was able to delete all that fluff.&lt;/p&gt;
    &lt;p&gt;Significant blocks are about which React patterns I prefer, database migration management, testing, using and writing ast-grep rules. (If you don’t know or don’t use ast-grep as codebase linter, stop here and ask your model to set this up as a git hook to block commits)&lt;/p&gt;
    &lt;p&gt;I also experimented and started using a text-based “design system” for how things should look, the verdict is still out on that one.&lt;/p&gt;
    &lt;head rend="h2"&gt;So GPT-5-Codex is perfect?&lt;/head&gt;
    &lt;p&gt;Absolutely not. Sometimes it refactors for half an hour and then panics and reverts everything, and you need to re-run and soothen it like a child to tell it that it has enough time. Sometimes it forgets that it can do bash commands and it requires some encouragement. Sometimes it replies in russian or korean. Sometimes the monster slips and sends raw thinking to bash. But overall these are quite rare and it’s just so insanely good in almost everything else that I can look past these flaws. Humans aren’t perfect either.&lt;/p&gt;
    &lt;p&gt;My biggest annoyance with codex is that it “loses” lines, so scrolling up quickly makes parts of the text disappear. I really hope this is on top of OpenAI’s bug roster, as it’s the main reason I sometimes have to slow down, so messages don’t disappear.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Don’t waste your time on stuff like RAG, subagents, Agents 2.0 or other things that are mostly just charade. Just talk to it. Play with it. Develop intuition. The more you work with agents, the better your results will be.&lt;/p&gt;
    &lt;p&gt;Simon Willison’s article makes an excellent point - many of the skills needed to manage agents are similar to what you need when managing engineers - almost all of these are characteristics of senior software engineers.&lt;/p&gt;
    &lt;p&gt;And yes, writing good software is still hard. Just because I don’t write the code anymore doesn’t mean I don’t think hard about architecture, system design, dependencies, features or how to delight users. Using AI simply means that expectations what to ship went up.&lt;/p&gt;
    &lt;p&gt;PS: This post is 100% organic and hand-written. I love AI, I also recognize that some things are just better done the old-fashioned way. Keep the typos, keep my voice. 🚄✌️&lt;/p&gt;
    &lt;p&gt;PPS: Credit for the header graphic goes to Thorsten Ball.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45588689</guid><pubDate>Wed, 15 Oct 2025 06:21:04 +0000</pubDate></item><item><title>Show HN: Firm, a text-based work management system</title><link>https://github.com/42futures/firm</link><description>&lt;doc fingerprint="3a9ba6182815e7b8"&gt;
  &lt;main&gt;
    &lt;p&gt;A text-based work management system for technologists.&lt;/p&gt;
    &lt;p&gt;Modern businesses are natively digital, but lack a unified view. Your data is scattered across SaaS tools you don't control, so you piece together answers by jumping between platforms.&lt;/p&gt;
    &lt;p&gt;Your business is a graph: customers link to projects, projects link to tasks, people link to organizations. Firm lets you define these relationships in plain text files (you own!).&lt;/p&gt;
    &lt;p&gt;Version controlled, locally stored and structured as code with the Firm DSL. This structured representation of your work, business-as-code, makes your business readable to yourself and to the robots that help you run it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Everything in one place: Organizations, contacts, projects, and how they relate.&lt;/item&gt;
      &lt;item&gt;Own your data: Plain text files and tooling that runs on your machine.&lt;/item&gt;
      &lt;item&gt;Open data model: Tailor to your business with custom schemas.&lt;/item&gt;
      &lt;item&gt;Automate anything: Search, report, integrate, whatever. It's just code.&lt;/item&gt;
      &lt;item&gt;AI-ready: LLMs can read, write, and query your business structure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Firm CLI is available to download via Github Releases. Install scripts are provided for desktop platforms to make that process easy.&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/42futures/firm/main/install.sh | sudo bash&lt;/code&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/42futures/firm/main/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;Firm operates on a "workspace": a directory containing all your &lt;code&gt;.firm&lt;/code&gt; DSL files. The Firm CLI processes every file in this workspace to build a unified, queryable graph of your business.&lt;/p&gt;
    &lt;p&gt;The first step is to add an entity to your workspace. You can do this either by using the CLI or by writing the DSL yourself.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;firm add&lt;/code&gt; to interactively generate new entities. Out of the box, Firm supports a set of pre-built entity schemas for org mapping, customer relations and work management. The CLI will prompt you for the necessary info and generate corresponding DSL.&lt;/p&gt;
    &lt;code&gt;$ firm add&lt;/code&gt;
    &lt;code&gt;Adding new entity

&amp;gt; Type: organization
&amp;gt; ID: megacorp
&amp;gt; Name: Megacorp Ltd.
&amp;gt; Email: mega@corp.com
&amp;gt; Urls: ["corp.com"]

Writing generated DSL to file my_workspace/generated/organization.firm
&lt;/code&gt;
    &lt;p&gt;Alternatively, you can create a &lt;code&gt;.firm&lt;/code&gt; file and write the DSL yourself.&lt;/p&gt;
    &lt;code&gt;organization megacorp {
  name = "Megacorp Ltd."
  email = "mega@corp.com"
  urls = ["corp.com"]
}
&lt;/code&gt;
    &lt;p&gt;Both of these methods achieve the same result: a new entity defined in your Firm workspace.&lt;/p&gt;
    &lt;p&gt;Once you have entities in your workspace, you can query them using the CLI.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;firm list&lt;/code&gt; to see all entities of a specific type.&lt;/p&gt;
    &lt;code&gt;$ firm list task&lt;/code&gt;
    &lt;code&gt;Found 7 entities with type 'task'

ID: task.design_homepage
Name: Design new homepage
Is completed: false
Assignee ref: person.jane_doe

...
&lt;/code&gt;
    &lt;p&gt;To view the full details of a single entity, use &lt;code&gt;firm get&lt;/code&gt; followed by the entity's type and ID.&lt;/p&gt;
    &lt;code&gt;$ firm get person john_doe&lt;/code&gt;
    &lt;code&gt;Found 'person' entity with ID 'john_doe'

ID: person.john_doe
Name: John Doe
Email: john@doe.com
&lt;/code&gt;
    &lt;p&gt;The power of Firm lies in its ability to travel a graph of your business. Use &lt;code&gt;firm related&lt;/code&gt; to explore connections to/from any entity.&lt;/p&gt;
    &lt;code&gt;$ firm related contact john_doe&lt;/code&gt;
    &lt;code&gt;Found 1 relationships for 'contact' entity with ID 'john_doe'

ID: interaction.megacorp_intro
Type: Call
Subject: Initial discussion about Project X
Interaction date: 2025-09-30 09:45:00 +02:00
Initiator ref: person.jane_smith
Primary contact ref: contact.john_doe
&lt;/code&gt;
    &lt;p&gt;You've seen the basic commands for interacting with a Firm workspace. The project is a work-in-progress, and you can expect to see more sophisticated features added over time, including a more powerful query engine and tools for running business workflows directly from the CLI.&lt;/p&gt;
    &lt;p&gt;Beyond the CLI, you can integrate Firm's core logic directly into your own software using the &lt;code&gt;firm_core&lt;/code&gt; and &lt;code&gt;firm_lang&lt;/code&gt; Rust packages. This allows you to build more powerful automations and integrations on top of Firm.&lt;/p&gt;
    &lt;p&gt;First, add the Firm crates to your &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[dependencies]
firm_core = { git = "https://github.com/42futures/firm.git" }
firm_lang = { git = "https://github.com/42futures/firm.git" }&lt;/code&gt;
    &lt;p&gt;You can then load a workspace, build the entity graph, and query it programmatically:&lt;/p&gt;
    &lt;code&gt;use firm_lang::workspace::Workspace;
use firm_core::EntityGraph;

// Load workspace from a directory
let mut workspace = Workspace::new();
workspace.load_directory("./my_workspace")?;
let build = workspace.build()?;

// Build the graph from the workspace entities
let mut graph = EntityGraph::new();
graph.add_entities(build.entities)?;
graph.build();

// Query the graph for a specific entity
let lead = graph.get_entity(&amp;amp;EntityId::new("lead.ai_validation_project"))?;

// Traverse a relationship to another entity
let contact_ref = lead.get_field(FieldId::new("contact_ref"))?;
let contact = contact_ref.resolve_entity_reference(&amp;amp;graph)?;&lt;/code&gt;
    &lt;p&gt;This gives you full access to the underlying data structures, providing a foundation for building custom business automations.&lt;/p&gt;
    &lt;p&gt;Firm is organized as a Rust workspace with three crates:&lt;/p&gt;
    &lt;p&gt;Core data structures and graph operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Entity data model&lt;/item&gt;
      &lt;item&gt;Typed fields with references&lt;/item&gt;
      &lt;item&gt;Relationship graph with query capabilities&lt;/item&gt;
      &lt;item&gt;Entity schemas and validation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DSL parsing and generation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tree-sitter-based parser for &lt;code&gt;.firm&lt;/code&gt;files&lt;/item&gt;
      &lt;item&gt;Conversion between DSL and entities&lt;/item&gt;
      &lt;item&gt;Workspace support for multi-file projects&lt;/item&gt;
      &lt;item&gt;DSL generation from entities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Grammar is defined in tree-sitter-firm.&lt;/p&gt;
    &lt;p&gt;Command-line interface, making the Firm workspace interactive.&lt;/p&gt;
    &lt;p&gt;Firm's data model is built on a few key concepts. Each concept is accessible declaratively through the &lt;code&gt;.firm&lt;/code&gt; DSL for human-readable definitions, and programmatically through the Rust packages for building your own automations.&lt;/p&gt;
    &lt;p&gt;Entities are the fundamental business objects in your workspace, like people, organizations, or projects. Each entity has a unique ID, a type, and a collection of fields.&lt;/p&gt;
    &lt;p&gt;In the DSL, you define an entity with its type and ID, followed by its fields in a block:&lt;/p&gt;
    &lt;code&gt;person john_doe {
    name = "John Doe"
    email = "john@doe.com"
}
&lt;/code&gt;
    &lt;p&gt;In Rust, this corresponds to an &lt;code&gt;Entity&lt;/code&gt; struct:&lt;/p&gt;
    &lt;code&gt;let person = Entity::new(EntityId::new("john_doe"), EntityType::new("person"))
    .with_field(FieldId::new("name"), "John Doe")
    .with_field(FieldId::new("email"), "john@doe.com");&lt;/code&gt;
    &lt;p&gt;Fields are typed key-value pairs attached to an entity. Firm supports a rich set of types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;String&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Integer&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Float&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Boolean&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Currency&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;DateTime&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;List&lt;/code&gt;of other values&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Reference&lt;/code&gt;to other fields or entities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Path&lt;/code&gt;to a local file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the DSL, the syntax maps directly to these types:&lt;/p&gt;
    &lt;code&gt;my_task design_homepage {
    title = "Design new homepage"        // String
    priority = 1                         // Integer
    completed = false                    // Boolean
    budget = 5000.00 USD                 // Currency
    due_date = 2024-12-01 at 17:00 UTC   // DateTime
    tags = ["ui", "ux"]                  // List
    assignee = person.jane_doe           // Reference
    deliverable = path"./homepage.zip"   // Path
}
&lt;/code&gt;
    &lt;p&gt;In Rust, these are represented by the &lt;code&gt;FieldValue&lt;/code&gt; enum:&lt;/p&gt;
    &lt;code&gt;let value = FieldValue::Integer(42);&lt;/code&gt;
    &lt;p&gt;The power of Firm comes from connecting entities. You create relationships using &lt;code&gt;Reference&lt;/code&gt; fields.&lt;/p&gt;
    &lt;p&gt;When Firm processes your workspace, it builds the entity graph representing of all your entities (as nodes) and their relationships (as directed edges). This graph is what allows for traversal and querying.&lt;/p&gt;
    &lt;p&gt;In the DSL, creating a relationship is as simple as referencing another entity's ID.&lt;/p&gt;
    &lt;code&gt;contact john_at_acme {
    person_ref = person.john_doe
    organization_ref = organization.acme_corp
}
&lt;/code&gt;
    &lt;p&gt;In Rust, you build the graph by loading entities and calling the &lt;code&gt;.build()&lt;/code&gt; method, which resolves all references into queryable links.&lt;/p&gt;
    &lt;code&gt;let mut graph = EntityGraph::new();
graph.add_entities(workspace.build()?.entities)?;
graph.build(); // Builds relationships from references

// Now you can traverse the graph
let contact = graph.get_entity(&amp;amp;EntityId::new("contact.john_at_acme"))?;
let person_ref = contact.get_field(FieldId::new("person_ref"))?;
let person = person_ref.resolve_entity_reference(&amp;amp;graph)?;&lt;/code&gt;
    &lt;p&gt;Schemas allow you to define and enforce a structure for your entities, ensuring data consistency. You can specify which fields are required or optional and what their types should be.&lt;/p&gt;
    &lt;p&gt;In the DSL, you can define a schema that other entities can adhere to:&lt;/p&gt;
    &lt;code&gt;schema custom_project {
    field {
        name = "title"
        type = "string"
        required = true
    }
    field {
        name = "budget"
        type = "currency"
        required = false
    }
}

custom_project my_project {
    title  = "My custom project"
    budget = 42000 EUR
}
&lt;/code&gt;
    &lt;p&gt;In Rust, you can define schemas programmatically to validate entities.&lt;/p&gt;
    &lt;code&gt;let schema = EntitySchema::new(EntityType::new("project"))
    .with_required_field(FieldId::new("title"), FieldType::String)
    .with_optional_field(FieldId::new("budget"), FieldType::Currency);

schema.validate(&amp;amp;some_project_entity)?;&lt;/code&gt;
    &lt;p&gt;Firm includes schemas for a range of built-in entities like Person, Organization, and Industry.&lt;/p&gt;
    &lt;p&gt;Firm's entity taxonomy is built on the REA model (Resources, Events, Agents) with inspiration from Schema.org, designed for flexible composition and efficient queries.&lt;/p&gt;
    &lt;p&gt;Every entity maps to a Resource (thing with value), an Event (thing that happens), or an Agent (thing that acts).&lt;/p&gt;
    &lt;p&gt;We separate objective reality from business relationships:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fundamental entities represent things that exist independently (&lt;code&gt;Person&lt;/code&gt;,&lt;code&gt;Organization&lt;/code&gt;,&lt;code&gt;Document&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Contextual entities represent your business relationships and processes (&lt;code&gt;Contact&lt;/code&gt;,&lt;code&gt;Lead&lt;/code&gt;,&lt;code&gt;Project&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Entities reference each other rather than extending. One &lt;code&gt;Person&lt;/code&gt; can be referenced by multiple &lt;code&gt;Contact&lt;/code&gt;, &lt;code&gt;Employee&lt;/code&gt;, and &lt;code&gt;Partner&lt;/code&gt; entities simultaneously.&lt;/p&gt;
    &lt;p&gt;When the entity graph is built, all &lt;code&gt;Reference&lt;/code&gt; values automatically create directed edges between entities. This enables traversal queries like "find all Tasks for Opportunities whose Contacts work at Organization X" without complex joins.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45588959</guid><pubDate>Wed, 15 Oct 2025 07:01:53 +0000</pubDate></item><item><title>Europe's Digital Sovereignty Paradox – "Chat Control" Update</title><link>https://www.process-one.net/blog/chat-control-update-oct-2025/</link><description>&lt;doc fingerprint="2df72237b3c75cdb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Europe's Digital Sovereignty Paradox - "Chat Control" update&lt;/head&gt;
    &lt;p&gt;Can you build tech independence while breaking encryption?&lt;/p&gt;
    &lt;p&gt;October 14th was supposed to be the day the European Council voted to mandate scanning of all private communications, encrypted or not.&lt;/p&gt;
    &lt;p&gt;The vote was pulled at the last minute.&lt;/p&gt;
    &lt;p&gt;Germany withdrew support, creating a blocking minority that blocked the Danish Presidency's hope to get the text approved. Denmark still hopes to push this through by the end of its EU presidency in December. I personally would like to be optimistic and think that the tech community managed to raise enough concerns with EU policymakers.&lt;/p&gt;
    &lt;p&gt;Hundreds of European companies such as Proton, NordVPN, Tuta, Murena, Element, ProcessOne voiced their concerns about Chat Control. These companies are building the European alternatives we need for digital sovereignty. They offer what the EuroStack coalition is demanding: local infrastructure, values-driven technology, independence from US hyperscalers.&lt;/p&gt;
    &lt;p&gt;And EU policy trying to force them to break the very protocols that make sovereignty possible does not seem like the wisest strategic move.&lt;/p&gt;
    &lt;p&gt;What policymakers are missing is that encryption is a built-in foundation of most communication protocols. You cannot turn it on or off depending on what is considered right in a given place at a given moment. You either have secure end-to-end encryption or you don't. There is no "just this once" exception that doesn't become an exploitable technical or administrative vulnerability.&lt;/p&gt;
    &lt;p&gt;When Denmark's Justice Minister suggested that the "completely misguided perception" is that everyone has a right to secure communication, he revealed the fundamental gap: policymakers who don't understand that secure infrastructure is the core of today's Internet backbone, not just for the pure sake of democracy (I swear it hurts to have to explain this), but also for the existential security of European countries.&lt;/p&gt;
    &lt;p&gt;Today, European countries are prioritizing defense spending while missing that digital infrastructure is the battlefield. Networks allow us to control drones, spread misinformation, they are vectors of attacks on critical infrastructure.&lt;/p&gt;
    &lt;p&gt;It is time for Europe to develop a coherent tech strategy. Can we build digital sovereignty while simultaneously undermining the protocols that enable it? Can we demand independence from US tech giants while forcing European alternatives to adopt vulnerabilities that US companies will try to avoid through commercial pressure?&lt;/p&gt;
    &lt;p&gt;The October postponement is an opportunity. Two months for actual infrastructure builders and engineers to inform policy. Two months to bridge the gap between Brussels' political vision and the technical reality of how secure systems actually work.&lt;/p&gt;
    &lt;p&gt;This is exactly the gap I work to bridge: between policymakers who understand the geopolitical stakes and engineers who understand protocol layers. Europe's path to digital sovereignty requires both.&lt;/p&gt;
    &lt;p&gt;Denmark's December push will show us whether Europe is serious about learning from its own technical community, or whether we're condemned to keep making policy that contradicts our stated goals.&lt;/p&gt;
    &lt;p&gt;The European way should be: tech with purpose, built on sound engineering, serving democratic values. Not tech policy that undermines the very infrastructure we need to achieve independence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45589327</guid><pubDate>Wed, 15 Oct 2025 07:52:40 +0000</pubDate></item><item><title>Britain has wasted £1,112,293,718 switching off wind turbines in 2025</title><link>https://wastedwind.energy/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45590236</guid><pubDate>Wed, 15 Oct 2025 10:10:20 +0000</pubDate></item></channel></rss>