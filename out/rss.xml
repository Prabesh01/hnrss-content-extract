<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 29 Jan 2026 10:02:37 +0000</lastBuildDate><item><title>Kyber (YC W23) Is Hiring a Staff Engineer</title><link>https://www.ycombinator.com/companies/kyber/jobs/GPJkv5v-staff-engineer-tech-lead</link><description>&lt;doc fingerprint="f15442ba0e572026"&gt;
  &lt;main&gt;
    &lt;p&gt;Instantly draft, review, and send complex regulatory notices.&lt;/p&gt;
    &lt;p&gt;At Kyber, we're building the next-generation document platform for enterprises. Today, our AI-native solution transforms regulatory document workflows, enabling insurance claims organizations to consolidate 80% of their templates, spend 65% less time drafting, and compress overall communication cycle times by 5x. Our vision is for every enterprise to seamlessly leverage AI templates to generate every document.&lt;/p&gt;
    &lt;p&gt;Over the past 18 months, we’ve:&lt;/p&gt;
    &lt;p&gt;Kyber is backed by top Silicon Valley VCs, including Y Combinator and Fellows Fund.&lt;/p&gt;
    &lt;p&gt;We're seeking a Staff Engineer with a clear line of sight to CTO. This role is ideal for someone who is already operating as a 10x engineer, thrives in early stage environments, and is excited to design and scale mission-critical AI systems from first principles.&lt;/p&gt;
    &lt;p&gt;Responsibilities:&lt;/p&gt;
    &lt;p&gt;What We’re Looking For in You:&lt;/p&gt;
    &lt;p&gt;Join us in building and scaling a game-changing enterprise product powered by state-of-the-art AI. At Kyber, your contributions will directly impact how businesses handle some of their most critical workflows and customer interactions.&lt;/p&gt;
    &lt;p&gt;If you’re obsessed with building, AI, and transforming enterprise workflows, we’d love to hear from you!&lt;/p&gt;
    &lt;p&gt;We want to hear from extraordinary individuals who are ready to shape the future of enterprise documents. To stand out, ask someone you’ve worked with to send your resume or LinkedIn profile, along with a brief 2-3 sentence endorsement, directly to arvind [at] askkyber.com.&lt;/p&gt;
    &lt;p&gt;Referrals matter. They help us understand the impact you’ve already had and the kind of teammate you’ll be. A strong referee can elevate your application, so choose someone who knows your skills and character well.&lt;/p&gt;
    &lt;p&gt;Apply today and help us bring enterprise documents into the AI-native age.&lt;/p&gt;
    &lt;p&gt;With Kyber, companies operating in regulated industries can quickly draft, review, and send complex regulatory notices. For example, when Branch Insurance's claims team has to settle a claim, instead of spending hours piecing together evidence to draft a complex notice, they can simply upload the details of the claim to Kyber, auto-generate multiple best in-class drafts, easily assign reviewers, collaborate on notices in real-time, and then send the letter to the individual the notice is for. Kyber not only saves these teams time, it also improves overall quality, accountability, and traceability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46794231</guid><pubDate>Wed, 28 Jan 2026 12:00:08 +0000</pubDate></item><item><title>Airfoil (2024)</title><link>https://ciechanow.ski/airfoil/</link><description>&lt;doc fingerprint="b77d1a2b09aeb189"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Airfoil&lt;/head&gt;
    &lt;p&gt;The dream of soaring in the sky like a bird has captivated the human mind for ages. Although many failed, some eventually succeeded in achieving that goal. These days we take air transportation for granted, but the physics of flight can still be puzzling.&lt;/p&gt;
    &lt;p&gt;In this article we’ll investigate what makes airplanes fly by looking at the forces generated by the flow of air around the aircraft’s wings. More specifically, we’ll focus on the cross section of those wings to reveal the shape of an airfoil â you can see it presented in yellow below:&lt;/p&gt;
    &lt;p&gt;We’ll find out how the shape and the orientation of the airfoil helps airplanes remain airborne. We’ll also learn about the behavior and properties of air and other flowing matter. In the demonstration below, you can see a fluid flowing around a gray cube. Using the slider to change just one property of this substance, we can end up with vastly different effects on the liveliness of that flow:&lt;/p&gt;
    &lt;p&gt;Over the course of this blog post we’ll build some intuitions for why these different effects happen to airfoils and other objects placed in flowing air. We’ll start this journey by looking at some of the methods we can use to visualize the motion of the air.&lt;/p&gt;
    &lt;head rend="h1"&gt;Visualizing Flow&lt;/head&gt;
    &lt;p&gt;If you’ve ever been outside in a grassy area on a windy fall day, you may have witnessed something similar to the little scene seen below. The slider lets you control the speed of time to observe in detail how the falling leaves and the bending blades of grass are visibly affected by the wind sweeping through this area:&lt;/p&gt;
    &lt;p&gt;We intuitively understand that it’s the flowing air that pushes the vegetation around, but note that we only observe the effects that the wind has on other objects â we can’t see the motion of the air itself. I could show you a similarly windy scene without the grass and leaves, and I could try to convince you that there is something going on there, but that completely empty demonstration wouldn’t be very gratifying.&lt;/p&gt;
    &lt;p&gt;Since the air’s transparency prevents us from tracking its movement directly, we have to come up with some other ways that can help us see its motion. Thankfully, the little outdoor scene already provides us with some ideas.&lt;/p&gt;
    &lt;p&gt;Notice that as the wind hits a blade of grass, that blade naturally bends in the direction of the blowing gust, and the faster that gust, the stronger the bending. AÂ single blade indicates the direction and speed of the flow of air in that area.&lt;/p&gt;
    &lt;p&gt;In the next demonstration we’re looking at the same grassy field from above. When seen from this perspective, all the blades form short lines that are locally aligned with the wind. The more leaned over a blade of grass is, the longer the line it forms. We can mimic this behavior with a collection of small arrows placed all over the area, as seen on the right side:&lt;/p&gt;
    &lt;p&gt;Each arrow represents the direction and the speed of the flow of air at that location â the longer the arrow, the faster the flow. In these windy conditions the flow varies from place to place and it also changes over time, which we can clearly see in the motion of the arrows.&lt;/p&gt;
    &lt;p&gt;Note that we have some flexibility in how the speed of wind corresponds to the length of an arrow. I adjusted the lengths of the arrows to prevent them from visually overlapping, but I also made sure to maintain their relative lengths â if one arrow is twice as long as the other, then the flow at that location is also twice as fast.&lt;/p&gt;
    &lt;p&gt;For visual clarity I’m also not packing the arrows as densely as the blades of grass are placed, but it’s important to note that every point in the flow has its own velocity which contributes to the complete velocity field present in this area. If we wanted to, we could draw a velocity arrow at any of the seemingly empty spots on the right side.&lt;/p&gt;
    &lt;p&gt;The arrows are convenient, but the grassy scene also has another aid for visualizing flows. Many light objects like leaves, flower petals, dust, or smoke are very easily influenced by the motion of the surrounding air. They quickly change their velocity to match the flow of the wind. We can replicate the behavior of these light objects with little markers that are pushed around by that flow. You can see them on the right side:&lt;/p&gt;
    &lt;p&gt;These little markers also show us the motion of the air. Each marker represents an object so small and light that it instantly picks up the speed of the surrounding airflow. We’d have a hard time seeing these miniscule specks at their actual sizes, so I’m drawing the markers as visible dots.&lt;/p&gt;
    &lt;p&gt;In fact, the motion of each marker is equivalent to the motion of the parcel of air right around it. If you slow down time, you’ll be able to see how each marker just moves in the direction of the arrows underneath it. I also made each marker leave a little ghost trail behind it â this lets us track the path the air, as represented by the marker, took on the way to its current position.&lt;/p&gt;
    &lt;p&gt;Let’s pause for a second to emphasize what the grass-like arrows and leaf-like markers represent âÂ they both show the velocity of the flow of air, but in slightly different ways. An arrow is attached to its fixed point in space, so it represents the current direction and speed of the flow at that location. The whole collection of arrows lets us easily see what the entire flow is doing at the moment.&lt;/p&gt;
    &lt;p&gt;On the other hand, the little markers are actively following the flow, letting us see how the air is actually moving through space, with the ghosty trails giving us some historical overview of where this parcel of air has come from.&lt;/p&gt;
    &lt;p&gt;The two methods we’ve seen so far are very versatile, but sometimes we don’t care about the local direction of the flow, only its speed â in the middle of this grassy field one might get cold from a fast blowing wind regardless of the direction from which that wind is coming. This brings us the third way of visualizing flow:&lt;/p&gt;
    &lt;p&gt;In this method we show the speed of the airflow using colors of varying brightness â the faster the wind, the brighter the color. You can see the whole spectrum of colors in the scale below the plot.&lt;/p&gt;
    &lt;p&gt;This method shows the speed of the flow at all locations giving us a more fine-grained insight into the motion of air at the cost of the directional information. To help with that I’ll sometimes overlay the regular arrows on top to let us know where the flow is going as well.&lt;/p&gt;
    &lt;p&gt;You may have noticed that all these methods present a flat, two dimensional view of the flow. It’s based on the assumption that the wind in our little scene doesn’t change with elevation, and that it also doesn’t blow towards or away from the ground.&lt;/p&gt;
    &lt;p&gt;In reality, the air velocity could vary in all three dimensions, and that air could also flow upwards or downwards. Thankfully, the air flows we’ll consider in this article will be two dimensional and the simple flat drawings will suffice.&lt;/p&gt;
    &lt;p&gt;Before we finish this section, let me bring up visualization of a simple airflow, but this time I’ll give you some control over its direction, which you can change using the second slider. The first one once more controls the speed of time:&lt;/p&gt;
    &lt;p&gt;Don’t be misled by the frozen arrows, the wind is actually blowing there. Remember that the arrows represent the local velocity of the flow of air, so while the velocity doesn’t change, the position of each packet of air does. You can see those changes by tracking the markers moving around with the flow. This demonstration represents a steady flow, which means that its properties don’t change over time.&lt;/p&gt;
    &lt;p&gt;So far we’ve been exploring the notion of airflow’s velocity on a more intuitive level, with a general understanding that’s it’s “the air” moving around in some direction and at some speed. I illustrated that concept using simple arrowsâ, markersÂ â¢, and varying colors, but we’re now ready to investigate the details hiding behind those straightforward graphical representations.&lt;/p&gt;
    &lt;p&gt;To do that, we have to look at individual particles of air. Although I briefly discussed the particle nature of air before, this time around we’re going to take a closer look at the motion of these molecules, and what it means for airflow as a whole.&lt;/p&gt;
    &lt;head rend="h1"&gt;Velocity&lt;/head&gt;
    &lt;p&gt;Let’s take a look at the air particles in a small, marked out volume of space seen in the demonstration below â you can drag the cube around to change the viewing angle. The slider controls the speed of time:&lt;/p&gt;
    &lt;p&gt;You’re witnessing the motion of over twelve thousand air particles. It may seem like a lot, but this cube is extremely tiny, its sides are only 80 nanometers long. To put this in perspective using more familiar sizes, if that cube’s side measured just 1 inch1 centimeter, it would contain around 410 quintillion, or 4.1Ã102025 quintillion, or 2.5Ã1019 particles.&lt;/p&gt;
    &lt;p&gt;The particles are zipping around in random directions, constantly entering and leaving this region. However, despite all this motion what you’re seeing here is a simulation of still air.&lt;/p&gt;
    &lt;p&gt;To understand how all this movement ends up creating still conditions, we first have to look at the velocity of each particle â I’ll visualize it with a small arrow in the direction of motion. To make things a easier to see, I’ll also highlight a few of the particles while fading out the rest of them:&lt;/p&gt;
    &lt;p&gt;The length of an arrow is proportional to the speed of a particle, so when you freeze the time you should be able to see how some particles are slower and some are faster. This speed variation follows a certain distribution that’s related to temperature â the warmer the air, the faster the motion of its particles.&lt;/p&gt;
    &lt;p&gt;At room temperature the average speed of a particle in air is an astonishing 1030Â mph1650Â km/h, which is many times higher than even the most severe hurricanes. Given the size of the cube, this means that even at the fastest speed of simulation everything happens 11 billion times slower than in real life.&lt;/p&gt;
    &lt;p&gt;If you paid close attention, you may have also noticed that sometimes the particles randomly change direction and speed of their motion â this happens when molecules collide. Each particle experiences roughly ten billion collisions per second. We’ll get back to these interactions later on, but for now let’s try to figure out how all this turmoil creates still air.&lt;/p&gt;
    &lt;p&gt;Having just seen the small velocity arrows of individual particles, let’s calculate the average velocity of a group of three particles, using the process shown below. We first take the velocity arrows from each particle and place them head to toe, one after another. Then we connect the start of the first arrow with the end of the last arrow to create the sum of all velocities. Finally, we divide, or scale down, the length of this sum by the number of particles to get the average velocity:&lt;/p&gt;
    &lt;p&gt;In the next demonstration we’re repeating this whole procedure by tallying up all the particles inside the red box. You can change the size of that region with the second slider. The large arrow in the middle shows the average velocity of particles in the box. To make that central arrow visible, I’m making it much larger than the tiny arrows tied to particles:&lt;/p&gt;
    &lt;p&gt;The counter in the bottom part of the demonstration tracks the current number of particles in the red cube. That value fluctuates as the molecules enter and leave that region. While aggregating over a small number of particles creates a very noisy readout, it doesn’t take that many particles to get a much steadier measure.&lt;/p&gt;
    &lt;p&gt;Recall that the scale of the large central arrow is much larger than the scale of individual tiny arrows attached to each particle. Despite that increase in size, the arrow practically disappears when we average out a larger number of particles and we can clearly see that the average velocity of particles is more or less zero even in this extremely small volume.&lt;/p&gt;
    &lt;p&gt;In still conditions, all these motions in different directions average out to nothing. As some particles enter the area from a random direction, the others also leave it in a random way. The bulk of air doesn’t really go anywhere and the particles just meander in a random fashion.&lt;/p&gt;
    &lt;p&gt;An imperfect, but convenient analogy is to imagine a swarm of bees flying in the air. While all the individual insects are actively roaming around at different speeds, the group as a whole may steadily stay in one place.&lt;/p&gt;
    &lt;p&gt;All these experiments form the key to understanding what happens when wind sweeps through an area. In the demonstration below, we’re once again watching a small volume of space, but this time you can control the speed of the blowing wind:&lt;/p&gt;
    &lt;p&gt;Notice the mphkm/h speedometer in the bottom of the demonstration. This is not a mistake âÂ even with hurricane-level wind speeds it’s very hard to see any difference in the motion of the particles. Perhaps you’ve managed to see the tiniest shifts in the small particle arrows as you drag the second slider around with time paused, but it’s difficult to even perceive from which direction the wind is blowing.&lt;/p&gt;
    &lt;p&gt;However, when we use the procedure of averaging the velocity of all the particles, we can reveal the motion of their group in the box of a given size, at a specific speed of the flow:&lt;/p&gt;
    &lt;p&gt;Because the motion of each individual particle is so disordered, we have to look at many of them at once to discern any universal characteristics. And when we do just that, from all the chaos emerges order.&lt;/p&gt;
    &lt;p&gt;It’s important to note that with this approach we’re tracking the velocity of the flow within the same region of space outlined by the red box â the molecules keep entering and leaving this area as the flow moves and the arrow in the middle shows the average velocity of the air’s particles in that area.&lt;/p&gt;
    &lt;p&gt;This is exactly what the grass-like arrows we’ve played with in the previous section represent â each one shows the average velocity of air particles in that local region of space. The big arrow we just saw in the middle of the swarm in the averaging red box is equivalent to each of the arrows seen below:&lt;/p&gt;
    &lt;p&gt;Naturally, the averaging box needs to be large enough to avoid the jitteriness related to aggregation of too few particles, but at any scale that we could care about the noisy readout completely disappears.&lt;/p&gt;
    &lt;p&gt;The average motion of particles is very different than the motion of each individual molecule. Even in very fast flows, many of the molecules move in the opposite direction than what the arrow indicates, but if we tally up all the particle motion, the air as a whole does make forward progress in the direction of velocity.&lt;/p&gt;
    &lt;p&gt;Up to this point, we’ve mostly looked at the flow of air by looking at wind and the way it moves through space, but what we consider a motion of air is relative. Let’s see how, by merely changing the point of view, we can create a motion of air in otherwise windless conditions.&lt;/p&gt;
    &lt;head rend="h1"&gt;Relative Velocity&lt;/head&gt;
    &lt;p&gt;Let’s zoom away from the world of microscopic particles to look at the motion of larger bodies. In the demonstration below, you can see two different views of the same car driving in the left direction. In the top part, the camera stays firmly on the ground, but in the bottom part, the camera tracks the motion of the vehicle. If needed, you can restart the scene with the button in the bottom left corner or tweak the speed of time with the slider:&lt;/p&gt;
    &lt;p&gt;These two views show the exact same scene â we’re just changing what the camera is focusing on. As seen in the top part, from the perspective of the static camera, it’s only the car that has some velocity in the left direction.&lt;/p&gt;
    &lt;p&gt;On the other hand, from the perspective of the camera focused on the vehicle, the car doesn’t move, but everything else does. The poles and road markings all move to the right with a speed equal to that of the car. This shouldn’t come as a surprise from daily experience in any form of transportation â when you’re sitting in a moving vehicle, static things in the surrounding environment seem to move towards and past you.&lt;/p&gt;
    &lt;p&gt;The very same rules apply to any region of air â I’ve outlined some of them with dashed boxes up in the sky. For the observer on the ground that air is still, but from the car’s perspective, that air is moving.&lt;/p&gt;
    &lt;p&gt;With that in mind, let’s see the same scene, but this time I’ll add the familiar small arrows showing the air’s velocity as “seen” by the camera:&lt;/p&gt;
    &lt;p&gt;From the point of view of the car, as seen in the bottom view, the air is moving to the right, as if there was some wind blowing right at the vehicle. You’ve probably felt this many times by sticking your hand out the window â it feels no different than if you were standing still on the ground with the wind hitting your fingers.&lt;/p&gt;
    &lt;p&gt;In fact, there is absolutely no difference between “regular” wind and wind experienced by the car or your hand sticking out the window â both are simply a motion of air relative to some object. This means that we can use our arrows to represent any motion of air, as long as we note what that motion is relative to.&lt;/p&gt;
    &lt;p&gt;You may have also noticed that the moving car affects the motion of air in its vicinity. Let me bring up the previous demonstration one more time:&lt;/p&gt;
    &lt;p&gt;In the top view, we can see how the front of the vehicle pushes the air forward, and how the air “bends” and speeds up around the shape of the car to roughly follow its shape, only to end up circling right behind the machine.&lt;/p&gt;
    &lt;p&gt;The same effects are seen in the bottom view â they’re just experienced differently. For example, the air right in front of the car slows down, while the air on top moves even faster than the rest of the undisturbed, distant air.&lt;/p&gt;
    &lt;p&gt;We’ll soon explore why the air behaves this way when flowing around an object, but for now let’s raise above the ground to see the motion of an airplane flying in the sky. We’ll use the familiar setup of a camera kept steady relative the ground, as seen in the top part, and a camera that follows the airplane, seen in the bottom part:&lt;/p&gt;
    &lt;p&gt;Before we continue, notice that it’s getting a little hard to pay close attention to what happens to the moving objects in the ground-fixed camera view â the bodies quickly leave the field of view of the demonstrations. For the rest of this article I’ll stick to the camera style seen in the bottom part of the demonstration â this will let us directly track the interaction between the object and the air that flows around that object.&lt;/p&gt;
    &lt;p&gt;From the point of view of the airplane, it also experiences a flow of incoming air as seen by the air “boxes” approaching the plane, which is very similar to the car example. What’s completely different from the car example is the fact that the airplane somehow stays suspended in the air, despite gravity pulling it down towards the ground. This means that there must be some other force acting on it to prevent the plane from falling from the sky.&lt;/p&gt;
    &lt;p&gt;Let’s compare these two vehicles by looking at the basic forces affecting their motion, starting with the diagram of forces acting on the car:&lt;/p&gt;
    &lt;p&gt;The down-pulling gravity force is counteracted by the reaction forces from the ground â they act through the car’s tires to prevent the car from sinking. The air drag and other forms of resistance push the car back, but the car’s tires powered by the engine keep propelling the car forward.&lt;/p&gt;
    &lt;p&gt;In my previous article I presented a more elaborate description of the interplay between forces and objects, but to briefly recap here, if forces acting on an object are balanced, then that object will maintain its current velocity.&lt;/p&gt;
    &lt;p&gt;All forces on the car are balanced and the vehicle moves forward with constant speed, and it doesn’t move at all in the up or down direction â the object’s velocity is indeed constant.&lt;/p&gt;
    &lt;p&gt;Let’s draw a similar diagram of forces for the flying plane:&lt;/p&gt;
    &lt;p&gt;We still have the air drag that pushes the vehicle back, and the plane’s propeller powered by the engine keeps pushing it forward. As a result the plane moves forward with constant speed.&lt;/p&gt;
    &lt;p&gt;We also have the down-pulling gravity. This time, however, that gravity is not countered by the reaction forces from the ground, but instead it’s balanced by lift, a force that pushes the plane up. When gravity and lift are equalized, the plane doesn’t move up or down either.&lt;/p&gt;
    &lt;p&gt;Airplanes create most of their lift with wings, which are carefully designed to generate that force. While length, area, and the overall geometry of the wings are very important, in this article we’ll focus on the shape of the cross-section of a wing which I highlighted below in yellow:&lt;/p&gt;
    &lt;p&gt;This is an airfoil, the protagonist of this article. This airfoil has a smooth, rounded front and a sharp trailing edge. Let’s take a closer look at the flow of air around this airfoil using the grass-like arrows that show the velocity of air at that location:&lt;/p&gt;
    &lt;p&gt;These arrows paint an interesting picture, but in the demonstration below I’ve also added the little leaf-like markers that track the motion of air parcels in the flow. IÂ steadily release a whole line of them from the left side, but you can also clicktap anywhere in the flow to drop a marker at that location. You can do this in any demonstration that has a little hand symbol in the bottom right corner:&lt;/p&gt;
    &lt;p&gt;The markers show that the flow splits ahead of the airfoil, then it gently changes direction to glide above and below the shape. Moreover, the markers right in front of the airfoil gradually slow down and lag behind their neighbors. The air somehow senses the presence of the body.&lt;/p&gt;
    &lt;p&gt;It may be hard to see, but the top and bottom sections of this airfoil aren’t symmetric. This asymmetric design is very important, but right now it will needlessly complicate our discussion on how the flow around this shape arises.&lt;/p&gt;
    &lt;p&gt;To simplify things a little, let’s use a less complicated shape of a symmetric airfoil â you can see it in the demonstration below. I overlay the previous asymmetric shape with a dashed outline to show the difference between the two:&lt;/p&gt;
    &lt;p&gt;The motion of air around this airfoil is very similar â the flow changes its direction and speed when it passes around an object. Until now we’ve simply been observing that the flow changes to adapt to the shape of the body, but it’s finally time to understand why it happens. To explain that behavior we need to go back to the world of air particles to discuss the concept of pressure.&lt;/p&gt;
    &lt;head rend="h1"&gt;Pressure&lt;/head&gt;
    &lt;p&gt;As we’ve discussed, even in the seemingly steady conditions the particles of air are zipping around at high speeds colliding with each other at an incredible rate. The surface of any object placed in the air will also experience these bounces.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, you can see air particles bombarding a small box. Every time a collision happens I briefly mark it with a dark spot on the surface of that cube:&lt;/p&gt;
    &lt;p&gt;To understand the implications of these collisions, let’s first take a look at objects with more ordinary sizes. In the demonstration below, tennis balls are hitting a large cardboard box from the left and right side. By dragging the slider you can change the intensity of both streams of balls:&lt;/p&gt;
    &lt;p&gt;When a tennis ball hits the box, the collision imparts some force on it, causing the box to move. However, in this simulation the collisions from all the balls on each side balance each other out, so the box doesn’t make any consistent progress in either direction.&lt;/p&gt;
    &lt;p&gt;In real air, the situation is similar, but at vastly different scales. The mass of each particle constituting air is absolutely miniscule, so the impact of an individual collision on any object of meaningful size is completely imperceptible.&lt;/p&gt;
    &lt;p&gt;Moreover, each air particle hitting an object has a different speed, and it strikes the surface of that object at a different angle â some hit the object straight on, but some barely graze it. Due to the enormous number of these collisions happening at every instant of time, all these variations average out, and even a small section of surface of any body experiences uniform bombardment.&lt;/p&gt;
    &lt;p&gt;In aggregate, we say that the air exerts pressure on any object present in that air. The magnitude of this pressure depends on the intensity of these collisions across an area.&lt;/p&gt;
    &lt;p&gt;Let’s see how this pressure manifests on our tiny cube. In the demonstration below, you can use the second slider to control the number of air molecules present in this volume:&lt;/p&gt;
    &lt;p&gt;The black arrows you see on the sides of the cube symbolize the magnitude of pressure on these walls. As we uniformly increase the number of particles in this volume, the intensity of collisions, and thus the pressure, also increases. Because the collisions happen at more or less the same rate on every side of the box, the net balance of forces is also maintained and the cube doesn’t move, regardless of how big or small the overall pressure is.&lt;/p&gt;
    &lt;p&gt;This is exactly what happens in the Earth’s atmosphere â everything is constantly squeezed by relatively high pressure caused by the barrage of countless air particles. That pressure is typically balanced either by an object’s material, which resists compression like a spring, or by the air itself that fills the insides of the object. When that inner air is removed, the seemingly innocuous atmospheric pressure reveals its might.&lt;/p&gt;
    &lt;p&gt;The underlying particle nature also shows us that pressure is never negative. Without any particle collisions, we reach the lowest possible pressure of zero. Beyond that, any impacts on the surface of an object create some amount of positive pressure.&lt;/p&gt;
    &lt;p&gt;In the demonstrations we’ve seen so far, the balanced number of collisions on each wall was very important for keeping the objects steady. Unsurprisingly, more interesting things happen when this harmony isn’t maintained. Let’s first investigate this scenario using the tennis balls. In the demonstration below, the slider controls if it’s the left side or the right side that’s shooting more balls:&lt;/p&gt;
    &lt;p&gt;As you can see, if one of the sides has a higher number of collisions, the forces acting on the box are no longer balanced and the box starts to move.&lt;/p&gt;
    &lt;p&gt;The very same situation happens in air, which you can witness in the simulation below. Notice that the volume in which the tiny cube exists has more particles on one side than the other. Observe what happens to cube once you let the time run using the slider:&lt;/p&gt;
    &lt;p&gt;The higher number of particle collisions on one side of the cube creates higher pressure forces on that wall. The uneven forces end up pushing the block to the side. In this demonstration, the pressure re-balances after a while and the cube stops moving.&lt;/p&gt;
    &lt;p&gt;Intuitively, the air exerts an imbalanced net force on the cube only when different parts of that object experience different pressure â it’s the spatial variation in pressure that creates an acting net force. When the difference in pressure between any two points increases, the net force acting on the object also grows.&lt;/p&gt;
    &lt;p&gt;It’s easy to see that a larger number of collisions on the left side of an object would start to exert a net force pushing that object to the right, but, perhaps surprisingly, the same rules apply to any chunk of air itself.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, I once again made one half of the test volume contain more particles than the other half. As you unpause the demonstration, observe the average velocity of molecules in the marked out section of air:&lt;/p&gt;
    &lt;p&gt;The particles on the more occupied side can easily travel to the less crowded side, because there are fewer particles there to collide with and bounce back from. Additionally, each particle in the less populated section is more likely to hit a particle in the more populated section, which will typically cause that particle from the desolate side to bounce back where it came from.&lt;/p&gt;
    &lt;p&gt;The particles end up, on average, traveling from the area of high pressure to the area of lower pressure. Even though we don’t have any clean borders between different sections, we can still see the bulk of particles getting accelerated towards the less dense section.&lt;/p&gt;
    &lt;p&gt;Once again, the initial pressure differences in the test volume dissipate after a while. On their own, these freely suspended pressure variations quickly disappear, but we will soon see how, with the aid of airflow, these areas of different pressure can be sustained indefinitely.&lt;/p&gt;
    &lt;p&gt;In the examples we’ve been playing with, the notion of increased pressure came from an increased number of collisions, which in turn came from an increased number of particles in the area. This shows that, all other things being equal, pressure is tied to the local density of the air, which was very easy to perceive in an increased concentration of molecules.&lt;/p&gt;
    &lt;p&gt;However, the pressure can also grow due to increased average speed of the particles, which in turn comes with increased temperature. As particles get faster, each collision gets more impactful and it pushes on an object or other particles a bit harder, causing the overall pressure to also increase. In the demonstration below, we can simulate this with tennis balls hitting the cardboard box at the same rate, but with different speeds, which you can control with the slider:&lt;/p&gt;
    &lt;p&gt;As we make the balls on one side of the box faster, their impacts also become stronger and the package starts moving to the right, even though the number of collisions per second is equal on both sides.&lt;/p&gt;
    &lt;p&gt;The important point from these discussions is that air pressure exerts force on everything inside it, be it a solid object or any parcel of air. It’s a little unintuitive that the air itself both exerts the pressure and it also “feels” the pressure, but it’s all just a consequence of very rapid motions of particles and the collisions between them happening at an enormous rate.&lt;/p&gt;
    &lt;p&gt;Recall that even in small volumes of air there are billions of billions of particles, and each particle experiences roughly ten billion collisions per second. What we’ve simulated at a micro scale and in slow motion as countable, individual interactions, very quickly smooths out into a uniform and uninterrupted notion of force-exerting pressure.&lt;/p&gt;
    &lt;p&gt;This fact lets us abandon the molecules and their collisions yet again. It’s not a big loss, since counting the number and intensity of collisions was never convenient in the first place, but we can now investigate some other ways of visualizing pressure in a region of air.&lt;/p&gt;
    &lt;head rend="h1"&gt;Visualizing Pressure&lt;/head&gt;
    &lt;p&gt;As we’ve seen in the particle simulations, pressure can vary from place to place. One of the most convenient ways to express this variation is to use colors of different intensities. Let’s see how that simple approach could work here. In the demonstration below, the dashed circles represent regions of high and low pressure â you can drag them around to change their position:&lt;/p&gt;
    &lt;p&gt;This map of pressure is colored with varying shades of red as indicated by the scale below â the redder the color, the higher the pressure. The small triangle â¼ in the middle of the scale indicates the location of the base, static pressure present in the atmosphere.&lt;/p&gt;
    &lt;p&gt;In this simulation we have complete control over where the different locations of lower and higher pressure are. To make things more interesting, each draggable pressure circle has a different strength and range. You can infer this variation from color changes around these points.&lt;/p&gt;
    &lt;p&gt;Let’s put an airfoil in this area to see how it’s affected by the pressure of the surrounding air. The arrows seen below symbolize the force that pressure exerts on the surface of the airfoil at that location. They’re the exact same arrows that we’ve seen acting on the walls of the tiny yellow cube, here we just see them at a larger scale:&lt;/p&gt;
    &lt;p&gt;As you move around the locations of lower and higher pressure, the forces acting on the surface of the airfoil also change, matching what we’ve seen with little cubes bombarded by air particles. The static pressure always exerts some base load, but in the areas of higher pressure the surface forces are higher, and in the areas of lower pressure the surface forces are lower than these base forces.&lt;/p&gt;
    &lt;p&gt;Note that you can also move the pressure circles into the airfoil, but it only serves as a convenience to let you customize the shape of the air pressure field around that body â we don’t particularly care about the pressure inside the solid itself.&lt;/p&gt;
    &lt;p&gt;When we tally up all the pressure forces acting on each piece of the airfoil’s surface, we end up with the net force acting on that object. In the demonstration below, I’m showing it with the big arrow at the center of the airfoil:&lt;/p&gt;
    &lt;p&gt;By changing the distribution of pressure around the airfoil, we can affect the total force that this object feels.&lt;/p&gt;
    &lt;p&gt;The reddish plots we’ve been looking at are correct, but a little inconvenient. Recall that final net force on the object depends only on the differences of pressure â when we uniformly increased the number of collisions on the walls of the tiny cube, it steadily remained in place.&lt;/p&gt;
    &lt;p&gt;This means that the static background pressure doesn’t matter for the cumulative forces acting on an object. It’s only the differences relative to that static pressure that affect the overall balance. This lets us overhaul our visual representation of pressure â we can use no color where the pressure has the static value, use blue color when the pressure is lower than the static pressure, and use red color when the pressure is higher than the static pressure:&lt;/p&gt;
    &lt;p&gt;This is the exact same distribution of pressure that we’ve just seen. All the pressure demos in this section are connected, and here we simply changed the reference point against which we present the pressure variation.&lt;/p&gt;
    &lt;p&gt;If we then throw in the airfoil back into the mix we can now also adjust the arrows representing the forces that the pressure exerts on the surface of that object:&lt;/p&gt;
    &lt;p&gt;The areas of higher pressure still seem to push on the surface of the airfoil, but the areas of lower pressure now seem to pull it. However, I need to emphasize once more that pressure always pushes on the object, and we can only talk about a pulling force when we discard that uniform, pushing contribution coming from the static pressure. In those “pulling” areas the pressure is still pushing, it just pushes less intensely.&lt;/p&gt;
    &lt;p&gt;I will also use the convenient terms of positive and negative pressure, but remember that this refers to their difference from the static pressure. The phrase “pressure lower than static pressure” is a mouthful, so the expression “negative pressure” is very handy, even when it hides the fact that pressure is always positive.&lt;/p&gt;
    &lt;p&gt;While the color variations used here show the true nature of the smoothly varying pressure changes, they make it a little hard to see how quickly those changes happen. To fix this, I’ll also draw the contour lines that join the locations of the same pressure â they’re very similar to lines of the same altitude you may have seen on maps:&lt;/p&gt;
    &lt;p&gt;Every point on one of those contour lines has the same value of pressure, and each subsequent line is drawn at the same increment of pressure â you can see this in the scale placed below the plot. This means that the closer the lines are together, the more quickly the pressure changes in that area.&lt;/p&gt;
    &lt;p&gt;The mathematical concept that describes the direction and rapidness of these changes is known as a gradient. Informally, gradient describes how some property changes from one point to another, and, thankfully, this notion tracks closely with how this word is used in graphic design to describe smooth color changes. Wherever you see a color gradient , this also implies that there is a pressure gradient â the pressure changes from place to place.&lt;/p&gt;
    &lt;p&gt;This spatial variation is particularly important for the motion of air. Recall that the air pressure differences don’t just exert forces on solid objects, but also on the air itself â any small parcel of air is subject to the same whims of pressure forces.&lt;/p&gt;
    &lt;p&gt;Those spatial variations in pressure end up pushing the air around, changing its velocity. Let’s see this in action using the little leaf-like markers that are moved around by pressure differences. In the demonstration below, I’m steadily releasing the markers from the left side âÂ notice how their trajectory changes when you modify the pressure field:&lt;/p&gt;
    &lt;p&gt;You may still find it a little difficult to grasp how pressure differences affect the motion of a parcel of air. Luckily, we can draw parallels between the contour lines of pressure seen on these pressure maps and the contour lines of elevation seen on traditional maps. This lets us build a little pressure-landscape analogy.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, the very same distribution of pressure is expressed as a mountainy landscape. Positive pressure lifts the ground above the base level and negative pressure depresses it below the base level. A parcel of air moves like a marble that loses speed when climbing uphill and accelerates when rolling downhill. You can drag the demo around to change the viewing angle:&lt;/p&gt;
    &lt;p&gt;Notice that when the pressure changes more rapidly and the contour lines are closer, the steepness of the corresponding hill or valley also increases, and so do the forces acting on a parcel of air. If the pressure is increasing by a large amount, it may even make the marker go back. This landscape analogy also shows that the static pressure doesn’t matter for the motion of air parcels, as any changes in static pressure would just lift all the areas by the same amount without changing their steepness.&lt;/p&gt;
    &lt;p&gt;When watching these air parcels move around, you may have noticed that things were a little bit off. For example, it’s possible for air parcels coming from different directions to arrive at the same location, and then continue to travel in different directions. You can see an example of that on the left side of the demonstration below, with the slider letting you scrub back and forth in time:&lt;/p&gt;
    &lt;p&gt;Recall that the markers always follow the local velocity of air, so the motion seen in the left part implies that the air at the location of the meetup of the two markers has two different velocities at the same time, which is not realistic.&lt;/p&gt;
    &lt;p&gt;It’s worth pointing out that the situation seen on right side, where one marker merely intersects the historical path of the other, can be realistic, as long as we’re dealing with an unsteady flow, where the velocity of the air at the crossing location has changed since the first marker was there. For steady conditions in which no changes occur over time, the scenario seen on the right is also not physically correct.&lt;/p&gt;
    &lt;p&gt;We’ll look at some unsteady flows later in the article, but for now we’re interested in steady conditions so the crossing paths of our markers indicate implausible velocities. Even more dubious result happen when we simulate the motion of these markers with an airfoil present in the flow:&lt;/p&gt;
    &lt;p&gt;For most distributions of pressure, the air markers will flow right through the body. This is clearly wrong! The demonstrations we’ve seen so far correctly represent what would happen to individual air parcels and bodies placed in these pressure fields, but those pressure fields themselves were completely made up and didn’t correspond to any physical reality. Our mistake was that we completely ignored any interactions between the pressure of the air and the motion of that air.&lt;/p&gt;
    &lt;p&gt;The flow of air, the pressure of air, and the shape of the objects placed in that air are all tied together â for a given incoming flow speed and the shape of the object, we can’t just arbitrarily arrange the pressure field like we did in our artificial demonstrations. Instead, that pressure field will arise on its own.&lt;/p&gt;
    &lt;p&gt;Let’s see a real distribution of pressure around this airfoil and witness how it affects the motion of air parcels around it:&lt;/p&gt;
    &lt;p&gt;The behavior of air parcels now matches our intuitive expectations â the markers don’t go through the body, and in these steady conditions they also don’t cross paths.&lt;/p&gt;
    &lt;p&gt;We’re now one step closer to understanding how the flow of air takes its shape to move around an airfoil â it’s the pressure differences that cause the flow to change its direction and speed.&lt;/p&gt;
    &lt;p&gt;The pressure field we’ve just seen clearly works â regions of lower and higher pressure guide the air around the airfoil. However, it’s still unclear how these areas emerged in the first place. Let’s try to follow nature’s path to see how this pressure distribution is created and sustained in a flow.&lt;/p&gt;
    &lt;head rend="h1"&gt;Airfoil Flow&lt;/head&gt;
    &lt;p&gt;Before we start building the correct pressure field from scratch, let’s first establish two guiding principles that the flow around any object has to follow.&lt;/p&gt;
    &lt;p&gt;Firstly, the air can’t penetrate solid walls. A valid pressure field should either completely stop the flow at the surface of the object, or redirect that flow to make it travel in the direction perpendicular to the walls. This means that the markers that we track can never get inside the object.&lt;/p&gt;
    &lt;p&gt;Secondly, we also have the restrictions on the relative motion of the markers. For now we’ll only be interested in steady conditions, which means that the markers can’t cross their paths â we expect the ghostly historical trails to never intersect.&lt;/p&gt;
    &lt;p&gt;Let’s first focus on the pressure field in front of the airfoil. In the demonstration below, I created an artificial pressure field in that frontal region, you can control it using the slider:&lt;/p&gt;
    &lt;p&gt;It should quickly become clear that to prevent the approaching air from getting into the object, the pressure in the frontal region has to be positive, so that it pushes the incoming air away.&lt;/p&gt;
    &lt;p&gt;If that positive pressure in front is too low the air can still erroneously flow through the object. If that pressure is too high, the air parcels arriving at the airfoil will turn back and incorrectly cross paths with the incoming air. When the pressure is just right, the air parcels don’t go through the wall, and, at least in front of the object, they also don’t cross their paths.&lt;/p&gt;
    &lt;p&gt;The faster the incoming flow, the higher the pushing force required to slow down and redirect the incoming air. In the demonstration below, you can also control the speed of that incoming air using the second slider:&lt;/p&gt;
    &lt;p&gt;While for slow flows, only a small amount of positive pressure is enough to stop the incoming air, for fast flows, the pressure in front of the airfoil has to become much higher.&lt;/p&gt;
    &lt;p&gt;The pressure needed to stop air at a given velocity is known as stagnation pressure and it’s proportional to the square of that velocity â twice as high speed requires four times larger pressure. Naturally, when there is no flow, no pressure is required as the air no longer tries to flow through the object.&lt;/p&gt;
    &lt;p&gt;In the previous two demonstrations, we manually adjusted the pressure to get the correct result, but in nature this process happens on its own â it’s the flow itself that creates this region of increased pressure in front of the object.&lt;/p&gt;
    &lt;p&gt;As the incoming parcels of air arrive at the surface of the airfoil, they can’t continue going forward, but air parcels from further up ahead continuously want to keep flowing into this region. This compresses the air close to the object, which causes the pressure in front to increase, which then helps to slow down the incoming flow.&lt;/p&gt;
    &lt;p&gt;This mechanism is self-balancing â if the pressure is too low to push away the incoming air parcels, the air parcels will compact the existing air more, causing an increase in pressure. If the pressure is too high, it will easily push the incoming air away, which relieves the frontal area, causing the pressure to decrease. Any fluctuations quickly settle to an equilibrium that balances the pressure in the entire frontal region.&lt;/p&gt;
    &lt;p&gt;Let’s look at the distribution of the positive frontal pressure once more:&lt;/p&gt;
    &lt;p&gt;Notice that the positive pressure isn’t limited to just the close vicinity of the airfoil, but it spreads out much further ahead to gradually reach the value of the static pressure, far away from the airfoil itself.&lt;/p&gt;
    &lt;p&gt;All in all, we have a large area of increasing pressure that starts far away from the body and ends at its surface. Those pressure differences create a pressure “hill” that not only gradually slows the incoming air down, but it also redirects that air to flow around the object.&lt;/p&gt;
    &lt;p&gt;It seems that with our frontal pressure field we’ve easily completed our goal of preventing the air from flowing through the walls of the body. However, our second guideline of non-crossing marker paths is still not fulfilled â this condition is broken above and below the airfoil.&lt;/p&gt;
    &lt;p&gt;Let’s first try to rectify this manually. In the demonstration below, you can control the pressure in these two regions using the slider:&lt;/p&gt;
    &lt;p&gt;While positive values of pressure in those zones make the problem worse, negative values get us much closer to the expected behavior â in the top and bottom areas the markers no longer veer off into different directions. However, that pressure can’t be too low, otherwise it will pull the markers back into the body.&lt;/p&gt;
    &lt;p&gt;In real flow, these regions of lower pressure arise on their own, but the explanation for this phenomenon is a little less straightforward than what I’ve described for the area of positive pressure in the frontal region. We can get some, albeit a bit hand-wavy, understanding by observing what happens to the air markers when those negative regions are missing.&lt;/p&gt;
    &lt;p&gt;In that scenario, the incoming air parcels no longer reach those areas above and below the airfoil, causing some local depletion of air that has since left those zones. This decreases the pressure in those regions, and that lower pressure attracts the surrounding air to flow into those less occupied spaces.&lt;/p&gt;
    &lt;p&gt;If that lower pressure is too negative, more air will come in and the pressure will rise. If the pressure is not negative enough, those region will get depleted again. Once again, it’s the flow itself that creates the balancing system â without the flow no pressure differences would arise.&lt;/p&gt;
    &lt;p&gt;As we’ll see later on, in more extreme scenarios that negative pressure can alter the flow more dramatically, and the regions of “missing” air get filled through other means, but for now let’s close things up by tweaking the pressure in the rear part of the airfoil:&lt;/p&gt;
    &lt;p&gt;Some amount of positive pressure in the rear prevents the air parcels from smashing into each other after leaving the airfoil. Intuitively, this pressure arises naturally from the flow, because as the air slides off from the ends of the top and bottom sides, it all arrives into the same region, creating some compression.&lt;/p&gt;
    &lt;p&gt;If that compressive pressure in the rear is too low, more air will manage to get in, which will further increase the pressure. If that pressure is too high, it will push the incoming air away, which depletes the area and the pressure decreases. The system balances itself yet again.&lt;/p&gt;
    &lt;p&gt;The quite informal description of these balances that I’ve presented can be formalized mathematically using the NavierâStokes equations. These equations describe the motion of liquids and gasses, collectively known as fluids, subject to various forces like gravity, or, most importantly for us, pressure.&lt;/p&gt;
    &lt;p&gt;NavierâStokes equations are notoriously difficult to solve analytically, but a lot of insight about the behavior of fluids can be gained with computer simulations with various degrees of complexity.&lt;/p&gt;
    &lt;p&gt;In this article, I’m also employing simulations to investigate the flow of air around objects. However, the computer models used here are quite simplified and they don’t reflect the full richness of physics involved in the motion of air. These slow-motion demonstrations are intended to present the broad strokes of the delicate interaction between the air and the airfoil, but I would advise against relying on them when building an airworthy airplane.&lt;/p&gt;
    &lt;p&gt;With all of these caveats in place, let’s get back to the pressure distribution around a symmetric airfoil. We’re done recreating the nature-made pressure field, but there is one small aspect that we haven’t yet accounted for.&lt;/p&gt;
    &lt;p&gt;For our experiments, I kept the pressure steady in time so that we could focus on its general outlines. In practice, a pressure field imposed by a fast flow around any object will experience some degree of instability, which you can see in the demonstration below. You can once more drop the markers at any location to track the flow in the area:&lt;/p&gt;
    &lt;p&gt;As the pressure builds up on one side, it redirects the flow, which changes the pressure again. The pressure ends up oscillating back and forth like a swing. The pressure distribution and the flow direction are once again at the mercy of their mutual balance, one affecting the other. We’ll soon see some other examples of these unstable behaviors.&lt;/p&gt;
    &lt;p&gt;As we’ve just seen, the variation in pressure doesn’t just happen in the close vicinity of the airfoil, but it stretches quite far away from the body itself. This means that the velocity of the flow is also affected quite far away from the shape.&lt;/p&gt;
    &lt;p&gt;However, when it comes to the forces exerted on the airfoil, it’s only the pressure right at the surface of the airfoil that matters. Let’s bring back the two tools we’ve used before: surface arrows that show how the air pushes or “pulls” on the airfoil, and the net force arrow that tallies up the net results of these forces:&lt;/p&gt;
    &lt;p&gt;As the pressure field fluctuates, the resulting net force also moves around. Let’s decompose this force into two different components, one perpendicular to the flow, and one parallel to it:&lt;/p&gt;
    &lt;p&gt;The force acting in the direction perpendicular to the flow is known as lift, and the one acting in the direction of the flow is known as pressure drag, or form drag. As the name implies, this component of drag is created by the distribution of pressure around the shape.&lt;/p&gt;
    &lt;p&gt;For this airfoil, the pressure drag is very tiny. While airfoils are specifically designed to minimize the overall drag, most of that force hindering their motion comes from another source â we’ll discuss it soon enough.&lt;/p&gt;
    &lt;p&gt;Notice that as this flow fluctuates, the lift force jumps around, but averaged over time the upward and downward swings of that force end up balancing each other. This airfoil in this configuration doesn’t generate any continuous lift.&lt;/p&gt;
    &lt;p&gt;This shouldn’t come as a surprise since this situation is completely symmetric, so the pressure forces on the upper and lower sides of the airfoil are, on average, completely balanced. However, there is an easy way to disturb that symmetry. In the demonstration below, we’re once again meeting the plain, symmetric airfoil, but this time we can gently tilt it using the slider:&lt;/p&gt;
    &lt;p&gt;The slider controls the so-called angle of attack, which is spanned between some reference line on the body, like the one joining the front and back, and the direction of the incoming flow. I’m showing this angle right in the middle of the airfoil.&lt;/p&gt;
    &lt;p&gt;As we change the angle of attack, the shape that the airflow “sees” is no longer symmetrical relative to the incoming direction of that flow. The velocity and pressure fields adapt in their mutual push and pull to form a new, asymmetric distribution. Notice that the stagnation point of high pressure has moved around, and the little markers that indicate the motion of air now travel on very different paths below and above and below the airfoil.&lt;/p&gt;
    &lt;p&gt;If we then put the pressure arrows back in, we can tally them all up to get the resulting lift and pressure drag. When compared to the previous simulation, I’m scaling down all the arrows to make them fit in the bounds of the demonstration:&lt;/p&gt;
    &lt;p&gt;When this symmetric airfoil is tilted up, the asymmetric pressure distribution generates a lift force that pushes the object up. Conversely, for a downward tilted airfoil, the pressure forces push the airfoil down.&lt;/p&gt;
    &lt;p&gt;Naturally, we’re typically interested in upward-pointing forces, and when the lift generated by the wings is equal to the weight of the plane, the plane will stay in the air without raising or falling to the ground â we’re finally flying.&lt;/p&gt;
    &lt;p&gt;Let’s plot the dependence between the lifting force and the angle of attack of an airfoil â you can see it in the right side of the demonstration below. Note that this plot presents time-averaged and settled values, so you may have to wait a little for the flow to normalize and the lift to start oscillating around the expected value:&lt;/p&gt;
    &lt;p&gt;Clearly, as the angle of attack increases, so does the generated lift. The same thing happens on the other end of the spectrum, where a more negative angle of attack creates more negative lift. Note that for this symmetric airfoil the positive and negative sides of the diagram are just mirror images of each other, so let’s focus only on positive angles of attack.&lt;/p&gt;
    &lt;p&gt;One could naively hope that we could keep increasing the angle of attack to generate more and more lift. Let’s see what happens in practice:&lt;/p&gt;
    &lt;p&gt;Initially, the lift force indeed keeps increasing with the angle of attack, but at some point it plateaus. Once that critical angle of attack is surpassed, the lift force starts to fall after the flow fully develops.&lt;/p&gt;
    &lt;p&gt;What we’re witnessing here is known as a stall. The onset of a stall imposes limits on how much lift the wings of an airplane can generate from merely increasing the angle of attack.&lt;/p&gt;
    &lt;p&gt;Notice that when the stall happens, the pressure distribution on the upper part of the airfoil becomes very erratic â it’s not only the surface pressure arrows that are changing rapidly, but the whole pressure field in that area is very disturbed.&lt;/p&gt;
    &lt;p&gt;Let’s bring in the velocity arrows and markers to get a better feel on what’s going on in that region:&lt;/p&gt;
    &lt;p&gt;At high angles of attack, the flow above the upper part of the airfoil becomes very complicated. If you clicktap in that region to drop a few markers, you’ll notice that the air is trapped in various swirling eddies that are eventually shed to fly away with rest of the flow.&lt;/p&gt;
    &lt;p&gt;We’re witnessing flow separation, where the main part of the flow detaches from the surface and doesn’t follow its shape anymore. The interactions in the complicated flow right above the airfoil affect the pressure field, which then decreases lift.&lt;/p&gt;
    &lt;p&gt;There is a lot going on there, but to understand how these effects arise we have to talk about a property that affects the flow of every fluid: viscosity.&lt;/p&gt;
    &lt;head rend="h1"&gt;Viscosity&lt;/head&gt;
    &lt;p&gt;You might have heard the term viscosity used to describe “thickness” of different liquids, with a classic example that contrasts the slowness of the flow of honey to the rapidness of the flow of water.&lt;/p&gt;
    &lt;p&gt;Viscosity is also a property of gasses like air, but before I describe this concept more formally, we’ll first build an intuitive understanding of what viscosity is and what it does to the flow of different fluids.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, the fluid flows in from the left side, but note that the flow in the top half is faster than the flow in the bottom half, which is reflected by the different lengths of the arrows. Dragging the slider to the left decreases the viscosity of the fluid, and dragging the slider to the right increases viscosity:&lt;/p&gt;
    &lt;p&gt;While we can see some changes to the arrows as we move the slider around, you probably agree that, for this flow, the arrow-based visualization isn’t very rewarding. Let’s add the color-based visualization of speed distribution in this flow:&lt;/p&gt;
    &lt;p&gt;We can now see how viscosity blends the speed variation between different sections of the fluid. For highly viscous fluids, this mixing behavior spreads very easily and the initially distinct velocities of the two layers average out quite rapidly.&lt;/p&gt;
    &lt;p&gt;At lower viscosity these two layers with different speeds remain quite separated. If you make the viscosity low enough, you may even notice that, after a while, the flow develops some interesting wave-like phenomena â we’ll get back to these soon.&lt;/p&gt;
    &lt;p&gt;All this mixing behavior may remind you of a diffusion process, where some quantity, like temperature or concentration, evens out over time. Let’s see some basic diffusion in action. In the simulation below, I filled half of the bottle with with red-dyed water, while the other half is filled with blue-dyed water. The slider lets you control the speed of time:&lt;/p&gt;
    &lt;p&gt;As time passes, the sharp difference between the two layer blends more and more to eventually completely disappear. Clearly, there is some similarity between the diffusion of differently colored dyes and the averaging of velocity that we’ve seen in the earlier example.&lt;/p&gt;
    &lt;p&gt;In our flow demonstrations, viscosity seemed to have controlled the diffusion of velocity. To define it more precisely, viscosity controls the diffusion of momentum, which is a product of velocity and mass. The simplified fluids we’re looking at have more or less constant density, so each equally-sized parcel of those fluids has the same mass. Therefore, if it makes things easier for you, wherever you see the word momentum you can think of velocity, but in more complex scenarios these differences can matter.&lt;/p&gt;
    &lt;p&gt;Let me bring in the previous flow simulation one more time:&lt;/p&gt;
    &lt;p&gt;You’ve probably noticed that, as the flow moves to the right, the size of this blended region increases. When the regions of fluid with different momentums meet for the first time, they barely have any time to average out, and the blending is minimal. As time passes, these regions of fluid get to average out more, similarly to how two different layers of dyed water mix more over time.&lt;/p&gt;
    &lt;p&gt;However, as time is passing, these parcels also keep moving, and that stronger blending happens further to the right. The downstream regions had more time to mix and average out, so the visible thickness of the blended region on the right side is also larger.&lt;/p&gt;
    &lt;p&gt;With higher viscosity, the size of blended region grows much more quickly, which lets us be more precise about our working definition â viscosity controls the rate of the diffusion of momentum.&lt;/p&gt;
    &lt;p&gt;So far we’ve only observed flows with nicely separated horizontal layers, but viscosity averages momentum between any two regions of fluids. In the demonstration below, you can witness how viscosity affects a swirly motion of fluid in a vortex:&lt;/p&gt;
    &lt;p&gt;Notice that with high viscosity any differences in velocity are very quickly diluted out into nothing, but with low viscosity the revolving motion can survive for quite a while.&lt;/p&gt;
    &lt;p&gt;Viscosity has a damping or smoothing effect that makes it much harder to sustain any large variation in a velocity field. Let’s see how this affects the motion of objects in fluids of various viscosity. In the demonstration below, we’re tracking a velocity field close to a very thin plate put directly in the stream of an incoming fluid of adjustable viscosity:&lt;/p&gt;
    &lt;p&gt;With high viscosity, there is a large region of slow down around the plate that regains its speed fairly quickly behind the object. At lower viscosity that surrounding region is much smaller, but it extends much further behind the plate. For very low viscosity we’re once again seeing some more unusual behavior that we’ll get back to in a minute.&lt;/p&gt;
    &lt;p&gt;From the dark colors we can easily see that right by the surface of the plate the fluid doesn’t move at all â it sticks to that surface. This velocity difference between the halted flow at the wall and the moving outer flow gets smoothed out over time by viscosity, similar to how it blended in the flow between two different layers of fluid.&lt;/p&gt;
    &lt;p&gt;As before, with higher viscosity, the velocity averaging process becomes more rapid, and the blended region becomes more widespread. This averaging effect doesn’t just alter the velocity of fluid, but it also affects the plate. In some sense, the viscosity also wants to make the velocity of the surface of the plate to be more like the velocity of the surrounding flow.&lt;/p&gt;
    &lt;p&gt;The viscosity makes the flow want to pull the plate with it, which creates a shearing force that tries to slide the surface of this object away. The net effect is that that viscosity creates additional drag known as skin friction drag that wants to slow down any object moving in it.&lt;/p&gt;
    &lt;p&gt;All of these effects underline why highly viscous fluids are “thick”. Viscosity not only quickly averages any local differences in velocity, which prevents those fluids from flowing easily, but it also represses motion of objects in those fluids â you’ve likely experienced the difficulty of moving a spoon through a jar of honey.&lt;/p&gt;
    &lt;p&gt;The flow of any fluid exhibits tiny, random disturbances. In fluids with high viscosity, these variations are very quickly dispersed, so their motion is rarely erratic. Fluids with low viscosity aren’t as effective at damping motion, and these disturbances can grow to create oscillatory patterns. We’ve seen glimpses of them in the previous simulations, but here is another example:&lt;/p&gt;
    &lt;p&gt;At lower viscosity the flow becomes quite wave-y. Those instabilities happen at the border of regions of fluid with different velocities, like where the slow wake behind a plate is in contact with the fast external flow. In those regions, any tiny random intrusion of slower flow into the faster flow can get magnified and rolled over like a wave.&lt;/p&gt;
    &lt;p&gt;In our discussion of the motion of air around an airfoil, we’ve seen how the flow, the pressure field, and the shape of the body have effects on each other. These influences can be quite dynamic in nature, with distributions of velocity and pressure swinging back and forth in a never-ending fight for dominance.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, we can see a more dramatic example of these battles, where, depending on the viscosity, the flow around a gray cube can take many different forms:&lt;/p&gt;
    &lt;p&gt;With very high viscosity, the flow is completely stable, but as viscosity decreases, it starts to regularly oscillate from side to side, shedding vortices in the process. At very low viscosity, the motion becomes even more erratic.&lt;/p&gt;
    &lt;p&gt;While I can’t easily simulate it here, with further decrease in viscosity, the flow can develop full featured turbulence in which highly irregular and chaotic mixing motions occur at different scales. Turbulent flow stands in contrast to laminar flow, in which neighboring areas of fluid move in an orderly way past each other without any varying fluctuations.&lt;/p&gt;
    &lt;p&gt;Although we’ve put most of our focus on viscosity, which is often denoted with the Greek letter Î¼, the general behavior of the flow also depends on its velocity u, density Ï, and the size L of the body or container involved in the flow. These parameters are tied together by the Reynolds number Re:&lt;/p&gt;
    &lt;p&gt;Flows with the same Reynolds numbers exhibit similar behavior, which means that if we make the obstacle size L twice as large and we halve the speed of the flow u, the Reynolds number won’t change and neither will the characteristics of the flow â it will exhibit the same smooth or oscillatory motion.&lt;/p&gt;
    &lt;p&gt;The Reynolds number also “predicts” the onset of turbulence. When we increase the speed of the flow u, or decrease the viscosity Î¼, the Reynolds number rises. When it reaches a high enough value, turbulence is likely to occur.&lt;/p&gt;
    &lt;p&gt;Let’s quantify the difference in viscosity between different fluids. The precise values aren’t that important to us, but to briefly be a bit more formal, viscosity is expressed in units of pascal-seconds, or PaÂ·s. To let us use more manageable numbers, the following table uses millipascal-seconds, or mPaÂ·s:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;honey&lt;/cell&gt;
        &lt;cell&gt;~10000 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;olive oil&lt;/cell&gt;
        &lt;cell&gt;~100 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;water&lt;/cell&gt;
        &lt;cell&gt;1.0 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;air&lt;/cell&gt;
        &lt;cell&gt;0.018 mPa·s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These values are measured at 68 Â°F20 Â°C, but many fluids like oil get much less viscous with increased temperature. As expected, honey is significantly more viscous than water. Compared to water, the viscosity of air is around 50 times less still, but even a very low viscosity has effects on flow and its interaction with solid walls.&lt;/p&gt;
    &lt;p&gt;To understand how viscosity arises in gasses like air, we have to once more get back to the world of particles. So far we’ve been watching them from a distance, with individual collisions barely perceptible in the moving swarm. This time we’re going take a closer look at these interactions.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, you can experience a simplified simulation of two molecules colliding in space. Each molecule represents nitrogen or oxygen â these two elements constitute the vast majority of air, and, in normal conditions, each one consists of two atoms.&lt;/p&gt;
    &lt;p&gt;You can drag the orange particle around, and once you let go I’ll automatically aim it so that it hits the blue particle. The speed of the orange molecule is four times larger than the speed of the blue one:&lt;/p&gt;
    &lt;p&gt;Notice that after the collision, it’s the orange molecule that’s slow, and it’s the blue one that’s fast. In this demonstration the two particles have the same mass and they collide straight on, so they simply end up trading velocities.&lt;/p&gt;
    &lt;p&gt;More generally, particles of different masses that strike each other at different angles will exchange some amount of momentum. Recall that the heavier the particle, or the faster it moves, the higher its momentum.&lt;/p&gt;
    &lt;p&gt;Let’s see how this behavior ends up affecting the average velocities of larger quantities of molecules. In the paused demonstration below, air molecules are grouped into two different parts. The air in the blue region has higher velocity than the air in the red region, which you can see in the black arrows showing the average velocity in those regions. Notice what happens to these averages as you let time flow by dragging the slider:&lt;/p&gt;
    &lt;p&gt;At the very beginning, the average velocities in these two sections are visibly different, but they quickly even out when fast particles from the blue region flow into the slower red region, and the slower particles from the red region move into the faster blue region, balancing the initial velocity differences.&lt;/p&gt;
    &lt;p&gt;Moreover, some of the faster particles collide with slower particles in the red region and some of the slower particles collide with faster particles from above. The faster particles lose some of their higher momentum, while the slower particles gain some of the momentum. All of these effects “dilute” some of those average velocity differences between the two regions.&lt;/p&gt;
    &lt;p&gt;You may also remember that when we observed a flow of fluid around a flat plate, that fluid wasn’t moving at all right on the surface of that plate, because it was stuck to it. Let’s see how this behavior may arise on a microscopic scale.&lt;/p&gt;
    &lt;p&gt;In the demonstration below, we’re watching the familiar air particles right next to the surface of an object. To make tracking easier, I’m highlighting some of the particles in the vicinity of this surface:&lt;/p&gt;
    &lt;p&gt;When seen at a very large magnification, this surface, like almost all surfaces, isn’t perfectly smooth and has various peaks and valleys. The particles hitting these irregularities get bounced in more or less random directions. Some of the unlucky molecules can even get stuck for a while in these local crevices.&lt;/p&gt;
    &lt;p&gt;Close to the surface, the random collisions with peaks and valleys prevent the particles from making bulk progress in any direction. The average velocity of the air flow by the wall is more or less zero. Some molecular interactions between the particles and the surface can also prevent the fluid from moving.&lt;/p&gt;
    &lt;p&gt;This sticking behavior is known as the noâslip condition and it holds true for most typical flows of fluids that we experience day to day. It’s only in extreme conditions of very rarified gasses in the upper parts of the atmosphere or flows in microscopic capillaries that can break this assumption.&lt;/p&gt;
    &lt;p&gt;Let’s leave the world of particles behind for the last time and see how these two effects play an important role of influencing the airflow close to the surface of any object.&lt;/p&gt;
    &lt;head rend="h1"&gt;Boundary Layer&lt;/head&gt;
    &lt;p&gt;Let’s take another look at a thin plate placed in the stream of incoming fluid:&lt;/p&gt;
    &lt;p&gt;From this broader perspective, it’s hard to see how the flow interacts with the surface of that plate, because the effects of viscosity are limited to the region close to that surface. Let’s focus our attention on the small area that I’ve outlined with a dashed line, right in the top part of the plate. Here it is zoomed up close:&lt;/p&gt;
    &lt;p&gt;We can once more see that, due to the no-slip condition, the velocity is zero at the wall, and then it grows to meet the velocity of the flow further away from the surface itself. What we’re seeing here is known as the boundary layer, which spans the region between the surface of the object and the “outer” flow, which is mostly unaffected by the presence of the object.&lt;/p&gt;
    &lt;p&gt;Because the velocity in the boundary layer smoothly approaches the speed of the outer flow, it doesn’t have a well-defined end point. One of the choices is to agree that the boundary layer ends where the speed reaches 99% of the speed of the surrounding flow far away from the solid surface. Let me visualize this boundary in the flow using a dashed line:&lt;/p&gt;
    &lt;p&gt;As we move with the flow along the distance of the plate, the viscosity keeps averaging out the velocity differences, making the boundary layer thicker â this is similar to what we’ve seen at larger scales with highly viscous flows around objects.&lt;/p&gt;
    &lt;p&gt;Let’s quantify the distribution of speed in the boundary layer a little more precisely. In the demonstration below, I put the velocity arrows back in. I then connected the ends of these arrows with a thin line to show a profile of velocity at that location along the surface:&lt;/p&gt;
    &lt;p&gt;Notice that, initially, the velocity close to the wall increases almost linearly, but then it smoothly tapers to reach the speed of the external flow. The velocity profile close to the surface has a certain steepness, which I’m showing with the white dotted line. This line determines the amount of skin friction drag at that spot â the closer to the surface, or more horizontal, the line is, the higher the skin drag.&lt;/p&gt;
    &lt;p&gt;As the differences in velocity become less severe, the force with which viscosity wants to drag the surface with the flow also decreases. In the conditions present in the demonstration, the skin friction drag decreases over distance.&lt;/p&gt;
    &lt;p&gt;At this point you hopefully have an intuitive grasp of how viscosity affects the flow close to the surface of the object. From our earlier discussion, you may also remember that pressure differences also affect how the flow behaves, with parcels of air slowing down when climbing the hill of increasing pressure and accelerating on the downhill of the decreasing pressure.&lt;/p&gt;
    &lt;p&gt;In the boundary layer flows we played with, the pressure distribution was more or less constant in the investigated region. Let’s see how the flow changes when we vary that pressure.&lt;/p&gt;
    &lt;p&gt;In the top part of the demonstration below we see the exact same view of velocity we’ve experimented with so far. In the bottom part of the demonstration below you can see the pressure distribution in the boundary layer, which you can change using the slider below.&lt;/p&gt;
    &lt;p&gt;If the pressure decreases in the direction of the flow in the boundary layer, we say that the pressure gradient is favorable. Favorable pressure gradient accelerates the air, and the boundary layer doesn’t grow as quickly, since the slowdown caused by viscosity is opposed by that acceleration.&lt;/p&gt;
    &lt;p&gt;When the pressure increases in the direction of the flow, we say that the pressure gradient is adverse. Adverse pressure gradient pushes against the direction of motion of the air. Far away from the surface, the air has enough momentum that the adverse pressure merely slows the flow down. However, close to the surface, the flow in the boundary layer was slow in the first place, so a pushing adverse pressure gradient may even reverse the direction of the flow.&lt;/p&gt;
    &lt;p&gt;When the flow in the boundary layer gets reversed, we say that the boundary layer separates. This region of reversed flow can form a sort of wedge that can lift the rest of the flow away from the surface.&lt;/p&gt;
    &lt;p&gt;Let’s take a step back from the subtleties of boundary layers to see how what we’ve learned corresponds to behavior of a flow around an airfoil. Let me once more bring up the demonstration that brought us here in the first place:&lt;/p&gt;
    &lt;p&gt;As we move across the surface of the airfoil, the high pressure at the stagnation point up front gradually decreases to reach minimum close to the “peak” of that curved surface. Across this transition the pressure gradient is favorable, and that distribution works in our favor â the boundary layer stays nicely attached to the surface.&lt;/p&gt;
    &lt;p&gt;However, as the air reaches the valley of the lowest pressure, it then has to start climbing back up to reach the slightly positive pressure in the rear of the airfoil. For small values of the angle of attack, the pressure pit from which the air has to climb out is not very deep and the adverse pressure gradient isn’t very strong, so the boundary layer remains attached.&lt;/p&gt;
    &lt;p&gt;As we increase the angle of attack of the airfoil, the pressure on top becomes lower and lower. For even higher angles, the adverse pressure gradient becomes so strong that it eventually reverses the flow in the boundary layer, creating separation. Let’s look at this region up close to see how the arrows of velocity in the separated region point in the other direction:&lt;/p&gt;
    &lt;p&gt;If you clicktap to add markers in the bottom right corner of the simulation you’ll notice that many of them move against the bulk of the flow â the boundary layer and the flow have separated.&lt;/p&gt;
    &lt;p&gt;We’ll get back to looking at airfoils soon enough, but we still have a few things to wrap up in the world of boundary layers.&lt;/p&gt;
    &lt;p&gt;The boundary layers we’ve looked at so far were laminar â the layers of fluid with different velocities flowed in an orderly way on top of each other. However, at higher flow speeds and over larger distances, or at high Reynolds numbers, the flow in the boundary layer transitions to a turbulent flow:&lt;/p&gt;
    &lt;p&gt;Be aware that what you’re seeing here is a very simplified simulation of a turbulent boundary layer. Turbulence is inherently three dimensional and it contains various evolving structures of different sizes that are extremely computationally expensive to evaluate in detail. Thankfully, you can find many videos of computer simulations and real flows showing turbulent boundary layers.&lt;/p&gt;
    &lt;p&gt;While the laminar boundary layers we’ve seen in the past exhibited very organized flows, the turbulent one is very chaotic, with large and small swirls causing the flow to mix very rapidly. The transition from laminar to turbulent boundary layer happens spontaneously, but for a given flow speed, the location of the transition depends on surface roughness, steadiness of the flow outside of the boundary layer, and presence of pressure gradients.&lt;/p&gt;
    &lt;p&gt;At any given moment, the velocity profile in the turbulent boundary layer is very unsteady, but it can be averaged over time to get the mean distribution of speed. Let’s compare the time-averaged profiles of the laminar and turbulent boundary layers:&lt;/p&gt;
    &lt;p&gt;In the dynamic simulation of the turbulent boundary layer, we saw how the slower flow close to the surface rapidly mixed with the upper regions of the flow. This slows down those faster sections, and we need to go farther away from the surface for these sluggish intrusions to stop affecting the flow. For this reason, the turbulent boundary layer is thicker and grows faster than a laminar boundary layer.&lt;/p&gt;
    &lt;p&gt;On the other hand, the strong turbulent mixing causes the fast external flow to get close to the body, so the overall velocity profile by the surface increases much more quickly in the turbulent case as opposed to laminar case â I’m showing that with white dotted lines.&lt;/p&gt;
    &lt;p&gt;Recall that the more horizontal the velocity profile at the surface of the object, the bigger the skin friction drag â a turbulent boundary layer has higher skin friction drag than a laminar layer. Despite the cost of increased friction drag, a turbulent boundary layer is often beneficial.&lt;/p&gt;
    &lt;p&gt;Because of that higher velocity closer to the surface, a turbulent boundary layer is more resistant to adverse pressure gradients and it can stay attached to the surface of an object for longer distances.&lt;/p&gt;
    &lt;p&gt;For some objects like golf balls, which purposefully make their boundary layer turbulent by roughing up the surface with little dimples, the delayed separation also decreases the pressure drag caused by uneven pressure distribution. That reduction more than compensates for the increased skin friction drag, making the dimply golf balls fly farther than equivalent smooth balls.&lt;/p&gt;
    &lt;p&gt;For airfoils, a turbulent boundary layer delays separation of the flow, which can help prevent stall at higher angles of attack, but at normal cruising conditions the increased skin friction becomes an important drawback. For many aerodynamic shapes in typical conditions, the skin friction drag is the primary contributor to the total drag that these objects experience.&lt;/p&gt;
    &lt;p&gt;As we’ve seen, by increasing the angle of attack on an airfoil, the lift force grows up to a certain limit, at which the boundary layer separates over most of the upper surface. By staying under this limit, a symmetric airfoil can safely generate lift force.&lt;/p&gt;
    &lt;p&gt;However, when it comes to angle of attack and lift, the shape of an airfoil isn’t particularly unique in its lift-creation capabilities. Most simple elongated shapes generate lift when put in a flow at an angle of attack. In the demonstration below, you can tilt a flat plate and see the forces exerted by the pressure field around it:&lt;/p&gt;
    &lt;p&gt;You may be surprised to see that, at small angles of attack, this flat plate also generates lift. An airfoil-like shape is not a requirement for lift generation. After all, paper airplanes with their flat wings can fly just fine. Lift is just an outcome of the pressure distribution created and sustained by the flow.&lt;/p&gt;
    &lt;p&gt;Although it doesn’t take a sophisticated shape to generate lift at an angle of attack, a well-designed airfoil can often create more lift and with lower drag. In the last section of this article, we’ll explore how other variations to the shape of an airfoil can affect its characteristics.&lt;/p&gt;
    &lt;head rend="h1"&gt;Airfoil Shapes&lt;/head&gt;
    &lt;p&gt;Let’s go back to the simple symmetric airfoil we’ve been playing with thus far. This time, however, we’re able to control its thickness using the slider:&lt;/p&gt;
    &lt;p&gt;Notice that as we increase the thickness of the airfoil, the pressure on the top and bottom sections of the shape becomes more negative. For this symmetric airfoil at 0Â° angle of attack the thickness doesn’t change much other than increasing the pressure drag.&lt;/p&gt;
    &lt;p&gt;However, if we break the symmetry of the shape, we can use thickness-dependence to make one side of the airfoil have a higher negative pressure than the other. In the demonstration below, you can control the “thickness” of the upper surface of the airfoil using the slider:&lt;/p&gt;
    &lt;p&gt;Notice that an asymmetric shape creates an asymmetric pressure distribution, which ends up creating lift without any changes to angle of attack. With some slight tweaking of this shape we finally recreated the asymmetric shape we first saw on the airplane in the early sections of this article.&lt;/p&gt;
    &lt;p&gt;Naturally, when combined with an increasing angle of attack, this airfoil will generate even more lift until it eventually reaches stalling conditions:&lt;/p&gt;
    &lt;p&gt;While symmetric airfoils are sometimes used in acrobatic airplanes, which often find themselves flying upside down, most typical planes use an asymmetric airfoil shape.&lt;/p&gt;
    &lt;p&gt;The underlying mechanism of lift generation by changing the angle of attack or by shaping the object differently is ultimately the same â we’re changing the placement and orientation of the surface of the body relative to the incoming flow. The flow reacts by changing the velocity and pressure distribution, and the resulting pressure field creates the forces on that object.&lt;/p&gt;
    &lt;p&gt;This all means that we have a lot of flexibility in how an airfoil is shaped, as long as the resulting pressure distribution fulfills the design goals of achieving a certain amount of lift while minimizing drag.&lt;/p&gt;
    &lt;p&gt;For example, in some applications it’s important to minimize the skin friction drag caused by a turbulent boundary layer. Some laminar flow airfoils achieve this by shaping the airfoil to move the “pit” of negative pressure further to the back of the airfoil:&lt;/p&gt;
    &lt;p&gt;The favorable pressure gradient between the front and the lowest pressure point extends over a longer distance across the surface of this airfoil, which, at least in principle, helps to keep the boundary layer laminar to keep the skin friction low.&lt;/p&gt;
    &lt;p&gt;Notice that even this unusual airfoil had a rounded front and a sharp back. The roundness of the front helps the air smoothly flow around this area at different angles of attack, and the sharp back reduces the pressure drag by avoiding the separation of the flow.&lt;/p&gt;
    &lt;p&gt;The velocity of the flow around the airfoil is also a contributing factor to the design of the shape. Let’s look at the speed distribution in the flow around a simple asymmetric airfoil using the varying colors and markers:&lt;/p&gt;
    &lt;p&gt;The flow above the airfoil is faster than the incoming flow as indicated by brighter colors. The markers that start in the same line don’t end up sliding off the airfoil in the same formation â the ones on top are further ahead. This is particularly visible for larger values of the angles of attack.&lt;/p&gt;
    &lt;p&gt;This acceleration in the upper part becomes another point of consideration for airfoil design. While commercial airliners don’t fly faster than the speed of sound, the accelerated flow in the top part of an airfoil can break that barrier. This creates a shockwave that can sometimes be seen in flight. Modern airliners use supercritical airfoils that are designed to reduce these drag-causing shockwaves by carefully controlling the speed of the flow around the wing.&lt;/p&gt;
    &lt;p&gt;Planes designed to fly above the speed of sound use supersonic airfoils that are quite different from the shapes we’ve seen. These airfoils have a thin profile and their front edge is sharp and not rounded. Supersonic flows of air are more complicated than what we’ve explored in this article, as variations in density and temperature become an important component of the behavior of the flow.&lt;/p&gt;
    &lt;p&gt;Many of the airfoils used today are designed specifically for the plane they’ll be used in. Moreover, that cross-sectional shape may change across the length of the wing. Real airplanes are three dimensional and the overall shape of the wings also significantly affects the lift and drag of an airplane, but ultimately all the resulting forces are an outcome of interactions between the flow and the body.&lt;/p&gt;
    &lt;head rend="h1"&gt;Further Reading and Watching&lt;/head&gt;
    &lt;p&gt;John Anderson’s Fundamentals of Aerodynamics is a very well-written textbook on aerodynamics. Over the course of over a thousand pages, the author presents a classic exposition of the motion of fluids and their interactions with bodies put in those flows.&lt;/p&gt;
    &lt;p&gt;Understanding Aerodynamics by Doug McLean is a great textbook that takes a different approach of explaining aerodynamic phenomena using physical reasoning. For me, the crowning achievement of the publication is showing that many popular explanations of the origins of lift are either incorrect or they’re based on merely mathematically convenient theorems. The author’s video lecture gives an overview of some of these misconceptions.&lt;/p&gt;
    &lt;p&gt;In this article, I’m using computational fluid dynamics to simulate the flow of air around different objects. For an approachable introduction to these methods I enjoyed Tony Saad’s series of lectures on the topic. For an alternative, and slightly more rigorous approach, Lorena Barba created 12 steps to Navier-Stokes. That website is also accompanied by video lectures.&lt;/p&gt;
    &lt;p&gt;Finally, YouTuber braintruffle created a series of beautiful videos that start with the behavior of fluids on a quantum scale and build up increasingly abstract models that can be used in more practical applications. The videos are packed with interesting takes on fluid mechanics, and they’re worth watching for their visuals alone.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;If you were to sit on a flying airplane and look out the window to glance at its wings, you’d often have a hard time seeing anything going on. However, in that crisp clearness of air whose invisible flow sustains the varied pressure field, lies the hidden source of lift that overcomes the might of gravity to keep the plane safely above the ground.&lt;/p&gt;
    &lt;p&gt;Since the first human flight, we’ve now mastered the art of soaring in the skies by bending the flow of air to our will, using physical quantities like pressure and velocity to help shape our designs. These tangible concepts are ultimately just a manifestation of motions and collisions of billions of inanimate air particles that somehow conspire to assemble the forces we need.&lt;/p&gt;
    &lt;p&gt;I hope this deeper, technical exploration of airfoils hasn’t diminished your appreciation of the greatness of flight. Perhaps paradoxically, by seeing how all the pieces fit together, you’ll find the whole thing even more magical.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46795908</guid><pubDate>Wed, 28 Jan 2026 14:32:30 +0000</pubDate></item><item><title>Oban, the job processing framework from Elixir, has come to Python</title><link>https://www.dimamik.com/posts/oban_py/</link><description>&lt;doc fingerprint="2e18825df1a004c7"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Setting the Stage&lt;/head&gt;
    &lt;p&gt;I’ve used Oban in Elixir for almost as long as I’ve been writing software in Elixir, and it has always been an essential tool for processing jobs. I always knew Oban was cool, but I never dug deeper. This article is a collection of my notes and observations on how the Python implementation of Oban works and what I’ve learned while exploring its codebase. I’ll also try to compare it with the Elixir version and talk about concurrency in general.&lt;/p&gt;
    &lt;head rend="h2"&gt;Surface Level&lt;/head&gt;
    &lt;p&gt;Oban allows you to insert and process jobs using only your database. You can insert the job to send a confirmation email in the same database transaction where you create the user. If one thing fails, everything is rolled back.&lt;/p&gt;
    &lt;p&gt;Additionally, like most job processing frameworks, Oban has queues with local and global queue limits. But unlike others, it stores your completed jobs and can even keep their results if needed. It has built-in cron scheduling and many more features to control how your jobs are processed.&lt;/p&gt;
    &lt;p&gt;Oban comes in two versions - Open Source Oban-py and commercial Oban-py-pro.&lt;/p&gt;
    &lt;p&gt;OSS Oban has a few limitations, which are automatically lifted in the Pro version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-threaded asyncio execution - concurrent but not truly parallel, so CPU-bound jobs block the event loop.&lt;/item&gt;
      &lt;item&gt;No bulk inserts - each job is inserted individually.&lt;/item&gt;
      &lt;item&gt;No bulk acknowledgements - each job completion is persisted individually.&lt;/item&gt;
      &lt;item&gt;Inaccurate rescues - jobs that are long-running might get rescued even if the producer is still alive. Pro version uses smarter heartbeats to track producer liveness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition, Oban-py-pro comes with a few extra features you’d configure separately, like workflows, relay, unique jobs, and smart concurrency.&lt;/p&gt;
    &lt;p&gt;OSS Oban-py is a great start for your hobby project, or if you’d want to evaluate Oban philosophy itself, but for any bigger scale - I’d go with Oban Pro. The pricing seems very compelling, considering the amount of work put into making the above features work.&lt;/p&gt;
    &lt;p&gt;I obviously can’t walk you through the Pro version features, but let’s start with the basics. How Oban Py works under the hood, from the job insertion until the job execution. Stay tuned.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going Deeper - Job Processing Path&lt;/head&gt;
    &lt;p&gt;Let’s get straight to it. You insert your job:&lt;/p&gt;
    &lt;code&gt;from oban import job

@job(queue="default")
async def send_email(to: str, subject: str, body: str):
    # Simple and clean, but no access to job context
    await smtp.send(to, subject, body)

await send_email.enqueue("[email protected]", "Hello", "World")
&lt;/code&gt;
    &lt;p&gt;After the insertion, the job lands in the &lt;code&gt;oban_jobs&lt;/code&gt; database table with &lt;code&gt;state = 'available'&lt;/code&gt;. Oban fires off a PostgreSQL &lt;code&gt;NOTIFY&lt;/code&gt; on the &lt;code&gt;insert&lt;/code&gt; channel:&lt;/p&gt;
    &lt;code&gt;# oban.py:414-419
# Single inserts go through bulk insert path
result = await self._query.insert_jobs(jobs)
queues = {job.queue for job in result if job.state == "available"}
await self._notifier.notify("insert", [{"queue": queue} for queue in queues])
&lt;/code&gt;
    &lt;p&gt;Every Oban node listening on that channel receives the notification. The Stager on each node gets woken up, but each Stager only cares about queues it’s actually running. Be aware that each node decides which queues it runs, so if the current node runs this queue, the producer is notified:&lt;/p&gt;
    &lt;code&gt;# _stager.py:95-99
async def _on_notification(self, channel: str, payload: dict) -&amp;gt; None:
    queue = payload["queue"]

    if queue in self._producers:
        self._producers[queue].notify()
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;notify()&lt;/code&gt; call sets an &lt;code&gt;asyncio.Event&lt;/code&gt;, breaking the Producer out of its wait loop, so it can dispatch the jobs to the workers:&lt;/p&gt;
    &lt;code&gt;# _producer.py:244-262
async def _loop(self) -&amp;gt; None:
    while True:
        try:
            # &amp;lt;--- This is where the event is received ---&amp;gt;
            await asyncio.wait_for(self._notified.wait(), timeout=1.0)
        except asyncio.TimeoutError:
            continue
        except asyncio.CancelledError:
            break

        # &amp;lt;--- Reset the event so it can be triggered for the next batch ---&amp;gt;
        self._notified.clear()

        try:
            # &amp;lt;--- A little debounce to potentially process multiple jobs at once ---&amp;gt;
            await self._debounce()
            # &amp;lt;--- Dispatch (Produce) the jobs from the database to the workers ---&amp;gt;
            await self._produce()
        except asyncio.CancelledError:
            break
        except Exception:
            logger.exception("Error in producer for queue %s", self._queue)
&lt;/code&gt;
    &lt;p&gt;Before fetching the jobs, the producer persists all pre-existing job completions (acks) to the database to make sure queue limits are respected. Next, it fetches new jobs, transitioning their state to executing at the same time. A slightly more complex version of this SQL is used:&lt;/p&gt;
    &lt;code&gt;-- fetch_jobs.sql (simplified)
WITH locked_jobs AS (
  SELECT priority, scheduled_at, id
  FROM
  oban_jobs
  WHERE state = 'available' AND queue = %(queue)s
  ORDER BY priority ASC, scheduled_at ASC, id ASC
  LIMIT %(demand)s
  FOR UPDATE SKIP LOCKED
)
UPDATE oban_jobs oj
SET
  attempt = oj.attempt + 1,
  attempted_at = timezone('UTC', now()),
  attempted_by = %(attempted_by)s,
  state = 'executing'
FROM locked_jobs
WHERE oj.id = locked_jobs.id
&lt;/code&gt;
    &lt;p&gt;And this is the first really cool part.&lt;/p&gt;
    &lt;p&gt;Segue to FOR UPDATE SKIP LOCKED.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;FOR UPDATE&lt;/code&gt;- Locks the selected rows so no other transaction can modify them until this transaction completes. This prevents two producers from grabbing the same job.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SKIP LOCKED&lt;/code&gt;- If a row is already locked by another transaction, skip it instead of waiting. This is crucial for concurrency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this matters for job queues: Imagine two producer instances (A and B) trying to fetch jobs simultaneously:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Without SKIP LOCKED&lt;/cell&gt;
        &lt;cell role="head"&gt;With SKIP LOCKED&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A locks job #1&lt;/cell&gt;
        &lt;cell&gt;A locks job #1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B waits for job #1 to unlock&lt;/cell&gt;
        &lt;cell&gt;B skips job #1, takes job #2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Slow, sequential processing&lt;/cell&gt;
        &lt;cell&gt;Fast, parallel processing&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Back in Python, we know that the jobs we just fetched should be processed immediately. When we fetched the job, we already transitioned its state and respected the queue demand.&lt;/p&gt;
    &lt;p&gt;Each job gets dispatched as an async task:&lt;/p&gt;
    &lt;code&gt;jobs = await self._get_jobs()
for job in jobs:
    task = self._dispatcher.dispatch(self, job)
    task.add_done_callback(
        lambda _, job_id=job.id: self._on_job_complete(job_id)
    )

    self._running_jobs[job.id] = (job, task)
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;add_done_callback&lt;/code&gt; ensures that independent of success or failure, we can attach a callback to handle job completion.&lt;/p&gt;
    &lt;p&gt;The dispatcher controls how exactly the job is run. For the non-pro Oban version, it just uses &lt;code&gt;asyncio.create_task&lt;/code&gt; to run the job in the event loop:&lt;/p&gt;
    &lt;code&gt;# _producer.py:69-71
class LocalDispatcher:
    def dispatch(self, producer: Producer, job: Job) -&amp;gt; asyncio.Task:
        return asyncio.create_task(producer._execute(job))
&lt;/code&gt;
    &lt;p&gt;For pro version, local asyncio dispatcher is automatically replaced with a pool of processes, so you don’t need to do anything to have true parallelism across multiple cores.&lt;/p&gt;
    &lt;p&gt;After the job is dispatched, the Executor takes over. It resolves your worker class from the string name, runs it, and pattern-matches the result:&lt;/p&gt;
    &lt;code&gt;# _executor.py:73-83
async def _process(self) -&amp;gt; None:
  self.worker = resolve_worker(self.job.worker)()
  self.result = await self.worker.process(self.job)
&lt;/code&gt;
    &lt;code&gt;# _executor.py:95-133
match result:
    case Exception() as error:
        # Retry or discard based on attempt count
    case Cancel(reason=reason):
        # Mark cancelled
    case Snooze(seconds=seconds):
        # Reschedule with decremented attempt
    case _:
        # Completed successfully
&lt;/code&gt;
    &lt;p&gt;And that’s the second cool part! You see how similar it is to Elixir’s pattern matching? I love how it’s implemented!&lt;/p&gt;
    &lt;p&gt;When execution finishes, the result gets queued for acknowledgement:&lt;/p&gt;
    &lt;code&gt;# _producer.py:315
self._pending_acks.append(executor.action)
&lt;/code&gt;
    &lt;p&gt;The completion callback notifies the Producer to wake up again-fetch more jobs, and batch-ack the finished ones in a single query.&lt;/p&gt;
    &lt;p&gt;That’s the hot path: &lt;code&gt;Insert → Notify → Fetch (with locking) → Execute → Ack.&lt;/code&gt; Five hops from your code to completion. What about the background processes? What about errors and retries? What about periodic jobs, cron, and all these other pieces? Stay tuned.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Undercurrents - Background Processes&lt;/head&gt;
    &lt;p&gt;Oban runs several background loops that keep the system healthy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leader Election&lt;/head&gt;
    &lt;p&gt;In a cluster, you don’t want every node pruning jobs or rescuing orphans. Oban elects a single leader:&lt;/p&gt;
    &lt;code&gt;# _leader.py:107-113
async def _election(self) -&amp;gt; None:
    self._is_leader = await self._query.attempt_leadership(
        self._name, self._node, int(self._interval), self._is_leader
    )
&lt;/code&gt;
    &lt;code&gt;-- Cleanup expired leaders first
DELETE FROM
  oban_leaders
WHERE
  expires_at &amp;lt; timezone('UTC', now())
&lt;/code&gt;
    &lt;code&gt;-- If current node is a leader, it re-elects itself
INSERT INTO oban_leaders (name, node, elected_at, expires_at)
VALUES (
  %(name)s,
  %(node)s,
  timezone('UTC', now()),
  timezone('UTC', now()) + interval '%(ttl)s seconds'
)
ON CONFLICT (name) DO UPDATE SET
  -- Only update if we're the same node (i.e. current leader re-electing itself).
  -- Other nodes can't overwrite an active leader's lease.
  expires_at = EXCLUDED.expires_at
WHERE
  oban_leaders.node = EXCLUDED.node
RETURNING node
&lt;/code&gt;
    &lt;code&gt;-- Try to insert as a new leader if no leader exists
INSERT INTO oban_leaders (
  name, node, elected_at, expires_at
) VALUES (
  %(name)s,
  %(node)s,
  timezone('UTC', now()),
  timezone('UTC', now()) + interval '%(ttl)s seconds'
)
ON CONFLICT (name) DO NOTHING
RETURNING node
&lt;/code&gt;
    &lt;p&gt;The leader refreshes twice as often to hold onto the role:&lt;/p&gt;
    &lt;code&gt;# _leader.py:101-105
# Sleep for half interval if leader (to boost their refresh interval and allow them to
# retain leadership), full interval otherwise
sleep_duration = self._interval / 2 if self._is_leader else self._interval
&lt;/code&gt;
    &lt;p&gt;When a node shuts down cleanly, it resigns and notifies the cluster:&lt;/p&gt;
    &lt;code&gt;# _leader.py:83-87
if self._is_leader:
    payload = {"action": "resign", "node": self._node, "name": self._name}

    await self._notifier.notify("leader", payload)
    await self._query.resign_leader(self._name, self._node)
&lt;/code&gt;
    &lt;p&gt;And that’s the third cool part! Leader election is delegated entirely to PostgreSQL. Oban uses &lt;code&gt;INSERT ... ON CONFLICT&lt;/code&gt; with a TTL-based lease - no Raft, no consensus protocol, no external coordination service. If the leader dies, its lease expires and the next node to run the election query takes over. Simple, effective, and zero additional infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lifeline: Rescuing Orphaned Jobs&lt;/head&gt;
    &lt;p&gt;Workers crash. Containers get killed. When that happens, jobs can get stuck executing indefinitely. The Lifeline process (leader-only) rescues them:&lt;/p&gt;
    &lt;code&gt;# _lifeline.py:73-77
async def _rescue(self) -&amp;gt; None:
    if not self._leader.is_leader:
        return

    await use_ext("lifeline.rescue", _rescue, self._query, self._rescue_after)
&lt;/code&gt;
    &lt;p&gt;Oban-py rescue mechanics are purely time-based - any job in &lt;code&gt;executing&lt;/code&gt; state longer than &lt;code&gt;rescue_after&lt;/code&gt; (default: 5 minutes) gets moved back. Unlike the Oban Pro version, it doesn’t check whether the producer that owns the job is still alive. This means legitimately long-running jobs could be rescued and executed a second time.&lt;/p&gt;
    &lt;p&gt;The takeaway is that you should set &lt;code&gt;rescue_after&lt;/code&gt; higher than your longest expected job duration, and design workers to be idempotent.&lt;/p&gt;
    &lt;p&gt;The SQL itself is straightforward - jobs stuck executing get moved back to available or discarded if they’ve exhausted retries:&lt;/p&gt;
    &lt;code&gt;-- rescue_jobs.sql (simplified)
UPDATE oban_jobs
SET
  state = CASE
    WHEN attempt &amp;gt;= max_attempts THEN 'discarded'
    ELSE 'available'
  END,
  meta = CASE
    WHEN attempt &amp;gt;= max_attempts THEN meta
    ELSE meta || jsonb_build_object('rescued', coalesce((meta-&amp;gt;&amp;gt;'rescued')::int, 0) + 1)
  END
WHERE
  state = 'executing'
  AND attempted_at &amp;lt; timezone('UTC', now()) - make_interval(secs =&amp;gt; %(rescue_after)s)
&lt;/code&gt;
    &lt;p&gt;The rescued counter in meta lets you track how often jobs needed saving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pruner: Cleaning Up Old Jobs&lt;/head&gt;
    &lt;p&gt;Without pruning, your oban_jobs table grows forever. The Pruner (also leader-only) deletes terminal jobs older than max_age (default: 1 day):&lt;/p&gt;
    &lt;code&gt;-- prune_jobs.sql
WITH jobs_to_delete AS (
SELECT id FROM oban_jobs
WHERE
(state = 'completed' AND completed_at &amp;lt;= timezone('UTC', now()) - make_interval(secs =&amp;gt; %(max_age)s)) OR
(state = 'cancelled' AND cancelled_at &amp;lt;= timezone('UTC', now()) - make_interval(secs =&amp;gt; %(max_age)s)) OR
(state = 'discarded' AND discarded_at &amp;lt;= timezone('UTC', now()) - make_interval(secs =&amp;gt; %(max_age)s))
ORDER BY id ASC
LIMIT %(limit)s
)
DELETE FROM oban_jobs WHERE id IN (SELECT id FROM jobs_to_delete)
&lt;/code&gt;
    &lt;p&gt;The LIMIT prevents long-running deletes from blocking other operations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Retry &amp;amp; Backoff Mechanics&lt;/head&gt;
    &lt;p&gt;When a job raises an exception, the Executor decides its fate:&lt;/p&gt;
    &lt;code&gt;# _executor.py:96-109
match result:
    case Exception() as error:
        if self.job.attempt &amp;gt;= self.job.max_attempts:
            self.action = AckAction(
                job=self.job,
                state="discarded",
                error=self._format_error(error),
            )
        else:
            self.action = AckAction(
                job=self.job,
                state="retryable",
                error=self._format_error(error),
                schedule_in=self._retry_backoff(),
            )
&lt;/code&gt;
    &lt;p&gt;Simple rule: under &lt;code&gt;max_attempts&lt;/code&gt; - retry, otherwise - discard.&lt;/p&gt;
    &lt;p&gt;The default backoff uses jittery-clamped exponential growth with randomness to prevent thundering herds:&lt;/p&gt;
    &lt;code&gt;# _backoff.py:66-87
def jittery_clamped(attempt: int, max_attempts: int, *, clamped_max: int = 20) -&amp;gt; int:
    if max_attempts &amp;lt;= clamped_max:
        clamped_attempt = attempt
    else:
        clamped_attempt = round(attempt / max_attempts * clamped_max)

    time = exponential(clamped_attempt, mult=1, max_pow=100, min_pad=15)

    return jitter(time, mode="inc")
&lt;/code&gt;
    &lt;p&gt;And that’s the fourth cool thing! Backoff includes jitter to prevent thundering herds - without it, all failed jobs from the same batch would retry at the exact same moment, spiking load all over again.&lt;/p&gt;
    &lt;p&gt;The formula: 15 + 2^attempt seconds, with up to 10% added jitter. Attempt 1 waits ~17s. Attempt 5 waits ~47s. Attempt 10 waits ~1039s (~17 minutes).&lt;/p&gt;
    &lt;p&gt;The clamping handles jobs with high &lt;code&gt;max_attempts&lt;/code&gt; - if you set &lt;code&gt;max_attempts=100&lt;/code&gt;, it scales the attempt number down proportionally so you don’t wait years between retries.&lt;/p&gt;
    &lt;p&gt;Workers can override this with custom backoff:&lt;/p&gt;
    &lt;code&gt;@worker(queue="default")
class MyWorker:
    async def process(self, job: Job):
        ...

    def backoff(self, job: Job) -&amp;gt; int:
        # Linear backoff: 60s, 120s, 180s...
        return job.attempt * 60
&lt;/code&gt;
    &lt;head rend="h2"&gt;Surfacing - Takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL does the heavy lifting. &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt;for concurrent job fetching,&lt;code&gt;LISTEN/NOTIFY&lt;/code&gt;for real-time signaling,&lt;code&gt;ON CONFLICT&lt;/code&gt;for leader election - the database isn’t just storage, it’s the coordination layer. There’s no Redis, no ZooKeeper, no external broker. One less thing to operate.&lt;/item&gt;
      &lt;item&gt;Oban-py is concurrent, but not parallel. Async IO allows multiple jobs to be in-flight, but the event loop is single-threaded. For I/O-bound workloads, this is fine. For CPU-bound tasks, consider using the Pro version with a process pool.&lt;/item&gt;
      &lt;item&gt;Leader election is simple and effective. No consensus protocol, no Raft - just an &lt;code&gt;INSERT ... ON CONFLICT&lt;/code&gt;with a TTL. The leader refreshes at 2x the normal rate to hold the lease. If it dies, the lease expires and another node takes over. Good enough for pruning and rescuing.&lt;/item&gt;
      &lt;item&gt;The codebase is a pleasure to read. Clear naming, consistent patterns, and well-separated concerns - exploring it felt more like reading a well-written book than understanding a library.&lt;/item&gt;
      &lt;item&gt;OSS gets you far, Pro fills the gaps. Bulk operations, smarter rescues, and true parallelism are all Pro-only - but for what you get, Pro license feels like a great deal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Overall, Oban.py is a clean and well-structured port. If you’re coming from Elixir and miss Oban, or if you’re in Python and want a database-backed job queue that doesn’t require external infrastructure beyond PostgreSQL - it’s worth looking at.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46797594</guid><pubDate>Wed, 28 Jan 2026 16:32:00 +0000</pubDate></item><item><title>Mousefood – Build embedded terminal UIs for microcontrollers</title><link>https://github.com/ratatui/mousefood</link><description>&lt;doc fingerprint="8687611f4686e2e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Mousefood - a no-std embedded-graphics backend for Ratatui!&lt;/p&gt;
    &lt;p&gt;Add mousefood as a dependency:&lt;/p&gt;
    &lt;code&gt;cargo add mousefood&lt;/code&gt;
    &lt;p&gt;Exemplary setup:&lt;/p&gt;
    &lt;code&gt;use mousefood::embedded_graphics::{mock_display::MockDisplay, pixelcolor::Rgb888};
use mousefood::prelude::*;
use ratatui::widgets::{Block, Paragraph};
use ratatui::{Frame, Terminal};

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    // replace this with your display driver
    // e.g. ILI9341, ST7735, SSD1306, etc.
    let mut display = MockDisplay::&amp;lt;Rgb888&amp;gt;::new();

    let backend = EmbeddedBackend::new(&amp;amp;mut display, EmbeddedBackendConfig::default());
    let mut terminal = Terminal::new(backend)?;

    terminal.draw(draw)?;
    Ok(())
}

fn draw(frame: &amp;amp;mut Frame) {
    let block = Block::bordered().title("Mousefood");
    let paragraph = Paragraph::new("Hello from Mousefood!").block(block);
    frame.render_widget(paragraph, frame.area());
}&lt;/code&gt;
    &lt;p&gt;Embedded-graphics includes bitmap fonts that have a very limited set of characters to save space (ASCII, ISO 8859 or JIS X0201). This makes it impossible to draw most of Ratatui's widgets, which heavily use box-drawing glyphs, Braille, and other special characters.&lt;/p&gt;
    &lt;p&gt;Mousefood by default uses &lt;code&gt;embedded-graphics-unicodefonts&lt;/code&gt;,
which provides embedded-graphics fonts with a much larger set of characters.&lt;/p&gt;
    &lt;p&gt;In order to save space and speed up rendering, the &lt;code&gt;fonts&lt;/code&gt; feature can be disabled by turning off the default crate features.
&lt;code&gt;ibm437&lt;/code&gt; is a good alternative that includes
some drawing characters, but is not as large as embedded-graphics-unicodefonts.&lt;/p&gt;
    &lt;p&gt;Bold and italic modifiers are supported, but this requires providing fonts through &lt;code&gt;EmbeddedBackendConfig&lt;/code&gt;.
If only regular font is provided, it serves as a fallback.
All fonts must be of the same size.&lt;/p&gt;
    &lt;code&gt;use mousefood::embedded_graphics::{mock_display::MockDisplay, pixelcolor::Rgb888};
use mousefood::{EmbeddedBackend, EmbeddedBackendConfig, fonts};
use ratatui::Terminal;

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let mut display = MockDisplay::&amp;lt;Rgb888&amp;gt;::new();
    let config = EmbeddedBackendConfig {
        font_regular: fonts::MONO_6X13,
        font_bold: Some(fonts::MONO_6X13_BOLD),
        font_italic: Some(fonts::MONO_6X13_ITALIC),
        ..Default::default()
    };
    let backend = EmbeddedBackend::new(&amp;amp;mut display, config);
    let _terminal = Terminal::new(backend)?;
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;Colors can be remapped using &lt;code&gt;color_theme&lt;/code&gt; on &lt;code&gt;EmbeddedBackendConfig&lt;/code&gt;.
By default the ANSI palette is used.&lt;/p&gt;
    &lt;code&gt;use mousefood::{ColorTheme, EmbeddedBackend, EmbeddedBackendConfig};
use mousefood::embedded_graphics::{mock_display::MockDisplay, pixelcolor::Rgb888};

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let mut display = MockDisplay::&amp;lt;Rgb888&amp;gt;::new();
    let theme = ColorTheme {
        background: Rgb888::new(5, 5, 5),
        foreground: Rgb888::new(240, 240, 240),
        yellow: Rgb888::new(255, 200, 0),
        ..ColorTheme::ansi()
    };

    let config = EmbeddedBackendConfig {
        color_theme: theme,
        ..Default::default()
    };
    let backend = EmbeddedBackend::new(&amp;amp;mut display, config);
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;Mousefood includes popular color themes that can be used directly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ColorTheme::ansi()&lt;/code&gt;- Standard ANSI colors (default)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ColorTheme::tokyo_night()&lt;/code&gt;- Tokyo Night dark theme with blue/purple tones&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Mousefood can be run in a simulator using embedded-graphics-simulator crate.&lt;/p&gt;
    &lt;p&gt;Run simulator example:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/ratatui/mousefood.git
cd mousefood/examples/simulator
cargo run&lt;/code&gt;
    &lt;p&gt;For more details, view the simulator example.&lt;/p&gt;
    &lt;p&gt;Support for EPD (e-ink displays) produced by WeAct Studio (&lt;code&gt;weact-studio-epd&lt;/code&gt; driver) can be enabled using &lt;code&gt;epd-weact&lt;/code&gt; feature.&lt;/p&gt;
    &lt;p&gt;This driver requires some additional configuration. Follow the &lt;code&gt;weact-studio-epd&lt;/code&gt;
crate docs and apply the same &lt;code&gt;flush_callback&lt;/code&gt; pattern used in the Waveshare example below.&lt;/p&gt;
    &lt;head&gt;Setup example&lt;/head&gt;
    &lt;code&gt;use mousefood::prelude::*;
use weact_studio_epd::graphics::Display290BlackWhite;
use weact_studio_epd::WeActStudio290BlackWhiteDriver;

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    // Configure SPI + GPIO + delay provider for your board.
    // let (spi_interface, busy, rst, delay) = ...;

    let mut driver = WeActStudio290BlackWhiteDriver::new(spi_interface, busy, rst, delay);
    let mut display = Display290BlackWhite::new();

    driver.init()?;

    let config = EmbeddedBackendConfig {
        flush_callback: Box::new(move |d| {
            driver.full_update(d).expect("epd update failed");
        }),
        ..Default::default()
    };

    let backend = EmbeddedBackend::new(&amp;amp;mut display, config);
    let _terminal = Terminal::new(backend)?;
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;Support for EPD (e-ink displays) produced by Waveshare Electronics (&lt;code&gt;epd-waveshare&lt;/code&gt; driver) can be enabled using &lt;code&gt;epd-waveshare&lt;/code&gt; feature.&lt;/p&gt;
    &lt;head&gt;Setup example&lt;/head&gt;
    &lt;code&gt;use mousefood::prelude::*;
use epd_waveshare::{epd2in9_v2::*, prelude::*};

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    // Configure SPI + GPIO + delay provider for your board.
    // let (mut spi_device, busy, dc, rst, mut delay) = ...;

    let mut epd = Epd2in9::new(&amp;amp;mut spi_device, busy, dc, rst, &amp;amp;mut delay, None)?;
    let mut display = Display2in9::default();

    let config = EmbeddedBackendConfig {
        flush_callback: Box::new(move |d| {
            epd.update_and_display_frame(&amp;amp;mut spi_device, d.buffer(), &amp;amp;mut delay)
                .expect("epd update failed");
        }),
        ..Default::default()
    };

    let backend = EmbeddedBackend::new(&amp;amp;mut display, config);
    let _terminal = Terminal::new(backend)?;
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;See the full embedded example at &lt;code&gt;examples/epd-waveshare-demo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Flash memory on most embedded devices is very limited. Additionally, to achieve high frame rate when using the &lt;code&gt;fonts&lt;/code&gt; feature,
it is recommended to use &lt;code&gt;opt-level = 3&lt;/code&gt;,
which can make the resulting binary even larger.&lt;/p&gt;
    &lt;p&gt;Mousefood is hardware-agnostic. Successfully tested on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ESP32 (Xtensa)&lt;/item&gt;
      &lt;item&gt;ESP32-C6 (RISC-V)&lt;/item&gt;
      &lt;item&gt;STM32&lt;/item&gt;
      &lt;item&gt;RP2040&lt;/item&gt;
      &lt;item&gt;RP2350&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Full API docs are available on docs.rs.&lt;/p&gt;
    &lt;p&gt;All contributions are welcome!&lt;/p&gt;
    &lt;p&gt;Before opening a pull request, please read the contributing guidelines.&lt;/p&gt;
    &lt;p&gt;Here are some projects built using Mousefood:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tuitar - A portable guitar training tool.&lt;/item&gt;
      &lt;item&gt;Mnyaoo32 - An eccentric way to consume IRC messages using ESP32.&lt;/item&gt;
      &lt;item&gt;Phone-OS - A modern phone OS for ESP32 CYD.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Send a pull request to add your project here!&lt;/p&gt;
    &lt;p&gt;Mousefood is dual-licensed under Apache 2.0 and MIT terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46798402</guid><pubDate>Wed, 28 Jan 2026 17:20:31 +0000</pubDate></item><item><title>Computer History Museum Launches Digital Portal to Its Collection</title><link>https://computerhistory.org/press-releases/computer-history-museum-launches-digital-portal-to-its-vast-collection/</link><description>&lt;doc fingerprint="bdf8b7dc9943ebbd"&gt;
  &lt;main&gt;
    &lt;p&gt;Gordon and Betty Moore Foundation Funded OpenCHM to Digitize One-of-a-Kind Archive&lt;/p&gt;
    &lt;p&gt;MOUNTAIN VIEW, Calif. – January 21, 2026 – The Computer History Museum (CHM), a leader in decoding technology—its computing past, digital present, and future impact on humanity—announced the launch of OpenCHM, a new digital portal providing global access to its unparalleled collection.&lt;/p&gt;
    &lt;p&gt;“OpenCHM is designed to inspire discovery, spark curiosity, and make the stories of the digital age more accessible to everyone, everywhere,” said CHM President and CEO Marc Etkind. “We’re unlocking the collection for new audiences to explore.”&lt;/p&gt;
    &lt;p&gt;OpenCHM is funded by the Gordon and Betty Moore Foundation and other generous donors, and this launch represents a major milestone in CHM's multi-year digitization initiative. Designed in collaboration with KeepThinking, the portal is powered by their innovative Qi collection management system.&lt;/p&gt;
    &lt;p&gt;“We were excited by the prospect of CHM opening up their unique collections to broader audiences, from scholars and teachers to students and the public. The balance of the engaging, curated narratives by CHM’s own historians and field experts along with the tools and capabilities to explore one’s own interests makes the platform truly compelling. The Moore Foundation also values the OpenCHM team's commitment to thoughtful design and documentation, which we hope will inspire and enable other organizations to share their collections more openly.”—Janet Coffey, Program Director, Science, Gordon and Betty Moore Foundation&lt;/p&gt;
    &lt;p&gt;The OpenCHM platform expands worldwide access to CHM’s vast collection through a digital portal, and ongoing digitization regularly adds more historical materials. Along with the collection, the portal introduces new digital storytelling and discovery tools designed to bring the history of the technology revolution to life for both experts and general audiences.&lt;/p&gt;
    &lt;p&gt;OpenCHM features include:&lt;/p&gt;
    &lt;p&gt;OpenCHM advances CHM’s mission to preserve and interpret the history of technology while making it broadly accessible as a public resource.&lt;/p&gt;
    &lt;p&gt;About CHM &lt;lb/&gt; The Computer History Museum (CHM) is the leading museum decoding computing’s ongoing impact on our world. We are uniquely positioned to cull the key lessons of the past and—through our research, exhibits, events, and incomparable collection of computing artifacts—create informed digital citizens empowered to make the choices that will shape a better future. &lt;/p&gt;
    &lt;p&gt;Press contact: Carina Sweet, [email protected], 650.810.1059&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46798994</guid><pubDate>Wed, 28 Jan 2026 17:54:54 +0000</pubDate></item><item><title>LM Studio 0.4</title><link>https://lmstudio.ai/blog/0.4.0</link><description>&lt;doc fingerprint="2e5b9d5b3934d4fa"&gt;
  &lt;main&gt;&lt;p&gt;Introducing LM Studio 0.4.0&lt;/p&gt;&lt;p&gt;Today we are thrilled to share LM Studio 0.4.0, the next generation of LM Studio.&lt;/p&gt;&lt;p&gt;This release introduces parallel requests with continuous batching for high throughput serving, all-new non-GUI deployment option, new stateful REST API, and a refreshed user interface.&lt;/p&gt;&lt;p&gt;LM Studio 0.4.0 highlights include:&lt;/p&gt;&lt;code&gt;/v1/chat&lt;/code&gt; that allows using local MCPs.&lt;p&gt;Read on for more details!&lt;/p&gt;&lt;p&gt;Today we're introducing &lt;code&gt;llmster&lt;/code&gt;: it's the core of the LM Studio desktop app, but packaged to be server-native, without reliance on the GUI. We've rearchitected our software to separate the GUI from the core functionality, allowing &lt;code&gt;llmster&lt;/code&gt; to run as a standalone daemon.&lt;/p&gt;&lt;p&gt;This means &lt;code&gt;llmster&lt;/code&gt; can be run completely independently of the app and deployed anywhere: Linux boxes, cloud servers, your GPU rig, or even Google Colabs. It can of course still be run on your local machine without the GUI, for those who prefer terminal-based workflows.&lt;/p&gt;&lt;code&gt;llmster&lt;/code&gt;&lt;p&gt;Linux / Mac&lt;/p&gt;&lt;quote&gt;&lt;code&gt;curl -fsSL https://lmstudio.ai/install.sh | bash&lt;/code&gt;&lt;/quote&gt;&lt;p&gt;Windows&lt;/p&gt;&lt;quote&gt;&lt;code&gt;irm https://lmstudio.ai/install.ps1 | iex&lt;/code&gt;&lt;/quote&gt;&lt;code&gt;llmster&lt;/code&gt;&lt;code&gt;lms daemon up&lt;/code&gt;&lt;code&gt;lms get &amp;lt;model&amp;gt;&lt;/code&gt;&lt;code&gt;lms server start&lt;/code&gt;&lt;code&gt;lms chat&lt;/code&gt;&lt;code&gt;lms runtime update llama.cpp&lt;/code&gt; (and &lt;code&gt;lms runtime update mlx&lt;/code&gt; on macOS)&lt;p&gt;Alongside LM Studio 0.4.0, our llama.cpp engine is graduating to version 2.0.0. With it we're introducing support for concurrent inference requests to the same model.&lt;/p&gt;&lt;p&gt;Run parallel requests in the app with Split View&lt;/p&gt;&lt;p&gt;You will find 2 new load options in the model loader dialog:&lt;/p&gt;&lt;p&gt;Max Concurrent Predictions: sets the maximum number of concurrent requests that can be processed by the model. Requests beyond this limit will be queued.&lt;/p&gt;&lt;p&gt;Unified KV Cache: when enabled, preallocated resources will not be hard-partitioned per concurrent request, allowing varying request sizes per request. This is enabled by default.&lt;/p&gt;&lt;p&gt;Parallel requests work thanks to llama.cpp's open-source continuous batching implementation, adopted in LM Studio's llm-engine. This capability has not yet made it into our MLX engine, but it is actively in the works and will land soon.&lt;/p&gt;&lt;p&gt;We have refreshed LM Studio's user interface from the ground up for a more consistent and pleasant experience.&lt;/p&gt;&lt;p&gt;Export your chats&lt;/p&gt;&lt;p&gt;You can now export your chats to PDF, markdown, or plain text. Click the ••• menu on a chat and head to "Export" for all available options.&lt;/p&gt;&lt;p&gt;You can now open multiple chat sessions side by side using Split View. Click the new Split View icon in the top right corner of the chat window to open a new chat pane.&lt;/p&gt;&lt;p&gt;Developer Mode is a new setting that exposes advanced options in the app. You can enable it from Settings &amp;gt; Developer. Once enabled, it'll reveal all advanced options across the app, including in the model loader dialog and sidebars.&lt;/p&gt;&lt;p&gt;Head over to the Developer tab to see the new in-app documentation. It covers the new REST API, CLI commands, and advanced configuration options.&lt;/p&gt;&lt;p&gt;New CLI experience&lt;/p&gt;&lt;code&gt;lms chat&lt;/code&gt;&lt;p&gt;With LM Studio 0.4.0, we're introducing a brand-new CLI experience centered around the &lt;code&gt;lms chat&lt;/code&gt; command. This command opens an interactive chat session directly in your terminal, allowing you to chat with your models and download new ones.&lt;/p&gt;&lt;p&gt;Run &lt;code&gt;lms chat --help&lt;/code&gt; to see all available options.&lt;/p&gt;&lt;code&gt;/v1/chat&lt;/code&gt;&lt;code&gt;/v1/chat&lt;/code&gt; endpoint&lt;p&gt;&lt;code&gt;/v1/chat&lt;/code&gt; is a new first-party REST endpoint for chatting with local models from your apps.&lt;/p&gt;&lt;p&gt;Unlike typical "stateless" chat APIs, &lt;code&gt;/v1/chat&lt;/code&gt; is stateful: you can start a conversation, get back a &lt;code&gt;response_id&lt;/code&gt;, and then continue it by passing &lt;code&gt;previous_response_id&lt;/code&gt; on your next request. This keeps requests small and makes it easy to build multi-step workflows on top of LM Studio.&lt;/p&gt;&lt;p&gt;Responses also include detailed stats (tokens in/out, speed, time to first token), so you can track performance and tune load/inference settings.&lt;/p&gt;&lt;p&gt;And when you need tools, &lt;code&gt;/v1/chat&lt;/code&gt; can also enable your locally configured MCPs - gated by permission keys.&lt;/p&gt;&lt;p&gt;To allow you to control which client accesses your LM Studio server, we've introduced permission keys. You can generate and manage permission keys from the Settings &amp;gt; Server tab in the app.&lt;/p&gt;&lt;p&gt;Please let us know how you like it! We'd love to hear your feedback.&lt;/p&gt;&lt;p&gt;Special thanks to the 0.4.0 beta group. Your feedback and bug reports have been invaluable &amp;lt;3.&lt;/p&gt;&lt;p&gt;Below is the full list of release notes items.&lt;/p&gt;&lt;quote&gt;&lt;code&gt;### LM Studio 0.4.0 - Release Notes Welcome to LM Studio 0.4.0 👾! - We're excited to introduce the next generation of LM Studio. - New features include: - `llmster`: the LM Studio Daemon for headless deployments w/o GUI on servers or cloud instances - Parallel inference requests (instead of queued) for high throughput use cases - New stateful REST API with local MCP server support - `POST /v1/chat` - A completely revamped UI experience ✨ **Build 17** - MCPs will now only be loaded when needed, instead of at app startup - Fixed a bug where some fields in app settings could get reset after update **Build 16** - New icons and placements for Discover, My Models buttons - Fixed a bug where generators wouldn't show in the top bar model picker when selected - Fixed a bug which prevented additional quantizations from being downloaded for staff pick models that were already downloaded - Fixed a bug where `lms import` will sometimes not work properly if llmster (daemon) is also installed - Fixed a bug in `/api/v1/chat` that caused server errors when inputs were empty or `top_k` exceeded 500 - Fixed a bug where `lms ls` and `lms load` sometimes would fail after waking up the LM Studio service - Fixed a bug where sometimes token counting would not work properly for gpt-oss models **Build 15** - Introduce Parallel Requests with Continuous Batching 🚀 - When loading a model, you can now select n_parallel to allow multiple requests to be processed in parallel. - When enabled, instead of queuing requests one by one, the model will process up to N requests simultaneously. - By default, parallel slots are set to 4 (with unified KV set to true, which should result in no additional memory overhead). - This is supported for LM Studio's llama.cpp engine, with MLX coming later. - Introducing Split View in Chat: view two chats side by side. - Drag and drop chat tabs to either half of the window to split the view. - Close one side of the split view with the 'x' button in the top right of each pane. - Introducing 🔧 Developer Mode: a simplification of the previous Developer/Power User/User 3 mode switch. - Developer Mode combines the previous Developer and Power User modes into a single mode with all advanced features enabled. - You can turn on Developer Mode in Settings &amp;gt; Developer. - New setting: enforce allowing only one new empty chat at a time (default: enabled) - Change in Settings &amp;gt; Chat - New 🔭 Model Search experience - Access via the 🔍 button on the top right or by pressing Cmd/Ctrl + Shift + M - Model format filter preferences persist between app restarts - Modal is resizable and remembers its size between app restarts - Limit number of open tabs to 1 per pane. Support showing 2 side-by-side chat tabs. - Selecting a new chat replaces the current tab in that pane. - Add button to create a new chat in the sidebar - Pressing Cmd/Ctrl + L while the model picker is open will dismiss it - On narrow window size show right hand sidebar as an ephemeral overlay - Support for the LFM2 tool call format - CLI now uses commit hash for versioning instead of semantic version numbers - Updates to UI details in hardware settings - Fixed a bug where moving large number of conversations would sometimes only move part of them - Fixed a bug where `lms ls` sometimes would show incomplete list of models on startup - Fixed a bug in deleting tool confirmation preferences in settings - Fixed a UI bug in app onboarding - Fixed a visual bug in Models Table selected row affecting the Architecture and Format columns - Fixed a bug where undoing pasted content in chat input would not work as expected - Fixed a bug where a leading decimal in a numeric input would parse as a 0 - Fixed a bug rendering multiple images in a conversation message - Fixed a bug where a documentation sidebar section would sometimes get stuck in expanded state - Fixed a bug where chat names would sometimes be empty - Fixed a visual bug in rendering keyboard shortcuts on Windows and Linux - Fixed a bug where model loader would sometimes close due to mouse move shortly after opening - Fixed a bug rendering titles in preset conflict resolver dialog - Fixed a bug where reloading with new load parameters would not apply next time the same model is used for a chat - Fixed a bug where the model loading will get stuck if the cpu moe slider is maxed out - Fixed a bug where exporting chats with very large images to PDF would fail - Fixed a responsive UI overlap bug in the app header - [Windows] Fixed a bug where the default embedding model will not be available after in-app update - Adds download, copy, and reveal in working directory buttons to generated images in chat **Build 14** - (Build 14 was skipped) **Build 13** - App setting to control primary navigation position: 'top' or 'left' - [Mac] New tray menu icon 👾 (experimental, might change) - `/api/v1` endpoints and `/v1/responses` API now return better formatted errors - Significantly reduce the size of the app update asset **Build 12** - Bugfix: New chats to be created with the same model as the previously focused chat - Bring back gear button to change load parameters for currently loaded model - Bring back context fullness indicator and current input token counter - New in My Models: right-click on tab header to choose which columns to show/hide - New in My Models: Capabilities and Format columns - Fixed a flicker in model picker floating panel upon first open - P.S. you can open the model picker from anywhere in the app with Cmd/Ctrl + L - Fixed focus + Enter on Eject button not working inside model picker - Updated chat terminal and messages colors and style - Fixed dragging and dropping chats/folders in the sidebar **Build 11** - ✨👾 Completely revamped UI - this is a work in progress, give us feedback! - [CLI] New `lms chat` experience! - Support slash commands, thinking highlighting and pasting larger content - Slash commands available: /model, /download, /system-prompt, /help and /exit - [CLI] New: `lms runtime survey` to print info about available GPUs! - FunctionGemma support - Added a slider to control n_cpu_moe - New REST API endpoint: `api/v1/models/unload` to unload models - Breaking change: in `api/v1/models/load` endpoint response, introduced in this beta, `model_instance_id` has been renamed to `instance_id`. - Display live processing status for each loaded LLM on the Developer page - Prompt processing progress percentage → token generation count - Improved PDF rendering quality for tool requests and responses - Significantly increased the reliability and speed of deleting multiple chats at once - Updated style of chat message generation info - Updated layout of Hardware settings page and other settings rows - Fixed a bug where sometimes models are indexed before all files are downloaded - Fixed a bug where exporting larger PDFs would sometimes fail - Fixed a bug where pressing the chat clear hotkey multiple times would open multiple confirmation dialogs - Fixed a bug where pressing the chat clear hotkey would sometimes duplicate the chat - Fixed a bug where pressing the duplicate hotkey on the release notes would create a glitched chat tab - Fixed a bug where `lms help` would not work - Fixed a bug where deleting models or canceling downloads would leave behind empty folders - Fixed a styling bug in the GPU section on the Hardware page - [MLX] Fixed a bug where the bf16 model format was not recognized as a valid quantization **Build 10** - (Build 10 was skipped) **Build 9** - (Build 9 was skipped) **Build 8** - Fixed a bug where the default system prompt was still sent to the model even after the system prompt field was cleared. - Fixed a bug where exported chats did not include the correct system prompt. - Fixed a bug where the token count was incorrect when a default system prompt existed but the system prompt field was cleared. - Fix a bug where sometimes the tool call results are not being added to the context correctly - Fix chat clearing with hotkey (Cmd/Ctrl + Shift + Option/Alt + D) would clear wrong chat - Fix a bug where Ctrl/Cmd + N would sometimes create two new chats - Updated style for Integrations panel and select - Fixed cURL copy button for embedding models displaying additional incorrect requests - Fix "ghost chats" caused by moving conversations/deleting conversations **Build 7** - Fix jinja prompt formatting bug for some models where EOS tokens were not being included properly - Bring back release notes viewer for Runtime available update - Prevent tooltip from staying open when hovering tooltip content - Fix a bug in deleting multiple chats at once - Minor fix to overlapping labels in model loader - Support for EssentialAI's rnj-1 model **Build 6** - Fixed a bug where Qwen3-Next user messages would not appear in formatted prompts properly **Build 5** - Fixed a bug where quickly deleting multiple conversations will sometimes soft-lock the app - Fixed another bug that prevented the last remaining open tab from being closed **Build 4** - Fixed a bug where the last remaining open tab sometimes could not be closed - Fixed a bug where `lms log stream` would exit immediately - Fixed a bug where the server port would get printed as [object Object] - Image validation checks in `v1/chat` and `v1/responses` REST API now run without model loading - Fixed a bug where images without extensions were not classified correctly - Fix bug in move-to-trash onboarding dialog radio selection where some parts of the label were not clickable - Fix several clickable areas bugs in Settings windows buttons - Fixed a bug where certain settings may get adjusted unexpectedly when using llmster (for example, the JIT model loading may become disabled) - New and improved Runtime page style and structure - Fixes a bug where guardrail settings were not showing up in User UI mode - Fixed a bug where `lms log stream` would exit immediately **Build 3** - Introducing 'llmster': the LM Studio Daemon! - True headless, no GUI version of the process that powers LM Studio - Run it on servers, cloud instances, or any machine without a graphical interface - Load models on CPU/GPU and serve them, use via `lms` CLI or our APIs - To install: - Linux/Mac: `curl -fsSL https://lmstudio.ai/install.sh | bash` - Windows: `irm https://lmstudio.ai/install.ps1 | iex` - Support for MistralAI Ministral models (3B, 8B, 13B) - Improved `lms` output and help messages style. Run `lms --help` to explore! - Get llama.cpp level logs with `lms log stream -s runtime` in the terminal - `lms get` interactive mode now shows the latest model catalog options - New and improved style for Downloads panel - New and improved style for App Settings - We're trying something out: Model search now in its own tab - still iterating on the UI for this page, please give us feedback! **Build 2** - Show release notes in a dedicated tab after app updates - Add support to display images in exported PDFs and exported markdown files - Quick Docs is now Developer Docs, with refreshed documentation and direct access from the welcome page. - Allow creating permission tokens without allowed MCP permissions - Fixed a bug where sometimes images created by MCPs are not showing up - Fixed a bug where sometimes the plugin chips not working - Fixed a bug where the "thinking" blocks will sometimes expand erroneously - Fixed a bug where certain tabs would not open correctly - Fixed a bug where sometimes the model list would not load - Fixed a bug where in-app docs article titles would sometimes wiggle on scroll - Fixed a visual bug in Preset 'resolve conflicts' modal - Fixed a bug where sometimes Download button would continue showing for an already downloaded model - Fixed a bug where chat sidebar buttons wouldn't be visible on narrow screens - Display model indexing errors as buttons rather than hints **Build 1** - Welcome to the 0.4.0 Beta!&lt;/code&gt;&lt;/quote&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46799477</guid><pubDate>Wed, 28 Jan 2026 18:23:14 +0000</pubDate></item><item><title>Show HN: A MitM proxy to see what your LLM tools are sending</title><link>https://github.com/jmuncor/sherlock</link><description>&lt;doc fingerprint="2e17f14e9e9a19e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Token Tracker for LLM CLI Tools&lt;/p&gt;
    &lt;p&gt;Installation • Quick Start • Features • Commands • Contributing&lt;/p&gt;
    &lt;p&gt;Sherlock tracks token usage for LLM CLI tools with a live terminal dashboard. See exactly how many tokens you're using in real-time.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Track Token Usage: See exactly how many tokens each request consumes&lt;/item&gt;
      &lt;item&gt;Monitor Context Windows: Visual fuel gauge shows cumulative usage against your limit&lt;/item&gt;
      &lt;item&gt;Debug Prompts: Automatically saves every prompt as markdown and JSON for review&lt;/item&gt;
      &lt;item&gt;Zero Configuration: No certificates, no setup - just install and go&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/jmuncor/sherlock.git
cd sherlock
pip install -e .&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.10+&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sherlock start&lt;/code&gt;
    &lt;p&gt;You'll be prompted to choose where to save captured prompts, then the dashboard appears:&lt;/p&gt;
    &lt;code&gt;┌─────────────────────────────────────────────────────────────┐
│  SHERLOCK - LLM Traffic Inspector                           │
├─────────────────────────────────────────────────────────────┤
│  Context Usage  ████████████░░░░░░░░░░░░░░░░  42%           │
│                 (84,231 / 200,000 tokens)                   │
├─────────────────────────────────────────────────────────────┤
│  Time     Provider    Model                      Tokens     │
│  14:23:01 Anthropic   claude-sonnet-4-20250514   12,847     │
│  14:23:45 Anthropic   claude-sonnet-4-20250514   8,234      │
│  14:24:12 Anthropic   claude-sonnet-4-20250514   15,102     │
├─────────────────────────────────────────────────────────────┤
│  Last Prompt: "Can you help me refactor this function..."   │
└─────────────────────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;code&gt;# For Claude Code
sherlock claude

# For Gemini CLI (see known issues)
sherlock gemini

# For OpenAI Codex
sherlock codex&lt;/code&gt;
    &lt;p&gt;That's it! Watch the dashboard update in real-time as you work.&lt;/p&gt;
    &lt;p&gt;Real-time token tracking with color-coded fuel gauge:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Green: &amp;lt; 50% of limit&lt;/item&gt;
      &lt;item&gt;Yellow: 50-80% of limit&lt;/item&gt;
      &lt;item&gt;Red: &amp;gt; 80% of limit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every intercepted request is saved to your chosen directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Markdown - Human-readable format with metadata&lt;/item&gt;
      &lt;item&gt;JSON - Raw API request body for debugging&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you exit, see your total usage:&lt;/p&gt;
    &lt;code&gt;Session complete. Total: 84,231 tokens across 12 requests.
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock start&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start the proxy and dashboard&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock claude&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run Claude Code with proxy configured&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock gemini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run Gemini CLI with proxy configured&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock codex&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run OpenAI Codex CLI with proxy configured&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock run --provider &amp;lt;name&amp;gt; &amp;lt;cmd&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run any command with proxy configured&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;sherlock start [OPTIONS]

Options:
  -p, --port NUM    Proxy port (default: 8080)
  -l, --limit NUM   Token limit for fuel gauge (default: 200000)&lt;/code&gt;
    &lt;code&gt;sherlock claude [OPTIONS] [ARGS]...

Options:
  -p, --port NUM    Proxy port (default: 8080)&lt;/code&gt;
    &lt;code&gt;┌─────────────────────────────────────────────────────────────────┐
│  Terminal 1: sherlock start                                      │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  HTTP Proxy (localhost:8080)                                ││
│  │  + Dashboard                                                ││
│  │  + Prompt Archive                                           ││
│  └─────────────────────────────────────────────────────────────┘│
└───────────────────────────────┬─────────────────────────────────┘
                                │ HTTP
                                │
┌───────────────────────────────┴─────────────────────────────────┐
│  Terminal 2: sherlock claude                                     │
│  ┌─────────────────────────────────────────────────────────────┐│
│  │  Sets ANTHROPIC_BASE_URL=http://localhost:8080              ││
│  │  Runs: claude                                               ││
│  └─────────────────────────────────────────────────────────────┘│
└─────────────────────────────────────────────────────────────────┘
                                │
                                │ HTTPS
                                ▼
                      ┌───────────────────┐
                      │ api.anthropic.com │
                      └───────────────────┘
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Provider&lt;/cell&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Anthropic (Claude Code)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock claude&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Google (Gemini CLI)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock gemini&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blocked by upstream issue&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;OpenAI (Codex)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sherlock codex&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Gemini CLI currently has a known issue where it ignores custom base URLs when using OAuth authentication. Sherlock's Gemini support will work automatically once the Gemini CLI team fixes this issue.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Here's how you can help:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Commit your changes (&lt;code&gt;git commit -m 'Add amazing feature'&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Push to the branch (&lt;code&gt;git push origin feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/jmuncor/sherlock.git
cd sherlock
python -m venv venv
source venv/bin/activate
pip install -e .&lt;/code&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;See what's really being sent to the LLM. Track. Learn. Optimize.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46799898</guid><pubDate>Wed, 28 Jan 2026 18:52:24 +0000</pubDate></item><item><title>Apple to soon take up to 30% cut from all Patreon creators in iOS app</title><link>https://www.macrumors.com/2026/01/28/patreon-apple-tax/</link><description>&lt;doc fingerprint="a48ccb0c510e1558"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Apple to Soon Take Up to 30% Cut From All Patreon Creators in iOS App&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;Apple has set a new deadline of November 1, 2026 for all Patreon creators to switch from Patreon's legacy billing system to the App Store's in-app purchase system in the Patreon app on the iPhone and iPad, as reported by TechCrunch.&lt;/p&gt;
          &lt;div&gt;Note: This image has been edited to include a pile of cash.&lt;/div&gt;
          &lt;p&gt;Patreon is a platform where creators such as YouTubers can receive payments from fans, which can be a valuable revenue stream alongside ads and sponsorships. &lt;/p&gt;
          &lt;p&gt;Apple initially told Patreon that its creators must move to the App Store's in-app purchase system by November 2025, or else Patreon would risk removal from the App Store, but the deadline was pushed back. Apple considers payments from supporters to creators on Patreon to be digital goods that it is entitled to receive a commission on.&lt;/p&gt;
          &lt;p&gt;Apple receives a 30% commission on in-app purchases and subscriptions, but this drops to 15% for a subscription that has been ongoing for more than a year.&lt;/p&gt;
          &lt;p&gt;Patreon gives creators the option to either increase their prices in the iOS app only, or absorb the fee themselves, keeping prices the same across platforms.&lt;/p&gt;
          &lt;p&gt;On the iPhone and iPad, Patreon users who wish to support a creator can sidestep the App Store's commission by completing their payment via Patreon's website.&lt;/p&gt;
          &lt;p&gt;Patreon said it is disappointed with how Apple has navigated this policy.&lt;/p&gt;
          &lt;p&gt;According to TechCrunch, only 4% of Patreon creators are still using the platform's legacy billing system, with the rest having already switched over.&lt;/p&gt;
          &lt;p&gt;Patreon has shared a FAQ with more details for creators.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today introduced its first two physical products of 2026: a second-generation AirTag and the Black Unity Connection Braided Solo Loop for the Apple Watch. Read our coverage of each announcement to learn more:Apple Unveils New AirTag With Longer Range, Louder Speaker, and More Apple Introduces New Black Unity Apple Watch BandBoth the new AirTag and the Black Unity Connection Braided...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Alongside iOS 26.2.1, Apple today released an updated version of iOS 12 for devices that are still running that operating system update, eight years after the software was first released. iOS 12.5.8 is available for the iPhone 5s and the iPhone 6, meaning Apple is continuing to support these devices for 13 and 12 years after launch, respectively. The iPhone 5s came out in September 2013,...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple today introduced the second-generation AirTag, with key features including longer range for tracking items and a louder speaker. For those who are not familiar, the AirTag is a small accessory that you can attach to your backpack, keys, or other items. Then, you can track the location of those items in the Find My app on the iPhone, iPad, Mac, Apple Watch, and iCloud.com. The new...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Update: Apple Creator Studio is now available. Apple Creator Studio launches this Wednesday, January 28. The all-in-one subscription provides access to the Final Cut Pro, Logic Pro, Pixelmator Pro, Motion, Compressor, and MainStage apps, with U.S. pricing set at $12.99 per month or $129 per year. A subscription to Apple Creator Studio also unlocks "intelligent features" and "premium...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;2026 promises to be yet another busy year for Apple, with the company rumored to be planning more than 20 product announcements over the coming months. Beyond the usual updates to iPhones, iPads, Macs, and Apple Watches, Apple is expected to release its all-new smart home hub, which was reportedly delayed until the more personalized version of Siri is ready. Other unique products rumored for ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46801419</guid><pubDate>Wed, 28 Jan 2026 20:59:30 +0000</pubDate></item><item><title>Somebody used spoofed ADSB signals to raster the meme of JD Vance</title><link>https://alecmuffett.com/article/143548</link><description>&lt;doc fingerprint="53add34e651c96be"&gt;
  &lt;main&gt;
    &lt;p&gt;This, if it is still visible:&lt;/p&gt;
    &lt;p&gt;https://globe.adsbexchange.com/?icao=adfdf9&amp;amp;lat=26.678&amp;amp;lon=-80.030&amp;amp;zoom=14.4&amp;amp;showTrace=2026-01-28&lt;/p&gt;
    &lt;p&gt;Via:&lt;/p&gt;
    &lt;p&gt;Next up, age verification for ADSB?&lt;/p&gt;
    &lt;p&gt;by Alec Muffett&lt;/p&gt;
    &lt;p&gt;This, if it is still visible:&lt;/p&gt;
    &lt;p&gt;https://globe.adsbexchange.com/?icao=adfdf9&amp;amp;lat=26.678&amp;amp;lon=-80.030&amp;amp;zoom=14.4&amp;amp;showTrace=2026-01-28&lt;/p&gt;
    &lt;p&gt;Via:&lt;/p&gt;
    &lt;p&gt;Next up, age verification for ADSB?&lt;/p&gt;
    &lt;p&gt;Rasterize. (I know, nobody likes nitpicking)&lt;/p&gt;
    &lt;p&gt;Back when I was using real Tektronix T4014s, we didn’t bother to nitpick.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46802067</guid><pubDate>Wed, 28 Jan 2026 21:50:47 +0000</pubDate></item><item><title>Bf-Tree: modern read-write-optimized concurrent larger-than-memory range index</title><link>https://github.com/microsoft/bf-tree</link><description>&lt;doc fingerprint="27165b52f1c2df85"&gt;
  &lt;main&gt;
    &lt;p&gt;Bf-Tree is a modern read-write-optimized concurrent larger-than-memory range index in Rust from MSR.&lt;/p&gt;
    &lt;p&gt;You can find the Bf-Tree research paper here. You can find more design docs here.&lt;/p&gt;
    &lt;p&gt;Bf-Tree is written in Rust, and is available as a Rust crate. You can add Bf-Tree to your &lt;code&gt;Cargo.toml&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;[dependencies]
bf-tree = "0.1.0"&lt;/code&gt;
    &lt;p&gt;An example use of Bf-Tree:&lt;/p&gt;
    &lt;code&gt;use bf_tree::BfTree;
use bf_tree::LeafReadResult;

let mut config = bf_tree::Config::default();
config.cb_min_record_size(4);
let tree = BfTree::with_config(config, None).unwrap();
tree.insert(b"key", b"value");

let mut buffer = [0u8; 1024];
let read_size = tree.read(b"key", &amp;amp;mut buffer);

assert_eq!(read_size, LeafReadResult::Found(5));
assert_eq!(&amp;amp;buffer[..5], b"value");&lt;/code&gt;
    &lt;p&gt;PRs are accepted and preferred over feature requests. Feel free to reach out if you have any design questions.&lt;/p&gt;
    &lt;p&gt;Bf-Tree supports Linux, Windows, and macOS, although only a recently version of Linux is rigorously tested. Bf-Tree is written in Rust, which you can install here.&lt;/p&gt;
    &lt;p&gt;Please install pre-commit hooks to ensure that your code is formatted and linted in the same way as the rest of the project; the coding style will be enforced in CI, these hooks act as a pre-filter.&lt;/p&gt;
    &lt;code&gt;# If on Ubuntu
sudo apt update &amp;amp;&amp;amp; sudo apt install pre-commit
pre-commit install&lt;/code&gt;
    &lt;code&gt;cargo build --release&lt;/code&gt;
    &lt;code&gt;cargo test&lt;/code&gt;
    &lt;p&gt;Concurrent systems are nondeterministic, and subject to exponential amount of different thread interleaving. We use shuttle to deterministically and systematically explore different thread interleaving to uncover the bugs caused by subtle multithread interactions.&lt;/p&gt;
    &lt;code&gt;cargo test --features "shuttle" --release shuttle_bf_tree_concurrent_operations&lt;/code&gt;
    &lt;p&gt;(Takes about 5 minute to run)&lt;/p&gt;
    &lt;p&gt;Fuzz testing is a bug finding technique that generates random inputs to the system and test for crash. Bf-Tree employs fuzzing to generate random operation sequences (e.g., insert, read, scan) to the system and check that none of the operation sequence will crash the system or lead to inconsistent state. Check the fuzz folder for more details.&lt;/p&gt;
    &lt;p&gt;Check the benchmark folder for more details.&lt;/p&gt;
    &lt;code&gt;cd benchmark
env SHUMAI_FILTER="inmemory" MIMALLOC_LARGE_OS_PAGES=1 cargo run --bin bftree --release&lt;/code&gt;
    &lt;p&gt;More advanced benchmarking, with metrics collecting, numa-node binding, huge page, etc:&lt;/p&gt;
    &lt;code&gt;env MIMALLOC_SHOW_STATS=1 MIMALLOC_LARGE_OS_PAGES=1 MIMALLOC_RESERVE_HUGE_OS_PAGES_AT=0 numactl --membind=0 --cpunodebind=0 cargo bench --features "metrics-rt" micro&lt;/code&gt;
    &lt;p&gt;This project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.&lt;/p&gt;
    &lt;p&gt;Please see CONTRIBUTING.md.&lt;/p&gt;
    &lt;p&gt;See SECURITY.md for security reporting details.&lt;/p&gt;
    &lt;p&gt;This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft’s Trademark &amp;amp; Brand Guidelines. Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46802210</guid><pubDate>Wed, 28 Jan 2026 22:05:05 +0000</pubDate></item><item><title>Tesla ending Models S and X production</title><link>https://www.cnbc.com/2026/01/28/tesla-ending-model-s-x-production.html</link><description>&lt;doc fingerprint="a20590dc3996f1fb"&gt;
  &lt;main&gt;
    &lt;p&gt;Tesla CEO Elon Musk said on Wednesday that the automaker is ending production of its Model S and X vehicles, and will use the factory in Fremont, California, to build Optimus humanoid robots.&lt;/p&gt;
    &lt;p&gt;"It's time to basically bring the Model S and X programs to an end with an honorable discharge," Musk said on the company's fourth-quarter earnings call. "If you're interested in buying a Model S and X, now would be the time to order it."&lt;/p&gt;
    &lt;p&gt;After the original Roadster, the two models are Tesla's oldest vehicles, and in recent years the company has slashed prices as global competition for electric vehicles has soared. Tesla started selling the Model S sedan in 2012, and the Model X SUV three years later.&lt;/p&gt;
    &lt;p&gt;On Tesla's website, the Model S currently starts at about $95,000, while the Model X starts at around $100,000&lt;/p&gt;
    &lt;p&gt;Tesla's far more popular models are the 3 and Y, which accounted for 97% of the company's 1.59 million deliveries last year. The Model 3 now starts at about $37,000, and the Model Y is around $40,000. Tesla debuted more affordable versions of the vehicles late last year.&lt;/p&gt;
    &lt;p&gt;In its earnings announcement on Wednesday, Tesla reported its first annual revenue decline on record, with sales falling in three of the past four quarters. Musk has been trying to turn attention away from traditional EVs and toward a future of driverless cars and humanoid robots, areas where the company currently has virtually no business.&lt;/p&gt;
    &lt;p&gt;Tesla is developing Optimus with the aim of someday selling it as a bipedal, intelligent robot capable of everything from factory work to babysitting. The company said in the release that it plans to unveil the third generation of Optimus this quarter, its "first design meant for mass production."&lt;/p&gt;
    &lt;p&gt;Musk said on the call that Tesla is replacing its production line for S and X in Fremont "with a 1 million unit per year line of Optimus."&lt;/p&gt;
    &lt;p&gt;"Because it is a completely new supply chain," Musk said, "there's really nothing from the existing supply chain that exists in Optimus."&lt;/p&gt;
    &lt;p&gt;Tesla expects to boost headcount at the Fremont facility, Musk added, "and to significantly increase output."&lt;/p&gt;
    &lt;p&gt;This is breaking news. Please refresh for updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46802867</guid><pubDate>Wed, 28 Jan 2026 22:53:54 +0000</pubDate></item><item><title>Render Mermaid diagrams as SVGs or ASCII art</title><link>https://github.com/lukilabs/beautiful-mermaid</link><description>&lt;doc fingerprint="27182a7a4f4986aa"&gt;
  &lt;main&gt;
    &lt;p&gt;Render Mermaid diagrams as beautiful SVGs or ASCII art&lt;/p&gt;
    &lt;p&gt;Ultra-fast, fully themeable, zero DOM dependencies. Built for the AI era.&lt;/p&gt;
    &lt;p&gt;Diagrams are essential for AI-assisted programming. When you're working with an AI coding assistant, being able to visualize data flows, state machines, and system architecture—directly in your terminal or chat interface—makes complex concepts instantly graspable.&lt;/p&gt;
    &lt;p&gt;Mermaid is the de facto standard for text-based diagrams. It's brilliant. But the default renderer has problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Aesthetics — Might be personal preference, but wished they looked more professional&lt;/item&gt;
      &lt;item&gt;Complex theming — Customizing colors requires wrestling with CSS classes&lt;/item&gt;
      &lt;item&gt;No terminal output — Can't render to ASCII for CLI tools&lt;/item&gt;
      &lt;item&gt;Heavy dependencies — Pulls in a lot of code for simple diagrams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We built &lt;code&gt;beautiful-mermaid&lt;/code&gt; at Craft to power diagrams in Craft Agents. It's fast, beautiful, and works everywhere—from rich UIs to plain terminals.&lt;/p&gt;
    &lt;p&gt;The ASCII rendering engine is based on mermaid-ascii by Alexander Grooff. We ported it from Go to TypeScript and extended it Thank you Alexander for the excellent foundation! (And inspiration that this was possible.)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;5 diagram types — Flowcharts, State, Sequence, Class, and ER diagrams&lt;/item&gt;
      &lt;item&gt;Dual output — SVG for rich UIs, ASCII/Unicode for terminals&lt;/item&gt;
      &lt;item&gt;15 built-in themes — And dead simple to add your own&lt;/item&gt;
      &lt;item&gt;Full Shiki compatibility — Use any VS Code theme directly&lt;/item&gt;
      &lt;item&gt;Live theme switching — CSS custom properties, no re-render needed&lt;/item&gt;
      &lt;item&gt;Mono mode — Beautiful diagrams from just 2 colors&lt;/item&gt;
      &lt;item&gt;Zero DOM dependencies — Pure TypeScript, works everywhere&lt;/item&gt;
      &lt;item&gt;Ultra-fast — Renders 100+ diagrams in under 500ms&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;npm install beautiful-mermaid
# or
bun add beautiful-mermaid
# or
pnpm add beautiful-mermaid&lt;/code&gt;
    &lt;code&gt;import { renderMermaid } from 'beautiful-mermaid'

const svg = await renderMermaid(`
  graph TD
    A[Start] --&amp;gt; B{Decision}
    B --&amp;gt;|Yes| C[Action]
    B --&amp;gt;|No| D[End]
`)&lt;/code&gt;
    &lt;code&gt;import { renderMermaidAscii } from 'beautiful-mermaid'

const ascii = renderMermaidAscii(`graph LR; A --&amp;gt; B --&amp;gt; C`)&lt;/code&gt;
    &lt;code&gt;┌───┐     ┌───┐     ┌───┐
│   │     │   │     │   │
│ A │────►│ B │────►│ C │
│   │     │   │     │   │
└───┘     └───┘     └───┘
&lt;/code&gt;
    &lt;p&gt;The theming system is the heart of &lt;code&gt;beautiful-mermaid&lt;/code&gt;. It's designed to be both powerful and dead simple.&lt;/p&gt;
    &lt;p&gt;Every diagram needs just two colors: background (&lt;code&gt;bg&lt;/code&gt;) and foreground (&lt;code&gt;fg&lt;/code&gt;). That's it. From these two colors, the entire diagram is derived using &lt;code&gt;color-mix()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;const svg = await renderMermaid(diagram, {
  bg: '#1a1b26',  // Background
  fg: '#a9b1d6',  // Foreground
})&lt;/code&gt;
    &lt;p&gt;This is Mono Mode—a coherent, beautiful diagram from just two colors. The system automatically derives:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Element&lt;/cell&gt;
        &lt;cell role="head"&gt;Derivation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 100%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Secondary text&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 60% into &lt;code&gt;--bg&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Edge labels&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 40% into &lt;code&gt;--bg&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Connectors&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 30% into &lt;code&gt;--bg&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow heads&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 50% into &lt;code&gt;--bg&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Node fill&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 3% into &lt;code&gt;--bg&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Node stroke&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--fg&lt;/code&gt; at 20% into &lt;code&gt;--bg&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For richer themes, you can provide optional "enrichment" colors that override specific derivations:&lt;/p&gt;
    &lt;code&gt;const svg = await renderMermaid(diagram, {
  bg: '#1a1b26',
  fg: '#a9b1d6',
  // Optional enrichment:
  line: '#3d59a1',    // Edge/connector color
  accent: '#7aa2f7',  // Arrow heads, highlights
  muted: '#565f89',   // Secondary text, labels
  surface: '#292e42', // Node fill tint
  border: '#3d59a1',  // Node stroke
})&lt;/code&gt;
    &lt;p&gt;If an enrichment color isn't provided, it falls back to the &lt;code&gt;color-mix()&lt;/code&gt; derivation. This means you can provide just the colors you care about.&lt;/p&gt;
    &lt;p&gt;All colors are CSS custom properties on the &lt;code&gt;&amp;lt;svg&amp;gt;&lt;/code&gt; element. This means you can switch themes instantly without re-rendering:&lt;/p&gt;
    &lt;code&gt;// Switch theme by updating CSS variables
svg.style.setProperty('--bg', '#282a36')
svg.style.setProperty('--fg', '#f8f8f2')
// The entire diagram updates immediately&lt;/code&gt;
    &lt;p&gt;15 carefully curated themes ship out of the box:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Theme&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Background&lt;/cell&gt;
        &lt;cell role="head"&gt;Accent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;zinc-light&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#FFFFFF&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Derived&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;zinc-dark&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#18181B&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Derived&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;tokyo-night&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#1a1b26&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#7aa2f7&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;tokyo-night-storm&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#24283b&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#7aa2f7&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;tokyo-night-light&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#d5d6db&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#34548a&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;catppuccin-mocha&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#1e1e2e&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#cba6f7&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;catppuccin-latte&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#eff1f5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#8839ef&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;nord&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#2e3440&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#88c0d0&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;nord-light&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#eceff4&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#5e81ac&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;dracula&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#282a36&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#bd93f9&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;github-light&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#ffffff&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#0969da&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;github-dark&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#0d1117&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#4493f8&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;solarized-light&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Light&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#fdf6e3&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#268bd2&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;solarized-dark&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#002b36&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#268bd2&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;one-dark&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#282c34&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#c678dd&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;import { renderMermaid, THEMES } from 'beautiful-mermaid'

const svg = await renderMermaid(diagram, THEMES['tokyo-night'])&lt;/code&gt;
    &lt;p&gt;Creating a theme is trivial. At minimum, just provide &lt;code&gt;bg&lt;/code&gt; and &lt;code&gt;fg&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;const myTheme = {
  bg: '#0f0f0f',
  fg: '#e0e0e0',
}

const svg = await renderMermaid(diagram, myTheme)&lt;/code&gt;
    &lt;p&gt;Want richer colors? Add any of the optional enrichments:&lt;/p&gt;
    &lt;code&gt;const myRichTheme = {
  bg: '#0f0f0f',
  fg: '#e0e0e0',
  accent: '#ff6b6b',  // Pop of color for arrows
  muted: '#666666',   // Subdued labels
}&lt;/code&gt;
    &lt;p&gt;Use any VS Code theme directly via Shiki integration. This gives you access to hundreds of community themes:&lt;/p&gt;
    &lt;code&gt;import { getSingletonHighlighter } from 'shiki'
import { renderMermaid, fromShikiTheme } from 'beautiful-mermaid'

// Load any theme from Shiki's registry
const highlighter = await getSingletonHighlighter({
  themes: ['vitesse-dark', 'rose-pine', 'material-theme-darker']
})

// Extract diagram colors from the theme
const colors = fromShikiTheme(highlighter.getTheme('vitesse-dark'))

const svg = await renderMermaid(diagram, colors)&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;fromShikiTheme()&lt;/code&gt; function intelligently maps VS Code editor colors to diagram roles:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Editor Color&lt;/cell&gt;
        &lt;cell role="head"&gt;Diagram Role&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;editor.background&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bg&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;editor.foreground&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;fg&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;editorLineNumber.foreground&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;line&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;focusBorder&lt;/code&gt; / keyword token&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;accent&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;comment token&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;muted&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;editor.selectionBackground&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;surface&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;editorWidget.border&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;border&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;graph TD
  A[Start] --&amp;gt; B{Decision}
  B --&amp;gt;|Yes| C[Process]
  B --&amp;gt;|No| D[End]
  C --&amp;gt; D
&lt;/code&gt;
    &lt;p&gt;All directions supported: &lt;code&gt;TD&lt;/code&gt; (top-down), &lt;code&gt;LR&lt;/code&gt; (left-right), &lt;code&gt;BT&lt;/code&gt; (bottom-top), &lt;code&gt;RL&lt;/code&gt; (right-left).&lt;/p&gt;
    &lt;code&gt;stateDiagram-v2
  [*] --&amp;gt; Idle
  Idle --&amp;gt; Processing: start
  Processing --&amp;gt; Complete: done
  Complete --&amp;gt; [*]
&lt;/code&gt;
    &lt;code&gt;sequenceDiagram
  Alice-&amp;gt;&amp;gt;Bob: Hello Bob!
  Bob--&amp;gt;&amp;gt;Alice: Hi Alice!
  Alice-&amp;gt;&amp;gt;Bob: How are you?
  Bob--&amp;gt;&amp;gt;Alice: Great, thanks!
&lt;/code&gt;
    &lt;code&gt;classDiagram
  Animal &amp;lt;|-- Duck
  Animal &amp;lt;|-- Fish
  Animal: +int age
  Animal: +String gender
  Animal: +isMammal() bool
  Duck: +String beakColor
  Duck: +swim()
  Duck: +quack()
&lt;/code&gt;
    &lt;code&gt;erDiagram
  CUSTOMER ||--o{ ORDER : places
  ORDER ||--|{ LINE_ITEM : contains
  PRODUCT ||--o{ LINE_ITEM : "is in"
&lt;/code&gt;
    &lt;p&gt;For terminal environments, CLI tools, or anywhere you need plain text, render to ASCII or Unicode box-drawing characters:&lt;/p&gt;
    &lt;code&gt;import { renderMermaidAscii } from 'beautiful-mermaid'

// Unicode mode (default) — prettier box drawing
const unicode = renderMermaidAscii(`graph LR; A --&amp;gt; B`)

// Pure ASCII mode — maximum compatibility
const ascii = renderMermaidAscii(`graph LR; A --&amp;gt; B`, { useAscii: true })&lt;/code&gt;
    &lt;p&gt;Unicode output:&lt;/p&gt;
    &lt;code&gt;┌───┐     ┌───┐
│   │     │   │
│ A │────►│ B │
│   │     │   │
└───┘     └───┘
&lt;/code&gt;
    &lt;p&gt;ASCII output:&lt;/p&gt;
    &lt;code&gt;+---+     +---+
|   |     |   |
| A |----&amp;gt;| B |
|   |     |   |
+---+     +---+
&lt;/code&gt;
    &lt;code&gt;renderMermaidAscii(diagram, {
  useAscii: false,      // true = ASCII, false = Unicode (default)
  paddingX: 5,          // Horizontal spacing between nodes
  paddingY: 5,          // Vertical spacing between nodes
  boxBorderPadding: 1,  // Padding inside node boxes
})&lt;/code&gt;
    &lt;p&gt;Render a Mermaid diagram to SVG. Auto-detects diagram type.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;text&lt;/code&gt;— Mermaid source code&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;options&lt;/code&gt;— Optional&lt;code&gt;RenderOptions&lt;/code&gt;object&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RenderOptions:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;bg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#FFFFFF&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;fg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;#27272A&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Foreground color&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;line&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Edge/connector color&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;accent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Arrow heads, highlights&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;muted&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Secondary text, labels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;surface&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Node fill tint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;border&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Node stroke color&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;font&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;Inter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Font family&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;transparent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Render with transparent background&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Render a Mermaid diagram to ASCII/Unicode text. Synchronous.&lt;/p&gt;
    &lt;p&gt;AsciiRenderOptions:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;useAscii&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;boolean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Use ASCII instead of Unicode&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;paddingX&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;number&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Horizontal node spacing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;paddingY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;number&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Vertical node spacing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;boxBorderPadding&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;number&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Inner box padding&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Extract diagram colors from a Shiki theme object.&lt;/p&gt;
    &lt;p&gt;Object containing all 15 built-in themes.&lt;/p&gt;
    &lt;p&gt;Default colors (&lt;code&gt;#FFFFFF&lt;/code&gt; / &lt;code&gt;#27272A&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The ASCII rendering engine is based on mermaid-ascii by Alexander Grooff. We ported it from Go to TypeScript and extended it with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sequence diagram support&lt;/item&gt;
      &lt;item&gt;Class diagram support&lt;/item&gt;
      &lt;item&gt;ER diagram support&lt;/item&gt;
      &lt;item&gt;Unicode box-drawing characters&lt;/item&gt;
      &lt;item&gt;Configurable spacing and padding&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you Alexander for the excellent foundation!&lt;/p&gt;
    &lt;p&gt;MIT — see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Built with care by the team at Craft&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46804828</guid><pubDate>Thu, 29 Jan 2026 02:08:40 +0000</pubDate></item><item><title>Maine’s ‘Lobster Lady’ who fished for nearly a century dies aged 105</title><link>https://www.theguardian.com/us-news/2026/jan/28/maine-lobster-lady-dies-aged-105</link><description>&lt;doc fingerprint="f2911183d4bb9605"&gt;
  &lt;main&gt;
    &lt;p&gt;Maine’s governor has hailed the life of a woman who spent nearly 100 years fishing for lobsters as “amazing” and expressed hopes that her memory inspires “the next century of hardworking” fishers in the state.&lt;/p&gt;
    &lt;p&gt;The subject of Governor Janet Mills’ tribute, Virginia “Ginny” Oliver, died on 21 January at age 105, according to an obituary published on Monday by her family.&lt;/p&gt;
    &lt;p&gt;Some regard stories such as that of Oliver, who came to be known as her state’s “Lobster Lady”, as evidence of the growing number of Americans who extend their working days well past the typical retirement age as the cost of living in the US has soared, wages have stagnated and many therefore have been unable to save.&lt;/p&gt;
    &lt;p&gt;Nonetheless, as recently as 2021, Oliver told the Associated Press she fell in love with trapping lobsters from the moment she started in the business at eight years old, alongside her father and older brother.&lt;/p&gt;
    &lt;p&gt;“I like doing it – I like being along the water,” she said when discussing her career in the largely male-dominated industry she chose. “And so I’m going to keep on doing it just as long as I can.”&lt;/p&gt;
    &lt;p&gt;Oliver would get up before dawn and use small fish colloquially known as poagies to lure lobsters from her boat, the Virginia, which was first owned by her late husband. As she established a remarkable 97-year tenure on the waters, and word of it spread, she became the subject of documentaries, major US television networks’ news stories and children’s books, including one titled The Lobster Lady, her obituary recounted.&lt;/p&gt;
    &lt;p&gt;Mark Hamill, the famed actor, was among those who joined the following that Oliver developed throughout the years. Hamill, best known for his role as Luke Skywalker in the Star Wars film saga, “celebrated her tenacity on social media”, Oliver’s obituary noted.&lt;/p&gt;
    &lt;p&gt;The obituary also said that Oliver at one point earned an honorary invitation to join Great Britain’s Cardiff Royal Naval Association. Mills once presented Oliver with a special recognition on her birthday.&lt;/p&gt;
    &lt;p&gt;“Despite her fame, friends and family said she remained humble and spirited,” Oliver’s obituary added. “Her personal aesthetic delighted her fans – she wore lipstick and earrings every day she went out on the boat, because, as she said, ‘you never know who you are going to see.’”&lt;/p&gt;
    &lt;p&gt;Lobster evolved from working-class food to a pricey restaurant delicacy over the course of Oliver’s fishing life. Its price per pound swelled from 28 cents when she first started to $6.14 – or 22 times more expensive.&lt;/p&gt;
    &lt;p&gt;Oliver fished for lobster until a fall at age 103, said a statement from her friend, author and Pulitzer prize-winning journalist Barbara Walsh.&lt;/p&gt;
    &lt;p&gt;Walsh joined Mills in paying tribute to Oliver, saying the late fisher “believed in living, laughing and doing what she loved”.&lt;/p&gt;
    &lt;p&gt;“She was sassy and spirited, always declaring on land and at sea, ‘I’m the boss,’” Walsh’s tribute statement said. “Sail on, sweet Ginny. May your spirit forever soar above the sea.”&lt;/p&gt;
    &lt;p&gt;Meanwhile, the Maine Lobster festival, which once designated her the grand marshal of its parade, issued a statement honoring Oliver as “more than a local icon”.&lt;/p&gt;
    &lt;p&gt;“Virginia was … a living piece of Maine’s maritime history,” the festival’s statement said.&lt;/p&gt;
    &lt;p&gt;Oliver’s survivors include her children and grandchildren, according to her obituary.&lt;/p&gt;
    &lt;p&gt;Associated Press contributed reporting&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46804854</guid><pubDate>Thu, 29 Jan 2026 02:11:42 +0000</pubDate></item><item><title>Questom (YC F25) is hiring an engineer</title><link>https://www.ycombinator.com/companies/questom/jobs/UBebsyO-founding-engineer</link><description>&lt;doc fingerprint="b5fe30170ec5bb2a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;We’re looking for a Founding Engineer to help build the core systems that power Questom.&lt;/p&gt;
      &lt;p&gt;This is not a framework-specialist role. It’s not about perfect abstractions on day one. It is about systems thinking, ownership, and the ability to stitch together complex infrastructure into something that works — fast.&lt;/p&gt;
      &lt;p&gt;You’ll be one of a very small group shaping not just the product, but how the company builds.&lt;/p&gt;
      &lt;head rend="h2"&gt;What You’ll Do&lt;/head&gt;
      &lt;head rend="h3"&gt;Build systems that connect everything&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Design and build high-performance systems that connect: &lt;list rend="ul"&gt;&lt;item&gt;Communication platforms (voice, SMS, email, chat)&lt;/item&gt;&lt;item&gt;Agentic workflows&lt;/item&gt;&lt;item&gt;External tools like CRMs and internal APIs&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Stitch together APIs, workflows, and agents into cohesive systems that actually run in production&lt;/item&gt;
        &lt;item&gt;Think deeply about how these systems scale across many customers with different data sources and configurations&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Build agentic workflows&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Create workflows where agents: &lt;list rend="ul"&gt;&lt;item&gt;Handle live conversations&lt;/item&gt;&lt;item&gt;Pull context from systems like Salesforce&lt;/item&gt;&lt;item&gt;Log outcomes in tools like HubSpot&lt;/item&gt;&lt;item&gt;Take real actions on behalf of users&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;Focus on giving agents the right information at the right time and designing systems that make this repeatable across customers&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Turn ambiguity into working software&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Start from half-baked ideas, rough customer conversations, or incomplete requirements&lt;/item&gt;
        &lt;item&gt;Ship a 90% working solution quickly&lt;/item&gt;
        &lt;item&gt;Iterate toward something more general, scalable, and high quality&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Own what you build&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Take end-to-end ownership: design → build → ship → improve&lt;/item&gt;
        &lt;item&gt;Break things thoughtfully — and fix them yourself&lt;/item&gt;
        &lt;item&gt;Make judgment calls without needing constant guidance&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;What Success Looks Like&lt;/head&gt;
      &lt;p&gt;In the first 30 days&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Sit in a few customer conversations&lt;/item&gt;
        &lt;item&gt;Identify a real pain point&lt;/item&gt;
        &lt;item&gt;Propose a product or feature to solve it&lt;/item&gt;
        &lt;item&gt;Build and ship an end-to-end solution that delivers real value&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In the first 90 days&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Take learnings from multiple customer builds&lt;/item&gt;
        &lt;item&gt;Start shaping more general-purpose systems&lt;/item&gt;
        &lt;item&gt;Help lay the foundation for a platform that can scale to many customers — and eventually very large companies&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;How We Build&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;We use modern tools aggressively and expect you to learn quickly: &lt;list rend="ul"&gt;&lt;item&gt;Agentic workflow frameworks&lt;/item&gt;&lt;item&gt;Communication infrastructure&lt;/item&gt;&lt;item&gt;AI agent SDKs&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
        &lt;item&gt;We rely heavily on coding agents and automation — curiosity and systems understanding matter more than syntax mastery&lt;/item&gt;
        &lt;item&gt;We build fast, then make it better&lt;/item&gt;
        &lt;item&gt;We’re opinionated, iterative, and comfortable changing direction when a simpler solution delivers customer value&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;What Matters (and What Doesn’t)&lt;/head&gt;
      &lt;head rend="h3"&gt;What matters a lot&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Systems thinking&lt;/item&gt;
        &lt;item&gt;Ownership&lt;/item&gt;
        &lt;item&gt;Comfort with ambiguity&lt;/item&gt;
        &lt;item&gt;Curiosity and fast learning&lt;/item&gt;
        &lt;item&gt;Product taste (what’s worth building vs not)&lt;/item&gt;
        &lt;item&gt;Respect for how other people think&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;What doesn’t matter much&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Deep expertise in any single framework or language&lt;/item&gt;
        &lt;item&gt;Pixel-perfect UI work&lt;/item&gt;
        &lt;item&gt;Writing perfect SQL by hand&lt;/item&gt;
        &lt;item&gt;Knowing our exact stack on day one&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;If you understand how modern web systems, APIs, workflows, and agents fit together — and you’re excited to learn the rest — you’re in great shape.&lt;/p&gt;
      &lt;head rend="h2"&gt;How We Work&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Fast, opinionated, iterative&lt;/item&gt;
        &lt;item&gt;Disagreements are encouraged and happen openly&lt;/item&gt;
        &lt;item&gt;We build first, think later — but we do think&lt;/item&gt;
        &lt;item&gt;You won’t be micromanaged&lt;/item&gt;
        &lt;item&gt;You will be trusted with real responsibility early&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Who You Are&lt;/head&gt;
      &lt;p&gt;You likely describe yourself as:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Extremely curious&lt;/item&gt;
        &lt;item&gt;Highly independent&lt;/item&gt;
        &lt;item&gt;Comfortable operating without a map&lt;/item&gt;
        &lt;item&gt;Someone who enjoys connecting dots across systems&lt;/item&gt;
        &lt;item&gt;High-EQ and collaborative&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;You want to be deeply involved — not just shipping tickets, but helping build a company.&lt;/p&gt;
      &lt;head rend="h2"&gt;Who Should Not Apply&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;If you need clear specs and guardrails to do your best work&lt;/item&gt;
        &lt;item&gt;If you prefer working in isolation&lt;/item&gt;
        &lt;item&gt;If you’re uncomfortable with constant ambiguity&lt;/item&gt;
        &lt;item&gt;If you don’t enjoy discussing ideas, systems, and tradeoffs with a small team&lt;/item&gt;
        &lt;item&gt;If you want a narrow role instead of broad ownership&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;If reading this makes you feel slightly intimidated but very excited, that’s intentional — and probably a good sign.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46805439</guid><pubDate>Thu, 29 Jan 2026 03:29:53 +0000</pubDate></item><item><title>We can't send mail farther than 500 miles (2002)</title><link>https://web.mit.edu/jemorris/humor/500-miles</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46805665</guid><pubDate>Thu, 29 Jan 2026 03:58:33 +0000</pubDate></item><item><title>Why do RSS readers look like email clients?</title><link>https://www.terrygodier.com/phantom-obligation</link><description>&lt;doc fingerprint="2ae2295b3bb629bb"&gt;
  &lt;main&gt;
    &lt;p&gt;There's a particular kind of guilt that visits me when I open my feed reader after a few days away. It's not the guilt of having done something wrong, exactly. It's more like the feeling of walking into a room where people have been waiting for you, except when you look around, the room is empty. There's no one there. There never was.&lt;/p&gt;
    &lt;p&gt;I've been thinking about this feeling for a long time. Longer than I probably should, given that it concerns something as mundane as reading articles on the internet. But I've come to believe that these small, repeated experiences shape us more than we like to admit.&lt;/p&gt;
    &lt;p&gt;So let me start with a question that's been nagging at me: why do RSS readers look like email clients?&lt;/p&gt;
    &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.&lt;/p&gt;
    &lt;p&gt;Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit.&lt;/p&gt;
    &lt;p&gt;The shape is so ubiquitous it feels inevitable.&lt;/p&gt;
    &lt;p&gt;If you've used almost any RSS reader in the past two decades, you know this layout intimately. There's a sidebar with your feeds organized into folders. There's a list of items, sorted by date, with little dots indicating what you haven't read yet. There's a reading pane where the content appears when you click.&lt;/p&gt;
    &lt;p&gt;The shape is so ubiquitous that it feels inevitable. But of course nothing in design is inevitable. Someone made a choice, and then other people followed that choice, and eventually the choice calcified into convention.&lt;/p&gt;
    &lt;p&gt;I know exactly who made that first choice,&lt;lb/&gt;because I asked.&lt;/p&gt;
    &lt;p&gt;His name is Brent Simmons. In 2002 he released NetNewsWire, the app that established the template nearly every RSS reader still follows today.&lt;/p&gt;
    &lt;p&gt;"I know the answer, or at least part of it. I wrote the first one of these. NetNewsWire Lite 1.0 was released in 2002, and it was the first RSS reader to resemble an email app."&lt;/p&gt;
    &lt;p&gt;"I was actually thinking about Usenet, not email, but whatever. The question I asked myself then was how would I design a Usenet app for (then-new) Mac OS X in the year 2002?"&lt;/p&gt;
    &lt;p&gt;"The answer was pretty clear to me: instead of multiple windows, a single window with a sidebar, list of posts, and detail view."&lt;/p&gt;
    &lt;p&gt;He made a pragmatic decision, not an ideological one. RSS was unknown to most people in 2002. By using a familiar layout, something people already understood from email, he reduced the learning curve to almost nothing.&lt;/p&gt;
    &lt;p&gt;It worked. NetNewsWire took off. Google Reader took off. A thousand readers bloomed, and nearly all of them borrowed Brent's basic shape.&lt;/p&gt;
    &lt;p&gt;But here's what struck me about the end of his response:&lt;/p&gt;
    &lt;p&gt;"The part I don't understand and can't explain is why RSS readers are still mostly following this UI."&lt;/p&gt;
    &lt;p&gt;"But every new RSS reader ought to consider not being yet another three-paned-aggregator. There are surely millions of users who might prefer a river of news or other paradigms."&lt;/p&gt;
    &lt;p&gt;"Why not have some fun and do something new, or at least different?"&lt;/p&gt;
    &lt;p&gt;The person who designed the original paradigm was asking, twenty-two years later, why everyone was still copying him.&lt;/p&gt;
    &lt;p&gt;When you dress a new thing in old clothes, people don't just learn the shape. They inherit the feelings, the assumptions, the emotional weight. You can't borrow the layout of an inbox without also borrowing some of its psychology.&lt;/p&gt;
    &lt;p&gt;Nothing happened. Nobody knows you're here.&lt;/p&gt;
    &lt;p&gt;Email's unread count means something specific: these are messages from real people who wrote to you and are, in some cases, actively waiting for your response. The number isn't neutral information. It's a measure of social debt.&lt;/p&gt;
    &lt;p&gt;But when we applied that same visual language to RSS (the unread counts, the bold text for new items, the sense of a backlog accumulating) we imported the anxiety without the cause.&lt;/p&gt;
    &lt;p&gt;Nobody is waiting.&lt;/p&gt;
    &lt;p&gt;I've been trying to find the right name for this phenomenon, and I think I've finally landed on it:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46806245</guid><pubDate>Thu, 29 Jan 2026 05:40:45 +0000</pubDate></item><item><title>Generative Music with the Muse</title><link>https://computerhistory.org/blog/generative-music-with-the-muse/</link><description>&lt;doc fingerprint="e291121859b28cbd"&gt;
  &lt;main&gt;
    &lt;p&gt;Strolling around the Computer History Museum, there are exhibits that are immediately recognizable. All we need is a glimpse of the Altair 8800 or Apple I, and we just… know. We walk over and stand in front of these pieces, instinctively lowering our voices and giving a quiet nod to anyone nearby. Like noticing the chisel marks on a marble statue or the brushstrokes on an oil painting, we’re struck by the realization that what we’re seeing is the result of human imagination and ingenuity. What was once abstract and almost mythical is there right in front of us.&lt;/p&gt;
    &lt;p&gt;However, there are also items on display at the Computer History Museum whose significance isn’t immediately apparent. Take the Triadex Muse. You might mistake this wedge of metal, switches, and wood panels for an obsolete piece of stereo equipment or a Cold War-era intercom.&lt;/p&gt;
    &lt;p&gt;While it looks like something you’d find on some forgotten warehouse shelf, the Triadex Muse is an important piece of electronic music history. Developed by Edward Fredkin and Marvin Minsky at MIT in 1969, and commercially released in the early 1970s, it was the first algorithm-based sequencer/synthesizer intended for home consumers. It’s estimated that there were only 280 – 300 produced, making it a rare piece of gear.&lt;/p&gt;
    &lt;p&gt;You won’t find a friendly and familiar set of piano keys. The only controls are an orderly series of sliders. Its industrial design has more in common with that of a home appliance than a musical instrument. Like some sort of mystical radio receiver, beckoning users to adjust its controls until they land on some strange and alien wavelength.&lt;/p&gt;
    &lt;p&gt;The beauty of the Triadex Muse is in its simplicity. There’s no memory, no CPU, and no firmware. Just integrated circuits and electricity. Unlike the 1959 IBM 7090 mainframe computer, which was fed programming instructions via paper punch cards in coaxing out such party hits like Frère Jacques on the 1962 album Music from Mathematics, the only user input here is positioning sliders and flipping a couple switches.&lt;/p&gt;
    &lt;p&gt;If you were to randomly move the sliders to different positions and fire up the Triadex Muse, you’d likely hear the chirp of a square wave melody from its internal speaker and see a vertical band of blue and green lights twinkling in time. You might think that the Triadex Muse is like a modern step sequencer or drum machine. Orderly and predictable. This is 1972. Forget it.&lt;/p&gt;
    &lt;p&gt;You wouldn’t be wrong that the blinking lights correspond to a beat. At each tick of the Muse’s internal clock, this single column of lights shows the complete state of zeroes and ones. The Muse is essentially a 40 x 8 matrix, with zeroes and ones evaluated at each “tick” of its internal clock, triggering sounds, and with each state potentially changing what happens next.&lt;/p&gt;
    &lt;p&gt;While anyone with a background in computer science would know what these lamps represent, the average home consumer would have no idea. These glittering lights further add to its sense of mystery. Although you do get some control over the Muse’s output, you can’t play it like a regular instrument. Even the owner’s manual admits that, “The Muse isn’t a music box.” Depending on how you set the sliders, it’s unlikely you’re going to come up with a tune you can hum along with while you wash the dishes.&lt;/p&gt;
    &lt;p&gt;While the Muse may sound like a robotic run-on sentence, it speaks in the familiar language of musical intervals.&lt;/p&gt;
    &lt;p&gt;The INTERVAL block has four sliders that select notes from the major scale. The positions of A, B, and C determine the pitch, while D adds an octave. There are no flatted thirds or minor scales. You can't play the blues. Though you might want to if you once owned the Triadex Muse and look up how much they’re currently going for—$$$.&lt;/p&gt;
    &lt;p&gt;The four INTERVAL switches, like much of the Muse, are deceptively straightforward. You might flip several to the same row and think you’ll hear a chord. Once again the Muse doesn’t follow modern conventions.&lt;/p&gt;
    &lt;p&gt;The Muse isn’t polyphonic. There is no harmony or overtones. This is binary arithmetic. Each interval is a weighted four-digit binary number. When summed they generate a 4-bit number that determines a new pitch. Music from mathematics… indeed.&lt;/p&gt;
    &lt;p&gt;The Muse’s intervals are triggered when the positions that they’re set to get a one bit from either the C or B section.&lt;/p&gt;
    &lt;p&gt;If you were to stare at the blue lights marked C ½ to C6 long enough, along with your friends asking you if you were feeling okay, you’d notice that they follow a set pattern. And if you knew how to count in binary (and let’s face it we know there are some of you out there), you’d see that the lamps C1, C2, C4, and C8 form a four-bit counter, cycling from 1 (0001) to 15 (1111) in a continuous loop. C ½ is simply the clock itself, a square wave that turns on and off at a rate linked to the tempo. C3 and C6 form a separate two-bit counter that increments every three cycles. By setting groups of threes against fours adds variety to the sequence.&lt;/p&gt;
    &lt;p&gt;In most instances, if you set the INTERVALS only to C positions, you’d get repeating patterns of tones about as musically exciting as the 1978 memory game Simon (no offense to Simon, which you can also see on display at the CHM).&lt;/p&gt;
    &lt;p&gt;The B region is where the Muse becomes more than a repeating blooper of bleeps, but a generator of pseudo-random chaos. You might observe the flickering green lights occupying B1 – B31, and just when you’ve identified a pattern, the sequence feeds back and morphs into something new.&lt;/p&gt;
    &lt;p&gt;To understand what’s happening, first you need to know what the four THEME sliders actually do. If you think that flipping these will give you a familiar musical preset like rock, jazz, or a rumba, the Muse once again betrays you.&lt;/p&gt;
    &lt;p&gt;Each THEME slider is technically a “tap.” A tap is like a tiny beacon, monitoring the binary state of a specific point in the C or B region, and sending that off to an XNOR logic gate.&lt;/p&gt;
    &lt;p&gt;I know. This sounds complex. But understanding the B register is like finding the answer to a riddle. Once you see it, it’s oh-so-obvious.&lt;/p&gt;
    &lt;p&gt;At every click, the XNOR (Exclusive NOR) logic gate makes a decision based on what it receives from the taps. If the gate receives an even number of ones (including all zeroes), it sends a one to B1. If the number of ones is odd, it sends a zero to B1. In its default state, B1 – B31 will be a band of green.&lt;/p&gt;
    &lt;p&gt;Every decision the Muse makes lurches forward, sometimes syncing up as the same conditions persist, and then suddenly shifting when the inputs change. B1 – B31 is what is known as a Linear Shift Register, an ever-evolving bucket brigade of bits, where a new zero or one is handed to the top, while the bottom bit falls out.&lt;/p&gt;
    &lt;p&gt;The Triadex Muse is a dead-end in the history of electronic music. While one could daisy chain it to other Muses, it used a proprietary I/O. There are no control voltages you could patch into modular synthesizers. And MIDI was a decade away. Its entire ecosystem was The Triadex Muse itself, an external speaker, and if the march of blue and green squares wasn’t enough visual stimulation, you could also buy a light unit with Gaussian-like blurs of psychedelic colors flowing with the beat.&lt;/p&gt;
    &lt;p&gt;Like DNA from extinct species whose fragments persist in modern organisms, the Muse’s influence lives on in algorithmic composition.&lt;/p&gt;
    &lt;p&gt;In a 2001 interview, Sean Booth of Autechre, pioneers of generative electronic music said about their process that, “There’s absolutely nothing random about what we do. There might be a lot of number crunching going on, but there’s nothing random in there.”&lt;/p&gt;
    &lt;p&gt;This is exactly what the Muse was doing in 1972.&lt;/p&gt;
    &lt;p&gt;Marvin Minsky, the co-creator of the Muse, said in his 1981 paper “Music, Mind, and Meaning” that the challenge of composing music is that, “Whatever the intent, control is required or novelty will turn to nonsense.” This philosophy is hardwired into the Muse.&lt;/p&gt;
    &lt;p&gt;The melodies of the Muse could sound familiar. Or even strange. But its output still sounds like music, with underlying rules and logic that our brains detect as patterns, even though the Muse makes it almost impossible to predict what it will play next.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46806505</guid><pubDate>Thu, 29 Jan 2026 06:24:18 +0000</pubDate></item><item><title>Europe's next-generation weather satellite sends back first images</title><link>https://www.esa.int/Applications/Observing_the_Earth/Meteorological_missions/meteosat_third_generation/Europe_s_next-generation_weather_satellite_sends_back_first_images</link><description>&lt;doc fingerprint="f28ce6731ff45f8c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Europe’s next-generation weather satellite sends back first images&lt;/head&gt;
    &lt;p&gt;The first images from the Meteosat Third Generation-Sounder satellite have been shared at the European Space Conference in Brussels, showing how the mission will provide data on temperature and humidity, for more accurate weather forecasting over Europe and northern Africa.&lt;/p&gt;
    &lt;p&gt;The images from Meteosat Third Generation-Sounder (MTG-S) show a full-disc image of Earth as seen from geostationary orbit, about 36 000 km above Earth’s surface. These images were captured on 15 November 2025 by the satellite’s Infrared Sounder instrument.&lt;/p&gt;
    &lt;p&gt;In the ‘temperature’ image (below), the Infrared Sounder used a long-wave infrared channel, which measured Earth’s surface temperature as well as the temperature at the top of clouds. Dark red corresponds to high temperatures, mainly on the warmer land surfaces, while blue corresponds to lower temperatures, typically on the top of clouds.&lt;/p&gt;
    &lt;p&gt;As would be expected, most of the warmest (dark red) areas in this image are on the continents of Africa and South America. In the top-centre of the image, the outline of the coast of western Africa is clearly visible in dark red, with the Cape Verde peninsula, home to Senegal’s capital Dakar, visible as among the warmest areas in this image. In the bottom-right of the image, the western coast of Namibia and South Africa are also visible in red beneath a swirl of cold cloud shown in blue, while the northeast coast of Brazil is visible in dark red on the left of the image.&lt;/p&gt;
    &lt;p&gt;The ‘humidity’ image (below) was captured using the Infrared Sounder’s medium-wave infrared channel, which measures humidity in Earth’s atmosphere. Blue colours correspond to regions in the atmosphere with higher humidity, while red colours correspond to lower humidity in the atmosphere.&lt;/p&gt;
    &lt;p&gt;The outlines of landmasses are not visible in this image. The areas of least atmospheric humidity, shown in dark red, are seen approximately over the Sahara Desert and the Middle East (top of image), while a large area of ‘dry’ atmosphere also covers part of the South Atlantic Ocean (centre of image). Numerous patches of high humidity are seen in dark blue over the eastern part of the African continent as well as in high and low latitudes.&lt;/p&gt;
    &lt;p&gt;Below we see a close-up from MTG-Sounder of the European continent and part of northern Africa. Like the first image above, here we see heat from land surfaces and temperatures at the top of clouds. The heat from the African continent is seen in red in the lower part of the image, while a dark blue weather front covers Spain and Portugal. The Italian peninsula is in the centre of the image.&lt;/p&gt;
    &lt;p&gt;And the animation (below) uses data from the MTG-Sounder satellite to track the eruption of Ethiopia's Hayli Gubbi volcano on 23 November 2025. The background imagery shows surface temperature changes while infrared channels highlight the developing ash plume. The satellite's timely observations enable tracking of the evolving ash plume over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Next-generation weather forecasting&lt;/head&gt;
    &lt;p&gt;MTG is a world-class Earth observation mission developed by the European Space Agency (ESA) with European partners to address scientific and societal challenges. The mission provides game-changing data for forecasting weather and air quality over Europe.&lt;/p&gt;
    &lt;p&gt;The satellite’s geostationary position above the equator means it maintains a fixed position relative to Earth, following the same area on the planet’s surface as we rotate. This enables it to provide coverage of Europe and part of northern Africa on a 15-minute repeat cycle. It supplies new data on temperature and humidity over Europe every 30 minutes, supplying meteorologists with a complete weather picture of the region and complementing data on cloud formation and lightning from the MTG-Imager (MTG-I) satellite.&lt;/p&gt;
    &lt;p&gt;ESA’s Director of Earth Observation Programmes, Simonetta Cheli, said, “Seeing the first Infrared Sounder images from the MTG-Sounder satellite really brings this mission and its potential to life. We expect data from this mission to change the way we forecast severe storms over Europe – and this is very exciting for communities and citizens, as well as for meteorologists and climatologists. As ever, the outstanding work done by our teams in collaboration with long-standing partners, including Eumetsat, the European Commission and dozens of European industry teams, means we now have the ability to predict extreme weather events in more accurate and timely ways than ever before.”&lt;/p&gt;
    &lt;head rend="h4"&gt;A hyperspectral view over Europe&lt;/head&gt;
    &lt;p&gt;The Infrared Sounder instrument on board MTG-S is the first European hyperspectral sounding instrument in geostationary orbit. It is designed to generate a completely new type of data product. It uses interferometric techniques, which analyse miniscule patterns in light waves, to capture data on temperature and humidity, as well as being able to measure wind and trace gases in the atmosphere. The data will eventually be used to generate three-dimensional maps of the atmosphere, helping to improve the accuracy of weather forecasting, especially for nowcasting rapidly evolving storms.&lt;/p&gt;
    &lt;p&gt;“It’s fantastic to see the first images from this groundbreaking mission,” said James Champion, ESA’s MTG Project Manager. “This satellite has been 15 years in development and will revolutionise weather forecasting and especially nowcasting. The ability to vertically profile the full Earth’s disk with a repeat cycle of only 30 minutes for Europe is an incredible accomplishment!”&lt;/p&gt;
    &lt;p&gt;“I’m excited that we can share these first images from the Infrared Sounder, which showcase just a small selection of the 1700 infrared channels continuously acquired by the instrument as it observes Earth,” said Pieter Van den Braembussche, MTG System and Payload Manager at ESA. “By combining all 1700 channels, we will soon be able to generate three dimensional maps of temperature, humidity and even trace gases in the atmosphere. This capability will offer a completely new perspective on Earth’s atmosphere, not previously available in Europe, and is expected to help forecasters predict severe storms earlier than is possible today.”&lt;/p&gt;
    &lt;head rend="h2"&gt;About MTG-Sounder&lt;/head&gt;
    &lt;p&gt;The MTG mission currently has two satellites in orbit: MTG-I and MTG-S. The second Imager will be launched later in 2026.&lt;/p&gt;
    &lt;p&gt;MTG-S was launched on 1 July 2025. Thales Alenia Space is the prime contractor for the overall MTG mission, with OHB Systems responsible for the MTG-Sounder satellite. Mission control and data distribution are managed by Eumetsat.&lt;/p&gt;
    &lt;p&gt;The MTG-S satellite also hosts the Copernicus Sentinel-4 mission, which consists of an ultraviolet, visible and near-infrared (UVN) imaging spectrometer. Sentinel-4 delivered its first images last year.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46806773</guid><pubDate>Thu, 29 Jan 2026 07:07:17 +0000</pubDate></item><item><title>The Only Moat Left Is Knowing Things</title><link>https://growtika.com/blog/authenticity-edge</link><description>&lt;doc fingerprint="3142311b9bb742b9"&gt;
  &lt;main&gt;&lt;p&gt;I run a marketing agency. We use Claude, ChatGPT, Ahrefs, Semrush. Same tools as everyone else. Same access to the same APIs.&lt;/p&gt;&lt;p&gt;This is the honest part: our tools are not our advantage.&lt;/p&gt;&lt;p&gt;I'm obsessed with AI. As a non-native English speaker, for years my ideas were better than my grammar. AI finally closed that gap. It handles the syntax so I can focus on the substance. I use it to remove the friction between my brain and the page, not the friction between the page and your brain. What follows is not an anti-AI take; it's about the parts of creativity that can't be automated.&lt;/p&gt;&lt;p&gt;The data backs this up. 54% of LinkedIn posts are now likely AI-written (Originality.ai). 15% of Reddit posts too, up 146% since 2021. Every competitor has the same capability to generate keyword-optimized, structurally correct, grammatically polished content. In about twelve seconds.&lt;/p&gt;&lt;p&gt;So what's left?&lt;/p&gt;&lt;head rend="h2"&gt;The Inversion&lt;/head&gt;&lt;p&gt;For years, content creation was the bottleneck. Knowing what to write about was relatively easy. Actually producing the content required real investment: research, drafting, editing, iteration.&lt;/p&gt;&lt;p&gt;That equation has flipped.&lt;/p&gt;&lt;p&gt;Production is now trivial. The bottleneck has moved upstream to the input: what you know that isn't in the training data. What you've observed that hasn't been published. What you've learned from doing the work that can't be scraped from the internet.&lt;/p&gt;&lt;p&gt;The question used to be: can you produce enough content? Now it's: do you have anything worth producing?&lt;/p&gt;&lt;head rend="h2"&gt;The Authenticity Test&lt;/head&gt;&lt;p&gt;Not all content needs to pass this filter. Informational articles, how-to guides, reference pages: these serve a purpose. They're somewhere between Wikipedia and a blog post. They cover ground. They answer questions. They're fine.&lt;/p&gt;&lt;p&gt;But what about the content that makes you remembered? The content that gives you a voice? That shapes your character, or your company's character?&lt;/p&gt;&lt;p&gt;For that kind of content, we apply a simple test: could an LLM with access to Google produce this with a single prompt?&lt;/p&gt;&lt;p&gt;If yes, it might serve a coverage function, but it won't differentiate. Someone else will produce the same thing. Probably already has.&lt;/p&gt;&lt;p&gt;If no, because it requires data we collected, patterns we observed, failures we experienced, specific scenarios we've lived through, then there's something there. Not because of how it's written, but because of what it contains.&lt;/p&gt;&lt;head rend="h2"&gt;Proof of Work&lt;/head&gt;&lt;p&gt;There is a second layer to the filter, one that addresses the laziness epidemic.&lt;/p&gt;&lt;p&gt;We are seeing a flood of "one-click content." Articles generated by a single prompt, reviewed for 30 seconds, and shipped. The problem isn't just that they are generic; it's that the reader can feel the lack of effort.&lt;/p&gt;&lt;p&gt;If I subconsciously detect that you spent 12 seconds creating this, why should I invest five minutes reading it?&lt;/p&gt;&lt;p&gt;In the age of AI, difficulty is a feature, not a bug.&lt;/p&gt;&lt;p&gt;We now look for "Proof of Work" signals. We want to see that the author didn't just ask an LLM to "write a blog post," but used AI as a force multiplier for a complex, difficult process. We want to see custom visualizations, interactive elements, or synthesis of multiple disparate sources. Things that require human orchestration.&lt;/p&gt;&lt;head rend="h3"&gt;The "Bookmark" Game&lt;/head&gt;&lt;p&gt;Before publishing, we play a quick game. We score the piece against these four questions. If the answer is "No," we don't ship.&lt;/p&gt;&lt;p&gt;Does this contain a custom visualization, unique dataset, or interactive element that an LLM couldn't hallucinate?&lt;/p&gt;&lt;p&gt;Did you combine at least three distinct sources (e.g., a sales call, support ticket, market report) to reach this conclusion?&lt;/p&gt;&lt;p&gt;Was this physically difficult to write? If it flowed out effortlessly in one go, it's usually fluff.&lt;/p&gt;&lt;p&gt;Would a stranger save this for later? Not just "read and nod," but "save and reference."&lt;/p&gt;&lt;p&gt;This isn't about being anti-AI. It's about respect for the reader. If you want attention in a noisy world, you have to pay for it with effort.&lt;/p&gt;&lt;head rend="h2"&gt;What Actually Differentiates&lt;/head&gt;&lt;p&gt;We work with B2B software companies, mostly cybersecurity and developer tools. The patterns we've seen that actually create content advantage:&lt;/p&gt;&lt;p&gt;Internal data nobody else has. Benchmarks from real deployments. Aggregated patterns from customer implementations. Performance metrics that require access to production systems.&lt;/p&gt;&lt;p&gt;Documented failures. What we tried that didn't work, and why. This is oddly hard for AI to generate convincingly because it requires having actually tried things.&lt;/p&gt;&lt;p&gt;Opinions with receipts. A position backed by specific experience. Not "we believe X" but "we ran Y for 18 months and X is what happened."&lt;/p&gt;&lt;p&gt;Access-dependent insights. Conversations with practitioners. Observations from inside client organizations. Context that requires being in the room.&lt;/p&gt;&lt;head rend="h2"&gt;The Zero-Volume Keyword Problem&lt;/head&gt;&lt;p&gt;Here's something we learned the hard way over the past year: everyone sees the same data in SEO tools. Same keyword volumes. Same difficulty scores. Same "opportunity" lists.&lt;/p&gt;&lt;p&gt;Which means everyone targets the same terms.&lt;/p&gt;&lt;p&gt;The actual gold is in conversations that never show up in Ahrefs or Semrush. Sales calls. Support tickets. Board meetings. These are where you hear the language prospects actually use, the specific problems they're trying to solve, the emerging industry terms that haven't hit search volume yet.&lt;/p&gt;&lt;p&gt;We've seen this pattern repeatedly: a term shows "0 volume" in every tool, but it's the exact phrase a CISO uses when they have budget and urgency. Those searches convert at 10x the rate of high-volume head terms. The tools can't see them because there aren't enough searches to register. But each search represents someone ready to buy.&lt;/p&gt;&lt;p&gt;The secret is that these three layers complete each other. High-volume content builds domain authority and captures top-of-funnel awareness. Authority content establishes expertise and earns backlinks. Zero-volume content from internal insights converts the buyers who know exactly what they need.&lt;/p&gt;&lt;p&gt;Skip the bottom layer and you have no foundation. Skip the top layer and you're leaving money on the table. Most companies over-invest in volume and under-invest in the content that comes from actually talking to customers.&lt;/p&gt;&lt;p&gt;If your competitive advantage is "we write good content," you don't have a competitive advantage. Neither do we. The advantage comes from having something to write about that others don't have access to.&lt;/p&gt;&lt;head rend="h2"&gt;The Tools Question&lt;/head&gt;&lt;p&gt;Someone will ask: if AI tools aren't the moat, why use them?&lt;/p&gt;&lt;p&gt;Because they're infrastructure now. Like spreadsheets. Like email. Using them isn't an advantage, but not using them is a disadvantage. They handle the production part so we can focus on the part that actually matters: acquiring the novel input that makes content worth creating.&lt;/p&gt;&lt;p&gt;We're not against AI tools. We use them constantly. What we're against is the idea that using them well is a strategy. It's a baseline.&lt;/p&gt;&lt;head rend="h3"&gt;The Takeaway&lt;/head&gt;&lt;p&gt;The moat isn't the tool. It's what you feed it. And the only thing worth feeding it is knowledge you've earned by doing work that hasn't been scraped, indexed, and trained on yet.&lt;/p&gt;&lt;head rend="h4"&gt;Yuval Halevi&lt;/head&gt;&lt;p&gt;Helping SaaS companies and developer tools get cited in AI answers since before it was called "GEO." 10+ years in B2B SEO, 50+ cybersecurity and SaaS tools clients.&lt;/p&gt;&lt;head rend="h3"&gt;Related Articles&lt;/head&gt;&lt;head rend="h4"&gt;15 Content Strategies That Rank in Search and Get Cited by AI&lt;/head&gt;&lt;p&gt;Tested content strategies that work in 2026. From self-contained sections to extraction-ready formats.&lt;/p&gt;AI Visibility&lt;head rend="h4"&gt;AI Visibility for B2B SaaS: 15 Best Practices That Actually Work&lt;/head&gt;&lt;p&gt;Field-tested tactics for getting cited by ChatGPT, Claude, and Perplexity.&lt;/p&gt;Product-Led Growth&lt;head rend="h4"&gt;Your PLG Strategy Has an AI Blind Spot&lt;/head&gt;&lt;p&gt;PLG companies obsess over activation metrics but miss where the real shortlist forms: AI conversations.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46806959</guid><pubDate>Thu, 29 Jan 2026 07:32:19 +0000</pubDate></item><item><title>OpenAI's Unit Economics</title><link>https://www.exponentialview.co/p/inside-openais-unit-economics-epoch-exponentialview</link><description>&lt;doc fingerprint="2b1e8d784bbeb7d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;👀 Inside OpenAI's unit economics&lt;/head&gt;
    &lt;head rend="h3"&gt;A case study by Exponential View x Epoch AI&lt;/head&gt;
    &lt;p&gt;AI companies are being priced into the hundreds of billions. That forces one awkward question to the front: do the unit economics actually work?&lt;/p&gt;
    &lt;p&gt;Jevons’ paradox suggests that as tokens get cheaper, demand explodes. You’ve likely felt some version of this in the last year. But as usage grows, are these models actually profitable to run?&lt;/p&gt;
    &lt;p&gt;In our collaboration with Epoch AI, we tackle that question using OpenAI’s GPT-5 as the case study. What looks like a simple margin calculation is closer to a forensic exercise: we triangulate reported details, leaks, and Sam Altman’s own words to bracket plausible revenues and costs.&lt;/p&gt;
    &lt;p&gt;Here’s the breakdown.&lt;/p&gt;
    &lt;p&gt;— Azeem&lt;/p&gt;
    &lt;head rend="h2"&gt;Can AI companies become profitable?&lt;/head&gt;
    &lt;head rend="h4"&gt;Lessons from GPT-5’s economics&lt;/head&gt;
    &lt;p&gt;Originally published on Epoch AI’s blog. Analysis by Jaime Sevilla, Exponential View’s Hannah Petrovic, and Anson Ho&lt;/p&gt;
    &lt;p&gt;Are AI models profitable? If you ask Sam Altman and Dario Amodei, the answer seems to be yes — it just doesn’t appear that way on the surface.&lt;/p&gt;
    &lt;p&gt;Here’s the idea: running each AI model generates enough revenue to cover its own R&amp;amp;D costs. But that surplus gets outweighed by the costs of developing the next big model. So, despite making money on each model, companies can lose money each year.&lt;/p&gt;
    &lt;p&gt;This is big if true. In fast-growing tech sectors, investors typically accept losses today in exchange for big profits down the line. So if AI models are already covering their own costs, that would paint a healthy financial outlook for AI companies.&lt;/p&gt;
    &lt;p&gt;But we can’t take Altman and Amodei at their word — you’d expect CEOs to paint a rosy picture of their company’s finances. And even if they’re right, we don’t know just how profitable models are.&lt;/p&gt;
    &lt;p&gt;To shed light on this, we looked into a notable case study: using public reporting on OpenAI’s finances,1 we made an educated guess on the profits from running GPT-5, and whether that was enough to recoup its R&amp;amp;D costs. Here’s what we found:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Whether OpenAI was profitable to run depends on which profit margin you’re talking about. If we subtract the cost of compute from revenue to calculate the gross margin (on an accounting basis),2 it seems to be about 50% — lower than the norm for software companies (where 60-80% is typical) but still higher than many industries.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;But if you also subtract other operating costs, including salaries and marketing, then OpenAI most likely made a loss, even without including R&amp;amp;D.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Moreover, OpenAI likely failed to recoup the costs of developing GPT-5 during its 4-month lifetime. Even using gross profit, GPT-5’s tenure was too short to bring in enough revenue to offset its own R&amp;amp;D costs. So if GPT-5 is at all representative, then at least for now, developing and running AI models is loss-making.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This doesn’t necessarily mean that models like GPT-5 are a bad investment. Even an unprofitable model demonstrates progress, which attracts customers and helps labs raise money to train future models — and that next generation may earn far more. What’s more, the R&amp;amp;D that went into GPT-5 likely informs future models like GPT-6. So these labs might have a much better financial outlook than it might initially seem.&lt;/p&gt;
    &lt;p&gt;Let’s dig into the details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part I: How profitable is running AI models?&lt;/head&gt;
    &lt;p&gt;To answer this question, we consider a case study which we call the “GPT-5 bundle”.3 This includes all of OpenAI’s offerings available during GPT-5’s lifetime as the flagship model — GPT-5 and GPT-5.1, GPT-4o, ChatGPT, the API, and so on.4 We then estimate the revenue and costs of running the bundle.5&lt;/p&gt;
    &lt;p&gt;Revenue is relatively straightforward: since the bundle includes all of OpenAI’s models, this is just their total revenue over GPT-5’s lifetime, from August to December last year.6 This works out to $6.1 billion.7&lt;/p&gt;
    &lt;p&gt;At first glance, $6.1 billion sounds healthy, until you juxtapose it with the costs of running the GPT-5 bundle. These costs come from four main sources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Inference compute: $3.2 billion. This is based on public estimates of OpenAI’s total inference compute spend in 2025, and assuming that the allocation of compute during GPT-5’s tenure was proportional to the fraction of the year’s revenue raised in that period.8&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Staff compensation: $1.2 billion, which we can back out from OpenAI staff counts, reports on stock compensation, and things like H1B filings. One big uncertainty with this: how much of the stock compensation goes toward running models, rather than R&amp;amp;D? We assume 40%, matching the fraction of compute that goes to inference. Whether staffing follows the same split is uncertain, but it’s our best guess.910&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sales and marketing (S&amp;amp;M): $2.2 billion, assuming OpenAI’s spending on this grew between the first and second halves of last year.1112&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Legal, office, and administrative costs: $0.2 billion, assuming this grew between 1.6× and 2× relative to their 2024 expenses. This accounts for office expansions, new office setups, and rising administrative costs with their growing workforce.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So what are the profits? One option is to look at gross profits. This only counts the direct cost of running a model, which in this case is just the inference compute cost of $3.2 billion. Since the revenue was $6.1 billion, this leads to a profit of $2.9 billion, or gross profit margin of 48%, and in line with other estimates.13 This is lower than other software businesses (typically 70-80%) but high enough to eventually build a business on.&lt;/p&gt;
    &lt;p&gt;On the other hand, if we add up all four cost types, we get close to $6.8 billion. That’s somewhat higher than the revenue, so on these terms the GPT-5 bundle made an operating loss of $0.7 billion, with an operating margin of -11%.14&lt;/p&gt;
    &lt;p&gt;Stress-testing the analysis with more aggressive or conservative assumptions doesn’t change the picture much:15&lt;/p&gt;
    &lt;p&gt;And there’s one more hiccup: OpenAI signed a deal with Microsoft to hand over about 20% of their $6.1 billion revenue,16 making their losses even larger still.17 This doesn’t mean that the revenue deal is entirely harmful to OpenAI — for example, Microsoft also shares revenue back to OpenAI.18 And the deal probably shouldn’t significantly affect how we see model profitability — it seems more to do with OpenAI’s economic structure rather than something fundamental to AI models. But the fact that OpenAI and Microsoft have been renegotiating this deal suggests it’s a real drag on OpenAI’s path to profitability.&lt;/p&gt;
    &lt;p&gt;In short, running AI models is likely profitable in the sense of having decent gross margins. But OpenAI’s operating margin, which includes marketing and staffing, is likely negative. For a fast-growing company, though, operating margins can be misleading — S&amp;amp;M costs typically grow sublinearly with revenue, so gross margins are arguably a better proxy for long-run profitability.&lt;/p&gt;
    &lt;p&gt;So our numbers don’t necessarily contradict Altman and Amodei yet. But so far we’ve only seen half the story — we still need to account for R&amp;amp;D costs, which we’ll turn to now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part II: Are models profitable over their lifecycle?&lt;/head&gt;
    &lt;p&gt;Let’s say we buy the argument that we should look at gross margins. On those terms, it was profitable to run the GPT-5 bundle. But was it profitable enough to recoup the costs of developing it?&lt;/p&gt;
    &lt;p&gt;In theory, yes — you just have to keep running them, and sooner or later you’ll earn enough revenue to recoup these costs. But in practice, models might have too short a lifetime to make enough revenue. For example, they could be outcompeted by products from rival labs, forcing them to be replaced.&lt;/p&gt;
    &lt;p&gt;So to figure out the answer, let’s go back to the GPT-5 bundle. We’ve already figured out its gross profits to be around $3 billion. So how do these compare to its R&amp;amp;D costs?&lt;/p&gt;
    &lt;p&gt;Estimating this turns out to be a finicky business. We estimate that OpenAI spent $16 billion on R&amp;amp;D in 2025,19 but there’s no conceptually clean way to attribute some fraction of this to the GPT-5 bundle. We’d need to make several arbitrary choices: should we count the R&amp;amp;D effort that went into earlier reasoning models, like o1 and o3? Or what if experiments failed, and didn’t directly change how GPT-5 was trained? Depending on how you answer these questions, the development cost could vary significantly.&lt;/p&gt;
    &lt;p&gt;But we can still do an illustrative calculation: let’s conservatively assume that OpenAI started R&amp;amp;D on GPT-5 after o3’s release last April. Then there’d still be four months between then and GPT-5’s release in August,20 during which OpenAI spent around $5 billion on R&amp;amp;D.21 But that’s still higher than the $3 billion of gross profits. In other words, OpenAI spent more on R&amp;amp;D in the four months preceding GPT-5, than it made in gross profits during GPT-5’s four-month tenure.&lt;/p&gt;
    &lt;p&gt;So in practice, it seems like model tenures might indeed be too short to recoup R&amp;amp;D costs. Indeed, GPT-5’s short tenure was driven by external competition — Gemini 3 Pro had arguably surpassed the GPT-5 base model within three months.&lt;/p&gt;
    &lt;p&gt;One way to think about this is to treat frontier models like rapidly-depreciating infrastructure: their value must be extracted before competitors or successors render them obsolete. So to evaluate AI products, we need to look at both profit margins in inference as well as the time it takes for users to migrate to something better. In the case of the GPT-5 bundle, we find that it’s decidedly unprofitable over its full lifecycle, even from a gross margin perspective.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part III: Will AI models become profitable?&lt;/head&gt;
    &lt;p&gt;So the finances of the GPT-5 bundle are less rosy than Altman and Amodei suggest. And while we don’t have as much direct evidence on other models from other labs, they’re plausibly in a similar boat — for instance, Anthropic has reported similar gross margins to OpenAI. So it’s worth thinking about what it means if the GPT-5 bundle is at all representative of other models.&lt;/p&gt;
    &lt;p&gt;The most crucial point is that these model lifecycle losses aren’t necessarily cause for alarm. AI models don’t need to be profitable today, as long as companies can convince investors that they will be in the future. That’s standard for fast-growing tech companies.&lt;/p&gt;
    &lt;p&gt;Early on, investors value growth over profit, believing that once a company has captured the market, they’ll eventually figure out how to make it profitable. The archetypal example of this is Uber — they accumulated a $32.5 billion deficit over 14 years of net losses, before their first profitable year in 2023. By that measure, OpenAI is thriving: revenues are tripling annually, and projections show continued growth. If that trajectory holds, profitability looks very likely.&lt;/p&gt;
    &lt;p&gt;And there are reasons to even be really bullish about AI’s long-run profitability — most notably, the sheer scale of value that AI could create. Many higher-ups at AI companies expect AI systems to outcompete humans across virtually all economically valuable tasks. If you truly believe that in your heart of hearts, that means potentially capturing trillions of dollars from labor automation. The resulting revenue growth could dwarf development costs even with thin margins and short model lifespans.&lt;/p&gt;
    &lt;p&gt;That’s a big leap, and some investors won’t buy the vision. Or they might doubt that massive revenue growth automatically means huge profits — what if R&amp;amp;D costs scale up like revenue? These investors might pay special attention to the profit margins of current AI, and want a more concrete picture of how AI companies could be profitable in the near term.&lt;/p&gt;
    &lt;p&gt;There’s an answer for these investors, too. Even if you doubt that AI will become good enough to spark the intelligence explosion or double human lifespans, there are still ways that AI companies could turn a profit. For example, OpenAI is now rolling out ads to some ChatGPT users, which could add between $2 to 15 billion in yearly revenue even without any user growth.22 They’re moving beyond individual consumers and increasingly leaning on enterprise adoption. Algorithmic innovations mean that running models could get many times cheaper each year, and possibly much faster. And there’s still a lot of room to grow their user base and usage intensity — for example, ChatGPT has close to a billion users, compared to around six billion internet users. Combined, these could add many tens of billions of revenue.&lt;/p&gt;
    &lt;p&gt;It won’t necessarily be easy for AI companies to do this, especially because individual labs will need to come face-to-face with AI’s “depreciating infrastructure” problem. In practice, the “state-of-the-art” is often challenged within months of a model’s release, and it’s hard to make a profit from the latest GPT if Claude and Gemini keep drawing users away.&lt;/p&gt;
    &lt;p&gt;But this inter-lab competition doesn’t stop all AI models from being profitable. Profits are often high in oligopolies because consumers have limited alternatives to switch to. One lab could also pull ahead because they have some kind of algorithmic “secret sauce”, or they have more compute.23 Or they develop continual learning techniques that make it harder for consumers to switch between model providers.&lt;/p&gt;
    &lt;p&gt;These competitive barriers can also be circumvented. Companies could form their own niches, and we’ve already seen that to some degree: Anthropic is pursuing something akin to a “code is all you need” mission, Google DeepMind wants to “solve intelligence” and use that to solve everything from cancer to climate change, and Meta strives to make AI friends too cheap to meter. This lets individual companies gain revenue for longer.&lt;/p&gt;
    &lt;p&gt;So will AI models (and hence AI companies) become profitable? We think it’s very possible. While our analysis of the GPT-5 bundle is more conservative than Altman and Amodei hint at, what matters more is the trend: Compute margins are falling, enterprise deals are stickier, and models can stay relevant longer than the GPT-5 cycle suggests.&lt;/p&gt;
    &lt;p&gt;Authors’ note: We’d like to thank JS Denain, Josh You, David Owen, Yafah Edelman, Ricardo Pimentel, Marija Gavrilov, Caroline Falkman Olsson, Lynette Bye, Jay Tate, Dwarkesh Patel, Juan García, Charles Dillon, Brendan Halstead, Isabel Johnson and Markov Gray for their feedback and support on this post. Special thanks to Azeem Azhar for initiating this collaboration and vital input, and Benjamin Todd for in-depth feedback and discussion.&lt;/p&gt;
    &lt;p&gt;Our main sources of information include claims by OpenAI and their staff, and reporting by The Information, CNBC and the Wall Street Journal. We’ve linked our primary sources through the document.&lt;/p&gt;
    &lt;p&gt;Technically, gross margins should also include staff costs that were essential to delivering the product, such as customer service. But these are likely a small fraction of salaries, which are in turn dominated by compute costs — so it won’t affect our analysis much, as we’ll see.&lt;/p&gt;
    &lt;p&gt;We focus on OpenAI models because we have the most financial data available on them.&lt;/p&gt;
    &lt;p&gt;Should we include Sora 2 in this bundle? You could argue that we shouldn’t, because it runs on its own platform and is heavily subsidized to kickstart a new social network, making its economics quite different. However, we find that it’s likely a rounding error for revenues, since people don’t use it much. In particular, the Sora app had close to 9 million downloads by December, compared to around 900 million weekly active users of ChatGPT.&lt;/p&gt;
    &lt;p&gt;Now, while it likely didn’t make much revenue, it might have been costly to serve — apparently making TikTok-esque AI short-form videos using Sora 2 cost OpenAI several hundred million dollars. Here’s a rough estimate: In November (when app downloads peaked), Sora 2 had “almost seven million generations happening a day”. Assuming generations were proportional to weekly active users over time, this would mean 330 million videos in total. The API cost is $0.1/s, so if the average video was 10s long, and assuming the API compute profit margin was 20%, this adds up to 330 million × $0.1 × 10 / 1.2 ≈ $250 million. This is significant, but it’s minor compared to OpenAI’s overall inference compute spend.&lt;/p&gt;
    &lt;p&gt;Ideally we’d have only looked at a single model, but we only have data on costs and revenues at the company-level, not at the release-level, so we do the next best thing.&lt;/p&gt;
    &lt;p&gt;For the purposes of this post, we assume that GPT-5’s lifetime started when GPT-5 was released (Aug 7th) and ended when GPT-5.2 was released (Dec 11th). That might seem a bit odd — after all, isn’t GPT-5.2 based on GPT-5? We thought so too, but GPT-5.2 has a new knowledge cutoff, and is apparently “built on a new architecture”, so it might have a different base model from the other models under the GPT-5 moniker.&lt;/p&gt;
    &lt;p&gt;Admittedly, we don’t know for sure that GPT-5.2 uses a different base model, but it’s a convenient way to bound the timeframe of our analysis. And it shouldn’t matter much for our estimates of profit margins, because we’re simply comparing revenues and costs over the same time period.&lt;/p&gt;
    &lt;p&gt;Also note that GPT-5 and GPT-5.1 are still available through ChatGPT and OpenAI’s API, so their useful life hasn’t strictly ended. We assume, for simplicity, that usage has been largely displaced by GPT-5.2.&lt;/p&gt;
    &lt;p&gt;In July, OpenAI had its first month with over $1 billion in revenue, and it closed the year with an annualized revenue of over $20 billion ($1.7 billion per month). If this grew exponentially, the average revenue over the four months of GPT-5’s tenure would’ve been close to $1.5 billion, giving a total of $6 billion during the period.&lt;/p&gt;
    &lt;p&gt;Last year, OpenAI earned about $13 billion in full-year revenue, compared to $6.1 billion for the GPT-5 bundle. At the same time, they spent around $7 billion running all models last year, so if we assume revenue and inference compute are proportional throughout the year, they spent 6.1 billion / 13 billion × 7 billion ≈ $3.3 billion. But in practice, these likely didn’t grow proportionally, because the compute profit margin for paid users increased from 56% in January to 68% in October. This means that inference grew cheaper relative to revenue, saving about 10% in costs, which is $300 million-ish (importantly, both free and paying users grew around 2.6× over this period from January to October).&lt;/p&gt;
    &lt;p&gt;This is then offset by an additional $200 million from other sources of IT spending, including e.g., servers and networking equipment. The total is then still around $3.3 billion - $0.3 billion + $0.2 billion = $3.2 billion.&lt;/p&gt;
    &lt;p&gt;H1B filings suggest an average base salary of $310,000 in 2025, ranging from $150,000 to $685,000. This seems broadly consistent with data from levels.fyi, which reports salaries ranging from $144,275 to $1,274,139 as we’re writing this. Overall, let’s go with an average of $310,000 plus around 40% in benefits. We also know that OpenAI’s staff count surged from 3,000 in mid-2025 to 4,000 by the end of 2025. We smoothly interpolate between these to get an average staff count of 3,500 employees during GPT-5’s lifetime.&lt;/p&gt;
    &lt;p&gt;Then the base salary comes to: 3,500 employees × $310,000 base salary × 1.4 benefits × 40% share of employees working on serving GPT-5 × 127 / 365 period serving ≈ $0.2 billion (the 127 comes from the number of days in GPT-5’s lifetime).&lt;/p&gt;
    &lt;p&gt;We then need to account for stock compensation. In 2025, OpenAI awarded $6 billion to employees in stock compensation. Assuming they awarded them proportionally to staff count over the year, and given the exponential increase of staff counts, that would indicate that over 42% of the stock was awarded during GPT-5’s lifetime. Assuming 40% goes to operations as before, that results in $6 billion x 42% x 40% = $1 billion stock expense for operating the GPT-5 bundle. The total staff compensation would then be around $1.2 billion.&lt;/p&gt;
    &lt;p&gt;It’s debatable whether the very high compensation packages for technical staff will continue as the industry matures.&lt;/p&gt;
    &lt;p&gt;In the first half of 2025, OpenAI spent $2 billion on S&amp;amp;M, which we can convert into a daily rate of $11 million per day. This grew over time (S&amp;amp;M spending doubled from 2024 to H1 2025), so the average pace over GPT-5’s lifetime is higher (we estimate about $17 million a day). If we multiply this by the 127 days in the window, we get a rough total of $2.2 billion.&lt;/p&gt;
    &lt;p&gt;This corresponds to around 30% of revenue during the period, which isn’t unusual compared to other large software companies. For example, Adobe, Intuit, Salesforce and ServiceNow all spent around 27% to 35% of their 2024-2025 revenue in S&amp;amp;M. That said, there are certainly examples with lower spends — for example, Microsoft and Oracle spend 9 to 15% of their revenue on marketing, though note that these are relatively mature firms — younger firms may spend higher fractions on S&amp;amp;M.&lt;/p&gt;
    &lt;p&gt;Last year, OpenAI reported a gross profit margin of 48%, which is consistent with our estimates. From the same article, Anthropic expects a similar gross profit margin, suggesting this might be representative of the industry.&lt;/p&gt;
    &lt;p&gt;How does this compare to previous years? The Information reported that in 2024 OpenAI made $4 billion in revenue, and spent $2.4 billion in inference compute and hosting, $700 million in employee salaries, $600 million in G&amp;amp;A, and $300 million in S&amp;amp;M. This implies a gross margin of 40% and an operating margin of 0% (excluding stock compensation).&lt;/p&gt;
    &lt;p&gt;In broad strokes, we perform a sensitivity analysis by considering a range of possible values for each cost component, then sampling from each to consider a range of plausible scenarios (a Monte Carlo analysis). The largest uncertainties that feed into this analysis are how much staff compensation goes to inference instead of R&amp;amp;D, S&amp;amp;M spending in the second half of 2025, and revenue during GPT-5’s tenure.&lt;/p&gt;
    &lt;p&gt;Two more caveats to add: first, this 20% rate isn’t publicly confirmed by OpenAI or Microsoft, at least in our knowledge. Second, the revenue sharing agreement is also more complex than just this number. Microsoft put a lot of money and compute into OpenAI, and in return it gets a significant ownership stake, special rights to use OpenAI’s technology, and some of OpenAI’s revenue. There also isn’t a single well-defined “end date”: some rights are set to last into the early 2030s, while other parts (including revenue sharing) continue until an independent panel confirms OpenAI has reached “AGI”.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, a revenue share agreement is often seen as an expense that would impact gross margins. But we’re more interested in the unit economics that generalize across models, rather than those that are unique to OpenAI’s financial situation.&lt;/p&gt;
    &lt;p&gt;The deal was signed in 2019, a year before GPT-3 was released, and at this time it may have been an effective way to access compute resources and get commercial distribution. This could’ve been important for OpenAI to develop GPT-5 in the first place.&lt;/p&gt;
    &lt;p&gt;OpenAI’s main R&amp;amp;D spending is on compute, salaries and data. In 2025, they spent $9 billion on R&amp;amp;D AI compute, and about $1 billion on data (which includes paying for human experts and RL environments). We can estimate salary payouts in the same way we did in the previous section on inference, except we consider 60% of staff compensation rather than 40%, resulting in an expense of $4.6 billion. Finally, we add about $400 million in offices and administrative expenses, and $600 million in other compute costs (including e.g. networking costs). This adds up to about $16 billion.&lt;/p&gt;
    &lt;p&gt;In fact, we could be substantially lowballing the R&amp;amp;D costs. GPT-5 has been in the works for a long time — for example, early reasoning models like o1 probably helped develop GPT-5’s reasoning abilities. GPT-5.1 was probably being developed between August and November, covering a good chunk of the GPT-5 bundle’s tenure. But there’s a countervailing consideration: some of the R&amp;amp;D costs for GPT-5 probably help develop future models like “GPT-6”. So it’s hard to say what the exact numbers are, but we’re pretty confident that our overall point still stands.&lt;/p&gt;
    &lt;p&gt;Because OpenAI’s expenses are growing exponentially, we can’t just estimate the share of R&amp;amp;D spending in this period as one-third of the annual total. Assuming a 2.3× annual growth rate in R&amp;amp;D expenses — comparable to the increase in OpenAI’s R&amp;amp;D compute spending from 2024 to 2025 — the costs incurred between April 16 and August 7 would account for approximately 35% of total yearly R&amp;amp;D expenses.&lt;/p&gt;
    &lt;p&gt;OpenAI was approaching 900 million weekly active users in December last year. For ads, they project a revenue of $2 per free user for 2026, and up to $15 per free user for 2030. Combining these numbers gives our estimate of around $2 billion to $15 billion.&lt;/p&gt;
    &lt;p&gt;For the investors who are willing to entertain more extreme scenarios, an even stronger effect is when “intelligence explosion” dynamics kick in — if OpenAI pulls ahead at the right time, they could use their better AIs to accelerate their own research, amplifying a small edge into a huge lead. This might sound like science fiction to a lot of readers, but representatives from some AI companies have publicly set these as goals. For instance, Sam Altman claims that one of OpenAI’s goals is to have a “true automated AI researcher” by March 2028.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46807201</guid><pubDate>Thu, 29 Jan 2026 08:11:51 +0000</pubDate></item></channel></rss>