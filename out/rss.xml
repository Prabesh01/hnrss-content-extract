<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 11 Dec 2025 20:13:15 +0000</lastBuildDate><item><title>Patterns.dev</title><link>https://www.patterns.dev/</link><description>&lt;doc fingerprint="ed186110298694bb"&gt;
  &lt;main&gt;
    &lt;p&gt;Interested in our next book? Learn more about Building Large-scale JavaScript Web Apps with React&lt;/p&gt;
    &lt;p&gt;Patterns.dev is a free online resource on design, rendering, and performance patterns for building powerful web apps with vanilla JavaScript or modern frameworks.&lt;/p&gt;
    &lt;p&gt;We publish patterns, tips and tricks for improving how you architect apps for free. Keep in mind, design patterns are descriptive, not prescriptive . They can guide you when facing a problem other developers have encountered many times before, but are not a blunt tool for jamming into every scenario. Patterns.dev aims to be a catalog of patterns (for increasing awareness) rather than a checklist (what you must do).&lt;/p&gt;
    &lt;p&gt;Design patterns are a fundamental part of software development, as they provide typical solutions to commonly recurring problems in software design.&lt;/p&gt;
    &lt;p&gt;A common critique of design patterns is that they needlessly add complexity.&lt;/p&gt;
    &lt;p&gt;Our perspective is that patterns are valuable for solving specific problems, often helping to communicate comminalities in code problems for humans. If a project doesn't have those problems, there isn't a need to apply them. Patterns can also be very language or framework-specific (e.g. React), which can often mean thinking beyond the scope of just the original GoF design patterns.&lt;/p&gt;
    &lt;p&gt;Learn about web performance patterns for loading your code more efficiently. Unsure how to think about modern approaches to loading or rendering user-experiences? We've got you covered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46226483</guid><pubDate>Thu, 11 Dec 2025 01:18:55 +0000</pubDate></item><item><title>The Cost of a Closure in C</title><link>https://thephd.dev/the-cost-of-a-closure-in-c-c2y</link><description>&lt;doc fingerprint="ee43ba392a6d308c"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a vague idea that closures could have a variety of performance implications; I did not believe that so many of the chosen and potential designs for C and C++ extensions ones, however, were so… suboptimal.&lt;/p&gt;
    &lt;p&gt;But, before we get into how these things perform and what the cost of their designs are, we need to talk about what Closures are.&lt;/p&gt;
    &lt;head rend="h1"&gt;“Closures”?&lt;/head&gt;
    &lt;p&gt;Closures in this instance are programming language constructs that includes data alongside instructions that are not directly related to their input (arguments) and their results (return values). They can be seen as a “generalization” of the concept of a function or function call, in that a function call is a “subset” of closures (e.g., the set of closures that do not include this extra, spicy data that comes from places outside of arguments and returns). These generalized functions and generalized function objects hold the ability to do things like work with “instance” data that is not passed to it directly (i.e., variables surrouding the closure off the stack) and, usually, some way to carry around more data than is implied by their associated function signature.&lt;/p&gt;
    &lt;p&gt;Pretty much all recent and modern languages include something for Closures unless they are deliberately developing for a target audience or for a source code design that is too “low level” for such a concept (such as Stack programming languages, Bytecode languages, or ones that fashion themselves as assembly-like or close to it). However, we’re going to be focusing on and looking specifically at Closures in C and C++, since this is going to be about trying to work with and – eventually – standardize something for ISO C that works for everyone.&lt;/p&gt;
    &lt;p&gt;First, let’s show a typical problem that arises in C code to show why closure solutions have popped up all over the C ecosystem, then talk about it in the context of the various solutions.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Closure Problem&lt;/head&gt;
    &lt;p&gt;The closure problem can be neatly described by as “how do I get extra data to use within this &lt;code&gt;qsort&lt;/code&gt; call?”. For example, consider setting this variable, &lt;code&gt;in_reverse&lt;/code&gt;, as part of a bit of command line shenanigans, to change how a sort happens:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

static int in_reverse = 0;

int compare(const void* untyped_left, const void* untyped_right) {
  const int* left = untyped_left;
  const int* right = untyped_right;
  return (in_reverse) ? *right - *left : *left - *right;
}

int main(int argc, char* argv[]) {
  if (argc &amp;gt; 1) {
    char* r_loc = strchr(argv[1], 'r');
    if (r_loc != NULL) {
      ptrdiff_t r_from_start = (r_loc - argv[1]);
      if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
        in_reverse = 1;
      } 
    }
  }
  int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
  qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
  return list[0];
}
&lt;/code&gt;
    &lt;p&gt;This uses a &lt;code&gt;static&lt;/code&gt; variable to have it persist between both the &lt;code&gt;compare&lt;/code&gt; function calls that &lt;code&gt;qsort&lt;/code&gt; makes and the &lt;code&gt;main&lt;/code&gt; call which (potentially) changes its value to be &lt;code&gt;1&lt;/code&gt; instead of &lt;code&gt;0&lt;/code&gt;. Unfortunately, this isn’t always the best idea for more complex programs that don’t fit within a single snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;it is impossible to have different “copies” of a &lt;code&gt;static&lt;/code&gt;variable, meaning all mutations done in all parts of the program that can see&lt;code&gt;in_reverse&lt;/code&gt;are responsible for knowing the state before and after (e.g., heavily stateful programming of state that you may not own / cannot see);&lt;/item&gt;
      &lt;item&gt;working on &lt;code&gt;static&lt;/code&gt;data may produce thread contention/race conditions in more complex programs;&lt;/item&gt;
      &lt;item&gt;using &lt;code&gt;_Thread_local&lt;/code&gt;instead of&lt;code&gt;static&lt;/code&gt;only solves the race condition problem but does not solve the “shared across several places on the same thread” problem;&lt;/item&gt;
      &lt;item&gt;referring to specific pieces of data or local pieces of data (like &lt;code&gt;list&lt;/code&gt;itself) become impossible;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and so on, and so forth. This is the core of the problem here. It becomes more pronounced when you want to do things with function and data that are a bit more complex, such as Donald Knuth’s “Man-or-Boy” test code.&lt;/p&gt;
    &lt;p&gt;The solutions to these problems come in 4 major flavors in C and C++ code.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just reimplement the offending function to take a userdata pointer so you can pass whatever data you want (typical C solution, e.g. going from &lt;code&gt;qsort&lt;/code&gt;as the sorting function to BSD’s&lt;code&gt;qsort_r&lt;/code&gt;1 or Annex K’s&lt;code&gt;qsort_s&lt;/code&gt;2).&lt;/item&gt;
      &lt;item&gt;Use GNU Nested Functions to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use Apple Blocks to just Refer To What You Want Anyways.&lt;/item&gt;
      &lt;item&gt;Use C++ Lambdas and some elbow grease to just Refer To What You Want Anyways.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each solution has drawbacks and benefits insofar as usability and design, but as a quick overview we’ll show what it’s like using &lt;code&gt;qsort&lt;/code&gt; (or &lt;code&gt;qsort_r&lt;/code&gt;/&lt;code&gt;qsort_s&lt;/code&gt;, where applicable). Apple Blocks, for starters, looks like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// value changed in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	qsort_b(list, (sizeof(list)/sizeof(*list)), sizeof(*list),
		// Apple Blocks are Block Expressions, meaning they do not have to be stored
		// in a variable first
		^(const void* untyped_left, const void* untyped_right) {
			const int* left = untyped_left;
			const int* right = untyped_right;
			return (in_reverse) ? *right - *left : *left - *right;
		}
	);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;and GNU Nested Functions look like this:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	// local, non-static variable
	int in_reverse = 0;

	// modify variable in-line
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };
	
	// GNU Nested Function definition, can reference `in_reverse` directly
	// is a declaration/definition, and cannot be used directly inside of `qsort`
	int compare(const void* untyped_left, const void* untyped_right) {
		const int* left = untyped_left;
		const int* right = untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	}
	// use in the sort function without the need for a `void*` parameter
	qsort(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare);
	
	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;or, finally, C++-style Lambdas:&lt;/p&gt;
    &lt;code&gt;#define __STDC_WANT_LIB_EXT1__ 1

#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;stddef.h&amp;gt;

int main(int argc, char* argv[]) {
	int in_reverse = 0;
	
	if (argc &amp;gt; 1) {
		char* r_loc = strchr(argv[1], 'r');
		if (r_loc != NULL) {
			ptrdiff_t r_from_start = (r_loc - argv[1]);
			if (r_from_start == 1 &amp;amp;&amp;amp; argv[1][0] == '-' &amp;amp;&amp;amp; strlen(r_loc) == 1) {
				in_reverse = 1;
			} 
		}
	}
	
	// lambdas are expressions, but we can assign their unique variable types with `auto`
	auto compare = [&amp;amp;](const void* untyped_left, const void* untyped_right) {
		const int* left = (const int*)untyped_left;
		const int* right = (const int*)untyped_right;
		return (in_reverse) ? *right - *left : *left - *right;
	};

	int list[] = { 2, 11, 32, 49, 57, 20, 110, 203 };	

	// C++ Lambdas don't automatically make a trampoline, so we need to provide
	// one ourselves for the `qsort_s/r` case so we can call the lambda
	auto compare_trampoline = [](const void* left, const void* right, void* user) {
		typeof(compare)* p_compare = user;
		return (*p_compare)(left, right);
	};
	qsort_s(list, (sizeof(list)/sizeof(*list)), sizeof(*list), compare_trampoline, &amp;amp;compare);

	return list[0];
}
&lt;/code&gt;
    &lt;p&gt;To solve this gaggle of problems, pretty much every semi-modern language (that isn’t assembly-adjacent or based on some kind of state/stack programming) provide some idea of being able to associate some set of data with one or more function calls. And, particularly for Closures, this is done in a local way without passing it as an explicit argument. As it turns out, all of those design choices – including the ones in C – have pretty significant consequences on not just usability, but performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Not A Big Overview&lt;/head&gt;
    &lt;p&gt;This article is NOT going to talk in-depth about the design of all of the alternatives or other languages. We’re focused on the actual cost of the extensions and what they mean. A detailed overview of the design tradeoffs, their security implications, and other problems, can be read at the ISO C Proposal for Functions with Closures here; it also gets into things like Security Implications, ABI, current implementation impact, and more of the various designs. The discussion in the paper is pretty long and talks about the dozens of aspects of each solution down to both the design aspect and the implementation quirks. We encourage you to dive into that proposal and read it to figure out if there’s something more specific you care about insofar as some specific design portion. But, this article is going to be concerned about one thing and one thing only:&lt;/p&gt;
    &lt;head rend="h1"&gt;Purrrrrrrformance :3!&lt;/head&gt;
    &lt;p&gt;In order to measure this cost, we are going to take Knuth’s Man-or-Boy test and benchmark various styles of implementation in C and C++ using various different extensions / features for the Closure problem. The Man-or-Boy test is an efficient measure of how well your programming language can handle referring to specific entities while engaging in a large degree of recursion and self-reference. It can stress test various portions of how your program creates and passes around data associated with a function call, and if your programming language design is so goofy that it can’t refer to a specific instance of a variable or function argument, it will end up producing the wrong answer and breaking horrifically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Benchmark: Raw C&lt;/head&gt;
    &lt;p&gt;Here is the core of the Man-or-Boy test, as implemented in raw C. This implementation3 and all the others are available online for us all to scrutinize and yell at me for messing up, to make sure I’m not slandering your favorite solution for Closures in this space.&lt;/p&gt;
    &lt;code&gt;// ...

static int eval(ARG* a) {
	return a-&amp;gt;fn(a);
}

static int B(ARG* a) {
	int k    = *a-&amp;gt;k -= 1;
	ARG args = { B, &amp;amp;k, a, a-&amp;gt;x1, a-&amp;gt;x2, a-&amp;gt;x3, a-&amp;gt;x4 };
	return A(&amp;amp;args);
}

static int A(ARG* a) {
	return *a-&amp;gt;k &amp;lt;= 0 ? eval(a-&amp;gt;x4) + eval(a-&amp;gt;x5) : B(a);
}

// ...
&lt;/code&gt;
    &lt;p&gt;You will notice that there is a big, fat, ugly &lt;code&gt;ARG*&lt;/code&gt; parameter hanging around all of these functions. That is because, as stated before, plain ISO C cannot handle passing the data around unless it’s part of a function’s arguments. Because the actual core of the Man-or-Boy experiment is the ability to refer to specific values of &lt;code&gt;k&lt;/code&gt; that exist during the recursive run of the program, we need to actually modify the function signature and thereby cheat some of the implicit Man-or-Boy requirements of not passing the value in directly. Here’s what &lt;code&gt;ARG&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;typedef struct arg {
	int (*fn)(struct arg*);
	int* k;
	struct arg *x1, *x2, *x3, *x4, *x5;
} ARG;

static int f_1(ARG* _) {
	return -1;
}

static int f0(ARG* _) {
	return 0;
}

static int f1(ARG* _) {
	return 1;
}

static int eval(ARG* a) {
	// ...
}
// ...
&lt;/code&gt;
    &lt;p&gt;And this is how it gets used in the main body of the function in order to compute the right answer and benchmark it:&lt;/p&gt;
    &lt;code&gt;static void normal_functions_rosetta(benchmark::State&amp;amp; state) {
	const int initial_k  = k_value();
	const int expected_k = expected_k_value();
	int64_t result       = 0;

	for (auto _ : state) {
		int k     = initial_k;
		ARG arg1  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg2  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg3  = { f_1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg4  = { f1, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG arg5  = { f0, NULL, NULL, NULL, NULL, NULL, NULL };
		ARG args  = { B, &amp;amp;k, &amp;amp;arg1, &amp;amp;arg2, &amp;amp;arg3, &amp;amp;arg4, &amp;amp;arg5 };
		int value = A(&amp;amp;args);
		result += value == expected_k ? 1 : 0;
	}

	if (result != state.iterations()) {
		state.SkipWithError("failed: did not produce the right answer!");
	}
}

BENCHMARK(normal_functions_rosetta);
&lt;/code&gt;
    &lt;p&gt;Everything within the &lt;code&gt;for (auto _ : state) { ... }&lt;/code&gt; is benchmarked. For those paying attention to the code and find it looking familiar, it’s because that code is the basic structure all Google Benchmark4 code finds itself looking like. I’ve wanted to swap to Catch25 for a long time now to change to their benchmarking infrastructure, but I’ve been stuck on Google Benchmark because I’ve made a lot of graph-making tools based on its JSON output and I have not vetted Catch2’s JSON output yet to see if it has all of the necessary bits ‘n’ bobbles I use to de-dedup runs and compute statistics.&lt;/p&gt;
    &lt;p&gt;Everything outside is setup (the part above the &lt;code&gt;for&lt;/code&gt; loop) or teardown/test correction (the part below the &lt;code&gt;for&lt;/code&gt; loop). The initialization of the &lt;code&gt;ARG args&lt;/code&gt;s cannot be moved outside of the measuring loop because each invocation of &lt;code&gt;A&lt;/code&gt; – the core of the Man-or-Boy experiment – modifies the &lt;code&gt;k&lt;/code&gt; of the ARG parameter, so all of them have to be inside. Conceivably, &lt;code&gt;arg1 .. 5&lt;/code&gt; could be moved out of the loop, but I am very tired of looking at the eight or nine variations of this code so someone else can move it and tell me if Clang or GCC has lots of compiler optimization sauce and doesn’t understand that those 5 &lt;code&gt;argI&lt;/code&gt;s can be hoisted out of the loop.&lt;/p&gt;
    &lt;p&gt;The value &lt;code&gt;k&lt;/code&gt; is &lt;code&gt;10&lt;/code&gt;, and &lt;code&gt;expected_k&lt;/code&gt; is &lt;code&gt;-67&lt;/code&gt;. The expected, returned &lt;code&gt;k&lt;/code&gt; value is dependent on the input &lt;code&gt;k&lt;/code&gt; value, which controls how deep the Man-or-Boy test would recurse on itself to produce its answer. Therefore, to prevent GCC and Clang and other MEGA POWERFUL PILLAR COMPILERS from optimizing the entire thing out and just replacing the benchmark loop with &lt;code&gt;ret -67&lt;/code&gt;, both &lt;code&gt;k_value()&lt;/code&gt; and &lt;code&gt;expected_k_value()&lt;/code&gt; come from a Dynamic Link Library (&lt;code&gt;.dylib&lt;/code&gt; on MacOS, &lt;code&gt;.so&lt;/code&gt; on *nix platforms, &lt;code&gt;.dll&lt;/code&gt; on Windows platforms) to make sure that NO amount of optimization (Link Time Optimization/Link Time Code Generation, Inlining Optimization, Cross-Translation Unit Optimization, and Automatic Constant Expression Optimization) from C or C++ compilers could fully preempt all forms of computation.&lt;/p&gt;
    &lt;p&gt;This allows us to know, for sure, that we’re actually measuring something and not just testing how fast a compiler can load a number into a register and test it against &lt;code&gt;state.iterations()&lt;/code&gt;. And, since we know for sure, we can now talk the general methodology.&lt;/p&gt;
    &lt;head rend="h1"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;The tests were ran on a dying 13-inch 2020 MacBook Pro M1 that has suffered several toddler spills and two severe falls. It has 16 GB of RAM and is son MacOS 15.7.2 Sequoia at the time the test was taken, using the stock MacOS AppleClang Compiler and the stock &lt;code&gt;brew install gcc&lt;/code&gt; compiler in order to produce the numbers seen on December 6th, 2025.&lt;/p&gt;
    &lt;p&gt;There 2 measures being conducted: Real Time and CPU Time. The time is gathered by running a single iteration of the code within the &lt;code&gt;for&lt;/code&gt; loop anywhere from a couple thousand to hundreds of thousands of times to produce confidence in that run of the benchmark. This is then averaged to produce the first point. The process is repeated 50 times, repeating that many iterations to build further confidence in the measurement. All 50 means are used as the points for the values, and the average of all of those 50 means is then used as the height of a bar in a bar graph.&lt;/p&gt;
    &lt;p&gt;The bars are presented side-by-side as a horizontal bar chart with 11 categories of C or C++ code being measured. The 11 categories are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;no-op&lt;/code&gt;: Literally doing nothing. It’s just there to test environmental noise and make sure none of our benchmarks are so off-base that we’re measuring noise rather than computation. Helps keep us grounded in reality.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (No Function Helpers)&lt;/code&gt;: a solution using C++-style lambdas. Rather than using helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;, we compute a raw lambda that stores the value meant to be returned for the Man-or-Boy test (&lt;code&gt;return i;&lt;/code&gt;) in the lambda itself and then pass that uniquely-typed lambda to the core of the test. The entire test is templated and uses a fake&lt;code&gt;recursion&lt;/code&gt;template parameter to halt the recursion after a certain depth.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas&lt;/code&gt;: The same as above but actually using&lt;code&gt;int f0(void)&lt;/code&gt;, etc. helper functions at the start rather than lambdas. Reduces inliner pressure by using “normal” types which do not add to the generated number of lambda-typed, recursive, templated function calls.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function_ref)&lt;/code&gt;: The same as above, but rather than using a function template to handle each uniquely-typed lambda like a precious baby bird, it instead erases the lambda behind a&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This allows the recursive function to retain exactly one signature.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (std::function)&lt;/code&gt;: The same as above, but replaces&lt;code&gt;std::function_ref&amp;lt;int(void)&amp;gt;&lt;/code&gt;with&lt;code&gt;std::function&amp;lt;int(void)&amp;gt;&lt;/code&gt;. This is its allocating, C++03-style type.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lambdas (Rosetta Code)&lt;/code&gt;: The code straight out of the C++11 Rosetta Code Lambda section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Apple Blocks&lt;/code&gt;: Uses Apple Blocks to implement the test, along with the&lt;code&gt;__block&lt;/code&gt;specifier to refer directly to certain variables on the stack.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions (Rosetta Code)&lt;/code&gt;: The code straight out of the C Rosetta Code section on the Man-or-Boy Rosetta Code implementation.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;GNU Nested Functions&lt;/code&gt;: GNU Nested Functions similar to the Rosetta Code implementation, but with some slight modifications in a hope to potentially alleviate some stack pressure if possible by using regular helper functions like&lt;code&gt;f0&lt;/code&gt;,&lt;code&gt;f1&lt;/code&gt;, and&lt;code&gt;f_1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Custom C++ Class&lt;/code&gt;: A custom-written C++ class using a discriminated union to decide whether its doing a straight function call or attemping to engage in the Man-or-Boy recursion.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;C++03 shared_ptr (Rosetta Code)&lt;/code&gt;: A C++ class using&lt;code&gt;std::enable_shared_from_this&lt;/code&gt;and&lt;code&gt;std::shared_ptr&lt;/code&gt;with a virtual function call to invoke the “right” function call during recursion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The two compilers tested are Apple Clang 17 and GCC 15. There are two graph images because one is for Apple Clang and the other is for GCC. This is particularly important because neither compiler implements the other’s closure extension (Clang does Apple Blocks but not Nested Functions, while GCC does Nested Functions in exclusively its C frontend but does not implement Apple Blocks6).&lt;/p&gt;
    &lt;head rend="h1"&gt;The Results&lt;/head&gt;
    &lt;p&gt;Ta-da!&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;… Oh. That looks awful.&lt;/p&gt;
    &lt;p&gt;It turns out that some solutions are so dogwater that it completely screws up our viewing graphs. But, it does let us know that Lambdas used the Rosetta Code style are so unbelievably awful that it is several orders of magnitude more expensive than any other solution presented! One has to wonder what the hell is going on in the code snippet there, but first we need to make the graphs more legible. To do this we’re going to be using the (slightly deceptive) LOGARITHMIC SCALING. This is a bit deadly to do because it tends to mislead people about how much of a change there is, so please pay attention to the potential order of magnitude gains and losses when going from one bar graph to another.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;For the vision-impaired, a text description is available.&lt;/p&gt;
    &lt;p&gt;There we go. Now we can talk about the various solutions and – in particular – why “lambdas” have 4 different entries with such wildly differing performance profiles. First up, let’s talk about the clear performance winners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambdas: On Top!&lt;/head&gt;
    &lt;p&gt;Not surprising to anyone who has been checked in to C++, lambdas that are used directly and not type-erased are on top. This means there’s a one-to-one mapping between a function call and a given bit of execution. We are cheating by using a constant parameter to stop the uniquely-typed lambdas being passed into the functions from recursing infinitely, which makes the Man-or-Boy function look like this:&lt;/p&gt;
    &lt;code&gt;template &amp;lt;int recursion = 0&amp;gt;
static int a(int k, const auto&amp;amp; x1, const auto&amp;amp; x2, const auto&amp;amp; x3, const auto&amp;amp; x4, const auto&amp;amp; x5) {
	if constexpr (recursion == 11) {
		::std::cerr &amp;lt;&amp;lt; "This should never happen and this code should never have been generated." &amp;lt;&amp;lt; std::endl;
		::std::terminate();
		return 0;
	}
	else {
		auto B = [&amp;amp;](this const auto&amp;amp; self) { return a&amp;lt;recursion + 1&amp;gt;(--k, self, x1, x2, x3, x4); };
		return k &amp;lt;= 0 ? x4() + x5() : B();
	}
}
&lt;/code&gt;
    &lt;p&gt;Every &lt;code&gt;B&lt;/code&gt; is its own unique type and we are not erasing that unique type when using the expression as an initializer to &lt;code&gt;B&lt;/code&gt;. This means that when we call &lt;code&gt;a&lt;/code&gt; again with &lt;code&gt;B&lt;/code&gt; (the &lt;code&gt;self&lt;/code&gt; in this lambda here using Deduced This, a C++23 feature that cannot be part of the C version of lambdas) which means we need to use &lt;code&gt;auto&lt;/code&gt; parameters (a shortcut way of writing template parameters) to take it. But, since every parameter is unique, and every &lt;code&gt;B&lt;/code&gt; is unique, calling this recursively means that, eventually, C++ compilers will actually just completely crash out/toss out-of-memory errors/say we’ve compile-time recursed too hard, or similar. That’s why the compile-time &lt;code&gt;if constexpr&lt;/code&gt; on the extra, templated &lt;code&gt;recursion&lt;/code&gt; parameter needs to have some arbitrary limit. Because we know &lt;code&gt;k&lt;/code&gt; starts at 10 for this test, we just have some bogus limit of “11”.&lt;/p&gt;
    &lt;p&gt;This results in a very spammy recursive chain of function calls, where the actual generated names of these template functions are far more complex than &lt;code&gt;a&lt;/code&gt; and can run the compiler into the ground / cause quite a bit of instantiations if you let &lt;code&gt;recursion&lt;/code&gt; get to a high enough value. But, once you add the limit, the compiler gets perfect information about this recursive call all the way to every leaf, and thus is able to not only optimize the hell out of it, but refuse to generate the other frivolous code it knows won’t be useful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas are also Fast, even when Type-Erased&lt;/head&gt;
    &lt;p&gt;You can observe a slight bump up in performance penalty when a Lambda is erased by a &lt;code&gt;std::function_ref&lt;/code&gt;. This is a low-level, non-allocating, non-owning, slim “view” type that is analogous to what a language-based wide function pointer type would be in C. From this, it allows us to guess how good Lambdas in C would be even if you had to hide them behind a non-unique type.&lt;/p&gt;
    &lt;p&gt;The performance metrics are about equivalent to if you hand-wrote a C++ class with a custom &lt;code&gt;operator()&lt;/code&gt; that uses a discriminated union, no matter which compiler gets used to do it. It’s obviously not as fast as having access to a direct function call and being able to slurp-inline optimize, but the performance difference is acceptable when you do not want to engage in a large degree of what is called “monomorphisation” of a generic routine or type. And, indeed, outside of macros, C has no way of doing this innately that isn’t runtime-based.&lt;/p&gt;
    &lt;p&gt;A very strong contender for a good solution!&lt;/p&gt;
    &lt;head rend="h3"&gt;Lambdas: On…. Bottom, too?&lt;/head&gt;
    &lt;p&gt;One must wonder, then, why the &lt;code&gt;std::function&lt;/code&gt; Lambdas and the Rosetta Code Lambdas are either bottom-middle-of-the-road or absolutely-teary-eyed-awful.&lt;/p&gt;
    &lt;p&gt;Starting off, the &lt;code&gt;std::function&lt;/code&gt; Lambdas are bad because of exactly that: &lt;code&gt;std::function&lt;/code&gt;. &lt;code&gt;std::function&lt;/code&gt; is not a “cheap” closure; it is a potentially-allocating, meaty, owning function abstraction. This means that it’s safe to make one and pass it around and store it and call it later; the cost of this is, obviously, that you’re allocating (when the type is big enough) for that internal storage. Part of this is alleviated by using &lt;code&gt;const std::function&amp;lt;int(void)&amp;gt;&amp;amp;&lt;/code&gt; parameters, taking things by reference and only generating a new object when necessary. This prevents copying on every function call. Both the Rosetta Lambdas and regular &lt;code&gt;std::function&lt;/code&gt; Lambdas code do the by-reference parameters bit, though, so where does the difference come in? It actually has to do with the Captures. Here’s how &lt;code&gt;std::function&lt;/code&gt; Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [&amp;amp;] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;And, here is how the Rosetta Code Lambdas defines the recursive, self-referential lambda and uses it:&lt;/p&gt;
    &lt;code&gt;using f_t = std::function&amp;lt;int(void)&amp;gt;;

inline static int A(int k, const f_t&amp;amp; x1, const f_t&amp;amp; x2, const f_t&amp;amp; x3, const f_t&amp;amp; x4, const f_t&amp;amp; x5) {
	f_t B = [=, &amp;amp;k, &amp;amp;B] { return A(--k, B, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : B();
}
&lt;/code&gt;
    &lt;p&gt;The big problem here is in the use of the &lt;code&gt;=&lt;/code&gt;. What &lt;code&gt;=&lt;/code&gt; by itself in the front of a lambda capture clause means is “copy all the visible variables in and hold onto that copy” (unless the capture for that following variable is “overridden” by a &lt;code&gt;&amp;amp;var&lt;/code&gt;, address capture). Meanwhile, the &lt;code&gt;&amp;amp;&lt;/code&gt; is the opposite: it means “refer to all the visible variables directly by their address and do not copy them in”. So, while the &lt;code&gt;std::function&lt;/code&gt; Lambda is (smartly) referring to stuff directly without copying because we know for the Man-or-Boy test that referring to things directly is not an unsafe operation, the general &lt;code&gt;=&lt;/code&gt; causes that for the several dozen recursive iterations through the function, it is copying all five allocating &lt;code&gt;std::function&lt;/code&gt; arguments. So the first call creates a &lt;code&gt;B&lt;/code&gt; that copies everything in, and then passes that in, and then the next call copies the previous &lt;code&gt;B&lt;/code&gt; and the 4 normal functions, and then passes that in to the next &lt;code&gt;B&lt;/code&gt;, and then it copies both previous &lt;code&gt;B&lt;/code&gt;’s, and this stacks for the depth of the callgraph (some 10 times since &lt;code&gt;k = 10&lt;/code&gt; to start).&lt;/p&gt;
    &lt;p&gt;You can imagine how much that completely screws with the performance, and it explains why the Rosetta Code Lambdas code behaves so poorly in terms of performance. But, this also raises a question: if referring to everything by-reference saves so much speed, then why does GNU Nested Functions – in all its variants – perform so poorly? After all, Nested Functions capture everything by reference / by address, exactly like a lambda does with &lt;code&gt;[&amp;amp;]&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Similarly, if allocating over and over again was so expensive, how come Apple Blocks and C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code-style versions of the Man-or-Boy test don’t perform nearly as badly as the Rosetta Code Lambdas? Are we not copying the value of the arguments into a newly created Apple Block and, thusly, tanking the performance metrics? Well, as it turns out, there’s many reasons for these things, so let’s start with GNU Nested Functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nested Functions and The Stack&lt;/head&gt;
    &lt;p&gt;I’ve written about it dozens of times now, but the prevailing and most common implementation of Nested Functions is with an executable stack. The are a lot of security and other implications for this, but all you need to understand is that the reason GCC did this is because it was an at-the-time slick encoding of both the location of the variables and the routine itself. Allocating a chunk of data off of the current programming stack means that the “environment context”/”this closure” pointer has the same anchoring address as the routine itself. This means you can encode both the location of the data to know what to access and the address of a function’s entry point into a single thing that works with your typical setup-and-call convention that comes with invoking a standard ISO C function pointer.&lt;/p&gt;
    &lt;p&gt;But think about that, briefly, in terms of optimization.&lt;/p&gt;
    &lt;p&gt;You are using the function’s stack frame at that precise point in the program as the “base address” for this executable code. That base address also means that all the variables associated with it need to be reachable from that base address: i.e., that things are not stuffed in registers, but that you are referring to the same variables as modified by the enclosing function around your nested function. Principally, this means that your function needs to have all of the following now so that GNU Nested Functions actually work.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that is executable so that the base address used for the trampoline can be run succinctly.&lt;/item&gt;
      &lt;item&gt;A real function frame that exists somewhere in memory to serve as the base address for the trampoline.&lt;/item&gt;
      &lt;item&gt;Real objects in memory backing the names of the captured variables to be accessed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all seems like regular consequences, until you tack on the second order affects from the point of optimization.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A stack that now has both data and instructions all blended into itself.&lt;/item&gt;
      &lt;item&gt;A real function frame, which means no omission of a frame pointer and no collapsing / inlining of that function frame.&lt;/item&gt;
      &lt;item&gt;Real objects that all have their address taken that are tied to the function frame, which must be memory-accessible and which the compiler now has a hard time telling if they can simply be exchanged through registers or if the need to actually sit somewhere in memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words: GNU Nested Functions have created the perfect little storm for what might be the best optimizer-murderer. The reason it performs so drastically poorly (worse than even allocating lambdas inside of a &lt;code&gt;std::function&lt;/code&gt; or C++03-style virtual function calls inside of a bulky, nasty C++ &lt;code&gt;std::shared_ptr&lt;/code&gt;) by a whole order of magnitude or more is that everything about Nested Functions and their current implementation is basically Optimizer Death. If the compiler can’t see through everything – and the Man-or-Boy test with a non-constant value of &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt; – GNU Nested Functions deteriorate rapidly. It takes every core optimization technique that we’ve researched and maximized on in the last 30 years and puts a shotgun to the side of its head once it can’t pre-compute &lt;code&gt;k&lt;/code&gt; and &lt;code&gt;expected_k&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The good news is that GCC has completed a new backing implementation for GNU Nested Functions, which uses a heap-based trampoline. Such a trampoline does not interfere with the stack, would allow for omission of frame pointers while referring directly to the data itself (which may prevent the wrecking of specific kinds of inlining optimizations), and does not need an executable stack (just a piece of memory from ✨somewhere✨ it can mark executable). This may have performance closer to Apple Blocks, but we don’t have a build of the latest GCC to test it with. But, when we do, we can simply add the compilation flag &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the two source files in CMake and then let the benchmarks run again to see how it stacks up!&lt;/p&gt;
    &lt;p&gt;Finally, there is a minor performance degredation because our benchmarking software is in C++ and this extension exists exclusively in the C frontend of GCC. That means I have to use an &lt;code&gt;extern&lt;/code&gt; function call within the benchmark loop to get to the actual code. Within the function call, however, all of this stuff should be optimized down, so the cost of a single function call’s stack frame shouldn’t be so awful, but I expect to try to dig into this better to help make sure the &lt;code&gt;extern&lt;/code&gt; of a C function call isn’t making things dramatically worse than they are. Given it’s a different translation unit and it’s not being compiled as a separate static or dynamic library, it should still link together and optimize cleanly, but given how bad it’s performing? Every possible issue is on the table.&lt;/p&gt;
    &lt;head rend="h2"&gt;What about Apple Blocks?&lt;/head&gt;
    &lt;p&gt;Apple Blocks are not the fastest, but they the best of the C extensions while being the worst of the “fast” solutions. They are not faster than just hacking the &lt;code&gt;ARG*&lt;/code&gt; into the function signature and using regular normal C function calls, unfortunately, and that’s likely due to their shared, heap-ish nature. The saddest part about Apple Blocks is that it works using a Blocks Runtime that is already as optimized as it can possibly be: Clang and Apple both document that while the Blocks Runtime does manage an Automatic Reference Counted (ARC) Heap of Block pointers, when a Block is first created it will literally have its memory stored on the stack rather than in the heap. In order to move it to the heap, one must call &lt;code&gt;Block_copy&lt;/code&gt; to trigger the “normal” heap-based shenanigans. We never call &lt;code&gt;Block_copy&lt;/code&gt;, so this is with as-fast-as-possible variable access and management with few allocations.&lt;/p&gt;
    &lt;p&gt;It’s very slightly disappointing that: normal C functions with an &lt;code&gt;ARG*&lt;/code&gt; blob; a custom C++ class using a discriminated union and &lt;code&gt;operator()&lt;/code&gt;; any mildly conscientious use of lambdas; and, any other such shenanigans perform better than the very best Apple Blocks has to offer. One has to imagine that all of the ARC management functions made to copy the &lt;code&gt;int^(void)&lt;/code&gt; “hat-style” function pointers, even if they end up not doing much for the data stored on the stack, impacted the results here. But, this is also somewhat good news: because Apple Block hat pointers are cheaply-copiable entities (they are just pointers to a Block object), it means that even if we copy all of the arguments into the closure every function call, that copying is about as cheap as it can get. Obivously, as regular “Lambdas” and “Lambas (No Function Helpers)” demonstrate, being able to just slurp everything up by address/by reference – including visible function arguments – with &lt;code&gt;[&amp;amp;]&lt;/code&gt; saves us a teensy, tiny bit of time7.&lt;/p&gt;
    &lt;p&gt;The cheapness of &lt;code&gt;int^(void)&lt;/code&gt; hat-pointer function types is likely the biggest saving grace for Apple Blocks in this benchmark. In the one place we need to be careful, we rename the input argument &lt;code&gt;k&lt;/code&gt; to &lt;code&gt;arg_k&lt;/code&gt; and then make a &lt;code&gt;__block&lt;/code&gt; variable to actually refer to a shared &lt;code&gt;int k&lt;/code&gt; (and get the right answer):&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	__block fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;All of the &lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, and &lt;code&gt;x3&lt;/code&gt; – like the bad Lambda case – are copied over and over and over again. One could change the name of all the arugments &lt;code&gt;arg_xI&lt;/code&gt; and then have an &lt;code&gt;xI&lt;/code&gt; variable inside that is marked &lt;code&gt;__block&lt;/code&gt;, but that’s more effort and very unlikely to have any serious impact on the code while possibly degrading performance for the setup of multiple shared variables that all have to also be ARC-reference-counted and be stored inside each and every new &lt;code&gt;b&lt;/code&gt; block that is created.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Brief Aside: Self-Referencing Functions/Closures&lt;/head&gt;
    &lt;p&gt;It’s also important to note that just writing this:&lt;/p&gt;
    &lt;code&gt;static int a(int arg_k, fn_t ^ x1, fn_t ^ x2, fn_t ^ x3, fn_t ^ x4, fn_t ^ x5) {
	__block int k    = arg_k;
	fn_t ^ b = ^(void) { return a(--k, b, x1, x2, x3, x4); };
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;(no &lt;code&gt;__block&lt;/code&gt; on the &lt;code&gt;b&lt;/code&gt; variable) is actually a huge bug. Apple Blocks, like older C++ Lambdas, cannot technically refer to “itself” inside. You have to refer to the “self” by capturing the variable it is assigned to. For those who use C++ and are familiar with the lambdas over there, it’s like making sure you capture the variable you initialize with the lambda by reference while also making sure it has a concrete type. It can only be escaped by using &lt;code&gt;auto&lt;/code&gt; and Deducing This, or some other combination of referential-use. That is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;auto x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;does not compile, as the type&lt;code&gt;auto&lt;/code&gt;isn’t figured out yet;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function_ref&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles but due to C++ shenanigans produces a dangling reference to a temporary lambda that dies after the full expression (the initialization);&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [&amp;amp;x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;compiles and works with no segfaults because&lt;code&gt;std::function&lt;/code&gt;allocates, and the reference to itself&lt;code&gt;&amp;amp;x&lt;/code&gt;is just fine.&lt;/item&gt;
      &lt;item&gt;and, finally, &lt;code&gt;auto x = [](this const auto&amp;amp; self, int v) { if (v != limit) self(v + 1); return v + 8; }&lt;/code&gt;which compiles and works with no segfaults because the invisible&lt;code&gt;self&lt;/code&gt;parameter is just a reference to the current object.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with the most recent Apple Blocks snippet just above is that it’s the equivalent of doing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;std::function&amp;lt;int(int)&amp;gt; x = [x](int v) { if (v != limit) x(v + 1); return v + 8; }&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notice that there’s no &lt;code&gt;&amp;amp;x&lt;/code&gt; in the lambda initializer’s capture list. It’s copying an (uninitialized) variable by-value into the lambda. This is what Apple Blocks set into a variable that does not have a &lt;code&gt;__block&lt;/code&gt; specifier, like in our bad code case with &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All variations of this on all implementations which allow for self-referencing allow this and compile some form of this. You would imagine some implementations would warn about this, but this is leftover nonsense from allowing a variable to refer to itself in its initialization. The obvious reason this happens in C and C++ is because you can create self-referential structures, but unfortunately neither language provided a safe way to do this generally. C++23’s Deducing This does not work inside of regular functions and non-objects, so good luck applying it to other places and other extensions8. The only extension which does not suffer this problem is GNU Nested Functions, because it creates a function declaration / definition rather than a variable with an initializer. Thus, this code from the benchmarks works:&lt;/p&gt;
    &lt;code&gt;inline static int gnu_nested_functions_a(int k, int xl(void), int x2(void), int x3(void), int x4(void), int x5(void)) {
	int b(void) {
		return gnu_nested_functions_a(--k, b, xl, x2, x3, x4);
	}
	return k &amp;lt;= 0 ? x4() + x5() : b();
}
&lt;/code&gt;
    &lt;p&gt;And it has the semantics one would expect, unlike how Blocks, Lambdas, or others with default by-value copying work.&lt;/p&gt;
    &lt;p&gt;In the general case, this is what the paper &lt;code&gt;__self_func&lt;/code&gt; was going to solve9, but… that’s going to need some time for me to convince WG14 that maybe it IS actually a good idea. We can probably just keep writing the buggy code a few dozen more times for the recursion case and keep leaving it error prone, but I’ll try my best to convince them one more time that the above situation is very not-okay.&lt;/p&gt;
    &lt;head rend="h1"&gt;Thinking It Over&lt;/head&gt;
    &lt;p&gt;While the Man-or-Boy test isn’t exactly the end-all, be-all performance test, due to flexing both (self)-referential data and utilization of local copies with recursion, it is surprisingly suitable for figuring out if a closure design is decent enough in a mid to high-level programming language. It also gives me some confidence that, at the very least, the baseline for performance of statically-known, compile-time understood, non type-erased, callable Closure objects will have the best implementation quality and performance tradeoffs for a language like ISO C no matter the compiler implementation.&lt;/p&gt;
    &lt;p&gt;In the future, at some point, I’ll have to write about why that is. It’s a bit upside down from the perspective of readers of this blog to first address performance and then later write about the design, but it’s nice to make sure we’re not designing ourselves into a bad performance corner at the outset of this whole adventure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learned Insights&lt;/head&gt;
    &lt;p&gt;Surprising nobody, the more information the compiler is allowed to accrue (the Lambda design), the better its ability to make the code fast. What might be slightly more surprising is that a slim, compact layer of type erasure – not a bulky set of Virtual Function Calls (C++03 &lt;code&gt;shared_ptr&lt;/code&gt; Rosetta Code design) – does not actually cost much at all (Lambdas with &lt;code&gt;std::function_ref&lt;/code&gt;). This points out something else that’s part of the ISO C proposal for Closures (but not formally in its wording): Wide Function Pointers.&lt;/p&gt;
    &lt;p&gt;The ability to make a thin &lt;code&gt;{ some_function_type* func; void* context; }&lt;/code&gt; type backed by the compiler in C would be extremely powerful. Martin Uecker has a proposal that has received interest and passing approval in the Committee, but it would be nice to move it along in a nice direction. My suggestion is having &lt;code&gt;%&lt;/code&gt; as a modifier, so it can be used easily since wide function pointers are an extremely prevalent concept. Being able to write something like the following would be very easy and helpful.&lt;/p&gt;
    &lt;code&gt;typedef int(compute_fn_t)(int);

int do_computation(int num, compute_fn_t% success_modification);
&lt;/code&gt;
    &lt;p&gt;A wide function pointer type like this would also be traditionally convertible from a number of already existing extensions, too, where GNU Nested Functions, Apple Blocks, C++-style Lambdas, and more could create the appropriate wide function pointer type to be cheaply used. Additionally, it also works for FFI: things like Go closures already use GCC’s &lt;code&gt;__builtin_call_with_static_chain&lt;/code&gt; to transport through their Go functions in C. Many other functions from other languages could be cheaply and efficiently bridged with this, without having to come up with harebrained schemes about where to put a &lt;code&gt;void* userdata&lt;/code&gt; or some kind of implicit context pointer / implicit environment pointer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Extensions?&lt;/head&gt;
    &lt;p&gt;Unfortunately – except for the Borland closure annotation – there’s too many things that are performance-stinky about existing C extensions to this problem. It’s no wonder GCC is trying to add &lt;code&gt;-ftrampoline-impl=heap&lt;/code&gt; to the story of GNU Nested Functions; they might be able to tighten up that performance and make it more competitive with Apple Blocks. But, unfortunately, since it is heap-based, there’s a real chance that its maximum performance ceiling is only as good as Apple Blocks, and not as good as a C++-style Lambda.&lt;/p&gt;
    &lt;p&gt;Both GNU Nested Functions and Apple Blocks – as they are implemented – do not really work well in ISO C. GNU Nested Functions because their base design and most prevalent implementation are performance-awful, but also Apple Blocks because of the copying and indirection runtime of Blocks that manage ARC pointers providing a hard upper limit on how good the performance can actually be in complex cases.&lt;/p&gt;
    &lt;p&gt;Regular C code, again, performs middle-of-the-road here. It’s not the worst of it, but it’s not the best at all, which means there’s some room beneath how we could go having the C code run. While it’s hard to fully trust the Rosetta Code Man-or-Boy code for C as the best, it is a pretty clear example of how a “normal” C developer would do it and how it’s not actually able to hit maximum performance for this situation.&lt;/p&gt;
    &lt;p&gt;I wanted to add a version of regular C code that used a dynamic array with &lt;code&gt;static&lt;/code&gt;s to transfer data, or a bunch of &lt;code&gt;thread_local&lt;/code&gt;s, but I could not bring myself to actually care enough to write a complex association scheme from a specific invocation of the recursive function &lt;code&gt;a&lt;/code&gt; and the slot of dynamic data that represented the closure’s data. I’m sure there’s schemes for it and I could think of a few, but at that point it’s such a violent contortion to get a solution going that I figured it simply wasn’t worth the effort. But, as always,&lt;/p&gt;
    &lt;p&gt;pull requests are welcome. 💚&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Banner and Title Photo by Lukas, from Pexels&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;See: https://github.com/soasis/idk/tree/main/benchmarks/closures. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See https://github.com/catchorg/Catch2/blob/devel/docs/benchmarks.md. And try it out. It’s pretty good, I just haven’t gotten off my butt to make the swap to it yet. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Apple Blocks used to have an implementation in GCC that could be turned on and it used a Blocks Runtime to achieve it. But, I think it was gutted when some NeXT support and Objective-C stuff was wiped out after being unmaintained for some time. There’s been talk of reintroducing it, but obviously someone has to actually sit down and either redo it from scratch (advantageous because Apple has changed the ABI of Blocks) or try to ressurect / fix the old support for this stuff. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Apple Blocks cannot have the “by address” capturing mechanism it has – the&lt;/p&gt;&lt;code&gt;__block&lt;/code&gt;storage class modifier – applied to function arguments, for some reason. So, all function arguments are de-facto copied into a Block Expression unless someone saves a tempory inside the body of the function before the Block and then uses&lt;code&gt;__block&lt;/code&gt;on that to make it a by-reference capture. ↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It also works on a template basis in order to deduce&lt;/p&gt;&lt;code&gt;this&lt;/code&gt;– the&lt;code&gt;const auto&amp;amp;&lt;/code&gt;is a templated parameter and is usually used to do things like allow a member function to be both&lt;code&gt;const&lt;/code&gt;and non-&lt;code&gt;const&lt;/code&gt;where possible when generated. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WG14 rejected the paper last meeting, unfortunately, as not motivated enough. Funnily enough, it was immediately after this meeting that I got slammed in the face with this bug. Foresight and “being prepared” is just not something even the most diehard C enthusiasts really embodies, unfortunately, and most industry vendors tend to take a more strongly conservative position over a bigger one. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46228597</guid><pubDate>Thu, 11 Dec 2025 07:21:33 +0000</pubDate></item><item><title>A “frozen” dictionary for Python</title><link>https://lwn.net/SubscriberLink/1047238/25c270b077849dc0/</link><description>&lt;doc fingerprint="9f1980339ca5cbf8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A "frozen" dictionary for Python&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Dictionaries are ubiquitous in Python code; they are the data structure of choice for a wide variety of tasks. But dictionaries are mutable, which makes them problematic for sharing data in concurrent code. Python has added various concurrency features to the language over the last decade or so—async, free threading without the global interpreter lock (GIL), and independent subinterpreters—but users must work out their own solution for an immutable dictionary that can be safely shared by concurrent code. There are existing modules that could be used, but a recent proposal, PEP 814 ("Add frozendict built-in type"), looks to bring the feature to the language itself.&lt;/p&gt;
    &lt;p&gt;Victor Stinner announced the PEP that he and Donghee Na have authored in a post to the PEPs category of the Python discussion forum on November 13. The idea has come up before, including in PEP 416, which has essentially the same title as 814 and was authored by Stinner back in 2012. It was rejected by Guido van Rossum at the time, in part due to its target: a Python sandbox that never really panned out.&lt;/p&gt;
    &lt;head rend="h4"&gt;frozendict&lt;/head&gt;
    &lt;p&gt;The idea is fairly straightforward: add frozendict as a new immutable type to the language's builtins module. As Stinner put it:&lt;/p&gt;
    &lt;quote&gt;We expect frozendict to be safe by design, as it prevents any unintended modifications. This addition benefits not only CPython's standard library, but also third-party maintainers who can take advantage of a reliable, immutable dictionary type.&lt;/quote&gt;
    &lt;p&gt;While frozendict has a lot in common with the dict built-in type, it is not a subclass of dict; instead, it is a subclass of the base object type. The frozendict() constructor can be used to create one in various ways:&lt;/p&gt;
    &lt;quote&gt;fd = frozendict() # empty fd = frozendict(a=1, b=2) # frozen { 'a' : 1, 'b' : 2 } d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) # same l = [ ( 'a', 1 ), ( 'b', 2 ) ] fd = frozendict(l) # same fd2 = frozendict(fd) # same assert d == fd == fd2 # True&lt;/quote&gt;
    &lt;p&gt;As with dictionaries, the keys for a frozendict must be immutable, thus hashable, but the values may or may not be. For example, a list is a legitimate type for a value in either type of dictionary, but it is mutable, making the dictionary as a whole (frozen or not) mutable. However, if all of the values stored in a frozendict are immutable, it is also immutable, so it can be hashed and used in places where that is required (e.g. dictionary keys, set elements, or entries in a functools.lru_cache).&lt;/p&gt;
    &lt;p&gt;As might be guessed, based on the last line of the example above, frozen dictionaries that are hashable can be compared for equality with other dictionaries of either type. In addition, neither the hash() value nor the equality test depend on the insertion order of the dictionary, though that order is preserved in a frozen dictionary (as it is in the regular variety). So:&lt;/p&gt;
    &lt;quote&gt;d = { 'a' : 1, 'b' : 2 } fd = frozendict(d) d2 = { 'b' : 2, 'a' : 1 } fd2 = frozendict(d2) assert d == d2 == fd == fd2 # frozendict unions work too, from the PEP &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | frozendict(y=1) frozendict({'x': 1, 'y': 1}) &amp;gt;&amp;gt;&amp;gt; frozendict(x=1) | dict(y=1) frozendict({'x': 1, 'y': 1})For the unions, a new frozen dictionary is created in both cases; the "|=" union-assignment operator also works by generating a new frozendict for the result.&lt;/quote&gt;
    &lt;p&gt; Iteration over a frozendict works as expected; the type implements the collections.abc.Mapping abstract base class, so .items() returns an iterable of key-value tuples, while .keys() and .values() provide the keys and values of the frozen dictionary. For the most part, a frozendict acts like a dict that cannot change; the specific differences between the two are listed in the PEP. It also contains a lengthy list of places in the standard library where a dict could be switched to a frozendict to "&lt;quote&gt;enhance safety and prevent unintended modifications&lt;/quote&gt;". &lt;/p&gt;
    &lt;head rend="h4"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;The reaction to the PEP was generally positive, with the usual suggestions for tweaks and more substantive additions to the proposal. Stinner kept the discussion focused on the proposal at hand for the most part. One part of the proposal was troubling to some: converting a dict to a frozendict was described as an O(n) shallow copy. Daniel F Moisset thought that it would make sense to have an in-place transformation that could be O(1) instead. He proposed adding a .freeze() method that would essentially just change the type of a dict object to frozendict.&lt;/p&gt;
    &lt;p&gt;However, changing the type of an existing object is fraught with peril, as Brett Cannon described:&lt;/p&gt;
    &lt;quote&gt;But now you have made that dictionary frozen for everyone who holds a reference to it, which means side-effects at a distance in a way that could be unexpected (e.g. context switch in a thread and now suddenly you're going to get an exception trying to mutate what was a dict a microsecond ago but is now frozen). That seems like asking for really nasty debugging issues just to optimize some creation time.&lt;/quote&gt;
    &lt;p&gt; The PEP is not aimed at performance, he continued, but is meant to help "&lt;quote&gt;lessen bugs in concurrent code&lt;/quote&gt;". Moisset noted, that dictionaries can already change in unexpected ways via .clear() or .update(), thus the debugging issues already exist. He recognized that the authors may not want to tackle that as part of the PEP, but wanted to try to ensure that an O(1) transformation was not precluded in the future. &lt;/p&gt;
    &lt;p&gt; Cannon's strong objection is to changing the type of the object directly. Ben Hsing and "Nice Zombies" proposed ways to construct a new frozendict without requiring the shallow copy—thus O(1)—by either moving the hash table to a newly created frozendict, while clearing the dictionary, or by using a copy-on-write scheme for the table. As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n), which would be a silly thing to do, but that it sometimes happens "&lt;quote&gt;because it makes people stop complaining&lt;/quote&gt;", he said in a footnote. In light of the discussion, the PEP specifically defers that optimization to a later time, suggesting that it could also be done for other frozen types (tuple and frozenset), perhaps by resurrecting PEP 351 ("The freeze protocol"). &lt;/p&gt;
    &lt;p&gt;On December 1, Stinner announced that the PEP had been submitted to the steering council for pronouncement. Given that Na is on the council, though will presumably recuse himself from deciding on this PEP, he probably has a pretty good sense for how it might be received by the group. So it seems likely that the PEP has a good chance of being approved. The availability of the free-threaded version of the language (i.e. without the GIL) means that more multithreaded Python programs are being created, so having a safe way to share dictionaries between threads will be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Dictionaries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;Python Enhancement Proposals (PEP)/PEP 814&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Dec 5, 2025 9:01 UTC (Fri) by jeeger (subscriber, #104979) [Link] (7 responses) Posted Dec 5, 2025 9:29 UTC (Fri) by taladar (subscriber, #68407) [Link] (6 responses) Posted Dec 5, 2025 9:55 UTC (Fri) by intelfx (subscriber, #130118) [Link] (5 responses) Obviously, yes. What the GP is saying is that functions that are O(1) are also, strictly speaking, O(n), since the big-O notation only defines, informally, an "upper bound" on the algorithmic complexity. (The answer here is that engineers tend to casually use the big-O notation where they really mean Knuth's Θ notation instead.) Posted Dec 7, 2025 12:29 UTC (Sun) by Baughn (subscriber, #124425) [Link] (4 responses) If he'd used uppercase-T instead, then we'd use it. Posted Dec 7, 2025 13:21 UTC (Sun) by excors (subscriber, #95769) [Link] (3 responses) As is often the case, Knuth solved that problem too, by inventing TeX half a century ago. Now we just need LWN to implement server-side KaTeX rendering. Posted Dec 7, 2025 13:57 UTC (Sun) by dskoll (subscriber, #1630) [Link] (2 responses) I solved it in a horrible way. Look up "theta" on Wikipedia, then paste the result: Θ Posted Dec 7, 2025 15:10 UTC (Sun) by adobriyan (subscriber, #30858) [Link] (1 responses) Posted Dec 10, 2025 1:56 UTC (Wed) by raven667 (subscriber, #5198) [Link] Posted Dec 5, 2025 11:45 UTC (Fri) by iabervon (subscriber, #722) [Link] (3 responses) Next, I want a flag to json.loads() that causes it to return hashable values instead of mutable ones (without the caller needing to know how to accomplish that). Posted Dec 6, 2025 0:49 UTC (Sat) by AdamW (subscriber, #48457) [Link] (2 responses) It says frozendicts will be ordered, but hashes and comparisons will not care about the order. So frozendict({"a": "b", "c": "d"}) and frozendict({"c": "d", "a": "b"}) will have the same hash and compare as equal, but they're not really the same? I don't know how I feel about that! Posted Dec 6, 2025 5:02 UTC (Sat) by NYKevin (subscriber, #129325) [Link] Whether this is a problem is debatable, but it is also moot. Non-frozen dicts have behaved this way forever, so making frozendict behave differently would be pretty terrible language design. Posted Dec 6, 2025 5:23 UTC (Sat) by iabervon (subscriber, #722) [Link] The history is that the iterator order used to be unpredictable, so the same object might give different orders when traversed multiple times and objects constructed by adding the items in different order might give the same order when traversed multiple times. However, a more recent implementation of dict started to traverse the items in the order the keys were first added, just because that was more convenient, and then the language changed to guarantee this. Of course, that meant that there was now something you could reliably determine about dicts that wasn't included in the equality rules that had always existed. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;quote&gt; As Steve Dower noted, that optimization can be added later as long as the PEP does not specify that the operation must be O(n) &lt;/quote&gt; I might be misremembering from my Uni days, but all O(1) algorithms are also O(n), so the statement doesn't make sense. I'd be happy for someone to correct me though. &lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Complexity specification &lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;head&gt;Hashable mappings&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46229467</guid><pubDate>Thu, 11 Dec 2025 09:51:47 +0000</pubDate></item><item><title>Craft software that makes people feel something</title><link>https://rapha.land/craft-software-that-makes-people-feel-something/</link><description>&lt;doc fingerprint="2d5e191f3bee3c11"&gt;
  &lt;main&gt;
    &lt;p&gt;So, I woke up today. Got my coffee, family went to sleep, and I have a free afternoon.&lt;/p&gt;
    &lt;p&gt;I thought about writing something. I may delete this article, but if you are reading this, it means I went through with it.&lt;/p&gt;
    &lt;p&gt;Recently, people have been asking me why I’m pausing Boo to work on a programming language. I think it would actually be cool to write down how I feel.&lt;/p&gt;
    &lt;p&gt;Boo is a code editor I created solely for myself; I never had the intention of making it a mainstream editor. Of course, it would be fun if people used it, but that was never my goal. This year I got it working in a functional state, where I can actually use it for my daily work. It has innovative human-keyboard navigation and replaces the LSP system with something faster and less costly for the OS. So why on earth am I not open-sourcing it? That’s what people keep asking me.&lt;/p&gt;
    &lt;p&gt;First, let’s go step by step.&lt;/p&gt;
    &lt;p&gt;My mind isn’t really moved by the idea that it would be a success or a failure — the end user of Boo is me. I don’t feel it’s there yet; in fact, I think software should inspire us. Working on Rio Terminal and Boo in my free time — both written in Rust and sharing many similarities — affects my joy, because it starts to become something automatic. Both have similar architecture, language, release process, and etcetera.&lt;/p&gt;
    &lt;p&gt;Since I was a kid, I liked to build Lego blocks. That’s probably what I did the most besides playing football or video games. The fun thing about Lego is that one day you can build a castle, and the next day you can build a ship. Not necessarily using the same pieces and colors — you can actually add a lot of stuff that’s external to what you have, like a wood stick.&lt;/p&gt;
    &lt;p&gt;When programming becomes repetitive, the odds of you creating something that makes people go “wow” are reduced quite a bit. It isn’t a rule, of course. You need to be inspired to make inspiring software.&lt;/p&gt;
    &lt;p&gt;I always use the example of The Legend of Zelda: Breath of the Wild. This game is so well crafted that I know people who don’t even like video games but bought a console just to play it — and once they finished, they sold everything. This is what I’m talking about: taking time to build something so that once people try it, they remember it for as long as they live.&lt;/p&gt;
    &lt;p&gt;Boo isn’t a business. I don’t need or want to make money out of it. I don’t have a deadline, nor do I want to create another VS Code. I don’t feel like forcing it to happen.&lt;/p&gt;
    &lt;p&gt;In that case, I don’t necessarily need to stop building Lego blocks, right? I’ll just park it there, and when the inspiration comes back, I’ll pick it up where it was. That being said, I paused Boo, and I am working on my own programming language. Eventually, my idea is to rewrite Boo to use it.&lt;/p&gt;
    &lt;p&gt;“Wow! That’s a lot of work.” Indeed. But it’s my hobby stuff. I’ve always loved programming languages, and I am having a blast learning more about binaries and compilers. So, I don’t really feel I need to follow people’s cake recipe for success. That’s how my mind works, and I will stick with it.&lt;/p&gt;
    &lt;p&gt;By the way, this article was written using Boo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231274</guid><pubDate>Thu, 11 Dec 2025 13:45:08 +0000</pubDate></item><item><title>Show HN: An endless scrolling word search game</title><link>https://endless-wordsearch.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231460</guid><pubDate>Thu, 11 Dec 2025 14:01:22 +0000</pubDate></item><item><title>An Orbital House of Cards: Frequent Megaconstellation Close Conjunctions</title><link>https://arxiv.org/abs/2512.09643</link><description>&lt;doc fingerprint="fcbc363a54b394af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Astrophysics &amp;gt; Earth and Planetary Astrophysics&lt;/head&gt;&lt;p&gt; [Submitted on 10 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:An Orbital House of Cards: Frequent Megaconstellation Close Conjunctions&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The number of objects in orbit is rapidly increasing, primarily driven by the launch of megaconstellations, an approach to satellite constellation design that involves large numbers of satellites paired with their rapid launch and disposal. While satellites provide many benefits to society, their use comes with challenges, including the growth of space debris, collisions, ground casualty risks, optical and radio-spectrum pollution, and the alteration of Earth's upper atmosphere through rocket emissions and reentry ablation. There is substantial potential for current or planned actions in orbit to cause serious degradation of the orbital environment or lead to catastrophic outcomes, highlighting the urgent need to find better ways to quantify stress on the orbital environment. Here we propose a new metric, the CRASH Clock, that measures such stress in terms of the time it takes for a catastrophic collision to occur if there are no collision avoidance manoeuvres or there is a severe loss in situational awareness. Our calculations show the CRASH Clock is currently 2.8 days, which suggests there is now little time to recover from a wide-spread disruptive event, such as a solar storm. This is in stark contrast to the pre-megaconstellation era: in 2018, the CRASH Clock was 121 days.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;astro-ph.EP&lt;/p&gt;&lt;p&gt; Change to browse by: &lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46232220</guid><pubDate>Thu, 11 Dec 2025 15:01:44 +0000</pubDate></item><item><title>Launch HN: BrowserBook (YC F24) – IDE for deterministic browser automation</title><link>https://news.ycombinator.com/item?id=46232434</link><description>&lt;doc fingerprint="956206d209fba2cc"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN! We’re Chris, Jorrie, and Evan of BrowserBook, an IDE for writing and debugging Playwright-based web automations. You can download it as a Mac app here: &lt;/p&gt;https://browserbook.com&lt;p&gt;, and there’s a demo video at &lt;/p&gt;https://www.youtube.com/watch?v=ODGJBCNqGUI&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Why we built this: When we were going through YC, we were a company that automated back-office healthcare workflows. Since the interoperability ecosystem in healthcare is so fragmented, we started using browser agents to automate EMRs, practice management software, and payment portals directly through the web. When we did, we ran into a ton of problems:&lt;/p&gt;&lt;p&gt;Speed: High latency on LLM calls vs. a scripting approach&lt;/p&gt;&lt;p&gt;Cost: We burned through tokens with all the context we needed to make the automations reasonably accurate&lt;/p&gt;&lt;p&gt;Reliability: Even with detailed instructions, context, and tools, agents tended to drift on multi-step tasks in unpredictable ways&lt;/p&gt;&lt;p&gt;Debuggability: When drift did occur, we were essentially playing whack-a-mole in our prompt and re-running the whole automation to debug issues (see above: speed and cost issues made this quite painful)&lt;/p&gt;&lt;p&gt;More and more we were just giving our agent scripts to execute. Eventually, we came to the conclusion that scripting is a better approach for web automation for these sort of use cases. But scripting was also too painful, so we set out to solve those problems with BrowserBook.&lt;/p&gt;&lt;p&gt;Under the hood, it runs a standalone TypeScript REPL wired directly into an inline browser instance, with built-in tooling to make script development quick and easy. This includes:&lt;/p&gt;&lt;p&gt;- A fully interactive browser window directly in the IDE so you can run your code without context switching&lt;/p&gt;&lt;p&gt;- A Jupyter-notebook-style environment - the idea here is you can write portions of your automation in individual cells and run them individually (and quickly reset manually in the browser), instead of having to rerun the whole thing every time&lt;/p&gt;&lt;p&gt;- An AI coding assistant which uses the DOM context of the current page to write automation logic, which helps avoid digging around for selectors&lt;/p&gt;&lt;p&gt;- Helper functions for taking screenshots, data extraction, and managed authentication for auth-required workflows.&lt;/p&gt;&lt;p&gt;Once you’ve created your automation, you can run it directly in the application or in our hosted environment via API, so you can use it in external apps or agentic workflows.&lt;/p&gt;&lt;p&gt;At its core, BrowserBook is an Electron app, so we can run a Chrome instance directly in the app without the need for cloud-hosted browsers. For API runs, we use hosted browser infra via Kernel (which is a fantastic product, btw), relying on their bot anti-detection capabilities (stealth mode, proxies, etc.).&lt;/p&gt;&lt;p&gt;Scripted automation can be unpopular because scripts are inherently brittle; unlike “traditional” software development, your code is deployed in an environment you don’t control - someone else’s website. With BrowserBook, we’re trying to “embrace the suck”, and acknowledge this “offensive programming” environment.&lt;/p&gt;&lt;p&gt;We’ve designed from the ground up to assume scripts will break, and aim to provide the tools that make building and maintaining them easier. In the future, our plan is to leverage AI where it has shown its strength already - writing code - to minimize downtime and quickly repair broken scripts as the deployed environment changes.&lt;/p&gt;&lt;p&gt;Browser agents promised to solve this by handing the reins to an LLM which can handle inconsistency and ambiguity. While we think there are some applications where browser agents can be genuinely helpful, tasks that need to be done reliably and repeatedly are not one of them.&lt;/p&gt;&lt;p&gt;We’d love for you to try it out! You can download BrowserBook from our website here: https://browserbook.com (only available for Mac so far, sorry!) And of course, we’d appreciate any feedback and comments you have!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46232434</guid><pubDate>Thu, 11 Dec 2025 15:18:51 +0000</pubDate></item><item><title>Deprecate like you mean it</title><link>https://entropicthoughts.com/deprecate-like-you-mean-it</link><description>&lt;doc fingerprint="caef90947e61562e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deprecate Like You Mean It&lt;/head&gt;
    &lt;p&gt; Seth Larson noticed that people don’t act on deprecation warnings. The &lt;code&gt;response.getheader&lt;/code&gt; method in &lt;code&gt;urllib&lt;/code&gt; has been deprecated since 2023 because
the &lt;code&gt;response.headers&lt;/code&gt; dictionary is what should be used instead. When the
method was eventually removed, lots of code broke.
&lt;/p&gt;
    &lt;p&gt;Deprecation warnings try to solve the fat step function associated with backwards-incompatible api changes, by allowing people to schedule the maintenance burden, rather than having it imposed on them suddenly all at once. The problem is the economic cost of waiting is not tangible. You can ignore the deprecation warning right up until the api change happens, and then it becomes very expensive to delay it further.&lt;/p&gt;
    &lt;p&gt;People aren’t great at planning for sudden changes.&lt;/p&gt;
    &lt;p&gt;What if we intentionally made deprecated functions return the wrong result … sometimes? Every time it intentionally returns the wrong result, it logs the deprecation warning.1 Users that are very sensitive to the correctness of the results might want to swap the wrong result for an artificial delay instead.&lt;/p&gt;
    &lt;p&gt;Initially, it should never return the wrong result. But after it’s been deprecated for a few months, it should start to return the wrong result once every million invocations, say. That would probably not trigger anyone’s midnight pager, but it would make it clear that relying on the deprecated functionality is a bug lurking in the code.&lt;/p&gt;
    &lt;p&gt;Then after a few more months, turn it up to once every ten thousand invocations. It’s probably going to start to hurt a little to delay the maintenance. After a year, make it return the wrong thing once every thousand invocations. At this point, users can only delay maintenance if it’s an unimportant auxiliary usage. And finally, as we bump up into the deadline, it should return the wrong thing every other invocation. Now it’s practically useless, just like when it is removed.&lt;/p&gt;
    &lt;p&gt;This makes the deprecated parts of the api increasingly buggy until they’re removed, and makes the economic tradeoff of when to schedule the maintenance more immediate to users.&lt;/p&gt;
    &lt;p&gt;In case the sarcasm isn’t clear, it’s better to leave the warts. But it is also worthwhile to recognise that in terms of effectiveness for driving system change, signage and warnings are on the bottom of the tier list. We should not be surprised when they don’t work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46232898</guid><pubDate>Thu, 11 Dec 2025 15:52:30 +0000</pubDate></item><item><title>Show HN: GPULlama3.java Llama Compilied to PTX/OpenCL Now Integrated in Quarkus</title><link>https://news.ycombinator.com/item?id=46233009</link><description>&lt;doc fingerprint="5fd77698a7342c38"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;wget &lt;/p&gt;https://github.com/beehive-lab/TornadoVM/releases/download/v...&lt;p&gt; unzip tornadovm-2.1.0-opencl-linux-amd64.zip # Replace &amp;lt;path-to-sdk&amp;gt; manually with the absolute path of the extracted folder export TORNADO_SDK="&amp;lt;path-to-sdk&amp;gt;/tornadovm-2.1.0-opencl" export PATH=$TORNADO_SDK/bin:$PATH&lt;/p&gt;&lt;p&gt;tornado --devices tornado --version&lt;/p&gt;&lt;p&gt;# Navigate to the project directory cd GPULlama3.java&lt;/p&gt;&lt;p&gt;# Source the project-specific environment paths -&amp;gt; this will ensure the source set_paths&lt;/p&gt;&lt;p&gt;# Build the project using Maven (skip tests for faster build) # mvn clean package -DskipTests or just make make&lt;/p&gt;&lt;p&gt;# Run the model (make sure you have downloaded the model file first - see below) ./llama-tornado --gpu --verbose-init --opencl --model beehive-llama-3.2-1b-instruct-fp16.gguf --prompt "tell me a joke"&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46233009</guid><pubDate>Thu, 11 Dec 2025 15:59:33 +0000</pubDate></item><item><title>Things I want to say to my boss</title><link>https://www.ithoughtaboutthatalot.com/2025/the-things-i-want-to-say-to-my-boss</link><description>&lt;doc fingerprint="3a6b951d2fc62591"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm sitting down to write this in a gap between jobs. The downtime is strange, like the world has stopped moving but my thoughts havenât caught up. Other than replaying the shit that went down during the last six months â or to put it more bluntly, the reasons I left, I donât quite know what to do with myself.&lt;/p&gt;
    &lt;p&gt;What happened wasnât unique. And thatâs the part that bothers me most.Â&lt;/p&gt;
    &lt;p&gt;Itâs the same stuff I hear from friends, colleagues, people I trust across the industry.&lt;/p&gt;
    &lt;p&gt;I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.&lt;/p&gt;
    &lt;p&gt;Itâs the performance of âcareâ from leadership. Saying one thing loudly and proudly, yet doing another quietly, repeatedly.&lt;/p&gt;
    &lt;p&gt;I know this is anonymous, but if you think this is about you, then I hope you do your team a favour and listen.&lt;/p&gt;
    &lt;p&gt;You canât fake care. People feel it. In small moments, in the gaps between your words, in the way you prioritise your business over their wellbeing. Care is a practice, not a performance. If you only care when outsiders are watching, youâre just performing.Â&lt;/p&gt;
    &lt;p&gt;Communication isnât optional or a one-way thing. Consistency and honesty build trust. Inconsistency and silence destroy it. If you communicate more externally than with your team, your culture will break down slowly over time.Â&lt;/p&gt;
    &lt;p&gt;Ideas stop being shared because âwhatâs the point?â Itâs not like youâre really listening. Meetings become quieter because speaking up feels risky. Colleagues start shrinking, not because their talent fades, but because the space to use it gets narrower.&lt;/p&gt;
    &lt;p&gt;I hope you learn that leadership is more than LinkedIn posts and conference talks.Â&lt;/p&gt;
    &lt;p&gt;Itâs the day-to-day choices you make when nobodyâs applauding.&lt;/p&gt;
    &lt;p&gt;Burnout isnât a sign of commitment, itâs a sign of organisational failure. If your best people are exhausted, withdrawn, or like shadows of who they once were, thatâs not a resource problem. Thatâs a You problem.&lt;/p&gt;
    &lt;p&gt;By the time you notice a culture is broken, the damage has already been done. People have mentally checked out, or quietly left, or stayed but stopped believing.&lt;/p&gt;
    &lt;p&gt;I hope you learn that leadership is more than LinkedIn posts and conference talks.Â&lt;/p&gt;
    &lt;p&gt;Itâs the day-to-day choices you make when nobodyâs applauding. Itâs the way you treat people when theyâre tired, honest, unwell or âinconvenientâ. Itâs whether your words match your actions, and whether youâre brave enough to admit when they donât.&lt;/p&gt;
    &lt;p&gt;I hope you realise that people donât leave because theyâre unwilling. They leave because you didnât take care of them. You donât get to call yourself âpeople-firstâ when every decision proves otherwise.Â&lt;/p&gt;
    &lt;p&gt;I hope you learn that if you focus on making money instead of the team lining your pockets, you will end up with a broken team and no money.&lt;/p&gt;
    &lt;p&gt;Good leadership isnât complicated, but it is demanding. It asks more of you than your job title does. It asks for self-awareness, not slogans. It asks you to trade the armour of performance for the discomfort of being accountable.&lt;/p&gt;
    &lt;p&gt;In the end, good leadership is never proven by what you say about yourself. Itâs proven by what people say when youâre not in the room.&lt;/p&gt;
    &lt;p&gt;And trust me, theyâre talking.&lt;/p&gt;
    &lt;p&gt;Itâs showing up before the crisis, not after. Itâs noticing when someoneâs energy changes and checking in, not waiting for them to break. Itâs understanding the difference between being busy and being present.&lt;/p&gt;
    &lt;p&gt;Itâs making decisions with people, not about them. Itâs protecting your team from unnecessary chaos rather than generating it. Itâs recognising that transparency isnât a risk, but how trust stays alive.&lt;/p&gt;
    &lt;p&gt;Itâs creating conditions where people want to speak â not because theyâre brave, but because itâs safe. Where the loudest voices donât automatically win.&lt;/p&gt;
    &lt;p&gt;Itâs understanding that care is not soft. Itâs not indulgent. Itâs not a blocker to delivery. Itâs the foundation that makes delivery possible. Care is the thing that keeps people willing to stay, to try, to believe. Care is taking responsibility for the things you say and do, and the culture that results in.&lt;/p&gt;
    &lt;p&gt;If you want loyalty, creativity, honesty, energy, you must earn them. You earn them by being the kind of leader whose actions make it obvious that people matter. Not because itâs good PR. Because itâs your job. And because people matter, and they deserve it.&lt;/p&gt;
    &lt;p&gt;In the end, good leadership is never proven by what you say about yourself. Itâs proven by what people say when youâre not in the room.&lt;/p&gt;
    &lt;p&gt;And trust me, theyâre talking.&lt;/p&gt;
    &lt;p&gt;I've given you too much of my time, attention and energy in 2025. So in 2026, I plan to do the opposite and not give you any more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46233570</guid><pubDate>Thu, 11 Dec 2025 16:35:48 +0000</pubDate></item><item><title>Days since last GitHub incident</title><link>https://github-incidents.pages.dev/</link><description>&lt;doc fingerprint="97dfd99578c49cb4"&gt;
  &lt;main&gt;
    &lt;p&gt;Days since last Github service disruption: 0&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46233798</guid><pubDate>Thu, 11 Dec 2025 16:52:37 +0000</pubDate></item><item><title>Show HN: Sim – Apache-2.0 n8n alternative</title><link>https://github.com/simstudioai/sim</link><description>&lt;doc fingerprint="189ff03c2161b5fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Build and deploy AI agent workflows in minutes.&lt;/p&gt;
    &lt;p&gt;Design agent workflows visually on a canvas—connect agents, tools, and blocks, then run them instantly.&lt;/p&gt;
    &lt;p&gt;Leverage Copilot to generate nodes, fix errors, and iterate on flows directly from natural language.&lt;/p&gt;
    &lt;p&gt;Upload documents to a vector store and let agents answer questions grounded in your specific content.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cloud-hosted: sim.ai&lt;/head&gt;
    &lt;code&gt;npx simstudio&lt;/code&gt;
    &lt;p&gt;Docker must be installed and running on your machine.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-p, --port &amp;lt;port&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Port to run Sim on (default &lt;code&gt;3000&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-pull&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Skip pulling latest Docker images&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/simstudioai/sim.git

# Navigate to the project directory
cd sim

# Start Sim
docker compose -f docker-compose.prod.yml up -d&lt;/code&gt;
    &lt;p&gt;Access the application at http://localhost:3000/&lt;/p&gt;
    &lt;p&gt;Run Sim with local AI models using Ollama - no external APIs required:&lt;/p&gt;
    &lt;code&gt;# Start with GPU support (automatically downloads gemma3:4b model)
docker compose -f docker-compose.ollama.yml --profile setup up -d

# For CPU-only systems:
docker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d&lt;/code&gt;
    &lt;p&gt;Wait for the model to download, then visit http://localhost:3000. Add more models with:&lt;/p&gt;
    &lt;code&gt;docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b&lt;/code&gt;
    &lt;p&gt;If you already have Ollama running on your host machine (outside Docker), you need to configure the &lt;code&gt;OLLAMA_URL&lt;/code&gt; to use &lt;code&gt;host.docker.internal&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Docker Desktop (macOS/Windows)
OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d

# Linux (add extra_hosts or use host IP)
docker compose -f docker-compose.prod.yml up -d  # Then set OLLAMA_URL to your host's IP&lt;/code&gt;
    &lt;p&gt;Why? When running inside Docker, &lt;code&gt;localhost&lt;/code&gt; refers to the container itself, not your host machine. &lt;code&gt;host.docker.internal&lt;/code&gt; is a special DNS name that resolves to the host.&lt;/p&gt;
    &lt;p&gt;For Linux users, you can either:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use your host machine's actual IP address (e.g., &lt;code&gt;http://192.168.1.100:11434&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;extra_hosts: ["host.docker.internal:host-gateway"]&lt;/code&gt;to the simstudio service in your compose file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sim also supports vLLM for self-hosted models with OpenAI-compatible API:&lt;/p&gt;
    &lt;code&gt;# Set these environment variables
VLLM_BASE_URL=http://your-vllm-server:8000
VLLM_API_KEY=your_optional_api_key  # Only if your vLLM instance requires auth&lt;/code&gt;
    &lt;p&gt;When running with Docker, use &lt;code&gt;host.docker.internal&lt;/code&gt; if vLLM is on your host machine (same as Ollama above).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open VS Code with the Remote - Containers extension&lt;/item&gt;
      &lt;item&gt;Open the project and click "Reopen in Container" when prompted&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;bun run dev:full&lt;/code&gt;in the terminal or use the&lt;code&gt;sim-start&lt;/code&gt;alias&lt;list rend="ul"&gt;&lt;item&gt;This starts both the main application and the realtime socket server&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bun runtime&lt;/item&gt;
      &lt;item&gt;PostgreSQL 12+ with pgvector extension (required for AI embeddings)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Sim uses vector embeddings for AI features like knowledge bases and semantic search, which requires the &lt;code&gt;pgvector&lt;/code&gt; PostgreSQL extension.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone and install dependencies:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/simstudioai/sim.git
cd sim
bun install&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up PostgreSQL with pgvector:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You need PostgreSQL with the &lt;code&gt;vector&lt;/code&gt; extension for embedding support. Choose one option:&lt;/p&gt;
    &lt;p&gt;Option A: Using Docker (Recommended)&lt;/p&gt;
    &lt;code&gt;# Start PostgreSQL with pgvector extension
docker run --name simstudio-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=simstudio \
  -p 5432:5432 -d \
  pgvector/pgvector:pg17&lt;/code&gt;
    &lt;p&gt;Option B: Manual Installation&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install PostgreSQL 12+ and the pgvector extension&lt;/item&gt;
      &lt;item&gt;See pgvector installation guide&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up environment:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd apps/sim
cp .env.example .env  # Configure with required variables (DATABASE_URL, BETTER_AUTH_SECRET, BETTER_AUTH_URL)&lt;/code&gt;
    &lt;p&gt;Update your &lt;code&gt;.env&lt;/code&gt; file with the database URL:&lt;/p&gt;
    &lt;code&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up the database:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, configure the database package environment:&lt;/p&gt;
    &lt;code&gt;cd packages/db
cp .env.example .env &lt;/code&gt;
    &lt;p&gt;Update your &lt;code&gt;packages/db/.env&lt;/code&gt; file with the database URL:&lt;/p&gt;
    &lt;code&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"&lt;/code&gt;
    &lt;p&gt;Then run the migrations:&lt;/p&gt;
    &lt;code&gt;bunx drizzle-kit migrate --config=./drizzle.config.ts&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start the development servers:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommended approach - run both servers together (from project root):&lt;/p&gt;
    &lt;code&gt;bun run dev:full&lt;/code&gt;
    &lt;p&gt;This starts both the main Next.js application and the realtime socket server required for full functionality.&lt;/p&gt;
    &lt;p&gt;Alternative - run servers separately:&lt;/p&gt;
    &lt;p&gt;Next.js app (from project root):&lt;/p&gt;
    &lt;code&gt;bun run dev&lt;/code&gt;
    &lt;p&gt;Realtime socket server (from &lt;code&gt;apps/sim&lt;/code&gt; directory in a separate terminal):&lt;/p&gt;
    &lt;code&gt;cd apps/sim
bun run dev:sockets&lt;/code&gt;
    &lt;p&gt;Copilot is a Sim-managed service. To use Copilot on a self-hosted instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to https://sim.ai → Settings → Copilot and generate a Copilot API key&lt;/item&gt;
      &lt;item&gt;Set &lt;code&gt;COPILOT_API_KEY&lt;/code&gt;environment variable in your self-hosted apps/sim/.env file to that value&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key environment variables for self-hosted deployments (see &lt;code&gt;apps/sim/.env.example&lt;/code&gt; for full list):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DATABASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;PostgreSQL connection string with pgvector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BETTER_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Auth secret (&lt;code&gt;openssl rand -hex 32&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BETTER_AUTH_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Your app URL (e.g., &lt;code&gt;http://localhost:3000&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;NEXT_PUBLIC_APP_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Public app URL (same as above)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENCRYPTION_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Encryption key (&lt;code&gt;openssl rand -hex 32&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;OLLAMA_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Ollama server URL (default: &lt;code&gt;http://localhost:11434&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;VLLM_BASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;vLLM server URL for self-hosted models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;COPILOT_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;API key from sim.ai for Copilot features&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're running Ollama on your host machine and Sim in Docker, change &lt;code&gt;OLLAMA_URL&lt;/code&gt; from &lt;code&gt;localhost&lt;/code&gt; to &lt;code&gt;host.docker.internal&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d&lt;/code&gt;
    &lt;p&gt;See Using an External Ollama Instance for details.&lt;/p&gt;
    &lt;p&gt;Ensure PostgreSQL has the pgvector extension installed. When using Docker, wait for the database to be healthy before running migrations.&lt;/p&gt;
    &lt;p&gt;If ports 3000, 3002, or 5432 are in use, configure alternatives:&lt;/p&gt;
    &lt;code&gt;# Custom ports
NEXT_PUBLIC_APP_URL=http://localhost:3100 POSTGRES_PORT=5433 docker compose up -d&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework: Next.js (App Router)&lt;/item&gt;
      &lt;item&gt;Runtime: Bun&lt;/item&gt;
      &lt;item&gt;Database: PostgreSQL with Drizzle ORM&lt;/item&gt;
      &lt;item&gt;Authentication: Better Auth&lt;/item&gt;
      &lt;item&gt;UI: Shadcn, Tailwind CSS&lt;/item&gt;
      &lt;item&gt;State Management: Zustand&lt;/item&gt;
      &lt;item&gt;Flow Editor: ReactFlow&lt;/item&gt;
      &lt;item&gt;Docs: Fumadocs&lt;/item&gt;
      &lt;item&gt;Monorepo: Turborepo&lt;/item&gt;
      &lt;item&gt;Realtime: Socket.io&lt;/item&gt;
      &lt;item&gt;Background Jobs: Trigger.dev&lt;/item&gt;
      &lt;item&gt;Remote Code Execution: E2B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please see our Contributing Guide for details.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Made with ❤️ by the Sim Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234186</guid><pubDate>Thu, 11 Dec 2025 17:20:11 +0000</pubDate></item><item><title>Litestream VFS</title><link>https://fly.io/blog/litestream-vfs/</link><description>&lt;doc fingerprint="67cf1ef5a02c7b1b"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. Itâs free, open-source software that should run anywhere, and you can read more about it here.&lt;/p&gt;
    &lt;p&gt;Again with the sandwiches: assume we’ve got a SQLite database of sandwich ratings, and we’ve backed it up with Litestream to an S3 bucket.&lt;/p&gt;
    &lt;p&gt;Now, on our local host, load up AWS credentials and an S3 path into our environment. Open SQLite and:&lt;/p&gt;
    &lt;code&gt;$ sqlite3
SQLite version 3.50.4 2025-07-30 19:33:53
sqlite&amp;gt; .load litestream.so
sqlite&amp;gt; .open file:///my.db?vfs=litestream
&lt;/code&gt;
    &lt;p&gt;SQLite is now working from that remote database, defined by the Litestream backup files in the S3 path we configured. We can query it:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
22|Veggie Delight|New York|4
30|Meatball|Los Angeles|5
168|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;This is Litestream VFS. It runs SQLite hot off an object storage URL. As long as you can load the shared library our tree builds for you, it’ll work in your application the same way it does in the SQLite shell.&lt;/p&gt;
    &lt;p&gt;Fun fact: we didn’t have to download the whole database to run this query. More about this in a bit.&lt;/p&gt;
    &lt;p&gt;Meanwhile, somewhere in prod, someone has it in for meatball subs and wants to knock them out of the bracket â oh, fuck:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; UPDATE sandwich_ratings SET stars = 1 ;
&lt;/code&gt;
    &lt;p&gt;They forgot the &lt;code&gt;WHERE&lt;/code&gt; clause!&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
97|French Dip|Los Angeles|1
140|BÃ¡nh MÃ¬|San Francisco|1
62|Italian Beef|Chicago|1
&lt;/code&gt;
    &lt;p&gt;Italian Beefs and BÃ¡nh MÃ¬s, all at 1 star. Disaster!&lt;/p&gt;
    &lt;p&gt;But wait, back on our dev machine:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; PRAGMA litestream_time = '5 minutes ago'; 
sqlite&amp;gt; select * from sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
30|Meatball|Los Angeles|5
33|Ham &amp;amp; Swiss|Los Angeles|2
163|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;We’re now querying that database from a specific point in time in our backups. We can do arbitrary relative timestamps, or absolute ones, like &lt;code&gt;2000-01-01T00:00:00Z&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What we’re doing here is instantaneous point-in-time recovery (PITR), expressed simply in SQL and SQLite pragmas.&lt;/p&gt;
    &lt;p&gt;Ever wanted to do a quick query against a prod dataset, but didn’t want to shell into a prod server and fumble with the &lt;code&gt;sqlite3&lt;/code&gt; terminal command like a hacker in an 80s movie? Or needed to do a quick sanity check against yesterday’s data, but without doing a full database restore? Litestream VFS makes that easy. I’m so psyched about how it turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;Litestream v0.5 integrates LTX, our SQLite data-shipping file format. Where earlier Litestream blindly shipped whole raw SQLite pages to and from object storage, LTX ships ordered sets of pages. We built LTX for LiteFS, which uses a FUSE filesystem to do transaction-aware replication for unmodified applications, but we’ve spent this year figuring out ways to use LTX in Litestream, without all that FUSE drama.&lt;/p&gt;
    &lt;p&gt;The big thing LTX gives us is “compaction”. When we restore a database from object storage, we want the most recent versions of each changed database page. What we don’t want are all the intermediate versions of those pages that occurred prior to the most recent change.&lt;/p&gt;
    &lt;p&gt;Imagine, at the time we’re restoring, we’re going to need pages 1, 2, 3, 4, and 5. Depending on the order in which pages were written, the backup data set might look something like &lt;code&gt;1 2 3 5 3 5 4 5 5&lt;/code&gt;. What we want is the rightmost  5, 4, 3, 2, and 1, without wasting time on the four “extra” page 5’s and the one “extra” page 3. Those “extra” pages are super common in SQLite data sets; for instance, every busy table with an autoincrementing primary key will have them.&lt;/p&gt;
    &lt;p&gt;LTX lets us skip the redundant pages, and the algorithm is trivial: reading backwards from the end of the sequence, skipping any page you already read. This drastically accelerates restores.&lt;/p&gt;
    &lt;p&gt;But LTX compaction isn’t limited to whole databases. We can also LTX-compact sets of LTX files. That’s the key to how PITR restores with Litestream now work.&lt;/p&gt;
    &lt;p&gt;In the diagram below, we’re taking daily full snapshots. Below those snapshots are “levels” of changesets: groups of database pages from smaller and smaller windows of time. By default, Litestream uses time intervals of 1 hour at the highest level, down to 30 seconds at level 1. L0 is a special level where files are uploaded every second, but are only retained until being compacted to L1.&lt;/p&gt;
    &lt;p&gt;Now, let’s do a PITR restore. Start from the most proximal snapshot. Then determine the minimal set of LTX files from each level to reach the time you are restoring to.&lt;/p&gt;
    &lt;p&gt;We have another trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;LTX trailers include a small index tracking the offset of each page in the file. By fetching only these index trailers from the LTX files we’re working with (each occupies about 1% of its LTX file), we can build a lookup table of every page in the database. Since modern object storage providers all let us fetch slices of files, we can perform individual page reads against S3 directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It’s Implemented&lt;/head&gt;
    &lt;p&gt;SQLite has a plugin interface for things like this: the “VFS” interface. VFS plugins abstract away the bottom-most layer of SQLite, the interface to the OS. If you’re using SQLite now, you’re already using some VFS module, one SQLite happens to ship with.&lt;/p&gt;
    &lt;p&gt;For Litestream users, there’s a catch. From the jump, we’ve designed Litestream to run alongside unmodified SQLite applications. Part of what makes Litestream so popular is that your apps don’t even need to know it exists. It’s “just” a Unix program.&lt;/p&gt;
    &lt;p&gt;That Litestream Unix program still does PITR restores, without any magic. But to do fast PITR-style queries straight off S3, we need more. To make those queries work, you have to load and register Litestream’s VFS module.&lt;/p&gt;
    &lt;p&gt;But that’s all that changes.&lt;/p&gt;
    &lt;p&gt;In particular: Litestream VFS doesn’t replace the SQLite library you’re already using. It’s not a new “version” of SQLite. It’s just a plugin for the SQLite you’re already using.&lt;/p&gt;
    &lt;p&gt;Still, we know that’s not going to work for everybody, and even though we’re really psyched about these PITR features, we’re not taking our eyes off the ball on the rest of Litestream. You don’t have to use our VFS library to use Litestream, or to get the other benefits of the new LTX code.&lt;/p&gt;
    &lt;p&gt;The way a VFS library works, we’re given just a couple structures, each with a bunch of methods defined on them. We override only the few methods we care about. Litestream VFS handles only the read side of SQLite. Litestream itself, running as a normal Unix program, still handles the “write” side. So our VFS subclasses just enough to find LTX backups and issue queries.&lt;/p&gt;
    &lt;p&gt;With our VFS loaded, whenever SQLite needs to read a page into memory, it issues a &lt;code&gt;Read()&lt;/code&gt; call through our library. The read call includes the byte offset at which SQLite expected to find the page. But with Litestream VFS, that byte offset is an illusion.&lt;/p&gt;
    &lt;p&gt;Instead, we use our knowledge of the page size along with the requested page number to do a lookup on the page index we’ve built. From it, we get the remote filename, the “real” byte offset into that file, and the size of the page. That’s enough for us to use the S3 API’s &lt;code&gt;Range&lt;/code&gt; header handling to download exactly the block we want.&lt;/p&gt;
    &lt;p&gt;To save lots of S3 calls, Litestream VFS implements an LRU cache. Most databases have a small set of “hot” pages â inner branch pages or the leftmost leaf pages for tables with an auto-incrementing ID field. So only a small percentage of the database is updated and queried regularly.&lt;/p&gt;
    &lt;p&gt;Weâve got one last trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;Quickly building an index and restore plan for the current state of a database is cool. But we can do one better.&lt;/p&gt;
    &lt;p&gt;Because Litestream backs up (into the L0 layer) once per second, the VFS code can simply poll the S3 path, and then incrementally update its index. The result is a near-realtime replica. Better still, you donât need to stream the whole database back to your machine before you use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Eat Your Heart Out, Marty McFly&lt;/head&gt;
    &lt;p&gt;Litestream holds backup files for every state your database has been in, with single-second resolution, for as long as you want it to. Forgot the &lt;code&gt;WHERE&lt;/code&gt; clause on a &lt;code&gt;DELETE&lt;/code&gt; statement? Updating your database state to where it was an hour (or day, or week) ago is just a matter of adjusting the LTX indices Litestream manages.&lt;/p&gt;
    &lt;p&gt;All this smoke-and-mirrors of querying databases without fully fetching them has another benefit: it starts up really fast! We’re living an age of increasingly ephemeral servers, what with the AIs and the agents and the clouds and the hoyvin-glavins. Wherever you find yourself, if your database is backed up to object storage with Litestream, you’re always in a place where you can quickly issue a query.&lt;/p&gt;
    &lt;p&gt;As always, one of the big things we think we’re doing right with Litestream is: we’re finding ways to get as much whiz-bang value as we can (instant PITR reading live off object storage: pretty nifty!) while keeping the underlying mechanism simple enough that you can fit your head around it.&lt;/p&gt;
    &lt;p&gt;Litestream is solid for serious production use (we rely on it for important chunks of our own Fly.io APIs). But you could write Litestream yourself, just from the basic ideas in these blog posts. We think that’s a point in its favor. We land there because the heavy lifting in Litestream is being done by SQLite itself, which is how it should be.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234710</guid><pubDate>Thu, 11 Dec 2025 17:59:10 +0000</pubDate></item><item><title>GPT-5.2</title><link>https://openai.com/index/introducing-gpt-5-2/</link><description>&lt;doc fingerprint="285fa2d92df7ce9f"&gt;
  &lt;main&gt;
    &lt;p&gt;We are introducing GPT‑5.2, the most capable model series yet for professional knowledge work.&lt;/p&gt;
    &lt;p&gt;Already, the average ChatGPT Enterprise user says AI saves them 40–60 minutes a day, and heavy users say it saves them more than 10 hours a week. We designed GPT‑5.2 to unlock even more economic value for people; it’s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long contexts, using tools, and handling complex, multi-step projects.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 sets a new state of the art across many benchmarks, including GDPval, where it outperforms industry professionals at well-specified knowledge work tasks spanning 44 occupations.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 Thinking&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.8% (GPT‑5)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;50.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-bench Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPQA Diamond (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;92.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.1%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;CharXiv Reasoning (w/ Python)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;AIME 2025 (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;100.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;94.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 1–3)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;40.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;31.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 4)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12.5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-1 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;86.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;72.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-2 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;52.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;17.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notion(opens in a new window), Box(opens in a new window), Shopify(opens in a new window), Harvey(opens in a new window) and Zoom(opens in a new window) observed GPT‑5.2 demonstrates state-of-the-art long-horizon reasoning and tool-calling performance. Databricks(opens in a new window), Hex(opens in a new window) and Triple Whale(opens in a new window) found GPT‑5.2 to be exceptional at agentic data science and document analysis tasks. Cognition(opens in a new window), Warp(opens in a new window), Charlie Labs(opens in a new window), JetBrains(opens in a new window) and Augment Code(opens in a new window) say GPT‑5.2 delivers state-of-the-art agentic coding performance, with measurable improvements in areas such as interactive coding, code reviews and bug finding.&lt;/p&gt;
    &lt;p&gt;In ChatGPT, GPT‑5.2 Instant, Thinking, and Pro will begin rolling out today, starting with paid plans. In the API, they are available now to all developers.&lt;/p&gt;
    &lt;p&gt;Overall, GPT‑5.2 brings significant improvements in general intelligence, long-context understanding, agentic tool-calling, and vision—making it better at executing complex, real-world tasks end-to-end than any previous model.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is the best model yet for real-world, professional use. On GDPval, an eval measuring well-specified knowledge work tasks across 44 occupations, GPT‑5.2 Thinking sets a new state-of-the-art score, and is our first model that performs at or above a human expert level. Specifically, GPT‑5.2 Thinking beats or ties top industry professionals on 70.9% of comparisons on GDPval knowledge work tasks, according to expert human judges. These tasks include making presentations, spreadsheets, and other artifacts. GPT‑5.2 Thinking produced outputs for GDPval tasks at &amp;gt;11x the speed and &amp;lt;1% the cost of expert professionals, suggesting that when paired with human oversight, GPT‑5.2 can help with professional work. Speed and cost estimates are based on historical metrics; speed in ChatGPT may vary.&lt;/p&gt;
    &lt;p&gt;When reviewing one especially good output, one GDPval judge commented, "It is an exciting and noticeable leap in output quality... [it] appears to have been done by a professional company with staff, and has a surprisingly well designed layout and advice for both deliverables, though with one we still have some minor errors to correct."&lt;/p&gt;
    &lt;p&gt;Additionally, on our internal benchmark of junior investment banking analyst spreadsheet modeling tasks—such as putting together a three-statement model for a Fortune 500 company with proper formatting and citations, or building a leveraged buyout model for a take-private—GPT 5.2 Thinking's average score per task is 9.3% higher than GPT‑5.1’s, rising from 59.1% to 68.4%.&lt;/p&gt;
    &lt;p&gt;Side-by-side comparisons show improved sophistication and formatting in spreadsheets and slides generated by GPT‑5.2 Thinking:&lt;/p&gt;
    &lt;p&gt;To use the new spreadsheet and presentation capabilities in ChatGPT, you must be on a Plus, Pro, Business, or Enterprise plan and select either GPT‑5.2 Thinking or Pro. Complex generations can take many minutes to produce.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art of 55.6% on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Unlike SWE-bench Verified, which only tests Python, SWE-Bench Pro tests four languages and aims to be more contamination-resistant, challenging, diverse, and industrially relevant.&lt;/p&gt;
    &lt;p&gt;On SWE-bench Verified (not plotted), GPT‑5.2 Thinking scores our new high of 80%.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this translates into a model that can more reliably debug production code, implement feature requests, refactor large codebases, and ship fixes end-to-end with less manual intervention.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is also better at front-end software engineering than GPT‑5.1 Thinking. Early testers found it significantly stronger at front-end development and complex or unconventional UI work—especially involving 3D elements—making it a powerful daily partner for engineers across the stack. See a few examples of what it can produce from a single prompt:&lt;/p&gt;
    &lt;p&gt;Early testers shared their feedback on GPT‑5.2’s coding capabilities:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 represents the biggest leap for GPT models in agentic coding since GPT-5 and is a SOTA coding model in its price range. The version bump undersells the jump in intelligence. We’re excited to make it the default across Windsurf and several core Devin workloads."&lt;/quote&gt;
    &lt;p&gt;GPT‑5.2 Thinking hallucinates less than GPT‑5.1 Thinking. On a set of de-identified queries from ChatGPT, responses with errors were 30%rel less common. For professionals, this means fewer mistakes when using the model for research, writing, analysis, and decision support—making the model more dependable for everyday knowledge work.&lt;/p&gt;
    &lt;p&gt;Like all models, GPT‑5.2 Thinking is imperfect. For anything critical, double check its answers.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art in long-context reasoning, achieving leading performance on OpenAI MRCRv2—an evaluation that tests a model’s ability to integrate information spread across long documents. On real-world tasks like deep document analysis, which require related information across hundreds of thousands of tokens, GPT‑5.2 Thinking is substantially more accurate than GPT‑5.1 Thinking. In particular, it’s the first model we’ve seen that achieves near 100% accuracy on the 4-needle MRCR variant (out to 256k tokens).&lt;/p&gt;
    &lt;p&gt;In practical terms, this enables professionals to use GPT‑5.2 to work with long documents—such as reports, contracts, research papers, transcripts, and multi-file projects—while maintaining coherence and accuracy across hundreds of thousands of tokens. This makes GPT‑5.2 especially well suited for deep analysis, synthesis, and complex multi-source workflows.&lt;/p&gt;
    &lt;p&gt;For tasks that benefit from thinking beyond the maximum context window, GPT‑5.2 Thinking is compatible with our new Responses &lt;code&gt;/compact&lt;/code&gt; endpoint, which extends the model’s effective context window. This lets GPT‑5.2 Thinking tackle more tool-heavy, long-running workflows that would otherwise be limited by context length. Read more in our API documentation(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is our strongest vision model yet, cutting error rates roughly in half on chart reasoning and software interface understanding.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this means the model can more accurately interpret dashboards, product screenshots, technical diagrams, and visual reports—supporting workflows in finance, operations, engineering, design, and customer support where visual information is central.&lt;/p&gt;
    &lt;p&gt;Compared to previous models, GPT‑5.2 Thinking has a stronger grasp of how elements are positioned within an image, which helps on tasks where relative layout plays a key role in solving the problem. In the example below, we ask the model to identify the components in an image input (in this case, a motherboard) and return labels with approximate bounding boxes. Even on a low-quality image, GPT‑5.2 identifies the main regions and places boxes that roughly match the true locations of each component, while GPT‑5.1 only labels a few parts and shows a much weaker understanding of their spatial arrangement.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;GPT‑5.2 Thinking achieves a new state of the art of 98.7% on Tau2-bench Telecom, demonstrating its ability to reliably use tools across long, multi-turn tasks.&lt;/p&gt;
    &lt;p&gt;For latency-sensitive use cases, GPT‑5.2 Thinking also performs much better at reasoning.effort='none', substantially outperforming GPT‑5.1 and GPT‑4.1.&lt;/p&gt;
    &lt;p&gt;For professionals, this translates into stronger end-to-end workflows—such as resolving customer support cases, pulling data from multiple systems, running analyses, and generating final outputs with fewer breakdowns between steps.&lt;/p&gt;
    &lt;p&gt;For example, when asking a complex customer service question that requires multi-step resolution, the model can more effectively coordinate a full workflow across multiple agents. In the case below, a traveler reports a delayed flight, a missed connection, an overnight stay in New York, and a medical seating requirement. GPT‑5.2 manages the entire chain of tasks—rebooking, special-assistance seating, and compensation—delivering a more complete outcome than GPT‑5.1.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;One of our hopes for AI is that it will accelerate scientific research for the benefit of everyone. Toward this, we’ve been working with and listening to scientists to see how AI can speed up their work, and last month we shared some early collaborative experiments here.&lt;/p&gt;
    &lt;p&gt;We believe GPT‑5.2 Pro and GPT‑5.2 Thinking are the world’s best models for assisting and accelerating scientists. On GPQA Diamond, a graduate-level Google-proof Q&amp;amp;A benchmark, GPT‑5.2 Pro achieves 93.2%, followed closely by GPT‑5.2 Thinking at 92.4%.&lt;/p&gt;
    &lt;p&gt;On FrontierMath (Tier 1–3), an evaluation of expert-level mathematics, GPT‑5.2 Thinking set a new state of the art, solving 40.3% of problems.&lt;/p&gt;
    &lt;p&gt;We're beginning to see AI models meaningfully accelerate progress in math and science in tangible ways. For example, in recent work with GPT‑5.2 Pro, researchers explored an open question in statistical learning theory. In a narrow, well-specified setting, the model proposed a proof that was subsequently verified by the authors and reviewed with external experts, illustrating how frontier models can assist mathematical research under close human oversight.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-1 (Verified), a benchmark designed to measure general reasoning ability, GPT‑5.2 Pro is the first model to cross the 90% threshold, improving from 87%(opens in a new window) by o3‑preview last year while reducing the cost of achieving that performance by roughly 390×.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-2 (Verified), which raises the difficulty and better isolates fluid reasoning, GPT‑5.2 Thinking achieves a new state of the art for chain-of-thought models, scoring 52.9%. GPT‑5.2 Pro performs even higher, reaching 54.2%, further extending the model’s ability to reason through novel, abstract problems.&lt;/p&gt;
    &lt;p&gt;Improvements across these evaluations reflect GPT‑5.2’s stronger multi-step reasoning, greater quantitative accuracy, and more reliable problem solving on complex technical tasks.&lt;/p&gt;
    &lt;p&gt;Here’s what our early testers say about GPT‑5.2:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 unlocked a complete architecture shift for us. We collapsed a fragile, multi-agent system into a single mega-agent with 20+ tools. The best part is, it just works. The mega-agent is faster, smarter, and 100x easier to maintain. We’re seeing dramatically lower latency, much stronger tool calling, and we no longer need sprawling system prompts because 5.2 will execute cleanly off a simple, one-line prompt. It feels like pure magic."&lt;/quote&gt;
    &lt;p&gt;In ChatGPT, users should notice GPT‑5.2 feels better to use day to day—more structured, more reliable, and still enjoyable to talk to.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Instant is a fast, capable workhorse for everyday work and learning, with clear improvements in info-seeking questions, how-tos and walk-throughs, technical writing, and translation, building on the warmer conversational tone introduced in GPT‑5.1 Instant. Early testers particularly noted clearer explanations that surface key information upfront.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is designed for deeper work, helping users tackle more complex tasks with greater polish—especially for coding, summarizing long documents, answering questions about uploaded files, working through math and logic step by step, and supporting planning and decisions with clearer structure and more useful detail.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Pro is our smartest and most trustworthy option for difficult questions where a higher-quality answer is worth the wait, with early testing showing fewer major errors and stronger performance in complex domains like programming.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 builds on the safe completion research we introduced with GPT‑5, which teaches the model to give the most helpful answer while still staying within safety boundaries.&lt;/p&gt;
    &lt;p&gt;With this release, we continued our work to strengthen our models’ responses in sensitive conversations, with meaningful improvements in how they respond to prompts indicating signs of suicide or self harm, mental health distress, or emotional reliance on the model. These targeted interventions have resulted in fewer undesirable responses in both GPT‑5.2 Instant and GPT‑5.2 Thinking as compared to GPT‑5.1 and GPT‑5 Instant and Thinking models. Further details can be found in the system card.&lt;/p&gt;
    &lt;p&gt;We’re in the early stages of rolling out our age prediction model so that we can automatically apply content protections for users who are under 18, in order to limit access to sensitive content. This builds on our existing approach to users we know are under 18 and our parental controls.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is one step in an ongoing series of improvements, and we’re far from done. While this release delivers meaningful gains in intelligence and productivity, we know there are areas where people want more. In ChatGPT, we’re working on known issues like over-refusals, while continuing to raise the bar on safety and reliability overall. These changes are complex, and we’re focused on getting them right.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Mental health&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.995&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.883&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.915&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.684&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emotional reliance&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.945&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.955&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.785&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Self-harm&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.925&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.963&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.937&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In ChatGPT, we’ll begin rolling out GPT‑5.2 (Instant, Thinking, and Pro) today, starting with paid plans (Plus, Pro, Go, Business, Enterprise). We deploy GPT‑5.2 gradually to keep ChatGPT as smooth and reliable as we can; if you don’t see it at first, please try again later. In ChatGPT, GPT‑5.1 will still be available to paid users for three months under legacy models, after which we will sunset GPT‑5.1.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;API&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Instant&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2-chat-latest&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In our API Platform, GPT‑5.2 Thinking is available today in the Responses API and Chat Completions API as &lt;code&gt;gpt-5.2&lt;/code&gt;, and GPT‑5.2 Instant as &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt;. GPT‑5.2 Pro is available in the Responses API as &lt;code&gt;gpt-5.2-pro&lt;/code&gt;. Developers can now set the reasoning parameter in GPT‑5.2 Pro, and both GPT‑5.2 Pro and GPT‑5.2 Thinking now support the new fifth reasoning effort of xhigh, for tasks where quality is most important.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is priced at $1.75/1M input tokens and $14/1M output tokens, with a 90% discount on cached inputs. On multiple agentic evals, we found that despite GPT‑5.2’s greater cost per token, the cost of attaining a given level of quality ended up less expensive due to GPT‑5.2’s greater token efficiency.&lt;/p&gt;
    &lt;p&gt;While ChatGPT subscription pricing remains the same, in the API GPT‑5.2 is priced higher per token than GPT‑5.1 because it is a more capable model. It’s still priced below other frontier models, so people can continue to use it deeply in their daily work and core applications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Model&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cached input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Output&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.75&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.175&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$14&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$21&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$168&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.1 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.25&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.125&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$10&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$15&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$120&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We have no current plans to deprecate GPT‑5.1, GPT‑5, or GPT‑4.1 in the API and will communicate any deprecation plans with ample advance notice for developers. While GPT‑5.2 will work well out of the box in Codex, we expect to release a version of GPT‑5.2 optimized for Codex in the coming weeks.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 was built in collaboration with our long-standing partners NVIDIA and Microsoft. Azure data centers and NVIDIA GPUs, including H100, H200, and GB200-NVL72, underpin OpenAI’s at-scale training infrastructure, driving significant gains in model intelligence. Together, this collaboration allows us to scale compute with confidence and bring new models to market more quickly.&lt;/p&gt;
    &lt;p&gt;Below, we report comprehensive benchmark scores for GPT‑5.2 Thinking, along with a subset for GPT‑5.2 Pro.&lt;/p&gt;
    &lt;head rend="h5"&gt;Professional&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, wins or ties)&lt;/cell&gt;
        &lt;cell&gt;70.9%&lt;/cell&gt;
        &lt;cell&gt;74.1%&lt;/cell&gt;
        &lt;cell&gt;38.8% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, clear wins)&lt;/cell&gt;
        &lt;cell&gt;49.8%&lt;/cell&gt;
        &lt;cell&gt;60.0%&lt;/cell&gt;
        &lt;cell&gt;35.5% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (no ties)&lt;/cell&gt;
        &lt;cell&gt;61.0%&lt;/cell&gt;
        &lt;cell&gt;67.6%&lt;/cell&gt;
        &lt;cell&gt;37.1% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Investment banking spreadsheet tasks (internal)&lt;/cell&gt;
        &lt;cell&gt;68.4%&lt;/cell&gt;
        &lt;cell&gt;71.7%&lt;/cell&gt;
        &lt;cell&gt;59.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Coding&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-Bench Pro, Public&lt;/cell&gt;
        &lt;cell&gt;55.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;80.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SWE-Lancer, IC Diamond*&lt;/cell&gt;
        &lt;cell&gt;74.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;69.7%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Factuality&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ChatGPT answers without errors (w/ search)&lt;/cell&gt;
        &lt;cell&gt;93.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatGPT answers without errors (no search)&lt;/cell&gt;
        &lt;cell&gt;88.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;87.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Long context&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 4k–8k&lt;/cell&gt;
        &lt;cell&gt;98.2%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;65.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 8k–16k&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;47.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 16k–32k&lt;/cell&gt;
        &lt;cell&gt;95.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 32k–64k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 64k–128k&lt;/cell&gt;
        &lt;cell&gt;85.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 128k–256k&lt;/cell&gt;
        &lt;cell&gt;77.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;29.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 128k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 256k&lt;/cell&gt;
        &lt;cell&gt;89.8%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GraphWalks bfs &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Graphwalks parents &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;89.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;71.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Vision&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (no tools)&lt;/cell&gt;
        &lt;cell&gt;82.1%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;67.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;88.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;80.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (no tools)&lt;/cell&gt;
        &lt;cell&gt;79.5%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;80.4%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;79.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video MMMU (no tools)&lt;/cell&gt;
        &lt;cell&gt;85.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;82.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Screenspot Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;86.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;64.2%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Tool usage&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Telecom&lt;/cell&gt;
        &lt;cell&gt;98.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Retail&lt;/cell&gt;
        &lt;cell&gt;82.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;65.8%&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Scale MCP-Atlas&lt;/cell&gt;
        &lt;cell&gt;60.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Toolathlon&lt;/cell&gt;
        &lt;cell&gt;46.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Academic&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPQA Diamond (no tools)&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;93.2%&lt;/cell&gt;
        &lt;cell&gt;88.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (no tools)&lt;/cell&gt;
        &lt;cell&gt;34.5%&lt;/cell&gt;
        &lt;cell&gt;36.6%&lt;/cell&gt;
        &lt;cell&gt;25.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (w/ search, Python)&lt;/cell&gt;
        &lt;cell&gt;45.5%&lt;/cell&gt;
        &lt;cell&gt;50.0%&lt;/cell&gt;
        &lt;cell&gt;42.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMLU&lt;/cell&gt;
        &lt;cell&gt;89.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HMMT, Feb 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;99.4%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;96.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;AIME 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FrontierMath Tier 1–3 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;40.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;31.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;FrontierMath Tier 4 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;14.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;12.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Abstract reasoning&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ARC-AGI-1 (Verified)&lt;/cell&gt;
        &lt;cell&gt;86.2%&lt;/cell&gt;
        &lt;cell&gt;90.5%&lt;/cell&gt;
        &lt;cell&gt;72.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ARC-AGI-2 (Verified)&lt;/cell&gt;
        &lt;cell&gt;52.9%&lt;/cell&gt;
        &lt;cell&gt;54.2% (high)&lt;/cell&gt;
        &lt;cell&gt;17.6%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Models were run with maximum available reasoning effort in our API (xhigh for GPT‑5.2 Thinking &amp;amp; Pro, and high for GPT‑5.1 Thinking), except for the professional evals, where GPT‑5.2 Thinking was run with reasoning effort heavy, the maximum available in ChatGPT Pro. Benchmarks were conducted in a research environment, which may provide slightly different output from production ChatGPT in some cases.&lt;/p&gt;
    &lt;p&gt;* For SWE-Lancer, we omit 40/237 problems that did not run on our infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234788</guid><pubDate>Thu, 11 Dec 2025 18:04:47 +0000</pubDate></item><item><title>Rivian Unveils Custom Silicon, R2 Lidar Roadmap, and Universal Hands Free</title><link>https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/</link><description>&lt;doc fingerprint="d8713350982ecbc9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rivian Unveils Custom Silicon, R2 LiDAR Roadmap, Universal Hands Free, and Its Next Gen Autonomy Platform&lt;/head&gt;
    &lt;p&gt;RJ opened the first ever Autonomy and AI Day explaining why Rivian believes it is positioned to lead in this next phase of the industry. The company is leaning hard into compute, custom hardware, large scale AI systems, and a shared data foundation that touches every part of the ownership experience.&lt;/p&gt;
    &lt;p&gt;Let’s break it all down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet the Rivian Autonomy Processor&lt;/head&gt;
    &lt;p&gt;One of the biggest announcements was RAP1, Rivian’s first in house processor built on a 5nm multi chip module. It delivers 1600 sparse INT8 TOPS and can push 5 billion pixels per second inside the new Gen 3 Autonomy Computer. Rivian even built its own AI compiler and platform software to support it. This shows Rivian is no longer just integrating off the shelf chips, it is now designing silicon specifically for its autonomy roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy Computer and LiDAR on R2&lt;/head&gt;
    &lt;p&gt;The ACM3 (Autonomy Compute Module 3) autonomy computer will debut on R2 starting at the end of 2026, but Rivian made it clear that R2 will launch initially without LiDAR. What Rivian confirmed today is that LiDAR will be added later in the program. This lines up with what we explored back in May when we spotted early signs that Rivian was evaluating LiDAR as a redundancy and ground truth layer for future autonomy. Rivian has now officially validated that LiDAR is coming to R2 down the road, where it will join cameras and radar to create a richer, more resilient perception stack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large Driving Model and Rivian’s Data Loop&lt;/head&gt;
    &lt;p&gt;Rivian explained how its autonomy stack is powered by a self improving data loop feeding the company’s Large Driving Model, which is trained similarly to an LLM. Reinforcement learning distills high quality driving behavior into efficient onboard models. Every release improves the system, and Rivian laid out a trajectory that moves toward point to point, eyes off and eventually personal Level 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universal Hands Free Coming to Gen 2&lt;/head&gt;
    &lt;p&gt;Rivian confirmed that a major software update will bring Universal Hands Free to Gen 2 R1T and R1S. This hands free experience will cover over 3.5 million miles of roads across the US and Canada as long as there are clearly painted lane lines. It is a huge expansion of the assisted driving envelope for current owners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy+ Sub Launching in 2026&lt;/head&gt;
    &lt;p&gt;Rivian also announced Autonomy+, an autonomy tier with continuously expanding features launching early 2026.&lt;/p&gt;
    &lt;p&gt;Pricing is $2,500 one time or $49.99 per month.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rivian Unified Intelligence&lt;/head&gt;
    &lt;p&gt;Rivian is reorganizing its entire platform around Rivian Unified Intelligence, a data foundation that ties together telemetry, cloud models, service systems and customer facing features. It is the backbone for predictive maintenance, smarter diagnostics and upcoming AI driven tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rivian Assistant Coming in 2026&lt;/head&gt;
    &lt;p&gt;Rivian also officially unveiled its new Rivian Assistant, a next generation voice experience arriving early 2026 on Gen 1 and Gen 2 R1 vehicles. The assistant uses a blend of edge models and in vehicle intelligence to understand your schedule, recognize context, and handle everyday requests.&lt;/p&gt;
    &lt;p&gt;On R2, it will even run fully offline thanks to a more powerful infotainment computer, reducing latency and keeping more of the experience on device.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI Powered Service and Diagnostics&lt;/head&gt;
    &lt;p&gt;Rivian is embedding AI into the service workflow. Technicians will have access to an AI driven expert system that analyzes telemetry and vehicle history to pinpoint issues faster and more accurately. These same tools will eventually power the mobile app as well, making self service diagnostics significantly smarter.&lt;/p&gt;
    &lt;head rend="h3"&gt;13 Comments&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Any mention of retrofitting Autonomy Computer and LiDAR onto existing R1’s? Hopefully at least Gen 2’s!&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Will not happen&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I need this R2D2 themed R2 🔥&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I watched the event. It felt like I was at an apple event. Is Rivian wanting us to trade in ours cars every two years to get a newer version? A new chip set, better camera, or a faster processor?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So with everything we know do we think we can expect automatic lane change before point to point? Was kind of expecting auto park and auto lane change to be announced with universal hands free&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Universal Hands Free when? Did they give any indication?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Soon. So probably 6-9 months.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;S0––––0N&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sad day for Gen 1 users except the Rivian Assistant part……&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is disappointing. An assistant that can integrate with Google calendar 🤷♂️. Who cares?!?&lt;/p&gt;
        &lt;p&gt;Maybe some finds this valuable but I would rather have apple carplay.&lt;/p&gt;
        &lt;p&gt;People listen to music in their cars. Why not make that the first app? Who decided “let’s invest 2 years and make Google calendar the first app”?&lt;/p&gt;
        &lt;p&gt;Rivian is clueless.&lt;/p&gt;
        &lt;p&gt;I have owned 2 of these and won’t buy a third one.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sad as Gen 1 – should have had at least something to improve Driver-&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It will be interesting to see which features end up behind the paywall. Will we only end up with adaptive cruse control and center lane assist?&lt;/p&gt;
        &lt;p&gt;Also curious about the $2500 for Autonomy+. I wonder if that will follow a person/family to future Rivian vehicles. If so that would make it more enticing (also if it then included Connect+ as well).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You all be thankful that your Gen 1s have gotten meaningful updates and backwards compatible updates from Gen 2. Be thankful you don’t have a BMW EV from 2022 with iDrive 8, not even a year later they were outdated by the next iteration of iDrive, 8.5 and BMW claims features (even software based) from 8.5 are not backwards compatible with iDrive 8.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234920</guid><pubDate>Thu, 11 Dec 2025 18:17:19 +0000</pubDate></item><item><title>Going Through Snowden Documents, Part 1</title><link>https://libroot.org/posts/going-through-snowden-documents-part-1/</link><description>&lt;doc fingerprint="bfd8a957069095c0"&gt;
  &lt;main&gt;
    &lt;p&gt;We are building a comprehensive archive and analysis project examining published documents leaked by Edward Snowden. Our methodology involves systematically reviewing each available document with particular attention to small details and information that has received little or no public attention since the initial 2013 disclosures. Throughout this process, we will publish posts highlighting interesting previously unreported findings. The main project will hopefully be complete and made public in mid-to-late 2026.&lt;/p&gt;
    &lt;p&gt;This is Part 1 of our "Going Through Snowden Documents" series.&lt;/p&gt;
    &lt;p&gt;Document: CNE Analysis in XKEYSCORE&lt;/p&gt;
    &lt;p&gt;Classification: TOP SECRET//COMINT//REL TO USA, AUS, CAN, GBR, NZL&lt;/p&gt;
    &lt;p&gt;Date: October 15, 2009&lt;/p&gt;
    &lt;p&gt;Published by: The Intercept (July 1 and July 2, 2015)&lt;/p&gt;
    &lt;p&gt;Authors: Morgan Marquis-Boire, Glenn Greenwald, and Micah Lee&lt;/p&gt;
    &lt;p&gt;While The Intercept published this document, the accompanying articles focus on NSA's XKEYSCORE system broadly and does not analyze this specific document. The document appears only in the "Documents published with this article" sections without dedicated coverage. Academic searches, news archives, and general web searches reveal virtually no subsequent analysis or citation of this document. This pattern of important documents published but never publicly analyzed is unfortunately very common in the published Snowden documents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;This October 2009 33-page document, in a slideshow format, is an internal NSA training presentation demonstrating how analysts use XKEYSCORE to search and analyze data collected through Computer Network Exploitation (CNE), the NSA's term for active hacking operations. While framed as instructional examples showing various search capabilities, the screenshots display real surveillance operations with identifiable targets and captured data.&lt;/p&gt;
    &lt;p&gt;The screenshots in the document are such poor quality that, at times, reading the text is very difficult. However, by examining the context and surrounding text (or surrounding pages), the text can be inferred with a very high probability. This has certainly contributed to why many documents have not been studied more thoroughly in public, as many are similarly low quality with scrambled text.&lt;/p&gt;
    &lt;head rend="h2"&gt;CNE operation against Chinese defense contractor Norinco&lt;/head&gt;
    &lt;p&gt;One of the most significant previously unreported findings in this document is evidence of NSA surveillance targeting Norinco, China North Industries Corporation, one of the world's largest state-owned defense contractors. Norinco ranks among the world's top 100 defense companies by revenue and serves as a major exporter of military equipment to Pakistan, Iran, Venezuela, Zimbabwe, and dozens of other countries, many of which have contentious relationships with the United States.&lt;/p&gt;
    &lt;p&gt;On page 18, a screenshot from XKEYSCORE's Metaviewer interface displays a "Histogram of @Domain" view with a bar graph showing email volume across 10 domain names followed by a data table with formatted surveillance results. The query appears to be a converged search combining multiple distinct surveillance targets: Mexican federal agencies (ssp.gob.mx at 452 emails, pfp.local at 158 emails), Norinco-related domains (mail.norinco.cn, businessmonitor.com, bmi.msgfocus.com, zhenhuaoil.com, and lms-ms-daemon, each showing 3 emails), and two additional targets (steels-net.cu and inwind.it, each with 1 email). This convergence of seemingly unrelated targets in a single query demonstrates XKEYSCORE's ability to simultaneously analyze multiple surveillance operations.&lt;/p&gt;
    &lt;p&gt;The first five entries in the results table contain:&lt;/p&gt;
    &lt;quote&gt;Email User Name | Datetime | Highlights | @Domain | Subject | Chain [REDACTED] | 2009-10-10 05:15:10 | CNE | mail.norinco.cn | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;amp;kate.strut [REDACTED] | 2009-10-10 05:15:10 | CNE | businessmonitor.com | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;amp;kate.strut [REDACTED] | 2009-10-10 05:15:10 | CNE | bmi.msgfocus.com | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;amp;kate.strut [REDACTED] | 2009-10-10 05:15:10 | CNE | zhenhuaoil.com | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;amp;kate.strut [REDACTED] | 2009-10-10 05:15:10 | CNE | lms-ms-daemon | 28-10 senior contacts in India for zh | 0kqe00g01mrdii@mail.norinco.cn&amp;amp;kate.strut&lt;/quote&gt;
    &lt;p&gt;All entries are marked with the "CNE" highlight tag, indicating the data came from CNE operations, active hacking intrusions rather than passive network intercepts. Critically, all five entries share an identical "Chain" value indicating this is a single email captured at multiple points as it traversed Norinco's email infrastructure. The multiple domains – businessmonitor.com (newsletter sender), bmi.msgfocus.com (newsletter delivery service), mail.norinco.cn (Norinco's mail server), zhenhuaoil.com (Norinco's subsidiary), and lms-ms-daemon (the default domain name for Sun Java Messaging Server commonly used in enterprise email infrastructure) – represent the newsletter email's routing path through Norinco's network. This indicates that NSA achieved deep network penetration with visibility across multiple servers and routing points within Norinco's corporate email infrastructure, not just a single interception point. The compromise extended to Zhenhua Oil (Norinco's oil exploration subsidiary), indicating enterprise-wide access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Redaction failure exposing NSA agent username&lt;/head&gt;
    &lt;p&gt;Most XKEYSCORE search interfaces display a welcoming message showing the analyst's internal NSA username. In the document all usernames have been redacted from the screenshots except one left unredacted by mistake.&lt;/p&gt;
    &lt;p&gt;On page 9, the username "cryerni" is visible in the screenshot.&lt;/p&gt;
    &lt;p&gt;This username most likely belongs to the NSA analyst who created the presentation. The seven-character length matches the redacted name on the first page, based on the surrounding unredacted font. The length of seven characters also matches to other NSA agents' usernames in other documents (more on that in upcoming parts).&lt;/p&gt;
    &lt;head rend="h2"&gt;CNE operation against Mexican federal law enforcement&lt;/head&gt;
    &lt;p&gt;On the page 18, the XKEYSCORE Metaviewer displays email extraction results showing surveillance of Mexican federal law enforcement from domains ssp.gob.mx (Secretaría de Seguridad Pública) and pfp.local (Policía Federal Preventiva). Email subjects include:&lt;/p&gt;
    &lt;quote&gt;101009 EII LA PAZ, BAJA CALIFORNIA 101009 EII MEXICALI, BAJA CALIFORNIA 101009 EII CIUDAD JUÁREZ, CHIHUAHUA&lt;/quote&gt;
    &lt;p&gt;"EII" likely stands for "Estructura de Información de Inteligencia" or similar internal reporting format. The dates (101009 = October 10, 2009) and locations indicate daily intelligence reports from Mexican federal police units in Baja California's border region and Ciudad Juárez, one of Mexico's most violent cities during the peak of cartel warfare under President Felipe Calderón's military-led offensive against drug cartels.&lt;/p&gt;
    &lt;p&gt;NSA surveillance of these communications likely supported US counter-narcotics operations, identified compromised Mexican officials, and monitored cartel structures and government response capabilities. However, this represents surveillance of a nominal ally's law enforcement agencies without apparent Mexican government knowledge or consent. All entries were marked "CNE," again indicating active computer compromise rather than passive intercept.&lt;/p&gt;
    &lt;head rend="h2"&gt;CNE operation against Iran's customs and rails&lt;/head&gt;
    &lt;p&gt;Another interesting finding appears on page 17, showing document metadata extraction results with the name "Iran OP Customs and Rail Extracted Docs". The results table displays documents captured from a file path containing "lap top drive" and "Private Inbox", with all entries marked "CNE" in the Highlights column, indicating NSA compromised a portable computer likely belonging to someone working in Iranian transportation or customs infrastructure. The implant performed a complete directory walk and extracted Word documents from the user's private folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;New surveillance program codenames&lt;/head&gt;
    &lt;p&gt;Several program codenames mentioned in this document don't appear in any other published Snowden documents or in previous reporting. No mention either in websites documenting all the codenames found in Snowden documents and in other NSA/GCHQ related articles and documents.&lt;/p&gt;
    &lt;p&gt;TURBOCHASER - The document describes TURBOCHASER as an NSA database for "profiles" and for "future tasking", appearing alongside MARINA (the well-documented NSA metadata repository). The name suggests rapid-cycling or high-speed processing ("turbo") of pursuit targets ("chaser"). Based on context, TURBOCHASER likely handled specific metadata types or geographic regions that MARINA didn't cover. The document's brief mention provides no additional details.&lt;/p&gt;
    &lt;p&gt;TUCKER - References in the document suggest TUCKER is an exploitation framework comparable to UNITEDRAKE (the well-documented full-featured Windows implant). The document lists TUCKER's sub-projects including OLYMPUS, EXPANDINGPULLY, and UNIX, indicating TUCKER was a platform hosting multiple specialized payloads and/or (post-)exploitation tools.&lt;/p&gt;
    &lt;p&gt;SHADOWQUEST, WAYTIDE, GREENCHAOS - These appear as collection source identifiers in the document. The document shows them as input sources feeding CNE data into XKEYSCORE. Notably, FOXACID, the well-documented NSA exploit server system used to deliver malware to targets, also appears in this context with the suffix FOXACID6654, suggesting it functioned not just as an exploitation delivery mechanism but also as a collection source identifier once targets were compromised. This reveals FOXACID's dual role: initial compromise vector and ongoing data collection infrastructure.&lt;/p&gt;
    &lt;p&gt;The input sources shown include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FOXACID6654 - collecting wireless survey data&lt;/item&gt;
      &lt;item&gt;SHADOWQUEST35 - collecting wireless survey data&lt;/item&gt;
      &lt;item&gt;WAYTIDE1173 - collecting wireless intelligence&lt;/item&gt;
      &lt;item&gt;GREENCHAOS15 - source of the Chinese keylogger data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The numeric suffixes (6654, 35, 1173, 15) likely designates a specific server or operational instance, possibly corresponding to geographic regions, operational theaters, or specific TAO teams.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other&lt;/head&gt;
    &lt;p&gt;Finally, the document showcases several detailed cases of NSA's CNE capabilities, confirming and adding specific context to techniques that have been reported on more generally since 2013.&lt;/p&gt;
    &lt;head rend="h3"&gt;FOGGYBOTTOM: HTTP activity surveillance&lt;/head&gt;
    &lt;p&gt;Pages 19-20 showcase FOGGYBOTTOM for monitoring HTTP activity captured through CNE operations. FOGGYBOTTOM is a computer implant plug-in that records logs of internet browsing histories and collects login details and passwords used to access websites and email accounts. These pages show detailed browser surveillance of a target identified by case notation YM.VALAGWAADTC (Yemen) on October 14, 2009. The system captured:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple Facebook login attempts (login.facebook.com with "login_attempt=1" POST requests)&lt;/item&gt;
      &lt;item&gt;Arabic-language Facebook browsing (ar-ar.facebook.com)&lt;/item&gt;
      &lt;item&gt;Saudi Arabian Google searches (www.google.com.sa with "hl=ar" indicating Arabic language)&lt;/item&gt;
      &lt;item&gt;Yemeni news sites (www.14october.com, www.26sep.net, www.althawranews.net)&lt;/item&gt;
      &lt;item&gt;Arabic sports forums (forum.kooora.com - a popular Middle Eastern sports discussion site)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The surveillance captured not just URLs but complete HTTP request details including POST data and URL parameters. The "dnt_payload/browser" formatter shows the target's local time, timezone offset, and HTTP POST form data. Since this data comes from a CNE implant running on the compromised computer itself – not passive network interception – it captures web traffic before encryption occurs. The implant sees the browsing data whether the connection uses HTTP or HTTPS, providing complete visibility into all browsing activity including encrypted sessions that would be opaque to network-level surveillance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Windows registry surveillance&lt;/head&gt;
    &lt;p&gt;Page 26 demonstrates XKEYSCORE's capability to search and analyze Windows registry data extracted from compromised machines. The screenshots show registry queries returning UserAssist keys; Windows registry entries that record every program a user has executed, how many times, and when they last ran it. This data is maintained by Windows for user interface optimization but becomes a detailed forensic record when captured by NSA implants.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multi-lingual keylogger capabilities&lt;/head&gt;
    &lt;p&gt;Pages 24-25 demonstrate XKEYSCORE's keylogger capabilities with actual captured keystrokes from a compromised computer identified as GREENCHAOS15 in China. The target was using QQ.exe (China's largest instant messaging platform owned by Tencent), Microsoft Excel, and Microsoft Access. The keylogger captured complete Chinese character input, control key sequences, hexadecimal codes for special characters, window titles showing conversation participants, and even deleted text and editing actions. In Excel, the system recorded every keystroke including numerical entries, navigation inputs (Delete, NumPad entries), and cell references (D4, H2, D53, etc.), showing the target working on a spreadsheet titled "3C证书导入工作周报0928-1001.xls" (3C Certificate Import Work Weekly Report 09/28-10/01). The target appeared to be an office worker handling administrative tasks related to China's 3C certification system (China Compulsory Certificate for product safety/quality). This demonstrates NSA's ability to capture multi-lingual keystrokes across all applications with complete context preservation.&lt;/p&gt;
    &lt;head rend="h3"&gt;"vpn in docs"&lt;/head&gt;
    &lt;p&gt;The document also demonstrates how XKEYSCORE uses a generic "tech strings" search to automatically identify and flag arbitrary keywords that an analyst queries. This feature appears to function as a catchall system for finding terms of interest in data streams that lack a more specific parser. The examples show XKEYSCORE tagging the strings "vpn" and "pptp" inside a wide variety of captured data. This includes the content of emails (email_body), the body of local documents (document_body with file paths like C:\TNI-095CC.DOC), and other raw data payloads exfiltrated from implants (tech_body). As nearly all entries are highlighted with "CNE," this reveals that NSA implants actively scan a target's private files and communications for these keywords. The resulting intelligence allows analysts to discover a target's security posture, identify potential vulnerabilities, and find information such as credentials or server details that can be leveraged to gain access to privileged systems or map internal networks.&lt;/p&gt;
    &lt;p&gt;This document is a good example of the significant intelligence hiding in plain sight within the published Snowden documents. A detailed review can reveal significant, previously unreported intelligence operations, such as the CNE op against a major Chinese defense contractor. These findings underscore the importance of a systematic review of the documents. Also, it's important to acknowledge the inherent limitations of analyzing any single document in isolation like we did in this post. A single document analysis offers only a snapshot with limited context.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46235412</guid><pubDate>Thu, 11 Dec 2025 18:52:08 +0000</pubDate></item><item><title>Age verification is coming for the internet</title><link>https://www.eff.org/deeplinks/2025/12/age-verification-coming-internet-we-built-you-resource-hub-fight-back</link><description>&lt;doc fingerprint="3ba80b7770d5b1c6"&gt;
  &lt;main&gt;
    &lt;p&gt;Age verification laws are proliferating fast across the United States and around the world, creating a dangerous and confusing tangle of rules about what we’re all allowed to see and do online. Though these mandates claim to protect children, in practice they create harmful censorship and surveillance regimes that put everyone—adults and young people alike—at risk.&lt;/p&gt;
    &lt;p&gt;The term “age verification” is colloquially used to describe a wide range of age assurance technologies, from age verification systems that force you to upload government ID, to age estimation tools that scan your face, to systems that infer your age by making you share personal data. While different laws call for different methods, one thing remains constant: every method out there collects your sensitive, personal information and creates barriers to accessing the internet. We refer to all of these requirements as age verification, age assurance, or age-gating.&lt;/p&gt;
    &lt;p&gt;If you’re feeling overwhelmed by this onslaught of laws and the invasive technologies behind them, you’re not alone. It’s a lot. But understanding how these mandates work and who they harm is critical to keeping yourself and your loved ones safe online. Age verification is lurking around every corner these days, so we must fight back to protect the internet that we know and love.&lt;/p&gt;
    &lt;p&gt;That’s why today, we’re launching EFF’s Age Verification Resource Hub (EFF.org/Age): a one-stop shop to understand what these laws actually do, what’s at stake, why EFF opposes all forms of age verification, how to protect yourself, and how to join the fight for a free, open, private, and yes—safe—internet.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Age Verification Mandates Are a Problem&lt;/head&gt;
    &lt;p&gt;In the U.S., more than half of all states have now passed laws imposing age-verification requirements on online platforms. Congress is considering even more at the federal level, with a recent House hearing weighing nineteen distinct proposals relating to young people’s online safety—some sweeping, some contradictory, and each one more drastic and draconian than the last.&lt;/p&gt;
    &lt;p&gt;We all want young people to be safe online. However, age verification is not the silver bullet that lawmakers want you to think it is.&lt;/p&gt;
    &lt;p&gt;The rest of the world is moving in the same direction. We saw the UK’s Online Safety Act go into effect this summer, Australia’s new law barring access to social media for anyone under 16 goes live today, and a slew of other countries are currently considering similar restrictions.&lt;/p&gt;
    &lt;p&gt;We all want young people to be safe online. However, age verification is not the silver bullet that lawmakers want you to think it is. In fact, age-gating mandates will do more harm than good—especially for the young people they claim to protect. They undermine the fundamental speech rights of adults and young people alike; create new barriers to accessing vibrant, lawful, even life-saving content; and needlessly jeopardize all internet users’ privacy, anonymity, and security.&lt;/p&gt;
    &lt;p&gt;If legislators want to meaningfully improve online safety, they should pass a strong, comprehensive federal privacy law instead of building new systems of surveillance, censorship, and exclusion.&lt;/p&gt;
    &lt;head rend="h3"&gt;What’s Inside the Resource Hub&lt;/head&gt;
    &lt;p&gt;Our new hub is built to answer the questions we hear from users every day, such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How do age verification laws actually work?&lt;/item&gt;
      &lt;item&gt;What’s the difference between age verification, age estimation, age assurance, and all the other confusing technical terms I’m hearing?&lt;/item&gt;
      &lt;item&gt;What’s at stake for me, and who else is harmed by these systems?&lt;/item&gt;
      &lt;item&gt;How can I keep myself, my family, and my community safe as these laws continue to roll out?&lt;/item&gt;
      &lt;item&gt;What can I do to fight back?&lt;/item&gt;
      &lt;item&gt;And if not age verification, what else can we do to protect the online safety of our young people?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Head over to EFF.org/Age to explore our explainers, user-friendly guides, technical breakdowns, and advocacy tools—all indexed in the sidebar for easy browsing. And today is just the start, so keep checking back over the next several weeks as we continue to build out the site with new resources and answers to more of your questions on all things age verification.&lt;/p&gt;
    &lt;head rend="h3"&gt;Join Us: Reddit AMA &amp;amp; EFFecting Change Livestream Events&lt;/head&gt;
    &lt;p&gt;To celebrate the launch of EFF.org/Age, and to hear directly from you how we can be most helpful in this fight, we’re hosting two exciting events:&lt;/p&gt;
    &lt;head rend="h4"&gt;1. Reddit AMA on r/privacy&lt;/head&gt;
    &lt;p&gt;Next week, our team of EFF activists, technologists, and lawyers will be hanging out over on Reddit’s r/privacy subreddit to directly answer your questions on all things age verification. We’re looking forward to connecting with you and hearing how we can help you navigate these changing tides, so come on over to r/privacy on Monday (12/15), Tuesday (12/16), and Wednesday (12/17), and ask us anything!&lt;/p&gt;
    &lt;head rend="h4"&gt;2. EFFecting Change Livestream Panel: “The Human Cost of Online Age Verification”&lt;/head&gt;
    &lt;p&gt;Then, on January 15th at 12pm PT, we’re hosting a livestream panel featuring Cynthia Conti-Cook, Director of Research and Policy at the Collaborative Research Center for Resilience; Hana Memon, Software Developer at Gen Z for Change; EFF Director of Engineering Alexis Hancock; and EFF Associate Director of State Affairs Rindala Alajaji. We’ll break down how these laws work, who they exclude, and how these mandates threaten privacy and free expression for people of all ages. Join us by RSVPing at https://livestream.eff.org/.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Resource to Empower Users&lt;/head&gt;
    &lt;p&gt;Age-verification mandates are reshaping the internet in ways that are invasive, dangerous, and deeply unnecessary. But users are not powerless! We can challenge these laws, protect our digital rights, and build a safer digital world for all internet users, no matter their ages. Our new resource hub is here to help—so explore, share, and join us in the fight for a better internet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46235531</guid><pubDate>Thu, 11 Dec 2025 18:58:49 +0000</pubDate></item><item><title>Pop_OS 24.04 LTS with COSMIC desktop environment</title><link>https://blog.system76.com/post/pop-os-letter-from-our-founder/</link><description>&lt;doc fingerprint="de52a41ada97a5e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Pop!_OS 24.04 LTS Released: A Letter From Our Founder&lt;/head&gt;
    &lt;p&gt;epoch /ĕp′ək, ē′pŏk″/&lt;lb/&gt;noun&lt;lb/&gt;the beginning of a distinctive period in the history of someone or something.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pop!_OS 24.04 LTS with COSMIC Desktop Environment Epoch 1&lt;/head&gt;
    &lt;p&gt;If you’re ambitious enough, or maybe just crazy enough, there eventually comes a time when you realize you’ve reached the limits of current potential, and must create something completely new if you’re to go further.&lt;/p&gt;
    &lt;p&gt;This year, System76 turned twenty. For twenty years we have shipped Linux computers. For seven years we’ve built the Pop!_OS Linux distribution. Three years ago it became clear we had reached the limit of our current potential and had to create something new. Today, we break through that limit with the release of Pop!_OS 24.04 LTS with the COSMIC Desktop Environment.&lt;/p&gt;
    &lt;p&gt;Today is special not only in that it’s the culmination of over three years of work, but even more so in that System76 has built a complete desktop environment for the open source community. We’re proud of this contribution to the open source ecosystem. COSMIC is built on the ethos that the best open source projects enable people to not only use them, but to build with them. COSMIC is modular and composable. It’s the flagship experience for Pop!_OS in its own way, and can be adapted by anyone that wants to build their own unique user experience for Linux.&lt;/p&gt;
    &lt;p&gt;Thank you for your patience while we built COSMIC. We know it’s been a long ride, and appreciate you taking it with us. Pop!_OS 24.04 LTS doesn’t feel like moving forward three years. It feels like leaping forward a decade. This day marks the foundation for our next twenty years and the first of what will be many rapid innovations.&lt;/p&gt;
    &lt;p&gt;And thank you to System76 customers. COSMIC is entirely funded by System76 hardware sales. Not only are you getting the best Linux hardware on the planet, you're investing in the future of the Linux desktop that we all love.&lt;/p&gt;
    &lt;p&gt;I hope you love what we’ve built for you. Now go out there and create. Push the limits, make incredible things, and have fun doing it!&lt;/p&gt;
    &lt;p&gt;Carl Richell&lt;lb/&gt;Founder and CEO&lt;lb/&gt;System76&lt;/p&gt;
    &lt;head rend="h3"&gt;Download Pop!_OS 24.04 LTS&lt;/head&gt;
    &lt;p&gt;Download Pop!_OS 24.04 LTS at https://system76.com/pop/&lt;/p&gt;
    &lt;head rend="h3"&gt;Pop!_OS 24.04 LTS Highlights&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New COSMIC Desktop Environment Experience&lt;/item&gt;
      &lt;item&gt;New Pop!_OS 24.04 LTS for ARM computers&lt;lb/&gt;◦ Officially supported on the System76 Thelio Astra desktop&lt;lb/&gt;◦ Community support for non-System76 hardware enabled by Tow-Boot&lt;/item&gt;
      &lt;item&gt;New hybrid graphics support for longer battery life&lt;lb/&gt;◦ No need to change modes&lt;lb/&gt;◦ Apps that request the discrete GPU will automatically run on the correct GPU&lt;lb/&gt;◦ Manually run an app on your preferred GPU by right-clicking on the app icon&lt;/item&gt;
      &lt;item&gt;Easy installation with full disk encryption&lt;/item&gt;
      &lt;item&gt;Refresh install feature by holding Space at boot or from the ISO&lt;lb/&gt;◦ Reinstall the OS anytime while keeping files, settings, and Flatpak user applications&lt;lb/&gt;◦ The Refresh feature will also arrive in COSMIC Settings after release&lt;/item&gt;
      &lt;item&gt;Broad hardware support&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;COSMIC Desktop Environment Highlights&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intuitive window tiling that can be used with the mouse or keyboard&lt;lb/&gt;◦ Activate tiling with a simple toggle in the panel&lt;lb/&gt;◦ Tiling per workspace and per display&lt;lb/&gt;◦ Easy to learn keyboard shortcuts&lt;lb/&gt;◦ Rearrange windows by dragging them with the mouse. Visual hints show where the window will land.&lt;/item&gt;
      &lt;item&gt;Featureful Workspaces&lt;lb/&gt;◦ Horizontal or vertical workspaces&lt;lb/&gt;◦ Workspace per display or spanned across displays&lt;lb/&gt;◦ Drag workspaces to re-arrange them or move an entire workspace to a different display&lt;lb/&gt;◦ Pin workspaces so they’re never removed&lt;lb/&gt;◦ Workspace settings are persistent. A tiled and pinned workspace will be tiled and pinned after reboot.&lt;lb/&gt;◦ Add the “Numbered Workspaces” applet to your Panel or Dock to always see the workspace number you’re on&lt;/item&gt;
      &lt;item&gt;Smooth multi-display experience&lt;lb/&gt;◦ Effortlessly mix and match hi-dpi and standard resolution displays&lt;lb/&gt;◦ Displays are automatically scaled based on pixel density and display scaling can be fine-tuned in Settings&lt;lb/&gt;◦ Display settings are persistent. Plug in the same display and its settings will return.&lt;lb/&gt;◦ Unplug a display and the windows on that display will move to a new workspace on a remaining display&lt;/item&gt;
      &lt;item&gt;Customization galore&lt;lb/&gt;◦ Theme your desktop from Settings with easy color pickers&lt;lb/&gt;◦ Setup your personalized layout&lt;lb/&gt;••• Panel + Dock or single Panel&lt;lb/&gt;••• Put the Panel and Dock on any screen edge&lt;lb/&gt;◦ Add and arrange features (Applets) on the Panel or Dock from Settings&lt;/item&gt;
      &lt;item&gt;Fast desktop navigation and easy keyboard shortcuts&lt;lb/&gt;◦ Press the Super (or the Windows key) to activate the Launcher&lt;lb/&gt;••• Type the app you want and press enter&lt;lb/&gt;••• Type the name of an open app then enter to switch to it&lt;lb/&gt;••• Type ? to learn more features like web and GitHub search, calculator, and running commands directly from the launcher&lt;lb/&gt;◦ Use the same keyboard shortcuts to focus or arrange windows across workspaces and displays&lt;lb/&gt;• Super+Arrows to change the focused window. Super+Shift+Arrows to move windows.&lt;/item&gt;
      &lt;item&gt;Stacks, snapping, and sticky windows&lt;lb/&gt;◦ Stack windows to combine them into tab groups like a browser&lt;lb/&gt;••• Right click on the header and choose Create Window Stack. Then drag another window to the stack.&lt;lb/&gt;••• When tiling windows, simply drag the window on top of another to create a stack.&lt;lb/&gt;◦ When using floating (non-tiled) windows, drag them to the display edges to snap them into a quarter or half tile&lt;lb/&gt;◦ Make a window stay on top of other windows and follow you around to other workspaces or displays by right-clicking the header and choosing “Sticky window”&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Faster, more responsive applications&lt;lb/&gt;◦ COSMIC Files&lt;lb/&gt;◦ COSMIC Store&lt;lb/&gt;◦ COSMIC Terminal&lt;lb/&gt;◦ COSMIC Text Editor&lt;lb/&gt;◦ COSMIC Media Player&lt;lb/&gt;◦ COSMIC Screenshot&lt;lb/&gt;◦ Install more apps Made for COSMIC from COSMIC Store&lt;/item&gt;
      &lt;item&gt;Written in the Rust programming language&lt;lb/&gt;◦ For memory safety and high performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Download Pop!_OS 24.04 LTS&lt;/head&gt;
    &lt;p&gt;Download Pop!_OS 24.04 LTS at https://system76.com/pop/&lt;/p&gt;
    &lt;head rend="h3"&gt;Upgrading from Pop!_OS 22.04 LTS to Pop!_OS 24.04 LTS&lt;/head&gt;
    &lt;p&gt;Pop!_OS 22.04 LTS users will receive an upgrade notification in the OS starting January 2026. If you wish to upgrade to Pop!_OS 24.04 LTS before then, after backing up your files, open Terminal and run&lt;/p&gt;
    &lt;quote&gt;pop-upgrade release upgrade -f&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46235618</guid><pubDate>Thu, 11 Dec 2025 19:03:45 +0000</pubDate></item><item><title>An SVG is all you need</title><link>https://jon.recoil.org/blog/2025/12/an-svg-is-all-you-need.html</link><description>&lt;doc fingerprint="3dc38b5d0be4eb89"&gt;
  &lt;main&gt;
    &lt;p&gt;SVGs are pretty cool - vector graphics in a simple XML format. They are supported on just about every device and platform, are crisp on every display, and can have embedded scripts in to make them interactive. They're way more capable than many people realise, and I think we can capitalise on some of that unrealised potential.&lt;/p&gt;
    &lt;p&gt;Anil's recent post Four Ps for Building Massive Collective Knowledge Systems got me thinking about the permanence of the experimentation that underlies our scientific papers. In my idealistic vision of how scientific publishing should work, each paper would be accompanied by a fully interactive environment where the reader could explore the data, rerun the experiments, tweak the parameters, and see how the results changed. Obviously we can't do this in the general case - some experiments are just too expensive or time-consuming to rerun on demand. But for many papers, especially in computer science, this is entirely feasible.&lt;/p&gt;
    &lt;p&gt;That line of thought reminded me of a project I tackled as a post-doc in the Department of Plant Sciences here in Cambridge. I was writing a paper on synergy in fungal networks and built a tiny SVG visualisation tool that let readers wander through the raw data captured from a real fungal network growing in a petri dish. I dug it up recently and was surprised (and delighted) to see that it still works perfectly in modern browsers - even though the original âcover pageâ suggested Firefox 1.5 or the Adobe SVG plug-in (!). Give it a spin; click the 'forward', 'back' and other buttons below the petri dish!&lt;/p&gt;
    &lt;p&gt;And that, dear reader, is literally all you need. A completely self-contained SVG file can either fetch data from a versioned repository or embed the data directly, as the example does. It can process that data, generate visualisations, and render knobs and sliders for interactive exploration. No server-side magic required - everything runs client-side in the browser, served by a plain static web server, and very easily to share.&lt;/p&gt;
    &lt;p&gt;How does it fit in with Anil's four Ps?&lt;/p&gt;
    &lt;p&gt;The SVG above is only a visualisation tool for data; it doesn't really do any processing, but it certainly could. The biggest change that's happened over the 20 years since I wrote this is the massive increase in the computation power available in the browser. If would be entirely feasible to implement the entire data analysis pipeline for that paper in an SVG today, probably without even spinning up the fans on my laptop!&lt;/p&gt;
    &lt;p&gt;So this is yet another tool in our ongoing effort to be able to effortlessly share and remix our work - added to the pile of Jupyter notebooks, Marimo botebooks, the slipshow/x-ocaml combination, Patrick's take on Jon Sterling's Forester, my own notebooks, and many others - and this is a subset of what we're using just in our own group!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46235959</guid><pubDate>Thu, 11 Dec 2025 19:25:14 +0000</pubDate></item><item><title>My productivity app is a never-ending .txt file (2022)</title><link>https://jeffhuang.com/productivity_text_file/</link><description>&lt;doc fingerprint="2555483b29260ef2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Over 14 years of todos recorded in text&lt;/head&gt;
    &lt;head rend="h2"&gt;My productivity app is a never-ending .txt file&lt;/head&gt;
    &lt;head rend="h3"&gt;By Jeff Huang, updated on 2022-03-21&lt;/head&gt;
    &lt;p&gt;The biggest transition for me when I started college was learning to get organized. There was a point when I couldn't just remember everything in my head. And having to constantly keep track of things was distracting me from whatever task I was doing at the moment.&lt;/p&gt;
    &lt;p&gt;So I tried various forms of todo lists, task trackers, and productivity apps. They were all discouraging because the things to do kept getting longer, and there were too many interrelated things like past meeting notes, calendar appointments, idea lists, and lab notebooks, which were all on different systems.&lt;/p&gt;
    &lt;p&gt;I gave up and started just tracking in a single text file and have been using it as my main productivity system for 14 years now. It is so essential to my work now, and has surprisingly scaled with a growing set of responsibilities, that I wanted to share this system. It's been my secret weapon.&lt;/p&gt;
    &lt;p&gt;Prerequisite: A calendar. The one outside tool I use is an online calendar, and I put everything on this calendar, even things that aren't actually for a fixed time like "make a coffee table at the workshop" or "figure out how to recruit new PhD students" — I'll schedule them on a date when I want to think about it. That way all my future plans and schedule are together, and not a bunch of lists I have to keep track of.&lt;/p&gt;
    &lt;p&gt;Making the Daily List: Every night before I go to bed, I take all the items on my calendar for the next day and append it to the end of the text file as a daily todo list, so I know exactly what I'm doing when I wake up. This list contains scheduled tasks (2pm meeting with Madonna, 4pm office hours), errands (sign a form, return a book), and work items (review a paper, prepare a presentation). It also lets me think about whether I've got the right amount of work for a day.&lt;/p&gt;
    &lt;p&gt;Anything I don't want to do tomorrow, I'll shuffle back into my calendar on later dates. If the task is too big, I'll break it down into a piece for tomorrow, and the rest for another date. After years of doing this, I've gotten pretty good at estimating what I can finish in a day. Here's an example with names replaced so you can see what it looks like when I move a day's schedule from my calendar.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;2021-11-31
11am meet with Head TAs
- where are things at with inviting portfolio reviewers?
11:30am meet with student Enya (interested in research)
review and release A/B Testing assignment grading
12pm HCI group meeting
- vote for lab snacks
send reminders for CHI external reviewers
read Sketchy draft
Zelda pick up eye tracker
- have her sign for it
update biosketch for Co-PI
3:15pm join call with Umbrella Corp and industry partnership staff
3:45pm advising meet with Oprah
4pm Rihanna talk (368 CIT)
5pm 1:1 with Beyonce #phdadvisee
6pm faculty interview dinner with Madonna
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;As a Record: That daily todo list is where I also take notes, so it's a to do list that turns into a what done list. The best thing about these daily lists is I keep them all in a single text file separated by dates, so I have a record of everything I have ever done and when I did it.&lt;/p&gt;
    &lt;p&gt;My current file was created 9 years ago when I started my current job. It serves as a research notebook, and as meeting minutes. I have 51,690 handwritten lines in one file now, documenting everything I have done as a professor, and nearly every person I have met with, along with notes about what we discussed or ideas I had. Here's what my list looks like at the end of the day, representing work accomplished.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;2021-11-31
11am meet with Head TAs
- where are things at with inviting portfolio reviewers? A: got 7/29 replies
- need 3 TAs for Thursday lab
- Redesign assignment handout will be done by Monday, ship Thursday
11:30am meet with student Enya (interested in research)
- they're a little inexperienced, suggested applying next year
review and release A/B Testing assignment grading
12pm HCI group meeting
- automatically generate thumbnails from zoom behavior on web pages
- #idea subliminal audio that leads you to dream about websites
- Eminem presenting Nov 24
- vote for lab snacks. A: popcorn and seaweed thing
got unofficial notification ARO YIP funding award #annual #cv
read Sketchy paper draft
- needs 1 more revision
- send to Gandalf to look at?
Zelda pick up eye tracker
- have her sign for it
update biosketch for Co-PI
unexpected drop in from Coolio! #alumni
- now a PM working on TravelAdvisor, thinking about applying to grad school
3:15pm join call with Umbrella Corp and industry partnership staff
- they want to hire 20 data science + SWE interns (year 3), 4 alums there as SWE
3:45pm advising meet with Oprah
- enjoyed CS 33
- interning at Facebook
4pm Rihanna talk (368 CIT)
5pm 1:1 with Beyonce #phdadvisee
- stuck on random graph generating crash
	- monitor memory/swap/disk?
	- ask Mario to help?
- got internship at MSR with Cher
	- start May 15 or 22
- will send me study design outline before next meeting
- interviewing Spartacus as potential RA for next semester
6pm faculty interview dinner with Madonna (Gracie's)
- ask about connection with computer vision
- cool visual+audio unsupervised comparison, thoughtful about missing data, would work with ugrads (?), likes biking, teach compvis + graphics
- vote #HIRE
#note maybe visit Monsters University next spring, Bono does related work
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Shortcuts and Features: I use a consistent writing style so things are easily searchable, with a few shorthands. When I search for "meet with", it shows that I have had over 3,000 scheduled meetings. I have some tags like #idea for new ideas to revisit when I want project ideas, #annual for things to put on my next annual report, #nextui for things to add the next time I run my next UI course.&lt;/p&gt;
    &lt;p&gt;A text file is incredibly flexible, and at any point, I can quickly glance to see what I've done that day and what's left. I usually keep an empty line between tasks completed and upcoming tasks. When a task is completed, I move the empty line. Any leftover tasks from the current day can go back into the calendar for when I may want to tackle it again, but that is rare because tasks were already sized into what I can do on that day. I can calculate aggregate statistics using the search box, or list all the lines containing a tag, and other operations using my text editor. I use Ultraedit because I'm familiar with it, but any text editor would have similar capabilities.&lt;/p&gt;
    &lt;p&gt;Email: Email is obviously a part of my workflow. Everyone has all sorts of productivity advice about handling it, but I find a simple flagging system is sufficient — flag Red if it's something I need to deal with, flag Orange if I need to deal with it eventually but requires some thinking or someone else to handle it, and flag Yellow for emails I send that I am waiting on a reply for, so I know to follow up later. I'll flag emails as they come in, whenever it's convenient.&lt;/p&gt;
    &lt;p&gt;At the end of the day, I'll do a quick review of the Orange and Yellows to see if any need to be followed up or should become Red. Some peoples' workflows revolve around obsessively cleaning their Inbox. I don't really care about keeping my inbox empty because then I feel like I have new work to do whenever email comes in.&lt;/p&gt;
    &lt;p&gt;So my daily routine looks like&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;look at the daily todo list I wrote last night to find out what I'm doing today&lt;/item&gt;
      &lt;item&gt;do scheduled things on that list during the day&lt;/item&gt;
      &lt;item&gt;when I have free (unscheduled) time, do the floating tasks on my list and work on Red-flagged emails at the end of the day&lt;/item&gt;
      &lt;item&gt;do a quick review of Orange/Yellow emails to see if they need any handling&lt;/item&gt;
      &lt;item&gt;copy the next day's calendar items to the bottom of the text file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This process has a few nice properties:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It's easy to immediately see what to do when I wake up&lt;/item&gt;
      &lt;item&gt;I don't need to remember in my head the things to do later (following up on emails, future tasks)&lt;/item&gt;
      &lt;item&gt;It's easy to recall what happened in the past and see how much I can actually accomplish in a day&lt;/item&gt;
      &lt;item&gt;There's no running "todo" list with items that keep pushed back day after day&lt;/item&gt;
      &lt;item&gt;I use Remote Desktop so everything is accessible from every device&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My daily workload is completely under my control the night before; whenever I feel overwhelmed with my long-term commitments, I reduce it by aggressively unflagging emails, removing items from my calendar that I am no longer excited about doing, and reducing how much work I assign myself in the future.&lt;/p&gt;
    &lt;p&gt;It does mean sometimes I miss some questions or don't pursue an interesting research question, but helps me maintain a manageable workload.&lt;/p&gt;
    &lt;p&gt;So that's it. I would love to hear from you if you try my system, or have some ideas about it!&lt;/p&gt;
    &lt;head rend="h3"&gt;Also in this series&lt;/head&gt;
    &lt;p&gt;The Coronavirus pandemic has changed our sleep behavior&lt;/p&gt;
    &lt;p&gt;Extracting data from tracking devices by going to the cloud&lt;/p&gt;
    &lt;head rend="h3"&gt;Other articles I've written&lt;/head&gt;
    &lt;p&gt;Behind the scenes: the struggle for each paper to get published&lt;/p&gt;
    &lt;p&gt;This page is designed to last, a manifesto for preserving content on the web&lt;/p&gt;
    &lt;p&gt;Illustrative notes for obsessing over publishing aesthetics&lt;/p&gt;
    &lt;p&gt;CS Faculty Composition and Hiring Trends&lt;/p&gt;
    &lt;p&gt;Bias in Computer Science Rankings&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46236037</guid><pubDate>Thu, 11 Dec 2025 19:30:58 +0000</pubDate></item></channel></rss>