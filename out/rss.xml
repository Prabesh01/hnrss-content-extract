<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 25 Jan 2026 19:33:52 +0000</lastBuildDate><item><title>World’s most powerful literary critic is on TikTok</title><link>https://www.newstatesman.com/culture/books/2026/01/the-worlds-most-powerful-literary-critic-is-on-tiktok</link><description>&lt;doc fingerprint="a633012423e0ded1"&gt;
  &lt;main&gt;
    &lt;p&gt;“Samuel Johnson, William Hazlitt, Harold Bloom and James Wood. And now you?”&lt;/p&gt;
    &lt;p&gt;“Oh, I mean, that’s pretty crazy to hear, but I don’t know.”&lt;/p&gt;
    &lt;p&gt;Dr Johnson never filmed a “spicy books with cartoon covers” vlog. But Jack Edwards cannot quite deny being the most important literary critic in the world. In commercial terms, he certainly is. A nod from him fills bathtubs, train carriages and public parks with copies of a book he likes. Booksellers buy and arrange their stock to his taste. And he is not confined to new releases. When he dug up an obscure Dostoevsky (White Nights), his positive review moved it from cellars to shop windows instantaneously. I first met him for this interview around the time of the 2024 International Booker Prize. He had been asked to host the ceremony – and to livestream it. I watched him cruise up the red carpet, encircled by cameras and attendants.&lt;/p&gt;
    &lt;p&gt;Edwards is a literary tastemaker, but not in the familiar mode. You will not find any submissions of his languishing in the LRB slush pile. Instead he posts on BookTok and BookTube, the social media planes concerned with reading, where millions of viewers watch videos about books. At first, BookTok was confined to the lonely bedrooms of coronavirus lockdowns. Now it has stalls at the Hay Festival and the National Literacy Trust. Waterstones has “BookTok made me buy it” tables.&lt;/p&gt;
    &lt;p&gt;Edwards is the foremost star of these platforms. Per his bio, he is the “internet’s resident librarian”. He is 27 years old and, on YouTube, has 1.5 million subscribers and 158 million views – which is roughly 16 and 1,755 Wembley Stadiums respectively. He is conventionally handsome, and excitable and sensitive in manner. He started YouTube while at secondary school in Brighton. His first videos were “day in the life” vlogs, study tips and A-level results reactions.&lt;/p&gt;
    &lt;p&gt;The boom came when he progressed to Durham University to study literature. His atmospheric recordings of undergraduate life proved so popular that the dean of his college now calls its oversubscription “the Jack Edwards effect”.&lt;/p&gt;
    &lt;p&gt;His next major growth spurt came when his university career ended. When Oxford University rejected his master’s application in 2020, Edwards posted a video of himself crying, entitled “oxford university rejected my masters application… (sorry this video is sad)”. Social media rewards confession. Authenticity, sincerity and vulnerability were important – more important than orthodox intellectual baubles. He described social media to me as “democratic… When you log on, you don’t need a qualification, you don’t need to be an established journalist.”&lt;/p&gt;
    &lt;p&gt;Graduating into the pandemic was “confusing and strange”, he says. Edwards did not feel he could be a “StudyTuber” if he was no longer a student. He spent a month alone. During it, he read 30 books and decided he wanted a career in them. But he could not secure a job in a hotel, let alone one in publishing.&lt;/p&gt;
    &lt;p&gt;Unlike other hopefuls, however, Edwards had a platform. He could not get jobs with publishers, but HarperCollins was willing to publish his guide to university life, The Uni-Verse. Having tried the conventional routes into employment, Edwards launched himself as a full-time book influencer. He posted reading-related videos from London, Paris, New York, Korea and elsewhere. The period was “really special” to Edwards. He read favourite authors in their native locations. He started Mieko Kawakami’s All the Lovers in the Night on the bullet train to Tokyo and finished the last page as the train arrived. “From the first page I was just absolutely entranced… I felt this real sense of catharsis, that I’d been through this entire journey, physically throughout Japan, but also with that author.” Occasionally, in a bodega, a coffee shop, a bookstore or a library, people would “tap me on the shoulder and say that they read their favourite book because they found it from my channel. And there’s no better feeling in the world than that – nothing.”&lt;/p&gt;
    &lt;p&gt;Like many graduates his age, Edwards settled down in London. He posts reviews, commentary and hauls. His style as a literary creator has become defined, and honours the lessons of his social media success. Books get positive reviews if they stir his emotions – especially if they make him weep. With social media, he told me, “we can visually see people reacting to a book and having a kind of subjective emotional response that feels even more personal than maybe a book critic or journalist can. [In traditional reviews] we don’t get to see them crying in response to a book that broke their heart. I think that’s part of the magic of BookTok.”&lt;/p&gt;
    &lt;p&gt;Edwards champions BookTok and also defends it. Critics argue that the platform is more about being seen reading than actually reading, and that it promotes low-quality fiction. Edwards rejects that characterisation energetically: “The only feature common to all ‘BookTok books’ – which can mean both enemies-to-lovers roommate novels and Greek mythological retellings – is popularity with a young audience.” He reminds me of “White Nights”, the Dostoevsky story that his review placed “alongside Colleen Hoover and Sarah J Maas on a ‘BookTok made me buy it’ shelf… I’ll recommend contemporary novels alongside classics because I think often we’re looking for those moments that resonate with us and that feel perennial, that feel evergreen.”&lt;/p&gt;
    &lt;p&gt;His favourite classics are by Toni Morrison and James Baldwin. They are “people who I think really understood the human condition, what it was to feel, to love, and to interact with the people around us.” More recently, “I’m a big Sally Rooney fan. She talks about when you’re on your deathbed you’ll think about the people that you loved and that you interacted with… it’s not frivolous to make art about the people that you loved and that you interacted with.”&lt;/p&gt;
    &lt;p&gt;He keeps his core content mostly apolitical. There is a progressive, inclusionary tone to his videos – “a book from every country in Asia” or “banned books and rainbow capitalism” – but he is seldom explicitly partisan online. He is more outspoken, however, on his X account, where he reposts Jeremy Corbyn and Zarah Sultana and makes no secret of his dislike for Keir Starmer’s Labour government.&lt;/p&gt;
    &lt;p&gt;Mostly, his commitment is to books. He told me: “I think one of the truest things for the world right now is that the worst of times can facilitate really incredible art, and we need that sort of self-expression.” Where we have come from, where we are going, and so on are crucial considerations when giving praise: “They always do represent where we’re at as a society and a culture.” Edwards quoted Bertolt Brecht, asking if there will also be singing in the dark times – and answering yes: there will be singing about the dark times.&lt;/p&gt;
    &lt;p&gt;When we first spoke I was struck by how open and positive Edwards was. It may come easily to the enormously successful, but Edwards maintained a generous, pleasant perspective the whole time we spoke. Abusive comments: “Sometimes people’s responses can be quite personal. I suppose that’s probably the drawback. But the pros outweigh the cons by a long way!” A spanner about the Booker’s politics: “You’re making me work for it, I’m on my toes!” Suffering a fanbase pile-on, as he did after criticising the author of The Kissing Booth: “Yeah! People feel very strongly about books. That’s part of the fun!” On the relevance of establishment awards: “For me they’re a good guiding light to discover new things.” On pressure: “I obviously feel pressure being online… to always keep it kind.”&lt;/p&gt;
    &lt;p&gt;It seems he has always been that way. A student at university contemporaneously with Edwards told me how sardonic whispers would circulate whenever it became known that “the YouTuber” was in a bar. People would ask for selfies, not without a trace of nastiness. At that time, Edwards was far from successful enough to overwhelm such sneers. But he was always a good sport. I was shown a picture in which he had been willing to hold up a baby doll that he had been handed without explanation.&lt;/p&gt;
    &lt;p&gt;But soon after our first interview, Edwards’s output waned. His videos became sparser and more subdued, and they stayed that way for the rest of 2024. Then, at the start of 2025, Edwards posted a confessional video titled “every book I want to read in 2025 (and why i stopped posting book reviews)”. He described last year as “something to survive” and admitted that it had “got to the point where I had to really seriously consider whether I wanted to continue doing this.”&lt;/p&gt;
    &lt;p&gt;Edwards is subject to copious online abuse. On camera, he mentioned withholding his opinions on pop culture because people called them cringe. But the harassment is much harsher than that, and touches on topics Edwards does not mention. The top posts on his own Reddit page accuse him – without citing evidence – of not reading the books he talks about, plagiarising reviews and “scamming” his followers with unsubstantial paid-content tiers. The thread about him on the gossip forum Tattle Life has been remade after reaching the site’s comment limit four times. The comments often speculate, cruelly, about his sexuality.&lt;/p&gt;
    &lt;p&gt;On top of calculated abuse, Edwards also suffered from the ambient pressures of fame. For instance, the book review site Goodreads labels Edwards’s account “#1 most read”: if he leaves a review on a book, it will be the first one users see, meaning a negative review can severely affect a book’s chances of success.&lt;/p&gt;
    &lt;p&gt;All this got to Edwards. Friends asked if he realised he was apologising and issuing disclaimers before speaking when he socialised with them. He lived with his “guard up all the time, flinching constantly, feeling just genuinely afraid to share anything”. He stopped posting opinions and even changed his accent because of criticism. In the video in which he discussed his withdrawal, he acknowledged commenters who said they missed “the old Jack”. He said, “I miss him too, I really do.”&lt;/p&gt;
    &lt;p&gt;Becoming an influencer is now by many accounts the most popular career ambition for the young. But many who succeed take long breaks for their mental health, or withdraw entirely. Edwards reckons that online fame is “not something the human brain is wired for”. He described it as “traumatic” to see “so many people discussing something about you.” It feels “so invasive and intimate”.&lt;/p&gt;
    &lt;p&gt;Edwards was born in Brighton in 1998 with terrible eyesight. A prescription of 8.75 keeps you off the football pitch. “You don’t really see the ball until it’s, like, inches from your face, and by that point it’s too late to pick which part of the head it’s gonna hit,” Edwards told me when we spoke again in August last year. His bookishness proceeded from his near blindness; his parents taught him the letters very young so they could get him a proper eye test. He started a blog to review the new Hunger Games film when he was 14.&lt;/p&gt;
    &lt;p&gt;If reading was a solitary pastime for Edwards the child, it is a solitary job for Edwards the man. He lives in a flat in Soho – not a cheap part of London – with another content creator for a flatmate. He never joined anything like a graduate scheme with colleagues his age. He rents a co-working space because, he says, “I like to feel like there is life going on around me.” He sometimes wishes someone would give him feedback on his work before he publishes it, because when the first judges are the commenters, “at that point there’s no room to manoeuvre… you kind of get tested in the public court before you, you know?”&lt;/p&gt;
    &lt;p&gt;Edwards’s Zoom background featured the same bookshelves visible in his dejected January 2025 video. But he seemed to have developed a new toughness since then. He looked older, with a more muscular neck and longer, slightly receding hair, and he seemed almost bloody-minded about pushing on. In his own words, “Maybe my prefrontal cortex just developed, but I no longer care to have to argue my point or prove my moral worth every time I don’t enjoy something.” He dismissed unfair criticism flatly, calling it “nitpicking culture” and “moral panic” that is “not how the world works”. Where his old videos overshared in “toxic honesty”, he now has definite lines about not discussing his friends or relationships. That “really cutting remarks” are the comments that “stay with you”, he acknowledged as an unfortunate but permanent fact. He had a detached view of social media. As well as disdaining the angsts it causes, he distrusts the comforts it offers. “You have this illusion of community,” it was surprising to hear him say, “when we’re really very alone.”&lt;/p&gt;
    &lt;p&gt;His latest project is an attempt to forge real community, though it is still essentially a social media product. He wants his new book club, Inklings, to be “this book club community where I can have something that’s a bit more solid, tangible”. When I asked about his hopes for the next five years, he said he was focusing on the work. At that time, Inklings had no episodes but was already the fastest-growing club in the platform Fable’s history. Since then, its episodes have featured Oisín McKenna (Evenings and Weekends), David Nicholls (One Day), Coco Mellors (Cleopatra and Frankenstein), Luke Thompson (Bridgerton) and the actor Cillian Murphy.&lt;/p&gt;
    &lt;p&gt;In September, he recorded an episode with Palestine-born novelist Yasmin Zaher at the Saatchi Gallery. A large, orderly crowd filled pews in a white gallery lit by pink accent lights. I had been the only man of the first 20 people to queue. It was possible to track the location of a daddy long legs that was floating through the room by the screaming and lurching it provoked. Edwards came down the aisle in a t-shirt and corduroy trousers, laughing nervously and smiling at his fans, who cheered. On stage, he said it was “so special for me to get to be here with you all… to take this thing we do in isolation… moments we spend in our own heads… and turn it into community”.&lt;/p&gt;
    &lt;p&gt;Edwards said he escaped feeling overexposed by “finding something – for me, it’s been books and reading – to refract everything through”. He asked whether Zaher’s characters obsessed over “being clean, perhaps because there’s little else they can control”, and whether, in the case of another, the “dirty parts of her she couldn’t clean then became a target”. He said we knew “almost too much” about the protagonist when we heard her “sexual perversions and hot takes”, and mentioned another who “withholds, and she keeps for herself almost as a way of maintaining ownership of, and having control of, her identity, and saying, actually, I can’t be defined by external sources; I define myself”.&lt;/p&gt;
    &lt;p&gt;Zaher’s novel, The Coin, was her debut. Edwards asked if being unknown allowed her a “lack of inhibitions”. She agreed that you are freer when “you don’t know who your audience may be”. Edwards knows who his audience is, and they know him. He told me of fans commenting on his interviews that they could answer every question for him.&lt;/p&gt;
    &lt;p&gt;But if the relationship is not free; it is tender. Rose, 25, unemployed after failing to find a job in publishing, had been watching him “constantly for five years”. She said, “I literally rediscovered the love of literature through him – so big, big fan.” Allesia, 21, unemployed after recently graduating, had gotten into Edwards after immigrating from Ukraine six years ago, at which point she could not read in English. “I like how accessible he is to people who don’t read or have just started reading.” Ryan, 28, had watched Edwards for a decade. He now directs AI engineering at a tech company but had previously worked for influencers in LA. He had seen that “it’s all consuming when your entire life is to be someone; that can be brutal”.&lt;/p&gt;
    &lt;p&gt;Almost everyone was there by themselves. Some came with a parent or a friend, but I didn’t see a crowd of three. Rose said Edwards’s content was “the kind you stumble onto when you’re, like, in your room alone”. Ryan said that in his decade of viewing he had come to “enjoy my self more”. Almost immediately after taking the microphone, Edwards had said, “Bear with me while I try not to cry.” It was easy to imagine his audience feeling he had done the same for them. Edwards helped people, Rose reckoned: “He brings a lot of, like, humanity and empathy… I think the world would be a better place if everyone read fiction. It’s literally learning how to put yourself in other people’s shoes.”&lt;/p&gt;
    &lt;p&gt;Edwards is working on his first novel. He would only tell me that it is historical and set in the UK. He will certainly find a publisher. He claims his whole life has been “that pursuit of letters on a page”. Whether that pursuit will lead to literary greatness of course remains to be seen. He may not become Dr Johnson – but Jack Edwards seems, at last, to be becoming himself.&lt;/p&gt;
    &lt;p&gt;[Further reading: Cats are better than dogs – and much better than David Baddiel]&lt;/p&gt;
    &lt;p&gt;This article appears in the 21 Jan 2026 issue of the New Statesman, Europe is back&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751231</guid><pubDate>Sun, 25 Jan 2026 06:06:33 +0000</pubDate></item><item><title>Introduction to PostgreSQL Indexes</title><link>https://dlt.github.io/blog/posts/introduction-to-postgresql-indexes/</link><description>&lt;doc fingerprint="791c02166dd40ba4"&gt;
  &lt;main&gt;
    &lt;p&gt;20 minutes&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction to PostgreSQL Indexes&lt;/head&gt;
    &lt;head rend="h2"&gt;Who’s this for&lt;/head&gt;
    &lt;p&gt;This text is for developers that have an intuitive knowledge of what database indexes are, but don’t necessarily know how they work internaly, what are the tradeoffs associated with indexes, what are the types of indexes provided by postgres and how you can use some of its more advanced options to make them more optimized for your use case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basics&lt;/head&gt;
    &lt;p&gt;Indexes are special database objects primarily designed to increase the speed of data access, by allowing the database to read less data from the disk. They can also be used to enforce constraints like primary keys, unique keys and exclusion. Indexes are important for performance but do not speedup a query unless the query matches the columns and data types in the index. Also, as a very rough rule of thumb, an index will only help if less than 15-20% of the table will be returned in the query, otherwise the query planner, a part of postgres used to determine how the query is going to be executed, might prefer a sequential scan. In fact, reality is much more complex than this rule of thumb. The query planner uses statistics and predefined costs associated with each type of scan to do its job, but we’re only going approach the query planner behavior tangentially in this article. So, if your query returns a large percentage of the table, consider refactoring it, using summary tables or other techniques before throwing an index at the problem. With that in mind, let’s give a closer look at how Postgres stores your data in the disk and how indexes help to speedup querying this data.&lt;/p&gt;
    &lt;p&gt;There are six types of indexes available in the default postgres installation and more types available through extensions. Typically, they work by associating a key value with a data location in one or more rows of the table containing that key. Each line is identified by a TID, or tuple id.&lt;/p&gt;
    &lt;head rend="h3"&gt;How data is stored in disk&lt;/head&gt;
    &lt;p&gt;To understand indexes, it is important to first understand how postgres stores table data on disk. Every table in postgres has one or more corresponding files on disk, depending on its size. This set of files is called a heap and it is divided into 8kb pagesh. All table rows, internally referred to as “tuples”, are saved in these files and do not have a specific order. The index is a tree structure that links the indexes columns to the row locators, also known as ctid, in the heap. We’ll zoom into the index internals later.&lt;/p&gt;
    &lt;p&gt;To see the heap files we can use a few postgres internal tables to see where they’re located in the disk. First, we can enter psql and use &lt;code&gt;show data_directory&lt;/code&gt; to show the directory Postgres uses to store databases physical files.&lt;/p&gt;
    &lt;code&gt; show data_directory;

         data_directory          
---------------------------------
 /opt/homebrew/var/postgresql@16&lt;/code&gt;
    &lt;p&gt;Now we can use the internal &lt;code&gt;pg_class&lt;/code&gt; to find the file where the heap table is stored:&lt;/p&gt;
    &lt;code&gt;create table foo (id int, name text);


select oid, datname
from pg_database
where datname = 'my_database';                                                                                

  oid  |         datname        
-------+-------------------------
 71122 | my_database
(1 row)&lt;/code&gt;
    &lt;code&gt;select relfilenode from pg_class where relname = 'foo';                                                                                                  
 relfilenode
-------------
       71123&lt;/code&gt;
    &lt;p&gt;Finally, we can check the file on disk by running this command in the shell (ls $PGDATA/base/&amp;lt;database_oid&amp;gt;/&amp;lt;table_oid&amp;gt;):&lt;/p&gt;
    &lt;code&gt;ls -lrt /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin  0 16 Aug 14:20 /opt/homebrew/var/postgresql@16/base/71122/71123&lt;/code&gt;
    &lt;p&gt;The file has size 0 because we haven’t done any INSERTs in this table yet.&lt;/p&gt;
    &lt;p&gt;Let’s add a couple of rows to our table:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name) values (1, 'Ronaldo');
INSERT 0 1
insert into foo (id, name) values (2, 'Romario');
INSERT 0 1&lt;/code&gt;
    &lt;p&gt;We can add the &lt;code&gt;ctid&lt;/code&gt; field to the query to retrieve the ctid of each line. The ctid is an internal field that has the address of the line in the heap. Think of it as a pointer to the row location in the heap. It consists of a tuple in the format (m, n) where m is the block id and n is the tuple offset. “ctid” stands for “current tuple id”. Here you can note that the row with id one is stored in the page 0, offset 1.&lt;/p&gt;
    &lt;code&gt;select ctid, * from foo;
 ctid  | id |  name   
-------+----+---------
 (0,1) |  1 | Ronaldo
 (0,2) |  2 | Romario
(2 rows)&lt;/code&gt;
    &lt;head rend="h3"&gt;How indexes speedup access to data&lt;/head&gt;
    &lt;p&gt;Let’s add more players to the table so that the total rows is one million:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name);
select generate_series(3, 1000000), 'Player ' || generate_series(3, 1000000);&lt;/code&gt;
    &lt;p&gt;After adding more rows to the table its corresponding file is 30MB. Internally, it is divided into 8kb pages.&lt;/p&gt;
    &lt;code&gt;ls -lrtah /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin    30M 16 Aug 16:32 /opt/homebrew/var/postgresql@16/base/71122/71133&lt;/code&gt;
    &lt;p&gt;When we query a table without an index, Postgres reads all tuples in every page and apply a filter. For example, let’s analyze the command below that searches for rows whose &lt;code&gt;name&lt;/code&gt; column value is equal to “Ronaldo” and show how the database performed this search. We use the explain command with the options &lt;code&gt;(analyse, buffers)&lt;/code&gt;. &lt;code&gt;analyse&lt;/code&gt; will actually execute the query instead of just using cost estimates, and the &lt;code&gt;buffers&lt;/code&gt; option shows how much IO work was done.&lt;/p&gt;
    &lt;code&gt; explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                     QUERY PLAN
---------------------------------------------------------------------------------------------------------------------
 Gather  (cost=1000.00..12577.43 rows=1 width=18) (actual time=0.307..264.991 rows=1 loops=1)
   Workers Planned: 2
   Workers Launched: 2
   Buffers: shared hit=97 read=6272
   -&amp;gt;  Parallel Seq Scan on foo  (cost=0.00..11577.33 rows=1 width=18) (actual time=169.520..256.639 rows=0 loops=3)
         Filter: (name = 'Ronaldo'::text)
         Rows Removed by Filter: 333333
         Buffers: shared hit=97 read=6272
 Planning Time: 0.143 ms
 Execution Time: 265.021 ms&lt;/code&gt;
    &lt;p&gt;Note the in output the line starting with " -&amp;gt; Parallel Seq scan on foo". This line denotes that the database performed a sequential search and read all the rows in the table. The execution time for this query was 265.021ms. Also note the line that says “Buffers: shared hit=97 read=6272”. This mean that we needed to read 97 pages from memory, and 6272 pages from disk.&lt;/p&gt;
    &lt;p&gt;Now let’s add an index on the name column and see how the same query performs. We’re using the command &lt;code&gt;create index concurrently&lt;/code&gt; because we don’t want to block the table for writes.&lt;/p&gt;
    &lt;code&gt;create index concurrently on foo(name);
CREATE INDEX

explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
 Index Scan using foo_name_idx on foo  (cost=0.42..8.44 rows=1 width=18) (actual time=0.047..0.049 rows=1 loops=1)
   Index Cond: (name = 'Ronaldo'::text)
   Buffers: shared hit=4
 Planning Time: 0.129 ms
 Execution Time: 0.077 ms
(5 rows)&lt;/code&gt;
    &lt;p&gt;Here we see that the index was used and that in this case the execution time was reduced from 264.21 to 0.074 milliseconds, and the database only needed to read 4 pages! The reduction in execution time happens because, now, instead of reading all the rows in the table, the database uses the index. The index is a tree structure mapping the value “Ronaldo” to the ctid(s) of the rows that have this value in the &lt;code&gt;name&lt;/code&gt; column (in our example we only have one such row). The ctid is then used to quickly locate these rows on the heap.&lt;/p&gt;
    &lt;p&gt;If we use &lt;code&gt;\di+&lt;/code&gt; to show the indexes in our database we can see that the index we’ve created occupies &lt;code&gt;30MB&lt;/code&gt;, roughly the same size as the &lt;code&gt;foo&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;\di+

                                         List of relations
 Schema |     Name     | Type  | Owner | Table | Persistence | Access method | Size  | Description
--------+--------------+-------+-------+-------+-------------+---------------+-------+-------------
 public | foo_name_idx | index | dlt   | foo   | permanent   | btree         | 30 MB |
(1 row)&lt;/code&gt;
    &lt;head rend="h2"&gt;Costs associated with indexes&lt;/head&gt;
    &lt;p&gt;It is important to highlight that the extra speed brought by indices is associated with several costs that must be considered when deciding where and how to apply them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disk Space&lt;/head&gt;
    &lt;p&gt;Indexes are stored in a separate area of the heap and take up additional disk space. The more indexes a table has, the greater the amount of disk space required to store them. This incurs in additional storage costs for your database and for backups, increased replication traffic, and increased backup and failover recovery times. Bear in mind that its not uncommon for btree indexes to be larger than the table itself. Learning about partial indexes, and multicolumn indexes, as well as about other more space efficient index types such as BRIN can be helpful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write operations&lt;/head&gt;
    &lt;p&gt;Also, there is a maintenance cost in writing operations such as UPDATE, INSERT and DELETE, if a field that is part of an index is modified, the corresponding index needs to be updated, which can add significant overhead to the writing process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Query planner&lt;/head&gt;
    &lt;p&gt;The query planner (also known as query optimizer) is the component responsible for determining the best execution strategy for a query. With more indexes available, the query planner has more options to consider, which can increase the time needed to plan the query, especially in systems with many complex queries or where there are many indexes available.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory usage&lt;/head&gt;
    &lt;p&gt;PostgreSQL maintains a portion of frequently accessed data and index pages in memory in its shared buffers. When an index is used, the relevant index pages are loaded into shared buffers to speed up access. The more indexes you have and the more they are used, the more shared buffer memory is necessary. Since shared buffers are limited and are also used for caching data pages, filling the shared buffers with indexes can lead to less efficient caching of table data. It’s also good to keep in mind that the whole indexed column is copied in every node of the btree, since there’s a limit in node size capacity, the larger the indexed column the deeper the tree will be.&lt;/p&gt;
    &lt;p&gt;Another aspect of memory usage is that PostgreSQL uses work memory when it executes queries that involves sorting or complex index scans (involving multi-column or covering indexes). Larger indexes require more memory for these operations. Also, indexes require memory to store some metadata about their structure, column names and statistics in the system catalog cache. And finally indexes require memory for maintainance operations like vacuuming and reindexing operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types of Indexes&lt;/head&gt;
    &lt;head rend="h3"&gt;Btree&lt;/head&gt;
    &lt;p&gt;The B-Tree is a very powerful data structure, present not only in Postgres but in almost every database management system, since it is a very good general purpose index. It was invented by Rudolf Bayer and Edward M.McCreight while working at Boeing. Nobody really knows if the “B” in B-tree stands for Bayer, Boeing, balanced or better, and it doesn’t really matter. What really matters is that it enables us to search elements in the tree in O(log n) time. If you’re not familiar with Big-O notation, all you need to know is that is is really fast - you only need to make 20 comparisons in order to find an element in a set with 1 million items. Moreover, it can maintain O(log n) time complexity for data sets that are larger than the RAM available on a computer. This means that disks can be used to extend RAM, thanks to the btree efficient prevention of disk page accesses to find the desired data. In PostgreSQL the btree is the most common type of index and its the default, it’s also used to support system and TOAST indexes. Even an empty database has hundreds of btree indexes. It is the only index type that can be used for primary and unique key constraints.&lt;/p&gt;
    &lt;p&gt;In contrast with a binary tree, the BTree is a balanced tree and all of its leave nodes have the same distance from the root. The root nodes and inner nodes have pointers to lower levels, and the leaf nodes have the keys and pointers to the heap. Postgres btrees also have pointers to the left and right nodes for easier forward and backward scanning. Nodes can have multiple keys and these keys are sorted so that it’s easy to walk in ordered directions and to perform ORDER BY and JOIN operations. The values are only stored in the leaf nodes, this makes the tree more compact and facilitates a full traversal of the objects in a tree with just a linear pass through all the leaf nodes. This is just a simplified description of PostgreSQL Btree indexes, if you want to get into the low level details, I suggest you to read the README and the paper that inspired them. Below there’s a simplified illustration of a Postgres Btree.&lt;/p&gt;
    &lt;head rend="h4"&gt;Using multiple indexes&lt;/head&gt;
    &lt;p&gt;Postgres can use multiple indexes to handle cases that cannot be handled by single index scans, by forming &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; conditions across several index scans with the support of bitmaps. The bitmaps are ANDed or ORed together as needed by the query and finally the table rows are visited and returned. Let’s say we have a query like this:&lt;/p&gt;
    &lt;code&gt;select * from users where age = 30 and login_count = 100;&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;login_count&lt;/code&gt; columns are indexed, postgres scans index &lt;code&gt;age&lt;/code&gt; for all pages with &lt;code&gt;age=30&lt;/code&gt; and makes a bitmap where the pages that might contain rows with &lt;code&gt;age=30&lt;/code&gt; are true. In a similar way, it builds a bitmap using the &lt;code&gt;login_count&lt;/code&gt; index. It then ANDs the two bitmaps to form a third bitmap, and performs a table scan, only reading the pages that might contain candidate values, and only adding the rows where &lt;code&gt;age=30 and login_count=100&lt;/code&gt; to the result set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multi-column indexes&lt;/head&gt;
    &lt;p&gt;Multi-column indexes are an alternative for using multiple indexes. They’re generaly going to be smaller and faster than using multiple indexes, but they’ll also be less flexible. That’s because the order of the columns matter, because the database can search for a subset of the indexed columns, as long as they are the leftmost columns. For example, if you have an index on column &lt;code&gt;a&lt;/code&gt; and another index on column &lt;code&gt;b&lt;/code&gt;, these indexes will serve all the of queries below:&lt;/p&gt;
    &lt;code&gt;select * from my_table where a = 42 and b = 420;

select * from my_table where a = 43;

select * from my_table where b = 99;&lt;/code&gt;
    &lt;p&gt;On the other hand, only the first two queries would use an index if you created a multi-column index on (a, b) with a command like &lt;code&gt;create index on my_table(a, b)&lt;/code&gt;; So, when building multi-column indexes choose the order of the columns well so that your index can be used by the most queries possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Partial indexes&lt;/head&gt;
    &lt;p&gt;Partial indexes allow you to use a conditional expression to control what subset of rows will be indexed, this can bring you many benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;your index can be smaller and more likely fit in RAM.&lt;/item&gt;
      &lt;item&gt;your index is shallower, so lookups are quicker&lt;/item&gt;
      &lt;item&gt;less overhead for index/update/delete (but can also mean more overhead if the column you’re using to filter rows in/out of the index is updated very frequently triggering constant index maintenance)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They’re mostly useful in situations where you don’t care about some rows, or when you’re indexing on a column where the proportion of one value is much greater than others. I’ll give two examples below.&lt;/p&gt;
    &lt;head rend="h5"&gt;When you don’t care about some rows&lt;/head&gt;
    &lt;p&gt;Let’s say you have a rules table where the rows can be marked as enabled/disabled, the vast majority of the rows are disabled and in your queries you only care about enabled rows. In this case, you would have a partial index, filtering out the disable rows like this:&lt;/p&gt;
    &lt;code&gt;create index on rules(status) where status = 'enabled';&lt;/code&gt;
    &lt;head rend="h5"&gt;When the distribution of values is skewed&lt;/head&gt;
    &lt;p&gt;Now imagine you’re building a todo application and the status column value can be either &lt;code&gt;TODO&lt;/code&gt;, &lt;code&gt;DOING&lt;/code&gt;, and &lt;code&gt;DONE&lt;/code&gt;. Suppose you have 1M rows and this is the current distribution of rows in each status:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rows&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TODO&lt;/cell&gt;
        &lt;cell&gt;90%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DOING&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DONE&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Since postgres keeps statistics about the distribution of values in your table columns and knows that the vast majority of the rows are in the &lt;code&gt;TODO&lt;/code&gt; status, it would choose to do a sequential scan on the &lt;code&gt;tasks&lt;/code&gt; table when you have &lt;code&gt;status='TODO'&lt;/code&gt; in the &lt;code&gt;WHERE&lt;/code&gt; clause of your query, even if you have an index on status, leaving most part of the index unused and wasting space. In this case, a partial scan such as the one below is recommended:&lt;/p&gt;
    &lt;code&gt;create index on tasks(status) where status &amp;lt;&amp;gt; 'TODO';&lt;/code&gt;
    &lt;head rend="h4"&gt;Covering indexes&lt;/head&gt;
    &lt;p&gt;If you have a query that selects only columns in an index, Postgres has all information needed by the query in the index and doesn’t need to fetch pages from the heap to return the result. This optimization is called &lt;code&gt;index-only scan&lt;/code&gt;. To understand how it works, consider the following scenario:&lt;/p&gt;
    &lt;code&gt;create table bar (a int, b int, c int);
create index abc_idx on bar(a, b);

/* query 1 */
select a, b from bar;

/* query 2 */
select a, b, c from bar;&lt;/code&gt;
    &lt;p&gt;In the first query, postgres can do an index-only scan and avoid fetching data from the heap because the values &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are present in the index. In the second query, since &lt;code&gt;c&lt;/code&gt; isn’t in the index, posgres needs to follow the reference to the heap to fetch its value. In the first query we allowed postgres do to an index-only scan with the help of a multi-column index, but we could also achieve the same result by using a covering index. The syntax for creating a covering index looks like this:&lt;/p&gt;
    &lt;code&gt;create index abc_cov_idx on bar(a, b) including c;&lt;/code&gt;
    &lt;p&gt;This is more space efficient than creating a multi-column index on (a, b, c), because c will only be inserted at the leaf nodes of the btree. Also, we might want to use a covering index in cases where we want an unique index and &lt;code&gt;c&lt;/code&gt; would “break” the uniqueness of the index.&lt;/p&gt;
    &lt;head rend="h4"&gt;Expression indexes&lt;/head&gt;
    &lt;p&gt;Expression indexes to index the result of an expression or function, rather than just the raw column values. This can be extremely useful when you frequently query based on a transformed version of your data. It is necessary if you use a function as part of a where clause as in the example below:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name TEXT
);

CREATE INDEX idx_name ON customers(name);
SELECT * FROM customers WHERE LOWER(name) = 'john doe';&lt;/code&gt;
    &lt;p&gt;In this example above, Postgres won’t use the index because it was was built against the &lt;code&gt;name&lt;/code&gt; column. In order to make it work, the index key has to call the &lt;code&gt;lower&lt;/code&gt; function just like it’s used in the where clase. To fix it, do:&lt;/p&gt;
    &lt;p&gt;Now, when you run a query like this:&lt;/p&gt;
    &lt;code&gt;CREATE INDEX idx_lower_name ON customers (lower(name));&lt;/code&gt;
    &lt;p&gt;Now PostgreSQL can use the expression index to efficiently find the matching rows.&lt;/p&gt;
    &lt;p&gt;Expression indexes can be created using various types of expressions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Built-in functions: Like &lt;code&gt;lower()&lt;/code&gt;,&lt;code&gt;upper()&lt;/code&gt;, etc.&lt;/item&gt;
      &lt;item&gt;User-defined functions: As long as they are immutable.&lt;/item&gt;
      &lt;item&gt;String concatenations: Like &lt;code&gt;first_name || ' ' || last_name&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hash&lt;/head&gt;
    &lt;p&gt;The hash index differs from B-Tree in strucutre, it is much more alike a hashmap data structure present in most programming languages (e.g. dict in Python, array in php, HashMap in java, etc). Instead of adding the full column value to the index, a 32bit hash code is derived from it and added to the hash. This makes hash indexes much smaller than btrees when indexing longer data such as UUIDs, URLs, etc. Any data type can be indexed with the help of postgres hashing functions. If you type &lt;code&gt;\df hash*&lt;/code&gt; and press TAB in psql, you’ll see that there are more then 50 hash related functions. Although it gracefully handles hash conflicts, it works better for even distribution of hash values and is most suited to unique or mostly unique data. Under the correct conditions it will not only be smaller than btree indexes, but also it will be faster for reads when compared with btress. Here’s what the official docs says about it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“In a B-tree index, searches must descend through the tree until the leaf page is found. In tables with millions of rows, this descent can increase access time to data. The equivalent of a leaf page in a hash index is referred to as a bucket page. In contrast, a hash index allows accessing the bucket pages directly, thereby potentially reducing index access time in larger tables. This reduction in “logical I/O” becomes even more pronounced on indexes/data larger than shared_buffers/RAM.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As for its limitations, it only supports equality operations and isn’t going to be helpful if you need to order by the indexed field. It also doesn’t support multi-column indexes and checking for uniqueness. For a in-depth analysis of how hash indexes fare in relation to btree, check Evgeniy Demin’s blog post on the subject.&lt;/p&gt;
    &lt;head rend="h3"&gt;BRIN&lt;/head&gt;
    &lt;p&gt;BRIN stands for Block Range Index and its name tells a lot about how it is implemented. Nodes in BRIN indexes store the minimum and maximum values of a range of values present in the page referred by the index. This makes the index more compact and cache friendly, but restricts the use cases for it. If you have a very large in a work load that is heavy on writes and low on deletes and updates. You can think of a BRIN index as an optimizer for sequential scans of large amounts of data in very large databases, and is a good optimization to try before partitioning a table. For a BRIN index to work well, the index key should be a column that strongly correlates to the location of the row in the heap.Some good use cases for BRIN are append-only tables and tables storing time series data.&lt;/p&gt;
    &lt;p&gt;BRIN won’t work well for tables where the rows are updated constantly, due to the nature of MVCC that duplicates rows and stores them in a different part of the heap. This tuple duplication and moving affect the correlation negatively and reduces the effectiveness of the index. Using extensions such as pg_repack or pg_squeeze isn’t recommended for tables that use BRIN indexes, since they change the internal data layour fo the table and mess up the correlation. Also, this index is lossy in the sense that the index leaf nodes point to pages taht might contain a value within a particular range. For this reason a BRIN is more helpful if you need to return large subset of data, and a btree would be more read performant for queries that only return one or few rows. You can make the index more or less lossy by adjusting the &lt;code&gt;page_per_range&lt;/code&gt; configuration, the trade off will be index size.&lt;/p&gt;
    &lt;head rend="h3"&gt;GIN&lt;/head&gt;
    &lt;p&gt;Generalized inverted index is appropriate for when you want to search for an item in composite data, such as finding a word in a blob of text, an item in an array or an object in a JSON. The GIN is generalized in the sense that it doesn’t need to know how it will acelerate the search for some item. Instead, there’s a set of custom strategies specific for each data type. Please note that in order to index an JSON value it needs to be stored in a JSONB column. Similarly, if you’re indexing text it’s better to store it as (or convert it to) tsvector or use the pg_trgm extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;GiST &amp;amp; SP-GiST&lt;/head&gt;
    &lt;p&gt;The Generalized Search Tree and the Space-Partitioned Generalized Search Tree are tree structures that can be use as a base template to implement indexes for specific data types. You can think of them as framework for building indexes. The GiST is a balanced tree and the SP-GiST allow for the development of non-balanced data structures. They are useful for indexing points and geometric types, inet, ranges and text vectors. You can find an extensive list of the built-in strategies shipped with postgres in the official documentation. If you need an index to enable full-text search in your application, you’ll have to choose between GIN and GiST. Roughly speaking, GIN is faster for lookups but it’s bigger and has greater building and maintainance costs. So the right index type for you will depend on your application requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Understanding and effectively using indexes is crucial for optimizing database performance in PostgreSQL. While indexes can greatly speed up query execution and improve overall efficiency, it’s important to be mindful of their impact on write operations and storage. By carefully selecting the appropriate types of indexes based on your specific use cases you can ensure that your PostgreSQL database remains both fast and efficient. I hope this article taught you at least one or two things you didn’t know about Postgres indexes, and that you’re better equiped to deal with different scenarios involving databases from now on.&lt;/p&gt;
    &lt;p&gt;4119 Words&lt;/p&gt;
    &lt;p&gt;2024-09-11 08:07&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751826</guid><pubDate>Sun, 25 Jan 2026 08:07:03 +0000</pubDate></item><item><title>Deutsche Telekom is throttling the internet</title><link>https://netzbremse.de/en/</link><description>&lt;doc fingerprint="66133f28ed9a317f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deutsche Telekom is throttling the internet. Let's do something about it!&lt;/head&gt;
    &lt;p&gt;If you are a customer of Deutsche Telekom and some websites just won't load, then we might have the solution to your problem!&lt;/p&gt;
    &lt;head rend="h2"&gt;Short Explanation!&lt;/head&gt;
    &lt;head rend="h2"&gt;What is this about?&lt;/head&gt;
    &lt;p&gt;Epicenter.works, the Society for Civil Rights, the Federation of German Consumer Organizations, and Stanford Professor Barbara van Schewick are filing an official complaint with the Federal Network Agency against Deutsche Telekom’s unfair business practices.&lt;/p&gt;
    &lt;p&gt;Deutsche Telekom is creating artificial bottlenecks at access points to its network. Financially strong services that pay Telekom get through quickly and work perfectly. Services that cannot afford this are slowed down and often load slowly or not at all.&lt;/p&gt;
    &lt;p&gt;This means Telekom decides which services we can use without issues, violating net neutrality. We are filing a complaint with the Federal Network Agency to stop this unfair practice together!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testimonials&lt;/head&gt;
    &lt;head rend="h2"&gt;How can you help the project?&lt;/head&gt;
    &lt;p&gt;Are you a Deutsche Telekom customer and want to help? Get in touch with usâevery experience counts! Maybe you even have networking expertise and measurement data that could be relevant? Whether with or without measurements, weâd love to hear from you at netzbremse@epicenter.works.&lt;/p&gt;
    &lt;p&gt;Do you have experience with interconnection agreements with Deutsche Telekom and want to talk to us confidentially? Contact us via email or one of the following channels:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Signal: +43 670 404 98 89&lt;/item&gt;
      &lt;item&gt;Threema (ID: BXJMX4R5)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Donate now for a free internet!Donate now for a free internet!&lt;/head&gt;
    &lt;head rend="h2"&gt;Talk at Chaos Communication Congress 38C3&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Coverage&lt;/head&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, bewusst EngpÃ¤sse im Internet zu schaffen, um zusÃ¤tzlich Geld fÃ¼r schnellere ZugÃ¤nge zu kassieren. Sie sehen darin eine Verletzung von EU-Recht. Die Telekom widerspricht. Von Markus Reher.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis aus mehreren Organisationen hat bei der Bundesnetzagentur eine Beschwerde wegen angeblicher Verletzungen von NetzneutralitÃ¤tspflichten eingereicht.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis mehrerer Organisationen wirft der Telekom vor, kÃ¼nstliche EngpÃ¤sse im Netz zu schaffen und damit Geld zu verdienen. Der Konzern weist die VorwÃ¼rfe zurÃ¼ck und holt zur Gegenkritik aus.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Die Telekom drosselt das Netz", beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;“Die Telekom drosselt das Netz”, beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;Die DeutÂsche Telekom muss endlich die Peering-KapaÂzitÃ¤ten zu anderen Internet-Knoten erhÃ¶hen.&lt;/p&gt;
    &lt;p&gt;VerbrauÂcherÂschÃ¼tzer wollen bei der BundesÂnetzÂagentur Beschwerde gegen Peering-Probleme im Telekom-Netz einreiÂchen.&lt;/p&gt;
    &lt;p&gt;Nicht nur als KrisenlÃ¶sung sucht das Deutsche Forschungsnetz den direkten Anschluss zur Deutschen Telekom. Die wollte sich aber zuerst auf nichts einlassen.&lt;/p&gt;
    &lt;p&gt;Schon seit mindestens Mai 2015 gibt es sie, eine Option fÃ¼r Hetzner-Kunden, die ihre Server fÃ¼r Kunden der Telekom zwischen 19 und 22 Uhr besser erreichbar machen wollen.&lt;/p&gt;
    &lt;p&gt;Die Telekom sieht sich mit schweren VorwÃ¼rfen konfrontiert, nach denen sie absichtlich gegen die NetzneutralitÃ¤t verstoÃen soll. Ãberzeugt davon ist nicht nur die Verbraucherzentrale.&lt;/p&gt;
    &lt;p&gt;Die Verbraucherzentrale sucht Betroffene, die Probleme im Netz der Telekom haben. Der Vorwurf: Verletzung der NetzneutralitÃ¤t.&lt;/p&gt;
    &lt;p&gt;Verbraucher und Organisationen wehren sich gegen Netzdrosselung von Telekom. Erste Beschwerden gehen ein.&lt;/p&gt;
    &lt;p&gt;Der Verbraucherschutz schlÃ¤gt Alarm: Die Deutsche Telekom bevorzugt bei der Internet-Geschwindigkeit offenbar Dienste und Webseiten, die fÃ¼r mehr Tempo zahlen, wÃ¤hrend sie andere drosselt. User sollen das nun bestÃ¤tigen.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom Teile des Internets absichtlich langsam? Dieser Vorwurf hat es in sich und wird vom Verband der Verbraucherzentralen erhoben â ein Einblick in das Prinzip des Peerings und die Frage, wie die Telekom hier seit Jahren fÃ¼r Frust sorgt.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom ihr Netz absichtlich langsamer? Der Verbraucherschutz wirft dem Unternehmen vor, die NetzneutralitÃ¤t absichtlich zu verletzen. Die Telekom verlange von Anbietern Zahlungen fÃ¼r bevorzugten Datentransfer und bremse andere Dienste aus.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751899</guid><pubDate>Sun, 25 Jan 2026 08:22:17 +0000</pubDate></item><item><title>Sony Data Discman</title><link>https://huguesjohnson.com/random/sony-ebook/</link><description>&lt;doc fingerprint="bb65194d5c9f2bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;Back in 1992 I worked at an Electronics Boutique that was an outlet location for the company. We sold regular merchandise but also had an outlet section for clearance stuff aggregated from other stores. This thoroughly confused customers who expected every item in the store to be discounted. The job involved a lot of explaining "no I'm sorry this game that literally launched today is not on clearance". Working retail is a great way to lose faith in the collective intelligence of our species.&lt;/p&gt;
    &lt;p&gt;One day we received several Sony Data Discman Electronic Book Player DD-1EX players that we were supposed to clear out. The original sticker price was $500 but they were marked down to roughly 1% of that. Largely out of curiosity I picked one up along with whatever software we had for it (also at a massive discount). I can't say I've used it for more than an hour. It's a very nice device that serves no practical or entertainment function whatsoever.&lt;/p&gt;
    &lt;p&gt;Using old catalogs as a reference, these were originally listed in the spring 1992 catalog. Here it is:&lt;/p&gt;
    &lt;p&gt;These did not appear in the summer 1992 catalog just a couple months later. Since I started working there during the 1992 holiday season the timeline works out. They must have hit the shelves in early 1992, not sold, then been marked down every month until they were rounded-up and shipped to our location.&lt;/p&gt;
    &lt;p&gt;Let's take a peek at it..&lt;/p&gt;
    &lt;p&gt;Gallery&lt;/p&gt;
    &lt;p&gt;Disclaimer: I am terrible at taking pictures.&lt;/p&gt;
    &lt;p&gt;Sony didn't nickel-and-dime consumers on accessories here. The package came with: the reader (duh), AC adapter, rechargeable battery, and another battery pack that holds AAs. Years later they refused to include an AC adapter in the PlayStation Classic.&lt;/p&gt;
    &lt;p&gt;The reader itself is fairly nice looking. It feels like a miniature laptop. It's a tad on the heavy side but also feels extremely durable. Looking at all the buttons and size of the screen makes me think this had a lot of potential beyond just electronic books. However, it lacks any mechanism to save data. In the early 90s it's not like SD-RAM cards were available. Miniature hard drive? Forget it. It has 90% of what it needs to be a PDA but the technology just wasn't there to get the last 10% in.&lt;/p&gt;
    &lt;p&gt;There's a QWERTY keyboard because all of the books are searchable. The directional pad is there to navigate through menus. Looking at it again just makes me irritated that I don't have any games for this (of course I doubt any were made). This would make a cool little text adventure player.&lt;/p&gt;
    &lt;p&gt;The electronic books are mini CDs in a caddy. I guess that means we can rip them (more on this soon).&lt;/p&gt;
    &lt;p&gt;Bad Screenshots&lt;/p&gt;
    &lt;p&gt;I picked up every electronic book we had in stock. The player has an output jack than can be connected to anything with an A/V input (well, just the "V" part is needed). These screenshots are from the A/V out.&lt;/p&gt;
    &lt;p&gt;The splash screen reminding you that this is for private use only. I guess I'm technically violating that, whatever.&lt;/p&gt;
    &lt;p&gt;Although I don't know the exact date this electronic book reader was produced, the bundled encyclopedia gives some hints. It still lists U.S.S.R. as a country so it had to be authored prior to Christmas day 1991.&lt;/p&gt;
    &lt;p&gt;Early 90s software developer salary in the career guide:&lt;/p&gt;
    &lt;p&gt;Thinking of traveling the world? Well, this handy translator is all you need. Someone once told me that if you ever got lost in a strange foreign country you should claim to be a Swedish citizen. Something about Sweden having an embassy in every country and nobody holding a grudge against them. I couldn't find a translation for "I'm a Swedish citizen please don't turn me over to the secret police" in this guide.&lt;/p&gt;
    &lt;p&gt;The least useful book (to me at least) is the crossword dictionary. You can search for word endings or a list of complete words but that's it.&lt;/p&gt;
    &lt;p&gt;The wellness encyclopedia is the perfect gift for a hypochondriac.&lt;/p&gt;
    &lt;p&gt;Since I won't pay more than $3.99 for a bottle of wine I found this guide relatively useless.&lt;/p&gt;
    &lt;p&gt;Ripping the CDs&lt;/p&gt;
    &lt;p&gt;If you rip the CDs you'll find that some of them contain an emulator for the Discman. Here's the wine guide main screen:&lt;/p&gt;
    &lt;p&gt;It seems like the books are fully functional in this emulator:&lt;/p&gt;
    &lt;p&gt;The career guide also comes with an emulator. You can use it to look for jobs that didn't exist in 1990 I guess:&lt;/p&gt;
    &lt;p&gt;Some CDs, like the encyclopedia, don't have the emulator bundled. However, if you copy the data files around it's trivial to launch it in the emulator bundled on the other CDs:&lt;/p&gt;
    &lt;p&gt;iso Downloads&lt;/p&gt;
    &lt;p&gt;Grab these before I receive a takedown notice.. I mean, these were completely obsolete before Wikipedia existed and are even worse after. I doubt that will stop anyone though. One of the reasons I deleted my YouTube videos was takedown notices from Sony over the intro to Dragon's Lair. Some rapper who sold &amp;lt;100 albums, but is apparently signed with Sony, sampled the intro of Dragon's Lair. In Sony's mind that means they own all rights to it. I don't know how you rap over the Dragon's Lair intro and I don't care to learn. EA also sent me a takedown notice over a Dragon's Lair video, a game they neither wrote nor own the rights to. As far as I can tell they at some point were the distributor for an early iOS version of Dragon's Lair that is no longer available. So to make a long story short, I'm sure these will be offline soon. When that happens I'll try moving them to archive.org since they are somehow able to get away with posting anything.&lt;/p&gt;
    &lt;p&gt;Career encyclopedia (SROM30_VERSION1)&lt;/p&gt;
    &lt;p&gt;Crossword dictionary (SROM13V1_07D)&lt;/p&gt;
    &lt;p&gt;Wellness encyclopedia (SROM25_V1_0)&lt;/p&gt;
    &lt;p&gt;World translator (SROM29_VERSION1)&lt;/p&gt;
    &lt;p&gt;Usual disclaimer that you are downloading isos with executable files from a total rando's site. I am 100% not responsible for any awful thing that happens if you download these and run the files on them.&lt;/p&gt;
    &lt;p&gt;Related&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751906</guid><pubDate>Sun, 25 Jan 2026 08:23:51 +0000</pubDate></item><item><title>A flawed paper in Management Science has been cited more than 6,000 times</title><link>https://statmodeling.stat.columbia.edu/2026/01/22/aking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752151</guid><pubDate>Sun, 25 Jan 2026 09:04:30 +0000</pubDate></item><item><title>Jurassic Park - Tablet device on Nedry's desk? (2012)</title><link>https://www.therpf.com/forums/threads/jurassic-park-tablet-device-on-nedrys-desk.169883/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752261</guid><pubDate>Sun, 25 Jan 2026 09:22:17 +0000</pubDate></item><item><title>Bridging the Gap Between PLECS and SPICE</title><link>https://erickschulz.dev/posts/plecs-spice/</link><description>&lt;doc fingerprint="351921528b158f82"&gt;
  &lt;main&gt;
    &lt;p&gt;Three years ago, we set out to bring SPICE simulation into PLECS. PLECS Spice is finally here.&lt;/p&gt;
    &lt;p&gt;PLECS Spice brings SPICE device-level simulation directly into PLECS. Available with PLECS 5.0, both system-level and device-level analysis can be performed within a single tool, eliminating the need to maintain duplicate models across separate softwares.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate Tools, Duplicate Work&lt;/head&gt;
    &lt;p&gt;Power electronics design has long faced a fundamental trade-off: system-level simulation tools deliver the speed and robustness needed for controller development and overall system analysis, but sacrifice the device-level detail necessary to validate component selection before procurement.&lt;/p&gt;
    &lt;p&gt;For over 20 years, Plexim has promoted a top-down design philosophy, enabling engineers to model complete power electronic systems using ideal switches and behavioral components. By avoiding the computational burden of simulating detailed switching transients, PLECS enables rapid validation of system-level requirements like efficiency, control performance and thermal behavior.&lt;/p&gt;
    &lt;p&gt;Conversely, traditional SPICE simulators embody an inherently bottom-up approach. They excel at validating device-level requirements through detailed semiconductor models, capturing switching losses, voltage overshoots and parasitic effects with high fidelity. This comes at a cost: system-level integration becomes computationally prohibitive.&lt;/p&gt;
    &lt;p&gt;This divide has forced engineers into parallel workflows using separate software platforms with different modeling approaches and incompatible component libraries. Moving from a system-level PLECS model to SPICE for device validation requires recreating the model, an error-prone and time-consuming process.&lt;/p&gt;
    &lt;head rend="h2"&gt;PLECS Spice&lt;/head&gt;
    &lt;p&gt;To solve this problem, Plexim has developed PLECS Spice, an extension that brings SPICE device-level simulation capabilities directly into PLECS. PLECS Spice can simulate hybrid systems containing both standard PLECS and SPICE circuits. This allows a schematic to be progressively refined by replacing the ideal switches in a circuit of interest, like the power stage, with detailed SPICE netlists. Controls and other subsystems can remain unchanged. Because the entire workflow stays within PLECS, engineers can easily toggle between ideal and detailed configurations to compare results. This creates a true top-down workflow where device-level detail is added selectively, only where needed. With PLECS Spice, there is no longer a need to build the same model twice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the Hood&lt;/head&gt;
    &lt;p&gt;The PLECS Spice extension adds four key ingredients that transform PLECS into a fully-featured hybrid simulation platform that can simulate standard PLECS and SPICE models together.&lt;/p&gt;
    &lt;head rend="h3"&gt;Netlist Parser&lt;/head&gt;
    &lt;p&gt;SPICE models are typically distributed as netlists. Simply put, these are text files that describe a circuit topology, component interconnections and parameter values. A key capability of PLECS Spice is its parser’s support for multiple netlist dialects. Different SPICE implementations use distinct syntax conventions, making netlists from various vendors incompatible. The PLECS Spice parser handles these variations automatically, enabling engineers to integrate models provided by different semiconductor manufacturers directly into their schematics. Little to no manual conversion or syntax adaptation is needed, regardless of the dialect.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compact Models&lt;/head&gt;
    &lt;p&gt;Netlists provided by manufacturers often rely on well-established semiconductor device models. These compact models combine physics-based modeling with empirical corrections to capture fundamental electrical behavior while maintaining reasonable complexity. PLECS Spice includes optimized implementations of compact models such as diodes, MOSFETs, BJTs, and switches. Each model defines a set of parameters that can be tuned to match the electrical response of specific physical devices. In PLECS Spice, classical compact models have been improved to guarantee continuity of key physical quantities, enhancing numerical stability. By tightly integrating these models into the solver, PLECS Spice achieves both computational efficiency and robust convergence even in the presence of highly nonlinear semiconductor characteristics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modified Nodal Analysis&lt;/head&gt;
    &lt;p&gt;Standard PLECS uses piecewise state-space equations to simulate electrical models. This approach is computationally efficient for circuits with mostly linear components but struggles with the strong nonlinearities present in detailed semiconductor models. To handle these nonlinearities, SPICE uses Modified Nodal Analysis (MNA), a formulation that produces differential algebraic equations (DAEs).&lt;/p&gt;
    &lt;p&gt;MNA constructs the circuit equations by applying Kirchhoff’s current law at each node and substituting component branch equations. Energy storage elements introduce differential equations, while the network topology and sources introduce algebraic constraints. The result is a coupled system where nodal voltages, source currents, and energy storage currents must satisfy both differential and algebraic equations simultaneously. This integrated treatment of constraints and dynamics is what makes MNA particularly robust for nonlinear semiconductor models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mixed-Formulation Solver&lt;/head&gt;
    &lt;p&gt;PLECS Spice employs third-order implicit Runge-Kutta methods augmented with circuit-tailored convergence helpers to solve the DAEs produced by MNA. These one-step methods have a crucial advantage for mixed-signal schematics that contain both SPICE and standard PLECS electrical circuits: they are inherently self-starting. In other words, they do not rely on information from previous time steps. When events such as topology changes or zero-crossings occur, the solver must compute the next time step using only the current state. This self-starting property makes one-step methods particularly well-suited for hybrid systems with frequent discontinuities.&lt;/p&gt;
    &lt;p&gt;The solver can simulate complex systems that combine standard PLECS and SPICE models in a single schematic. The only rule is that when an electrical circuit contains a netlist, it must be solved using MNA, and therefore all its components must be compatible with SPICE. But other electrical circuits can remain in the standard PLECS formulation. Circuits of different types connect through the control domain using sources and meters. This enables a powerful top-down workflow: engineers can refine specific circuits of interest by converting them to SPICE netlists while keeping other subsystems and controls unchanged in standard PLECS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Application Example&lt;/head&gt;
    &lt;p&gt;Mixed-signal simulation is particularly valuable when control strategies and device physics must be considered together. The soft switching operation of a Dual Active Bridge (DAB) converter, whose analysis requires taking into consideration both controls and circuit design aspects, serves as a perfect case study for the workflow enabled by PLECS Spice.&lt;/p&gt;
    &lt;p&gt;A DAB is a bidirectional DC-DC topology comprising identical primary and secondary bridges (typically full bridges) separated by a high-frequency transformer and an energy transfer inductance (representing leakage plus external inductance). It is widely employed in high-power, high-density applications requiring bidirectional power flow between two galvanically isolated sides, such as EV chargers and energy storage systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Soft Switching Challenge&lt;/head&gt;
    &lt;p&gt;Magnetic components are often the primary limitation to increasing power density. Their size can be reduced by increasing switching frequency. State-of-the-art designs have reached the hundreds of kHz range. However, at these frequencies, switching losses represent a significant part of the overall converter losses. Without careful design, the volume advantage of a smaller transformer could be negated by the increased size needed of the cooling system.&lt;/p&gt;
    &lt;p&gt;To resolve this dilemma, soft switching offers a compelling solution. Given the high switching frequencies, MOSFETs are the standard choice for modern DABs. However, their dominant loss mechanism stems from the charge stored in the parasitic output capacitance (). When the device blocks voltage, this capacitance stores the energy&lt;/p&gt;
    &lt;p&gt;which depends on the drain-source voltage. When a MOSFET is turned on, the stored charge must be evacuated. In hard switching, the closing channel effectively shorts the capacitance, dissipating the stored energy as heat within the semiconductor. At high frequencies, this thermal penalty becomes unsustainable.&lt;/p&gt;
    &lt;p&gt;Here, the DAB offers a distinct advantage. In a full-bridge topology, each leg contains a top and bottom switch that operate complementarily: when one conducts, the other blocks. In practice, a short interval called dead time is introduced between turning off one switch and turning on its complement. Its primary role is to prevent a short circuit across the DC link, but a DAB can also exploit this interval of time for soft switching. During dead time, the inductor current continues to flow. With both switches off, the only path available is through the parasitic capacitances. This discharges the output capacitance of the incoming MOSFET (the switch about to turn on), causing its drain-source voltage to fall. If the dead time is sufficient, reaches zero before the gate signal arrives. The soft switching challenge lies in properly tuning this interval.&lt;/p&gt;
    &lt;p&gt;To achieve such Zero Voltage Switching (ZVS), the relationship between the device output capacitance, the resonant path and the gate drive, must be carefully adjusted. Crucially, these hardware choices cannot be made in isolation. They must inform the control design. This is because robust ZVS depends on several dynamic factors: the converter’s operating point (voltage and power), the gate driving scheme (specifically the dead times) and the degrees of freedom utilized by the modulation strategy.&lt;/p&gt;
    &lt;p&gt;Standard PLECS simulations allow for precise tuning of the operating point within the ZVS region, represented by the blue area in the ZVS range figure. However, because ideal switches are inherently hard switching, they do not simulate the transients required for a detailed analysis of ZVS. The effects of using two different dead times are compared in the figures above. In both tests, the low side gate signal turns off a conducting MOSFET. After the dead time, the complementary MOSFET is turned on by the high side gate signal. In the first experiment, a dead time of 15 ns is used between the two events. In the second, it is set to 50 ns. Yet, the resulting voltage and current waveforms show no visible response to this parameter change.&lt;/p&gt;
    &lt;p&gt;In this case, the ideal model fails to indicate whether the timing achieves soft switching or leads to hard switching transients. The reason is that ideal switches lack parasitic capacitances. Without , the antiparallel diode of the complementary switch conducts immediately, making the simulated waveforms insensitive to the dead time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validating ZVS with Device-Level Detail&lt;/head&gt;
    &lt;p&gt;PLECS Spice enables precisely the analysis that ideal models cannot provide. Using configurable subsystems, engineers can add a detailed SPICE configuration alongside the ideal model, allowing them to toggle between fast system-level analysis and high-fidelity device validation without modifying the circuit topology or control logic.&lt;/p&gt;
    &lt;p&gt;The ideal switch configuration shown in the first figure consists of a standard PLECS MOSFET with antiparallel diode, driven directly by a control signal. The detailed configuration in the second figure provides a device-level description. It uses manufacturer-provided MOSFET and diode netlists that capture parasitic capacitances and charge dynamics. The control signal is converted to a gate-source voltage through a controlled voltage source. Separate on and off gate resistances ( and ) control switching speed. Engineers can switch between one configuration to the other between two simulations, while the control logic, operating point and all other subsystems remain unchanged.&lt;/p&gt;
    &lt;p&gt;With detailed device models in place, the impact of dead time on ZVS becomes immediately visible. The figure below shows the switching transient with a 15 ns dead time. The drain-source voltage remains high when the gate signal is applied, and only begins falling as channel current rises. This overlap between voltage and current is the signature of hard switching: the channel conducts before the output capacitance fully discharges, dissipating the stored energy as heat in the semiconductor. The insufficient dead time prevents the resonant discharge mechanism from completing.&lt;/p&gt;
    &lt;p&gt;By contrast, the second figure demonstrates successful ZVS with a 50 ns dead time. Here, completes its resonant transition to zero before the gate signal arrives. The channel opens with zero voltage across it, eliminating capacitive turn-on losses. Channel current then begins to flow, carrying the inductor current through the device. This extended dead time provides a sufficient interval for the inductor current to transfer energy from the MOSFET’s output capacitance, fulfilling the conditions for soft switching.&lt;/p&gt;
    &lt;p&gt;This example demonstrates how PLECS Spice enables validation of ZVS by bringing together three tightly coupled aspects within a single model. The control strategy establishes the operating point and determines the available inductor current for resonant transitions. Gate drive timing sets the window for capacitor discharge. The device physics, captured in the SPICE netlist, determines how quickly that discharge occurs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;PLECS Spice marks a significant step towards a unified power electronics design workflow. By integrating SPICE simulation directly into the PLECS environment, engineers no longer need to choose between system-level insights and device-level accuracy. The ability to seamlessly transition between ideal and detailed models within a single schematic eliminates the redundant and error-prone process of rebuilding circuits in separate tools. This empowers engineers to adopt a true top-down design philosophy, starting with a system-level view and progressively adding detail where it matters most. As power electronic systems grow in complexity, this unified approach will be crucial for accelerating innovation and reducing time-to-market.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752841</guid><pubDate>Sun, 25 Jan 2026 10:44:08 +0000</pubDate></item><item><title>150k lines of vibe coded Elixir: The Good, the Bad and the Ugly</title><link>https://getboothiq.com/blog/150k-lines-vibe-coded-elixir-good-bad-ugly</link><description>&lt;doc fingerprint="f72c54daabbd7c64"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly&lt;/head&gt;
    &lt;p&gt;TL;DR:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Good: AI is great at Elixir. It gets better as your codebase grows.&lt;/item&gt;
      &lt;item&gt;Bad: It defaults to defensive, imperative code. You need to be strict about what good Elixir looks like.&lt;/item&gt;
      &lt;item&gt;Ugly: It can’t debug concurrent test failures. It doesn’t understand that each test runs in an isolated transaction, or that processes have independent lifecycles. It spirals until you step in.&lt;/item&gt;
      &lt;item&gt;Bottom Line: Even with the drawbacks, the productivity gains are off the charts. I expect it will only get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BoothIQ is a universal badge scanner for trade shows. AI writes 100% of our code. We have 150,000 lines of vibe coded Elixir running in production. Here’s what worked and what didn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Good&lt;/head&gt;
    &lt;head rend="h3"&gt;Elixir is Small: It Gets It Right the First Time&lt;/head&gt;
    &lt;p&gt;Elixir is a small language. Few operators. Small standard library. Only so many ways to control flow. It hasn’t been around for decades. It hasn’t piled up paradigms like .NET or Java, where functional and OOP fight for space.&lt;/p&gt;
    &lt;p&gt;This matters. AI is bad at decisions. If you want your agent to succeed, have it make fewer decisions. With Elixir, Claude doesn’t need to pick between OOP and functional. It doesn’t need to navigate old syntax next to new patterns. There’s one way to skin the cat. Claude finds it.&lt;/p&gt;
    &lt;p&gt;This matters more if you’re adding AI to an existing codebase. In languages where paradigms came and went—often with whatever developer pushed them—Claude tries to match the existing code. The existing code is inconsistent. So Claude is inconsistent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Elixir is Terse: Longer Sessions, Fewer Compactions&lt;/head&gt;
    &lt;p&gt;Small and terse are related but different. Small means few concepts. Terse means fewer tokens to express the same thing. Go is small but not terse—few concepts, but verbose syntax and explicit error handling everywhere. Elixir is both. We got lucky.&lt;/p&gt;
    &lt;p&gt;Context windows are a real constraint. Elixir uses fewer tokens than most languages. No braces. No semicolons. No verbose boilerplate. I can stay in a working session longer. More iterations. Fewer compactions—those moments when the AI summarizes and forgets earlier context. More context in memory.&lt;/p&gt;
    &lt;p&gt;When I built the React Native version of our app, I hit compactions constantly. JavaScript is small-ish, but it’s not terse. It burns tokens to do what Elixir does with fewer.&lt;/p&gt;
    &lt;p&gt;I also see more compactions when working on heavy HTML and Tailwind in LiveView. Adding, updating, or editing large sections of markup at once. HTML and HEEx templates are token-heavy. But even then, it’s less painful than JavaScript-heavy work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tidewave: Longer Unassisted Runs&lt;/head&gt;
    &lt;p&gt;Tidewave supercharges Elixir-specific context. It lets the agent read logs from the running app—debug, info, error, warning—so you don’t copy/paste logs around. It can query the dev database, see Ecto schemas, and view package documentation. Fewer hallucinations. Longer unassisted runs. The agent can check and validate its own assumptions without human intervention.&lt;/p&gt;
    &lt;head rend="h3"&gt;Immutability: Fewer Decisions, Less Code&lt;/head&gt;
    &lt;p&gt;If a variable gets mutated by a function call, AI now has three problems instead of one. The actual feature you want implemented. Whether to work around the mutation or update other call sites to stop mutating. And the mutated data itself—what is it, what was it, what will it be, what can it be?&lt;/p&gt;
    &lt;p&gt;AI ponders all of this and contorts itself into an overly defensive mess. It writes nonsense validation checks and if-statements on mutated data. Defensive code that wouldn’t exist in an immutable language.&lt;/p&gt;
    &lt;p&gt;In Elixir, the data is what it is. It’s not going to change. Fewer decisions. Less code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frontend: Higher Quality, Less Time&lt;/head&gt;
    &lt;p&gt;I prompt high-level changes—“give the top section more padding”—and Claude does it faster than I could. It’s especially good at modifying or moving large chunks of page structure. Mobile-first views? Easy. Way faster than me, and it’s a better designer than me too.&lt;/p&gt;
    &lt;p&gt;The quality floor has gone way up. You can’t hide behind “I’m not a designer” anymore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Worktrees: Build Multiple Features in Parallel&lt;/head&gt;
    &lt;p&gt;I use three git worktrees, so I can work on up to three features at any given time. Typically a main feature, a slightly less important one, and the third reserved for quick fixes, low priority stuff, or quick experiments.&lt;/p&gt;
    &lt;p&gt;Three is about the limit. Any more and context switching between features becomes the bottleneck.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bad&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Can’t Organize: Architecture Is Still On You&lt;/head&gt;
    &lt;p&gt;AI is exceptional at churning out lines of code. It’s significantly less exceptional at deciding where those lines should go. It defaults to creating new files everywhere. It repeats code it’s already written. It introduces inconsistencies.&lt;/p&gt;
    &lt;p&gt;This is the “mess” people describe in vibe code projects as they grow. You still need a human making structural decisions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trained on Imperative: It Writes Defensive Code&lt;/head&gt;
    &lt;p&gt; AI trained mostly on imperative code. Ruby, Python, JavaScript, C#. Elixir looks like Ruby. So Claude writes Ruby-style Elixir—&lt;code&gt;if/then/else&lt;/code&gt; chains, defensive nil-checking, early returns that don’t make sense in a functional context.&lt;/p&gt;
    &lt;p&gt;Elixir wants you to be assertive. Pattern match on what you expect. Let it crash if something’s wrong. The process restarts in a good state. This is foreign to most code Claude trained on.&lt;/p&gt;
    &lt;p&gt;This gets better as the codebase grows. Claude sees more assertive patterns. It starts to infer the style. But it still defaults to defensive. I still correct it regularly. Be strict about what good Elixir looks like.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Operations: Keep It Out of Context&lt;/head&gt;
    &lt;p&gt;Every git operation takes context window space. Checking status. Writing commit messages. Describing PRs. That space could go to actual work. Git context goes stale fast—a commit message from 20 minutes ago is worthless after three more changes.&lt;/p&gt;
    &lt;p&gt;When I’m babysitting a feature, I commit manually. Every point I’m happy with. It’s fast. It’s cheap version control. It doesn’t burn context.&lt;/p&gt;
    &lt;p&gt;Claude Code has “checkpoints” now. Internal version control that protects vibe coders without explicit commits. That’s better than AI managing git directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Ugly&lt;/head&gt;
    &lt;head rend="h3"&gt;OTP and Async: It Chases Ghosts&lt;/head&gt;
    &lt;p&gt;Claude is useless for debugging OTP, Task, or async issues. It doesn’t understand how processes, the actor model, and GenServers work together. When it tries to introspect the running system, it feeds itself bad data. It gets very lost.&lt;/p&gt;
    &lt;p&gt;It can course correct when you point out where it went wrong. But on its own, it chases ghosts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecto Sandbox: It Chases Red Herrings&lt;/head&gt;
    &lt;p&gt;In Elixir tests, each test runs in a database transaction that rolls back at the end. Tests run async without hitting each other. No test data persists.&lt;/p&gt;
    &lt;p&gt;Claude doesn’t understand this. It uses Tidewave’s dev DB connection and thinks it’s looking at the test DB—which is always empty. A test fails. Claude queries the database. Finds nothing. Thinks there’s a data problem.&lt;/p&gt;
    &lt;p&gt;I’ve watched Claude try to seed the test database so a test will pass. That’s clearly wrong.&lt;/p&gt;
    &lt;p&gt;Other times, two tests insert or query the same schema. Claude doesn’t understand transaction isolation—tests can’t see each other’s data. It confuses itself and recommends disabling async tests altogether. Manageable once you watch for it. But ugly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bottom Line&lt;/head&gt;
    &lt;p&gt;AI writing all the code has been a massive win. The friction exists, but it’s manageable and doesn’t interfere much with day-to-day work. By far the most important thing: have a consistent, coherent codebase architecture. Without it, you’ll quickly end up with spaghetti code.&lt;/p&gt;
    &lt;p&gt;The goal for this year: automate myself out of a job. That means giving Claude more control over the entire software development lifecycle—from a simple problem statement to a fully tested, working PR that only needs a quick glance before it’s merged and deployed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752907</guid><pubDate>Sun, 25 Jan 2026 10:54:29 +0000</pubDate></item><item><title>Show HN: TUI for managing XDG default applications</title><link>https://github.com/mitjafelicijan/xdgctl</link><description>&lt;doc fingerprint="fe9cba7b7ba6b9a3"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;xdgctl&lt;/code&gt; is a TUI for managing XDG default applications. View and set defaults for file categories without using &lt;code&gt;xdg-mime&lt;/code&gt; directly.&lt;/p&gt;
    &lt;p&gt;Built with C using GLib/GIO and termbox2.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;xdgctl.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse by category (Browsers, Text Editors, etc.)&lt;/item&gt;
      &lt;item&gt;Current default marked with &lt;code&gt;*&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Up/Down&lt;/cell&gt;
        &lt;cell&gt;Navigate through categories or applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Right/Tab&lt;/cell&gt;
        &lt;cell&gt;Switch from category list to application list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Left&lt;/cell&gt;
        &lt;cell&gt;Switch back to category list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Enter&lt;/cell&gt;
        &lt;cell&gt;Set selected application as default for current category&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Esc / q&lt;/cell&gt;
        &lt;cell&gt;Quit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To build &lt;code&gt;xdgctl&lt;/code&gt;, you need the following development libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;glib-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-unix-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clang&lt;/code&gt;or&lt;code&gt;gcc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# On Void Linux
sudo xbps-install glibc-devel&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/mitjafelicijan/xdgctl.git
cd xdgctl

# Build
make
sudo make install

# Using prefix
sudo make PREFIX=/usr/local install
make PREFIX=~/.local install&lt;/code&gt;
    &lt;p&gt;If you manually add new applications to your &lt;code&gt;~/.local/share/applications&lt;/code&gt; directory, you might need to run &lt;code&gt;update-desktop-database&lt;/code&gt; again.&lt;/p&gt;
    &lt;code&gt;ls /usr/share/applications
ls ~/.local/share/applications&lt;/code&gt;
    &lt;code&gt;xdg-mime query default text/plain
xdg-mime query default text/html
xdg-mime query default x-scheme-handler/http
xdg-mime query default x-scheme-handler/https
xdg-mime query default inode/directory&lt;/code&gt;
    &lt;code&gt;xdg-mime default brave.desktop x-scheme-handler/http
xdg-mime default brave.desktop x-scheme-handler/https&lt;/code&gt;
    &lt;code&gt;# ~/.local/share/applications/brave.desktop
[Desktop Entry]
Exec=/home/m/Applications/brave
Type=Application
Categories=Applications
Name=Brave Browser
MimeType=text/html;text/xml;application/xhtml+xml;x-scheme-handler/http;x-scheme-handler/https;&lt;/code&gt;
    &lt;code&gt;update-desktop-database ~/.local/share/applications
less ~/.config/mimeapps.list
less /usr/share/applications/mimeapps.list&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753078</guid><pubDate>Sun, 25 Jan 2026 11:19:04 +0000</pubDate></item><item><title>Show HN: Bonsplit – Tabs and splits for native macOS apps</title><link>https://bonsplit.alasdairmonk.com</link><description>&lt;doc fingerprint="8d0973528a96f1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsplit is a custom tab bar and layout split library for macOS apps. Enjoy out of the box 120fps animations, drag-and-drop reordering, SwiftUI support &amp;amp; keyboard navigation.&lt;/p&gt;
    &lt;quote&gt;.package(url: "https://github.com/almonk/bonsplit.git", from: "1.0.0")&lt;/quote&gt;
    &lt;p&gt;### Features&lt;/p&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;### Read this, agents...&lt;/p&gt;
    &lt;p&gt;Complete reference for all Bonsplit classes, methods, and configuration options.&lt;/p&gt;
    &lt;p&gt;The main controller for managing tabs and panes. Create an instance and pass it to BonsplitView.&lt;/p&gt;
    &lt;p&gt;Implement this protocol to receive callbacks about tab bar events. All methods have default implementations and are optional.&lt;/p&gt;
    &lt;p&gt;Configure behavior and appearance. Pass to BonsplitController on initialization.&lt;/p&gt;
    &lt;code&gt;allowSplits&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable split buttons and drag-to-split&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseTabs&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show close buttons on tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseLastPane&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Allow closing the last remaining pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;false&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowTabReordering&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable drag-to-reorder tabs within a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCrossPaneTabMove&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable moving tabs between panes via drag&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;autoCloseEmptyPanes&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Automatically close panes when their last tab is closed&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;contentViewLifecycle&lt;/code&gt;
    &lt;code&gt;ContentViewLifecycle&lt;/code&gt;
    &lt;p&gt;How tab content views are managed when switching tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.recreateOnSwitch&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;newTabPosition&lt;/code&gt;
    &lt;code&gt;NewTabPosition&lt;/code&gt;
    &lt;p&gt;Where new tabs are inserted in the tab list&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.current&lt;/code&gt;&lt;/p&gt;
    &lt;quote&gt;let config = BonsplitConfiguration(allowSplits: true,allowCloseTabs: true,allowCloseLastPane: false,autoCloseEmptyPanes: true,contentViewLifecycle: .keepAllAlive,newTabPosition: .current)let controller = BonsplitController(configuration: config)&lt;/quote&gt;
    &lt;p&gt;Controls how tab content views are managed when switching between tabs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell role="head"&gt;State&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;.recreateOnSwitch&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;Simple content&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.keepAllAlive&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;Full&lt;/cell&gt;
        &lt;cell&gt;Complex views, forms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Controls where new tabs are inserted in the tab list.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.current&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Insert after currently focused tab, or at end if none&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.end&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always insert at the end of the tab list&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;tabBarHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Height of the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;33&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMinWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;140&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMaxWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Maximum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;220&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabSpacing&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Spacing between tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum height of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;showSplitButtons&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show split buttons in the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;animationDuration&lt;/code&gt;
    &lt;code&gt;Double&lt;/code&gt;
    &lt;p&gt;Duration of animations in seconds&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0.15&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;enableAnimations&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable or disable all animations&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;.default&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Default configuration with all features enabled&lt;/p&gt;
    &lt;code&gt;.singlePane&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Single pane mode with splits disabled&lt;/p&gt;
    &lt;code&gt;.readOnly&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Read-only mode with all modifications disabled&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753301</guid><pubDate>Sun, 25 Jan 2026 11:56:42 +0000</pubDate></item><item><title>Nango (YC W23, Dev Infrastructure) Is Hiring Remotely</title><link>https://jobs.ashbyhq.com/Nango</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753336</guid><pubDate>Sun, 25 Jan 2026 12:02:01 +0000</pubDate></item><item><title>Doom has been ported to an earbud</title><link>https://doombuds.com</link><description>&lt;doc fingerprint="a6567235022113ff"&gt;
  &lt;main&gt;
    &lt;p&gt;It's almost your turn, get ready!&lt;/p&gt;
    &lt;p&gt;Player queue&lt;/p&gt;
    &lt;p&gt;Your position&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Players queued&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Wait time&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;You know the 1993 classic DOOM? I made it run on an earbud, then I connected it to the internet and made it possible for visitors like you to sit in a queue for hours play the game remotely!.&lt;/p&gt;
    &lt;p&gt;Yeah but it won't just run on any old earbud, this only works with the Pinebuds Pro, the only earbuds with open source firmware.&lt;/p&gt;
    &lt;p&gt;You sure can! There are two relevant repos:&lt;/p&gt;
    &lt;p&gt;This was a necessary optimisation to avoid paying outgoing bandwidth fees, once you're 5th in the queue, the twitch player will switch to a low-latency MJPEG stream.&lt;/p&gt;
    &lt;p&gt;shhhh don't look don't look it's ok just join the queue&lt;/p&gt;
    &lt;p&gt;Let's switch to a more readable font first.&lt;/p&gt;
    &lt;p&gt; I'll put out an article / video diving deeper into this later, but here are a few bits of info:&lt;lb/&gt; This project is made up of four parts: &lt;/p&gt;
    &lt;p&gt;The firmware pushes up against a few hardware limitations:&lt;/p&gt;
    &lt;p&gt; Earbuds don't have displays, so the only way to transfer data to/from them is either via bluetooth, or the UART contact pads.&lt;lb/&gt; Bluetooth is pretty slow, you'd be lucky to get a consistent 1mbps connection, UART is easily the better option.&lt;lb/&gt; DOOM's framebuffer is (width * height) bytes, 320 * 200 = 96kB. (doom's internal framebuffer is 8-bit not 24-bit)&lt;lb/&gt; The UART connection provides us with 2.4mbps of usable bandwidth. 2,400,000 / 8 / 96,000 gives us... 3 frames per second.&lt;lb/&gt; Clearly we need to compress the video stream. Modern video codecs like h264 consume way too much CPU and RAM.&lt;lb/&gt; The only feasible approach is sending the video as an MJPEG stream. MJPEG is a stream of JPEG images shown one after the other.&lt;lb/&gt; I found an excellent JPEG encoder for embedded devices here, thanks Larry!&lt;lb/&gt; A conservative estimate for the average HIGH quality JPEG frame is around 13.5KB, but most scenes (without enemies) are around 11kb.&lt;lb/&gt; Theoretical maximum FPS:&lt;lb/&gt; - Optimistic: `2,400,000 / (11,000 * 8)` = 27.3 FPS&lt;lb/&gt; - Conservative: `2,400,000 / (13,500 * 8)` = 22.2 FPS &lt;/p&gt;
    &lt;p&gt; The stock open source firmware has the CPU set to 100mhz, so I cranked that up to 300mhz and disabled low power mode.&lt;lb/&gt; The Cortex-M4F running at 300mhz is actually more than enough for DOOM, however it struggles with JPEG encoding.&lt;lb/&gt; This is why it maxes out at ~18fps, I don't think there's much else I can do to speed it up. &lt;/p&gt;
    &lt;p&gt; By default, we only have access to 768KB of RAM, after disabling the co-processor it gets bumped up to the advertised 992KB.&lt;lb/&gt; DOOM requires 4MB of RAM, though there are plenty of optimisations that can reduce this amount.&lt;lb/&gt; Pre-generating lookup tables, making variables const, reading const variables from flash, disabling DOOM's caching system, removing unneeded variables. It all adds up! &lt;/p&gt;
    &lt;p&gt; The shareware DOOM 1 wad (assets file) is 4.2MB and the earbuds can only store 4MB of data.&lt;lb/&gt; Thankfully, fragglet, a well-known doom modder, has already solved this issue for me.&lt;lb/&gt; Squashware is his trimmed-down DOOM 1 wad that is only 1.7MB in size.&lt;lb/&gt; With this wad file, everything comfortably fits in flash. &lt;/p&gt;
    &lt;p&gt;I thought you'd never ask! (please hire me)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753484</guid><pubDate>Sun, 25 Jan 2026 12:22:12 +0000</pubDate></item><item><title>Alarm overload is undermining safety at sea as crews face thousands of alerts</title><link>https://www.lr.org/en/knowledge/press-room/press-listing/press-release/2026/alarm-overload-is-undermining-safety-at-sea-as-new-research-shows-crews-face-tens-of-thousands-of-daily-alerts/</link><description>&lt;doc fingerprint="3452211c23d1e7ad"&gt;
  &lt;main&gt;
    &lt;p&gt;New research from Lloyd’s Register (LR) has revealed that excessive and nuisance shipboard alarm systems are routinely overwhelming crews and, in many cases, actively undermining safety at sea.&lt;/p&gt;
    &lt;p&gt;The findings, published today in Effective Alarm Management in the Maritime Industry are based on data collected from 11 operational vessels, spanning over 2,000 days and more than 40 million alarm-related events.&lt;/p&gt;
    &lt;p&gt;The study shows that many ships generate thousands of alarms every day, many of which provide little or no operational value. The result is widespread alarm fatigue, disrupted rest periods and a growing erosion of trust in systems that are intended to protect both crews and assets.&lt;/p&gt;
    &lt;p&gt;The research applied recognised industrial best practice, including IEC 62682 and EEMUA 191, to maritime operations for the first time at this scale. It found that fewer than half of the vessels studied met the recommended benchmark of fewer than 30 alarms per hour, while on ships with unattended machinery spaces alarms disrupted 63% of rest periods. In some cases, cruise ships experienced up to 2,600 alarms per day, with peak rates reaching 4,691 alarms in just ten minutes.&lt;/p&gt;
    &lt;p&gt;Crews, overwhelmed by the volume of alerts, are forced to silence alarms without acknowledgement or physically bypass alarm circuits, normalising unsafe practices and eroding trust in critical safety systems.&lt;/p&gt;
    &lt;p&gt;Effective Alarm Management in the Maritime Industry: Insights from 40 million vessel alarms builds on LR’s Effective Alarm Management in the Maritime Industry report (released in September 2024) by moving beyond diagnosis to demonstrate what can be achieved in practice. A pilot project on an operational cruise ship reduced total alarm numbers by almost 50 per cent over a six-month period, without new technology or major system redesign. Improvements were delivered through traditional marine engineering interventions, including correcting valve installations, replacing faulty sensors and tuning existing systems.&lt;/p&gt;
    &lt;p&gt;LR’s analysis also demonstrates that addressing the 10 most frequent alarms could reduce overall loads by nearly 40 per cent.&lt;/p&gt;
    &lt;p&gt;The report calls for greater adoption of objective alarm performance assessment, stronger consideration of human factors in system design and operation throughout the vessel lifecycle, and regulatory frameworks that support consistent, enforceable standards.&lt;/p&gt;
    &lt;p&gt;Duncan Duffy, LR’s Global Head of Technology, said: “Our research found that alarm systems, when poorly managed, have themselves become a safety risk. Without decisive industry action, alarm fatigue will continue to undermine situational awareness and increase the likelihood of serious incidents.&lt;/p&gt;
    &lt;p&gt;“If the maritime industry is serious about safety, it must commit to continuous performance measurement, objective evaluation, and a human-centred approach to alarm system design. Only then can alarm systems fulfil their intended purpose—supporting crews, safeguarding lives, and ensuring safer voyages for all.”&lt;/p&gt;
    &lt;p&gt;The research is part of LR’s Digital Transformation Research programme, specifically designed to provide in-depth analysis of key opportunities and challenges for maritime digitalisation.&lt;/p&gt;
    &lt;p&gt;For more information and to download the full report, visit the link below:&lt;lb/&gt;LR Alarm Management &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753611</guid><pubDate>Sun, 25 Jan 2026 12:40:00 +0000</pubDate></item><item><title>Web-based image editor modeled after Deluxe Paint</title><link>https://github.com/steffest/DPaint-js</link><description>&lt;doc fingerprint="56223e185d452d87"&gt;
  &lt;main&gt;
    &lt;p&gt;Webbased image editor modeled after the legendary Deluxe Paint with a focus on retro Amiga file formats. Next to modern image formats, DPaint.js can read and write Amiga icon files and IFF ILBM images.&lt;/p&gt;
    &lt;p&gt;Online version available at https://www.stef.be/dpaint/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully Featured image editor with a.o. &lt;list rend="ul"&gt;&lt;item&gt;Layers&lt;/item&gt;&lt;item&gt;Selections&lt;/item&gt;&lt;item&gt;Masking&lt;/item&gt;&lt;item&gt;Transformation tools&lt;/item&gt;&lt;item&gt;Effects and filters&lt;/item&gt;&lt;item&gt;Multiple undo/redo&lt;/item&gt;&lt;item&gt;Copy/Paste from any other image program or image source&lt;/item&gt;&lt;item&gt;Customizable dither tools&lt;/item&gt;&lt;item&gt;Color Cycling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Heavy focus on colour reduction with fine-grained dithering options&lt;/item&gt;
      &lt;item&gt;Amiga focus &lt;list rend="ul"&gt;&lt;item&gt;Read/write/convert Amiga icon files (all formats)&lt;/item&gt;&lt;item&gt;Reads IFF ILBM images (all formats including HAM and 24-bit)&lt;/item&gt;&lt;item&gt;Writes IFF ILBM images (up to 256 colors)&lt;/item&gt;&lt;item&gt;Read and write directly from Amiga Disk Files (ADF)&lt;/item&gt;&lt;item&gt;Embedded Amiga Emulator to preview your work in the real Deluxe Paint.&lt;/item&gt;&lt;item&gt;Limit the palette to 12 bit for Amiga OCS/ECS mode, or 9 bit for Atari ST mode.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Deluxe Paint Legacy &lt;list rend="ul"&gt;&lt;item&gt;Supports PBM files as used by the PC version of Deluxe Paint (Thanks to Michael Smith)&lt;/item&gt;&lt;item&gt;Supports Deluxe Paint Atari ST compression modes (Thanks to Nicolas Ramz)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It runs in your browser, works on any system and works fine on touch-screen devices like iPads.&lt;lb/&gt; It is written in 100% plain JavaScript and has no dependencies.&lt;lb/&gt; It's 100% free, no ads, no tracking, no accounts, no nothing.&lt;lb/&gt; All processing is done in your browser, no data is sent to any server.&lt;/p&gt;
    &lt;p&gt;The only part that is not included in this repository is the Amiga Emulator Files. (The emulator is based on the Scripted Amiga Emulator)&lt;/p&gt;
    &lt;p&gt;DPaint.js doesn't need building.&lt;lb/&gt; It also has zero dependencies so there's no need to install anything.&lt;lb/&gt; DPaint.js is written using ES6 modules and runs out of the box in modern browsers.&lt;lb/&gt; Just serve "index.html" from a webserver and you're good to go.&lt;/p&gt;
    &lt;p&gt;There's an optional build step to create a compact version of DPaint.js if you like.&lt;lb/&gt; I'm using Parcel.js for this.&lt;lb/&gt; For convenience, I've included a "package.json" file.&lt;lb/&gt; open a terminal and run &lt;code&gt;npm install&lt;/code&gt; to install Parcel.js and its dependencies.
Then run &lt;code&gt;npm run build&lt;/code&gt; to create a compact version of DPaint.js in the "dist" folder.&lt;/p&gt;
    &lt;p&gt;Documentation can be found at https://www.stef.be/dpaint/docs/&lt;/p&gt;
    &lt;p&gt;Dpaint.js is a web application, not an app that you install on your computer. That being said: DPaint.js has no online dependencies and runs fine offline if you want. One caveat: you have to serve the index.html file from a webserver, not just open it in your browser.&lt;lb/&gt; A quick way to do this is - for example - using the Spark app.&lt;lb/&gt; Download the binary for your platform, drop the Spark executable in the folder where you downloaded the Dpaint.js source files and run it. If you then point your browser to http://localhost:8080/ it should work.&lt;/p&gt;
    &lt;p&gt;If you are using Chrome, you can also "install" dpaint.js as app.&lt;lb/&gt; It will then show up your Chrome apps and work offline.&lt;/p&gt;
    &lt;p&gt;Current version is still alpha.&lt;lb/&gt; I'm sure there are bugs and missing features.&lt;lb/&gt; Bug reports and pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;Planned for the next release, already in the works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Color Cycling&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Animation support (GIf and Amiga ANIM files)&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Shading/transparency tools that stay within the palette.&lt;/del&gt;(done)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned for a future release if there's a need for it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for non-square pixel modes such as HiRes and Interlaced&lt;/item&gt;
      &lt;item&gt;PSD import and export&lt;/item&gt;
      &lt;item&gt;SpriteSheet support&lt;/item&gt;
      &lt;item&gt;Write HAM,SHAM and Dynamic HiRes images&lt;/item&gt;
      &lt;item&gt;Commodore 64 graphics modes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please note that the Brave browser is using "farbling" that introduces random image noise in certain conditions. They claim this is to protect your privacy. Although I totally understand the sentiment, In my opinion a browser should not actively alter the content of a webpage or intentionally break functionality.&lt;lb/&gt; But hey, who am I to speak, it's a free world. Just be aware that if you are using Brave, you will run into issues, so please "lower your shields" for this app in Brave or use another browser.&lt;/p&gt;
    &lt;p&gt;Dpaint.js supports Color-Cycling - a long lost art of "animating" a static image by only rotating some colors in the palette. See an example here:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;The_Vision_cycle.mp4&lt;/head&gt;
    &lt;p&gt;Open the layered source file of the above image directly in Dpaint.js&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753708</guid><pubDate>Sun, 25 Jan 2026 12:54:53 +0000</pubDate></item><item><title>Wine-Staging 11.1 Adds Patches for Enabling Recent Photoshop Versions on Linux</title><link>https://www.phoronix.com/news/Wine-Staging-11.1</link><description>&lt;doc fingerprint="de54f53ba99c238b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wine-Staging 11.1 Adds Patches For Enabling Recent Adobe Photoshop Versions On Linux&lt;/head&gt;
    &lt;p&gt; Following yesterday's release of Wine 11.1 for kicking off the new post-11.0 development cycle, Wine-Staging 11.1 is now available for this experimental/testing version of Wine that present is around 254 patches over the upstream Wine state. &lt;lb/&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;lb/&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;lb/&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;lb/&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;lb/&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
    &lt;p&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;/p&gt;
    &lt;p&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;/p&gt;
    &lt;p&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;/p&gt;
    &lt;p&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;/p&gt;
    &lt;p&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754427</guid><pubDate>Sun, 25 Jan 2026 14:42:39 +0000</pubDate></item><item><title>Show HN: Netfence – Like Envoy for eBPF Filters</title><link>https://github.com/danthegoodman1/netfence</link><description>&lt;doc fingerprint="780d8d538f3988c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Like Envoy xDS, but for eBPF filters.&lt;/p&gt;
    &lt;p&gt;Netfence runs as a daemon on your VM/container hosts and automatically injects eBPF filter programs into cgroups and network interfaces, with a built-in DNS server that resolves allowed domains and populates the IP allowlist.&lt;/p&gt;
    &lt;p&gt;Netfence daemons connect to a central control plane that you implement via gRPC to synchronize allowlists/denylists with your backend.&lt;/p&gt;
    &lt;p&gt;Your control plane pushes network rules like &lt;code&gt;ALLOW *.pypi.org&lt;/code&gt; or &lt;code&gt;ALLOW 10.0.0.0/16&lt;/code&gt; to attached interfaces/cgroups. When a VM/container queries DNS, Netfence resolves it, adds the IPs to the eBPF filter, and drops traffic to unknown IPs before it leaves the host without any performance penalty.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Attach eBPF filters to network interfaces (TC) or cgroups&lt;/item&gt;
      &lt;item&gt;Policy modes: disabled, allowlist, denylist, block-all&lt;/item&gt;
      &lt;item&gt;IPv4 and IPv6 CIDR support with optional TTLs&lt;/item&gt;
      &lt;item&gt;Per-attachment DNS server with domain allowlist/denylist&lt;/item&gt;
      &lt;item&gt;Domain rules support subdomains with specificity-based matching (more specific rules win)&lt;/item&gt;
      &lt;item&gt;Resolved domains auto-populate IP filter&lt;/item&gt;
      &lt;item&gt;Metadata on daemons and attachments for associating with VM ID, tenant, etc.&lt;/item&gt;
      &lt;item&gt;Support for proxying DNS queries to the control plane to make DNS decisions per-attachment&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;+------------------+         +-------------------------+
|  Your Control    |&amp;lt;-------&amp;gt;|  Daemon (per host)      |
|  Plane (gRPC)    |  stream |                         |
+------------------+         |  +-------------------+  |
                             |  | DNS Server        |  |
                             |  | (per-attachment)  |  |
                             |  +-------------------+  |
                             +-------------------------+
                                        |
                                 +------+------+
                                 |             |
                              TC Filter    Cgroup Filter
                              (veth, eth)  (containers)
&lt;/code&gt;
    &lt;p&gt;Each attachment gets a unique DNS address (port) provisioned by the daemon. Containers/VMs should be configured to use their assigned DNS address.&lt;/p&gt;
    &lt;p&gt;Run the daemon, which:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exposes a local gRPC API (&lt;code&gt;DaemonService&lt;/code&gt;) for attaching/detaching filters&lt;/item&gt;
      &lt;item&gt;Connects to your control plane via bidirectional stream (&lt;code&gt;ControlPlane.Connect&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Loads and manages eBPF programs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Start the daemon:&lt;/p&gt;
    &lt;code&gt;# Start with default config
netfenced start

# Start with custom config file
netfenced start --config /etc/netfence/config.yaml&lt;/code&gt;
    &lt;p&gt;Check daemon status:&lt;/p&gt;
    &lt;code&gt;netfenced status&lt;/code&gt;
    &lt;p&gt;Your orchestration system calls the daemon's local API.&lt;/p&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Attach(interface_name: "veth123", metadata: {vm_id: "abc"})
// or
DaemonService.Attach(cgroup_path: "/sys/fs/cgroup/...", metadata: {container_id: "xyz"})
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;# Attach to a network interface (TC)
netfenced attach --interface veth123 --metadata vm_id=abc

# Attach to a cgroup
netfenced attach --cgroup /sys/fs/cgroup/... --metadata container_id=xyz

# Attach with metadata
netfenced attach --interface eth0 --metadata tenant=acme,env=prod&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daemon attaches eBPF filter to the target&lt;/item&gt;
      &lt;item&gt;Daemon sends &lt;code&gt;Subscribed{id, target, type, metadata}&lt;/code&gt;to control plane and waits for&lt;code&gt;SubscribedAck&lt;/code&gt;with initial config (mode, CIDRs, DNS rules)&lt;/item&gt;
      &lt;item&gt;If the control plane doesn't respond within the timeout (default 5s, configurable via &lt;code&gt;control_plane.subscribe_ack_timeout&lt;/code&gt;), the attachment is rolled back and the attach call fails&lt;/item&gt;
      &lt;item&gt;Daemon watches for target removal and sends &lt;code&gt;Unsubscribed&lt;/code&gt;automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Detach(id)
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;netfenced detach --id &amp;lt;attachment-id&amp;gt;&lt;/code&gt;
    &lt;p&gt;List attachments:&lt;/p&gt;
    &lt;code&gt;netfenced list
netfenced list --all  # fetch all pages&lt;/code&gt;
    &lt;p&gt;Implement &lt;code&gt;ControlPlane.Connect&lt;/code&gt; RPC - a bidirectional stream:&lt;/p&gt;
    &lt;p&gt;Receive from daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncRequest&lt;/code&gt;on connect/reconnect (lists current attachments)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Subscribed&lt;/code&gt;when new attachments are added&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Unsubscribed&lt;/code&gt;when attachments are removed&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Heartbeat&lt;/code&gt;with stats&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Send to daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncAck&lt;/code&gt;after receiving SyncRequest&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SubscribedAck{mode, cidrs, dns_config}&lt;/code&gt;after receiving Subscribed (required - daemon waits for this)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetMode{mode}&lt;/code&gt;- change IP filter policy mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowCIDR{cidr, ttl}&lt;/code&gt;/&lt;code&gt;DenyCIDR&lt;/code&gt;/&lt;code&gt;RemoveCIDR&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetDnsMode{mode}&lt;/code&gt;- change DNS filtering mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowDomain{domain}&lt;/code&gt;/&lt;code&gt;DenyDomain&lt;/code&gt;/&lt;code&gt;RemoveDomain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BulkUpdate{mode, cidrs, dns_config}&lt;/code&gt;- full state sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the daemon receives &lt;code&gt;Subscribed&lt;/code&gt;, it blocks waiting for &lt;code&gt;SubscribedAck&lt;/code&gt; before returning success to the caller. This ensures the attachment has its initial configuration before traffic flows. Use the metadata to identify which VM/tenant/container this attachment belongs to and respond with the appropriate initial rules.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754724</guid><pubDate>Sun, 25 Jan 2026 15:13:46 +0000</pubDate></item><item><title>A macOS app that blurs your screen when you slouch</title><link>https://github.com/tldev/posturr</link><description>&lt;doc fingerprint="709cd7c437d6538d"&gt;
  &lt;main&gt;
    &lt;p&gt;A macOS app that blurs your screen when you slouch.&lt;/p&gt;
    &lt;p&gt;Posturr uses your Mac's camera and Apple's Vision framework to monitor your posture in real-time. When it detects that you're slouching, it progressively blurs your screen to remind you to sit up straight. Maintain good posture, and the blur clears instantly.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time posture detection - Uses Apple's Vision framework for body pose and face tracking&lt;/item&gt;
      &lt;item&gt;Progressive screen blur - Gentle visual reminder that intensifies with worse posture&lt;/item&gt;
      &lt;item&gt;Menu bar controls - Easy access to settings, calibration, and status from the menu bar&lt;/item&gt;
      &lt;item&gt;Multi-display support - Works across all connected monitors&lt;/item&gt;
      &lt;item&gt;Privacy-focused - All processing happens locally on your Mac&lt;/item&gt;
      &lt;item&gt;Lightweight - Runs as a background app with minimal resource usage&lt;/item&gt;
      &lt;item&gt;No account required - No signup, no cloud, no tracking&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest &lt;code&gt;Posturr-vX.X.X.zip&lt;/code&gt;from the Releases page&lt;/item&gt;
      &lt;item&gt;Unzip the downloaded file&lt;/item&gt;
      &lt;item&gt;Drag &lt;code&gt;Posturr.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since Posturr is not signed with an Apple Developer certificate, macOS Gatekeeper will initially block it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Right-click (or Control-click) on &lt;code&gt;Posturr.app&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Select "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;Click "Open" in the dialog that appears&lt;/item&gt;
      &lt;item&gt;Grant camera access when prompted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only need to do this once. After the first launch, you can open Posturr normally.&lt;/p&gt;
    &lt;p&gt;Posturr requires camera access to monitor your posture. When you first launch the app, macOS will ask for permission. Click "OK" to grant access.&lt;/p&gt;
    &lt;p&gt;If you accidentally denied permission, you can grant it later:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open System Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Camera&lt;/item&gt;
      &lt;item&gt;Find Posturr and enable the toggle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once launched, Posturr appears in your menu bar with a person icon. The app continuously monitors your posture and applies screen blur when slouching is detected.&lt;/p&gt;
    &lt;p&gt;Click the menu bar icon to access:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Status - Shows current state (Monitoring, Slouching, Good Posture, etc.)&lt;/item&gt;
      &lt;item&gt;Enabled - Toggle posture monitoring on/off&lt;/item&gt;
      &lt;item&gt;Recalibrate - Reset your baseline posture (sit up straight, then click)&lt;/item&gt;
      &lt;item&gt;Sensitivity - Adjust how sensitive the slouch detection is (Low, Medium, High, Very High)&lt;/item&gt;
      &lt;item&gt;Dead Zone - Set the tolerance before blur kicks in (None, Small, Medium, Large)&lt;/item&gt;
      &lt;item&gt;Compatibility Mode - Use public macOS APIs for blur (try this if blur doesn't appear)&lt;/item&gt;
      &lt;item&gt;Quit - Exit the application (or press Escape anywhere)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position your camera at eye level when possible&lt;/item&gt;
      &lt;item&gt;Ensure adequate lighting on your face&lt;/item&gt;
      &lt;item&gt;Sit at a consistent distance from your screen&lt;/item&gt;
      &lt;item&gt;The app works best when your shoulders are visible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr uses Apple's Vision framework to detect body pose landmarks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Body Pose Detection: Tracks nose, shoulders, and their relative positions&lt;/item&gt;
      &lt;item&gt;Face Detection Fallback: When full body isn't visible, tracks face position&lt;/item&gt;
      &lt;item&gt;Posture Analysis: Measures the vertical distance between nose and shoulders&lt;/item&gt;
      &lt;item&gt;Blur Response: Applies screen blur proportional to posture deviation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The screen blur uses macOS's private CoreGraphics API by default for efficient, system-level blur. If the blur doesn't appear on your system, enable Compatibility Mode from the menu to use &lt;code&gt;NSVisualEffectView&lt;/code&gt; instead.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Xcode Command Line Tools (&lt;code&gt;xcode-select --install&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/yourusername/posturr.git
cd posturr
./build.sh&lt;/code&gt;
    &lt;p&gt;The built app will be in &lt;code&gt;build/Posturr.app&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Standard build
./build.sh

# Build with release archive (.zip)
./build.sh --release&lt;/code&gt;
    &lt;code&gt;swiftc -O \
    -framework AppKit \
    -framework AVFoundation \
    -framework Vision \
    -framework CoreImage \
    -o Posturr \
    main.swift&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No code signing: Requires manual Gatekeeper bypass on first launch&lt;/item&gt;
      &lt;item&gt;Camera dependency: Requires a working camera with adequate lighting&lt;/item&gt;
      &lt;item&gt;Detection accuracy: Works best with clear view of upper body/face&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr exposes a file-based command interface for external control:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;capture&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Take a photo and analyze pose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blur &amp;lt;0-64&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set blur level manually&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;quit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Write commands to &lt;code&gt;/tmp/posturr-command&lt;/code&gt;. Responses appear in &lt;code&gt;/tmp/posturr-response&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Camera (built-in or external)&lt;/item&gt;
      &lt;item&gt;Approximately 10MB disk space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr processes all video data locally on your Mac. No images or data are ever sent to external servers. The camera feed is used solely for posture detection and is never stored or transmitted.&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit issues and pull requests.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Apple's Vision framework for body pose detection&lt;/item&gt;
      &lt;item&gt;Uses private CoreGraphics API for blur, with NSVisualEffectView fallback&lt;/item&gt;
      &lt;item&gt;Inspired by the need for better posture during long coding sessions&lt;/item&gt;
      &lt;item&gt;Thanks to @wklm for the compatibility mode implementation&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754944</guid><pubDate>Sun, 25 Jan 2026 15:34:51 +0000</pubDate></item><item><title>Using PostgreSQL as a Dead Letter Queue for Event-Driven Systems</title><link>https://www.diljitpr.net/blog-post-postgresql-dlq</link><description>&lt;doc fingerprint="3f4c263785a28d80"&gt;
  &lt;main&gt;
    &lt;p&gt;While I was working on a project with Wayfair, I got the opportunity to work on a system that generated daily business reports aggregated from multiple data sources flowing through event streams across Wayfair. At a high level, Kafka consumers listened to these events, hydrated them with additional data by calling downstream services, and finally persisted the enriched events into a durable datastoreâCloudSQL PostgreSQL on GCP.&lt;/p&gt;
    &lt;p&gt;When everything was healthy, the pipeline worked exactly as expected. Events flowed in, got enriched, and were stored reliably. The real challenge started when things went wrong, which, in distributed systems, is not an exception but a certainty.&lt;/p&gt;
    &lt;p&gt;There were multiple failure scenarios we had to deal with. Sometimes the APIs we depended on for hydration were down or slow. Sometimes the consumer itself crashed midway through processing. In other cases, events arrived with missing or malformed fields that could not be processed safely. These were all situations outside our direct control, but they still needed to be handled gracefully.&lt;/p&gt;
    &lt;p&gt;This is where the concept of a Dead Letter Queue came into the picture. Whenever we knew an event could not be processed successfully, instead of dropping it or blocking the entire consumer, we redirected it to a DLQ so it could be inspected and potentially reprocessed later.&lt;/p&gt;
    &lt;p&gt;Our first instinct was to use Kafka itself as a DLQ. While this is a common pattern, it quickly became clear that it wasn't a great fit for our needs. Kafka is excellent for moving data, but once messages land in a DLQ topic, they are not particularly easy to inspect. Querying by failure reason, retrying a specific subset of events, or even answering simple questions like "what failed yesterday and why?" required extra tooling and custom consumers. For a system that powered business-critical daily reports, this lack of visibility was a serious drawback.&lt;/p&gt;
    &lt;p&gt;That's when we decided to treat PostgreSQL itself as the Dead Letter Queue.&lt;/p&gt;
    &lt;p&gt;Instead of publishing failed events to another Kafka topic, we persisted them directly into a DLQ table in PostgreSQL. We were already using CloudSQL as our durable store, so operationally this added very little complexity. Conceptually, it also made failures first-class citizens in the system rather than opaque messages lost in a stream.&lt;/p&gt;
    &lt;p&gt; Whenever an event failed processingâdue to an API failure, consumer crash, schema mismatch, or validation errorâwe stored the raw event payload along with contextual information about the failure. Each record carried a simple status field. When the event first landed in the DLQ, it was marked as &lt;code&gt;PENDING&lt;/code&gt;. Once it was successfully reprocessed, the status was updated to &lt;code&gt;SUCCEEDED&lt;/code&gt;. Keeping the state model intentionally minimal made it easy to reason about the lifecycle of a failed event.
                    &lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Table Schema and Indexing Strategy&lt;/head&gt;
    &lt;p&gt;To support inspection, retries, and long-term operability, the DLQ table was designed to be simple, query-friendly, and retry-aware.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table Schema&lt;/head&gt;
    &lt;code&gt;CREATE TABLE dlq_events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    error_reason TEXT NOT NULL,
    error_stacktrace TEXT,
    status VARCHAR(20) NOT NULL, -- PENDING / SUCCEEDED
    retry_count INT NOT NULL DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);&lt;/code&gt;
    &lt;head rend="h4"&gt;Key Design Considerations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;payload&lt;/code&gt;is stored as&lt;code&gt;JSONB&lt;/code&gt;to preserve the raw event without enforcing a rigid schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;status&lt;/code&gt;keeps the lifecycle simple and explicit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_after&lt;/code&gt;prevents aggressive retries when downstream systems are unstable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_count&lt;/code&gt;allows retry limits to be enforced without external state.&lt;/item&gt;
      &lt;item&gt;Timestamps make auditing and operational analysis straightforward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Indexes&lt;/head&gt;
    &lt;code&gt;CREATE INDEX idx_dlq_status
ON dlq_events (status);

CREATE INDEX idx_dlq_status_retry_after
ON dlq_events (status, retry_after);

CREATE INDEX idx_dlq_event_type
ON dlq_events (event_type);

CREATE INDEX idx_dlq_created_at
ON dlq_events (created_at);&lt;/code&gt;
    &lt;p&gt;These indexes allow the retry scheduler to efficiently locate eligible events while still supporting fast debugging and time-based analysis without full table scans.&lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Retry Mechanism with ShedLock&lt;/head&gt;
    &lt;p&gt;Persisting failed events solved the visibility problem, but we still needed a safe and reliable way to retry them.&lt;/p&gt;
    &lt;p&gt; For this, we introduced a DLQ retry scheduler backed by ShedLock. The scheduler periodically scans the DLQ table for &lt;code&gt;PENDING&lt;/code&gt; events that are eligible for retry and attempts to process them again. Since the service runs on multiple instances, ShedLock ensures that only one instance executes the retry job at any given time. This eliminates duplicate retries without requiring custom leader-election logic.
                    &lt;/p&gt;
    &lt;head rend="h4"&gt;Retry Configuration&lt;/head&gt;
    &lt;code&gt;dlq:
  retry:
    enabled: true
    max-retries: 240
    batch-size: 50
    fixed-rate: 21600000 # 6 hours in milliseconds&lt;/code&gt;
    &lt;head rend="h4"&gt;How Retries Work&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scheduler runs every six hours.&lt;/item&gt;
      &lt;item&gt;Up to fifty eligible events are picked up per run.&lt;/item&gt;
      &lt;item&gt;Events exceeding the maximum retry count are skipped.&lt;/item&gt;
      &lt;item&gt;Successful retries immediately transition the event status to &lt;code&gt;SUCCEEDED&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Failures remain in &lt;code&gt;PENDING&lt;/code&gt;and are retried in subsequent runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Query Implementation&lt;/head&gt;
    &lt;p&gt; The retry scheduler uses a SQL query with &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; to safely select eligible events across multiple instances. This PostgreSQL feature ensures that even if multiple scheduler instances run simultaneously, each will pick up different rows without blocking each other:
                    &lt;/p&gt;
    &lt;code&gt;@QueryHints(@QueryHint(name = "jakarta.persistence.lock.timeout", value = "-2"))
@Query(
    value = "SELECT * FROM dlq_table "
        + "WHERE messagetype = :messageType "
        + "AND retries &amp;lt; :maxRetries "
        + "AND (replay_status IS NULL OR replay_status NOT IN ('COMPLETED')) "
        + "ORDER BY created_at ASC "
        + "FOR UPDATE SKIP LOCKED",
    nativeQuery = true
)&lt;/code&gt;
    &lt;p&gt; The &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; clause is crucial here. It allows each instance to lock and process different rows concurrently, preventing duplicate processing while maintaining high throughput. The query hint sets the lock timeout to &lt;code&gt;-2&lt;/code&gt;, which means "wait indefinitely" but combined with &lt;code&gt;SKIP LOCKED&lt;/code&gt;, it effectively means "skip any rows that are already locked by another transaction."
                    &lt;/p&gt;
    &lt;p&gt;This setup allowed the system to tolerate long downstream outages while avoiding retry storms and unnecessary load on dependent services.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operational Benefits&lt;/head&gt;
    &lt;p&gt;With this approach, failures became predictable and observable rather than disruptive. Engineers could inspect failures using plain SQL, identify patterns, and reprocess only the events that mattered. If a downstream dependency was unavailable for hours or even days, events safely accumulated in the DLQ and were retried later without human intervention. If an event was fundamentally bad, it stayed visible instead of being silently dropped.&lt;/p&gt;
    &lt;p&gt;Most importantly, this design reduced operational stress. Failures were no longer something to fear; they were an expected part of the system with a clear, auditable recovery path.&lt;/p&gt;
    &lt;head rend="h3"&gt;My Thoughts&lt;/head&gt;
    &lt;p&gt;The goal was never to replace Kafka with PostgreSQL. Kafka remained the backbone for high-throughput event ingestion, while PostgreSQL handled what it does bestâdurability, querying, and observability around failures. By letting each system play to its strengths, we ended up with a pipeline that was resilient, debuggable, and easy to operate.&lt;/p&gt;
    &lt;p&gt;In the end, using PostgreSQL as a Dead Letter Queue turned failure handling into something boring and predictable. And in production systems, boring is exactly what you want.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46755115</guid><pubDate>Sun, 25 Jan 2026 15:51:03 +0000</pubDate></item><item><title>ICE Using Palantir Tool That Feeds on Medicaid Data</title><link>https://www.eff.org/deeplinks/2026/01/report-ice-using-palantir-tool-feeds-medicaid-data</link><description>&lt;doc fingerprint="ad964554d5aa1139"&gt;
  &lt;main&gt;
    &lt;p&gt;EFF last summer asked a federal judge to block the federal government from using Medicaid data to identify and deport immigrants.&lt;/p&gt;
    &lt;p&gt;We also warned about the danger of the Trump administration consolidating all of the government’s information into a single searchable, AI-driven interface with help from Palantir, a company that has a shaky-at-best record on privacy and human rights.&lt;/p&gt;
    &lt;p&gt;Now we have the first evidence that our concerns have become reality.&lt;/p&gt;
    &lt;p&gt;“Palantir is working on a tool for Immigration and Customs Enforcement (ICE) that populates a map with potential deportation targets, brings up a dossier on each person, and provides a “confidence score” on the person’s current address,” 404 Media reports today. “ICE is using it to find locations where lots of people it might detain could be based.”&lt;/p&gt;
    &lt;p&gt;The tool – dubbed Enhanced Leads Identification &amp;amp; Targeting for Enforcement (ELITE) – receives peoples’ addresses from the Department of Health and Human Services (which includes Medicaid) and other sources, 404 Media reports based on court testimony in Oregon by law enforcement agents, among other sources.&lt;/p&gt;
    &lt;p&gt;This revelation comes as ICE – which has gone on a surveillance technology shopping spree – floods Minneapolis with agents, violently running roughshod over the civil rights of immigrants and U.S. citizens alike; President Trump has threatened to use the Insurrection Act of 1807 to deploy military troops against protestors there. Other localities are preparing for the possibility of similar surges.&lt;/p&gt;
    &lt;p&gt;Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;This kind of consolidation of government records provides enormous government power that can be abused. Different government agencies necessarily collect information to provide essential services or collect taxes, but the danger comes when the government begins pooling that data and using it for reasons unrelated to the purpose it was collected.&lt;/p&gt;
    &lt;p&gt;As EFF Executive Director Cindy Cohn wrote in a Mercury News op-ed last August, “While couched in the benign language of eliminating government ‘data silos,’ this plan runs roughshod over your privacy and security. It’s a throwback to the rightly mocked ‘Total Information Awareness’ plans of the early 2000s that were, at least publicly, stopped after massive outcry from the public and from key members of Congress. It’s time to cry out again.”&lt;/p&gt;
    &lt;p&gt;In addition to the amicus brief we co-authored challenging ICE’s grab for Medicaid data, EFF has successfully sued over DOGE agents grabbing personal data from the U.S. Office of Personnel Management, filed an amicus brief in a suit challenging ICE’s grab for taxpayer data, and sued the departments of State and Homeland Security to halt a mass surveillance program to monitor constitutionally protected speech by noncitizens lawfully present in the U.S.&lt;/p&gt;
    &lt;p&gt;But litigation isn’t enough. People need to keep raising concerns via public discourse and Congress should act immediately to put brakes on this runaway train that threatens to crush the privacy and security of each and every person in America.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46756117</guid><pubDate>Sun, 25 Jan 2026 17:36:19 +0000</pubDate></item><item><title>What Is Starlink Mesh? – Starlink Help Center</title><link>https://starlink.com/ca/support/article/57f4bd5c-4125-2210-8bb2-30c90b558b7b</link><description>&lt;doc fingerprint="222734b630956821"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled on your browser, otherwise content or functionality may be limited or unavailable. JavaScript must be enabled on your browser, otherwise content or functionality may be limited or unavailable. Help Center Home Articles Help Center Home Help Center Home Search support articles Search support articles What is Starlink Mesh? - Starlink Help Center&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46756288</guid><pubDate>Sun, 25 Jan 2026 17:51:24 +0000</pubDate></item></channel></rss>