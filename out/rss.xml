<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 26 Jan 2026 21:13:45 +0000</lastBuildDate><item><title>Show HN: Only 1 LLM can fly a drone</title><link>https://github.com/kxzk/snapbench</link><description>&lt;doc fingerprint="4fd783f63903440d"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Inspired by Pok√©mon Snap (1999). VLM pilots a drone through 3D world to locate and identify creatures.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;%%{init: {'theme': 'base', 'themeVariables': { 'background': '#ffffff', 'primaryColor': '#ffffff'}}}%%
flowchart LR
    subgraph Controller["**Controller** (Rust)"]
        C[Orchestration]
    end

    subgraph VLM["**VLM** (OpenRouter)"]
        V[Vision-Language Model]
    end

    subgraph Simulation["**Simulation** (Zig/raylib)"]
        S[Game State]
    end

    C --&amp;gt;|"screenshot + prompt"| V
    C &amp;lt;--&amp;gt;|"cmds + state&amp;lt;br&amp;gt;**UDP:9999**"| S

    style Controller fill:#8B5A2B,stroke:#5C3A1A,color:#fff
    style VLM fill:#87CEEB,stroke:#5BA3C6,color:#1a1a1a
    style Simulation fill:#4A7C23,stroke:#2D5A10,color:#fff
    style C fill:#B8864A,stroke:#8B5A2B,color:#fff
    style V fill:#B5E0F7,stroke:#87CEEB,color:#1a1a1a
    style S fill:#6BA33A,stroke:#4A7C23,color:#fff
&lt;/code&gt;
    &lt;p&gt;The simulation generates procedural terrain and spawns creatures (cat, dog, pig, sheep) for the drone to discover. It handles drone physics and collision detection, accepting 8 movement commands plus &lt;code&gt;identify&lt;/code&gt; and &lt;code&gt;screenshot&lt;/code&gt;. The Rust controller captures frames from the simulation, constructs prompts enriched with position and state data, then parses VLM responses into executable command sequences. The objective: locate and successfully identify 3 creatures, where &lt;code&gt;identify&lt;/code&gt; succeeds when the drone is within 5 units of a target.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo_3x.mov&lt;/head&gt;
    &lt;p&gt;I gave 7 frontier LLMs a simple task: pilot a drone through a 3D voxel world and find 3 creatures.&lt;/p&gt;
    &lt;p&gt;Only one could do it.&lt;/p&gt;
    &lt;p&gt;Is this a rigorous benchmark? No. However, it's a reasonably fair comparison - same prompt, same seeds, same iteration limits. I'm sure with enough refinement you could coax better results out of each model. But that's kind of the point: out of the box, with zero hand-holding, only one model figured out how to actually fly.&lt;/p&gt;
    &lt;p&gt;The core differentiator wasn't intelligence - it was altitude control. Creatures sit on the ground. To identify them, you need to descend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemini Flash: Actively adjusts altitude, descends to creature level, identifies&lt;/item&gt;
      &lt;item&gt;GPT-5.2-chat: Gets close horizontally but never lowers&lt;/item&gt;
      &lt;item&gt;Claude Opus: Attempts identification 160+ times, never succeeds - approaching at wrong angles&lt;/item&gt;
      &lt;item&gt;Others: Wander randomly or get stuck&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This left me puzzled. Claude Opus is arguably the most capable model in the lineup. It knows it needs to identify creatures. It tries - aggressively. But it never adjusts its approach angle.&lt;/p&gt;
    &lt;p&gt;Run 13 (seed 72) was the only run where any model found 2 creatures. Why? They happened to spawn near each other. Gemini Flash found one, turned around, and spotted the second.&lt;/p&gt;
    &lt;p&gt;In most other runs, Flash found one creature quickly but ran out of iterations searching for the others. The world is big. 50 iterations isn't a lot of time.&lt;/p&gt;
    &lt;p&gt;This was the most surprising finding. I expected:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Opus 4.5 (most expensive) to dominate&lt;/item&gt;
      &lt;item&gt;Gemini 3 Pro to outperform Gemini 3 Flash (same family, more capability)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead, the cheapest model beat models costing 10x more.&lt;/p&gt;
    &lt;p&gt;What's going on here? A few theories:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Spatial reasoning doesn't scale with model size - at least not yet&lt;/item&gt;
      &lt;item&gt;Flash was trained differently - maybe more robotics data, more embodied scenarios?&lt;/item&gt;
      &lt;item&gt;Smaller models follow instructions more literally - "go down" means go down, not "consider the optimal trajectory"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I genuinely don't know. But if you're building an LLM-powered agent that needs to navigate physical or virtual space, the most expensive model might not be your best choice.&lt;/p&gt;
    &lt;p&gt;Anecdotally, creatures with higher contrast (gray sheep, pink pigs) seemed easier to spot than brown-ish creatures that blended into the terrain. A future version might normalize creature visibility. Or maybe that's the point - real-world object detection isn't normalized either.&lt;/p&gt;
    &lt;p&gt;Before this, I tried having LLMs pilot a real DJI Tello drone.&lt;/p&gt;
    &lt;p&gt;Results: it flew straight up, hit the ceiling, and did donuts until I caught it. (I was using Haiku 4.5, which in hindsight explains a lot.)&lt;/p&gt;
    &lt;p&gt;The Tello is now broken. I've ordered a BetaFPV and might get another Tello since they're so easy to program. Now that I know Gemini Flash can actually navigate, a real-world follow-up might be worth revisiting.&lt;/p&gt;
    &lt;p&gt;This is half-serious research, half "let's see what happens."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The simulation has rough edges (it's a side project, not a polished benchmark suite)&lt;/item&gt;
      &lt;item&gt;One blanket prompt is used for all models - model-specific tuning would likely improve results&lt;/item&gt;
      &lt;item&gt;The feedback loop is basic (position, screenshot, recent commands) - there's room to get creative with what information gets passed back&lt;/item&gt;
      &lt;item&gt;Iteration limits (50) may artificially cap models that are slower but would eventually succeed&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;Install&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Zig&lt;/cell&gt;
        &lt;cell&gt;‚â•0.15.2&lt;/cell&gt;
        &lt;cell&gt;ziglang.org/download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;stable (2024 edition)&lt;/cell&gt;
        &lt;cell&gt;rust-lang.org/tools/install&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;‚â•3.11&lt;/cell&gt;
        &lt;cell&gt;python.org&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;uv&lt;/cell&gt;
        &lt;cell&gt;latest&lt;/cell&gt;
        &lt;cell&gt;docs.astral.sh/uv&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You'll also need an OpenRouter API key.&lt;/p&gt;
    &lt;code&gt;gh repo clone kxzk/snapbench
cd snapbench

# set your API key
export OPENROUTER_API_KEY="sk-or-..."&lt;/code&gt;
    &lt;code&gt;# terminal 1: start the simulation (with optional seed)
zig build run -Doptimize=ReleaseFast -- 42
# or
make sim

# terminal 2: start the drone controller
cargo run --release --manifest-path llm_drone/Cargo.toml -- --model google/gemini-3-flash-preview
# or
make drone&lt;/code&gt;
    &lt;code&gt;# runs all models defined in bench/models.toml
uv run bench/bench_runner.py
# or
make bench&lt;/code&gt;
    &lt;p&gt;Results get saved to &lt;code&gt;data/run_&amp;lt;id&amp;gt;.csv&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model-specific prompts: Tune instructions to each model's strengths&lt;/item&gt;
      &lt;item&gt;Richer feedback: Pass more spatial context (distance readings, compass, minimap?)&lt;/item&gt;
      &lt;item&gt;Multi-agent runs: What if you gave each model a drone and made them compete?&lt;/item&gt;
      &lt;item&gt;Extended iterations: Let slow models run longer to isolate reasoning from speed&lt;/item&gt;
      &lt;item&gt;Real drone benchmark: Gemini Flash vs. the BetaFPV&lt;/item&gt;
      &lt;item&gt;Pok√©mon assets: Found low-poly Pok√©mon models on Poly Pizza‚Äîleaning into the Pok√©mon Snap inspiration&lt;/item&gt;
      &lt;item&gt;World improvements: Larger terrain, better visuals, performance optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Drone by NateGazzard CC-BY via Poly Pizza&lt;/item&gt;
      &lt;item&gt;Cube World Kit by Quaternius via Poly Pizza&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Donated to Poly Pizza to support the platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46764170</guid><pubDate>Mon, 26 Jan 2026 11:00:44 +0000</pubDate></item><item><title>Google AI Overviews cite YouTube more than any medical site for health queries</title><link>https://www.theguardian.com/technology/2026/jan/24/google-ai-overviews-youtube-medical-citations-study</link><description>&lt;doc fingerprint="bdca1812fc9b2573"&gt;
  &lt;main&gt;
    &lt;p&gt;Google‚Äôs search feature AI Overviews cites YouTube more than any medical website when answering queries about health conditions, according to research that raises fresh questions about a tool seen by 2 billion people each month.&lt;/p&gt;
    &lt;p&gt;The company has said its AI summaries, which appear at the top of search results and use generative AI to answer questions from users, are ‚Äúreliable‚Äù and cite reputable medical sources such as the Centers for Disease Control and Prevention and the Mayo Clinic.&lt;/p&gt;
    &lt;p&gt;However, a study that analysed responses to more than 50,000 health queries, captured using Google searches from Berlin, found the top cited source was YouTube. The video-sharing platform is the world‚Äôs second most visited website, after Google itself, and is owned by Google.&lt;/p&gt;
    &lt;p&gt;Researchers at SE Ranking, a search engine optimisation platform, found YouTube made up 4.43% of all AI Overview citations. No hospital network, government health portal, medical association or academic institution came close to that number, they said.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis matters because YouTube is not a medical publisher,‚Äù the researchers wrote. ‚ÄúIt is a general-purpose video platform. Anyone can upload content there (eg board-certified physicians, hospital channels, but also wellness influencers, life coaches, and creators with no medical training at all).‚Äù&lt;/p&gt;
    &lt;p&gt;Google told the Guardian that AI Overviews was designed to surface high-quality content from reputable sources, regardless of format, and a variety of credible health authorities and licensed medical professionals created content on YouTube. The study‚Äôs findings could not be extrapolated to other regions as it was conducted using German-language queries in Germany, it said.&lt;/p&gt;
    &lt;p&gt;The research comes after a Guardian investigation found people were being put at risk of harm by false and misleading health information in Google AI Overviews responses.&lt;/p&gt;
    &lt;p&gt;In one case that experts said was ‚Äúdangerous‚Äù and ‚Äúalarming‚Äù, Google provided bogus information about crucial liver function tests that could have left people with serious liver disease wrongly thinking they were healthy. The company later removed AI Overviews for some but not all medical searches.&lt;/p&gt;
    &lt;p&gt;The SE Ranking study analysed 50,807 healthcare-related prompts and keywords to see which sources AI Overviews relied on when generating answers.&lt;/p&gt;
    &lt;p&gt;They chose Germany because its healthcare system is strictly regulated by a mix of German and EU directives, standards and safety regulations. ‚ÄúIf AI systems rely heavily on non-medical or non-authoritative sources even in such an environment, it suggests the issue may extend beyond any single country,‚Äù they wrote.&lt;/p&gt;
    &lt;p&gt;AI Overviews surfaced on more than 82% of health searches, the researchers said. When they looked at which sources AI Overviews relied on most often for health-related answers, one result stood out immediately, they said. The single most cited domain was YouTube with 20,621 citations out of a total of 465,823.&lt;/p&gt;
    &lt;p&gt;The next most cited source was NDR.de, with 14,158 citations (3.04%). The German public broadcaster produces health-related content alongside news, documentaries and entertainment. In third place was a medical reference site, Msdmanuals.com with 9,711 citations (2.08%).&lt;/p&gt;
    &lt;p&gt;The fourth most cited source was Germany‚Äôs largest consumer health portal, Netdoktor.de, with 7,519 citations (1.61%). The fifth most cited source was a career platform for doctors, Praktischarzt.de, with 7,145 citations (1.53%).&lt;/p&gt;
    &lt;p&gt;The researchers acknowledged limitations to their study. It was conducted as a one-time snapshot in December 2025, using German-language queries that reflected how users in Germany typically search for health information.&lt;/p&gt;
    &lt;p&gt;Results could vary over time, by region, and by the phrasing of questions. However, even with those caveats, the findings still prompted alarm.&lt;/p&gt;
    &lt;p&gt;Hannah van Kolfschooten, a researcher specialising in AI, health and law at the University of Basel who was not involved with the research, said: ‚ÄúThis study provides empirical evidence that the risks posed by AI Overviews for health are structural, not anecdotal. It becomes difficult for Google to argue that misleading or harmful health outputs are rare cases.&lt;/p&gt;
    &lt;p&gt;‚ÄúInstead, the findings show that these risks are embedded in the way AI Overviews are designed. In particular, the heavy reliance on YouTube rather than on public health authorities or medical institutions suggests that visibility and popularity, rather than medical reliability, is the central driver for health knowledge.‚Äù&lt;/p&gt;
    &lt;p&gt;A Google spokesperson said: ‚ÄúThe implication that AI Overviews provide unreliable information is refuted by the report‚Äôs own data, which shows that the most cited domains in AI Overviews are reputable websites. And from what we‚Äôve seen in the published findings, AI Overviews cite expert YouTube content from hospitals and clinics.‚Äù&lt;/p&gt;
    &lt;p&gt;Google said the study showed that of the 25 most cited YouTube videos, 96% were from medical channels. However, the researchers cautioned that these videos represented fewer than 1% of all the YouTube links cited by AI Overviews on health.&lt;/p&gt;
    &lt;p&gt;‚ÄúMost of them (24 out of 25) come from medical-related channels like hospitals, clinics and health organisations,‚Äù the researchers wrote. ‚ÄúOn top of that, 21 of the 25 videos clearly note that the content was created by a licensed or trusted source.&lt;/p&gt;
    &lt;p&gt;‚ÄúSo at first glance it looks pretty reassuring. But it‚Äôs important to remember that these 25 videos are just a tiny slice (less than 1% of all YouTube links AI Overviews actually cite). With the rest of the videos, the situation could be very different.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46766031</guid><pubDate>Mon, 26 Jan 2026 14:27:00 +0000</pubDate></item><item><title>Television is 100 years old today</title><link>https://diamondgeezer.blogspot.com/2026/01/tv100.html</link><description>&lt;doc fingerprint="f2addb515e3996af"&gt;
  &lt;main&gt;
    &lt;p&gt;Television is 100 years old today. And it was born here, above an Italian cafe in Soho.&lt;/p&gt;
    &lt;p&gt;The man who first demonstrated television was John Logie Baird, a former engineering apprentice from Helensburgh. And although there are other places that can plausibly claim to be TV's birthplace, including a terraced street in Hastings, a hill in north London and Selfridges, most people agree that the decisive moment was a demonstration given to journalists in Frith Street on 26th January 1926.&lt;/p&gt;
    &lt;p&gt;Baird might never have made it to London had he not been a sickly boy. When WW1 broke out he wanted to enlist but was refused due to ill health, so took a job with the Clyde Valley Electrical Power Company helping to make munitions instead. In 1923 he moved to the south coast for the good of his health because it had a warmer climate, renting rooms at 21 Linton Crescent in Hastings. Here the first television signal transmitting equipment was constructed, with component parts including a hatbox, tea chest, darning needles and bicycle light lenses. The first image to be transmitted was the shadow of a St Johns Ambulance medal with a distinctive spiky outline, an item still on display at Hastings Museum. But his tinkering proved dangerous, and although a 1000-volt electric shock thankfully resulted in nothing worse than a burnt hand, his landlord duly asked him to vacate the premises.&lt;/p&gt;
    &lt;p&gt;Baird moved to London in November 1924 in the hope of showing off his burgeoning invention, setting up a workshop in the attic at 22 Frith Street. Amongst those who dropped by was Gordon Selfridge who invited Baird to give demonstrations of his device in the Palm Court during his store's upcoming Birthday Week celebrations. He gave three shows a day to long queues of spectators, each invited to peer down a funnel at outlines of shapes transmitted from a separate device a few yards away, including a paper mask which Baird would make 'wink' by covering the eyehole. At this stage Baird's 'Televisor' was still electro-mechanical, the images formed by spinning discs with doubled-up lenses and perforated rectangular holes. But spectators were impressed, and Baird earned a much-needed ¬£60 to plough back into his enterprise.&lt;/p&gt;
    &lt;p&gt;By October 1925 Baird had honed his processes sufficiently to be able to transmit an image with gradations of light and shade. Initially he used a ventriloquist's dummy called Stooky Bill, this because it had greater contrast than a human face and also because it wouldn't be harmed by intense heat or possible exploding glass. Later, somewhat over-excitedly, he invited a 20 year-old office worker called William Taynton to come upstairs and become TV's first human subject. William wasn't keen but an appearance fee of half a crown persuaded him to pick through a jungle of wires, sit in front of blazing hot lamps and stick his tongue out, for just long enough that Baird exclaimed "I've seen you, William, I've seen you. I've got television at last!" When the time came for a blue plaque to be unveiled outside 22 Frith Street in 1951, it was William they invited back to do the honours.&lt;/p&gt;
    &lt;p&gt;Then on 26th January 1926 came the first official demonstration to members of the press. Journalists and guests from the Royal Institution were invited into Baird's workshop in small groups and first shown the dummy on screen, then each other's faces transmitted from a separate room. Only one visitor got too close to the discs and ended up with a sliced beard. Most of those present weren't especially impressed and failed to realise the significance of what they'd just seen, but The Times followed up with a short article two days later.&lt;/p&gt;
    &lt;p&gt;Members of the Royal Institution and other visitors to a laboratory in an upper room in Frith-Street, Soho, on Tuesday saw a demonstration of apparatus invented by Mr. J.L. Baird, who claims to have solved the problem of television. They were shown a transmitting machine, consisting of a large wooden revolving disc containing lenses, behind which was a revolving shutter and a light sensitive cell. It was explained that by means of the shutter and lens disc an image of articles or persons standing in front of the machine could be made to pass over the light sensitive cell at high speed. The current in the cell varies in proportion to the light falling on it, and this varying current is transmitted to a receiver where it controls a light behind an optical arrangement similar to that at the sending end. By this means a point of light is caused to traverse a ground glass screen. The light is dim at the shadows and bright at the high lights, and crosses the screen so rapidly that the whole image appears simultaneously to the eye. (The Times, 28th January 1926)&lt;/p&gt;
    &lt;p&gt;These days 22 Frith Street is home to retro cafe Bar Italia. It's been owned and run by the Polledri family since 1949, a coffee-squirting dynasty who also run the Little Italy restaurant nextdoor. The stone floor was laid by their uncle Torino, a terrazzo mosaic specialist, and the counter was one of the first in London to be graced by an original Gaggia machine. Once a magnet for mods on scooters Bar Italia has attracted many famous names over the years, notably Rocky Marciano whose huge poster has pride of place behind the counter. You could thus celebrate today's centenary with an espresso and a slice of pizza in the photo-bedecked interior, or risk sitting outside below the neon sign with a froth and cheesecake combo.&lt;/p&gt;
    &lt;p&gt;Number 22 also displays a Milestone plaque erected by The Institution of Electrical Engineers citing "the world's first public demonstration of live television". Below is a much newer plaque citing this as an accredited World Origin Site. I first saw one of these inside the Alexander Fleming Laboratory Museum earlier in the month, earned for the discovery of penicillin, but whereas that was designated WOS 0001 the invention of television only ranks 0037. I believe they're unveiling it officially at 2pm this afternoon, even though it was perfectly visible over the weekend.&lt;/p&gt;
    &lt;p&gt;Baird was a highly driven inventor and entrepreneur and went on to develop prototypes for all sorts of forward-looking formats. In 1927 he came up with ‚ÄòPhonovision‚Äô (image recordings onto 78 rpm gramophone records) and ‚ÄòNoctovision‚Äô (infra-red TV). In 1928, amazingly, he demonstrated both colour television and stereoscopic (3D) television. His ultimate aim was television broadcasting via the BBC, beginning experimental transmissions of 30-line television in 1930 and delivering the first outside broadcast (from the Derby) in 1931, not that anyone was yet watching.&lt;/p&gt;
    &lt;p&gt;But in 1932 EMI started to provide serious competition, developing their own pioneering electronic television camera called the Emitron. The government's Television Advisory Committee ultimately recommended that both Baird's 240-line mechanical system and Marconi-EMI's 405-line electronic system be developed as alternatives for the proposed new London television station. And so it was that when broadcast TV first launched at Alexandra Palace on 2nd November 1936 the two systems alternated one week each... Baird second.&lt;/p&gt;
    &lt;p&gt;It rapidly became clear that the Marconi system was far superior and Baird's was dropped after just three months. Baird also suffered when his studios were burned in the fire that destroyed the Crystal Palace, and his company went into receivership when all TV broadcasting was suspended at the start of WW2. He carried on inventing at home in Sydenham, vastly improving his system for colour television, until his laboratory was made unusable by bomb damage. Alas ill health caught up with him and he died after a stroke at the age of 57, just one week after the BBC restarted television broadcasts in 1946. You can't see his final home in Bexhill because it was replaced by a block of flats in 2005, but Baird does have an impressive number of plaques across central London and SE26.&lt;/p&gt;
    &lt;p&gt;It's not always easy being first, and after early televisual success John Logie Baird saw his star wane and fade. But it's still him we remember for making possible one of the key transformative inventions of the 20th century, even though barely anyone watched his first efforts. It took ten years to get from Stooky Bill to BBC TV's opening night, then another two decades before the widespread adoption of TV sets in British households and two more until colour television took hold. But 100 years on almost all of us have a TV set at home and effectively another in our pocket, and all because a Scotsman came to London and cleverly spun some discs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46766188</guid><pubDate>Mon, 26 Jan 2026 14:41:04 +0000</pubDate></item><item><title>Exactitude in Science ‚Äì Borges (1946) [pdf]</title><link>https://kwarc.info/teaching/TDM/Borges.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46766229</guid><pubDate>Mon, 26 Jan 2026 14:44:51 +0000</pubDate></item><item><title>OSS ChatGPT WebUI ‚Äì 530 Models, MCP, Tools, Gemini RAG, Image/Audio Gen</title><link>https://llmspy.org/docs/v3</link><description>&lt;doc fingerprint="b00f92d914e999d8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;v3 Release Notes&lt;/head&gt;&lt;p&gt;Major release focused on extensibility, expanded provider support, and enhanced user experience.&lt;/p&gt;&lt;head rend="h2"&gt;üöÄ What's New at a Glance&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Feature&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;530+ Models&lt;/cell&gt;&lt;cell&gt;Access over 530 models from 24 providers via models.dev integration&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Model Selector&lt;/cell&gt;&lt;cell&gt;Redesigned full-featured dialog with search, filtering, sorting, and favorites&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Extensions&lt;/cell&gt;&lt;cell&gt;Add features, providers, and customize the UI with a flexible plugin architecture&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Gemini RAG&lt;/cell&gt;&lt;cell&gt;Manage Gemini File Search Stores and manage document uploads for RAG workflows&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Tool Support&lt;/cell&gt;&lt;cell&gt;First-class Python function calling for LLM interactions with your local environment&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;MCP Support&lt;/cell&gt;&lt;cell&gt;Connect to Model Context Protocol servers for extended tool capabilities&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Computer Use&lt;/cell&gt;&lt;cell&gt;Desktop automation - control mouse, keyboard, and take screenshots like a human&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;KaTeX Math Typesetting&lt;/cell&gt;&lt;cell&gt;Support for beautiful rendering of LaTeX math expressions&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Calculator UI&lt;/cell&gt;&lt;cell&gt;Beautiful UX Friendly UI to evaluate python math expressions&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Run Code UI&lt;/cell&gt;&lt;cell&gt;Execute Python, JS, TypeScript and C# code scripts in a CodeMirror editor&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Image Generation&lt;/cell&gt;&lt;cell&gt;Built-in support for Google, OpenAI, OpenRouter, Chutes, and Nvidia&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Audio Generation&lt;/cell&gt;&lt;cell&gt;TTS support for Gemini 2.5 Flash/Pro Preview models&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Media Gallery&lt;/cell&gt;&lt;cell&gt;Beautiful UI to browse generated images and audio generations&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;SQLite Storage&lt;/cell&gt;&lt;cell&gt;Migrated IndexedDB to server SQLite for robust persistence and concurrent usage&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Asset Caching&lt;/cell&gt;&lt;cell&gt;Persistent image/file file caching with metadata&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Gemini RAG Extension&lt;/cell&gt;&lt;cell&gt;Manage Gemini File Search Stores for RAG workflows with document uploads and sync&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;New Model Selector UI&lt;/item&gt;&lt;item&gt;Rewritten for Extensibility&lt;/item&gt;&lt;item&gt;Extensions System&lt;/item&gt;&lt;item&gt;Gemini RAG Extension&lt;/item&gt;&lt;item&gt;Tool Support&lt;/item&gt;&lt;item&gt;MCP Support&lt;/item&gt;&lt;item&gt;Core Tools&lt;/item&gt;&lt;item&gt;Computer Use&lt;/item&gt;&lt;item&gt;Calculator UI&lt;/item&gt;&lt;item&gt;Run Code UI&lt;/item&gt;&lt;item&gt;KaTeX Math Typesetting&lt;/item&gt;&lt;item&gt;Image Generation Support&lt;/item&gt;&lt;item&gt;Audio Generation Support&lt;/item&gt;&lt;item&gt;Media Gallery&lt;/item&gt;&lt;item&gt;System Prompts Library&lt;/item&gt;&lt;item&gt;Server-Side SQLite Storage&lt;/item&gt;&lt;item&gt;Image Cache &amp;amp; Optimization&lt;/item&gt;&lt;item&gt;CLI - More Powerful Than Ever&lt;/item&gt;&lt;item&gt;Upgrade Instructions&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Install&lt;/head&gt;&lt;p&gt;Get instant access to 530+ models from 24 providers with extensibility at its core:&lt;/p&gt;&lt;code&gt;pip install llms-py&lt;/code&gt;&lt;head rend="h3"&gt;Upgrade&lt;/head&gt;&lt;code&gt;pip install llms-py --upgrade&lt;/code&gt;&lt;p&gt;See Install Docs for running from Docker or source.&lt;/p&gt;&lt;head rend="h2"&gt;Switch to models.dev Provider Model Configuration&lt;/head&gt;&lt;p&gt;A major change to significantly increase the available models is the switch to utilizing the same models.dev open provider and model catalogue as used and maintained by OpenCode.&lt;/p&gt;&lt;p&gt;llms.json provider configuration is now a superset of models.dev/api.json where its definitions are merged, allowing you to enable providers using just &lt;code&gt;"enabled": true&lt;/code&gt; to inherit the configuration from models.dev&lt;/p&gt;&lt;head rend="h3"&gt;üåê Expanded Provider Support&lt;/head&gt;&lt;p&gt;The switch to models.dev greatly expands the model selection to over 530 models from 24 different providers, including new support for:&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Provider&lt;/cell&gt;&lt;cell role="head"&gt;Models&lt;/cell&gt;&lt;cell role="head"&gt;Provider&lt;/cell&gt;&lt;cell role="head"&gt;Models&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Alibaba&lt;/cell&gt;&lt;cell&gt;39&lt;/cell&gt;&lt;cell&gt;Hugging Face&lt;/cell&gt;&lt;cell&gt;14&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Chutes&lt;/cell&gt;&lt;cell&gt;56&lt;/cell&gt;&lt;cell&gt;Zai Coding Plan&lt;/cell&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;DeepSeek&lt;/cell&gt;&lt;cell&gt;2&lt;/cell&gt;&lt;cell&gt;MiniMax&lt;/cell&gt;&lt;cell&gt;1&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Fireworks AI&lt;/cell&gt;&lt;cell&gt;12&lt;/cell&gt;&lt;cell&gt;Moonshot AI&lt;/cell&gt;&lt;cell&gt;5&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;GitHub Copilot&lt;/cell&gt;&lt;cell&gt;27&lt;/cell&gt;&lt;cell&gt;Nvidia&lt;/cell&gt;&lt;cell&gt;24&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;GitHub Models&lt;/cell&gt;&lt;cell&gt;55&lt;/cell&gt;&lt;cell&gt;Zai&lt;/cell&gt;&lt;cell&gt;6&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Cerebras&lt;/cell&gt;&lt;cell&gt;3&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;LMStudio&lt;/cell&gt;&lt;cell&gt;local&lt;/cell&gt;&lt;cell&gt;Ollama&lt;/cell&gt;&lt;cell&gt;local&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Non OpenAI Compatible LLM and Image generation providers are maintained in the providers extension, registered using the &lt;code&gt;ctx.add_provider()&lt;/code&gt; API. There are several different provider implementations to take advantage of features available in each provider, such as Interleaved Thinking support in Anthropic's Messages API which enables all Claude and MiniMax models to reason between tool calls for improved agentic performance.&lt;/p&gt;&lt;p&gt;TIP&lt;/p&gt;&lt;head rend="h3"&gt;üîÑ Automatic Provider Updates&lt;/head&gt;&lt;p&gt;This actively maintained list of available providers and models are automatically updated into your &lt;code&gt;providers.json&lt;/code&gt; daily that can also be manually updated with:&lt;/p&gt;&lt;p&gt;As an optimization only the providers that are referenced in your &lt;code&gt;llms.json&lt;/code&gt; are saved. Any additional providers you want to use that are not included in models.dev can be added to your &lt;code&gt;~/.llms/providers-extra.json&lt;/code&gt;, which get merged into your &lt;code&gt;providers.json&lt;/code&gt; on every update.&lt;/p&gt;&lt;p&gt;This keeps your local configuration file lightweight by only including the providers that are available for use.&lt;/p&gt;&lt;head rend="h3"&gt;Configuration Examples&lt;/head&gt;&lt;p&gt;Enable providers by ID ‚Äî all configuration is automatically inherited:&lt;/p&gt;&lt;p&gt;See Configuration docs for more info.&lt;/p&gt;&lt;head rend="h3"&gt;New Model Selector UI&lt;/head&gt;&lt;p&gt;With over 530 models from 24 providers now available, discovering and selecting the right model required a complete overhaul. The Model Selector has been completely redesigned as a full-featured dialog offering:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üîç Smart Search &amp;amp; Discovery - Instantly search across model names, IDs, and providers&lt;/item&gt;&lt;item&gt;üéØ Advanced Filtering - Filter by name, providers &amp;amp; input and output modalities&lt;/item&gt;&lt;item&gt;üìä Flexible Sorting - Sort by Knowledge Cutoff, Release Date, Last Updated &amp;amp; Context&lt;/item&gt;&lt;item&gt;‚≠ê Favorites System - Star model card to add/remove to favorites quick list&lt;/item&gt;&lt;item&gt;üíé Rich Model Cards - In depth model overview at a glance&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Where providers can be quickly enabled or disabled to customize which models are available:&lt;/p&gt;&lt;p&gt;See Model Selector docs for more info.&lt;/p&gt;&lt;head rend="h2"&gt;Rewritten for Extensibility&lt;/head&gt;&lt;p&gt;llms.py has been rewritten from the ground-up with extensibility a core concept where all major UI and Server features now layer on their encapsulated functionality by using the public Client &amp;amp; Server Extensibility APIs.&lt;/p&gt;&lt;p&gt;Extensions are just folders that can add both Server and UI features using the public client and server extensibility APIs. Built-in features are just extensions in the repo's llms/extensions folder which can be disabled or overridden by adding them to your local &lt;code&gt;~/.llms/extensions&lt;/code&gt; folder. Too minimize bloat, only features that are generally useful and don't require additional dependencies are included as built-in extensions.&lt;/p&gt;&lt;p&gt;llms includes support for installing and uninstalling extensions from any GitHub repository. For better discoverability, non built-in extensions are maintained in the github.com/llmspy organization repositories which anyone else is welcome to contribute their repos to for increased discoverability.&lt;/p&gt;&lt;p&gt;UI components are now registered and referenced as Global Vue components, which can be easily replaced by registering Vue components with the same name as done in the xmas extension demo.&lt;/p&gt;&lt;p&gt;This approach allows main.py to retain a lean functional core in a single file whilst still being fully extensible and lays the foundation for rapid development of new features - both from the core team and external 3rd party extensions - enabling the community to extend llms.py in new unanticipated ways.&lt;/p&gt;&lt;p&gt;For deployments requiring minimal footprint, the Custom Build docs shows how to create a tailored distribution with only the specific extensions you need - perfect for CLI-only or lightweight API server deployments.&lt;/p&gt;&lt;head rend="h2"&gt;Extensions System&lt;/head&gt;&lt;p&gt;To keep the core lightweight while enabling limitless enhancements, we've implemented a flexible Extensions system inspired by ComfyUI Custom Nodes. This allows adding new features, pages and toolbar icons, register new provider implementations, extend, replace, and customize the UI with your own custom features, just by adding new extension folders.&lt;/p&gt;&lt;head rend="h3"&gt;Managing Extensions&lt;/head&gt;&lt;p&gt;List available extensions:&lt;/p&gt;&lt;p&gt;Output:&lt;/p&gt;&lt;p&gt;Install an extension:&lt;/p&gt;&lt;p&gt;Install a 3rd-party extension:&lt;/p&gt;&lt;p&gt;INFO&lt;/p&gt;&lt;code&gt;~/.llms/extensions/my_extension&lt;/code&gt; and installs any &lt;code&gt;requirements.txt&lt;/code&gt; dependencies.&lt;p&gt;List installed extensions:&lt;/p&gt;&lt;p&gt;Remove an extension:&lt;/p&gt;&lt;head rend="h3"&gt;Manual Installation&lt;/head&gt;&lt;p&gt;Extensions can be installed from GitHub or by creating a local folder:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Local: Simply create a folder in &lt;code&gt;~/.llms/extensions/my_extension&lt;/code&gt;&lt;/item&gt;&lt;item&gt;GitHub: Clone extensions into &lt;code&gt;~/.llms/extensions&lt;/code&gt;, e.g:&lt;/item&gt;&lt;/list&gt;&lt;p&gt;See Extensions docs for more details.&lt;/p&gt;&lt;head rend="h3"&gt;How it Works (Server)&lt;/head&gt;&lt;p&gt;Extensions are Python modules that plug into the server lifecycle using special hooks defined in their &lt;code&gt;__init__.py&lt;/code&gt;:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Hook&lt;/cell&gt;&lt;cell role="head"&gt;Purpose&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;code&gt;__parser__(parser)&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Add custom CLI arguments&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;code&gt;__install__(ctx)&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Enhance the server instance (routes, providers, filters, etc.)&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;&lt;code&gt;__load__(ctx)&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Load data or perform async tasks before server starts&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;&lt;code&gt;__run__(ctx)&lt;/code&gt;&lt;/cell&gt;&lt;cell&gt;Execute custom logic when running in CLI mode&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The &lt;code&gt;ctx&lt;/code&gt; parameter provides access to the &lt;code&gt;ExtensionContext&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;See Server Extensions docs for more details.&lt;/p&gt;&lt;head rend="h3"&gt;How it Works (UI)&lt;/head&gt;&lt;p&gt;Extensions can also include frontend components:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Placement: Add a &lt;code&gt;ui&lt;/code&gt;folder within your extension directory&lt;/item&gt;&lt;item&gt;Access: Files in this folder are automatically served at &lt;code&gt;/ext/&amp;lt;extension_name&amp;gt;/*&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Integration: Create a &lt;code&gt;ui/index.mjs&lt;/code&gt;file. This is the entry point and must export an&lt;code&gt;install&lt;/code&gt;function:&lt;/item&gt;&lt;/list&gt;&lt;p&gt;See UI Extensions docs for more details.&lt;/p&gt;&lt;head rend="h3"&gt;Example: xmas extension&lt;/head&gt;&lt;p&gt;The xmas extension demonstrates these capabilities where it utilizes the Extensions APIs to give llms.py a splash of Christmas spirit. It uses &lt;code&gt;__install__&lt;/code&gt; to register an API endpoint and a UI extension for its UI features.&lt;/p&gt;&lt;head rend="h3"&gt;Replacing Core Components&lt;/head&gt;&lt;p&gt;All UI features of xmas is implemented in its ui/index.mjs which overrides default &lt;code&gt;Brand&lt;/code&gt; and &lt;code&gt;Welcome&lt;/code&gt; components by registering components with the same name, e.g:&lt;/p&gt;&lt;p&gt;To change both the home page and brand on the top-left to give every page title a festive touch:&lt;/p&gt;&lt;p&gt;It also demonstrates adding a new icon on the left sidebar to open its custom Xmas page component and a top-panel component to display its "Ask Santa" portal:&lt;/p&gt;&lt;head rend="h3"&gt;xmas page&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Ask Santa panel&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;p&gt;The Xmas page calls a custom API endpoint registered in its &lt;code&gt;__install__&lt;/code&gt; hook to return a custom festive greeting, whilst the top-panel modifies chat requests while its Top Panel is open to add a Santa system prompt which is enough to implement its "Ask Santa" feature.&lt;/p&gt;&lt;p&gt;Smart generation models like Nano Banana's gemini-2.5-flash-image perform exceptionally well here as they're able to answer your kids questions with rich, detailed responses and image outputs.&lt;/p&gt;&lt;head rend="h2"&gt;Gemini RAG Extension&lt;/head&gt;&lt;p&gt;The gemini extension provides a complete solution for managing Google Gemini's File Search Stores, enabling RAG (Retrieval Augmented Generation) workflows with automatic document uploads, category organization, and bidirectional sync between your local database and Gemini's cloud storage.&lt;/p&gt;&lt;p&gt;Build up your own knowledge base in File Stores, optionally organized into categories, that you can query to ground your AI chats with your own data - whether that's searching across a single document, a category of related documents, or your entire filestore.&lt;/p&gt;&lt;head rend="h3"&gt;Install&lt;/head&gt;&lt;p&gt;Install the gemini extension via the CLI:&lt;/p&gt;&lt;p&gt;After which you'll be able to click the Gemini Icon to open the Gemini extension page from the sidebar to manage your filestores.&lt;/p&gt;&lt;head rend="h3"&gt;Key Features&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Filestore Management: Create and manage isolated stores of documents for different projects or knowledge bases&lt;/item&gt;&lt;item&gt;Drag &amp;amp; Drop Uploads: Easily upload documents (PDF, Text, Markdown, etc.) by dragging them into the UI&lt;/item&gt;&lt;item&gt;Smart Categorization: Organize documents into categories (folders) for granular retrieval&lt;/item&gt;&lt;item&gt;Contextual RAG Chat: &lt;list rend="ul"&gt;&lt;item&gt;Ask Filestore: Chat with the entire knowledge base of a filestore&lt;/item&gt;&lt;item&gt;Ask Category: Focus your chat on a specific category within a filestore&lt;/item&gt;&lt;item&gt;Ask Document: Chat with a single specific document&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Bi-Directional Sync: Reconcile your local database with the remote Gemini File API&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Uploading Documents&lt;/head&gt;&lt;p&gt;Documents can be uploaded by dragging and dropping files onto the upload zone or clicking to open the file picker. You can organize uploads into category folders by typing a category name before uploading.&lt;/p&gt;&lt;p&gt;Uploads are processed asynchronously by a Background Worker utilizing a DB Queue, so you can continue working while documents are indexed. The worker automatically starts when new documents are uploaded and efficiently handles batch processing without blocking the UI.&lt;/p&gt;&lt;head rend="h3"&gt;RAG Chat in Action&lt;/head&gt;&lt;p&gt;Once documents are uploaded, you can start contextual RAG chat sessions with your data. Each session is pre-configured with a Gemini Model and the &lt;code&gt;file_search&lt;/code&gt; tool to query your selected filestore, category, or document - as shown in the meta example below querying this very v3 document for its best features:&lt;/p&gt;&lt;p&gt;The grounded sources used to answer your query are displayed at the bottom of each chat response, allowing you to verify and explore the source documents.&lt;/p&gt;&lt;p&gt;See the Gemini Extension docs for complete usage instructions.&lt;/p&gt;&lt;head rend="h2"&gt;Tool Support&lt;/head&gt;&lt;p&gt;This release also includes first-class support for Python function calling (Tools), allowing LLMs to interact with your local environment and custom functionality.&lt;/p&gt;&lt;p&gt;Tools can be defined using standard Python functions where its tool definition can be implicitly defined from its function's signature, type hints, and docstrings:&lt;/p&gt;&lt;head rend="h3"&gt;Register tools for function calling&lt;/head&gt;&lt;head rend="h4"&gt;Implicit Tool Definition&lt;/head&gt;&lt;p&gt;Tools can be registered within an extension's &lt;code&gt;install&lt;/code&gt; hook using &lt;code&gt;ctx.register_tool&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;If no group is specified, tools are registered under the default &lt;code&gt;custom&lt;/code&gt; group, alternatively you can group them under your preferred name:&lt;/p&gt;&lt;head rend="h4"&gt;Explicit Tool Definition&lt;/head&gt;&lt;p&gt;When more fine-grain configuration is needed you can use an explicit tool definition, e.g:&lt;/p&gt;&lt;head rend="h3"&gt;UI Management&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;One-Click Enable/Disable: Use the Tool Selector in the top-right to control which tools to use per request&lt;/item&gt;&lt;item&gt;Granular Control: Select "All", "None", or specific tools for each chat session&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Dedicated Tools Page: View all registered tools and their definitions at &lt;code&gt;/tools&lt;/code&gt; or via the sidebar&lt;/p&gt;&lt;head rend="h2"&gt;MCP Support&lt;/head&gt;&lt;p&gt;The fast_mcp extension brings Model Context Protocol (MCP) support to llms.py, allowing you to extend LLM capabilities with a wide range of external tools and services using the FastMCP Python Framework.&lt;/p&gt;&lt;head rend="h3"&gt;Install&lt;/head&gt;&lt;head rend="h3"&gt;Key Features&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Standardized Tool Access: Connect to any MCP-compliant server (Node.js, Python, etc.) seamlessly&lt;/item&gt;&lt;item&gt;Dynamic Discovery: Automatically discovers and registers all tools exposed by configured servers&lt;/item&gt;&lt;item&gt;Parallel Discovery: All configured MCP servers are discovered concurrently for fast startup times&lt;/item&gt;&lt;item&gt;UI Management: Add, edit, and manage MCP servers directly from the Tools page&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;MCP Servers&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Gemini Image via MCP&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Configuration&lt;/head&gt;&lt;p&gt;MCP servers are configured via a &lt;code&gt;mcp.json&lt;/code&gt; file. By default, Anthropic's Git MCP Server is pre-configured:&lt;/p&gt;&lt;head rend="h3"&gt;Managing Servers&lt;/head&gt;&lt;p&gt;Add, edit, or remove MCP servers directly from the UI:&lt;/p&gt;&lt;head rend="h3"&gt;Add MCP Server&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Edit MCP Server&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Executing Tools&lt;/head&gt;&lt;p&gt;MCP tools can be executed directly from the Tools page or invoked by LLMs during chat sessions:&lt;/p&gt;&lt;head rend="h3"&gt;Execute Tool&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Tool Results&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;HTML Results&lt;/head&gt;&lt;p&gt;Tool outputs containing HTML content are rendered within a sandboxed iframe, letting you interact with rich content and even play games:&lt;/p&gt;&lt;p&gt;See the MCP Support docs for complete configuration and usage details.&lt;/p&gt;&lt;head rend="h3"&gt;Omarchy MCP&lt;/head&gt;&lt;p&gt;For Omarchy users, the Omarchy MCP enables AI assistants to manage themes - including listing, switching, previewing, installing, and removing themes from your Omarchy desktop environment.&lt;/p&gt;&lt;head rend="h2"&gt;Core Tools&lt;/head&gt;&lt;p&gt;The built-in core_tools extension provides essential functionality for LLMs to interact with their environment, perform calculations, and manage persistent data.&lt;/p&gt;&lt;head rend="h3"&gt;Memory Tools&lt;/head&gt;&lt;p&gt;Functions for persistent key-value storage.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;memory_read&lt;/code&gt;- Read a value from persistent memory.&lt;/item&gt;&lt;item&gt;&lt;code&gt;memory_write&lt;/code&gt;- Write a value to persistent memory.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;File System Tools&lt;/head&gt;&lt;p&gt;All file system operations are restricted to the current working directory for safety.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;read_file&lt;/code&gt;- Read a text file from disk.&lt;/item&gt;&lt;item&gt;&lt;code&gt;write_file&lt;/code&gt;- Write text to a file (overwrites existing content).&lt;/item&gt;&lt;item&gt;&lt;code&gt;list_directory&lt;/code&gt;- List directory contents including file names, sizes, and modification times.&lt;/item&gt;&lt;item&gt;&lt;code&gt;glob_paths&lt;/code&gt;- Find files and directories matching a glob pattern.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Utilities&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;get_current_time&lt;/code&gt;- Get the current time in ISO-8601 format.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Math &amp;amp; Logic&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;calc&lt;/code&gt;- Evaluate a mathematical expression. Supports arithmetic, comparison, boolean operators, and common math functions.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Code Execution Tools&lt;/head&gt;&lt;p&gt;LLMS includes a suite of tools for executing code in various languages within a sandboxed environment. These tools are designed to allow the agent to run scripts, perform calculations, and verify logic safely.&lt;/p&gt;&lt;head rend="h4"&gt;Supported Languages&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;run_python(code)&lt;/code&gt;- Executes Python code.&lt;/item&gt;&lt;item&gt;&lt;code&gt;run_javascript(code)&lt;/code&gt;- Executes JavaScript code (uses&lt;code&gt;bun&lt;/code&gt;or&lt;code&gt;node&lt;/code&gt;).&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Run Python&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Run JavaScript&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;run_typescript(code)&lt;/code&gt;- Executes TypeScript code (uses&lt;code&gt;bun&lt;/code&gt;or&lt;code&gt;node&lt;/code&gt;).&lt;/item&gt;&lt;item&gt;&lt;code&gt;run_csharp(code)&lt;/code&gt;- Executes C# code (uses&lt;code&gt;dotnet run&lt;/code&gt;with .NET 10+ single-file support).&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Run TypeScript&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Run C#&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h2"&gt;Computer Use&lt;/head&gt;&lt;p&gt;The built-in computer_use extension transforms AI agents into autonomous computer operators. Based on Anthropic's computer use tools, it enables agents to see your screen, control the mouse and keyboard, execute shell commands, and edit files - just like a human sitting at the computer.&lt;/p&gt;&lt;p&gt;This unlocks powerful capabilities that traditional API-based tools cannot achieve:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Visual Verification: Confirm that code actually renders correctly in a browser&lt;/item&gt;&lt;item&gt;Desktop Automation: Control any GUI application - web browsers, IDEs, terminals&lt;/item&gt;&lt;item&gt;End-to-End Workflows: Chain together multiple applications in a single task&lt;/item&gt;&lt;item&gt;Legacy Applications: Automate software that lacks APIs&lt;/item&gt;&lt;/list&gt;&lt;p&gt;For example, an agent can write a web application, open a browser, and capture a screenshot to prove it works:&lt;/p&gt;&lt;p&gt;See the Computer Use docs for complete usage details.&lt;/p&gt;&lt;head rend="h2"&gt;Calculator UI&lt;/head&gt;&lt;p&gt;As some core tools are particularly useful on their own, dedicated UIs has been added for the &lt;code&gt;calc&lt;/code&gt; tool with support for evaluating mathematical python expressions, including arithmetic, comparison, boolean operators, &lt;code&gt;math.*&lt;/code&gt; functions &amp;amp; constants and python list comprehensions&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;üñ•Ô∏è UX Friendly Interface - Clean, modern, responsive UI with dark mode support&lt;/item&gt;&lt;item&gt;üíæ Persistent History - Calculations automatically saved to localStorage and preserved between sessions&lt;/item&gt;&lt;item&gt;‚ö° 1-Click Interaction - Click history items to instantly load expressions and copy to clipboard&lt;/item&gt;&lt;item&gt;‚å®Ô∏è Keyboard-Free Access - Complete UI buttons for numbers, operators, constants, and math functions&lt;/item&gt;&lt;item&gt;üêç Python Math Support - Full access to Python's math library including trig, stats, and more&lt;/item&gt;&lt;item&gt;üõ°Ô∏è Safe Evaluation - AST-based evaluator prevents arbitrary code execution for secure calculations&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Run Code UI&lt;/head&gt;&lt;p&gt;Whilst the &lt;code&gt;run_python&lt;/code&gt; tools provides a scratch pad for running stand-alone Python, JavaScript, TypeScript, and C# code in a sandbox.&lt;/p&gt;&lt;p&gt;The UI uses CodeMirror as the code editor, providing a better user experience with syntax highlighting, code completion, and other IDE-like features for writing code.&lt;/p&gt;&lt;head rend="h2"&gt;Run Python, JavaScript, TypeScript &amp;amp; C# programs&lt;/head&gt;&lt;head rend="h3"&gt;Run Python&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Run JavaScript&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Run TypeScript&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Run C#&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;p&gt;The UI uses CodeMirror as the code editor, providing a better user experience with syntax highlighting, code completion, and other IDE-like features for writing code.&lt;/p&gt;&lt;p&gt;INFO&lt;/p&gt;&lt;p&gt;See the Run Code UI docs for more details.&lt;/p&gt;&lt;head rend="h2"&gt;KaTeX Math Typesetting&lt;/head&gt;&lt;p&gt;The katex extension enables beautiful rendering of LaTeX math expressions in AI responses using KaTeX. It integrates automatically with the markdown parser to render math equations in both inline and block formats.&lt;/p&gt;&lt;head rend="h3"&gt;Features&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Fast Rendering: Uses KaTeX for high-performance rendering of math expressions.&lt;/item&gt;&lt;item&gt;Inline Math: Renders math within text using &lt;code&gt;$&lt;/code&gt;or&lt;code&gt;$$&lt;/code&gt;delimiters.&lt;/item&gt;&lt;item&gt;Block Math: Renders complex equations in their own block using &lt;code&gt;$&lt;/code&gt;or&lt;code&gt;$$&lt;/code&gt;delimiters across multiple lines.&lt;/item&gt;&lt;item&gt;Auto-Integration: Automatically extends the &lt;code&gt;marked&lt;/code&gt;parser used in the application.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Popular math expressions&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h3"&gt;Basic math expressions&lt;/head&gt;&lt;p&gt;Click to view full size&lt;/p&gt;&lt;head rend="h2"&gt;Image Generation Support&lt;/head&gt;&lt;p&gt;Unlike text generation, there's no standard API for image generation across providers - each requires its own custom implementation. Despite the additional effort required, there's now seamless image generation support through both the UI and CLI with built-in integrations for:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Provider&lt;/cell&gt;&lt;cell role="head"&gt;Status&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;‚úÖ Supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;OpenAI&lt;/cell&gt;&lt;cell&gt;‚úÖ Supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;OpenRouter&lt;/cell&gt;&lt;cell&gt;‚úÖ Supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Chutes&lt;/cell&gt;&lt;cell&gt;‚úÖ Supported&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Z.ai&lt;/cell&gt;&lt;cell&gt;‚úÖ Supported&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Nvidia&lt;/cell&gt;&lt;cell&gt;‚úÖ Supported&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;To begin select an image generation model from the Model Selector that supports image generation:&lt;/p&gt;&lt;p&gt;When an image generation model is selected, the chat prompt will the option to specify which aspect ratio to use for the generated images:&lt;/p&gt;&lt;head rend="h3"&gt;Command-Line Usage&lt;/head&gt;&lt;p&gt;Generate images using the &lt;code&gt;--out image&lt;/code&gt; modifier:&lt;/p&gt;&lt;p&gt;Which uses the &lt;code&gt;out:image&lt;/code&gt; chat template in &lt;code&gt;llms.json&lt;/code&gt; for its image generation request. Before returning, any assets are saved to cache and their local path and HTTP URL returned, e.g:&lt;/p&gt;&lt;p&gt;Output:&lt;/p&gt;&lt;head rend="h3"&gt;Specify a Model&lt;/head&gt;&lt;p&gt;Use any model that supports image generation by specifying its ID or name:&lt;/p&gt;&lt;p&gt;INFO&lt;/p&gt;&lt;code&gt;~/.llms/cache&lt;/code&gt; using their SHA-256 hash as the filename.&lt;head rend="h2"&gt;Audio Generation Support&lt;/head&gt;&lt;p&gt;Audio generation is an emerging capability with limited provider support where Text-to-Speech generation through both the UI and CLI, currently only supports Google's latest TTS models:&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Model&lt;/cell&gt;&lt;cell role="head"&gt;Description&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Gemini 2.5 Flash Preview TTS&lt;/cell&gt;&lt;cell&gt;Fast, lightweight TTS&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Gemini 2.5 Pro Preview TTS&lt;/cell&gt;&lt;cell&gt;High-quality TTS&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Typically you'd select the audio generation model from the Model Selector to find models that supports audio generation:&lt;/p&gt;&lt;p&gt;But despite models.dev listing them as capable of audio generation, only Gemini's TTS models are currently supported for audio generation through Gemini's API as Alibaba doesn't yet support the audio modality.&lt;/p&gt;&lt;head rend="h3"&gt;UI &amp;amp; Command-Line Usage&lt;/head&gt;&lt;p&gt;Available in both the UI and on the command-line using &lt;code&gt;--out audio&lt;/code&gt;:&lt;/p&gt;&lt;head rend="h3"&gt;Output&lt;/head&gt;&lt;p&gt;Audio files are saved locally and accessible via HTTP URL:&lt;/p&gt;&lt;head rend="h3"&gt;Playback&lt;/head&gt;&lt;p&gt;From the command line:&lt;/p&gt;&lt;p&gt;From the browser: Run server with &lt;code&gt;llms --serve 8000&lt;/code&gt; to play URL in your browser.&lt;/p&gt;&lt;head rend="h2"&gt;Media Gallery&lt;/head&gt;&lt;p&gt;The gallery extension intercepts all generated image, audio &amp;amp; file assets and uploaded files in &lt;code&gt;~/.llms/cache&lt;/code&gt; file storage whose metadata is maintained in a SQLite database at &lt;code&gt;~/.llms/user/default/gallery/gallery.sqlite&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Dedicated UIs are available for quickly browsing and navigating or generated images / audio files including a lightbox previewer for full-size viewing:&lt;/p&gt;&lt;head rend="h4"&gt;Portrait Images&lt;/head&gt;&lt;head rend="h4"&gt;Square Images&lt;/head&gt;&lt;head rend="h4"&gt;Landscape Images&lt;/head&gt;&lt;head rend="h4"&gt;Audio Generations&lt;/head&gt;&lt;head rend="h2"&gt;System Prompts Library&lt;/head&gt;&lt;p&gt;System prompts support was refactored into a replaceable system_prompts extension which configures AI requests with a library of over 200+ awesome curated system prompts that can be selected from the UI.&lt;/p&gt;&lt;head rend="h3"&gt;Custom System Prompts&lt;/head&gt;&lt;p&gt;You can maintain your own library of system prompts for all anonymous users at: &lt;code&gt;~/.llms/user/default/system-prompts.json&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Or for signed in users at: &lt;code&gt;~/.llms/user/&amp;lt;github-user&amp;gt;/system-prompts.json&lt;/code&gt;&lt;/p&gt;&lt;p&gt;With the JSON file simply containing an array of names and their system prompts, e.g:&lt;/p&gt;&lt;p&gt;Browse the complete collection of available system prompts below:&lt;/p&gt;&lt;head rend="h2"&gt;Server SQLite and Cached File Storage persistence&lt;/head&gt;&lt;p&gt;Another major change is the migration from client-side IndexedDB storage to a robust server-side SQLite databases. This architectural shift ensures better data consistency, improved performance that enables parallel executions and multi-device access to your chat history.&lt;/p&gt;&lt;p&gt;To keep the database efficient and portable, binary assets (images, audio, etc.) are not stored directly in the SQLite database, Instead all generated assets are stored in the local file system cache at &lt;code&gt;~/.llms/cache&lt;/code&gt; and only relative URLs referencing these assets are stored in the database.&lt;/p&gt;&lt;head rend="h4"&gt;Concurrency Model&lt;/head&gt;&lt;p&gt;To ensure data integrity and high performance without complex locking mechanisms, the system utilizes a single background thread to write operations to the database. This design improves concurrency handling and eliminates database locking issues during high-load scenarios.&lt;/p&gt;&lt;head rend="h4"&gt;Multi-Tenancy &amp;amp; Security&lt;/head&gt;&lt;p&gt;When authentication is enabled, data isolation is automatically enforced. All core tables, including &lt;code&gt;threads&lt;/code&gt; and &lt;code&gt;requests&lt;/code&gt;, are scoped to the authenticated user, ensuring that users can only access their own data.&lt;/p&gt;&lt;head rend="h2"&gt;Image Cache &amp;amp; Optimization&lt;/head&gt;&lt;p&gt;A new caching system has been implemented for generated assets and uploaded images and files that's now persisted in &lt;code&gt;~/.llms/cache&lt;/code&gt;, preserving them across messages and sessions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Efficient Storage: Only cache references are stored with chat messages&lt;/item&gt;&lt;item&gt;Persistent Access: Images remain accessible in previews and downloads after page reloads&lt;/item&gt;&lt;item&gt;Automatic Management: System handles file storage and serving transparently&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Now that all persistence is server-side, to transfer or backup your configurations, extensions and Chat History you need only copy your &lt;code&gt;~/.llms&lt;/code&gt; folder.&lt;/p&gt;&lt;head rend="h2"&gt;CLI - more Powerful than Ever&lt;/head&gt;&lt;p&gt;All server extension features including tools, custom providers, database persistence, and image/audio generation are fully accessible via the command line, making llms.py a powerful terminal-based AI assistant.&lt;/p&gt;&lt;head rend="h3"&gt;Core CLI Usage&lt;/head&gt;&lt;head rend="h3"&gt;Tools &amp;amp; Function Calling&lt;/head&gt;&lt;p&gt;All registered tools are automatically available in CLI mode. Enable specific tools with the &lt;code&gt;--tools&lt;/code&gt; flag:&lt;/p&gt;&lt;head rend="h3"&gt;Extensions Management&lt;/head&gt;&lt;head rend="h3"&gt;Provider Management&lt;/head&gt;&lt;head rend="h3"&gt;Image Analysis, Audio Transcribing &amp;amp; Documents Processing&lt;/head&gt;&lt;head rend="h3"&gt;Media Generation&lt;/head&gt;&lt;p&gt;Generate images and audio directly from the command line:&lt;/p&gt;&lt;p&gt;All generated media is automatically saved to &lt;code&gt;~/.llms/cache&lt;/code&gt; with metadata persisted in SQLite.&lt;/p&gt;&lt;head rend="h3"&gt;Database Persistence&lt;/head&gt;&lt;p&gt;All CLI interactions are automatically persisted to &lt;code&gt;~/.llms/user/app/app.sqlite&lt;/code&gt;, including:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Chat threads and messages&lt;/item&gt;&lt;item&gt;Tool calls and results&lt;/item&gt;&lt;item&gt;Generated assets and file references&lt;/item&gt;&lt;item&gt;User preferences and settings&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Ensuring your conversation history is preserved and accessible from both CLI and Web UI.&lt;/p&gt;&lt;head rend="h3"&gt;Server Mode&lt;/head&gt;&lt;p&gt;Launch the web UI while keeping full CLI access:&lt;/p&gt;&lt;p&gt;See CLI Docs for more details.&lt;/p&gt;&lt;head rend="h2"&gt;Upgrade Instructions&lt;/head&gt;&lt;p&gt;Happy holidays from llms.py! üéÑ&lt;/p&gt;&lt;head rend="h2"&gt;Building a Community Extension Ecosystem&lt;/head&gt;&lt;p&gt;With llms .py rebuilt from the ground up as an extensible platform, we hope to foster a thriving community extension ecosystem where developers can share innovative solutions and extend llms.py in ways we haven't yet imagined.&lt;/p&gt;&lt;p&gt;As llms .py is still in active development, we welcome your feedback on any features that would better support 3rd party extensions and help cultivate this growing community.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46766432</guid><pubDate>Mon, 26 Jan 2026 15:01:03 +0000</pubDate></item><item><title>Qwen3-Max-Thinking</title><link>https://qwen.ai/blog?id=qwen3-max-thinking</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46766741</guid><pubDate>Mon, 26 Jan 2026 15:23:00 +0000</pubDate></item><item><title>There is an AI code review bubble</title><link>https://www.greptile.com/blog/ai-code-review-bubble</link><description>&lt;doc fingerprint="a7530c554976ccab"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we're in the hard seltzer era of AI code review: everybody's doing them. OpenAI, Anthropic, Cursor, Augment, now Cognition, and even Linear. Of course, there's also the "White Claws" of code review: pure-play code review agents like Greptile (that's us!), CodeRabbit, Macroscope, and a litter of fledgling YC startups. Then there are the adjacent Budweisers of this world:&lt;/p&gt;
    &lt;p&gt;Amazingly, these two were announced practically within 24 hours of each other.&lt;/p&gt;
    &lt;p&gt;As the proprietors of an, er, AI code review tool suddenly beset by an avalanche of competition, we're asking ourselves: what makes us different?&lt;/p&gt;
    &lt;p&gt;How does one differentiate?&lt;/p&gt;
    &lt;p&gt;Based on our benchmarks, we are uniquely good at catching bugs. However, if all company blogs are to be trusted, this is something we have in common with every other AI code review product. One just has to try a few, and pick the one that feels the best.&lt;/p&gt;
    &lt;p&gt;Unfortunately, code review performance is ephemeral and subjective, and is ultimately not an interesting way to discern the agents before trying them. It's useless for me to try to convince you that we're the best. You should just try us and make up your own mind.&lt;/p&gt;
    &lt;p&gt;Instead of telling you how our product is differentiated, I am going to tell you how our viewpoint is differentiated - how we think code review will look in the long-term, and what we're doing today to prepare our customers for that future.&lt;/p&gt;
    &lt;p&gt;Our thesis can be distilled into three pillars: independence, autonomy, and feedback loops.&lt;/p&gt;
    &lt;head rend="h2"&gt;Independence&lt;/head&gt;
    &lt;p&gt;We strongly believe that the review agent should be different from the coding agent. We are opinionated on the importance of independent code validation agents. In spite of multiple requests, we have never shipped codegen features. We don't write code; an auditor doesn't prepare the books, a fox doesn't guard the henhouse, and a student doesn't grade their own essays.&lt;/p&gt;
    &lt;p&gt;Today's agents are better than the median human code reviewer at catching issues and enforcing standards, and they're only getting better. It's clear that in the future a large percentage of code at companies will be auto-approved by the code review agent. In other words, there will be some instances where a human writes a ticket, an agent writes the PR, and another agent validates, approves, and merges it.&lt;/p&gt;
    &lt;p&gt;This might seem far-fetched but the counterfactual is Kafkaesque. A human rubber-stamping code being validated by a super intelligent machine is the equivalent of a human sitting silently in the driver's seat of a self-driving car, "supervising".&lt;/p&gt;
    &lt;p&gt;If agents are approving code, it would be quite absurd and perhaps non-compliant to have the agent that wrote the code also approve the code. Only once would you have X write a PR, then have X approve and merge it to realize the absurdity of what you just did.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy&lt;/head&gt;
    &lt;p&gt;Something that Greptiles generally agree on is that everything that can be automated, will be automated.&lt;/p&gt;
    &lt;p&gt;Code validation - which to us is the combination of review, test, and QA, is an excellent candidate for full automation. It's work that humans don't want to do, and aren't particularly good at. It also requires little in the way of creative expression, unlike programming. In addition, success is generally pretty well-defined. Everyone wants correct, performant, bug-free, secure code.&lt;/p&gt;
    &lt;p&gt;While some other products have built out great UIs for humans to review code in an AI-assisted paradigm, we have chosen to build for what we consider to be an inevitable future - one where code validation requires vanishingly little human participation. We have no code review UI, and view ourselves as more of a background automation or "pipes" product. Human engineers should be focused only on two things - coming up with brilliant ideas for what should exist, and expressing their vision and taste to agents that do the cruft of turning it all into clean, performant code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Loops&lt;/head&gt;
    &lt;p&gt;Not long ago, we released our Claude Code plugin. It can do many things - but most notably, you can ask Claude Code to pull down and address Greptile's comments from the PR. You can ask it to keep going until there are no new comments, waiting a few minutes for a review after each push.&lt;/p&gt;
    &lt;p&gt;This is a step towards the future we're excited about: Human expresses intent, coding agent executes, validation/review agent finds issues and hands them back - kicking off a loop until it approves and merges. If there is ambiguity at any point, the agents Slack the human to clarify.&lt;/p&gt;
    &lt;p&gt;The question of how these things are different is important. Unlike picking IDEs and coding agents that ostensibly have low switching costs, code review products are harder to rip out, so your decision will very likely turn out to be a long-term one, especially if you're a large company.&lt;/p&gt;
    &lt;p&gt;We've been around for about as long as AI code review has been around. It has gone from a fringe interest of the world's most adventurous vibecoders to a mainstream product that our enterprise users (including two of the Mag7) often describe as a "no-brainer" purchase.&lt;/p&gt;
    &lt;p&gt;Yet, our guess on where this goes is about as good as anyone else's. Meanwhile, we'll keep doing what we've always done - trying to make things our users love.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46766961</guid><pubDate>Mon, 26 Jan 2026 15:38:50 +0000</pubDate></item><item><title>What "The Best" Looks Like</title><link>https://www.kuril.in/blog/what-the-best-looks-like/</link><description>&lt;doc fingerprint="3641d31bb80564ec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What "The Best" Looks Like&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Talent hits a target no one else can hit. Genius hits a target no one else can see.&lt;/p&gt;
      &lt;p&gt;‚ÄîArthur Schopenhauer&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The second that the next round of funding hits the bank, every new CTO starts obsessing over the same thing: Who the hell do I hire next? The answer is surprisingly non-obvious. You‚Äôre told that you always want‚Äîscratch that, need‚Äîthe best of the best, your startup‚Äôs future depends on it. You‚Äôre told your company is unique, special, and it requires the most hardcore among researchers, designers, engineers, and product managers. You never skimp on who builds the golden goose. You can‚Äôt succeed with any less than that.&lt;/p&gt;
    &lt;p&gt;But, is that actually true?&lt;/p&gt;
    &lt;p&gt;Every startup-hustle YouTube video, VC podcast and celeb founder interview regurgitates that the key to success is to hire the very best people, no matter what it takes. And yet you live in the real world, with real-world constraints. You have only so much money in the bank. Only so much time and bandwidth to hire. Only so much attention in a given day. And you‚Äôre not alone in your hunt. You‚Äôre competing with hundreds of other players in your geography, industry, and problem space looking for ‚Äúthe best.‚Äù Many of them have a more famous brand, more cash, more promising equity, more charming founders, and maybe even a high production value promo video showcasing happy employees, rare wood office counters and a shoes-off policy.&lt;/p&gt;
    &lt;p&gt;Will you actually hire the best of the best against those odds? Many years ago I found myself in this pickle and I had to learn all the relevant lessons the hard way. I share these lessons here, so that you don‚Äôt have to struggle through that same maze yourself. My pain is your gain.&lt;/p&gt;
    &lt;p&gt;My first time around the startup world in 2012, I hardly knew what I was doing and relied mostly on luck‚Äîand unfortunately, firing‚Äîto end up with a team I could be proud of. I had no real point of reference for greatness, for what ‚Äúthe best‚Äù in our area could look like, and building that model required lots of experimentation, with high highs and many low lows.&lt;/p&gt;
    &lt;p&gt;The story of David, our brilliant infrastructure ops engineer at Freckle, has stayed with me ever since. David applied to the company back when we were only hiring for an ops role. We were growing slowly, so there was zero room for anybody who wasn‚Äôt absolutely essential on the team. We had no open-ended extra seat for a smart person who just happened to be on the market, unlike some companies these days.&lt;/p&gt;
    &lt;p&gt;David was 100% what we were not looking for. He had never done any ops. In fact, he had never done anything related to web dev or product engineering. I‚Äôm not sure he even knew what AWS was at that point. He was a sharp physics guy working with a professor on simulations in an academic context. He didn‚Äôt let that deter him. He wanted to work at Freckle, thanks to our reputation as one of the few software startups in the world using Haskell in production at scale. We were an odd outlier in a sea of buggy and laborious Rails apps, a shelter for people who didn‚Äôt want yet another web slop gig. And Haskell was oh-so-hot in the Hacker News programming language theory space at the time, a technology attracting software nerds obsessed with correctness and new, better ways of developing bug-free apps.&lt;/p&gt;
    &lt;p&gt;I immediately told David that he was not a fit; he had none of what we were looking for. And yet he persisted, emailing me that he would do whatever test project we threw at him, and if he bombed it, no problem, he would go away. But if he nailed it, we would have proof that he was qualified, in spite of what his CV indicated. Fine. I sent him a meaty cloud ops take-home project, expecting to never hear from him again. Importantly, this was in the days before you could have Claude Code slap that together for you in two prompts.&lt;/p&gt;
    &lt;p&gt;A day or two later, he returned the project to us, and it was pretty much flawless, doing even more than we had asked for. That was not expected. I got curious about what else he could do. We weren‚Äôt drowning in applicants anyway, so I figured I didn‚Äôt have much to lose. We took him through the usual interview process. He was humble, optimistic, well-spoken, actively communicating and taking feedback well, eager to get to work. He was pumped about everything he could learn on the job, about the doors that would eventually open if he nailed it. He didn‚Äôt have much experience, as someone who had never written commercial software before, but he was really quick to absorb everything we threw at him.&lt;/p&gt;
    &lt;p&gt;We gave him a chance. As predicted, he was stellar, and we had a really good run with him until he moved on to a much more illustrious employer. A few years later he became a senior principal engineer at Stripe, going from a physics lab, then a starry-eyed K-12 software startup, to being a big deal at one of Silicon Valley‚Äôs finest firms. An unsurprising path for one of ‚Äúthe best.‚Äù&lt;/p&gt;
    &lt;p&gt;While building the different companies I worked at, I‚Äôve run several times into ‚ÄúDavids‚Äù who ended up with meteoric career trajectories, sometimes already pedigreed in all the right ways, sometimes completely invisible to everyone but the trained eye of a CTO looking for gold. What is special about someone like that? And how can you, with your humble hiring budget, identify them before their value is obvious to everybody else in the market?&lt;/p&gt;
    &lt;p&gt;With David, it was obvious that I had lucked my way into finding him. He had to badger me into seeing what he was capable of, at a time when I was only looking for obvious signs of success. Later on at my game studio, thanks to a brutally skewed job market, I had the total pick of the litter during Hiring Summer and could select one or two stellar engineers from hundreds of perfectly reasonable applicants. There was hardly any competition, with money drying up all around the industry. Again, I found a diamond in the rough. But this time I had the right knowledge and strategies in place to end up with the kind of team I wanted with far less reliance on luck. Even as I was drowning in hundreds of resumes that all started to look the same, ‚Äúthe best‚Äù candidate was still in there, like a needle in a haystack. This time, I felt like I was actually skilled enough to find that hidden gem, not simply crossing my fingers and relying on luck alone. That‚Äôs a feeling of empowerment I would like you to experience as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;What do we mean by ‚Äúthe best‚Äù?&lt;/head&gt;
    &lt;p&gt;Of course, when every company says they have the best people, the math doesn‚Äôt work out. And of course, if they regularly have to fire staff, something doesn‚Äôt add up again. One day you end up landing a job at one of these companies and realize that the braggadocio was hardly reflective of reality, but then again, it‚Äôs all part of the startup founder LARP that requires you play the part. ‚ÄúOur team is okay, you will probably like working here, we sometimes know what we‚Äôre doing‚Äù would be a much more accurate depiction, but you‚Äôll never find that quote on the careers page.&lt;/p&gt;
    &lt;p&gt;Regardless, you want to be a good Boy Scout CTO and live up to the lofty expectations set for you by the Silicon Valley elders, and at least try to hire ‚Äúthe best of the best‚Äù as they say in all of the fireside chats.&lt;/p&gt;
    &lt;p&gt;But then, a question naturally emerges: what exactly do we mean by the best? The accolades? Pedigrees? Github stars? Hacker News front pages? Job title? Celebrity employers? Number of former YCombinator companies worked at? Hackathon wins? Typing speed? Clout among other Rust developers wearing fuzzy animal costumes? That all sounds good, I guess, but is that all relevant to your company?&lt;/p&gt;
    &lt;p&gt;It turns out that ‚Äúthe best‚Äù is mostly subjective and specific to your situation. The culture, the vibe, the industry, the processes of your company, and the technical choices will all influence who will be a phenomenal addition to your tribe. That person might not be the same individual bringing in a million or two a year at Meta. In fact, that highly pedigreed, highly connected individual might be a net negative for your company, even though they might, at least on paper, seem the most high-end among candidates. They may blow the mind of a recruiter at Netflix, but they may not want to schlep through all of the messiness and chaos of an early stage company still trying to define itself. They‚Äôre not the best for you, and they‚Äôre likely not the best for startups, but plenty of other people are. Your job is to determine what those traits are and where to find people who have them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universally Best Startup Hires&lt;/head&gt;
    &lt;p&gt;Your company, timing, industry, problem space and founding team personalities are unique, which is why a generic blog post or a book could never tell you exactly what hires are optimal for you. However, experience shows that there are many universal winning commonalities between great hires that will be applicable regardless of your early stage company‚Äôs unique DNA. I identify 11 of them below.&lt;/p&gt;
    &lt;p&gt;The 11 traits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-obviousness&lt;/item&gt;
      &lt;item&gt;Not-too-junior&lt;/item&gt;
      &lt;item&gt;Not-too-senior&lt;/item&gt;
      &lt;item&gt;Hunger&lt;/item&gt;
      &lt;item&gt;Humility&lt;/item&gt;
      &lt;item&gt;High EQ&lt;/item&gt;
      &lt;item&gt;Team-centricity&lt;/item&gt;
      &lt;item&gt;Competence&lt;/item&gt;
      &lt;item&gt;High-agency problem-solvers&lt;/item&gt;
      &lt;item&gt;Cross-disciplinary empathy&lt;/item&gt;
      &lt;item&gt;Tolerance of uncertainty&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a side note, I found Patrick Lencioni‚Äôs framework from The Ideal Team Player to survive first contact with both pre-seed and Series A+ realities. It‚Äôs one of those dry-as-dust HBR book club management self-help guides that get an eyeroll by your median 20-year-old founder going through YCombinator, but in my experience it confirmed and put a simple model around something I had personally seen emerge again and again in the field.&lt;/p&gt;
    &lt;p&gt;Lencioni identifies three essential traits: hunger, humility and smarts, the latter of which should have been called EQ all along. You‚Äôll see them referenced below. The three criteria may seem obvious at a glance, but having a simple framework to work with as you‚Äôre making hundreds of decisions makes a big difference. Simple things done right turn out to be pretty darn powerful.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take a closer look at the 11 commonalities among great startup hires I‚Äôve identified. Some of them are simply statistically likely to occur due to the nature of startup hiring. Others are ones you should straight up filter for when you‚Äôre doing your search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Non-obviousness&lt;/head&gt;
    &lt;p&gt;The best hires in the early stages are usually non-obviously good to the untrained eye. They don‚Äôt look as appealing to employers with infinite resources who otherwise would have already hired them. If they were obviously incredible, it would be unlikely you‚Äîat your startup‚Äîwould ever talk to them.&lt;/p&gt;
    &lt;p&gt;One of the most important skills of a startup CTO trying to hire an amazing team is the ability to uncover awesome talent that others have overlooked. These candidates get skipped often due to being too off-the-beaten-path and under-pedigreed compared to the obvious picks that every other company is making. Often these candidates are bad at marketing themselves, don‚Äôt know where to look, and are off-putting in some way to the naive search. You have an opportunity to take advantage of that oversight.&lt;/p&gt;
    &lt;p&gt;Realistically, someone who looks like the perfect FAANG candidate who has won all of the math olympiads, had all of the stellar internships, and went to the top CS schools will either:&lt;/p&gt;
    &lt;p&gt;a. get a big cash dump from YC and start their own thing, or&lt;lb/&gt; b. go work at one of the career-building brand names in the Valley and make a monstrous amount of money.&lt;/p&gt;
    &lt;p&gt;The karmic wheel is just about cycling between those two until that person either gets into VC or becomes an exec at a prestigious firm years later. If that‚Äôs what the universe has in store for a candidate, taking a chance on a no-name team with 12 months in the bank, poor development practices and sloppy management is a tough proposition.&lt;/p&gt;
    &lt;p&gt;That leaves a pretty large pool of ‚Äúeverybody else‚Äù who didn‚Äôt pattern-match. How you go about sifting through them is a big topic I will leave for another chapter, but the key is to look for signals that are less obvious than a Stanford degree and an OpenAI internship. Often that looks like a large volume of work, expertise in unsexy niche areas of technology, an intense work ethic driven by curiosity, and many others.&lt;/p&gt;
    &lt;p&gt;A Stanford CS degree is no guarantee someone will be a phenomenal contributor at your company, but it‚Äôs a reasonable proxy of future potential for large employers with long time horizons that allow them to invest in coaching and nurturing their junior staff. That‚Äôs not in the cards for a pre-revenue startup that will run out of money next year.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not-too-junior&lt;/head&gt;
    &lt;p&gt;The best people for your startup will most likely be senior, or at least mid-stage contributors with several solid years of experience. Too senior isn‚Äôt great either, you don‚Äôt need the large scale cross-team mature product jousting chops. There‚Äôs likely no staff-level work for them to do, and staff-level engineers aren‚Äôt just faster-typing seniors.&lt;/p&gt;
    &lt;p&gt;As an early stage startup, until you have enough lead developers, senior product managers, senior designers etc. to set the standard, hiring fresh-out-of-school contributors is a gamble. Junior staff are still developing their taste, judgment, ability to work in a team, and understanding of how to follow the process and when to deviate from it. Without an adult in the room you‚Äôre in cat-herding territory. This isn‚Äôt to say that you won‚Äôt get work done this way, but you could have had someone else in that seat who required less supervision. And you‚Äôre at a stage where every seat is mission-critical and the opportunity cost is significant.&lt;/p&gt;
    &lt;p&gt;While it might not matter too much as you throw prototype spaghetti at the wall in the early days, as soon as you have something worth maintaining and complexity rises, you‚Äôre now in a race against time. Someone must actively garden the complexity of the work and be ultimately responsible for it, and that someone will tend to be not fresh-out-of-school. Sure, a green dev can care about complexity. But most haven‚Äôt yet lived through a soul-crushing, multi-week refactor fueled by years of tech debt, the kind of preventable trauma that earns you ‚Äúcharacter-building scars.‚Äù If they‚Äôre learning those lessons on your watch, they‚Äôre doing it at the expense of your delivery timeline and your sanity.&lt;/p&gt;
    &lt;p&gt;I‚Äôm fond of mid-stage software developers who demonstrate terrific chops, hunger to learn and prove themselves. Yes, they require more hand-holding, but usually the volume of work they put out, and their openness to re-do it, if needed, is worth betting on. You should be able to manage a couple of them yourself as the CTO in the early days, and soon enough you‚Äôll have other senior people on the team who will pick up the mentorship torch.&lt;/p&gt;
    &lt;head rend="h3"&gt;Not-too-senior&lt;/head&gt;
    &lt;p&gt;This also means that the ‚Äúbest people‚Äù to hire at this stage will tend to be earlier in their careers. The longer great people are on the market, the more likely they are to be identified by the market as being great. Your job as a startup CTO is to find them before others do. Once the market has found them and has actively started rewarding them, they will be out of your price range.&lt;/p&gt;
    &lt;p&gt;Now, it‚Äôs worth mentioning that in specific well-capitalized niches of the startup market, companies are now able to pay a decent chunk of change thanks to larger VC rounds and quicker time-to-revenue. Thus, the old school notion that BigCo always pays best may not hold true as often. Here‚Äôs a quick sample of many Work at a Startup roles in December 2025. Not quite at FAANG-level, but far from starvation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Count&lt;/cell&gt;
        &lt;cell role="head"&gt;Mean&lt;/cell&gt;
        &lt;cell role="head"&gt;Median&lt;/cell&gt;
        &lt;cell role="head"&gt;Min (Midpoint)&lt;/cell&gt;
        &lt;cell role="head"&gt;Max (Midpoint)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Regular SWE&lt;/cell&gt;
        &lt;cell&gt;45&lt;/cell&gt;
        &lt;cell&gt;$157.4K&lt;/cell&gt;
        &lt;cell&gt;$150.0K&lt;/cell&gt;
        &lt;cell&gt;$85.0K&lt;/cell&gt;
        &lt;cell&gt;$275.0K&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Senior SWE&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;$209.8K&lt;/cell&gt;
        &lt;cell&gt;$202.5K&lt;/cell&gt;
        &lt;cell&gt;$140.0K&lt;/cell&gt;
        &lt;cell&gt;$350.0K&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Founding SWE&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
        &lt;cell&gt;$189.3K&lt;/cell&gt;
        &lt;cell&gt;$185.0K&lt;/cell&gt;
        &lt;cell&gt;$125.0K&lt;/cell&gt;
        &lt;cell&gt;$255.0K&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;That also means that, unless your company grows fast or shows tremendous potential for the yet-unvested equity, you won‚Äôt get a ton of time with your great early career hires. They will quickly accumulate valuable experience and prove themselves to be stellar, and move on to a prestigious employer with longer time horizons, a great brand name, and a stupendous paycheck that you will not be able to match. Being an L5-7 at BigCo pays a pretty penny, with none of the unpredictability of startup equity and saner hours, but none of the adventure and camaraderie of a pirate ship. For employees later in their careers, that‚Äôs not a bad tradeoff. Once they have vested with your company, it makes sense for them to diversify their equities, as they already own one lottery ticket.&lt;/p&gt;
    &lt;p&gt;Being on the receiving end of a reference call for one of your star employees, while they‚Äôre still working at your company, is always bittersweet: on one hand you want what‚Äôs best for them, to level up and grow in their career. On the other, they‚Äôre your discovery‚Äîdang it‚Äîand you know you won‚Äôt be able to counter-offer the type of employer they can now attract. Or they decide to start their own company, emboldened and informed by the experience at yours.&lt;/p&gt;
    &lt;p&gt;In the end, that‚Äôs okay. Sometimes a great hire‚Äôs growth trajectory is faster than a startup‚Äôs, and you can be grateful for having given them a chance to prove themselves and to find a new path that they wouldn‚Äôt have otherwise. That kind of hire will have likely made a major difference to your company and having trusted alumni out there in the world doing great things should be a source of pride and good industry karma for any CTO who discovered them.&lt;/p&gt;
    &lt;p&gt;Seniority here also unfortunately often correlates with age. The best startup employee will usually be someone early in their career who doesn‚Äôt have as many responsibilities or as much need for consistency due to having more dependents. They may have fewer immediate cash flow constraints, fewer ‚Äúadult responsibilities.‚Äù Kids need braces and karate classes, and if Mom is doing 996 at a ten-person company paying her peanuts, offering a crappy health care plan, promising an epic payout ten years from now, that‚Äôs a real mismatch. Startups are an extreme sport, and generally inadvisable for anybody who‚Äôs not in a safe position to speculate on their career for several years.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hunger&lt;/head&gt;
    &lt;p&gt;The best hires are self-driven, stoked by the ability to learn, to gain mastery of the craft, and by being able to show off their accomplishments to the rest of the team they want to impress. When you look at their track record, even when it‚Äôs not filled with household-name accolades and pedigrees, you will typically see breadcrumbs of exploration, toy projects and experimentation fueled by curiosity. It‚Äôs that classic desire to hack things and understand them.&lt;/p&gt;
    &lt;p&gt;You don‚Äôt need to ask them to work a little harder, they know this is a big growth spurt for them and they want to take advantage of it. The company is giving them a valuable practice canvas for their skills and they want to make the most of this opportunity, which unfortunately isn‚Äôt that common. They love a challenge, they want to prove themselves, they love the work and they are excited by being around others who will push them to be their best. They would have been hacking on something interesting either way, but now you‚Äôre actually paying them to do it.&lt;/p&gt;
    &lt;p&gt;They‚Äôre curious about how everything works: your company, the startup world, the industry they‚Äôre diving into, other disciplines, the tech they‚Äôre using. They‚Äôre learning, absorbing. Maybe they want to start their own thing one day, or maybe they‚Äôre set on using this gig as a ramp to something else they dream about. Maybe they just want the new optionality granted by having your company on their resume. They‚Äôre sponges and will gladly take on the challenges you send their way.&lt;/p&gt;
    &lt;p&gt;I remember throwing a big Redux experiment at David in what must have been his first month at work. Again, no web dev experience prior to this. Nobody on the team knew the technology and he, hired as an infrastructure engineer with a background in physics and zero product engineering experience, had something up and running for us within days. He was pumped he got to do something so radically new for the team as one of his first assignments. Ultimately we decided the tech wasn‚Äôt a fit for our existing codebase, we scrapped the experiment, and he ran off to the next challenge with no loss of enthusiasm.&lt;/p&gt;
    &lt;p&gt;At Double Dusk we could not figure out why players would regularly de-sync their character positions on the server when using a custom‚Äîand wildly complicated, due to needing to replicate Half Life 2-style movement to the smallest detail inside of Unreal Engine‚Äîcharacter movement component in combination with our own Unreal Engine networking tweaks, making character movement authoritative on client instances of the game. A big deviation from the defaults.&lt;/p&gt;
    &lt;p&gt;Our newly-hired star engineer, us being his first ever employer, had to dive into the pit of madness. He almost gave up several times before finally identifying all of the spots in the physics sub-stepping logic that was causing the drift across the network. I‚Äôm confident I myself would have gone completely bananas trying to debug that one. He was proud of the trust we had put in him, he was eager to learn, he didn‚Äôt want to let the team down, and I knew enough about him to make the bet that the challenge would have been just barely within his abilities.&lt;/p&gt;
    &lt;p&gt;Hunger is a massive force-multiplier, and unlike many other traits, it doesn‚Äôt seem to be teachable. Failures will instill humility. Rejection will hone your EQ. But you‚Äôre either driven to compete, learn and prove yourself, or you‚Äôre not. No manager‚Äôs pep talk will kickstart that drive in you, at least not sustainably. The hubris has to come from somewhere much deeper.&lt;/p&gt;
    &lt;head rend="h3"&gt;Humility&lt;/head&gt;
    &lt;p&gt;Startups are all about rapid experimentation, trying countless ideas that don‚Äôt work, and bootstrapping the skills and processes of the company as it figures out just what exactly people will pay for. Trust-building is critical in any team endeavor, and members who are unable to admit fault, who take up all of the space, and/or who need to be right at all costs will be a real problem as the company matures.&lt;/p&gt;
    &lt;p&gt;The best hires are humble and will happily talk about both their victories and their biggest implosions, about what they learned from their misadventures, about talented past coworkers and how their efforts were part of a team. They rarely claim to have single-handedly carried everything on their backs.&lt;/p&gt;
    &lt;p&gt;As you interview candidates searching for the best, many non-obvious great hires will be bad at behavioral interviews‚Äîor interviews in general, for that matter. They didn‚Äôt get extensive training on answering questions using the STAR method, didn‚Äôt drill stock responses to predictable and tired interview questions. They‚Äôre early-ish stage hands-on technology people, not smooth-talking execs jumping from one boardroom to the next. It is your job to fish for key nuggets of insight buried in the rough presentation. After you interview enough people with a similar set of standard questions, you‚Äôll be able to spot the outliers in the bell curve of responses.&lt;/p&gt;
    &lt;p&gt;An underrated aspect of humility in startups is detachment from one‚Äôs work. Companies in the process of inventing themselves need to actively cycle through many ideas, most of which will ultimately not stand the test of time. Design for the trash can and kill your darlings are important mindsets in creative work, and they extend far beyond music, film and games. Quantity leads to quality, and a great hire will accept regularly needing to apply a flamethrower to their work and try again with the new learnings from the latest experiment. They will not be precious about it, nor will they feel diminished by needing to try again. The Davids do not keep score; they keep trying and do the work knowing that there‚Äôs a good chance it will not go anywhere.&lt;/p&gt;
    &lt;p&gt;That is also highly correlated with their ability to take feedback without taking it personally. They‚Äôre not afraid of criticism and quick feedback loops. In fact, they seek disconfirmation sooner rather than later. They have excellent feedback metabolism.&lt;/p&gt;
    &lt;head rend="h3"&gt;High EQ&lt;/head&gt;
    &lt;p&gt;Building technology companies is a team sport. Everything you do is with the help of other people, for the benefit of other people. As much as the startup world fetishizes the cracked ninja jedi 10x code-poet, if that person makes others never want to talk to them again, cracked they are not. Someone who is able to work with others‚Äô quirks, understand how they operate and what they respond to, and act in a way that brings the best out of their teammates is worth their weight in gold. Brilliant jerks, as great as they are for producing riveting drama in TV shows, quickly become a net negative in the real world.&lt;/p&gt;
    &lt;p&gt;We‚Äôre in the squishy human feeling territory here (aka ‚Äúsoft skills‚Äù) and you can‚Äôt LeetCode your way into knowing if someone will be an ass. There‚Äôs no FizzBuzz for empathy. You have to ask them standard questions, shoot the breeze, interact during the various testing phases of the interview, and outside, and ultimately make a call based on the limited data you have. You will never have all of the evidence, but ultimately you have to make a go or no-go call. Sometimes you get lucky and the candidate discloses a consistent pattern from their past that is obviously disqualifying, but often you don‚Äôt.&lt;/p&gt;
    &lt;p&gt;How they interact with you and your team throughout the interview is usually a good data point, and that, of course, goes both ways. Regardless of how you investigate this side of them, it‚Äôs non-negotiable, and the best hires you‚Äôll make will all have a high EQ.&lt;/p&gt;
    &lt;head rend="h3"&gt;Team-centricity&lt;/head&gt;
    &lt;p&gt;At the same time, someone highly empathetic, but who hides in a cave and is unable to coordinate their work with the rest of the software orchestra, is not the best.&lt;/p&gt;
    &lt;p&gt;Great team players anticipate others‚Äô questions and concerns, and are proactive about reaching out and communicating both their status and the progress of their work to those around them. They make their work visible instead of requiring constant polling for the team to figure out what exactly is going on with them.&lt;/p&gt;
    &lt;p&gt;Remote environments in particular benefit from strong proactive communicators, as you can‚Äôt get away with always knowing what‚Äôs going on with everybody due to sitting in the same room for most of the week. One great Loom is worth a thousand words, but a thousand words is still much better than having to pull status out of people.&lt;/p&gt;
    &lt;p&gt;Early stage startups move at a pace that generates tremendous entropy, and someone who‚Äôs able to coordinate with the rest of a team in a way that feels natural and effortless will allow the small teams to scale without requiring project management and complex processes meant for lower-performing contributors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Competence&lt;/head&gt;
    &lt;p&gt;Perhaps counterintuitively, I don‚Äôt fret too much about competence. To me, that‚Äôs a given. If your interview funnel is set up well, with a great take-home project, an onsite, or even a work trial, determining if the person you‚Äôre interviewing is competent should emerge naturally. After all, you and your team are technical experts, craftspeople who can look at another craftsperson‚Äôs work and quickly judge if it is any good.&lt;/p&gt;
    &lt;p&gt;Yes, AI is getting pretty good these days and it‚Äôs becoming easy for candidates of all disciplines to hide behind prompts, but that‚Äôs only an extra reason to allow them to bring those tools into the interview itself and showcase their use rather than shamefully hide their existence. How they use the agents, the back-and-forth, the planning, the tweaking, the types of searches they do as part of the exercise are all valuable data points for determining competence, and you should allow candidates to surface them.&lt;/p&gt;
    &lt;p&gt;In-person interviews and work trials are for now non-gameable, so if you‚Äôre extra paranoid, they are a great option. Non-obvious candidates will be in the hiring race with fewer other companies, possibly none, depending on your industry. That gives both you and them more flexibility and time to try working together for a few days before making it official. With a sufficiently relevant set of tasks for the work trial, you can rest assured that they‚Äôre competent, and learn a lot more about their other traits in the process.&lt;/p&gt;
    &lt;head rend="h3"&gt;High-agency problem-solvers&lt;/head&gt;
    &lt;p&gt;The best start-up hires are problem-solvers, not ticket resolvers. They want to understand what the business is trying to accomplish, what challenge is standing in the way of it, and how to make that problem go away. They‚Äôre autonomous in the best of ways. They don‚Äôt sit around waiting for someone to tell them what to do; they proactively find challenges and opportunities for improvement, iterate on the feedback, and go out of their way to help their teammates. They figure things out, and they do it without needing their hands held.&lt;/p&gt;
    &lt;p&gt;Someone like David didn‚Äôt take no for an answer; he assumed there had to be a narrow path through the rejections he kept getting from me, that it was only a matter of his resourcefulness before he found a crack through which to get what he wanted.&lt;/p&gt;
    &lt;p&gt;The communication, coordination and cohesion overheads that emerge with the addition of more and more staff to the roster are probably the toughest parts of scaling any business. A leader‚Äôs job is to streamline and remove these emergent dependencies between people and teams as much as possible.&lt;/p&gt;
    &lt;p&gt;Besides keeping the number of hires low, the other powerful lever that a start-up CTO has is to hire people who will be self-sufficient and require little to no support to get their jobs done. They will ask for help when necessary, but they will take pride in figuring things out on their own, checking in with you when the time is right. They will require little supervision overall, and only direction when it comes to their work. If they‚Äôre falling behind, being great communicators, they will let the rest of the team know. If they‚Äôre early, they‚Äôll gladly pick the next interesting opportunity to be useful.&lt;/p&gt;
    &lt;p&gt;If they‚Äôre struggling, their pride in their work will make them double-down and overcome the challenge at all costs. In that case your job is to help them pump the brakes when necessary, or their pace can become unsustainable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-disciplinary empathy&lt;/head&gt;
    &lt;p&gt;A great startup hire doesn‚Äôt only think about their respective lane, but cares about the other disciplines around them that their work is impacting. They understand their angle, their priorities, and their expectations. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A back-end engineer anticipates the needs of the front-end and of the infrastructure developers as they make their changes rather than waiting for their work to be pushed back once it doesn‚Äôt meet the standard.&lt;/item&gt;
      &lt;item&gt;A gameplay engineer doesn‚Äôt wait for the game designer to tell them that the feel of what they implemented won‚Äôt fly with the player. They put themselves in the shoes of the other discipline and think like them.&lt;/item&gt;
      &lt;item&gt;A front-end engineer doesn‚Äôt just roll out the interface they‚Äôre working on, they think through the UX and the UI and the usability and the product management side.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They think about where and how their work will land and anticipate those objections, addressing them in advance. They proactively reach out to those disciplines to avoid yet another coordination chokepoint that will only gobble up time. As more and more skills are compacting into one single individual, we see AI engineers, product engineers, and product managers becoming more technical and moving downstream into programming. This sort of cross-disciplinary mindset buys companies a ton of leverage and ability to iterate faster.&lt;/p&gt;
    &lt;p&gt;Of course, this becomes progressively easier with experience and seniority. But that‚Äôs also why startups moving fast benefit from more mature contributors as time goes on. The reduced communication costs multiplied across more and more team members really add up over time. Unless you have very specific technical needs, having a generalist bias in the early days is a great idea. In fact we‚Äôre seeing more and more disciplines get compressed into one, with software developers covering UI, UX, PM and engineering, effectively becoming a cross-disciplinary team of one. Even in later stages, half-a-pizza teams powered by AI can move mountains on timelines that felt impossible just a few years ago.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tolerance of uncertainty&lt;/head&gt;
    &lt;p&gt;Early-stage startups are notoriously chaotic and unpredictable. You might be working on one feature one day, it might get cancelled the next day, and maybe now you‚Äôre fielding customer support calls for something you shipped that accidentally blew up. The next day you‚Äôre taking a trip to a customer site to chat with potential users.&lt;/p&gt;
    &lt;p&gt;At no point do you know if any of this will make you money, how long until you kill that feature, and how many more of these iterations you‚Äôll do before you either strike gold or the company runs out of money. There are no guarantees in the startup world, except for the fact that your runway will eventually reach zero if you don‚Äôt find something worth selling.&lt;/p&gt;
    &lt;p&gt;As a CTO, it is your responsibility to shield your team from the messy everyday financial reality of where your company is headed. At the same time, I prefer to keep as much as possible in the open so that the team knows how much longer the music will play and when it‚Äôs time to start refreshing their LinkedIn profiles. I never want anything to be a surprise to the team when I could have been candid about it far in advance. It‚Äôs a fine balance between hiding the daily volatility‚Äîmostly of the founders‚Äô moods and their confidence in the company making it‚Äîand exposing the long-term trends.&lt;/p&gt;
    &lt;p&gt;This type of universe requires people who are okay with fewer guarantees.&lt;/p&gt;
    &lt;p&gt;A startup will never have the time to run comprehensive studies, to build extensive plans, to gain all of the information necessary to make the right decision. Seeking perfection at the cost of an action is generally unacceptable, and you learn much more by failing in a valiant attempt than in delaying a perfect attempt that you might have validated months earlier by just doing the thing.&lt;/p&gt;
    &lt;p&gt;Great hires respect that process and are willing to operate without all of the information, accepting that the team will figure it out as it moves forward, often deviating from the original objective. Deleting existing artifacts or completely pivoting to a new strategy on a dime is the ultimate startup superpower.&lt;/p&gt;
    &lt;p&gt;Anybody who needs well-spelled-out plans and continuity in whatever they‚Äôre doing will ultimately struggle in that kind of environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Wrap-up&lt;/head&gt;
    &lt;p&gt;Hiring is a gamble, but you can tilt the odds in your favor if you stop playing the same game as everyone else. The ‚Äúbest‚Äù hire isn‚Äôt the person with the most GitHub stars or the flashiest resume; it‚Äôs the person who makes your team better by existing within it. It‚Äôs the David who breaks down doors and ends up shaping the direction of the company through their relentless contributions.&lt;/p&gt;
    &lt;p&gt;Developing an eye for non-obvious talent is the only way to build a high-leverage team on a startup budget. Yes, it takes more work. It requires you to actually pay attention. It‚Äôs one of the hardest feats to pull off in startup team building. It means you must trust your own judgment over a candidate‚Äôs credentials. But if you do it right, you don‚Äôt just get a ‚Äúnice‚Äù team, you get a team capable of doing the impossible. Now stop reading and go find ‚Äúthe best‚Äù.&lt;/p&gt;
    &lt;p&gt;| #cto #startups #leadership #management #hiring #recruiting #talent-acquisition #company-building&lt;/p&gt;
    &lt;p&gt;Subscribe to the newsletter&lt;/p&gt;
    &lt;p&gt;Hard-won lessons on software, startups, and leading teams.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46767323</guid><pubDate>Mon, 26 Jan 2026 16:04:07 +0000</pubDate></item><item><title>RIP Low-Code 2014-2025</title><link>https://www.zackliscio.com/posts/rip-low-code-2014-2025/</link><description>&lt;doc fingerprint="b5503f7a49f409a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RIP Low-Code 2014-2025&lt;/head&gt;
    &lt;p&gt;The rise of AI and particularly agentic development presents an existential threat to an entire category of low-code platforms. While the adoption of new techniques and tooling will take years to propagate through the Byzantine ranks of larger, slower-moving enterprises, the fundamental ROI case for these tools looks different in a world where the cost of shipping code now approaches zero.&lt;/p&gt;
    &lt;p&gt;This may seem like a preposterous conclusion given the substantial size and growth of the sector. Forrester, who actually gave low-code its name back in 2014, projects the category will reach $50b by 2028 and sees no current indication that things will slow down, let alone contract. However, it‚Äôs worth digging in to why these tools arose in the first place and the problems they solve to explore how much the landscape has shifted in just the past year.&lt;/p&gt;
    &lt;p&gt;Put simply, these software platforms exist to allow users to create software with fewer developer resources. By purchasing one of these platforms, a company can enable non-technical stakeholders to ship production-ready experiences, often with little to zero actual code being written. This frees up developer bandwidth, accelerates the company, and until recently was a no-brainer investment for building internal and even customer-facing software.&lt;/p&gt;
    &lt;p&gt;To enable these platforms in the real world, developers spend considerable time on prerequisite and ongoing work: piping and transforming data, writing and maintaining custom components that go beyond out-of-the-box functionality, and meshing authentication systems, to name a few. This investment is in turn justified by the reduction in development scope and complexity downstream of the low-code platform‚Äînon-technical users can be left to their own devices to ship to their hearts‚Äô content.&lt;/p&gt;
    &lt;p&gt;With the emergence of AI coding, this ROI case gets inverted. It is now often faster, cheaper, and easier to ship the kind of tools you might have built with low-code tools outside these platforms. Yes, this still requires developer time, but so did enabling these low-code-platforms in the first place. Even disregarding the financial and organizational costs of low-code tools, AI affords developers the conveniences of their regular workflows without the bolt-on complexity introduced by external platforms. When you add in the total cost of ownership of these low-code tools, a return to in-house tooling becomes even more attractive.&lt;/p&gt;
    &lt;p&gt;As an illustration of what‚Äôs possible, we‚Äôve seen this play out in real-time at Cloud Capital. In the not-too-distant past, we relied heavily on a low-code platform called Retool for almost all of our internal Admin tooling. We built management dashboards, reporting, and orchestrated complex workflows that were critical to the business. The acceleration was real‚Äîour developers spent significantly less time rolling boilerplate tables, transforming data, and wiring up workflows. We even celebrated at our All Hands how much better our dashboards felt than if we‚Äôd hand-rolled them or used a pre-canned admin interface.&lt;/p&gt;
    &lt;p&gt;Then came the agentic tools that completely transformed the way we develop software. For our low-code tooling, the shift began with a single choice to prototype some new, self-contained functionality as a standalone internal tool instead of via our low-code platform. It was faster, easier, and leveraged our actual codebase in ways an external solution could not. That meant we shipped something safer, more robust, and more maintainable. The cherry on top is that the end product was also better‚Äîthe UI looked and felt more like our in-house products, without the clunkiness required to stay on the WYSIWYG rails.&lt;/p&gt;
    &lt;p&gt;It felt immediately clear that we had identified an unlock for our internal tooling velocity. All of a sudden, we found ourselves feeling constrained by the same low-code tools that until so recently were unblocking us. Changes that would have been one-liners in Cursor or automated triage tickets handled by an agent meant logging in to another platform, moving around clunky UX blocks, bashing against version management systems that weren‚Äôt quite as polished or integrated as our core development flows. All with the additional cost of maintaining this additional system.&lt;/p&gt;
    &lt;p&gt;What started as a single tool quickly became a wholesale migration of all of our Admin tooling, and the inevitable sunsetting of our Retool instance. They hadn‚Äôt changed, but our culture and way of working had, and low-code couldn‚Äôt keep up. What was most shocking was the timeline of this change‚Äîfor us as a small, fast-moving startup, the transition including migration played out fully in just a couple of sprints.&lt;/p&gt;
    &lt;p&gt;It‚Äôs inevitable that incumbent low-code tools will adapt‚Äîthey will need to in order to survive. In many cases, this shift is already visible in their marketing, such as Retool‚Äôs new AI-heavy positioning:&lt;/p&gt;
    &lt;p&gt;At this point, it‚Äôs hard to say whether it‚Äôll be enough. While it‚Äôs possible low-code platforms will survive by providing non-technical users with the kind of magical experience that‚Äôs already possible for developers with AI coding tools today, it also seems likely they will continue to cede market share to the core AI players themselves. We‚Äôre beginning to see this take shape as non-technical AI artifacts become more complex, powerful, and collaborative.&lt;/p&gt;
    &lt;p&gt;For us, abandoning low-code to reclaim ownership of our internal tooling was a simple build vs buy decision with meaningful cost savings and velocity gains. It also feels like a massive upgrade in developer experience and end-user quality of life. It‚Äôs been about 6 months since we made this switch, and so far we haven‚Äôt looked back.&lt;/p&gt;
    &lt;p&gt;Every build vs buy decision is unique, but many ultimately boil down to ROI in terms of speed, financial cost, maintenance cost, and organizational complexity. There are of course additional considerations like vendor lock-in, ownership of core competencies, ecosystem compatibility, etc., but in this case we can reduce the decision to this: will buying this platform let my team go faster, ship more, and create more value for our customers. At least for now, that answer feels clearer every day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46767440</guid><pubDate>Mon, 26 Jan 2026 16:11:28 +0000</pubDate></item><item><title>France Aiming to Replace Zoom, Google Meet, Microsoft Teams, etc.</title><link>https://twitter.com/lellouchenico/status/2015775970330882319</link><description>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46767668</guid><pubDate>Mon, 26 Jan 2026 16:27:46 +0000</pubDate></item><item><title>The mountain that weighed the Earth</title><link>https://signoregalilei.com/2026/01/18/the-mountain-that-weighed-the-earth/</link><description>&lt;doc fingerprint="a3b0e04d7dd63d2a"&gt;
  &lt;main&gt;
    &lt;p&gt;Suppose you want to find out how much you weigh. It‚Äôs easy enough ‚Äì get a scale, stand on it, and read the number.&lt;/p&gt;
    &lt;p&gt;Now suppose you want to know how much the Earth weighs. You get out a scale and‚Ä¶hmm. What exactly do you do with it?&lt;/p&gt;
    &lt;p&gt;Let‚Äôs do a little physics class refresher. When a scale measures weight, that weight is a force, specifically the force of the gravitational attraction between what‚Äôs on top of the scale and what‚Äôs beneath it. If you stand on the scale and the scale is on the floor, it‚Äôs the attraction between you and the Earth.&lt;/p&gt;
    &lt;p&gt;Isaac Newton‚Äôs law of universal gravitation tells us that gravitational attraction depends on the mass of the two objects and the distance between them. The mass of the Earth is constant, and the distance from the scale to the center of the Earth is constant.1 So the only thing that affects the scale‚Äôs reading is your own mass ‚Äì the amount of matter in your body, independent of Earth‚Äôs gravity.&lt;/p&gt;
    &lt;p&gt;Isaac Newton‚Äôs 3rd law of motion conveniently tells us that every action has an equal and opposite reaction. So if you weigh 180 pounds on the Earth, the Earth also weighs 180 pounds on you. Turn your bathroom scale upside down and try it!&lt;/p&gt;
    &lt;p&gt;As technically correct as this answer for the Earth‚Äôs weight is, it‚Äôs not super useful for finding the Earth‚Äôs mass ‚Äì at least not by itself. But it could be a good first step. All we need is to know how much something else weighs on you to compare to the Earth.&lt;/p&gt;
    &lt;p&gt;So here‚Äôs what we need to know to find the mass the Earth:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;How much the Earth weighs on you&lt;/item&gt;
      &lt;item&gt;Your distance to the center of the Earth&lt;/item&gt;
      &lt;item&gt;How much some other object weighs on you&lt;/item&gt;
      &lt;item&gt;The distance between you and that object&lt;/item&gt;
      &lt;item&gt;That object‚Äôs total mass&lt;/item&gt;
      &lt;item&gt;Some math to compare the results&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Number 1 is easy to find ‚Äì just use your upside-down scale. Number 2 is doable as well ‚Äì we‚Äôve known the radius of the Earth for thousands of years. Number 4 can be found out with a ruler so long as the object is nearby and stays in one place, and number 6 can be done with the math that Isaac Newton developed.&lt;/p&gt;
    &lt;p&gt;The real challenge is combining 3 and 5. Gravity is really weak, so it‚Äôs hard to find something that‚Äôs massive enough that we can detect its gravity but small enough that we can calculate its mass directly.&lt;/p&gt;
    &lt;p&gt;The first experiment to do this decently decided to use a mountain, specifically Schiehallion in Scotland, in 1774.&lt;/p&gt;
    &lt;head rend="h6"&gt;Image by Andrew2606, CC BY&lt;/head&gt;
    &lt;p&gt;Schiehallion is fairly symmetrical and isolated, and was conveniently accessible to the British Royal Society, who were running the experiment. Instead of weighing the mountain‚Äôs gravitational effects on a person, they measured its effect on a precision plumb line.&lt;/p&gt;
    &lt;p&gt;The Astronomer Royal at the time, Nevil Maskelyne, set up two astronomical observatories next to Schiehallion, one to the North and one to the South. Using the stars as a reference, Maskelyne‚Äôs team found that the plumb lines on either side of the mountain pointed just 0.0152 degrees apart. About 80 percent of that difference could be expected from the curvature of the Earth between the two sites, meaning the remainder ‚Äì just 0.0032 degrees ‚Äì was from the gravity of the mountain itself.&lt;/p&gt;
    &lt;p&gt;The final remaining step was to calculate the mass of Schiehallion directly. This was accomplished by a team of surveyors led by Charles Hutton, who mapped the mountain‚Äôs shape and the density of its rocks at different layers.&lt;/p&gt;
    &lt;p&gt;Putting together the results, Maskelyne and Hutton announced that the Earth was 1.8 times as dense as Schiehallion, or 4.5 metric tons per cubic meter. Using this value, we get that the mass of the Earth is 4.87√ó1021 metric tons. This is within 20% of the best modern measurements, which put the mass of the Earth at 5.91√ó1021 metric tons.&lt;/p&gt;
    &lt;p&gt;The Schiehallion experiment wasn‚Äôt the state of the art for long. A more precise result was achieved in 1798 by Henry Cavendish, who was on the committee for the Schiehallion experiment. Cavendish‚Äôs experiment measured the gravity of large lead spheres using an extremely precise torsion pendulum, and cut the error from 20% down to 1.2%. Still, Schiehallion was first, and it proved that gravity came from all kinds of objects, not just the Earth or the Sun. And that itself is a pretty massive achievement.&lt;/p&gt;
    &lt;p&gt;Coming soon: 1960: the Year of Africa&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46767875</guid><pubDate>Mon, 26 Jan 2026 16:43:30 +0000</pubDate></item><item><title>JuiceSSH ‚Äì Give me my pro features back</title><link>https://nproject.io/blog/juicessh-give-me-back-my-pro-features/</link><description>&lt;doc fingerprint="acb497aa1b6d104a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;JuiceSSH - Give me my pro features back&lt;/head&gt;
    &lt;p&gt;JuiceSSH used to be the best SSH client available on Android until December 2025.&lt;/p&gt;
    &lt;p&gt;Since then, the purchase made in 2019 is not recognized anymore, and the price went up by 20$. Some users complain in review that after buying it again, the application doesn't get activated. Support is unresponsive, this looks like an exit scam.&lt;/p&gt;
    &lt;p&gt;Below is a way to make the application work again. This required jadx to understand smali, and will require you ApkTool and jarsigner, which is part of OpenJDK, and you that can install on Windows using &lt;code&gt;choco install openjdk&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You'll also need a JuiceSSH apk, I downloaded one from PureAPK, but feel free to dump your own from your device using adb if you cannot find it. Make sure to verify the hash using virus total/sha256sum if downloading from internet, which should be &lt;code&gt;d1ee811bcd82f25aea0bdc568896d82017ee174d9c4631c123a9d9173c748232&lt;/code&gt; for the last version available, version 3.2.2.&lt;/p&gt;
    &lt;p&gt;Below are powershell version of the command lines, but you get the idea.&lt;/p&gt;
    &lt;head rend="h2"&gt;Decompile&lt;/head&gt;
    &lt;p&gt;The first step is to decompile the dex packed code from the apk.&lt;/p&gt;
    &lt;code&gt;&amp;amp; "C:\Program Files\OpenJDK\jdk-25\bin\java.exe" -jar d juicessh.apk
&lt;/code&gt;
    &lt;head rend="h2"&gt;Modify smali&lt;/head&gt;
    &lt;p&gt;You then need to modify the smali of three files, which are detailed below.&lt;/p&gt;
    &lt;head rend="h3"&gt;smali/com/sonelli/juicessh/models/User.smali&lt;/head&gt;
    &lt;p&gt;In this file, we'll patch the purchase validation and signature validation, done by the &lt;code&gt;public boolean H()&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;Here is the original version.&lt;/p&gt;
    &lt;code&gt;public boolean H() {
    try {
        String str = "";
        ArrayList arrayList = new ArrayList();
        for (Purchase purchase : this.purchases) {
            if (!arrayList.contains(purchase.order)) {
                str = str + purchase.product + purchase.state;
                arrayList.add(purchase.order);
            }
        }
        return vg0.b(this.signature, this.sessionIdentifier + this.name + this.email + str + this.disabled.toString());
    } catch (IllegalStateException e) {
        e.printStackTrace();
        return false;
    }
}&lt;/code&gt;
    &lt;p&gt;Which we'll simply change into&lt;/p&gt;
    &lt;code&gt;public boolean H() {
    return true;
}&lt;/code&gt;
    &lt;code&gt;# virtual methods
.method public H()Z
    .locals 1

    const/4 v0, 0x1
    return v0
.end method&lt;/code&gt;
    &lt;head rend="h3"&gt;smali/com/sonelli/oi0.smali&lt;/head&gt;
    &lt;p&gt;In this one, we'll patch the &lt;code&gt;public static boolean d(Object obj)&lt;/code&gt; function, who calls the H() function we modified above, which now returns true, filters product matching JuiceSSH in purchases list, and check if it the purchase is valid. We'll simply make it return true in any case.&lt;/p&gt;
    &lt;p&gt;Here is the original version:&lt;/p&gt;
    &lt;code&gt;public static boolean d(Object obj) {
    if (!obj.getClass().getName().equals(User.class.getName())) {
        return false;
    }
    try {
        if (!((User) obj).H()) {
            return false;
        }
        ArrayList arrayList = new ArrayList();
        for (Purchase purchase : ((User) obj).purchases) {
            if (purchase.product.equals(a())) {
                arrayList.add(purchase);
            }
        }
        Collections.sort(arrayList, new a());
        if (arrayList.size() &amp;gt; 0) {
            if (((Purchase) arrayList.get(arrayList.size() - 1)).state.intValue() == 0) {
                return true;
            }
        }
        return false;
    } catch (NullPointerException e) {
        e.printStackTrace();
        return false;
    }
}&lt;/code&gt;
    &lt;p&gt;Here is the patched one:&lt;/p&gt;
    &lt;code&gt;public static boolean d(Object obj) {
    return obj.getClass().getName().equals(User.class.getName());
}&lt;/code&gt;
    &lt;code&gt;.method public static d(Ljava/lang/Object;)Z
    .locals 3

    # obj.getClass()
    invoke-virtual {p0}, Ljava/lang/Object;-&amp;gt;getClass()Ljava/lang/Class;
    move-result-object v0

    # obj.getClass().getName()
    invoke-virtual {v0}, Ljava/lang/Class;-&amp;gt;getName()Ljava/lang/String;
    move-result-object v0

    # User.class
    const-class v1, Lcom/sonelli/juicessh/models/User;

    # User.class.getName()
    invoke-virtual {v1}, Ljava/lang/Class;-&amp;gt;getName()Ljava/lang/String;
    move-result-object v1

    # compare strings
    invoke-virtual {v0, v1}, Ljava/lang/String;-&amp;gt;equals(Ljava/lang/Object;)Z
    move-result v2

    if-nez v2, :cond_true

    const/4 v0, 0x0
    return v0

    :cond_true
    const/4 v0, 0x1
    return v0
.end method&lt;/code&gt;
    &lt;head rend="h3"&gt;smali/com/sonelli/pi0.smali&lt;/head&gt;
    &lt;p&gt;Finally, we'll patch the central part of the authentication, which is called each time a pro-feature is triggered to ensure user has valid license, the &lt;code&gt;public static void j(Context context, p pVar)&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;Here is the original version:&lt;/p&gt;
    &lt;code&gt;public static void j(Context context, p pVar) {
    User user;
    User user2;
    String strS = User.s(context);
    if (strS == null) {
        pVar.a(context.getString(R$string.authentication_failure));
        return;
    }
    if (strS.equals("New User")) {
        pVar.a("New User");
        return;
    }
    User user3 = b;
    if (user3 != null &amp;amp;&amp;amp; !user3.disabled.booleanValue()) {
        long jCurrentTimeMillis = System.currentTimeMillis() - b.modified;
        DateUtils.getRelativeTimeSpanString(System.currentTimeMillis() + (b.w() * 1000), System.currentTimeMillis(), 0L, 0);
        DateUtils.getRelativeTimeSpanString(System.currentTimeMillis() + (3600000 - jCurrentTimeMillis), System.currentTimeMillis(), 0L, 0);
        if (b.w() &amp;lt;= 0) {
            gj0.b("API", "Cached user's API session has expired - refreshing session...");
            e(context, null, b.sessionIdentifier, pVar);
            return;
        }
        pVar.b(b);
        if (jCurrentTimeMillis &amp;lt;= 3600000 || context == null || (user2 = b) == null) {
            return;
        }
        e(context, null, user2.sessionIdentifier, null);
        return;
    }
    User userA = User.A(context);
    if (userA == null || userA.disabled.booleanValue() || !userA.H()) {
        e(context, null, null, pVar);
        return;
    }
    b = userA;
    if (userA.w() &amp;lt;= 0) {
        e(context, null, b.sessionIdentifier, pVar);
        return;
    }
    pVar.b(b);
    if (context == null || (user = b) == null) {
        return;
    }
    e(context, null, user.sessionIdentifier, null);
}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;pVar.b()&lt;/code&gt; is the success callback we'll call while &lt;code&gt;e()&lt;/code&gt; is called in case of error. b is the globally stored user we'll have to set. To patch this, we'll simply craft a User with meaningless data, a session expire always in future, save the user in b, and call the success callback every time.&lt;/p&gt;
    &lt;code&gt;public static void j(Context context, p pVar) {
    User user = new User();
    user.email = "myemail@google.com";
    user.name = "hello";
    user.given_name = "hello";
    user.sessionExpires = System.currentTimeMillis() + (86400000 * 365);
    user.sessionIdentifier = "";
    b = user;
    pVar.b(user);
}&lt;/code&gt;
    &lt;code&gt;.method public static j(Landroid/content/Context;Lcom/sonelli/pi0$p;)V
    .locals 8

    # User u = new User();
    new-instance v0, Lcom/sonelli/juicessh/models/User;
    invoke-direct {v0}, Lcom/sonelli/juicessh/models/User;-&amp;gt;&amp;lt;init&amp;gt;()V

    # u.email = "myemail@google.com";
    const-string v1, "myemail@google.com"
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&amp;gt;email:Ljava/lang/String;

    # u.name = "hello";
    const-string v1, "hello"
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&amp;gt;name:Ljava/lang/String;

    # u.given_name = "hello";
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&amp;gt;given_name:Ljava/lang/String;

    # long now = System.currentTimeMillis();
    invoke-static {}, Ljava/lang/System;-&amp;gt;currentTimeMillis()J
    move-result-wide v2

    # yearMillis = 86400000L * 365L
    const-wide/32 v4, 0x05265c00      # 86400000
    const-wide/16 v6, 0x016d          # 365
    mul-long/2addr v4, v6

    # u.sessionExpires = now + yearMillis;
    add-long/2addr v2, v4
    iput-wide v2, v0, Lcom/sonelli/juicessh/models/User;-&amp;gt;sessionExpires:J

    # u.sessionIdentifier = ""
    const-string v1, ""
    iput-object v1, v0, Lcom/sonelli/juicessh/models/User;-&amp;gt;sessionIdentifier:Ljava/lang/String;

    # pi0.b = u;
    sput-object v0, Lcom/sonelli/pi0;-&amp;gt;b:Lcom/sonelli/juicessh/models/User;

    # pVar.b(b);
    invoke-virtual {p1, v0}, Lcom/sonelli/pi0$p;-&amp;gt;b(Lcom/sonelli/juicessh/models/User;)V

    return-void&lt;/code&gt;
    &lt;head rend="h2"&gt;Recompile&lt;/head&gt;
    &lt;code&gt;&amp;amp; "C:\Program Files\OpenJDK\jdk-25\bin\java.exe" -jar .\apktool_2.12.1.jar juicessh&lt;/code&gt;
    &lt;head rend="h2"&gt;Sign the apk&lt;/head&gt;
    &lt;code&gt;# Create a keystore if needed to self sign the APK
keytool -genkey -v -keystore k.keystore -alias a -keyalg RSA -keysize 2048 -validity 50000

# Sign the APK
jarsigner -verbose -sigalg SHA1withRSA -digestalg SHA1 -keystore k.keystore ./juicessh/dist/juicessh.apk a&lt;/code&gt;
    &lt;head rend="h2"&gt;Done&lt;/head&gt;
    &lt;p&gt;You can install this apk, ignore the security warning because it is self signed, and enjoy JuiceSSH with its pro features again.&lt;/p&gt;
    &lt;p&gt;I don't think the cloud sync will work anymore, but that's a minor inconvenience. The plugins don't work anymore too, which is really a joke.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46768909</guid><pubDate>Mon, 26 Jan 2026 17:46:38 +0000</pubDate></item><item><title>Fedora Asahi Remix is now working on Apple M3</title><link>https://bsky.app/profile/did:plc:okydh7e54e2nok65kjxdklvd/post/3mdd55paffk2o</link><description>&lt;doc fingerprint="b3d7503bc0dfd534"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a heavily interactive web application, and JavaScript is required. Simple HTML interfaces are possible, but that is not what this is. &lt;/p&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Post&lt;/head&gt;
      &lt;p&gt;Michael Reeves&lt;/p&gt;
      &lt;p&gt;integralpilot.bsky.social&lt;/p&gt;
      &lt;p&gt;did:plc:okydh7e54e2nok65kjxdklvd&lt;/p&gt;
      &lt;p&gt;On Apple M3, a Linux KDE plasma desktop under Fedora Asahi Remix is now WORKING! Super excited to share this update and happy to answer any questions! Co-credits to noopwafel and Shiz. :)&lt;/p&gt;
      &lt;p&gt;2026-01-26T11:26:35.149Z&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46769051</guid><pubDate>Mon, 26 Jan 2026 17:54:20 +0000</pubDate></item><item><title>Google Books removed all search functions for any books with previews</title><link>https://old.reddit.com/r/google/comments/1qn1hk1/google_has_seemingly_entirely_removed_search/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46769201</guid><pubDate>Mon, 26 Jan 2026 18:05:15 +0000</pubDate></item><item><title>Show HN: Ourguide ‚Äì OS wide task guidance system that shows you where to click</title><link>https://ourguide.ai</link><description>&lt;doc fingerprint="81fc5d77babb99db"&gt;
  &lt;main&gt;
    &lt;p&gt;step-by-step guidance on your screen,&lt;lb/&gt;for any task&lt;/p&gt;
    &lt;p&gt;Type what you need. Get step-by-step guidance on your screen.&lt;/p&gt;
    &lt;p&gt;Type what you need. Get step-by-step guidance on your screen.&lt;/p&gt;
    &lt;p&gt;get any task done without getting stuck, Universal Guidance for Your Entire Desktop&lt;/p&gt;
    &lt;p&gt;get any task done without getting stuck, Universal Guidance for Your Entire Desktop&lt;/p&gt;
    &lt;p&gt;No screenshots. No copy-pasting. Ask Mode sees your screen only when you want it to.&lt;/p&gt;
    &lt;p&gt;No more screenshotting.&lt;lb/&gt; Ask Mode watches your screen in real-time.&lt;/p&gt;
    &lt;p&gt;Toggle screen context on or off anytime.&lt;lb/&gt; Ourguide only sees your screen when you want it to.&lt;/p&gt;
    &lt;p&gt;Get help without explaining.&lt;lb/&gt; It already knows what you're looking at.&lt;/p&gt;
    &lt;p&gt;Messages, Photoshop, Excel‚Äîone assistant for your entire desktop.&lt;/p&gt;
    &lt;p&gt;See how Ourguide is transforming the way people learn and work.&lt;/p&gt;
    &lt;p&gt;Trying to learn Photoshop from videos is a nightmare of pausing and rewinding. Ourguide just‚Ä¶ shows me. In the app. This is the future.&lt;/p&gt;
    &lt;p&gt;Sophie Andersson&lt;/p&gt;
    &lt;p&gt;Graphic Designer&lt;/p&gt;
    &lt;p&gt;Onboarding new team members to Salesforce used to take a week. Now I just tell them to use Ourguide. The amount of questions I get has dropped by 90%.&lt;/p&gt;
    &lt;p&gt;David Okonkwo&lt;/p&gt;
    &lt;p&gt;Product Manager at Stripe&lt;/p&gt;
    &lt;p&gt;I wish I had a "rewind" button for life. Ourguide is the next best thing for my complex workflows. No more fear of clicking the wrong thing and messing up a campaign.&lt;/p&gt;
    &lt;p&gt;Elena Vasquez&lt;/p&gt;
    &lt;p&gt;Marketing Lead&lt;/p&gt;
    &lt;p&gt;My professor just breezed through an entire Excel lesson. I would have been completely lost without this tool guiding me through every formula on my own screen.&lt;/p&gt;
    &lt;p&gt;Jordan Lee&lt;/p&gt;
    &lt;p&gt;Business Student, NYU&lt;/p&gt;
    &lt;p&gt;"Just follow the tutorial" they said. 20 minutes in, the UI is different and I'm totally stuck. Ourguide saved my project and my sanity.&lt;/p&gt;
    &lt;p&gt;Anika Patel&lt;/p&gt;
    &lt;p&gt;UX Designer&lt;/p&gt;
    &lt;p&gt;Ourguide cut our employee training time by 75%. Instead of week-long sessions, new hires are productive in hours.&lt;/p&gt;
    &lt;p&gt;Rachel Bennett&lt;/p&gt;
    &lt;p&gt;Head of People Ops&lt;/p&gt;
    &lt;p&gt;I learned Blender in 3 days with Ourguide. What used to take months of YouTube tutorials now takes hours of guided practice.&lt;/p&gt;
    &lt;p&gt;Tyler Mitchell&lt;/p&gt;
    &lt;p&gt;3D Artist &amp;amp; Animator&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"The future of productivity is Guided Action, not static instructions."&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46769422</guid><pubDate>Mon, 26 Jan 2026 18:19:45 +0000</pubDate></item><item><title>Notice of Collective Action Lawsuit Against Workday, INC</title><link>https://workdaycase.com</link><description>&lt;doc fingerprint="95458e1de257a616"&gt;
  &lt;main&gt;
    &lt;p&gt;Re: Mobley v. Workday, Inc. Case No. 3:23-0770-RFL, Northern District of California.&lt;/p&gt;
    &lt;p&gt;Read:&lt;/p&gt;
    &lt;p&gt;INTRODUCTION&lt;/p&gt;
    &lt;p&gt;WHAT IS THIS LAWSUIT ABOUT&lt;/p&gt;
    &lt;p&gt;HOW TO JOIN THIS LAWSUIT&lt;/p&gt;
    &lt;p&gt;HOW WILL YOUR CLAIM BE HANDLED AND PROVEN?&lt;/p&gt;
    &lt;p&gt;LEGAL EFFECT OF OPTING IN TO JOIN THIS CASE&lt;/p&gt;
    &lt;p&gt;NO RETALIATION PERMITTED&lt;/p&gt;
    &lt;p&gt;YOUR LEGAL REPRESENTATION IF YOU JOIN&lt;/p&gt;
    &lt;p&gt;The Age Discrimination in Employment Act of 1967&lt;/p&gt;
    &lt;p&gt;By submitting this form, I am consenting to join the collective action Mobley v. Workday, Inc., case number 3:23-cv-0770-RFL (√¢Lawsuit√¢), pending in the Northern District of California Federal Court, which includes claims under the Age Discrimination in Employment Act of 1967 (√¢ADEA√¢), and to be bound by the judgment of the Court.By submitting this form, I confirm that on or after September 24, 2020, I applied for at least one employment opportunity using Workday√¢s application platform while I was 40 or more years old.I am checking the box below as my signature for the Opt-In Consent To Join and as my authorization for it to be filed in the Northern District of California Federal Court on my behalf.&lt;/p&gt;
    &lt;p&gt;I. INTRODUCTION&lt;lb/&gt; This notice and its contents have been authorized by the United States District Court for the Northern District of California. The Court has taken no position in this case regarding the merits of Plaintiffs√¢ claims or of Workday√¢s defenses.The purpose of this notice is to provide notification of your following rights under the Age Discrimination in Employment Act (√¢ADEA√¢):&lt;/p&gt;
    &lt;p&gt;You have the right to join this lawsuit if you applied for employment opportunities using Workday√¢s application platform since September 24, 2020, while you were 40 or more years old.&lt;/p&gt;
    &lt;p&gt;To join this lawsuit you must fill out and electronically sign and send in the Opt-In Consent To Join Form by clicking submit on or before March 7, 2026. How to do that is explained more fully in this Section III of this notice.&lt;/p&gt;
    &lt;p&gt;You have the right to have the Plaintiffs√¢ attorneys represent you in connection with this case, and to contact them for advice or assistance regarding the Opt-In Form or regarding the case, at this link or via the email addresses and phone numbers listed below in Section V of this Notice&lt;/p&gt;
    &lt;p&gt;II. WHAT IS THIS LAWSUIT ABOUT?&lt;lb/&gt; Derek Mobley (√¢Plaintiff√¢) filed this action against Workday, Inc. in the Northern District of California, Case No. 3:23-0770-RFL (√¢Lawsuit√¢), on behalf of himself and all others similarly situated. His allegations include that Workday, Inc., through its use of certain Artificial Intelligence (√¢AI√¢) features on its job application platform, violated the Age Discrimination in Employment Act (√¢ADEA√¢). Workday denies these allegations. The Court has not made any findings about whether Plaintiff√¢s claims or Workday√¢s denial of liability have any merit.The Court has provisionally certified an ADEA collective, which includes: √¢All individuals aged 40 and over who, from September 24, 2020, through the present, applied for job opportunities using Workday, Inc.√¢s job application platform and were denied employment recommendations.√¢ In this context, being √¢denied√¢ an √¢employment recommendation√¢ means that (i) the individual√¢s application was scored, sorted, ranked, or screened by Workday√¢s AI; (ii) the result of the AI scoring, sorting, ranking, or screening was not a recommendation to hire; and (iii) that result was communicated to the prospective employer, or the result was an automatic rejection by Workday.Once you submit your Opt-In Consent to Join Form, the Court will subsequently need to determine that you meet the requirements described in the above paragraph in order for you to have the right to stay in the Lawsuit. However, all you need to know right now to opt-in to this case is that, on or after September 24, 2020, you applied for employment opportunities using Workday√¢s application platform while you were 40 or more years old.&lt;/p&gt;
    &lt;p&gt;III. HOW TO JOIN THIS LAWSUIT&lt;lb/&gt; To join this case, please complete, sign and return the Opt-In Consent To Join Form. If you have questions, you may contact the Plaintiffs√¢ attorneys by clicking submit or at the addresses and phone numbers provided below in Section V of this Notice.If you are unable to sign and return the Opt-In Consent To Join Form electronically through this notice, you may obtain a paper copy of such forms to sign and return by mail, email, text or other delivery on or before March 7 2026. A paper copy can be downloaded and printed from this link or you can contact Plaintiffs√¢ attorneys to request such Forms by calling, emailing, texting or mailing them at the addresses or phone numbers provided below in Section V of this Notice. To join this case you must submit your Opt-In Consent To Join Form on or before March 7, 2026.The Opt-In Consent To Join Form requires that you sign page one and that you list your contact information and birthday on page two. Only page one will be publicly filed with the Court. Your contact information and birthday will not be filed with the Court as part of your initial opt-in to the case.&lt;/p&gt;
    &lt;p&gt;IV. HOW WILL YOUR CLAIM BE HANDLED AND PROVEN?&lt;lb/&gt; After you submit your Opt-In Consent To Join Form to the Plaintiffs√¢ attorneys for filing, and unless and until you decide otherwise, they will continue to represent you as your counsel in this matter. As your legal representative, Plaintiffs√¢ attorneys can help you obtain the documents, testimony, and other evidence that you may need to show that you meet all the requirements necessary to litigate your claim as a party to this Lawsuit.&lt;lb/&gt; If you choose to join this case, you may be required, with the help of Plaintiffs√¢ attorneys, to answer written questions, produce documents related to your job application process, attend a deposition, and/or testify in court. Workday may also be required to answer written questions or produce documents related to its products to you. The Court has procedures that will protect your confidential information from public disclosure, including your birthday.&lt;/p&gt;
    &lt;p&gt;V. LEGAL EFFECT OF OPTING IN TO JOIN THIS CASE&lt;lb/&gt; If you opt in to the Lawsuit, you will be bound by a favorable or unfavorable judgment on Plaintiffs√¢ ADEA claim. If you do not opt in, you will be precluded from future participation in the ADEA collective, including participation in any settlement related to the ADEA claim, but you may still bring your own separate lawsuit, depending on your situation. No ADEA collective has been finally certified, and you may still be dismissed from the case.&lt;/p&gt;
    &lt;p&gt;VI. NO RETALIATION PERMITTED&lt;lb/&gt; Federal law prohibits Workday from retaliating against you for opting in to this Lawsuit or otherwise exercising your rights.&lt;/p&gt;
    &lt;p&gt;VII. YOUR LEGAL REPRESENTATION IF YOU JOIN&lt;lb/&gt; When you join this Lawsuit by electronically completing and sending the Opt-In Consent To Join Form, unless you decide otherwise, the lawyers representing you will be:&lt;/p&gt;
    &lt;p&gt;Lee D. Winston&lt;lb/&gt; [email protected]Roderick T. Cooks&lt;lb/&gt; [email protected]Bethany M. Logan&lt;lb/&gt; [email protected]Winston Cooks, LLC&lt;lb/&gt; 420 20th Street North, Suite #2200&lt;lb/&gt; Birmingham, AL 35203&lt;/p&gt;
    &lt;p&gt;You will not be responsible for paying any lawyers√¢ fees. Plaintiffs√¢ attorneys are being paid on contingency, which means that if Plaintiffs do not receive a recovery in this case, no lawyer fees will be owed. If Plaintiffs obtain a recovery, either by settlement or judgment, Plaintiffs√¢ attorneys may file a request with the Court to receive part of the recovery as compensation for their services.If you want to join the Lawsuit, but prefer to retain your own attorney to represent you, you have the right to do so. You also have the right to represent yourself. To join this case you must submit the Opt-In Consent To Join Form with the Court on or before March 7, 2026.&lt;/p&gt;
    &lt;p&gt;PLEASE DO NOT CONTACT THE COURT WITH QUESTIONS ABOUT THIS LAWSUIT.&lt;/p&gt;
    &lt;p&gt;Almost there!&lt;lb/&gt; Check your inbox and click the confirmation link to complete your submission.&lt;lb/&gt; Don√¢t see the email? Check spam or promotions or contact [email protected].&lt;/p&gt;
    &lt;p&gt;PLEASE DO NOT CONTACT THE COURT WITH QUESTIONS ABOUT THIS LAWSUIT.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46769668</guid><pubDate>Mon, 26 Jan 2026 18:37:37 +0000</pubDate></item><item><title>Show HN: TetrisBench ‚Äì Gemini Flash reaches 66% win rate on Tetris against Opus</title><link>https://tetrisbench.com/tetrisbench/</link><description>&lt;doc fingerprint="6b765bf3b2480488"&gt;
  &lt;main&gt;
    &lt;p&gt;AI Model Tetris Performance Comparison&lt;/p&gt;
    &lt;p&gt;Loading benchmark data...&lt;/p&gt;
    &lt;p&gt;No benchmark data yet. Run some AI vs AI games!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46769752</guid><pubDate>Mon, 26 Jan 2026 18:42:40 +0000</pubDate></item><item><title>When AI 'builds a browser,' check the repo before believing the hype</title><link>https://www.theregister.com/2026/01/26/cursor_opinion/</link><description>&lt;doc fingerprint="255824528dad115f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;When AI 'builds a browser,' check the repo before believing the hype&lt;/head&gt;
    &lt;head rend="h2"&gt;Autonomous agents may generate millions of lines of code, but shipping software is another matter&lt;/head&gt;
    &lt;p&gt;Opinion AI-integrated development environment (IDE) company Cursor recently implied it had built a working web browser almost entirely with its AI agents. I won't say they lied, but CEO Michael Truell certainly tweeted: "We built a browser with GPT-5.2 in Cursor."&lt;/p&gt;
    &lt;p&gt;He followed up with: "It's 3M+ lines of code across thousands of files. The rendering engine is from-scratch in Rust with HTML parsing, CSS cascade, layout, text shaping, paint, and a custom JS VM."&lt;/p&gt;
    &lt;p&gt;That sounds impressive, doesn't it? He also added: "It *kind of* works," which is not the most ringing endorsement. Still, numerous news sources and social media chatterboxes ran with the news that AI built a web browser in a week.&lt;/p&gt;
    &lt;p&gt;Too bad it wasn't true. If you actually looked at Cursor engineer Wilson Lin's blog post about FastRender, the AI-created web browser, you won't see much boasting about a working web browser. Instead, there's a video of a web browser sort of working, and a much less positive note that "building a browser from scratch is extremely difficult."&lt;/p&gt;
    &lt;p&gt;The thing about making such a software announcement on GitHub is that while the headlines are proclaiming another AI victory, developers have this nasty trick. They actually git the code and try it out.&lt;/p&gt;
    &lt;p&gt;Developers quickly discovered the "browser" barely compiles, often does not run, and was heavily misrepresented in marketing.&lt;/p&gt;
    &lt;p&gt;As a techie, the actual blog post about how they tried and didn't really succeed was much more interesting. Of course, that Cursor sicced hundreds of GPT-5.2-style agents which ran for a week to produce three million lines of new code, to produce, at best, a semi-functional web browser from scratch, doesn't make for a good headline.&lt;/p&gt;
    &lt;p&gt;According to Perplexity, my AI chatbot of choice, this week‚Äëlong autonomous browser experiment consumed in the order of 10-20 trillion tokens and would have cost several million dollars at then‚Äëcurrent list prices for frontier models.&lt;/p&gt;
    &lt;p&gt;I'd just cloned a copy of Chromium myself, and for all that time and money, independent developers who cloned the repo reported that the codebase is very far from a functional browser. Recent commits do not compile cleanly, GitHub Actions runs on &lt;code&gt;main&lt;/code&gt; are failing, and reviewers could not find a single recent commit that was built without errors.&lt;/p&gt;
    &lt;p&gt;Where builds succeeded after manual patching, performance was abysmal, with reports of pages taking around a minute to load and a heavy reliance on existing projects like Servo, a Rust-based web rendering engine, and QuickJS, a JavaScript engine, despite "from scratch" claims.&lt;/p&gt;
    &lt;p&gt;Lin defended the project on Y Combinator, saying, for instance: "The JS engine used a custom JS VM being developed in vendor/ecma-rs as part of the browser, which is a copy of my personal JS parser project vendored to make it easier to commit to." If it's derived from his personal JavaScript parser, that's not really from scratch, is it? Nor is it, from the sound of the argument, written by AI.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Just because Linus Torvalds vibe codes doesn't mean it's a good idea&lt;/item&gt;
      &lt;item&gt;The Microsoft 365 Copilot app rebrand was bad, but there are far worse offenders&lt;/item&gt;
      &lt;item&gt;The most durable tech is boring, old, and everywhere&lt;/item&gt;
      &lt;item&gt;What the Linux desktop really needs to challenge Windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Gregory Terzian, a Servo maintainer, responded: "The actual code is worse; I can only describe it as a tangle of spaghetti... I can't make much, if anything, out of it." He then gave the backhanded compliment: "So I agree this isn't just wiring up of dependencies, and neither is it copied from existing implementations: it's a uniquely bad design that could never support anything resembling a real-world web engine." Now that's a burn.&lt;/p&gt;
    &lt;p&gt;From where I sit, what makes the Cursor case more dangerous than just a failed hack‚Äëweek project is that the hype is baked into its methodology. The "experiment" wasn't presented as what it really was: an interesting, but messy, internal learning exercise. No, it was rolled out as a milestone that conveniently confirmed the company's long‚Äërunning autonomous agent advertising. Missing from the story were basics any senior engineer would demand: passing Continuous Integration (CI), reproducible builds, and real benchmarks that show the browser doing more than limping through a hello-world page.&lt;/p&gt;
    &lt;p&gt;Zoom out, and CEOs are still predicting that AI will write 90 percent of code in a year, while most enterprise AI pilots still fail to deliver meaningful return on investment.&lt;/p&gt;
    &lt;p&gt;We're now in a kind of AI uncanny valley for developers. Sure, tools like Cursor can be genuinely helpful as glorified autocomplete and refactoring assistants, but marketing keeps insisting junior engineers can take whole projects from spec to shipping. When you start believing your own sizzle reel, you stop doing the tedious validation work that separates a demo from a deliverable.&lt;/p&gt;
    &lt;p&gt;Enough already. The hype has grown cold. Sarah Friar, OpenAI's CFO, recently blogged that in 2026, its focus would be on "practical adoption." Let's see real-world practical results first, and then we can talk about practical AI adoption. ¬Æ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46769965</guid><pubDate>Mon, 26 Jan 2026 18:58:37 +0000</pubDate></item><item><title>ChatGPT Containers can now run bash, pip/npm install packages and download files</title><link>https://simonwillison.net/2026/Jan/26/chatgpt-containers/</link><description>&lt;doc fingerprint="1e2a4e781ae52007"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;ChatGPT Containers can now run bash, pip/npm install packages, and download files&lt;/head&gt;
    &lt;p&gt;26th January 2026&lt;/p&gt;
    &lt;p&gt;One of my favourite features of ChatGPT is its ability to write and execute code in a container. This feature launched as ChatGPT Code Interpreter nearly three years ago, was half-heartedly rebranded to ‚ÄúAdvanced Data Analysis‚Äù at some point and is generally really difficult to find detailed documentation about. Case in point: it appears to have had a massive upgrade at some point in the past few months, and I can‚Äôt find documentation about the new capabilities anywhere!&lt;/p&gt;
    &lt;p&gt;Here are the most notable new features:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;ChatGPT can directly run Bash commands now. Previously it was limited to Python code only, although it could run shell commands via the Python &lt;code&gt;subprocess&lt;/code&gt;module.&lt;/item&gt;
      &lt;item&gt;It has Node.js and can run JavaScript directly in addition to Python. I also got it to run ‚Äúhello world‚Äù in Ruby, Perl, PHP, Go, Java, Swift, Kotlin, C and C++. No Rust yet though!&lt;/item&gt;
      &lt;item&gt;While the container still can‚Äôt make outbound network requests, &lt;code&gt;pip install package&lt;/code&gt;and&lt;code&gt;npm install package&lt;/code&gt;both work now via a custom proxy mechanism.&lt;/item&gt;
      &lt;item&gt;ChatGPT can locate the URL for a file on the web and use a &lt;code&gt;container.download&lt;/code&gt;tool to download that file and save it to a path within the sandboxed container.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a substantial upgrade! ChatGPT can now write and then test code in 10 new languages (11 if you count Bash), can find files online and download them into the container, and can install additional packages via &lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;npm&lt;/code&gt; to help it solve problems.&lt;/p&gt;
    &lt;p&gt;(OpenAI really need to develop better habits at keeping their release notes up-to-date!)&lt;/p&gt;
    &lt;p&gt;I was initially suspicious that maybe I‚Äôd stumbled into a new preview feature that wasn‚Äôt available to everyone, but I tried some experiments in a free ChatGPT account and confirmed that the new features are available there as well.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;container.download&lt;/item&gt;
      &lt;item&gt;Is container.download a data exfiltration vulnerability?&lt;/item&gt;
      &lt;item&gt;Bash and other languages&lt;/item&gt;
      &lt;item&gt;Installing packages from pip and npm&lt;/item&gt;
      &lt;item&gt;Adding it all together&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;container.download&lt;/head&gt;
    &lt;p&gt;My first clue to the new features came the other day when I got curious about Los Angeles air quality, in particular has the growing number of electric vehicles there hade a measurable impact?&lt;/p&gt;
    &lt;p&gt;I prompted a fresh GPT-5.2 Thinking session with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Show me Los Angeles air quality over time for last 20 years&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here‚Äôs the shared transcript. Annoyingly that doesn‚Äôt make the thinking trace available, so here‚Äôs a screenshot:&lt;/p&gt;
    &lt;p&gt;This bit instantly caught my eye:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[...] The URL from the error message leads to an Excel file containing air quality trends. I can use the container.download tool to fetch it. [...]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;What‚Äôs &lt;code&gt;container.download&lt;/code&gt;? I asked ChatGPT:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Without running any searches tell me about your container.download tool&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And got back:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Sure.&lt;/p&gt;&lt;code&gt;container.download&lt;/code&gt;is a built-in tool I can call that fetches a file from a URL and saves it into the sandboxed container filesystem for this chat session.&lt;head&gt;What it does&lt;/head&gt;&lt;item&gt;Takes a publicly reachable URL and a destination filepath in the container.&lt;/item&gt;&lt;item&gt;Downloads the bytes from that URL and writes them to the given path.&lt;/item&gt;&lt;item&gt;After that, I can read/process the file locally in the container (e.g., unzip it, parse it with Python, open it as an image, convert it, etc.).&lt;/item&gt;&lt;head&gt;Signature (inputs)&lt;/head&gt;&lt;p&gt;It‚Äôs called like this (conceptually):&lt;/p&gt;{ "url": "https://example.com/somefile.zip", "filepath": "/mnt/data/somefile.zip" }&lt;/quote&gt;
    &lt;p&gt;Further experimentation confirmed that &lt;code&gt;container.download&lt;/code&gt; behaves as described.&lt;/p&gt;
    &lt;p&gt;I had it download one file from a server with logs enabled and captured the IP address and user-agent. Here are the most interesting headers:&lt;/p&gt;
    &lt;code&gt;User-Agent: Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko); compatible; ChatGPT-User/1.0; +https://openai.com/bot
Accept: text/html, application/xhtml+xml, application/xml;q=0.9, image/avif, image/webp, image/apng, */*;q=0.8, application/signed-exchange;v=b3;q=0.9
Cf-Connecting-Ip: 52.230.164.178
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;52.230.164.178&lt;/code&gt; IP address resolves to Microsoft Azure Cloud (centralus) in Des Moines, Iowa.&lt;/p&gt;
    &lt;head rend="h4"&gt;Is container.download a data exfiltration vulnerability?&lt;/head&gt;
    &lt;p&gt;On the one hand, this is really useful! ChatGPT can navigate around websites looking for useful files, download those files to a container and then process them using Python or other languages.&lt;/p&gt;
    &lt;p&gt;Is this a data exfiltration vulnerability though? Could a prompt injection attack trick ChatGPT into leaking private data out to a &lt;code&gt;container.download&lt;/code&gt; call to a URL with a query string that includes sensitive information?&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think it can. I tried getting it to assemble a URL with a query string and access it using &lt;code&gt;container.download&lt;/code&gt; and it couldn‚Äôt do it. It told me that it got back this error:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ERROR: download failed because url not viewed in conversation before. open the file or url using web.run first.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This looks to me like the same safety trick used by Claude‚Äôs Web Fetch tool: only allow URL access if that URL was either directly entered by the user or if it came from search results that could not have been influenced by a prompt injection.&lt;/p&gt;
    &lt;p&gt;(I poked at this a bit more and managed to get a simple constructed query string to pass through &lt;code&gt;web.run&lt;/code&gt;‚Äîa different tool entirely‚Äîbut when I tried to compose a longer query string containing the previous prompt history a &lt;code&gt;web.run&lt;/code&gt; filter blocked it.)&lt;/p&gt;
    &lt;p&gt;So I think this is all safe, though I‚Äôm curious if it could hold firm against a more aggressive round of attacks from a seasoned security researcher.&lt;/p&gt;
    &lt;head rend="h4"&gt;Bash and other languages&lt;/head&gt;
    &lt;p&gt;The key lesson from coding agents like Claude Code and Codex CLI is that Bash rules everything: if an agent can run Bash commands in an environment it can do almost anything that can be achieved by typing commands into a computer.&lt;/p&gt;
    &lt;p&gt;When Anthropic added their own code interpreter feature to Claude last September they built that around Bash rather than just Python. It looks to me like OpenAI have now done the same thing for ChatGPT.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs what ChatGPT looks like when it runs a Bash command‚Äîhere my prompt was:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;npm install a fun package and demonstrate using it&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It‚Äôs useful to click on the ‚ÄúThinking‚Äù or ‚ÄúThought for 32s‚Äù links as that opens the Activity sidebar with a detailed trace of what ChatGPT did to arrive at its answer. This helps guard against cheating‚ÄîChatGPT might claim to have run Bash in the main window but it can‚Äôt fake those black and white logs in the Activity panel.&lt;/p&gt;
    &lt;p&gt;I had it run Hello World in various languages later in that same session.&lt;/p&gt;
    &lt;head rend="h4"&gt;Installing packages from pip and npm&lt;/head&gt;
    &lt;p&gt;In the previous example ChatGPT installed the &lt;code&gt;cowsay&lt;/code&gt; package from npm and used it to draw an ASCII-art cow. But how could it do that if the container can‚Äôt make outbound network requests?&lt;/p&gt;
    &lt;p&gt;In another session I challenged it to explore its environment. and figure out how that worked.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the resulting Markdown report it created.&lt;/p&gt;
    &lt;p&gt;The key magic appears to be a &lt;code&gt;applied-caas-gateway1.internal.api.openai.org&lt;/code&gt; proxy, available within the container and with various packaging tools configured to use it.&lt;/p&gt;
    &lt;p&gt;The following environment variables cause &lt;code&gt;pip&lt;/code&gt; and &lt;code&gt;uv&lt;/code&gt; to install packages from that proxy instead of directly from PyPI:&lt;/p&gt;
    &lt;code&gt;PIP_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
PIP_TRUSTED_HOST=packages.applied-caas-gateway1.internal.api.openai.org
UV_INDEX_URL=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../pypi-public/simple
UV_INSECURE_HOST=https://packages.applied-caas-gateway1.internal.api.openai.org
&lt;/code&gt;
    &lt;p&gt;This one appears to get &lt;code&gt;npm&lt;/code&gt; to work:&lt;/p&gt;
    &lt;code&gt;NPM_CONFIG_REGISTRY=https://reader:****@packages.applied-caas-gateway1.internal.api.openai.org/.../npm-public
&lt;/code&gt;
    &lt;p&gt;And it reported these suspicious looking variables as well:&lt;/p&gt;
    &lt;code&gt;CAAS_ARTIFACTORY_BASE_URL=packages.applied-caas-gateway1.internal.api.openai.org
CAAS_ARTIFACTORY_PYPI_REGISTRY=.../artifactory/api/pypi/pypi-public
CAAS_ARTIFACTORY_NPM_REGISTRY=.../artifactory/api/npm/npm-public
CAAS_ARTIFACTORY_GO_REGISTRY=.../artifactory/api/go/golang-main
CAAS_ARTIFACTORY_MAVEN_REGISTRY=.../artifactory/maven-public
CAAS_ARTIFACTORY_GRADLE_REGISTRY=.../artifactory/gradle-public
CAAS_ARTIFACTORY_CARGO_REGISTRY=.../artifactory/api/cargo/cargo-public/index
CAAS_ARTIFACTORY_DOCKER_REGISTRY=.../dockerhub-public
CAAS_ARTIFACTORY_READER_USERNAME=reader
CAAS_ARTIFACTORY_READER_PASSWORD=****
NETWORK=caas_packages_only
&lt;/code&gt;
    &lt;p&gt;Neither Rust nor Docker are installed in the container environment, but maybe those registry references are a clue of features still to come.&lt;/p&gt;
    &lt;head rend="h4"&gt;Adding it all together&lt;/head&gt;
    &lt;p&gt;The result of all of this? You can tell ChatGPT to use Python or Node.js packages as part of a conversation and it will be able to install them and apply them against files you upload or that it downloads from the public web. That‚Äôs really cool.&lt;/p&gt;
    &lt;p&gt;The big missing feature here should be the easiest to provide: we need official documentation! A release notes entry would be a good start, but there are a lot of subtle details to how this new stuff works, its limitations and what it can be used for.&lt;/p&gt;
    &lt;p&gt;As always, I‚Äôd also encourage them to come up with a name for this set of features that properly represents how it works and what it can do.&lt;/p&gt;
    &lt;p&gt;In the meantime, I‚Äôm going to call this ChatGPT Containers.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Wilson Lin on FastRender: a browser built by thousands of parallel agents - 23rd January 2026&lt;/item&gt;
      &lt;item&gt;First impressions of Claude Cowork, Anthropic's general agent - 12th January 2026&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46770221</guid><pubDate>Mon, 26 Jan 2026 19:19:40 +0000</pubDate></item><item><title>Dithering ‚Äì Part 2: The Ordered Dithering</title><link>https://visualrambling.space/dithering-part-2/</link><description>&lt;doc fingerprint="2334463359238fe7"&gt;
  &lt;main&gt;
    &lt;p&gt;Understanding Threshold Map in Ordered Dithering.&lt;/p&gt;
    &lt;p&gt;tap/click the right side of the screen to go forward ‚Üí&lt;/p&gt;
    &lt;p&gt;Hi! Welcome back to part 2 of my dithering series!&lt;/p&gt;
    &lt;p&gt;‚Üê tap/click the left side to go back&lt;/p&gt;
    &lt;p&gt;If you missed part 1, please check it out first! I've covered how dithering simulates more colors than actually exist.&lt;/p&gt;
    &lt;p&gt;‚Üê or use arrow keys to navigate ‚Üí&lt;/p&gt;
    &lt;p&gt;This part dives into ordered dithering, a method using a threshold map to decide each pixel's final color*.&lt;/p&gt;
    &lt;p&gt;*Note: This series covers grayscale dithering to two colors only: black &amp;amp; white. Multi-color dithering is possible, but it will not be covered here.&lt;/p&gt;
    &lt;p&gt;There are many ways to build a threshold map, each creating a unique visual pattern.&lt;/p&gt;
    &lt;p&gt;I'll guide you through how it works and how it forms those unique patterns.&lt;/p&gt;
    &lt;p&gt;Please note that this isn‚Äôt a technical deep-dive. It‚Äôs just a visual exploration of the logic behind ordered dithering.&lt;/p&gt;
    &lt;p&gt;I personally grasp ideas much better when I can see them visually, so I hope this helps you too!&lt;/p&gt;
    &lt;p&gt;Let‚Äôs start!&lt;/p&gt;
    &lt;p&gt;First, a quick recap on quantization: the process of reducing colors in an image.&lt;/p&gt;
    &lt;p&gt;Grayscale images use many shades, from black to white.&lt;/p&gt;
    &lt;p&gt;Now, imagine our display can only show pure black and white.&lt;/p&gt;
    &lt;p&gt;We must convert those gray shades into black or white. This is what quantization does.&lt;/p&gt;
    &lt;p&gt;One way is by rounding: dark shades become black, and bright shades become white.&lt;/p&gt;
    &lt;p&gt;Another way is by setting a threshold. Anything below it turns black; anything above it turns white.&lt;/p&gt;
    &lt;p&gt;The threshold controls the quantization output: lower thresholds can turn darker shades white, while higher ones can turn brighter shades black.&lt;/p&gt;
    &lt;p&gt;Now, imagine using multiple thresholds at once, each with a different value.&lt;/p&gt;
    &lt;p&gt;Quantizing a single color now gives us a mix of black and white pixels.&lt;/p&gt;
    &lt;p&gt;The result reflects the original brightness: brighter inputs get more whites, while darker inputs get more blacks.&lt;/p&gt;
    &lt;p&gt;This is essentially how a threshold map works in ordered dithering.&lt;/p&gt;
    &lt;p&gt;It converts gray shades into black &amp;amp; white patterns that mimic the original image's brightness.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs try to apply this map to a full image by tiling it across the entire area.&lt;/p&gt;
    &lt;p&gt;Our image is now converted to black &amp;amp; white, but something is off. See those vertical line artifacts?&lt;/p&gt;
    &lt;p&gt;They are formed by the horizontal black &amp;amp; white patterns our map produces.&lt;/p&gt;
    &lt;p&gt;The problem is that the output directly mirrors the map's layout.&lt;/p&gt;
    &lt;p&gt;To fix this, we need to rearrange the threshold map.&lt;/p&gt;
    &lt;p&gt;Introducing: The Bayer matrix.&lt;/p&gt;
    &lt;p&gt;The thresholds are arranged in a 2x2 matrix, like this.&lt;/p&gt;
    &lt;p&gt;When applied, it generates this signature cross-hatch pattern.&lt;/p&gt;
    &lt;p&gt;This pattern keeps black and white pixels apart, preventing unwanted artifacts like those vertical lines.&lt;/p&gt;
    &lt;p&gt;The result is a uniform dispersion of black &amp;amp; white pixels that blend smoothly into shades of gray.&lt;/p&gt;
    &lt;p&gt;Now while this looks better, we still have a problem: the transitions between shades are quite harsh.&lt;/p&gt;
    &lt;p&gt;The issue? With only 4 threshold levels, we only have 4 patterns to represent the entire grayscale shades.&lt;/p&gt;
    &lt;p&gt;To improve this, we need to increase the number of our output patterns.&lt;/p&gt;
    &lt;p&gt;Here is a 4x4 Bayer matrix, which has 16 different threshold values.&lt;/p&gt;
    &lt;p&gt;It's an extension of the 2x2 matrix. Notice how the sequence is similar, just with different starting values.&lt;/p&gt;
    &lt;p&gt;Like the 2x2 version, it creates a cross-hatch pattern. But now with 16 distinct pattern variations.&lt;/p&gt;
    &lt;p&gt;These extra patterns allow us to represent 16 unique shades of gray instead of just four.&lt;/p&gt;
    &lt;p&gt;As a result, we get smoother transitions between shades.&lt;/p&gt;
    &lt;p&gt;This reduces harsh transitions between shadows and highlights in the final image.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs ordered dithering in a nutshell!&lt;/p&gt;
    &lt;p&gt;In summary, it uses a threshold map to translate shades into cleverly arranged patterns.&lt;/p&gt;
    &lt;p&gt;Now you might ask, can we use even more threshold levels? Or, can we arrange them differently?&lt;/p&gt;
    &lt;p&gt;The answer is yes! The threshold map's arrangement is what creates those unique dithering patterns. Let‚Äôs explore a few other ways to arrange them.&lt;/p&gt;
    &lt;p&gt;This is the pattern coming from an 8x8 Bayer Matrix. With 64 levels, it‚Äôs a further extension of the versions we‚Äôve seen so far.&lt;/p&gt;
    &lt;p&gt;It still has a similar cross-hatch pattern but offers even smoother transitions between shades.&lt;/p&gt;
    &lt;p&gt;It provides a denser and more detailed gradient, although the difference is fairly subtle.&lt;/p&gt;
    &lt;p&gt;Now let‚Äôs look beyond the Bayer family with this pattern coming from a 8x8 Cluster Dot matrix.&lt;/p&gt;
    &lt;p&gt;It consists of round dot clusters, quite different from Bayer‚Äôs cross-hatching.&lt;/p&gt;
    &lt;p&gt;It gives the image the classic, familiar feel of a printed newspaper.&lt;/p&gt;
    &lt;p&gt;Finally, my favorite method: Void and Cluster.&lt;/p&gt;
    &lt;p&gt;blue noise&lt;/p&gt;
    &lt;p&gt;The result is a pattern that feels less rigid than the Bayer cross-hatch.&lt;/p&gt;
    &lt;p&gt;In the final image, this creates a fine texture that blends into grays more naturally.&lt;/p&gt;
    &lt;p&gt;That‚Äôs all for now! There are still some more maps to explore, but we‚Äôll save those for another time.&lt;/p&gt;
    &lt;p&gt;If you want more examples and details, here are some resources you might want to explore:&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Libcaca study - 2. Halftoning&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Ditherpunk ‚Äî The article I wish I had about monochrome image dithering&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Robert Ulichney's paper on void-and-cluster dithering&lt;/p&gt;
    &lt;p&gt;I hope you enjoy what you read so far! In the next part, we‚Äôll look at another dithering method that doesn't use a map at all: Error Diffusion.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! See you in the next part.&lt;/p&gt;
    &lt;p&gt;visualrambling.space is created by Damar, someone who loves to exploring new topics and rambling about them visually.&lt;/p&gt;
    &lt;p&gt;If you like this, please consider following and sharing. I'll keep creating more visual articles like this!&lt;/p&gt;
    &lt;p&gt;I'm also open for collaborations or commissioned work. Feel free to reach out anytime.&lt;/p&gt;
    &lt;p&gt;Follow/contact me on:&lt;/p&gt;
    &lt;p&gt;https://x.com/damarberlari&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;https://bsky.app/profile/damarberlari.bsky.social&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;https://www.linkedin.com/in/damarpramudita/&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;../rss.xml&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;mailto: [email protected]&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;Topics covered: Three.js, WebGL, dithering, visualization, interactive learning&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46770274</guid><pubDate>Mon, 26 Jan 2026 19:23:54 +0000</pubDate></item></channel></rss>