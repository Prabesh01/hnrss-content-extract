<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 22 Dec 2025 13:49:19 +0000</lastBuildDate><item><title>CO2 batteries that store grid energy take off globally</title><link>https://spectrum.ieee.org/co2-battery-energy-storage</link><description>&lt;doc fingerprint="267d315709b6a538"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grid-Scale Bubble Batteries Will Soon Be Everywhere&lt;/head&gt;
    &lt;p&gt;When the sun sets on solar panels, these gas-filled domes take over&lt;/p&gt;
    &lt;p&gt;This giant bubble on the island of Sardinia holds 2,000 tonnes of carbon dioxide. But the gas wasn’t captured from factory emissions, nor was it pulled from the air. It came from a gas supplier, and it lives permanently inside the dome’s system to serve an eco-friendly purpose: to store large amounts of excess renewable energy until it’s needed.&lt;/p&gt;
    &lt;p&gt;Developed by the Milan-based company Energy Dome, the bubble and its surrounding machinery demonstrate a first-of-its-kind “CO2 Battery,” as the company calls it. The facility compresses and expands CO2 daily in its closed system, turning a turbine that generates 200 megawatt-hours of electricity, or 20 MW over 10 hours. And in 2026, replicas of this plant will start popping up across the globe.&lt;/p&gt;
    &lt;p&gt;We mean that literally. It takes just half a day to inflate the bubble. The rest of the facility takes less than two years to build and can be done just about anywhere there’s 5 hectares of flat land.&lt;/p&gt;
    &lt;p&gt;The first to build one outside of Sardinia will be one of India’s largest power companies, NTPC Limited. The company expects to complete its CO2 Battery sometime in 2026 at the Kudgi power plant in Karnataka, in India. In Wisconsin, meanwhile, the public utility Alliant Energy received the all clear from authorities to begin construction of one in 2026 to supply power to 18,000 homes.&lt;/p&gt;
    &lt;p&gt;And Google likes the concept so much that it plans to rapidly deploy the facilities in all of its key data-center locations in Europe, the United States, and the Asia-Pacific region. The idea is to provide electricity-guzzling data centers with round-the-clock clean energy, even when the sun isn’t shining or the wind isn’t blowing. The partnership with Energy Dome, announced in July, marked Google’s first investment in long-duration energy storage.&lt;/p&gt;
    &lt;p&gt;“We’ve been scanning the globe seeking different solutions,” says Ainhoa Anda, Google’s senior lead for energy strategy, in Paris. The challenge the tech giant has encountered is not only finding a long-duration storage option, but also one that works with the unique specs of every region. “So standardization is really important, and this is one of the aspects that we really like” about Energy Dome, she says. “They can really plug and play this.”&lt;/p&gt;
    &lt;p&gt;Google will prioritize placing the Energy Dome facilities where they’ll have the most impact on decarbonization and grid reliability, and where there’s a lot of renewable energy to store, Anda says. The facilities can be placed adjacent to Google’s data centers or elsewhere within the same grid. The companies did not disclose the terms of the deal.&lt;/p&gt;
    &lt;p&gt;Anda says Google expects to help the technology “reach a massive commercial stage.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting creative with long-duration energy storage&lt;/head&gt;
    &lt;p&gt;All this excitement is based on Energy Dome’s one full-size, grid-connected plant in Ottana, Sardinia, which was completed in July. It was built to help solve one of the energy transition’s biggest challenges: the need for grid-scale storage that can provide power for more than 8 hours at a time. Called long-duration energy storage, or LDES in industry parlance, the concept is the key to maximizing the value of renewable energy.&lt;/p&gt;
    &lt;p&gt;When sun and wind are abundant, solar and wind farms tend to produce more electricity than a grid needs. So storing the excess for use when these resources are scarce just makes sense. LDES also makes the grid more reliable by providing backup and supplementary power.&lt;/p&gt;
    &lt;p&gt;The problem is that even the best new grid-scale storage systems on the market—mainly lithium-ion batteries—provide only about 4 to 8 hours of storage. That’s not long enough to power through a whole night, or multiple cloudy and windless days, or the hottest week of the year, when energy demand hits its peak.&lt;/p&gt;
    &lt;p&gt;After the CO2 leaves the dome, it is compressed, cooled, reduced to a liquid, and stored in pressure vessels. To release the energy, the process reverses: The liquid is evaporated, heated, expanded, and then fed through a turbine that generates electricity. Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;Lithium-ion battery systems could be increased in size to store more and last longer, but systems of that size usually aren’t economically viable. Other grid-scale battery chemistries and approaches are in development, such as sodium-based, iron-air, and vanadium redox flow batteries. But the energy density, costs, degradation, and funding complications have challenged the developers of those alternatives.&lt;/p&gt;
    &lt;p&gt;Researchers have also experimented with storing energy by compressing air, heating up blocks or sand, using hydrogen or methanol, pressurizing water deep underground, and even dangling heavy objects in the air and dropping them. (The creativity devoted to LDES is impressive.) But geologic constraints, economic viability, efficiency, and scalability have hindered the commercialization of these strategies.&lt;/p&gt;
    &lt;p&gt;The tried-and-true grid-scale storage option—pumped hydro, in which water is pumped between reservoirs at different elevations—lasts for decades and can store thousands of megawatts for days. But these systems require specific topography, a lot of land, and can take up to a decade to build.&lt;/p&gt;
    &lt;p&gt;CO2 Batteries check a lot of boxes that other approaches don’t. They don’t need special topography like pumped-hydro reservoirs do. They don’t need critical minerals like electrochemical and other batteries do. They use components for which supply chains already exist. Their expected lifetime stretches nearly three times as long as lithium-ion batteries. And adding size and storage capacity to them significantly decreases cost per kilowatt-hour. Energy Dome expects its LDES solution to be 30 percent cheaper than lithium-ion.&lt;/p&gt;
    &lt;p&gt;China has taken note. China Huadian Corp. and Dongfang Electric Corp. are reportedly building a CO2-based energy-storage facility in the Xinjiang region of northwest China. Media reports show renderings of domes but give widely varying storage capacities—including 100 MW and 1,000 MW. The Chinese companies did not respond to IEEE Spectrum’s requests for information.&lt;/p&gt;
    &lt;p&gt;“What I can say is that they are developing something very, very similar [to Energy Dome’s CO2 Battery] but quite large in scale,” says Claudio Spadacini, Energy Dome’s founder and CEO. The Chinese companies “are good, they are super fast, and they have a lot of money,” he says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is Google investing in CO2 Batteries?&lt;/head&gt;
    &lt;p&gt;When I visited Energy Dome’s Sardinia facility in October, the CO2 had just been pumped out of the dome, so I was able to peek inside. It was massive, monochromatic, and pretty much empty. The inner membrane, which had been holding the uncompressed CO2, had collapsed across the entire floor. A few pockets of the gas remained, making the off-white sheet billow up in spots.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the translucent outer dome allowed some daylight to pass through, creating a creamy glow that enveloped the vast space. With no structural framing, the only thing keeping the dome upright was the small difference in pressure between the inside and outside air.&lt;/p&gt;
    &lt;p&gt;“This is incredible,” I said to my guide, Mario Torchio, Energy Dome’s global marketing and communications director.&lt;/p&gt;
    &lt;p&gt;“It is. But it’s physics,” he said.&lt;/p&gt;
    &lt;p&gt;Outside the dome, a series of machines connected by undulating pipes moves the CO2 out of the dome for compressing and condensing. First, a compressor pressurizes the gas from 1 bar (100,000 pascals) to about 55 bar (5,500,000 pa). Next, a thermal-energy-storage system cools the CO2 to an ambient temperature. Then a condenser reduces it into a liquid that is stored in a few dozen pressure vessels, each about the size of a school bus. The whole process takes about 10 hours, and at the end of it, the battery is considered charged.&lt;/p&gt;
    &lt;p&gt;To discharge the battery, the process reverses. The liquid CO2 is evaporated and heated. It then enters a gas-expander turbine, which is like a medium-pressure steam turbine. This drives a synchronous generator, which converts mechanical energy into electrical energy for the grid. After that, the gas is exhausted at ambient pressure back into the dome, filling it up to await the next charging phase.&lt;/p&gt;
    &lt;p&gt;Energy Dome engineers inspect the dryer system, which keeps the gaseous CO₂ in the dome at optimal dryness levels at all times.Luigi Avantaggiato&lt;/p&gt;
    &lt;p&gt;It’s not rocket science. Still, someone had to be the first to put it together and figure out how to do it cost-effectively, which Spadacini says his company has accomplished and patented. “How we seal the turbo machinery, how we store the heat in the thermal-energy storage, how we store the heat after condensing…can really cut costs and increase the efficiency,” he says.&lt;/p&gt;
    &lt;p&gt;The company uses pure, purpose-made CO2 instead of sourcing it from emissions or the air, because those sources come with impurities and moisture that degrade the steel in the machinery.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happens if the dome is punctured?&lt;/head&gt;
    &lt;p&gt;On the downside, Energy Dome’s facility takes up about twice as much land as a comparable capacity lithium-ion battery would. And the domes themselves, which are about the height of a sports stadium at their apex, and longer, might stand out on a landscape and draw some NIMBY pushback.&lt;/p&gt;
    &lt;p&gt;And what if a tornado comes? Spadacini says the dome can withstand wind up to 160 kilometers per hour. If Energy Dome can get half a day’s warning of severe weather, the company can just compress and store the CO2 in the tanks and then deflate the outer dome, he says.&lt;/p&gt;
    &lt;p&gt;If the worst happens and the dome is punctured, 2,000 tonnes of CO2 will enter the atmosphere. That’s equivalent to the emissions of about 15 round-trip flights between New York and London on a Boeing 777. “It’s negligible compared to the emissions of a coal plant,” Spadacini says. People will also need to stay back 70 meters or more until the air clears, he says.&lt;/p&gt;
    &lt;p&gt;Worth the risk? The companies lining up to build these systems seem to think so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid-Scale Battery Stabilizes Scottish Power Supply ›&lt;/item&gt;
      &lt;item&gt;Backing Up the Power Grid With Green Methanol ›&lt;/item&gt;
      &lt;item&gt;DOE Places Compressed-Air Energy Storage Loan Under Review ›&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345506</guid><pubDate>Sun, 21 Dec 2025 15:27:36 +0000</pubDate></item><item><title>Show HN: Books mentioned on Hacker News in 2025</title><link>https://hackernews-readings-613604506318.us-west1.run.app</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46345897</guid><pubDate>Sun, 21 Dec 2025 16:21:04 +0000</pubDate></item><item><title>Rue: Higher level than Rust, lower level than Go</title><link>https://rue-lang.dev/</link><description>&lt;doc fingerprint="d268deffe87878fc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Memory Safe&lt;/head&gt;
    &lt;p&gt;No garbage collector, no manual memory management. A work in progress, though.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple Syntax&lt;/head&gt;
    &lt;p&gt;Familiar syntax inspired by various programming languages. If you know one, you'll feel at home with Rue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fast Compilation&lt;/head&gt;
    &lt;p&gt;Direct compilation to native code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, Rue&lt;/head&gt;
    &lt;code&gt;// It's a classic for a reason
 

 
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348262</guid><pubDate>Sun, 21 Dec 2025 20:46:02 +0000</pubDate></item><item><title>More on whether useful quantum computing is “imminent”</title><link>https://scottaaronson.blog/?p=9425</link><description>&lt;doc fingerprint="6f11911b6025ddfc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;More on whether useful quantum computing is “imminent”&lt;/head&gt;
    &lt;p&gt;These days, the most common question I get goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A decade ago, you told people that scalable quantum computing wasn’t imminent. Now, though, you claim it plausibly is imminent. Why have you reversed yourself??&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I appreciated the friend of mine who paraphrased this as follows: “A decade ago you said you were 35. Now you say you’re 45. Explain yourself!”&lt;/p&gt;
    &lt;p&gt;A couple weeks ago, I was delighted to attend Q2B in Santa Clara, where I gave a keynote talk entitled “Why I Think Quantum Computing Works” (link goes to the PowerPoint slides). This is one of the most optimistic talks I’ve ever given. But mostly that’s just because, uncharacteristically for me, here I gave short shrift to the challenge of broadening the class of problems that achieve huge quantum speedups, and just focused on the experimental milestones achieved over the past year. With every experimental milestone, the little voice in my head that asks “but what if Gil Kalai turned out to be right after all? what if scalable QC wasn’t possible?” grows quieter, until now it can barely be heard.&lt;/p&gt;
    &lt;p&gt;Going to Q2B was extremely helpful in giving me a sense of the current state of the field. Ryan Babbush gave a superb overview (I couldn’t have improved a word) of the current status of quantum algorithms, while John Preskill’s annual where-we-stand talk was “magisterial” as usual (that’s the word I’ve long used for his talks), making mine look like just a warmup act for his. Meanwhile, Quantinuum took a victory lap, boasting of their recent successes in a way that I considered basically justified.&lt;/p&gt;
    &lt;p&gt;After returning from Q2B, I then did an hour-long podcast with “The Quantum Bull” on the topic “How Close Are We to Fault-Tolerant Quantum Computing?” You can watch it here:&lt;/p&gt;
    &lt;p&gt;As far as I remember, this is the first YouTube interview I’ve ever done that concentrates entirely on the current state of the QC race, skipping any attempt to explain amplitudes, interference, and other basic concepts. Despite (or conceivably because?) of that, I’m happy with how this interview turned out. Watch if you want to know my detailed current views on hardware—as always, I recommend 2x speed.&lt;/p&gt;
    &lt;p&gt;Or for those who don’t have the half hour, a quick summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In quantum computing, there are the large companies and startups that might succeed or might fail, but are at least trying to solve the real technical problems, and some of them are making amazing progress. And then there are the companies that have optimized for doing IPOs, getting astronomical valuations, and selling a narrative to retail investors and governments about how quantum computing is poised to revolutionize optimization and machine learning and finance. Right now, I see these two sets of companies as almost entirely disjoint from each other.&lt;/item&gt;
      &lt;item&gt;The interview also contains my most direct condemnation yet of some of the wild misrepresentations that IonQ, in particular, has made to governments about what QC will be good for (“unlike AI, quantum computers won’t hallucinate because they’re deterministic!”)&lt;/item&gt;
      &lt;item&gt;The two approaches that had the most impressive demonstrations in the past year are trapped ions (especially Quantinuum but also Oxford Ionics) and superconducting qubits (especially Google but also IBM), and perhaps also neutral atoms (especially QuEra but also Infleqtion and Atom Computing).&lt;/item&gt;
      &lt;item&gt;Contrary to a misconception that refuses to die, I haven’t dramatically changed my views on any of these matters. As I have for a quarter century, I continue to profess a lot of confidence in the basic principles of quantum computing theory worked out in the mid-1990s, and I also continue to profess ignorance of exactly how many years it will take to realize those principles in the lab, and of which hardware approach will get there first.&lt;/item&gt;
      &lt;item&gt;But yeah, of course I update in response to developments on the ground, because it would be insane not to! And 2025 was clearly a year that met or exceeded my expectations on hardware, with multiple platforms now boasting &amp;gt;99.9% fidelity two-qubit gates, at or above the theoretical threshold for fault-tolerance. This year updated me in favor of taking more seriously the aggressive pronouncements—the “roadmaps”—of Google, Quantinuum, QuEra, PsiQuantum, and other companies about where they could be in 2028 or 2029.&lt;/item&gt;
      &lt;item&gt;One more time for those in the back: the main known applications of quantum computers remain (1) the simulation of quantum physics and chemistry themselves, (2) breaking a lot of currently deployed cryptography, and (3) eventually, achieving some modest benefits for optimization, machine learning, and other areas (but it will probably be a while before those modest benefits win out in practice). To be sure, the detailed list of quantum speedups expands over time (as new quantum algorithms get discovered) and also contracts over time (as some of the quantum algorithms get dequantized). But the list of known applications “from 30,000 feet” remains fairly close to what it was a quarter century ago, after you hack away the dense thickets of obfuscation and hype.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m going to close this post with a warning. When Frisch and Peierls wrote their now-famous memo in March 1940, estimating the mass of Uranium-235 that would be needed for a fission bomb, they didn’t publish it in a journal, but communicated the result through military channels only. As recently as February 1939, Frisch and Meitner had published in Nature their theoretical explanation of recent experiments, showing that the uranium nucleus could fission when bombarded by neutrons. But by 1940, Frisch and Peierls realized that the time for open publication of these matters had passed.&lt;/p&gt;
    &lt;p&gt;Similarly, at some point, the people doing detailed estimates of how many physical qubits and gates it’ll take to break actually deployed cryptosystems using Shor’s algorithm are going to stop publishing those estimates, if for no other reason than the risk of giving too much information to adversaries. Indeed, for all we know, that point may have been passed already. This is the clearest warning that I can offer in public right now about the urgency of migrating to post-quantum cryptosystems, a process that I’m grateful is already underway.&lt;/p&gt;
    &lt;p&gt;Update: Someone on Twitter who’s “long $IONQ” says he’ll be posting about and investigating me every day, never resting until UT Austin fires me, in order to punish me for slandering IonQ and other “pure play” SPAC IPO quantum companies. And also, because I’ve been anti-Trump and pro-Biden. He confabulates that I must be trying to profit from my stance (eg by shorting the companies I criticize), it being inconceivable to him that anyone would say anything purely because they care about what’s true.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348318</guid><pubDate>Sun, 21 Dec 2025 20:53:34 +0000</pubDate></item><item><title>A guide to local coding models</title><link>https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude</link><description>&lt;doc fingerprint="2f539dd2343f89db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;[Revised] You Don’t Need to Spend $100/mo on Claude Code: Your Guide to Local Coding Models&lt;/head&gt;
    &lt;head rend="h3"&gt;What you need to know about local model tooling and the steps for setting one up yourself&lt;/head&gt;
    &lt;p&gt;[Edit 1] This article has been edited after initial release for clarity. Both the tl;dr and the end section have added information.&lt;/p&gt;
    &lt;p&gt;[Edit 2] This hypothesis was actually wrong and thank you to everyone who commented!&lt;/p&gt;
    &lt;p&gt;Here’s a full explanation of where I went wrong. I want to address this mistake as I realize it might have a meaningful impact on someone's financial position.&lt;/p&gt;
    &lt;p&gt;I’m not editing the actual article except where absolutely necessary so it doesn’t look like I’m covering up the mistake—I want to address it. Instead, I’ve included the important information below.&lt;/p&gt;
    &lt;p&gt;There is one takeaway this article provides that definitely holds true:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local models are far more capable than they’re given credit for, even for coding.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also explains the process of setting up a local coding model and technical information about doing so which is helpful for anyone wanting to set up a local coding model. I would still recommend doing so.&lt;/p&gt;
    &lt;p&gt;But do I want someone reading this to immediately drop their coding subscription and buy a maxed out MacBook Pro? No, and for that reason I need to correct my hypothesis from ‘Yes, with caveats’ to ‘No’.&lt;/p&gt;
    &lt;p&gt;This article was not an empirical assessment, but should have been to make these claims. Here’s where I went wrong:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;While local models can likely complete ~90% of the software development tasks that something like Claude Code can, the last 10% is the most important. When it comes to your job, that last 10% is worth paying more for to get that last bit of performance.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I realized I looked at this more from the angle of a hobbiest paying for these coding tools. Someone doing little side projects—not someone in a production setting. I did this because I see a lot of people signing up for $100/mo or $200/mo coding subscriptions for personal projects when they likely don’t need to. I would not recommend running local models as a company instead of giving employees access to a tool like Claude Code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;While larger local models are very capable, as soon as you run other development tools (Docker, etc.) that also eat into your RAM, your model needs to be much smaller and becomes a lot less capable. I didn’t factor this in in my experiment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, really, the takeaway should be that these are incredible supplemental models to frontier models when coding and could potentially save you on your subscription by dropping it down a tier, but practically they’re not worth the effort in situations that might affect your livelihood.&lt;/p&gt;
    &lt;p&gt;Exactly a month ago, I made a hypothesis: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price (and have better hardware too!).&lt;/p&gt;
    &lt;p&gt;So, to create by far the most expensive article I’ve ever written, I put my money where my mouth is and bought a MacBook Pro with 128 GB of RAM to get to work. My idea was simple: Over the life of the MacBook I’d recoup the costs of it by not paying for an AI coding subscription.&lt;/p&gt;
    &lt;p&gt;After weeks of experimenting and setting up local AI models and coding tools, I’ve come to the conclusion that my hypothesis was &lt;del&gt;correct, with nuance&lt;/del&gt;, not correct [see edit 2 above] which I’ll get into later in this article.&lt;/p&gt;
    &lt;p&gt;In this article, we cover:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Why local models matter and the benefits they provide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How to view memory usage and make estimates for which models can run on your machine and the RAM demands for coding applications.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Walk through setting up your own local coding model and tool step-by-step.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Don’t worry if you don’t have a high-RAM machine! You can still follow this guide. I’ve included some models to try out with a lower memory allotment. I think you’ll be surprised at how performant even the smallest of models is. In fact, there hasn’t really been a time during this experiment that I’ve been disappointed with model performance.&lt;/p&gt;
    &lt;p&gt;If you’re only here for the local coding tool setup, skip to the section at the bottom. I’ve even included a link to my modelfiles in that section to make setup even easier for you. Otherwise, let’s get into what you need to know.&lt;/p&gt;
    &lt;head rend="h2"&gt;tl;dr:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Local coding models are very capable. Using the right model and the right tooling feels only half a generation behind the frontier cloud tools. I would say that for about 90% of developer work local models are more than sufficient. Even small 7B parameter models can be very capable. [Edited to add in this next part] Local models won’t compete with frontier models at the peak of performance, but can complete many coding tasks just as well for a fraction of the cost. They’re worth running to bring costs down on plenty of tasks but potentially not worth using if there’s a free tier available that performs better.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools matter a lot. This is where I experienced the most disappointment. I tried many different tools with many different models and spent a lot of time tinkering. I ran into situations where the models wouldn’t call tools properly or their thinking traces wouldn’t close. Both of these rendered the tool essentially useless. Currently, tooling seems very finicky and if there’s anything developers need to be successful, it’s good tools.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There’s a lot to consider when you’re actually working within hardware constraints. We take the tooling set up for us in the cloud for granted. When setting up local models, I had to think a lot about trade-offs in performance versus memory usage, how different tools compared and affected performance, nuances in types of models, how to quantize, and other user-facing factors such as time-to-first-token and tokens per second.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Google threw a wrench into my hypothesis. The local setup is almost a no-brainer when compared to a $100/mo+ subscription. Compared to free or nearly-free tooling (such as Gemini CLI, Jules, or Antigravity) there isn’t quite as strong of a monetary justification to spend more on hardware. There are benefits to local models outside of code, though, and I discuss those below.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the tl;dr was helpful, don’t forget to subscribe to get more in your inbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why local models?&lt;/head&gt;
    &lt;p&gt;You might wonder why local models are worth investing in at all. The obvious answer is cost. By using your own hardware, you don’t need to pay a subscription fee to a cloud provider for your tool. There are also a few less obvious and underrated reasons that make local models useful.&lt;/p&gt;
    &lt;p&gt;First: Reliability. Each week there seems to be complaints about performance regression within AI coding tools. Many speculate companies are pulling tricks to save resources that hurt model performance. With cloud providers, you’re at the mercy of the provider for when this happens. With local models, this only happens when you cause it to.&lt;/p&gt;
    &lt;p&gt;Second: Local models can apply to far more applications. Just the other day I was having a discussion with my dad about AI tooling he could use to streamline his work. His job requires studying a lot of data—a perfect application for an LLM-based tool—but his company blocks tools like Gemini and ChatGPT because a lot of this analysis is done on intellectual property. Unfortunately, he isn’t provided a suitable alternative to use.&lt;/p&gt;
    &lt;p&gt;With a local model, he wouldn’t have to worry about these IP issues. He could run his analyses without data ever leaving his machine. Of course, any tool calling would also need to ensure data never leaves the machine, but local models get around one of the largest hurdles for useful enterprise AI adoption. Running models on a local machine opens up an entire world of privacy- and security-centric AI applications that are expensive for cloud providers to provide.&lt;/p&gt;
    &lt;p&gt;Finally: Availability. Local models are available to you as long as your machine is. This means no worrying about your provider being down or rate limiting you due to high traffic. It also means using AI coding tools on planes or in other situations where internet access is locked down (think highly secure networks).&lt;/p&gt;
    &lt;p&gt;While local models do provide significant cost savings, the flexibility and reliability they provide can be even more valuable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Understanding memory&lt;/head&gt;
    &lt;p&gt;To get going with local models you must understand the memory needed to run them on your machine. Obviously, if you have more memory you’ll be able to run better models, but understanding the nuances of that memory management will help you pick out the right model for your use case.&lt;/p&gt;
    &lt;p&gt;Local AI has two parts that eat up your memory: The model itself and the model’s context window.&lt;/p&gt;
    &lt;p&gt;The actual model has billions of parameters and all those parameters need to fit into your memory at once. Excellent local coding models start at around 30 billion (30B, for short) parameters in size. By default, these models use 16 bits to represent parameters. At 16 bits with 30B parameters, a model will take 60 GB of space in RAM (16 bits = 2 bytes per parameter, 30 billion parameters = 60 billion bytes which equals about 60 GB).&lt;/p&gt;
    &lt;p&gt;The second (and potentially larger) memory consuming part of local AI is the model’s context window. This is the model inputs and outputs that are stored so the model can reference them in future requests. This gives the model memory.&lt;/p&gt;
    &lt;p&gt;When coding with AI, we prefer this window to be as large as it can because we need to fit our codebase (or pieces of it) within our context window. This means we target a context window of 64,000 tokens or larger. All of these tokens will also be stored in RAM.&lt;/p&gt;
    &lt;p&gt;The important thing to understand about context windows is that the memory requirement per-token for a model depends on the size of that model. Models with more parameters tend to have large architectures (more hidden layers and larger dimensions to those layers). Larger architectures mean the model must store more information for each token within its key-value cache (context window) because it stores information for each token for each layer.&lt;/p&gt;
    &lt;p&gt;This means choosing an 80B parameter model over a 30B parameter model requires more memory for the model itself and also more memory for the same size context window. For example, a 30B parameter model might have a hidden dimension of 5120 with 64 layers while an 80B model has a hidden dimension of 8192 with 80 layers. Doing some back-of-the-napkin math shows us that the larger model requires approximately 2x more RAM to maintain the same context window as the 30B parameter model (see formula below).&lt;/p&gt;
    &lt;p&gt;Luckily, there are tricks to better manage memory. First, there are architectural changes that can be made to make model inference more efficient so it requires less memory. The model we set up at the end of this article uses Hybrid Attention which enables a much smaller KV cache enabling us to fit our model and context window in less memory. I won’t get into more detail in this article, but you can read more about that model and how it works here.&lt;/p&gt;
    &lt;p&gt;The second trick is quantizing the values you’re working with. Quantization means converting a continuous set of values into a smaller amount of distinct values. In our case, that means taking a set of numbers represented by a certain number of bits (16, for example) and reducing it to a set of numbers represented by fewer bits (8, for example). To put it simply, in our case we’re converting the numbers representing our model to a smaller bit representation to save memory while keeping the value representations within the model relatively equal.&lt;/p&gt;
    &lt;p&gt;You can quantize both your model weights and the values stored in your context window. When you quantize your model weights, you “remove intelligence” from the model because it’s less precise in its representation of innate information. I’ve also found the performance hit when going from 16 to 8 bits within the model to be much less than 8 to 4.&lt;/p&gt;
    &lt;p&gt;We can also quantize the values in our context window to reduce its memory requirement. This means we’re less precisely representing the model’s memory. Generally speaking, KV cache (context window) quantization is considered more destructive to model performance than weight quantization because it causes the model to forget details in long reasoning traces. Thus, you should test quantizing the KV cache to ensure it doesn’t degrade model performance for your specific task.&lt;/p&gt;
    &lt;p&gt;In reality, like the rest of machine learning, optimizing local model performance is an experimentation process and real-world machine learning requires understanding the practical limitations and capabilities of models when applied to specific applications.&lt;/p&gt;
    &lt;p&gt;Here are a few more factors to understand when setting up a local coding model on your hardware:&lt;/p&gt;
    &lt;head rend="h3"&gt;Instruct versus non-instruct&lt;/head&gt;
    &lt;p&gt;Instruct models are post-trained to be well-suited for chat-based interactions. They’re given chat pairings in their training to be optimized for excellent back-and-forth chat output. Non-instruct models are still trained LLMs, but focus on next-token prediction instead of chatting with a user. For our case, when using a chat-based coding tool (CLI or chat agent in your IDE) we need to use an instruct model. If you’re setting up an autocomplete model, you’ll want to find a model specifically post-trained for it (such as Qwen2.5-Coder-Base or DeepSeek-Coder-V2).&lt;/p&gt;
    &lt;head rend="h3"&gt;Serving tools&lt;/head&gt;
    &lt;p&gt;You need a tool to serve your local LLM for your coding tool to send it requests. On a MacBook, there are two primary options: MLX and Ollama.&lt;/p&gt;
    &lt;p&gt;Ollama is the industry standard and works on non-Mac hardware. It’s a great serving setup on top of llama.cpp that makes model serving almost plug-and-play. Users can download model weights from Ollama easily and can configure modelfiles with custom parameters for serving. Ollama can also serve a model once and make it available to multiple tools.&lt;/p&gt;
    &lt;p&gt;MLX is a Mac-specific framework for machine learning that is optimized specifically for Mac hardware. It also retrieves models for the user from a community collection. I’ve found Ollama to be very reliable in its model catalog, while MLX’s catalog is community sourced and can sometimes be missing specific models. Models are sourced from the community so a user can convert a model to MLX format themselves. MLX requires a bit more setup on the user’s end, but serves models faster because it doesn’t have a layer providing the niceties of Ollama on top of it.&lt;/p&gt;
    &lt;p&gt;Either of these is great, but I chose MLX to maximize what I can get with my RAM, but Ollama is probably the more beginner-friendly tool here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Time-to-first-token and tokens per second&lt;/head&gt;
    &lt;p&gt;In real-world LLM applications it’s important that the model is able to serve its first token for a request in a reasonable amount of time and continue serving tokens at a speed that enables the user to use the model for its given purpose. If we have a high-performance model running locally, but it only serves a few tokens per second, it wouldn’t be useful for coding.&lt;/p&gt;
    &lt;p&gt;This is something taken for granted with cloud-hosted models that is a real consideration when working locally on constrained hardware. Another reason I chose MLX as my serving platform is because it served tokens up to 20% faster than Ollama. In reality, Ollama served tokens fast enough so I don’t think using MLX is necessary specifically for this reason for the models I tried.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance trade-offs&lt;/head&gt;
    &lt;p&gt;There are many ways to optimize local models and save RAM. It’s difficult to know which optimization method works best and the impact each has on a model especially when using them in tandem with other methods.&lt;/p&gt;
    &lt;p&gt;The right optimization method also depends on the application. In my experience, I find it best to prioritize larger models with more aggressive model quantization over smaller models with more precise model weights. Since our application is coding, I would also prioritize a less-quantized KV cache and using a smaller model to ensure reasoning works properly while not sacrificing the size of our context window.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding tools&lt;/head&gt;
    &lt;p&gt;There are many tools to code with local models and I suggest trying until you find one you like. Some top recommendations are OpenCode, Aider, Qwen Code, Roo Code, and Continue. Make sure to use a tool compatible with OpenAI’s API standard. While this should be most tools, this ensures a consistent model/tool connection. This makes it easier to switch between tools and models as needed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting set up&lt;/head&gt;
    &lt;p&gt;I’ll spare you the trial and error I experienced getting this set up. The one thing I learned is that tooling matters a lot. Not all coding tools are created equal and not all of the models interact with tools equally. I experienced many times where tool calling or even running a tool at all was broken. I also had to tinker quite a bit with many of them to get them to work.&lt;/p&gt;
    &lt;p&gt;If you’re a PC enthusiast, an apt comparison to setting up local coding tools versus using the cloud offerings available is the difference between setting up a MacBook versus a Linux Laptop. With the Linux laptop, you might get well through the distro installation only to find that the drivers for your trackpad aren’t yet supported. Sometimes it felt like that with local models and hooking them to coding tools.&lt;/p&gt;
    &lt;p&gt;For my tool, I ended up going with Qwen Code. It was pretty plug-and-play as it’s a fork of Gemini CLI. It supports the OpenAI compatibility standard so I can easily sub in different models and affords me all of the niceties built into Gemini CLI that I’m familiar with using. I also know it’ll be supported because both the Qwen team and Google DeepMind are behind the tool. The tool is also open source so anyone can support it as needed.&lt;/p&gt;
    &lt;p&gt;For models, I focused on GPT-OSS and Qwen3 models since they were around the size I was looking for and had great reviews for coding. I ended up deciding to use Qwen3-Coder models because I found it performed best and because GPT-OSS frequently gave me “I cannot fulfill this request” responses when I asked it to build features.&lt;/p&gt;
    &lt;p&gt;I decided to serve my local models on MLX, but if you’re using a non-Mac device give Ollama a shot. A MacBook is an excellent machine for serving local models because of its unified memory architecture. This means the RAM can be allotted to the CPU or GPU as needed. MacBooks can also be configured with a ton of RAM. For serving local coding models, more is always better.&lt;/p&gt;
    &lt;p&gt;I’ve shared my modelfiles repo for you to reference and use as needed. I’ve got a script set up that automates much of the below process. Feel free to fork it and create your own modelfiles or star it to come back later.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install MLX or download Ollama (the rest of this guide will continue with MLX but details for serving on Ollama can be found here).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increase the VRAM limitation on your MacBook. macOS will automatically limit VRAM to 75% of the total RAM. We want to use more than that. Run sudo sysctl iogpu.wired_limit_mb=110000 in your terminal to set this up (adjust the mb setting according to the RAM on your MacBook). This needs to be set each time you restart your MacBook.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run pip install -U mlx-lm to install MLX for serving community models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Serve the model as an OpenAI compatible API using python -m mlx_lm.server --model mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit. This command both runs the server and downloads the model for you if you haven’t yet. This particular model is what I’m using with 128GB of RAM. If you have less RAM, check out smaller models such as mlx-community/Qwen3-4B-Instruct-2507-4bit (8 GB RAM), mlx-community/Qwen2.5-14B-Instruct-4bit (16 GB RAM), mlx-community/Qwen3-Coder-30B-A3B-Instruct-4bit (32 GB RAM), or mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit (64-96 GB RAM).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download Qwen Code. You might need to install Node Package Manager for this. I recommend using Node Version Manager (nvm) for managing your npm version.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Set up your tool to access an OpenAI compatible API by entering the following settings:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Base URL: http://localhost:8080/v1 (should be the default MLX serves your model at)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;API Key: mlx&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Model Name: mlx-community/Qwen3-Next-80B-A3B-Instruct-8bit (or whichever model you chose).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Voila! Your coding model tool should be working with your local coding model.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I recommend opening Activity Monitor on your Mac to monitor memory usage. I’ve had cases where I thought a model should fit within my memory allotment but it didn’t and I ended up using a lot of swap memory. When this happens your model will run very slowly.&lt;/p&gt;
    &lt;p&gt;One tip I have for using local coding models: Focus on managing your context. This is a great skill even with cloud-based models. People tend to YOLO their chats and fill their context window, but I’ve found greater performance by ensuring that just what my model needs is sitting in my context window. This is even more important with local models that may need an extra boost in performance and are limited in their context.&lt;/p&gt;
    &lt;head rend="h2"&gt;Was my hypothesis correct?&lt;/head&gt;
    &lt;p&gt;My original hypothesis was: Instead of paying $100/mo+ for an AI coding subscription, my money would be better spent upgrading my hardware so I can run local coding models at a fraction of the price.&lt;/p&gt;
    &lt;p&gt;I would argue that&lt;del&gt;—yes!—&lt;/del&gt;no [see edit 2 above], it is correct. If we crunch the numbers, a MacBook with 128 GB is $4700 plus tax. If I spend $100/mo for 5 years, a coding subscription would cost $6000 in that same amount of time. Not only do I save money, but I also get a much more capable machine for anything else I want to do with it.&lt;/p&gt;
    &lt;p&gt;[This paragraph was added in after initial release of this article] It’s important to note that local models will not reach the peak performance of frontier models; however, they will likely be able to do most tasks just as well. The value of using a local model doesn’t come from raw performance, but from supplementing the cost of higher performance models. A local model could very well let you drop your subscription tier for a frontier coding tool or utilize a free tier as needed for better performance and run the rest of your tasks for free.&lt;/p&gt;
    &lt;p&gt;It’s also important to note that local models are only going to get better and smaller. This is the worst your local coding model will perform. I also wouldn’t be surprised if cloud-based AI coding tools get more expensive. If you figure you’re using greater than the $100/mo tier right now or that the $100/mo tier will cost $200/mo in the future, the purchase is a no-brainer. It’s just difficult to stomach the upfront cost.&lt;/p&gt;
    &lt;p&gt;From a performance standpoint, I would say the maximum model running on my 128 GB RAM MacBook right now feels about half a generation behind the frontier coding tools. That’s excellent, but something to keep in mind as that half a generation might matter to you.&lt;/p&gt;
    &lt;p&gt;One wrench thrown into my experiment is how much free quota Google hands out with their different AI coding tools. It’s easy to purchase expensive hardware when it saves you money in the long run. It’s much more difficult when the alternative is free.&lt;/p&gt;
    &lt;p&gt;Initially, I considered my local coding setup to be a great pair to Google’s free tier. It definitely performs better than Gemini 2.5 Flash and makes a great companion to Gemini 3 Pro. Gemini 3 Pro can solve more complex tasks with the local model doing everything else. This not only saves quota on 3 Pro but also provides a very capable fallback for when quota is hit.&lt;/p&gt;
    &lt;p&gt;However, this is foiled a bit now that Gemini 3 Flash was just announced a few days ago. It shows benchmark numbers much more capable than Gemini 2.5 Flash (and even 2.5 Pro!) and I’ve been very impressed with its performance. If that’s the free tier Google offers, it makes local coding models less fiscally reasonable. The jury is still out on how well Gemini 3 Flash will perform and how quota will be structured, but we’ll have to see if local models can keep up.&lt;/p&gt;
    &lt;p&gt;I’m very curious to hear what you think! Tell me about your local coding setup or ask any questions below.&lt;/p&gt;
    &lt;p&gt;Thanks for reading!&lt;/p&gt;
    &lt;p&gt;Always be (machine) learning,&lt;/p&gt;
    &lt;p&gt;Logan&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348329</guid><pubDate>Sun, 21 Dec 2025 20:55:15 +0000</pubDate></item><item><title>Show HN: Rust/WASM lighting data toolkit – parses legacy formats, generates SVGs</title><link>https://eulumdat.icu</link><description>&lt;doc fingerprint="714bd42804c754b4"&gt;
  &lt;main&gt;
    &lt;p&gt;This application requires JavaScript to run.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348344</guid><pubDate>Sun, 21 Dec 2025 20:56:54 +0000</pubDate></item><item><title>Disney Imagineering Debuts Next-Generation Robotic Character, Olaf</title><link>https://disneyparksblog.com/disney-experiences/robotic-olaf-marks-new-era-of-disney-innovation/</link><description>&lt;doc fingerprint="ebf30b7df696007b"&gt;
  &lt;main&gt;
    &lt;p&gt;Disneyland Paris saw a groundbreaking moment today, where Bruce Vaughn, President and Chief Creative Officer of Walt Disney Imagineering, and Natacha Rafalski, Présidente of Disneyland Paris, introduced a next-generation robotic character representing Olaf, the beloved snowman from Walt Disney Animation Studios’ Frozen.&lt;/p&gt;
    &lt;p&gt;This debut marks a new chapter in Disney character innovation, one where technology, storytelling, and collaboration come together to bring screen to reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Innovation at the Core: From Screen to Reality&lt;/head&gt;
    &lt;p&gt;From the way he moves to the way he looks, every gesture and detail is crafted to reflect the Olaf audiences have seen in the film — alive, curious, and unmistakably himself. As for his snow-like shimmer that catches the light just like fresh snow, this was enhanced by iridescent fibers. These details make Olaf one of the most expressive and true-to-life characters built, and he’s soon making his debut at Disney parks.&lt;/p&gt;
    &lt;p&gt;Our roots are in animation with Walt Disney pioneering early hand-drawn films and today, Walt Disney Animation Studios and Pixar Animation Studios continue that tradition. We collaborated closely with the film’s original animators at Walt Disney Animation Studios to ensure every gesture felt true to the character. This isn’t just about replicating the animation, it’s about emulating the creators’ intent.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Technology Behind the Magic&lt;/head&gt;
    &lt;p&gt;Home to some of the best storytellers in the world, we’re continuously pushing the boundaries of innovation and technology — in fact it is in our DNA.&lt;/p&gt;
    &lt;p&gt;Like everything at Disney, we always start with the story, and our number one priority is to build storytelling technology that empowers our Disney Imagineers to breathe life into our characters.&lt;/p&gt;
    &lt;p&gt;While the BDX droids — the Star Wars free roaming robotic characters that mimic movements in a simulation — have been interacting with guests for a while now, Olaf presents a far greater challenge: an animated character with non-physical movements. To make Olaf as authentic as possible, the team used a branch of artificial intelligence called reinforcement learning, pushing the limits of hardware to achieve the creative intent of the artists.&lt;/p&gt;
    &lt;p&gt;It takes humans years to master walking and even longer to perform graceful motions. Deep reinforcement learning helps him acquire these skills in a fraction of the time.&lt;/p&gt;
    &lt;p&gt;Olaf’s “snow” also moves differently than the hard shells of other robotic characters, and he can fully articulate his mouth, eyes, and removable carrot nose and arms. Most importantly, Olaf can speak and engage in conversations, creating a truly one-of-a-kind experience.&lt;/p&gt;
    &lt;p&gt;Innovation takes many forms across our parks, experiences, and products – all focused on improving the guest experience and bringing joy to fans around the world. And what’s most exciting is that we’re just getting started!&lt;/p&gt;
    &lt;p&gt;The BDX Droids, self-balancing H.E.R.B.I.E., and now Olaf represent increasing levels of performance and innovation in bringing Disney characters to life. The speed at which we can create new characters and introduce them to guests is unprecedented. We’re scaling bigger than ever, working to bring more emotive, expressive, and surprising characters to our experiences around the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Guests Can See Olaf&lt;/head&gt;
    &lt;p&gt;Olaf will soon venture out into the unknown, eager to see guests at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arendelle Bay Show in World of Frozen, the new immersive world coming soon to Disney Adventure World at Disneyland Paris.&lt;/item&gt;
      &lt;item&gt;Limited-time special appearances at World of Frozen at Hong Kong Disneyland Resort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Looking for a warm hug now? You can discover how Olaf, along with other exciting breakthroughs from Walt Disney Imagineering Research &amp;amp; Development, came to life at in the latest episode of We Call It Imagineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46348847</guid><pubDate>Sun, 21 Dec 2025 21:46:20 +0000</pubDate></item><item><title>ONNX Runtime and CoreML May Silently Convert Your Model to FP16</title><link>https://ym2132.github.io/ONNX_MLProgram_NN_exploration</link><description>&lt;doc fingerprint="6d59dd17b5120e18"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ONNX Runtime &amp;amp; CoreML May Silently Convert Your Model to FP16 (And How to Stop It)&lt;/head&gt;
    &lt;p&gt;Running an ONNX model in ONNX RunTime (ORT) with the CoreMLExecutionProvider may change the predictions your model makes implicitly and you may observe differences when running with PyTorch on MPS or ONNX on CPU. This is because the default arguments ORT uses when converting your model to CoreML will cast the model to FP16.&lt;/p&gt;
    &lt;p&gt;The fix is to use the following setup when creating the InferenceSession:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;p&gt;This ensures your model remains in FP32 when running on a Mac GPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Uncovering an Issue in ONNX Runtime - Benchmarking the EyesOff Model&lt;/head&gt;
    &lt;p&gt;Having trained the EyesOff model, I began evaluating the model and its run time. I was looking into the ONNX format and using it to run the model efficiently. I setup a little test bench in which I ran the model using PyTorch and ONNX with ONNX Runtime (ORT), both using MPS and CPU. While checking the outputs, I noticed that the metrics from the model ran on ONNX on MPS had a different output to those on ONNX CPU and PyTorch CPU and MPS. Note, the metrics from PyTorch on CPU and MPS were the same.&lt;/p&gt;
    &lt;p&gt;When I say ORT and MPS, this is achieved through ORT’s execution providers. To run an ONNX model on the Mac GPU you have to use the CoreMLExecutionProvider (more on this to come).&lt;/p&gt;
    &lt;p&gt;Now in Figure 1 and 2, observe the metric values - the PyTorch ones (Figure 1) are the same across CPU and MPS, this isn’t the same story for ONNX (Figure 2):&lt;/p&gt;
    &lt;p&gt;Wow, look at the diff in Figure 2! When I saw this it was quite concerning, floating point math can lead to differences in the calculations carried out across the GPU and CPU but the values here don’t appear to be a result of floating point math, the values are too large.&lt;/p&gt;
    &lt;p&gt;Given the difference in metrics, I was worried that running the model with ORT was changing the output of the model and hence the behaviour. The reason the metrics change is because some of the model predictions around the threshold flipped to the opposite side of the threshold (which is 0.5), this can be seen in the confusion matrices for the ONNX CPU run and MPS run:&lt;/p&gt;
    &lt;head rend="h4"&gt;FP32 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;207 (TN)&lt;/cell&gt;
        &lt;cell&gt;24 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;69 (FN)&lt;/cell&gt;
        &lt;cell&gt;164 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;FP16 Confusion Matrix&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Predicted Negative&lt;/cell&gt;
        &lt;cell role="head"&gt;Predicted Positive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Actual Negative&lt;/cell&gt;
        &lt;cell&gt;206 (TN)&lt;/cell&gt;
        &lt;cell&gt;25 (FP)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Actual Positive&lt;/cell&gt;
        &lt;cell&gt;68 (FN)&lt;/cell&gt;
        &lt;cell&gt;165 (TP)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So two predictions flipped from negative to positive.&lt;/p&gt;
    &lt;p&gt;Having said that, the first thing I did was to make my life easier, by simplifying the scenario from the large EyesOff model to a simple one layer MLP and using that to run the experiments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why am I Using ONNX and ONNX RunTime?&lt;/head&gt;
    &lt;p&gt;Before going on it’s worth discussing what ONNX and ORT are, and why I’m even using them in the first place.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX1&lt;/head&gt;
    &lt;p&gt;ONNX stands for Open Neural Network Exchange. It can be thought of as a common programming language in which to describe ML models. Under the hood ONNX models are represented as graphs, these graphs outline the computation path of a model and it shows the operators and transformations required to get from input to prediction. These graphs are called ONNX graphs.&lt;/p&gt;
    &lt;p&gt;The use of a common language to describe models makes deployment easier and in some cases can add efficiency in terms of resource usage + or inference speed. Firstly, the ONNX graph itself can be optimised. Take PyTorch for example, you train the model in it and sure PyTorch is very mature and extremely optimised but it’s such a large package some things can be overlooked or difficult to change. By converting the model to ONNX, you take advantage of the fact that ONNX was built specifically with inference in mind and with that comes optimisations which the PyTorch team could implement but have not yet.&lt;/p&gt;
    &lt;p&gt;Furthermore, ONNX models can be ran cross platform in specialised runtimes. These runtimes are specially optimised for different architectures and add another layer of efficiency gains.&lt;/p&gt;
    &lt;head rend="h3"&gt;ONNX RunTime (ORT)2&lt;/head&gt;
    &lt;p&gt;ORT is one of these runtimes. ORT actually runs the model, it can be thought of as an interpreter, it takes the ONNX graph and actually implements the operators and runs them on the specified hardware. ORT has a lot of magic built into it, the operators are extremely optimised and through the use of execution providers they target a wide range of hardware. Each execution provider is optimised for the specific hardware it refers to, this enables the ORT team to implement extremely efficient operators giving us another efficiency gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML3&lt;/head&gt;
    &lt;p&gt;As mentioned before, I used the CoreMLExecutionProvider to run the model on a Mac GPU. This execution provider informs ORT to make use of CoreML. CoreML is an apple developed framework which lets models (neural networks and classical ML models) run on Apple hardware, CPU, GPU and ANE. ORT’s purpose in this phase is to take the ONNX graph and convert it to a CoreML model. CoreML is Apple’s answer to running efficient on device models on Apple hardware.&lt;/p&gt;
    &lt;p&gt;Note, that all of this doesn’t always mean the model will run faster. Some models may run faster in PyTorch, TensorRT or any other framework. This is why it is important to benchmark and test as many approaches as makes sense.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Source of the CPU vs MPS Difference - With an MLP&lt;/head&gt;
    &lt;p&gt;The MLP used is very simple it has a single layer, with 4 inputs, 3 outputs and the bias turned off. So, I pretty much created a fancy matrix multiplication.&lt;/p&gt;
    &lt;p&gt;To understand where the issue was coming from I ran this MLP through some different setups:&lt;/p&gt;
    &lt;code&gt;- PyTorch CPU
- PyTorch MPS
- ORT CPU
- ORT MPS
- CoreML FP32
- CoreML FP16&lt;/code&gt;
    &lt;p&gt;The goal of this exercise is to find out if 1 - the difference in outputs is seen in a simple model and 2 - to figure out where exactly the issue arises.&lt;/p&gt;
    &lt;p&gt;Before showing the full results, I want to explain why I included the CoreML FP16 and FP32 runs - specifically why the FP16 one. When I initially ran the MLP experiment I only ran PyTorch, ORT and CoreML FP32 but the output numbers of ORT MPS looked like FP16 numbers. So, I tested if they were and also if the outputs from the other runs were true FP32 numbers. You can do this with a “round trip” test, by converting a number to FP16 and back to FP32. If after this process the number is unchanged then it is an FP16 number but if it changes then it was a true FP32. The number changes as FP16 can represent fewer floating point numbers than FP32. It’s a very simple check to carry out:&lt;/p&gt;
    &lt;code&gt;import numpy as np

= np.array([0.6480752, -0.34015813, 1.4329923], dtype=np.float32)
 onnx_cpu = np.array([0.6484375, -0.34033203, 1.4326172], dtype=np.float32)  # We cast the ort MPS numbers up to FP32, if they were FP16 this has no effect
 onnx_coreml 
= onnx_cpu.astype(np.float16).astype(np.float32)
 cpu_roundtrip = onnx_coreml.astype(np.float16).astype(np.float32)
 coreml_roundtrip 
print("ORT CPU values:")
print("  Original:", onnx_cpu)
print("  fp16 roundtrip:", cpu_roundtrip)
print("  Changed?", not np.allclose(onnx_cpu, cpu_roundtrip, atol=0))

print("\nORT CoreML values:")
print("  Original:", onnx_coreml)
print("  fp16 roundtrip:", coreml_roundtrip)
print("  Changed?", not np.allclose(onnx_coreml, coreml_roundtrip, atol=0))&lt;/code&gt;
    &lt;p&gt;The output of this is:&lt;/p&gt;
    &lt;p&gt;The CPU values change and the MPS values don’t! Now it’s getting interesting - perhaps when using the CoreML execution provider the output is FP16? This prompted adding the CoreML direct run in FP16 precision.&lt;/p&gt;
    &lt;p&gt;I tested this theory with an experiment. Originally, when benchmarking it was all about inference speed, now it’s about floating point precision and figuring out where the diffs come from.&lt;/p&gt;
    &lt;p&gt;Running on PyTorch CPU and MPS gives a strong baseline, PyTorch is a very mature ecosystem and I used the results from that as my ground truth. It being so close together is what drove me to understand what caused ORT runs on different hardware to have a difference. Then using CoreML FP32 and FP16 aimed to show if the issue was an ONNX one or a CoreML one.&lt;/p&gt;
    &lt;p&gt;Check Figure 4 for the outputs and Figure 5 for differences in the outputs here:&lt;/p&gt;
    &lt;p&gt;Wow, would you look at that - once again PyTorch + ORT CPU match and so does PyTorch CPU + CoreML FP32. Also note that CoreML FP16 and ORT MPS match! This is a big insight into what is happening and why the metrics output differed before. Along with the round trip experiment this proves our model is being ran in FP16 when using the CoreML execution provider in ORT!&lt;/p&gt;
    &lt;p&gt;Floating points numbers are defined by three values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sign: 1 bit to define if the number is positive or negative&lt;/item&gt;
      &lt;item&gt;Significand: Contains the numbers digits&lt;/item&gt;
      &lt;item&gt;Exponent: This says where the decimal place should be placed relative to the beginning of the significand&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Floating point numbers are often expressed in scientific notation, e.g:&lt;/p&gt;
    &lt;p&gt;FP16 and FP32 specifically, have the following specification:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Total bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Significand bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Exponent bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Smallest number&lt;/cell&gt;
        &lt;cell role="head"&gt;Largest number&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Single precision&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;23 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;\(1.2 * 10^{-38}\)&lt;/cell&gt;
        &lt;cell&gt;\(3.4 * 10^{38}\)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Half precision&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;10 + 1 sign&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;\(5.96 * 10^{-8}\)&lt;/cell&gt;
        &lt;cell&gt;\(6.55 * 10^{4}\)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So as FP16 is half the size it affords a couple benefits, firstly it requires half the memory to store and secondly it can be quicker to do computations with too. However, this comes at a cost of precision, FP16 cannot represent very small numbers and the distances between small numbers as accurately as FP32.&lt;/p&gt;
    &lt;p&gt;An example of FP16 vs FP32 - The Largest Number Below 1&lt;/p&gt;
    &lt;p&gt;As you see FP32 can represent a value much closer to 1.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Link to the ONNX Issue&lt;/head&gt;
    &lt;p&gt;Having said all that, going back to the issue at hand we observe a ~\(1.17*e^{-7}\) error between PyTorch and CoreML FP32 which is typical of FP32. But, then ORT and CoreML when ran on MPS have a difference of ~\(3.7*e{-4}\) which is much more representative of FP16, this is what prompted the round trip experiment.&lt;/p&gt;
    &lt;p&gt;If you need a quick refresher on FP values, please expand the box above. If you already read that or you know enough about FP already let’s look at why some predictions flip.&lt;/p&gt;
    &lt;p&gt;In my model the base threshold for a 0 or 1 class is 0.5. Both FP16 and FP32 can represent 0.5 exactly:&lt;/p&gt;
    &lt;code&gt;= np.array([0.5], dtype=np.float32)
 fp_32_05 = np.array([0.5], dtype=np.float16)
 fp_16_05 
 fp_32_05.item(), fp_16_05.item()
0.5, 0.5) (&lt;/code&gt;
    &lt;p&gt;But we know that FP representations cannot represent every single number, so there will be some values around 0.5 which cannot be represented and hence will get rounded either up or down. Let’s look into that and find the threshold, this will show why some predictions of the EyesOff model were flipped when changing the model to run in FP16. Also, note by flipped we mean they go from a negative (0) prediction to a positive (1) class prediction, the rounding means it’d have to be below 0.5 and then be rounded up to cross the threshold boundary. Any other scenario would keep the label the same, i.e if it’s above 0.5 and gets rounded to 0.5 that’s fine as the predicted class is still the same.&lt;/p&gt;
    &lt;p&gt;The first step is to find the next representable number below 0.5:&lt;/p&gt;
    &lt;code&gt;# Show the representable values just below 0.5
= np.nextafter(np.float32(0.5), np.float32(0.0))
 fp32_below = np.nextafter(np.float16(0.5), np.float16(0.0))
 fp16_below 
= 0.5 - fp32_below
 fp32_gap = 0.5 - fp16_below
 fp16_gap 
print(f"\nClosest value BELOW 0.5:")
print(f"FP32: {fp32_below:.20f}")
print(f"FP16: {fp16_below:.20f}")

print(f"\nGap from threshold (0.5):")
print(f"FP32: {fp32_gap:.2e}")
print(f"FP16: {fp16_gap:.2e}")&lt;/code&gt;
    &lt;code&gt;Closest value BELOW 0.5:
FP32: 0.49999997019767761230
FP16: 0.49975585937500000000

Gap from threshold (0.5):
FP32: 2.98e-08
FP16: 2.44e-04&lt;/code&gt;
    &lt;p&gt;Taking this gap between 0.5 and the next representable number below 0.5 in FP16 we can calculate the threshold for values which will get rounded up to 0.5:&lt;/p&gt;
    &lt;code&gt;# Given the gap is 2.44e-04, we need to divide it by 2 and calculate the midpoint between 0.499755859375 and 0.5. This midpoint determines whether the FP16 value will be rounded down if below it or up it equal to or greater than. 

# Convert to FP32 as the midpoint is not representable in FP16
= np.float32(fp16_below)
 fp_16_below_fp32 
# Calculate the gap and midpoint
= 0.5 - fp16_below
 fp16_gap = fp_16_below_fp32 + (fp16_gap / 2)
 midpoint 
print(f"  Midpoint (rounding boundary): {midpoint:.15f}")&lt;/code&gt;
    &lt;code&gt;Midpoint: 0.499877929687500&lt;/code&gt;
    &lt;p&gt;Finally let’s see some examples of numbers being rounded up to 0.5 if they are above the midpoint between the representable values of FP16:&lt;/p&gt;
    &lt;code&gt;# Firstly, the midpoint itself is rounded up
0.499877929687500).item() -&amp;gt; 0.5
 np.float16(0.4999).item() -&amp;gt; 0.5
 np.float16(
# For completeness here's a number slightly smaller than the midpoint which gets rounded down
0.4998779296874).item() -&amp;gt; 0.499755859375 np.float16(&lt;/code&gt;
    &lt;p&gt;In short, any number between \([0.4998779296875, 0.5)\) will be rounded up to 0.5. This means, the predictions which were flipped were in this range.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Does the Model Switch to FP16?&lt;/head&gt;
    &lt;p&gt;Now that we know what the issue is, it’s time to find out what caused it.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Bug or Intended Behaviour - It Must be a Bug Right…?&lt;/head&gt;
    &lt;p&gt;Originally I thought this behaviour was a bug with the ORT repo. Knowing the cast occured in the phase where ORT takes the ONNX model and converts it to a CoreML model my initial thinking was either ORT casts it to FP16 somewhere or calls CoreML with a hardcode FP16 flag or something similar.&lt;/p&gt;
    &lt;p&gt;Having little background in cpp, Claude came in useful here. I gave it the structure of the repo and it told me where I ought to place breakpoints to debug the ORT package (turns out you can debug a cpp package from python, clone the repo, build from src and then link it to your python code using the PID). However, upon running the code the breakpoints weren’t being hit. I was puzzled for a bit, but then I realised why the code wasn’t being hit. It turns out CoreML has two model formats “NeuralNetwork” and “MLProgram”, I will call them NN and MLP formats respectively. The behaviour of the ORT repo changes depending on which you want, as does the behaviour of CoreML, with the default being the NN format. So, the breakpoints weren’t hit as the code was regarding the MLP format whereas I was not setting this so the code flowed through the default NN code. Knowing this I took a step back and began experimenting with NN vs MLP format.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix - NeuralNetwork vs MLProgram CoreML Format&lt;/head&gt;
    &lt;p&gt;So, CoreML has two model formats, these represent how the model is stored and ran with CoreML. The NeuralNetwork (NN) format is older and the MLProgram (MLP) format is newer. ORT specifies NN format by default, but it does allow you to pass a flag to use MLP format.&lt;/p&gt;
    &lt;p&gt;Testing the MLP format revealed it as the solution! See below in figure 6 the final output, which includes both ORT MLP and NN format ran on the GPU.&lt;/p&gt;
    &lt;p&gt;So ORT on MPS with NN format has the same difference from the PyTorch CPU baseline as CoreML FP16, whereas ORT with MLP format matches - this is exactly what I wanted. Mystery solved! By setting the model format to be the newer MLProgram format no implicit cast to FP16 takes place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why MLProgram Format Worked and Neural Network Didnt?&lt;/head&gt;
    &lt;p&gt;To understand the difference in behaviours of these two models formats we need to take a deep dive on the internals of CoreML, its goals and the two formats themselves. Let’s begin with CoreML.&lt;/p&gt;
    &lt;head rend="h3"&gt;CoreML&lt;/head&gt;
    &lt;p&gt;ORT implements methods to convert the ONNX graph into CoreML model formats. CoreML has two types of model format, this defines how the model is represented in the CoreML framework, how it’s stored and how it’s ran. The older is the NeuralNetwork format and the newer one which solved our issue is the MLProgram format. The reason MLProgram keeps the model at FP32 when running on MPS is due to the differences in model representation in these two formats. Let’s take a look at both of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Neural Network Format&lt;/head&gt;
    &lt;p&gt;As stated, the NN format is the older one, it came out in 2017. It stored models as a Directed Acyclic Graph (DAG). Each layer in the model is a node in the DAG, and they encode information on layer type, list of input names, output names and a collection of parameters specific to the layer type7. We can observe the model which is created by ORT’s InferenceSession call with the following code:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir
= ort.InferenceSession(
 ort =["CoreMLExecutionProvider"]
     onnx_path, providers
 )= ort.run(None, {"input": numpy_input})[0]
 nn_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# NeuralNetwork models are .mlmodel files
= list(temp_dir.glob("*.mlmodel"))
 nn_files 
for model_path in nn_files:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;This outputs the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-40975D85-7412-4309-A6F7-4E51CA3D2FE8-7682-0000BF11C24C3150.model.mlmodel:
 specificationVersion: 4
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
neuralNetwork {
  layers {
    name: "node_linear"
    input: "input"
    output: "output"
    innerProduct {
      inputChannels: 4
      outputChannels: 3
      weights {
        floatValue: 0.0349225402
        floatValue: -0.301196814
        floatValue: 0.159211695
        floatValue: 0.156890273
        floatValue: -0.267238438
        floatValue: -0.0749385953
        floatValue: -0.292913973
        floatValue: 0.129736364
        floatValue: -0.134683847
        floatValue: 0.351268351
        floatValue: 0.354943156
        floatValue: 0.0509352088
      }
    }
  }
  arrayInputShapeMapping: EXACT_ARRAY_MAPPING
}&lt;/code&gt;
    &lt;p&gt;NeuralNetwork format has typed input and output to the model, but the nodes themselves are not typed. This is why the model gets cast to FP16, in the NN format the default behaviour is to run in FP16 on the MPS GPU. This quirk of the NN format is what threw off my results8. The CoreML runtime also specifies which parts of the model operators can run on which hardware9 and each hardware has different abilities in terms of what FP values it can handle with the NN format. Take a look at Figure 7 for Apple’s guide on the hardware and FP types they can handle:&lt;/p&gt;
    &lt;p&gt;When running on CPU the NN format model will run in FP32, as we observed. However, on GPU it is implicitly cast to FP16 even though the input and output are specified to be FP32 as you see in the inspection code above. This is an inherent limitation of the NN format. The DAG structure of the model does not store any information on the types of intermediate layers. You can see this in the inspection output, the part beginning neuralNetwork stored info on the actual layer node, in our case a single linear layer. Observe that there is no information on the FP precision of the node itself, hence CoreML implicitly sets it to FP16.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Does CoreML Implicitly Use FP16&lt;/head&gt;
    &lt;p&gt;From the typed execution docs for coremltools, the goal of CoreML is to run ML models in the most performant way and FP16 happens to be more performant than FP32 (which makes sense as it’s half the precision) on Apple GPUs. Also, they state that most of the time the reduced precision doesn’t matter for inference - this whole blog post shows why this is false and a pitfall of the NN format, the user should choose which precision the model is ran in, it should never be implicit.&lt;/p&gt;
    &lt;head rend="h5"&gt;MatMul Test - Is FP16 Faster on Apple Hardware?&lt;/head&gt;
    &lt;p&gt;To test Apple’s claim that FP16 is more performant on Apple hardware I carried out a large matmul. Taking a 16384x16384 matrix and multiplying it with another 16384x16384 matrix should show us if FP16 is faster. The size is arbitrary I just wanted something large.&lt;/p&gt;
    &lt;p&gt;The matmul was ran 10 times, in both FP32 and FP16 on the MPS hardware, and we take the average:&lt;/p&gt;
    &lt;code&gt;FP32 Average Time: 8.6521 seconds
FP16 Average Time: 6.7691 seconds

Speedup Factor: 1.28x faster&lt;/code&gt;
    &lt;p&gt;So FP16 is quicker, which sheds a bit of light on why the NN format has implicit casting to FP16, on paper if you only care about speed then it’s the better option.&lt;/p&gt;
    &lt;p&gt;Final point on the NeuralNetwork format, it’s surprising as the weights themselves are stored as FP32 values (a roundtrip test verifies this) but it still executes that layer in FP16, once again showing the NN format doesn’t respect the FP precision of the layer but just casts it to FP16.&lt;/p&gt;
    &lt;p&gt;All that is to say this, this was not a bug but rather an explicit design choice, which funnily enough involves implicitly going against what the user wants. The NN format has its downsides, which is why Apple introduced the MLProgram format, let’s look into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MLProgram (MLP) Format&lt;/head&gt;
    &lt;p&gt;The MLP format is the newer and better model format in CoreML, released in 2021, the core thing we care about is that the intermediate tensors are typed, i.e. there is no implicit casting when using the MLP format - the user controls whether the model is ran in FP16 or FP32.&lt;/p&gt;
    &lt;p&gt;MLP format allows for this as it uses a different representation of ML models, instead of a DAG it uses a programmatic representation of the models. By representing the model as code, it allows for greater control over the operations.&lt;/p&gt;
    &lt;p&gt;Let’s see what this looks like in the stored model format and how it differs to the NN format inspection.&lt;/p&gt;
    &lt;p&gt;The code to do so is pretty similar:&lt;/p&gt;
    &lt;code&gt;# First create the InferenceSession and run the model. This ensures the CoreML model files are added to a temp dir. Also
# this time we specify the ModelFormat to be MLProgram 
= ort.InferenceSession(
 ort_mlp =[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]
     onnx_path, providers
 )= ort_mlp.run(None, {"input": numpy_input})[0]
 mlp_output 
import coremltools as ct

def get_coreml_dtype_from_spec(path):
"""Extract model type and dtypes by reading the spec."""
     
     = ct.models.MLModel(str(path))
     model = model.get_spec()
     spec 
print(f"\nModel Spec for {path.name}:\n {spec}\n")
     
# Find created models
= Path(tempfile.gettempdir())
 temp_dir 
# MLProgram models are in onnxruntime-* directories (not .mlmodelc)
= [d for d in temp_dir.glob("onnxruntime-*")
 mlp_dirs if d.is_dir() and not str(d).endswith('.mlmodelc')]
             
for model_path in mlp_dirs:
= get_coreml_dtype_from_spec(model_path)     info &lt;/code&gt;
    &lt;p&gt;The output of this is the following:&lt;/p&gt;
    &lt;code&gt;Model Spec for onnxruntime-752039B9-BA73-47E3-9ED4-AE029184DA69-9443-0000BF278CD8396E:
 specificationVersion: 8
description {
  input {
    name: "input"
    type {
      multiArrayType {
        shape: 1
        shape: 4
        dataType: FLOAT32
      }
    }
  }
  output {
    name: "output"
    type {
      multiArrayType {
        shape: 1
        shape: 3
        dataType: FLOAT32
      }
    }
  }
}
mlProgram {
  version: 1
  functions {
    key: "main"
    value {
      inputs {
        name: "input"
        type {
          tensorType {
            dataType: FLOAT32
            rank: 2
            dimensions {
              constant {
                size: 1
              }
            }
            dimensions {
              constant {
                size: 4
              }
            }
          }
        }
      }
      opset: "CoreML7"
      block_specializations {
        key: "CoreML7"
        value {
          outputs: "output"
          operations {
            type: "const"
            outputs {
              name: "linear_weight"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                  dimensions {
                    constant {
                      size: 4
                    }
                  }
                }
              }
            }
            attributes {
              key: "val"
              value {
                type {
                  tensorType {
                    dataType: FLOAT32
                    rank: 2
                    dimensions {
                      constant {
                        size: 3
                      }
                    }
                    dimensions {
                      constant {
                        size: 4
                      }
                    }
                  }
                }
                blobFileValue {
                  fileName: "@model_path/weights/weight.bin"
                  offset: 64
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "linear_weight"
                    }
                  }
                }
              }
            }
          }
          operations {
            type: "linear"
            inputs {
              key: "x"
              value {
                arguments {
                  name: "input"
                }
              }
            }
            inputs {
              key: "weight"
              value {
                arguments {
                  name: "linear_weight"
                }
              }
            }
            outputs {
              name: "output"
              type {
                tensorType {
                  dataType: FLOAT32
                  rank: 2
                  dimensions {
                    constant {
                      size: 1
                    }
                  }
                  dimensions {
                    constant {
                      size: 3
                    }
                  }
                }
              }
            }
            attributes {
              key: "name"
              value {
                type {
                  tensorType {
                    dataType: STRING
                  }
                }
                immediateValue {
                  tensor {
                    strings {
                      values: "node_linear__0"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you see in the inspection of the MLP format model the linear layer is explicitly typed. To make it a bit easier to see let’s bring back the NeuralNetwork format inspection and compare the linear layer setup in both:&lt;/p&gt;
    &lt;head rend="h4"&gt;Linear Layer in NeuralNetwork &amp;amp; MLProgram Format&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;NeuralNetwork Linear Layer&lt;/cell&gt;
        &lt;cell role="head"&gt;MLProgram Linear Layer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;quote&gt;neuralNetwork { layers { name: "node_linear" input: "input" output: "output" innerProduct { inputChannels: 4 outputChannels: 3 weights { floatValue: 0.0349225402 floatValue: -0.301196814 floatValue: 0.159211695 floatValue: 0.156890273 floatValue: -0.267238438 floatValue: -0.0749385953 floatValue: -0.292913973 floatValue: 0.129736364 floatValue: -0.134683847 floatValue: 0.351268351 floatValue: 0.354943156 floatValue: 0.0509352088 } } }&lt;/quote&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;quote&gt;operations { type: "linear" inputs { key: "x" value { arguments { name: "input" } } } inputs { key: "weight" value { arguments { name: "linear_weight" } } } outputs { name: "output" type { tensorType { dataType: FLOAT32 rank: 2 dimensions { constant { size: 1 } } dimensions { constant { size: 3 } } } } }&lt;/quote&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Observe in the NN format, there is no explicit mention of the input or output type also the model weights are stored with the layer. Now, in the MLProgram layer, the output is explicitly typed as FP32. No more pesky implicit casting to FP16! This is one the big changes in MLProgram vs NN format, secondly notice how the layer weights are not stored along with the spec, they’re stored elsewhere. This aspect also makes the MLP format more efficient as the actual model spec is lighter.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Why Does MLProgram Have Typed Layers?&lt;/head&gt;
    &lt;p&gt;So we’ve come to the end of the journey, we found that NeuralNetwork format lacks types in the intermediate layers of the model and MLProgram doesn’t. So, setting ORT to use MLProgram keeps the model at FP32 and our output predictions remain the same when running in PyTorch and ORT. But why, why does NeuralNetwork not include types? Answering this requires a look into how ML models have been represented in the past and how this has evolved over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Choices, Design Choices - How Goals of ML Optimisation Evolved Over Time&lt;/head&gt;
    &lt;p&gt;When the NeuralNetwork format was released in 2017, it came into a much different environment than the one MLProgram was born into in 2021. The goal of NeuralNetwork was to act as a configuration file to be ran by hardware, as we saw above it defines the layers and the weights without much other info and lets the hardware figure out the rest. This is indicative of the trends in ML at the time, models were still being optimised so the added complexity wasn’t yet needed, the DAG representation worked well.&lt;/p&gt;
    &lt;p&gt;In essence, the NN format assumes that if the weights are stored in FP32, the input is FP32 and the output is too then the intermediate layers will also be FP32 - but as it doesn’t explicitly type these intermediate layers the hardware is free to choose and the Apple GPU chooses FP16 by default!&lt;/p&gt;
    &lt;p&gt;As time went on the demands in the ML world changed, these hardware based quirks became known, optimisations advanced and overall the industry moved away from the splintered (splintered in the sense that many frameworks implemented their own) config style DAGs and began to utilise learnings from the world of compilers&lt;/p&gt;
    &lt;head rend="h3"&gt;Changes From 2017 to 2021 Which Lead to Greater Adoption of Intermediate Representations&lt;/head&gt;
    &lt;p&gt;Firstly, for Apple specifically the hardware available expanded, now you have the CPU, GPU and ANE chips - making it very difficult to assume any given piece of hardware will run a specific FP type. Also, the lack of typing leads to other issues namely the compiler cannot make some optimisations, as they depend on knowing the types before runtime. Furthermore, things like mixed FP training and quantization became a thing, once again highlighting the need for explicit typing.&lt;/p&gt;
    &lt;p&gt;Lastly, in 2017 DAGs and other forms of model compilers were very fragmented and modern times have seen a push towards standardisation10, as the compiler community consolidated on tools like LLVM the ML community has too. Intermediate Representations(IR) began to be used in ML, an IR is a hardware agnostic specification of a program which a compiler can optimise. CoreML introduced their own IR, called MIL (Model Intermediate Language) and it implements the output we see in the stored MLProgram output.&lt;/p&gt;
    &lt;head rend="h3"&gt;The MIL Approach&lt;/head&gt;
    &lt;p&gt;MIL and IRs in general afford a lot of benefits. They are inherently designed for optimisation and by providing a general framework you can extract maximal value as all optimisation engineers can work on a common goal. In MIL specifically, some of the changes we’ve discussed between NN and MLProgram format, are implemented by it. Namely, each variable within the model has an explicit dtype.&lt;/p&gt;
    &lt;p&gt;Note, the MLProgram serialises and stores the output of the MIL phase, we’ve already observed how it differs to the the NeuralNetwork model, with the biggest difference being in the explicit types.&lt;/p&gt;
    &lt;head rend="h4"&gt;Further Reading on ML Compilers&lt;/head&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;head rend="h3"&gt;The Fix&lt;/head&gt;
    &lt;p&gt;The solution to all the issues we discussed today is, if you are using the CoreMLExecutionProvider in ORT then be sure to specify ModelFormat is MLProgram, this will ensure that whatever precision your model was trained it will be ran with that - which in my case was FP32 (whereas the default ModelFormat NeuralNetwork casts the model to FP16).&lt;/p&gt;
    &lt;p&gt;You can implement this as such:&lt;/p&gt;
    &lt;code&gt;= ort.InferenceSession(onnx_model_path, providers=[("CoreMLExecutionProvider", {"ModelFormat": "MLProgram"})]) ort_session &lt;/code&gt;
    &lt;head rend="h3"&gt;The Cause&lt;/head&gt;
    &lt;p&gt;The issue was the differing model formats employed by CoreML to represent ML models. The NeuralNetwork format utilised a more historic DAG based approach which was developed during a time in which types and precision wasn’t a huge concern in the ML community and hardware decisions were left to the hardware. Whereas the MLProgram format used a programmatic approach, in which types are explicit letting the software influence how the model is run on the hardware.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lessons?&lt;/head&gt;
    &lt;p&gt;This whole thing taught me the importance of being thorough, it’s not acceptable to test your model in one setup and deploy it in another. We really need to test our model runs across all the platforms we intend to deploy to. Secondly, implicit defaults can be particularly damaging, in my case it wasn’t a huge issue but it easily could have been. Implicit defaults in this case also killed reproducibility, which can be problematic.&lt;/p&gt;
    &lt;p&gt;Lastly, I leave you with this:&lt;/p&gt;
    &lt;p&gt;1https://onnx.ai/onnx/intro/concepts.html&lt;/p&gt;
    &lt;p&gt;3https://developer.apple.com/documentation/coreml&lt;/p&gt;
    &lt;p&gt;4https://en.wikipedia.org/wiki/Single-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;5https://en.wikipedia.org/wiki/Half-precision_floating-point_format&lt;/p&gt;
    &lt;p&gt;6https://floating-point-gui.de/formats/fp/&lt;/p&gt;
    &lt;p&gt;7https://apple.github.io/coremltools/mlmodel/Format/NeuralNetwork.html&lt;/p&gt;
    &lt;p&gt;8https://apple.github.io/coremltools/docs-guides/source/typed-execution.html&lt;/p&gt;
    &lt;p&gt;9https://github.com/microsoft/onnxruntime/issues/21271#issuecomment-3637845056&lt;/p&gt;
    &lt;p&gt;10https://www.modular.com/blog/democratizing-ai-compute-part-8-what-about-the-mlir-compiler-infrastructure&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350075</guid><pubDate>Mon, 22 Dec 2025 00:27:04 +0000</pubDate></item><item><title>Build Android apps using Rust and Iced</title><link>https://github.com/ibaryshnikov/android-iced-example</link><description>&lt;doc fingerprint="3d82cc104505dfd0"&gt;
  &lt;main&gt;
    &lt;p&gt;There are NativeActivity and GameActivity examples here.&lt;/p&gt;
    &lt;p&gt;Based on several other examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;na-mainloop&lt;/code&gt;and&lt;code&gt;agdk-mainloop&lt;/code&gt;from android-activity&lt;/item&gt;
      &lt;item&gt;na-winit-wgpu from &lt;code&gt;rust-android-examples&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;integration from &lt;code&gt;iced&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;iced integration example&lt;/p&gt;
    &lt;p&gt;You can also run most of the examples from iced. For this omit the scene rendering part and set the background of the root container.&lt;/p&gt;
    &lt;p&gt;Text input partially works, unresolved issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;window doesn't resize on show/hide soft keyboard&lt;/item&gt;
      &lt;item&gt;how to change input language of soft keyboard&lt;/item&gt;
      &lt;item&gt;ime is not supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Copy/paste and show/hide soft keyboard is implemented by calling Java&lt;/p&gt;
    &lt;p&gt;Check &lt;code&gt;android-activity&lt;/code&gt; crate for detailed instructions.
During my tests I was running the following command and using android studio afterwards:&lt;/p&gt;
    &lt;code&gt;export ANDROID_NDK_HOME="path/to/ndk"
export ANDROID_HOME="path/to/sdk"

rustup target add x86_64-linux-android
cargo install cargo-ndk

cargo ndk -t x86_64 -o app/src/main/jniLibs/  build&lt;/code&gt;
    &lt;p&gt;My setup is the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;archlinux 6.9.6&lt;/item&gt;
      &lt;item&gt;jdk-openjdk 22&lt;/item&gt;
      &lt;item&gt;target api 35&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to &lt;code&gt;android-activity&lt;/code&gt; we can already build android apps in Rust, and
key crates such as &lt;code&gt;winit&lt;/code&gt; and &lt;code&gt;wgpu&lt;/code&gt; also support building for android.
&lt;code&gt;iced&lt;/code&gt; doesn't support android out of the box, but it can be integrated with
existing graphics pipelines, as shown in
integration example.
As a result, it was possible to convert existing example running &lt;code&gt;winit&lt;/code&gt; + &lt;code&gt;wgpu&lt;/code&gt; to
use &lt;code&gt;iced&lt;/code&gt; on top.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46350641</guid><pubDate>Mon, 22 Dec 2025 02:14:32 +0000</pubDate></item><item><title>I announced my divorce on Instagram and then AI impersonated me</title><link>https://eiratansey.com/2025/12/20/i-announced-my-divorce-on-instagram-and-then-ai-impersonated-me/</link><description>&lt;doc fingerprint="767cce1f3dd3c327"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Captcha Check&lt;/head&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352004</guid><pubDate>Mon, 22 Dec 2025 07:13:33 +0000</pubDate></item><item><title>Debian's Git Transition</title><link>https://diziet.dreamwidth.org/20436.html</link><description>&lt;doc fingerprint="767cce1f3dd3c327"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Captcha Check&lt;/head&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
    &lt;p&gt;Hello, you've been (semi-randomly) selected to take a CAPTCHA to validate your requests. Please complete it below and hit the button!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352231</guid><pubDate>Mon, 22 Dec 2025 08:24:32 +0000</pubDate></item><item><title>Inverse Parentheses</title><link>https://kellett.im/a/inverse-parentheses</link><description>&lt;doc fingerprint="1f1817d1eda55258"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you ever noticed that lots of programming languages let you use parentheses to group operands, but none use them to ungroup them? No? Well letâs pretend this is a normal thing to be thinking about, and see what we can do about it.&lt;/p&gt;
    &lt;p&gt;Grouping with parentheses is relatively easy to add to a language grammar. The rule that accepts atomic things like &lt;code&gt;37&lt;/code&gt; simply needs to also accept an opening paren,1 at which point it will recursively parse an entire expression, and then eat a closing paren.&lt;/p&gt;
    &lt;code&gt;def parse_atom(lex):
    r = next(lex)
    if r[0] == 'integer':
        return int(r[1])
    elif r[0] == '(':
        expr = parse_expr(lex)
        s = next(lex)
        if s[0] != ')':
            raise ParseError("missing close paren")
        return expr
    else:
        raise ParseError(f"unexpected {r[0]}")
&lt;/code&gt;
    &lt;p&gt;Anti-grouping isnât quite as straightforward. Our parser canât follow the structure of the parentheses, because then it wouldnât be following the structure of the expressionâthe whole point is that these are dissimilar.&lt;/p&gt;
    &lt;p&gt;I donât know if itâs possible to write a pure parser that does this. But purity is overrated anyway. I decided to take inspiration from another language with a weird grouping system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Python&lt;/head&gt;
    &lt;p&gt;Did you know that Pythonâs grammar has braces? You just donât type them. The tokeniser3 keeps track of the indentation level and inserts special tokens when it changes. The parser itself doesnât need to worry about counting whitespace; it just sees blocks of statements bracketed by &lt;code&gt;INDENT&lt;/code&gt; and &lt;code&gt;DEDENT&lt;/code&gt;,4 which are easy to parse.&lt;/p&gt;
    &lt;p&gt;As it happens, Pythonâs tokeniser also knows when itâs inside a parenthesised expression. Indentation inside parens is not significant, and this is implemented by tracking the paren nesting depth and suppressing &lt;code&gt;INDENT&lt;/code&gt; and &lt;code&gt;DEDENT&lt;/code&gt; while itâs non-zero.&lt;/p&gt;
    &lt;p&gt;What if we used the same trick? Instead of trying to do all this in the parser somehow, the tokeniser could track its nesting depth, and emit a âfriendlinessâ score for each token. Then we can simply parse operators in ascending order of friendliness.&lt;/p&gt;
    &lt;p&gt;In this model &lt;code&gt;1 + (2 * 3)&lt;/code&gt; will yield the following token stream:&lt;/p&gt;
    &lt;code&gt;1
+ (0)
(
2
* (-1)
3
)
&lt;/code&gt;
    &lt;p&gt;Weâll leave the parentheses in the token stream, but all the parser needs to do with them is generate a syntax error if it finds one in the wrong place. Grouping will be handled entirely by the precedence levels embedded in the token stream.5&lt;/p&gt;
    &lt;head rend="h2"&gt;A not-so-infinite climb&lt;/head&gt;
    &lt;p&gt;The tokeniser hack solves our parsing problem, but it creates another one: our language now has infinitely many precedence levels. I donât feel like trying to do that with handrolled recursive descent, but a rummage through school textbooks suggests a precedence climbing parser is what we need. It deals with operators in the order it meets them, so having infinitely many possible precedences wonât bother it.&lt;/p&gt;
    &lt;p&gt;I hacked this together and itâs appropriately silly:&lt;/p&gt;
    &lt;code&gt;&amp;gt; (1 + 2) * 3
1 + 2 * 3
&amp;gt; 1 + (2 * 3)
(1 + 2) * 3
&lt;/code&gt;
    &lt;p&gt;Something I particularly enjoy about the implementation I landed on is that if you increase friendliness instead of decreasing it, you end up with an ordinary6 parser. Itâs also a good platform for other questionable syntactic innovations, like a language with no parentheses at all, using whitespace to weaken binding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;While weâve achieved a lot here today,7 weâve also raised some important new questions. For instance, is it always necessary to double-parenthesise expressions in more complex cases?&lt;/p&gt;
    &lt;code&gt;&amp;gt; ((1 * 2)) + (3 * 4)
1 * ((2 + 3) * 4)
&lt;/code&gt;
    &lt;p&gt;And is it possible to have an anti-grouping parser that gives an involution when hooked up to an ordinary printer?&lt;/p&gt;
    &lt;p&gt;These are promising avenues for deeper study, and Iâd love to hear from anyone who chooses to take them on.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Does it bother anyone else that âparenthesesâ (meaning round brackets) doesnât seem to have a satisfactory2 singular form? ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To me, âparenthesisâ sounds like a pair of parentheses, possibly together with the text they bracket, rather than a single paren. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Also called a lexer or occasionally a scanner. Iâm going to stick with âtokeniserâ because it feels the most like a real word. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sadly these are just internal names and not spellings, so you canât use them to write exciting one-liners. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I have ignored the order of operations in this post to keep things simple, but my implementation does handle it. Operators are looked up in a table and their ânaturalâ precedence is paired with their friendliness, so in fact the precedences of&lt;/p&gt;&lt;code&gt;+&lt;/code&gt;and&lt;code&gt;*&lt;/code&gt;from the example would be (0,0) and (-1,1) respectively. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Outwardly. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually early 2024, but it took me until today to get around to writing it up. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352248</guid><pubDate>Mon, 22 Dec 2025 08:29:03 +0000</pubDate></item><item><title>Show HN: Backlog – a public repository of real work problems</title><link>https://www.worldsbacklog.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352310</guid><pubDate>Mon, 22 Dec 2025 08:42:31 +0000</pubDate></item><item><title>Cartoon Network channel errors (1995 – 2025)</title><link>https://cnas.fandom.com/wiki/Channel_Errors</link><description>&lt;doc fingerprint="c02dcae3af8611af"&gt;
  &lt;main&gt;
    &lt;p&gt;The following is a work in progress list of known times that Cartoon Network and Adult Swim, alongside Cartoonito, had errors with screenbugs, their split screen credits, any identifications, and even during the shows themselves.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1995&lt;/cell&gt;
        &lt;cell&gt;The Checkerboard Technical Difficulties slide appeared after a promo for The Pirates of Dark Water, staying onscreen for approximately 30 seconds before going to commercials.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 26, 1997&lt;/cell&gt;
        &lt;cell&gt;18 seconds into the 4:00pm showing of the Taz-Mania episode Pup Goes the Wendal / I'm Okay, You're Taz, the show's audio glitched out for 4 seconds. The error was then fixed, only for it to happen again 9 seconds later. The error then happened on and off until 1:35 into the show, after which it faded into the Checkerboard Technical Difficulties slide. After 18 seconds, the slide disappeared and the show returned.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 16, 1997&lt;/cell&gt;
        &lt;cell&gt;The ThunderCats episode Fond Memories encountered substantial technical difficulties. Following the show's custom Toonami intro was just under a minute of blank screen followed by the QTTV Technical Difficulties slide. The slide abruptly cuts to the first act already in process as Mumm-Ra is explaining his plan to trap Lion-O, accompanied by the TV-Y7-FV ratings bug fading in and out. The following establishing shot of Mumm-Ra walking through a cave was then interrupted by a Toonami ThunderCats bumper, followed by the commercial break playing ahead of schedule. After the commercial break ends and a few seconds of blank screen followed by a brief gray screen, the episode starts again, halfway through its title card / credits. The episode then aired properly, albeit with about five seconds of black and gray screens preceding each act of the episode (including the epilogue), and the commercial break being replaced with a Toonami bumper. The screenbug was completely absent for the entirety of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 4, 1998&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the Robotech episode False Start, the screenbug disappeared in the first and last few minutes of the second act.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 14, 1998&lt;/cell&gt;
        &lt;cell&gt;The 11:00pm airing of the Cow and Chicken episode The Karate Chick / Yard Sale was faced with a significant amount of technical difficulties. Prior to the start of the show, the Starburst Cartoon Cartoons intro is used despite its corresponding era having recently been retired at that point. Right after it ends, a Powerhouse bumper introducing the show plays, only for it to abruptly fade out midway through. Lastly, the screenbug is not present throughout the entire duration of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 18, 1998&lt;/cell&gt;
        &lt;cell&gt;The TV-G rating was accidentally kept on screen during the first commercial break for the Cartoon Theatre premiere of An American Tail (1986).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 1, 1999, August 29, 1999&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug is off-center during the premiere of the ReBoot episode Infected. The same issue happened again during a Toonami Midnight Run airing of the Robotech episode Blitzkrieg.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 15, 1999&lt;/cell&gt;
        &lt;cell&gt;Following the first act of the Dragon Ball Z episode Elite Fighters of the Universe... the Ginyu Force, over a minute of black screen interspersed with the Technical Difficulties slide was seen instead of going straight to commercial. This was followed by the All Cartoons All the Time CN promo already in progress. After the promo ended, a split second of the Cartoon Library CN promo could be seen, followed by the proper Toonami Dragon Ball Z bumper and commercial break.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 27, 1999&lt;/cell&gt;
        &lt;cell&gt;The premiere of the ToonHeads episode Toro! Toro! had some significant technical difficulties. Instead of airing Bully for Bugs and Senor Droopy, the cartoons The Mice will Play and Sheep Wrecked aired instead. Intermission segments from the previous week's episode, The Nice Mice of Warner Bros. also played. The episode would premiere properly on October 1, 1999.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 31, 1999&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug did not return for the majority of the second act during the 3:30pm airing of A Pup Named Scooby Doo. The screenbug eventually did return, but because it happened at the end of the episode, it is only on-screen for less than five seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 14, 1999&lt;/cell&gt;
        &lt;cell&gt;The 4:00pm airing of the Tom &amp;amp; Jerry Kids episode The Little Thinker / Rap Rat is Where It's At / My Pet was mistakenly rated TV-PG instead of the show's usual TV-G.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 4, 1999&lt;/cell&gt;
        &lt;cell&gt;Preceding the Robotech episode Sweet Sixteen is a promo for ReBoot claiming that the show will be airing next. During the episode itself, the ratings bug fades in closer to the left side of the screen than normal, and the Cartoon Network screenbug is off-center.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 11, 1999&lt;/cell&gt;
        &lt;cell&gt;At exactly 3:25am, during the break between today's episodes of The Yogi Bear Show and The Huckleberry Hound Show, during the Shortie City-E-Scape, the Cartoon Network screenbug appeared on-screen for several seconds then disappeared for the rest of the Shortie.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2000&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Cartoon Fridays screenbug was mistakenly used during Space Ghost Coast to Coast.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 28, 2000&lt;/cell&gt;
        &lt;cell&gt;The poll segment for tonight's episode of JBVO labeled that week's poll question as a "Brak or..." question instead of the proper "Should" question (said poll question was about if Shaggy should put on some weight). This was an oversight on the production end of the show, as episodes would be ready for airing a few days before the episode's airdate and thus this error was included on the master tape.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 27, 2001&lt;/cell&gt;
        &lt;cell&gt;During the 8:30pm airing of The Chuck Jones Show, the Robert McKimson short Gone Batty accidentally aired.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 17, 2001&lt;/cell&gt;
        &lt;cell&gt;During the 8:30pm airing of The Chuck Jones Show, the Robert McKimson short Bedevilled Rabbit accidentally aired.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 2, 2001&lt;/cell&gt;
        &lt;cell&gt;During the 1:00am airing of Late Night Black &amp;amp; White, the 2 Stupid Dogs episode Hobo Hounds aired. While the episode was intended to mimic a black and white cartoon, it's unknown if it's airing on the block was intentional or an error. The episode would air during Late Night Black &amp;amp; White again on January 27, 2002.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 19, 2001&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 16, 2001&lt;/cell&gt;
        &lt;cell&gt;The version of the Looney Tunes short Tom Turk and Daffy that aired today had no opening credits, nor it wasn't the usual dubbed version they air. In addition, the music video styled banner that was used during airings of The Bugs and Daffy Show in the mid 90s could be seen near the end of the short.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 31, 2001&lt;/cell&gt;
        &lt;cell&gt;A duplicate screenbug appeared, disappeared and reappeared throughout tonight's airing of the special Night of the Living Doo.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 1, 2001&lt;/cell&gt;
        &lt;cell&gt;The TV-G rating bug stayed for the majority of Dragon Ball Z: Dead Zone, and the screenbug was absent for the entirety of the movie.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 17, 2001&lt;/cell&gt;
        &lt;cell&gt;An encore edition of Toonami Midnight Run: Special Edition was initially rated TV-G, before switching to a TV-Y7-FV rating a few seconds later. The screenbug also does not appear during the broadcast.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 30, 2001&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug was used during the first part of the 1:00am airing of Acme Hour.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 28, 2001&lt;/cell&gt;
        &lt;cell&gt;During the 3:00am airing of The Bugs &amp;amp; Daffy Show, the short Kit for Cat had its beginning played in Portuguese. After switching back to English, the screenbug disappeared for the rest of the short.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 11, 2002&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug was used during the 1:00am airing of The Ruff and Reddy Show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 1, 2002&lt;/cell&gt;
        &lt;cell&gt;At the beginning of an ad break during the 10:00am airing of Cats Don't Dance, a minute of the Mission Hill episode "Kevin's Problem (or Porno for Pyro)" is broadcast before abruptly fading to a Cartoon Theatre bumper. This error was most likely due to a time mismatch, as the Mission Hill episode would properly air at 11:00pm that night.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 15, 2003&lt;/cell&gt;
        &lt;cell&gt;The first half of the Rurouni Kenshin episode First Train Ride: Danger on a Runaway Locomotive! aired instead of the first half of the premiere of the Hamtaro episode Boss, the Cool Ham of the Sea!. Despite this, the opening theme to the latter was still shown before the former started.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 12, 2003, June 23, 2003, August 31, 2003&lt;/cell&gt;
        &lt;cell&gt;Typically, during Season 2 episodes of Hamtaro, Cartoon Network would replace the second theme song ("H-A-M-T-A-R-O") with the first ("It's Hamtaro Time"). However, during the premieres of The Little Bandits! and Clubhouse Intruders, both openings are used. This also occurs in the airing of Happy Birthday Hamtaro Special!, likely because the master of the special episode began with a title card before cutting to the second theme song, so Cartoon Network simply began with the first instead of replacing the theme song.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 23, 2003&lt;/cell&gt;
        &lt;cell&gt;About one minute into the 6:30pm airing of the Rurouni Kenshin episode Sanosuke's Betrayal, the screen went completely black. A minute later, the Powerhouse Technical Difficulties slide was shown, but it is unknown how long it lasted.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 25, 2003&lt;/cell&gt;
        &lt;cell&gt;All cartoons shown during the 7:00pm airing of The Chuck Jones Show were Yosemite Sam cartoons directed by Friz Freleng.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 12, 2003&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug did not appear during the first half of today's broadcast of X-Men: Evolution. The bug would return during the second half.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 5, 2003&lt;/cell&gt;
        &lt;cell&gt;During the final act of the 8:00pm Fridays presentation of A Flintstones Christmas Carol, Portuguese audio was erroneously broadcast in the right audio channel, with the original audio broadcasting in the left channel. This was fixed a few minutes afterwards.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 19, 2004 - October 16, 2004&lt;/cell&gt;
        &lt;cell&gt;Before .hack//Legend of the Twilight Bracelet at 1:00 AM, a Now/Then bumper implying .hack//SIGN was coming up next aired. It's likely due to Cartoon Network being contractually obligated to continue airing .hack//SIGN, but Bandai wanting them to swap it out for Legend ahead of time.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 10, 2004&lt;/cell&gt;
        &lt;cell&gt;During the second half of the premiere of the Mobile Suit Gundam SEED episode Stars Falling in Space, the Checkerboard screenbug was used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 2004&lt;/cell&gt;
        &lt;cell&gt;During an airing of Ed, Edd n Eddy, a scene repeated multiple times before going to the Technical Difficulties slide. After the slide disappeared, the commercial break began as if nothing happened.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 6, 2004&lt;/cell&gt;
        &lt;cell&gt;The 3:00pm airing of The Grim Adventures of Billy &amp;amp; Mandy suddenly went to a Technical Difficulties slide 10 minutes into the show, with the Cartoon Network screenbug remaining intact. 10 minutes later, the show briefly returned for 50 seconds, only to cut back to the aforementioned slide. Despite the screenbug also lingering into that, it disappeared over the slide before cutting to the show's credits, and then going to the next episode of the show later than the normal time, which was 3:30pm.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 16, 2005&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Dragon Ball GT episode Giru's Checkered Past was supposed to air at 10:30pm; however a repeat of the special recap episode A Grand Problem aired instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 30, 2005&lt;/cell&gt;
        &lt;cell&gt;An audio promo advertising .hack//Legend of the Twilight Bracelet cuts off (in which the final line for the announcer was ".hack is next. This is Cartoon Network.") during the credits of the Mobile Suit Gundam SEED episode Its Name: Gundam, before the station identification plays.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 17, 2005&lt;/cell&gt;
        &lt;cell&gt;During the 11:30pm airing of the Ed, Edd n Eddy episode An Ed in the Bush, the Cartoon Network Summer screenbug disappears 6 minutes into the episode. A minute later, a larger version of the screenbug is used for the remainder for the segment. Its sister episode, See No Ed, would air with the correct screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 29, 2005&lt;/cell&gt;
        &lt;cell&gt;An audio promo advertising more The Grim Adventures of Billy &amp;amp; Mandy mistakenly played during the last 20 seconds of the premiere of the episode The Firebird Sweet (obscuring the episode's audio), when The Life &amp;amp; Times of Juniper Lee was actually the next show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 22, 2005&lt;/cell&gt;
        &lt;cell&gt;During the 11:00am broadcast of Scooby-Doo and the Monster of Mexico (2003), the menu bumpers incorrectly label the direct-to-video film as a New Scooby Movie. This is done despite the fact that the menu bumper shown during the prior program (Tickle U) had the correct movie labeled as being next.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 17, 2005&lt;/cell&gt;
        &lt;cell&gt;The screenbug was absent during the second segment of today's airing of the ¡Mucha Lucha! episode The Incredible Penny Plutonium until about five minutes into the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 31, 2006&lt;/cell&gt;
        &lt;cell&gt;The intro to G.I. Joe: A Real American Hero replaces Evil Con Carne's intro during the 10:40pm airing of The Cartoon Cartoon Show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 3, 2006&lt;/cell&gt;
        &lt;cell&gt;The screenbug was absent during the entire first act of the series premiere of Pokémon Chronicles.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 15, 2006&lt;/cell&gt;
        &lt;cell&gt;Between 2:00pm and 2:30pm, audio promos to promote My Gym Partner's a Monkey and Tom and Jerry: Shiver Me Whiskers are repeatedly played every bumper and airings of The Grim Adventures of Billy &amp;amp; Mandy. Also, the 2:30pm airing of Billy &amp;amp; Mandy had a small glitch for the intro after a partial audio promo.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 22, 2006&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the Squirrel Boy episode(s) Harried Treasure / Rod Squad, the screenbug was absent.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 11, 2006 - May 26, 2007&lt;/cell&gt;
        &lt;cell&gt;During the series premiere of .hack//Roots, a banner would pop up claiming that My Gym Partner's a Monkey was airing instead. This was never fixed for the rest of the series' run.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 24, 2007&lt;/cell&gt;
        &lt;cell&gt;The end of the premiere of the Naruto episode The Third Hokage, Forever... was cut off due to technical difficulties. Toward the end of the episode, during the scene with the Third Hokage's funeral, Kakashi's "I did, I've been here since d-" (dawn) was interrupted by the screen turning black for a few seconds (with the screenbug still on screen) before cutting to the Technical Difficulties fence sign (with no screenbug). The scene was repeated (the rest of this had no screenbug) before cutting off in the middle of Kakashi's sentence at the same point, then the black screen and fence were shown again, then it cut to the credits four minutes early, causing the One Piece episode Trading Faces to start four minutes early. The entire episode would later premiere on April 7, 2007.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 19, 2007&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Naruto OVA Mission: Protect the Waterfall Village! had a TV-Y7-FV rating, despite the promo and encore airings giving it a TV-PG rating. This is likely due to Cartoon Network forgetting to update the ratings prompt, as it aired in the timeslot normally held by TV-Y7-FV shows.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 21, 2007&lt;/cell&gt;
        &lt;cell&gt;During the first act of Pokémon: Lucario and the Mystery of Mew, a banner erroneously stated that Scooby Doo! and the Monster of Mexico was still airing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 17, 2007&lt;/cell&gt;
        &lt;cell&gt;The 7:00pm episode of Naruto had significant technical issues after the first break. Midway through the episode, the episode had a tape error, eventually cutting to the Cartoon Network fence slide. The intro restarted after a minute before cutting again to the fence before cutting back into the episode in progress. Approximately five minutes of episode time was cut due to the difficulties. The screenbug also disappears during the first act of the following episode, which also ended ten minutes early. No commercials aired during for the following hour except for those locally inserted by the provider, instead cutting to the fence, For the remainder of the night, the marathon aired twenty minutes ahead of schedule, allowing the network to squeeze in another episode, which aired commercial free.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 18, 2007&lt;/cell&gt;
        &lt;cell&gt;The 4:00pm airing of Naruto started midway through the episode A Feeling of Yearning, a Flower Full of Hope, only for the tape to rewind to the beginning a minute later, cutting to the fence card and skipping the intro entirely. Like last night, commercials were skipped in favor of the technical difficulties card for this episode only and the screenbug disappeared for 10 minutes after the error. &lt;p&gt;The first commercial break of Naruto's 10:30pm airing is entirely skipped, possibly to save time.&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 24, 2007&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug was accidentally shown during the first three minutes of the 10:30pm airing of Codename: Kids Next Door before switching to the regular screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 25, 2007&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug was accidentally shown during the last few minutes during the premiere of the One Piece episode Wreckers Reef.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 28, 2007&lt;/cell&gt;
        &lt;cell&gt;During the premiere of Code Lyoko episode Lost at Sea, the Fried Dynamite promotional banner is passing through the Chill Out, Scooby-Doo! promotional banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 23, 2008&lt;/cell&gt;
        &lt;cell&gt;The screenbug was absent for much of the 11:00am airing of Transformers: Animated.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 10, 2008&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of My Gym Partner's a Monkey erroneously used the regular screenbug instead of the "New Episode" screenbug. This was likely because it was a last minute change, as Camp Lazlo was originally going to air in that slot.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 15, 2008&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Chop Socky Chooks had the CN Thursday Nights "Up Next" banner using the wrong font.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 18, 2008&lt;/cell&gt;
        &lt;cell&gt;There was no "New Episode" banner during the 10:30pm premiere episode of Naruto.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 27, 2008&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the My Gym Partner's a Monkey episode A Thanksgiving Carol, the "NEW EPISODE" indicator was not used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 5, 2008&lt;/cell&gt;
        &lt;cell&gt;A banner advertising a new episode of 6teen at 4:00pm was shown during ¡Mucha Lucha!, despite the premiere being replaced at the last-minute.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 11, 2008&lt;/cell&gt;
        &lt;cell&gt;The 8:45pm airing of The Marvelous Misadventures of Flapjack episode Lead 'Em and Weep had a "NEW EPISODE" indicator, despite the fact that it premiered a week ago.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 18, 2008&lt;/cell&gt;
        &lt;cell&gt;The 6:30am airing of Bakugan Battle Brawlers aired without a screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 10, 2009&lt;/cell&gt;
        &lt;cell&gt;The Pokémon: Diamond and Pearl: Battle Dimension episode Up Close and Personal was presented without a screenbug. However, said screenbug was shown for a split second during the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 11, 2009&lt;/cell&gt;
        &lt;cell&gt;During the 12:00pm airing of Tom and Jerry, a copyright notice for Pokémon: Diamond and Pearl: Battle Dimension meant for the split-screen credits mistakenly popped up for a few seconds. The screenbug also disappeared for a second before returning immediately after the error was fixed.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 29, 2009&lt;/cell&gt;
        &lt;cell&gt;Various technical issues happened during the premiere of the Yu-Gi-Oh! 5D's episode The Profiler. After returning from a commercial break, the TV-Y7-FV rating bug appeared on a black screen for several seconds. A minute later, the program was heavily pixelated, with the Cartoon Network bug intact. The episode was abruptly interrupted by promos, causing the ending of the episode to be cut off. Due to the error, Ed, Edd n Eddy began airing four minutes ahead of schedule.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 3, 2009&lt;/cell&gt;
        &lt;cell&gt;The "New Episode" bug did not appear on today's new episode of Yu-Gi-Oh! 5D's.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 18, 2009&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 19, 2009&lt;/cell&gt;
        &lt;cell&gt;The 9:00pm airing of Bugs Bunny's Howl-O-Ween Special was marked as a premiere, even though it first aired on the network 12 years earlier.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 8, 2009&lt;/cell&gt;
        &lt;cell&gt;During the "Big Picture Show Weekend" marathon, the episodes of Ed, Edd n Eddy scheduled for 6:30pm, Pick an Ed and Cool Hand Ed aired without the marathon screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 15, 2009&lt;/cell&gt;
        &lt;cell&gt;During the Looney Tunes marathon, the 6:00-7:00pm block aired without the marathon screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 5, 2010&lt;/cell&gt;
        &lt;cell&gt;A broken countdown for Adventure Time appeared during the premiere of Trouble in Lumpy Space.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 24, 2010&lt;/cell&gt;
        &lt;cell&gt;The "NEW EPISODE" banner did not appear during that night's new Adventure Time.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 31, 2010&lt;/cell&gt;
        &lt;cell&gt;The Flapjack episode Off with His Hat aired with a screenbug indicating that it was a new episode, even though it had debuted nearly a year ago.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 14, 2010&lt;/cell&gt;
        &lt;cell&gt;The Adventure Time episode Business Time aired with a screenbug indicating that it was a new episode, even though it had aired multiple times in the past.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 3, 2010&lt;/cell&gt;
        &lt;cell&gt;The 6:30pm airing of The Garfield Show episode Curse of the Were-Dog / Meet the Parents accidentally played its credits sequence full-screen, followed by the standard split-screen credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 7, 2010&lt;/cell&gt;
        &lt;cell&gt;All episodes in the Chowder Grows Up Pop-Up Marathon had PAL speedup. This was mainly due to the network using the Australian feed's pop-up masters, which were designed for the PAL format.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 31, 2010&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the The Marvelous Misadventures of Flapjack episode(s) Catch Me if You Candy and Fish Out of Water, the "NEW EPISODE" banner was not used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 12, 2011&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Pokémon: Black &amp;amp; White episode In the Shadow of Zekrom! used a up next bumper for a new episode of Pokémon: Diamond and Pearl: Sinnoh League Victors despite the new episode being of another episode of Pokémon: Black &amp;amp; White.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 2, 2011&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the Sym-Bionic Titan episode Disenfranchised at 7:00pm did not use the NEW EPISODE banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 16, 2011&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Pokémon did not use the "NEW EPISODE" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 16, 2011&lt;/cell&gt;
        &lt;cell&gt;Tonight's rerun of Adventure Time used the "NEW EPISODE" banner, despite the fact that the episode aired, Storytelling, had already premiered.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 15, 2011&lt;/cell&gt;
        &lt;cell&gt;Tonight's new Problem Solverz featured an error where the generic screenbug appeared for eight seconds, then disappeared for three minutes and thirty-four seconds. Eventually, the proper "NEW EPISODE" bug appeared halfway into the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 12, 2011, December 17, 2011&lt;/cell&gt;
        &lt;cell&gt;Typically, episodes of Bakugan Battle Brawlers have the screenbug in the top right corner of the screen, presumably to avoid obscuring the scores. However, the "Up Next" banner, which at the time was an extension of the lower right hand corner screenbug, appeared at the bottom of the screen twice during the premieres of the episodes Countdown to Doomsday and Evil Evolution, with the words "Cartoon Network" not visible on the logo, causing the episodes to temporarily have two screenbugs.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 3, 2012&lt;/cell&gt;
        &lt;cell&gt;Today's airing of The Flintstones episode No Help Wanted accidentally aired with multiple language tracks being played at the same time.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 19, 2012&lt;/cell&gt;
        &lt;cell&gt;During today's airing of Regular Show episode Weekend at Benson's, a slightly transparent banner advertising a new episode was used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 24, 2012&lt;/cell&gt;
        &lt;cell&gt;During the 6:00pm airing of Adventure Time episode Another Way, the Level Up countdown timer appears, but then disappears a split-second later and does not return for the rest of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 14, 2012&lt;/cell&gt;
        &lt;cell&gt;A banner advertising tonight's new episode of The Amazing World of Gumball mistakenly addressed the show as "The Amazing Adventures of Gumball".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 28, 2012&lt;/cell&gt;
        &lt;cell&gt;During the 8:00pm premiere of Regular Show episode Dead at Eight, the TV-Y7-FV rating bug was accidentally shown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 16, 2012&lt;/cell&gt;
        &lt;cell&gt;During the encore airing of the Beyblade: Metal Fury Destroyer Dome "movie", a banner appears advertising new episodes of Beywheelz: Powered By Beyblade that coming Saturday at 9:00am, when the show was moving timeslots to 8:00am.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 27, 2012&lt;/cell&gt;
        &lt;cell&gt;The audio playing during the Looney Tunes short Stooge for a Mouse accidentally aired with an overlapping Portuguese dubbed track. It was fixed by the time it aired on the West feed.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 12, 2012&lt;/cell&gt;
        &lt;cell&gt;As soon as the premiere of the Regular Show episode Bald Spot started, the SD feed went to a blank screen for an unknown length of time. In addition, the SD feed was accidentally simulcast on Cartoon Network HD for the duration of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 10, 2013&lt;/cell&gt;
        &lt;cell&gt;The 11:30am airing of Looney Tunes used the TV-14 rating bug accidentally.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 10, 2013&lt;/cell&gt;
        &lt;cell&gt;The Looney Tunes episode Chariots of Fur had audio sync issues through the runtime.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 27, 2013&lt;/cell&gt;
        &lt;cell&gt;The second half of today's new Pokémon was presented without the "NEW EPISODE" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 12, 2013&lt;/cell&gt;
        &lt;cell&gt;The audio during the two Regular Show episodes played during 6:30-7:00pm was interrupted by a train noise that had apparently trailed off from a Progressive commercial.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 17, 2013&lt;/cell&gt;
        &lt;cell&gt;Technical difficulties ensued during the 11:00am airing of the Looney Tunes compilation "The Wearing of the Grin / Go Fly a Kit / Captain Hareblower / Compressed Hare / Plop Goes the Weasel / Rabbitson Crusoe / Red Riding Hoodwinked", though it is unclear as to what exactly happened.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 1, 2013&lt;/cell&gt;
        &lt;cell&gt;During the premiere of The Amazing World of Gumball episode The Castle, the New Episode screenbug disappeared 7 minutes and 29 seconds into the episode. It then returned 21 seconds later, albeit off-center and in a higher position then usual.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 18, 2013&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Regular Show episode Power Tower had its colors more saturated than usual.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 4, 2014&lt;/cell&gt;
        &lt;cell&gt;Despite both being premieres, today's 7:30am airing of Tenkai Knights and 8:00am airing of Beyblade: Shogun Steel did not use "NEW EPISODE" screenbugs at any point during their broadcasts, instead using the normal screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 3, 2014&lt;/cell&gt;
        &lt;cell&gt;The screenbug disappeared for a few minutes during the 6:15pm airing of The Amazing World of Gumball episode The Castle.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 7, 2014&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 9, 2014&lt;/cell&gt;
        &lt;cell&gt;The New Episode banner did not appear during the premiere of the Steven Universe episode Steven the Sword Fighter.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 18, 2014&lt;/cell&gt;
        &lt;cell&gt;The credits during today's airing of Diary of a Wimpy Kid did not fill up the split screen credits template.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 21, 2014&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 19, 2014&lt;/cell&gt;
        &lt;cell&gt;A promo that aired today claimed that a new episode of Adventure Time was on tonight even though said episode was a rerun.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 10, 2014&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of The Tom and Jerry Show aired without the "NEW EPISODE" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 6, 2014&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Tenkai Knights aired without a "NEW EPISODE" screenbug. This would persist on all premieres until September 27, 2014.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 23, 2014&lt;/cell&gt;
        &lt;cell&gt;The 6:45pm airing of the Steven Universe episode Watermelon Steven had the ratings and caption bug pop up early. The end of the intro was also cut off.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 27, 2014&lt;/cell&gt;
        &lt;cell&gt;The 2:45pm airing of The Amazing World of Gumball episode The Castle had the screenbug disappear two minutes in before reappearing near the end of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 29, 2014&lt;/cell&gt;
        &lt;cell&gt;The "NEW EPISODE" screenbug appeared during the 7:15am airing of the Sonic Boom episode Buster.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 1, 2014&lt;/cell&gt;
        &lt;cell&gt;The premiere of the "Uncle Grandpa episode Not Funny at 5:45pm did not use the "NEW EPISODE" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 5, 2014&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 2, 2015&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 5, 2015&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Adventure Time episode Hot Diggity Doom and The Comet at 6:00pm did not use the "NEW EPISODE" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 10, 2015&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Uncle Grandpa episode The Great Spaghetti Western at 6:00pm did not use the NEW EPISODE banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 18, 2015&lt;/cell&gt;
        &lt;cell&gt;A Gumball emoji appears for a split second during an up next/later bumper after the premiere of the Steven Universe episode We Need To Talk.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 29, 2015&lt;/cell&gt;
        &lt;cell&gt;A CHECK it. 3.0 up next/later bumper played during the premiere of the Ninjago: Masters of Spinjitzu episode Winds of Change.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 20, 2015&lt;/cell&gt;
        &lt;cell&gt;Despite the fact We Bare Bears shedded the use of the "New Series" banner in the previous week's new episode, tonight's new episode of the show would use the banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 7, 2015&lt;/cell&gt;
        &lt;cell&gt;Tonight's broadcast of Kung Fu Panda (2008) doesn't use the "PREMIERE" label, instead using the regular bug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 10, 2015&lt;/cell&gt;
        &lt;cell&gt;A CHECK it. 3.0 up next/later bumper played right before The Amazing World of Gumball episode The Pest.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 12, 2015&lt;/cell&gt;
        &lt;cell&gt;The intro to tonight's airings of Steven Universe were cut off as soon as Steven says "...and Steven!".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 28, 2015&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 29, 2015&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 3, 2015&lt;/cell&gt;
        &lt;cell&gt;The 5:15pm airing of The Amazing World of Gumball episode The Question uses the "SPECIAL" screenbug. This was likely because it was paired with Christmas.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 9, 2015&lt;/cell&gt;
        &lt;cell&gt;The theme to We Bare Bears played immediately after the first episode in the 7:00pm airing for the show, then it is followed by an up next bumper. The credits for both episodes that night also played in full.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 20, 2015&lt;/cell&gt;
        &lt;cell&gt;The 7:45pm airing of The Amazing World of Gumball episode Christmas was rated TV-PG instead of the usual TV-Y7-FV.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 23, 2015&lt;/cell&gt;
        &lt;cell&gt;The 11:30am airing of the We Bare Bears episode Food Truck aired without a screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 26, 2015&lt;/cell&gt;
        &lt;cell&gt;A CHECK it. 3.0 up next bumper played right before Escape from Planet Earth.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 31, 2015&lt;/cell&gt;
        &lt;cell&gt;The 11:00am airing of the Ninjago episode Curseworld (Part 1) aired without a screenbug. This was fixed after the first commercial break.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 4, 2016&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 18, 2016&lt;/cell&gt;
        &lt;cell&gt;The up next bumper for the premiere of the Clarence episode The Interrogation used a Gumball emoji instead of a Clarence emoji. It was later fixed on January 28, 2016 for the premiere of the episode Ren Faire.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 6, 2016&lt;/cell&gt;
        &lt;cell&gt;The premiere episode of Bunnicula today aired without credits. The split screen credits template did show up with a promo for Clarence airing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 13, 2016&lt;/cell&gt;
        &lt;cell&gt;The 10:15am airing of Wabbit: A Looney Tunes Production Carrot Before the Horse / Trunk with Power had a new episode screenbug, despite that the episode(s) premiered a week earlier. The bug also continued into the split screen credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 24, 2016&lt;/cell&gt;
        &lt;cell&gt;Due to a last minute change, the 5:00pm airing of the Clarence episode Freedom Cactus aired with a "NEW EPISODE" screenbug. The episode that originally was scheduled to premiere, Sneaky Peeky, aired the week afterwards.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 25, 2016&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for We Bare Bears during the Basket Fulla Premieres block had no clip.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 25, 2016&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 12, 2016&lt;/cell&gt;
        &lt;cell&gt;The second new Steven Universe episode shown tonight, Gem Drill used the Yoursday bug typically shown on reruns during the block, despite the episode being new.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 3, 2016&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for Clarence aired before tonight's new Regular Show episode on the Yoursday block. The same happened afterwards with a Powerpuff Girls up next bumper right before the signoff.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 11, 2016&lt;/cell&gt;
        &lt;cell&gt;The screenbug popped up in a commercial break during the premiere of Horton Hears a Who!.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 27, 2016&lt;/cell&gt;
        &lt;cell&gt;The Burst lower thirds were accidentally used today. In addition, the network also aired promos for last week's Yoursday block.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 4, 2016&lt;/cell&gt;
        &lt;cell&gt;A bumper claiming that more Teen Titans Go! was next during an airing of Alvin and the Chipmunks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 13, 2016&lt;/cell&gt;
        &lt;cell&gt;A TV-PG icon appeared in the split screen credits right before the 4:00pm airing of Clarence.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 19, 2016&lt;/cell&gt;
        &lt;cell&gt;On the 11:30am airing of The Amazing World of Gumball, The Password accidentally aired twice instead of airing The Mothers first, and then The Password.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 23, 2016&lt;/cell&gt;
        &lt;cell&gt;A "SERIES PREMIERE" screenbug appeared during today's airing of Alvin and the Chipmunks: The Squeakuel.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 25, 2016, July 28, 2016&lt;/cell&gt;
        &lt;cell&gt;No commercials were shown between the We Bare Bears episodes Slumber Party and Shush Ninjas. An extended commercial break was shown directly after the two instead. The same thing happened three days later, this time with two bumpers instead, cutting off before the episode Everyday Bears aired.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 31, 2016&lt;/cell&gt;
        &lt;cell&gt;No credits were shown for the Ben 10 episodes that aired today.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 4, 2016&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for Steven Universe aired during a block of new Supernoobs episodes at 4:00pm, despite the latter continuing for another 30 minutes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 19, 2016&lt;/cell&gt;
        &lt;cell&gt;During tonight's new episode of The Amazing World of Gumball, a banner promoting said new episode aired during it.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 20, 2016&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for the Teen Titans Go! special Island Adventures instead reuses announcer audio from a Scooby-Doo! Music of the Vampire (2012) next bumper.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 22, 2016&lt;/cell&gt;
        &lt;cell&gt;A promo for The Amazing World of Gumball claimed a new episode was about to start right now, but said episode, The Hug, already premiered.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 25, 2016&lt;/cell&gt;
        &lt;cell&gt;During The Amazing World of Gumball episode The Egg, the Cartoon Network screenbug disappears for 10 seconds. A "Coming Up Next" banner was likely meant to show up during that time.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 22, 2016&lt;/cell&gt;
        &lt;cell&gt;An up next bumper during tonight's airing of We Bare Bears had significant freezing issues.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 27, 2016&lt;/cell&gt;
        &lt;cell&gt;A Burst CN Sayin filler aired during today's episodes of Wabbit..&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 29, 2016&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 30, 2016&lt;/cell&gt;
        &lt;cell&gt;The rerun of the Regular Show episode Ugly Moons used a "NEW EPISODE" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 6, 2016&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 12, 2016&lt;/cell&gt;
        &lt;cell&gt;The 7:30am airing of Steven Universe did not play the intro.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 13, 2016&lt;/cell&gt;
        &lt;cell&gt;The credits to tonight's Steven Universe episodes did not air.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 20, 2016&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 25, 2016&lt;/cell&gt;
        &lt;cell&gt;A Burst bumper aired today, notably one dedicated to Scooby-Doo.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 27, 2016&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 3, 2016&lt;/cell&gt;
        &lt;cell&gt;The screenbug disappeared during tonight's new Regular Show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 16, 2016&lt;/cell&gt;
        &lt;cell&gt;The premiere of the Clarence episode Fishing Trip aired without a “NEW EPISODE” banner intact to the screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 1, 2016&lt;/cell&gt;
        &lt;cell&gt;The split screen credits were glitched before the premiere of the Uncle Grandpa episode Chill Out.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 11, 2016&lt;/cell&gt;
        &lt;cell&gt;The 7:30pm airing of The Amazing World of Gumball during the Wonderful Wattersons Weekend marathon used the full credits sped up in the standard split screen credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 30, 2016&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for today's Steven Universe episodes aired as normal before cutting to black for 20 seconds, followed by a portion of the 12 Days of Magiswords promo before cutting straight to the intro, which then gets cut off once it gets to "...and Steven!" before going to straight to the episode shown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 18, 2017&lt;/cell&gt;
        &lt;cell&gt;A CHECK it. 3.0 up next bumper played right before Scooby-Doo! Curse of the Lake Monster (2010).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 23, 2017&lt;/cell&gt;
        &lt;cell&gt;The up next bumpers before and during tonight's new Adventure Time episodes were for We Bare Bears.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 24, 2017&lt;/cell&gt;
        &lt;cell&gt;A Burst up next bumper for Adventure Time aired before tonight's new episodes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 30, 2017&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for new Adventure Time aired before tonight's airing of Steven Universe.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 10, 2017&lt;/cell&gt;
        &lt;cell&gt;The up next bumpers for tonight's episodes of Teen Titans Go! and Steven Universe were advertised as new, even though those episodes were repeats.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 15, 2017&lt;/cell&gt;
        &lt;cell&gt;The "NEW" screenbug was used during the 12:00pm hour.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 8, 2017&lt;/cell&gt;
        &lt;cell&gt;The crunch the split screen credits use was applied twice to today's airing of Mighty Magiswords.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 10, 2017&lt;/cell&gt;
        &lt;cell&gt;A CHECK it. 3.0 up next bumper played right before Diary of a Wimpy Kid: Rodrick Rules (2011).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 16, 2017&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug appeared during most of the Teen Titans Go! marathon that day. Along with that, the Cartoon Network screenbug appeared on The Brak Show at 8:00pm, before fixing itself later on Space Ghost Coast To Coast at 8:15pm.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 21, 2017&lt;/cell&gt;
        &lt;cell&gt;A promo claimed that a new episode of We Bare Bears would air tonight, with said episode instead being a rerun.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 26, 2017&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 27, 2017&lt;/cell&gt;
        &lt;cell&gt;The screenbug appeared on the intro for today's Steven Universe compilation block.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 2, 2017&lt;/cell&gt;
        &lt;cell&gt;Today's Steven Universe episodes aired without credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 29, 2017&lt;/cell&gt;
        &lt;cell&gt;The screenbug was transparent during the premiere of LEGO DC Super Hero Girls: Galactic Wonder. At various points throughout the day, the picture also had a ghosting effect.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 8, 2017&lt;/cell&gt;
        &lt;cell&gt;Banners promoting the month long premiere bomb of Clarence and Uncle Grandpa used the Arial font instead of the Dimensional branding's generic font.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 27, 2017&lt;/cell&gt;
        &lt;cell&gt;The promos featured on the split screen credits for today's airings of Clarence and The Amazing World of Gumball had audio for different promos compared to those that actually aired.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 12, 2017&lt;/cell&gt;
        &lt;cell&gt;The screenbug appeared during a commercial break right before the 5:30pm airing of Teen Titans Go!.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 12, 2017&lt;/cell&gt;
        &lt;cell&gt;Two banners advertising nothing appeared during the 7:00 and 7:15am airings of Teen Titans Go!.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 17, 2017&lt;/cell&gt;
        &lt;cell&gt;A banner promoting The Powerpuff Girls special The Power of Four during Over the Garden Wall was glitched.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 2, 2017&lt;/cell&gt;
        &lt;cell&gt;An up next bumper for the 2005 incarnation of Ben 10 aired instead of a bumper for the 2017 incarnation. Ironically, the original show would appear on the schedule starting later that week.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 20, 2017&lt;/cell&gt;
        &lt;cell&gt;The "NEW NEW NEW NEW" banner did not appear during the premiere of the We Bare Bears episode Spa Day.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 19, 2017&lt;/cell&gt;
        &lt;cell&gt;During a commercial break for tonight's airing of Teen Titans: Trouble in Tokyo (2006), the network screenbug did not disappear. After said break, no screenbug appeared. The split screen credits were also scaled in an unusual way.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 21, 2017&lt;/cell&gt;
        &lt;cell&gt;Today's airing of Dr. Seuss' The Cat in the Hat (2003) had its audio said to Spanish, even on the English audio track.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 22, 2017&lt;/cell&gt;
        &lt;cell&gt;The credits for Kung Fu Panda (2008) aired twice in a row, with the second time also being sped up similar to most movie airings on the network.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 27, 2017&lt;/cell&gt;
        &lt;cell&gt;Tonight's airing of The Amazing World of Gumball episode The Society had issues with its intro. It first froze, then cut straight to a Dreamland Arcade ad, then the intro was repeated again.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 28, 2017&lt;/cell&gt;
        &lt;cell&gt;Today's sneak preview airing of the Unikitty! episode Sparkle Matter Matters cut to a black screen midway through the episode and the audio had skipping issues. Before the episode ended, it cut straight into a Teen Titans Go! episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 3, 2017&lt;/cell&gt;
        &lt;cell&gt;The split screen credits after today's Unikitty! airing calls the show "Unitkitty".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 21, 2017&lt;/cell&gt;
        &lt;cell&gt;A broken up next banner for Ben 10 appeared during The Amazing World of Gumball, simply showing the letter B and cutting off.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 29, 2017&lt;/cell&gt;
        &lt;cell&gt;The split screen credits for tonight's new Steven Universe episodes froze on one credit.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 25, 2018&lt;/cell&gt;
        &lt;cell&gt;The split screen credits for Adventure Time and OK K.O.! Let's Be Heroes! froze, while the ratings bug and screenbug appeared early.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 23, 2018&lt;/cell&gt;
        &lt;cell&gt;The second part of the Teen Titans Go! episode BBRAE did not air today, with Secret Garden airing instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 3, 2018&lt;/cell&gt;
        &lt;cell&gt;The split screen credits for the 10:30am airing of Teen Titans Go! were split screened as well.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 17, 2018&lt;/cell&gt;
        &lt;cell&gt;The split screen credits for Ben 10 and OK K.O.! Let's Be Heroes aired during the Saint Paddy's Day block of Teen Titans Go!. This was likely a result of a last minute change.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 31, 2018&lt;/cell&gt;
        &lt;cell&gt;A banner advertising Craig of the Creek lists the show as "Craig of the Week".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 6-7, 2018&lt;/cell&gt;
        &lt;cell&gt;Bumpers for The Powerpuff Girls special The Power of Four and Teen Titans aired after episodes of Craig of the Creek and Teen Titans Go! respectively, despite the two not on the schedule on those days.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 8, 2018&lt;/cell&gt;
        &lt;cell&gt;A next banner during today's new The Powerpuff Girls episodes was bugged.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 9, 2018&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 16, 2018&lt;/cell&gt;
        &lt;cell&gt;Due to a last minute change, the split screen credits for OK K.O.! Let's Be Heroes still aired even though Ben 10 was airing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 7, 2018&lt;/cell&gt;
        &lt;cell&gt;The split screen credits for tonight's new episodes of Steven Universe aired after tonight's Craig of the Creek.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 10, 2018&lt;/cell&gt;
        &lt;cell&gt;A banner advertising Craig of the Creek lists the show as "Craig of the Craig".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 14, 2018&lt;/cell&gt;
        &lt;cell&gt;A bumper mix-up occured before a block of The Amazing World of Gumball episodes. A bumper for OK K.O.! Let's Be Heroes! aired before switching midway to a Gumball bumper.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 28, 2018&lt;/cell&gt;
        &lt;cell&gt;A different font is used for the split screen credits for Alvin and the Chipmunks: The Road Chip.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 2, 2018&lt;/cell&gt;
        &lt;cell&gt;A up next bumper for the 2005 incarnation of Ben 10 was used right before the 2016 incarnation of Ben 10.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 4, 2018&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 15, 2018&lt;/cell&gt;
        &lt;cell&gt;Audio issues occured during today's Steven Universe airing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 17, 2018&lt;/cell&gt;
        &lt;cell&gt;Some issues happened during today's Clarence airings: &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 1, 2018, July 15, 2018&lt;/cell&gt;
        &lt;cell&gt;Today's episodes of The Powerpuff Girls were incorrectly advertised as new.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 2, 2018&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 2, 2018 - July 4, 2018&lt;/cell&gt;
        &lt;cell&gt;The New banner did not appear during new episodes of Steven Universe.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 4, 2018&lt;/cell&gt;
        &lt;cell&gt;The split-screen credits to tonight's OK K.O.! Let's Be Heroes airings ended early, interfering with a Summer Camp Island promo before hanging on a black screen for 10 seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 5, 2018&lt;/cell&gt;
        &lt;cell&gt;The Steven Universe episode Serious Steven aired with a NEW bug even though said episode premiered 4 years ago.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 16, 2018&lt;/cell&gt;
        &lt;cell&gt;During an airing of The Amazing World of Gumball episode The Brain, the credits began airing earlier than expected, but were cut off at the last second.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 19, 2018&lt;/cell&gt;
        &lt;cell&gt;A bumper for Unikitty! falsely claimed that a new episode was coming up.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 15, 2018&lt;/cell&gt;
        &lt;cell&gt;The intros for today's episodes of Unikitty! and Craig of the Creek were skipped.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 22, 2018&lt;/cell&gt;
        &lt;cell&gt;A banner promoting the series premiere of Total DramaRama had minor text glitches.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 9, 2018&lt;/cell&gt;
        &lt;cell&gt;A bumper claiming that The Amazing World of Gumball was starting instead cut to commercials.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 11, 2018&lt;/cell&gt;
        &lt;cell&gt;The 4:45pm airing of The Amazing World of Gumball aired in Turner's Flexview format with no screenbug in sight until the last minute.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 21, 2018&lt;/cell&gt;
        &lt;cell&gt;The NEW banner did not appear during today's new OK K.O.! Let's Be Heroes episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 2, 2018&lt;/cell&gt;
        &lt;cell&gt;The 3:45pm airing of The Amazing World of Gumball episode The Castle did not present a screenbug until halfway through.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 3, 2018&lt;/cell&gt;
        &lt;cell&gt;A banner for the premiere of Night at the Museum: Secret of the Tomb (2014) had the PREMIERE TONIGHT 7P repeated twice.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 10, 2018&lt;/cell&gt;
        &lt;cell&gt;The NEW banner did not appear during today's new Transformers: Cyberverse episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 17, 2018&lt;/cell&gt;
        &lt;cell&gt;In-episode credits for Steven Universe were shown during The Amazing World of Gumball episode The Lie.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 24, 2018&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim signoff bumper aired during a commercial break during the 3:00pm airing of The Amazing World of Gumball.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 1, 2019&lt;/cell&gt;
        &lt;cell&gt;Today's episodes of Total Drama Island were labelled as a special before being removed after the theme song played.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 4, 2019&lt;/cell&gt;
        &lt;cell&gt;Today's airing of the Total Drama Island episode Not So Happy Campers (Part 2) aired in native 4:3 and without credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 24, 2019&lt;/cell&gt;
        &lt;cell&gt;During the 7:45pm airing of The Amazing World of Gumball episode "The Decisions", an up next banner for American Dad! was used accidentally, instead of having no banner present.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 26, 2019&lt;/cell&gt;
        &lt;cell&gt;At the end of the 7:45pm airing of The Amazing World of Gumball episode "The Stories", the screenbug spilled over into the commercial break for about a second and a half.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 7, 2019&lt;/cell&gt;
        &lt;cell&gt;During a rerun of the OK K.O.! Let's Be Heroes episode Crossover Nexus, The SPECIAL Banner appeared depsite the fact even though said it's the a year ago, placeholder lower-third credits for Teen Titans Go! featuring credits for then-unaired episodes were used. In addition, a promotional banner advertising "Next New DC Super Hero Girls" appeared over top of the credits in place of a proper Next banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 7, 2019, February 12, 2022&lt;/cell&gt;
        &lt;cell&gt;When Cartoon Network signs off at 8:00pm, the sign-on bumper is used instead of its usual sign-off bumper.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 25, 2020&lt;/cell&gt;
        &lt;cell&gt;At the end of the 5:30pm airing of the Total DramaRama episode "Look Who's Clocking", the intro to said show was used in place of the episode's credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 6, 2020&lt;/cell&gt;
        &lt;cell&gt;The 12:45pm airing of Mao Mao: Heroes of Pure Heart episode Captured Clops had a NEW bug, despite the episode premiering less than a year before.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 7, 2020&lt;/cell&gt;
        &lt;cell&gt;The 12:45pm airing of Mao Mao: Heroes of Pure Heart episode Flyaway had a NEW bug, despite the episode premiering less than a year before.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 22, 2020&lt;/cell&gt;
        &lt;cell&gt;The on-screen credits for the 10:45am airing of the Craig of the Creek episode "The Curse" erroneously utilized Apple &amp;amp; Onion's Cartoon Network Studios vanity plate.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 14, 2020&lt;/cell&gt;
        &lt;cell&gt;The split-screen credits for tonight's airing of Teen Titans Go! vs. Teen Titans (2019) did not mute the audio from the movie's closing credits, resulting in heavily distorted, overlapping audio due to those credits being massively sped up to fit into the split-screen credits, combined with the audio from the promo within the split-screen credits continuing to play on top.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 29, 2021&lt;/cell&gt;
        &lt;cell&gt;The credits for The LEGO Movie 2: The Second Part (2019) were used instead of How to Train Your Dragon's (2010) during that night's airing of the latter.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 2, 2021&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 3, 2021&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 19, 2021&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Total DramaRama aired without the "NEW" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 4, 2021&lt;/cell&gt;
        &lt;cell&gt;A Burst bumper aired before the premiere of the Total DramaRama episode Erase Yer Head.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 15, 2021&lt;/cell&gt;
        &lt;cell&gt;A Cartoonito screen bug appeared during the airing of The Amazing World of Gumball episode The Club.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 19, 2021&lt;/cell&gt;
        &lt;cell&gt;During that day's airings of the Teen Titans Go! episodes A Farce and Wally T, the screenbug is absent. This does not affect promotional materials.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 26, 2021&lt;/cell&gt;
        &lt;cell&gt;During the 5:45pm airing of The Amazing World of Gumball episode The Vegging, the up next banner shows the premiere of Cartoon Network is up next instead of Man of Steel (2013).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 9, 2021&lt;/cell&gt;
        &lt;cell&gt;The "NEW" banner was not used for any of that day's episode premieres.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 17, 2021&lt;/cell&gt;
        &lt;cell&gt;The premiere screenbug was used during Detective Pikachu (2019), despite the movie premiering a few months earlier.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 23, 2021&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 3, 2021&lt;/cell&gt;
        &lt;cell&gt;A broken up next banner appeared during Teen Titans Go!, simply showing the word "NEXT" and cutting off.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 4, 2021&lt;/cell&gt;
        &lt;cell&gt;After the premiere of Teenage Mutant Ninja Turtles II: Secret of the Ooze (1991) ended, the continues next bumper for the film played even though has already ended when actually Teen Titans Go! was up next. In addition, the 8:45pm airing of Teen Titans Go! aired with a TV-G rating instead of the usual TV-PG rating.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 18, 2021&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug was used during the last 15 minutes of the 6:00pm airing of Scooby-Doo (2002).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 19, 2021, December 24, 2021&lt;/cell&gt;
        &lt;cell&gt;The "Mashup" split screen credits and "Dimensional" bumpers are accidentally used on the older shows instead of the normal Redraw Your World split screen credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 26, 2021&lt;/cell&gt;
        &lt;cell&gt;Despite airing on Cartoon Network before, tonight's ACME Night broadcast of Tim Burton's Corpse Bride (2005) mistakenly uses the "PREMIERE" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 27, 2021&lt;/cell&gt;
        &lt;cell&gt;During the 6:00pm airing of TMNT (2007), the continues next bumper used was one for Teenage Mutant Ninja Turtles (1990) instead of one for TMNT (2007).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 1, 2022&lt;/cell&gt;
        &lt;cell&gt;The Cartoonito sign on was used immediately after the regular sign on, despite there being no Cartoonito programming that day.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 8, 2022&lt;/cell&gt;
        &lt;cell&gt;During the 1:15pm airing of the Teen Titans Go! episode Trans Oceanic Magical Cruise, the screenbug was absent. This did not affect promotional materials.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 29, 2022&lt;/cell&gt;
        &lt;cell&gt;During Puss in Boots (2011), the Next banner said that "Cartoon Network" was next.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 14, 2022&lt;/cell&gt;
        &lt;cell&gt;The credits for Minions (2015) were used instead of the credits for Rio 2 (2014). This is due to the former movie originally being scheduled for this date, and the latter originally being scheduled for the following week.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 27, 2022&lt;/cell&gt;
        &lt;cell&gt;During the start of the 6:30am airing of Ben 10 and the 7:30am airing of Teen Titans, the TV Rating wasn't present at the beginning of the episodes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 20, 2022&lt;/cell&gt;
        &lt;cell&gt;The credits for Minions (2015) were used instead of the credits for Spider-Man 3 (2007) during that night's ACME Night.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 17, 2022&lt;/cell&gt;
        &lt;cell&gt;Despite airing on Cartoon Network before, tonight's ACME Night broadcast of Nacho Libre (2006) mistakenly uses a "PREMIERE" banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 24, 2022&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 2, 2022&lt;/cell&gt;
        &lt;cell&gt;During the Teen Titans Go! episode Them Soviet Boys, the banner promoted We Baby Bears new episodes for Tomorrow TBD.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 3 - May 5, 2022&lt;/cell&gt;
        &lt;cell&gt;The premieres of We Baby Bears episodes Ice Bear's Pet, Dragon Pests, and No Land, All Air! all lacked new episode screenbugs.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 7, 2022&lt;/cell&gt;
        &lt;cell&gt;A blank Next banner appeared during the 9:45am rerun of We Baby Bears.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 8, 2022&lt;/cell&gt;
        &lt;cell&gt;During the 5:45pm airing of The Amazing World of Gumball the on-screen credits went overtime continuing into commercials for 30 seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 9, 2022&lt;/cell&gt;
        &lt;cell&gt;A blank Next banner appeared during the 4:15pm rerun of We Baby Bears.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 28, 2022&lt;/cell&gt;
        &lt;cell&gt;The premiere banner did not appear during the premiere of Teen Titans Go! &amp;amp; DC Super Hero Girls: Mayhem in the Multiverse. Additionally, the split screen credits were for Teen Titans Go! to the Movies (2018) instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 6, 2022&lt;/cell&gt;
        &lt;cell&gt;Despite Cartoonito losing the 10:00am hour, its screenbug was used during said times.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 12, 2022&lt;/cell&gt;
        &lt;cell&gt;The 7:00pm airing of The Amazing World of Gumball episode The Pact (which had premiered over four years prior), accidentally used the NEW banner.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 24, 2022&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug shows up during the first acts of that night's Bob's Burgers episodes, airing as part of ACME Night during the unbranded 8:00pm slot.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 22, 2022&lt;/cell&gt;
        &lt;cell&gt;The 5:00pm We Baby Bears episode "Tooth Fairy Tech" airing had a "Next New" banner for new episodes of We Baby Bears up next, even though the episode up next is a rerun. In addition to this the following episodes aired after it: "High Baby Fashion", "Teddi Bear", "A Gross Worm" and "A Real Crayon", all used new episode bugs despite all the episodes premiering earlier this week.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 30, 2022&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 3, 2022&lt;/cell&gt;
        &lt;cell&gt;The credits for Steven Universe: The Movie were used instead of the ones for Teen Titans: Trouble in Tokyo (2006) at 8:00am. Later that day, the same thing happened the other way around; the credits for Teen Titans: Trouble in Tokyo (2006) were used instead of the ones for Steven Universe: The Movie (2019) at 4:00pm.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 3, 2022&lt;/cell&gt;
        &lt;cell&gt;Despite Cartoon Network regaining the 6am hour on weekdays, the Cartoonito screenbug was used during said times.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 4, 2022&lt;/cell&gt;
        &lt;cell&gt;A "Redraw Your World" banner promoting the premiere of Straight Outta Nowhere: Scooby-Doo Meets Courage The Cowardly Dog (2021) appeared during the 6pm airing of The Amazing World of Gumball, despite Cartoon Network phasing out the branding by this point.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 10, 2022, October 31, 2022&lt;/cell&gt;
        &lt;cell&gt;During the 7:00pm airing of Scooby-Doo and the Monster of Mexico (2003), a pop up banner advertising the premiere of "Text ByMaterialID '$Child' InChannel 'Scheduled Channel';" was advertised. This was likely used to test the banner. The same banner appeared again during an airing of Over the Garden Wall later that month, now being advertised as "NEW All Next Week 4".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 13, 2022&lt;/cell&gt;
        &lt;cell&gt;The New banner appeared during the 6:45am airing of The Amazing World of Gumball episode The Banana, even though said episode premiered 10 years ago.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 3, 2022, November 21, 2022&lt;/cell&gt;
        &lt;cell&gt;The Cartoonito short intro was used before select episodes of Lamput.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 4, 2022&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 10, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 18, 2023&lt;/cell&gt;
        &lt;cell&gt;The Sticker up next bumper for The LEGO Movie 2: The Second Part (2019) was accidentally used instead of the normal Split Screen up next bumper.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 19, 2023&lt;/cell&gt;
        &lt;cell&gt;During The LEGO Movie 2: The Second Part (2019), a banner appeared with no text whatsoever. This happened again during the 5:45pm rerun of Teen Titans Go!.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 25, 2023&lt;/cell&gt;
        &lt;cell&gt;The 6:00pm airing of Scooby-Doo!: Legend of the Phantosaur aired with the film's end credits and closing logos being shown in their entirety before both were shown again in the split-screen credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 26, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 26, 2023&lt;/cell&gt;
        &lt;cell&gt;The 3:49pm airing of the Teen Titans Go! episode Real Art used the TV-Y7-FV rating instead of its normal TV-PG rating. The episodes of the show that aired during 10am-12pm also had the Marathon screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 1, 2023&lt;/cell&gt;
        &lt;cell&gt;The 9:30am airing of The Looney Tunes Show episode Reunion had a NEW bug, despite the episode premiering over a decade ago.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 4, 2023&lt;/cell&gt;
        &lt;cell&gt;The screenbug disappeared during the second half of the Foster's Home for Imaginary Friends episode Challenge of the Super Friends. When it disappeared seemed to be dependent on which feed it was on.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 14, 2023&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug shows up during that night's Futurama episodes, airing as part of ACME Night during the bugless purgatory hour.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 20, 2023&lt;/cell&gt;
        &lt;cell&gt;Cartoonito's split screen credits template was shown at the end of the Teen Titans Go! episode Toilet Water.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 2, 2023, June 9, 2023, June 16, 2023&lt;/cell&gt;
        &lt;cell&gt;The TV-Y rating bug was shown at the beginning of the first and second act of the The Looney Tunes Show episode Muh-Muh-Muh-Murder, as opposed to the episode's actual TV-PG rating. This error would persist for the re-runs of episodes Point, Laser, Point and Bobcats on Three!.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 4, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 11, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 19, 2023&lt;/cell&gt;
        &lt;cell&gt;The screenbug disappeared during the second half of the New Looney Tunes episode(s) Bigs Bunny / Wahder, Wahder, Everywhere and Computer Bugs / Oil's Well That Ends Well.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 28, 2023-June 30, 2023&lt;/cell&gt;
        &lt;cell&gt;The Lamput shorts Orange Rose and Orange Umbrella airing after the Craig of the Creek episode The Ice Pop Trio were advertised as new on the "up next" banner, despite already premiering months prior. A similar situation occurred during the Craig of the Creek episodes The Last Game of Summer on June 29th and Afterschool Snackdown on June 30th.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 30, 2023&lt;/cell&gt;
        &lt;cell&gt;The on-screen credits shown at the end of the Adventure Time episode "The Real You" continued into the first few seconds of the commercial break.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 9, 2023&lt;/cell&gt;
        &lt;cell&gt;The premiere of All-Star Superman (2011) used the old Helvetica ratings bug, used Adult Swim "River" split screen credits, and during the first act, there was no screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 13, 2023&lt;/cell&gt;
        &lt;cell&gt;During the airings of The Looney Tunes Show episodes The Grand Old Duck of York and The Shelf, the bumpers for classic Looney Tunes were used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 16, 2023&lt;/cell&gt;
        &lt;cell&gt;About 17 minutes into the 1:00pm airing of the Teen Titans Go! episode "And the Award for Sound Design Goes to Rob / Some of Their Parts", the feed blacked out for six seconds, then the screenbug disappeared for the rest of the episode. The screenbug returned during the 1:30pm airing of the episode "Cat's Fancy".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 23, 2023&lt;/cell&gt;
        &lt;cell&gt;Today's new episodes of Summer Camp Island erroneously used the regular screenbug instead of the "NEW" screenbug. This was likely because of scheduling changes shifting the premieres of the episodes The Babies Chapter 1: Breakfast Errands, The Babies Chapter 2: Teacup Giant and The Babies Chapter 3: Lem Is Nothing to Sunday rather than the usual weekday slot.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 28, 2023&lt;/cell&gt;
        &lt;cell&gt;After the 4:45pm airing of The Amazing World of Gumball episode The BFFs, an Up Next bumper for The Amazing World of Gumball played, despite Cartoon Network signing off at 5:00pm on this day.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 9, 2023&lt;/cell&gt;
        &lt;cell&gt;The 12pm hour encores of Tiny Toons Looniversity aired with the new episode screenbug despite both premiering earlier at 9am hour.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 23, 2023&lt;/cell&gt;
        &lt;cell&gt;The repeat of the Teen Titans Go! episode Utility Belt at 9:45am used the New screenbug and had the "new" bumpers before it. This is very likely due to it and the early premiere of Intro replacing the WB 100th Anniversary episode at the last minute.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 25, 2023&lt;/cell&gt;
        &lt;cell&gt;Several on-air elements and bumpers aired incorrectly as a result of the transition from Turner's network operations center in Atlanta, Georgia to Discovery's cloud-based network operations center in Sterling, Virginia. See this page for more info.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 26, 2023 - September 27, 2023&lt;/cell&gt;
        &lt;cell&gt;Continuing from the Discovery migration, the classic Looney Tunes bumper still played before and during the Baby Looney Tunes episode(s) A Bully for Bugs / The Wheel Deal and Oh Brother, Warehouse Art Thou? / Flu the Coop at 9:00am and 9:30am respectively on the 26th. On the 27th, the Teen Titans bumper played before the 2:00pm showing of the Teen Titans Go! episode Booby Trap House.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 9, 2023&lt;/cell&gt;
        &lt;cell&gt;At around 6:15am, during an episode of The Amazing World of Gumball, the screenbug disappears, but comes back at around 7:15am.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 29, 2023&lt;/cell&gt;
        &lt;cell&gt;The Up Next bumper for The Looney Tunes Show was accidentally shown before the 12:30pm airing of the Tiny Toons Looniversity episode Tears of a Clone.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 18, 2023&lt;/cell&gt;
        &lt;cell&gt;The "New" screenbug was used during the 9:45am airing of the Teen Titans Go! episode Thanksgiving despite it first airing in 2014.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 10, 2023&lt;/cell&gt;
        &lt;cell&gt;The ratings bug and screenbug appeared in the credits of the 3:00pm airing of The Amazing World of Gumball episode Christmas.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 15, 2023&lt;/cell&gt;
        &lt;cell&gt;The 3:30pm airing of the Adventure Time episode Holly Jolly Secrets had its colors heavily desaturated.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 24, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 5, 2024&lt;/cell&gt;
        &lt;cell&gt;Screenbugs were absent for about four hours after Cartoon Network's sign-on.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 8, 2024&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Jessica's Big Little World aired without the "New" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 20, 2024&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Batwheels aired without the "New" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 26, 2024&lt;/cell&gt;
        &lt;cell&gt;The rerun of the Steven Universe episode Political Power had its colors heavily desaturated.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 27, 2024&lt;/cell&gt;
        &lt;cell&gt;Today's new episode of Jessica's Big Little World aired without the "New" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 9, 2024, July 7, 2024&lt;/cell&gt;
        &lt;cell&gt;Both airings of The Scooby-Doo Show on June 9, 2024 were preceded by the Up Next bumper for Scooby-Doo and Guess Who?. Similar to the error affecting Baby Looney Tunes the preceding September, this was likely because a Prism bumper for the show that actually came on didn’t exist yet. The error was fixed once the show aired again on June 16, 2024, only to return (albeit only during the second half-hour broadcast of the series that day instead of both airings) on July 7, 2024.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 14, 2024 - present&lt;/cell&gt;
        &lt;cell&gt;All premieres of episodes that either premiered on other Warner Bros. Discovery networks and/or were released on Max prior to their first airing on Cartoon Network no longer have the "NEW" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 6, 2024&lt;/cell&gt;
        &lt;cell&gt;An Adult Swim advertisement bug promoting the R-rated movie Joker: Folie à Deux appeared during the 6:00am airing of the Be Cool, Scooby-Doo! episode Some Fred Time.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 18, 2024&lt;/cell&gt;
        &lt;cell&gt;The 2:30pm airing of The Polar Express was mistakenly rated TV-PG instead of the usual TV-G.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 20, 2024&lt;/cell&gt;
        &lt;cell&gt;The 2:30pm airing of 8-Bit Christmas aired without a ratings bug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 21, 2024&lt;/cell&gt;
        &lt;cell&gt;The 4:45pm rerun of the Regular Show episode This is My Jam' had its colors heavily desaturated.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 23, 2024&lt;/cell&gt;
        &lt;cell&gt;During the early morning airings of Teen Titans Go!, the up next banner accidentally appeared during the intro of the show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 25, 2024&lt;/cell&gt;
        &lt;cell&gt;During the airing of Teen Titans Go! episode "Sweet Revenge", an up next banner for Ed, Edd n Eddy was used accidentally, instead of having no banner present.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 30, 2024&lt;/cell&gt;
        &lt;cell&gt;The "NEW" screenbug did not appear during the premiere of the Teen Titans Go! episode "Four Hundred".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 17, 2025&lt;/cell&gt;
        &lt;cell&gt;During the entirety of the 1pm hour Teen Titans Go! airings, the up next bumper used was the Teen Titans Go! vs. Teen Titans one instead of the normal Teen Titans Go! one.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 3, 2025&lt;/cell&gt;
        &lt;cell&gt;The 10:30am airing of the The Powerpuff Girls episode Just Another Manic Mojo / Mime for a Change accidentally aired in widescreen.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 6, 2025&lt;/cell&gt;
        &lt;cell&gt;During both episodes of Total DramaRama, the Total Drama Island (2023) up next bumper was used instead of the one for Total DramaRama.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 18, 2025&lt;/cell&gt;
        &lt;cell&gt;The Up Next banner graphic for Adventure Time did not appear during the airing of We Bare Bears episode "Slumber Party", with the Cartoon Network screenbug disappearing instead of transitioning into any such graphic (a la during whatever is playing right before Adult Swim comes on) during the tail end of said broadcast.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 25, 2025&lt;/cell&gt;
        &lt;cell&gt;Up Next banners were absent throughout the majority of the day; they returned briefly following airings of The Looney Tunes Show at 11:00am only to vanish again before Teen Titans Go! airings at 2:00pm.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 14, 2025&lt;/cell&gt;
        &lt;cell&gt;The "NEW" screenbug did not appear during the premiere of both Total Drama Island (2023) and Tiny Toons Looniversity episodes "You Poor Saps" and "Ask Not What Bunnies Can Do For You" respectively.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 2025-present&lt;/cell&gt;
        &lt;cell&gt;During certain weekday airings of Teen Titans Go! in the 1:00pm hour, the up next banner appears during the credits instead of the last scene of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 16, 2025&lt;/cell&gt;
        &lt;cell&gt;During the 1:45pm airing of the Teen Titans Go! episode Pie Bros, the season 2 intro inexplicably played instead of the season 1 intro.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 19, 2025&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 28, 2025&lt;/cell&gt;
        &lt;cell&gt;The credits for The Amazing World of Gumball episode The Parents was shown after the Cartoon Network sign-off.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 5, 2025&lt;/cell&gt;
        &lt;cell&gt;The screenbug did not appear during the Adventure Time episode Lemonhope.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 9, 2025&lt;/cell&gt;
        &lt;cell&gt;During the 1:30pm airing of The Tom and Jerry Show, epilepsy advisory cards preceded the opening credits, along with every individual short. Notably, the 1:00pm airing of the same show prior contained no such warnings.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 28, 2025&lt;/cell&gt;
        &lt;cell&gt;The screenbug did not appear during The Amazing World of Gumball episode The End.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 6, 2025&lt;/cell&gt;
        &lt;cell&gt;The 6:45am rerun of the Regular Show episode Death Punchies had its colors heavily desaturated.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 20, 2025&lt;/cell&gt;
        &lt;cell&gt;Today's premiere of Krypto Saves the Day! aired without the "NEW" screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 3, 2025&lt;/cell&gt;
        &lt;cell&gt;The 11:00am airing of Scooby Doo and the Samurai Sword used a 4:3 print.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 6, 2025&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the Bugs Bunny Builders episode Miss Prissy's Market, the SPECIAL bug is used in place of the NEW bug. This marks the first time the SPECIAL bug has been used since the Discovery playout takeover back in 2023. In addition, the premiere of the Bugs Bunny Builders episode Spellbound, also lacked a NEW bug instead using the normal bug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 19, 2025&lt;/cell&gt;
        &lt;cell&gt;During the 10:45am airing of the New Looney Tunes episode "Littlechin and the Wood Fairy," a 10 to 15 second audio delay was present, with a brief silence at the end, and partially during the credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 31, 2025&lt;/cell&gt;
        &lt;cell&gt;A Scoobtober bumper aired in place of the Cartoon Network sign off.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 1, 2025&lt;/cell&gt;
        &lt;cell&gt;The promo for Boomerang Theater was used instead of the Cartoon Network sign-off. This marks the first time in decades that a promo for Boomerang has aired on Cartoon Network.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;December 21, 2025&lt;/cell&gt;
        &lt;cell&gt;The up next bumper for Tiny Toons Looniversity was not shown before airing at 1PM.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 2, 2001 - September 10, 2001&lt;/cell&gt;
        &lt;cell&gt;Adult Swim programming would have the ratings bug linger throughout the episode. This might have been done deliberately to indicate that these programs were not for kids.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 2, 2001&lt;/cell&gt;
        &lt;cell&gt;A CN Next bumper for A Pup Named Scooby-Doo followed by 3 hours of The Flintstones replaced the proper Adult Swim bumper after the premiere of the Goldfish episode of The Brak Show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 21, 2001&lt;/cell&gt;
        &lt;cell&gt;During the 11:45pm airing of the Space Ghost Coast to Coast episode Girl Hair, the ratings bug lasted much longer than usual, appearing on-screen for two and a half minutes in total. Additionally, while a "TV-PG" bug was initially used, it switched to "TV-14" partway in--appropriately, right when Space Ghost punched Zorak in the mouth.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 29, 2001&lt;/cell&gt;
        &lt;cell&gt;Cartoon Network's screenbug was accidentally used between 10:30pm to 11:15pm.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 20, 2002&lt;/cell&gt;
        &lt;cell&gt;The first half of The Lewis Lectures aired in Spanish, then switched to English in the second half.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 17, 2002&lt;/cell&gt;
        &lt;cell&gt;The TV-14-D screenbug remained on screen for the entirety of Mission Hill.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 10, 2002&lt;/cell&gt;
        &lt;cell&gt;Cartoon Network's screenbug was accidentally used between 10:00pm to 10:45pm, with the correct screenbug being used during the second half of The Groovenians.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 8, 2003, May 20, 2003, June 23, 2003&lt;/cell&gt;
        &lt;cell&gt;During the premieres of the Family Guy episodes Fore Father, The Kiss Seen Around the World, and Peter, Peter, Caviar Eater, the Cartoon Network screenbug appeared for the first half of the episodes. In the first and third cases, it switched to the correct screenbug for the rest of the episodes, but in the second case, the remainder had neither screenbug shown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 17, 2003, August 18, 2003&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug was used during airings of the Aqua Teen Hunger Force episode Mayhem of the Mooninites and the first half of the Sealab 2021 episode Lost in Time.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 26, 2003&lt;/cell&gt;
        &lt;cell&gt;The Big O's series finale The Show Must Go On was originally scheduled to premiere that night, but a rerun of the episode Stripes was accidentally aired in its place. It would instead premiere on November 2, 2003. Said program change also delayed the American television premiere of the Family Guy episode When You Wish Upon a Weinstein by a week.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 19, 2004&lt;/cell&gt;
        &lt;cell&gt;During the first segment of the 1:30am airing of The Popeye Show, the TV-PG parental guidelines rating (most likely for the following program, the Futurama episode Mars University) accidentally popped in for a few seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 5, 2004&lt;/cell&gt;
        &lt;cell&gt;During an airing of the Family Guy episode The Kiss Seen 'Round the World, the Cartoon Network screenbug appeared for a few minutes before switching back to the Adult Swim screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 25, 2004&lt;/cell&gt;
        &lt;cell&gt;During the premiere of The Venture Brothers episode Mid-Life Chrysalis the Cartoon Network screenbug appeared for a few seconds. It then switched to the correct screenbug for the rest of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2005&lt;/cell&gt;
        &lt;cell&gt;An airing of American Dad! was initially rated TV-14-DSV. After the closed captioning bug disappeared, the TV-MA rating bug inexplicably appeared. Later, about 2 minutes into the episode, the former rating appeared again, given that a broadcast engineer managed to catch the TV-MA error.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 5, 2005&lt;/cell&gt;
        &lt;cell&gt;During the 11:00pm airing of Futurama, the Cartoon Network screenbug appeared for a few minutes before switching back to the Adult Swim screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 18, 2005&lt;/cell&gt;
        &lt;cell&gt;During the 10:00pm airing of American Dad!, the rating bug lingered for a minute throughout a commercial break.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 28, 2006&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the Robot Chicken episode Massage Chair a few seconds before the intro concludes, the TV-PG-LV bug appeared for about 30 seconds before disappearing for the remainder of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 26, 2006&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug accidentally appeared for a few seconds in the 10:00pm broadcast of Futurama. The Adult Swim bug would later appear after a minute of no present screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 21, 2007&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the Bleach episode Reclaim! The Power of the Shinigami! shortly after the show returns from the commercial break, the picture freaks out in a way similar to a satellite glitch. However, the [as] bug remains intact indicating this is a problem with the master for the episode itself.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 29, 2007&lt;/cell&gt;
        &lt;cell&gt;The final episode of Eureka Seven was aired with four and a half minutes of footage cut, specifically, the opening monologue and all but the first few seconds of the episode's epilogue. The full version of the episode was available a day earlier for twelve hours at Adult Swim Fix, the free online streaming service provided by the network, before airing it on television in full the following week.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 22, 2007&lt;/cell&gt;
        &lt;cell&gt;At the beginning of the Inuyasha episode Seekers of the Sacred Jewel, the Cartoon Network Invaded screenbug accidentally appears for a short few seconds. It would switch to the Adult Swim screenbug shortly thereafter.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 21, 2007&lt;/cell&gt;
        &lt;cell&gt;During the second half of the Inuyasha episode Forever with Lord Sesshomaru, the Cartoon Network screenbug accidentally appears and would remain on screen for the remainder of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 9, 2008&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network screenbug accidentally appeared for around a minute in the 10:00pm broadcast of Family Guy. The Adult Swim bug would later appear after about 15 seconds of no present screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 27, 2008&lt;/cell&gt;
        &lt;cell&gt;The Adult Swim screenbug was absent for the first 7 minutes of the 10:00pm Family Guy broadcast. The screenbug would later appear after about 7 minutes of no screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 19, 2009&lt;/cell&gt;
        &lt;cell&gt;Technical issues that have been present on the West Coast feed since the prior night (see the Cartoon Network tab for more info) affects the season premiere of The Venture Bros., where the first half of the episode was repeated instead of the second half. To make up for this issue, Adult Swim aired reruns of the episode for the following week at 12:30am and 4:30am. Again, the East Coast feed was not affected.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 20, 2011&lt;/cell&gt;
        &lt;cell&gt;During that night's airing of the Family Guy episode Let's Go to the Hop after the commercial break, the rating of the episode changed from TV-14-SV at the start of the program to TV-PG-D when the show returned from commercials.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 24, 2011&lt;/cell&gt;
        &lt;cell&gt;The rating bug appears twice during the opening of that night's airing of the Family Guy episode Road to Germany, making the closed captions bug not appear during that night's broadcast.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 14, 2014&lt;/cell&gt;
        &lt;cell&gt;The screenbug lingered throughout the first commercial break during tonight's Rick and Morty.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 6, 2016&lt;/cell&gt;
        &lt;cell&gt;That night's new episode of The Eric Andre Show aired with a TV-PG-L rating instead of the TV-MA-LS rating listed on the schedule. This was corrected in all reruns.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 30, 2016&lt;/cell&gt;
        &lt;cell&gt;During the "12 Days of Magiswords" event on Cartoon Network, viewers could collect Magiswords (from Mighty Magiswords) using the "MagiMobile" app when a certain icon appeared on screen. This event was exclusive to Cartoon Network, but the King of the Hill episodes Dale Not Be Proud and Apres Hank, Le Deluge at 8:00pm and 8:30pm had the Collect icon on screen, despite the event not involving Adult Swim and the inability to collect Magiswords during Adult Swim.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 4, 2017&lt;/cell&gt;
        &lt;cell&gt;Despite not cutting to a commercial break, the second half of the 1:15am airing of the Space Ghost Coast to Coast episode Jerk used a small TV-PG rating bug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 5, 2017&lt;/cell&gt;
        &lt;cell&gt;Somewhat similar to the above error in an inverse way, the Adult Swim bug disappeared during the second half of the 4:45am airing of the Space Ghost Coast to Coast episode Sleeper, and would not return for the remainder of the episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 7, 2017&lt;/cell&gt;
        &lt;cell&gt;During the second half of the 12:30am airing of The Boondocks, the TV-Y rating bug is accidentally shown.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 15, 2017 - April 16, 2017&lt;/cell&gt;
        &lt;cell&gt;The Cartoon Network logo appeared during the opening for tonight's episode of Dragon Ball Super. Said occurrence of both screenbugs appearing at the same time would continue throughout the following day on both networks, eventually stopping on a rerun of The Brak Show a day after.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 6, 2017&lt;/cell&gt;
        &lt;cell&gt;The 10:00pm airing of the Samurai Jack finale used a TV-PG-V rating instead of its original TV-14-V rating.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 17, 2019&lt;/cell&gt;
        &lt;cell&gt;Six minutes into the premiere of the Sword Art Online: Alicization episode Project Alicization, during the scene where Higa Takeru is arguing with his own Fluctlight, the show froze on one frame in the middle of the Fluctlight's sentence ("What? No way! I'm Higa! L-" [Let me out and you'll see!]). The frame remained like that for a few minutes before the show cut to a commercial break, but when returning from said break, the freeze frame from the first act persisted. Because of this, the Megalobox episode Leap Over the Edge of Death started early, then aired in full in its normal slot.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 20, 2020&lt;/cell&gt;
        &lt;cell&gt;A promo before the premiere of Wonder Woman: Bloodlines erroneously states that the regular Toonami lineup would return the week after, when a Cowboy Bebop marathon was already scheduled for that day. This may be because the Cowboy Bebop marathon was originally scheduled to air November 29, 2020 instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 21, 2021&lt;/cell&gt;
        &lt;cell&gt;That night's American Dad! and Family Guy episodes used Cartoon Network's TV-PG DLV rating bug instead of the proper ratings bug for each episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 8, 2021&lt;/cell&gt;
        &lt;cell&gt;The Harley Quinn season 1 finale, The Killing Joke, was supposed to air at 4:30am, but was accidentally replaced at the last minute with a rerun of Finding Mr. Right.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 28, 2021&lt;/cell&gt;
        &lt;cell&gt;During the beginning of the 11:30pm airing of the Rick and Morty episode Rickternal Friendshine of the Spotless Mort, the TV-Y7 rating was used instead with a different font of its standard TV-14 rating. Additionally, an E/I bug temporarily appeared in the bottom right corner of the screen. Notably, this E/I bug was used back in the late 1990s to identify an E/I compliant program on Turner's channels, which in Cartoon Network's case, was used during Big Bag and Small World.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 12, 2021&lt;/cell&gt;
        &lt;cell&gt;The Yashahime: Princess Half-Demon season 1 finale, Sesshomaru's Daughter, was supposed to air at 1:30am, but was accidentally replaced at the last minute with a rerun of Secret of the Rainbow Pearls.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 27, 2021 - February 2, 2022&lt;/cell&gt;
        &lt;cell&gt;The "Mashup" split screen credits was used during the 8:00pm hour instead of Adult Swim's standard split screen credits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 20, 2022&lt;/cell&gt;
        &lt;cell&gt;The second episode of One Piece that night had a TV-Y7-FV rating bug instead of its regular TV-PG bug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 26, 2022&lt;/cell&gt;
        &lt;cell&gt;During the first half of the Naruto: Shippuden episode One Worthy as a Leader, the TV-PG rating bug remained on screen the entire time until it cut for commercial break. Once the episode resumed after the break, the smaller version of the rating bug disappeared as intended.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 25, 2022&lt;/cell&gt;
        &lt;cell&gt;The screenbug disappeared during the Adult Swim Yule Log.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 4, 2023 - July 2, 2024&lt;/cell&gt;
        &lt;cell&gt;Every scheduling of the Robot Chicken episode "Crushed by a Steamroller on My 53rd Birthday" past April 4, 2023 (except for November 7, 2023) has the Aqua Child Hunger Force version of "Revenge of the Mooninites" air in its place instead. This was corrected with its scheduled May 19, 2025 airing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 4, 2023 - July 23, 2023&lt;/cell&gt;
        &lt;cell&gt;During the midnight airing of the Unicorn: Warriors Eternal episode The Mystery of Secrets on June 4, the Adult Swim: Stream On Max bug was visible for the first act, but the regular bug returned during the second act. This error also occurred in the 11:30pm airing of the American Dad! episode Camp Refoogee on June 10. In the Unicorn: Warriors Eternal episode The Heart of Kings that aired immediately after, the regular screenbug was used in the first act, then the Max bug in the second, then the regular in the third. This continued into the American Dad! episodes Saturday nights at 11:30pm and Unicorn: Warriors Eternal and My Adventures with Superman episodes Sundays at midnight.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 16, 2023&lt;/cell&gt;
        &lt;cell&gt;The Dr. Stone episode Treasure Box was supposed to air, but was accidentally skipped and Ray of Despair, Ray of Hope aired instead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 11, 2023&lt;/cell&gt;
        &lt;cell&gt;The Aqua Teen Hunger Force episode Space Conflict from Beyond Pluto aired twice in a row, replacing an airing of the episode Balloonstein at 2:00am.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 25, 2023 - June 27, 2025&lt;/cell&gt;
        &lt;cell&gt;Starting September 25, 2023, the Checkered Past bumper would air before the Adult Swim advisory sign-on. It has finally stopped on June 27, 2025.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 26, 2023 - present&lt;/cell&gt;
        &lt;cell&gt;At numerous times, the Adult Swim screenbug completely vanishes during programming. The West feed is sometimes unaffected, but the error currently happens often on the East feed.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 30, 2023 - present&lt;/cell&gt;
        &lt;cell&gt;Two Adult Swim advisory bumpers were played when Adult Swim signed on at 5:00pm. This has happened every Saturday since September 30th, as well as every day since June 30, 2025, and is still ongoing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 1, 2023&lt;/cell&gt;
        &lt;cell&gt;From 12am to 6am the [adult swim] screenbug did not appear.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 2, October 5 and October 9, 2023&lt;/cell&gt;
        &lt;cell&gt;Various scheduled programs were replaced with unscheduled programs without any notice. The first instance occurred on October 2nd with The Grim Adventures of Billy &amp;amp; Mandy being substituted with a then-unaired episode of Grim &amp;amp; Evil by mistake. The second and third instances on October 5th and October 9th saw regularly scheduled reruns of American Dad! being replaced with the HLN series Very Scary People and Forensic Files II respectively.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 7, 2023&lt;/cell&gt;
        &lt;cell&gt;Right before the 9:00pm show of the American Dad! episode American Dream Factory, the Toonami disclaimer intro plays.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 8, 2023 - October 9, 2023&lt;/cell&gt;
        &lt;cell&gt;From 5pm on the 8th to 6am on the 9th the [adult swim] screenbug did not appear.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 9, 2023&lt;/cell&gt;
        &lt;cell&gt;The 5:30pm airing of the Ed, Edd n Eddy episode(s) An Ed in the Bush / See No Ed accidentally aired in 16:9.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 25, 2023&lt;/cell&gt;
        &lt;cell&gt;A Toonami content warning played before the Bob's Burgers episode Tina Tailor Soldier Spy at 7:00pm.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 30, 2023&lt;/cell&gt;
        &lt;cell&gt;The 5:30pm airing of the Ed, Edd n Eddy episode(s) Rock-a-Bye Ed / O-Ed Eleven accidentally aired in 16:9. The [adult swim] screenbug also disappeared during the last few minutes of the second act.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 1, 2023&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 4, 2023&lt;/cell&gt;
        &lt;cell&gt;The end credits for the Dexter's Laboratory episode(s) That Magic Moment / A Silent Cartoon / Opposites Attract that aired at 5:00pm accidentally cut to the Cartoon Network Studios card early.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 24, 2023&lt;/cell&gt;
        &lt;cell&gt;The 8:30pm airing of the King of the Hill episode Ms. Wakefield started roughly a minute early, due to the commercial break beginning at 8:25 instead of 8:27.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 12, 2024&lt;/cell&gt;
        &lt;cell&gt;Multiple Checkered Past bumpers aired during the King of the Hill episodes Tears of an Inflatable Clown and The Minh Who Knew Too Much that aired at 7:00pm and 7:30pm respectively, even though the block already ended before then.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 14 - January 20, 2024&lt;/cell&gt;
        &lt;cell&gt;At the end of Demon Slayer despite the episode "Infiltrating the Entertainment District" being the next episode a promo for the episode "Defeating an Upper Rank Demon" aired instead, said episode would not premiere until March 10th, the incorrect promo would air throughout the week.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 27, 2024&lt;/cell&gt;
        &lt;cell&gt;The screenbug was not visible during the 5:00pm airing of King of the Hill episode Uh-Oh Canada before it returned a few minutes into the 5:30pm airing of To Sirloin with Love.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 18, 2024&lt;/cell&gt;
        &lt;cell&gt;The screenbug did not appear during The Princess Bride and two episodes of Bob's Burgers. The screenbug finally returned during the second act of the Bob's Burgers episode Into the Mild after 3 hours of no screenbug.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 17, 2024&lt;/cell&gt;
        &lt;cell&gt;The up next bumper for Lycoris Recoil was accidentally played a second time instead of the intro bumper for the show.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 31, 2024&lt;/cell&gt;
        &lt;cell&gt;The screenbug did not appear for the first 45 minutes of Young Frankenstein (1974).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 1, 2024 - present&lt;/cell&gt;
        &lt;cell&gt;Airings of the American Dad episode Brains, Brains and Automobiles were stretched and zoomed out.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 7, 2024&lt;/cell&gt;
        &lt;cell&gt;During the 1:30am slot a promo for the Zom 100 episode Bucket List of the Dead had aired advertising the episode as premiering "tonight at 12:30", even though the episode had premiered an hour prior.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 5, 2024&lt;/cell&gt;
        &lt;cell&gt;After Dragon Ball Z Kai ended the live feed went to a black screen for about a minute.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;May 15, 2024&lt;/cell&gt;
        &lt;cell&gt;The [adult swim] screen bug disappeared halfway through the Billy &amp;amp; Mandy segments Attack of the Clowns and Complete and Utter Chaos!.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 16, 2024&lt;/cell&gt;
        &lt;cell&gt;A "we'll be right back" bumper for Dragon Ball Z Kai aired instead of the Up Next bumper for Zom 100: Bucket List of the Dead.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 27, 2024 - July 28, 2024&lt;/cell&gt;
        &lt;cell&gt;The ratings bugs did not appear during the midnight airings of Smiling Friends and the first three hours of the My Adventures with Superman marathon.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 8, 2024&lt;/cell&gt;
        &lt;cell&gt;During the 1:00am premiere of the One Piece episode With a Rumbling of the Ground! The God of Destruction - Giant Pica Descends!, the commercial break began in the middle of a character's sentence (Kyros saying "I'm going to..."). When the show returned, the next scene had begun. The Video on Demand print had the full scene play out ("I'm going to take back our Dressrosa, no matter the cost!").&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 5, 2024&lt;/cell&gt;
        &lt;cell&gt;The airing of the American Dad! episode Don't Look a Smith Horse in the Mouth was zoomed out.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 6, 2024&lt;/cell&gt;
        &lt;cell&gt;During the commercial break of today's episode of Demon Slayer, an ad for the show coming up next played, followed by an ad saying the show had returned from break, but another ad played before the show actually returned, with no bumper. This may have something to do with Uzumaki running for 45 minutes and the rest of the block running off the clock.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;October 22, 2024&lt;/cell&gt;
        &lt;cell&gt;Warner's networks suffered from a "Video unavailable" screen along with stuttering video, also affecting the NBA season opener on TNT. On Adult Swim, it aired during tonight's Rick and Morty episodes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;November 3, 2024&lt;/cell&gt;
        &lt;cell&gt;For the 2:00am and 2:30am airings of Invincible Fight Girl the same bumpers and promos used during the midnight hour were used, most which advertised the return of Blue Exorcist which had returned hours prior. Additionally a next bumper for Blue Exorcist played even though Rick and Morty: The Anime was next.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 6, 2024&lt;/cell&gt;
        &lt;cell&gt;A "Later" bumper aired instead of the "Disclaimer" bumper before the airing of the Dragon Ball Z Kai episode Surprise! Goku Is Ginyu and Ginyu Is Goku?.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 18, 2024, May 23, 2025&lt;/cell&gt;
        &lt;cell&gt;The 5:30pm airing of the Ed, Edd n Eddy episode Tight End Ed / Tween a Rock and an Ed Place accidentally aired in widescreen.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 21, 2024&lt;/cell&gt;
        &lt;cell&gt;After the 10:30pm airing of American Dad!, a bumper for We Baby Bears on Cartoon Network accidentally played before the commercial break.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 3, 2025&lt;/cell&gt;
        &lt;cell&gt;The screenbug did not appear during the Family Guy episode Quagmire and Meg.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 26, 2025&lt;/cell&gt;
        &lt;cell&gt;During the premiere of the MASHLE: MAGIC AND MUSCLES episode Mash Burnedead and the Accelerated Battle at midnight, an ad for Barney's World on Cartoonito aired instead of the intended review for the Indiana Jones and the Great Circle video game. The review aired next week, during the MASHLE episode Mash Burnedead and the Divine Visionary.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 2, 2025&lt;/cell&gt;
        &lt;cell&gt;Following the premiere of the MASHLE: MAGIC AND MUSCLES episode Mash Burnedead and the Divine Visionary, a preview for next week's episode Mash Burnedead and the Survival of the Fittest was supposed to air. Instead, the preview for the episode that just aired was shown, complete with "Tonight at midnight" at the end when it was already approaching 12:30am.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 8, 2025&lt;/cell&gt;
        &lt;cell&gt;After the 4:00am airing of The Jellies! episode Mervin for Mayor An ad advertising a new episode of Common Side Effects played saying it was "coming up next", even though Loiter Squad was next&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 9, 2025&lt;/cell&gt;
        &lt;cell&gt;The airing of the Naruto episode You Failed! Kakashi's Final Decision used the old edited version from Cartoon Network, complete with "Rise, Power!" as the opening and ending (both of which were visually compressed) and the pillarbox bars were pitch black instead of using the "space" versions that 4:3 Toonami programming typically uses.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 25, 2025&lt;/cell&gt;
        &lt;cell&gt;The 2:00am airing of the Family Guy episode Barely Legal used the SD, but uncut, master tape seen on Adult Swim during the channel's original run of the series - as opposed to the HD, but edited, print given to Adult Swim when the network got the show back and aired there the previous night.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 16, 2025&lt;/cell&gt;
        &lt;cell&gt;During the 3:30am airing of The Venture Bros. episode Home Is Where the Hate Is part of a line spoken by Brock (more specifically, "I'm wa-"; the audio got stuck on the last syllable mentioned) repeated for the last seven minutes of the episode. This issue was fixed on the West feed airing of the same episode.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 29, 2025&lt;/cell&gt;
        &lt;cell&gt;During the 12:32am premiere of the Dragon Ball DAIMA episode Daima, the Closed Captions appeared to be baked into the master of the episode. They would show up on screen even if the Closed Caption function was not toggled on. This remained in the Video On Demand print.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;July 25, 2025, October 3, 2025, October 7, 2025&lt;/cell&gt;
        &lt;cell&gt;Reruns of the Rick and Morty episode Rattlestar Ricklactica have been inexplicably been replaced with Promortyus last minute.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 27, 2025&lt;/cell&gt;
        &lt;cell&gt;The screenbug did not appear during the first act of the Family Guy episode Padre de Familia.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;August 28, 2025&lt;/cell&gt;
        &lt;cell&gt;The 12:30am airing of the Aqua Teen Hunger Force episode She Creature was both letterboxed and pillarboxed.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;December 21, 2025&lt;/cell&gt;
        &lt;cell&gt;After the 2:30am airing of the Dragon Ball DAIMA episode Collar, the intro bumper for Blue Exorcist aired instead of an Up Next bumper for Naruto.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 5, 2021&lt;/cell&gt;
        &lt;cell&gt;The Cartoonito "NEW" screenbug wasn't present during the premiere of the Love Monster episode Elder Kitten Day.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;December 20, 2021&lt;/cell&gt;
        &lt;cell&gt;A production card could be seen before the Pocoyo short Finger Family during that day's block of Pocoyo episodes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 28, 2022 - September 22, 2023&lt;/cell&gt;
        &lt;cell&gt;The voiceover promo at the end of the Cartoonito block tells the viewer to tune in tomorrow for more Cartoonito, despite the block being removed from weekends at this point. This was never fixed. The Cartoonito screenbug also appears of a split second.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;February 24, 2022&lt;/cell&gt;
        &lt;cell&gt;The 7:50am airing of Love Monster was replaced with a repeat of the 7:30am airing of Pocoyo, which cuts off during Pocoyo's Band. To make up for the lost time, Bing replaced that day's 8:30am airing of Lucas the Spider.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;June 6, 2022&lt;/cell&gt;
        &lt;cell&gt;When the Up Next banner appeared during the Cocomelon short Winter Song (Fun in the Snow) in its latest slot, the entire screen, banner aside, went pitch black for 17 seconds. Before the premiere of the Lellobee City Farm episode Wheels on the Bus / Five Little Ducks - Playtime at the Farm! / Wash Your Hands, Scrub That Soap / ABC Song Learn Phonics, before cutting to this song.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;January 2, 2023&lt;/cell&gt;
        &lt;cell&gt;During a repeat of the Thomas &amp;amp; Friends: All Engines Go episode Snowplow Struttin' at 9:45am and the Bugs Bunny Builders episodes Tweety-Go-Round and Looney Science respectively at 10:30am and 10:45am, the NEW screenbug was still used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;March 10, 2023&lt;/cell&gt;
        &lt;cell&gt;The airings of the Mecha Builders episodes The Need For Feed / Zipline Sisters Of Treetop Woods, Thomas &amp;amp; Friends: All Engines Go episodes The Super-Long Shortcut, More Cowbell, A Light Delivery and Sir Topham Hatt's Hat, and Bugs Bunny Builders episodes Play Day and Splash Zone from 9am-11am used the regular Cartoon Network screenbug in lieu of that of Cartoonito. This was more than likely done in preparation for Cartoonito being reduced to an hour and a half that Monday.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 3, 2023&lt;/cell&gt;
        &lt;cell&gt;A promo for today's airing of Thomas &amp;amp; Friends: The Mystery of Lookout Mountain suggested that it would air at noon instead of 7:30am.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;September 25 - 26, 2023&lt;/cell&gt;
        &lt;cell&gt;Typically, airings of Cocomelon on Cartoonito move the screenbug to the top right corner, presumably to avoid obscuring the song lyrics at the bottom of the screen. However, the airings at 7:30am, 8:00am, and 8:30am moved the bug back to the bottom.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Various dates from October 6, 2023 to January 20, 2025&lt;/cell&gt;
        &lt;cell&gt;In an apparent error, the Cartoon Network screenbug was used during Cartoonito's regularly scheduled timeslots.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Every Friday from April 18, 2025 to May 9, 2025&lt;/cell&gt;
        &lt;cell&gt;The Cartoonito "NEW" screenbug wasn't present during the premiere of Barney's World episodes "Every Little Thing", "Finders Not Keepers", "It Takes Three", and "Too Far".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;April 28, 2025 - May 2, 2025&lt;/cell&gt;
        &lt;cell&gt;The Cartoonito "NEW" screenbug wasn't present during the premiere of Batwheels episodes "Batwing and the Nets", "Clay Date", "Wish Upon a Car", "Bibi's Bat-Belt Blunder" and "Lend Me Your Volunteer".&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;May 22, 2025&lt;/cell&gt;
        &lt;cell&gt;During the airing of the Silly Sundays episode, "Kitty Adventure", the Cartoonito screenbug as well as the TV-Y rating bug did not appear during the episode.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352405</guid><pubDate>Mon, 22 Dec 2025 09:01:11 +0000</pubDate></item><item><title>BMW Patents Proprietary Screws That Only Dealerships Can Remove</title><link>https://carbuzz.com/bmw-roundel-logo-screw-patent/</link><description>&lt;doc fingerprint="bb90485e6df30eb7"&gt;
  &lt;main&gt;
    &lt;p&gt;German automakers have a decades-long reputation for forcing people to buy specialty tools. BMW has managed to take that specialty tool requirement and crank it up to a level we've never seen before. Its latest patent uncovered by CarBuzz is for a new screw head that will have mechanics seeing red. Or maybe that's seeing blue and white? Either way, it's a screw with a head that is the BMW roundel logo, and it can't be removed with normal tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;Europe Loves Specialized Fasteners&lt;/head&gt;
    &lt;p&gt;If you've worked on anything from the VW Group, Mercedes-Benz, or BMW, you know you're going to find fasteners you won't find anywhere else. Triple-squares, massive Torx and hex bits, even the infuriating E-Torx, just for a start. That's before you get into the giant wrenches or sockets you need just to take off the oil filter housing.&lt;/p&gt;
    &lt;p&gt;This one, though, has the mechanics at CarBuzz buzzing with rage. Because if BMW decides to use them, it will be a long time before you can pick up the right set of tools at Harbor Freight, or anywhere other than a BMW authorized retailer.&lt;/p&gt;
    &lt;p&gt;"The invention relates to a screw," the patent starts. We assume that refers to screwing over a mechanic. Okay, okay. We'll get to the point.&lt;/p&gt;
    &lt;p&gt;It's the head of the screw, more precisely a form of socket head screw. Instead of the familiar slot of a flat head, cross of a Phillips, or even the Robertson square or a hex, it's BMW's historic roundel. Two of the four quadrants are recessed, where the screwdriver bit goes, and two are flush. For some extra flair, the BMW logo is embossed in the edge of the screw head. Just to remind you who has created this abomination.&lt;/p&gt;
    &lt;head rend="h2"&gt;This Is About Keeping You Out&lt;/head&gt;
    &lt;p&gt;This isn't about adding style, or at least not completely. BMW really does want to use these to stop people from working on their own cars. It spells out in the patent that "the shape of the engagement recesses prevents the screw from being loosened or tightened using common counter-drive structures, e.g. by unauthorized persons." That's straight from BMW.&lt;/p&gt;
    &lt;p&gt;The idea is to use it for places where fasteners are normally visible. Seat mounting places, for example. Or other parts like connecting "the cockpit to the load-bearing body structure."&lt;/p&gt;
    &lt;p&gt;Using it for those applications will make it even tougher for DIYers. These are locations where large fasteners and moderately high levels of torque are used. With a two-prong driver and the trim ring taking up much of the screw's surface, breaking drivers could be common.&lt;/p&gt;
    &lt;p&gt;The socket head isn't the only one, either. BMW includes versions with flat heads but normal cone-shaped shanks and even a round head screw with a flat shank. That means more places BMW could use these fasteners... and more tools you're going to need.&lt;/p&gt;
    &lt;head rend="h5"&gt;Brembo May Have Finally Solved The Brake Dust Problem&lt;/head&gt;
    &lt;p&gt;In addition, the new system could last up to 80% longer.&lt;/p&gt;
    &lt;p&gt;We think its proposed new bolts are nuts. All we can do is hope that these stay on the patent side and don't make the jump to production. Ever. Otherwise, owners could end up spending a lot more time at the dealer or a lot more time looking for a good independent shop willing to touch it.&lt;/p&gt;
    &lt;p&gt;Patent filings do not guarantee the use of such technology in future vehicles and are often used exclusively as a means of protecting intellectual property. Such a filing cannot be construed as confirmation of production intent.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Source: EUIPO&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352459</guid><pubDate>Mon, 22 Dec 2025 09:13:57 +0000</pubDate></item><item><title>The ancient monuments saluting the winter solstice</title><link>https://www.bbc.com/culture/article/20251219-the-ancient-monuments-saluting-the-winter-solstice</link><description>&lt;doc fingerprint="bd08521d25e3a112"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'It's a moment of death and rebirth': The ancient monuments saluting the winter solstice&lt;/head&gt;
    &lt;p&gt;Dozens of mysterious structures across the Northern Hemisphere – some nearly 5,000 years old – align precisely to frame the rising and setting Sun during midwinter's shortest day. What motivated people to construct these solar-calibrated masterpieces?&lt;/p&gt;
    &lt;p&gt;The winter solstice, which usually falls on 21 or 22 December in the Northern Hemisphere each year, marks the moment that one yearly cycle comes to an end and another is born. It is the day with the smallest number of sunlight hours in the calendar, and once it's over, the days lengthen again incrementally until the summer solstice in June.&lt;/p&gt;
    &lt;p&gt;The significance of this day is manifested in ancient monuments that were designed to acknowledge and celebrate its passing. One example is Maeshowe tomb in Orkney. To the untrained eye this burial cairn, created around 2800BC, looks like a grassy hillock – but it conceals a cuboid, stone-clad sepulchre and a 33ft (10m) long entry corridor oriented to the south-west. During midwinter, three weeks either side of the winter solstice, the setting Sun aims directly down the corridor and emanates its light into the tomb.&lt;/p&gt;
    &lt;p&gt;When the sky is cloudless, the light seems to carve a golden aperture into the tomb's rear wall – a sacrament of pure light. These days of radiance are interrupted by the solstice itself, when blackness temporarily takes over. But daylight reappears soon after, to blaze for another few days as if in celebration of the promise of nature's rejuvenation in spring.&lt;/p&gt;
    &lt;p&gt;We will probably never know the specific beliefs and rituals that inspired Maeshowe tomb. But it's nonetheless possible to understand the enormous significance of the winter solstice as the "year's midnight", both as the darkest moment in the calendar and the pivot to six future months of greater illumination. It was a moment of death and rebirth, and a reminder of the cyclical nature of time.&lt;/p&gt;
    &lt;p&gt;In the deep past, understanding the markers of nature's clockwork – including solstices – was a matter of survival. Predicting the recurrent patterns of animal migration, for example, could help successful hunting and fishing. Knowing when the climate was likely to change meant being able to adapt and survive. In pre-agricultural societies, it helped people anticipate the availability and location of edible roots, nuts and plants.&lt;/p&gt;
    &lt;p&gt;After the introduction of farming, around 9000BC, it was essential – for successful planting and harvesting – to anticipate the timing of seasonal changes. Monuments that calculated time had practical value, but it's likely that they also embodied spiritual beliefs in Neolithic times too, with the winter solstice being of particular importance. This very ancient recognition of the solstice's significance even echoes through to the modern world. The word "Yule", now associated with the winter holiday period, derives from the historic Norse festival of Jól, which was based around the winter solstice. Modern Christmas traditions recall bygone midwinter celebrations like the Roman holiday of Saturnalia, which involved feasting and gift-giving. And the solstice continues to be acknowledged in hundreds of traditions across the world, such as the Inca celebration of Inti Raymi, and the Dōngzhì festival in China.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Nature's sublime power'&lt;/head&gt;
    &lt;p&gt;Alongside Maeshowe tomb, archaeologists have discovered dozens of Neolithic monuments that stare directly at the Sun on the winter solstice. There's Stonehenge (England), whose tallest trilithon once framed the setting sun; Newgrange (Ireland), which has a passageway aligned to sunrise on this auspicious day; and the standing stones at Callanish (Outer Hebrides) which create similar solar sightlines. In Brittany, north-western France, is La Roche aux Fées: a megalithic passageway constructed from 41 blocks of stone, some of which weigh over 40 tonnes (40,000kg). At sunrise on the winter solstice, it breathes in its annual dose of restorative midwinter light. Legends once told that fairies constructed it over the course of one night, but it is actually a dolmen (tomb) created by Neolithic architects around 2750BC.&lt;/p&gt;
    &lt;p&gt;In the 20th and 21st Centuries there has been a resurgence of Neolithic-inspired solar-oriented artworks. Nancy Holt's seminal land art piece, Sun Tunnels (1973-76) is one example, set in the Great Basin Desert of Utah, and comprising of four 22-tonne (22,000kg) concrete tubes arranged in an X-shape formation. The view down each of them perfectly frames the Sun as it rises and sets on the winter and summer solstices. Holt bought the land in 1975 and created her artwork with the help of engineers, an astrophysicist, an astronomer and a team of contractors.&lt;/p&gt;
    &lt;p&gt;It's best to understand Sun Tunnels in the context of the Land Art movement of the 1960s and 70s. Artists like Holt who are associated with this movement worked with the landscape rather than within traditional studios and galleries, and aimed to reconnect people with the awe of nature. Unlike its Neolithic predecessors, Sun Tunnels has no religious significance – Holt explained that she simply wanted "to bring the vast space of the desert back to human scale". It is also a response to modern concerns about nature. In an age where humans seem hell-bent on despoiling and exploiting nature, Sun Tunnels turns our attention back to its sublime power and rhythmic patterns.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• Seven of the greatest rivalries in art history&lt;/p&gt;
    &lt;p&gt;Another masterpiece of Land Art, James Turrell's Roden Crater (begun 1979), does this on an even more epic scale than Sun Tunnels. It occupies a volcanic cinder cone in the Painted Desert region of northern Arizona and houses multiple spaces from which to watch celestial phenomena. One of them is a 900ft- (274m) long tunnel drilled through the volcanic cone. It acts like a camera obscura, focusing an image of the midwinter sun (via a glass lens halfway down the passage) on to a slab of white marble in a central chamber. Like Maeshowe tomb's passage, it aligns with the Sun's position around 21 December each year, and drinks down the Sun's light from 10 days before the solstice to the 10th day after it.&lt;/p&gt;
    &lt;p&gt;Enoura Observatory in Kanagawa Prefecture, Japan (completed 2017) was designed by photographer and architect Hiroshi Sugimoto. Its various buildings are all calibrated towards the movement of the Sun, to create what the artist describes as a "new Neolithic aesthetic". He wanted to correct what he saw as a lack of purpose in contemporary art by exploring the primal concerns of our ancient ancestors – our status within the infinite wilderness of the cosmos, our sense of time, and our notion of a human identity within the natural order.&lt;/p&gt;
    &lt;p&gt;One of its structures, the "Winter Solstice Light-Worship Tunnel", points directly at the spot on the horizon where the Sun rises at about 06:48 local time on 21 December each year. The solstice sunlight floods this 230ft-(70m) long chamber made of Corten steel and illuminates a stone medieval wellhead that is situated halfway along its length. It passes underneath another structure which aligns with the Sun on the summer solstice. The entire site, which took a decade to build, was intended by Sugimoto to act like a living clock, and to make an artwork with the ancient function of helping humans "identify their place within the vastness of the universe".&lt;/p&gt;
    &lt;p&gt;Holt's, Turrell's and Sugimoto's structures put us back in contact with seasonal patterns and the rhythms of nature, just as Maeshowe tomb and La Roche aux Fées once did. These monuments and artworks orient us in time, to the landscape, to our place within nature and to reoccurring celestial events. The winter solstice – which they all respond to directly – has always been of critical importance to humans, enshrining the significance of light, and honouring death and rebirth in the annual calendar. If the spectacle of these solar-aligned structures lining up perfectly with the rising and setting solstice Sun quickens the soul, it's because it triggers a primal recognition that the darkest hours of the year have passed. It's the first sign of spring's promised return, and future days of increased lightness and warmth.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for the Essential List newsletter – a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352565</guid><pubDate>Mon, 22 Dec 2025 09:30:58 +0000</pubDate></item><item><title>Well being in times of algorithms</title><link>https://www.ssp.sh/blog/well-being-algorithms/</link><description>&lt;doc fingerprint="3410d04703f78891"&gt;
  &lt;main&gt;
    &lt;p&gt;To be able to live well, to avoid anxiety and depression, it’s not that easy anymore in 2025. Everything is designed to grab our attention, and our attention span is declining and disallowing people to think and learn. Algorithms know more about us than our spouses while grabbing our attention constantly.&lt;/p&gt;
    &lt;p&gt;This is bad for our health. Bad for our spirit. Bad for our well-being. We need a strong foundation that keeps us grounded, not bringing us on the wrong track. The internet, a connected web of websites as it was originally designed, is no more.&lt;/p&gt;
    &lt;p&gt;Today everyone uses a handful of websites, a couple of algorithm-dominated pages that are hitting our brain with dopamine as much as possible. So how can we stay sane in a large tech-dominated world? How do we get quality time when everyone is distracted, fighting for the attention of our friends and families against their phones? That’s why we most often default to just using our phone.&lt;/p&gt;
    &lt;p&gt;I’m currently staying in the Philippines, and the algorithm-dominated web and big tech dependency is even more apparent. People have to use their products such as Facebook Messenger, which is the default for communication and calling. Businesses make sales on Facebook pages, or sell and buy products there.&lt;/p&gt;
    &lt;p&gt;This essay investigates the ground pillars for a well-lived life, and how the open web can help us with that.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Groundwork for Life and Well Being&lt;/head&gt;
    &lt;p&gt;We as individuals have lots of distractions besides the obvious. We are often greedy, egoistic, longing for a lot of money, a big career. And then we try to grind, we try to get a lot of followers on social media, trying to fulfill a Vanity Metric, we try to get more money, a bigger career, a bigger house.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Pillars of Life: Health, Family and Self-Contentment&lt;/head&gt;
    &lt;p&gt;Health, a non-greedy life, knowing when we have enough, and family are the three pillars. We need to recognize them. We cannot do it ourselves.&lt;/p&gt;
    &lt;p&gt;A non-greedy life can be anything like being spiritual, doing ice baths, praying, or doing anything slowly, going inwards. Anything that takes us out of our egocentric way.&lt;/p&gt;
    &lt;p&gt;Health is a prerequisite. As long as we are healthy, we don’t think too much about it. Only when we get sick, we learn that everything is dependent on it. It can help us to get out of an egocentric life too. If we are sick, we need to accept help from friends and loved ones. We learn that we need family to help us in difficult times.&lt;/p&gt;
    &lt;p&gt;Some don’t grow up with a family, so this is a hard one to fill. But family can be your cousins, your stepdad, friends, or anyone who truly cares for you. But without family, or that surrounding bond, it’s easier to fall into the wrong path and get bad habits of drinking, drugs, or similar. If we don’t have it, a helpful foundation, one that has held our society for the past centuries, is the church.&lt;/p&gt;
    &lt;p&gt;Everyone is welcome, no stereotypes or prejudice. You just come and receive love and friendship. A community to participate in, to receive (first), and submit to and give the longer you are part of it.&lt;/p&gt;
    &lt;head rend="h4"&gt;Purpose of Life&lt;/head&gt;
    &lt;p&gt;I wrote a long article Â«Why are we here on EarthÂ», where I elaborate on my journey of purpose, life, and some of the foundations. As I discuss there as well, principles is another way to navigate life and have a healthy lifestyle once you find your true principles to live for. It’s like a compass guiding your life. With them set in place, you will have an easier time deciding and when to say no.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bad Mood: Not Being Present&lt;/head&gt;
    &lt;p&gt;Getting out of ourselves, being asleep as Anthony de Mello writes, most of us are asleep our whole lives. He says that the key to waking up and being aware is learning something new, being open, and most importantly, being present.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Evil: A Downward Spiral of Negativity&lt;/head&gt;
    &lt;p&gt;If the foundation pillars are not met, we will get into bad moods at first. If it happens over a longer period, we might get chronically bad moods. This reflects on other people, who are less likely to spend time with us. We isolate ourselves. If there’s a death or life-changing incident, it’s easy to fall into a deep hole, an evil downward spiral of negativity, potentially into Nihilism.&lt;/p&gt;
    &lt;p&gt;Money and financial stability is another pillar that might look or be considered a main pillar, as without it, it’s hard to be happy. But to be honest, if we have a lot of money, we use it. If we have little, we use it too. So with the above-mentioned pillars, I still think these are more important and independent of money, though if we have no money at all, all these we discuss are irrelevant.&lt;/p&gt;
    &lt;p&gt;In a way, money is just a distraction from living a meaningful life. It certainly helps, but a life centered around it will be the start of a toxic life in most cases.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Status Game: Greed&lt;/head&gt;
    &lt;p&gt;Another way of a downward spiral that happens usually in more well-developed countries, where our main pillars are met, but we are just greedy and want more. Better career, more money, anything to get a better status.&lt;/p&gt;
    &lt;head rend="h3"&gt;What We’re After: Quality Time&lt;/head&gt;
    &lt;p&gt;The ultimate goal for me is to get quality time with loved ones, people that mean something to us. Giving us happiness as backed by a long-term study from Harvard. It’s a deep happiness compared to just a Shallow Happiness followed to get the outside reputation, signaling to the outside that we are happy. However, deep inside, we might not be.&lt;/p&gt;
    &lt;p&gt;Not being present is the opposite of quality time. It’s what we lack when we use the phone and social media too much. We get addicted to social media, we break relationships and lose self-confidence, killing the very foundation of well-being and time well spent with our loved ones.&lt;/p&gt;
    &lt;p&gt;A healthy, deep, meaningful, happy life might actually look boring from the outside. This Boring Life can be an enabler to more happiness I believe, as I write and think more about it.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Do We Get Out of It?&lt;/head&gt;
    &lt;p&gt;Soon we approach the new year of 2026. So, how do we get out of this distracted, algorithm-dominated world? To a place where everybody can share and be in control, not big tech?&lt;/p&gt;
    &lt;p&gt;I feel that people are fed up with the internet for the first time since its creation. With all the AI-generated content, generating several complete books in a day (the Amazon Kindle limit is 10 per week which is a joke and offensive for any author). At least I am.&lt;/p&gt;
    &lt;p&gt;It’s getting harder and harder to tell what is written by a person and what is generated. Sometimes I’d rather see the prompt, as this is where the genuineness, the soul and curiosity of the person is. I wrote more on Finding Flow and Escaping Digital Distractions Through Deep Work and Slow Living, but flow is an essential piece of happiness and purpose of us doing something.&lt;/p&gt;
    &lt;p&gt;With AI, people feel they don’t need to learn programming, writing, or any craft. But I think that’s the wrong conclusion, especially on a personal level. As AI won’t replace human thinking, we will lose the muscle of thinking like we lost the ability to do simple math or remember a phone number as phones and calculators have replaced it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Algorithm-Free Internet: An Open Web&lt;/head&gt;
    &lt;p&gt;To get away from algorithms, away from being locked in and dependent on the platform, away from big tech chasing our attention, back to real connections as opposed to losing our followers with the Death of the Follower. We need open platforms such as Open Social Media and an open web, where the power isn’t in the platform, but in us as the producers. We need to get out of the algorithms, free from big tech, and back to real connections. But how?&lt;/p&gt;
    &lt;p&gt;A concept that Patreon’s creator Jack Conte explains well: we can’t even reach our own followers anymore on social media platforms, though they decided to follow us. See more on Death of the Follower and linked YouTube video by Jack.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Internet: How It Was Designed&lt;/head&gt;
    &lt;p&gt;Initially, for a long time, the internet was a nice place to be. You could search for information from AltaVista, Bluewin (in Switzerland) to find information about the Siberian tiger for making a presentation in school. Or you could share your thoughts, usually in forums or personal websites or blogs.&lt;/p&gt;
    &lt;p&gt;It was interconnected. Each website usually had backlinks to its favorite websites so that people could explore similar websites. It was a web of information, and the algorithm by the people’s recommendations on each website instead of a centralized company or algorithm.&lt;/p&gt;
    &lt;head rend="h3"&gt;Back to the Producer: Direct Connection&lt;/head&gt;
    &lt;p&gt;Instead of depending on the algorithm for sharing our content, we need to have direct connections and followers, like we did with the newsletter all along. That’s why Substack1 and newsletters are so popular, because we get direct connections with our readers.&lt;/p&gt;
    &lt;p&gt;But we don’t want only to rely on newsletters (emails). We also want a direct connection via links in the World Wide Web. Google is still the place for discovery in my opinion. Nobody goes to ChatGPT for a good essay or a deep moving piece. This is where Google or real blog posts, papers, and websites shine, though not all anymore, as there’s so much spam or even slop with AI.&lt;/p&gt;
    &lt;p&gt;Compared to algorithms, direct connections let us choose what we want to see. We have the choice, unlike with the Twitter, LinkedIn, and Facebook algorithms. No surprise people get mad when the algorithms change, as recently with LinkedIn or YouTube many times, making it impossible to reach your audience anymore.&lt;/p&gt;
    &lt;p&gt;LinkedIn people left for Substack, but it’s only a question of time until Substack changes its algorithm for Notes (Twitter copy) or does something wrong (there is already quite some controversy with people migrating away). The best is still to own your content, have your own website and domain, or use a totally new social media approach called open social media.&lt;/p&gt;
    &lt;head rend="h3"&gt;Open Web and an Open Social Media&lt;/head&gt;
    &lt;p&gt;One way is the open web that Dan Abramov is explaining so well in Open Social with the AT Protocol, an open protocol where people own their data (followers, shared data, etc.) and applications can be built on top of that. For example, there is a Twitter called Bluesky that is based on AT Protocol. There’s a HackerNews called Frontpage, Tangled, a git alternative or Instagram, TikTok and many more, all implemented with the same ID and AT Protocol.&lt;/p&gt;
    &lt;p&gt;You can even host the data on your own server with a so-called Personal Data Server (PDS) and be able to take them offline whenever you want. It’s still unclear if AT Protocol will be fully mainstream, but I’m very confident that people who are tired of being dependent can use this happily as I do daily. With each algorithm change, more will follow and there’s already a 41 million active users.&lt;/p&gt;
    &lt;p&gt;With the AI slop being promoted on the major social media platform’s algorithm, I believe we will go back to following real humans. Back to followers, where we decide who we want to see.&lt;/p&gt;
    &lt;p&gt;It takes time and hard work to move to another platform, but if you invest in something else, it should be open and owned by us. That’s why AT Protocol will only get more popular and stronger the longer it exists due to its open foundation.&lt;/p&gt;
    &lt;p&gt;Bluesky is maturing more and people can share their content (not penalized by sharing links) and even host their own content, integrating it into other applications such as discussed above. For example, I integrated comments on my second brain based on Bluesky. It’s like the early Twitter, open APIs, even the data is fully open and can be fetched by attaching to the Jetstream. With all these features and characteristics, it will be a very long-term thing. Even if you can’t go viral, which is by design, you can reach your followers. As always, it depends on what is most important to you.&lt;/p&gt;
    &lt;p&gt;And at some point, Bluesky will also implement an algorithm that can make your posts a little more viral. Today they have one called “Discover Feed”, which is not perfect yet, but will be with the hired data scientist to improve it. But again, it will never be the same as Twitter or others, as it’s especially made for people to follow people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Personal Blogs&lt;/head&gt;
    &lt;p&gt;I also believe in a comeback of personal blogs. Bringing back uniqueness and soul in times of unification of social media and newsletters. There are still many WordPress pages out there, which are more customized and unique than what we have and had with Medium, Substack and Open Subscription Platforms.&lt;/p&gt;
    &lt;p&gt;Patreon is another alternative, focusing on direct connections with your followers and audience instead of going viral. I believe this will be the future: real connections, real content, real stories. Unlike artificially generated content just for fame and vanity metrics.&lt;/p&gt;
    &lt;p&gt;I also hope to see more connected websites, sites like public second brains where visitors can explore content multi-dimensionally instead of single-level. In addition to a traditional blog, providing backlinks and a graph view, diving into the content and life of the creator, finding connections you might not otherwise.&lt;/p&gt;
    &lt;p&gt;And with personal blogs, there are no artificial algorithms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping Up&lt;/head&gt;
    &lt;p&gt;With the recent investments from governments in Denmark and others to ban social media for kids under fifteen and other measures to regulate social media, I think we are heading in the right direction. Big tech does not have the interest of the people. Instead of helping us, which they did in the beginning, they want to profit more off us and make us more addicted. And as with alcohol and other drugs, these need to be regulated, especially when there’s such a huge monopoly with just a few companies.&lt;/p&gt;
    &lt;p&gt;This will increase our well-being, and we will have more time to spend on the main pillars of life. We will be less anxious, less negative. We will be happier, more free.&lt;/p&gt;
    &lt;p&gt;This might not show an immediate effect, but as it’s compounding, it will in the long term of our life and career. Like the Pathless Path and its book by Paul Millerd suggest, following your Instinct more and being open, unexpected things will happen, things we can’t plan ahead of time. Like Steve Jobs already said before about connecting the dots:&lt;/p&gt;
    &lt;quote&gt;You can’t connect the dots looking forward; you can only connect them looking backward. So, you have to trust that the dots will somehow connect in your future.&lt;/quote&gt;
    &lt;p&gt;Trust in your principles, in the process. Spend more time offline, more time with loved ones, and then good things will happen. Time away from algorithms, and more time being well.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;and other Open Subscription Platforms ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352747</guid><pubDate>Mon, 22 Dec 2025 09:58:50 +0000</pubDate></item><item><title>A year of vibes</title><link>https://lucumr.pocoo.org/2025/12/22/a-year-of-vibes/</link><description>&lt;doc fingerprint="375c195f06b20cf9"&gt;
  &lt;main&gt;
    &lt;p&gt;written on December 22, 2025&lt;/p&gt;
    &lt;p&gt;2025 draws to a close and it’s been quite a year. Around this time last year, I wrote a post that reflected on my life. Had I written about programming, it might have aged badly, as 2025 has been a year like no other for my profession.&lt;/p&gt;
    &lt;p&gt;2025 was the year of changes. Not only did I leave Sentry and start my new company, it was also the year I stopped programming the way I did before. In June I finally felt confident enough to share that my way of working was different:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Where I used to spend most of my time in Cursor, I now mostly use Claude Code, almost entirely hands-off. […] If you would have told me even just six months ago that I’d prefer being an engineering lead to a virtual programmer intern over hitting the keys myself, I would not have believed it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While I set out last year wanting to write more, that desire had nothing to do with agentic coding. Yet I published 36 posts — almost 18% of all posts on this blog since 2007. I also had around a hundred conversations with programmers, founders, and others about AI because I was fired up with curiosity after falling into the agent rabbit hole.&lt;/p&gt;
    &lt;p&gt;2025 was also a not so great year for the world. To make my peace with it, I started a separate blog to separate out my thoughts from here.&lt;/p&gt;
    &lt;p&gt;It started with a growing obsession with Claude Code in April or May, resulting in months of building my own agents and using others’. Social media exploded with opinions on AI: some good, some bad.&lt;/p&gt;
    &lt;p&gt;Now I feel I have found a new stable status quo for how I reason about where we are and where we are going. I’m doubling down on code generation, file systems, programmatic tool invocation via an interpreter glue, and skill-based learning. Basically: what Claude Code innovated is still state of the art for me. That has worked very well over the last few months, and seeing foundation model providers double down on skills reinforces my belief in this approach.&lt;/p&gt;
    &lt;p&gt;I’m still perplexed by how TUIs made such a strong comeback. At the moment I’m using Amp, Claude Code, and Pi, all from the command line. Amp feels like the Apple or Porsche of agentic coding tools, Claude Code is the affordable Volkswagen, and Pi is the Hacker’s Open Source choice for me. They all feel like projects built by people who, like me, use them to an unhealthy degree to build their own products, but with different trade-offs.&lt;/p&gt;
    &lt;p&gt;I continue to be blown away by what LLMs paired with tool execution can do. At the beginning of the year I mostly used them for code generation, but now a big number of my agentic uses are day-to-day things. I’m sure we will see some exciting pushes towards consumer products in 2026. LLMs are now helping me with organizing my life, and I expect that to grow further.&lt;/p&gt;
    &lt;p&gt;Because LLMs now not only help me program, I’m starting to rethink my relationship to those machines. I increasingly find it harder not to create parasocial bonds with some of the tools I use. I find this odd and discomforting. Most agents we use today do not have much of a memory and have little personality but it’s easy to build yourself one that does. An LLM with memory is an experience that is hard to shake off.&lt;/p&gt;
    &lt;p&gt;It’s both fascinating and questionable. I have tried to train myself for two years, to think of these models as mere token tumblers, but that reductive view does not work for me any longer. These systems we now create have human tendencies, but elevating them to a human level would be a mistake. I increasingly take issue with calling these machines “agents,” yet I have no better word for it. I take issue with “agent” as a term because agency and responsibility should remain with humans. Whatever they are becoming, they can trigger emotional responses in us that can be detrimental if we are not careful. Our inability to properly name and place these creations in relation to us is a challenge I believe we need to solve.&lt;/p&gt;
    &lt;p&gt;Because of all this unintentional anthropomorphization, I’m really struggling at times to find the right words for how I’m working with these machines. I know that this is not just me; it’s others too. It creates even more discomfort when working with people who currently reject these systems outright. One of the most common comments I read in response to agentic coding tool articles is this rejection of giving the machine personality.&lt;/p&gt;
    &lt;p&gt;An unexpected aspect of using AI so much is that we talk far more about vibes than anything else. This way of working is less than a year old, yet it challenges half a century of software engineering experience. So there are many opinions, and it’s hard to say which will stand the test of time.&lt;/p&gt;
    &lt;p&gt;I found a lot of conventional wisdom I don’t agree with, but I have nothing to back up my opinions. How would I? I quite vocally shared my lack of success with MCP throughout the year, but I had little to back it up beyond “does not work for me.” Others swore by it. Similar with model selection. Peter, who got me hooked on Claude early in the year, moved to Codex and is happy with it. I don’t enjoy that experience nearly as much, though I started using it more. I have nothing beyond vibes to back up my preference for Claude.&lt;/p&gt;
    &lt;p&gt;It’s also important to know that some of the vibes come with intentional signalling. Plenty of people whose views you can find online have a financial interest in one product over another, for instance because they are investors in it or they are paid influencers. They might have become investors because they liked the product, but it’s also possible that their views are affected and shaped by that relationship.&lt;/p&gt;
    &lt;p&gt;Pick up a library from any AI company today and you’ll notice they’re built with Stainless or Fern. The docs use Mintlify, the site’s authentication system might be Clerk. Companies now sell services you would have built yourself previously. This increase in outsourcing of core services to companies specializing in it meant that the bar for some aspects of the user experience has risen.&lt;/p&gt;
    &lt;p&gt;But with our newfound power from agentic coding tools, you can build much of this yourself. I had Claude build me an SDK generator for Python and TypeScript — partly out of curiosity, partly because it felt easy enough. As you might know, I’m a proponent of simple code and building it yourself. This makes me somewhat optimistic that AI has the potential to encourage building on fewer dependencies. At the same time, it’s not clear to me that we’re moving that way given the current trends of outsourcing everything.&lt;/p&gt;
    &lt;p&gt;This brings me not to predictions but to wishes for where we could put our energy next. I don’t really know what I’m looking for here, but I want to point at my pain points and give some context and food for thought.&lt;/p&gt;
    &lt;p&gt;My biggest unexpected finding: we’re hitting limits of traditional tools for sharing code. The pull request model on GitHub doesn’t carry enough information to review AI generated code properly — I wish I could see the prompts that led to changes. It’s not just GitHub, it’s also git that is lacking.&lt;/p&gt;
    &lt;p&gt;With agentic coding, part of what makes the models work today is knowing the mistakes. If you steer it back to an earlier state, you want the tool to remember what went wrong. There is, for lack of a better word, value in failures. As humans we might also benefit from knowing the paths that did not lead us anywhere, but for machines this is critical information. You notice this when you are trying to compress the conversation history. Discarding the paths that led you astray means that the model will try the same mistakes again.&lt;/p&gt;
    &lt;p&gt;Some agentic coding tools have begun spinning up worktrees or creating checkpoints in git for restore, in-conversation branch and undo features. There’s room for UX innovation that could make these tools easier to work with. This is probably why we’re seeing discussions about stacked diffs and alternative version control systems like Jujutsu.&lt;/p&gt;
    &lt;p&gt;Will this change GitHub or will it create space for some new competition? I hope so. I increasingly want to better understand genuine human input and tell it apart from machine output. I want to see the prompts and the attempts that failed along the way. And then somehow I want to squash and compress it all on merge, but with a way to retrieve the full history if needed.&lt;/p&gt;
    &lt;p&gt;This is related to the version control piece: current code review tools assign strict role definitions that just don’t work with AI. Take the GitHub code review UI: I regularly want to use comments on the PR view to leave notes for my own agents, but there is no guided way to do that. The review interface refuses to let me review my own code, I can only comment, but that does not have quite the same intention.&lt;/p&gt;
    &lt;p&gt;There is also the problem that an increased amount of code review now happens between me and my agents locally. For instance, the Codex code review feature on GitHub stopped working for me because it can only be bound to one organization at a time. So I now use Codex on the command line to do reviews, but that means a whole part of my iteration cycles is invisible to other engineers on the team. That doesn’t work for me.&lt;/p&gt;
    &lt;p&gt;Code review to me feels like it needs to become part of the VCS.&lt;/p&gt;
    &lt;p&gt;I also believe that observability is up for grabs again. We now have both the need and opportunity to take advantage of it on a whole new level. Most people were not in a position where they could build their own eBPF programs, but LLMs can. Likewise, many observability tools shied away from SQL because of its complexity, but LLMs are better at it than any proprietary query language. They can write queries, they can grep, they can map-reduce, they remote-control LLDB. Anything that has some structure and text is suddenly fertile ground for agentic coding tools to succeed. I don’t know what the observability of the future looks like, but my strong hunch is that we will see plenty of innovation here. The better the feedback loop to the machine, the better the results.&lt;/p&gt;
    &lt;p&gt;I’m not even sure what I’m asking for here, but I think that one of the challenges in the past was that many cool ideas for better observability — specifically dynamic reconfiguration of services for more targeted filtering — were user-unfriendly because they were complex and hard to use. But now those might be the right solutions in light of LLMs because of their increased capabilities for doing this grunt work. For instance Python 3.14 landed an external debugger interface which is an amazing capability for an agentic coding tool.&lt;/p&gt;
    &lt;p&gt;This may be a little more controversial, but what I haven’t managed this year is to give in to the machine. I still treat it like regular software engineering and review a lot. I also recognize that an increasing number of people are not working with this model of engineering but instead completely given in to the machine. As crazy as that sounds, I have seen some people be quite successful with this. I don’t yet know how to reason about this, but it is clear to me that even though code is being generated in the end, the way of working in that new world is very different from the world that I’m comfortable with. And my suspicion is that because that world is here to stay, we might need some new social contracts to separate these out.&lt;/p&gt;
    &lt;p&gt;The most obvious version of this is the increased amount of these types of contributions to Open Source projects, which are quite frankly an insult to anyone who is not working in that model. I find reading such pull requests quite rage-inducing.&lt;/p&gt;
    &lt;p&gt;Personally, I’ve tried to attack this problem with contribution guidelines and pull request templates. But this seems a little like a fight against windmills. This might be something where the solution will not come from changing what we’re doing. Instead, it might come from vocal people who are also pro-AI engineering speaking out on what good behavior in an agentic codebase looks like. And it is not just to throw up unreviewed code and then have another person figure the shit out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352875</guid><pubDate>Mon, 22 Dec 2025 10:19:57 +0000</pubDate></item><item><title>If you don't design your career, someone else will (2014)</title><link>https://gregmckeown.com/if-you-dont-design-your-career-someone-else-will/</link><description>&lt;doc fingerprint="227b983565a77a21"&gt;
  &lt;main&gt;
    &lt;p&gt;A client once responded to one of my questions by saying, “Oh Greg, I am too busy living to think about life!” His off-the-cuff comment named a trap all of us fall into sometimes. In just one example, it is easy to become so consumed in our careers we fail to really think about our careers.&lt;/p&gt;
    &lt;p&gt;To avoid this trap, I suggest carving out a couple of hours over the holiday break to follow these simple steps for reflecting on your career.&lt;/p&gt;
    &lt;p&gt;Step 1: Review the last 12 months. Review the year, month by month. Make a list of where you spent your time: include your major projects, responsibilities and accomplishments. No need to overcomplicate this.&lt;/p&gt;
    &lt;p&gt;Step 2: Ask, “What is the news?” Look over your list and reflect on what is really going on. Think like a journalist and ask yourself: Why does this matter? What are the trends here? What happens if these trends continue?&lt;/p&gt;
    &lt;p&gt;Step 3: Ask “What would I do in my career if I could do anything?” Just brainstorm with no voice of criticism to hold you back. Just write out all the ideas that come to mind.&lt;/p&gt;
    &lt;p&gt;Step 4: Go back and spend a bit more time on Step 3. Too often we begin our career planning with our second best option in mind. We have a sense of what we would most love to do but we immediately push it aside. Why? Typically because “it is not realistic” which is code for, “I can’t make money doing this.” In this economy—in any economy—I understand why making money is critical. However, sometimes we pass by legitimate career paths because we set them aside too quickly.&lt;/p&gt;
    &lt;p&gt;Step 5: Write down six objectives for the next 12 months. Make a list of the top six items you would like to accomplish in your career this year and place them in priority order.&lt;/p&gt;
    &lt;p&gt;Step 6: Cross off the bottom five. Once you’re back to the whirlwind of work you’ll benefit from having a single “true north” career objective for the year.&lt;/p&gt;
    &lt;p&gt;Step 7: Make an action plan for this month. Make a list of some quick wins you’d like to have in place over the next 3-4 weeks.&lt;/p&gt;
    &lt;p&gt;Step 8: Decide what you will say no to. Make a list of the “good” things that will keep you from achieving your one “great” career objective. Think about how to delete, defer or delegate these other tasks. Ralph Waldo Emerson said, “The crime which bankrupts men and nations is that of turning aside from one’s main purpose to serve a job here and there.”&lt;/p&gt;
    &lt;p&gt;Many years ago I followed this process and, without exaggeration, it changed the course of my life. The insight I gained led me to quit law school, leave England and move to America and start down the path as a teacher and author. You’re reading this because of that choice. It remains the single most important career decision of my life.&lt;/p&gt;
    &lt;p&gt;Two hours spent wisely over the next couple of weeks could easily improve the quality of your life over the 8760 hours of the next year–and perhaps far beyond. After all, if we don’t design our careers, someone else will.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46352930</guid><pubDate>Mon, 22 Dec 2025 10:27:52 +0000</pubDate></item><item><title>The biggest CRT ever made: Sony's PVM-4300</title><link>https://dfarq.homeip.net/the-biggest-crt-ever-made-sonys-pvm-4300/</link><description>&lt;doc fingerprint="73489d35a9a732be"&gt;
  &lt;main&gt;
    &lt;p&gt;Move over, GE Widescreen 1000. In 1989 in Japan, Sony introduced to the largest Trinitron CRT ever built, the KV-45ED1, also known as the PVM-4300. And in 1990, they imported 20 of them to the United States, just in time for the recession. About 34 years later, one of these enigmatic TVs surfaced.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sony’s PVM-4300/KV-45ED1&lt;/head&gt;
    &lt;p&gt;Sony’s part number suggests it has a 45 inch tube inside. But in a rare case of truth in advertising, Sony advertised it as a 43-inch model. It weighed about 450 pounds, stood about 27 inches tall, and it wouldn’t fit through a standard door frame. That’s probably okay, it’s not like someone was going to use this as a bedroom TV. This thing was going in your living room.&lt;/p&gt;
    &lt;p&gt;In Japan, it sold for 2.6 million yen, but in the United States, it retailed for $40,000, a significant markup. To be fair, shipping them across the Atlantic and then throughout the United States must have been expensive. And news articles in 1990 said Sony dealers would not allow any bickering. They would throw in a couple of options like the separate tuner or speakers. But no discounts.&lt;/p&gt;
    &lt;p&gt;Sony said at the time they hoped to sell 80 of them that year, but the recession may have kept that from happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;The biggest conventional CRT ever&lt;/head&gt;
    &lt;p&gt;The Sony PVM-4300 was a conventional CRT, unlike the GE Widescreen 1000, which was a projection set. Projection TVs could be bigger and cheaper. But if you wanted the clearest picture, a big CRT was where it was at.&lt;/p&gt;
    &lt;p&gt;It was a conventional CRT that worked with over the air signals, but like many larger TVs of the era, it used a technology called IDTV to enhance the picture quality. The “ID” stood for “improved definition.” IDTV sets had a buffer so they would store successive frames and interpolate them rather than interlacing them the way a conventional CRT TV worked. They also had circuitry to detect motion and perform image stabilization to further enhance the image. The result wasn’t as good as HDTV. But it gave high rollers a better picture until HDTV. HDTV arrived in 1998, but articles at the time estimated 2005. The Chicago Tribune warned in 1990 that these $40,000 TVs would be obsolete in 15 years, but the salesperson countered that every TV would be obsolete in 15 years.&lt;/p&gt;
    &lt;p&gt;It’s also likely that someone in the market for a $40,000 TV didn’t worry about obsolescence. In 1990, the GE Widescreen 1000 looked dated and it wasn’t 15 years old yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why so expensive?&lt;/head&gt;
    &lt;p&gt;The KV-45ED1 or PVM-4300 cost about 8 times as much as Sony’s second most expensive model at the time, which had a 29-inch screen. That’s largely because the KV-45ED1 had to be built by hand. Sony could mass produce its smaller TVs. This was a product for buyers who weren’t worried about the price.&lt;/p&gt;
    &lt;p&gt;Sony continued making CRTs into the 21st century, bowing out with its high-def KD-34XBR970, 36-inch KD-36FS170, 32-inch KV-32FS170 and 27-inch KV27FS170 in February 2006.&lt;/p&gt;
    &lt;p&gt;It is unclear how many of these enormous 43-inch units Sony sold, and some people even questioned if it was ever built. A Chicago area dealer told the Chicago Tribune in 1990 that someone had purchased one, but that the buyer wanted to remain anonymous. That was good enough for me; a TV dealer wasn’t going to tell a newspaper that they have a $40,000 item and then not have it. That’s just bad business. Good business is taking the free advertising, having an example on display to show knowing most won’t buy it, but they may buy one of the smaller units. But luring someone into the store with a lie makes it much more difficult to sell anything.&lt;/p&gt;
    &lt;p&gt;And the Tribune wasn’t going to make something like this up. It would anger the TV dealers and risk losing their advertising. And someone who could afford a $40,000 TV was likely a business owner or high-ranking executive who could pull their advertising. In the days of print newspapers, advertisers and potential advertisers held a lot of sway. This could be both good and bad. I’m not going to say capitalism solves every problem but this was a case where it helped keep people honest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shank Mods’ surviving Sony PVM-4300&lt;/head&gt;
    &lt;p&gt;On December 22, 2024, Youtuber Shank Mods released a video telling the story of a Sony PVM-4300 and how he acquired it. One of the photos of a purported surviving unit turned out to be very real. It was taken in a restaurant in Japan, and the owner was actually aware of the photo. Unfortunately the restaurant was having to move, and needed to get rid of the set. Shank Mods was able to contact some people in Japan who could help race against time and remove the TV from the restaurant and then ship it to the United States.&lt;/p&gt;
    &lt;p&gt;The 35-minute video is well worth watching if you have interest in vintage CRTs, or even if you just like stories of strangers coming together and helping each other just for the sake of being helpful. Actually, I take that back. At the end of the video, Shank Mods played a prank on his fellow CRT fans that is absolutely hilarious and makes the video worth watching for that reason alone. I won’t ruin it for you.&lt;/p&gt;
    &lt;p&gt;We can only guess how many other examples may survive. But we now know that at least one survives and is in the hands of a retro hobbyist.&lt;/p&gt;
    &lt;p&gt;David Farquhar is a computer security professional, entrepreneur, and author. He has written professionally about computers since 1991, so he was writing about retro computers when they were still new. He has been working in IT professionally since 1994 and has specialized in vulnerability management since 2013. He holds Security+ and CISSP certifications. Today he blogs five times a week, mostly about retro computers and retro gaming covering the time period from 1975 to 2000.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46353777</guid><pubDate>Mon, 22 Dec 2025 12:54:29 +0000</pubDate></item></channel></rss>