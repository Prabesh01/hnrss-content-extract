<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 31 Aug 2025 07:31:55 +0000</lastBuildDate><item><title>Cognitive load is what matters</title><link>https://github.com/zakirullin/cognitive-load</link><description>&lt;doc fingerprint="373978120d250fed"&gt;
  &lt;main&gt;&lt;p&gt;Readable version | Chinese translation | Korean translation | Turkish translation&lt;/p&gt;&lt;p&gt;It is a living document, last update: August 2025. Your contributions are welcome!&lt;/p&gt;&lt;p&gt;There are so many buzzwords and best practices out there, but most of them have failed. We need something more fundamental, something that can't be wrong.&lt;/p&gt;&lt;p&gt;Sometimes we feel confusion going through the code. Confusion costs time and money. Confusion is caused by high cognitive load. It's not some fancy abstract concept, but rather a fundamental human constraint. It's not imagined, it's there and we can feel it.&lt;/p&gt;&lt;p&gt;Since we spend far more time reading and understanding code than writing it, we should constantly ask ourselves whether we are embedding excessive cognitive load into our code.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Cognitive load is how much a developer needs to think in order to complete a task.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When reading code, you put things like values of variables, control flow logic and call sequences into your head. The average person can hold roughly four such chunks in working memory. Once the cognitive load reaches this threshold, it becomes much harder to understand things.&lt;/p&gt;&lt;p&gt;Let's say we have been asked to make some fixes to a completely unfamiliar project. We were told that a really smart developer had contributed to it. Lots of cool architectures, fancy libraries and trendy technologies were used. In other words, the author had created a high cognitive load for us.&lt;/p&gt;&lt;p&gt;We should reduce the cognitive load in our projects as much as possible.&lt;/p&gt;&lt;p&gt;Intrinsic - caused by the inherent difficulty of a task. It can't be reduced, it's at the very heart of software development.&lt;/p&gt;&lt;p&gt;Extraneous - created by the way the information is presented. Caused by factors not directly relevant to the task, such as smart author's quirks. Can be greatly reduced. We will focus on this type of cognitive load.&lt;/p&gt;&lt;p&gt;Let's jump straight to the concrete practical examples of extraneous cognitive load.&lt;/p&gt;&lt;p&gt;We will refer to the level of cognitive load as follows:&lt;code&gt;🧠&lt;/code&gt;: fresh working memory, zero cognitive load&lt;code&gt;🧠++&lt;/code&gt;: two facts in our working memory, cognitive load increased&lt;code&gt;🤯&lt;/code&gt;: cognitive overload, more than 4 facts&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Our brain is much more complex and unexplored, but we can go with this simplistic model.&lt;/p&gt;&lt;/quote&gt;&lt;code&gt;if val &amp;gt; someConstant // 🧠+
    &amp;amp;&amp;amp; (condition2 || condition3) // 🧠+++, prev cond should be true, one of c2 or c3 has be true
    &amp;amp;&amp;amp; (condition4 &amp;amp;&amp;amp; !condition5) { // 🤯, we are messed up by this point
    ...
}&lt;/code&gt;&lt;p&gt;Introduce intermediate variables with meaningful names:&lt;/p&gt;&lt;code&gt;isValid = val &amp;gt; someConstant
isAllowed = condition2 || condition3
isSecure = condition4 &amp;amp;&amp;amp; !condition5 
// 🧠, we don't need to remember the conditions, there are descriptive variables
if isValid &amp;amp;&amp;amp; isAllowed &amp;amp;&amp;amp; isSecure {
    ...
}&lt;/code&gt;&lt;code&gt;if isValid { // 🧠+, okay nested code applies to valid input only
    if isSecure { // 🧠++, we do stuff for valid and secure input only
        stuff // 🧠+++
    }
} &lt;/code&gt;&lt;p&gt;Compare it with the early returns:&lt;/p&gt;&lt;code&gt;if !isValid
    return
 
if !isSecure
    return

// 🧠, we don't really care about earlier returns, if we are here then all good

stuff // 🧠+&lt;/code&gt;&lt;p&gt;We can focus on the happy path only, thus freeing our working memory from all sorts of preconditions.&lt;/p&gt;&lt;p&gt;We are asked to change a few things for our admin users: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt;&lt;p&gt;&lt;code&gt;AdminController extends UserController extends GuestController extends BaseController&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Ohh, part of the functionality is in &lt;code&gt;BaseController&lt;/code&gt;, let's have a look: &lt;code&gt;🧠+&lt;/code&gt;&lt;lb/&gt; Basic role mechanics got introduced in &lt;code&gt;GuestController&lt;/code&gt;: &lt;code&gt;🧠++&lt;/code&gt;&lt;lb/&gt; Things got partially altered in &lt;code&gt;UserController&lt;/code&gt;: &lt;code&gt;🧠+++&lt;/code&gt;&lt;lb/&gt; Finally we are here, &lt;code&gt;AdminController&lt;/code&gt;, let's code stuff! &lt;code&gt;🧠++++&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Oh, wait, there's &lt;code&gt;SuperuserController&lt;/code&gt; which extends &lt;code&gt;AdminController&lt;/code&gt;. By modifying &lt;code&gt;AdminController&lt;/code&gt; we can break things in the inherited class, so let's dive in &lt;code&gt;SuperuserController&lt;/code&gt; first: &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Prefer composition over inheritance. We won't go into detail - there's plenty of material out there.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Method, class and module are interchangeable in this context&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mantras like "methods should be shorter than 15 lines of code" or "classes should be small" turned out to be somewhat wrong.&lt;/p&gt;&lt;p&gt;Deep module - simple interface, complex functionality&lt;lb/&gt; Shallow module - interface is relatively complex to the small functionality it provides&lt;/p&gt;&lt;p&gt;Having too many shallow modules can make it difficult to understand the project. Not only do we have to keep in mind each module responsibilities, but also all their interactions. To understand the purpose of a shallow module, we first need to look at the functionality of all the related modules. Jumping between such shallow components is mentally exhausting, linear thinking is more natural to us humans.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Information hiding is paramount, and we don't hide as much complexity in shallow modules.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I have two pet projects, both of them are somewhat 5K lines of code. The first one has 80 shallow classes, whereas the second one has only 7 deep classes. I haven't been maintaining any of these projects for one year and a half.&lt;/p&gt;&lt;p&gt;Once I came back, I realised that it was extremely difficult to untangle all the interactions between those 80 classes in the first project. I would have to rebuild an enormous amount of cognitive load before I could start coding. On the other hand, I was able to grasp the second project quickly, because it had only a few deep classes with a simple interface.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The best components are those that provide powerful functionality yet have a simple interface.&lt;/p&gt;&lt;lb/&gt;John K. Ousterhout&lt;/quote&gt;&lt;p&gt;The interface of the UNIX I/O is very simple. It has only five basic calls:&lt;/p&gt;&lt;code&gt;open(path, flags, permissions)
read(fd, buffer, count)
write(fd, buffer, count)
lseek(fd, offset, referencePosition)
close(fd)&lt;/code&gt;&lt;p&gt;A modern implementation of this interface has hundreds of thousands of lines of code. Lots of complexity is hidden under the hood. Yet it is easy to use due to its simple interface.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;This deep module example is taken from the book A Philosophy of Software Design by John K. Ousterhout. Not only does this book cover the very essence of complexity in software development, but it also has the greatest interpretation of Parnas' influential paper On the Criteria To Be Used in Decomposing Systems into Modules. Both are essential reads. Other related readings: A Philosophy of Software Design vs Clean Code, It's probably time to stop recommending Clean Code, Small Functions considered Harmful.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;P.S. If you think we are rooting for bloated God objects with too many responsibilities, you got it wrong.&lt;/p&gt;&lt;p&gt;All too often, we end up creating lots of shallow modules, following some vague "a module should be responsible for one, and only one, thing" principle. What is this blurry one thing? Instantiating an object is one thing, right? So MetricsProviderFactoryFactory seems to be just fine. The names and interfaces of such classes tend to be more mentally taxing than their entire implementations, what kind of abstraction is that? Something went wrong.&lt;/p&gt;&lt;p&gt;We make changes to our systems to satisfy our users and stakeholders. We are responsible to them.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A module should be responsible to one, and only one, user or stakeholder.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is what this Single Responsibility Principle is all about. Simply put, if we introduce a bug in one place, and then two different business people come to complain, we've violated the principle. It has nothing to do with the number of things we do in our module.&lt;/p&gt;&lt;p&gt;But even now, this rule can do more harm than good. This principle can be understood in as many different ways as there are individuals. A better approach would be to look at how much cognitive load it all creates. It's mentally demanding to remember that change in one place can trigger a chain of reactions across different business streams. And that's about it, no fancy terms to learn.&lt;/p&gt;&lt;p&gt;This shallow-deep module principle is scale-agnostic, and we can apply it to microservices architecture. Too many shallow microservices won't do any good - the industry is heading towards somewhat "macroservices", i.e., services that are not so shallow (=deep). One of the worst and hardest to fix phenomena is so-called distributed monolith, which is often the result of this overly granular shallow separation.&lt;/p&gt;&lt;p&gt;I once consulted a startup where a team of five developers introduced 17(!) microservices. They were 10 months behind schedule and appeared nowhere close to the public release. Every new requirement led to changes in 4+ microservices. It took an enormous amount of time to reproduce and debug an issue in such a distributed system. Both time to market and cognitive load were unacceptably high. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Is this the right way to approach the uncertainty of a new system? It's enormously difficult to elicit the right logical boundaries in the beginning. The key is to make decisions as late as you can responsibly wait, because that is when you have the most information at hand. By introducing a network layer up front, we make our design decisions hard to revert right from the start. The team's only justification was: "The FAANG companies proved microservices architecture to be effective". Hello, you got to stop dreaming big.&lt;/p&gt;&lt;p&gt;The Tanenbaum-Torvalds debate argued that Linux's monolithic design was flawed and obsolete, and that a microkernel architecture should be used instead. Indeed, the microkernel design seemed to be superior "from a theoretical and aesthetical" point of view. On the practical side of things - three decades on, microkernel-based GNU Hurd is still in development, and monolithic Linux is everywhere. This page is powered by Linux, your smart teapot is powered by Linux. By monolithic Linux.&lt;/p&gt;&lt;p&gt;A well-crafted monolith with truly isolated modules is often much more flexible than a bunch of microservices. It also requires far less cognitive effort to maintain. It's only when the need for separate deployments becomes crucial, such as scaling the development team, that you should consider adding a network layer between the modules, future microservices.&lt;/p&gt;&lt;p&gt;We feel excited when new features got released in our favourite language. We spend some time learning these features, we build code upon them.&lt;/p&gt;&lt;p&gt;If there are lots of features, we may spend half an hour playing with a few lines of code, to use one or another feature. And it's kind of a waste of time. But what's worse, when you come back later, you would have to recreate that thought process!&lt;/p&gt;&lt;p&gt;You not only have to understand this complicated program, you have to understand why a programmer decided this was the way to approach a problem from the features that are available. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;These statements are made by none other than Rob Pike.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Reduce cognitive load by limiting the number of choices.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Language features are OK, as long as they are orthogonal to each other.&lt;/p&gt;&lt;head&gt;Thoughts from an engineer with 20 years of C++ experience ⭐️&lt;/head&gt;&lt;p&gt;I was looking at my RSS reader the other day and noticed that I have somewhat three hundred unread articles under the "C++" tag. I haven't read a single article about the language since last summer, and I feel great!&lt;/p&gt;&lt;p&gt;I've been using C++ for 20 years for now, that's almost two-thirds of my life. Most of my experience lies in dealing with the darkest corners of the language (such as undefined behaviours of all sorts). It's not a reusable experience, and it's kind of creepy to throw it all away now.&lt;/p&gt;&lt;p&gt;Like, can you imagine, the token&lt;/p&gt;&lt;code&gt;||&lt;/code&gt; has a different meaning in &lt;code&gt;requires ((!P&amp;lt;T&amp;gt; || !Q&amp;lt;T&amp;gt;))&lt;/code&gt; and in &lt;code&gt;requires (!(P&amp;lt;T&amp;gt; || Q&amp;lt;T&amp;gt;))&lt;/code&gt;. The first is the constraint disjunction, the second is the good-old logical OR operator, and they behave differently.&lt;p&gt;You can't allocate space for a trivial type and just&lt;/p&gt;&lt;code&gt;memcpy&lt;/code&gt; a set of bytes there without extra effort - that won't start the lifetime of an object. This was the case before C++20. It was fixed in C++20, but the cognitive load of the language has only increased.&lt;p&gt;Cognitive load is constantly growing, even though things got fixed. I should know what was fixed, when it was fixed, and what it was like before. I am a professional after all. Sure, C++ is good at legacy support, which also means that you will face that legacy. For example, last month a colleague of mine asked me about some behaviour in C++03.&lt;/p&gt;&lt;code&gt;🤯&lt;/code&gt;&lt;p&gt;There were 20 ways of initialization. Uniform initialization syntax has been added. Now we have 21 ways of initialization. By the way, does anyone remember the rules for selecting constructors from the initializer list? Something about implicit conversion with the least loss of information, but if the value is known statically, then...&lt;/p&gt;&lt;code&gt;🤯&lt;/code&gt;&lt;p&gt;This increased cognitive load is not caused by a business task at hand. It is not an intrinsic complexity of the domain. It is just there due to historical reasons (extraneous cognitive load).&lt;/p&gt;&lt;p&gt;I had to come up with some rules. Like, if that line of code is not as obvious and I have to remember the standard, I better not write it that way. The standard is somewhat 1500 pages long, by the way.&lt;/p&gt;&lt;p&gt;By no means I am trying to blame C++. I love the language. It's just that I am tired now.&lt;/p&gt;&lt;p&gt;Thanks to 0xd34df00d for writing.&lt;/p&gt;&lt;p&gt;On the backend we return:&lt;code&gt;401&lt;/code&gt; for expired jwt token&lt;code&gt;403&lt;/code&gt; for not enough access&lt;code&gt;418&lt;/code&gt; for banned users&lt;/p&gt;&lt;p&gt;The engineers on the frontend use backend API to implement login functionality. They would have to temporarily create the following cognitive load in their brains:&lt;code&gt;401&lt;/code&gt; is for expired jwt token // &lt;code&gt;🧠+&lt;/code&gt;, ok just temporary remember it&lt;code&gt;403&lt;/code&gt; is for not enough access // &lt;code&gt;🧠++&lt;/code&gt;&lt;code&gt;418&lt;/code&gt; is for banned users // &lt;code&gt;🧠+++&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Frontend developers would (hopefully) introduce some kind &lt;code&gt;numeric status -&amp;gt; meaning&lt;/code&gt; dictionary on their side, so that subsequent generations of contributors wouldn't have to recreate this mapping in their brains.&lt;/p&gt;&lt;p&gt;Then QA engineers come into play: "Hey, I got &lt;code&gt;403&lt;/code&gt; status, is that expired token or not enough access?"
QA engineers can't jump straight to testing, because first they have to recreate the cognitive load that the engineers on the backend once created.&lt;/p&gt;&lt;p&gt;Why hold this custom mapping in our working memory? It's better to abstract away your business details from the HTTP transfer protocol, and return self-descriptive codes directly in the response body:&lt;/p&gt;&lt;code&gt;{
    "code": "jwt_has_expired"
}&lt;/code&gt;&lt;p&gt;Cognitive load on the frontend side: &lt;code&gt;🧠&lt;/code&gt; (fresh, no facts are held in mind)&lt;lb/&gt; Cognitive load on the QA side: &lt;code&gt;🧠&lt;/code&gt;&lt;/p&gt;&lt;p&gt;The same rule applies to all sorts of numeric statuses (in the database or wherever) - prefer self-describing strings. We are not in the era of 640K computers to optimise for memory.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;People spend time arguing between&lt;/p&gt;&lt;code&gt;401&lt;/code&gt;and&lt;code&gt;403&lt;/code&gt;, making decisions based on their own mental models. New developers are coming in, and they need to recreate that thought process. You may have documented the "whys" (ADRs) for your code, helping newcomers to understand the decisions made. But in the end it just doesn't make any sense. We can separate errors into either user-related or server-related, but apart from that, things are kind of blurry.&lt;/quote&gt;&lt;p&gt;P.S. It's often mentally taxing to distinguish between "authentication" and "authorization". We can use simpler terms like "login" and "permissions" to reduce the cognitive load.&lt;/p&gt;&lt;p&gt;Do not repeat yourself - that is one of the first principles you are taught as a software engineer. It is so deeply embedded in ourselves that we can not stand the fact of a few extra lines of code. Although in general a good and fundamental rule, when overused it leads to the cognitive load we can not handle.&lt;/p&gt;&lt;p&gt;Nowadays, everyone builds software based on logically separated components. Often those are distributed among multiple codebases representing separate services. When you strive to eliminate any repetition, you might end up creating tight coupling between unrelated components. As a result changes in one part may have unintended consequences in other seemingly unrelated areas. It can also hinder the ability to replace or modify individual components without impacting the entire system. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;In fact, the same problem arises even within a single module. You might extract common functionality too early, based on perceived similarities that might not actually exist in the long run. This can result in unnecessary abstractions that are difficult to modify or extend.&lt;/p&gt;&lt;p&gt;Rob Pike once said:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A little copying is better than a little dependency.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;We are tempted to not reinvent the wheel so strong that we are ready to import large, heavy libraries to use a small function that we could easily write by ourselves.&lt;/p&gt;&lt;p&gt;All your dependencies are your code. Going through 10+ levels of stack trace of some imported library and figuring out what went wrong (because things go wrong) is painful.&lt;/p&gt;&lt;p&gt;There's a lot of "magic" in frameworks. By relying too heavily on a framework, we force all upcoming developers to learn that "magic" first. It can take months. Even though frameworks enable us to launch MVPs in a matter of days, in the long run they tend to add unnecessary complexity and cognitive load.&lt;/p&gt;&lt;p&gt;Worse yet, at some point frameworks can become a significant constraint when faced with a new requirement that just doesn't fit the architecture. From here onwards people end up forking a framework and maintaining their own custom version. Imagine the amount of cognitive load a newcomer would have to build (i.e. learn this custom framework) in order to deliver any value. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;By no means do we advocate to invent everything from scratch!&lt;/p&gt;&lt;p&gt;We can write code in a somewhat framework-agnostic way. The business logic should not reside within a framework; rather, it should use the framework's components. Put a framework outside of your core logic. Use the framework in a library-like fashion. This would allow new contributors to add value from day one, without the need of going through debris of framework-related complexity first.&lt;/p&gt;&lt;p&gt;There is a certain engineering excitement about all this stuff.&lt;/p&gt;&lt;p&gt;I myself was a passionate advocate of Hexagonal/Onion Architecture for years. I used it here and there and encouraged other teams to do so. The complexity of our projects went up, the sheer number of files alone had doubled. It felt like we were writing a lot of glue code. On ever changing requirements we had to make changes across multiple layers of abstractions, it all became tedious. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Abstraction is supposed to hide complexity, here it just adds indirection. Jumping from call to call to read along and figure out what goes wrong and what is missing is a vital requirement to quickly solve a problem. With this architecture’s layer uncoupling it requires an exponential factor of extra, often disjointed, traces to get to the point where the failure occurs. Every such trace takes space in our limited working memory. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;This architecture was something that made intuitive sense at first, but every time we tried applying it to projects it made a lot more harm than good. In the end, we gave it all up in favour of the good old dependency inversion principle. No port/adapter terms to learn, no unnecessary layers of horizontal abstractions, no extraneous cognitive load.&lt;/p&gt;&lt;p&gt;If you think that such layering will allow you to quickly replace a database or other dependencies, you're mistaken. Changing the storage causes lots of problems, and believe us, having some abstractions for the data access layer is the least of your worries. At best, abstractions can save somewhat 10% of your migration time (if any), the real pain is in data model incompatibilities, communication protocols, distributed systems challenges, and implicit interfaces.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;With a sufficient number of users of an API,&lt;/p&gt;&lt;lb/&gt;it does not matter what you promise in the contract:&lt;lb/&gt;all observable behaviors of your system&lt;lb/&gt;will be depended on by somebody.&lt;/quote&gt;&lt;p&gt;We did a storage migration, and that took us about 10 months. The old system was single-threaded, so the exposed events were sequential. All our systems depended on that observed behaviour. This behavior was not part of the API contract, it was not reflected in the code. A new distributed storage didn't have that guarantee - the events came out-of-order. We spent only a few hours coding a new storage adapter, thanks to an abstraction. We spent the next 10 months on dealing with out-of-order events and other challenges. It's now funny to say that abstractions helps us replace components quickly.&lt;/p&gt;&lt;p&gt;So, why pay the price of high cognitive load for such a layered architecture, if it doesn't pay off in the future? Plus, in most cases, that future of replacing some core component never happens.&lt;/p&gt;&lt;p&gt;These architectures are not fundamental, they are just subjective, biased consequences of more fundamental principles. Why rely on those subjective interpretations? Follow the fundamental rules instead: dependency inversion principle, single source of truth, cognitive load and information hiding. Your business logic should not depend on low-level modules like database, UI or framework. We should be able to write tests for our core logic without worrying about the infrastructure, and that's it. Discuss.&lt;/p&gt;&lt;p&gt;Do not add layers of abstractions for the sake of an architecture. Add them whenever you need an extension point that is justified for practical reasons.&lt;/p&gt;&lt;p&gt;Layers of abstraction aren't free of charge, they are to be held in our limited working memory.&lt;/p&gt;&lt;p&gt;Domain-driven design has some great points, although it is often misinterpreted. People say, "We write code in DDD", which is a bit strange, because DDD is more about the problem space rather than the solution space.&lt;/p&gt;&lt;p&gt;Ubiquitous language, domain, bounded context, aggregate, event storming are all about problem space. They are meant to help us learn the insights about the domain and extract the boundaries. DDD enables developers, domain experts and business people to communicate effectively using a single, unified language. Rather than focusing on these problem space aspects of DDD, we tend to emphasise particular folder structures, services, repositories, and other solution space techniques.&lt;/p&gt;&lt;p&gt;Chances are that the way we interpret DDD is likely to be unique and subjective. And if we build code upon this understanding, i.e., if we create a lot of extraneous cognitive load - future developers are doomed. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Team Topologies provides a much better, easier to understand framework that helps us split the cognitive load across teams. Engineers tend to develop somewhat similar mental models after learning about Team Topologies. DDD, on the other hand, seems to be creating 10 different mental models for 10 different readers. Instead of being common ground, it becomes a battleground for unnecessary debates.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The problem is that familiarity is not the same as simplicity. They feel the same — that same ease of moving through a space without much mental effort — but for very different reasons. Every “clever” (read: “self-indulgent”) and non-idiomatic trick you use incurs a learning penalty for everyone else. Once they have done that learning, then they will find working with the code less difficult. So it is hard to recognise how to simplify code that you are already familiar with. This is why I try to get “the new kid” to critique the code before they get too institutionalised!&lt;/p&gt;&lt;p&gt;It is likely that the previous author(s) created this huge mess one tiny increment at a time, not all at once. So you are the first person who has ever had to try to make sense of it all at once.&lt;/p&gt;&lt;p&gt;In my class I describe a sprawling SQL stored procedure we were looking at one day, with hundreds of lines of conditionals in a huge WHERE clause. Someone asked how anyone could have let it get this bad. I told them: “When there are only 2 or 3 conditionals, adding another one doesn’t make any difference. By the time there are 20 or 30 conditionals, adding another one doesn’t make any difference!”&lt;/p&gt;&lt;p&gt;There is no “simplifying force” acting on the code base other than deliberate choices that you make. Simplifying takes effort, and people are too often in a hurry.&lt;/p&gt;&lt;p&gt;Thanks to Dan North for his comment.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;If you've internalized the mental models of the project into your long-term memory, you won't experience a high cognitive load.&lt;/p&gt;&lt;p&gt;The more mental models there are to learn, the longer it takes for a new developer to deliver value.&lt;/p&gt;&lt;p&gt;Once you onboard new people on your project, try to measure the amount of confusion they have (pair programming may help). If they're confused for more than ~40 minutes in a row - you've got things to improve in your code.&lt;/p&gt;&lt;p&gt;If you keep the cognitive load low, people can contribute to your codebase within the first few hours of joining your company.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Our architecture is a standard CRUD app architecture, a Python monolith on top of Postgres&lt;/item&gt;&lt;item&gt;How Instagram scaled to 14 million users with only 3 engineers&lt;/item&gt;&lt;item&gt;The companies where we were like ”woah, these folks are smart as hell” for the most part failed&lt;/item&gt;&lt;item&gt;One function that wires up the entire system. If you want to know how the system works - go read it&lt;/item&gt;&lt;/list&gt;&lt;p&gt;These architectures are quite boring and easy to understand. Anyone can grasp them without much mental effort.&lt;/p&gt;&lt;p&gt;Involve junior developers in architecture reviews. They will help you to identify the mentally demanding areas.&lt;/p&gt;&lt;p&gt;Maintaining software is hard, we would need every bit of mental effort we can save.&lt;/p&gt;&lt;p&gt;Imagine for a moment that what we inferred in the second chapter isn’t actually true. If that’s the case, then the conclusion we just negated, along with the conclusions in the previous chapter that we had accepted as valid, might not be correct either. &lt;code&gt;🤯&lt;/code&gt;&lt;/p&gt;&lt;p&gt;Do you feel it? Not only do you have to jump all over the article to get the meaning (shallow modules!), but the paragraph in general is difficult to understand. We have just created an unnecessary cognitive load in your head. Do not do this to your colleagues.&lt;/p&gt;&lt;p&gt;We should reduce any cognitive load above and beyond what is intrinsic to the work we do.&lt;/p&gt;&lt;head&gt;Comments&lt;/head&gt;&lt;p&gt;Rob Pike&lt;lb/&gt;Nice article.&lt;/p&gt;&lt;p&gt;Andrej Karpathy (ChatGPT, Tesla)&lt;lb/&gt;Nice post on software engineering. Probably the most true, least practiced viewpoint.&lt;/p&gt;&lt;p&gt;Elon Musk&lt;lb/&gt;True.&lt;/p&gt;&lt;p&gt;Addy Osmani (Chrome, the most complex software system in the world)&lt;lb/&gt;I've seen countless projects where smart developers created impressive architectures using the latest design patterns and microservices. But when new team members tried to make changes, they spent weeks just trying to understand how everything fits together. The cognitive load was so high that productivity plummeted and bugs multiplied.&lt;/p&gt;&lt;p&gt;The irony? Many of these complexity-inducing patterns were implemented in the name of "clean code."&lt;/p&gt;&lt;p&gt;What really matters is reducing unnecessary cognitive burden. Sometimes this means fewer, deeper modules instead of many shallow ones. Sometimes it means keeping related logic together instead of splitting it into tiny functions.&lt;/p&gt;&lt;p&gt;And sometimes it means choosing boring, straightforward solutions over clever ones. The best code isn't the most elegant or sophisticated - it's the code that future developers (including yourself) can understand quickly.&lt;/p&gt;&lt;p&gt;Your article really resonates with the challenges we face in browser development. You're absolutely right about modern browsers being among the most complex software systems. Managing that complexity in Chromium is a constant challenge that aligns perfectly with many of the points you made about cognitive load.&lt;/p&gt;&lt;p&gt;One way we try to handle this in Chromium is through careful component isolation and well-defined interfaces between subsystems (like rendering, networking, JavaScript execution, etc.). Similar to your deep modules example with Unix I/O - we aim for powerful functionality behind relatively simple interfaces. For instance, our rendering pipeline handles incredible complexity (layout, compositing, GPU acceleration) but developers can interact with it through clear abstraction layers.&lt;/p&gt;&lt;p&gt;Your points about avoiding unnecessary abstractions really hit home too. In browser development, we constantly balance between making the codebase approachable for new contributors while handling the inherent complexity of web standards and compatibility.&lt;/p&gt;&lt;p&gt;Sometimes the simplest solution is the best one, even in a complex system.&lt;/p&gt;&lt;p&gt;antirez (Redis)&lt;lb/&gt;Totally agree about it :) Also, what I believe is missing from mentioned "A Philosophy of Software Design" is the concept of "design sacrifice". That is, sometimes you sacrifice something and get back simplicity, or performances, or both. I apply this idea continuously, but often is not understood.&lt;/p&gt;&lt;p&gt;A good example is the fact that I always refused to have hash items expires. This is a design sacrifice because if you have certain attributes only in the top-level items (the keys themselves), the design is simpler, values will just be objects. When Redis got hash expires, it was a nice feature but required (indeed) many changes to many parts, raising the complexity.&lt;/p&gt;&lt;p&gt;Another example is what I'm doing right now, Vector Sets, the new Redis data type. I decided that Redis would not be the source of truth about vectors, but that it can just take an approximate version of them, so I was able to do on-insert normalization, quantization without trying to retain the large floats vector on disk, and so forth. May vector DBs don't sacrifice the fact of remembering what the user put inside (the full precision vector).&lt;/p&gt;&lt;p&gt;These are just two random examples, but I apply this idea everywhere. Now the thing is: of course one must sacrifice the right things. Often, there are 5% features that account for a very large amount of complexity: that is a good thing to kill :D&lt;/p&gt;&lt;p&gt;A developer from the internet&lt;lb/&gt;You would not hire me... I sell myself on my track record of released enterprise projects.&lt;/p&gt;&lt;p&gt;I worked with a guy that could speak design patterns. I could never speak that way, though I was one of the few that could well understand him. The managers loved him and he could dominate any development conversation. The people working around him said he left a trail of destruction behind him. I was told that I was the first person that could understand his projects. Maintainability matters. I care most about TCO. For some firms, that's what matters.&lt;/p&gt;&lt;p&gt;I logged into Github after not being there for a while and for some reason it took me to an article in a repository by someone that seemed random. I was thinking "what is this" and had some trouble getting to my home page, so I read it. I didn't really register it at the time, but it was amazing. Every developer should read it. It largely said that almost everything we've been told about programming best practices leads to excessive "cognitive load", meaning our minds are getting kicked by the intellectual demands. I've known this for a while, especially with the demands of cloud, security and DevOps.&lt;/p&gt;&lt;p&gt;I also liked it because it described practices I have done for decades, but never much admit to because they are not popular... I write really complicated stuff and need all the help I can get.&lt;/p&gt;&lt;p&gt;Consider, if I'm right, it popped up because the Github folks, very smart people, though that developers should see it. I agree.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45074248</guid></item><item><title>Bcachefs Goes to "Externally Maintained"</title><link>https://lwn.net/Articles/1035736/</link><description>&lt;doc fingerprint="2df3981fd8a75e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bcachefs goes to "externally maintained"&lt;/head&gt;
    &lt;p&gt; Posted Aug 29, 2025 17:30 UTC (Fri) by intelfx (subscriber, #130118) [Link] (40 responses) &amp;gt; This change also suggests, though, that the immediate removal of bcachefs from the mainline kernel is not in the cards. How would that work? If further changes are not accepted into mainline, but fs/bcachefs/ is not being removed thereof, then what exactly happens to it (the physical code living under that path)? Is it going to slowly bit-rot, with users being instructed to ignore in-tree code and use some other tree instead? Posted Aug 29, 2025 17:42 UTC (Fri) by zdzichu (subscriber, #17118) [Link] (39 responses) Posted Aug 29, 2025 19:21 UTC (Fri) by NYKevin (subscriber, #129325) [Link] (38 responses) But there are a number of unstated assumptions here. The most important problem is how this upstreaming process will work. I can think of a few different alternatives, but the most straightforward option is for Kent to designate somebody. That person would then be responsible for all interaction with the kernel process, including sending emails, responding to code reviews, changing the code as requested (or telling Kent to do so and relaying his responses), etc. This strikes me as a highly difficult and thankless job that I certainly would not want to do. You could easily end up recreating exactly the same argument that Kent was regularly getting into (over release schedules, merge windows, etc.), but by proxy instead of directly. Ideally, Kent stops caring about the kernel's release processes altogether, and takes a mindset of "the kernel is [designee's] problem, and I don't have to deal with it aside from applying a few patches every now and then." Most of the alternatives are worse. If we instead suppose that the kernel takes bcachefs code without Kent's explicit approval or involvement, then the kernel upstream is a de facto fork of bcachefs (or they're just mirroring him, but there are governance problems with that). I'm not convinced that Linus et al. want to maintain a fork in this situation. Posted Aug 29, 2025 22:11 UTC (Fri) by koverstreet (✭ supporter ✭, #4296) [Link] (37 responses) And release process is something I care deeply about, for the simple reason that I support my code. I respond to nearly all of the user bug reports and stare at the test dashboards; I want users to have the most stable and trustworthy code I can provide. Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports. So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says. Posted Aug 30, 2025 4:57 UTC (Sat) by NYKevin (subscriber, #129325) [Link] (3 responses) The distros are downstreams. If they want to package and ship your code, in whatever way they see fit, you've already given them permission to do so. And they are not shy about exercising that permission. I remember several years ago, jwz asked Debian to stop shipping XScreenSaver, because he disagreed with their practice of backporting fixes to old versions. Debian said no, and XScreenSaver is still in the repository today. As you might imagine, some rather harsh words were exchanged, but in the end, both sides went back to their respective corners of the internet and proceeded to mostly ignore each other. Linus, however, is not in the business of playing that game. If there's nobody actively maintaining (his copy of) bcachefs, then I find it hard to believe it's going to be allowed to stick around indefinitely. &amp;gt; So I'm equally curious where we go from here, I'm no more in the loop than anyone else. Exciting times, as the Chinese proverb says. My interpretation of events is that there are only three long-term paths that make sense here: * You accept that you cannot control what appears in Linus's tree, but would prefer that some recent-ish version of bcachefs is there (as opposed to no bcachefs or a very old bcachefs). You designate somebody as I've described upthread, they upstream patches at whatever rate Linus is willing to take them, and everybody is more or less willing to live with the result. I do not see any plausible outcome where you are allowed to control what appears in Linus's tree. He has very explicitly closed the door on that. For expository purposes, and because you are a functioning adult, I have assumed that you will accept this lack of control, but that does not actually matter - one of the above scenarios will inevitably play out, regardless of your opinion of it. The only choice you have at this point is whether it's the first bullet or one of the other two. I do not say this to be cruel. Based on your words in this and other threads, I genuinely believe that this process has been very painful for you, and I doubt you enjoy being reminded that Linus's tree does, in fact, belong to Linus (I'm sure other developers have screamed that at you enough times by now). Unfortunately, this is not a matter of right or wrong. It is a matter of power. You are aggrieved about something that neither Linus, nor anybody else on LKML, is prepared to recognize as an injury to you. Regardless of whether that is the right way or the wrong way of looking at it, Linus is going to conduct the kernel's release cycle as he sees fit. The *healthy* way of looking at it is to accept that that is not within your power to change, and redirect your attention to the things you can change. Posted Aug 30, 2025 16:30 UTC (Sat) by ttuttle (subscriber, #51118) [Link] Posted Aug 30, 2025 20:48 UTC (Sat) by linuxrocks123 (subscriber, #34648) [Link] (1 responses) But I disagree with him that you should focus any attention on that. The healthiest way to approach life is not to try to control what other people do, nor to think about what they might do in response to what you do. Rather, the healthiest approach to life, and the only way to be spiritually free in life, is to do whatever you want to do, so long as what you want to do is both legal and ethical. What other people may or may not do in response to that is their own problem, because, as long as what you are doing is legal, they can't make it your problem. (Unless you let them, so don't let them.) Applying that philosophy to this situation, I think you should just shrug, ignore Linus, and wash your hands of the in-tree version of bcachefs. The in-tree version of the code is what someone else is doing, not what you're doing, so it's not your problem. DKMS will work fine for you. Btw, I have a question for you, because I am interested in bcachefs but have not tried it yet. What I want out of my filesystem is as much speed as possible: because I have backups, I have no concern about data integrity after an unclean shutdown. I currently use ext4 with no journaling and the following knobs set in sysctl.conf: vm.dirty_ratio = 90 I also have libeatmydata.so listed in ld.so.preload to make sure programs can't override my preference for speed by calling fsync(). Given my preferences, would you recommend bcachefs for me over my current ext4-based setup, and, if so, how could I configure bcachefs for my desired speed versus integrity tradeoff? Thanks for any help. Posted Aug 30, 2025 23:41 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] Yup, agreed 100%. My goal is to develop a bulletproof modern filesystem that people can depend on; I said even before bcachefs was merged that I'm not going to let kernel drama take over my life, so I'm OK with bcachefs shipping as a DKMS module. But if you look at where this leaves Linux for filesystems, it is a sad state of affairs. XFS has been burning through maintainers; they're on their third, and it's not good for a project to be losing that continuity of leadership and experience, and "upstream burnout" (read: working with Linus) was the major factor, from speaking with them. btrfs is still eating filesystems - they may be better than they were, but I still see reports regularly, and they don't seem to be taking reliability seriously. It's starting to look like the Linux filesystem world in general is imploding, and at this point all I can do is put on a good pair of running shoes and hope DKMS works out. &amp;gt; Btw, I have a question for you, because I am interested in bcachefs but have not tried it yet. What I want out of my filesystem is as much speed as possible: because I have backups, I have no concern about data integrity after an unclean shutdown. I currently use ext4 with no journaling and the following knobs set in sysctl.conf: If speed is all you want you might not want to run bcachefs yet. On the one hand, benchmarks don't tell the whole story - I've had a lot of users tell me that things do feel more responsive on bcachefs than btrfs, but there are definitely areas where we're slower than we should be (e.g. journalling overhead is higher than it should be, and I have WIP stuff for that) and still some outright performance bugs to chase down. But robustness and reliability is waaaaaay more of a priority for me than performance, and the userbase seems to be more interested in erasure coding and the management stuff than performance, so it'll be awhile before I'm working on performance in earnest. Posted Aug 30, 2025 7:57 UTC (Sat) by paravoid (subscriber, #32869) [Link] (29 responses) Debian was not even close to the topic at hand, and yet you felt the need to bring it up, just to attack someone, and with information that is misrepresenting the truth. This is something you've done before, and you were very recently called out in lkml for it. Stop. To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. It was ultimately removed from unstable because noone was able to get through. Source: I am one of them. Posted Aug 30, 2025 11:44 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (13 responses) He did so anyways, and then swapped out bindgen for an old version that was explicitly unsupported according to the Cargo.toml, which broke the build, and he sat on it and Debian users stopped getting updates (I didn't even see a report until months later). This resulted in users being unable to access their filesystems. There was briefly a buggy version of bcachefs-tools that couldn't pass mount options correctly; users in every other distro got a fix quickly, but Debian users did not - and we found out about this when a lot of users weren't able to mount in degraded mode after having a drive die. What you're doing is conflating technical criticism with personal, and then using that as an excuse to ramp up the drama. Technical criticism, including pointing out failures of processes, has to be ok for engineering to function, otherwise we don't learn from our mistakes. That can make for a harsh learning environment, but when you're shipping critical system components that have to work, that's what you signed up for; we have responsibilities. The person in question was warned explicitly that what he was doing was a bad idea; he could have at any point said "this is too complicated an issue for me to handle; I'll let someone else take this one" (and there are mechanisms in Debian process for obtaining exceptions to process rules that could have avoided this, by simply skipping the Rust dependency unbundling with a clear explanation of why); he ignored advice and plowed ahead, and a lot of people were affected by those actions. When we work on this kind of code, we have to be responsible for the work we do, including our mistakes. Posted Aug 30, 2025 14:25 UTC (Sat) by ma4ris8 (subscriber, #170509) [Link] (12 responses) Listen part: I'm trying to repeat roughly the same as you wrote above, to show that I listened you: First you state that maintainer switched Rust dependencies for the packaged versions from Debian. He changed Rust dependencies anyways, and then swapped out bindgen into older version, Important end question: Did I repeat (re-phrase in text) precisely what you wrote? Answer part: For me it sounds like there were some mistakes done by both you and others. How to communicate (listen) effectively, to heal relationships? This way of listening is mentioned in What I showed, is one way to restore human relationships, with Linus and others: If you get a backslash, you was just given an opportunity to listen the complaint. By doing this just very slightly to not burden others, I've seen that sometimes this listening technique helps on-line, in addition of meeting face to face. Posted Aug 30, 2025 18:21 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (11 responses) Why are you trying to bothsides this? You seem to have the facts straight, but I'm not at all clear on what you think I did wrong. All this was explained clearly, calmly and patiently to the Debian package maintainer when he started; he decided to do it his way, and when the breakage became apparent I asked if he was going to fix it and he just said "nope, too complicated" and walked off. So I got stuck with warning bcachefs users away from Debian, and he wrote a screed of a blog post about how impossible I am to work with. Sorry, but from where I sit that just looks crazy. I'm all about focusing on the human aspect, sitting down with people and having open and honest conversations. I do that regularly, and believe me I and others have tried ratcheting down the tensions, bringing the focus back to the technical and looking for ways to make this easier and take things in little steps. The whole rest of the 6.16 merge cycle after the journal_rewind fiasco was just that, from myself and others; we've tried to bridge the gap, bring the focus back to the technical, look for ways to make things work - it doesn't seem to be getting us anywhere. Posted Aug 30, 2025 21:00 UTC (Sat) by josh (subscriber, #17465) [Link] (10 responses) You don't demonstrate any degree of understanding of why requirements other than your own matter. You talk about what the Debian maintainer did, and how you told them not to. You don't talk about why those requirements exist and what you did to help them meet those requirements. You act like the story begins and ends with "I told them no and they didn't obey". This is on par with what happens with the Linux kernel. You don't demonstrate and communicate that you understand requirements other than your own and place weight on them. You just act like they're obstacles to getting *your* requirements met, and try to work around them. Posted Aug 30, 2025 21:48 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (9 responses) It's not "he didn't obey", it's "he did something stupid that I warned him was a bad idea and then he didn't stick around to resolve the situation and a has to deal with the fallout". It's not an authority thing, it's just about making good decisions being responsible for your decisions. Posted Aug 30, 2025 22:09 UTC (Sat) by josh (subscriber, #17465) [Link] (6 responses) Package upstreams vs Debian process typically ends with "your package is not more important than our consistency"; that is a reliably predictable outcome. If you want to *change* Debian process or policy, that's a conversation that requires a detailed case for doing so, which requires understanding of why the requirements are what they are, not just why you want them to be different. Posted Aug 30, 2025 22:43 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (5 responses) Posted Aug 31, 2025 1:29 UTC (Sun) by comex (subscriber, #71521) [Link] (4 responses) (1) In April 2024, Debian unstable was shipping too-old versions of some packages. In particular, bcachefs-tools wanted bindgen 0.69.4 (released upstream 2 months prior), while Debian unstable was shipping 0.66.1 (released upstream 8 months prior). (2) In April 2024, Debian unstable was shipping too-*new* versions of some packages. In particular, bcachefs-tools wanted rust-errno 0.2.x, while rust-errno 0.3.0 had released upstream 14 months prior, and Debian unstable was shipping 0.3.8. (3) Despite these conflicts happening in Debian unstable, the Debian maintainer seemed more concerned about how bcachefs-tools would be maintained in the future in Debian stable. To me these seem like three different problems with three different solutions. (1) If Debian unstable was shipping old versions of some dependencies, then Debian should have updated those packages. Perhaps other dependents would have broken with newer versions of the dependencies, but AFAICT there was no specific evidence of this. 2 months (the age of bindgen 0.69.4 at the time) sounds to me like a reasonable lead time for a dependency. If Debian’s processes make it too hard to update Rust packages at a reasonable pace *in unstable*, then maybe they need to be changed, but I don’t know whether that’s true or whether the issue was something else; perhaps the maintainer's stated lack of experience with Rust packaging. (2) If bcachefs-tools was depending on old versions of some packages, then bcachefs-tools should have been updated. The maintainer could have submitted a PR upstream. That would be easier said than done if this were something like Kubernetes [2], but in this case the blog post only cited 2 packages that needed to be updated. As for (3), I don’t fully understand the problem. Debian stable freezes the entire set of packages. That includes the Rust packages, but also bcachefs-tools and the kernel. Some Linux distros have “hardware enablement” branches where they upgrade the kernel separately from the rest of the system, but AFAIK Debian does not. So why would someone maintaining bcachefs-tools on stable care what is happening upstream? Overall - I'm sure there are some factors I'm missing. But every time I've seen this come up, even the knowledgeable commenters seem to smoosh the issue into "bcachefs-tools is not stable enough for Debian", and to me that really seems like an oversimplification and misunderstanding. Does anyone have additional light to shed? Sources: Posted Aug 31, 2025 1:49 UTC (Sun) by koverstreet (✭ supporter ✭, #4296) [Link] (3 responses) bcachefs-tools updates probably can't follow the Debian "hard freeze for two years" model, and this comes up in other critical system packages, too. _Maybe_ they can, but it's too early to be making those kinds of assumptions and locking us down any particular path. The big concern is that just because a user is running Debian stable they may be running a newer kernel (for drivers, generally), and we want bcachefs-tools to be in sync with the kernel. It's not strictly necessary, we have more compat options than other filesystems (due to in-kernel repair being first class), but it puts us in an uncomfortable situation. Debian may not have official "hardware enablement", but it's still commonplace to pull in a newer kernel from a different channel, and that's expected to work. The kernel has hard requirements about not breaking userspace for exactly the same reason; bcachefs takes the same approach. Upgrades and downgrades should always work; that's a huge part of what we've been working through in the experimental phase. If we have to ship/backport a new bcachefs-tools for Debian stable users, unbundling Rust dependencies at all completely breaks that. But the bigger point is that it's too early to even know what backports are going to like for bcachefs, and we don't want to be in Debian stable at all yet. _But_, for the people in Debian and are running bcachefs now, they still need a supported and working filesystem and process for shipping bugfixes. That's the issue that needs to be solved today for any Debian users to be running bcachefs, not "how do we support Debian stable users for the non-experimental version of bcachefs that will be getting backports and doesn't even exist yet". The other big thing to note that makes debundling really problematic is that Debian is not the only distro. If other distros were unbundling (thank god we got Fedora to agree not to), and their Rust library versions are not in sync - see where that puts us? The last think I want is to get sucked into dealing with is different distros with different conflicting library requirements. It's not the end of the world for things like rust-errno; I would have groaned at that one, but swapping that one out for a different version is unlikely to cause real breakage. Bindgen, OTOH - FFI stuff has the very real potential to introduce the nastiest sort of heisenbugs which won't be caught by the compiler (they have happened and they are _not_ fun), and even I wouldn't trust my test coverage to catch all of those - and Debian does not replicate that testing. Swapping out bindgen was actively dangerous, and never should have even been attempted. I specifically told the Debian package maintainer that that one was dangerous to change, and he did it anyways... Posted Aug 31, 2025 2:11 UTC (Sun) by koverstreet (✭ supporter ✭, #4296) [Link] (1 responses) We had another example of that from just yesterday: Arch flipped on LTO, and it turns out that produces a miscompilation, because the final link is now done by rustc which has different rules than C code about eliding bounds checks. This one was minor, it just caused the progress indicators on data jobs to display incorrectly, but it's quite the scary bug. If distros want to make these changes (and LTO is a perfectly fine thing in principle), we really want them contributed upstream so they can get proper testing and QA. Posted Aug 31, 2025 3:40 UTC (Sun) by jmalcolm (subscriber, #8876) [Link] Seems very reasonable Posted Aug 31, 2025 3:47 UTC (Sun) by jmalcolm (subscriber, #8876) [Link] Thank you for this. In my experience you have succeeded. &amp;gt; bcachefs-tools updates probably can't follow the Debian "hard freeze for two years" model Agreed. In a distro like Debian, I do not see how you adopt something like bcachefs until bcachefs itself has stabilized enough to flow into Debian Stable. If you are going to try, you have to be getting the kernel and userland from outside of Debian. &amp;gt; it's still commonplace to pull in a newer kernel from a different channel, and that's expected to work Sure. But when there is a userspace component, a "working" kernel is not enough. Posted Aug 30, 2025 22:10 UTC (Sat) by lordsutch (guest, #53) [Link] (1 responses) If an upstream doesn't want to play by Debian's rules or thinks the release process is too slow, they can set up their own package repository. Posted Aug 30, 2025 23:42 UTC (Sat) by pizza (subscriber, #46) [Link] I'm sorry, but if "the distribution's rules" result in the distributed package being so broken that it directly leads to user data loss, then those rules are not fit for purpose. Fortunately for Debian, "the rules" provide a mechanism for exceptions where necessary. If a major data loss bug isn't sufficient to qualify for a necessary exception, then I repeat myself about those rules not being fit for purpose. Posted Aug 30, 2025 12:24 UTC (Sat) by muase (subscriber, #178466) [Link] (12 responses) I know it's not the distros' fault; it's simply how LTS has to work in practice – however I can understand the frustration that arises if there seems to be an opportunity to finally update a package(set)... and then that opportunity is missed, and now the dev knows that they have to endure those obsolete bug reports for another n-year release cycle. It definitely didn't read as "just to attack someone". &amp;gt; To correct the record: bcachefs-tools is not in Debian because Kent was impossible to work with and personally attacked, smeared and/or alienated multiple sets of distinct contributors that attempted to work with him in good faith, one after another. Tbh, the only personal attack I see here is from you; and as an outsider, this is not very informative – your frustration may be absolutely legit, but this reply doesn't suit your case. If the communication is public, do you have a link or something? :) Posted Aug 30, 2025 18:27 UTC (Sat) by paravoid (subscriber, #32869) [Link] (11 responses) Kent in https://lore.kernel.org/linux-bcachefs/wona7sjqodu7jgchtx... called part of a maintainer's job as "bullshit, make-work job", told Debian to "develop a better and more practical minded attitude" and to "stop wasting my time with this stupid bullshit and I can get back to real work". The issues we had spent a lot of our volunteer time to fix were very real issues, many of them upstream, and one in the Rust ecosystem. At the time this was sent, all issues were fixed, or were on the way to be fixed, and a recent bcachefs-tools package with all of the appropriate dependencies was a few weeks away from getting to Debian testing. bcachefs-tools was orphaned by its maintainer a few weeks later; myself and another contributor (the two of us had done all recent advancements), stopped investing our time as well. The package has remained orphaned since, for about a year. Anyone can pick it up, but noone has, and that's not because of technical difficulties (as far as packages go, it's pretty trivial). As an aside, the very existence of this thread was as a "PSA" to his users to avoid Debian and Fedora, telling them that "you'll want to be on a more modern distro". *Two weeks later*, he responded in https://lore.kernel.org/lkml/nxyp62x2ruommzyebdwincu26kmi... to Linus that he expects the "major distros" to pick up bcachefs soon. Whether he was dishonest or just naive, I'll leave that to your judgement. The above was just a small sample. There were literally dozens of responses of this style at time, random offensive comments etc., across multiple mediums (mailing lists, IRC, Reddit, etc.). I am not keeping a file though, as I don't feel the need to convince anyone with hard evidence. You don't know me, and I understand that my opinion may not be of much value to you. I hope, though, that you and others may see this as one tiny part of a broader pattern of countless long-time contributors across multiple projects expressing that they have been alienated and driven away by Kent's conduct and sense of entitlement, and that they have good reasons for it. Posted Aug 30, 2025 19:20 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (10 responses) But please do try to put yourself in my shoes; that was after getting a bunch of bug reports from Debian users, and there had been a _lot_ of fail at that point in how the Debian packaging was handled. I do have to reiterate: the unbundling of Rust dependencies should not have happened for bcachefs-tools, there was no technical reason for that, all my explanations were met with "but that's our policy", and no amount of reasoning was getting anywhere; and the Debian packager breaking the build and sitting on it just should not have happened. I do sincerely hope you can analyze how things went from the other end and ask yourself what could have been done better to avoid this, because from my end, this was an intensely frustrating issue, and it wasn't being taken seriously and it had very real effects. Before you start focusing on language and diplomacy, you really need ask yourself if the technical decisionmaking leading up to that point was sound. When we get breakage as bad as what happened with the Debian package, you can expect the kind of frustration I was voicing there, and "bullshit, make-work projects" still seems to accurately describe what Debain's been doing with Rust dependency unbundling. When we're dealing with critical system components, you cannot focus just on language and diplomacy and ignore the decisionmaking; that's ignoring our most basic responsibilities. Posted Aug 30, 2025 21:17 UTC (Sat) by josh (subscriber, #17465) [Link] (9 responses) No, you really don't. There is no universe in which the things you said produced useful outcomes. The fact that they resulted in someone deciding they no longer wish to work with you or put work into being the downstream maintainer of your software is an *unsurprising outcome*. &amp;gt; When we're dealing with critical system components, you cannot focus just on language and diplomacy and ignore the decisionmaking; that's ignoring our most basic responsibilities. You also cannot completely neglect language and diplomacy and understanding other people's requirements, either, as you absolutely did in the messages being quoted here. Your words will produce responses and actions from others. No amount of wishing things were different will enable you to say things that will predictably produce undesired actions and then have a leg to stand on when being annoyed that those predictable responses and actions happen. Your words are a lever to be used, just like your code. Write the words that produce the results you want, and if you want to be happier, learn to not resent that as a means of effecting change. To be clear: the words have to actually match the actions. You can't *just* say the right words but then have them mismatch your actions; down that path you'd find people whose words and truth lack even a passing familiarity. But it's important to, for instance, give people confidence that you care about the requirements they deal with, in some fashion *other* than "what windmill can I burn down so that you don't have to meet those requirements anymore and can do what I want instead". Posted Aug 30, 2025 22:29 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (8 responses) Maybe you guys should just admit there was a screw up so we can all move on? Posted Aug 30, 2025 22:35 UTC (Sat) by josh (subscriber, #17465) [Link] (4 responses) Posted Aug 30, 2025 22:41 UTC (Sat) by josh (subscriber, #17465) [Link] Posted Aug 30, 2025 22:48 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] (2 responses) Do you have a rebuttal? I'd love to hear it. Posted Aug 31, 2025 0:40 UTC (Sun) by SLi (subscriber, #53131) [Link] (1 responses) You keep saying "there are no technical reasons" as if that made it true. They may or may not be the best rules, but they are there for a reason. If you think distro maintainers change version bounds on packages for no reason other than to annoy upstreams, that alone should be a big hint telling you that you probably don't understand something. Or want to understand. I'm not sure which is more true nor which is more flattering. Posted Aug 31, 2025 3:37 UTC (Sun) by ben0x539 (guest, #119600) [Link] Posted Aug 30, 2025 22:39 UTC (Sat) by josh (subscriber, #17465) [Link] If your inclination is to believe this is *in any way* a question that should be redirected into an exploration of your specific requirements that you believe you were right about, you have missed the point. Posted Aug 30, 2025 22:51 UTC (Sat) by sheepdestroyer (guest, #54968) [Link] (1 responses) Posted Aug 31, 2025 6:42 UTC (Sun) by zejn (guest, #116440) [Link] If every package would bundle libraries, it would be impossible to update packages and you'd get a security nightmare as one can see in docker containers using old base images or independent software packages, that freeze a dependency on old version of an open source library. If a distribution is full of security holes, who's going to use it? Posted Aug 31, 2025 3:34 UTC (Sun) by jmalcolm (subscriber, #8876) [Link] (1 responses) I have been critical of Kent so let me defend him here. My understanding of the issue with bcachefs-tools in Debian was that bcachefs required a newer version of Rust than Debian wanted. This is a technical issue and being uncompromising on a technical issue is completely different than a philosophical or process issue. Also, bcachefs is hardly the only project that has had dependency issues with Debian. For all its benefits, Debian is poorly suited to new and evolving technologies (in my view at least). Look at Wayland in Debian vs other distros for another example. Even Debian 13 ships with NVIDIA drivers that lack explicit sync which means Wayland will still not work for many people despite working well in other distros for some time already. I am on Kent's side here. &amp;gt; Debian was not even close to the topic at hand, and yet you felt the need to bring it up Posted Aug 31, 2025 3:41 UTC (Sun) by koverstreet (✭ supporter ✭, #4296) [Link] Well, if I didn't bring it up someone else always does in these discussions. I've never named names and I haven't been lobbing personal attacks, I'm just talking about the process issues bcachefs has faced, and there's a real common thread between that one and the kernel issues. Posted Aug 30, 2025 17:12 UTC (Sat) by DemiMarie (subscriber, #164188) [Link] (1 responses) For anything that has to happen before the filesystem can be accessed at all, it might make sense to have an option for the userspace mount helper to do the work. In this case, the userspace helper has far fewer disadvantages I know of. My dream would be for bcachefs to have SQLite’s level of testing and input validation, or (even better) formal verification. Either would massively reduce the rate of bugs making it into a release, but neither is reasonable to ask for outside of a suitably-priced commercial engagement. Posted Aug 30, 2025 22:24 UTC (Sat) by koverstreet (✭ supporter ✭, #4296) [Link] The way I use assertions does heavily mimic what you'd do with dependent types, though. My primary approach to reliability is instead to mainly focus on debugability: you can't debug what you can't see, but if you can see everything the system is doing, at runtime, in production, debugging is easy. Also note that filesystems are in a different boat from SQLite, because we have to be able to recover from arbitrary outside damage. If your repair is bulletproof, that makes everything else easier: most bugs are not things you have to lose sleep over. Posted Aug 30, 2025 18:53 UTC (Sat) by ATLief (subscriber, #166135) [Link] That’s entirely expected given the circumstances; if longstanding Linux developers collectively don’t want to work with you, then they wouldn’t want to individually work with you either. Posted Aug 29, 2025 17:38 UTC (Fri) by ahippo (subscriber, #154692) [Link] (5 responses) This one works better for me somehow: Posted Aug 29, 2025 17:50 UTC (Fri) by Poliorcetics (subscriber, #165001) [Link] (3 responses) Posted Aug 29, 2025 18:39 UTC (Fri) by ewen (subscriber, #4772) [Link] (Looks like a fairly recent fix too — post is 2025-08-28 — so might take a while for the fix to roll out.) Ewen Posted Aug 29, 2025 19:03 UTC (Fri) by ahippo (subscriber, #154692) [Link] (1 responses) Posted Aug 29, 2025 21:03 UTC (Fri) by alspnost (guest, #2763) [Link] Posted Aug 30, 2025 13:17 UTC (Sat) by Baughn (subscriber, #124425) [Link] I can’t access Anubis-protected pages on my iPad. They take several minutes to pass the test, assuming it doesn’t overheat first. Posted Aug 29, 2025 17:39 UTC (Fri) by JMB (guest, #74439) [Link] (4 responses) And I am hoping that quality of code is still key ... And it is unfortunately connected to the 'external maintenance' Posted Aug 29, 2025 17:47 UTC (Fri) by zdzichu (subscriber, #17118) [Link] (3 responses) Posted Aug 30, 2025 0:28 UTC (Sat) by josh (subscriber, #17465) [Link] (2 responses) Posted Aug 30, 2025 5:17 UTC (Sat) by awilfox (guest, #124923) [Link] Posted Aug 31, 2025 0:58 UTC (Sun) by SLi (subscriber, #53131) [Link] Posted Aug 29, 2025 18:13 UTC (Fri) by mb (subscriber, #50428) [Link] (2 responses) &amp;gt;the immediate removal of bcachefs from the mainline kernel is not in the cards. What? Please just remove it instead of leaving code in the mainline that doesn't receive fixes for known bugs any longer. Posted Aug 29, 2025 18:49 UTC (Fri) by tux3 (subscriber, #101245) [Link] One could still hope for things to resolve differently. Perhaps some other way or other person is found to keep patches flowing. I'm not holding my breath, but there'll be plenty of time to delete things if and when that's what it comes down to. Posted Aug 29, 2025 22:47 UTC (Fri) by hailfinger (subscriber, #76962) [Link] Posted Aug 29, 2025 21:36 UTC (Fri) by birdie (guest, #114905) [Link] Posted Aug 30, 2025 22:01 UTC (Sat) by julian67 (guest, #99845) [Link] (1 responses) Posted Aug 30, 2025 23:44 UTC (Sat) by pizza (subscriber, #46) [Link] You left out "if you do nothing, the people you claim to care for have already lost, and you still get blamed." Posted Aug 30, 2025 22:33 UTC (Sat) by sheepdestroyer (guest, #54968) [Link] (1 responses) Posted Aug 31, 2025 0:41 UTC (Sun) by willy (subscriber, #9762) [Link] Posted Aug 31, 2025 1:01 UTC (Sun) by SLi (subscriber, #53131) [Link] &lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;lb/&gt; &amp;gt;&lt;lb/&gt; &amp;gt; Broken release process is exactly why bcachefs-tools isn't in Debian as well; the package maintainer who took it upon himself to package bcachefs-tools in Debian put project rules ahead of shipping working code, then broke the build and sat in it - and I got stuck with the bug reports.&lt;lb/&gt; * You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They continue to ship an old bcachefs for (at least) the rest of the current release cycle, but eventually it bitrots and they delete it. You might or might not choose to ship it out-of-tree like ZFS, and various distros might or might not package some version of it for you (whether you want them to or not).&lt;lb/&gt; * You accept that you cannot control what appears in Linus's tree, and decide to cease all engagement with him and the rest of the kernel folks. They fork bcachefs or mirror it from your out-of-tree version, and slightly-old or modified versions continue to appear in the kernel indefinitely. As I explained, I think this is less likely, but I don't want to entirely discount it.&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;lb/&gt; vm.dirty_background_ratio = 50&lt;lb/&gt; vm.dirty_expire_centisecs = 360000&lt;lb/&gt; vm.dirty_writeback_centisecs = 60000&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;lb/&gt; I hope that you get my point of listening well in order to carefully heal the relationships.&lt;lb/&gt; You explained that it was a bad idea, for multiple reasons: statically linked dependencies, and&lt;lb/&gt; invalidating all your active testing.&lt;lb/&gt; which broke the build for Debian, and file system users stopped getting updates.&lt;lb/&gt; You wrote many items into one message. I answered only for the first one,&lt;lb/&gt; to keep the answer small enough. Some progress, but further messages&lt;lb/&gt; could increase coverage.&lt;lb/&gt; The unfortunate end result was, that Debian users had problems with the bug.&lt;lb/&gt; I didn't get from your message, the outcome of the relationships between persons:&lt;lb/&gt; whether personal relationships were worsened, stayed the same, or healed in the&lt;lb/&gt; end (each relation individually).&lt;lb/&gt; https://www.verywellmind.com/what-is-active-listening-302...&lt;lb/&gt; "Paraphrasing and reflecting back what has been said"&lt;lb/&gt; ( Those who know psychology, know these things ).&lt;lb/&gt; You could try to restore relationships with just listening others. Choose carefully&lt;lb/&gt; messaging cases, in which you think that you won't cause much backslash,&lt;lb/&gt; but you could have progress with healing the relationship by listening to the other.&lt;lb/&gt; Repeat in nearly the same words the whole complaint, &lt;lb/&gt; so that the other one feels of being heard fully.&lt;lb/&gt; Try to at least have progress, thus please listen carefully the mentioned&lt;lb/&gt; complaint by repeating it. You can have pauses, like answering another day, to reduce the burden.&lt;lb/&gt; Please don't open up any new problems. If you do (I do mistakes sometimes),&lt;lb/&gt; and get a backslash as a heated answer, please listen and repeat it carefully,&lt;lb/&gt; to reduce the impact.&lt;lb/&gt; you could both improve your communication skills,&lt;lb/&gt; and perhaps others could learn from it too,&lt;lb/&gt; and perhaps then relations with other stakeholders, like maintainers,&lt;lb/&gt; and Linus, could be restored into a level that you can co-operate efficiently together again.&lt;lb/&gt; I'm trying to improve my communicating skills in the contexts of&lt;lb/&gt; change management for "OWASP top 10", and AI adoption.&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;Debian&lt;/head&gt;&lt;lb/&gt; [1]: https://jonathancarter.org/2024/08/29/orphaning-bcachefs-...&lt;lb/&gt; [2]: https://lwn.net/Articles/835599/&lt;lb/&gt; For version history:&lt;lb/&gt; https://crates.io/crates/bindgen/versions?sort=semver&lt;lb/&gt; https://tracker.debian.org/pkg/rust-bindgen-cl&lt;head&gt;Debian&lt;/head&gt;&lt;head&gt;Debian&lt;/head&gt;&lt;head&gt;Debian&lt;/head&gt;&lt;head&gt;Debian&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;lb/&gt; Absolutely. Even in a thread where the discussion was explicitly about how nice it is to see people taking the high road, Kent waltzes in and starts lobbing grenades. He is a passionate curator of other people's faults but I have never seen him confess to his own--even when confronted with significant evidence. If there is a problem, the blame lies elsewhere by definition in his world. Watching him burn the bridges that allow me to use bcachefs and then claiming to care about his users has really started to rub me the wrong way. Do what you want but stop telling us your choices are other people's fault. Kent fights for one person. That is more of a passion for him than his filesystem and that sucks (for me). Kent is the scorpion to my frog.&lt;lb/&gt; Bcachefs is a pretty great filesystem. I wish I could keep using it.&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;A few suggestions (which you don’t have to follow)&lt;/head&gt;&lt;head&gt;A few suggestions (which you don’t have to follow)&lt;/head&gt;&lt;head&gt;So what exactly *is* in the cards, then?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;lb/&gt; https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/...&lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;lb/&gt; Thank you for pointing me to that blog post!&lt;lb/&gt; My phone indeed has an odd number of cores.&lt;head/&gt; Fascinating - this is a whole new thing to me, but I guess I'm also "vulnerable", since I have a Pixel 8 Pro with 9 cores! &lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;A broken link?&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;lb/&gt; Maybe the E-Mail has a typo ... b -&amp;gt; v ???&lt;lb/&gt; but when 6.15 is EoL and one is recommended to change to 6.16&lt;lb/&gt; just before a version with just ext4 bugs fixed ... and still further&lt;lb/&gt; ext4 bugs fixed after that ... it seem to be a general problem of&lt;lb/&gt; quality concerning Linux FSs / automatic testing not in good shape.&lt;lb/&gt; which is the topic here.&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;FS code quality of Linux seems not to be as one would wish for ...&lt;/head&gt;&lt;head&gt;Why not removed?&lt;/head&gt;&lt;lb/&gt; This is the most harmful (to the users) thing that could have been done now.&lt;head&gt;Why not removed?&lt;/head&gt;&lt;head&gt;Why not removed?&lt;/head&gt;&lt;lb/&gt; 1. Minimal result: Users can continue to use bcachefs with newer kernels without having to patch the kernel, they just won't get bug fixes, but there will be no functional regression&lt;lb/&gt; 2. Improvement with some effort by users: Users willing to patch the kernel can still apply any patches provided by Kent&lt;lb/&gt; 3. Optimal result: A unicorn with the ability to work with Linus and Kent at the same time may appear, resulting in fixes from Kent being merged with the timing and criteria wanted by Linus&lt;head&gt;Not so bad&lt;/head&gt;&lt;head&gt;Winning the battles, losing the war&lt;/head&gt;&lt;head&gt;Winning the battles, losing the war&lt;/head&gt;&lt;head&gt;Mediation?&lt;/head&gt;&lt;head&gt;Mediation?&lt;/head&gt;&lt;head&gt;Are Linus' patches posted to the mailing lists?&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45074312</guid></item><item><title>AI models need a virtual machine</title><link>https://blog.sigplan.org/2025/08/29/ai-models-need-a-virtual-machine/</link><description>&lt;doc fingerprint="bd5fd5d284b383c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Applications using AI embed the AI model in a framework that interfaces between the model and the rest of the system, providing needed services such as tool calling, context retrieval, etc. Software for early chatbots took user input, called the LLM, and returned the result to the user; essentially just a read-eval-print loop. But, as the capabilities of LLMs have evolved and extension mechanisms, such as MCP were defined, the complexities of the control software that calls the LLM have increased. AI software systems require the same qualities that an operating system provides, including security, isolation, extensibility, and portability. For example, when an AI model needs to be given a file as part of its context, access control must be established that determines if the model should be allowed to view that file. We believe it is time to consider standardizing the ways in which the AI models are embedded into software and think of that control software layer as a virtual machine, where one of the machine instructions, albeit a super-powerful one, is to call the LLM.&lt;/p&gt;
    &lt;p&gt;Our approach decouples model development from integration logic, allowing any model to “plug in” to a rich software ecosystem that includes tools, security controls, memory abstractions, etc. Similar to the impact that the Java Virtual Machine had, creating a specification of a VM for the AI orchestrator could enable a “write once, run anywhere” execution environment for AI models while at the same time providing familiar constraints and governance to maintain security and privacy in existing software systems. Below we outline related work in this direction, the motivation behind it, and the key benefits of an AI Model VM.&lt;/p&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;AI models are being leveraged in existing software as application copilots, embedded in IDEs, and with the rise of the MCP protocol, are increasingly able to use tools, implement agents, etc. This rapid evolution of valuable use cases brings with it a greater need to ensure that the AI-powered applications maintain privacy, are secure, and operate correctly. Guarantees of security and privacy are best provided if the underlying system is secure by design and not added on to systems as an afterthought. We take the Java Virtual Machine (JVM) as our inspiration in making the case for the importance of a standard AI Virtual Machine. The Java Virtual Machine guarantees memory safety by design, defines access control policies, and prevents code injection with bytecode verification. These properties allow Java programs running on the JVM to be executed with trust despite being shipped remotely, enabling “write once, run anywhere” software distribution.&lt;/p&gt;
    &lt;p&gt;How does the JVM relate to applications that use AI models? We used the following example to explain:&lt;/p&gt;
    &lt;p&gt;The diagram illustrates the role of the software layer that interacts with an AI model, which we call the Model Virtual Machine (MVM). That layer intermediates between the model and the rest of the world. For example, a chatbot user might type a prompt (1) that the MVM then sends unmodified to the AI model (2). In practice, the MVM will add additional context, including the system prompt, chat history, to the AI model input as well. The AI model generates a response, which in the example requires a specific tool to be called (3). This response has a specific format that is mutually agreed upon between the model and the MVM, such as MCP. In our example, because it is important to restrict the model from making undesired tool calls, the MVM first consults the list of allowed tools (4) before deciding to call the tool the model requested (5). This check (4) guarantees that the model doesn’t make unauthorized tool calls. Every commercial system using AI models requires some version of this control software.&lt;/p&gt;
    &lt;p&gt;We make the analogy that the interface with the LLM should be a virtual machine. If that is the case, what are the instructions that the machine can execute? Here are examples of operations that existing AI model interfaces have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Certifying, loading, initializing, and unloading a given AI model&lt;/item&gt;
      &lt;item&gt;Calling a model with context&lt;/item&gt;
      &lt;item&gt;Parsing the output from the model&lt;/item&gt;
      &lt;item&gt;Certifying, loading, initializing, and unloading tools&lt;/item&gt;
      &lt;item&gt;Calling a tool&lt;/item&gt;
      &lt;item&gt;Parsing the results from a tool call&lt;/item&gt;
      &lt;item&gt;Storing the results from a tool call into memory&lt;/item&gt;
      &lt;item&gt;Asking the user for input&lt;/item&gt;
      &lt;item&gt;Adding content to a history memory&lt;/item&gt;
      &lt;item&gt;Standard control constructs such as conditionals, sequencing, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A VM would support all of these operations in a well-typed context where constraints are placed on the calls made, the arguments passed, etc.&lt;/p&gt;
    &lt;p&gt;Existing Work Informs What is Needed&lt;/p&gt;
    &lt;p&gt;Some of the required elements of a well-specified interface are emerging in AI systems explored in academic work and in applications that are widely deployed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI’s Structured Tool Calling Protocols: OpenAI introduced a JSON-based function calling API that lets models invoke code-defined functions in a structured way. This approach, along with OpenAI’s plugin system (which uses OpenAPI specifications for tools), showed how structured tool-calling protocols can reduce ambiguity and simplify integration.&lt;/item&gt;
      &lt;item&gt;Anthropic’s Model Context Protocol (MCP, 2024): MCP is an open protocol for connecting AI assistants to external data and tools, explicitly aiming to be a universal interface. “Think of MCP like a USB-C port for AI applications,” Anthropic explains. Instead of every service having a custom AI integration, MCP provides a common schema and client-server approach. Despite being relatively new, MCP adoption, including in large companies, has been rapid.&lt;/item&gt;
      &lt;item&gt;Secure Orchestrators – FIDES &amp;amp; AC4A (2025): Security remains a weak point in current AI systems. Two recent projects propose runtime-level controls. FIDES (by Microsoft Research) enforces information-flow policies on agents by tracking data confidentiality labels and adding new agent actions like “inspect” to limit what agents can access (where a quarantined LLM can safely summarize restricted data) (paper). AC4A (Access Control for Agents) (manuscript in preparation) takes an OS-style approach: All tools and data are organized into hierarchies (like files and folders), and the agent must request read/write access for each resource. AC4A’s runtime intercepts every agent action and blocks anything not permitted, forcing a least-privilege operation mode. These projects show how a standard AI VM could include built-in security and access control, just as modern operating systems do. Even with strong access controls built into a VM specification, AI models present new security challenges that need to be considered in the design. For example, an AI model, when prevented from accessing a particular item of data, might use its chain-of-thought reasoning to devise ways to gather accessible data that allows it to infer the inaccessible item. As such, security researchers have to devise new mitigations to prevent AI models taking adversarial actions even with the virtual machine constraints.&lt;/item&gt;
      &lt;item&gt;Open-Source Agent Runtimes: Several projects are actively building general-purpose runtimes for AI. For example, langchain and Semantic Kernel provide numerous common runtime services that make writing reliable AI-enabled applications easier. The AI Controller Interface (AICI) (later renamed llguidance), integrates a lightweight VM into the model-serving pipeline, allowing developers to script and constrain model behavior at a low level (e.g., control of generations token-by-token).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Defining a specification for a VM interface for AI systems from these emerging approaches will require more than an agreement on protocols and APIs. Because AI systems derive their behavior from training data, model training data must reflect the specification of the VM interface so that the models and the VM model interface can co-evolve. This will enable otherwise diverse models to exhibit broadly compatible behavior with respect to the VM interface specification.&lt;/p&gt;
    &lt;p&gt;Benefits of a Well-Specified AI Model VM&lt;/p&gt;
    &lt;p&gt;As mentioned, many applications that leverage AI models require reliability, privacy, and security. In addition, new models are developed almost daily and updating the model being used by an application is often necessary. Given this confluence of factors, creating robust AI software presents significant engineering challenges. We believe that a specification of the interface between the AI model and the surrounding software that interfaces to it will address some of these challenges.&lt;/p&gt;
    &lt;p&gt;The need for an AI Model VM specification is driven by several clear motivations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Separation of Concerns: An interface specification enforces a clean separation between model logic and integration logic. This means models become interchangeable components. You could swap in a new model (or move an agent to a different platform) and, as long as both adhere to the standard, everything still works. Likewise, virtual machine implementors can increase the performance, security, and tooling of the virtual machine while maintaining compatibility with the AI model interfaces.&lt;/item&gt;
      &lt;item&gt;Built-in Safety and Governance: A VM specification can enforce safety by design. By routing all tool usage and external access through a well-defined interface, it becomes easier to apply permission checks, audit logs, and fail-safes. As shown by projects like AC4A, the VM can act as a gatekeeper, restricting what models can do unless explicitly authorized. This creates a safer deployment solution for powerful AI systems: even if the model behaves unpredictably, the VM layer can contain its effects. Standards bodies could even define security requirements (e.g., certain calls must always require user confirmation), creating a shared foundation of trust. Similar to the benefits of signed assemblies in the Common Language Runtime, have a certification process around loading and unloading models and tools ensures the end-to-end security of the supply chain.&lt;/item&gt;
      &lt;item&gt;Transparent Performance &amp;amp; Resource Tracking: A VM specification could also give developers visibility to runtime diagnostics. Post-execution manifests could report model performance, resource consumption, and data access level which helps developers evaluate overall efficiency and performance. Benchmarks for accuracy, utility, and responsiveness can be supported directly in the VM interface across models and platforms.&lt;/item&gt;
      &lt;item&gt;Verifiability of Model Output: Leveraging a VM specification, experts can explore integrating formal methods to verify their model behavior. Techniques such as zero-knowledge proofs could confirm the integrity of model outputs without sensitive internal logic. While still emerging, this possibility hints at new levels of trust and accountability in AI systems and should be carefully considered during development.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;We argue that a well-specified AI Model Virtual Machine is needed. Developments occurring in multiple directions, including work from tech companies, startups, and academia, all motivate the need for a VM specification that lets AI models safely and seamlessly interact with the world around them. The motivation is clear – reducing complexity and unlocking interoperability – and the potential benefits range from technical (faster development, modular upgrades) to strategic (cross-platform AI ecosystems, improved safety). From enforcing controls for security and privacy, to potentially formal proof capabilities for trust, the opportunities are wide-ranging. Learning a lesson from older generations of software virtualization, a VM specification can increase AI systems portability, interoperability, security, and reliability. The purpose of this document is to highlight these issues and start engaging with the community on building a consensus that such a specification is needed and what it should include.&lt;/p&gt;
    &lt;p&gt;Biographies:&lt;/p&gt;
    &lt;p&gt;Shraddha Barke is a Senior Researcher at Microsoft Research in Redmond, Washington in the Research in Software Engineering (RiSE) group. Her research interests include AI for proof generation, training AI models for program-reasoning tasks using RL and improving the reliability of AI agents.&lt;/p&gt;
    &lt;p&gt;Betül Durak is a Principal Researcher at Microsoft Research in Redmond, Washington in Security, Privacy, and Cryptography group. Her research interests broadly include security analysis as well as secure and private protocol designs motivated from real world problems.&lt;/p&gt;
    &lt;p&gt;Dan Grossman is a Professor at the University of Washington and the Vice Director of the Paul G. Allen School of Computer Science &amp;amp; Engineering. His research interests are in programming languages, particularly in applying programming languages concepts and analyses to emerging domains.&lt;/p&gt;
    &lt;p&gt;Peli de Halleux is a Principal Research Software Developer Engineer in Redmond, Washington working in the Research in Software Engineering (RiSE) group. His research interests include empowering individuals to build LLM-powered applications more efficiently.&lt;/p&gt;
    &lt;p&gt;Emre Kıcıman is a Senior Principal Research Manager and Head of Research for Copilot Tuning at Microsoft. His research interests include causal methods, the security of AI, and applications of LLM and AI-based systems, together with their implications for people and society.&lt;/p&gt;
    &lt;p&gt;Reshabh K Sharma is a PhD student at the University of Washington. His research lies at the intersection of PL/SE and LLMs, focusing on developing infrastructure and tools to create better LLM-based system that are easier to develop reliably and correctly.&lt;/p&gt;
    &lt;p&gt;Ben Zorn is a Partner Researcher at Microsoft Research in Redmond, Washington working in (and previously having co-managed) the Research in Software Engineering (RiSE) group. His research interests include programming language design and implementation, end-user programing, and AI software including technology for ensuring responsible AI.&lt;/p&gt;
    &lt;p&gt;Disclaimer: These posts are written by individual contributors to share their thoughts on the SIGPLAN blog for the benefit of the community. Any views or opinions represented in this blog are personal, belong solely to the blog author and do not represent those of ACM SIGPLAN or its parent organization, ACM.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45074467</guid></item><item><title>Condor's Cuzco RISC-V Core at Hot Chips 2025</title><link>https://chipsandcheese.com/p/condors-cuzco-risc-v-core-at-hot</link><description>&lt;doc fingerprint="c2da901fa4165780"&gt;
  &lt;main&gt;
    &lt;p&gt;Condor Computing, a subsidiary of Andes Technology that creates licensable RISC-V cores, has a business model with parallels to Arm (the company) and SiFive. Andes formed Condor in 2023, so Condor is a relatively young player on the RISC-V scene. However, Andes does have RISC-V design experience prior to Condor’s formation with a few RISC-V cores under their belt from years past.&lt;/p&gt;
    &lt;p&gt;Condor is presenting their Cuzco core at Hot Chips 2025. This core is a heavyweight within the RISC-V scene, with wide out-of-order execution and a modern branch predictor and some new time based tricks. It’s in the same segment as high performance RISC-V designs like SiFive’s P870 and Veyron’s V1. Like those cores, Cuzco should stand head and shoulders above currently in-silicon RISC-V cores like Alibaba T-HEAD’s C910 and SiFive’s P550.&lt;/p&gt;
    &lt;p&gt;Besides being a wide out-of-order design, Cuzco uses mostly static scheduling in the backend to save power and reduce complexity. Condor calls this a “time-based” scheduling scheme. I’ll cover more on this later, but it’s important to note that this is purely an implementation detail. It doesn’t require ISA modifications or special treatment from the compiler for optimal performance.&lt;/p&gt;
    &lt;head rend="h1"&gt;Core Overview&lt;/head&gt;
    &lt;p&gt;Cuzco is a 8-wide out-of-order core with a 256 entry ROB and clock speed targets around 2 GHz SS (Slow-Slow) to 2.5 GHz (Typical-Typical) on TSMC’s 5nm process. The pipeline has 12 stages counting from instruction fetch to data cache access completion. However, a 10 cycle mispredict penalty probably more accurately describes the core’s pipeline length relative to its competitors.&lt;/p&gt;
    &lt;p&gt;As a licensed core, Cuzco is meant to be highly configurable to widen its target market. The core is built from a variable number of execution slices. Customization options also include L2 TLB size, off-cluster bus widths, and L2/L3 capacity. Condor can also adjust the size of various internal core structures to meet customer performance requirements. Cuzco cores are arranged into clusters with up to eight cores. Clusters interface with the system via a CHI bus, so customers can bring their own network-on-chip (NoC) to hit higher core counts via multi-cluster setups.&lt;/p&gt;
    &lt;head rend="h1"&gt;Frontend&lt;/head&gt;
    &lt;p&gt;Cuzco’s frontend starts with a sophisticated branch predictor, as is typical for modern cores targeting any reasonable performance level. Conditional branches are handled via a TAGE-SC-L predictor. TAGE stands for Tagged Geometric, a technique that uses multiple tables each handling a different history length. It seeks to efficiently use branch predictor storage by selecting the most appropriate history length for each branch, as opposed to older techniques that use a fixed history length. The SC (Statistical Corrector) part handles the small subset of branches where TAGE doesn’t work well, and can invert the prediction if it sees TAGE often getting things wrong under certain circumstances. Finally, L indicates a loop predictor. A loop predictor is simply a set of counters that come into play for branches that are taken a certain number of times, then not taken once. If the branch predictor detects such loop behavior, the loop predictor can let it avoid mispredicting on the last iteration of the loop. Basically, TAGE-SC-L is an augmented version of the basic TAGE predictor.&lt;/p&gt;
    &lt;p&gt;AMD’s Zen 2, Ampere’s AmpereOne, and Qualcomm’s Oryon also use TAGE predictors of some sort, and achieve excellent branch prediction accuracy. AMD, Ampere, and Qualcomm also likely augment the basic TAGE prediction strategy in some way. How Cuzco’s TAGE predictor performs will depend on how large its history tables are, as well as how well the predictor is tuned (selection of index vs tag bits, history lengths, distribution of storage budget across TAGE tables, etc). For Cuzco’s part, they’ve disclosed that the TAGE predictor’s base component uses a 16K entry table of bimodal counters.&lt;/p&gt;
    &lt;p&gt;Branch target caching on Cuzco is provided by a 8K entry branch target buffer (BTB) split into two levels. Condor’s slides show the BTB hit/miss occurring on the cycle after instruction cache access starts, so a taken branch likely creates a single pipeline bubble. Returns are predicted using a 32 entry return stack. Cuzco also has an indirect branch predictor, which is typical on modern CPUs.&lt;/p&gt;
    &lt;p&gt;Cuzco’s instruction fetch logic feeds from a 64 KB 8-way set associative instruction cache, and speeds up address translations with a 64 entry fully associative TLB. The instruction fetch stages pull an entire 64B cacheline into the ICQ (instruction cache queue), and then pull instructions from that into an instruction queue (XIQ). The decoders feed from the XIQ, and can handle up to eight instructions per cycle.&lt;/p&gt;
    &lt;head rend="h1"&gt;Rename and Allocate&lt;/head&gt;
    &lt;p&gt;Much of the action in Condor’s presentation relates to the rename and allocate stage, which acts as a bridge between the frontend and out-of-order backend. In most out-of-order cores, the renamer carries out register renaming and allocates resources in the backend. Then, the backend dynamically schedules instructions as their dependencies become available. Cuzco’s renamer goes a step further and predicts instruction schedules as well.&lt;/p&gt;
    &lt;p&gt;One parallel to this is Nvidia’s static scheduling in Kepler and subsequent GPU architectures. Both simplify scheduling by telling an instruction to execute a certain number of cycles in the future, rather than having hardware dynamically check for dependencies. But Nvidia does this in their compiler because GPU ISAs aren’t standardized. Cuzco still uses hardware to create dynamic schedules, but moves that job into the rename/allocate stage rather than the schedulers in the backend. Schedulers can be expensive structures in conventional out-of-order CPUs, because they have to check whether instructions are ready to execute every cycle. On Cuzco, the backend schedulers can simply wait a specified number of cycles, and then issue an instruction knowing the dependencies will be ready by then.&lt;/p&gt;
    &lt;p&gt;To carry out time-based scheduling, Cuzco maintains a Time Resource Matrix (TRM), which tracks utilization of various resources like execution ports, functional units, and data buses for a certain number of cycles in the future. The TRM can look 256 cycles into the future, which keeps storage requirements under control. Because searching a 256 row matrix in hardware would be extremely expensive, Cuzco only looks for available resources in a small window after an instruction’s dependencies are predicted to be ready. Condor found searching a window of eight cycles provided a good tradeoff. Because the renamer can handle up to eight instructions, it at most has to access 64 rows in the TRM per cycle. If the renamer can’t find free resources in the search window, the instruction will be stalled at the ID2 stage.&lt;/p&gt;
    &lt;p&gt;Another potential limitation is the TRM size, which could be a limitation for long latency instructions. However, the longest latency instructions tend to be loads that miss cache. Cuzco always assumes a L1D hit for TRM scheduling, and uses replay to handle L1D misses. That means stalls at ID2 from TRM size limitations should also be rare.&lt;/p&gt;
    &lt;p&gt;Compared to a hypothetical “greedy” setup, where the core is able to create a perfect schedule with execution resource limitations in mind, limiting the TRM search window decreases performance by a few percent. Condor notes that creating a core to match the “greedy” figure may not even be possible. A conventional out-of-order core wouldn’t have TRM-related restrictions, but may face difficulties creating an optimal schedule for other reasons. For example, a distributed scheduler may have several micro-ops become ready in one scheduling queue, and face “false” delays even though free execution units may be available on other scheduling queues.&lt;/p&gt;
    &lt;p&gt;Static scheduling only works when instruction latencies are known ahead of time. Some instructions have variable latency, like loads that can miss caches or TLBs, encounter bank conflicts, or require store forwarding. As mentioned before, Cuzco uses instruction replay to handle variable latency instructions and the associated dynamic behavior. The renamer does take some measures to reduce replays, like checking to see if a load gets its address from the same register as a prior store. However, it doesn’t attempt to predict memory dependencies like Intel’s Core 2, and also doesn’t try to predict whether a load will miss cache.&lt;/p&gt;
    &lt;head rend="h1"&gt;Out-of-Order Backend&lt;/head&gt;
    &lt;p&gt;Out of order execution in Cuzco is relatively simple, because the rename/allocate stage takes care of figuring out when instructions will execute. Each instruction is simply held within the schedulers until a specified number of cycles pass, after which it’s sent for execution. If the rename/allocate stage guesses wrong, replay gets handled via “poison” bits. The erroneously executed instruction’s result data is effectively marked as poisoned, and any instructions consuming that data will get re-executed. Replaying instructions costs power and wastes execution throughput, so replays should ideally be a rare event. 70.07 replays per 1000 instructions feels like a bit of a high figure, but likely isn’t a major problem because execution resources are rarely a limitation in an out-of-order core. Taking about 7% more execution resources may be an acceptable tradeoff, considering most modern chips rarely use their core width in a sustained fashion.&lt;/p&gt;
    &lt;p&gt;Execution resources are grouped into slices, each of which have a pair of pipelines. A slice can execute all of the core’s supported RISC-V instructions, making it easy to scale execution resources by changing slice count. Each slice consists of a set of execution queues (XEQs), which hold micro-ops waiting for a functional unit. Cuzco has XEQs per functional unit, unlike conventional designs that tend to have a scheduling queue that feeds all functional units attached to an execution port. Four register read ports supply operands to the slice, and two write ports handle result writeback. Bus conflicts are handled by the TRM as well. A slice cannot execute more than two micro-ops per cycle, even doing so would not oversubscribe the register read ports. For example, a slice can’t issue an integer add, a branch, and a load in the same cycle even though that would only require four register inputs.&lt;/p&gt;
    &lt;p&gt;XEQs are sized to match workload characteristics, much like tuning a distributed scheduler. While XEQ sizes can be set to match customer requirements, Condor was able to give some figures for a baseline configuration. ALUs get 16 entry queues, while branches and address generation units (LS) get 8 entry queues. XEQ sizes are adjustable in powers of two, from 2 to 32 entries. There’s generally a single cycle of latency for forwarding between slices. The core can be configured to do zero cycle cross-slice forwarding, but that would be quite difficult to pull off.&lt;/p&gt;
    &lt;p&gt;On the vector side, Cuzco supports 256/512-bit VLENs via multiple micro-ops, which are distributed across the execution slices. Execution units are natively 64 bits wide. There’s one FMA unit per slice, so peak FP32 throughput is eight FMA operations per cycle, or 16 FLOPS when counting the add and multiply as separate operations. FP adds execute with 2 cycle latency, while FP multiplies and multiply-adds have four cycle latency. The two cycle FP add latency is nice to see, and matches recent cores like Neoverse N1 and Intel’s Golden Cove, albeit at much lower clocks.&lt;/p&gt;
    &lt;head rend="h1"&gt;Load/Store&lt;/head&gt;
    &lt;p&gt;Cuzco’s load/store unit has a 64 entry load queue, a 64 entry store queue, and a 64 entry queue for data cache misses. Loads can leave the load queue after accessing the data cache, likely creating behavior similar to AMD’s Zen series where the out-of-order backend can have far more loads pending retirement than the documented load queue capacity would suggest. The core has four load/store pipelines in a four slice configuration, or one pipeline per slice. Maximum load bandwidth is 64B/cycle, achievable with vector loads.&lt;/p&gt;
    &lt;p&gt;The L1D is physically indexed and physically addressed (PIPT), so address translation has to complete before L1D access.To speed up address translation, Cuzco has a 64 entry fully associative data TLB. The L2 TLB is 4-way set associative, and can have 1K, 2K, or 4K entries. Cuzco’s core private, unified L2 cache has configurable capacity as well. An example 2 MB L2 occupies 1.04 mm2 on TSMC 5nm.&lt;/p&gt;
    &lt;p&gt;Eight cores per cluster share a L3 cache, which is split into slices to handle bandwidth demands from multiple cores. Each slice can deliver 64B/cycle, and slice count matches core count. Thus Cuzco enjoys 64B/cycle of load bandwidth throughout the cache hierarchy, of course with the caveat that L3 bandwidth may be lower if accesses from different cores clash into the same slice. Cores and L3 slices within a cluster are linked by a crossbar. The L3 cache can run at up to core clock. Requests to the system head out through a 64B/cycle CHI interface. System topology beyond the cluster is up to the implementer.&lt;/p&gt;
    &lt;p&gt;Replays for cache misses are carried out by rescheduling the data consumer to a later time when data is predicted to be ready. Thus a L3 hit would cause a consuming instruction to be executed three times - once for the predicted L1D hit, once for the predicted L2 hit, and a final time for the L3 hit with the correct data.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;High performance CPU design has settled down over the past couple decades, and converged on an out-of-order execution model. There’s no denying that out-of-order execution is difficult. Numerous alternatives have been tried through the years but didn’t have staying power. Intel’s Itanium sought to use an ISA-based approach, but failed to unseat the company’s own x86 cores that used out-of-order execution. Nvidia’s Denver tried to dynamically compile ARM instructions into microcode bundles, but that approach was not carried forward. All successful high performance designs today generally use the same out-of-order execution strategy, albeit with plenty of variation. That’s driven by the requirements of ISA compatibility, and the need to deliver high single threaded performance across a broad range of applications. Breaking from the mould is obviously fraught with peril.&lt;/p&gt;
    &lt;p&gt;Condor seeks to break from the mould, but does so deep in the core in a way that should be invisible to software a functional perspective, and mostly invisible from a performance perspective. The core runs RISC-V instructions and thus benefits from that software ecosystem, unlike Itanium. It doesn’t rely on a compiled microcode cache like Denver, so it doesn’t end up running in a degraded performance beyond what a typical OoO core would see when dealing with poor code locality. Finally, instruction replay effectively creates dynamic schedules and handles cache misses&lt;/p&gt;
    &lt;p&gt;If you like the content then consider heading over to the Patreon or PayPal if you want to toss a few bucks to Chips and Cheese. Also consider joining the Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45074895</guid></item><item><title>You Have to Feel It</title><link>https://mitchellh.com/writing/feel-it</link><description>&lt;doc fingerprint="b887f3b997e7a4e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mitchell Hashimoto&lt;/head&gt;
    &lt;head rend="h1"&gt;You Have to Feel It&lt;/head&gt;
    &lt;p&gt;You see a series of checkboxes checked. Schedules met. Requirements satisfied. Demos delivered. It's a good day. Good job, you, good job! A promotion is in sight.&lt;/p&gt;
    &lt;p&gt;But you didn't feel it. You didn't feel it.&lt;/p&gt;
    &lt;p&gt;We, as people, feel something with every interaction. Frustration, joy, relief, confidence. A feeling. A person interacts with our work. Our work evokes a feeling. The feeling matters. The feeling is part of the work. The desired feeling is part of the requirements.&lt;/p&gt;
    &lt;p&gt;When you feel it, you know. The feature makes you smile when you use it. It fits right in, like it was always meant to be there. You want to use it again. You want to tell people about it.&lt;/p&gt;
    &lt;p&gt;This is the difference. This is what metrics, specifications, and demos miss. They don't capture the feeling. For the people who will use and live in the work, the feeling is part of their daily experience. Which means you can't stop at checking the boxes on paper. You have to sit with it, use it, live with it.&lt;/p&gt;
    &lt;p&gt;You have to feel it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45075048</guid></item><item><title>LandChad, a site dedicated to turning internet peasants into Internet Landlords</title><link>https://landchad.net</link><description>&lt;doc fingerprint="9d8b14078d06c8c"&gt;
  &lt;main&gt;
    &lt;p&gt;This is LandChad.net, a site dedicated to turning internet peasants into Internet Landlords by showing them how to setup websites, email servers, chat servers and everything in between.&lt;/p&gt;
    &lt;p&gt;Starting a website is something that can be done in a lazy afternoon and costs pocket change.&lt;/p&gt;
    &lt;p&gt;Most of the internet’s problems could be solved if more people had their own personal platforms, so the objective of this site is to guide any normal person through the process of installing a website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Start a website&lt;/head&gt;
    &lt;p&gt;This is the basic “course.” Follow these quick tutorials and you’ll have a fully functioning basic web page on the domain name of your choice.&lt;/p&gt;
    &lt;p&gt;â³ This “basic course” can take as little as an hour or even less.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Build your own platform!”&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AlpsAlps is a simple and extensible webmail. It offers a web interface for IMAP, SMTP and other upstream servers.&lt;/item&gt;
      &lt;item&gt;CalibreA public or private digital library.&lt;/item&gt;
      &lt;item&gt;CgitA hyperfast web frontend for git repositories.&lt;/item&gt;
      &lt;item&gt;CoturnA STUN and TURN server that allows users to perform WebRTC calls while being behind NATs.&lt;/item&gt;
      &lt;item&gt;DnsmasqHost your own DNS server to block ads and social media.&lt;/item&gt;
      &lt;item&gt;DokuWikiA simple wiki with clean syntax and no databases.&lt;/item&gt;
      &lt;item&gt;ejabberdA chat server based on XMPP.&lt;/item&gt;
      &lt;item&gt;FosspayA self-hosted payment and donation gateway interfaced with Stripe.&lt;/item&gt;
      &lt;item&gt;Git ServerHosting your own basic git server.&lt;/item&gt;
      &lt;item&gt;GiteaA fully-featured Github-like git website for serious software projects and communities.&lt;/item&gt;
      &lt;item&gt;i2pA private and uncensorable web-layer similar to Tor.&lt;/item&gt;
      &lt;item&gt;IRCSelf-hosting the Internet's classic chat protocol.&lt;/item&gt;
      &lt;item&gt;JitsiVideo-chat software.&lt;/item&gt;
      &lt;item&gt;Matrix DendriteA faster server implementation of Matrix.&lt;/item&gt;
      &lt;item&gt;Matrix SynapseAn encrypted chat server sleek and accessible even to normies.&lt;/item&gt;
      &lt;item&gt;MoneroThe ideal private cryptocurrency for the Internet.&lt;/item&gt;
      &lt;item&gt;Monero NodeContribute to the Monero network.&lt;/item&gt;
      &lt;item&gt;MumbleOpen Source, Low Latency, High Quality Voice Chat.&lt;/item&gt;
      &lt;item&gt;NextcloudA free and private Google Drive-like cloud storage system.&lt;/item&gt;
      &lt;item&gt;PeerTubeYour own self-hosted video-site also compatible with Activity Pub.&lt;/item&gt;
      &lt;item&gt;PleromaA federated Twitter-like microblogging system.&lt;/item&gt;
      &lt;item&gt;ProsodyA minimalist XMPP chat server.&lt;/item&gt;
      &lt;item&gt;RadicaleA private calendar, contact and to-do list system.&lt;/item&gt;
      &lt;item&gt;RainloopA graphical website for accessing a mail server.&lt;/item&gt;
      &lt;item&gt;RSS BridgeCreates RSS feeds for normie sites like Facebook.&lt;/item&gt;
      &lt;item&gt;SearXNGPolls dozens of search engines to give you private and complete search results.&lt;/item&gt;
      &lt;item&gt;TorSet your site up privately on the 'dark web.'&lt;/item&gt;
      &lt;item&gt;TransmissionDecentralized file-sharing with BitTorrent.&lt;/item&gt;
      &lt;item&gt;WireguardFast, Modern, Secure VPN Tunnel&lt;/item&gt;
      &lt;item&gt;YarrA self-hosted, web-based feed aggregator&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Host your own services, social media and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup an Email Server&lt;/head&gt;
    &lt;p&gt;This is the email “course”. Follow these modules to learn how to setup an email server headache free.&lt;/p&gt;
    &lt;p&gt;â³ This entire “email course” may take about an hour. The approval for opening email ports with your VPS should take no less than a day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Maintaining a Server&lt;/head&gt;
    &lt;p&gt;Tips and articles on mastering your server and learning about GNU/Linux systems administration.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Certbot on Standalone Domains and Subdomains&lt;/item&gt;
      &lt;item&gt;Cronjobs&lt;/item&gt;
      &lt;item&gt;GeminiA minimalist alternative to HTTP with a modern twist.&lt;/item&gt;
      &lt;item&gt;Log on with SSH Keys&lt;/item&gt;
      &lt;item&gt;Maintaining a Server&lt;/item&gt;
      &lt;item&gt;OpenAlias&lt;/item&gt;
      &lt;item&gt;Page Quality&lt;/item&gt;
      &lt;item&gt;Requiring Passwords for Webpages (HTTP Authentication)&lt;/item&gt;
      &lt;item&gt;Rsync: Upload and Sync Files and Websites&lt;/item&gt;
      &lt;item&gt;Self hosting&lt;/item&gt;
      &lt;item&gt;Server-Side Scripting with CGI&lt;/item&gt;
      &lt;item&gt;SSH - Advanced Usage&lt;/item&gt;
      &lt;item&gt;Using UFW as a Firewall&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Support LandChad.net&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BTC: &lt;code&gt;bc1q9f3tmkhnxj8gduytdktlcw8yrnx3g028nzzsc5&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;XMR: &lt;code&gt;84RXmrsE7ffCe1ADprxLMHRpmyhZuWYScDR4YghE8pFRFSyLtiZFYwD6EPijVzD3aZiEpg57MfHEr1pGJNPXyJgENMnWrSh&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45075384</guid></item><item><title>New research reveals longevity gains slowing, life expectancy of 100 unlikely</title><link>https://lafollette.wisc.edu/news/new-research-reveals-longevity-gains-slowing-life-expectancy-of-100-unlikely/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45075813</guid></item><item><title>The Default Trap: Why Anthropic's Data Policy Change Matters</title><link>https://natesnewsletter.substack.com/p/the-default-trap-why-anthropics-data</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45076274</guid></item><item><title>Are we decentralized yet?</title><link>https://arewedecentralizedyet.online/</link><description>&lt;doc fingerprint="35d167d9c00e34be"&gt;
  &lt;main&gt;
    &lt;p&gt;This page measures the concentration of user data on the Fediverse and the Atmosphere according to the HerfindahlâHirschman Index (HHI), an indicator from economics used to measure competition between firms in an industry. Mathematically, HHI is the sum of the squares of market shares of all servers.&lt;/p&gt;
    &lt;p&gt;Values close to zero indicate perfectly competitive markets (eg. many servers, with users spread evenly), while values close to 10000 indicate highly concentrated monopolies (eg. most users on a single server). In economics, values below 100 are considered "Highly Competitive", below 1500 is "Unconcentrated", and above 2500 is considered "Highly Concentrated".&lt;/p&gt;
    &lt;p&gt;This site currently measures the concentration of user data for active users: in the Fediverse, this data is on servers (also known as instances); in the Atmosphere, it is on the PDSes that host users' data repos. All PDSes run by the company Bluesky Social PBC are aggregated in this dataset, since they are under the control of a single entity. Similarly, mastodon.social and mastodon.online are combined as they are run by the same company.&lt;/p&gt;
    &lt;p&gt;The location of user data is not the only interesting measure of centralization. On a technical level, there is the network structure (peer to peer, relays, etc.), identity management, the infrastructure on which it is hosted, etc. On a legal level, there are issues regarding the jurisdictions where servers are located, companies are located, etc. On a social level, there are issues around where human power is concentrated in and on the platform, and whether that power is disproportionately held by certain groups. If you would like to help contribute other measures of decentralization, get in touch.&lt;/p&gt;
    &lt;p&gt;Code and data are available on GitHub. Comments and pull requests, including other metrics for measuring distribution and resiliency, are welcome!&lt;/p&gt;
    &lt;p&gt;By Rob Ricci: @ricci@discuss.systems / @ricci.io&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45077291</guid></item><item><title>Why did books start being divided into chapters? A new history</title><link>https://sydneyreviewofbooks.com/reviews/just-a-little-longer</link><description>&lt;doc fingerprint="bbc99a03246082b0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Perhaps it is the inevitable fate of any convention, but literary history does not, it turns out, have many examples of people appreciating great chaptering. In The History of English Prose Rhythm (1912) – one of the sources for James Joyce’s virtuosic-or-unreadable parodies of the evolution of English prose in Ulysses – George Saintsbury remarks on Thomas Malory’s decision to insert a chapter break at a decisive moment in his fifteenth-century Morte d’Arthur. At the end of chapter ten of the Morte, Lancelot rides into a castle, having slayed its gatekeeper, only to hear from the castle’s residents ‘in doors and windows that said “Fair Knight: thou art unhappy.”’ Saintsbury praises Malory’s sense of timing here. The chapter break introduces a pause, leaving those words, as Dames puts it, ‘hovering in the air’. The next chapter begins with Lancelot successfully freeing captives from the prison; as such, the chapter has served to elongate the narrative incident and heighten the tension.&lt;/p&gt;
      &lt;p&gt;The only problem is that this was not Malory’s division, but rather one added by the printer William Caxton (c.1422-92). This fact was only discovered in 1934 when an edition of the Morte predating Caxton was discovered at Winchester College. As it turns out, the Winchester version had no chapters. The modulations of time are the work of Caxton’s specific ‘remediation’. He creates an ‘artful segmentation, a resonant silence, in the printed volume’s visual patterning’. Caxton is paired in this chapter of The Chapter with the anonymous fifteenth-century remediators who transformed Chrétien de Troyes’s great twelfth-century Arthurian verse into prose. Unlike Caxton’s their results are not acclaimed; like the authors of movie novelisations today, they are vulgarisers, profaning the sacred bonds between form and content. In their hands, Chrétien’s flowing verse – praised in Mimesis by Auerbach as ‘light and almost easy’ – is not only segmented with red ink, but also crowded with insistent explanations in the register of narrative history (‘How the king kissed Enide’). Again, in the manner of movie novelisers, moments of introspection are reduced while battle sequences are dilated with a vigour that may equally be judged ‘clumsy technique’ or ‘daring maneuver’. More charitably, we might say these remediators practise what Dames calls, after Roman Jakobson, ‘intralingual translation’ – a phrase that calls back to mind (there it is again!) Davis’ experiment with Sterne. Like Davis, the remediators are working across an historical gap between time-feelings, transforming the internal temporality of Chrétien’s verse to fit their own prosaic times. Dames speculates on the reasons for this transformation. Could it be that the new and uncertain ruling clique in Burgundy – ‘freshly arrived at what would be its historical apex’ – preferred these ‘modes of intense now-time’ to the subtle continuities of Chrétien’s verse? Admitting the possibility of such an ‘ideological effect’, Dames also notes that it is equally likely that these ‘new temporalities’ were simply an ‘accident’. &lt;/p&gt;
      &lt;p&gt;Here one notices a difference between Dames’ previous books and The Chapter, whose broader subject matter perhaps helped it to become a finalist for the National Book Critics Circle Award. As brilliant works of literary history, Amnesiac Selves and The Physiology of the Novel both have the density of specialist knowledge and the sensitivity of immersive textual studies. Each book reconstructs a forgotten discourse: the first book reassembles the understanding of memory in the Victorian period, as explored through close readings of key Victorian novelists and scientific writers; the second builds on this interest by turning to the forgotten paradigm of ‘physiological’ novel theory and its exemplars, the philosopher-scientist-critics GH Lewes, ES Dallas, and Alexander Bain, who explored the embodied rhythms of reading. The physiological basis for a literary theory of form was ultimately swept away by more abstract formalisms espoused, on the one hand, by Henry James and his acolyte Percy Lubbock, and, on the other, by the practico-critical poetics of IA Richards (who effectively banished the novel from the classroom). Though Dames’ close readings in The Chapter are no less attentive and sinuous than in these earlier books, they are perforce more limited by the widened scope. I don’t intend to downplay the brilliance of Dames as a reader of individual texts or as a literary historian. However, as the study twists and turns, the density of historical detail together with the vast scope can at times induce a kind of mental torsion, with the dual impulses to historicise and taxonomise pulling in different directions.&lt;/p&gt;
      &lt;p&gt;In any case, the taxonomic conclusion Dames draws from the Burgundian remediators of Chrétien is that while their clumsy cuts are just that – cuts in a continuous weave – Caxton’s interventions are more like the ‘fade’, offering ‘aeration’ to the narrative text. In this respect Caxton’s edits are oriented not towards reference, but ‘narrative progression and rhythm’. The paradoxical outcome of this intervention is to unify Malory’s text precisely by dividing it; the Morte now comprises ‘semi-discrete moments in a single process, rather than entirely different moments’. Unification-through-division of this sort highlights two logics of narrative time: discontinuous and immersive reading. Chaptering itself comes to generate a ‘feeling of presentness’ by adding white space, a species of visual fermata between narrative actions – ‘emptiness [with] a temporal intensity.’&lt;/p&gt;
      &lt;p&gt;In their evocation of ‘presentness’, blank intensities of this kind recall a much longer-running theological dispute – between Augustine and the great English theologian Bede – on the divisibility of time. Where, after all, is the present? For Augustine, it is impossible to isolate something like ‘presentness’, for it is composed – as he put it in the Confessions – of ‘fugitive moments’, suspended in the future or always being sucked away into the past. The present is thus not measurable by a distinctive unit. Bede, in his eighth-century work The Reckoning of Time, argued to the contrary that there is a ‘minimal’ or ‘atomic’ unit of time. He made his case through a thought experiment. Say you are just about to be punched in the face. As a reflex, you flinch and close your eyes. Between these two moments – that ‘tiniest interval of time in which the lids of our eyes move when a blow is launched’ – is where ‘Bede’s present’ may be found: the atomic unit of presentness. Dames’ point is not that this theological argument directly influenced Caxton and the Burgundian prosateurs, but rather that the disagreement between the two great theologians reflects different investments in literary forms and their relationships to subjectivity in time. For Augustine a poem ‘held entire’ in the mind of a reciter approximates divine omniscience; for Bede, meanwhile, the atomic present is best accessed via a ‘punctuated continuity and directionality’ that might just be the hallmark of well-divided prose – consequently it is ‘seriality, not the transcendence of seriality, [that] is our access to the divine’. It is only in interrupting the present that we are able to perceive it.&lt;/p&gt;
      &lt;p&gt;But it is left to the early novel (as an historian of the form, Dames is candid about this bias) to develop fully the space between Augustine’s durationless void and Bede’s serial present. Leaping forward another two hundred-odd years, then, Dames shows this binary of discontinuous and immersive reading exploding into an array of conceptual possibilities. ‘The eighteenth-century synthesis’, as Dames calls it, spans the period from the picaresque to the first flourishing of the English novel in the middle of the eighteenth century, with the antics of Sterne and Henry Fielding. Functions inherited from older reference-based chapters are here experimentally set in tension with the narrative innovations first explored in the fifteenth-century remediations: the eighteenth-century chapter struggles with the relationship between the strange and the commonplace, the ‘striking and singular’ and the ‘categorizable’. Hence the initial distinction between discontinuous and immersive reading turns out to contain other oppositions that structure it in turn: between space and time; and between the time narrated and the time it takes to narrate or read.&lt;/p&gt;
      &lt;p&gt;Figuring all this is that moment on the staircase from the middle of Tristram Shandy, a kind of novelistic freezeframe, in which Sterne fixes Walter Shandy in place to reflect upon chaptering. In Dames’ account, this metachapter makes explicit the chapter’s full conceptual field: it has a direct address; it narrates both an incident and an interruption. What stands out as the real ‘heart’ of the metachapter is the staircase itself, which serves as a kind of symbolic definition of the chapter’s function. The staircase ‘captures the chapter’s double chronometry, that tension expressed by the simultaneous binaries of space versus time and narrated versus narrating times’. Fielding famously compared his chapters to inns along the road of a long journey, where the reader may ‘stop and take a glass’, but Dames thinks the staircase a better figure. Fielding’s coach trip is merely ‘linear, starting and stopping’; Sterne’s staircase, on the other hand, ‘unpacks two complementary but opposed dimensions’. Walter and Toby head down the stairs, troping narrative progress, while at the same time the sequence of steps and landings displays the segmentation of linearity ‘into discrete stages’. Sterne’s novel is a kind of ‘funhouse mirror’ of temporality: instead of proceeding steadily along a horizontal axis, our temporal schema is thrown down the stairs.&lt;/p&gt;
      &lt;p&gt;Later, in what JGA Pocock once called the ‘second eighteenth century’, the so-called Age of Revolutions, the chapter mutates again. Now ‘elongated’, the chapter is studied in two works that each seem in different ways to dissolve its earlier functions. In The Interesting Narrative of Olaudah Equiano (1789), the famous autobiography of a Nigerian slave who eventually regained his freedom and lived in Britain, Dames observes a mismatch between the protocols of chaptering and the life that these protocols divide up. Equiano’s chapters offer extensive summaries in the manner of a picaresque novel, but seem at the same time to show the inefficacy of that paratextual structure for capturing the experience of domination and eventual manumission. ‘How then to describe the chapter in Equiano, or more bluntly, why bother to do so?’ It is perhaps relevant precisely because the apparent orderliness of chaptering – its ability meaningfully to sculpt time – is shown, against the absolute alienation of slavery, to be unfit for its usual purpose of segmentation. Thus, the intensively expository chapter summaries of the Narrative not only fail to coordinate with the abbreviated summaries in the table of contents, but they also introduce chapters of far greater length (on average, Dames tells us, these are 6,500 words: up to four times longer than is typical for this period). So, then, what is the meaning of this technical decision? ‘To say,’ Dames writes, coming perilously close to ventriloquising Equiano, ‘a life cannot be measured this way, not this kind of life.’&lt;/p&gt;
      &lt;p&gt;As the self-testimony of a former slave, published in the same year as the storming of the Bastille, Equiano’s Narrative is certainly a sign of the times. It is perhaps as iconic a testament to the ‘new epoch’ of the nineteenth century as Girodet’s portrait of Jean-Baptiste Belley, a former slave from Saint-Domingue who would eventually be elected to the French National Convention. ‘New epoch’: this is the legendary, and perhaps apocryphal, phrase of Goethe, uttered in response to the defeat of the Prussians at Valmy in 1792. ‘From this place and from this day a new epoch in world history begins and you can say you were there to see it.’ We might observe that he, for one, did not reach here for the metaphor of the chapter – too ‘partial, fleeting, unhistorical’, according to Dames, to register this period’s epochal shifts. In Goethe’s Wilhem Meister’s Apprenticeship (1795-96), for instance, the chapter becomes even more elongated (one of them is 20,000 words!), doubling in size in the novel’s second half, which was composed after Valmy. Wilhem Meister’s Apprenticeship is a ‘triple turning point’, tying together ‘a world-historical transition, a maturational transition’, and a ‘career transition’ as Goethe, now older and on the other side of the revolution, has to produce fresh material rather than merely revising old writing. It is the very incongruity and ‘dilation’ of the chapter that ‘itself is historical’. Jane Austen’s career is also adduced as an example of the eighteenth century’s passing into the nineteenth, with the three youthful novels drafted in the 1790s averaging chapter lengths of around 2,000 words, while the ‘mature’ novels of the 1810s are nearer 3,500.&lt;/p&gt;
      &lt;p&gt;However sceptical we might like to be about periodisation, and nasty but inevitable grand narratives, it’s observable that history has, well, happened; historical experience makes ‘norms’ normal, and it is potentially why – to return to Davis’ question with which I began – more people still read Austen for pleasure than Smollett, Fielding, Defoe, or, um, John Bunyan. Not unrelatedly, I recently invited some students to read paragraphs from the fourteenth, fifteenth, sixteenth, seventeenth, and eighteenth centuries (respectively, Margery Kempe, Edmund Spenser, Margaret Cavendish, Eliza Haywood, and Sterne: I welcome criticisms of my selections) and one of them said, in so many words, ‘Perhaps some things are forgotten for a reason.’ Perhaps. But we might also wonder: to what extent do novels instruct their readers in how to think, feel, and act? &lt;/p&gt;
      &lt;p&gt;This has been one of the questions that Dames has posed most insistently across his career, with a special emphasis on the contributions of the Victorian novel to readerly subjectivity. At the end of Amnesiac Selves, he speculates on the way that Victorian fiction inculcates a special kind of nostalgia – its warm selective memory is the flipside of the alienating nausea of the historical difference that makes you want to throw a book out the window (or, in homage to Sterne, down the stairs). Yet, as Dames noted then, the cultural prestige of Victorian fiction is ‘increasingly seen in an elegiac manner, as a strange fact that, as the twenty-first century begins, will not last much longer’. As the nineteenth century disappears further and further from view, ‘the Victorians will eventually, if belatedly, make Victorian fiction stranger and less attractive’. Since the publication of Amnesiac Selves in 2000, the Victorians have only receded further away from us in time.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45077735</guid></item><item><title>Six months into tariffs, businesses have no idea how to price anything</title><link>https://www.wsj.com/business/retail/trump-tariff-business-price-impact-37b630c8</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45077937</guid></item><item><title>Are people's bosses making them use AI tools?</title><link>https://piccalil.li/blog/are-peoples-bosses-really-making-them-use-ai/</link><description>&lt;doc fingerprint="f63489551bf52ed3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Are people’s bosses really making them use AI tools?&lt;/head&gt;
    &lt;p&gt;This is not the usual type of content you will have come to expect from Piccalilli, but I feel like this topic, specifically, is an important aspect of our work to cover because as I see it, making or encouraging your development staff to use AI tools in their work is extremely short-sighted and risky.&lt;/p&gt;
    &lt;p&gt;I want to support that stance with some conversations I’ve had with people actually doing the work and their mostly less than favourable experiences.&lt;/p&gt;
    &lt;p&gt;I asked this across question social media:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Is your boss encouraging you to/making you use AI tools for development?&lt;/p&gt;
      &lt;p&gt;I’m thinking about working on a piece about that on Piccalilli.&lt;/p&gt;
      &lt;p&gt;It’s sensitive for sure, so more than happy for people to be anonymised.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The reason I asked was because — as you can imagine — I speak with a lot of developers on a day-to-day basis. A lot of the time in my personal network, these are very experienced, senior developers, but I’m hearing the same stories from juniors too. It boils down to:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;My boss is making/encouraging me to use AI every day and during every part of my work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I had an urge to explore this further — making sure I wasn’t in an echo chamber — and, yeh, you’re probably not going to enjoy what I discovered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Before we dig in, allow me to set some ground rules and factspermalink&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you’re a fan of AI companies/tools, this article is not a personal attack on your preference&lt;/item&gt;
      &lt;item&gt;This article does not de-value the good stuff you might feel like you’re doing with AI&lt;/item&gt;
      &lt;item&gt;All participants are completely anonymised for their privacy and protection&lt;/item&gt;
      &lt;item&gt;I have re-worked some of the responses to assist with point 3&lt;/item&gt;
      &lt;item&gt;Any opinions are mine unless specified&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What I learned from my conversationspermalink&lt;/head&gt;
    &lt;p&gt;I’ve had several conversations with developers and designers working across the industry for this piece, all with varying experience levels.&lt;/p&gt;
    &lt;p&gt;I spoke with a developer working in the science industry who told me, “I saw your post on Bluesky about bosses encouraging AI use. Mine does but in a really weird way. We’re supposed to paste code into ChatGPT and have it make suggestions about structure, performance optimisations”&lt;/p&gt;
    &lt;p&gt;I pressed further and asked if overall this policy is causing problems with the PR processes.&lt;/p&gt;
    &lt;p&gt;In reference to their boss, “It’s mostly frustrating, because they completely externalise the review to ChatGPT. Sometimes they just paste hundreds of lines into a comment and tell the developer to check it. Especially the juniors hit problems because the code doesn’t work anymore and they have trouble debugging it.”&lt;/p&gt;
    &lt;p&gt;“If you ask them technical questions it’s very likely you get a ChatGPT response. Not exactly what I expect from a tech lead.”&lt;/p&gt;
    &lt;p&gt;Immediately, I thought their boss has outsourced their role to ChatGPT, so I asked if that’s the case.&lt;/p&gt;
    &lt;p&gt;“Sounds about right. Same with interview questions for new candidates and we can see a lot of the conversations because the company shares a single ChatGPT account.”&lt;/p&gt;
    &lt;p&gt;I asked for further details and they responded, “People learned to use the chats that disappear after a while.”&lt;/p&gt;
    &lt;p&gt;That’s pretty horrifying, I’ve got to say. Not just some of it, but all of it. Maybe I’m sensitive because I am people’s boss and couldn’t fathom outsourcing my responsibilities to a technology that often gets things completely wrong.&lt;/p&gt;
    &lt;p&gt;Let’s move on to another conversation I had with with a team lead in an agency. Something I have a lot of experience with!&lt;/p&gt;
    &lt;p&gt;“My company is pushing AI tools across the company in branding, copywriting, design, stock photo creation and of course development. They want to be the ‘first AI agency’ and are basically telling us to get on board or you’re not a fit here any longer.”&lt;/p&gt;
    &lt;p&gt;Pretty harrowing stuff. This isn’t much of a surprise to me though so far because unfortunately, a reasonable portion of agencies will do everything to cut corners on a project to increase profits.&lt;/p&gt;
    &lt;p&gt;It’s been that way forever and it’s fundamentally why — understandably — organisations don’t trust agencies. A lot of my work is building that trust in the sales process to counter that.&lt;/p&gt;
    &lt;p&gt;I asked how their agency is currently billing clients and whether it’s retainers or fixed-fees.&lt;/p&gt;
    &lt;p&gt;“We have production clients who are on an overall fixed fee and monthly retainer clients whose contracts are set by the number of hours they want to buy.”&lt;/p&gt;
    &lt;p&gt;Seems like a pretty standard agency setup to me. I wanted to dig deeper on the culture though, so I asked, in reference to “…telling us to get on board or you’re not a fit here any longer” is causing fear amongst their colleagues.&lt;/p&gt;
    &lt;p&gt;They responded, “I would definitely say it’s causing some fear that they aren’t good enough / falling behind if they aren’t using it regularly.”&lt;/p&gt;
    &lt;p&gt;“The managers like to quote ‘AI won’t replace you, but a developer using AI would’ as a way to motivate certain team members to use the tools”&lt;/p&gt;
    &lt;p&gt;Sounds more like a threat to me than a motivation. I asked if their managers are effectively threatening to replace people, not on board with AI and if that worries them.&lt;/p&gt;
    &lt;p&gt;“In a way yeah but I don’t think they could directly get rid of them for that reason, more potentially make it uncomfortable so they’d leave.”&lt;/p&gt;
    &lt;p&gt;“It’s not a great feeling to have and to be honest, I think it’s a wider worry about the industry as a whole, as I feel a lot of agencies will be jumping on the AI train.”&lt;/p&gt;
    &lt;p&gt;“…I do worry about some of my team members and the direction of the company overall — I’m struggling to find the same motivation I had 12-18 months ago”&lt;/p&gt;
    &lt;p&gt;AI use and a culture of encouraging staff to use it isn’t isolated to this one agency. I spoke with a designer at another agency and they said, “Yes I work at a small digital agency and we’re being encouraged to use tools. Not particularly for image generation (outside of ideation or mood-boarding), but more for summaries, research and some copywriting.”&lt;/p&gt;
    &lt;p&gt;“I was very vocal about it at the beginning and think I managed to get a bit of a reputation for being difficult for my views, so I’ve yielded a little bit on certain tools, but still [I’m] very clear about what I think is appropriate for clients and almost certainly make sure we disclose when we use it.”&lt;/p&gt;
    &lt;p&gt;Even though AI tools seemingly aren’t directly used for the creative design output, they’re being used during the creative ideation process which for me, as an agency founder is terrifying, especially with some of the non-disclosure agreements (NDAs) I’ve signed over the years.&lt;/p&gt;
    &lt;p&gt;I also can’t fathom using AI for copywriting either.&lt;/p&gt;
    &lt;p&gt;I also spoke with a software engineer for a huge, global retailer whose organisation is conducting a big push to leverage AI. AI has now become a requirement there. I asked if the organisation has provided correspondence that people’s jobs will be at risk if they’re not on board.&lt;/p&gt;
    &lt;p&gt;“Thankfully I have not seen any wind of it. There is a lot of discussion about how to embrace it [AI] and how we can help the next wave of engineers coming in from colleges to be ready for the transformations that will happen to the way we work.”&lt;/p&gt;
    &lt;p&gt;“I’m sure as the tech matures and we adopt specific tools organisation-wide, those discussions may happen. Right now we are still piloting different tools and figuring out what does and doesn’t work for the organisation.”&lt;/p&gt;
    &lt;p&gt;I asked if this was more of a pragmatic process, rather than a rushed, reactive process.&lt;/p&gt;
    &lt;p&gt;“I think it’s being done with a sense of urgency to embrace new technologies and how they can help us but not to the point where the average engineer would feel overly pressured or threatened by the push.”&lt;/p&gt;
    &lt;p&gt;“I’ve been enjoying my journey into how to leverage AI but I think for newer engineers or engineers looking to climb the ladder, like myself, it inherently adds pressure to be an earlier adopter and be one to spread knowledge early.”&lt;/p&gt;
    &lt;p&gt;I asked if there had been any disasters.&lt;/p&gt;
    &lt;p&gt;“Thankfully nothing in my area and I’m sure any team would try to keep those slip ups under wraps. I will say that in my personal experience little edge case bugs are more prevalent and no matter how careful I am and reread all code over and over I still manage to miss stuff when I don’t manually type it.”&lt;/p&gt;
    &lt;p&gt;“I’m pretty proud of raising solid PRs with low rates of bugs reported, but these last two sprints, I’ve had a few things slip past me due to the change of workflow. I’m sure this is something a mindful engineer won’t struggle with for long as we adjust to the new way of working.”&lt;/p&gt;
    &lt;p&gt;Let’s take a look at one more because I’m aware this article is getting very long. Sorry about that.&lt;/p&gt;
    &lt;p&gt;I had a developer reach out about AI very much being forced in their organisation.&lt;/p&gt;
    &lt;p&gt;“The CTO at my previous job tried Claude Code and really liked it so he said that all the devs had to use Claude Code in our work for generating code, generating tests, debugging, and validating design.”&lt;/p&gt;
    &lt;p&gt;“If we asked him a question on something he would tell us to ask Claude first. I never found Claude useful. It couldn’t debug anything and I didn’t like how carefully I’d have to comb through the code it generated to find the subtle bugs it would inject. The design validation was basically just telling us what we wanted to hear, which the CTO loved”&lt;/p&gt;
    &lt;p&gt;This is the thing about AI tools. They are by design going to honour your prompt, which often results in your AI tool agreeing with you, even if you’re wrong.&lt;/p&gt;
    &lt;p&gt;I asked this person if their boss was effectively off-loading their responsibilities to AI too. I also asked about more information about design validation.&lt;/p&gt;
    &lt;p&gt;“He was trying to off-load responsibilities to Claude but Claude never gave us a good answer so it would just add an extra layer of back-and-forth to solving the problem.”&lt;/p&gt;
    &lt;p&gt;“Design validation is looking for possible performance, security, or concurrency issues in the design of our system. Claude would always have some generic answer that didn’t fit our specific circumstances so it was taken as validation that our design was good”&lt;/p&gt;
    &lt;p&gt;Again, AI tools will validate you, even if you’re wrong.&lt;/p&gt;
    &lt;p&gt;“I think the fact that Claude didn’t have anything real to say about our designs was taken as validation that the design was good.”&lt;/p&gt;
    &lt;p&gt;“There was an assumption that if the design had an issue then Claude would catch that and say something relevant to it. I don’t think he ever considered the possibility that Claude wasn’t saying relevant things because it couldn’t do that. It would only be able to regurgitate generic advice you find on the internet about good software design”&lt;/p&gt;
    &lt;p&gt;A lot of design critique is based in analytical, creative and soft skills, along with lots of experience. AI is completely incapable of doing all of that.&lt;/p&gt;
    &lt;p&gt;LLM is an acronym for large language model and that is exactly what these tools are — language processing systems. So am I surprised that Claude is just regurgitating generic design advice? No.&lt;/p&gt;
    &lt;p&gt;These tools are incapable of creating and analysing. They are only capable of pattern matching and regurgitating what has been fed into them during the training process.&lt;/p&gt;
    &lt;p&gt;Remember when Google’s AI summaries were encouraging people to add glue to their pizza sauce, for example? Yeh, that was a Reddit joke comment which has now been removed but it’s a good example of AI regurgitating what it has learned without to capability of determining the fact that comment was a joke.&lt;/p&gt;
    &lt;p&gt;Regurgiting, not creating. It’s what these tools do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some advice for navigating all of thispermalink&lt;/head&gt;
    &lt;p&gt;I’d say my overarching advice, based on how difficult tech recruitment is right now, is to sadly play along. But — and I cannot stress this enough — make sure you document everything.&lt;/p&gt;
    &lt;p&gt;What I mean by that is every single time AI tools cause problems, slow-downs and other disappointing outcomes, document that outcome and who was responsible for that decision. Make sure you document your opposition and professional advice too.&lt;/p&gt;
    &lt;p&gt;In fact, away from AI, I’d recommend documenting this stuff in general. It’ll be vital if you ever find yourself in a disciplinary and/or tribunal situation. You don’t know where the code your tool spat out came from either. Documenting who is responsible helps to protect you, individually, if litigation is raised against your organisation. If it is not your decision to use these tools, make that known, officially.&lt;/p&gt;
    &lt;p&gt;It’s quite clear we’re in a bubble with AI, or at best, a hype cycle. For example 95% of generative AI pilots failed in a recent report from MIT and even when developers thought AI was making them faster, it was actually making them slower in another study.&lt;/p&gt;
    &lt;p&gt;Billions are being poured into this technology but big tech companies — the so-called “magnificent 7” (cringe) — can afford those losses, generally. Ethics from decision makers are clearly not even a factor any more in these organisations too.&lt;/p&gt;
    &lt;p&gt;My worry is that — as always — workers will be the ones to suffer as the bubble/hype cycle bursts. What I’m directly advising you to do is protect your interests right now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping uppermalink&lt;/head&gt;
    &lt;p&gt;To everyone who I spoke to and haven’t featured in this article, I’m sorry for excluding your input. This article is very long already. Just know, the conversation we had was incredibly useful in producing this article and I really enjoyed talking to you. Thank you.&lt;/p&gt;
    &lt;p&gt;This article has been really hard to write, I’m not going to lie. I really enjoyed my conversations but quietly, I was hoping I was wrong with my theory that AI tools were being forced, rather than being used on merit.&lt;/p&gt;
    &lt;p&gt;It’s so typical of the tech industry to jump on a shiny new thing, completely forgetting how much harm it causes, such as ChatGPT teaching someone how to more effectively commit suicide (extreme content warning on that link). Let’s not forget how much content was stolen in the first place — and continues to be “scraped” — to train models too. Legal cases have rightly been raised.&lt;/p&gt;
    &lt;p&gt;I would say I’m very much an AI-sceptic. That’s not a position from refusing to try AI tools though. We’ve tried these tools for quite a while and after that extended period, found them to be more often than not, a complete hindrance, albeit quite useful in certain contexts. Your mileage may vary though! We know our position and that’s what’s important to us.&lt;/p&gt;
    &lt;p&gt;Regardless of your opinion of AI, forcing the usage of it is almost certainly going to end in disaster. Just be prepared for that disaster and protect yourself is my overall advice.&lt;/p&gt;
    &lt;p&gt;Enjoyed this article? You can support us by leaving a tip via Open Collective&lt;/p&gt;
    &lt;head rend="h2"&gt;Newsletter&lt;/head&gt;
    &lt;p&gt;Loading, please wait…&lt;/p&gt;
    &lt;p&gt;Powered by Postmark - Privacy policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45079911</guid></item><item><title>My phone is an ereader now</title><link>https://www.davepagurek.com/blog/minimal-phone/</link><description>&lt;doc fingerprint="e73881166b47bbdb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My phone is an ereader now&lt;/head&gt;
    &lt;p&gt;I got a Kobo in 2016 after borrowing my mom's old one for a year before that. It probably is responsible for getting me reading again after high school. I used to be an avid reader, the sort of kid who would have to be told to put down the book and go to sleep, and who would then creep slowly to the bookshelf to pick it up again without arousing suspicion after the light had been turned out. I think I slowed my reading for fun as the work load of school increased, and stopped when moving every four months for internships in university. Having something small and portable that I could load books onto changed that and got my momentum going again. I now loosely grade how much I'm thriving by how much I'm reading, as an indirect indicator of how not burnt out I am.&lt;/p&gt;
    &lt;p&gt;It fared me quite well, but I had a few issues with it. Library books would randomly not work on it, even if they would show up on, for example, the Libby app on my phone. It also came with a way to sync articles to it via Pocket, but it always required a little too much forethought for me: I had to remember to find and save articles beforehand in order to then read them later. There were some services to do this automatically via RSS but the syncing process itself was just slow enough that I found myself rarely doing it. Also, Mozilla has now killed Pocket as a service. In the middle of writing this, they announced support for Instapaper instead, but it has the same workflow issues for me. My partner reads on her phone, but something about reading on a screen grates on me after a while, and makes it too easy to jump to something else.&lt;/p&gt;
    &lt;p&gt;So I was intrigued when I heard about the Minimal Phone, an Android phone with an epaper display. It wasn't the first epaper Android device I'd seenâI've seen reviews saying the Boox Palma is actually pretty greatâbut it was the idea of this being an actual phone that can take a sim card that really get me interested. What if I could read the news and blogs on what looks like paper while commuting, without having the forethought of downloading or syncing something? I might otherwise spend that time staring into space or looking at nothing on Bluesky. I'd probably rather be reading a bit of a book, or other longer-form writing. So I ordered one as a gift to myself.&lt;/p&gt;
    &lt;p&gt;I figured I might be a good fit for this device. I don't really watch videos on my phone. I send messages a bit, but not urgently. Most of the time I'm at or near a full keyboard anyway. I take some photos, but not that many any more. I feel like the photo winds changed for me sometime in university and I now feel weird posting Nice Photos to social media. Who are those for, really? I now send quick photos directly to friends mostly, and they don't have to be print quality or anything. They just have to be visible.&lt;/p&gt;
    &lt;p&gt;With that in mind, I went in treating it like an experiment. I still have my Kobo that has its annoyances but works. I still have a fully functional Pixel 8 phone. I don't need this to work. At worst, this could just be an alternate ereader for me. So when it arrived mid-July, I started testing it full-time to see how it'd go, with my normal phone in my bag just in case.&lt;/p&gt;
    &lt;p&gt;Overall, I actually really like it! I absolutely would not recommend this device to everyoneâI'll get into why laterâbut it's been working pretty well for me.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the Minimal Phone works&lt;/head&gt;
    &lt;p&gt;This phone is around the same size of my Pixel 8. It's just a tad shorter and just a tad wider. I don't really feel the shortness, but I do feel the wideness a bit, which makes it more comfortable to read on. The bottom third of the height is taken up by a physical keyboard, and the top two thirds are an epaper display.&lt;/p&gt;
    &lt;p&gt;It's just Android under there, with a black-and-white epaper display. It comes with a few launchers, and I use one that works like a pretty traditional launcher, but comes with some built in icon choices that look sharp on the display.&lt;/p&gt;
    &lt;p&gt;There's a side button between the phone's volume keys that you can tap to flash the display to clear ghosting. I don't find myself doing this oftenâghosting is not that badâbut if you press and hold it, it opens the display settings. This is something I do all the time.&lt;/p&gt;
    &lt;p&gt;From the settings screen, you can turn on and off the light on the display and on the keyboard, and also change the display light's colour temperature. I mostly leave those off; I only need those if I'm outside after dark, and the controls are big enough that I can turn them on easily enough in low light.&lt;/p&gt;
    &lt;p&gt;The most important setting is the refresh rate at the bottom. The slowest setting has the slowest refresh rate, but the highest quality visuals: always showing nice shades of grey, and with less ghosting but more flashing as it updates. The fastest setting (which, to be clear, is still not very fast) has much less flashing, a little more ghosting, and dithers pure black and white rather than showing any shades of grey. The middle setting, "hybrid" mode, is a combination of the two: it uses the faster setting while things are moving onscreen, and then updates to the slower, higher quality render when movement stops. I generally keep the phone in this hybrid mode, except for a few specific cases.&lt;/p&gt;
    &lt;p&gt;The keyboard feels pretty good, and it's a comfortable size to type on with two thumbs. I can't really one-handed type on this phone; it's a tad too wide for that, but the width is worth it for easier reading. I really appreciate them including the keyboard here, as the display looks great but is definitely not all that responsive, so typing would be a lot more frustrating without this.&lt;/p&gt;
    &lt;head rend="h2"&gt;The great parts&lt;/head&gt;
    &lt;p&gt;This thing is so nice to read on.&lt;/p&gt;
    &lt;p&gt;I hate reading on screens. Something about dark mode especially messes with my eyes, but even without that, I've never enjoyed reading articles on my phone. Too easy to get distracted, the minor eye strain... This device though, the epaper display looks great. It's not especially high resolution or anything, but I could spend a long time reading on this without issue. I just spent two flights (Toronto to Vancouver and back again) just reading books on this, and I'd do it again. It's really crisp and visible in the sun too.&lt;/p&gt;
    &lt;p&gt;It's super easy to queue up library book holds and read them all from the phone. I have had zero issues with that. Being able to add new things on-the-go has also made it really easy to grab another book on the spot once I finish one. I definitely have found myself reading more books this past month and a half.&lt;/p&gt;
    &lt;p&gt;I also now am more likely to read people's blogs on an RSS reader than scroll through social media. I wasn't setting out to fully purge social media or anything, but I certainly feel a little more fulfilled after reading something that someone has clearly put time and effort into.&lt;/p&gt;
    &lt;p&gt;Possibly as a consequence of the display technology, I also generally get 2 days of usage out of a charge. Most days I finish with 70% battery remaining, letting me go another day with some buffer room. On some really low usage days, I could maybe even go more, but already this is great. On a high usage day, I'll maybe end with 50%, which is still fine by me.&lt;/p&gt;
    &lt;p&gt;This is secondary, by far, but I also feel now that I can fully turn off autocorrect, as this phone has a physical keyboard. Most of the time (with important caveats), I don't make typos. So I no longer have to suffer through autocorrect changing programming terms (which I still type a lot of), changing my capitalization, or doing its own insane capitalization (why would it format "city Hall" with just one capital? Commit to capitals or no capitals, don't do this awkward mix!)&lt;/p&gt;
    &lt;p&gt;As another minor note, the fingerprint reader is actually quite fast. When it remembers my fingerprints, it's super reliable. ("What do you mean, when it remembers?" I'll get into it later, there's a pretty bad bug here. But in regular usage, it really does work well.)&lt;/p&gt;
    &lt;p&gt;Everything else this phone does, it does a little worse than a normal phone, but not so much worse that it's a problem. I assume it would be a lot worse at watching videos but I never really did that much on my old phone anyway. So on the whole, this phone works really well where I want it to, and generally gets out of my way for the usual stuff. I keep using it without really worrying about it.&lt;/p&gt;
    &lt;p&gt;The camera, once set up properly, is pretty passable. Well, the selfie camera is in a super awkward spot, but I don't really find myself using it anyway. But other photos look decent enough that I'm not embarrassed to send them to people!&lt;/p&gt;
    &lt;head rend="h2"&gt;Growing pains&lt;/head&gt;
    &lt;p&gt;Even though I do really like this thing, and am continuing to use it as my primary device, there are a lot of rough edges. This device is made by, primarily, two people (although they've been adding more developers in the past few weeks), so naturally there will be a lot of rough edges. You have to be willing to accept that if you're going to use this phone. They do make updates, but the pace is slow, and they are definitely bogged down by customer support and shipping/manufacturing logistics, so you need to not bank on fixes happening quickly.&lt;/p&gt;
    &lt;p&gt;There's a double-tap-to-wake feature that you can't turn off, and it takes a sec once locked to stop responding to inputs. Consequently, I now put this in my pocket with the display facing out, which is opposite of what I used to do, in order to prevent accidentally disturbing it in my pocket. Doing that, I haven't had issues, but it's an adjustment you have to make for this phone right now.&lt;/p&gt;
    &lt;p&gt;There are a few things you'll probably need to do to the device to make it work well for you. One of them involves the camera. By default, the camera super aggressively denoises its photos, resulting in images that look like they came off of my flip phone from 2008. However, if you use the Open Camera app, switch it to use the Camera 2 API, it then lets you turn off noise reduction in the settings. The resulting images look much crisper, and do have noise, but a tastefulâdare I say aesthetic?âamount of noise. There is no Pixel-style HDR in these photos, but now that that look is everywhere, the resulting photos are... kind of refreshing.&lt;/p&gt;
    &lt;p&gt;The phone also uses something called Duraspeed to aggressively turn off background apps. This works well in general, but it also can stifle some notifications that you do want, and also can affect background audio. I know some people fully turn Duraspeed off, but I've just turned it off for my messaging apps and my music/podcast apps. I've had no notification or background process related issues since doing so.&lt;/p&gt;
    &lt;p&gt;I also found that the backlight was way too bright, and I didn't really want any lights on most of the time anyway. I found that when opening the display settings, it'd turn all the lights back on. But if I save a preset, then it'd stick. You can do that by changing the settings, and then pressing and holding on the wrench icon to save it to your custom preset.&lt;/p&gt;
    &lt;p&gt;Finally, the hybrid refresh mode needs things to stop moving in order to lock in on a higher quality render. That means animated ads are somehow even more annoying than they normally are. Thankfully, Firefox for Android lets you install addons, such as uBlock Origin, to deal with that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bugs&lt;/head&gt;
    &lt;p&gt;The most annoying bug is that this phone will occasionally restart and forget your fingerprint, forcing you to enter your PIN. I don't know why this happens. I can go for a few weeks with it working fine, and then it'll just forget. I can still get in with the PIN, so it's not locking me out, but there's really never a good time to re-set up a fingerprint, and typing a PIN on the onscreen display is slow and cumbersome. This is the bug I hope gets fixed the most.&lt;/p&gt;
    &lt;p&gt;Another bug has something to do with the screen refresh rate, and something to do with responding to keyboard input. If you're on a slower refresh rate and are typing quickly, sometimes it misses keypresses, and you have to go back and fix things. This is also quite annoying, but doesn't seem to happen on the highest screen refresh rate. As a workaround, when I'm sending messages, I switch to the fastest refresh setting. This one-or-the-other approach isn't great though (I still want photos sent to me in messaging apps to look nice!), so I'd love to see that improved over time.&lt;/p&gt;
    &lt;p&gt;I also have to use the phone in the lowest refresh rate for Google Maps in order to see the streets on the map. The color scheme is just too low contrast for the high refresh rate's dithering. The hybrid setting doesn't work either: your location on the map is always slightly moving and so it never locks in and renders a higher-quality image. Arguably, this is a problem with Google Maps because they don't have a high contrast mode. Surely that would have accessibility benefits beyond just this weird device!&lt;/p&gt;
    &lt;p&gt;There's a software update that the Minimal team has been working on for almost two months that will apparently address the fingerprint forgetting issue, make double-tap-to-wake optional, significantly increase the refresh rate on the fast refresh mode, and let you save per-app refresh rate settings. That'll address some of my problems for sure! But it also hasn't shipped yet. To use this device is an exercise in patience, and being accepting of imperfections.&lt;/p&gt;
    &lt;head rend="h2"&gt;Feature Requests&lt;/head&gt;
    &lt;p&gt;None of these are dealbreakers for me, but here's what I'm hoping to see in the future:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I feel like the vibration on the phone is a tad aggressive. Not every vibration is, thoughâFacebook Messenger notifications feel like the right level. I'd love to be able to adjust the cap for vibration intensity!&lt;/item&gt;
      &lt;item&gt;I would love emoji search in the keyboard. But I also don't use that many different emoji or symbols, and by now the ones I do use are in the recents list, so it's fine. But the one time I need to use a weird one, it'd be nice to have!&lt;/item&gt;
      &lt;item&gt;I wish the hybrid refresh mode would work well with camera apps. As it is, I think too much of the screen is updating at once, so it flashes a large part of the display every frame, making it really hard to see. If I put the phone in fast mode, there's no flashing, which is great! But then when I take a photo and tap on the thumbnail to see it, I have to switch back to hybrid or slow mode to see a clearer, non-dithered version. This is a little annoying, and I feel could be improved, but then again I'm not really using this to take a lot of photos anyway.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Concluding the experiment&lt;/head&gt;
    &lt;p&gt;It's been more than a month, and despite not everything being perfect, I'm going to continue using this phone. I do occasionally switch to my Pixel 8 though. I use my Pixel 8 for running for its better waterproofing. When I needed to get actual good, postable photos from SIGGRAPH two weeks ago, I just used my Pixel 8. When seeing LCD Soundsystem last weekend, rather than worry about weirdness with the Ticketmaster app, I just took my Pixel 8. But I've used normal boarding passes for airplanes on my MP01, and I regularly go out without a backup phone. I do mostly rely on my partner to do Google Maps navigation since that's a little bit smoother, although in a pinch I can still use it myself (and the Transit app is a little better in hybrid mode.)&lt;/p&gt;
    &lt;p&gt;Basically, I use the right tool for the job, and this phone doesn't have to be that tool for all jobs. But it turns out I don't need my phone to do all that many jobs, and it's maybe a good thing for it to be doing less of them.&lt;/p&gt;
    &lt;p&gt;There are enough quirks that I wouldn't automatically recommend this experience. But if you know what you're getting into and have the right expectations, this is a really great little device!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45079962</guid></item><item><title>Red: A programming language inspired by REBOL</title><link>https://github.com/red/red</link><description>&lt;doc fingerprint="54eba1b9d3e3f86"&gt;
  &lt;main&gt;
    &lt;p&gt;Red is a programming language strongly inspired by Rebol, but with a broader field of usage thanks to its native-code compiler, from system programming to high-level scripting, while providing modern support for concurrency and multi-core CPUs.&lt;/p&gt;
    &lt;p&gt;Red tackles the software building complexity using a DSL-oriented approach (we call them dialects) . The following dialects are built-in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Red/System: a C-level system programming language compiled to native code&lt;/item&gt;
      &lt;item&gt;Parse: a powerful PEG parser&lt;/item&gt;
      &lt;item&gt;VID: a simple GUI layout creation dialect&lt;/item&gt;
      &lt;item&gt;Draw: a vector 2D drawing dialect&lt;/item&gt;
      &lt;item&gt;Rich-text: a rich-text description dialect&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Red has its own complete cross-platform toolchain, featuring an encapper, a native compiler, an interpreter, and a linker, not depending on any third-party library, except for a Rebol2 interpreter, required during the alpha stage. Once 1.0 is reached, Red will be self-hosted. Currently, Red is still at alpha stage and 32-bit only.&lt;/p&gt;
    &lt;p&gt;Red's main features are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Human-friendly syntax&lt;/item&gt;
      &lt;item&gt;Homoiconic (Red is its own meta-language and own data-format)&lt;/item&gt;
      &lt;item&gt;Functional, imperative, reactive and symbolic programming&lt;/item&gt;
      &lt;item&gt;Prototype-based object support&lt;/item&gt;
      &lt;item&gt;Multi-typing&lt;/item&gt;
      &lt;item&gt;Powerful pattern-matching Macros system&lt;/item&gt;
      &lt;item&gt;Rich set of built-in datatypes (50+)&lt;/item&gt;
      &lt;item&gt;Both statically and JIT-compiled(*) to native code&lt;/item&gt;
      &lt;item&gt;Cross-compilation done right&lt;/item&gt;
      &lt;item&gt;Produces executables of less than 1MB, with no dependencies&lt;/item&gt;
      &lt;item&gt;Concurrency and parallelism strong support (actors, parallel collections)(*)&lt;/item&gt;
      &lt;item&gt;Low-level system programming abilities through the built-in Red/System DSL&lt;/item&gt;
      &lt;item&gt;Powerful PEG parser DSL built-in&lt;/item&gt;
      &lt;item&gt;Fast and compacting Garbage Collector&lt;/item&gt;
      &lt;item&gt;Instrumentation built-in for the interpreter, lexer and parser.&lt;/item&gt;
      &lt;item&gt;Cross-platform native GUI system, with a UI layout DSL and a drawing DSL&lt;/item&gt;
      &lt;item&gt;Bridging to the JVM&lt;/item&gt;
      &lt;item&gt;High-level scripting and REPL GUI and CLI consoles included&lt;/item&gt;
      &lt;item&gt;Visual Studio Code plugin, with many helpful features&lt;/item&gt;
      &lt;item&gt;Highly embeddable&lt;/item&gt;
      &lt;item&gt;Low memory footprint&lt;/item&gt;
      &lt;item&gt;Single-file (~1MB) contains whole toolchain, full standard library and REPL (**)&lt;/item&gt;
      &lt;item&gt;No install, no setup&lt;/item&gt;
      &lt;item&gt;Fun guaranteed!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(*) Not implemented yet. (**) Temporarily split in two binaries&lt;/p&gt;
    &lt;p&gt;More information at red-lang.org.&lt;/p&gt;
    &lt;p&gt;Download a GUI or CLI console binary suitable for your operating system, rename it at your convenience, then run it from shell or by double-clicking on it (Windows). You should see the following output:&lt;/p&gt;
    &lt;code&gt;    ---== Red 0.6.5 ==--
    Type HELP for starting information.

    &amp;gt;&amp;gt;
&lt;/code&gt;
    &lt;p&gt;A simple Hello World would look like:&lt;/p&gt;
    &lt;code&gt;    &amp;gt;&amp;gt; print "Hello World!"
    Hello World!
&lt;/code&gt;
    &lt;p&gt;If you are on the GUI console, a GUI Hello World (prompt omitted):&lt;/p&gt;
    &lt;code&gt;    view [text "Hello World!"]
&lt;/code&gt;
    &lt;p&gt;A more sophisticated example that retrieves the last commits from this repo and displays their log messages in a scrollable list:&lt;/p&gt;
    &lt;code&gt;    view [
        text-list data collect [
            foreach event load https://api.github.com/repos/red/red/commits [
                keep event/commit/message
            ]
        ]
    ]
&lt;/code&gt;
    &lt;p&gt;Note: check also the following improved version allowing you to click on a given commit log and open the commit page on github.&lt;/p&gt;
    &lt;p&gt;You can now head to see and try some showcasing scripts here and there. You can run those examples from the console directly using Github's "raw" link. E.g.:&lt;/p&gt;
    &lt;code&gt;    &amp;gt;&amp;gt; do https://raw.githubusercontent.com/red/code/master/Showcase/calculator.red
&lt;/code&gt;
    &lt;p&gt;Note: If you are using the Wine emulator, it has some issues with the GUI-Console. Install the &lt;code&gt;Consolas&lt;/code&gt; font to fix the problem.&lt;/p&gt;
    &lt;p&gt;The Red toolchain comes as a single executable file that you can download for the big-3 platforms (32-bit only for now). Rename the file to &lt;code&gt;redc&lt;/code&gt; (or &lt;code&gt;redc.exe&lt;/code&gt; under Windows).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Put the downloaded redc binary in the working folder.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In a code or text editor, write the following Hello World program:&lt;/p&gt;
        &lt;code&gt;Red [ Title: "Simple hello world script" ] print "Hello World!"&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Save it under the name: hello.red&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Generate a compiled executable from that program: (first run will pre-compile libRedRT library)&lt;/p&gt;
        &lt;code&gt;$ redc -c hello.red $ ./hello&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Want to generate a compiled executable from that program with no dependencies?&lt;/p&gt;
        &lt;code&gt;$ redc -r hello.red $ ./hello&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Want to cross-compile to another supported platform?&lt;/p&gt;
        &lt;code&gt;$ redc -t Windows hello.red $ redc -t Darwin hello.red $ redc -t Linux-ARM hello.red&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The full command-line syntax is:&lt;/p&gt;
    &lt;code&gt;redc [command] [options] [file]
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;[file]&lt;/code&gt; any Red or Red/System source file.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The -c, -r and -u options are mutually exclusive.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;[options]&lt;/code&gt;
    &lt;/p&gt;
    &lt;code&gt;-c, --compile                  : Generate an executable in the working
                                 folder, using libRedRT. (development mode)

-d, --debug, --debug-stabs     : Compile source file in debug mode. STABS
                                 is supported for Linux targets.

-dlib, --dynamic-lib           : Generate a shared library from the source
                                 file.

-e, --encap                    : Compile in encap mode, so code is interpreted
                                 at runtime. Avoids compiler issues. Required
                                 for some dynamic code.

-h, --help                     : Output this help text.

-o &amp;lt;file&amp;gt;, --output &amp;lt;file&amp;gt;     : Specify a non-default [path/][name] for
                                 the generated binary file.

-r, --release                  : Compile in release mode, linking everything
                                 together (default: development mode).

-s, --show-expanded            : Output result of Red source code expansion by
                                 the preprocessor.

-t &amp;lt;ID&amp;gt;, --target &amp;lt;ID&amp;gt;         : Cross-compile to a different platform
                                 target than the current one (see targets
                                 table below).

-u, --update-libRedRT          : Rebuild libRedRT and compile the input script
                                  (only for Red scripts with R/S code).

-v &amp;lt;level&amp;gt;, --verbose &amp;lt;level&amp;gt;  : Set compilation verbosity level, 1-3 for
                                 Red, 4-11 for Red/System.

-V, --version                  : Output Red's executable version in x.y.z
                                 format.

--config [...]                 : Provides compilation settings as a block
                                 of `name: value` pairs.

--no-compress                  : Omit Redbin format compression.

--no-runtime                   : Do not include runtime during Red/System
                                 source compilation.

--no-view                      : Do not include VIEW module in the CLI console
                                 and the libRedRT.

--view &amp;lt;engine&amp;gt;                : Select the VIEW engine (native, terminal, GTK, test)

--red-only                     : Stop just after Red-level compilation.
                                 Use higher verbose level to see compiler
                                 output. (internal debugging purpose)

--show-func-map                : Output an address/name map of Red/System
                                 functions, for debugging purposes.
&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;[command]&lt;/code&gt;
    &lt;/p&gt;
    &lt;code&gt;build libRed [stdcall]         : Builds libRed library and unpacks the
                                 libRed/ folder locally.

clear [&amp;lt;path&amp;gt;]                 : Delete all temporary files from current
                                 or target &amp;lt;path&amp;gt; folder.
&lt;/code&gt;
    &lt;p&gt;Cross-compilation targets:&lt;/p&gt;
    &lt;code&gt;MSDOS        : Windows, x86, console (+ GUI) applications
Windows      : Windows, x86, GUI applications
WindowsXP    : Windows, x86, GUI applications, no touch API
Linux        : GNU/Linux, x86, console (+ GUI) applications
Linux-GTK    : GNU/Linux, x86, GUI only applications
Linux-musl   : GNU/Linux, x86, musl libc
Linux-ARM    : GNU/Linux, ARMv5, armel (soft-float)
RPi          : GNU/Linux, ARMv7, armhf (hard-float)
RPi-GTK      : GNU/Linux, ARMv7, armhf (hard-float), GUI only applications
Pico         : GNU/Linux, ARMv7, armhf (hard-float), uClibc
Darwin       : macOS Intel, console-only applications
macOS        : macOS Intel, applications bundles
Syllable     : Syllable OS, x86
FreeBSD      : FreeBSD, x86
NetBSD       : NetBSD, x86
Android      : Android, ARMv5
Android-x86  : Android, x86
&lt;/code&gt;
    &lt;p&gt;Note: The toolchain executable (&lt;code&gt;redc.exe&lt;/code&gt;) relies on Rebol encapper which does not support being run from a location specified in &lt;code&gt;PATH&lt;/code&gt; environment variable and you get &lt;code&gt;PROGRAM ERROR: Invalid encapsulated data&lt;/code&gt; error. If you are on Windows try using PowerShell instead of CMD. You can also provide the full path to the executable, put a copy of it in your working folder or wrap a shell script (see relevant tickets: #543 and #1547).&lt;/p&gt;
    &lt;p&gt;The compiler and linker are currently written in Rebol. Please follow the instructions for installing the compiler toolchain in order to run it from sources:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Clone this git repository or download an archive (&lt;/p&gt;&lt;code&gt;ZIP&lt;/code&gt;button above or from tagged packages).&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download a Rebol interpreter suitable for your OS: Windows, Linux (or Linux), Mac OS X, FreeBSD, OpenBSD, Solaris.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Extract the&lt;/p&gt;&lt;code&gt;rebol&lt;/code&gt;binary, put it in the root folder, that's all!&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Let's test it: run&lt;/p&gt;&lt;code&gt;./rebol&lt;/code&gt;, you'll see a&lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt;prompt appear. Windows users need to double-click on the&lt;code&gt;rebol.exe&lt;/code&gt;file to run it.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the REBOL console type:&lt;/p&gt;
        &lt;code&gt;&amp;gt;&amp;gt; do/args %red.r "%tests/hello.red"&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The compilation process should finish with a &lt;code&gt;...output file size&lt;/code&gt; message. The resulting binary is in the working folder. Windows users need to open a DOS console and run &lt;code&gt;hello.exe&lt;/code&gt; from there.&lt;/p&gt;
    &lt;p&gt;You can compile the Red console from source:&lt;/p&gt;
    &lt;code&gt;    &amp;gt;&amp;gt; do/args %red.r "-r %environment/console/CLI/console.red"
&lt;/code&gt;
    &lt;p&gt;To compile the Windows GUI console from source:&lt;/p&gt;
    &lt;code&gt;    &amp;gt;&amp;gt; do/args %red.r "-r -t Windows %environment/console/GUI/gui-console.red"
&lt;/code&gt;
    &lt;p&gt;Note: the &lt;code&gt;-c&lt;/code&gt; argument is not necessary when launching the Red toolchain from sources, as the default action is to compile the input script (the toolchain in binary form default action is to run the input script through the interpreter).
The &lt;code&gt;-r&lt;/code&gt; argument is needed when compiling the Red console to make additional runtime functions available.&lt;/p&gt;
    &lt;p&gt;Note: The red git repository does not include a &lt;code&gt;.gitignore&lt;/code&gt; file. If you run the automated tests, several files will be created that are not stored in the repository. Installing and renaming a copy of &lt;code&gt;.git/.gitignore-sample&lt;/code&gt; file will ignore these generated files.&lt;/p&gt;
    &lt;p&gt;If you want to contribute code to the Red project be sure to read the guidelines first.&lt;/p&gt;
    &lt;p&gt;It is usually a good idea to inform the Red team about what changes you are going to make in order to ensure that someone is not already working on the same thing. You can reach us through our chat room.&lt;/p&gt;
    &lt;p&gt;Satisfied with the results of your change and want to issue a pull request on Github?&lt;/p&gt;
    &lt;p&gt;Make sure the changes pass all the existing tests, add relevant tests to the test-suite, and please test on as many platforms as you can. You can run all the tests using (from Rebol console, at repository root):&lt;/p&gt;
    &lt;code&gt;    &amp;gt;&amp;gt; do %run-all-tests.r
&lt;/code&gt;
    &lt;p&gt;If you want git version included in your Red console built from sources, use this command:&lt;/p&gt;
    &lt;code&gt;call/show ""                                              ;-- patch call bug on Windows
save %build/git.r do %build/git-version.r                 ;-- lookup git version if available
do/args %red.r "-r %environment/console/CLI/console.red"  ;-- build Console
write %build/git.r "none^/"                               ;-- restore git repo status&lt;/code&gt;
    &lt;p&gt;Some anti-virus programs are a bit too sensitive and can wrongly report an alert on some binaries generated by Red (see here for the details). If that happens to you, please report it to your anti-virus vendor as a false positive.&lt;/p&gt;
    &lt;p&gt;Both Red and Red/System are published under BSD license, runtime is under BSL license. BSL is a bit more permissive license than BSD, more suitable for the runtime parts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080051</guid></item><item><title>Sheafification – The optimal path to mathematical mastery</title><link>https://sheafification.com/the-fast-track/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080388</guid></item><item><title>Intermittent fasting correlated with increased risk of cardiovascular disease</title><link>https://www.bbc.com/news/articles/c0l6ye6xe12o</link><description>&lt;doc fingerprint="f6ca05be0f8f1fe5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intermittent fasting: benefits or risks? Study raises questions about heart health&lt;/head&gt;
    &lt;p&gt;Intermittent fasting has become the diet trend of the decade.&lt;/p&gt;
    &lt;p&gt;It promises to hack biology without the drudgery of counting calories or cutting carbs: simply change when you eat, not necessarily what you eat. Tech moguls swear by it, Hollywood stars insist it keeps them trim. Britain's former prime minister Rishi Sunak once spoke of starting his week with a 36-hour fast.&lt;/p&gt;
    &lt;p&gt;So far the science has seemed supportive. Research suggests that extending the overnight fast may improve metabolism, aid cellular repair and perhaps even prolong life. Nutritionists, however, have long warned that skipping meals is no magic bullet - and may be risky for those with underlying conditions.&lt;/p&gt;
    &lt;p&gt;Intermittent fasting compresses eating into a short daily window, often eight hours, leaving a 16-hour gap without food. Other time-restricted diets, like the 5:2 plan, limit calories on certain days rather than hours.&lt;/p&gt;
    &lt;p&gt;Now, the first large-scale study of its kind raises a more serious red flag. Researchers, analysing data from more than 19,000 adults, found that those who confined their eating to less than eight hours a day faced a 135% higher risk of dying from cardiovascular disease - death due to heart and blood vessel diseases - than people who ate over 12-14 hours.&lt;/p&gt;
    &lt;p&gt;An elevated cardiovascular risk means that, based on a person's health, lifestyle and medical data, they are more likely than others in the study to develop heart-related problems such as heart attack or stroke.&lt;/p&gt;
    &lt;p&gt;The link to overall mortality - deaths from any cause - was weaker and inconsistent, but the cardiovascular risk persisted across age, sex and lifestyle groups even after rigorous testing.&lt;/p&gt;
    &lt;p&gt;In other words, the study found only a weak and inconsistent link between time-restricted eating and overall deaths. But the risk of dying from cardiovascular disease was sharply higher.&lt;/p&gt;
    &lt;p&gt;The authors stress that the study doesn't prove cause and effect. But the signal is striking enough to challenge the narrative of fasting as a risk-free path to better health.&lt;/p&gt;
    &lt;p&gt;Researchers tracked American adults over eight years. To understand their eating habits, participants were asked on two separate days - about two weeks apart - to recall everything they ate and drank. From these "dietary recalls", scientists estimated each person's average eating window and treated it as representative of their long-term routine.&lt;/p&gt;
    &lt;p&gt;Those who ate within an eight-hour window faced a higher risk of dying from cardiovascular disease than those who spread meals over 12-14 hours, the study found.&lt;/p&gt;
    &lt;p&gt;They found the elevated cardiovascular risk was consistent across socioeconomic groups, and strongest among smokers and people with diabetes or existing heart disease - suggesting they should be especially cautious about long-term, narrow eating windows. The link held even after adjusting for diet quality, meal and snack frequency, and other lifestyle factors, researchers found.&lt;/p&gt;
    &lt;p&gt;I asked the researchers how we should read the finding that heart-related deaths go up so dramatically, but overall deaths don't - is it biology, or bias in the data?&lt;/p&gt;
    &lt;p&gt;Diet is a major driver of diabetes and heart disease, so an association with higher cardiovascular mortality is not unexpected, said Victor Wenze Zhong, the lead author of the peer-reviewed study in Diabetes &amp;amp; Metabolic Syndrome: Clinical Research and Reviews.&lt;/p&gt;
    &lt;p&gt;"The unexpected finding is that sticking to a short eating window less than eight hours over years was linked to increased death risk from cardiovascular disease," says Prof Zhong, an epidemiologist at Shanghai Jiao Tong University School of Medicine in China.&lt;/p&gt;
    &lt;p&gt;That runs counter to the popular belief - supported by short-term studies lasting only a few months to a year - that time-restricted eating improves heart and metabolic health.&lt;/p&gt;
    &lt;p&gt;In an accompanying editorial in the same journal, Anoop Misra, a leading endocrinologist, weighs the promise and pitfalls of intermittent fasting.&lt;/p&gt;
    &lt;p&gt;On the upside, he says, multiple trials and analyses suggest it can promote weight loss, improve insulin sensitivity, lower blood pressure and enhance lipid profiles, with some evidence of anti-inflammatory benefits.&lt;/p&gt;
    &lt;p&gt;It may also help people manage blood sugar without rigid calorie counting, fits easily with cultural or religious fasting practices, and is simple to follow.&lt;/p&gt;
    &lt;p&gt;"However, the potential downsides include nutrient deficiencies, increased cholesterol, excessive hunger, irritability, headaches and reduced adherence over time," says Prof Misra.&lt;/p&gt;
    &lt;p&gt;"For people with diabetes, unmonitored fasting risks dangerous drops in blood sugar; and promotes junk food intake during eating window. For older adults or those with chronic conditions, prolonged fasting may worsen frailty or accelerate muscle loss."&lt;/p&gt;
    &lt;p&gt;This is not the first time intermittent fasting has faced scrutiny.&lt;/p&gt;
    &lt;p&gt;A rigorous three-month study published in JAMA Internal Medicine in 2020 found that participants lost only a small amount of weight, much of which may have come from muscle. Another study indicated that intermittent fasting may produce side effects such as weakness, hunger, dehydration, headaches and difficulty concentrating.&lt;/p&gt;
    &lt;p&gt;The new study, Prof Misra says, now adds a more troubling caveat - a possible link to higher cardiovascular risk, at least in certain groups.&lt;/p&gt;
    &lt;p&gt;I asked Prof Zhong what he would advise clinicians and the public to take away from the latest findings.&lt;/p&gt;
    &lt;p&gt;He said people with heart disease or diabetes should be cautious about adopting an eight-hour eating window. The findings point to the need for "personalised" dietary advice, grounded in health status and evolving evidence.&lt;/p&gt;
    &lt;p&gt;"Based on the evidence as of now, focusing on what people eat appears to be more important than focusing on the time when they eat. At least, people may consider not to adopt eight-hour eating window for a long time either for the purpose of preventing cardiovascular disease or for improving longevity."&lt;/p&gt;
    &lt;p&gt;Clearly, for now, the message is less about abandoning fasting altogether and more about tailoring it to an individual's risk profile. Until the evidence is clearer, the safest bet may be to focus less on the clock and more on the plate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080538</guid></item><item><title>Rick Beato is right to rant about music copyright strikes</title><link>https://savingcountrymusic.com/rick-beato-is-right-to-rant-about-music-copyright-strikes/</link><description>&lt;doc fingerprint="b4637d84ebf84dfb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rick Beato is Right to Rant About Music Copyright Strikes&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Go Beato, go! &lt;lb/&gt;For 15+ years, Saving Country Music has been on the warpath against the completely ludicrous intellectual property regime that disallows even a small snippet of music to be featured in a podcast without draconious repercussions, including removing episodes, and deleting entire accounts, while not offering any reasonable alternative solutions to the issue. &lt;lb/&gt;On YouTube videos, creators can freely filch copyrighted photos and other people’s videos virtually free of ramifications. You can take an entire 2 1/2 hour film, impose it over a background, and upload it to YouTube, and usually avoid any problems. But feature a barely audible 8 1/2-second clip of music underneath audio dialogue, and you could have your entire podcast career evaporate overnight. &lt;lb/&gt;Music labels have been leaving major opportunities to promote their catalogs and performers on the table with their punitive copyright claims that make it impossible to feature music on music podcasts and other platforms. And instead of trying to build a system where perhaps there’s more reasonable revenue sharing or other opportunities for artists and songwriters through these podcast platforms to promote their music, it’s the punitive measures of record labels that eliminate these opportunities in lose/lose scenarios. &lt;lb/&gt;Music video podcaster Rick Beato with his massive 5 million-plus subscriber base finally got fed up with it, and posted a rant on Tuesday, August 19th about this, and, it’s a thing of beauty. &lt;lb/&gt;“I hate making these videos, but I really need to because it seems the only thing that ever gets done is when you talk about this stuff,” Beato starts off. But one of the nauseating things about this issue is that we’ve been talking about it for going on two decades, and still nothing is getting done about it. Saving Country Music posted about this issue in 2024, and in 2020, and as far back as 2010, with no real movement on the issue. &lt;lb/&gt;Luckily though, Rick Beato has a much bigger bullhorn, though he has brought up the issue before to no avail, though not in such a dedicated and forceful manner. &lt;lb/&gt;Beato’s specific beef is with Universal Music Group, who is the most notorious actor for bringing these heavy handed claims against song clip uses that clearly fall under the fair use clauses of American copyright law, let alone are being brought against a guy who operates a massive music platform that promotes artists. But since there’s rarely humans making any of these decisions and it’s automated by bots, they don’t understand these claims are against Universal Music’s best interests. &lt;lb/&gt;“I’m doing interviews with people and playing the music that they either wrote or recorded, or they produced,” Beato explains. “You need to play people music to talk about it. That is the definition of fair use. These are interviews with people about their careers. Why are these record labels wanting to take down content about artist that have records on their label? What sense does that make?” &lt;lb/&gt;“Give it a rest,” he continues. “My God. If I didn’t fight these three claims, my channel would get taken down with my 2,000 videos. Is that ridiculous? It’s ridiculous to me. And they’re for interviews.”&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;People continue to ask, “Why doesn’t Saving Country Music has a podcast?” Because what’s the point of having a music podcast when you can’t feature music? In fact, after over a decade of refusing to start one, I finally did, music free. What happened? About a dozen episodes in, someone took out a claim, and not only were all the episodes deleted, so was the entire account, even though no music even appeared on any of the episodes. I was given absolutely no recourse to fight whatever false claim had been made. &lt;lb/&gt;For the record, Saving Country Music’s Country History X podcast remains live. But it’s been difficult to pursue it, knowing it could summarily disappear at any moment. People have lost thousands of episodes and videos over this issue. Meanwhile, you can use extended song clips on Instagram, TikTok, and Facebook. It has become an excellent way to promote songs and artists. These networks facilitate this music sharing, and credit the artists for their work. &lt;lb/&gt;The music industry continues to so colossal fail the artists and catalogs they represent, and the fans they’re supposed to serve with this current system of how podcasts are handled. If everything changes today thanks to the Rick Beato rant, it would still be 15 years too late. But at least it would happen. The powers that be in the music industry need to recognize that the power base in media is moving to podcasting, and they need to make music available to podcasters in a way that’s fair to all parties. &lt;lb/&gt;In fact in some respects we’re moving backwards on this issue. A couple of years ago, Spotify did offer the ability for podcasters to embed full songs in episodes. But they’ve since discontinued that program. &lt;lb/&gt;Let’s do this. It’s well beyond the time to solve this problem. You aren’t screwing podcasters. You’re screwing artists who could be using podcasts to help promote their music. Hopefully Rick Beato’s rant finally reaches the right ears. &lt;lb/&gt;– – – – – – –&lt;/p&gt;
    &lt;p&gt;If you found this article valuable, consider leaving Saving Country Music A TIP.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080618</guid></item><item><title>Git Diagramming "The Weave"</title><link>https://daverupert.com/2025/08/git-diagramming-the-weave/</link><description>&lt;doc fingerprint="763811cbeef8591f"&gt;
  &lt;main&gt;
    &lt;p&gt;We all know the current US President is one hell of an orator and often assures us that he has “the best words”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I went to an Ivy League school. I’m very highly educated. I know words. I have the best words.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The man knows words. Says so right there. While some might view his non-sequitur ramblings as the nascent stages of dementia or an unfiltered ADHD brain launching into successive short (at times racist) bullet-point diatribes based on the last word or phrase he said like a cursed game of word association, the President asserts this is not the case:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You know, I do the weave. You know what the weave is? I’ll talk about, like, nine different things, and they all come back brilliantly together. And it’s like - and friends of mine that are, like, English professors - they say, it’s the most brilliant thing I’ve ever seen.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;“The Weave” re-entered my consciousness this week after I watched a quick snippet of an Oval Office event where Trump says the (“radical left-wing”) CBO projects tariffs will reduce the deficit by $4 trillion USD. I was skeptical –and for good reason– but I tuned in. What shocked me was not the complete lack of specifics about the CBO projection, but rather the actual reason for the Oval Office meeting: a FIFA event? Wow.&lt;/p&gt;
    &lt;p&gt;For awhile now, I’ve clued into the cyclical pattern of his speeches, little snippets of “the best words” and talking points assembled like a ransom note cut from a magazine. I often wondered if it’s possible to diagram “the weave”. The “branching” narratives Trump uses made me think a git-graph-style visualization was apropos. So I grabbed a transcript and got to work.&lt;/p&gt;
    &lt;p&gt;For my first attempt, I used Mermaid.js’ GitGraph Diagram which worked well but only supports horizontal charts. As I sat with it I realized I wanted a chronological list of statements that read like a transcript. I repurposed the Mermaid’s GitGraph DSL and made a web component called &lt;code&gt;&amp;lt;git-graph&amp;gt;&lt;/code&gt; to help me visualize and document Trump’s derailing trains of thought from the above event.&lt;/p&gt;
    &lt;code&gt;
branch tarrifs
checkout tarrifs
commit id: "I was very happy that today, as you saw, the uh group that does this [the CBO], a government group,"
branch radical-left
checkout radical-left
commit id: "a radical left group, announced that Trump was right"
checkout tarrifs
merge radical-left
commit id: "took in $4 trillion worth of tariffs"
commit id: "The $4 trillion they're going to reduce the deficit by numbers far greater than they ever expected or heard of."
branch stock-market
checkout stock-market
commit id: "And by the way, the stock market went up a thousand points. That was as of 10 minutes ago."
commit id: "I can't tell you what happened. A lot of things happened, but the stock market's up almost a thousand points."
cherry-pick id: "I was very happy that today, as you saw, the uh group that does this [the CBO], a government group,"
commit id: "It's basically on the news that uh the release that just came out from government that uh the tariffs that everybody was talking about that"
branch world-respect
checkout world-respect
commit id: "the whole world respects us for because of what we did"
cherry-pick id: "took in $4 trillion worth of tariffs"
commit id: "The tariffs are going to be at $4 trillion." 
cherry-pick id: "The $4 trillion they're going to reduce the deficit by numbers far greater than they ever expected or heard of."
commit id: "They're going to reduce the deficit by $4 trillion."
branch ask-jd
checkout ask-jd
commit id: "[Seeks validation from JD Vance]"
checkout stock-market
merge ask-jd
cherry-pick id: "And by the way, the stock market went up a thousand points. That was as of 10 minutes ago."
commit id: "It's had a huge impact and the stock market is way up."
commit id: "But this will drive more than $30 billion in US economy"
commit id: "and create 185,000 American jobs."
branch fifa-event
checkout fifa-event
commit id: "No sporting event attracts more attention or more fans or anything else"
commmit id: "And I just look forward to the draw."
commit id: "So we're going to have the draw essentially, Gianni, at the Kennedy Center"
branch kennedy-center-remodel
checkout kennedy-center-remodel
commit id: "and by that time it'll be in even better shape. We're working on it."
commit id: "It's about a year project to make it."
commit id: "It'll be great. It'll be fantastic."
branch oval-office-remodel
checkout oval-office-remodel
commit id: "You see the way [the oval office] is looking?"
commit id: "Looks nice."
commit id: "I can't tell you how much that gold costs, a lot of money."
commit id: "There's nothing like gold and there's nothing like solid gold."
commit id: "But this beautiful office needed it."
commit id: "It had to be representative when we took it over."
commit id: "It was dirty, not clean."
commit id: "I immediately changed the chair and had the this beautiful desk renovated, brought out by the White House."
commit id: "People that do this, they did a great job."
commit id: "They sent it out. We have a craftsman who's great."
commit id: "But this was not appropriate for the Oval Office when I took over."
commit id: "And now you look at all those paintings [instructs to look at paintings]"
branch painting-vault
checkout painting-vault
commit id: "All of these are great presidents and they were all in the vaults."
commit id: "They were in vaults for in some cases much more than a hundred years."
commit id: "And now they're proudly hanging on the oval office walls and I can't imagine anybody changing it."
commit id: "But they were they were buried in vaults for over a hundred years, many of them."
checkout oval-office-remodel
merge painting-vault
commit id: "So it's very exciting. People come in, they really love it."
branch self-congratulations
checkout self-congratulations
commit id: "They love what we're doing here."
commit id: "They love what we're doing in DC [deploying the national guard against American citizens]"
commit id: "and they love what we're doing most importantly in the country in the world. [citation needed]"
checkout fifa-event
merge self-congratulations
commit id: "I'd like to ask Gianni to say a few words ... [flattery] ... he's got the biggest event in the world coming right here to the United States."
commit id: "We did a little for Canada,"
commit id: "we did a little for Mexico."
checkout self-congratulations
merge fifa-event
commit id: "We thought, see, I'm a good citizen. I said, let them have a little piece."
cherry-pick id: "We did a little for Canada,"
commit id: "So, we gave a little to Canada."
commit id: "See how nice I am."
cherry-pick id: "we did a little for Mexico."
commit id: "And we gave a little bit to Mexico."
checkout fifa-event
merge self-congratulations
commit id: "Gianni, please say a few words."
&lt;/code&gt;
    &lt;p&gt;By my count in that four-minute address there were ten distinct themes or “branches”, most of which are common grievances or rhetorical themes found in nearly all Trump speeches. I used “cherry-picking” to model callbacks to a previous statement (e.g. “the stock market is up”) that seem to reverberate into later trains of thought, a quintessential feature of the weave.&lt;/p&gt;
    &lt;p&gt;Four minutes of the weave was about all I could handle but I think this visualization models what I experience when trying to follow along to the President’s speeches. One or two phrases on a topic, then jumping to a new topic, weaving in a (sometimes unrelated) point from a previous topic to make the thought appear more cohesive and linear, then driving deep down an inconsequential topic. When he talks so long about ten different topics at a time, I’d forgive you for thinking he said something salient. But when you break it down you see it for what it is: a mishmash of talking points.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080720</guid></item><item><title>My Foray into Vlang</title><link>https://kristun.dev/posts/my-foray-into-vlang/</link><description>&lt;doc fingerprint="3e081710cb80169b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h1"&gt;A little bit about Go&lt;/head&gt;
    &lt;p&gt;I like Go. I actually donât mind writing &lt;code&gt;err != nil&lt;/code&gt; that much. Just set up a snippet and youâre good to Go. Although, I never really felt like I had a honeymoon period with Go. I learned the language, learned about channels, wrote a bunch of CRUDs and parsers and CLIs. It always felt strictly business. I thought it was because of where I am in my career. But I was wrong.&lt;/p&gt;
    &lt;p&gt;Go is vanilla. It just werks. You build it, you ship it. The language is simple and you donât need to try hard to make it performant.&lt;/p&gt;
    &lt;p&gt;But sometimes you just want a little spiceð¶ï¸ð¥µ&lt;/p&gt;
    &lt;p&gt;Do you ever wonder what else is out there? Hobby programming is a great meme. But I feel like weâre under too much pressure to produce the new unicorn SaaS with 10 million monthly active users.&lt;/p&gt;
    &lt;p&gt;You donât have to pick a tool then find the right job for it. You can just grab a hammer and start smashing stuff. The same nails youâve smashed before might feel different if you smash it with another hammer. Pick a Rusty hammer and you might end up obsessed with how important health and safety is.&lt;/p&gt;
    &lt;head rend="h1"&gt;So, wtf is Vlang?&lt;/head&gt;
    &lt;p&gt;I might have shot myself in the foot with the hammer analogy there, so letâs talk about ice cream. Ok so hereâs the gist: vanilla, drizzle some chocolate on top, peanuts? sure why not. You know this taste, you like it, it comes with more stuff on top. If you like vanilla then you might like vanilla++.&lt;/p&gt;
    &lt;p&gt;That how I see the current state of V. The syntax is similar to Go. It has extra features. The core of it is similar, you can cross compile, you have concurrency (which is also parallelism). Channels and message passing. Oh and &lt;code&gt;defer&lt;/code&gt; as well. All my bros love using &lt;code&gt;defer&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Anyway, letâs see some cool stuff.&lt;/p&gt;
    &lt;head rend="h2"&gt;Maps&lt;/head&gt;
    &lt;code&gt;// So simple!
simple_languages := {"elixir": {"score": 100, "width": 30}}

// Alternatively
mut languages := map[string]map[string]int{"elixir": {"score": 100, "width": 30}}
languages["elixir"] = {"score": 100}
languages["elixir"]["width"] = 30&lt;/code&gt;
    &lt;p&gt;Pretty cool! Much like Go, the maps require a fixed type, dynamic objects like JSON or JavaScript requires either a DTO or a type switch.&lt;/p&gt;
    &lt;p&gt;Ok, but what about the error handling?&lt;/p&gt;
    &lt;code&gt;elixir_score := languages["elixir"]["score"] or { -1 }

if racket := languages['racket'] {
  println('racket score ${racket['score']}')
  racket_width := racket['width'] or { 0 }
  println('racket width ${racket_width}')
}

// Another way to skin the cat
if 'haskell' in languages {
  if 'score' !in languages['haskell'] {
    println('where is my haskell score??')
  }
}

// Zeroth value
languages['this_dont_exist'] // {}
languages['this_dont_exist']['score'] // 0&lt;/code&gt;
    &lt;p&gt;Donât you miss destructuring?&lt;/p&gt;
    &lt;code&gt;languages_with_racket_ocaml := {
  ...languages
  'racket': {'score': 99}
  'ocaml': {'score': 98}
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Struct-licious&lt;/head&gt;
    &lt;code&gt;module main

struct Language {
pub mut:
	score int = -1
	name  string @[required]
}

fn (lr []Language) total() int {
	mut total := 0
	for l in lr {
      if l.score &amp;gt; 0 {
        total += l.score
      }
	}

	return total
}

fn (lr []Language) average() int {
	return lr.total() / lr.len
}

fn main() {
	racket := Language{98, 'racket'}
    // Simple arrays too!
	langs_arr := [racket, Language{102, 'ocaml'}]
	println(langs_arr)
	println(langs_arr.total())
	println(langs_arr.average())
}&lt;/code&gt;
    &lt;p&gt;Isnât that cool? We can have receiver methods on array types. Wait - did you see that? We had a required tag on the struct, that means the program wonât compile if you donât initialise it. Thatâs another cool thing I wish Go has. Not to mention, the initialiser value, Goâs struct is quite predictable in how the value turns out. However, Vâs struct allows you to be explicit. This came in very handy for my case!&lt;/p&gt;
    &lt;code&gt;@[xdoc: 'Server for GitHub language statistics']
@[name: 'v-gh-stats']
struct Config {
mut:
	show_help bool   @[long: help; short: h; xdoc: 'Show this help message']
	user      string = os.getenv('GH_USER')           @[long: user; short: u; xdoc: 'GitHub username env \$GH_USER']
	token     string = os.getenv('GH_TOKEN')          @[long: token; short: t; xdoc: 'GitHub personal access token env \$GH_TOKEN']
	debug     bool   = os.getenv('DEBUG') == 'true'   @[long: debug; short: d; xdoc: 'Enable debug mode env \$DEBUG']
	cache     bool   = os.getenv('CACHE') == 'true'   @[long: cache; short: c; xdoc: 'Enable caching env \$CACHE']
}&lt;/code&gt;
    &lt;p&gt;This example contains flags for running my SVG generation server, it allows you to define the flags yourself but if not, use the environmental value. Neato!&lt;/p&gt;
    &lt;head rend="h2"&gt;WithOption pattern&lt;/head&gt;
    &lt;p&gt;Ahh yes, another thing I had to put up with. TBH, I did end up liking the pattern quite a bit. In Go, no default variables are allowed, you have to use variadics. You end up with an Option struct with zeroth value passing around a few functions to finally one last giant private receiver function that creates the struct, fills the value then finally build and check. Imagine a SQL repository pattern where you want to perform a List operation but optionally join or ensure some field is present in a query. Letâs see how we can cook this.&lt;/p&gt;
    &lt;code&gt;module main

import time

@[params]
struct ListOption {
pub mut:
	created_since time.Time
}

@[params]
struct HeroListOption {
	ListOption
pub mut:
	universe string
	name     ?string
}

struct Hero {}

struct Repo[T] {}

struct Villain {}

fn (r Repo[T]) list(o ListOption) ![]T {
	$if T is Villain {
		return error('whoops you found Villain some how but its not implemented yet')
	}

	return error('whoops not implemented for ${T.name} use one of (Hero, ...)')
}

fn (r Repo[Hero]) list(o HeroListOption) ![]Hero {
	mut query := orm.build_query()

	if o.universe != '' {
		query.eq('universe', o.universe)
	}

	if o.created_since.unix() &amp;gt; 0 {
		query.gt('created_since', o.created_since)
	}

	if name := o.name {
		query.eq('name', name)
	}

	return r.psql(query.do()!)!
}

fn main() {
	r := Repo[Villain]{}
	r.list() or { println(err) }

	hero_repo := Repo[Hero]{}
	hero_repo.list()!
	hero_repo.list(name: 'bruce')!
	hero_repo.list(name: 'bruce', universe: 'dc')!
	hero_repo.list(name: 'bruce', universe: 'marvel')!
	hero_repo.list(created_since: time.Time{year: 1996})!
}&lt;/code&gt;
    &lt;p&gt;Thereâs a lot to unpack here. Letâs start with &lt;code&gt;@[params]&lt;/code&gt; which tells the V compiler that the struct as a whole can be omitted entirely so you can write the empty function and it will still works. Secondly, since generics are a compile time thing we can use reflection to check for the name of the type itself. See link below to see what is possible. You can reflect and check for field existence and field types as well as attributes (remember &lt;code&gt;@[required]&lt;/code&gt;?).&lt;/p&gt;
    &lt;p&gt;Alright, we keep seeing this bang (&lt;code&gt;!&lt;/code&gt;) everywhere. So what is it? Short answer: Result type. Medium answer: &lt;code&gt;(int, err) -&amp;gt; !int&lt;/code&gt;. You donât need the long answer. The bang can propagate although you must remember to handle this somewhere or it will cause a panic eventually. Finally, the optional type. I purposedly only use it for one of the field to show that it can be done, you can decide how you want to write your optionals. But damn! It feels great!&lt;/p&gt;
    &lt;p&gt;go-uber/functional-options vlang/trailing-struct-args vlang/compile-time-reflection vlang/optional-and-result-type&lt;/p&gt;
    &lt;head rend="h2"&gt;Enums??? In this economy?&lt;/head&gt;
    &lt;p&gt;Enums are so back baby. We can totally replace the previous sectionâs &lt;code&gt;universe&lt;/code&gt; field as such.&lt;/p&gt;
    &lt;code&gt;enum Universe {
  dc
  marvel
  nil
}

fn (u Universe) str() ?string {
	return match u {
      // V knows the enum there's no need to type Universe.dc
      .dc { 'dc' }
      .marvel { 'marvel' }
      else {''}
	}
}

@[params]
struct HeroListOption {
	ListOption
pub mut:
	universe Universe = .nil
	name     ?string
}

fn (r Repo[Hero]) list(o HeroListOption) ![]Hero {
	...

	if o.universe != .nil {
		query.eq('universe', o.universe.str())
	}

	...
}

fn main() {
	hero_repo := Repo[Hero]{}
	hero_repo.list(name: 'bruce', universe: .dc)!

	// functions not expecting enum requires the full path
	// auto str() conversion here - see Go fmt.Stringer() or your __str__, __toString()
	println('${Universe.dc}')
}&lt;/code&gt;
    &lt;p&gt;Optional type might be better here. Iâm okay with this though. There is backed enum as well but you can only have integer backed enums. Did you also notice? Receiver method on the backed enum baby.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lambda; the best kind of lamb&lt;/head&gt;
    &lt;p&gt;The array stucts have a set of methods you can use like the basic &lt;code&gt;filter&lt;/code&gt;, &lt;code&gt;map&lt;/code&gt; - there is a stdlib module called &lt;code&gt;arrays&lt;/code&gt; as well that you need to import. It provides more complex methods like &lt;code&gt;fold&lt;/code&gt; and the likes. I donât know about you but I am chuffed this exists.&lt;/p&gt;
    &lt;code&gt;import math

fn example() {
	// type hinting here to skip typing Universe.*
	mut universes := []Universe{}
	universes = [.dc, .marvel, .nil, .dc]
	dcs_or_marvel := universes.filter(it != .nil)
	nils := universes.filter(|u| u == .nil)

	// sorting in place
	[5, 2, 1, 3, 4].sort(a &amp;lt; b)
	sorted := [5, 2, 1, 3, 4].sorted(a &amp;lt; b)
}

struct XY {
	x int
	y int
}

fn (xy XY) dist_from_origin() f64 {
	return math.sqrt((xy.x * xy.x) + (xy.y * xy.y))
}

fn example2() {
	xys := [XY{1, 2}, XY{10, 20}, XY{-1, -69}]
	xys.sort(a.dist_from_origin() &amp;lt; b.dist_from_origin())
	y_asc := xys.sorted(a.y &amp;lt; b.y)
}&lt;/code&gt;
    &lt;p&gt;Thereâs a few caveats here. You gotta make sure the function youâre using actually allow for &lt;code&gt;it&lt;/code&gt; or &lt;code&gt;a &amp;lt; b&lt;/code&gt; expression, but lambda expression will work anywhere a function is accepted as an argument. However, you canât use lambda as a variable like &lt;code&gt;x_asc := |a, b| a.x &amp;lt; b.x&lt;/code&gt;. Still, neat. Use the LSP to check what is accepted.&lt;/p&gt;
    &lt;p&gt;vlang/lambdas vlang/array vlang/arrays&lt;/p&gt;
    &lt;head rend="h1"&gt;Some issues Iâve encountered&lt;/head&gt;
    &lt;p&gt;As fun as it has been learning the language and building an svg service - it is not without problems. The language is on the immature side of things. It has had some time to cook since I last tried it in 2023 and I like it even more. Letâs discuss some of the problems Iâve personally encountered.&lt;/p&gt;
    &lt;head rend="h2"&gt;
      &lt;code&gt;net.http&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;When I was trying to call the GraphQL endpoint using the &lt;code&gt;net.http&lt;/code&gt; module, I ran into issue where it would instantly timeout. This network issue described what is happening in my case precisely, adding the flag &lt;code&gt;-d use_openssl&lt;/code&gt; completely fixed my problem. This seems to be the case when building for Ubuntu 22.4 - when building the exe for my Windows 11 I did not need this flag.&lt;/p&gt;
    &lt;p&gt;If you are wondering what the &lt;code&gt;-d&lt;/code&gt; flag is about, it is a flag for compile-time code branching. See vlang/compiletime-code for more.&lt;/p&gt;
    &lt;head rend="h2"&gt;
      &lt;code&gt;veb&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Another weird quirk Iâve had when working with the &lt;code&gt;veb&lt;/code&gt; HTTP server is refusing to build when trying to use gzip. Take a look at this build error message.&lt;/p&gt;
    &lt;code&gt;/root/.local/v/vlib/veb/middleware.v:129:11: error: field `Ctx.return_type` is not public
127 |         handler: fn [T](mut ctx T) bool {
128 |             // TODO: compress file in streaming manner, or precompress them?
129 |             if ctx.return_type == .file {
    |                    ~~~~~~~~~~~
130 |                 return true
131 |             }&lt;/code&gt;
    &lt;p&gt;What do you think the issue could be? Maybe my version of the language is incorrect or my build was faulty? I purged the local V install and got a fresh version straight from master branch. Yet the issue still persists. Another &lt;code&gt;-d&lt;/code&gt; flag perhaps?&lt;/p&gt;
    &lt;p&gt;Luckily for me somebody already posted about this issue in GitHub, unluckily for me, I didnât search the error message first (whoops). Well, I canât really tell you what the issue is since I havenât delved into Vâs codebase itself. But I can tell you the resolution.&lt;/p&gt;
    &lt;p&gt;In my &lt;code&gt;main.v&lt;/code&gt;, since I was messing around with servers and running main with arguments I needed to import both modules. This was the &lt;code&gt;head -n5&lt;/code&gt; of my errorneous file.&lt;/p&gt;
    &lt;code&gt;module main

import os
import veb&lt;/code&gt;
    &lt;p&gt;The suggested fix?&lt;/p&gt;
    &lt;code&gt;module main

import veb
import os&lt;/code&gt;
    &lt;p&gt;Wow! The code now compiles! From a fresherâs perspective I have no clue why the order of import would affect code in different modules. Namespace should be sacred and completely independent of each other. The order of import should not matter at all. Both packages seems to be unrelated so wtf happened?&lt;/p&gt;
    &lt;head rend="h2"&gt;More complex build system&lt;/head&gt;
    &lt;p&gt;I had alluded to this earlier, there is a cost to using V over Go. Vâs main backend compiles to C and this comes with complexity. There are a bunch of performance optimisations you can do when building the binary itself. You can even build non-static binaries if you wish (in fact this is the default). This is a double-edged sword, with Go, you get what you got. With V, I got what I got but I wonder if what I got can be gotten differently.&lt;/p&gt;
    &lt;p&gt;This might also complicate cross-compilation, the Go team has done a lot of work to ensure things werk across different architectures and OSes. Iâve only tried compiling to Windows and Linux using the static flag. Hereâs my build command:&lt;/p&gt;
    &lt;code&gt;v -prod -compress -d use_openssl -cflags '-static -Os -flto' -o main .&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-d&lt;/code&gt; flag would have to be optional here depending on where I am trying to target as well, Iâd probably have to spend time learning whatâs possible for Macs as well. I know those platforms are definitely supported since their GitHub actions page contains the CI pipelines for these, but I would personally need to check if my specific implemntation, order of imports as well as &lt;code&gt;-d&lt;/code&gt; flags need to be there for those systems or not.&lt;/p&gt;
    &lt;p&gt;This is the one big point I have to give to Go. They really have the just werks philosophy down.&lt;/p&gt;
    &lt;p&gt;vlang/ci vlang/performance-optimisation&lt;/p&gt;
    &lt;head rend="h2"&gt;Concurrency&lt;/head&gt;
    &lt;p&gt;I wondered how the performance of the concurrency is compared to Go. The model is almost identical (which is good) but surely the implementation details are different. Luckily, there is a programming benchmark that exists already that answers my questions.&lt;/p&gt;
    &lt;p&gt;Since I brought up concurrency letâs take a look at the code to see the implementation.&lt;/p&gt;
    &lt;code&gt;module main

import os
import strconv

fn main() {
	mut n := 100
	if os.args.len &amp;gt; 1 {
		n = strconv.atoi(os.args[1]) or { n }
	}

	mut ch := chan int{cap: 1}
	spawn generate(ch)
	for _ in 0 .. n {
		prime := &amp;lt;-ch
		println(prime)
		ch_next := chan int{cap: 1}
		spawn filter(ch, ch_next, prime)
		ch = ch_next
	}
}

fn generate(ch chan int) {
	mut i := 2
	for {
		ch &amp;lt;- i++
	}
}

fn filter(chin chan int, chout chan int, prime int) {
	for {
		i := &amp;lt;-chin
		if i % prime != 0 {
			chout &amp;lt;- i
		}
	}
}&lt;/code&gt;
    &lt;p&gt;benchmark/sieve.go benchmark/sieve.v&lt;/p&gt;
    &lt;p&gt;tldr; itâs finding prime numbers by computing a running channel of previous prime numbers to feed into &lt;code&gt;n&lt;/code&gt; to check if &lt;code&gt;n&lt;/code&gt; is divisible by any previous primes.&lt;/p&gt;
    &lt;p&gt;It seems weird to me that Vâs version is timing out even though both implementation looks almost identical. So I ran the benchmark on my local machine. Hereâs my &lt;code&gt;justfile&lt;/code&gt; to run the benchmark using all I know so far about optimising V.&lt;/p&gt;
    &lt;code&gt;default:
    v -prod -gc boehm_full_opt -cc clang -cflags "-march=broadwell" -stats -showcc -no-rsp -o main_v 1.v
    go build -o main_go ./main.go
    hyperfine './main_v 100' './main_go 100' -N&lt;/code&gt;
    &lt;p&gt;And the result:&lt;/p&gt;
    &lt;code&gt;Benchmark 1: ./main_v 100
  Time (mean Â± Ï):      32.1 ms Â±   2.9 ms    [User: 42.6 ms, System: 166.4 ms]
  Range (min â¦ max):    22.1 ms â¦  40.7 ms    99 runs

Benchmark 2: ./main_go 100
  Time (mean Â± Ï):       1.8 ms Â±   0.2 ms    [User: 2.3 ms, System: 0.3 ms]
  Range (min â¦ max):     1.2 ms â¦   3.1 ms    1471 runs

Summary
  './main_go 100' ran
   18.18 Â± 2.81 times faster than './main_v 100'&lt;/code&gt;
    &lt;p&gt;This is exacerbated further when we run N=1000&lt;/p&gt;
    &lt;code&gt;Benchmark 1: ./main_v 1000
  Time (mean Â± Ï):      1.189 s Â±  0.340 s    [User: 4.410 s, System: 8.144 s]
  Range (min â¦ max):    0.806 s â¦  1.830 s    10 runs

Benchmark 2: ./main_go 1000
  Time (mean Â± Ï):      13.4 ms Â±   2.4 ms    [User: 132.5 ms, System: 12.3 ms]
  Range (min â¦ max):     8.6 ms â¦  21.2 ms    182 runs

Summary
  './main_go 1000' ran
   88.54 Â± 29.90 times faster than './main_v 1000'&lt;/code&gt;
    &lt;p&gt;Taking a look at the N=100 profiling we can see what happened exactly&lt;/p&gt;
    &lt;code&gt;â cat prof.txt | sort --key 2n -n | tail -n 10
           202          0.256ms         -1.819ms           1267ns sync__new_spin_lock
           404          0.064ms         -2.664ms            158ns sync__Semaphore_init
          4387      10644.653ms        540.655ms        2426408ns sync__Semaphore_wait
          8128       5572.567ms        739.231ms         685601ns sync__Channel_try_push_priv
          8172       9062.871ms        941.089ms        1109015ns sync__Channel_try_pop_priv
         15959        406.167ms         87.435ms          25451ns sync__Semaphore_post
         16160          6.993ms        -38.159ms            433ns sync__SpinLock_lock
         16174          3.412ms          0.754ms            211ns sync__SpinLock_unlock
       1766049        380.257ms       -434.470ms            215ns sync__Semaphore_try_wait&lt;/code&gt;
    &lt;p&gt;There is a ton of calls going to &lt;code&gt;Semaphore_try_wait&lt;/code&gt; with the actual &lt;code&gt;Sempahore_wait&lt;/code&gt; execution itself taking over &lt;code&gt;10_000 ms&lt;/code&gt; in total.&lt;/p&gt;
    &lt;p&gt;This suggests to me that while the concurrency is there, it exists and work similarly to the end user. Though in the current state, itâs no where near Goâs maturity and optimisation.&lt;/p&gt;
    &lt;head rend="h1"&gt;&amp;lt;/Thoughts&amp;gt;&lt;/head&gt;
    &lt;p&gt;I like V a lot. The abstraction over the syntax is so nice that made me enjoy writing the syntax as a whole. It makes me wish that Go could do more with what they have, but you and I know that Go would never. V isnât without itâs problems though, the ecosystem is still quite immature, compiler flags need grokking over even if youâre not a performance maximalist. IMO, the issue comes down to maturity, given enough time and contributor I believe the language will bloom beautifully. The syntax conveniences already had me sold. I know AI can write boilerplate but it feels good to not need it at all and write everything myself.&lt;/p&gt;
    &lt;p&gt;V has come a lot further than when I tried it in 2023. Iâll be actively using it from now on since my main job in Go leaves me wishing for more from time to time. If you enjoy Go anyway itâs worth checking out. Life it too short to mainline one language. Oh and check out my SVG service ktunprasert/v-github-stats&lt;/p&gt;
    &lt;head rend="h1"&gt;Links&lt;/head&gt;
    &lt;p&gt;vlang - https://vlang.io&lt;lb/&gt; vlang/maps - https://docs.vlang.io/v-types.html#maps&lt;lb/&gt; vlang/structs - https://docs.vlang.io/structs.html&lt;lb/&gt; go-uber/functional-options - https://github.com/uber-go/guide/blob/master/style.md#functional-options&lt;lb/&gt; vlang/trailing-struct-args - https://docs.vlang.io/structs.html#trailing-struct-literal-arguments&lt;lb/&gt; vlang/compile-time-reflection - https://docs.vlang.io/conditional-compilation.html#compile-time-reflection&lt;lb/&gt; vlang/optional-and-result-type - https://docs.vlang.io/type-declarations.html#optionresult-types-and-error-handling&lt;lb/&gt; vlang/enums - https://docs.vlang.io/type-declarations.html#enums&lt;lb/&gt; vlang/lambdas - https://docs.vlang.io/functions-2.html#lambda-expressions&lt;lb/&gt; vlang/array - https://modules.vlang.io/builtin.html#array&lt;lb/&gt; vlang/arrays - https://modules.vlang.io/arrays.html&lt;lb/&gt; svg service - https://github.com/ktunprasert/v-github-stats&lt;lb/&gt; network issue - https://github.com/vlang/v/issues/23717&lt;lb/&gt; vlang/compiletime-code - https://docs.vlang.io/conditional-compilation.html#compile-time-code&lt;lb/&gt; vlang/net.http - https://modules.vlang.io/net.http.html&lt;lb/&gt; vlang/veb - https://modules.vlang.io/veb.html&lt;lb/&gt; vlang/gzip-issue - https://github.com/vlang/v/issues/20865#issuecomment-1955101657&lt;lb/&gt; vlang/ci - https://github.com/vlang/v/actions&lt;lb/&gt; vlang/performance-optimisation - https://docs.vlang.io/performance-tuning.html&lt;lb/&gt; benchmark/coro-sieve-v-vs-go - https://programming-language-benchmarks.vercel.app/v-vs-go&lt;lb/&gt; benchmark/sieve.go - https://github.com/hanabi1224/Programming-Language-Benchmarks/blob/main/bench/algorithm/coro-prime-sieve/1.go&lt;lb/&gt; benchmark/sieve.v - https://github.com/hanabi1224/Programming-Language-Benchmarks/blob/main/bench/algorithm/coro-prime-sieve/1.v&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080808</guid></item><item><title>I Don't Like "AI"</title><link>https://ian.mccowan.space/2024/07/22/ai/</link><description>&lt;doc fingerprint="7e27831d9c6736f7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Don't Like “AI”&lt;/head&gt;
    &lt;p&gt;Since circumstances in my life require that I spend way more time than I want thinking about AI, I figure at least I can maybe get a blog post out of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Not “AI”&lt;/head&gt;
    &lt;p&gt;Okay, so first off: I don’t even like to refer to the recent in-vogue text and image generation tools as “AI.” I think it’s a poorly-defined term that is used more to evoke associations in people’s brains than to be an accurate representation of the technology we have in front of us. Throughout this rant I will refer to things like ChatGPT as “LLMs,” for “Large Language Models.” Because if LLMs are AI, so is autocorrect on your phone. So is every other “chat bot” that uses keyword matching to try to direct you to a website’s help pages to run interference on letting you talk to a human.&lt;/p&gt;
    &lt;head rend="h2"&gt;Its Boosters Are Misanthropic&lt;/head&gt;
    &lt;p&gt;Here’s Sam Altman, OpenAI’s CEO, being a dunce:&lt;/p&gt;
    &lt;p&gt;These quotes are a reference to a 2021 paperEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜. In Conference on Fairness, Accountability, and Transparency (FAccT ’21), March 3–10, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3442188.3445922&lt;lb/&gt; about LLMs. In it, its co-authors Emily Bender and Timnit Gebru define “stochastic parrot” thus:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;…an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In this context, what Altman is saying is clear: when you have thoughts and feelings and use them to express yourself with language, all you’re really doing is putting one word in front of another in a way that’s probabilistically consistent with the vast amount of other language you’ve been exposed to in your life.&lt;/p&gt;
    &lt;p&gt;Here’s some text I probabilistically generated with Sam Altman’s claims and the definition of “stochastic parrot” as input: not only is this wrong, it fucking sucks, and I hate it. I know it is wrong, and I know I hate it, because I read it and I feel disgust as a somatic experience not unlike mild nausea, along with an involuntary twist in my face that I have to consciously suppress and involuntary tenseness in my muscles that I have to consciously relax. My reaction is not just the result of neurons firing in response to linguistic entities. I feel it in my body. And even if I don’t choose to assemble my thoughts into words as I’m doing now, the feeling persists. What about dance? Sport? Can these bodily experiences and all the intuition they require to achieve be reproduced synthetically by consuming enough training data?This paragraph is a patchy, inadequate survey of a small portion of the ground covered in Siri Hustvedt’s tremendous essay “The Delusions of Certainty,” which can be found in her book A Woman Looking At Men Looking At Women. If you are unconvinced by my single paragraph of rebuttal here, please read that essay and consider how many of the delusional certainties discussed in it the humans-as-stochastic-parrots claim rests on.&lt;/p&gt;
    &lt;p&gt;I hate not just that Altman’s unsupported claim debases the richness of actual human experience, but that it does so in such a transparently self-serving way. If humans are just stochastic parrots, of course, anything they can do can be done instead by a language model, such as those conveniently produced by Altman’s company. This minimization of the sophistication of human experience is the counterpart of AI fearmongering, which posits AI as a possible “existential risk” in order to make it seem more powerful than it is. I don’t think Sam Altman really believes what he’s saying here, but whether he does or not, it’s incredibly cynical, and it sucks.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hype Sucks And It’s Encroachingly Ubiquitous&lt;/head&gt;
    &lt;p&gt;The hype around LLMs has been at the level of “fever pitch” for months that feel like years. It’s the most significant invention since the Internet. No, since the printing press. No, since electricity. No, seriously. In an editorial for the Wall Street Journal, Henry KissingerReally? This Henry Kissinger?&lt;lb/&gt; , Eric Schmidt and Daniel Huttenlocher say:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A new technology bids to transform the human cognitive process as it has not been shaken up since the invention of printing. The technology that printed the Gutenberg Bible in 1455 made abstract human thought communicable generally and rapidly. But new technology today reverses that process. Whereas the printing press caused a profusion of modern human thought, the new technology achieves its distillation and elaboration. In the process, it creates a gap between human knowledge and human understanding. If we are to navigate this transformation successfully, new concepts of human thought and interaction with machines will need to be developed. This is the essential challenge of the Age of Artificial Intelligence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Marc Andreessen says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The stakes here are high. The opportunities are profound. AI is quite possibly the most important – and best – thing our civilization has ever created, certainly on par with electricity and microchips, and probably beyond those.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This doesn’t make any fucking sense! You can’t run a so-called AI without electricity and microchips! How can AI be more important than the things its very existence depends on? But then, of course, Andreessen is a venture capitalist who invests in these technologies. So perhaps he’s not a wholly objective and impartial observer here. But so because what we call a “free” country is actually largely controlled by the whims of people like Marc Andreessen, it’s apparently become mandatory for every company on earth to cram some “AI” functionality that no one asked for into their already rapidly-enshittifying product, in order to…? Attract VC funding? Get a bump in their stock price? If I were a more inquisitive or cynical person I might have a better understanding of whatever asinine capitalist incentives are at play here. But we’re just coming off yet another record-breaking heatwave where I live, so I just can’t bring myself to give a shit about capitalist incentives, because look where they’ve gotten us.&lt;/p&gt;
    &lt;p&gt;On the other side of the coin, now that LLMs are in the hands of The People, they’re rapidly using them to flood the entire internet with dubiously-accurate laundered plagiarism. The uselessness of the internet for obtaining actual information has been intensifying for years, what with Google gradually turning their search engine into an advertisement delivery engine and the proliferation of SEO spam. But LLMs are going to make this a hundred times worse. Indeed, they’re already doing so.&lt;/p&gt;
    &lt;head rend="h2"&gt;Its Obsequity Is Annoying And Its Prose Is Vapid&lt;/head&gt;
    &lt;p&gt;Maybe I wouldn’t mind seeing LLM-extruded text everywhere if it were actually pleasant or interesting to read. But it’s not. It’s invariably a flavorless beige paste, as dead on the page as the entity that produced it.&lt;/p&gt;
    &lt;p&gt;Insofar as ChatGPT can be said to have a personality, it is deferential to the point of obsequity. ChatGPT backs down immediately and totally when challenged. Recently I used it to provide me an &lt;code&gt;rsync&lt;/code&gt; command to
transfer some files to my NAS, and it gave me a working command with no
issue. Pretty cool!I never had any doubt that this would work. GPT is
trained on the internet, which is replete with nerds talking about their
nerd shit. Nerds love nothing more than to explain things. As evidence,
see: this whole-ass web site&lt;lb/&gt; The network connection got interrupted, and I asked how to resume the transfer. ChatGPT gave me another command with the &lt;code&gt;--partial&lt;/code&gt; flag, and that worked too. Also pretty cool! Then
I told it that that the &lt;code&gt;--partial&lt;/code&gt; flag that I had just
successfully used doesn’t actually exist. It readily agreed and
“corrected” itself:&lt;/p&gt;
    &lt;p&gt;It’s hard to credit ChatGPT with a lot of new or original inventions, considering that it’s built completely off the stolen text of the actual humans who preceded it, but it seems to have invented a new kind of rhetorical technique which I’m pithily naming the “apology sandwich.” I view anyone who actually wants to be groveled at like this with extreme suspicion, but it does seem to be designed to take advantage of our tendency to anthromorphize, to lead us to think these things are actually intelligent and even sentient. They’re not.&lt;/p&gt;
    &lt;p&gt;Or see this chat where Jason Briggeman presses it on why it fails to generate a limerick that scans when a poetic form with very strict and clear parameters for meter and rhyme should be right up a piece of software’s alley. One half expects it to respond to a correction with “pwease i am just a smol chatbot who is twying its vewwy best.” I would almost prefer Bing Chat’s amoral, hostile intransigence if not for fear that it would spawn a thousand hysterical thinkpieces about “alignment“Alignment” is the term for making sure that some imaginary sentient AI’s incentives are “aligned” with humanity’s so that it doesn’t end up, like, launching nukes or turning us all into paperclips or whatever. Pure science fiction, but that’s the mode the kind of people who write about “alignment” want you to be operating in.&lt;lb/&gt; ” for every day it was allowed to continue running.&lt;/p&gt;
    &lt;head rend="h2"&gt;Its Boosters Don’t Give A Shit About Consent&lt;/head&gt;
    &lt;p&gt;It seems like barely a day goes by when I don’t read about some company adding a clause to their user agreement saying they’re going to be training LLMs on your data. This training is never opt-in. It’s enabled by default, and it will be happening unless you (a) find out about it and (b) wade through the inevitable thicket of dark patterns lovingly grown between you and the setting to turn it off. Assuming there is one.&lt;/p&gt;
    &lt;p&gt;But of course, why wouldn’t companies that use LLMs do this? Using people’s work without their consent is literally the foundation upon which LLMs are built. So of course they’re going to see nothing wrong with silently opting-in their users, or ignoring decades-old web scraping conventions, or selling their users’ data for LLM training. It’s the most important thing since electricity! Why do you hate progress???&lt;/p&gt;
    &lt;head rend="h2"&gt;So What Am I Even Fucking Doing Here&lt;/head&gt;
    &lt;p&gt;“Other people’s work,” in the last paragraph, includes mine, of course:&lt;/p&gt;
    &lt;p&gt;And so it’s like, why keep posting on the internet if these assholes are just going to scrape it into their insatiable maw, along with a million other actual people’s actual hard work, to be shat out in an unrecognizable(though, come to think of it, recognizable would be even worse)&lt;lb/&gt; homogeneous, anodyne slurry? I don’t think I’m a great writer but I think I’m OK at it, and I can’t put my feelings about this any better than maya, whose maya.land is the kind of intensely unique and flavorful and human old-school personal website that the web should be aspiring to—&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This site has been too much of an experiment for me to claim now that I have a grand theory about it, about similar sites – but certainly by now I have instincts, feelings… …and making my writing available for automated summarization? So someone can sell ads by a depersonalized version of my stuff? The feeling is nausea.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;—in her incredibly relatable “mr. openai i don’t feel so good.” Or, to TL;DR the very good How To Find Things Online, which you should also actually R, What Even Is The Point Anymore.I know it had already been over a year since my last blog post when ChatGPT arrived on the scene. But it’s the principle of the thing.&lt;/p&gt;
    &lt;p&gt;Maybe I am engaged in the valuable, noble act of Building A Stronger Web Without AI. Maybe by linking to real humans using their hard-earned brain cells to write thoughtful words on the internet, I can help make a web that feels like a web, interconnected and constructed by individuals, one not dependent on the whims of multi-billion-dollar advertising companies. Maybe I just want to add my voice to the others saying this stuff is bad, morally and aesthetically. Maybe since it contributed to taking away what little wind I had in my sails for blogging in the last couple years, it seemed only suitable that I use my first post back to slag it off. Anyway, here you go, ChatGPT. Go tell people how much I think you suck.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45080819</guid></item></channel></rss>