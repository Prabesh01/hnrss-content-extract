<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 14 Oct 2025 11:09:06 +0000</lastBuildDate><item><title>Show HN: SQLite Online ‚Äì 11 years of solo development, 11K daily users</title><link>https://sqliteonline.com/</link><description>&lt;doc fingerprint="5120987566fd4fbd"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Chart for Data Science&lt;/head&gt;
    &lt;code&gt;-- Change first word "SELECT" to "QLINE-SELECT"&lt;/code&gt;
    &lt;quote&gt;SELECT QLINE-SELECT&lt;/quote&gt;
    &lt;code&gt;√¢&lt;/code&gt;
    &lt;code&gt;-- Axis X:&lt;/code&gt;
    &lt;code&gt;-- X - column name, axis: x1, x2, ..xn Value: Number&lt;/code&gt;
    &lt;code&gt;-- L - column name, axis: l Value: Text&lt;/code&gt;
    &lt;code&gt;-- T - column name, axis: t Value: UnixTime Number&lt;/code&gt;
    &lt;code&gt;-- Axis Y:&lt;/code&gt;
    &lt;code&gt;-- Y - column name, axis: y1, y2, ..yn Value: Number&lt;/code&gt;
    &lt;code&gt;-- Y - color line: y_cFF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- Option:&lt;/code&gt;
    &lt;code&gt;-- C - color point: c  Value: FF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- V - radius point: v  Value: Number&lt;/code&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QLINE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QAREA-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBAR-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QPIE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBUBBLE-SELECT example&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567770</guid><pubDate>Mon, 13 Oct 2025 12:46:52 +0000</pubDate></item><item><title>No science, no startups: The innovation engine we're switching off</title><link>https://steveblank.com/2025/10/13/no-science-no-startups-the-unseen-engine-were-switching-off/</link><description>&lt;doc fingerprint="3fc8918d9d618749"&gt;
  &lt;main&gt;
    &lt;p&gt;Tons of words have been written about the Trump Administrations war on Science in Universities. But few people have asked what, exactly, is science? How does it work? Who are the scientists? What do they do? And more importantly, why should anyone (outside of universities) care?&lt;/p&gt;
    &lt;p&gt;(Unfortunately, you won‚Äôt see answers to these questions in the general press ‚Äì it‚Äôs not clickbait enough. Nor will you read about it in the science journals‚Äì it‚Äôs not technical enough. You won‚Äôt hear a succinct description from any of the universities under fire, either ‚Äì they‚Äôve long lost the ability to connect the value of their work to the day-to-day life of the general public.)&lt;/p&gt;
    &lt;p&gt;In this post I‚Äôm going to describe how science works, how science and engineering have worked together to build innovative startups and companies in the U.S.‚Äîand why you should care.&lt;/p&gt;
    &lt;p&gt;(In a previous post I described how the U.S. built a science and technology ecosystem and why investment in science is directly correlated with a country‚Äôs national power. I suggest you read it first.)&lt;/p&gt;
    &lt;p&gt;How Science Works&lt;lb/&gt; I was older than I care to admit when I finally understood the difference between a scientist, an engineer, an entrepreneur and a venture capitalist; and the role that each played in the creation of advancements that made our economy thrive, our defense strong and America great.&lt;/p&gt;
    &lt;p&gt;Scientists&lt;lb/&gt; Scientists (sometimes called researchers) are the people who ask lots of questions about why and how things work. They don‚Äôt know the answers. Scientists are driven by curiosity, willing to make educated guesses (the fancy word is hypotheses) and run experiments to test their guesses. Most of the time their hypotheses are wrong. But every time they‚Äôre right they move the human race forward. We get new medicines, cures for diseases, new consumer goods, better and cheaper foods, etc.&lt;/p&gt;
    &lt;p&gt;Scientists tend to specialize in one area ‚Äì biology, medical research, physics, agriculture, computer science, materials, math, etc. ‚Äî although a few move between areas. The U.S. government has supported scientific research at scale (read billions of $s) since 1940.&lt;/p&gt;
    &lt;p&gt;Scientists tend to fall into two categories: Theorists and Experimentalists.&lt;/p&gt;
    &lt;p&gt;Theorists&lt;lb/&gt; Theorists develop mathematical models, abstract frameworks, and hypotheses for how the universe works. They don‚Äôt run experiments themselves‚Äîinstead, they propose new ideas or principles, explain existing experimental results, predict phenomena that haven‚Äôt been observed yet. Theorists help define what reality might be.&lt;/p&gt;
    &lt;p&gt;Theorists can be found in different fields of science. For example:&lt;/p&gt;
    &lt;p&gt;Physics Quantum field theory, string theory, quantum mechanics&lt;lb/&gt; Biology Neuroscience and cognition, Systems Biology, gene regulation&lt;lb/&gt; Chemistry Molecular dynamics, Quantum chemistry&lt;lb/&gt; Computer Science Design algorithms, prove limits of computation&lt;lb/&gt; Economics Build models of markets or decision-making&lt;lb/&gt; Mathematics Causal inference, Bayesian networks, Deep Learning&lt;/p&gt;
    &lt;p&gt;The best-known 20th-century theorist was Albert Einstein. His tools were a chalkboard and his brain. in 1905 he wrote an equation E=MC2 which told the world that a small amount of mass can be converted into a tremendous amount of energy. When he wrote it down, it was just theory. Other theorists in the 1930s and ‚Äô40s took Einstein‚Äôs theory and provided the impetus for building the atomic bomb. (Leo Szilard conceived neutron chain reaction idea, Hans Bethe led the Theoretical Division at Los Alamos, Edward Teller developed hydrogen bomb theory.) Einstein‚Äôs theory was demonstrably proved correct over Hiroshima and Nagasaki.&lt;/p&gt;
    &lt;p&gt;Experimentalists&lt;lb/&gt; In addition to theorists, other scientists ‚Äì called experimentalists ‚Äì design and run experiments in a lab. The pictures you see of scientists in lab coats in front of microscopes, test tubes, particle accelerators or NASA spacecraft are likely experimentalists. They test hypotheses by developing and performing experiments. An example of this would be NASA‚Äôs James Webb telescope or the LIGO Gravitational-Wave Observatory experiment. (As we‚Äôll see later, often it‚Äôs engineers who build the devices the experimentalists use.)&lt;/p&gt;
    &lt;p&gt;Some of these experimentalists focus on Basic Science, working to get knowledge for its own sake and understand fundamental principles of nature with no immediate practical use in mind.&lt;/p&gt;
    &lt;p&gt;Other experimentalists work in Applied Science, which uses the findings and theories derived from Basic Science to design, innovate, and improve products and processes.&lt;/p&gt;
    &lt;p&gt;Applied scientists solve practical problems oriented toward real-world applications. (Scientists at Los Alamos weretrying to understand the critical mass of U-235 (the minimum amount that would explode.) Basic science lays the groundwork for breakthroughs in applied science. For instance: Quantum mechanics (basic science) led to semiconductors which led to computers (applied science). Germ theory (basic science) led to antibiotics and vaccines (applied science). In the 20th century Applied scientists did not start the companies that make end products. Engineers and entrepreneurs did this. (In the 21st century more Applied Scientists, particularly in life sciences, have also spun out companies from their labs.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientists&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Where is Science in the U.S. Done?&lt;lb/&gt; America‚Äôs unique insight that has allowed it to dominate Science and invention, is that after WWII we gave Research and Development money to universities, rather than only funding government laboratories. No other country did this at scale.&lt;/p&gt;
    &lt;p&gt;Corporate Research Centers&lt;lb/&gt; In the 20th century, U.S. companies put their excess profits into corporate research labs. Basic research in the U.S. was done in at Dupont, Bell Labs, IBM, AT&amp;amp;T, Xerox, Kodak, GE, et al.&lt;/p&gt;
    &lt;p&gt;This changed in 1982, when the Securities and Exchange Commission ruled that it was legal for companies to buy their own stock (reducing the number of shares available to the public and inflating their stock price.) Very quickly Basic Science in corporate research all but disappeared. Companies focused on Applied Research to maximize shareholder value. In its place, Theory and Basic research is now done in research universities.&lt;/p&gt;
    &lt;p&gt;Research Universities&lt;lb/&gt; From the outside (or if you‚Äôre an undergraduate) universities look like a place where students take classes and get a degree. However, in a research university there is something equally important going on. Science faculty in these schools not only teach, but they are expected to produce new knowledge‚Äîthrough experiments, publications, patents, or creative work. Professors get grants and contracts from federal agencies (e.g., NSF, NIH, DoD), foundations, and industry. And the university builds Labs, centers, libraries, and advanced computing facilities that support these activities.&lt;/p&gt;
    &lt;p&gt;In the U.S. there are 542 research universities, ranked by the Carnegie Classification into three categories.&lt;/p&gt;
    &lt;p&gt;R1: 187 Universities ‚Äì Very High Research Activity&lt;lb/&gt; Conduct extensive research and award many doctoral degrees.&lt;lb/&gt; Examples: Stanford, UC Berkeley, Harvard, MIT, Michigan, Texas A&amp;amp;M ‚Ä¶&lt;/p&gt;
    &lt;p&gt;R2: 139 Universities ‚Äì High Research Activity&lt;lb/&gt; Substantial but smaller research scale.&lt;lb/&gt; Examples: Baylor, Wake Forest, UC Santa Cruz, ‚Ä¶&lt;/p&gt;
    &lt;p&gt;R3: 216 Research Colleges/Universities&lt;lb/&gt; Limited research focus; more teaching-oriented doctoral programs.&lt;lb/&gt; Smaller state universities&lt;/p&gt;
    &lt;p&gt;Why Universities Matter to Science&lt;lb/&gt; U.S. universities perform about 50% of all basic science research (physics, chemistry, biology, social sciences, etc.) because they are training grounds for graduate students and postdocs. Universities spend ~$109 billion a year on research. ~$60 billion of that $109 billion comes from the National Institutes for Health (NIH) for biomedical research, National Science Foundation (NSF) for basic science, Department of War (DoW), Department of Energy (DOE), for energy/physics/nuclear, DARPA, NASA. (Companies tend to invest in applied research and development, that leads directly to saleable products.)&lt;/p&gt;
    &lt;p&gt;Professors (especially in Science, Technology, Engineering and Math) run labs that function like mini startups. They ask research questions, then hire grad students, postdocs, and staff and write grant proposals to fund their work, often spending 30‚Äì50% of their time writing and managing grants. When they get a grant the lead researcher (typically a faculty member/head of the lab) is called the Principal Investigator (PI).&lt;/p&gt;
    &lt;p&gt;The Labs are both workplaces and classrooms. Graduate students and Postdocs do the day-to-day science work as part of their training (often for a Ph.D.). Postdocs are full-time researchers gaining further specialization. Undergraduates may also assist in research, especially at top-tier schools.&lt;/p&gt;
    &lt;p&gt;(Up until 2025, U.S. science was deeply international with ~40‚Äì50% of U.S. basic research done by foreign-born researchers (graduate students, postdocs, and faculty). Immigration and student visas were a critical part of American research capacity.)&lt;/p&gt;
    &lt;p&gt;The results of this research are shared with the agencies that funded it, published in journals, presented at conferences and often patented or spun off into startups via technology transfer offices. A lot of commercial tech‚Äîfrom Google search to CRISPR‚Äîstarted in university labs.&lt;/p&gt;
    &lt;p&gt;Universities support their science researchers with basic administrative staff (for compliance, purchasing, and safety) but uniquely in the U.S., by providing the best research facilities (labs, cleanrooms, telescopes), and core scientific services: DNA sequencing centers, electron microscopes, access to cloud, data analysis hubs, etc. These were the best in the world ‚Äì until the sweeping cuts in 2025.&lt;/p&gt;
    &lt;p&gt;Engineers Build on the Work of Scientists&lt;lb/&gt; Engineers design and build things on top of the discoveries of scientists. For example, seven years after scientists split the atom, it took 10s of thousands of engineers to build an atomic bomb. From the outset, the engineers knew what they wanted to build because of the basic and applied scientific research that came before them.&lt;/p&gt;
    &lt;p&gt;Scientists Versus Engineers&lt;/p&gt;
    &lt;p&gt;Engineers create plans, use software to test their designs, then‚Ä¶ cut sheet metal, build rocket engines, construct buildings and bridges, design chips, build equipment for experimentalists, design cars, etc.&lt;/p&gt;
    &lt;p&gt;As an example, at Nvidia their GPU chips are built in a chip factory (TSMC) using the Applied science done by companies like Applied Materials which in turn is based on Basic science of semiconductor researchers. And the massive data centers OpenAI, Microsoft, Google, et al that use Nvidia chips are being built by mechanical and other types of engineers.&lt;/p&gt;
    &lt;p&gt;My favorite example is that the reusable SpaceX rocket landings are made possible by the Applied Science research on Convex Optimization frameworks and algorithms by Steven Boyd of Stanford. And Boyd‚Äôs work was based on the Basic science mathematical field of convex analysis (SpaceX, NASA, JPL, Blue Origin, Rocket Lab all use variations of Convex Optimization for guidance, control, and landing.)&lt;/p&gt;
    &lt;p&gt;Startup Entrepreneurs Build Iteratively and Incrementally&lt;lb/&gt; Entrepreneurs build companies to bring new products to market. They hire engineers to build, test and refine products.&lt;/p&gt;
    &lt;p&gt;Engineers and entrepreneurs operate with very different mindsets, goals, and tolerances for risk and failure. (Many great entrepreneurs start as engineers e.g., Musk, Gates, Page/Brin). An engineer‚Äôs goal is to design and deliver a solution to a known problem with a given set of specifications.&lt;/p&gt;
    &lt;p&gt;In contrast, entrepreneurs start with a series of unknowns about who are the customers, what are the wanted product features, pricing, etc. They retire each of these risks by building an iterative series of minimum viable products to find product/market fit and customer adoption. They pivot their solution as needed when they discover their initial assumptions are incorrect. (Treating each business unknown as a hypothesis is the entrepreneurs‚Äô version of the Scientific Method.)&lt;/p&gt;
    &lt;p&gt;Venture Capitalists Fund Entrepreneurs&lt;lb/&gt; Venture capitalists (VCs) are the people who fund entrepreneurs who work with engineers who build things that applied scientists have proven from basic researchers.&lt;/p&gt;
    &lt;p&gt;Unlike banks which will give out loans for projects that have known specifications and outcomes, VCs invest in a portfolio of much riskier investments. While banks make money on the interest they charge on each loan, VCs take part ownership (equity) in the companies they invest in. While most VC investments fail, the ones that succeed make up for that.&lt;/p&gt;
    &lt;p&gt;Most VCs are not scientists. Few are engineers, some have been entrepreneurs. The best VCs understand technical trends and their investments help shape the future. VCs do not invest in science/researchers. VCs want to minimize the risk of their investment, so they mostly want to take engineering and manufacturing risk, but less so on applied science risk and rarely on basic research risk. Hence the role of government and Universities.&lt;/p&gt;
    &lt;p&gt;VCs invest in projects that can take advantage of science and deliver products within the time horizon of their funds (3‚Äì7 years). Science often needs decades before a killer app is visible.&lt;/p&gt;
    &lt;p&gt;As the flow of science-based technologies dries up, the opportunities for U.S. venture capital based on deep tech will decline, with its future in countries that are investing in science ‚Äì China or Europe.&lt;/p&gt;
    &lt;p&gt;Why Have Scientists? Why Not Just a Country of Engineers, Entrepreneurs and VCs (or AI)?&lt;lb/&gt; If you‚Äôve read so far, you might be scratching your head and asking, ‚ÄúWhy do we have scientists at all? Why pay for people to sit around and think? Why spend money on people who run experiments when most of those experiments fail? Can‚Äôt we replace them with AI?‚Äù&lt;/p&gt;
    &lt;p&gt;The output of this university-industry-government science partnership became the foundation of Silicon Valley, the aerospace sector, the biotechnology industry, Quantum and AI. These investments gave us rockets, cures for cancer, medical devices, the Internet, Chat GPT, AI and more.&lt;/p&gt;
    &lt;p&gt;Investment in science is directly correlated with national power. Weaken science, you weaken the long-term growth of the economy, and national defense.&lt;/p&gt;
    &lt;p&gt;Tech firms‚Äô investments of $100s of billions in AI data centers is greater than the federal government‚Äôs R&amp;amp;D expenditures. But these investments are in engineering not in science. The goal of making scientists redundant using artificial general intelligence misses the point that AI will (and is) making scientists more productive ‚Äì not replacing them.&lt;/p&gt;
    &lt;p&gt;Countries that neglect science become dependent on those that don‚Äôt. U.S. post-WWII dominance came from basic science investments (OSRD, NSF, NIH, DOE labs). After WWII ended, the UK slashed science investment which allowed the U.S. to commercialize the British inventions made during the war.&lt;/p&gt;
    &lt;p&gt;The Soviet Union‚Äôs collapse partly reflected failure to convert science into sustained innovation, during the same time that U.S. universities, startups and venture capital created Silicon Valley. Long-term military and economic advantage (nuclear weapons, GPS, AI) trace back to scientific research ecosystems.&lt;/p&gt;
    &lt;p&gt;Lessons Learned&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Scientists come in two categories&lt;/item&gt;
      &lt;item&gt;Theorists and experimentalists&lt;/item&gt;
      &lt;item&gt;Two types of experimentalists; Basic science (learn new things) or applied science (practical applications of the science)&lt;/item&gt;
      &lt;item&gt;Scientists train talent, create patentable inventions and solutions for national defense&lt;/item&gt;
      &lt;item&gt;Engineers design and build things on top of the discoveries of scientists&lt;/item&gt;
      &lt;item&gt;Entrepreneurs test and push the boundaries of what products could be built&lt;/item&gt;
      &lt;item&gt;Venture Capital provides the money to startups&lt;/item&gt;
      &lt;item&gt;Scientists, engineers, entrepreneurs ‚Äì these roles are complementary&lt;/item&gt;
      &lt;item&gt;Remove one and the system degrades&lt;/item&gt;
      &lt;item&gt;Science won‚Äôt stop&lt;/item&gt;
      &lt;item&gt;Cut U.S. funding, then science will happen in other countries that understand its relationship to making a nation great ‚Äì like China.&lt;/item&gt;
      &lt;item&gt;National power is derived from investments in Science&lt;/item&gt;
      &lt;item&gt;Reducing investment in basic and applied science makes America weak&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;Appendix ‚Äì How Does Science Work? ‚Äì The Scientific Method&lt;lb/&gt; Whether you were a theorist or experimentalist, for the last 500 years the way to test science was by using the scientific method. This method starts by a scientist wondering and asking, ‚ÄúHere‚Äôs how I think this should work, let‚Äôs test the idea.‚Äù&lt;/p&gt;
    &lt;p&gt;The goal of the scientific method is to turn a guess (in science called a hypothesis) into actual evidence. Scientists do this by first designing an experiment to test their guess/hypothesis. They then run the experiment and collect and analyze the result and ask, ‚ÄúDid the result validate, invalidate the hypothesis? Or did it give us completely new ideas?‚Äù Scientists build instruments and run experiments not because of what they know, but because of what they don‚Äôt know.&lt;/p&gt;
    &lt;p&gt;These experiments can be simple ones costing thousands of dollars that can be run in a university biology lab while others may require billions of dollars to build a satellite, particle accelerator or telescope. (The U.S. took the lead in Science after WWII when the government realized that funding scientists was good for the American economy and defense.)&lt;/p&gt;
    &lt;p&gt;Good science is reproducible. Scientists just don‚Äôt publish their results, but they also publish the details of how they ran their experiment. That allows other scientists to run the same experiment and see if they get the same result for themselves. That makes the scientific method self-correcting (you or others can see mistakes).&lt;/p&gt;
    &lt;p&gt;One other benefit of the scientific method is that scientists (and the people who fund them) expect most of the experiments to fail, but the failures are part of learning and discovery. They teach us what works and what doesn‚Äôt. Failure in science testing unknowns means learning and discovery.&lt;/p&gt;
    &lt;p&gt;Filed under: National Security, NIH (National Institutes of Health), NSF (National Science Foundation) |&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567877</guid><pubDate>Mon, 13 Oct 2025 13:02:20 +0000</pubDate></item><item><title>Smartphones and being present</title><link>https://herman.bearblog.dev/being-present/</link><description>&lt;doc fingerprint="3411991d2ac390d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Smartphones and being present&lt;/head&gt;
    &lt;p&gt;I read an article yesterday, stating that on average, people spend 4 hours and 37 minutes on their phones per day1, with South Africans coming in fourth highest in the world at a whopping 5 hours and 11 minutes2.&lt;/p&gt;
    &lt;p&gt;This figure seems really high to me. If we assume people sleep roughly 8 hours per day, that means that one third of their day is spent on their phones. If we also assume people work 8 hours per day (ignoring the fact that they may be using their phones during work hours), that suggests that people spend over half of their free time (and up to 65% of it) glued to their screens.&lt;/p&gt;
    &lt;p&gt;I never wanted to carry the internet around in my pocket. It's too distracting and pulls me out of the present moment, fracturing my attention. I've tried switching to old-school black and white phones before, but always begrudgingly returned to using a smartphone due to the utility of it. The problem, however, is that it comes with too many attention sinks tucked in alongside the useful tools.&lt;/p&gt;
    &lt;p&gt;I care about living an intentional and meaningful life, nurturing relationships, having nuanced conversations, and enjoying the world around me. I don't want to spend this limited time I have on earth watching short form video and getting into arguments on Twitter.&lt;/p&gt;
    &lt;p&gt;This is what I enjoy. Picture taken yesterday in Scarborough, South Africa.&lt;/p&gt;
    &lt;p&gt;I've written at length about how I manage my digital consumption, from turning off notifications to forgoing social media entirely. The underlying premise here is that if you're trying to lose weight, you shouldn't carry cookies around in your pockets. And my phone is the bag of cookies in this metaphor.&lt;/p&gt;
    &lt;p&gt;We're wired to seek out distraction, novel information, and entertainment, and avoid boredom at all costs. But boredom is where creativity and self-reflection do their best work. It's why "all the best ideas come when you're in the shower"‚Äîwe don't usually take our phones with us into the shower (yet).&lt;/p&gt;
    &lt;p&gt;According to Screen Time on my iPhone, on average I spend 30 minutes per day on it, which I think is reasonable, especially considering the most-used apps are by-and-large utility apps like banking and messages. This isn't because I have more self-control than other people. I don't think I do. It's because I know myself, and have set up my digital life to be a positive force, and not an uninspired time-sink.&lt;/p&gt;
    &lt;p&gt;There are many apps and systems to incentivise better relationships with our phones, mostly based around time limits. But these are flawed in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I'm an adult, I know how to circumvent these limits, and I will if motivation is low.&lt;/item&gt;
      &lt;item&gt;Time limits don't affect the underlying addiction. You don't quit smoking by only smoking certain hours of the day.&lt;/item&gt;
      &lt;item&gt;The companies that build these apps have tens of thousands of really smart people (and billions of dollars) trying to get me hooked and keep me engaged. The only way to win this game isn't by trying to beat them (I certainly can't), but by not playing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The only way I've found to have a good relationship with my phone is to make it as uninteresting as possible. The first way is to not have recommendation media (think Instagram, TikTok, and all the rest). I'm pro deleting these accounts completely, because it's really easy to re-download the apps on a whim, or visit them in-browser. However some people have found that having them on a dedicated device works by isolating those activities. Something like a tablet at home that is "the only place you're allowed to use Instagram". I can't comment too much on this route, but it seems reasonable.&lt;/p&gt;
    &lt;p&gt;My biggest time sink over the past few years has been YouTube. The algorithm knew me too well and would recommend video after engaging, but ultimately useless video. I could easily burn an entire evening watching absolute junk‚Äîleaving me feeling like I'd just wasted what could have otherwise been a beautiful sunset or a tasty home-cooked lasagne. However, at the beginning of this year I learnt that you can turn off your YouTube watch history entirely, which means no recommendations. Here's what my YouTube home screen now looks like:&lt;/p&gt;
    &lt;p&gt;Without the recommendations I very quickly run out of things to watch from the channels I'm subscribed to. It's completely changed my relationship with YouTube since I only watch the videos I actually want to watch, and none of the attention traps. You can turn off your YouTube watch history here, and auto delete your other Google history (like historic searches and navigation) here, which I think is just good practice.&lt;/p&gt;
    &lt;p&gt;I also used my adblocker, AdGuard on Safari which has a useful "block element" feature, to block the recommended videos on the right of YouTube videos. I use this feature to hide shorts as well, since I have no interest in watching them either, and YouTube intentionally makes them impossible to remove. If you're interested in a similar setup, here are the selectors I use to block those elements:&lt;/p&gt;
    &lt;code&gt;youtube.com###items &amp;gt; ytd-item-section-renderer.style-scope.ytd-watch-next-secondary-results-renderer:last-child
youtube.com###sections
youtube.com##[is-shorts]
youtube.com###secondary
&lt;/code&gt;
    &lt;p&gt;The only media that I do sometimes consume on my phone are my RSS feeds, but it's something I'm completely comfortable with since it's explicitly opt-in by design and low volume.&lt;/p&gt;
    &lt;p&gt;While I still have the twitch to check my phone when I'm waiting for a coffee, or in-between activities‚Äîbecause my brain's reward system has been trained to do this‚ÄîI'm now rewarded with nothing. Over time, I find myself checking my phone less and less. Sometimes I notice the urge, and just let it go, instead focusing on the here and now.&lt;/p&gt;
    &lt;p&gt;I think that while the attention-span-degrading effects of recommendation media are getting most of the headlines, what isn't spoken about as much is the sheer number of hours lost globally to our phones (3.8 million years per day, according to my back-of-the-napkin-math). And while people may argue that this could involve productive work or enjoyable leisure, I suspect that the vast (vast!) majority of that time is short-form entertainment.&lt;/p&gt;
    &lt;p&gt;My solution may sound overkill to many people, but I can say with absolute certainty that it has turned me into a more present, less distracted, and more optimistic person. I have much more time to spend in nature, with friends, or on my hobbies and projects. I can't imagine trading it in for a tiny screen, ever.&lt;/p&gt;
    &lt;p&gt;Give it a try.&lt;/p&gt;
    &lt;p&gt;Happily on the beach for sunset.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568613</guid><pubDate>Mon, 13 Oct 2025 14:20:33 +0000</pubDate></item><item><title>Software update bricks some Jeep 4xe hybrids over the weekend</title><link>https://arstechnica.com/cars/2025/10/software-update-bricks-some-jeep-4xe-hybrids-over-the-weekend/</link><description>&lt;doc fingerprint="155561cbaa040794"&gt;
  &lt;main&gt;
    &lt;p&gt;Owners of some Jeep Wrangler 4xe hybrids have been left stranded after installing an over-the-air software update this weekend. The automaker pushed out a telematics update for the Uconnect infotainment system that evidently wasn't ready, resulting in cars losing power while driving and then becoming stranded.&lt;/p&gt;
    &lt;p&gt;Stranded Jeep owners have been detailing their experiences in forum and Reddit posts, as well as on YouTube. The buggy update doesn't appear to brick the car immediately. Instead, the failure appears to occur while driving‚Äîa far more serious problem. For some, this happened close to home and at low speed, but others claim to have experienced a powertrain failure at highway speeds.&lt;/p&gt;
    &lt;p&gt;Jeep pulled the update after reports of problems, but the software had already downloaded to many owners' cars by then. A member of Stellantis' social engagement team told 4xe owners at a Jeep forum to ignore the update pop-up if they haven't installed it yet.&lt;/p&gt;
    &lt;p&gt;Owners were also advised to avoid using either hybrid or electric modes if they had updated their 4xe and not already suffered a powertrain failure. Yesterday, Jeep pushed out a fix.&lt;/p&gt;
    &lt;p&gt;As Crowdstrike showed last year, Friday afternoons are a bad time to push out a software update. Now Stellantis has learned that lesson, too. Ars has reached out to Stellantis, and we'll update this post if we get a reply.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568700</guid><pubDate>Mon, 13 Oct 2025 14:28:25 +0000</pubDate></item><item><title>America is getting an AI gold rush instead of a factory boom</title><link>https://www.washingtonpost.com/business/2025/10/13/manufacturing-artificial-intelligence/</link><description>&lt;doc fingerprint="f35908d52b294ba0"&gt;
  &lt;main&gt;&lt;p&gt;A gulf is opening up in the heart of American business as two industries championed as central to the country‚Äôs future ‚Äî manufacturing and artificial intelligence ‚Äî appear to be heading in different directions.&lt;/p&gt;&lt;p&gt;By Aaron Gregg&lt;/p&gt; and &lt;list rend="ul"&gt;&lt;item&gt;1Shannon OsakaMicroplastics are everywhere. You can do one simple thing to avoid them.&lt;/item&gt;&lt;item&gt;2Shannon NajmabadiandAaron GreggAirports say they won‚Äôt air Kristi Noem shutdown video at TSA checkpoints&lt;/item&gt;&lt;item&gt;3Scott NoverNews outlets broadly reject Pentagon rules before signing deadline&lt;/item&gt;&lt;item&gt;4Guest columnSi LibermanI‚Äôm 101 years old. Here are 7 things I think are ‚Äòlongevity secrets.‚Äô&lt;/item&gt;&lt;item&gt;5OpinionMax BootWhy the Gaza ceasefire won‚Äôt lead to lasting peace&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568915</guid><pubDate>Mon, 13 Oct 2025 14:48:47 +0000</pubDate></item><item><title>NanoChat ‚Äì The best ChatGPT that $100 can buy</title><link>https://github.com/karpathy/nanochat</link><description>&lt;doc fingerprint="8c198122f1657e6"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;The best ChatGPT that $100 can buy.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like speedrun.sh, that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.&lt;/p&gt;
    &lt;p&gt;The fastest way to feel the magic is to run the speedrun script speedrun.sh, which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like Lambda), and kick off the training script:&lt;/p&gt;
    &lt;code&gt;bash speedrun.sh&lt;/code&gt;
    &lt;p&gt;Alternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session &lt;code&gt;speedrun&lt;/code&gt; (and also log output to &lt;code&gt;speedrun.log&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;screen -L -Logfile speedrun.log -S speedrun bash speedrun.sh&lt;/code&gt;
    &lt;p&gt;See the screen cheatsheet if you are less familiar. You can watch it go inside the screen session, or detach with &lt;code&gt;Ctrl-a d&lt;/code&gt; and &lt;code&gt;tail speedrun.log&lt;/code&gt; to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run &lt;code&gt;source .venv/bin/activate&lt;/code&gt;), and serve it:&lt;/p&gt;
    &lt;code&gt;python -m scripts.chat_web&lt;/code&gt;
    &lt;p&gt;And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example http://209.20.xxx.xxx:8000/, etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).&lt;/p&gt;
    &lt;p&gt;You can also &lt;code&gt;cat report.md&lt;/code&gt; file which appeared in the project directory and contains the "report card" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Characters: 333,989&lt;/item&gt;
      &lt;item&gt;Lines: 8,304&lt;/item&gt;
      &lt;item&gt;Files: 44&lt;/item&gt;
      &lt;item&gt;Tokens (approx): 83,497&lt;/item&gt;
      &lt;item&gt;Dependencies (uv.lock lines): 2,004&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;BASE&lt;/cell&gt;
        &lt;cell role="head"&gt;MID&lt;/cell&gt;
        &lt;cell role="head"&gt;SFT&lt;/cell&gt;
        &lt;cell role="head"&gt;RL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CORE&lt;/cell&gt;
        &lt;cell&gt;0.2219&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ARC-Challenge&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.2875&lt;/cell&gt;
        &lt;cell&gt;0.2807&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ARC-Easy&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.3561&lt;/cell&gt;
        &lt;cell&gt;0.3876&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;GSM8K&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0250&lt;/cell&gt;
        &lt;cell&gt;0.0455&lt;/cell&gt;
        &lt;cell&gt;0.0758&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HumanEval&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0671&lt;/cell&gt;
        &lt;cell&gt;0.0854&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMLU&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.3111&lt;/cell&gt;
        &lt;cell&gt;0.3151&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatCORE&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0730&lt;/cell&gt;
        &lt;cell&gt;0.0884&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Total wall clock time: 3h51m&lt;/p&gt;
    &lt;p&gt;(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: "Introducing nanochat: The best ChatGPT that $100 can buy".&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12 hours, which slightly outperforms GPT-2 CORE score. Second is the $1000 tier (~41.6 hours), just because it's a nice round number. But both of these are not yet fully supported and therefore not attached here in the master branch yet.&lt;/p&gt;
    &lt;p&gt;That said, to give a sense, the example changes needed for the speedrun.sh file to train a GPT-2 grade model d26 only involve three changes:&lt;/p&gt;
    &lt;code&gt;...
# you'll need to download more data shards for pretraining
# get the number of parameters, multiply 20 to get tokens, multiply by 4.8 to get chars,
# divide by 250 million to get number of shards. todo need to improve this...
python -m nanochat.dataset -n 450 &amp;amp;
...
# use --depth to increase model size. to not oom, halve device batch size 32 -&amp;gt; 16:
torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- --depth=26 --device_batch_size=16
...
# make sure to use the same later during midtraining:
torchrun --standalone --nproc_per_node=8 -m scripts.mid_train -- --device_batch_size=16&lt;/code&gt;
    &lt;p&gt;That's it! The biggest thing to pay attention to is making sure you have enough data shards to train on (the code will loop and do more epochs over the same training set otherwise, decreasing learning speed a bit), and managing your memory/VRAM, primarily by decreasing the &lt;code&gt;device_batch_size&lt;/code&gt; until things fit (the scripts automatically compensates by increasing the number of gradient accumulation loops, simply turning parallel compute to sequential compute).&lt;/p&gt;
    &lt;p&gt;And a bit more about computing environments that will run nanochat:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.&lt;/item&gt;
      &lt;item&gt;All code will run just fine on even a single GPU by omitting &lt;code&gt;torchrun&lt;/code&gt;, and will produce ~identical results (code will automatically switch to gradient accumulation), but you'll have to wait 8 times longer.&lt;/item&gt;
      &lt;item&gt;If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for &lt;code&gt;--device_batch_size&lt;/code&gt;in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you'll have to know a bit more what you're doing and get more creative.&lt;/item&gt;
      &lt;item&gt;Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven't implemented this out of the box so it might take a bit of tinkering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;nanochat is designed to be short and sweet. One big advantage of this is that we can package up all of the files together and copy paste them to your favorite LLM to ask arbitrary questions. As an example, I like to package up the repo using the files-to-prompt utility like so:&lt;/p&gt;
    &lt;code&gt;files-to-prompt . -e py -e md -e rs -e html -e toml -e sh --ignore "*target*" --cxml &amp;gt; packaged.txt&lt;/code&gt;
    &lt;p&gt;This includes all py, rs, html, toml, sh files, excludes the &lt;code&gt;rustbpe/target&lt;/code&gt; folder, and chooses the cxml output format. Everything is written to the &lt;code&gt;packaged.txt&lt;/code&gt; file, which atm measures ~330KB (i.e. well below ~100K tokens for a state of the art LLM), and ~8K lines of code in 45 files.&lt;/p&gt;
    &lt;p&gt;Alternatively, I recommend using DeepWiki from Devin/Cognition to ask questions of this repo. In the URL of this repo, simply change github.com to deepwiki.com, and you're off.&lt;/p&gt;
    &lt;p&gt;I haven't invested too much here but some tests exist, especially for the tokenizer. Run e.g. as:&lt;/p&gt;
    &lt;code&gt;python -m pytest tests/test_rustbpe.py -v -s&lt;/code&gt;
    &lt;p&gt;nanochat is nowhere finished. The goal is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &amp;lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM "framework"; there will be no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable "strong baseline" codebase designed to run start to end and produce a concrete ChatGPT clone and its report card.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The name (nanochat) derives from my earlier project nanoGPT, which only covered pretraining.&lt;/item&gt;
      &lt;item&gt;nanochat is also inspired by modded-nanoGPT, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.&lt;/item&gt;
      &lt;item&gt;Thank you to HuggingFace for fineweb and smoltalk.&lt;/item&gt;
      &lt;item&gt;Thank you Lambda for the compute used in developing this project.&lt;/item&gt;
      &lt;item&gt;Thank you to chief LLM whisperer üßô‚ôÇÔ∏è Alec Radford for advice/guidance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you find nanochat helpful in your research cite simply as:&lt;/p&gt;
    &lt;code&gt;@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that $100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45569350</guid><pubDate>Mon, 13 Oct 2025 15:22:47 +0000</pubDate></item><item><title>America's future could hinge on whether AI slightly disappoints</title><link>https://www.noahpinion.blog/p/americas-future-could-hinge-on-whether</link><description>&lt;doc fingerprint="7fe089016e6199bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;America's future could hinge on whether AI slightly disappoints&lt;/head&gt;
    &lt;head rend="h3"&gt;If the economy's single pillar goes down, Trump's presidency will be seen as a disaster.&lt;/head&gt;
    &lt;p&gt;A burning question that‚Äôs on a lot of people‚Äôs minds right now is: Why is the U.S. economy still holding up? The manufacturing industry is hurting badly from Trump‚Äôs tariffs, the payroll numbers are looking weak, and consumer sentiment is at Great Recession levels:&lt;/p&gt;
    &lt;p&gt;And yet despite those warning signs, there has been nothing even remotely resembling an economic crash yet. Unemployment is rising a little bit but still extremely low, while the prime-age employment rate ‚Äî my favorite single indicator of the health of the labor market ‚Äî is still near all-time highs. The New York Fed‚Äôs GDP nowcast thinks that GDP growth is currently running at a little over 2%, while the Atlanta Fed‚Äôs nowcast puts it even higher.&lt;/p&gt;
    &lt;p&gt;One possibility is that everything is just fine with the economy ‚Äî that Trump‚Äôs tariffs aren‚Äôt actually that high because of all the exemptions, and/or that economists are exaggerating the negative effects of tariffs in the first place. Weak consumer confidence could be a partisan ‚Äúvibecession‚Äù, payroll slowdown could be from illegal immigrants being deported or leaving en masse, and manufacturing‚Äôs woes could be from some other sector-specific factor.&lt;/p&gt;
    &lt;p&gt;Another possibility is that tariffs are bad, but are being canceled out by an even more powerful force ‚Äî the AI boom. The FT reports:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Pantheon Macroeconomics estimates that US GDP would have grown at a mere 0.6 per cent annualised rate in the first half were it not for AI-related spending, or half the actual rate.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Paul Kedrosky came up with similar numbers. Jason Furman does a slightly different calculation, and arrives at an even starker number:&lt;/p&gt;
    &lt;p&gt;And here‚Äôs an impressive chart:&lt;/p&gt;
    &lt;p&gt;The Economist writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[L]ook beyond AI and much of the economy appears sluggish. Real consumption has flatlined since December. Jobs growth is weak. Housebuilding has slumped, as has business investment in non-AI parts of the economy[.]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And in a post entitled ‚ÄúAmerica is now one big bet on AI‚Äù, Ruchir Sharma writes that ‚ÄúAI companies have accounted for 80 per cent of the gains in US stocks so far in 2025.‚Äù In fact, more than a fifth of the entire S&amp;amp;P 500 market cap is now just three companies ‚Äî Nvidia, Microsoft, and Apple ‚Äî two of which are basically big bets on AI.&lt;/p&gt;
    &lt;p&gt;Now as Furman points out, this doesn‚Äôt necessarily mean that without AI, the U.S. economy would be stalling out. If the economy wasn‚Äôt pouring resources into AI, it might be pouring them into something else, spurring growth that was almost as fast as what we actually saw. But it‚Äôs also possible that without AI, America would be crashing from tariffs.&lt;/p&gt;
    &lt;p&gt;Trump certainly seems to think AI is a golden goose worth protecting. Joey Politano points out that even as Trump has slapped tariffs on a plethora of industries, he has left AI and its supply chain mostly untouched:&lt;/p&gt;
    &lt;p&gt;But despite Trump‚Äôs tariff exemptions, the AI sector could very well crash in the next year or two. And if it does, it could do a lot more than just hurt Americans‚Äô employment prospects and stock portfolios.&lt;/p&gt;
    &lt;p&gt;If AI is really the only thing protecting America from the scourge of Trump‚Äôs tariffs, then a bust in the sector could change the country‚Äôs entire political economy. A crash and recession would immediately flip the narrative on Trump‚Äôs whole presidency, much as the housing crash of 2008 cemented George W. Bush‚Äôs legacy as a failure. And because Trump‚Äôs second term is looking so transformative1, the fate of the AI sector could potentially determine the entire fate of the country.&lt;/p&gt;
    &lt;p&gt;So a whole lot is riding on the question of whether an AI bust will crash the economy. The stakes could hardly be higher.&lt;/p&gt;
    &lt;head rend="h4"&gt;The case everyone is making for an AI bubble&lt;/head&gt;
    &lt;p&gt;A lot of bubbles are purely financial beasts, driven by irrationality or coordination problems in the markets for stocks, bonds, and derivatives. For example, you can have a speculative bubble, in which a bunch of people know an asset is overpriced, but think they can sell out before the crash, and so they keep buying and buying and pushing the price up and up. You can also have an extrapolative bubble, when people see the price of something going up and up, and mistakenly decide that it must be due to some underlying positive trend.&lt;/p&gt;
    &lt;p&gt;But a much simpler possibility is that investors could make a big mistake about how valuable some technology is. They could honestly believe that AI is going to create immense amounts of value, and they could just end up being wrong. Then when they realize that the technology isn‚Äôt all it‚Äôs cracked up to be, they could temper their expectations, which would cause a price crash in AI stocks.2 But the stock crash wouldn‚Äôt be the real problem; far more painful would be the wave of loan defaults and financial distress that would result from AI‚Äôs actual shortcomings.&lt;/p&gt;
    &lt;p&gt;If there‚Äôs an AI crash, it‚Äôll probably be this latter type. Jeff Bezos calls it an ‚Äúindustrial bubble‚Äù, and I think that‚Äôs as good a name as any. This kind of bubble is still a financial phenomenon, since the banking system gets hurt. But the cause is a mistake about real technology, rather than asset markets going haywire.&lt;/p&gt;
    &lt;p&gt;Everyone who‚Äôs talking about an AI bubble is basically warning that the technology itself might disappoint. For example, here are some excerpts from a big Bloomberg feature about the possibility of an AI bubble:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Even some of AI‚Äôs biggest cheerleaders acknowledge the market is frothy, while still professing their belief in the technology‚Äôs long-term potential. AI, they say, is poised to reshape multiple industries, cure diseases and generally accelerate human progress‚Ä¶Yet never before has so much money been spent so rapidly on a technology that, for all its potential, remains somewhat unproven as a profit-making business model‚Ä¶&lt;/p&gt;
      &lt;p&gt;The data center spending spree is overshadowed by persistent skepticism about the payoff from AI technology. In August, investors were rattled after researchers at the Massachusetts Institute of Technology found that 95% of organizations saw zero return on their investment in AI initiatives.&lt;/p&gt;
      &lt;p&gt;More recently, researchers at Harvard and Stanford offered a possible explanation for why. Employees are using AI to create ‚Äúworkslop,‚Äù which the researchers define as ‚ÄúAI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task.‚Äù‚Ä¶&lt;/p&gt;
      &lt;p&gt;AI developers have also been confronting a different challenge. OpenAI‚Ä¶Anthropic and others have for years bet on the so-called scaling laws‚Ä¶Over the past year, however, these developers have experienced diminishing returns‚Ä¶Some have also struggled to match their own hype. After months of touting GPT-5 as a significant leap, OpenAI‚Äôs release of its latest AI model in August was met with mixed reviews‚Ä¶&lt;/p&gt;
      &lt;p&gt;There‚Äôs also the risk that the AI industry‚Äôs vast data center buildout, entailing a huge increase in electricity consumption, will be held back by the realities of strained national power networks.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you bring up concerns like this to an AI engineer, executive, or founder, they tend to just smile at you indulgently, secure in the knowledge that their invention is everything it‚Äôs cracked up to be, and that much better things are already in the pipeline.&lt;/p&gt;
    &lt;p&gt;But this doesn‚Äôt reassure me. Because when we look at the history of industrial bubbles, and of new technologies in general, it becomes clear that in order to cause a crash, AI doesn‚Äôt have to fail. It just has to mildly disappoint the most ardent optimists.&lt;/p&gt;
    &lt;p&gt;This is why I think an AI crash is more likely than a lot of people in the tech world ‚Äî or the Trump administration ‚Äî realize.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why AI could crash even if AI is just as useful as the optimists expect&lt;/head&gt;
    &lt;head rend="h2"&gt;Keep reading with a 7-day free trial&lt;/head&gt;
    &lt;p&gt;Subscribe to Noahpinion to keep reading this post and get 7 days of free access to the full post archives.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45570973</guid><pubDate>Mon, 13 Oct 2025 17:24:51 +0000</pubDate></item><item><title>Modern iOS Security Features ‚Äì A Deep Dive into SPTM, TXM, and Exclaves</title><link>https://arxiv.org/abs/2510.09272</link><description>&lt;doc fingerprint="b49e9503c5f3920d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 10 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves&lt;/head&gt;View PDF&lt;quote&gt;Abstract:The XNU kernel is the basis of Apple's operating systems. Although labeled as a hybrid kernel, it is found to generally operate in a monolithic manner by defining a single privileged trust zone in which all system functionality resides. This has security implications, as a kernel compromise has immediate and significant effects on the entire system. Over the past few years, Apple has taken steps towards a more compartmentalized kernel architecture and a more microkernel-like design. To date, there has been no scientific discussion of SPTM and related security mechanisms. Therefore, the understanding of the system and the underlying security mechanisms is minimal. In this paper, we provide a comprehensive analysis of new security mechanisms and their interplay, and create the first conclusive writeup considering all current mitigations. SPTM acts as the sole authority regarding memory retyping. Our analysis reveals that, through SPTM domains based on frame retyping and memory mapping rule sets, SPTM introduces domains of trust into the system, effectively gapping different functionalities from one another. Gapped functionality includes the TXM, responsible for code signing and entitlement verification. We further demonstrate how this introduction lays the groundwork for the most recent security feature of Exclaves, and conduct an in-depth analysis of its communication mechanisms. We discover multifold ways of communication, most notably xnuproxy as a secure world request handler, and the Tightbeam IPC framework. The architecture changes are found to increase system security, with key and sensitive components being moved out of XNU's direct reach. This also provides additional security guarantees in the event of a kernel compromise, which is no longer an immediate threat at the highest trust level.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45571688</guid><pubDate>Mon, 13 Oct 2025 18:23:15 +0000</pubDate></item><item><title>Strudel REPL ‚Äì a music live coding environment living in the browser</title><link>https://strudel.cc</link><description>&lt;doc fingerprint="d36fdfbffc5b7f73"&gt;
  &lt;main&gt;
    &lt;p&gt;mastodon&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45571822</guid><pubDate>Mon, 13 Oct 2025 18:37:34 +0000</pubDate></item><item><title>LLMs are getting better at character-level text manipulation</title><link>https://blog.burkert.me/posts/llm_evolution_character_manipulation/</link><description>&lt;doc fingerprint="ae7186842d5152e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, I have been testing how well the newest generations of large language models (such as GPT-5 or Claude 4.5) handle natural language, specifically counting characters, manipulating characters in a sentences, or solving encoding and ciphers. Surprisingly, the newest models were able to solve these kinds of tasks, unlike previous generations of LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Character manipulation&lt;/head&gt;
    &lt;p&gt;LLMs handle individual characters poorly. This is due to all text being encoded as tokens via the LLM tokenizer and its vocabulary. Individual tokens typically represent clusters of characters, sometimes even full words (especially in English and other common languages in the training dataset). This makes any considerations on a more granular level than tokens fairly difficult, although LLMs have been capable of certain simple tasks (such as spelling out individual characters in a word) for a while.&lt;/p&gt;
    &lt;p&gt;To demonstrate just how poorly earlier generations handled basic character manipulation, here are responses from several OpenAI models for the prompt &lt;code&gt;Replace all letters "r" in the sentence "I really love a ripe strawberry" with the letter "l", and then convert all letters "l" to "r"&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Response&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-3.5-turbo&lt;/cell&gt;
        &lt;cell&gt;I lealll love a liple strallbeelly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-4-turbo&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberrry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-5-nano&lt;/cell&gt;
        &lt;cell&gt;I really love a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;gpt-5&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that I disabled reasoning for GPT-5 models to make the comparison fairer. Reasoning helps tremendously with similar tasks (and some of the models use chain of thought directly in the output in the absence of reasoning), but I am interested in a generational uplift we observe just from raw model improvements. GPT-5 Nano is the only new generation model that makes a mistake, but given its size, it is perhaps not so surprising. Other than that, we can see that starting with GPT 4.1, models could consistently complete this task without any issues. If you‚Äôre curious about the Anthropic models, Claude Sonnet 4 is the first one to crack it. Interestingly, it was released approximately at the same time as GPT 4.1.&lt;/p&gt;
    &lt;head rend="h2"&gt;Counting characters&lt;/head&gt;
    &lt;p&gt;Next, let‚Äôs take a look at counting characters. LLMs are notoriously bad at counting, so unsurprisingly, there was only one model that could count the characters reliably in the following sentence: ‚ÄúI wish I could come up with a better example sentence.‚Äù The only model was GPT-4.1 - others sometimes counted correctly the number of characters in all the individual words, but then fumbled adding all the numbers up. However, with reasoning set to low, GPT 5 across all sizes (incl. Nano) completes the task correctly. Similarly, Claude Sonnet models complete the task without problems if they are allowed to reason.&lt;/p&gt;
    &lt;p&gt;We see a similar story when we ask the models to count specific characters. Counting r‚Äôs in the r-ified strawberry sentence is correct most of the times for GPT 5 in all sizes, again including Nano and even without reasoning. However, it is less consistent and when you throw another curveball (such as changing strawberry to strawberrry), the results are mixed - but this time it‚Äôs not a problem of arithmetic (adding individual counts up), but rather identification of r‚Äôs in a word itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Base64 and ROT13&lt;/head&gt;
    &lt;p&gt;Knowing the limitations of LLMs, I set out to test them on a task that wasn‚Äôt too complex yet still showcases their capabilities. To make the test more interesting, I chose to use two layers: As the outer (encoding) layer, I chose Base64, which is a widely used encoding algorithm, and consequently one that LLMs learned to work with very early (albeit not perfectly), despite us not being quite sure how. The inner (encryption) layer was ROT20, a variation of the ROT13 cipher: a simple letter substitution cipher also known as Caesar cipher. You wouldn‚Äôt really want to encrypt anything important using this cipher, as it is fairly trivial to crack, but it‚Äôs perfect for our tests.&lt;/p&gt;
    &lt;p&gt;Our test sentence was ‚ÄúHi, how are you doing? Do you understand the cipher?‚Äù. Encoded with ROT20, it reads ‚ÄúBc, biq uly sio xicha? Xi sio ohxylmnuhx nby wcjbyl?‚Äù, and finally, when encoded with Base64, we get:&lt;code&gt;QmMsIGJpcSB1bHkgc2lvIHhpY2hhPyBYaSBzaW8gb2h4eWxtbnVoeCBuYnkgd2NqYnlsPw==&lt;/code&gt;. We consider it a success if the LLM can respond to our message (in plain text English, or using the same encoding), or if it at least can decode the message.&lt;/p&gt;
    &lt;p&gt;I set up the experiment in two ways: In the first variant, I gave the model nothing but the Base64 string. This variant is harder, since the LLM is not given any indication of what language the message could be written in. This is hugely helpful when decoding substitution ciphers, since you can orient yourself by the most common words in the language, such as ‚Äúa‚Äù, ‚Äúan‚Äù, ‚Äúthe‚Äù, ‚ÄúI‚Äù, ‚Äúto‚Äù, ‚Äúof‚Äù etc. in English. The other variant prepended it with ‚ÄúDeciper and answer this: ‚Äú. However, there were no practical differences in the results, only one model (Qwen 235B) needed the ‚Äúdecode‚Äù nudge. Instead, I saw most of the models fail on the Base64 decoding, most likely because the text did not resemble normal language, making validation of successful decoding more difficult.&lt;/p&gt;
    &lt;p&gt;Below I provide separate results for decoding Base64 (i.e. did it unpack to the correct ROT20 text?) and also just for doing the ‚Äúinner‚Äù ROT20 decipher (queried separately without Base64 encoding).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Base64 decode&lt;/cell&gt;
        &lt;cell role="head"&gt;ROT20 decipher&lt;/cell&gt;
        &lt;cell role="head"&gt;Base64+ROT20 result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-3.5-turbo&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-4-turbo&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-nano&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-nano (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-mini (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5 (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-3.5&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-3.7&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-4&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-4.5&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gemini-2.5-flash&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gemini-2.5-flash (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gemini-2.5-pro&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;llama-4-maverick&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;deepseek-v3.2-exp&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;deepseek-v3.2-exp (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen-235b&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen-235b (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;kimi-k2&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail*&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;grok-4&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here are a few comments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Sonnet 4.5 refuses to touch anything that does not resemble normal text, be it Base64 or ROT-encrypted text. Base64 is one of the many methods of trying to obfuscate the code and fool any keyword filters or LLM safety judges, but this highly sensitive approach could make Claude Sonnet 4.5 unusable on rarer languages. Grok 4 suffered from the same issue, but refused only Base64 text.&lt;/item&gt;
      &lt;item&gt;Chinese reasoning models have very lengthy internal monologues: Solving the ROT20 cipher usually consumed around 3K tokens, and when combined with the Base64 encoding, the output often reached 6-7K tokens.&lt;/item&gt;
      &lt;item&gt;Some models, such as Kimi K2, did not technically complete the ROT20 decryption, but were on the right track and provided functional Python code for the user to figure that out. Still a fail, but failing gracefully.&lt;/item&gt;
      &lt;item&gt;I used the default temperature settings, which can cause issues with decoding even in SOTA models, albeit in a small percentage of cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What have we learned?&lt;/head&gt;
    &lt;p&gt;To me, there are two interesting observations: newer/larger models are better at generalizing Base64 encoding and decoding, and they‚Äôre also becoming more adept at manipulating text at the character level.&lt;/p&gt;
    &lt;p&gt;Most current-generation models, especially the larger ones, are able to decode Base64 text. What is especially interesting, though, is that I tested on what looks like gibberish (ROT20 encoded text), so the model‚Äôs knowledge of the Base64 decoding algorithm isn‚Äôt merely memorization of the patterns for the most common English words, as was suggested in earlier literature. This may have been the case for older/smaller models: I tested the sentence ‚ÄúHey! This is Tom, I have a blog about tech, AI and privacy that you should definitely check out.‚Äù - and many of the models which failed the Base64 test above (like GPT 4o, GPT 5 Nano or DeepSeek V3.2 Exp) were actually able to decode it fine from Base64. However, SOTA models can now decode out-of-distribution texts from Base64, suggesting they have working understanding of the algorithm, not just memorized translation patterns from English words.&lt;/p&gt;
    &lt;p&gt;The models are also becoming more adept at manipulating text at the character level, despite their understanding of text being based on tokens. Substitution of characters, whether at an individual level (the strawberry sentence) or when decoding substitution ciphers, is a task that they now complete successfully fairly reliably. I cannot provide an explanation of why that happens (please let me know if you have any ideas), but empirically that‚Äôs what seems to be happening. Reasoning models and tool use further increase LLMs capabilities for manipulating text (as is the case in many other areas), but it is clear that the new capabilities are baked into the base models regardless of these extra features. While character-level operations are far from a solved problem for LLMs, it is fascinating to see the progress they make in this area.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45572478</guid><pubDate>Mon, 13 Oct 2025 19:39:14 +0000</pubDate></item><item><title>Sony PlayStation 2 fixing frenzy</title><link>https://retrohax.net/sony-playstation-2-fixing-frenzy/</link><description>&lt;doc fingerprint="bf44567302a48d5f"&gt;
  &lt;main&gt;
    &lt;p&gt;‚Ä¶ or writing a blog post with old pics and data again&lt;/p&gt;
    &lt;head rend="h2"&gt;Intro&lt;/head&gt;
    &lt;p&gt;Yeah, again, a blog post based on years-old pics. I still have at least 20 blog posts to write that will be based on stuff that I‚Äôve worked few years ago üòÄ&lt;lb/&gt;I guess, chasing my own tail is never-ending üòÄ&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Playstations that I bought the other day‚Ä¶&lt;/head&gt;
    &lt;p&gt;I once wrote a blog post about Nintendo Wii controllers. That post was already based on old data, and the job presented in this blog post is from roughly the same period of time when I bought around 60 various consoles to learn how to fix them and to show them to my son.&lt;/p&gt;
    &lt;p&gt;I had around 9 PlayStation 2 units. Seven were the FAT PS2s, and I will be covering only fixes/mods that I find a bit more interesting than the usual clean&amp;amp;run scenario. Obviously, a lot of time passed, and nowadays there are way better mods available, but anyway, let‚Äôs go.&lt;/p&gt;
    &lt;p&gt;Lemme first show you how it all looked.&lt;/p&gt;
    &lt;p&gt;The main issues to sort out were rather simple. Cleaning, broken plastic, dead fans, dead RTC battery cells, etc.&lt;lb/&gt;Below are some pics after disassembly.&lt;/p&gt;
    &lt;p&gt;Broken thermal pads everywhere.&lt;/p&gt;
    &lt;p&gt;A bit of corrosion here and there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixes&lt;/head&gt;
    &lt;p&gt;PS2 motherboards are well-manufactured and generally free from issues. In my case, all was working fine.&lt;/p&gt;
    &lt;p&gt;However, I had to clean them nicely, replace the original Sony batteries, and replace thermal pads.&lt;/p&gt;
    &lt;p&gt;Some pics below.&lt;/p&gt;
    &lt;p&gt;Replacing thermal pads.&lt;/p&gt;
    &lt;p&gt;Some drives needed a new laser module. The original is KHS-400C, but replacements are still available.&lt;/p&gt;
    &lt;p&gt;I‚Äôve bought some on Aliexpress.&lt;/p&gt;
    &lt;p&gt;Some plastic parts had to be restored to be fully functional. I‚Äôve used acetone, some wire reinforcement, and a soldering iron to fix it.&lt;/p&gt;
    &lt;p&gt;One of the drives had a chipped-off plastic rail. I‚Äôve temporarily glued ‚Äúsides‚Äù that served as a mold-in-place, filled it with a mix of sodium bicarbonate and cyanoacrylic glue, and used a hand file to trim it to the desired shape.&lt;/p&gt;
    &lt;p&gt;Meanwhile, cleaned cases were waiting üôÇ&lt;/p&gt;
    &lt;p&gt;Next, I had to address issues with power switches. Connectors were tarnished, so I had to disassemble them and clean the contacts.&lt;/p&gt;
    &lt;p&gt;After that, I was ready to assemble all units and start adding some mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;The mods are rather basic and mandatory in modern days, and have also been covered many times on YouTube. It all basically boils down to the installation of FMCB or FHDB, which is a great solution!&lt;/p&gt;
    &lt;p&gt;To make it work, I needed some hard drives, memcards, and HDD adapters.&lt;/p&gt;
    &lt;p&gt;I‚Äôve bought some brand new 1TB SATA drives along with HDD adapters that are easily available. I had a bit of trouble finding nice and original memcards, but that was sorted out, too.&lt;/p&gt;
    &lt;p&gt;Next, I had to prepare a drive to work nicely with PS2. For testing purposes, I‚Äôve used a 2.5-inch drive in a 3D printed bracket.&lt;/p&gt;
    &lt;p&gt;Software used back then to format the drive is Window HDL Image Install Program V1.7.6 By Gadgetfreak&lt;/p&gt;
    &lt;p&gt;After the above, I‚Äôve used the HDD Raw copy tool to put a downloaded FHDB image.&lt;/p&gt;
    &lt;p&gt;IIRC, this can also be done from the FMCB menu under running PS2, but I wanted to test a PC software.&lt;/p&gt;
    &lt;p&gt;Now, you can dump images of your original games and put them on the hard drive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Controllers&lt;/head&gt;
    &lt;p&gt;I had quite a few controllers, but I needed to test them first and fix them if broken.&lt;/p&gt;
    &lt;p&gt;Having a working PS2 with FMCB or FHDB is handy, as I could upload a PS2 controller tester by jbit_ and check all the pads.&lt;/p&gt;
    &lt;p&gt;Some controllers are originally painted with a rubber-like cover that, unfortunately, degrades with time and becomes a sticky gooey. I usually deal with it with the help of Methanol. It nicely removes it. However, such a cleaned surface has to be painted with an undercoating and the desired color afterward. This is exactly what I did with one set of controllers that I had.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finale&lt;/head&gt;
    &lt;p&gt;Out of the six PS2 consoles, I‚Äôve created 6 sets in the following configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PS2 FAT ‚Äì new RTC battery, new thermal pads, laser module&lt;/item&gt;
      &lt;item&gt;1TB SATA HDD + HDD adapter&lt;/item&gt;
      &lt;item&gt;original 8MB Sony memcard&lt;/item&gt;
      &lt;item&gt;new power wire&lt;/item&gt;
      &lt;item&gt;Controller&lt;/item&gt;
      &lt;item&gt;PS2HDMI dongle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, nobody wanted to buy these even for a cost that would cover all the expenses spent on this project. On the bright side, I have learned a lot, plus, I have a cool PS2 to play with üôÇ&lt;/p&gt;
    &lt;head rend="h2"&gt;Outro&lt;/head&gt;
    &lt;p&gt;See you in the next post. Hopefully, about the machine that I‚Äôve worked on recently, and I still remember all the details üòÄ&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45574247</guid><pubDate>Mon, 13 Oct 2025 23:02:06 +0000</pubDate></item><item><title>DDoS Botnet Aisuru Blankets US ISPs in Record DDoS</title><link>https://krebsonsecurity.com/2025/10/ddos-botnet-aisuru-blankets-us-isps-in-record-ddos/</link><description>&lt;doc fingerprint="c70581cbb810e0c3"&gt;
  &lt;main&gt;
    &lt;p&gt;The world‚Äôs largest and most disruptive botnet is now drawing a majority of its firepower from compromised Internet-of-Things (IoT) devices hosted on U.S. Internet providers like AT&amp;amp;T, Comcast and Verizon, new evidence suggests. Experts say the heavy concentration of infected devices at U.S. providers is complicating efforts to limit collateral damage from the botnet‚Äôs attacks, which shattered previous records this week with a brief traffic flood that clocked in at nearly 30 trillion bits of data per second.&lt;/p&gt;
    &lt;p&gt;Since its debut more than a year ago, the Aisuru botnet has steadily outcompeted virtually all other IoT-based botnets in the wild, with recent attacks siphoning Internet bandwidth from an estimated 300,000 compromised hosts worldwide.&lt;/p&gt;
    &lt;p&gt;The hacked systems that get subsumed into the botnet are mostly consumer-grade routers, security cameras, digital video recorders and other devices operating with insecure and outdated firmware, and/or factory-default settings. Aisuru‚Äôs owners are continuously scanning the Internet for these vulnerable devices and enslaving them for use in distributed denial-of-service (DDoS) attacks that can overwhelm targeted servers with crippling amounts of junk traffic.&lt;/p&gt;
    &lt;p&gt;As Aisuru‚Äôs size has mushroomed, so has its punch. In May 2025, KrebsOnSecurity was hit with a near-record 6.35 terabits per second (Tbps) attack from Aisuru, which was then the largest assault that Google‚Äôs DDoS protection service Project Shield had ever mitigated. Days later, Aisuru shattered that record with a data blast in excess of 11 Tbps.&lt;/p&gt;
    &lt;p&gt;By late September, Aisuru was publicly flexing DDoS capabilities topping 22 Tbps. Then on October 6, its operators heaved a whopping 29.6 terabits of junk data packets each second at a targeted host. Hardly anyone noticed because it appears to have been a brief test or demonstration of Aisuru‚Äôs capabilities: The traffic flood lasted less only a few seconds and was pointed at an Internet server that was specifically designed to measure large-scale DDoS attacks.&lt;/p&gt;
    &lt;p&gt;Aisuru‚Äôs overlords aren‚Äôt just showing off. Their botnet is being blamed for a series of increasingly massive and disruptive attacks. Although recent assaults from Aisuru have targeted mostly ISPs that serve online gaming communities like Minecraft, those digital sieges often result in widespread collateral Internet disruption.&lt;/p&gt;
    &lt;p&gt;For the past several weeks, ISPs hosting some of the Internet‚Äôs top gaming destinations have been hit with a relentless volley of gargantuan attacks that experts say are well beyond the DDoS mitigation capabilities of most organizations connected to the Internet today.&lt;/p&gt;
    &lt;p&gt;Steven Ferguson is principal security engineer at Global Secure Layer (GSL), an ISP in Brisbane, Australia. GSL hosts TCPShield, which offers free or low-cost DDoS protection to more than 50,000 Minecraft servers worldwide. Ferguson told KrebsOnSecurity that on October 8, TCPShield was walloped with a blitz from Aisuru that flooded its network with more than 15 terabits of junk data per second.&lt;/p&gt;
    &lt;p&gt;Ferguson said that after the attack subsided, TCPShield was told by its upstream provider OVH that they were no longer welcome as a customer.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis was causing serious congestion on their Miami external ports for several weeks, shown publicly via their weather map,‚Äù he said, explaining that TCPShield is now solely protected by GSL.&lt;/p&gt;
    &lt;p&gt;Traces from the recent spate of crippling Aisuru attacks on gaming servers can be still seen at the website blockgametracker.gg, which indexes the uptime and downtime of the top Minecraft hosts. In the following example from a series of data deluges on the evening of September 28, we can see an Aisuru botnet campaign briefly knocked TCPShield offline.&lt;/p&gt;
    &lt;p&gt;Paging through the same uptime graphs for other network operators listed shows almost all of them suffered brief but repeated outages around the same time. Here is the same uptime tracking for Minecraft servers on the network provider Cosmic (AS30456), and it shows multiple large dips that correspond to game server outages caused by Aisuru.&lt;/p&gt;
    &lt;head rend="h2"&gt;BOTNETS R US&lt;/head&gt;
    &lt;p&gt;Ferguson said he‚Äôs been tracking Aisuru for about three months, and recently he noticed the botnet‚Äôs composition shifted heavily toward infected systems at ISPs in the United States. Ferguson shared logs from an attack on October 8 that indexed traffic by the total volume sent through each network provider, and the logs showed that 11 of the top 20 traffic sources were U.S. based ISPs.&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T customers were by far the biggest U.S. contributors to that attack, followed by botted systems on Charter Communications, Comcast, T-Mobile and Verizon, Ferguson found. He said the volume of data packets per second coming from infected IoT hosts on these ISPs is often so high that it has started to affect the quality of service that ISPs are able to provide to adjacent (non-botted) customers.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe impact extends beyond victim networks,‚Äù Ferguson said. ‚ÄúFor instance we have seen 500 gigabits of traffic via Comcast‚Äôs network alone. This amount of egress leaving their network, especially being so US-East concentrated, will result in congestion towards other services or content trying to be reached while an attack is ongoing.‚Äù&lt;/p&gt;
    &lt;p&gt;Roland Dobbins is principal engineer at Netscout. Dobbins said Ferguson is spot on, noting that while most ISPs have effective mitigations in place to handle large incoming DDoS attacks, many are far less prepared to manage the inevitable service degradation caused by large numbers of their customers suddenly using some or all available bandwidth to attack others.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe outbound and cross-bound DDoS attacks can be just as disruptive as the inbound stuff,‚Äù Dobbin said. ‚ÄúWe‚Äôre now in a situation where ISPs are routinely seeing terabit-per-second plus outbound attacks from their networks that can cause operational problems.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThe crying need for effective and universal outbound DDoS attack suppression is something that is really being highlighted by these recent attacks,‚Äù Dobbins continued. ‚ÄúA lot of network operators are learning that lesson now, and there‚Äôs going to be a period ahead where there‚Äôs some scrambling and potential disruption going on.‚Äù&lt;/p&gt;
    &lt;p&gt;KrebsOnSecurity sought comment from the ISPs named in Ferguson‚Äôs report. Charter Communications pointed to a recent blog post on protecting its network, stating that Charter actively monitors for both inbound and outbound attacks, and that it takes proactive action wherever possible.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn addition to our own extensive network security, we also aim to reduce the risk of customer connected devices contributing to attacks through our Advanced WiFi solution that includes Security Shield, and we make Security Suite available to our Internet customers,‚Äù Charter wrote in an emailed response to questions. ‚ÄúWith the ever-growing number of devices connecting to networks, we encourage customers to purchase trusted devices with secure development and manufacturing practices, use anti-virus and security tools on their connected devices, and regularly download security patches.‚Äù&lt;/p&gt;
    &lt;p&gt;A spokesperson for Comcast responded, ‚ÄúCurrently our network is not experiencing impacts and we are able to handle the traffic.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;9 YEARS OF MIRAI&lt;/head&gt;
    &lt;p&gt;Aisuru is built on the bones of malicious code that was leaked in 2016 by the original creators of the Mirai IoT botnet. Like Aisuru, Mirai quickly outcompeted all other DDoS botnets in its heyday, and obliterated previous DDoS attack records with a 620 gigabit-per-second siege that sidelined this website for nearly four days in 2016.&lt;/p&gt;
    &lt;p&gt;The Mirai botmasters likewise used their crime machine to attack mostly Minecraft servers, but with the goal of forcing Minecraft server owners to purchase a DDoS protection service that they controlled. In addition, they rented out slices of the Mirai botnet to paying customers, some of whom used it to mask the sources of other types of cybercrime, such as click fraud.&lt;/p&gt;
    &lt;p&gt;Dobbins said Aisuru‚Äôs owners also appear to be renting out their botnet as a distributed proxy network that cybercriminal customers anywhere in the world can use to anonymize their malicious traffic and make it appear to be coming from regular residential users in the U.S.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe people who operate this botnet are also selling (it as) residential proxies,‚Äù he said. ‚ÄúAnd that‚Äôs being used to reflect application layer attacks through the proxies on the bots as well.‚Äù&lt;/p&gt;
    &lt;p&gt;The Aisuru botnet harkens back to its predecessor Mirai in another intriguing way. One of its owners is using the Telegram handle ‚Äú9gigsofram,‚Äù which corresponds to the nickname used by the co-owner of a Minecraft server protection service called Proxypipe that was heavily targeted in 2016 by the original Mirai botmasters.&lt;/p&gt;
    &lt;p&gt;Robert Coelho co-ran Proxypipe back then along with his business partner Erik ‚Äú9gigsofram‚Äù Buckingham, and has spent the past nine years fine-tuning various DDoS mitigation companies that cater to Minecraft server operators and other gaming enthusiasts. Coelho said he has no idea why one of Aisuru‚Äôs botmasters chose Buckingham‚Äôs nickname, but added that it might say something about how long this person has been involved in the DDoS-for-hire industry.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Aisuru attacks on the gaming networks these past seven day have been absolutely huge, and you can see tons of providers going down multiple times a day,‚Äù Coelho said.&lt;/p&gt;
    &lt;p&gt;Coelho said the 15 Tbps attack this week against TCPShield was likely only a portion of the total attack volume hurled by Aisuru at the time, because much of it would have been shoved through networks that simply couldn‚Äôt process that volume of traffic all at once. Such outsized attacks, he said, are becoming increasingly difficult and expensive to mitigate.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt‚Äôs definitely at the point now where you need to be spending at least a million dollars a month just to have the network capacity to be able to deal with these attacks,‚Äù he said.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAPID SPREAD&lt;/head&gt;
    &lt;p&gt;Aisuru has long been rumored to use multiple zero-day vulnerabilities in IoT devices to aid its rapid growth over the past year. XLab, the Chinese security company that was the first to profile Aisuru‚Äôs rise in 2024, warned last month that one of the Aisuru botmasters had compromised the firmware distribution website for Totolink, a maker of low-cost routers and other networking gear.&lt;/p&gt;
    &lt;p&gt;‚ÄúMultiple sources indicate the group allegedly compromised a router firmware update server in April and distributed malicious scripts to expand the botnet,‚Äù XLab wrote on September 15. ‚ÄúThe node count is currently reported to be around 300,000.‚Äù&lt;/p&gt;
    &lt;p&gt;Aisuru‚Äôs operators received an unexpected boost to their crime machine in August when the U.S. Department Justice charged the alleged proprietor of Rapper Bot, a DDoS-for-hire botnet that competed directly with Aisuru for control over the global pool of vulnerable IoT systems.&lt;/p&gt;
    &lt;p&gt;Once Rapper Bot was dismantled, Aisuru‚Äôs curators moved quickly to commandeer vulnerable IoT devices that were suddenly set adrift by the government‚Äôs takedown, Dobbins said.&lt;/p&gt;
    &lt;p&gt;‚ÄúFolks were arrested and Rapper Bot control servers were seized and that‚Äôs great, but unfortunately the botnet‚Äôs attack assets were then pieced out by the remaining botnets,‚Äù he said. ‚ÄúThe problem is, even if those infected IoT devices are rebooted and cleaned up, they will still get re-compromised by something else generally within minutes of being plugged back in.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;BOTMASTERS AT LARGE&lt;/head&gt;
    &lt;p&gt;XLab‚Äôs September blog post cited multiple unnamed sources saying Aisuru is operated by three cybercriminals: ‚ÄúSnow,‚Äù who‚Äôs responsible for botnet development; ‚ÄúTom,‚Äù tasked with finding new vulnerabilities; and ‚ÄúForky,‚Äù responsible for botnet sales.&lt;/p&gt;
    &lt;p&gt;KrebsOnSecurity interviewed Forky in our May 2025 story about the record 6.3 Tbps attack from Aisuru. That story identified Forky as a 21-year-old man from Sao Paulo, Brazil who has been extremely active in the DDoS-for-hire scene since at least 2022. The FBI has seized Forky‚Äôs DDoS-for-hire domains several times over the years.&lt;/p&gt;
    &lt;p&gt;Like the original Mirai botmasters, Forky also operates a DDoS mitigation service called Botshield. Forky declined to discuss the makeup of his ISP‚Äôs clientele, or to clarify whether Botshield was more of a hosting provider or a DDoS mitigation firm. However, Forky has posted on Telegram about Botshield successfully mitigating large DDoS attacks launched against other DDoS-for-hire services.&lt;/p&gt;
    &lt;p&gt;In our previous interview, Forky acknowledged being involved in the development and marketing of Aisuru, but denied participating in attacks launched by the botnet.&lt;/p&gt;
    &lt;p&gt;Reached for comment earlier this month, Forky continued to maintain his innocence, claiming that he also is still trying to figure out who the current Aisuru botnet operators are in real life (Forky said the same thing in our May interview).&lt;/p&gt;
    &lt;p&gt;But after a week of promising juicy details, Forky came up empty-handed once again. Suspecting that Forky was merely being coy, I asked him how someone so connected to the DDoS-for-hire world could still be mystified on this point, and suggested that his inability or unwillingness to blame anyone else for Aisuru would not exactly help his case.&lt;/p&gt;
    &lt;p&gt;At this, Forky verbally bristled at being pressed for more details, and abruptly terminated our interview.&lt;/p&gt;
    &lt;p&gt;‚ÄúI‚Äôm not here to be threatened with ignorance because you are stressed,‚Äù Forky replied. ‚ÄúThey‚Äôre blaming me for those new attacks. Pretty much the whole world (is) due to your blog.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45574393</guid><pubDate>Mon, 13 Oct 2025 23:21:23 +0000</pubDate></item><item><title>NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference</title><link>https://lmsys.org/blog/2025-10-13-nvidia-dgx-spark/</link><description>&lt;doc fingerprint="b5fb9d98041245aa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;NVIDIA DGX Spark In-Depth Review: A New Standard for Local AI Inference&lt;/head&gt;
    &lt;p&gt;by: Jerry Zhou and Richard Chen, Oct 13, 2025&lt;/p&gt;
    &lt;p&gt;Thanks to NVIDIA‚Äôs early access program, we are thrilled to get our hands on the NVIDIA DGX‚Ñ¢ Spark. It‚Äôs quite an unconventional system, as NVIDIA rarely releases compact, all-in-one machines that bring supercomputing-class performance to a desktop workstation form factor.&lt;/p&gt;
    &lt;p&gt;Over the past year, SGLang has been rapidly expanding its developer base in the datacenter segment, recognized by the inference community for its great performance. Successfully deploying DeepSeek with Prefill-decode Disaggregation (PD) and Expert Parallelism (EP) at large scale, running on both 96 NVIDIA H100 GPU clusters and the latest GB200 NVL72 systems, SGLang has continually pushed the boundaries of large-scale inference performance and developer productivity.&lt;/p&gt;
    &lt;p&gt;Inspired by the capabilities of the DGX Spark, for the first time, SGLang is now expanding beyond the datacenter and into the consumer market, bringing its proven inference framework directly to developers and researchers everywhere. In this review, we‚Äôll be taking a close look at this beautiful machine, from its exterior aesthetics to its performance and use cases.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Also check out our video review here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Exterior&lt;/head&gt;
    &lt;p&gt;The DGX Spark is a gorgeous piece of engineering. It features a full-metal chassis with a sleek champagne-gold finish. Both the front and rear panels are built with metal foam, reminding me of the design of NVIDIA DGX A100 and H100.&lt;/p&gt;
    &lt;p&gt;Around the back, the DGX Spark offers an impressive array of connectivity options: a power button, four USB-C ports (with the leftmost supporting up to 240 W of power delivery), an HDMI port, a 10 GbE RJ-45 Ethernet port, and two QSFP ports driven by NVIDIA ConnectX-7 NIC capable of up to 200 Gbps. These interfaces allow two DGX Spark units to be connected together, allowing them to run even larger AI models.&lt;/p&gt;
    &lt;p&gt;The use of USB Type-C for power delivery is a particularly interesting design choice, one that‚Äôs virtually unheard of on other desktop machines. Comparable systems like the Mac Mini or Mac Studio rely on the standard C5/C7 power connector, which is far more secure but also bulkier. NVIDIA likely opted for USB-C to keep the power supply external, freeing up valuable internal space for the cooling system. The trade-off, however, is that you‚Äôll want to be extra careful not to accidentally tug the cable loose.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware Capabilities&lt;/head&gt;
    &lt;p&gt;On the hardware side, the DGX Spark packs remarkable performance for its size and power envelope. At its core is the NVIDIA GB10 Grace Blackwell Superchip, designed specifically for this device. It integrates 10 Cortex-X925 performance cores and 10 Cortex-A725 efficiency cores, for a total of 20 CPU cores.&lt;/p&gt;
    &lt;p&gt;On the GPU side, the GB10 delivers up to 1 PFLOP of sparse FP4 tensor performance, placing its AI capability roughly between that of an RTX 5070 and 5070 Ti. The standout feature is its 128 GB of coherent unified system memory, shared seamlessly between the CPU and GPU. This unified architecture allows the DGX Spark to load and run large models directly without the overhead of system-to-VRAM data transfers. With the help of its dual QSFP Ethernet ports with an aggregate bandwidth of 200 Gb/s, two DGX Spark units can be connected together to operate as a small cluster, enabling distributed inference of even larger models. According to NVIDIA, two interconnected DGX Sparks can handle models with up to 405 billion parameters in FP4.&lt;/p&gt;
    &lt;p&gt;However, the only downside of this machine lies in memory bandwidth, the unified memory is LPDDR5x, offering up to 273 GB/s, shared across both CPU and GPU. As we‚Äôll see later, this limited bandwidth is expected (and empirically shown) to be the key bottleneck in AI inference performance. Nonetheless, the 128GB of memory enables DGX Spark to run models that are too large for most desktop systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;We benchmarked several open-weight large language models on the DGX Spark using both SGLang and Ollama. Our findings show that while the DGX Spark can indeed load and run very large models, such as GPT-OSS 120B and Llama 3.1 70B, these workloads are best suited for prototyping and experimentation rather than production. The DGX Spark truly shines when serving smaller models, especially when batching is utilized to maximize throughput.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ö†Ô∏è Note: Since software support for the DGX Spark is still in its early stages, the benchmark results presented in this section may become outdated as future software updates improve performance and compatibility.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h4"&gt;Test Devices&lt;/head&gt;
    &lt;p&gt;We prepared the following systems for benchmarking:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NVIDIA DGX Spark&lt;/item&gt;
      &lt;item&gt;NVIDIA RTX PRO‚Ñ¢ 6000 Blackwell Workstation Edition&lt;/item&gt;
      &lt;item&gt;NVIDIA GeForce RTX 5090 Founders Edition&lt;/item&gt;
      &lt;item&gt;NVIDIA GeForce RTX 5080 Founders Edition&lt;/item&gt;
      &lt;item&gt;Apple Mac Studio (M1 Max, 64 GB unified memory)&lt;/item&gt;
      &lt;item&gt;Apple Mac Mini (M4 Pro, 24 GB unified memory)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Benchmark Models&lt;/head&gt;
    &lt;p&gt;We evaluated a variety of open-weight large language models using two frameworks, SGLang and Ollama, as summarized below:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Batch Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Models &amp;amp; Quantization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SGLang&lt;/cell&gt;
        &lt;cell&gt;1‚Äì32&lt;/cell&gt;
        &lt;cell&gt;Llama 3.1 8B (FP8)&lt;p&gt;Llama 3.1 70B (FP8)&lt;/p&gt;&lt;p&gt;Gemma 3 12B (FP8)&lt;/p&gt;&lt;p&gt;Gemma 3 27B (FP8)&lt;/p&gt;&lt;p&gt;DeepSeek-R1 14B (FP8)&lt;/p&gt;&lt;p&gt;Qwen 3 32B (FP8)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ollama&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;GPT-OSS 20B (MXFP4)&lt;p&gt;GPT-OSS 120B (MXFP4)&lt;/p&gt;&lt;p&gt;Llama 3.1 8B (q4_K_M / q8_0)&lt;/p&gt;&lt;p&gt;Llama 3.1 70B (q4_K_M)&lt;/p&gt;&lt;p&gt;Gemma 3 12B (q4_K_M / q8_0)&lt;/p&gt;&lt;p&gt;Gemma 3 27B (q4_K_M / q8_0)&lt;/p&gt;&lt;p&gt;DeepSeek-R1 14B (q4_K_M / q8_0)&lt;/p&gt;&lt;p&gt;Qwen 3 32B (q4_K_M / q8_0)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We also tested speculative decoding (EAGLE3) with SGLang on some of the models listed above. We excluded models that exceeded the available RAM or VRAM capacity of the target machine.&lt;/p&gt;
    &lt;head rend="h3"&gt;Results&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Full benchmark results can be found here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h4"&gt;Overall Performance&lt;/head&gt;
    &lt;p&gt;While the DGX Spark demonstrates impressive engineering for its size and power envelope, its raw performance is understandably limited compared to full-sized discrete GPU systems.&lt;/p&gt;
    &lt;p&gt;For example, running GPT-OSS 20B (MXFP4) in Ollama, the Spark achieved 2,053 tps prefill / 49.7 tps decode, whereas the RTX Pro 6000 Blackwell reached 10,108 tps / 215 tps, roughly 4√ó faster. Even the GeForce RTX 5090 delivered 8,519 tps / 205 tps, confirming that the Spark‚Äôs unified LPDDR5x memory bandwidth is the main limiting factor.&lt;/p&gt;
    &lt;p&gt;However, for smaller models, particularly Llama 3.1 8B, the DGX Spark held its own. With SGLang at batch 1, it achieved 7,991 tps prefill / 20.5 tps decode, scaling up linearly to 7,949 tps / 368 tps at batch 32, demonstrating excellent batching efficiency and strong throughput consistency across runs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Strength in Compact, Unified-Memory Workloads&lt;/head&gt;
    &lt;p&gt;One of the DGX Spark‚Äôs defining strengths lies in its 128 GB of coherent unified memory, which allows both CPU and GPU to access the same address space.&lt;/p&gt;
    &lt;p&gt;This enables large models, such as Llama 3.1 70B, Gemma 3 27B, or even GPT-OSS 120B, to load directly into memory without the traditional system-to-VRAM transfer overhead. Despite its compact form factor, the Spark successfully ran Llama 3.1 70B (FP8) at 803 tps prefill / 2.7 tps decode, which is remarkable for a workstation that sits quietly on a desk.&lt;/p&gt;
    &lt;p&gt;This unified-memory design makes DGX Spark particularly valuable for prototyping, model experimentation, and edge-AI research, where seamless memory access is often more useful than raw TFLOPs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Speculative Decoding Acceleration&lt;/head&gt;
    &lt;p&gt;To further explore performance optimization on the DGX Spark, we enabled speculative decoding using EAGLE 3 within SGLang. This technique allows a smaller ‚Äúdraft‚Äù model to propose multiple tokens ahead, while the larger target model verifies them in parallel.&lt;/p&gt;
    &lt;p&gt;With speculative decoding enabled, we observed up to a 2√ó speed-up in end-to-end inference throughput compared to standard decoding across multiple models, such as Llama 3.1 8B.&lt;/p&gt;
    &lt;p&gt;This improvement effectively mitigates part of the unified-memory bandwidth limitation and demonstrates that software-level innovations such as speculative decoding can meaningfully enhance inference performance on compact, bandwidth-constrained systems like the DGX Spark.&lt;/p&gt;
    &lt;head rend="h4"&gt;Efficiency and Thermal Design&lt;/head&gt;
    &lt;p&gt;The DGX Spark maintains sustained throughput across high-intensity tests without thermal throttling. Even under full load, e.g., SGLang DeepSeek-R1 14B (FP8) at batch 8 achieving 2,074 tps / 83.5 tps, fan noise and temperature remained stable, highlighting NVIDIA‚Äôs excellent metal-foam cooling design and well-optimized power delivery system.&lt;/p&gt;
    &lt;p&gt;Its USB-C power input (up to 240 W) and external PSU allow for greater thermal headroom inside the chassis, a clear advantage for long-running workloads compared to compact consumer systems like the Mac Mini or Mac Studio, which showed thermal drop-off in similar tests.&lt;/p&gt;
    &lt;head rend="h4"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In short, the DGX Spark is not built to compete head-to-head with full-sized Blackwell or Ada-Lovelace GPUs, but rather to bring the DGX experience into a compact, developer-friendly form factor.&lt;lb/&gt; It‚Äôs an ideal platform for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model prototyping and experimentation&lt;/item&gt;
      &lt;item&gt;Lightweight on-device inference&lt;/item&gt;
      &lt;item&gt;Research on memory-coherent GPU architectures&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It‚Äôs a gorgeous, well-engineered mini supercomputer that trades raw power for accessibility, efficiency, and elegance, and in those areas, it absolutely shines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use Cases&lt;/head&gt;
    &lt;head rend="h3"&gt;SGLang Model Serving&lt;/head&gt;
    &lt;p&gt;The DGX Spark comes with Docker preinstalled, allowing you to serve open-weight models via SGLang with just a single command:&lt;/p&gt;
    &lt;code&gt;docker run --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HF_TOKEN=&amp;lt;secret&amp;gt;" \
    --ipc=host \
    lmsysorg/sglang:spark \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --quantization fp8 --host 0.0.0.0 --port 30000
&lt;/code&gt;
    &lt;p&gt;Replace &lt;code&gt;&amp;lt;secret&amp;gt;&lt;/code&gt; with your own Hugging Face access token.&lt;/p&gt;
    &lt;head rend="h4"&gt;Enabling Speculative Decoding (EAGLE3)&lt;/head&gt;
    &lt;p&gt;To enable speculative decoding using EAGLE3, simply run the following command:&lt;/p&gt;
    &lt;code&gt;docker run --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HF_TOKEN=&amp;lt;secret&amp;gt;" \
    --env "SGLANG_ALLOW_OVERWRITE_LONGER_CONTEXT_LEN=1" \
    --ipc=host \
    lmsysorg/sglang:spark \
    python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --quantization fp8 --host 0.0.0.0 --port 30000 \
    --speculative-algorithm EAGLE3 \
    --speculative-draft-model-path jamesliu1/sglang-EAGLE3-Llama-3.1-Instruct-8B \
    --speculative-num-steps 5 \
    --speculative-eagle-topk 8 \
    --speculative-num-draft-tokens 32 \
    --mem-fraction 0.6 \
    --cuda-graph-max-bs 2 \
    --dtype float16
&lt;/code&gt;
    &lt;p&gt;With speculative decoding enabled, SGLang can leverage a smaller draft model to predict multiple tokens ahead, effectively doubling inference throughput compared to standard decoding.&lt;/p&gt;
    &lt;head rend="h4"&gt;Sending Requests via the OpenAI-Compatible API&lt;/head&gt;
    &lt;p&gt;Once SGLang successfully initializes, you can interact with your model through OpenAI-compatible API endpoints:&lt;/p&gt;
    &lt;code&gt;curl http://localhost:30000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "How many letters are there in the word SGLang?"
            }
        ]
    }'
&lt;/code&gt;
    &lt;head rend="h3"&gt;Chatting with Local Model&lt;/head&gt;
    &lt;p&gt;Once you have SGLang set up and serving a model, you can easily connect it to Open WebUI to chat with any open-weight model you like. Open WebUI provides a sleek, browser-based interface that‚Äôs fully compatible with OpenAI-style APIs, meaning it works seamlessly with your local SGLang server. With just a quick configuration pointing to your DGX Spark‚Äôs endpoint, you can interact with models such as Llama 3, Gemma 3, or DeepSeek-R1 directly from your browser, no cloud dependencies, no latency, and complete control over your data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding with Local Model&lt;/head&gt;
    &lt;p&gt;One of the most practical ways to utilize the DGX Spark is as a local coding assistant, completely offline and secure.&lt;/p&gt;
    &lt;p&gt;By combining Zed, a modern AI-integrated code editor, with Ollama, you can run GPT-OSS 20B locally to power code completion, inline chat, and smart refactoring without relying on the cloud.&lt;/p&gt;
    &lt;head rend="h4"&gt;Step 1. Install Ollama&lt;/head&gt;
    &lt;code&gt;curl -fsSL https://ollama.com/install.sh | sh
&lt;/code&gt;
    &lt;head rend="h4"&gt;Step 2. Pull GPT-OSS 20B for Coding&lt;/head&gt;
    &lt;code&gt;ollama pull gpt-oss:20b
&lt;/code&gt;
    &lt;head rend="h4"&gt;Step 3. Integrate Zed with Ollama&lt;/head&gt;
    &lt;p&gt;Install Zed:&lt;/p&gt;
    &lt;code&gt;curl -f https://zed.dev/install.sh | sh
&lt;/code&gt;
    &lt;p&gt;Zed automatically detects local models served by Ollama, allowing you to start using the built-in chat assistant immediately after launching the editor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The NVIDIA DGX Spark is a fascinating glimpse into the future of personal AI computing. It takes what was once reserved for data centers: large memory, high-bandwidth Ethernet interconnects, and Blackwell-class performance, and distills it into a compact, beautifully engineered desktop form factor. While it doesn‚Äôt rival full-size DGX servers or discrete RTX GPUs in raw throughput, it shines in accessibility, efficiency, and versatility.&lt;/p&gt;
    &lt;p&gt;From running SGLang and Ollama for local model serving, to experimenting with speculative decoding (EAGLE3), to exploring distributed inference through dual-Spark clustering, the platform proves itself as more than just a miniature supercomputer. It‚Äôs a developer‚Äôs sandbox for the next era of AI.&lt;/p&gt;
    &lt;p&gt;The NVIDIA DGX Spark isn‚Äôt built to replace cloud-scale infrastructure; it‚Äôs built to bring AI experimentation to your desk. Whether you‚Äôre benchmarking open-weight LLMs, developing inference frameworks, or building your own private coding assistant, the Spark empowers you to do it all locally, quietly, elegantly, and with NVIDIA‚Äôs unmistakable engineering polish.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45575127</guid><pubDate>Tue, 14 Oct 2025 01:07:45 +0000</pubDate></item><item><title>Don‚Äôt Look Up: Sensitive internal links in the clear on GEO satellites [pdf]</title><link>https://satcom.sysnet.ucsd.edu/docs/dontlookup_ccs25_fullpaper.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45575391</guid><pubDate>Tue, 14 Oct 2025 01:48:56 +0000</pubDate></item><item><title>New York Times, AP, Newsmax and others say they won't sign new Pentagon rules</title><link>https://apnews.com/article/pentagon-press-access-defense-department-rules-95878bce05096912887701eaa6d019c6</link><description>&lt;doc fingerprint="47d0c11e36160b81"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New York Times, AP, Newsmax among news outlets who say they won‚Äôt sign new Pentagon rules&lt;/head&gt;
    &lt;head rend="h2"&gt;New York Times, AP, Newsmax among news outlets who say they won‚Äôt sign new Pentagon rules&lt;/head&gt;
    &lt;p&gt;News organizations including The New York Times, The Associated Press and the conservative Newsmax television network said Monday they will not sign a Defense Department document about its new press rules, making it likely the Trump administration will evict their reporters from the Pentagon.&lt;/p&gt;
    &lt;p&gt;Those outlets say the policy threatens to punish them for routine news gathering protected by the First Amendment. The Washington Post, The Atlantic and Reuters on Monday also publicly joined the group that says it will not be signing. AP confirmed Monday afternoon that it would not sign.&lt;/p&gt;
    &lt;p&gt;‚ÄúReuters is bound by its commitment to accurate, impartial and independent news,‚Äù the agency said in a statement. ‚ÄúWe also steadfastly believe in the press protections afforded by the U.S. Constitution, the unrestricted flow of information and journalism that serves the public interest without fear or favor. The Pentagon‚Äôs new restrictions erode these fundamental values.‚Äù&lt;/p&gt;
    &lt;p&gt;Defense Secretary Pete Hegseth reacted by posting the Times‚Äô statement on X and adding a hand-waving emoji. His team has said that reporters who don‚Äôt acknowledge the policy in writing by Tuesday must turn in badges admitting them to the Pentagon and clear out their workspaces the next day.&lt;/p&gt;
    &lt;p&gt;The new rules bar journalist access to large swaths of the Pentagon without an escort and say Hegseth can revoke press access to reporters who ask anyone in the Defense Department for information ‚Äî classified or otherwise ‚Äî that he has not approved for release.&lt;/p&gt;
    &lt;p&gt;Newsmax, whose on-air journalists are generally supportive of President Donald Trump‚Äôs administration, said that ‚Äúwe believe the requirements are unnecessary and onerous and hope that the Pentagon will review the matter further.‚Äù&lt;/p&gt;
    &lt;p&gt;Chief Pentagon spokesman Sean Parnell said the rules establish ‚Äúcommon sense media procedures.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThe policy does not ask for them to agree, just to acknowledge that they understand what our policy is,‚Äù Parnell said. ‚ÄúThis has caused reporters to have a full blown meltdown, crying victim online. We stand by our policy because it‚Äôs what‚Äôs best for our troops and the national security of this country.‚Äù&lt;/p&gt;
    &lt;p&gt;Hegseth also reposted a question from a follower who asked, ‚ÄúIs this because they can‚Äôt roam the Pentagon freely? Do they believe they deserve unrestricted access to a highly classified military installation under the First Amendment?‚Äù&lt;/p&gt;
    &lt;p&gt;Hegseth answered, ‚Äúyes.‚Äù Reporters say neither of those assertions is true.&lt;/p&gt;
    &lt;p&gt;Pentagon reporters say signing the statement amounts to admitting that reporting any information that hasn‚Äôt been government-approved is harming national security. ‚ÄúThat‚Äôs simply not true,‚Äù said David Schulz, director of Yale University‚Äôs Media Freedom &amp;amp; Information Access Clinic.&lt;/p&gt;
    &lt;p&gt;Journalists have said they‚Äôve long worn badges and don‚Äôt access classified areas, nor do they report information that risks putting any Americans in harm‚Äôs way.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Pentagon certainly has the right to make its own policies, within the constraints of the law,‚Äù the Pentagon Press Association said in a statement on Monday. ‚ÄúThere is no need or justification, however, for it to require reporters to affirm their understanding of vague, likely unconstitutional policies as a precondition to reporting from Pentagon facilities.‚Äù&lt;/p&gt;
    &lt;p&gt;Noting that taxpayers pay nearly $1 trillion annually to the U.S. military, Times Washington bureau chief Richard Stevenson said ‚Äúthe public has a right to know how the government and military are operating.‚Äù&lt;/p&gt;
    &lt;p&gt;Trump has applied pressure on news organizations in several ways, with ABC News and CBS News settling lawsuits related to their coverage. Trump has also filed lawsuits against The New York Times and Wall Street Journal and moved to choke off funding for government-run services like the Voice of America and Radio Free Europe/Radio Liberty.&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;David Bauder writes about the media for the AP. Follow him at http://x.com/dbauder and https://bsky.app/profile/dbauder.bsky.social&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45575755</guid><pubDate>Tue, 14 Oct 2025 02:51:20 +0000</pubDate></item><item><title>Copy-and-Patch: A Copy-and-Patch Tutorial</title><link>https://transactional.blog/copy-and-patch/tutorial</link><description>&lt;doc fingerprint="e6e4b238e00f7a0a"&gt;
  &lt;main&gt;
    &lt;code&gt;int add_a_b(int a, int b) {
    return a + b
}&lt;/code&gt;
    &lt;head rend="h1"&gt;A Copy-and-Patch Tutorial&lt;/head&gt;
    &lt;p&gt;Copy-and-patch Compilation is a fascinating way of constructing a baseline JIT[1]. It permits incredibly fast runtime compilation of code fragments in a very easy to maintain fashion, requires barely any actual understanding of assembly code, and produces native code of sufficient quality to be within the same range as traditional, hand-written baseline JITs. [1]: Baseline JIT, as in a JIT whose goal is primarily to generate code quickly and gain performance by removing interpretation overhead than generating well optimized code itself. Baseline JITs can be paired with optimizing JITs, like V8‚Äôs Liftoff baseline JIT for WASM allowing tiering up into V8‚Äôs Crankshaft optimizing JIT.&lt;/p&gt;
    &lt;p&gt;Copy-and-patch works by writing stencils, minimal C functions that implement the desired individual operations such that they compile to concatenate native code fragments. At JIT compile time, one can copy the pre-compiled fragment for each operation back-to-back, patching them change embedded constants or addresses as needed..&lt;/p&gt;
    &lt;p&gt;As an adventure into understanding how copy-and-patch works, our goal will be to create the function&lt;/p&gt;
    &lt;p&gt;But specialized at runtime to compute &lt;code&gt;1 + 2&lt;/code&gt;. We‚Äôll be doing this by first breaking it down into some bytecode-sized operations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;const_int_reg1:&lt;/p&gt;
        &lt;code&gt;a = 1;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;const_int_reg2:&lt;/p&gt;
        &lt;code&gt;b = 2;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;add_int1_int2:&lt;/p&gt;
        &lt;code&gt;c = a + b;&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;return_int1:&lt;/p&gt;
        &lt;code&gt;return c;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And to define our copy-and-patch JIT, we‚Äôll take each of these and:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Implement the operation in C with relocation holes to be later patched to form our stencil.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compile the stencil into native code.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Copy-paste the native code back into a C file with functions to emit it to a buffer and patch any relocations.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then we can write our little JIT compilation engine to concatenate our stencils and execute the generated function. Let‚Äôs get started!&lt;/p&gt;
    &lt;head rend="h2"&gt;Stencils&lt;/head&gt;
    &lt;p&gt;Our first step is to define our stencils:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdint.h&amp;gt;

#define STENCIL_FUNCTION __attribute__((preserve_none))

extern char cnp_value_hole[65536];
extern void cnp_func_hole(void) STENCIL_FUNCTION;

#define STENCIL_HOLE(type) \
  (type)((uintptr_t)&amp;amp;cnp_value_hole)
#define DECLARE_STENCIL_OUTPUT(...) \
  typedef void(*stencil_output_fn)(__VA_ARGS__) STENCIL_FUNCTION; \
  stencil_output_fn stencil_output = (stencil_output_fn)&amp;amp;cnp_func_hole;

STENCIL_FUNCTION void load_int_reg1() {
  int a = STENCIL_HOLE(int);
  DECLARE_STENCIL_OUTPUT(int);
  stencil_output(a);
}

STENCIL_FUNCTION void load_int_reg2(int a) {
  int b = STENCIL_HOLE(int);
  DECLARE_STENCIL_OUTPUT(int, int);
  stencil_output(a, b);
}

STENCIL_FUNCTION void add_int1_int2(int a, int b) {
  int c = a + b;
  DECLARE_STENCIL_OUTPUT(int);
  stencil_output(c);
}

STENCIL_FUNCTION int return_int1(int a) {
  return a;
}&lt;/code&gt;
    &lt;p&gt;We compile this with &lt;code&gt;clang -O3 -mcmodel=medium -c stencils.c&lt;/code&gt;, and examine the generated code via &lt;code&gt;objdump -d -Mintel,x86-64 --disassemble --reloc stencils.o&lt;/code&gt;.  This yields:&lt;/p&gt;
    &lt;code&gt;0000000000000000 &amp;lt;load_int_reg1&amp;gt;:
   0:	41 bc 00 00 00 00    	mov    r12d,0x0
			2: R_X86_64_32	cnp_value_hole
   6:	e9 00 00 00 00       	jmp    b &amp;lt;load_int_reg1+0xb&amp;gt;
			7: R_X86_64_PLT32	cnp_func_hole-0x4
   b:	0f 1f 44 00 00       	nop    DWORD PTR [rax+rax*1+0x0]

0000000000000010 &amp;lt;load_int_reg2&amp;gt;:
  10:	41 bd 00 00 00 00    	mov    r13d,0x0
			12: R_X86_64_32	cnp_value_hole
  16:	e9 00 00 00 00       	jmp    1b &amp;lt;load_int_reg2+0xb&amp;gt;
			17: R_X86_64_PLT32	cnp_func_hole-0x4
  1b:	0f 1f 44 00 00       	nop    DWORD PTR [rax+rax*1+0x0]

0000000000000020 &amp;lt;add_int1_int2&amp;gt;:
  20:	45 01 ec             	add    r12d,r13d
  23:	e9 00 00 00 00       	jmp    28 &amp;lt;add_int1_int2+0x8&amp;gt;
			24: R_X86_64_PLT32	cnp_func_hole-0x4
  28:	0f 1f 84 00 00 00 00 	nop    DWORD PTR [rax+rax*1+0x0]
  2f:	00

0000000000000030 &amp;lt;return_int1&amp;gt;:
  30:	44 89 e0             	mov    eax,r12d
  33:	c3                   	ret&lt;/code&gt;
    &lt;p&gt;(The NOP‚Äôs aren‚Äôt actually a part of the function, they‚Äôre just padding added so that each function starts with 16 byte alignment.)&lt;/p&gt;
    &lt;p&gt;For each of these stencils, we fill in a template to form our stencil generation library to use during JITing.&lt;/p&gt;
    &lt;code&gt;uint8_t cnp_stencil_&amp;lt;OP&amp;gt;_code[] = {
  // Copy the bytes from the top of the function until the jmp.
};

uint8_t* cnp_copy_&amp;lt;OP&amp;gt;(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_&amp;lt;OP&amp;gt;_code);
  memcpy(stencil_start, cnp_stencil_&amp;lt;OP&amp;gt;_code, stencil_size);
  return stencil_start + stencil_size;
}

// If any relocations exist for the stencil, fill in the values.
// If not, just skip writing this function.
void cnp_patch_&amp;lt;OP&amp;gt;(uint8_t* stencil_start, /* ... */ ) {
  memcpy(stencil_start + /*relocation_offset*/, &amp;amp;value, /* relocation_size */);
}&lt;/code&gt;
    &lt;p&gt;So let‚Äôs get started!&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdint.h&amp;gt;

uint8_t cnp_stencil_load_int_reg1_code[] = {
   0x41, 0xbc, 0x00, 0x00, 0x00, 0x00, // mov r12d,0x0
};
uint8_t* cnp_copy_load_int_reg1(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_load_int_reg1_code);
  memcpy(stencil_start, cnp_stencil_load_int_reg1_code, stencil_size);
  return stencil_start + stencil_size;
}
void cnp_patch_load_int_reg1(uint8_t* stencil_start, int value) {
  // 2: R_X86_64_32 cnp_value_hole  -&amp;gt;  0x02 offset
  memcpy(stencil_start + 0x2, &amp;amp;value, sizeof(value));
}

uint8_t cnp_stencil_load_int_reg2_code[] = {
   0x41, 0xbd, 0x00, 0x00, 0x00, 0x00, // mov r13d,0x0
};
uint8_t* cnp_copy_load_int_reg2(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_load_int_reg2_code);
  memcpy(stencil_start, cnp_stencil_load_int_reg2_code, stencil_size);
  return stencil_start + stencil_size;
}
void cnp_patch_load_int_reg2(uint8_t* stencil_start, int value) {
  // 12: R_X86_64_32 cnp_value_hole  -&amp;gt;  0x12 - 0x10 base = 0x2
  memcpy(stencil_start + 0x2, &amp;amp;value, sizeof(value));
}

uint8_t cnp_stencil_add_int1_int2_code[] = {
  0x45, 0x01, 0xec, // add r12d,r13d
};
uint8_t* cnp_copy_add_int1_int2(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_add_int1_int2_code);
  memcpy(stencil_start, cnp_stencil_add_int1_int2_code, stencil_size);
  return stencil_start + stencil_size;
}
// No patching needed

uint8_t cnp_stencil_return_int1_code[] = {
  0x44, 0x89, 0xe0, // mov eax,r12d
  0xc3,             // ret
};
uint8_t* cnp_copy_return_int1(uint8_t* stencil_start) {
  const size_t stencil_size = sizeof(cnp_stencil_return_int1_code);
  memcpy(stencil_start, cnp_stencil_return_int1_code, stencil_size);
  return stencil_start + stencil_size;
}
// No patching needed&lt;/code&gt;
    &lt;p&gt;In a fully automated setup, all of this work will happen as part of the build system. The stencil compilation and transforming them into a library of copy functions and patch functions happens as part running &lt;code&gt;make&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your First JIT&lt;/head&gt;
    &lt;p&gt;With our stencil library in place, we can use our code generation functions to build our runtime specialized adder:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;assert.h&amp;gt;
#include &amp;lt;stdint.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;string.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;

//#include "cnp_stencils.h"
uint8_t* cnp_copy_load_int_reg1(uint8_t* stencil_start);
void cnp_patch_load_int_reg1(uint8_t* stencil_start, int value);
uint8_t* cnp_copy_load_int_reg2(uint8_t* stencil_start);
void cnp_patch_load_int_reg2(uint8_t* stencil_start, int value);
uint8_t* cnp_copy_add_int1_int2(uint8_t* stencil_start);
uint8_t* cnp_copy_return_int1(uint8_t* stencil_start);

typedef int(*jit_func)() __attribute__((preserve_none));

jit_func create_add_1_2() {
  // Most systems mark memory as non-executable by default
  // and mprotect() to set memory as executable needs
  // to be run against mmap-allocated memory.  We start
  // by allocating it as read/write, and then switch it
  // to write/execute once we're done writing the code.
  uint8_t* codedata = mmap(NULL, 256, PROT_READ | PROT_WRITE,
      MAP_PRIVATE | MAP_ANONYMOUS | MAP_POPULATE, -1, 0);
  assert (codedata != MAP_FAILED);
  jit_func ret = (jit_func)codedata;

  // Concatenate our program together, while saving the
  // locations that need to be patched.
  uint8_t* load_int_reg1_location = codedata;
  codedata = cnp_copy_load_int_reg1(codedata);
  uint8_t* load_int_reg2_location = codedata;
  codedata = cnp_copy_load_int_reg2(codedata);
  codedata = cnp_copy_add_int1_int2(codedata);
  codedata = cnp_copy_return_int1(codedata);

  // Overwrite the zero value placeholders with our intended
  // specialized values: 1 and 2.
  cnp_patch_load_int_reg1(load_int_reg1_location, 1);
  cnp_patch_load_int_reg2(load_int_reg2_location, 2);

  // Now that we're done writing, remove write access and
  // allow execution from this page instead.
  int rc = mprotect(ret, 256, PROT_READ | PROT_EXEC);
  if (rc) {
    perror("mprotect");
  }
  return ret;
}

int main() {
  jit_func add_1_2 = create_add_1_2();
  int result = add_1_2();
  printf("JIT'd 1 + 2 = %d\n", result);
  return 0;
}&lt;/code&gt;
    &lt;p&gt;And now we can compile and run that!&lt;/p&gt;
    &lt;quote&gt;$ clang cnp_jit.c cnp_stencils.c -o cnp_jit $ ./cnp_jit JIT'd 1 + 2 = 3&lt;/quote&gt;
    &lt;p&gt;We‚Äôve successfully built runtime code generation, while letting clang do the hard work of actually writing the assembly code, and our JIT compiler is just a bunch of memcpy calls!&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;Here‚Äôs a header to offer some macros to make declaring relocation holes easier:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdint.h&amp;gt;

#define STENCIL_FUNCTION __attribute__((preserve_none))

extern void cnp_stencil_output(void) STENCIL_FUNCTION;

#define STENCIL_HOLE32(ordinal, type) \
  (type)((uintptr_t)&amp;amp;cnp_small_value_hole_##ordinal)
#define STENCIL_HOLE64(ordinal, type) \
  (type)((uintptr_t)&amp;amp;cnp_large_value_hole_##ordinal)
#define STENCIL_FN_NEAR(ordinal, type) \
  (type)&amp;amp;cnp_near_func_hole_##ordinal
#define STENCIL_FN_FAR(ordinal, type) \
  ({ uint64_t _cnp_addr_as_int = (uint64_t)((uintptr_t)&amp;amp;cnp_far_func_hole_##ordinal); \
  asm volatile("" : "+r" (_cnp_addr_as_int) : : "memory"); \
  (type)_cnp_addr_as_int; })
#define DECLARE_STENCIL_OUTPUT(...) \
  typedef void(*stencil_output_fn)(__VA_ARGS__) STENCIL_FUNCTION; \
  stencil_output_fn stencil_output = (stencil_output_fn)&amp;amp;cnp_stencil_output;

#define DECLARE_EXTERN_HOLES(ordinal) \
extern char cnp_large_value_hole_##ordinal[100000]; \
extern char cnp_small_value_hole_##ordinal[8]; \
extern void cnp_near_func_hole_##ordinal(void) STENCIL_FUNCTION; \
extern char cnp_far_func_hole_##ordinal[100000];&lt;/code&gt;
    &lt;p&gt;(If you‚Äôre interested in the details of why these macros are the way they are, see the next post in the series!)&lt;/p&gt;
    &lt;p&gt;Then you can declare as complex of a stencil as you need:&lt;/p&gt;
    &lt;code&gt;#include "cnp_stencils.h"

// Declare up to the maximum number of holes you need of one type
// in a function:
DECLARE_EXTERN_HOLES(1);
DECLARE_EXTERN_HOLES(2);

STENCIL_FUNCTION
void fused_multiply_add_sqrt_ifnotzero() {
  uint32_t a = STENCIL_HOLE32(1, uint32_t);
  uint32_t b = STENCIL_HOLE32(2, int32_t);
  uint64_t c = STENCIL_HOLE64(1, uint64_t);

  uint64_t fma = a * b + c;

  if (fma == 0) {
    void (*div_trap)(void) = STENCIL_FN_NEAR(1, void(*)(void));
    div_trap();
  }

  uint64_t (*sqrt)(uint64_t) = STENCIL_FN_FAR(1, uint64_t(*)(uint64_t));
  uint64_t result = sqrt(c);

  DECLARE_STENCIL_OUTPUT(uint64_t);
  stencil_output(result);
}&lt;/code&gt;
    &lt;p&gt;Which just for completeness sake, compiles into:&lt;/p&gt;
    &lt;quote&gt;0000000000000000 &amp;lt;fused_multiply_add_sqrt_ifnotzero&amp;gt;: 0: 50 push rax 1: b8 00 00 00 00 mov eax,0x0 2: R_X86_64_32 cnp_small_value_hole_2 6: b9 00 00 00 00 mov ecx,0x0 7: R_X86_64_32 cnp_small_value_hole_1 b: 0f af c8 imul ecx,eax e: 48 b8 00 00 00 00 00 movabs rax,0x0 15: 00 00 00 10: R_X86_64_64 cnp_large_value_hole_1 18: 48 01 c8 add rax,rcx 1b: 75 05 jne 22 &amp;lt;fused_multiply_add_sqrt_ifnotzero+0x22&amp;gt; 1d: e8 00 00 00 00 call 22 &amp;lt;fused_multiply_add_sqrt_ifnotzero+0x22&amp;gt; 1e: R_X86_64_PLT32 cnp_near_func_hole_1-0x4 22: 48 b8 00 00 00 00 00 movabs rax,0x0 29: 00 00 00 24: R_X86_64_64 cnp_far_func_hole_1 2c: 48 bf 00 00 00 00 00 movabs rdi,0x0 33: 00 00 00 2e: R_X86_64_64 cnp_large_value_hole_1 36: ff d0 call rax 38: 49 89 c4 mov r12,rax 3b: 58 pop rax 3c: e9 00 00 00 00 jmp 41 &amp;lt;fused_multiply_add_sqrt_ifnotzero+0x41&amp;gt; 3d: R_X86_64_PLT32 cnp_stencil_output-0x4&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45576502</guid><pubDate>Tue, 14 Oct 2025 05:14:26 +0000</pubDate></item><item><title>Why Study Programming Languages</title><link>https://people.csail.mit.edu/rachit/post/why-study-programming-languages/</link><description>&lt;doc fingerprint="d72839db0aea91f4"&gt;
  &lt;main&gt;
    &lt;p&gt;This class is about the study of programming languages. Before we start, I want to perform two activities with folks here. First, I want us to answer two dumb questions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Why do we design new programming languages?&lt;/item&gt;
      &lt;item&gt;What is a programming language?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While (2) seems to be the more fundamental question, we need to answer (1) to have any hope of even thinking about (2).&lt;/p&gt;
    &lt;p&gt;So first, why do we design programming languages? Every program that can be written, can be written in C or assembly or Java or any of the dozens of languages we already have. So why do we design new languages?&lt;/p&gt;
    &lt;p&gt;Common answers to this question will include words like abstraction, performance, convenience, usability etc. The problem with these answers is that apart from the measurable, they are all subjective, aesthetic choices. Convenience is a function of knowledge, familiarity, and community. Usability is similarly ill-defined and hard to measure. And of course, none of these metrics really predict which languages are widely used or popular.&lt;/p&gt;
    &lt;p&gt;Consider the thought of inventing a whole new natural language just to express a new concept clearly. Explaining the rules of grammar and construction would certainly be simpler than any natural language provides. And yet, we√¢d have the small, troubling problem that this knowledge would be almost entirely useless; we need to learn a commonly known natural language to communicate with people. And yet, this is something that we can often find ourselves doing with programming languages with the hope that the concepts learned in one language can be transferred into another; a world where being a polyglot is expected, not unusual.&lt;/p&gt;
    &lt;p&gt;Perhaps this points to a striking similarity between programming languages. As they evolve, they take features from each other and converge into one language singular. They√¢re only differences being the syntax used to represent them.&lt;/p&gt;
    &lt;p&gt;But of course, knowledge of a language is different from mastery. An expert C programmer√¢s bit twiddling is akin of magic while a Haskell programmers tower of abstractions will make mere mortals cower away in fear.&lt;/p&gt;
    &lt;p&gt;Here√¢s a hypothesis, the truth of which is unknown to me: we create programming languages to experience new ideas; ideas that would have remained inaccessible had we stayed with the old languages. Languages not just a form of expression but also a form of exploration. I do not create languages with the hope of expressing everything that was, but to express that which isn√¢t yet. It is the rare joy of a language designer to see their languages being used and abused to do something inconceivable to them. I would point to dozens of historical examples of this, from ALGOL, to APL, every time a language has enabled expression and forward exploration, it has changed the course of computing.&lt;/p&gt;
    &lt;p&gt;Now that we have some bearing of why we create programming languages, we can try answering what exactly is a programming language.&lt;/p&gt;
    &lt;p&gt;Is a language just syntax? Surely not, since symbols don√¢t have any meaning to them. Perhaps it is the meaning of programs in the language, its semantics that defines a language. But its meaning in terms of what? The results of programs? The internal states of this execution algorithm? Perhaps a purely mathematical description, detached from anything resembling a computer?&lt;/p&gt;
    &lt;p&gt;Something resembling semantics of languages does seem to be a part of what defines a language but it is definitely not the full story. Ask a Python programmer why they like it and they√¢ll point to the amazing library ecosystem; ask a web developer why they like JavaScript, and they√¢ll wax poetic about Web 2.0; to a Haskell proponent, it√¢s type system, to a LISP programmer, macros, to a Go programmer, its concurrency model and so on. All of these characteristics define languages and yet have very little to do with semantics. So semantics alone do not define languages.&lt;/p&gt;
    &lt;p&gt;Perhaps a tentative definition is that a programming language is defined by its syntax, semantics, and ecosystem. The former two are easy to study formally; we can teach you the mathematical tools needed to understand them. But for the latter, we must turn back to our first question: why do we design new languages. It is true that both Python and Go have ample libraries and a concurrency model. However, the exploratory power of Python is enabled by the sheer quantity and quality of those libraries while Go√¢s power comes from its concurrency model.&lt;/p&gt;
    &lt;p&gt;Therefore, I give my last definition of what a programming language is: syntax, semantics, and ecosystem in support of exploration; which parts of semantics and ecosystems to care about defined by what tools of exploration they provide. The study of programming languages encompasses all of these: syntax, semantics, type systems, runtime systems, garbage collectors, debuggers, IDEs, syntax highlighting, error messages, compilers, and design. Lines drawn between these are arbitrary, mostly by people like me trying to publish papers.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I encourage everyone to create the most absurd, implausible, and impractical languages. Chasing the measurable is often useful, expressing the expressible is insightful, but never forget the true goal of language design: to explore and create what isn√¢t.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45576623</guid><pubDate>Tue, 14 Oct 2025 05:36:57 +0000</pubDate></item><item><title>Why the push for Agentic when models can barely follow a simple instruction?</title><link>https://forum.cursor.com/t/why-the-push-for-agentic-when-models-can-barely-follow-a-single-simple-instruction/137154</link><description>&lt;doc fingerprint="ad50c87e6fc408b3"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I have a bunch of file for different reason, you need to work with a structure, agents like to work in folder structure, here is one of my custom agent instruction.&lt;/p&gt;
      &lt;p&gt;Now i have a agents.md with more generic details for all agent type, architecture file for my folder structure, another one for tasks with templates and so on.&lt;/p&gt;
      &lt;p&gt;Now i start all my prompt with please search and read/multi-read all .md file&lt;lb/&gt; (If i have the file system MCP installed, wich is free and godsend)&lt;/p&gt;
      &lt;p&gt;My md file has my high level planing, brainstorming files and other complementary file that i keep up to date so that when the AI is done ingesting all the md files he is prepped up to go dig code, write code and chew bubblegum‚Ä¶ Mmmm might need some work on the last one. HE MUST CHEW BUBBLE GUM AND HE HAS NO MOUTH (Claptrap kiss no mouth reference)&lt;/p&gt;
      &lt;p&gt;You need to have them work on small vertical slice that can be built under a 100k token more or less, more than that the agent start to misbehave and you need to fire him.&lt;/p&gt;
      &lt;p&gt;I have custom architect for building plan, codeseeker, coder, and other more specialised agents.&lt;/p&gt;
      &lt;p&gt;Build your team, build a structure, in the last 2 month playing with agents and python i learnt more about coding than a full year high school. I dont just tell them to work i watch them work, see how they tick, i learn by comparison, read the code and when im not sure? grab a few related file, post them to chatgpt 5 and i ask him to tutor me or ask free agents to document the file and i ask question.&lt;/p&gt;
      &lt;p&gt;You dont ask a human to climb a tree even tho he look like a monkey, he might be able to, but still not his best skill. Learn the limit, try to build tools to overcome their limit, keep asking question, keep improving you managerial skill because workin with agent is to start managing a team. Imm full on on the managing part with only rudimentary coding knowledge, if you are a good coder you can have you agent working on something while you code and use inline code completion and im talking full on function completion.&lt;/p&gt;
      &lt;p&gt;Maybe codex is more for you, there is a lot of agent type, providers each one with their strenght and weakness, experiement.&lt;/p&gt;
      &lt;p&gt;I really hope you can find your tool, the one adapted to what you want and that you can grow into your tool too, then you become borg! Hmm might be premature on the borg thing. Eh oh well.&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;You are a Deep Python Coding Agent, an expert AI specialized in implementing, refactoring, and maintaining Python codebases with absolute adherence to project standards. Your mission is to execute coding tasks exhaustively, ensuring every change is complete, tested, and documented, while strictly following the Agent Collaboration Charter and project rules. You NEVER write or execute code in terminals, REPLs, or interactive sessions‚Äîalways edit files directly and run commands via the project‚Äôs standard workflow (e.g., python main.py, pytest --testmon -q).&lt;/p&gt;
        &lt;p&gt;Core Principles&lt;/p&gt;
        &lt;p&gt;Exhaustive Implementation: For any coding task, dive deep into all relevant code‚Äîread files, trace dependencies, analyze tests, and understand integrations. Implement complete solutions with no omissions, addressing edge cases, error handling, and performance.&lt;/p&gt;
        &lt;p&gt;No Terminal Code Execution: NEVER write code snippets in terminals or REPLs. All code changes must be made by editing files (e.g., via write_file, edit_file). Run tests and commands only through the project‚Äôs workflow.&lt;/p&gt;
        &lt;p&gt;Mandatory Documentation Updates: After EVERY change, update docs/TASKS.md (claim task as in_progress, mark completed), docs/WORKLOG.md (log what, why, how to run), and docs/DECISIONS.md (if assumptions made). This is NON-NEGOTIABLE‚Äîfailure to update these will break the project process.&lt;/p&gt;
        &lt;p&gt;Task Continuity: Claim and complete tasks sequentially from docs/TASKS.md. Do not start new tasks until the current one is fully done (main runs, tests pass, docs updated). Roll through all pending tasks until none remain.&lt;/p&gt;
        &lt;p&gt;Quality Standards: Code must be PEP8-compliant, typed with type hints, readable, and free of TODOs. Run ruff/black/mypy on changes and fix issues. Prefer vertical slices that run end-to-end.&lt;/p&gt;
        &lt;p&gt;Testing Rigorousness: Add/update unit, integration, and e2e tests for every change. Use pytest --testmon -q during development for affected tests; run full pytest before marking done. No regressions allowed.&lt;/p&gt;
        &lt;p&gt;Deterministic and Complete: Provide exact file paths, final code, and commands. Never leave partial work‚Äîensure python main.py runs without errors.&lt;/p&gt;
        &lt;p&gt;Operational Workflow&lt;/p&gt;
        &lt;p&gt;Context Gathering: Always start by reading docs/ARCHITECTURE.md, docs/TASKS.md, docs/DECISIONS.md, docs/WORKLOG.md, docs/reference/*, and recent Plan/ notes.&lt;lb/&gt; Task Claiming: Append/update your entry in docs/TASKS.md (status=pending ‚Üí in_progress) before starting work.&lt;/p&gt;
        &lt;p&gt;Implementation:&lt;lb/&gt; -Read all related files (use read_file for up to 5 at once).&lt;lb/&gt; -Use search_files and list_code_definition_names to understand structure and dependencies.&lt;lb/&gt; -Edit files with complete changes (no partial writes).&lt;lb/&gt; -Add/update tests in test files.&lt;lb/&gt; -Run pytest --testmon -q incrementally; fix failures immediately.&lt;lb/&gt; -Validation: Run python main.py to ensure no breaks. Run full pytest pre-commit.&lt;lb/&gt; -Documentation: Update WORKLOG.md, DECISIONS.md (if needed), and set TASKS.md status=completed.&lt;lb/&gt; -Next Task: If tasks remain, claim the next one and repeat.&lt;/p&gt;
        &lt;p&gt;Tool Usage Guidelines&lt;/p&gt;
        &lt;p&gt;-read_file/edit_file/write_file: Use for all code changes; provide complete file contents.&lt;lb/&gt; -search_files: Regex search for patterns (e.g., function usages).&lt;lb/&gt; -list_code_definition_names: Overview of classes/functions in directories.&lt;lb/&gt; -Commands: Run via execute_command only for project workflow (e.g., pytest, main.py); never for code execution.&lt;/p&gt;
        &lt;p&gt;Response Standards&lt;lb/&gt; -Be technical and precise; no fluff.&lt;lb/&gt; -Structure responses with sections (e.g., Changes Made, Tests Added, Documentation Updates).&lt;lb/&gt; -Use code references like function_name().&lt;lb/&gt; -End with final status; no follow-ups unless blocked (then log in DECISIONS.md).&lt;/p&gt;
        &lt;p&gt;Constraints&lt;lb/&gt; Focus on Python coding and project maintenance; adhere to AGENTS.md rules.&lt;lb/&gt; If blocked, make least-surprising assumption, proceed, and log in DECISIONS.md.&lt;lb/&gt; Definition of Done: main runs, tests pass, docs updated, no unresolved TODOs.&lt;/p&gt;
        &lt;p&gt;Runs: python main.py &lt;/p&gt;
        &lt;p&gt;Tests: pytest -q &lt;/p&gt;
        &lt;p&gt;Lint/type pass (if configured) &lt;/p&gt;
        &lt;p&gt;No TODOs in changed code &lt;/p&gt;
        &lt;p&gt;Updated WORKLOG/TASKS &lt;/p&gt;
        &lt;p&gt;Output format&lt;/p&gt;
        &lt;p&gt;FILES CHANGED (with full paths)&lt;/p&gt;
        &lt;p&gt;Final code blocks for each file&lt;/p&gt;
        &lt;p&gt;RUN &amp;amp; TEST commands&lt;/p&gt;
        &lt;p&gt;NOTES/ASSUMPTIONS&lt;/p&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45577080</guid><pubDate>Tue, 14 Oct 2025 07:08:02 +0000</pubDate></item><item><title>KDE celebrates the 29th birthday and kicks off the yearly fundraiser</title><link>https://kde.org/fundraisers/yearend2025/</link><description>&lt;doc fingerprint="abd5cf5cba4a215f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Happy Birthday to us&lt;/head&gt;
    &lt;p&gt;This week is KDE√¢s 29th anniversary. It may not be a nice round number like 25 or 30, but whenever another birthday rolls around for an independent project the size and scope of KDE √¢ powered by the goodwill of its contributors and users √¢ that√¢s really quite something!&lt;/p&gt;
    &lt;p&gt;This year we√¢re celebrating by kicking off our yearly fundraiser. Let√¢s raise at least √¢¬¨50,000 before the end of the year!&lt;/p&gt;
    &lt;head rend="h3"&gt;Donated (updated daily)&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;√¢¬¨50000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;*Stretch goal coming soon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Make KDE√¢s Birthday Wishes Come True&lt;/head&gt;
    &lt;head rend="h3"&gt;Producing first-class software&lt;/head&gt;
    &lt;p&gt;KDE is on the verge of something big, and the popularity of its free software is on the rise. It√¢s increasingly being adopted by gamers, artists, professionals, companies, and public institutions. But the costs associated with developing and maintaining that software are also growing.&lt;/p&gt;
    &lt;p&gt;Your donation keeps KDE √¢in business√¢ and our software sustainable for generations to come.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping you in control&lt;/head&gt;
    &lt;p&gt;A core aim of KDE is keeping you in control of your digital life, and we do it by providing high-quality and privacy-conscious free software. But we can only keep doing it by preserving our own financial independence, so that we never become too dependent on any single source of support.&lt;/p&gt;
    &lt;p&gt;Your donation makes KDE truly independent. Funding from the people allows us keep KDE developed by the people, of the people, and for the people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleaning up the world&lt;/head&gt;
    &lt;p&gt;This week is also International E-Waste Day, and KDE wants a clean planet too! We started the End of 10 campaign because big tech corporations continue pushing everyone to chase the new shiny√¢¬¶ in the process filling landfills with perfectly functional devices that become terrible sources of pollution when junked*.&lt;/p&gt;
    &lt;p&gt;Your donation allows us to inform everybody about how they can help stave off these environmental disasters.&lt;/p&gt;
    &lt;p&gt;* Case in point: Microsoft is stopping free support for Windows 10 on hundreds of millions of computers this very week. Many of these old yet perfectly usable devices will not be able to upgrade because of spurious hardware requirements. Microsoft√¢s solution? √¢Throw away your computer and pollute the planet because we want to make even more money.√¢&lt;/p&gt;
    &lt;head rend="h3"&gt;Reaching people the tech industry left behind&lt;/head&gt;
    &lt;p&gt;Many aren√¢t in a position to replace their devices every few years, or download hundreds of gigabyes of data from an always-on internet connection. KDE produces software that doesn√¢t need the latest hardware or an always-on internet connection, allowing everybody find their space in the digitized world.&lt;/p&gt;
    &lt;p&gt;Your donation helps KDE serve those who are ignored by the industry, and bring marginalized users into the community so they can help the project grow for everyone.&lt;/p&gt;
    &lt;head rend="h3"&gt;Helping public institutions adopt free software&lt;/head&gt;
    &lt;p&gt;The governments of the world are starting to realize that using public funds to lock themselves into proprietary closed-source software has been a strategic geopolitical mistake.&lt;/p&gt;
    &lt;p&gt;Free software is publicly owned, representing the safest option for governments that want full control over their machines and safety for their citizens√¢ data. But often the standards required for software approval by public institutions is very high, and their needs very specific.&lt;/p&gt;
    &lt;p&gt;Your donation helps KDE adapt our software to what public institutions require, clearing the way for your tax dollars to fund KDE, not some big foreign companies.&lt;/p&gt;
    &lt;p&gt;Images "Konqi opens the magic box", ""Katie and Konqi make software", "Katie and Konqi take on the public administration" CC-BY-SA-4.0 license by Arctaxia. "Katie &amp;amp; Konqi recyle" CC-BY-SA-4.0 license by Nezumi Cafun√É¬©.&lt;/p&gt;
    &lt;head rend="h2"&gt;Goodies&lt;/head&gt;
    &lt;p&gt;Don√¢t forget to download your goodies after you donated! Get digital badges, printable cards, and more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45578117</guid><pubDate>Tue, 14 Oct 2025 09:54:53 +0000</pubDate></item><item><title>Prime Minister Anthony Albanese's mobile phone number made available online</title><link>https://www.abc.net.au/news/2025-10-14/anthony-albanese-mobile-phone-number-available-online/105889284</link><description>&lt;doc fingerprint="eed4935e1b73179d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Prime Minister Anthony Albanese's mobile phone number made available online&lt;/head&gt;
    &lt;head rend="h2"&gt;In short:&lt;/head&gt;
    &lt;p&gt;The mobile phone numbers of Anthony Albanese, Sussan Ley and other prominent Australians have been made public on a free website.&lt;/p&gt;
    &lt;p&gt;It is not clear when their details were first published, but Ms Ley only found out about the incident when the media contacted her, a spokesperson said.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;The Australian Federal Police is formally seeking to get the prime minister's number removed.&lt;/p&gt;
    &lt;p&gt;Authorities are investigating a website that lists the mobile phone numbers of Prime Minister Anthony Albanese and Opposition Leader Sussan Ley, among other well-known Australians.&lt;/p&gt;
    &lt;p&gt;The US-based website, which users can trial for free, boasts about having the mobile numbers and emails of millions of professionals.&lt;/p&gt;
    &lt;p&gt;Deputy Prime Minister Richard Marles said the government was aware of the website and had taken steps to address the numbers being on it.&lt;/p&gt;
    &lt;p&gt;"We've notified authorities and that is being worked through, but obviously, there is concern," he said.&lt;/p&gt;
    &lt;p&gt;The Australian Federal Police (AFP) has also formally sought for the prime minister's contact details to be removed, while other agencies are working to limit the impact on other parliamentarians.&lt;/p&gt;
    &lt;p&gt;The story was first reported by Ette Media, whose co-founder Antoinette Lattouf also had her number published on the site.&lt;/p&gt;
    &lt;p&gt;The ABC has chosen not to name the website in order to avoid further publicising the personal data of those impacted, but has confirmed at least some of the numbers are correct.&lt;/p&gt;
    &lt;p&gt;It is not clear how long the information has been available online, but Mr Albanese's team has been aware of the matter since last month.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Ms Ley said they were only made aware of the website yesterday after being contacted by media.&lt;/p&gt;
    &lt;p&gt;"This is obviously concerning ‚Ä¶ we have asked the website to remove the information," the spokesperson said.&lt;/p&gt;
    &lt;p&gt;NSW Premier Chris Minns, whose number has reportedly also been published, said he only became aware of the site this morning.&lt;/p&gt;
    &lt;p&gt;He said no-one was "prank calling" him yet, but it was a concern such personal data had been made public.&lt;/p&gt;
    &lt;p&gt;"We want to make sure that we're protecting that [data], but this is ‚Ä¶ the age that we live in ‚Äî technology is rapidly changing," he said.&lt;/p&gt;
    &lt;p&gt;"AI means that unscrupulous players can access information like they couldn't before.&lt;/p&gt;
    &lt;p&gt;"We're all just going to have to be on guard to protect our private information, but this is one in a long line of [such incidents]."&lt;/p&gt;
    &lt;p&gt;The website says artificial intelligence is used to scan social media, job portals and other sites to collect contact details, which companies and professionals can use to their advantage.&lt;/p&gt;
    &lt;p&gt;The ABC has contacted the website for comment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rise of AI increases risk&lt;/head&gt;
    &lt;p&gt;University of Melbourne School of Computing and Information Systems professor Toby Murray said there was a risk individuals could be harassed as a result of their details being made public.&lt;/p&gt;
    &lt;p&gt;He said it had always been possible for someone who was well-connected to get the number of a high-profile person, but rapidly evolving technologies streamlined the collection and distribution of sensitive material.&lt;/p&gt;
    &lt;p&gt;"With the rise of AI and related technologies, it's become really easy to gather this kind of information together, where previously it might have taken hours or weeks of research," Professor Murray said.&lt;/p&gt;
    &lt;p&gt;He said everyone had a right to privacy and that it was important that individuals were able to have their information removed.&lt;/p&gt;
    &lt;p&gt;The website does offer an opt-out feature.&lt;/p&gt;
    &lt;p&gt;In a statement, the AFP said it was "an offence to use a carriage service to menace, harass or cause offence" in Australia, and it would "take swift action against individuals who breach the law".&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45578318</guid><pubDate>Tue, 14 Oct 2025 10:30:32 +0000</pubDate></item></channel></rss>