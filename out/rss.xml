<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 17 Jan 2026 23:10:15 +0000</lastBuildDate><item><title>Counterfactual evaluation for recommendation systems</title><link>https://eugeneyan.com/writing/counterfactual-evaluation/</link><description>&lt;doc fingerprint="b472dbf60d6b017c"&gt;
  &lt;main&gt;
    &lt;p&gt;When I first started working on recommendation systems, I thought there was something weird about the way we did offline evaluation. First, we split customer interaction data into training and validation sets. Then, we train our recommenders on the training set before evaluating them on the validation set, usually on metrics such as recall, precision, and NDCG. This is similar to how we evaluate supervised machine learning models and doesn’t seem unusual at first glance.&lt;/p&gt;
    &lt;p&gt;But don’t our recommendations change how customers click or purchase? If customers can only interact with items shown to them, why do we perform offline evaluation on static historical data?&lt;/p&gt;
    &lt;p&gt;It took me a while to put a finger on it but I think this is why it felt weird: We’re treating recommendations as an observational problem when it really is an interventional problem.&lt;/p&gt;
    &lt;p&gt;Problems solved via supervised machine learning are usually observational problems. Given an observation such as product title, description, and image, we try to predict the product category. Our model learns P(category=phone|title=“…”, description=“…”, image=image01.jpeg).&lt;/p&gt;
    &lt;p&gt;On the other hand, recommendations are an interventional problem. We want to learn how different interventions (i.e., item recommendations) lead to different outcomes (i.e., clicks, purchases). By using logged customer interaction data as labels, the observational offline evaluation approach ignores the interventional nature of recommendations.&lt;/p&gt;
    &lt;p&gt;As a result, we’re not evaluating if users would click or purchase more due to our new recommendations; we’re evaluating how well the new recommendations fit logged data. Thus, what our model learns is P(view3=iphone|view1=pixel, view2=galaxy) when what we really want is P(click=True|recommend=iphone, view1=pixel, view2=galaxy).&lt;/p&gt;
    &lt;p&gt;The straightforward way to evaluate recommendations as an interventional problem is via A/B testing. Our interventions (i.e., new recommendations) are shown to users, we log their behavior attributed to our new recommendations, and then measure how metrics such as click-thru-rate and conversion change. However, it requires more effort relative to offline evaluation, experiment cycles may be long as we need enough data to make a judgement, and there’s the risk of deploying terrible experiments. Also, we may not have easy access to A/B testing we’re working on the research side of things.&lt;/p&gt;
    &lt;p&gt;The less direct approach is counterfactual evaluation. Counterfactual evaluation tries to answer “what would have happened if we show users our new recommendations instead of the existing recommendations?” This allows us to estimate the outcomes of potential A/B tests without actually running them.&lt;/p&gt;
    &lt;p&gt;The most widely known technique for counterfactual evaluation is Inverse Propensity Scoring (IPS). It’s sometimes also referred to as inverse probability weighting/sampling. The intuition behind it is that we can estimate how customer interactions will change—by reweighting how often each interaction will occur—based on how much more (or less) each item is shown by our new recommendation model. Here’s the IPS equation.&lt;/p&gt;
    &lt;p&gt;Let’s try to understand it by starting from the right. In section 1, &lt;code&gt;r&lt;/code&gt; represents the reward for an observation. This is the number of clicks or purchases or whatever metric is important to you in the logged data.&lt;/p&gt;
    &lt;p&gt;Next is the importance weight. The denominator (section 2a) represents our existing production recommender’s (&lt;code&gt;π0&lt;/code&gt;) probability of making a recommendation (aka action &lt;code&gt;a&lt;/code&gt;) given the context &lt;code&gt;x&lt;/code&gt;; the numerator (section 2b) represents the same probability but for our new recommender (&lt;code&gt;πe&lt;/code&gt;). (&lt;code&gt;π&lt;/code&gt; stands for recommendation policy.) For a user-to-item recommender, &lt;code&gt;x&lt;/code&gt; is the user; for an item-to-item recommender, &lt;code&gt;x&lt;/code&gt; is an item.&lt;/p&gt;
    &lt;p&gt;With the importance weight, we can compute how often a recommendation is made via the new model relative to the existing model. We can then use the ratio to update our logged rewards. For example, we have an old model (&lt;code&gt;π0&lt;/code&gt;) and new model (&lt;code&gt;πe&lt;/code&gt;) that recommend iPhone on the Pixel detail page, but with different probabilities:&lt;/p&gt;
    &lt;p&gt;In this scenario, the new model will recommend iPhone 0.6/0.4 = 1.5x as often as the old model. Thus, assuming a non-zero reward (i.e., the user clicked or purchased), we can reweight the logged reward to be worth 1.5x as much.&lt;/p&gt;
    &lt;p&gt;Finally, we average over our data (section 3) to get the IPS estimate (section 4) for our new recommender. This IPS estimate suggests how much reward (i.e., clicks, purchases) the new recommender would get relative to the production recommender if the new recommender was shown to users.&lt;/p&gt;
    &lt;p&gt;But how do we get the probability of making a recommendation (&lt;code&gt;a&lt;/code&gt;) given the context (&lt;code&gt;x&lt;/code&gt;)? Well, we can normalize the raw scores for each recommendation (via Plackett-Luce) to get each recommendation’s probability. Alternatively, if our recommendations are pre-computed, we can count the frequency of each recommendation in our recommendation store. My preferred approach is to use the impression count for each recommendation—I believe this is the most direct measure of the probability of making a recommendation and best adjusts for the presentation bias.&lt;/p&gt;
    &lt;p&gt;This dependence on recommendation probabilities or impressions likely explains why counterfactual evaluation isn’t more widely adopted in academic papers—most public datasets don’t include them. One exception is the Open Bandit Dataset which includes the recommendation probability (&lt;code&gt;action_prob&lt;/code&gt;) for each recommendation observation.&lt;/p&gt;
    &lt;p&gt;However, IPS has its pitfalls. One challenge is insufficient support. This happens when our new recommender being evaluated (&lt;code&gt;πe&lt;/code&gt;) makes a recommendation (&lt;code&gt;a&lt;/code&gt;) that our existing production recommender (&lt;code&gt;π0&lt;/code&gt;) didn’t make. Thus, &lt;code&gt;π0&lt;/code&gt;’s probability of &lt;code&gt;a&lt;/code&gt; is zero and we can’t compute the importance weight. We can mitigate this by deliberately showing random samples of non-recommended items on a sliver of traffic to log interactions for potential recommendations. (Spoiler: PMs might not like this.) A more palatable approach is ensure that all eligible items have a non-zero recommendation probability and then sample based on that probability. This gives all items a chance to be recommended.&lt;/p&gt;
    &lt;p&gt;IPS can also suffer from high variance when the new model (&lt;code&gt;πe&lt;/code&gt;) recommends very differently from the old model (&lt;code&gt;π0&lt;/code&gt;). Suppose &lt;code&gt;π0&lt;/code&gt; makes a recommendation (&lt;code&gt;a&lt;/code&gt;) with a probability of 0.001 and we logged a single click. If &lt;code&gt;πe&lt;/code&gt; makes the same recommendation (&lt;code&gt;a&lt;/code&gt;) with a probability of 0.1, we would reweight that single click by 100x—this is likely a severe overestimation. One solution is to ensure that the new recommenders being evaluated don’t differ too much from the production recommender, thus preventing the importance weight from exploding.&lt;/p&gt;
    &lt;p&gt;Another solution is Clipped IPS (CIPS). CIPS lets us set a maximum threshold for the importance weight. For example, if our threshold is 10, an importance weight greater than 10 is clipped to it. However, tuning the clipping parameter can be tricky.&lt;/p&gt;
    &lt;p&gt;Another approach is Self-Normalized IPS (SNIPS). SNIPS divides the IPS estimate by the importance weight. This rescaling prevents overinflated IPS estimates. Relative to CIPS, SNIPS is simpler and doesn’t require setting a parameter.&lt;/p&gt;
    &lt;p&gt;Which works better? At a recent RecSys 2021 tutorial, Yuta Saito compared various methods via experiments on synthetic data generated via Open Bandit Pipeline with 10 possible actions. He also assessed the direct method (DM) which we didn’t discuss. In a nutshell, DM trains a model to impute missing rewards. Think of it as similar to building an environment model for reinforcement learning, such as OpenAI gym or Criteo reco-gym, which we can then use to train and evaluate our RL models.&lt;/p&gt;
    &lt;p&gt;He found that IPS outperformed DM as the amount of logged data increases, and that CIPS didn’t perform much better than IPS. Overall, SNIPS performed the best (i.e., had the least error) and without the need for any parameter tuning. The tutorial goes on to discuss other estimators such as Doubly Robust (combining DM and SNIPS) as well as counterfactual learning—highly recommend checking it out.&lt;/p&gt;
    &lt;p&gt;Nonetheless, one downside of SNIPS is that it requires computing the importance weight for all observations; in IPS, we only need the importance weight for observations with non-zero reward. If we consider how most recommendations have zero reward (&amp;lt;10% CTR or conversion), SNIPS increases storage requirements of recommendation probabilities and computation of importance weights by 10x or more. That said, the authors of SNIPS found that the increase in computation is made up for via faster convergence.&lt;/p&gt;
    &lt;p&gt;Let me conclude by clarifying that I’m not suggesting for us to stop training and evaluating recsys models via the observational paradigm. Despite its limitations, it has several benefits. First, it’s an established evaluation framework with many public datasets and standard metrics. This makes it easier to compare various techniques. Second, we can collect training and evaluation data even before deploying our first recommender. Customer interaction data is generated organically when customers use our platforms. Thus, the conventional offline evaluation approach is a good place to start.&lt;/p&gt;
    &lt;p&gt;Nonetheless, if you’re keen to try a new evaluation approach, or find your offline metrics diverging from online A/B testing outcomes, consider counterfactual evaluation via SNIPS. In addition, though I’ve been discussing counterfactual evaluation in the context of recsys, it’s also applicable to other use cases where you want to simulate A/B tests offline.&lt;/p&gt;
    &lt;p&gt;Thanks to Arnab Bhadury, Vicki Boykis, and Yuta Saito for reading drafts of this.&lt;/p&gt;
    &lt;p&gt;If you found this useful, please cite this write-up as:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yan, Ziyou. (Apr 2022). Counterfactual Evaluation for Recommendation Systems. eugeneyan.com. https://eugeneyan.com/writing/counterfactual-evaluation/.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;@article{yan2022counterfactual,
  title   = {Counterfactual Evaluation for Recommendation Systems},
  author  = {Yan, Ziyou},
  journal = {eugeneyan.com},
  year    = {2022},
  month   = {Apr},
  url     = {https://eugeneyan.com/writing/counterfactual-evaluation/}
}&lt;/code&gt;
    &lt;p&gt;Join 11,800+ readers getting updates on machine learning, RecSys, LLMs, and engineering.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46655524</guid><pubDate>Sat, 17 Jan 2026 05:20:20 +0000</pubDate></item><item><title>Show HN: Streaming gigabyte medical images from S3 without downloading them</title><link>https://github.com/PABannier/WSIStreamer</link><description>&lt;doc fingerprint="286db0183daf5dc5"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern, cloud-native tile server for Whole Slide Images. One command to start serving tiles directly from S3.&lt;/p&gt;
    &lt;code&gt;# Installation (requires Rust, see alternatives below)
cargo install wsi-streamer

# On your local machine
wsi-streamer s3://my-slides-bucket --s3-region eu-west-3&lt;/code&gt;
    &lt;p&gt;That's it. No configuration files, no local storage, no complex setup. Open &lt;code&gt;http://localhost:3000/view/sample.svs&lt;/code&gt; in your browser to view a slide.&lt;/p&gt;
    &lt;p&gt;Whole Slide Images are large (1-3GB+) and typically live in object storage. Traditional viewers require downloading entire files before serving a single tile. WSIStreamer takes a different approach: it understands slide formats natively, fetches only the bytes needed via HTTP range requests, and returns JPEG tiles immediately.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Range-based streaming — fetches only the bytes needed for each tile, no local files&lt;/item&gt;
      &lt;item&gt;Built-in viewer — OpenSeadragon-based web viewer with pan, zoom, and dark theme&lt;/item&gt;
      &lt;item&gt;Native format support — Rust parsers for Aperio SVS and pyramidal TIFF&lt;/item&gt;
      &lt;item&gt;Production-ready — HMAC-SHA256 signed URL authentication&lt;/item&gt;
      &lt;item&gt;Multi-level caching — slides, blocks, and encoded tiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install from crates.io:&lt;/p&gt;
    &lt;code&gt;cargo install wsi-streamer&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/PABannier/WSIStreamer.git
cd WSIStreamer
cargo build --release&lt;/code&gt;
    &lt;p&gt;Or run with Docker:&lt;/p&gt;
    &lt;code&gt;# Pull from GitHub Container Registry
docker run -p 3000:3000 -e WSI_S3_BUCKET=my-bucket ghcr.io/pabannier/wsistreamer:latest

# Or use Docker Compose for local development with MinIO
docker compose up --build&lt;/code&gt;
    &lt;code&gt;# Serve slides from S3
wsi-streamer s3://my-slides

# Custom port
wsi-streamer s3://my-slides --port 8080

# S3-compatible storage (MinIO, etc.)
wsi-streamer s3://slides --s3-endpoint http://localhost:9000&lt;/code&gt;
    &lt;code&gt;# List slides
curl http://localhost:3000/slides

# Get slide metadata
curl http://localhost:3000/slides/sample.svs

# Fetch a tile (level 0, position 0,0)
curl http://localhost:3000/tiles/sample.svs/0/0/0.jpg -o tile.jpg

# Get thumbnail
curl "http://localhost:3000/slides/sample.svs/thumbnail?max_size=256" -o thumb.jpg&lt;/code&gt;
    &lt;code&gt;# Enable HMAC-SHA256 authentication
wsi-streamer s3://my-slides --auth-enabled --auth-secret "$SECRET"

# Generate signed URLs
wsi-streamer sign --path /tiles/slide.svs/0/0/0.jpg --secret "$SECRET" --base-url http://localhost:3000&lt;/code&gt;
    &lt;p&gt;The web viewer handles authentication automatically when enabled.&lt;/p&gt;
    &lt;code&gt;# Check S3 connectivity
wsi-streamer check s3://my-slides

# List available slides
wsi-streamer check s3://my-slides --list-slides

# Test a specific slide
wsi-streamer check s3://my-slides --test-slide sample.svs&lt;/code&gt;
    &lt;p&gt;All options can be set via CLI flags or environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Env Var&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_HOST&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Bind address&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_PORT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3000&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP port&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-bucket&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_BUCKET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;S3 bucket name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-endpoint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_ENDPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;Custom S3 endpoint&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--s3-region&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_S3_REGION&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;us-east-1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;AWS region&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-enabled&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable authentication&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--auth-secret&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;—&lt;/cell&gt;
        &lt;cell&gt;HMAC secret key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_SLIDES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max slides in cache&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--cache-tiles&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CACHE_TILES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;100MB&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Tile cache size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--jpeg-quality&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_JPEG_QUALITY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;80&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG quality (1-100)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--cors-origins&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;WSI_CORS_ORIGINS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;any&lt;/cell&gt;
        &lt;cell&gt;Allowed CORS origins&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;wsi-streamer --help&lt;/code&gt; for full details.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Endpoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /health&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Health check&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /view/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Web viewer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /tiles/{slide_id}/{level}/{x}/{y}.jpg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fetch tile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List slides&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Slide metadata&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/thumbnail&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Thumbnail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;GET /slides/{slide_id}/dzi&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;DZI descriptor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See API_SPECIFICATIONS.md for complete documentation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Extensions&lt;/cell&gt;
        &lt;cell role="head"&gt;Compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aperio SVS&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.svs&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Pyramidal TIFF&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;.tif&lt;/code&gt;, &lt;code&gt;.tiff&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;JPEG, JPEG 2000&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Files must be tiled (not stripped) and pyramidal.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 17th, 2026: front page of Hacker News and Rust subreddit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT. See LICENSE.&lt;/p&gt;
    &lt;p&gt;Issues and pull requests welcome. See CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46656358</guid><pubDate>Sat, 17 Jan 2026 08:46:08 +0000</pubDate></item><item><title>ClickHouse acquires Langfuse</title><link>https://langfuse.com/blog/joining-clickhouse</link><description>&lt;doc fingerprint="2bee90517ddf277e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Langfuse joins ClickHouse&lt;/head&gt;
    &lt;p&gt;Our goal continues to be building the best LLM engineering platform&lt;/p&gt;
    &lt;p&gt;ClickHouse has acquired Langfuse.&lt;/p&gt;
    &lt;p&gt;If you’re reading this as a Langfuse user, your first question is probably: What does this mean for me?&lt;/p&gt;
    &lt;p&gt;Our roadmap stays the same, our goal continues to be building the best LLM engineering platform, and we remain committed to open source and self-hosting. There are no immediate changes to how you use Langfuse and how you can reach out to us.&lt;/p&gt;
    &lt;p&gt;What does change is our ability to move faster. With ClickHouse behind us, we can invest more deeply into performance, reliability, and our roadmap that helps teams build and improve AI applications in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;What stays the same&lt;/head&gt;
    &lt;p&gt;This is the section we would want to read first, too.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Langfuse stays open source and self‑hostable. There are no planned changes to licensing. As you know, we leaned heavily into OSS over the last years.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud keeps running as‑is. Same product, same endpoints, same experience.&lt;/item&gt;
      &lt;item&gt;Support stays the same. Same channels, same SLAs for existing customers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What gets better now&lt;/head&gt;
    &lt;p&gt;Joining Clickhouse compresses years of operational learning into immediate, real customer benefits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More engineering leverage on the hardest parts. Langfuse is a data‑intensive product. Working closely with the ClickHouse engineering team helps us push performance and reliability.&lt;/item&gt;
      &lt;item&gt;Faster progress on enhanced enterprise-grade compliance and security, with the help of Clickhouse’s resources.&lt;/item&gt;
      &lt;item&gt;Learning from Clickhouse’s customer success and support playbook. This puts us years ahead and allows us to spend more time on what we really care about: our users.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;A quick look back&lt;/head&gt;
    &lt;p&gt;The longer version of how we got here is in our handbook.&lt;/p&gt;
    &lt;p&gt;Langfuse started the same way many LLM products start: we were building agents ourselves. And we constantly ran into the same problems.&lt;/p&gt;
    &lt;p&gt;Building LLM apps is easy to demo and hard to run in production. Debugging is different, quality is non‑deterministic, and the iteration loop is messy. When we did Y Combinator in early 2023, we saw this every week, both in our own projects and in what other founders in our cohort were working on.&lt;/p&gt;
    &lt;p&gt;So we built a duct tape version of what we wished existed: tracing and evaluation primitives that are easy to add, easy to self‑host, and actually useful for iterating.&lt;/p&gt;
    &lt;p&gt;The very first version was intentionally simple. It ran on Postgres, because speed of shipping mattered more than theoretical scaling. That got us to a real product and a real community fast.&lt;/p&gt;
    &lt;p&gt;Then people actually started to use the product more than we could have imagined.&lt;/p&gt;
    &lt;p&gt;As adoption grew, Postgres became the bottleneck for the workloads Langfuse needed to support (high‑throughput ingestion + fast analytical reads). With Langfuse v3, we switched the core data layer to ClickHouse to make Langfuse scale for production workloads, both in Cloud and self‑hosted deployments.&lt;/p&gt;
    &lt;p&gt;And if you like infrastructure deep dives, here’s the v3 migration write‑up.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why join ClickHouse&lt;/head&gt;
    &lt;p&gt;There are a lot of ways this could have gone. We didn’t plan to sell the company. Actually, we had Term Sheets for a great Series A and were looking forward to some days off over Christmas after an intense year.&lt;/p&gt;
    &lt;p&gt;What changed wasn’t our conviction in Langfuse, it was realizing how much faster we can go together with ClickHouse, while staying true to what makes Langfuse work: open source, self-hosting, and a product that’s built for real production workloads.&lt;/p&gt;
    &lt;head rend="h3"&gt;A shared history (before the acquisition)&lt;/head&gt;
    &lt;p&gt;This dialogue didn’t start with a term sheet. Because Langfuse runs on ClickHouse, we naturally ended up collaborating early and often.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’ve always been closely in touch with many teams at ClickHouse: sharing feedback with the database team, and using new features to make Langfuse more reliable. For example, compute-compute separation helps us to reduce the risk of noisy-neighbours on Langfuse Cloud.&lt;/item&gt;
      &lt;item&gt;Langfuse Cloud is a large customer of ClickHouse Cloud.&lt;/item&gt;
      &lt;item&gt;Teams at ClickHouse use Langfuse to improve their agentic applications.&lt;/item&gt;
      &lt;item&gt;We invested heavily in ClickHouse-backed self-hosting: documentation, templates, and deployment patterns, and collaborated closely with ClickHouse on improving that experience.&lt;/item&gt;
      &lt;item&gt;As a result, Langfuse introduced thousands of teams to ClickHouse when upgrading from Langfuse v2 to v3.&lt;/item&gt;
      &lt;item&gt;We’ve done community meetups together: a ClickHouse meetup at our Berlin office, another one in San Francisco, and an OpenHouse talk in Amsterdam.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Langfuse runs on ClickHouse, ClickHouse uses Langfuse to optimize its agentic products, we share lots of customers and OSS deployments; that gives ClickHouse every incentive to keep Langfuse fast, reliable, and boringly dependable at scale.&lt;/p&gt;
    &lt;p&gt;So in many ways, we operated like long-term partners. This acquisition is a way to make that partnership permanent — and invest aggressively together.&lt;/p&gt;
    &lt;p&gt;Max shared on how we use ClickHouse to keep product performance ahead of demand at ClickHouse Open House (recording) in Amsterdam.&lt;/p&gt;
    &lt;head rend="h3"&gt;Culture and engineering fit&lt;/head&gt;
    &lt;p&gt;The first time we met Aaron, Yury, Alexey, Tanya, Ryadh, and Pete in-person ended up in a long lunch in Amsterdam. It became obvious we share a similar view on building great developer tooling, how that drives everything within our companies, and how fast analytics is increasingly foundational for building and optimizing agentic products.&lt;/p&gt;
    &lt;p&gt;We already knew that ClickHouse is one of the best infrastructure engineering teams in the world. More importantly, the engineering culture feels like an instant match:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;open-source identity and stewardship&lt;/item&gt;
      &lt;item&gt;developer-first product instincts&lt;/item&gt;
      &lt;item&gt;performance and reliability as product features (not afterthoughts)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The whole Langfuse team will join ClickHouse to continue building Langfuse. All of these aspects were important to us and we couldn’t be more excited.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’re focused on next&lt;/head&gt;
    &lt;p&gt;Our north star doesn’t change: help teams ship useful, reliable agents by closing the loop from production data to better prompts, evaluations, and product decisions.&lt;/p&gt;
    &lt;p&gt;Concretely, we’re investing in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Production monitoring and analytics for real agent systems (not just offline evals).&lt;/item&gt;
      &lt;item&gt;Workflows across tracing, labeling, and experiments so iteration loops get shorter.&lt;/item&gt;
      &lt;item&gt;More performance and scale—especially for large self‑hosted and enterprise deployments.&lt;/item&gt;
      &lt;item&gt;More polish (UI/UX, developer experience, and docs) so the product stays simple even as the space gets more complex.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can always follow along on the public roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you&lt;/head&gt;
    &lt;p&gt;Langfuse exists because the community pushed it forward, through GitHub issues, PRs, feedback, and lots of Slack messages and spontaneous calls to dig into a product feature together.&lt;/p&gt;
    &lt;p&gt;We’re grateful for the trust you’ve put in us. Joining ClickHouse is our way of honoring that trust by putting more resources behind the thing we care about most: building a product you can rely on.&lt;/p&gt;
    &lt;p&gt;We’re excited for what’s next!&lt;lb/&gt; Max, Clemens, and Marc&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;Is Langfuse still open source?&lt;lb/&gt;Yes. No licensing changes planned.&lt;/p&gt;
    &lt;p&gt;Can I still self‑host Langfuse?&lt;lb/&gt;Yes. Self‑hosting is a first‑class path.&lt;/p&gt;
    &lt;p&gt;Does anything change for Langfuse Cloud customers today?&lt;lb/&gt;No. Same product, same endpoints, same contracts.&lt;/p&gt;
    &lt;p&gt;Where do I go for support?&lt;lb/&gt;No changes: https://langfuse.com/support&lt;/p&gt;
    &lt;p&gt;Will the Langfuse team stay on Langfuse?&lt;lb/&gt;Yes. The team is joining ClickHouse and will keep building Langfuse. Also, we continue hiring in Berlin and SF.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join the discussion&lt;/head&gt;
    &lt;p&gt;If you have any other questions, let’s discuss together on GitHub Discussions.&lt;/p&gt;
    &lt;p&gt;If you’re an enterprise customer and have additional questions, feel free to reach out to enterprise@langfuse.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46656552</guid><pubDate>Sat, 17 Jan 2026 09:15:45 +0000</pubDate></item><item><title>Show HN: I built a tool to assist AI agents to know when a PR is good to go</title><link>https://dsifry.github.io/goodtogo/</link><description>&lt;doc fingerprint="27e4e6575d9246b2"&gt;
  &lt;main&gt;
    &lt;p&gt;Deterministic PR readiness detection for AI coding agents&lt;/p&gt;
    &lt;p&gt;The missing piece in AI-assisted development: knowing when you’re actually done.&lt;/p&gt;
    &lt;p&gt;AI coding agents are transforming software development. They can write code, fix bugs, respond to review comments, and create pull requests. But they all share one fundamental problem:&lt;/p&gt;
    &lt;p&gt;They can’t reliably know when a PR is ready to merge.&lt;/p&gt;
    &lt;p&gt;Think about it. When you ask an AI agent to “fix the CI and address the review comments,” how does it know when it’s finished?&lt;/p&gt;
    &lt;p&gt;Without deterministic answers, agents either:&lt;/p&gt;
    &lt;p&gt;Good To Go provides a single command that answers the question definitively:&lt;/p&gt;
    &lt;code&gt;gtg 123
&lt;/code&gt;
    &lt;p&gt;That’s it. One command. One answer.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
        &lt;cell role="head"&gt;Meaning&lt;/cell&gt;
        &lt;cell role="head"&gt;What to Do&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;READY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;All clear&lt;/cell&gt;
        &lt;cell&gt;Merge it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ACTION_REQUIRED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Comments need fixes&lt;/cell&gt;
        &lt;cell&gt;Address them&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;UNRESOLVED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Open discussions&lt;/cell&gt;
        &lt;cell&gt;Resolve them&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;CI_FAILING&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Checks not passing&lt;/cell&gt;
        &lt;cell&gt;Fix the build&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;ERROR&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Failed to fetch data&lt;/cell&gt;
        &lt;cell&gt;Check token/network&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;No ambiguity. No guessing. No infinite loops.&lt;/p&gt;
    &lt;p&gt;Good To Go analyzes your PR across three dimensions:&lt;/p&gt;
    &lt;p&gt;Combines all GitHub check runs and commit statuses into a single pass/fail/pending state. Handles the complexity of multiple CI systems, required vs optional checks, and in-progress runs.&lt;/p&gt;
    &lt;p&gt;Not all review comments are created equal. Good To Go classifies each comment as:&lt;/p&gt;
    &lt;p&gt;Built-in parsers understand the patterns of popular automated reviewers:&lt;/p&gt;
    &lt;p&gt;Distinguishes between truly unresolved discussions and threads that are technically “unresolved” but already addressed in subsequent commits.&lt;/p&gt;
    &lt;p&gt;Good To Go is built specifically for how AI agents work:&lt;/p&gt;
    &lt;p&gt;Default mode returns &lt;code&gt;0&lt;/code&gt; for any analyzable state—because AI agents should parse the JSON output, not interpret exit codes as errors.&lt;/p&gt;
    &lt;code&gt;# AI-friendly (default): exit 0 + parse JSON
gtg 123 --format json

# Shell-script friendly: semantic exit codes
gtg 123 -q  # quiet mode, exit code only
&lt;/code&gt;
    &lt;p&gt;Every response includes exactly what an agent needs to take action:&lt;/p&gt;
    &lt;code&gt;{
  "status": "ACTION_REQUIRED",
  "action_items": [
    "Fix CRITICAL comment from coderabbit in src/db.py:42",
    "Resolve thread started by @reviewer in api.py"
  ],
  "actionable_comments": [...],
  "ci_status": {...},
  "threads": {...}
}
&lt;/code&gt;
    &lt;p&gt;Track what’s already been handled across agent sessions:&lt;/p&gt;
    &lt;code&gt;gtg 123 --state-path .goodtogo/state.db  # Remember dismissed comments
gtg 123 --refresh                         # Force fresh analysis
&lt;/code&gt;
    &lt;p&gt;Make &lt;code&gt;gtg&lt;/code&gt; a required status check. PRs can’t merge until they’re truly ready—not just “CI passed.”&lt;/p&gt;
    &lt;code&gt;# .github/workflows/pr-check.yml
- name: Check PR readiness
  run: gtg $ --semantic-codes
&lt;/code&gt;
    &lt;p&gt;Give your AI agent a definitive answer instead of endless polling:&lt;/p&gt;
    &lt;code&gt;result = subprocess.run(["gtg", pr_number, "--format", "json"], ...)
data = json.loads(result.stdout)

if data["status"] == "READY":
    merge_pr()
elif data["status"] == "ACTION_REQUIRED":
    for item in data["action_items"]:
        address_feedback(item)
&lt;/code&gt;
    &lt;p&gt;Monitor a PR through its entire lifecycle:&lt;/p&gt;
    &lt;code&gt;while true; do
  gtg 123 -q
  case $? in
    0) echo "Ready to merge!"; break ;;
    1) handle_comments ;;
    2) resolve_threads ;;
    3) wait_for_ci ;;
  esac
  sleep 60
done
&lt;/code&gt;
    &lt;code&gt;# Install
pip install gtg

# Set your GitHub token
export GITHUB_TOKEN=ghp_...

# Check a PR (auto-detects repo from git origin)
gtg 123

# Explicit repo
gtg 123 --repo owner/repo

# Human-readable output
gtg 123 --format text
&lt;/code&gt;
    &lt;p&gt;Good To Go embeds several opinions about PR workflows:&lt;/p&gt;
    &lt;p&gt; Made with Claude Code&lt;lb/&gt; by David Sifry &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46656759</guid><pubDate>Sat, 17 Jan 2026 09:55:56 +0000</pubDate></item><item><title>Map To Poster – Create Art of your favourite city</title><link>https://github.com/originalankur/maptoposter</link><description>&lt;doc fingerprint="a8f7d87e9ce1dad8"&gt;
  &lt;main&gt;
    &lt;p&gt;Generate beautiful, minimalist map posters for any city in the world.&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;
    &lt;code&gt;python create_map_poster.py --city &amp;lt;city&amp;gt; --country &amp;lt;country&amp;gt; [options]&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Short&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--city&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-c&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--country&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-C&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Country name&lt;/cell&gt;
        &lt;cell&gt;required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--theme&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Theme name&lt;/cell&gt;
        &lt;cell&gt;feature_based&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;--distance&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;-d&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Map radius in meters&lt;/cell&gt;
        &lt;cell&gt;29000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--list-themes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;List all available themes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Iconic grid patterns
python create_map_poster.py -c "New York" -C "USA" -t noir -d 12000           # Manhattan grid
python create_map_poster.py -c "Barcelona" -C "Spain" -t warm_beige -d 8000   # Eixample district

# Waterfront &amp;amp; canals
python create_map_poster.py -c "Venice" -C "Italy" -t blueprint -d 4000       # Canal network
python create_map_poster.py -c "Amsterdam" -C "Netherlands" -t ocean -d 6000  # Concentric canals
python create_map_poster.py -c "Dubai" -C "UAE" -t midnight_blue -d 15000     # Palm &amp;amp; coastline

# Radial patterns
python create_map_poster.py -c "Paris" -C "France" -t pastel_dream -d 10000   # Haussmann boulevards
python create_map_poster.py -c "Moscow" -C "Russia" -t noir -d 12000          # Ring roads

# Organic old cities
python create_map_poster.py -c "Tokyo" -C "Japan" -t japanese_ink -d 15000    # Dense organic streets
python create_map_poster.py -c "Marrakech" -C "Morocco" -t terracotta -d 5000 # Medina maze
python create_map_poster.py -c "Rome" -C "Italy" -t warm_beige -d 8000        # Ancient layout

# Coastal cities
python create_map_poster.py -c "San Francisco" -C "USA" -t sunset -d 10000    # Peninsula grid
python create_map_poster.py -c "Sydney" -C "Australia" -t ocean -d 12000      # Harbor city
python create_map_poster.py -c "Mumbai" -C "India" -t contrast_zones -d 18000 # Coastal peninsula

# River cities
python create_map_poster.py -c "London" -C "UK" -t noir -d 15000              # Thames curves
python create_map_poster.py -c "Budapest" -C "Hungary" -t copper_patina -d 8000  # Danube split

# List available themes
python create_map_poster.py --list-themes&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Distance&lt;/cell&gt;
        &lt;cell role="head"&gt;Best for&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4000-6000m&lt;/cell&gt;
        &lt;cell&gt;Small/dense cities (Venice, Amsterdam center)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;8000-12000m&lt;/cell&gt;
        &lt;cell&gt;Medium cities, focused downtown (Paris, Barcelona)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;15000-20000m&lt;/cell&gt;
        &lt;cell&gt;Large metros, full city view (Tokyo, Mumbai)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;17 themes available in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Theme&lt;/cell&gt;
        &lt;cell role="head"&gt;Style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;feature_based&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Classic black &amp;amp; white with road hierarchy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;gradient_roads&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Smooth gradient shading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;contrast_zones&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;High contrast urban density&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;noir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pure black background, white roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;midnight_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Navy background with gold roads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blueprint&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Architectural blueprint aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;neon_cyberpunk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Dark with electric pink/cyan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;warm_beige&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Vintage sepia tones&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pastel_dream&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Soft muted pastels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;japanese_ink&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Minimalist ink wash style&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;forest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deep greens and sage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ocean&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Blues and teals for coastal cities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;terracotta&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mediterranean warmth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sunset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Warm oranges and pinks&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;autumn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Seasonal burnt oranges and reds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;copper_patina&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Oxidized copper aesthetic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;monochrome_blue&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Single blue color family&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Posters are saved to &lt;code&gt;posters/&lt;/code&gt; directory with format:&lt;/p&gt;
    &lt;code&gt;{city}_{theme}_{YYYYMMDD_HHMMSS}.png
&lt;/code&gt;
    &lt;p&gt;Create a JSON file in &lt;code&gt;themes/&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;{
  "name": "My Theme",
  "description": "Description of the theme",
  "bg": "#FFFFFF",
  "text": "#000000",
  "gradient_color": "#FFFFFF",
  "water": "#C0C0C0",
  "parks": "#F0F0F0",
  "road_motorway": "#0A0A0A",
  "road_primary": "#1A1A1A",
  "road_secondary": "#2A2A2A",
  "road_tertiary": "#3A3A3A",
  "road_residential": "#4A4A4A",
  "road_default": "#3A3A3A"
}&lt;/code&gt;
    &lt;code&gt;map_poster/
├── create_map_poster.py          # Main script
├── themes/               # Theme JSON files
├── fonts/                # Roboto font files
├── posters/              # Generated posters
└── README.md
&lt;/code&gt;
    &lt;p&gt;Quick reference for contributors who want to extend or modify the script.&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐     ┌──────────────┐     ┌─────────────────┐
│   CLI Parser    │────▶│  Geocoding   │────▶│  Data Fetching  │
│   (argparse)    │     │  (Nominatim) │     │    (OSMnx)      │
└─────────────────┘     └──────────────┘     └─────────────────┘
                                                     │
                        ┌──────────────┐             ▼
                        │    Output    │◀────┌─────────────────┐
                        │  (matplotlib)│     │   Rendering     │
                        └──────────────┘     │  (matplotlib)   │
                                             └─────────────────┘
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Function&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Modify when...&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_coordinates()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;City → lat/lon via Nominatim&lt;/cell&gt;
        &lt;cell&gt;Switching geocoding provider&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_poster()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Main rendering pipeline&lt;/cell&gt;
        &lt;cell&gt;Adding new map layers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_colors_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road color by OSM highway tag&lt;/cell&gt;
        &lt;cell&gt;Changing road styling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_edge_widths_by_type()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Road width by importance&lt;/cell&gt;
        &lt;cell&gt;Adjusting line weights&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;create_gradient_fade()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Top/bottom fade effect&lt;/cell&gt;
        &lt;cell&gt;Modifying gradient overlay&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;load_theme()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JSON theme → dict&lt;/cell&gt;
        &lt;cell&gt;Adding new theme properties&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;z=11  Text labels (city, country, coords)
z=10  Gradient fades (top &amp;amp; bottom)
z=3   Roads (via ox.plot_graph)
z=2   Parks (green polygons)
z=1   Water (blue polygons)
z=0   Background color
&lt;/code&gt;
    &lt;code&gt;# In get_edge_colors_by_type() and get_edge_widths_by_type()
motorway, motorway_link     → Thickest (1.2), darkest
trunk, primary              → Thick (1.0)
secondary                   → Medium (0.8)
tertiary                    → Thin (0.6)
residential, living_street  → Thinnest (0.4), lightest&lt;/code&gt;
    &lt;p&gt;New map layer (e.g., railways):&lt;/p&gt;
    &lt;code&gt;# In create_poster(), after parks fetch:
try:
    railways = ox.features_from_point(point, tags={'railway': 'rail'}, dist=dist)
except:
    railways = None

# Then plot before roads:
if railways is not None and not railways.empty:
    railways.plot(ax=ax, color=THEME['railway'], linewidth=0.5, zorder=2.5)&lt;/code&gt;
    &lt;p&gt;New theme property:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add to theme JSON: &lt;code&gt;"railway": "#FF0000"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Use in code: &lt;code&gt;THEME['railway']&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add fallback in &lt;code&gt;load_theme()&lt;/code&gt;default dict&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All text uses &lt;code&gt;transform=ax.transAxes&lt;/code&gt; (0-1 normalized coordinates):&lt;/p&gt;
    &lt;code&gt;y=0.14  City name (spaced letters)
y=0.125 Decorative line
y=0.10  Country name
y=0.07  Coordinates
y=0.02  Attribution (bottom-right)
&lt;/code&gt;
    &lt;code&gt;# Get all buildings
buildings = ox.features_from_point(point, tags={'building': True}, dist=dist)

# Get specific amenities
cafes = ox.features_from_point(point, tags={'amenity': 'cafe'}, dist=dist)

# Different network types
G = ox.graph_from_point(point, dist=dist, network_type='drive')  # roads only
G = ox.graph_from_point(point, dist=dist, network_type='bike')   # bike paths
G = ox.graph_from_point(point, dist=dist, network_type='walk')   # pedestrian&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large &lt;code&gt;dist&lt;/code&gt;values (&amp;gt;20km) = slow downloads + memory heavy&lt;/item&gt;
      &lt;item&gt;Cache coordinates locally to avoid Nominatim rate limits&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;network_type='drive'&lt;/code&gt;instead of&lt;code&gt;'all'&lt;/code&gt;for faster renders&lt;/item&gt;
      &lt;item&gt;Reduce &lt;code&gt;dpi&lt;/code&gt;from 300 to 150 for quick previews&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46656834</guid><pubDate>Sat, 17 Jan 2026 10:13:57 +0000</pubDate></item><item><title>ASCII characters are not pixels: a deep dive into ASCII rendering</title><link>https://alexharri.com/blog/ascii-rendering</link><description>&lt;doc fingerprint="74d7db3c781d01ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ASCII characters are not pixels: a deep dive into ASCII rendering&lt;/head&gt;
    &lt;p&gt;Recently, I’ve been spending my time building an image-to-ASCII renderer. Below is the result — try dragging it around, the demo is interactive!&lt;/p&gt;
    &lt;p&gt;One thing I spent a lot of effort on is getting edges looking sharp. Take a look at this rotating cube example:&lt;/p&gt;
    &lt;p&gt;Try opening the “split” view. Notice how well the characters follow the contour of the square.&lt;/p&gt;
    &lt;p&gt;This renderer works well for animated scenes, like the ones above, but we can also use it to render static images:&lt;/p&gt;
    &lt;p&gt;The image of Saturn was generated with ChatGPT.&lt;/p&gt;
    &lt;p&gt;Then, to get better separation between different colored regions, I also implemented a cel shading-like effect to enhance contrast between edges. Try dragging the contrast slider below:&lt;/p&gt;
    &lt;p&gt;The contrast enhancement makes the separation between different colored regions far clearer. That was key to making the 3D scene above look as good as it does.&lt;/p&gt;
    &lt;p&gt;I put so much focus on sharp edges because they’re an aspect of ASCII rendering that is often overlooked when programmatically rendering images as ASCII. Consider this animated 3D scene from Cognition’s landing page that is rendered via ASCII characters:&lt;/p&gt;
    &lt;p&gt;Source: cognition.ai&lt;/p&gt;
    &lt;p&gt;It’s a cool effect, especially while in motion, but take a look at those blurry edges! The characters follow the cube contours very poorly, and as a result, the edges look blurry and jagged in places:&lt;/p&gt;
    &lt;p&gt;This blurriness happens because the ASCII characters are being treated like pixels — their shape is ignored. It’s disappointing to see because ASCII art looks so much better when shape is utilized. I don’t believe I’ve ever seen shape utilized in generated ASCII art, and I think that’s because it’s not really obvious how to consider shape when building an ASCII renderer.&lt;/p&gt;
    &lt;p&gt;I started building my ASCII renderer to prove to myself that it’s possible to utilize shape in ASCII rendering. In this post, I’ll cover the techniques and ideas I used to capture shape and build this ASCII renderer in detail.&lt;/p&gt;
    &lt;p&gt;We’ll start with the basics of image-to-ASCII conversion and see where the common issue of blurry edges comes from. After that, I’ll show you the approach I used to fix that and achieve sharp, high-quality ASCII rendering. At the end, we’ll improve on that by implementing the contrast enhancement effect I showed above.&lt;/p&gt;
    &lt;p&gt;Let’s get to it!&lt;/p&gt;
    &lt;head rend="h2"&gt;Image to ASCII conversion&lt;/head&gt;
    &lt;p&gt;ASCII contains 95 printable characters that we can use. Let’s start off by rendering the following image containing a white circle using those ASCII characters:&lt;/p&gt;
    &lt;p&gt;ASCII art is (almost) always rendered using a monospace font. Since every character in a monospace font is equally wide and tall, we can split the image into a grid. Each grid cell will contain a single ASCII character.&lt;/p&gt;
    &lt;p&gt;The image with the circle is &lt;/p&gt;
    &lt;p&gt;Monospace characters are typically taller than they are wide, so I made each grid cell a bit taller than it is wide.&lt;/p&gt;
    &lt;p&gt;Our task is now to pick which character to place in each cell. The simplest approach is to calculate a lightness value for each cell and pick a character based on that.&lt;/p&gt;
    &lt;p&gt;We can get a lightness value for each cell by sampling the lightness of the pixel at the cell’s center:&lt;/p&gt;
    &lt;p&gt;We want each pixel’s lightness as a numeric value between &lt;/p&gt;
    &lt;p&gt;We can use the following formula to convert an RGB color (with component values between &lt;/p&gt;
    &lt;p&gt;See relative luminance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mapping lightness values to ASCII characters&lt;/head&gt;
    &lt;p&gt;Now that we have a lightness value for each cell, we want to use those values to pick ASCII characters. As mentioned before, ASCII has 95 printable characters, but let’s start simple with just these characters:&lt;/p&gt;
    &lt;quote&gt;: - # = + @ * % .&lt;/quote&gt;
    &lt;p&gt;We can sort them in approximate density order like so, with lower-density characters to the left, and high-density characters to the right:&lt;/p&gt;
    &lt;quote&gt;. : - = + * # % @&lt;/quote&gt;
    &lt;p&gt;We’ll put these characters in a &lt;code&gt;CHARS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARS = [" ", ".", ":", "-", "=", "+", "*", "#", "%", "@"]&lt;/quote&gt;
    &lt;p&gt;I added space as the first (least dense) character.&lt;/p&gt;
    &lt;p&gt;We can then map lightness values between &lt;/p&gt;
    &lt;quote&gt;function getCharacterFromLightness(lightness: number) {const index = Math.floor(lightness * (CHARS.length - 1));return CHARS[index];}&lt;/quote&gt;
    &lt;p&gt;This maps low lightness values to low-density characters and high lightness values to high-density characters.&lt;/p&gt;
    &lt;p&gt;Rendering the circle from above with this method gives us:&lt;/p&gt;
    &lt;p&gt;That works... but the result is pretty ugly. We seem to always get &lt;code&gt;@&lt;/code&gt; for cells that fall within the circle and a space for cells that fall outside.&lt;/p&gt;
    &lt;p&gt;That is happening because we’ve pretty much just implemented nearest-neighbor downsampling. Let’s see what that means.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nearest neighbor downsampling&lt;/head&gt;
    &lt;p&gt;Downsampling, in the context of image processing, is taking a larger image (in our case, the &lt;/p&gt;
    &lt;p&gt;The simplest and fastest method of sampling is nearest-neighbor interpolation, where, for each cell (pixel), we only take a single sample from the higher resolution image.&lt;/p&gt;
    &lt;p&gt;Consider the circle example again. Using nearest-neighbor interpolation, every sample either falls inside or outside of the shape, resulting in either &lt;/p&gt;
    &lt;p&gt;If, instead of picking an ASCII character for each grid cell, we color each grid cell (pixel) according to the sampled value, we get the following pixelated rendering:&lt;/p&gt;
    &lt;p&gt;This pixelated rendering is pretty much equivalent to the ASCII rendering from before. The only difference is that instead of &lt;code&gt;@&lt;/code&gt;s we have white pixels, and instead of spaces we have black pixels.&lt;/p&gt;
    &lt;p&gt;These square, jagged looking edges are aliasing artifacts, commonly called jaggies. They’re a common result of using nearest-neighbor interpolation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Supersampling&lt;/head&gt;
    &lt;p&gt;To get rid of jaggies, we can collect more samples for each cell. Consider this line:&lt;/p&gt;
    &lt;p&gt;The line’s slope on the &lt;/p&gt;
    &lt;p&gt;Let’s try to get rid of the jagginess by taking multiple samples within each cell and using the average sampled lightness value as the cell’s lightness. The example below lets you vary the number of samples using the slider:&lt;/p&gt;
    &lt;p&gt;With multiple samples, cells that lie on the edge of a shape will have some of their samples fall within the shape, and some outside of it. Averaging those, we get gray in-between colors that smooth the downsampled image. Below is the same example, but with an overlay showing where the samples are taken:&lt;/p&gt;
    &lt;p&gt;This method of collecting multiple samples from the larger image is called supersampling. It’s a common method of spatial anti-aliasing (avoiding jaggies at edges). Here’s what the rotating square looks like with supersampling (using &lt;/p&gt;
    &lt;p&gt;Let’s look at what supersampling does for the circle example from earlier. Try dragging the sample quality slider:&lt;/p&gt;
    &lt;p&gt;The circle becomes less jagged, but the edges feel blurry. Why’s that?&lt;/p&gt;
    &lt;p&gt;Well, they feel blurry because we’re pretty much just rendering a low-resolution, pixelated image of a circle. Take a look at the pixelated view:&lt;/p&gt;
    &lt;p&gt;The ASCII and pixelated views are mirror images of each other. Both are just low-resolution versions of the original high-resolution image, scaled up to the original’s size — it’s no wonder they both look blurry.&lt;/p&gt;
    &lt;p&gt;Increasing the number of samples is insufficient. No matter how many samples we take per cell, the samples will be averaged into a single lightness value, used to render a single pixel.&lt;/p&gt;
    &lt;p&gt;And that’s the core problem: treating each grid cell as a pixel in an image. It’s an obvious and simple method, but it disregards that ASCII characters have shape.&lt;/p&gt;
    &lt;p&gt;We can make our ASCII renderings far more crisp by picking characters based on their shape. Here’s the circle rendered that way:&lt;/p&gt;
    &lt;p&gt;The characters follow the contour of the circle very well. By picking characters based on shape, we get a far higher effective resolution. The result is also more visually interesting.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can implement this.&lt;/p&gt;
    &lt;head rend="h2"&gt;Shape&lt;/head&gt;
    &lt;p&gt;So what do I mean by shape? Well, consider the characters &lt;code&gt;T&lt;/code&gt;, &lt;code&gt;L&lt;/code&gt;, and &lt;code&gt;O&lt;/code&gt; placed within grid cells:&lt;/p&gt;
    &lt;p&gt;The character &lt;code&gt;T&lt;/code&gt; is top-heavy. Its visual density in the upper half of the grid cell is higher than in the lower half. The opposite can be said for &lt;code&gt;L&lt;/code&gt; — it’s bottom-heavy. &lt;code&gt;O&lt;/code&gt; is pretty much equally dense in the upper and lower halves of the cell.&lt;/p&gt;
    &lt;p&gt;We might also compare characters like &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;J&lt;/code&gt;. The character &lt;code&gt;L&lt;/code&gt; is heavier within the left half of the cell, while &lt;code&gt;J&lt;/code&gt; is heavier in the right half:&lt;/p&gt;
    &lt;p&gt;We also have more “extreme” characters, such as &lt;code&gt;_&lt;/code&gt; and &lt;code&gt;^&lt;/code&gt;, that only occupy the lower or upper portion of the cell, respectively:&lt;/p&gt;
    &lt;p&gt;This is, roughly, what I mean by “shape” in the context of ASCII rendering. Shape refers to which regions of a cell a given character visually occupies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quantifying shape&lt;/head&gt;
    &lt;p&gt;To pick characters based on their shape, we’ll somehow need to quantify (put numbers to) the shape of each character.&lt;/p&gt;
    &lt;p&gt;Let’s start by only considering how much characters occupy the upper and lower regions of our cell. To do that, we’ll define two “sampling circles” for each grid cell — one placed in the upper half and one in the lower half:&lt;/p&gt;
    &lt;p&gt;It may seem odd or arbitrary to use circles instead of just splitting the cell into two rectangles, but using circles will give us more flexibility later on.&lt;/p&gt;
    &lt;p&gt;A character placed within a cell will overlap each of the cell’s sampling circles to some extent.&lt;/p&gt;
    &lt;p&gt;One can compute that overlap by taking a bunch of samples within the circle (for example, at every pixel). The fraction of samples that land inside the character gives us the overlap as a numeric value between &lt;/p&gt;
    &lt;p&gt;For T, we get an overlap of approximately &lt;/p&gt;
    &lt;p&gt;We can generate such a &lt;/p&gt;
    &lt;p&gt;Below are some ASCII characters and their shape vectors. I’m coloring the sampling circles using the component values of the shape vectors:&lt;/p&gt;
    &lt;p&gt;We can use the shape vectors as 2D coordinates — here’s every ASCII character on a 2D plot:&lt;/p&gt;
    &lt;head rend="h3"&gt;Shape-based lookup&lt;/head&gt;
    &lt;p&gt;Let’s say that we have our ASCII characters and their associated shape vectors in a &lt;code&gt;CHARACTERS&lt;/code&gt; array:&lt;/p&gt;
    &lt;quote&gt;const CHARACTERS: Array&amp;lt;{character: string,shapeVector: number[],}&amp;gt; = [...];&lt;/quote&gt;
    &lt;p&gt;We can then perform a nearest neighbor search like so:&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;findBestCharacter&lt;/code&gt; function gives us the ASCII character whose shape best matches the input lookup vector.&lt;/p&gt;
    &lt;p&gt;Note: this brute force search is not very performant. This becomes a bottleneck when we start rendering thousands of ASCII characters at &lt;/p&gt;
    &lt;p&gt;To make use of this in our ASCII renderer, we’ll calculate a lookup vector for each cell in the ASCII grid and pass it to &lt;code&gt;findBestCharacter&lt;/code&gt; to determine the character to display.&lt;/p&gt;
    &lt;p&gt;Let’s try it out. Consider the following zoomed-in circle as an example. It is split into three grid cells:&lt;/p&gt;
    &lt;p&gt;Overlaying our sampling circles, we see varying degrees of overlap:&lt;/p&gt;
    &lt;p&gt;When calculating the shape vector of each ASCII character, we took a huge number of samples. We could afford to do that because we only need to calculate those shape vectors once up front. After they’re calculated, we can use them again and again.&lt;/p&gt;
    &lt;p&gt;However, if we’re converting an animated image (e.g. canvas or video) to ASCII, we need to be mindful of performance when calculating the lookup vectors. An ASCII rendering might have hundreds or thousands of cells. Multiplying that by tens or hundreds of samples would be incredibly costly in terms of performance.&lt;/p&gt;
    &lt;p&gt;With that being said, let’s pick a sampling quality of &lt;/p&gt;
    &lt;p&gt;For the top sampling circle of the leftmost cell, we get one white sample and two black, giving us an average lightness of &lt;/p&gt;
    &lt;p&gt;From now on, instead of using the term “lookup vectors”, I’ll call these vectors, sampled from the image that we’re rendering as ASCII, sampling vectors. One sampling vector is calculated for each cell in the grid.&lt;/p&gt;
    &lt;p&gt;Anyway, we can use these sampling vectors to find the best-matching ASCII character. Let’s see what that looks like on our 2D plot — I’ll label the sampling vectors (from left to right) C0, C1, and C2:&lt;/p&gt;
    &lt;p&gt;Hmm... this is not what we want. Since none of the ASCII shape vector components exceed &lt;/p&gt;
    &lt;p&gt;We can fix this by normalizing the shape vectors. We’ll do that by taking the maximum value of each component across all shape vectors, and dividing the components of each shape vector by the maximum. Expressed in code, that looks like so:&lt;/p&gt;
    &lt;quote&gt;const max = [0, 0]for (const vector of characterVectors) {for (const [i, value] of Object.entries(vector)) {if (value &amp;gt; max[i]) {max[i] = value;}}}const normalizedCharacterVectors = characterVectors.map(vector =&amp;gt; vector.map((value, i) =&amp;gt; value / max[i]))&lt;/quote&gt;
    &lt;p&gt;Here’s what the plot looks like with the shape vectors normalized:&lt;/p&gt;
    &lt;p&gt;If we now map the sampling vectors to their nearest neighbors, we get a much more sensible result:&lt;/p&gt;
    &lt;p&gt;We get &lt;code&gt;'&lt;/code&gt;, &lt;code&gt;M&lt;/code&gt; and &lt;code&gt;$&lt;/code&gt;.  Let’s see how well those characters match the circle:&lt;/p&gt;
    &lt;p&gt;Nice! They match very well.&lt;/p&gt;
    &lt;p&gt;Let’s try rendering the full circle from before with the same method:&lt;/p&gt;
    &lt;p&gt;Much better than before! The picked characters follow the contour of the circle very well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limits of a 2D shape vector&lt;/head&gt;
    &lt;p&gt;Using two sampling circles — one upper and one lower — produces a much better result than the &lt;/p&gt;
    &lt;p&gt;For example, two circles don’t capture the shape of characters that fall in the middle of the cell. Consider &lt;code&gt;-&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;-&lt;/code&gt;, we get a shape vector of &lt;/p&gt;
    &lt;p&gt;The two upper-lower sampling circles also don’t capture left-right differences, such as the difference between &lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;We could use such differences to get better character picks, but our two sampling circles don’t capture them. Let’s add more dimensions to our shape to fix that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Increasing to 6 dimensions&lt;/head&gt;
    &lt;p&gt;Since cells are taller than they are wide (at least with the monospace font I’m using), we can use &lt;/p&gt;
    &lt;p&gt;&lt;code&gt;p&lt;/code&gt; and &lt;code&gt;q&lt;/code&gt;, while also capturing differences across the top, bottom, and middle regions of the cell, differentiating &lt;code&gt;^&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, and &lt;code&gt;_&lt;/code&gt;. They also capture the shape of “diagonal” characters like &lt;code&gt;/&lt;/code&gt; to a reasonable degree.&lt;/p&gt;
    &lt;p&gt;One problem with this grid-like configuration for the sampling circles is that there are gaps. For example, &lt;code&gt;.&lt;/code&gt; falls between the sampling circles:&lt;/p&gt;
    &lt;p&gt;To compensate for this, we can stagger the sampling circles vertically (e.g. lowering the left sampling circles and raising the right ones) and make them a bit larger. This causes the cell to be almost fully covered while not causing excessive overlap across the sampling circles:&lt;/p&gt;
    &lt;p&gt;We can use the same procedure as before to generate character vectors using these sampling circles, this time yielding a &lt;code&gt;L&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;For &lt;code&gt;L&lt;/code&gt;, we get the vector:&lt;/p&gt;
    &lt;p&gt;I’m presenting &lt;/p&gt;
    &lt;p&gt;The lightness values certainly look L-shaped! The 6D shape vector captures &lt;code&gt;L&lt;/code&gt;’s shape very well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nearest neighbor lookups in a 6D space&lt;/head&gt;
    &lt;p&gt;Now we have a 6D shape vector for every ASCII character. Does that affect character lookups (how we find the best matching character)?&lt;/p&gt;
    &lt;p&gt;Earlier, in the &lt;code&gt;findBestCharacter&lt;/code&gt; function, I referenced a &lt;code&gt;getDistance&lt;/code&gt; function. That function returns the Euclidean distance between the input points. Given two 2D points &lt;/p&gt;
    &lt;p&gt;This generalizes to higher dimensions:&lt;/p&gt;
    &lt;p&gt;Put into code, this looks like so:&lt;/p&gt;
    &lt;quote&gt;function getDistance(a: number[], b: number[]): number {let sum = 0;for (let i = 0; i &amp;lt; a.length; i++) {sum += (a[i] - b[i]) ** 2;}return Math.sqrt(sum);}&lt;/quote&gt;
    &lt;p&gt;Note: since we’re just using this for the purposes of finding the closest point, we can skip the expensive &lt;code&gt;Math.sqrt()&lt;/code&gt; call and just return the squared distance. It does not affect the result.&lt;/p&gt;
    &lt;p&gt;So, no, the dimensionality of our shape vector does not change lookups at all. We can use the same &lt;code&gt;getDistance&lt;/code&gt; function for both 2D and 6D.&lt;/p&gt;
    &lt;p&gt;With that out of the way, let’s see what the 6D approach yields!&lt;/p&gt;
    &lt;head rend="h3"&gt;Trying out the 6D approach&lt;/head&gt;
    &lt;p&gt;Our new 6D approach works really well for flat shapes, like the circle example we’ve been using:&lt;/p&gt;
    &lt;p&gt;Now let’s see how this approach works when we render a 3D scene with more shades of gray:&lt;/p&gt;
    &lt;p&gt;Firstly, the outer contours look nice and sharp. I also like how well the gradients across the sphere and cone look.&lt;/p&gt;
    &lt;p&gt;However, internally, the objects all kind of blend together. The edges between surfaces with different lightnesses aren’t sharp enough. For example, the lighter faces of the cubes all kind of blend into one solid color. When there is a change in color — like when two faces of a cube meet — I’d like to see more sharpness in the ASCII rendering.&lt;/p&gt;
    &lt;p&gt;To demonstrate what I mean, consider the following split:&lt;/p&gt;
    &lt;p&gt;It’s currently rendered like so:&lt;/p&gt;
    &lt;p&gt;The different shades result in &lt;code&gt;i&lt;/code&gt;s on the left and &lt;code&gt;B&lt;/code&gt;s on the right, but the boundary is not very sharp.&lt;/p&gt;
    &lt;p&gt;By applying some effects to the sampling vector, we can enhance the contrast at the boundary so that it appears sharper:&lt;/p&gt;
    &lt;p&gt;The added contrast makes a big difference in readability for the 3D scene. Let’s look at how we can implement this contrast enhancement effect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contrast enhancement&lt;/head&gt;
    &lt;p&gt;Consider cells overlapping a color boundary like so:&lt;/p&gt;
    &lt;p&gt;For the cells on the boundary, we get a 6D sampling vector that looks like so:&lt;/p&gt;
    &lt;p&gt;To make future examples easier to visualize, I’ll start drawing the sampling vector using &lt;/p&gt;
    &lt;p&gt;Currently, this sampling vector resolves to the character &lt;code&gt;T&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;That’s a sensible choice. The character &lt;code&gt;T&lt;/code&gt; is visually dense in the top half and less so in the bottom half, so it matches the image fairly well.&lt;/p&gt;
    &lt;p&gt;Still, I want the picked character to emphasize the shape of the boundary better. We can achieve that by enhancing the contrast of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To increase the contrast of our sampling vector, we might raise each component of the vector to the power of some exponent.&lt;/p&gt;
    &lt;p&gt;Consider how an exponent affects values between &lt;/p&gt;
    &lt;p&gt;The level of pull depends on the exponent. Here’s a chart of &lt;/p&gt;
    &lt;p&gt;This effect becomes more pronounced with higher exponents:&lt;/p&gt;
    &lt;p&gt;A higher exponent translates to a stronger pull towards zero.&lt;/p&gt;
    &lt;p&gt;Applying an exponent should make dark values darker more quickly than light ones. The example below allows you to vary the exponent applied to the sampling vector:&lt;/p&gt;
    &lt;p&gt;As the exponent is increased to &lt;/p&gt;
    &lt;p&gt;I don’t want that. I want to increase the contrast between the lighter and darker components of the sampling vector, not the vector in its entirety.&lt;/p&gt;
    &lt;p&gt;To achieve that, we can normalize the sampling vector to the range &lt;/p&gt;
    &lt;p&gt;The normalization to &lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = value / maxValue; // Normalizevalue = Math.pow(value, exponent);value = value * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;Here’s the same example, but with this normalization applied:&lt;/p&gt;
    &lt;p&gt;Very nice! The lightest component values are retained, and the contrast between the lighter and darker components is increased by “crunching” the lower values.&lt;/p&gt;
    &lt;p&gt;This affects which character is picked. The following example shows how the selected character changes as the contrast is increased:&lt;/p&gt;
    &lt;p&gt;Awesome! The pick of &lt;code&gt;"&lt;/code&gt; over &lt;code&gt;T&lt;/code&gt; emphasizes the separation between the lighter region above and the darker region below!&lt;/p&gt;
    &lt;p&gt;By enhancing the contrast of the sampling vector, we exaggerate its shape. This gives us a character that less faithfully represents the underlying image, but improves readability as a whole by enhancing the separation between different colored regions.&lt;/p&gt;
    &lt;p&gt;Let’s look at another example. Observe how the L-shape of the sampling vector below becomes more pronounced as the exponent increases, and how that affects the picked character:&lt;/p&gt;
    &lt;p&gt;Works really nicely! I love the transition from &lt;code&gt;&amp;amp; -&amp;gt; b -&amp;gt; L&lt;/code&gt; as the L-shape of the vector becomes clearer.&lt;/p&gt;
    &lt;p&gt;What’s nice about applying exponents to normalized sampling vectors is that it barely affects vectors that are uniform in value. If all component values are similar, applying an exponent has a minimal effect:&lt;/p&gt;
    &lt;p&gt;Because the vector is fairly uniform, the exponent only has a slight effect and doesn’t change the picked character.&lt;/p&gt;
    &lt;p&gt;This is a good thing! If we have a smooth gradient in our image, we want to retain it. We very much do not want to introduce unnecessary choppiness.&lt;/p&gt;
    &lt;p&gt;Compare the 3D scene ASCII rendering with and without this contrast enhancement:&lt;/p&gt;
    &lt;p&gt;We do see more contrast at boundaries, but this is not quite there yet. Some edges are still not sharp enough, and we also observe a “staircasing” effect happening at some boundaries.&lt;/p&gt;
    &lt;p&gt;Let’s look at the staircasing effect first. We can reproduce it with a boundary like so:&lt;/p&gt;
    &lt;p&gt;Below is the ASCII rendering of that boundary. Notice how the lower edge (the &lt;code&gt;!&lt;/code&gt;s) becomes “staircase-y” as you increase the exponent:&lt;/p&gt;
    &lt;p&gt;We see a staircase pattern like so:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;To understand why that’s happening, let’s consider the row in the middle of the canvas, progressing from left to right. As we start off, every sample is equally light, giving us &lt;code&gt;U&lt;/code&gt;s:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUU -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we reach the boundary, the lower right samples become a bit darker. Those darker components are crunched by contrast enhancement, giving us some &lt;code&gt;Y&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;So we get:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYY -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;As we progress further right, the middle and lower samples get darker, so we get some &lt;code&gt;f&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;This trend continues towards &lt;code&gt;"&lt;/code&gt;, &lt;code&gt;'&lt;/code&gt;, and finally, &lt;code&gt;`&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Giving us a sequence like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''` -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;That looks good, but at some point we get no light samples. Once we get no light samples, our contrast enhancement has no effect because every component is equally light. This causes us to always get &lt;code&gt;!&lt;/code&gt;s:&lt;/p&gt;
    &lt;p&gt;Making our sequence look like so:&lt;/p&gt;
    &lt;quote&gt;UUUUUUUUYYf""''`!!!!!!!!!! -&amp;gt;&lt;/quote&gt;
    &lt;p&gt;This sudden stop in contrast enhancement having an effect is what causes the staircasing effect:&lt;/p&gt;
    &lt;quote&gt;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;Let’s see how we can counteract this staircasing effect with another layer of contrast enhancement, this time looking outside of the boundary of each cell.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;We currently have sampling circles arranged like so:&lt;/p&gt;
    &lt;p&gt;For each of those sampling circles, we’ll specify an “external sampling circle”, placed outside of the cell’s boundary, like so:&lt;/p&gt;
    &lt;p&gt;Each of those external sampling circles is “reaching” into the region of a neighboring cell. Together, the samples that are collected by the external sampling circles constitute an “external sampling vector”.&lt;/p&gt;
    &lt;p&gt;Let’s simplify the visualization and consider a single example. Imagine that we collected a sampling vector and an external sampling vector that look like so:&lt;/p&gt;
    &lt;p&gt;The circles colored red are the external sampling vector components. Currently, they have no effect.&lt;/p&gt;
    &lt;p&gt;The “internal” sampling vector itself is fairly uniform, with values ranging from &lt;/p&gt;
    &lt;p&gt;To enhance this apparent boundary, we’ll darken the top-left and middle-left components of the sampling vector. We can do that by applying component-wise contrast enhancement using the values from the external vector.&lt;/p&gt;
    &lt;p&gt;In the previous contrast enhancement, we calculated the maximum component value across the sampling vector and normalized the vector using that value:&lt;/p&gt;
    &lt;quote&gt;const maxValue = Math.max(...samplingVector)samplingVector = samplingVector.map((value) =&amp;gt; {value = value / maxValue; // Normalizevalue = Math.pow(value, exponent);value = value * maxValue; // Denormalizereturn value;})&lt;/quote&gt;
    &lt;p&gt;But the new component-wise contrast enhancement will take the maximum value between each component of the sampling vector and the corresponding component in the external sampling vector:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i])// ...});&lt;/quote&gt;
    &lt;p&gt;Aside from that, the contrast enhancement is performed in the same way:&lt;/p&gt;
    &lt;quote&gt;samplingVector = samplingVector.map((value, i) =&amp;gt; {const maxValue = Math.max(value, externalSamplingVector[i]);value = value / maxValue;value = Math.pow(value, exponent);value = value * maxValue;return value;});&lt;/quote&gt;
    &lt;p&gt;The example below shows how light values in the external sampling vector push values in the sampling vector down:&lt;/p&gt;
    &lt;p&gt;I call this “directional contrast enhancement”, since each of the external sampling circles reaches outside of the cell in the direction of the sampling vector component that it is enhancing the contrast of. I describe the other effect as “global contrast enhancement” since it acts on all of the sampling vector’s components together.&lt;/p&gt;
    &lt;p&gt;Let’s see what this directional contrast enhancement does to get rid of the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Hmm, that’s not doing what I wanted. I wanted to see a sequence like so:&lt;/p&gt;
    &lt;quote&gt;..::!!..::!!!!!!!!..::!!!!!!!!!!!!!!&lt;/quote&gt;
    &lt;p&gt;But we just see &lt;code&gt;!&lt;/code&gt; changing to &lt;code&gt;:&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;This happens because the directional contrast enhancement doesn’t reach far enough into our sampling vector. The light upper values in the external vector do push the upper values of the sampling vector down, but because the lightness of the four bottom components is retained, we don’t get to &lt;code&gt;.&lt;/code&gt;, just &lt;code&gt;:&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Widening the directional contrast enhancement&lt;/head&gt;
    &lt;p&gt;I’d like to “widen” the directional contrast enhancement so that, for example, light external values at the top spread to the middle components of the sampling vector.&lt;/p&gt;
    &lt;p&gt;To do that, I’ll introduce a few more external sampling circles, arranged like so:&lt;/p&gt;
    &lt;p&gt;These are a total of &lt;/p&gt;
    &lt;p&gt;For each component of the internal sampling vector, we’ll calculate the maximum value across the external sampling vector components that affect it, and use that maximum to perform the contrast enhancement.&lt;/p&gt;
    &lt;p&gt;Let’s implement that. I’ll order the internal and external sampling circles like so:&lt;/p&gt;
    &lt;p&gt;We can then define a mapping from the internal circles to the external sampling circles that affect them:&lt;/p&gt;
    &lt;quote&gt;const AFFECTING_EXTERNAL_INDICES = [[0, 1, 2, 4],[0, 1, 3, 5],[2, 4, 6],[3, 5, 7],[4, 6, 8, 9],[5, 7, 8, 9],];&lt;/quote&gt;
    &lt;p&gt;With this, we can change the calculation of &lt;code&gt;maxValue&lt;/code&gt; to take the maximum affecting external value:&lt;/p&gt;
    &lt;quote&gt;// Beforeconst maxValue = Math.max(value, externalSamplingVector[i]);// Afterlet maxValue = value;for (const externalIndex of AFFECTING_EXTERNAL_INDICES[i]) {maxValue = Math.max(value, externalSamplingVector[externalIndex]);}&lt;/quote&gt;
    &lt;p&gt;Now look what happens if the top four external sampling circles are light: it causes the contrast enhancement to reach into the middle of the sampling vector, giving us the desired effect:&lt;/p&gt;
    &lt;p&gt;We now smoothly transition from &lt;code&gt;! -&amp;gt; : -&amp;gt; .&lt;/code&gt; — beautiful stuff!&lt;/p&gt;
    &lt;p&gt;Let’s see if this change resolves the staircasing effect:&lt;/p&gt;
    &lt;p&gt;Oh yeah, looks awesome! We get the desired effect. The boundary is nice and sharp while not being too jagged.&lt;/p&gt;
    &lt;p&gt;Here’s the 3D scene again. The contrast slider now applies both types of contrast enhancement at the same time — try it out:&lt;/p&gt;
    &lt;p&gt;This really enhances the contrast at boundaries, making the image far more readable!&lt;/p&gt;
    &lt;p&gt;Together, the 6D shape vector approach and contrast enhancement techniques have given us a really nice final ASCII rendering.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final words&lt;/head&gt;
    &lt;p&gt;This post was really fun to build and write! I hope you enjoyed reading it.&lt;/p&gt;
    &lt;p&gt;ASCII rendering is perhaps not the most useful topic to write about, but I think the idea of using a high-dimensional vector to capture shape is interesting and could easily be applied to many other problems. There are parallels to be drawn to word embeddings.&lt;/p&gt;
    &lt;p&gt;I started writing this ASCII renderer to see if the idea of using a vector to capture the shape of characters would work at all. That approach turned out to work very well, but the initial prototype was terribly slow — I only got single-digit FPS on my iPhone. To get the ASCII renderer running at a smooth &lt;/p&gt;
    &lt;p&gt;My colleagues, after reading a draft of this post, suggested many alternatives to the approaches I described in this post. For example, why not make the sampling vector &lt;code&gt;T&lt;/code&gt; far better — just look how &lt;code&gt;T&lt;/code&gt;’s stem falls between the two sampling circles in each row:&lt;/p&gt;
    &lt;p&gt;And yeah, he’s right! A &lt;/p&gt;
    &lt;p&gt;It’s really fun how large the solution space to the problem of ASCII rendering is. There are so, so many approaches and trade-offs to explore. I imagine you probably thought of a few yourself while reading this post!&lt;/p&gt;
    &lt;p&gt;One dimension I intentionally did not explore was using different colors or lightnesses for the ASCII characters themselves. This is for many reasons, but the two primary ones are that 1) it would have expanded the scope of this post too much, and 2) it’s just a different effect, and I personally don’t like the look.&lt;/p&gt;
    &lt;p&gt;At the time of writing these final words, around &lt;/p&gt;
    &lt;p&gt;Thanks for reading! And huge thanks to Gunnlaugur Þór Briem and Eiríkur Fannar Torfason for reading and providing feedback on a draft of this post.&lt;/p&gt;
    &lt;p&gt;— Alex Harri&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix I: Character lookup performance&lt;/head&gt;
    &lt;p&gt;Earlier in this post, I showed how can find the best character by finding the character with the shortest Euclidean distance to our sampling vector.&lt;/p&gt;
    &lt;quote&gt;function findBestCharacter(inputVector: number[]) {let bestCharacter = "";let bestDistance = Infinity;for (const { character, shapeVector } of CHARACTERS) {const dist = getDistance(shapeVector, inputVector);if (dist &amp;lt; bestDistance) {bestDistance = dist;bestCharacter = character;}}return bestCharacter;}&lt;/quote&gt;
    &lt;p&gt;I tried benchmarking this for &lt;/p&gt;
    &lt;p&gt;If we allow ourselves &lt;/p&gt;
    &lt;head rend="h3"&gt;k-d trees&lt;/head&gt;
    &lt;p&gt;Internally, &lt;/p&gt;
    &lt;p&gt;I won’t go into much detail on &lt;/p&gt;
    &lt;p&gt;One could also look at the hierarchical navigable small worlds (HNSW) algorithm, which Eiríkur pointed me to. It is used for approximate nearest neighbor lookups in vector databases, so definitely relevant.&lt;/p&gt;
    &lt;p&gt;Let’s see how it performs! We’ll construct a &lt;/p&gt;
    &lt;quote&gt;const kdTree = new KdTree(CHARACTERS.map(({ character, shapeVector }) =&amp;gt; ({point: shapeVector,data: character,})));&lt;/quote&gt;
    &lt;p&gt;We can now perform nearest-neighbor lookups on the &lt;/p&gt;
    &lt;quote&gt;const result = kdTree.findNearest(samplingVector);&lt;/quote&gt;
    &lt;p&gt;Running &lt;/p&gt;
    &lt;p&gt;That’s a lot of lookups per frame, but again, we’re benchmarking on a powerful machine. This is still not good enough.&lt;/p&gt;
    &lt;p&gt;Let’s see how we can eke out even more performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caching&lt;/head&gt;
    &lt;p&gt;An obvious avenue for speeding up lookups is to cache the result:&lt;/p&gt;
    &lt;quote&gt;function searchCached(samplingVector: number[]) {const key = generateCacheKey(samplingVector)if (cache.has(key)) {return cache.get(key)!;}const result = search(samplingVector);cache.set(key, result);return result;}&lt;/quote&gt;
    &lt;p&gt;But how does one generate a cache key for a &lt;/p&gt;
    &lt;p&gt;Well, one way is to quantize each vector component so that it fits into a set number of bits and packing those bits into a single number. JavaScript numbers give us &lt;/p&gt;
    &lt;p&gt;We can quantize a numeric value between &lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function quantizeTo5Bits(value: number) {return Math.min(RANGE - 1, Math.floor(value * RANGE));}&lt;/quote&gt;
    &lt;p&gt;Applying a max of &lt;code&gt;RANGE - 1&lt;/code&gt; is done so that a &lt;code&gt;value&lt;/code&gt; of exactly &lt;/p&gt;
    &lt;p&gt;We can quantize each of the sampling vector components in this manner and use bit shifting to pack all of the quantized values into a single number like so:&lt;/p&gt;
    &lt;quote&gt;const BITS = 5;const RANGE = 2 ** BITS;function generateCacheKey(vector: number[]): number {let key = 0;for (let i = 0; i &amp;lt; vector.length; i++) {const quantized = Math.min(RANGE - 1, Math.floor(vector[i] * RANGE));key = (key &amp;lt;&amp;lt; BITS) | quantized;}return key;}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;RANGE&lt;/code&gt; is current set to &lt;code&gt;2 ** 5&lt;/code&gt;, but consider how large that makes our key space. Each vector component is one of &lt;/p&gt;
    &lt;p&gt;Alright, &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory needed to store keys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;46,656&lt;/cell&gt;
        &lt;cell&gt;364 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;117,649&lt;/cell&gt;
        &lt;cell&gt;919 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;262,144&lt;/cell&gt;
        &lt;cell&gt;2.00 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;531,441&lt;/cell&gt;
        &lt;cell&gt;4.05 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;1,000,000&lt;/cell&gt;
        &lt;cell&gt;7.63 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;1,771,561&lt;/cell&gt;
        &lt;cell&gt;13.52 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2,985,984&lt;/cell&gt;
        &lt;cell&gt;22.78 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are trade-offs to consider here. As the range gets smaller, the quality of the results drops. If we pick a range of &lt;/p&gt;
    &lt;p&gt;At the same time, if we increase the possible number of keys, we need more memory to store them. Additionally, the cache hit rate might be very low, especially when the cache is relatively empty.&lt;/p&gt;
    &lt;p&gt;I ended up picking a range of &lt;/p&gt;
    &lt;p&gt;Cached lookups are incredibly fast — fast enough that lookup performance just isn’t a concern anymore (&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix II: GPU acceleration&lt;/head&gt;
    &lt;p&gt;Lookups were not the only performance concern. Just collecting the sampling vectors (internal and external) turned out to be terribly expensive.&lt;/p&gt;
    &lt;p&gt;Just consider the sheer amount of samples that need to be collected. The 3D scene I’ve been using as an example uses a &lt;/p&gt;
    &lt;p&gt;And that’s if we use a sampling quality of &lt;/p&gt;
    &lt;p&gt;Collecting these samples absolutely crushed performance on my iPhone, so I needed to either collect fewer samples or speed up the collection of samples. Collecting fewer samples would have meant rendering fewer ASCII characters or removing the directional contrast enhancement, neither of which was an appealing solution.&lt;/p&gt;
    &lt;p&gt;My initial implementation ran on the CPU, which could only collect one sample at a time. To speed this up, I moved the work of sampling collection and applying the contrast enhancement to the GPU. The pipeline for that looks like so (each of the steps listed is a single shader pass):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Collect the raw internal sampling vectors into a &lt;mjx-container/&gt;texture, using the canvas (image) as the input texture.&lt;/item&gt;
      &lt;item&gt;Do the same for the external sampling vectors.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum external value affecting each internal vector component into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply directional contrast enhancement to each sampling vector component, using the maximum external values texture.&lt;/item&gt;
      &lt;item&gt;Calculate the maximum value for each internal sampling vector into a &lt;mjx-container/&gt;texture.&lt;/item&gt;
      &lt;item&gt;Apply global contrast enhancement to each sampling vector component, using the maximum internal values texture.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m glossing over the details because I could spend a whole other post covering them, but moving work to the GPU made the renderer many times more performant than it was when everything ran on the CPU.&lt;/p&gt;
    &lt;p&gt;To be notified of new posts, subscribe to my mailing list.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46657122</guid><pubDate>Sat, 17 Jan 2026 11:15:26 +0000</pubDate></item><item><title>The recurring dream of replacing developers</title><link>https://www.caimito.net/en/blog/2025/12/07/the-recurring-dream-of-replacing-developers.html</link><description>&lt;doc fingerprint="d5de5d392af7abd9"&gt;
  &lt;main&gt;
    &lt;p&gt;07.12.2025, By Stephan Schwab&lt;/p&gt;
    &lt;p&gt;Every decade brings new promises: this time, we'll finally make software development simple enough that we won't need so many developers. From COBOL to AI, the pattern repeats. Business leaders grow frustrated with slow delivery and high costs. Developers feel misunderstood and undervalued. Understanding why this cycle persists for fifty years reveals what both sides need to know about the nature of software work.&lt;/p&gt;
    &lt;p&gt;When Neil Armstrong stepped onto the lunar surface in 1969, the world witnessed what organized human ingenuity could accomplish. Behind that achievement stood Margaret Hamilton and her team, writing Apollo’s guidance software by hand, catching critical errors through careful review, and proving that software could be mission-critical.&lt;/p&gt;
    &lt;p&gt;The Apollo program demonstrated that software development was essential to achieving the impossible. Yet it also revealed something that would frustrate business leaders for decades to come: writing software required specialized knowledge, intense focus, and significant time investment. The dream of making it easier—of needing fewer of these expensive specialists—began almost immediately.&lt;/p&gt;
    &lt;p&gt;The late 1960s and 1970s saw COBOL emerge with an explicit goal stated in its name: Common Business-Oriented Language. The vision was clear: make the language read like English sentences, and business analysts would write their own programs. No need for specialized programmers.&lt;/p&gt;
    &lt;p&gt;This vision had genuine appeal. Software was becoming essential to business operations, yet programmers remained a scarce, expensive resource. COBOL promised to democratize software creation.&lt;/p&gt;
    &lt;p&gt;What happened instead? COBOL became another programming language requiring specialized training. Business analysts who tried to write COBOL quickly discovered that readable syntax didn’t eliminate the complexity of logic, data structures, or system design. A new class of COBOL programmers emerged, and the dream of eliminating specialized developers remained unfulfilled.&lt;/p&gt;
    &lt;p&gt;Yet the dream didn’t die. It simply waited for the next technological wave.&lt;/p&gt;
    &lt;p&gt;Computer-Aided Software Engineering tools arrived in the 1980s with tremendous promise. Draw flowcharts and entity-relationship diagrams, and the tool would generate working code. The marketing message resonated: visual design was more intuitive than typing cryptic commands. Business experts could model their processes, and software would materialize.&lt;/p&gt;
    &lt;p&gt;Organizations invested heavily. Vendors promised productivity increases of 10x or more. Yet most CASE tool initiatives struggled or failed outright.&lt;/p&gt;
    &lt;p&gt;The generated code often required substantial manual intervention. Performance problems emerged. Maintenance became a nightmare when generated code diverged from the visual models. Most critically, drawing accurate diagrams required understanding the same logical complexity that programming demanded. The tool changed the interface but not the fundamental challenge.&lt;/p&gt;
    &lt;p&gt;Once again, the problem proved more stubborn than the solution.&lt;/p&gt;
    &lt;p&gt;The 1990s brought a different approach. Microsoft’s Visual Basic and Borland’s Delphi made building user interfaces dramatically easier. Drag components onto a form, set properties, write event handlers. Suddenly, creating a Windows application felt achievable for developers with modest experience.&lt;/p&gt;
    &lt;p&gt;This wave succeeded differently than COBOL or CASE tools. These environments acknowledged that programming knowledge was still necessary, but they reduced the barrier to entry. A broader range of people could create useful applications.&lt;/p&gt;
    &lt;p&gt;Yet the dream of eliminating developers persisted. “Power users” and “citizen developers” would build departmental applications. IT departments could focus on infrastructure while business units solved their own software needs.&lt;/p&gt;
    &lt;p&gt;Reality proved more nuanced. Simple applications were indeed accessible to more people. But as requirements grew in complexity—integration with existing systems, security considerations, performance under load, long-term maintenance—the need for experienced developers became evident. The tools expanded who could write software, but they didn’t eliminate the expertise required for substantial systems.&lt;/p&gt;
    &lt;p&gt;And so the cycle continued into the new millennium.&lt;/p&gt;
    &lt;p&gt;Each subsequent decade introduced new variations. Ruby on Rails promised convention over configuration. Low-code platforms offered visual development with minimal coding. No-code platforms claimed to eliminate programming entirely for common business applications.&lt;/p&gt;
    &lt;p&gt;Each wave delivered real value. Development genuinely became faster in specific contexts. More people could participate in creating software solutions. Yet professional software developers remained essential, and demand for their skills continued growing rather than shrinking.&lt;/p&gt;
    &lt;p&gt;Which brings us to the question: why does this pattern repeat?&lt;/p&gt;
    &lt;p&gt;The recurring pattern reveals something important about how we think about complexity. Software development looks like it should be simple because we can describe what we want in plain language. “When a customer places an order, check inventory, calculate shipping, process payment, and send a confirmation email.” That description sounds straightforward.&lt;/p&gt;
    &lt;p&gt;The complexity emerges in the details. What happens when inventory is temporarily reserved by another order? How do you handle partial payments? What if the email service is temporarily unavailable? Should you retry? How many times? What if the customer’s session expires during checkout? How do you prevent duplicate orders?&lt;/p&gt;
    &lt;p&gt;Each answer leads to more questions. The accumulated decisions, edge cases, and interactions create genuine complexity that no tool or language can eliminate. Someone must think through these scenarios. That thinking is software development, regardless of whether it’s expressed in COBOL, a CASE tool diagram, Visual Basic, or an AI prompt.&lt;/p&gt;
    &lt;p&gt;Which brings us to today’s excitement.&lt;/p&gt;
    &lt;p&gt;Today’s AI coding assistants represent the most capable attempt yet to assist with software creation. They can generate substantial amounts of working code from natural language descriptions. They can explain existing code, suggest improvements, and help debug problems.&lt;/p&gt;
    &lt;p&gt;This represents genuine progress. The assistance is real and valuable. Experienced developers use these tools to work more efficiently. People learning to code find the interactive guidance helpful.&lt;/p&gt;
    &lt;p&gt;Yet we’re already seeing the familiar pattern emerge. Initial excitement about AI replacing developers is giving way to a more nuanced understanding: AI changes how developers work rather than eliminating the need for their judgment. The complexity remains. Someone must understand the business problem, evaluate whether the generated code solves it correctly, consider security implications, ensure it integrates properly with existing systems, and maintain it as requirements evolve.&lt;/p&gt;
    &lt;p&gt;AI amplifies developer capability. It doesn’t replace the need for people who understand both the problem domain and the technical landscape.&lt;/p&gt;
    &lt;p&gt;Here’s the paradox that makes this pattern particularly poignant. We’ve made extraordinary progress in software capabilities. The Apollo guidance computer had 4KB of RAM. Your smartphone has millions of times more computing power. We’ve built tools and frameworks that genuinely make many aspects of development easier.&lt;/p&gt;
    &lt;p&gt;Yet demand for software far exceeds our ability to create it. Every organization needs more software than it can build. The backlog of desired features and new initiatives grows faster than development teams can address it.&lt;/p&gt;
    &lt;p&gt;This tension—powerful tools yet insufficient capacity—keeps the dream alive. Business leaders look at the backlog and think, “There must be a way to go faster, to enable more people to contribute.” That’s a reasonable thought. It leads naturally to enthusiasm for any tool or approach that promises to democratize software creation.&lt;/p&gt;
    &lt;p&gt;The challenge is that software development isn’t primarily constrained by typing speed or syntax knowledge. It’s constrained by the thinking required to handle complexity well. Faster typing doesn’t help when you’re thinking through how to handle concurrent database updates. Simpler syntax doesn’t help when you’re reasoning about security implications.&lt;/p&gt;
    &lt;p&gt;So what should leaders do with this understanding?&lt;/p&gt;
    &lt;p&gt;Understanding this pattern changes how you evaluate new tools and approaches. When someone promises that their platform will let business users build applications without developers, you can appreciate the aspiration while maintaining realistic expectations.&lt;/p&gt;
    &lt;p&gt;The right question isn’t “Will this eliminate our need for developers?” The right questions are:&lt;/p&gt;
    &lt;p&gt;These questions acknowledge that development involves irreducible complexity while remaining open to tools that provide genuine leverage.&lt;/p&gt;
    &lt;p&gt;And they point to something deeper about the nature of software work.&lt;/p&gt;
    &lt;p&gt;This fifty-year pattern teaches us something fundamental about software development itself. If the problem were primarily mechanical—too much typing, too complex syntax, too many steps—we would have solved it by now. COBOL made syntax readable. CASE tools eliminated typing. Visual tools eliminated syntax. AI can now generate entire functions from descriptions.&lt;/p&gt;
    &lt;p&gt;Each advancement addressed a real friction point. Yet the fundamental challenge persists because it’s not mechanical. It’s intellectual. Software development is thinking made tangible. The artifacts we create—whether COBOL programs, Delphi forms, or Python scripts—are the visible outcome of invisible reasoning about complexity.&lt;/p&gt;
    &lt;p&gt;You can’t shortcut that reasoning any more than you can shortcut the reasoning required to design a building or diagnose a medical condition. Better tools help. Experience helps. But someone must still think it through.&lt;/p&gt;
    &lt;p&gt;So how should we move forward, knowing all this?&lt;/p&gt;
    &lt;p&gt;The next wave of development tools will arrive. Some will provide genuine value. Some will repeat familiar promises with new technology. Having perspective on this recurring pattern helps you engage with new tools productively.&lt;/p&gt;
    &lt;p&gt;Use AI assistants. Evaluate low-code platforms. Experiment with new frameworks. But invest primarily in your people’s ability to think clearly about complexity. That capability remains the constraining factor, just as it was during the Apollo program.&lt;/p&gt;
    &lt;p&gt;The moon landing happened because brilliant people thought carefully about every detail of an extraordinarily complex challenge. They wrote software by hand because that was the available tool. If they’d had better tools, they would have used them gladly. But the tools wouldn’t have eliminated their need to think through the complexity.&lt;/p&gt;
    &lt;p&gt;We’re still in that same fundamental situation. We have better tools—vastly better tools—but the thinking remains essential.&lt;/p&gt;
    &lt;p&gt;Perhaps the recurring dream of replacing developers isn’t a mistake. Perhaps it’s a necessary optimism that drives tool creation. Each attempt to make development more accessible produces tools that genuinely help. The dream doesn’t come true as imagined, but pursuing it creates value.&lt;/p&gt;
    &lt;p&gt;COBOL didn’t let business analysts write programs, but it did enable a generation of developers to build business systems effectively. CASE tools didn’t generate complete applications, but they advanced our thinking about visual modeling. Visual Basic didn’t eliminate professional developers, but it brought application development to more people. AI won’t replace developers, but it will change how we work in meaningful ways.&lt;/p&gt;
    &lt;p&gt;The pattern continues because the dream reflects a legitimate need. We genuinely require faster, more efficient ways to create software. We just keep discovering that the constraint isn’t the tool—it’s the complexity of the problems we’re trying to solve.&lt;/p&gt;
    &lt;p&gt;Understanding this doesn’t mean rejecting new tools. It means using them with clear expectations about what they can provide and what will always require human judgment.&lt;/p&gt;
    &lt;p&gt;Let's talk about your real situation. Want to accelerate delivery, remove technical blockers, or validate whether an idea deserves more investment? Book a short conversation (20 min): I listen to your context and give 1–2 practical recommendations—no pitch, no obligation. If it fits, we continue; if not, you leave with clarity. Confidential and direct.&lt;/p&gt;
    &lt;p&gt;Prefer email? Write me: sns@caimito.net&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46658345</guid><pubDate>Sat, 17 Jan 2026 14:31:33 +0000</pubDate></item><item><title>There's no single best way to store information</title><link>https://www.quantamagazine.org/why-theres-no-single-best-way-to-store-information-20260116/</link><description>&lt;doc fingerprint="7721186709e90e89"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why There’s No Single Best Way To Store Information&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Just as there’s no single best way to organize your bookshelf, there’s no one-size-fits-all solution to storing information.&lt;/p&gt;
    &lt;p&gt;Consider the simple situation where you create a new digital file. Your computer needs to rapidly find a place to put it. If you later want to delete it, the machine must quickly find the right bits to erase. Researchers aim to design storage systems, called data structures, that balance the amount of time it takes to add data, the time it takes to later remove it, and the total amount of memory the system needs.&lt;/p&gt;
    &lt;p&gt;To get a feel for these challenges, imagine you keep all your books in a row on one long shelf. If they’re organized alphabetically, you can quickly pick out any book. But whenever you acquire a new book, it’ll take time to find its proper spot. Conversely, if you place books wherever there’s space, you’ll save time now, but they’ll be hard to find later. This trade-off between insertion time and retrieval time might not be a problem for a single-shelf library, but you can see how it could get cumbersome with thousands of books.&lt;/p&gt;
    &lt;p&gt;Instead of a shelf, you could set up 26 alphabetically labeled bins and assign books to bins based on the first letter of the author’s last name. Whenever you get a new book, you can instantly tell which bin it goes in, and whenever you want to retrieve a book, you will immediately know where to look. In certain situations, both insertion and removal can be a lot faster than they would be if you stored items on one long shelf.&lt;/p&gt;
    &lt;p&gt;Of course, this bin system comes with its own problems. Retrieving books is only instantaneous if you have one book per bin; otherwise, you’ll have to root around to find the right one. In an extreme scenario where all your books are by Asimov, Atwood, and Austen, you’re back to the problem of one long shelf, plus you’ll have a bunch of empty bins cluttering up your living room.&lt;/p&gt;
    &lt;p&gt;Computer scientists often study data structures called hash tables that resemble more sophisticated versions of this simple bin system. Hash tables calculate a storage address for each item from a known property of that item, called the key. In our example, the key for each book is the first letter of the author’s last name. But that simple key makes it likely that some bins will be much fuller than others. (Few authors writing in English have a last name that starts with X, for example.) A better approach is to start with the author’s full name, replace each letter in the name with the number corresponding to its position in the alphabet, add up all these numbers, and divide the sum by 26. The remainder is some number between zero and 25. Use that number to assign the book to a bin.&lt;/p&gt;
    &lt;p&gt;This kind of mathematical rule for transforming a key into a storage address is called a hash function. A cleverly designed hash function ensures that items will usually end up distributed relatively evenly across bins, so you won’t need to spend as much time searching in each bin.&lt;/p&gt;
    &lt;p&gt;If you want to reduce retrieval time further, you can use more bins. But that leads to another trade-off: Those bins will take up space even if they end up empty.&lt;/p&gt;
    &lt;p&gt;This trade-off between space and time is an inherent feature of hash tables — it’s the price you pay for avoiding the tension between insertion and retrieval time that plagues simpler data structures. More than 70 years after hash tables were invented, computer scientists are still discovering new things about their fundamental properties. Recently, they finally devised a version that strikes an ideal balance between space and time. And last year, an undergraduate student disproved a long-standing conjecture about the minimum amount of time needed to find a specific item in a hash table that’s almost full.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Heap of Priorities&lt;/head&gt;
    &lt;p&gt;Hash tables work well when you can’t anticipate which piece of data you’ll need to retrieve next. But that’s not always the case. Imagine you’re trying to complete tasks on a to-do list, but you’re constantly being assigned new tasks with different deadlines. You want to be able to quickly add new items to the to-do list, but you don’t care about retrieving items until they become your top priority.&lt;/p&gt;
    &lt;p&gt;In this case, your best bet is a type of data structure called a heap. As the name suggests, a heap is a somewhat haphazard approach to data storage. It’s basically a mathematical version of a pile of stuff: Some items are stored above others, and these higher items are easier to access. The highest-priority item is always at the top of the heap, where you can instantly pluck it off. Lower layers will be more disorganized, but you don’t need to worry about the relative positions of these low-priority items.&lt;/p&gt;
    &lt;p&gt;The simplest implementation of this basic idea uses a mathematical object called a binary tree, which is a network of nodes with a special shape: There’s a single node at the top, and each node is connected to two nodes directly below it.&lt;/p&gt;
    &lt;p&gt;Let’s imagine a binary tree that contains the items in a to-do list. Each node can store a single item, and each item is labeled with a number that represents its due date. High-priority items get smaller numbers.&lt;/p&gt;
    &lt;p&gt;Each new item is put into an empty slot in the current lowest layer.&lt;/p&gt;
    &lt;p&gt;Once the new item goes in, compare its due date to that of the item in the node directly above it. If the new task is due sooner, swap the items. Keep swapping until the new item ends up directly below an item that’s more urgent.&lt;/p&gt;
    &lt;p&gt;This procedure ensures that the highest-priority item will always rise to the top. What’s more, the procedure is extremely fast. Even in a nightmare scenario where you have 1,000 tasks on your to-do list and keep getting new assignments, storing them in a heap ensures that it takes no more than nine swaps to move each new item up to the appropriate position. Whenever you complete the most urgent task and remove it from the heap, you can quickly pull up your new top priority from the layer below.&lt;/p&gt;
    &lt;p&gt;Within computer science, heaps are widely used in algorithms for finding the shortest path from a given starting point in a network to every other point. In 2024, a team of researchers used an ingenious new heap design to transform a classic shortest-paths algorithm into one that is theoretically optimal for any network layout.&lt;/p&gt;
    &lt;p&gt;There’s no shortage of self-help books filled with contradictory advice about the best way to organize your belongings. If computer science offers any lesson, it’s that there is no perfect solution — every approach comes with trade-offs. But if some items are more important to you than others, don’t be afraid to leave a bit of a mess.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46659219</guid><pubDate>Sat, 17 Jan 2026 16:17:58 +0000</pubDate></item><item><title>Apples, Trees, and Quasimodes</title><link>https://systemstack.dev/2025/09/humane-computing/</link><description>&lt;doc fingerprint="fd9b9b1c22e783ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Apples, Trees, and Quasimodes&lt;/head&gt;
    &lt;p&gt;A while back, Ars Technica published a thoughtful piece about Jef Raskin, tracing his long pursuit of the “humane computer” and the cul-de-sacs where that pursuit ended. It’s a generous, well-told account of the designer who wanted to make machines simpler, kinder, and more aligned with the way people actually think.&lt;/p&gt;
    &lt;p&gt;But part of what makes Raskin interesting is that his story isn’t just Apple’s story. He came out of the same cultural current John Markoff chronicled in What the Dormouse Said—the Bay Area tradition that treated computers not as office appliances but as tools for thought, instruments of liberation. Read that way, the Canon Cat and Raskin’s other projects aren’t just an eccentric side quest from a frustrated Apple veteran. It’s evidence of how far the humane ideal could stretch, and how quickly it ran up against the limits of commercial computing.&lt;/p&gt;
    &lt;p&gt;Apple couldn’t deliver Raskin’s vision then, and it can’t deliver it now. Neither can any other big platform company. If we want to understand why, and what Raskin still tells us about humane computing, we have to put him back in the longer lineage he belonged to, and look at how his version of the dream carried that vision but also narrowed it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Prophets and participants&lt;/head&gt;
    &lt;p&gt;What the Dormouse Said documents how the Bay Area counterculture shaped early personal computing. LSD, communes, systems theory, amorphous defense research contracts, and Engelbart’s “augmentation” experiments all swirled together in a weird scene that accidentally (or maybe not so accidentally) created much of the modern world.&lt;/p&gt;
    &lt;p&gt;The story usually gets told with a neat list: Engelbart’s demo, Nelson’s Xanadu hypertext, Kay’s Dynabook, Brand’s Whole Earth. Xerox PARC, Steve Jobs, the World Wide Web. The familiar pantheon. But that version turns a messy, improvisational moment into a plaque. Engelbart’s system needed a whole research staff just to operate; Nelson’s Xanadu was (and is) more sermon than software; Kay’s Dynabook lived mostly on paper; Brand mostly supplied vocabulary and vibe. What bound them together wasn’t working code so much as the conviction that computers could be more than appliances and calculators, even if no one agreed on what “more” meant.&lt;/p&gt;
    &lt;p&gt;Ultimately all these weird white guys had a futurist vision: computers could be liberation machines. They weren’t just for business automation or scientific number-crunching; they could be deployed to expand consciousness and reshape how people thought and worked.&lt;/p&gt;
    &lt;p&gt;Raskin belonged to this current. Before Apple, he was an artist and a musician. He brought a humanist’s suspicion of machine logic into the design lab. He argued for humane interfaces: modeless, predictable, low-friction, focused on the human first. He wasn’t a prophet on his own crying in the wilderness so much as another strand of the same weave.&lt;/p&gt;
    &lt;p&gt;That said, his role was different than that of some of these other figures. He tried to pull those ideals out of the lab and into machines ordinary people might actually use. The Macintosh began under his hand, though what shipped was less a tool for thought than a polished derivative—what you might call a “popular religion” of computing, stripped of the harder doctrines.&lt;/p&gt;
    &lt;p&gt;The Canon Cat and its predecessors were Raskin’s counterargument: humane, text-first systems that tried to carry the spirit of the Dormouse tradition into the commercial world without sanding off everything that made it strange. It sort of worked, but only sort of.&lt;/p&gt;
    &lt;head rend="h2"&gt;Raskin’s Humane vision&lt;/head&gt;
    &lt;p&gt;Raskin’s principles are laid out most clearly in 2000’s The Humane Interface, but he’d been developing them since the late 1970s:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modelessness: eliminate modes generally, and especially when they confuse users or are hard to reason about.&lt;/item&gt;
      &lt;item&gt;Quasimodes: short-lived states (like holding a key down) that don’t trap the user.&lt;/item&gt;
      &lt;item&gt;Humane defaults: undo everywhere, consistent commands, predictable behavior.&lt;/item&gt;
      &lt;item&gt;Low cognitive load: interfaces designed around human memory and perception limits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These ideas are recognizably part of the “Tools for Thought” tradition. Like Engelbart and the others, he wanted to reduce friction between thought and machine. Like Nelson, he believed in fluidity and extension.&lt;/p&gt;
    &lt;p&gt;But there’s a subtle difference. For Engelbart, augmentation meant complexity: bootstrapping a system so wild it demanded co-evolution between user and tool. For Nelson, it meant endless layers of possibility. For Raskin, it often meant protection or constraint. Humane computing wasn’t only about empowerment… often it was about shielding users from mistakes, overload, and confusion.&lt;/p&gt;
    &lt;p&gt;That protective impulse would shape the systems he built.&lt;/p&gt;
    &lt;p&gt;Raskin’s first clear articulation of his humane ideals wasn’t hardware at all but The Macintosh Papers, his internal proposal at Apple for a low-cost, appliance-like computer that would boot straight into a simple, modeless interface. The Mac project that followed eventually diverged—under Steve Jobs it became a graphical machine aimed at competing with the Lisa, for the reasons we’ve all read about—but Raskin’s vision was considerably more radical. He imagined a computer that behaved less like a business workstation and more like a humane, everyday tool.&lt;/p&gt;
    &lt;p&gt;In tone, the Macintosh Papers have more in common with Ted Nelson’s Computer Lib than with any corporate white paper. They read like a manifesto: plainspoken, insistent, arguing that ordinary people deserved machines that bent to them rather than the other way around. Where Nelson declared that “you can and must understand computers now,” Raskin’s papers laid out what such a computer should look like if you started from human needs instead of technical conventions. Both belong to that peculiar genre of the 1970s and early ’80s: the computing manifesto as cultural text, half engineering and half tract.1&lt;/p&gt;
    &lt;p&gt;You could argue that the Swyft, built a few years later by his company Information Appliance Inc., was “the real Macintosh” in that sense. Compact and text-first, it booted instantly, eliminated modes, and introduced the Leap keys for fluid navigation. It was Raskin’s manifesto rendered in hardware. But the Swyft never made it to market; without a manufacturer to back it, Information Appliance pivoted to the SwyftCard, a fallback product that brought the same interface into the Apple II while IA waited to find a dance partner.&lt;/p&gt;
    &lt;p&gt;That partner came briefly in 1987, when Canon released the Canon Cat, the only mass-produced computer to carry Raskin’s humane vision into the world. The Cat retained the Swyft’s defining ideas: instant boot into a blank page, consistent commands, Leap-based navigation. Marketed as a word processor, it was framed as an appliance for the office rather than an exploratory tool for thought.&lt;/p&gt;
    &lt;p&gt;After its failure, Raskin returned to the same design principles in the 1990s with Archy, an unfinished software environment that tried once again to realize his humane interface on contemporary hardware. Archy never reached a finished state, but it shows how Raskin’s ideas kept circling back to the same point: computing stripped down to words, presented as simply and predictably as possible.&lt;/p&gt;
    &lt;p&gt;I’ve always had a real fondness for the Swyft/Cat lineage, and it’s certainly influenced what I think a computer can be. Each one of these attempts embodied humane design: a blank screen for writing, consistent commands, no modes to trip over. The Cat in particular was radical in its way—a computer designed to feel less like a computer and more like a natural extension of the mind. It truly could have changed everything about how we use our computers had it succeeded.&lt;/p&gt;
    &lt;p&gt;Unfortunately for all of us, by 1987, the market for dedicated word processors was already fading. Canon didn’t seem to know what to do with the Cat—whether to sell it as an office appliance, a PC competitor, or something stranger—and the result was that it fit nowhere. Raskin’s design pushed toward humane simplicity, but Canon’s marketing treated it like just another machine for typing memos.&lt;/p&gt;
    &lt;p&gt;It isn’t surprising that it failed, though it’s hard not to wonder how it might have landed a few years earlier, when the ground was more open. As it is, the Cat survives less as a commercial product than as an idea in hardware—a glimpse of what a computer could look like if the whole thing were rebuilt around text, consistency, and genuine care for the user.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Paradox of Openness&lt;/head&gt;
    &lt;p&gt;The Cat also embodies why Raskin’s philosophy was not necessarily on the same wavelength as some of those other visionary systems. On the surface, the Canon Cat looked open. It booted to a blank screen. Everything was text. You could jump anywhere, edit fluidly, undo anything. Compared to the modal labyrinth of DOS or early Mac software, it felt like freedom.&lt;/p&gt;
    &lt;p&gt;But look closer and you see the narrowing. The Cat gave you fewer ways to improvise. Its humane design was also constraining design. It reduced your options in order to keep you safe.&lt;/p&gt;
    &lt;p&gt;The real irony is that the Cat wasn’t even truly closed in the way a smartphone or Chromebook might be considered so today. Underneath, it ran on a Forth environment. You could, if you knew how, drop into Forth and even program directly in 68k assembler. In principle, it was as open as any hacker could want, at least from a software perspective.&lt;/p&gt;
    &lt;p&gt;The catch was cultural, not technical. From the Ars piece:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;IAI’s back door to Forth quietly shipped in every Cat, and the clue was a curious omission in the online help: USE FRONT-ANSWER. This otherwise unexplained and unused key combination was the gateway. If you entered the string&lt;/p&gt;&lt;code&gt;Enable Forth Language&lt;/code&gt;, highlighted it, and evaluated it with USE FRONT-ANSWER (not CALC; usually Control-Backspace in MAME), you’d get a Forth&lt;code&gt;ok&lt;/code&gt;prompt, and the system was now yours. Reset the Cat or type&lt;code&gt;re&lt;/code&gt;to return to the editor.&lt;/quote&gt;
    &lt;p&gt;Canon didn’t provide documentation that would have made that power accessible, and Raskin’s design philosophy treated it as outside the normal use case. Extensibility was there if you knew where to look for it, but it wasn’t encouraged. The humane interface was meant to keep most users away from the hood, even though what was under the hood was remarkably open.&lt;/p&gt;
    &lt;p&gt;That makes the Cat’s paradox sharper: it was a genuinely extensible software environment (up to a point) presented as a sealed appliance. The hardware mostly was a sealed appliance. Contrast this with Emacs or Smalltalk, where openness is the posture of the environment itself. You are expected to extend and reshape as you go, building your tools out of themselves. The Cat offered the same possibility–Forth is a remarkably flexible language, especially for microcomputers–but it discouraged you from taking it.&lt;/p&gt;
    &lt;p&gt;Humane computing, in Raskin’s hands, edged toward hermetic computing. He built openness in, but sealed it away behind an interface designed to keep it out of sight.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cul-de-sacs vs. Branches&lt;/head&gt;
    &lt;p&gt;All of this, to me, is why calling Raskin’s systems thinking a “cul-de-sac” misses the point, and is the wrong way to think about his legacy.&lt;/p&gt;
    &lt;p&gt;If “cul-de-sac” means “product that didn’t sell,” then sure, the Cat and SwyftCard qualify. They were total dead-ends. But by that same measure, Engelbart’s NLS, Nelson’s Xanadu, Kay’s Smalltalk, or even Lotus Agenda are dead-ends, too. By that measure, most of the “Tools for Thought” tradition didn’t lead anywhere.&lt;/p&gt;
    &lt;p&gt;The reality is different. These systems were branches. They were rhizomes, in the Deleuze and Guattari sense. They didn’t reach the mainstream, but they seeded ideas that echoed elsewhere, connecting threads that run throughout the history of computing. Hypertext, graphical interfaces, undo, modeless editing—all of these survived in one form or another.&lt;/p&gt;
    &lt;p&gt;Raskin’s branch is no exception. His machines exposed a fundamental tension inside the tradition: how far do you go in protecting the user from complexity? At what point does “humane” become “hermetic”? Those questions didn’t vanish with the Cat. They’re still with us every time a productivity app promises “simplicity” at the cost of agency.&lt;/p&gt;
    &lt;p&gt;Raskin’s humane ideals live on in obvious ways, to the benefit of anyone using a graphical computer today—undo everywhere, discoverability, and consistent commands and shortcuts are now interface common sense. But the deeper thread, the ethos that inspired him and others in the tradition of computers as tools for thought, survived mostly outside the mainstream. It persists in systems that never had to sell millions of units or satisfy quarterly targets, that never had to justify their existence to the mass of people using PCs—tools that could afford to remain strange, open, and humane on their own terms. Emacs, Oberon, and Smalltalk belong here, but so do newer experiments like Uxn and 9front.&lt;/p&gt;
    &lt;p&gt;The Cat failed partly because it tried to straddle two worlds: commercial appliance and humane machine, whereas something like Emacs survives precisely because it never had to. It’s as complex as you want it to be.&lt;/p&gt;
    &lt;p&gt;This is the sharper point: radical, humane, exploratory computing never survives in the mainstream. The mainstream is built for profit and predictability. Even Engelbart’s work was DARPA-funded, not venture-backed. When you put humane ideals through commercial constraints, they collapse into simplistic appliances, the “For Dummies” version of the original intent. That doesn’t mean the tradition is dead. But it does mean you have to look off to the side, away from the market’s center, to see it alive.&lt;/p&gt;
    &lt;head rend="h2"&gt;The dilemma(s)&lt;/head&gt;
    &lt;p&gt;Raskin’s story sharpens two dilemmas that haven’t gone away.&lt;/p&gt;
    &lt;p&gt;The first is practical: make a system too open, and it risks being overwhelming. Make it too humane, and it risks narrowing into something sealed and hermetic, and not useful enough. The Cat, while also a victim of other factors, tried to balance the two and ended up fitting nowhere.&lt;/p&gt;
    &lt;p&gt;The lesson isn’t that humane computing is impossible. It’s that humane computing can’t just mean protective computing. It has to mean trusting users with both simplicity and openness. That’s why Org mode and even Mac System 7 endure and the Cat does not.&lt;/p&gt;
    &lt;p&gt;The deeper implication is harder, but maybe truer: the true Tools for Thought we still wish existed will never come from Apple, Microsoft, Google, OpenAI, or any other large player in the software or hardware space. They can’t. These companies’ scale and incentives point elsewhere—toward lock-in, surveillance, and products that are safe enough to sell but never open enough to empower. The logic of scale makes them constitutionally incapable of building systems that are truly humane and open. The next humane systems, if they arrive, will have to come from outside those walls, as they always have: from margins, from hobbyists, from research labs, and from stubborn communities of practice. But as those platform companies make it more and more difficult to experiment, how do we keep pushing these philosophies forward?&lt;/p&gt;
    &lt;p&gt;Jef Raskin’s philosophy isn’t a cul-de-sac in computing history. He’s responsible for a branch of the “Tools for Thought” tradition—a branch that shows both the promise and the peril of humane design. His machines make clear how far you can go when you put the human first, and how easily that ideal can collapse into constraint once it’s pushed through commercial channels and turned into walled gardens.&lt;/p&gt;
    &lt;p&gt;The humane thread survives, but only outside the center—in the tools that don’t have to answer to quarterly earnings, in projects that refuse to die just because they don’t fit the market. The Dormouse lineage isn’t gone. It just doesn’t live where the money is, because it can’t. If you want your computer to be humane in the deeper sense—not an appliance, but an instrument for thought—you have to look to the margins. That’s where it has always been, and where it still is today. If it survives, that’s where it’ll still be.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;That genre, “photocopied computer manifesto,” is very much the reason this blog exists. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Published September 18, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46659465</guid><pubDate>Sat, 17 Jan 2026 16:44:45 +0000</pubDate></item><item><title>An Elizabethan mansion's secrets for staying warm</title><link>https://www.bbc.com/future/article/20260116-an-elizabethan-mansions-secrets-for-staying-warm</link><description>&lt;doc fingerprint="b42e10265ba205d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'The past is an underused tool': An Elizabethan mansion's secrets for staying warm&lt;/head&gt;
    &lt;p&gt;In a bleak, deadly period of cold weather known as the Little Ice Age, clever Elizabethan designs helped keep a magnificent stately home unusually warm. The house has lessons for how we can heat our homes more efficiently today.&lt;/p&gt;
    &lt;p&gt;England's longest river was usually flowing freely. But on New Year's Eve in 1564, the River Thames was frozen solid, from bank to bank. Bonfires crackled on the stuck-fast surface, oxen roasted on spits, and people danced on the ice. Some accounts say that Queen Elizabeth I even practised archery on the glacial river. This sort of thing wasn't a one off. It had happened before: King Henry VIII and his queen had dashed downriver in a sleigh nearly three decades previously in 1537.&lt;/p&gt;
    &lt;p&gt;These frosty conditions were the result of a climatic plot twist roughly between the 14th and 19th centuries, known today as the Little Ice Age. As well as festivals on the ice, this prolonged cold period brought periods of famine, and frightening unseasonable frosts. Soldiers froze to death in the middle of the European summer.&lt;/p&gt;
    &lt;p&gt;The cold forced Europeans to develop new ways of coping with extreme weather. One of the best-studied examples of architectural adaptation is Hardwick Hall in Derbyshire, England – a building whose design is a carefully choreographed effort to keep as warm as possible.&lt;/p&gt;
    &lt;p&gt;The same tricks for more efficient heating can be used in modern designs, helping reduce our reliance on fossil fuels today. And they can even inspire small changes in our existing homes to keep temperatures cosier through the winter without turning up the thermostat.&lt;/p&gt;
    &lt;head rend="h2"&gt;An 'exceptional' house&lt;/head&gt;
    &lt;p&gt;I drive the long, meandering driveway uphill to the house, confronted by the occasional long-horn cattle grazing between leafless oak trees. At the crest of the hill, I'm met with a striking sight: not one hall, but two.&lt;/p&gt;
    &lt;p&gt;Hardwick "old" Hall is massive, despite its ruinous state. I can tell it's been repeatedly extended over the years, as the bricks are misaligned at the joins of each extension and the windows are mismatched in style and size across the facade.&lt;/p&gt;
    &lt;p&gt;What caused the Little Ice Age?&lt;/p&gt;
    &lt;p&gt;It appears there's no single cause of the Little Ice Age, but a deadly and complex combination. Scientists have found evidence for reduced solar activity, increased volcanic eruptions, changes in ocean circulation and the natural fluctuations within the global climate system. In addition, the arrival of the Europeans in North America in the late 15th Century led to an estimated 56 million deaths of indigenous peoples, resulting in widespread abandonment of farming and regrowth of forests. More trees mean less planet-warming gases were circulating in the atmosphere, reducing the global average temperature.&lt;/p&gt;
    &lt;p&gt;Hardwick "new" Hall is a few dozen metres away. This pale yellow manor was built in the 1590s and is eye-pleasingly symmetrical, complete with three-story turrets and huge expanses of glass. Whoever quipped at the time of its construction that it was "more window than wall" was right. It is a magnificent display of wealth, built in a time when glass was extremely expensive.&lt;/p&gt;
    &lt;p&gt;Elizabeth (Bess), Countess of Shrewsbury, was the woman who had deep enough pockets to build it. She was mid-way through extending the massive, rambling Hardwick "old" Hall, but for some reason or another, stopped midway through and began afresh. The experts I spoke to say we don't know why she did that, but theories range from coming into money when her husband died and feeling the need to have a house in keeping with her elevated status, to using what she'd learnt in previous builds to design a house warm and cosy for a lady approaching her seventies and living through the Little Ice Age.&lt;/p&gt;
    &lt;p&gt;"The late 16th Century is really one of the coldest stretches of the Little Ice Age, and it's bitterly cold in England," says Dagomar Degroot, professor of environmental history at Georgetown University in Washington, DC, and author of The Frigid Golden Age.&lt;/p&gt;
    &lt;p&gt;Global average temperatures during the Little Ice Age dipped "at most" by 0.5C (less than 0.9F), with impacts mostly documented in the northern hemisphere. That figure is an average over about five centuries, so temperatures would have swung more dramatically year to year and region to region.&lt;/p&gt;
    &lt;head rend="h2"&gt;Turning to the Sun&lt;/head&gt;
    &lt;p&gt;A key difference between the old hall and the new hall is their orientation in relation to the Sun. The old hall is just off east-west. The new hall has been rotated by about 90 degrees, which means it can soak up much more sunshine and, therefore, heat.&lt;/p&gt;
    &lt;p&gt;"The incredible thing about Hardwick [new Hall] is… when you set it on the compass, it's almost exactly north-south," says Ranald Lawrence, a lecturer in architecture at the University of Liverpool in the UK. He's also published papers on Hardwick's design and thermal comfort. "And," he adds, "the whole internal planning of the [new] house is then based around that geometry."&lt;/p&gt;
    &lt;p&gt;Bess moved around the rooms, following the Sun's path. Her mornings were spent walking the 63m (200ft) east-facing Long Gallery, where the bright morning light hits. The afternoon and evening Sun illuminates the south-western flank of the building, where Bess' bed chambers were. And the darkest, coldest corner of the house in the north-west was where the kitchens were placed, which would have been handy in keeping food cool and fresh.&lt;/p&gt;
    &lt;p&gt;I experience this first hand as I walk around – the kitchens are much colder. Elena Williams, the senior house and collections manager at The National Trust, a UK charity which preserves historic sites, notices too. "It's a well-designed building that is also designed around comfort and that uses the natural environment to do that," she says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows, walls and fireplaces&lt;/head&gt;
    &lt;p&gt;It's not just the orientation that helps keep the house warm. As Williams shows me around, she points out that some of the windows on the north of the building are actually "blind" or fake. She explains that on the outside, there is a window, but on the inside, it's lined with lead and blocked up. Unlike south-facing windows, north-facing windows bring little thermal benefit, even in summer, Lawrence says.&lt;/p&gt;
    &lt;p&gt;Pretty much all the fireplaces I see are also built on the central spine of the building, meaning not much heat would be lost to the windows or exterior wall. It's not until we take a door through this spine that I realise that the girth of it is staggering – 1.37m (4.5ft) thick. This is yet another trick to keep its inhabitants warm.&lt;/p&gt;
    &lt;p&gt;"You have thermal mass, effectively," Lawrence says. "So something heavy like brick or stone, like you have at Hardwick, stores the heat from the fire and gives it out 12 hours later."&lt;/p&gt;
    &lt;p&gt;All these construction techniques appear to have made a difference. Lawrence has measured the temperature difference between inside and outside in modern times and depending on the season and weather, he told me it can feel around 10C (18F) warmer inside on a cold winter's day. Other, typical Elizabethan houses, he estimates, would have only feel 2-3C (3.6-5.4F) warmer.&lt;/p&gt;
    &lt;p&gt;The Tudors had other coping mechanisms, Williams says – like lining the walls with thick tapestries – adding further thermal mass and keeping out the drafts. Curtains were hung around the beds and over some of the windows too. And Elizabethan fashion of giant neck ruffs and layers and layers of linens, thick velvet and fur all helped people like Bess keep warm.&lt;/p&gt;
    &lt;p&gt;Other large and flashy manors at the time were using some of the same solar strategies. But Lawrence believes Hardwick is "exceptional" in the way these elements are carefully integrated and brought together.&lt;/p&gt;
    &lt;p&gt;Though Lawrence says there is no written evidence to suggest that the architectural designs were purposeful, he thinks that "it can't be coincidence". Williams agrees. "I think they definitely thought about using the Sun in the design of Hardwick," she says.&lt;/p&gt;
    &lt;p&gt;All this despite the fact that the Elizabethans may have been unaware that they were living through what is now known as the Little Ice Age, says Degroot. "Why would I expect somebody living 400 years ago would realise that their climate was 0.5C colder than the climate had been in their mother's or father's lifetime?"&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons for modern times&lt;/head&gt;
    &lt;p&gt;There are still lessons we can learn today when we build new homes and in the way we use the ones we've already got – especially with the need to heat and cool our homes more efficiently in order to save money and tackle climate change.&lt;/p&gt;
    &lt;p&gt;"The past is an underused tool," Degroot says. "I think by trying to identify the complex and diverse ways in which people responded to history's climate changes, we can come up with new tools for understanding how we might respond in the future and for identifying responses that are constructive versus destructive."&lt;/p&gt;
    &lt;p&gt;Brutalist architects Peter and Alison Smithson knew of and even admired Hardwick Hall. Some academics claim that it likely inspired their own designs, like the Solar Pavillion, in south-west England, which is only has glass on its east, west and south-facing walls. Sun-soaking designs aren't just the preserve of the rich, though. One of London's most striking council estates is on Alexandria Road in Camden, in the north of the city, and Lawrence says it too features south facing terraces with lots of concrete to store the Sun's heat.&lt;/p&gt;
    &lt;p&gt;But on the whole, he tells me, we generally don't use these Elizabethan building secrets. Instead, we use air conditioning and heating in an attempt to override building designs that are poorly suited to their climate.&lt;/p&gt;
    &lt;p&gt;"Our assumption that the solution to all of our problems is technological," Lawrence says. Glass box skyscrapers, now common in both cold and hot climates, are a good example of this. In winter, heat escapes through the glass, and require a lot of heating. Conversely in summer, the glass traps the heat – like a greenhouse – and require massive amounts of energy for cooling.&lt;/p&gt;
    &lt;p&gt;But without taking apart our existing housing and building it again from scratch, there are also micro-adjustments we can make.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hacks to keep your home warmer&lt;/item&gt;
      &lt;item&gt;The homes heated without fossil fuels&lt;/item&gt;
      &lt;item&gt;How living in a cold home affects your health&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I get a compass out at my house – for the first time – and begin to think about how I could follow the Sun's path throughout the day. Since it's winter, and cold, I move my desk to a south-eastern window. It brightens the mornings and if I wear another layer, I find I can lower the thermostat by 2C (3.6F). Longer term, I've been thinking about planting a tree just outside. In a couple of decades, it would shade my house from scorching heatwaves that are predicted to be much more common because of climate change.&lt;/p&gt;
    &lt;p&gt;These are modest changes, imperceptible to most, and they won't enable us to forgo active heating and cooling entirely. But they do echo a way of thinking which, today, is oft ignored. Hardwick Hall was designed with Sun, season and temperature in mind. It paid attention to the world outside its walls. As the climate becomes more volatile, architecture that works with its environment feels more urgent than ever.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For essential climate news and hopeful developments to your inbox, sign up to the Future Earth newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more science, technology, environment and health stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46659550</guid><pubDate>Sat, 17 Jan 2026 16:53:24 +0000</pubDate></item><item><title>Show HN: What if your menu bar was a keyboard-controlled command center?</title><link>https://extrabar.app/</link><description>&lt;doc fingerprint="21dcd83b7d1977a9"&gt;
  &lt;main&gt;&lt;p&gt;ExtraBar brings Mac users a customizable menu bar with instant access to apps, deep links, and custom actions. Requiring zero permissions to work.&lt;/p&gt;&lt;p&gt;Create custom actions for any app or file. Use Deep Links, trigger macOS Shortcuts, and so much more, tailored to your workflow.&lt;/p&gt;&lt;p&gt;Launch any app screen, setting, or feature directly. No menu clicking required. Get instant access to exactly where you want to be.&lt;/p&gt;&lt;p&gt;Switch between inline and floating bar mode. Choose inline mode for native menu bar integration, or floating mode for a customizable separate bar.&lt;/p&gt;&lt;p&gt;Assign one global hotkey to interact with ExtraBar. Use numbers or arrows to navigate easily between apps and menus.&lt;/p&gt;&lt;p&gt;Designers, developers, founders, everyone gets the same bar with the same limitations. Each one has different needs,&lt;/p&gt;&lt;p&gt;but the menu bar stays the same.&lt;/p&gt;&lt;p&gt; The macOS menu bar was designed to show system info and app icons.&lt;lb/&gt; It wasn't built to help you do things quickly. You get: &lt;/p&gt;&lt;p&gt;Designers, developers, founders, everyone gets the same bar with the same limitations. Each one has different needs,&lt;/p&gt;&lt;p&gt;but the menu bar stays the same.&lt;/p&gt;&lt;p&gt; The macOS menu bar was designed to show system info and app icons.&lt;lb/&gt; It wasn't built to help you do things quickly. You get: &lt;/p&gt;&lt;p&gt;ExtraBar is made for...&lt;/p&gt;&lt;p&gt;Designers&lt;/p&gt;&lt;p&gt;Developers&lt;/p&gt;&lt;p&gt;Managers&lt;/p&gt;&lt;p&gt;Power Users&lt;/p&gt;&lt;p&gt; Access your daily design tools instantly. Open specific Figma files, frames, and prototypes! &lt;lb/&gt; You can also open assets and folders straight from your menu bar. &lt;/p&gt;&lt;p&gt;Part of a team? Open your Slack channel directly from the menu.&lt;/p&gt;&lt;p&gt; Jump straight into your code project.&lt;lb/&gt; Open specific projects in your IDE, terminal sessions, and anything else with a deep link! &lt;/p&gt;&lt;p&gt;Client call? Hit a hotkey and jump straight into your Zoom call.&lt;/p&gt;&lt;p&gt;Stay on top of your team. Open specific dashboards, reports, and Slack channels! Effortlessly jump into WhatsApp chats with your most used contacts.&lt;/p&gt;&lt;p&gt;Running late? Join your Zoom or Google Meet directly from the menu bar.&lt;/p&gt;&lt;p&gt;Build your perfect workflow. Create custom actions, keyboard shortcuts, and deep links! Leverage tools like Raycast to unlock a new level of productivity with deep links.&lt;/p&gt;&lt;p&gt;One hotkey to rule them all, One hotkey to find them, One hotkey to launch them all, and in the menu bind them.&lt;/p&gt;&lt;p&gt;A glimpse into your future workflow. Clean, powerful, and always a single hotkey away.&lt;/p&gt;&lt;p&gt;Your Command Center — Organize apps, folders and files in one intuitive management screen.&lt;/p&gt;&lt;p&gt;Total Control — Define custom labels, choose action types, and configure every detail your way.&lt;/p&gt;&lt;p&gt;Deep Links — Skip the clicks. Land exactly where you need to be.&lt;/p&gt;&lt;p&gt;Full Control — Browse through action presets designed for your favorite apps.&lt;/p&gt;&lt;p&gt;Make It Yours — Personalize colors, layouts, and behaviors to match your setup.&lt;/p&gt;&lt;p&gt;The difference between&lt;/p&gt;&lt;p&gt;Apps like Bartender, Ice and Barbee are built to hide, organize, and manage menu bar icons. To achieve that, they require system-level permissions that allow them to record your screen, and manage accessibility.&lt;/p&gt;&lt;p&gt;ExtraBar is built for action, not icon management. It requires zero permissions to work. Simply download, add your apps and you're set. Optionally enable accessibility for enhanced keyboard navigation.&lt;/p&gt;&lt;p&gt;The difference between&lt;/p&gt;&lt;p&gt;Apps like Bartender, Ice and Barbee are built to hide, organize, and manage menu bar icons. To achieve that, they require system-level permissions that allow them to record your screen, and manage accessibility.&lt;/p&gt;&lt;p&gt;ExtraBar is built for action, not icon management. It requires zero permissions to work. Simply download, add your apps and you're set. Optionally enable accessibility for enhanced keyboard navigation.&lt;/p&gt;&lt;p&gt;We're excited to finally launch ExtraBar! During launch month, you can get lifetime access with one time payment.&lt;/p&gt;&lt;p&gt;After January 31, the price will be €24.99&lt;/p&gt;&lt;p&gt;Taxes might apply&lt;/p&gt;&lt;p&gt;We're excited to finally launch ExtraBar! During launch month, you can get lifetime access with one time payment.&lt;/p&gt;&lt;p&gt;After January 31, the price will be €24.99&lt;/p&gt;&lt;p&gt;Taxes might apply&lt;/p&gt;&lt;p&gt;One-time payment · Lifetime updates · 14-day money-back guarantee&lt;/p&gt;Start using ExtraBar&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46659943</guid><pubDate>Sat, 17 Jan 2026 17:31:21 +0000</pubDate></item><item><title>Raising money fucked me up</title><link>https://blog.yakkomajuri.com/blog/raising-money-fucked-me-up</link><description>&lt;doc fingerprint="7d4e111769e460bd"&gt;
  &lt;main&gt;
    &lt;p&gt;About four months ago I quit my job at Doublepoint and decided to start my own thing.&lt;/p&gt;
    &lt;p&gt;I'd been working on a little project with Pedrique (who would become my co-founder) for a bit over half-a-year and decided I had enough signal to determine he was someone I wanted to start a business with.&lt;/p&gt;
    &lt;p&gt;I was excited about the idea we were working on at the time, but being truly honest about my motivations, I mostly wanted to run my own thing. In a dream world I'd have had the "idea of my life" while working at PostHog or Doublepoint and have gone on to build that with maximum conviction but this wasn't the case, so I got tired of waiting for a spark and decided to go out and make it happen, with the idea we were working on being our best bet at the time.&lt;/p&gt;
    &lt;p&gt;Since I'd just quit my job, I had my finances well in order. Thus, my ideal scenario would have been to work on the idea we had the MVP for, try to get it off the ground, and if that didn't work, try something else, then something else, until something did indeed get off the ground, and only at that point we would consider whether or not to raise VC funding, depending on whether it made sense or not.&lt;/p&gt;
    &lt;p&gt;My ideal scenario wasn't going to work for Pedrique, though. He had told me for a while that the money he had saved up for trying to build his own thing was running out and that soon he'd need to start freelancing or something to make some income in order to sustain the search for a little longer. Prior to us working together, he had a bit of success with his MicroSaaS products but only just enough to increase his personal runway, which was now reasonably short.&lt;/p&gt;
    &lt;p&gt;We had spoken about this before, but with me now being 110% in, we had to do something about it. I had just come in full-time so we weren't about to go back to a dynamic where one person was full-time and the other part-time because they needed to make ends meet. The decision then became clear: we're gonna raise.&lt;/p&gt;
    &lt;p&gt;At that point, it was an easy decision to make. Again, we have two co-founders who have a lot of confidence in each other, and we don't want to let the opportunity pass us by. So while this wasn't my ideal choice, we were a business now and this was the best decision for the company. "Just don't die" goes the advice I think, and Skald had just then been born.&lt;/p&gt;
    &lt;p&gt;And so raise we did. We brought in four phenomenal angels, including, and this is relevant, my last few bosses (PostHog co-founders James and Tim and Doublepoint co-founder Ohto), and then decided to look for an early-stage fund. We eventually landed with Broom Ventures and passed up on a few other opportunities to limit dilution.&lt;/p&gt;
    &lt;p&gt;Great, right? I didn't need a salary yet, but for equality purposes, I now had one. Our investors are amazing. James and Ohto have been particularly helpful as angels (thank you!), and our investors are all founders of successful companies, including Jeff and Dan, the Broom GPs. We're super early, but Broom has been massively helpful and all-around just a great hands-off VC to deal with.&lt;/p&gt;
    &lt;p&gt;Most importantly, none of them put any pressure on us. All understand the nature of pre-seed investing well, and that can't be said about all the potential investors we took meetings with.&lt;/p&gt;
    &lt;p&gt;So some time passes and we decide to pivot. We're really excited about the new idea. We launch and get a bit of early traction. The open source project is doing well, but we're struggling to monetize. We fail to close a few customers and the traction wanes a bit.&lt;/p&gt;
    &lt;p&gt;Then I find myself fucked in the head.&lt;/p&gt;
    &lt;p&gt;And here's where we get to the point that I'm not sure I should be talking publicly about. Does this hurt my image a bit? Maybe. Do I look like I'm not cut for this? Potentially. But I've always appreciated when people share about the process rather than just talking about things in hindsight, and reflecting while things are happening + being super transparent publicly is how I am. You're witnessing my growth, live, as I type these words.&lt;/p&gt;
    &lt;p&gt;Anyway, so what happened is I found myself spending days with my head spinning, searching for ideas. I'm angry, I'm annoyed, and I'm not being super productive.&lt;/p&gt;
    &lt;p&gt;As I dug deeper into these feelings, I realized I was feeling pressured. We weren't making that much money, we weren't growing super fast. Then you look around and see "startup X gets to $1M ARR a month after launch" and shit like that and I'm feeling terrible about how we're barely growing. I'm thinking people that I really respect and admire have placed a bet on me and I'm letting them down.&lt;/p&gt;
    &lt;p&gt;Except they're not saying this, I am.&lt;/p&gt;
    &lt;p&gt;There's an interesting reflection that came up in a discussion between me and my girlfriend a few months prior that I realized applied to me, but in reverse. It's much more comfortable to be the person that "could be X" than to be the person that tries to actually do it. We were speaking about this regarding people who have a clear innate talent for something like music or sports but don't practice at all. Everyone says things like "you'd be the best at this if you just practiced more" but then they never do.&lt;/p&gt;
    &lt;p&gt;The thing is: it's a lot easier to live your life thinking you could have done X if you wanted to, than to "disappoint" these people that believed in you by trying and failing. You can always lean on this idea in your head of what you could have been, and how everyone believed in you so it must be true, but you just chose not to follow that path.&lt;/p&gt;
    &lt;p&gt;In my case, I found myself on the other side of that coin. Throughout my career, I've always had really high ownership roles, and have been actively involved in a couple 0 to 1 journeys. This led me throughout my career to get many comments about how great of a founder I'd be or how I have the "founder profile". I led teams, I wore a bunch of different hats, I worked hard as fuck, and I always thought about the big picture.&lt;/p&gt;
    &lt;p&gt;Those traits led my former bosses to then invest in me, and now suddenly I have to, in my head, live up to all of this. I can no longer take solace in some excuse like "I could have been a founder but working full-time was the best financial decision (it almost always is) so I never started my own thing". I set foot down a path from which there's no return. I've begun my attempt. I can of course stop and try again later. But from now on, I'm either gonna be a successful founder, or I'm not. And if I'm not, I'll have to deal with having broken with the expectations that people had of me.&lt;/p&gt;
    &lt;p&gt;There's a lot to unpack here, including what "success" means, and how most of what I say are other people's expectations are actually my own projected onto them (I've learned this about my relationship with my father too), but this post is already a bit too long so I'll save those for another time.&lt;/p&gt;
    &lt;p&gt;But the whole point here is not just that having raised this money from friends my head got a bit messy, but that I started to actually operate in a way that is counterproductive for my startup, while thinking I was actually doing what was best.&lt;/p&gt;
    &lt;p&gt;Ideas we considered when pivoting were looked more through a lens of "how big does this feel" rather than "what problem does this solve and for who". The slow growth was eating me, and while slow growth is terrible and can be a sign that you're on the wrong path it needs to be looked at from an objectively strategic lens. Didn't we say we were going to build an open source community and only later focus on monetization? Is that a viable strategy? Do we actually have a sound plan? Those were the things I should have been thinking about, rather than looping on "we need something that grows faster".&lt;/p&gt;
    &lt;p&gt;The people who invested in us, invested in us, not whatever idea we pitched them. And the best thing we can do is to follow our own process for building a great business based on what we believe and know, rather than focusing on making numbers look good so I can feel more relieved the next time I send over an investor update.&lt;/p&gt;
    &lt;p&gt;We have a ton to learn, particularly about sales (since we're both engineers), so we can't just be building shit for the sake of building shit because that's our comfort zone. But if our process is slower than company X on TechCrunch, that's fine. It's a marathon after all.&lt;/p&gt;
    &lt;p&gt;So after probably breaking many rules about what a founder should talk about publicly, what was my whole goal here? I mean, the main thing for me with posts like this is to get things off my chest. I've always said that the reason I publish writing that includes poems about my breakup, stories about falling in love, posts about my insecurities, and reflections about my dreams is that by there being the possibility of someone reading them (because technically it could be the case that nobody does) I can truly be who I really am in my day-to-day life. If I'm ok with there being the possibility of a friend I'll meet later today having read about how I felt during my last breakup, I can be myself with them without reservations, because I've made myself available to be seen. That's always been really freeing to me.&lt;/p&gt;
    &lt;p&gt;As a side effect, I'd hope that if this does get read by some people, particularly those starting or looking to start a business, that they can reflect about themselves, their lives, and their companies through listening to my story. I thought about writing a short bullet list about lessons I learned from raising money and dealing with its aftermath here, but honestly, that's best left to the reader to figure out. We're all different, and how one person reacts to a set of circumstances will differ from someone else. Some people don't feel pressure at all, or at least not from friends or investors. Or they only respond positively to pressure (because it certainly has benefits too). Maybe they're better off than me. Maybe they're not.&lt;/p&gt;
    &lt;p&gt;This is my story, after all. I wish you the best of luck with yours.&lt;/p&gt;
    &lt;p&gt;P.S. I'm doing good now. I'm motivated and sharp. If someone finds themselves in a similar situation, feel free to shoot me an email if you're keen to talk. Happy to go over what was useful for me, which fell outside of the scope of this post.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46660543</guid><pubDate>Sat, 17 Jan 2026 18:29:00 +0000</pubDate></item><item><title>The thing that brought me joy</title><link>https://www.stephenlewis.me/blog/the-thing-that-brought-me-joy/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46660663</guid><pubDate>Sat, 17 Jan 2026 18:42:39 +0000</pubDate></item><item><title>Why Twenty Years of DevOps Has Failed to Do It</title><link>https://www.honeycomb.io/blog/you-had-one-job-why-twenty-years-of-devops-has-failed-to-do-it</link><description>&lt;doc fingerprint="255053116fc506bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;“You Had One Job”: Why Twenty Years of DevOps Has Failed to Do it&lt;/head&gt;
    &lt;p&gt;I think the entire DevOps movement was a mighty, twenty year battle to achieve one thing: a single feedback loop connecting devs with prod. On those grounds, it failed.&lt;/p&gt;
    &lt;p&gt;By: Charity Majors&lt;/p&gt;
    &lt;p&gt;Let’s start with a question. What is DevOps all about?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Empathy!&lt;/item&gt;
      &lt;item&gt;Breaking down silos!&lt;/item&gt;
      &lt;item&gt;Forcing operations engineers to write more software!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’ll tell you my answer. In retrospect, I think the entire DevOps movement was a mighty, twenty year battle to achieve one thing: a single feedback loop connecting devs with prod.&lt;/p&gt;
    &lt;p&gt;On those grounds, it failed.&lt;/p&gt;
    &lt;p&gt;Not because software engineers weren’t good at their jobs, or didn’t care enough. It failed because the technology wasn’t good enough. The tools we gave them weren’t designed for this, so using them could easily double, triple, or quadruple the time it took to do their job: writing business logic.&lt;/p&gt;
    &lt;p&gt;This isn’t true everywhere. Please keep in mind that all data tools are effectively fungible if you can assume an infinite amount of time, money, and engineering skill. You can run production off an Excel spreadsheet if you have to, and some SREs have done so. That doesn’t make it a great solution, the right use of resources, or accessible to the median engineering org.&lt;/p&gt;
    &lt;head rend="h2"&gt;Good news and bad news&lt;/head&gt;
    &lt;p&gt;The good news is that AI has changed this. The technology we have now is good enough to create a feedback loop between developers and production systems for the median engineering team, for the first time ever.&lt;/p&gt;
    &lt;p&gt;The bad news is also that AI has changed this. Our existing feedback loops are unprepared to deal with the current amount of code slop. And I think we all know what the volume of code slop is about to do:&lt;/p&gt;
    &lt;p&gt;(Oh yeah, guess what I learned to do over the break? STICK ART, baby doll.)&lt;/p&gt;
    &lt;p&gt;I know this is a big, spicy claim. And since I got as pissed off as anyone when vendors were posting clickbait about “DEVOPS IS DEAD,” I’m going to back up and show you my argument from scratch. I’m not saying this in an accusatory, inflammatory kind of way. The truth is, we were all sensing and circling around the same problem, and it was the right one. We did the best we could with the tools that we had.&lt;/p&gt;
    &lt;head rend="h2"&gt;Value-generating feedback loops&lt;/head&gt;
    &lt;p&gt;If your business makes money by building products with software, this is what progress looks like: you build something new, ship it to users, and see what happens.&lt;/p&gt;
    &lt;p&gt;This is the theoretical feedback loop of generating business value with software. As our friends at Intercom like to say, “shipping is your company’s heartbeat.” The value-generating loop gets kicked off every time you deploy a new diff. In stick art, it looks like this: deploy -&amp;gt; observe -&amp;gt; learn.&lt;/p&gt;
    &lt;p&gt;“Build” isn’t shown because it doesn’t count. Value does not get captured until the code has been deployed. That’s one of the reasons why software experts are always haranguing us to ship frequently. Like this:&lt;/p&gt;
    &lt;p&gt;Or even…&lt;/p&gt;
    &lt;p&gt;(Look at all that learning! 😍)&lt;/p&gt;
    &lt;p&gt;Every deploy is a chance to learn something new about your product, your system, your users, your feature, etc. But what if you deploy new code and don’t observe?&lt;/p&gt;
    &lt;p&gt;If you don’t observe, you don’t learn anything. Your deploy becomes an open loop. You are shipping blind.&lt;/p&gt;
    &lt;p&gt;This is what observability does, by the way. It’s the sense mechanism that enables all your other feedback loops to function. It’s the information channel that connects the dots and closes the loops.&lt;/p&gt;
    &lt;p&gt;Now, let’s move from the theoretical loops of value generation to the actual feedback loops people are using today to develop new software and operate the software they already have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Actual developer feedback loops&lt;/head&gt;
    &lt;p&gt;Software developers typically spend most of the day in their development environment. They build stuff, they run tests, they build more stuff, they run more tests. (Or they conduct lengthy, increasingly intimate conversations with Cursor or Claude about how agents should do these things on their behalf, but for simplicity’s sake, let’s use the classical terms.)&lt;/p&gt;
    &lt;p&gt;Build-&amp;gt; test -&amp;gt; learn, build -&amp;gt; test -&amp;gt; learn. These are the actual feedback loops that drive a developer’s daily labor. When we’re ready to merge, we may get code review first.&lt;/p&gt;
    &lt;p&gt;If all tests pass, and our buddy approves, we merge! Joyous day. On to the next unit of work.&lt;/p&gt;
    &lt;p&gt;What did we “learn” by running tests? We learned that our tests pass. That’s all.&lt;/p&gt;
    &lt;p&gt;Tests are great, but from the business perspective, we don’t learn anything new by running tests. All of this work is important, but you don’t learn anything. You can’t learn anything until you deploy to production. Hold that thought.&lt;/p&gt;
    &lt;p&gt;Next, let’s look at the feedback loops involving production.&lt;/p&gt;
    &lt;head rend="h2"&gt;Actual production feedback loops&lt;/head&gt;
    &lt;p&gt;Most developers don’t interact with production on a daily basis, unless they’re hunting down a bug or something. Guess who does? Your operations crew—or as they are more likely to be called, cloud engineering, infrastructure, SREs, DevOps, or platform engineering.&lt;/p&gt;
    &lt;p&gt;Whatever you call them, somebody has to deal with operational feedback loops. They are the last line of defense for your system in the face of perpetual threats. In soccer terms, they are the goalie.&lt;/p&gt;
    &lt;p&gt;Operational feedback loops get kicked off any time someone gets paged (or a customer complains loudly enough to trigger an escalation). Day or night, rain or shine, someone hops on to production to investigate, triage, and fix the problem.&lt;/p&gt;
    &lt;p&gt;Operational feedback loops are always reactive. Sometimes you can tell what just changed (deploy? migration?), but often, you cannot. An unusual traffic pattern, a new client version, a database bug, a bug from two years ago just reached a tipping point… the possibilities are endless.&lt;/p&gt;
    &lt;p&gt;It might look more like this:&lt;/p&gt;
    &lt;p&gt;Or this:&lt;/p&gt;
    &lt;p&gt;The dirty little secret of infrastructure is how often things happen that we just don’t understand. Some of it can perhaps be handwaved away as emergent properties of complex systems, but a great deal more of it is due to how long, laggy, and lossy these feedback loops are.&lt;/p&gt;
    &lt;head rend="h2"&gt;Both of these feedback loops are vital&lt;/head&gt;
    &lt;p&gt;This might be a good time for me to pause and underline that both feedback loops are necessary. One is not better than the other one. We need both. As Stephen Jay Gould might say, these are non-overlapping magisteria. (Uh, except actually they do overlap, a lot.)&lt;/p&gt;
    &lt;p&gt;I want to be super clear on this point, because in this industry we have a tendency to throw rocks at each other and be like, “why are you stupid?”&lt;/p&gt;
    &lt;p&gt;“Stupid ops people, why don’t you just alert any time something changes in production, so we can learn from it?”&lt;/p&gt;
    &lt;p&gt;“Stupid developers, why don’t you just LOOK at your graphs after every deploy?”&lt;/p&gt;
    &lt;p&gt;I actually feel pretty bad about this, because I think I’ve been a key driver of the latter narrative. I’ve lost track of how many times I’ve told people to “put your developers on call!” to make them pay more attention to production. Mind you, I’m not necessarily saying this is a bad idea, nor is it necessarily a good idea. I’m saying that it doesn’t solve the problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ops and dev have different perspectives&lt;/head&gt;
    &lt;p&gt;The problem is that these are different domains, and they have fundamentally different perspectives. This doesn’t necessarily mean they need different tools (recall what I said about all data tools being fungible), but they are worth considering with equal weight.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ops perspective&lt;/head&gt;
    &lt;p&gt;Here we have the basic ops/dev contract, in its simplest form. Ops (or platform, or whatever) provides a place for devs to run their code, their queries, etc. Devs write the code that runs on it.&lt;/p&gt;
    &lt;p&gt;If we zoom out a little and simplify by a factor of ten million or so, it looks like this. The ops/platform/SRE mandate is to provide a stable, reliable place for lines of code to execute.&lt;/p&gt;
    &lt;p&gt;To do this, they collect a lot of telemetry from the perspective of the system and each of its constituent parts: disks, pods, network devices, databases, and so on. Most of this is third-party code, so you can’t change it; you just have to swallow whatever metrics or logs they sent you.&lt;/p&gt;
    &lt;head rend="h3"&gt;The dev perspective&lt;/head&gt;
    &lt;p&gt;What the devs care about is the ability to understand the product experience from the perspective of each customer. In practice, this can mean any combination or permutation of agent, user, mobile device type, laptop, desktop, point of sale device, and so on.&lt;/p&gt;
    &lt;p&gt;They also need to be able to slice and dice and combine this with any combination of build IDs, commit tags, feature flags, container pods, and anything else being collected by the application telemetry.&lt;/p&gt;
    &lt;p&gt;Devs can’t physically access every phone, laptop, and point of sale device in the world. But if they use the right tools, they can stream that telemetry back to the application in a format that preserves their ability to explore and ask open-ended, exploratory questions from the system side.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ops and dev have different concerns&lt;/head&gt;
    &lt;p&gt;Ops and dev also have different concerns.&lt;/p&gt;
    &lt;p&gt;Operational feedback loops exist to guard the system and its components against catastrophic threats. If it isn’t failing, broken, buggy, slow, etc., ops mostly don’t care; not their domain.&lt;/p&gt;
    &lt;p&gt;Devs, on the other hand, very much do care about things beyond bugs and catastrophes. The developer’s job is to create new value for the business. Build products, implement features, run experiments. Try something new, see if users run with it.&lt;/p&gt;
    &lt;p&gt;Think of it this way. Ops is the building inspector, dev is the architect. The inspector only shows up to look for code violations, structural problems, safety hazards. The architect spends most of their time imagining what could be built, how people might use the space, what will delight them. They both care about safety, but the inspector’s entire job is about managing risk while the architect’s job is possibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;“How do I get my developers to go look at it?”&lt;/head&gt;
    &lt;p&gt;I spent some time chatting with folks at LDX Berlin last November. It struck me afterwards that over half of the questions I heard—from staff+ engineers, directors, and execs—were some variation on a single theme: “But how do I get my developers to go look at it?” [their dashboards].&lt;/p&gt;
    &lt;p&gt;This might be the first time I ever truly thought through just how frustrating and confusing this has been for developers, from start to finish.&lt;/p&gt;
    &lt;head rend="h3"&gt;Instrumenting your code using ops tools: not easy&lt;/head&gt;
    &lt;p&gt;Think about it. You and your buddy Claude are building a new checkout feature together, and you want to capture a few valuable bits of telemetry, let’s say &lt;code&gt;user_name&lt;/code&gt; and &lt;code&gt;order_total&lt;/code&gt;. Where do they go?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Should these go in metrics?&lt;/item&gt;
      &lt;item&gt;Should they go in logs?&lt;/item&gt;
      &lt;item&gt;Should they be part of a trace? A span?&lt;/item&gt;
      &lt;item&gt;What if you want to see it alongside errors when checkout fails?&lt;/item&gt;
      &lt;item&gt;What about profiling data when checkout is slow?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Buckle in, we are just getting started. If it’s a metric, should it be a:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Counter?&lt;/item&gt;
      &lt;item&gt;Gauge?&lt;/item&gt;
      &lt;item&gt;Histogram?&lt;/item&gt;
      &lt;item&gt;Summary?&lt;/item&gt;
      &lt;item&gt;Rate?&lt;/item&gt;
      &lt;item&gt;Distribution?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If it’s a counter, when does it reset? If it’s a histogram or a summary, what are the bucket boundaries? Do I need to tag anything? Does cardinality matter, for the data and/or the tags? Am I supposed to worry about cost? Is there a naming convention?&lt;/p&gt;
    &lt;p&gt;Let’s say you figure out the metric part. Now, do they also need to go into logs or traces? All of the signal types? Should I append them to an existing log line (which one?), or make a new one? Do they need indexes? Can I index them? Is there a schema?&lt;/p&gt;
    &lt;p&gt;We could keep asking questions for another five pages or so. I don’t know that I ever truly considered just how much domain knowledge we presume when we tell developers to do this. All that time, fear and decision fatigue… it adds up.&lt;/p&gt;
    &lt;p&gt;At least it’s a one time expense and then you have it, right? In the end, it’s all worth it?&lt;/p&gt;
    &lt;p&gt;Ah. Right. About that…&lt;/p&gt;
    &lt;head rend="h3"&gt;Finding your telemetry using ops tools: also not easy&lt;/head&gt;
    &lt;p&gt;To look at your new instrumentation, you probably just need to wait for your code to get deployed, then find the right tool (or tools) for the telemetry you added, and create a new dashboard using those attributes. I’m going to fast forward through all this because it’s extremely vendor-specific and the specifics don’t matter much.&lt;/p&gt;
    &lt;p&gt;I want to get back to the question people were asking me in Berlin: how do you get your developers to go look at their telemetry?&lt;/p&gt;
    &lt;p&gt;The answer is: you don’t.&lt;/p&gt;
    &lt;p&gt;We have the technology now. We bring the telemetry to them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bring the telemetry to developers&lt;/head&gt;
    &lt;p&gt;No matter how much time we have spent yelling at them over the years, most developers really don’t want to leave their development environment (for anything other than Slack).&lt;/p&gt;
    &lt;p&gt;What if they were right all along?&lt;/p&gt;
    &lt;p&gt;Here, watch this demo. It’s only 3:37 long, and it shows Jessitron demoing some of the AI capabilities we released last September. Jump ahead to 2:21 if you’re impatient and want to see it in your development environment.&lt;/p&gt;
    &lt;p&gt;As it turns out, chat is kind of the perfect interface for interrogating your software and finding out how it’s doing in production.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bring them telemetry they can use&lt;/head&gt;
    &lt;p&gt;The cognitive overhead of three pillars instrumentation is one reason we historically struggled to get developers to use ops tools. The other is simpler: the tools weren’t worth it.&lt;/p&gt;
    &lt;p&gt;Imagine you did everything ops was asking you to do—you doubled, tripled the time it took you to ship your business logic in order to get the right instrumentation included. You shipped your changes, then went to explore what impact your changes were having on how users experienced the product. And finally, after all the wrestling, and waiting, and clicking around, you got…aggregates? Histograms? Fucking buckets?&lt;/p&gt;
    &lt;p&gt;If you’re trying to use tools to ask exploratory, open-ended questions about the quality of your product as experienced by your users, you are squarely in “running production off an Excel spreadsheet” territory.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI has changed the instrumentation game&lt;/head&gt;
    &lt;p&gt;In the past, instrumentation was laborious and required expertise in multiple domains, while auto-instrumentation generally alternated between sucking by doing too much and sucking by doing too little.&lt;/p&gt;
    &lt;p&gt;OpenTelemetry changed things by standardizing the way we instrument code, making instrumentation patterns consistent and well-documented, with a massive corpus of examples. LLMs have been trained on these, so they can follow instructions on how to implement it or add it to software projects.&lt;/p&gt;
    &lt;p&gt;The cost of instrumentation has effectively fallen to zero, and AI models and agents are able to understand important patterns in your code and generalize from the patterns they know how to instrument.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI has changed the analysis game&lt;/head&gt;
    &lt;p&gt;And if your agentic harness can run your code in dev, inspect the output, verify that it’s working as expected, etc., then by the time you’re ready to merge your changes to production, you’ve already validated that the instrumentation can explain itself back to you from production.&lt;/p&gt;
    &lt;p&gt;Automatic, end-to-end feedback loops are the key. Instead of spending hours poring through traces, looking for outliers and suspicious symptoms… let your little buddy do that. Instead of putting your IDE on the left side of the screen, and a long, detailed trace on the right side of the screen, and stepping line by line, span by span through the trace… let your buddy do that, too.&lt;/p&gt;
    &lt;p&gt;It is not impossible to get what you need out of ops tools and legacy models. Plenty of engineers and teams have done so. That doesn’t make it a great solution, the right use of resources, or accessible to the median engineering org.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI has changed the need for validation&lt;/head&gt;
    &lt;p&gt;The shape of the future is becoming clear. Developers will spend less time writing lines of code, and more time writing specs, thinking about the problem space, running experiments, and validating what they’ve built.&lt;/p&gt;
    &lt;p&gt;Instead of: write code -&amp;gt; test -&amp;gt; code review -&amp;gt; merge -&amp;gt; “hope it works!”&lt;/p&gt;
    &lt;p&gt;It becomes: write code (with AI) -&amp;gt; deploy -&amp;gt; observe and validate -&amp;gt; learn -&amp;gt; iterate.&lt;/p&gt;
    &lt;p&gt;See what happens? You are literally feeding what you learned from each change right back into the product in your next change. Shipping swiftly, with confidence, at the speed of AI.&lt;/p&gt;
    &lt;p&gt;The bottleneck shifts from, “How fast can I write code?” to, “How fast can I understand what’s happening and make good decisions about it?”&lt;/p&gt;
    &lt;p&gt;If AI makes code writing nearly free, then the ability to understand and validate what that code does in production becomes the primary constraint.&lt;/p&gt;
    &lt;p&gt;Engineers become more like scientists running experiments and interpreting results, less like typists translating specifications into syntax.&lt;/p&gt;
    &lt;head rend="h2"&gt;The freight train barrelling down the tracks at us&lt;/head&gt;
    &lt;p&gt;All this is tremendously exciting and great fun.&lt;/p&gt;
    &lt;p&gt;What’s mildly terrifying is that most companies, to this day, do all of their learning and observing about production via long, lossy, laggy operational feedback loops.&lt;/p&gt;
    &lt;p&gt;And most orgs are used to responding to a daytime alert by calling out, “Who just shipped that change?” assuming that whoever merged the diff surely understands how it works and can fix it post-haste.&lt;/p&gt;
    &lt;p&gt;What happens when nobody wrote the code you just deployed, and nobody really understands it?&lt;/p&gt;
    &lt;p&gt;I guess we’ll (all) find out. 😉&lt;/p&gt;
    &lt;head rend="h2"&gt;DevOps isn’t dead&lt;/head&gt;
    &lt;p&gt;The DevOps movement isn’t “dead.” It did an enormous amount of good in the world. It broke down silos, preached the value of empathy and collaboration, and reduced a ton of toil.&lt;/p&gt;
    &lt;p&gt;In retrospect, I’ve come to think that the entire effort was about trying to connect developers with the consequences of their code in production. We did not succeed, but it was hardly for lack of trying. We did the best we could with the tools we had.&lt;/p&gt;
    &lt;p&gt;And now we can do better.&lt;/p&gt;
    &lt;head rend="h1"&gt;A decade of observability hot takes and learning the hard way&lt;/head&gt;
    &lt;p&gt;Join the webinar, The Next Era of Observability: Founders' Reflections,&lt;/p&gt;
    &lt;p&gt;with Christine and Charity.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46661132</guid><pubDate>Sat, 17 Jan 2026 19:21:38 +0000</pubDate></item><item><title>Show HN: Docker.how – Docker command cheat sheet</title><link>https://docker.how/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46661630</guid><pubDate>Sat, 17 Jan 2026 20:17:49 +0000</pubDate></item><item><title>A programming language based on grammatical cases of Turkish</title><link>https://github.com/kip-dili/kip</link><description>&lt;doc fingerprint="4a813f1d0f75a8ca"&gt;
  &lt;main&gt;
    &lt;p&gt;Kip (meaning "grammatical mood" in Turkish) is an experimental programming language that uses Turkish grammatical cases as part of its type system. It demonstrates how natural language morphology—specifically Turkish noun cases and vowel harmony—can be integrated into programming language design.&lt;/p&gt;
    &lt;p&gt;This is a research/educational project exploring the intersection of linguistics and type theory, not a production programming language.&lt;/p&gt;
    &lt;p&gt;There is also a tutorial in Turkish and a tutorial in English that explains how to write Kip programs.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Kip is experimental. Expect changes in syntax and behavior over time.&lt;/p&gt;
    &lt;p&gt;For you to get a taste of what Kip looks like, here is an example program that prompts the user to enter a number and then prints that many of the Fibonacci numbers:&lt;/p&gt;
    &lt;code&gt;(* İlk n Fibonacci sayısını yazdırır. *)
(bu tam-sayıyı) (şu tam-sayıyı) (o tam-sayıyı) işlemek,
  (onla 0'ın eşitliği) doğruysa,
    durmaktır,
  yanlışsa,
    bunu yazıp,
    şunu (bunla şunun toplamını) (onla 1'in farkını) işlemektir.

çalıştırmak,
  "Bir sayı girin:" yazıp,
  isim olarak okuyup,
  ((ismin tam-sayı-hali)
    yokluksa,
      "Geçersiz sayı." yazmaktır,
    n'nin varlığıysa,
      0'ı 1'i n'yi işlemektir).

çalıştır.
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Language Features&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Example Program&lt;/item&gt;
      &lt;item&gt;WASM Playground&lt;/item&gt;
      &lt;item&gt;Bytecode Cache&lt;/item&gt;
      &lt;item&gt;Project Structure&lt;/item&gt;
      &lt;item&gt;Testing&lt;/item&gt;
      &lt;item&gt;Morphological Analysis&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Kip uses Turkish noun cases (ismin halleri) to determine argument relationships in function calls:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Case&lt;/cell&gt;
        &lt;cell role="head"&gt;Turkish Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Suffix&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nominative&lt;/cell&gt;
        &lt;cell&gt;Yalın hal&lt;/cell&gt;
        &lt;cell&gt;(none)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sıfır&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Accusative&lt;/cell&gt;
        &lt;cell&gt;-i hali&lt;/cell&gt;
        &lt;cell&gt;-i, -ı, -u, -ü&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayıyı&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Dative&lt;/cell&gt;
        &lt;cell&gt;-e hali&lt;/cell&gt;
        &lt;cell&gt;-e, -a&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayıya&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Locative&lt;/cell&gt;
        &lt;cell&gt;-de hali&lt;/cell&gt;
        &lt;cell&gt;-de, -da, -te, -ta&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;listede&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Ablative&lt;/cell&gt;
        &lt;cell&gt;-den hali&lt;/cell&gt;
        &lt;cell&gt;-den, -dan, -ten, -tan&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;listeden&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Genitive&lt;/cell&gt;
        &lt;cell&gt;Tamlayan eki&lt;/cell&gt;
        &lt;cell&gt;-in, -ın, -un, -ün&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayının&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Instrumental&lt;/cell&gt;
        &lt;cell&gt;-le eki&lt;/cell&gt;
        &lt;cell&gt;-le, -la, ile&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sayıyla&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Possessive (3s)&lt;/cell&gt;
        &lt;cell&gt;Tamlanan eki&lt;/cell&gt;
        &lt;cell&gt;-i, -ı, -u, -ü, -si, -sı&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ardılı&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Because Turkish cases mark grammatical relationships explicitly, Kip allows flexible argument ordering. These two calls are equivalent:&lt;/p&gt;
    &lt;code&gt;(5'le 3'ün farkını) yaz.
(3'ün 5'le farkını) yaz.
&lt;/code&gt;
    &lt;p&gt;As long as arguments have different case suffixes or different types, Kip can determine which argument is which.&lt;/p&gt;
    &lt;p&gt;Define algebraic data types with Turkish syntax:&lt;/p&gt;
    &lt;code&gt;Bir doğruluk ya doğru ya da yanlış olabilir.

Bir doğal-sayı
ya sıfır
ya da bir doğal-sayının ardılı
olabilir.
&lt;/code&gt;
    &lt;p&gt;Type variables are supported for generic data structures:&lt;/p&gt;
    &lt;code&gt;Bir (öğe listesi)
ya boş
ya da bir öğenin bir öğe listesine eki
olabilir.
&lt;/code&gt;
    &lt;p&gt;Pattern match using the conditional suffix &lt;code&gt;-sa/-se&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;(bu doğruluğun) tersi,
  bu doğruysa, yanlış,
  yanlışsa, doğrudur.
&lt;/code&gt;
    &lt;p&gt;Supports nested pattern matching, binders, and wildcard patterns (&lt;code&gt;değilse&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;(bu doğal-sayının) kopyası,
  bu sıfırsa, sıfır,
  öncülün ardılıysa, öncülün ardılıdır.
&lt;/code&gt;
    &lt;p&gt;Define named constants with &lt;code&gt;diyelim&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sıfırın ardılına bir diyelim.
birin ardılına iki diyelim.
&lt;/code&gt;
    &lt;p&gt;Sequencing with &lt;code&gt;-ip/-ıp/-up/-üp&lt;/code&gt; suffixes and binding with &lt;code&gt;olarak&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;selamlamak,
  isim olarak okuyup,
  ("Merhaba "yla ismin birleşimini) yazmaktır.
&lt;/code&gt;
    &lt;p&gt;Integers (&lt;code&gt;tam-sayı&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arithmetic: &lt;code&gt;toplamı&lt;/code&gt;,&lt;code&gt;farkı&lt;/code&gt;,&lt;code&gt;çarpımı&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Comparison: &lt;code&gt;eşitliği&lt;/code&gt;,&lt;code&gt;küçüklüğü&lt;/code&gt;,&lt;code&gt;büyüklüğü&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Other: &lt;code&gt;öncülü&lt;/code&gt;,&lt;code&gt;sıfırlığı&lt;/code&gt;,&lt;code&gt;faktöriyeli&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Strings (&lt;code&gt;dizge&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;uzunluğu&lt;/code&gt;- length&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;birleşimi&lt;/code&gt;- concatenation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tam-sayı-hali&lt;/code&gt;- parse as integer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I/O:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;yazmak&lt;/code&gt;/&lt;code&gt;yaz&lt;/code&gt;- print to stdout&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;okumak&lt;/code&gt;/&lt;code&gt;oku&lt;/code&gt;- read from stdin&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;(* This is a comment *)
&lt;/code&gt;
    &lt;code&gt;5'i yaz.              (* Integer literal with case suffix *)
"merhaba"'yı yaz.     (* String literal with case suffix *)
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Foma - finite-state morphology toolkit&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;macOS: &lt;code&gt;brew install foma&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Debian/Ubuntu: &lt;code&gt;apt install foma libfoma-dev&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Fedora: &lt;code&gt;dnf install foma foma-devel&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;macOS: &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stack - Haskell build tool&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;See haskellstack.org&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;If you only want to explore the language, you can start with &lt;code&gt;stack exec kip&lt;/code&gt; after a successful build.&lt;/p&gt;
    &lt;p&gt;Clone this repository, then:&lt;/p&gt;
    &lt;code&gt;# Quick install (macOS/Linux)
chmod +x install.sh
./install.sh

# Or manual build
stack build&lt;/code&gt;
    &lt;p&gt;The TRmorph transducer is bundled at &lt;code&gt;vendor/trmorph.fst&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Start REPL
stack exec kip

# Execute a file
stack exec kip -- --exec path/to/file.kip

# Install to PATH
stack install&lt;/code&gt;
    &lt;p&gt;A browser playground build is available under &lt;code&gt;playground/&lt;/code&gt;. It compiles the
non-interactive runner (&lt;code&gt;kip-playground&lt;/code&gt;) to &lt;code&gt;wasm32-wasi&lt;/code&gt; and ships a small
HTML/JS harness that runs Kip in the browser.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;playground/README.md&lt;/code&gt; for prerequisites, toolchain setup, and build steps.&lt;/p&gt;
    &lt;p&gt;Kip stores a cached, type-checked version of each &lt;code&gt;.kip&lt;/code&gt; file in a sibling &lt;code&gt;.iz&lt;/code&gt; file. When you run a file again, Kip will reuse the &lt;code&gt;.iz&lt;/code&gt; cache if both the source and its loaded dependencies are unchanged.&lt;/p&gt;
    &lt;p&gt;If you want to force a fresh parse and type-check, delete the &lt;code&gt;.iz&lt;/code&gt; file next to the source.&lt;/p&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;.iz&lt;/code&gt; files include a compiler hash. If the compiler changes, the cache is invalidated automatically.&lt;/p&gt;
    &lt;code&gt;(* Natural numbers *)
Bir doğal-sayı
ya sıfır
ya da bir doğal-sayının ardılı
olabilir.

(* Define some constants *)
sıfırın ardılına bir diyelim.
birin ardılına iki diyelim.
ikinin ardılına üç diyelim.

(* Addition function *)
(bu doğal-sayıyla) (şu doğal-sayının) toplamı,
  bu sıfırsa,
    şu,
  öncülün ardılıysa,
    (öncülle) (şunun ardılının) toplamıdır.

(* Print result *)
(ikiyle üçün toplamını) yaz.
&lt;/code&gt;
    &lt;code&gt;app/
└── Main.hs            - CLI entry point

src/
├── Kip/
│   ├── AST.hs         - Abstract syntax tree
│   ├── Cache.hs       - .iz cache handling
│   ├── Eval.hs        - Interpreter
│   ├── Parser.hs      - Parser
│   ├── Render.hs      - Pretty-printing with morphological inflection
│   └── TypeCheck.hs   - Type checker validating grammatical case usage
└── Language/
    └── Foma.hs        - Haskell bindings to Foma via FFI

lib/
├── giriş.kip          - Prelude module loaded by default
├── temel.kip           - Core types
├── temel-doğruluk.kip  - Boolean functions
├── temel-dizge.kip     - String functions
├── temel-etki.kip      - I/O primitives
├── temel-liste.kip     - List functions
└── temel-tam-sayı.kip  - Integer functions

tests/
├── succeed/            - Passing golden tests (.kip + .out + optional .in)
└── fail/               - Failing golden tests (.kip + .err)

vendor/
└── trmorph.fst        - TRmorph transducer
&lt;/code&gt;
    &lt;code&gt;stack test&lt;/code&gt;
    &lt;p&gt;Tests are in &lt;code&gt;tests/succeed/&lt;/code&gt; (expected to pass) and &lt;code&gt;tests/fail/&lt;/code&gt; (expected to fail).&lt;/p&gt;
    &lt;p&gt;Kip uses TRmorph for Turkish morphological analysis. When a word has multiple possible parses (e.g., "takası" could be "taka + possessive" or "takas + accusative"), Kip carries all candidates through parsing and resolves ambiguity during type checking.&lt;/p&gt;
    &lt;p&gt;For intentionally ambiguous words, use an apostrophe to force a specific parse: &lt;code&gt;taka'sı&lt;/code&gt; vs &lt;code&gt;takas'ı&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;See LICENSE file.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46661897</guid><pubDate>Sat, 17 Jan 2026 20:44:52 +0000</pubDate></item><item><title>The Death of Software Development</title><link>https://mike.tech/blog/death-of-software-development</link><description>&lt;doc fingerprint="3e5d513f5f7530fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Death of Software Development&lt;/head&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;I’m Michael Arnaldi, Founder and CEO of Effectful Technologies — the company behind Effect, the TypeScript library for building production-grade systems. I’ve been programming most of my life. I started at 11 with the goal of cracking video games. Since then, I’ve written code at every level: from kernel development to the highest abstractions in TypeScript.&lt;/p&gt;
    &lt;p&gt;Programming has been my life. And now it’s gone for good.&lt;/p&gt;
    &lt;p&gt;Let me explain.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Ralph Wiggum Moment&lt;/head&gt;
    &lt;p&gt;The broader community is only now waking up to the power of AI. Last week, Ralph Wiggum went viral. Ralph is a technique that prompts agents like Claude Code in a deterministic loop, iteratively building large systems from lists of small tasks. Twitter exploded.&lt;/p&gt;
    &lt;p&gt;The name comes from Geoffrey Huntley, a great engineer and a good friend who’s been exploring the extremes of AI for a long time.&lt;/p&gt;
    &lt;p&gt;But here’s the thing: what people don’t realize is that Ralph is just the beginning. The “AI power users” — the ones who’ve been living in this world for months — are already working with far more refined techniques. And they’re not just building simple things. They’re cloning entire companies in hours.&lt;/p&gt;
    &lt;head rend="h2"&gt;Missing the Point&lt;/head&gt;
    &lt;p&gt;The average software developer is not even close to understanding the extent of this change.&lt;/p&gt;
    &lt;p&gt;They’re obsessed with picking “the best model” — endlessly debating whether Claude is better than GPT, whether Gemini is catching up, whether open-source models can compete. They’re missing the point entirely.&lt;/p&gt;
    &lt;p&gt;The outcome is defined by the process, not the model. The model is just one piece of the puzzle. Think of it like traditional software development: not every developer is exceptional, but a team of good enough developers with the right process can build great software. The same principle applies here. A good enough model with the right process will beat a better model with no process — every single time.&lt;/p&gt;
    &lt;p&gt;Here’s the uncomfortable truth: the state of the art is not public knowledge. Power users are keeping their techniques to themselves — because sharing them is scary. The implications are too big, too disruptive. We’ll get there eventually, but not yet.&lt;/p&gt;
    &lt;p&gt;Tools like Ralph are a good start, but they’re fundamentally limited. If you think a model can decide by itself when a task is done and emit a token deterministically, you’re not even close to understanding what’s possible. That’s just scratching the surface.&lt;/p&gt;
    &lt;p&gt;In the next two years, you’ll hear more and more about things like Lean and TLA+. You’ll watch the industry evolve from “Coding Agents” to “Agentic Infrastructure for Coding.” The shift will be profound.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Real Example&lt;/head&gt;
    &lt;p&gt;Let me give you a concrete example.&lt;/p&gt;
    &lt;p&gt;I’ve always been passionate about finance. I used to be a regulated person — an executive director at a firm that created derivative products. Since moving on from that world, I still get the occasional urge to check macroeconomic data and dig into market dynamics.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, I decided to analyze Polymarket. I wanted to spot insider trading, whale activity, derive volatility — the kind of stuff only a finance nerd would care about.&lt;/p&gt;
    &lt;p&gt;In finance, there’s one tool that everybody serious ends up using: the Bloomberg Terminal. It’s so ubiquitous that terminals per capita is actually used as a metric for financial activity in different countries. And for that metric, the winner is New York! Nope — it’s actually Vatican City, with 1.9% of people having a terminal. Almost 4x Luxembourg.&lt;/p&gt;
    &lt;p&gt;There’s only one problem with the Bloomberg Terminal — it’s fucking expensive. And old. And clunky.&lt;/p&gt;
    &lt;p&gt;So I decided to “Ralph my way” and build a modern Bloomberg Terminal for Polymarket.&lt;/p&gt;
    &lt;p&gt;It took me 2 hours. I wrote 0 lines of code. I reviewed 0 lines of code.&lt;/p&gt;
    &lt;p&gt;Obviously, I didn’t clone the full Bloomberg Terminal — I built the subset I needed for Polymarket analysis. But here’s the thing: cloning the full terminal would probably take a day or two of token usage. Not months. Not years. Days.&lt;/p&gt;
    &lt;head rend="h2"&gt;Proving It&lt;/head&gt;
    &lt;p&gt;I know what you’re thinking: “This sounds like bullshit.” Fair enough. So I’m working on an open source project to prove it isn’t.&lt;/p&gt;
    &lt;p&gt;For my personal needs, I need an accounting application that manages multiple companies across multiple jurisdictions and currencies, with consolidated reporting — all respecting US GAAP standards. The kind of thing that would normally require a team and months of development. Instead, I’m Ralphing my way to a full-blown application in my weekend time. You can follow along at Accountability.&lt;/p&gt;
    &lt;p&gt;Here’s the catch: I’m deliberately not using state-of-the-art tooling. I’m building everything in the open, from first principles, to prove that this works. No secret sauce. No proprietary techniques. Just the basics, applied correctly.&lt;/p&gt;
    &lt;p&gt;Once it’s done, I’ll write a full article explaining how I did it — the process, the issues I found, and everything I learned along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Is Software Development?&lt;/head&gt;
    &lt;p&gt;Stop and think about that for a moment.&lt;/p&gt;
    &lt;p&gt;If an idiot like me can clone a product that costs $30k per month — in two hours — what even is software development?&lt;/p&gt;
    &lt;p&gt;It’s an interesting question. Software development used to be a craft. Something only a few people could do well. It required years of practice, deep knowledge, and hard-won experience. Now anyone can be a software developer.&lt;/p&gt;
    &lt;p&gt;A friend of mine with a legal background and almost no coding experience built a full-blown compliance solution to check privacy policies against the GDPR. He didn’t use tools like Lovable — he hacked it by chatting with Claude Code, using Effect and Next.js. I checked the code. It’s good.&lt;/p&gt;
    &lt;p&gt;The new software developer is no longer a craftsman. It’s the average operator, empowered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Software Engineering Is Alive and Well&lt;/head&gt;
    &lt;p&gt;But here’s the distinction that matters: while software development as we know it is dead, software engineering is alive and well.&lt;/p&gt;
    &lt;p&gt;The role has transformed. Engineers are no longer writing software — they’re designing higher-order systems. They’ve moved from crafting code to designing systems that write code. They build techniques. They build skills. They develop the mental models and architectural intuitions that guide AI toward good solutions. They can adopt new technologies in minutes — in the worst case, hours.&lt;/p&gt;
    &lt;p&gt;This new reality requires rethinking everything. Forty years of best practices are now outdated. The patterns we relied on, the team structures we built, the processes we followed — all of it needs to be reconsidered. Individuals are far more powerful than before. A single person with the right skills can now do what used to require an entire team. Engineering teams, as we’ve known them, are becoming unnecessary.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Economic Reality&lt;/head&gt;
    &lt;p&gt;We are living through the Industrial Revolution of Software.&lt;/p&gt;
    &lt;p&gt;We’re moving from a world where software is scarce to one where it’s abundant and cheap. Just like the original Industrial Revolution transformed manufacturing — making goods that were once handcrafted luxuries into mass-produced commodities — AI is doing the same to software.&lt;/p&gt;
    &lt;p&gt;The economic implications are drastic — and poorly understood.&lt;/p&gt;
    &lt;p&gt;That will be the topic of a series of future posts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46661936</guid><pubDate>Sat, 17 Jan 2026 20:49:15 +0000</pubDate></item><item><title>Show HN: ChunkHound, a local-first tool for understanding large codebases</title><link>https://github.com/chunkhound/chunkhound</link><description>&lt;doc fingerprint="18c0feb517868f1a"&gt;
  &lt;main&gt;
    &lt;p&gt;Local first codebase intelligence&lt;/p&gt;
    &lt;p&gt;Your AI assistant searches code but doesn't understand it. ChunkHound researches your codebase—extracting architecture, patterns, and institutional knowledge at any scale. Integrates via MCP.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;cAST Algorithm - Research-backed semantic code chunking&lt;/item&gt;
      &lt;item&gt;Multi-Hop Semantic Search - Discovers interconnected code relationships beyond direct matches&lt;/item&gt;
      &lt;item&gt;Semantic search - Natural language queries like "find authentication code"&lt;/item&gt;
      &lt;item&gt;Regex search - Pattern matching without API keys&lt;/item&gt;
      &lt;item&gt;Local-first - Your code stays on your machine&lt;/item&gt;
      &lt;item&gt;30 languages with structured parsing &lt;list rend="ul"&gt;&lt;item&gt;Programming (via Tree-sitter): Python, JavaScript, TypeScript, JSX, TSX, Java, Kotlin, Groovy, C, C++, C#, Go, Rust, Haskell, Swift, Bash, MATLAB, Makefile, Objective-C, PHP, Vue, Svelte, Zig&lt;/item&gt;&lt;item&gt;Configuration: JSON, YAML, TOML, HCL, Markdown&lt;/item&gt;&lt;item&gt;Text-based (custom parsers): Text files, PDF&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;MCP integration - Works with Claude, VS Code, Cursor, Windsurf, Zed, etc&lt;/item&gt;
      &lt;item&gt;Real-time indexing - Automatic file watching, smart diffs, seamless branch switching&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Visit chunkhound.github.io for complete guides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.10+&lt;/item&gt;
      &lt;item&gt;uv package manager&lt;/item&gt;
      &lt;item&gt;API keys (optional - regex search works without any keys) &lt;list rend="ul"&gt;&lt;item&gt;Embeddings: VoyageAI (recommended) | OpenAI | Local with Ollama&lt;/item&gt;&lt;item&gt;LLM (for Code Research): Claude Code CLI or Codex CLI (no API key needed) | Anthropic | OpenAI&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Install uv if needed
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install ChunkHound
uv tool install chunkhound&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create &lt;code&gt;.chunkhound.json&lt;/code&gt;in project root&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "embedding": {
    "provider": "voyageai",
    "api_key": "your-voyageai-key"
  },
  "llm": {
    "provider": "claude-code-cli"
  }
}&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note: Use&lt;/p&gt;&lt;code&gt;"codex-cli"&lt;/code&gt;instead if you prefer Codex. Both work equally well and require no API key.&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Index your codebase&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;chunkhound index&lt;/code&gt;
    &lt;p&gt;For configuration, IDE setup, and advanced usage, see the documentation.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;Capability&lt;/cell&gt;
        &lt;cell role="head"&gt;Scale&lt;/cell&gt;
        &lt;cell role="head"&gt;Maintenance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Keyword Search&lt;/cell&gt;
        &lt;cell&gt;Exact matching&lt;/cell&gt;
        &lt;cell&gt;Fast&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Traditional RAG&lt;/cell&gt;
        &lt;cell&gt;Semantic search&lt;/cell&gt;
        &lt;cell&gt;Scales&lt;/cell&gt;
        &lt;cell&gt;Re-index files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Knowledge Graphs&lt;/cell&gt;
        &lt;cell&gt;Relationship queries&lt;/cell&gt;
        &lt;cell&gt;Expensive&lt;/cell&gt;
        &lt;cell&gt;Continuous sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChunkHound&lt;/cell&gt;
        &lt;cell&gt;Semantic + Regex + Code Research&lt;/cell&gt;
        &lt;cell&gt;Automatic&lt;/cell&gt;
        &lt;cell&gt;Incremental + realtime&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Ideal for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large monorepos with cross-team dependencies&lt;/item&gt;
      &lt;item&gt;Security-sensitive codebases (local-only, no cloud)&lt;/item&gt;
      &lt;item&gt;Multi-language projects needing consistent search&lt;/item&gt;
      &lt;item&gt;Offline/air-gapped development environments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stop recreating code. Start with deep understanding.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46662078</guid><pubDate>Sat, 17 Jan 2026 21:03:52 +0000</pubDate></item><item><title>Congress Wants to Hand Your Parenting to Big Tech</title><link>https://www.eff.org/deeplinks/2026/01/congress-wants-hand-your-parenting-big-tech</link><description>&lt;doc fingerprint="bca4d11f28e68dc1"&gt;
  &lt;main&gt;
    &lt;p&gt;Lawmakers in Washington are once again focusing on kids, screens, and mental health. But according to Congress, Big Tech is somehow both the problem and the solution. The Senate Commerce Committee held a hearing today on “examining the effect of technology on America’s youth.” Witnesses warned about “addictive” online content, mental health, and kids spending too much time buried in screen. At the center of the debate is a bill from Sens. Ted Cruz (R-TX) and Brian Schatz (D-HI) called the Kids Off Social Media Act (KOSMA), which they say will protect children and “empower parents.”&lt;/p&gt;
    &lt;p&gt;That’s a reasonable goal, especially at a time when many parents feel overwhelmed and nervous about how much time their kids spend on screens. But while the bill’s press release contains soothing language, KOSMA doesn’t actually give parents more control.&lt;/p&gt;
    &lt;p&gt;Instead of respecting how most parents guide their kids towards healthy and educational content, KOSMA hands the control panel to Big Tech. That’s right—this bill would take power away from parents, and hand it over to the companies that lawmakers say are the problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kids Under 13 Are Already Banned From Social Media&lt;/head&gt;
    &lt;p&gt;One of the main promises of KOSMA is simple and dramatic: it would ban kids under 13 from social media. Based on the language of bill sponsors, one might think that’s a big change, and that today’s rules let kids wander freely into social media sites. But that’s not the case.&lt;/p&gt;
    &lt;p&gt;Every major platform already draws the same line: kids under 13 cannot have an account. Facebook, Instagram, TikTok, X, YouTube, Snapchat, Discord, Spotify, and even blogging platforms like WordPress all say essentially the same thing—if you’re under 13, you’re not allowed. That age line has been there for many years, mostly because of how online services comply with a federal privacy law called COPPA.&lt;/p&gt;
    &lt;p&gt;Of course, everyone knows many kids under 13 are on these sites anyways. The real question is how and why they get access.&lt;/p&gt;
    &lt;head rend="h3"&gt;Most Social Media Use By Younger Kids Is Family-Mediated&lt;/head&gt;
    &lt;p&gt;If lawmakers picture under-13 social media use as a bunch of kids lying about their age and sneaking onto apps behind their parents’ backs, they’ve got it wrong. Serious studies that have looked at this all find the opposite: most under-13 use is out in the open, with parents’ knowledge, and often with their direct help.&lt;/p&gt;
    &lt;p&gt;A large national study published last year in Academic Pediatrics found that 63.8% of under-13s have a social media account, but only 5.4% of them said they were keeping one secret from their parents. That means roughly 90% of kids under 13 who are on social media aren’t hiding it at all. Their parents know. (For kids aged thirteen and over, the “secret account” number is almost as low, at 6.9%.)&lt;/p&gt;
    &lt;p&gt;Earlier research in the U.S. found the same pattern. In a well-known study of Facebook use by 10-to-14-year-olds, researchers found that about 70% of parents said they actually helped create their child’s account, and between 82% and 95% knew the account existed. Again, this wasn’t kids sneaking around. It was families making a decision together.&lt;/p&gt;
    &lt;p&gt;A 2022 study by the UK’s media regulator Ofcom points in the same direction, finding that up to two-thirds of social media users below the age of thirteen had direct help from a parent or guardian getting onto the platform.&lt;/p&gt;
    &lt;p&gt;The typical under-13 social media user is not a sneaky kid. It’s a family making a decision together.&lt;/p&gt;
    &lt;head rend="h3"&gt;KOSMA Forces Platforms To Override Families&lt;/head&gt;
    &lt;p&gt;This bill doesn’t just set an age rule. It creates a legal duty for platforms to police families.&lt;/p&gt;
    &lt;p&gt;Section 103(b) of the bill is blunt: if a platform knows a user is under 13, it “shall terminate any existing account or profile” belonging to that user. And “knows” doesn’t just mean someone admits their age. The bill defines knowledge to include what is “fairly implied on the basis of objective circumstances”—in other words, what a reasonable person would conclude from how the account is being used. The reality of how services would comply with KOSMA is clear: rather than risk liability for how they should have known a user was under 13, they will require all users to prove their age to ensure that they block anyone under 13.&lt;/p&gt;
    &lt;p&gt;KOSMA contains no exceptions for parental consent, for family accounts, or for educational or supervised use. The vast majority of people policed by this bill won’t be kids sneaking around—it will be minors who are following their parents’ guidance, and the parents themselves.&lt;/p&gt;
    &lt;p&gt;Imagine a child using their parent’s YouTube account to watch science videos about how a volcano works. If they were to leave a comment saying, “Cool video—I’ll show this to my 6th grade teacher!” and YouTube becomes aware of the comment, the platform now has clear signals that a child is using that account. It doesn’t matter whether the parent gave permission. Under KOSMA, the company is legally required to act. To avoid violating KOSMA, it would likely lock, suspend, or terminate the account, or demand proof it belongs to an adult. That proof would likely mean asking for a scan of a government ID, biometric data, or some other form of intrusive verification, all to keep what is essentially a “family” account from being shut down.&lt;/p&gt;
    &lt;p&gt;Violations of KOSMA are enforced by the FTC and state attorneys general. That’s more than enough legal risk to make platforms err on the side of cutting people off.&lt;/p&gt;
    &lt;p&gt;Platforms have no way to remove “just the kid” from a shared account. Their tools are blunt: freeze it, verify it, or delete it. Which means that even when a parent has explicitly approved and supervised their child’s use, KOSMA forces Big Tech to override that family decision.&lt;/p&gt;
    &lt;head rend="h3"&gt;Your Family, Their Algorithms&lt;/head&gt;
    &lt;p&gt;KOSMA doesn’t appoint a neutral referee. Under the law, companies like Google (YouTube), Meta (Facebook and Instagram), TikTok, Spotify, X, and Discord will become the ones who decide whose account survives, whose account gets locked, who has to upload ID, and whose family loses access altogether. They won’t be doing this because they want to—but because Congress is threatening them with legal liability if they don’t.&lt;/p&gt;
    &lt;p&gt;These companies don’t know your family or your rules. They only know what their algorithms infer. Under KOSMA, those inferences carry the force of law. Rather than parents or teachers, decisions about who can be online, and for what purpose, will be made by corporate compliance teams and automated detection systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Families Lose&lt;/head&gt;
    &lt;p&gt;This debate isn’t really about TikTok trends or doomscrolling. It’s about all the ordinary, boring, parent-guided uses of the modern internet. It’s about a kid watching “How volcanoes work” on regular YouTube, instead of the stripped-down YouTube Kids. It’s about using a shared Spotify account to listen to music a parent already approves. It’s about piano lessons from a teacher who makes her living from YouTube ads.&lt;/p&gt;
    &lt;p&gt;These aren’t loopholes. They’re how parenting works in the digital age. Parents increasingly filter, supervise, and, usually, decide together with their kids. KOSMA will lead to more locked accounts, and more parents submitting to face scans and ID checks. It will also lead to more power concentrated in the hands of the companies Congress claims to distrust.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Can Be Done Instead&lt;/head&gt;
    &lt;p&gt;KOSMA also includes separate restrictions on how platforms can use algorithms for users aged 13 to 17. Those raise their own serious questions about speech, privacy, and how online services work, and need debate and scrutiny as well. But they don’t change the core problem here: this bill hands control over children’s online lives to Big Tech.&lt;/p&gt;
    &lt;p&gt;If Congress really wants to help families, it should start with something much simpler and much more effective: strong privacy protections for everyone. Limits on data collection, restrictions on behavioral tracking, and rules that apply to adults as well as kids would do far more to reduce harmful incentives than deputizing companies to guess how old your child is and shut them out.&lt;/p&gt;
    &lt;p&gt;But if lawmakers aren’t ready to do that, they should at least drop KOSMA and start over. A law that treats ordinary parenting as a compliance problem is not protecting families—it’s undermining them.&lt;/p&gt;
    &lt;p&gt;Parents don’t need Big Tech to replace them. They need laws that respect how families actually work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46662503</guid><pubDate>Sat, 17 Jan 2026 21:54:52 +0000</pubDate></item><item><title>Light Mode InFFFFFFlation</title><link>https://willhbr.net/2025/10/20/light-mode-infffffflation/</link><description>&lt;doc fingerprint="7ca4d0675e37ffd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Back in the day, light mode wasn’t called “light mode”. It was just the way that computers were, we didn’t really think about turning everything light or dark. Sure, some applications were often dark (photo editors, IDEs, terminals) but everything else was light, and that was fine.&lt;/p&gt;
    &lt;p&gt;What we didn’t notice is that light mode has been slowly getting lighter, and I’ve got a graph to prove it. I did what any normal person would do, I downloaded the same (or similar) screenshots from the MacOS Screenshot Library on 512 Pixels. This project would have been much more difficult without a single place to get well-organised screenshots from. I cropped each image so just a representative section of the window was present, here shown with a pinkish rectangle:&lt;/p&gt;
    &lt;p&gt;Then used Pillow to get the average lightness of each cropped image:&lt;/p&gt;
    &lt;code&gt;for file in sorted(os.listdir('.')):
  image = Image.open(file)
  greyscale = image.convert('L')
  stat = ImageStat.Stat(greyscale)
  avg_lightness = int(stat.mean[0])
  print(f"{file}\t{avg_lightness}")
&lt;/code&gt;
    &lt;p&gt;This ignores any kind of perceived brightness or the tinting that MacOS has been doing for a while based on your wallpaper colour. I could go down a massive tangent trying to work out exactly what the best way to measure this is, but given that the screenshots aren’t perfectly comparable between versions, comparing the average brightness of a greyscale image seems reasonable.&lt;/p&gt;
    &lt;p&gt;I graphed that on the release year of each OS version, doing the same for dark mode:&lt;/p&gt;
    &lt;p&gt;You can clearly see that the brightness of the UI has been steadily increasing for the last 16 years. The upper line is the default mode/light mode, the lower line is dark mode. When I started using MacOS in 2012, I was running Snow Leopard, the windows had an average brightness of 71%. Since then they’ve steadily increased so that in MacOS Tahoe, they’re at a full 100%.&lt;/p&gt;
    &lt;p&gt;What I’ve graphed here is just the brightness of the window chrome, which isn’t really representative of the actual total screen brightness. A better study would be looking at the overall brightness of a typical set of apps. The default background colour for windows, as well as the colours for inactive windows, would probably give a more complete picture.&lt;/p&gt;
    &lt;p&gt;For example, in Tahoe the darkest colour in a typical light-mode window is the colour of a section in an inactive settings window, at 97% brightness. In Snow Leopard the equivalent colour was 90%, and that was one of the brightest parts of the window, since the window chrome was typically darker than the window content.&lt;/p&gt;
    &lt;p&gt;I tried to remember exactly when I started using dark mode all the time on MacOS. I’ve always used a dark background for my editor and terminal, but I wasn’t sure when I swapped the system theme across. When it first came out I seem to remember thinking that it looked gross.&lt;/p&gt;
    &lt;p&gt;It obviously couldn’t be earlier than 2018, as that’s when dark mode was introduced in MacOS Mojave. I’m pretty sure that when I updated my personal laptop to an M1 MacBook Air at the end of 2020 that I set it to use dark mode. This would make sense, because the Big Sur update bumped the brightness from 85% to 97%, which probably pushed me over the edge.&lt;/p&gt;
    &lt;p&gt;I think the reason this happens is that if you look at two designs, photos, or whatever, it’s really easy to be drawn in to liking the brighter one more. Or if they’re predominantly dark, then the darker one. I’ve done it myself with this very site. If I’m tweaking the colours it’s easy to bump up the brightness on the background and go “ooh wow yeah that’s definitely cleaner”, then swap it back and go “ewww it looks like it needs a good scrub”. If it’s the dark mode colours, then a darker background will just look cooler.&lt;/p&gt;
    &lt;p&gt;I’m not a designer, but I assume that resisting this urge is something you learn in design school. Just like making a website look good with a non-greyscale background.&lt;/p&gt;
    &lt;p&gt;This year in iOS 26, some UI elements use the HDR screen to make some elements and highlights brighter than 100% white.1 This year it’s reasonably subtle, but the inflation potential is there. If you’ve ever looked at an HDR photo on an iPhone (or any other HDR screen) then looked at the UI that’s still being shown in SDR, you’ll know just how grey and sad it looks. If you’re designing a new UI, how tempting will it be to make just a little bit more of it just a little bit brighter?&lt;/p&gt;
    &lt;p&gt;As someone whose job involves looking at MacOS for a lot of the day, I find that I basically have to use dark mode to avoid looking at a display where all the system UI is 100% white blasting in my eyes. But the alternative doesn’t have to be near-black for that, I would happily have a UI that’s a medium grey. In fact what I’ve missed since swapping to using dark mode is that I don’t have contrast between windows. Everything looks the same, whether it’s a text editor, IDE, terminal, web browser, or Finder window. All black, all the time.&lt;/p&gt;
    &lt;p&gt;Somewhat in the spirit of Mavericks Forever2, if I were to pick an old MacOS design to go back to it would probably be Yosemite. I don’t have any nostalgia for skeuomorphic brushed metal or stitched leather, but I do quite like the flattened design and blur effects that Yosemite brought. Ironically Yosemite was a substantial jump in brightness from previous versions.&lt;/p&gt;
    &lt;p&gt;So if you’re making an interface or website, be bold and choose a 50% grey. My eyes will thank you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46662662</guid><pubDate>Sat, 17 Jan 2026 22:19:25 +0000</pubDate></item></channel></rss>