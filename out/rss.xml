<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 12 Dec 2025 12:21:12 +0000</lastBuildDate><item><title>From Text to Token: How Tokenization Pipelines Work</title><link>https://www.paradedb.com/blog/when-tokenization-becomes-token</link><description>&lt;doc fingerprint="1588a3a603808f18"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Text to Token: How Tokenization Pipelines Work&lt;/head&gt;
    &lt;p&gt;When you type a sentence into a search box, it’s easy to imagine the search engine seeing the same thing you do. In reality, search engines (or search databases) don’t store blobs of text, and they don’t store sentences. They don’t even store words in the way we think of them. They dismantle input text (both indexed and query), scrub it clean, and reassemble it into something slightly more abstract and far more useful: tokens. These tokens are what you search with, and what is stored in your inverted indexes to search over.&lt;/p&gt;
    &lt;p&gt;Let’s slow down and watch that pipeline in action, pausing at each stage to see how language is broken apart and remade, and how that affects results.&lt;/p&gt;
    &lt;p&gt;We’ll use a twist on "The quick brown fox jumps over the lazy dog" as our test case. It has everything that makes tokenization interesting: capitalization, punctuation, an accent, and words that change as they move through the pipeline. By the end, it’ll look different, but be perfectly prepared for search.&lt;/p&gt;
    &lt;p&gt;This isn’t a complete pipeline, just a look at some of the common filters you’ll find in lexical search systems. Different databases and search engines expose many of these filters as composable building blocks that you can enable, disable, or reorder to suit your needs.&lt;/p&gt;
    &lt;p&gt;The same general ideas apply whether you're using Lucene/Elasticsearch, Tantivy/ParadeDB, or Postgres full-text search.&lt;/p&gt;
    &lt;head rend="h2"&gt;Filtering Text With Case and Character Folding&lt;/head&gt;
    &lt;p&gt;Before we even think about breaking our text down we need to think about filtering out anything which isn’t useful. This usually means auditing the characters which make up our text string: transforming all letters to lower-case, and if we know we might have them folding any diacritics (like in résumé, façade, or Noël) to their base letter.&lt;/p&gt;
    &lt;p&gt;This step ensures that characters are normalized and consistent before tokenization begins. Café becomes cafe, and résumé becomes resume, allowing searches to match regardless of accents. Lowercasing ensures that database matches Database, though it can introduce quirks: like matching Olive (the name) with olive (the snack). Most systems accept this trade-off: false positives are better than missed results. Code search is a notable exception, since it often needs to preserve symbols and respect casing like camelCase or PascalCase.&lt;/p&gt;
    &lt;p&gt;Let’s take a look at how our input string is transformed. We are replacing the capital T with a lower-case one, and also folding the &lt;code&gt;é&lt;/code&gt; to an &lt;code&gt;e&lt;/code&gt;. Nothing too surprising here. All of these boxes are interactive, so feel free to put in your own sentences to see the results.&lt;/p&gt;
    &lt;p&gt;Of course, there are many more filters that can be applied here, but for the sake of brevity, let’s move on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Splitting Text Into Searchable Pieces with Tokenization&lt;/head&gt;
    &lt;p&gt;The tokenization phase takes our filtered text and splits it up into indexable units. This is where we move from dealing with a sentence as a single unit to treating it as a collection of discrete, searchable parts called tokens.&lt;/p&gt;
    &lt;p&gt;The most common approach for English text is simple whitespace and punctuation tokenization: split on spaces and marks, and you’ve got tokens. But even this basic step has nuances: tabs, line breaks, or hyphenated words like full-text can all behave differently. Each system has its quirks, the default Lucene tokenizer turns &lt;code&gt;it’s&lt;/code&gt; into &lt;code&gt;[it's]&lt;/code&gt;, while the Tantivy splits into &lt;code&gt;[it, s]&lt;/code&gt;1.&lt;/p&gt;
    &lt;p&gt;Generally speaking there are three classes of tokenizers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Word oriented tokenizers break text into individual words at word boundaries. This includes simple whitespace tokenizers that split on spaces, as well as more sophisticated language-aware tokenizers that understand non-English character sets2 . These work well for most search applications where you want to match whole words.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Partial Word Tokenizers split words into smaller fragments, useful for matching parts of words or handling compound terms. N-gram tokenizers create overlapping character sequences, while edge n-gram tokenizers focus on prefixes or suffixes. These are powerful for autocomplete features and fuzzy matching but can create noise in search results.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Structured Text Tokenizers are designed for specific data formats like URLs, email addresses, file paths, or structured data. They preserve meaningful delimiters and handle domain-specific patterns that would be mangled by general-purpose tokenizers. These can be essential when your content contains non-prose text that needs special handling.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For our example we will be using a simple tokenizer, but you can also toggle to a trigram (an n-gram with a length of 3) tokenizer below to get a feel for how different the output would be (don't forget you can change the text in the top box to play round).&lt;/p&gt;
    &lt;head rend="h2"&gt;Throwing Out Filler With Stopwords&lt;/head&gt;
    &lt;p&gt;Some words carry little weight. They appear everywhere, diluting meaning: "the", "and", "of", "are". These are stopwords. Search engines often throw them out entirely3, betting that what remains will carry more signal.&lt;/p&gt;
    &lt;p&gt;This is not without risk. In The Who, "the" matters. That's why stopword lists are usually configurable4 and not universal. In systems which support BM25 they are often left out altogether because the ranking formula gives less weight to very common terms, but in systems which don't support BM25 (like Postgres tsvector) stopwords are critically important.&lt;/p&gt;
    &lt;p&gt;Notice how removing stopwords immediately makes our token list more focused? We've gone from ten tokens to eight, and what remains carries more semantic weight.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cutting Down to the Root with Stemming&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;Jump&lt;/code&gt;, &lt;code&gt;jumps&lt;/code&gt;, &lt;code&gt;jumped&lt;/code&gt;, &lt;code&gt;jumping&lt;/code&gt;. Humans see the connection instantly. Computers don't, unless we give them a way5.&lt;/p&gt;
    &lt;p&gt;Enter stemming. A stemmer is a rule-based machine that chops words down to a common core. Sometimes this happens elegantly, and sometimes it happens brutally. The foundation for most modern English stemming comes from Martin Porter's 1980 algorithm, which defined the approach that gave search engines consistent rules for stripping suffixes while respecting word structure. Today many stemmers are based on the Snowball variant.&lt;/p&gt;
    &lt;p&gt;The results can look odd. &lt;code&gt;Database&lt;/code&gt; becomes &lt;code&gt;databas,&lt;/code&gt; &lt;code&gt;lazy&lt;/code&gt; becomes &lt;code&gt;lazi.&lt;/code&gt; But that's okay because stemmers don't care about aesthetics, they care about consistency. If every form of &lt;code&gt;lazy&lt;/code&gt; collapses to &lt;code&gt;lazi,&lt;/code&gt; the search engine can treat them as one6. There's also lemmatization, which uses linguistic knowledge to convert words to their dictionary forms, but it's more complex and computationally expensive than stemming's "good enough" approach7. &lt;/p&gt;
    &lt;p&gt;Here's the final transformation: our tokens have been reduced to their essential stems. &lt;code&gt;Jumped&lt;/code&gt; becomes &lt;code&gt;jump,&lt;/code&gt; &lt;code&gt;lazy&lt;/code&gt; becomes &lt;code&gt;lazi,&lt;/code&gt; and &lt;code&gt;database&lt;/code&gt; becomes &lt;code&gt;databas.&lt;/code&gt; These stems might not look like real words, but they serve a crucial purpose: they're consistent. Whether someone searches for &lt;code&gt;jumping,&lt;/code&gt; &lt;code&gt;jumped,&lt;/code&gt; or &lt;code&gt;jumps,&lt;/code&gt; they'll all reduce to &lt;code&gt;jump&lt;/code&gt; and match our indexed content. This is the power of stemming: bridging the gap between the many ways humans express the same concept. &lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Tokens&lt;/head&gt;
    &lt;p&gt;Our sentence has traveled through the complete pipeline. What started as "The full-text database jumped over the lazy café dog" has been transformed through each stage: stripped of punctuation and capitalization, split into individual words, filtered of common stopwords, and finally reduced to stems.&lt;/p&gt;
    &lt;p&gt;The result is a clean set of eight tokens:&lt;/p&gt;
    &lt;p&gt;This transformation is applied to any data we store in our inverted index, and also to our queries. When someone searches for "databases are jumping," that query gets tokenized: lowercased, split, stopwords removed, and stemmed. It becomes &lt;code&gt;databas&lt;/code&gt; and &lt;code&gt;jump&lt;/code&gt;, which will match our indexed content perfectly. &lt;/p&gt;
    &lt;head rend="h2"&gt;Why Tokenization Matters&lt;/head&gt;
    &lt;p&gt;Tokenization doesn’t get the glory. Nobody brags about their stopword filter at conferences. But it’s the quiet engine of search. Without it, &lt;code&gt;dogs&lt;/code&gt; wouldn’t match &lt;code&gt;dog&lt;/code&gt;, and &lt;code&gt;jumping&lt;/code&gt; wouldn’t find &lt;code&gt;jump&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Every search engine invests heavily here because everything else (scoring, ranking, relevance) depends on getting tokens right. It’s not glamorous, but it’s precise, and when you get this part right, everything else in search works better.&lt;/p&gt;
    &lt;p&gt;Get started with ParadeDB and see how modern search databases handle tokenization for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Which is better? That depends:&lt;/p&gt;&lt;code&gt;it's&lt;/code&gt;seems more correct and skips storing a useless&lt;code&gt;s&lt;/code&gt;token, but it wouldn't match on a search for&lt;code&gt;it&lt;/code&gt;. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;General purpose morphological libraries like Jieba and Lindera are often used to provide tokenizers that can deal with Chinese, Korean, Japanese and characters. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;When we remove stopwords we still keep the original position of each token in the document. This allows positional queries ("find&lt;/p&gt;&lt;code&gt;cat&lt;/code&gt;within five words of&lt;code&gt;dog&lt;/code&gt;" even though we have discarded words. ↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Lucene and Tantivy both have stopwords off by default, and when enabled for English they use the same default list:&lt;/p&gt;&lt;code&gt;[a, an, and, are, as, at, be, but, by, for, if, in,into, is, it, no, not, of, on, or, such, that, the, their, then, there, these, they, this, to, was, will, with]&lt;/code&gt;↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Another method is vector search, which trades lexical stemming for searching over semantic meaning. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Words can also be overstemmed, consider&lt;/p&gt;&lt;code&gt;university&lt;/code&gt;and&lt;code&gt;universe&lt;/code&gt;which both stem to&lt;code&gt;univers&lt;/code&gt;, but have very different meanings. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lemmatization uses actual word lists to make sure it only makes real words. To do this it accurately it needs to know the "part of speech" the source word is (noun, verb, etc..). ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46232003</guid><pubDate>Thu, 11 Dec 2025 14:45:49 +0000</pubDate></item><item><title>Show HN: Sim – Apache-2.0 n8n alternative</title><link>https://github.com/simstudioai/sim</link><description>&lt;doc fingerprint="189ff03c2161b5fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Build and deploy AI agent workflows in minutes.&lt;/p&gt;
    &lt;p&gt;Design agent workflows visually on a canvas—connect agents, tools, and blocks, then run them instantly.&lt;/p&gt;
    &lt;p&gt;Leverage Copilot to generate nodes, fix errors, and iterate on flows directly from natural language.&lt;/p&gt;
    &lt;p&gt;Upload documents to a vector store and let agents answer questions grounded in your specific content.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cloud-hosted: sim.ai&lt;/head&gt;
    &lt;code&gt;npx simstudio&lt;/code&gt;
    &lt;p&gt;Docker must be installed and running on your machine.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-p, --port &amp;lt;port&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Port to run Sim on (default &lt;code&gt;3000&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-pull&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Skip pulling latest Docker images&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/simstudioai/sim.git

# Navigate to the project directory
cd sim

# Start Sim
docker compose -f docker-compose.prod.yml up -d&lt;/code&gt;
    &lt;p&gt;Access the application at http://localhost:3000/&lt;/p&gt;
    &lt;p&gt;Run Sim with local AI models using Ollama - no external APIs required:&lt;/p&gt;
    &lt;code&gt;# Start with GPU support (automatically downloads gemma3:4b model)
docker compose -f docker-compose.ollama.yml --profile setup up -d

# For CPU-only systems:
docker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d&lt;/code&gt;
    &lt;p&gt;Wait for the model to download, then visit http://localhost:3000. Add more models with:&lt;/p&gt;
    &lt;code&gt;docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b&lt;/code&gt;
    &lt;p&gt;If you already have Ollama running on your host machine (outside Docker), you need to configure the &lt;code&gt;OLLAMA_URL&lt;/code&gt; to use &lt;code&gt;host.docker.internal&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Docker Desktop (macOS/Windows)
OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d

# Linux (add extra_hosts or use host IP)
docker compose -f docker-compose.prod.yml up -d  # Then set OLLAMA_URL to your host's IP&lt;/code&gt;
    &lt;p&gt;Why? When running inside Docker, &lt;code&gt;localhost&lt;/code&gt; refers to the container itself, not your host machine. &lt;code&gt;host.docker.internal&lt;/code&gt; is a special DNS name that resolves to the host.&lt;/p&gt;
    &lt;p&gt;For Linux users, you can either:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use your host machine's actual IP address (e.g., &lt;code&gt;http://192.168.1.100:11434&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;extra_hosts: ["host.docker.internal:host-gateway"]&lt;/code&gt;to the simstudio service in your compose file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sim also supports vLLM for self-hosted models with OpenAI-compatible API:&lt;/p&gt;
    &lt;code&gt;# Set these environment variables
VLLM_BASE_URL=http://your-vllm-server:8000
VLLM_API_KEY=your_optional_api_key  # Only if your vLLM instance requires auth&lt;/code&gt;
    &lt;p&gt;When running with Docker, use &lt;code&gt;host.docker.internal&lt;/code&gt; if vLLM is on your host machine (same as Ollama above).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open VS Code with the Remote - Containers extension&lt;/item&gt;
      &lt;item&gt;Open the project and click "Reopen in Container" when prompted&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;bun run dev:full&lt;/code&gt;in the terminal or use the&lt;code&gt;sim-start&lt;/code&gt;alias&lt;list rend="ul"&gt;&lt;item&gt;This starts both the main application and the realtime socket server&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bun runtime&lt;/item&gt;
      &lt;item&gt;PostgreSQL 12+ with pgvector extension (required for AI embeddings)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Sim uses vector embeddings for AI features like knowledge bases and semantic search, which requires the &lt;code&gt;pgvector&lt;/code&gt; PostgreSQL extension.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone and install dependencies:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/simstudioai/sim.git
cd sim
bun install&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up PostgreSQL with pgvector:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You need PostgreSQL with the &lt;code&gt;vector&lt;/code&gt; extension for embedding support. Choose one option:&lt;/p&gt;
    &lt;p&gt;Option A: Using Docker (Recommended)&lt;/p&gt;
    &lt;code&gt;# Start PostgreSQL with pgvector extension
docker run --name simstudio-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=simstudio \
  -p 5432:5432 -d \
  pgvector/pgvector:pg17&lt;/code&gt;
    &lt;p&gt;Option B: Manual Installation&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install PostgreSQL 12+ and the pgvector extension&lt;/item&gt;
      &lt;item&gt;See pgvector installation guide&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up environment:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd apps/sim
cp .env.example .env  # Configure with required variables (DATABASE_URL, BETTER_AUTH_SECRET, BETTER_AUTH_URL)&lt;/code&gt;
    &lt;p&gt;Update your &lt;code&gt;.env&lt;/code&gt; file with the database URL:&lt;/p&gt;
    &lt;code&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up the database:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, configure the database package environment:&lt;/p&gt;
    &lt;code&gt;cd packages/db
cp .env.example .env &lt;/code&gt;
    &lt;p&gt;Update your &lt;code&gt;packages/db/.env&lt;/code&gt; file with the database URL:&lt;/p&gt;
    &lt;code&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"&lt;/code&gt;
    &lt;p&gt;Then run the migrations:&lt;/p&gt;
    &lt;code&gt;bunx drizzle-kit migrate --config=./drizzle.config.ts&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start the development servers:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommended approach - run both servers together (from project root):&lt;/p&gt;
    &lt;code&gt;bun run dev:full&lt;/code&gt;
    &lt;p&gt;This starts both the main Next.js application and the realtime socket server required for full functionality.&lt;/p&gt;
    &lt;p&gt;Alternative - run servers separately:&lt;/p&gt;
    &lt;p&gt;Next.js app (from project root):&lt;/p&gt;
    &lt;code&gt;bun run dev&lt;/code&gt;
    &lt;p&gt;Realtime socket server (from &lt;code&gt;apps/sim&lt;/code&gt; directory in a separate terminal):&lt;/p&gt;
    &lt;code&gt;cd apps/sim
bun run dev:sockets&lt;/code&gt;
    &lt;p&gt;Copilot is a Sim-managed service. To use Copilot on a self-hosted instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to https://sim.ai → Settings → Copilot and generate a Copilot API key&lt;/item&gt;
      &lt;item&gt;Set &lt;code&gt;COPILOT_API_KEY&lt;/code&gt;environment variable in your self-hosted apps/sim/.env file to that value&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key environment variables for self-hosted deployments (see &lt;code&gt;apps/sim/.env.example&lt;/code&gt; for full list):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DATABASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;PostgreSQL connection string with pgvector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BETTER_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Auth secret (&lt;code&gt;openssl rand -hex 32&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BETTER_AUTH_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Your app URL (e.g., &lt;code&gt;http://localhost:3000&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;NEXT_PUBLIC_APP_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Public app URL (same as above)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENCRYPTION_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Encryption key (&lt;code&gt;openssl rand -hex 32&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;OLLAMA_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Ollama server URL (default: &lt;code&gt;http://localhost:11434&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;VLLM_BASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;vLLM server URL for self-hosted models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;COPILOT_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;API key from sim.ai for Copilot features&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're running Ollama on your host machine and Sim in Docker, change &lt;code&gt;OLLAMA_URL&lt;/code&gt; from &lt;code&gt;localhost&lt;/code&gt; to &lt;code&gt;host.docker.internal&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d&lt;/code&gt;
    &lt;p&gt;See Using an External Ollama Instance for details.&lt;/p&gt;
    &lt;p&gt;Ensure PostgreSQL has the pgvector extension installed. When using Docker, wait for the database to be healthy before running migrations.&lt;/p&gt;
    &lt;p&gt;If ports 3000, 3002, or 5432 are in use, configure alternatives:&lt;/p&gt;
    &lt;code&gt;# Custom ports
NEXT_PUBLIC_APP_URL=http://localhost:3100 POSTGRES_PORT=5433 docker compose up -d&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework: Next.js (App Router)&lt;/item&gt;
      &lt;item&gt;Runtime: Bun&lt;/item&gt;
      &lt;item&gt;Database: PostgreSQL with Drizzle ORM&lt;/item&gt;
      &lt;item&gt;Authentication: Better Auth&lt;/item&gt;
      &lt;item&gt;UI: Shadcn, Tailwind CSS&lt;/item&gt;
      &lt;item&gt;State Management: Zustand&lt;/item&gt;
      &lt;item&gt;Flow Editor: ReactFlow&lt;/item&gt;
      &lt;item&gt;Docs: Fumadocs&lt;/item&gt;
      &lt;item&gt;Monorepo: Turborepo&lt;/item&gt;
      &lt;item&gt;Realtime: Socket.io&lt;/item&gt;
      &lt;item&gt;Background Jobs: Trigger.dev&lt;/item&gt;
      &lt;item&gt;Remote Code Execution: E2B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please see our Contributing Guide for details.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Made with ❤️ by the Sim Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234186</guid><pubDate>Thu, 11 Dec 2025 17:20:11 +0000</pubDate></item><item><title>Litestream VFS</title><link>https://fly.io/blog/litestream-vfs/</link><description>&lt;doc fingerprint="67cf1ef5a02c7b1b"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. Itâs free, open-source software that should run anywhere, and you can read more about it here.&lt;/p&gt;
    &lt;p&gt;Again with the sandwiches: assume we’ve got a SQLite database of sandwich ratings, and we’ve backed it up with Litestream to an S3 bucket.&lt;/p&gt;
    &lt;p&gt;Now, on our local host, load up AWS credentials and an S3 path into our environment. Open SQLite and:&lt;/p&gt;
    &lt;code&gt;$ sqlite3
SQLite version 3.50.4 2025-07-30 19:33:53
sqlite&amp;gt; .load litestream.so
sqlite&amp;gt; .open file:///my.db?vfs=litestream
&lt;/code&gt;
    &lt;p&gt;SQLite is now working from that remote database, defined by the Litestream backup files in the S3 path we configured. We can query it:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
22|Veggie Delight|New York|4
30|Meatball|Los Angeles|5
168|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;This is Litestream VFS. It runs SQLite hot off an object storage URL. As long as you can load the shared library our tree builds for you, it’ll work in your application the same way it does in the SQLite shell.&lt;/p&gt;
    &lt;p&gt;Fun fact: we didn’t have to download the whole database to run this query. More about this in a bit.&lt;/p&gt;
    &lt;p&gt;Meanwhile, somewhere in prod, someone has it in for meatball subs and wants to knock them out of the bracket â oh, fuck:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; UPDATE sandwich_ratings SET stars = 1 ;
&lt;/code&gt;
    &lt;p&gt;They forgot the &lt;code&gt;WHERE&lt;/code&gt; clause!&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
97|French Dip|Los Angeles|1
140|BÃ¡nh MÃ¬|San Francisco|1
62|Italian Beef|Chicago|1
&lt;/code&gt;
    &lt;p&gt;Italian Beefs and BÃ¡nh MÃ¬s, all at 1 star. Disaster!&lt;/p&gt;
    &lt;p&gt;But wait, back on our dev machine:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; PRAGMA litestream_time = '5 minutes ago'; 
sqlite&amp;gt; select * from sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
30|Meatball|Los Angeles|5
33|Ham &amp;amp; Swiss|Los Angeles|2
163|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;We’re now querying that database from a specific point in time in our backups. We can do arbitrary relative timestamps, or absolute ones, like &lt;code&gt;2000-01-01T00:00:00Z&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What we’re doing here is instantaneous point-in-time recovery (PITR), expressed simply in SQL and SQLite pragmas.&lt;/p&gt;
    &lt;p&gt;Ever wanted to do a quick query against a prod dataset, but didn’t want to shell into a prod server and fumble with the &lt;code&gt;sqlite3&lt;/code&gt; terminal command like a hacker in an 80s movie? Or needed to do a quick sanity check against yesterday’s data, but without doing a full database restore? Litestream VFS makes that easy. I’m so psyched about how it turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;Litestream v0.5 integrates LTX, our SQLite data-shipping file format. Where earlier Litestream blindly shipped whole raw SQLite pages to and from object storage, LTX ships ordered sets of pages. We built LTX for LiteFS, which uses a FUSE filesystem to do transaction-aware replication for unmodified applications, but we’ve spent this year figuring out ways to use LTX in Litestream, without all that FUSE drama.&lt;/p&gt;
    &lt;p&gt;The big thing LTX gives us is “compaction”. When we restore a database from object storage, we want the most recent versions of each changed database page. What we don’t want are all the intermediate versions of those pages that occurred prior to the most recent change.&lt;/p&gt;
    &lt;p&gt;Imagine, at the time we’re restoring, we’re going to need pages 1, 2, 3, 4, and 5. Depending on the order in which pages were written, the backup data set might look something like &lt;code&gt;1 2 3 5 3 5 4 5 5&lt;/code&gt;. What we want is the rightmost  5, 4, 3, 2, and 1, without wasting time on the four “extra” page 5’s and the one “extra” page 3. Those “extra” pages are super common in SQLite data sets; for instance, every busy table with an autoincrementing primary key will have them.&lt;/p&gt;
    &lt;p&gt;LTX lets us skip the redundant pages, and the algorithm is trivial: reading backwards from the end of the sequence, skipping any page you already read. This drastically accelerates restores.&lt;/p&gt;
    &lt;p&gt;But LTX compaction isn’t limited to whole databases. We can also LTX-compact sets of LTX files. That’s the key to how PITR restores with Litestream now work.&lt;/p&gt;
    &lt;p&gt;In the diagram below, we’re taking daily full snapshots. Below those snapshots are “levels” of changesets: groups of database pages from smaller and smaller windows of time. By default, Litestream uses time intervals of 1 hour at the highest level, down to 30 seconds at level 1. L0 is a special level where files are uploaded every second, but are only retained until being compacted to L1.&lt;/p&gt;
    &lt;p&gt;Now, let’s do a PITR restore. Start from the most proximal snapshot. Then determine the minimal set of LTX files from each level to reach the time you are restoring to.&lt;/p&gt;
    &lt;p&gt;We have another trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;LTX trailers include a small index tracking the offset of each page in the file. By fetching only these index trailers from the LTX files we’re working with (each occupies about 1% of its LTX file), we can build a lookup table of every page in the database. Since modern object storage providers all let us fetch slices of files, we can perform individual page reads against S3 directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It’s Implemented&lt;/head&gt;
    &lt;p&gt;SQLite has a plugin interface for things like this: the “VFS” interface. VFS plugins abstract away the bottom-most layer of SQLite, the interface to the OS. If you’re using SQLite now, you’re already using some VFS module, one SQLite happens to ship with.&lt;/p&gt;
    &lt;p&gt;For Litestream users, there’s a catch. From the jump, we’ve designed Litestream to run alongside unmodified SQLite applications. Part of what makes Litestream so popular is that your apps don’t even need to know it exists. It’s “just” a Unix program.&lt;/p&gt;
    &lt;p&gt;That Litestream Unix program still does PITR restores, without any magic. But to do fast PITR-style queries straight off S3, we need more. To make those queries work, you have to load and register Litestream’s VFS module.&lt;/p&gt;
    &lt;p&gt;But that’s all that changes.&lt;/p&gt;
    &lt;p&gt;In particular: Litestream VFS doesn’t replace the SQLite library you’re already using. It’s not a new “version” of SQLite. It’s just a plugin for the SQLite you’re already using.&lt;/p&gt;
    &lt;p&gt;Still, we know that’s not going to work for everybody, and even though we’re really psyched about these PITR features, we’re not taking our eyes off the ball on the rest of Litestream. You don’t have to use our VFS library to use Litestream, or to get the other benefits of the new LTX code.&lt;/p&gt;
    &lt;p&gt;The way a VFS library works, we’re given just a couple structures, each with a bunch of methods defined on them. We override only the few methods we care about. Litestream VFS handles only the read side of SQLite. Litestream itself, running as a normal Unix program, still handles the “write” side. So our VFS subclasses just enough to find LTX backups and issue queries.&lt;/p&gt;
    &lt;p&gt;With our VFS loaded, whenever SQLite needs to read a page into memory, it issues a &lt;code&gt;Read()&lt;/code&gt; call through our library. The read call includes the byte offset at which SQLite expected to find the page. But with Litestream VFS, that byte offset is an illusion.&lt;/p&gt;
    &lt;p&gt;Instead, we use our knowledge of the page size along with the requested page number to do a lookup on the page index we’ve built. From it, we get the remote filename, the “real” byte offset into that file, and the size of the page. That’s enough for us to use the S3 API’s &lt;code&gt;Range&lt;/code&gt; header handling to download exactly the block we want.&lt;/p&gt;
    &lt;p&gt;To save lots of S3 calls, Litestream VFS implements an LRU cache. Most databases have a small set of “hot” pages â inner branch pages or the leftmost leaf pages for tables with an auto-incrementing ID field. So only a small percentage of the database is updated and queried regularly.&lt;/p&gt;
    &lt;p&gt;Weâve got one last trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;Quickly building an index and restore plan for the current state of a database is cool. But we can do one better.&lt;/p&gt;
    &lt;p&gt;Because Litestream backs up (into the L0 layer) once per second, the VFS code can simply poll the S3 path, and then incrementally update its index. The result is a near-realtime replica. Better still, you donât need to stream the whole database back to your machine before you use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Eat Your Heart Out, Marty McFly&lt;/head&gt;
    &lt;p&gt;Litestream holds backup files for every state your database has been in, with single-second resolution, for as long as you want it to. Forgot the &lt;code&gt;WHERE&lt;/code&gt; clause on a &lt;code&gt;DELETE&lt;/code&gt; statement? Updating your database state to where it was an hour (or day, or week) ago is just a matter of adjusting the LTX indices Litestream manages.&lt;/p&gt;
    &lt;p&gt;All this smoke-and-mirrors of querying databases without fully fetching them has another benefit: it starts up really fast! We’re living an age of increasingly ephemeral servers, what with the AIs and the agents and the clouds and the hoyvin-glavins. Wherever you find yourself, if your database is backed up to object storage with Litestream, you’re always in a place where you can quickly issue a query.&lt;/p&gt;
    &lt;p&gt;As always, one of the big things we think we’re doing right with Litestream is: we’re finding ways to get as much whiz-bang value as we can (instant PITR reading live off object storage: pretty nifty!) while keeping the underlying mechanism simple enough that you can fit your head around it.&lt;/p&gt;
    &lt;p&gt;Litestream is solid for serious production use (we rely on it for important chunks of our own Fly.io APIs). But you could write Litestream yourself, just from the basic ideas in these blog posts. We think that’s a point in its favor. We land there because the heavy lifting in Litestream is being done by SQLite itself, which is how it should be.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234710</guid><pubDate>Thu, 11 Dec 2025 17:59:10 +0000</pubDate></item><item><title>GPT-5.2</title><link>https://openai.com/index/introducing-gpt-5-2/</link><description>&lt;doc fingerprint="285fa2d92df7ce9f"&gt;
  &lt;main&gt;
    &lt;p&gt;We are introducing GPT‑5.2, the most capable model series yet for professional knowledge work.&lt;/p&gt;
    &lt;p&gt;Already, the average ChatGPT Enterprise user says AI saves them 40–60 minutes a day, and heavy users say it saves them more than 10 hours a week. We designed GPT‑5.2 to unlock even more economic value for people; it’s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long contexts, using tools, and handling complex, multi-step projects.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 sets a new state of the art across many benchmarks, including GDPval, where it outperforms industry professionals at well-specified knowledge work tasks spanning 44 occupations.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 Thinking&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.8% (GPT‑5)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;50.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-bench Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPQA Diamond (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;92.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.1%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;CharXiv Reasoning (w/ Python)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;AIME 2025 (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;100.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;94.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 1–3)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;40.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;31.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 4)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12.5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-1 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;86.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;72.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-2 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;52.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;17.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notion(opens in a new window), Box(opens in a new window), Shopify(opens in a new window), Harvey(opens in a new window) and Zoom(opens in a new window) observed GPT‑5.2 demonstrates state-of-the-art long-horizon reasoning and tool-calling performance. Databricks(opens in a new window), Hex(opens in a new window) and Triple Whale(opens in a new window) found GPT‑5.2 to be exceptional at agentic data science and document analysis tasks. Cognition(opens in a new window), Warp(opens in a new window), Charlie Labs(opens in a new window), JetBrains(opens in a new window) and Augment Code(opens in a new window) say GPT‑5.2 delivers state-of-the-art agentic coding performance, with measurable improvements in areas such as interactive coding, code reviews and bug finding.&lt;/p&gt;
    &lt;p&gt;In ChatGPT, GPT‑5.2 Instant, Thinking, and Pro will begin rolling out today, starting with paid plans. In the API, they are available now to all developers.&lt;/p&gt;
    &lt;p&gt;Overall, GPT‑5.2 brings significant improvements in general intelligence, long-context understanding, agentic tool-calling, and vision—making it better at executing complex, real-world tasks end-to-end than any previous model.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is the best model yet for real-world, professional use. On GDPval, an eval measuring well-specified knowledge work tasks across 44 occupations, GPT‑5.2 Thinking sets a new state-of-the-art score, and is our first model that performs at or above a human expert level. Specifically, GPT‑5.2 Thinking beats or ties top industry professionals on 70.9% of comparisons on GDPval knowledge work tasks, according to expert human judges. These tasks include making presentations, spreadsheets, and other artifacts. GPT‑5.2 Thinking produced outputs for GDPval tasks at &amp;gt;11x the speed and &amp;lt;1% the cost of expert professionals, suggesting that when paired with human oversight, GPT‑5.2 can help with professional work. Speed and cost estimates are based on historical metrics; speed in ChatGPT may vary.&lt;/p&gt;
    &lt;p&gt;When reviewing one especially good output, one GDPval judge commented, "It is an exciting and noticeable leap in output quality... [it] appears to have been done by a professional company with staff, and has a surprisingly well designed layout and advice for both deliverables, though with one we still have some minor errors to correct."&lt;/p&gt;
    &lt;p&gt;Additionally, on our internal benchmark of junior investment banking analyst spreadsheet modeling tasks—such as putting together a three-statement model for a Fortune 500 company with proper formatting and citations, or building a leveraged buyout model for a take-private—GPT 5.2 Thinking's average score per task is 9.3% higher than GPT‑5.1’s, rising from 59.1% to 68.4%.&lt;/p&gt;
    &lt;p&gt;Side-by-side comparisons show improved sophistication and formatting in spreadsheets and slides generated by GPT‑5.2 Thinking:&lt;/p&gt;
    &lt;p&gt;To use the new spreadsheet and presentation capabilities in ChatGPT, you must be on a Plus, Pro, Business, or Enterprise plan and select either GPT‑5.2 Thinking or Pro. Complex generations can take many minutes to produce.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art of 55.6% on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Unlike SWE-bench Verified, which only tests Python, SWE-Bench Pro tests four languages and aims to be more contamination-resistant, challenging, diverse, and industrially relevant.&lt;/p&gt;
    &lt;p&gt;On SWE-bench Verified (not plotted), GPT‑5.2 Thinking scores our new high of 80%.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this translates into a model that can more reliably debug production code, implement feature requests, refactor large codebases, and ship fixes end-to-end with less manual intervention.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is also better at front-end software engineering than GPT‑5.1 Thinking. Early testers found it significantly stronger at front-end development and complex or unconventional UI work—especially involving 3D elements—making it a powerful daily partner for engineers across the stack. See a few examples of what it can produce from a single prompt:&lt;/p&gt;
    &lt;p&gt;Early testers shared their feedback on GPT‑5.2’s coding capabilities:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 represents the biggest leap for GPT models in agentic coding since GPT-5 and is a SOTA coding model in its price range. The version bump undersells the jump in intelligence. We’re excited to make it the default across Windsurf and several core Devin workloads."&lt;/quote&gt;
    &lt;p&gt;GPT‑5.2 Thinking hallucinates less than GPT‑5.1 Thinking. On a set of de-identified queries from ChatGPT, responses with errors were 30%rel less common. For professionals, this means fewer mistakes when using the model for research, writing, analysis, and decision support—making the model more dependable for everyday knowledge work.&lt;/p&gt;
    &lt;p&gt;Like all models, GPT‑5.2 Thinking is imperfect. For anything critical, double check its answers.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art in long-context reasoning, achieving leading performance on OpenAI MRCRv2—an evaluation that tests a model’s ability to integrate information spread across long documents. On real-world tasks like deep document analysis, which require related information across hundreds of thousands of tokens, GPT‑5.2 Thinking is substantially more accurate than GPT‑5.1 Thinking. In particular, it’s the first model we’ve seen that achieves near 100% accuracy on the 4-needle MRCR variant (out to 256k tokens).&lt;/p&gt;
    &lt;p&gt;In practical terms, this enables professionals to use GPT‑5.2 to work with long documents—such as reports, contracts, research papers, transcripts, and multi-file projects—while maintaining coherence and accuracy across hundreds of thousands of tokens. This makes GPT‑5.2 especially well suited for deep analysis, synthesis, and complex multi-source workflows.&lt;/p&gt;
    &lt;p&gt;For tasks that benefit from thinking beyond the maximum context window, GPT‑5.2 Thinking is compatible with our new Responses &lt;code&gt;/compact&lt;/code&gt; endpoint, which extends the model’s effective context window. This lets GPT‑5.2 Thinking tackle more tool-heavy, long-running workflows that would otherwise be limited by context length. Read more in our API documentation(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is our strongest vision model yet, cutting error rates roughly in half on chart reasoning and software interface understanding.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this means the model can more accurately interpret dashboards, product screenshots, technical diagrams, and visual reports—supporting workflows in finance, operations, engineering, design, and customer support where visual information is central.&lt;/p&gt;
    &lt;p&gt;Compared to previous models, GPT‑5.2 Thinking has a stronger grasp of how elements are positioned within an image, which helps on tasks where relative layout plays a key role in solving the problem. In the example below, we ask the model to identify the components in an image input (in this case, a motherboard) and return labels with approximate bounding boxes. Even on a low-quality image, GPT‑5.2 identifies the main regions and places boxes that sometimes match the true locations of each component, while GPT‑5.1 only labels a few parts and shows a much weaker understanding of their spatial arrangement. Both models make clear mistakes, but GPT‑5.2 shows better comprehension of the image.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;GPT‑5.2 Thinking achieves a new state of the art of 98.7% on Tau2-bench Telecom, demonstrating its ability to reliably use tools across long, multi-turn tasks.&lt;/p&gt;
    &lt;p&gt;For latency-sensitive use cases, GPT‑5.2 Thinking also performs much better at reasoning.effort='none', substantially outperforming GPT‑5.1 and GPT‑4.1.&lt;/p&gt;
    &lt;p&gt;For professionals, this translates into stronger end-to-end workflows—such as resolving customer support cases, pulling data from multiple systems, running analyses, and generating final outputs with fewer breakdowns between steps.&lt;/p&gt;
    &lt;p&gt;For example, when asking a complex customer service question that requires multi-step resolution, the model can more effectively coordinate a full workflow across multiple agents. In the case below, a traveler reports a delayed flight, a missed connection, an overnight stay in New York, and a medical seating requirement. GPT‑5.2 manages the entire chain of tasks—rebooking, special-assistance seating, and compensation—delivering a more complete outcome than GPT‑5.1.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;One of our hopes for AI is that it will accelerate scientific research for the benefit of everyone. Toward this, we’ve been working with and listening to scientists to see how AI can speed up their work, and last month we shared some early collaborative experiments here.&lt;/p&gt;
    &lt;p&gt;We believe GPT‑5.2 Pro and GPT‑5.2 Thinking are the world’s best models for assisting and accelerating scientists. On GPQA Diamond, a graduate-level Google-proof Q&amp;amp;A benchmark, GPT‑5.2 Pro achieves 93.2%, followed closely by GPT‑5.2 Thinking at 92.4%.&lt;/p&gt;
    &lt;p&gt;On FrontierMath (Tier 1–3), an evaluation of expert-level mathematics, GPT‑5.2 Thinking set a new state of the art, solving 40.3% of problems.&lt;/p&gt;
    &lt;p&gt;We're beginning to see AI models meaningfully accelerate progress in math and science in tangible ways. For example, in recent work with GPT‑5.2 Pro, researchers explored an open question in statistical learning theory. In a narrow, well-specified setting, the model proposed a proof that was subsequently verified by the authors and reviewed with external experts, illustrating how frontier models can assist mathematical research under close human oversight.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-1 (Verified), a benchmark designed to measure general reasoning ability, GPT‑5.2 Pro is the first model to cross the 90% threshold, improving from 87%(opens in a new window) by o3‑preview last year while reducing the cost of achieving that performance by roughly 390×.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-2 (Verified), which raises the difficulty and better isolates fluid reasoning, GPT‑5.2 Thinking achieves a new state of the art for chain-of-thought models, scoring 52.9%. GPT‑5.2 Pro performs even higher, reaching 54.2%, further extending the model’s ability to reason through novel, abstract problems.&lt;/p&gt;
    &lt;p&gt;Improvements across these evaluations reflect GPT‑5.2’s stronger multi-step reasoning, greater quantitative accuracy, and more reliable problem solving on complex technical tasks.&lt;/p&gt;
    &lt;p&gt;Here’s what our early testers say about GPT‑5.2:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 unlocked a complete architecture shift for us. We collapsed a fragile, multi-agent system into a single mega-agent with 20+ tools. The best part is, it just works. The mega-agent is faster, smarter, and 100x easier to maintain. We’re seeing dramatically lower latency, much stronger tool calling, and we no longer need sprawling system prompts because 5.2 will execute cleanly off a simple, one-line prompt. It feels like pure magic."&lt;/quote&gt;
    &lt;p&gt;In ChatGPT, users should notice GPT‑5.2 feels better to use day to day—more structured, more reliable, and still enjoyable to talk to.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Instant is a fast, capable workhorse for everyday work and learning, with clear improvements in info-seeking questions, how-tos and walk-throughs, technical writing, and translation, building on the warmer conversational tone introduced in GPT‑5.1 Instant. Early testers particularly noted clearer explanations that surface key information upfront.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is designed for deeper work, helping users tackle more complex tasks with greater polish—especially for coding, summarizing long documents, answering questions about uploaded files, working through math and logic step by step, and supporting planning and decisions with clearer structure and more useful detail.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Pro is our smartest and most trustworthy option for difficult questions where a higher-quality answer is worth the wait, with early testing showing fewer major errors and stronger performance in complex domains like programming.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 builds on the safe completion research we introduced with GPT‑5, which teaches the model to give the most helpful answer while still staying within safety boundaries.&lt;/p&gt;
    &lt;p&gt;With this release, we continued our work to strengthen our models’ responses in sensitive conversations, with meaningful improvements in how they respond to prompts indicating signs of suicide or self harm, mental health distress, or emotional reliance on the model. These targeted interventions have resulted in fewer undesirable responses in both GPT‑5.2 Instant and GPT‑5.2 Thinking as compared to GPT‑5.1 and GPT‑5 Instant and Thinking models. Further details can be found in the system card.&lt;/p&gt;
    &lt;p&gt;We’re in the early stages of rolling out our age prediction model so that we can automatically apply content protections for users who are under 18, in order to limit access to sensitive content. This builds on our existing approach to users we know are under 18 and our parental controls.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is one step in an ongoing series of improvements, and we’re far from done. While this release delivers meaningful gains in intelligence and productivity, we know there are areas where people want more. In ChatGPT, we’re working on known issues like over-refusals, while continuing to raise the bar on safety and reliability overall. These changes are complex, and we’re focused on getting them right.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Mental health&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.995&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.883&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.915&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.684&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emotional reliance&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.945&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.955&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.785&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Self-harm&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.925&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.963&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.937&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In ChatGPT, we’ll begin rolling out GPT‑5.2 (Instant, Thinking, and Pro) today, starting with paid plans (Plus, Pro, Go, Business, Enterprise). We deploy GPT‑5.2 gradually to keep ChatGPT as smooth and reliable as we can; if you don’t see it at first, please try again later. In ChatGPT, GPT‑5.1 will still be available to paid users for three months under legacy models, after which we will sunset GPT‑5.1.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;API&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Instant&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2-chat-latest&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In our API Platform, GPT‑5.2 Thinking is available today in the Responses API and Chat Completions API as &lt;code&gt;gpt-5.2&lt;/code&gt;, and GPT‑5.2 Instant as &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt;. GPT‑5.2 Pro is available in the Responses API as &lt;code&gt;gpt-5.2-pro&lt;/code&gt;. Developers can now set the reasoning parameter in GPT‑5.2 Pro, and both GPT‑5.2 Pro and GPT‑5.2 Thinking now support the new fifth reasoning effort of xhigh, for tasks where quality is most important.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is priced at $1.75/1M input tokens and $14/1M output tokens, with a 90% discount on cached inputs. On multiple agentic evals, we found that despite GPT‑5.2’s greater cost per token, the cost of attaining a given level of quality ended up less expensive due to GPT‑5.2’s greater token efficiency.&lt;/p&gt;
    &lt;p&gt;While ChatGPT subscription pricing remains the same, in the API GPT‑5.2 is priced higher per token than GPT‑5.1 because it is a more capable model. It’s still priced below other frontier models, so people can continue to use it deeply in their daily work and core applications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Model&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cached input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Output&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.75&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.175&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$14&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$21&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$168&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.1 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.25&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.125&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$10&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$15&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$120&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We have no current plans to deprecate GPT‑5.1, GPT‑5, or GPT‑4.1 in the API and will communicate any deprecation plans with ample advance notice for developers. While GPT‑5.2 will work well out of the box in Codex, we expect to release a version of GPT‑5.2 optimized for Codex in the coming weeks.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 was built in collaboration with our long-standing partners NVIDIA and Microsoft. Azure data centers and NVIDIA GPUs, including H100, H200, and GB200-NVL72, underpin OpenAI’s at-scale training infrastructure, driving significant gains in model intelligence. Together, this collaboration allows us to scale compute with confidence and bring new models to market more quickly.&lt;/p&gt;
    &lt;p&gt;Below, we report comprehensive benchmark scores for GPT‑5.2 Thinking, along with a subset for GPT‑5.2 Pro.&lt;/p&gt;
    &lt;head rend="h5"&gt;Professional&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, wins or ties)&lt;/cell&gt;
        &lt;cell&gt;70.9%&lt;/cell&gt;
        &lt;cell&gt;74.1%&lt;/cell&gt;
        &lt;cell&gt;38.8% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, clear wins)&lt;/cell&gt;
        &lt;cell&gt;49.8%&lt;/cell&gt;
        &lt;cell&gt;60.0%&lt;/cell&gt;
        &lt;cell&gt;35.5% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (no ties)&lt;/cell&gt;
        &lt;cell&gt;61.0%&lt;/cell&gt;
        &lt;cell&gt;67.6%&lt;/cell&gt;
        &lt;cell&gt;37.1% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Investment banking spreadsheet tasks (internal)&lt;/cell&gt;
        &lt;cell&gt;68.4%&lt;/cell&gt;
        &lt;cell&gt;71.7%&lt;/cell&gt;
        &lt;cell&gt;59.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Coding&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-Bench Pro, Public&lt;/cell&gt;
        &lt;cell&gt;55.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;80.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SWE-Lancer, IC Diamond*&lt;/cell&gt;
        &lt;cell&gt;74.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;69.7%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Factuality&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ChatGPT answers without errors (w/ search)&lt;/cell&gt;
        &lt;cell&gt;93.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatGPT answers without errors (no search)&lt;/cell&gt;
        &lt;cell&gt;88.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;87.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Long context&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 4k–8k&lt;/cell&gt;
        &lt;cell&gt;98.2%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;65.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 8k–16k&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;47.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 16k–32k&lt;/cell&gt;
        &lt;cell&gt;95.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 32k–64k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 64k–128k&lt;/cell&gt;
        &lt;cell&gt;85.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 128k–256k&lt;/cell&gt;
        &lt;cell&gt;77.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;29.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 128k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 256k&lt;/cell&gt;
        &lt;cell&gt;89.8%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GraphWalks bfs &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Graphwalks parents &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;89.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;71.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Vision&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (no tools)&lt;/cell&gt;
        &lt;cell&gt;82.1%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;67.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;88.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;80.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (no tools)&lt;/cell&gt;
        &lt;cell&gt;79.5%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;80.4%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;79.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video MMMU (no tools)&lt;/cell&gt;
        &lt;cell&gt;85.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;82.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Screenspot Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;86.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;64.2%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Tool usage&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Telecom&lt;/cell&gt;
        &lt;cell&gt;98.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Retail&lt;/cell&gt;
        &lt;cell&gt;82.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;65.8%&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Scale MCP-Atlas&lt;/cell&gt;
        &lt;cell&gt;60.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Toolathlon&lt;/cell&gt;
        &lt;cell&gt;46.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Academic&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPQA Diamond (no tools)&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;93.2%&lt;/cell&gt;
        &lt;cell&gt;88.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (no tools)&lt;/cell&gt;
        &lt;cell&gt;34.5%&lt;/cell&gt;
        &lt;cell&gt;36.6%&lt;/cell&gt;
        &lt;cell&gt;25.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (w/ search, Python)&lt;/cell&gt;
        &lt;cell&gt;45.5%&lt;/cell&gt;
        &lt;cell&gt;50.0%&lt;/cell&gt;
        &lt;cell&gt;42.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMLU&lt;/cell&gt;
        &lt;cell&gt;89.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HMMT, Feb 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;99.4%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;96.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;AIME 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FrontierMath Tier 1–3 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;40.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;31.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;FrontierMath Tier 4 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;14.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;12.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Abstract reasoning&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ARC-AGI-1 (Verified)&lt;/cell&gt;
        &lt;cell&gt;86.2%&lt;/cell&gt;
        &lt;cell&gt;90.5%&lt;/cell&gt;
        &lt;cell&gt;72.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ARC-AGI-2 (Verified)&lt;/cell&gt;
        &lt;cell&gt;52.9%&lt;/cell&gt;
        &lt;cell&gt;54.2% (high)&lt;/cell&gt;
        &lt;cell&gt;17.6%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Models were run with maximum available reasoning effort in our API (xhigh for GPT‑5.2 Thinking &amp;amp; Pro, and high for GPT‑5.1 Thinking), except for the professional evals, where GPT‑5.2 Thinking was run with reasoning effort heavy, the maximum available in ChatGPT Pro. Benchmarks were conducted in a research environment, which may provide slightly different output from production ChatGPT in some cases.&lt;/p&gt;
    &lt;p&gt;* For SWE-Lancer, we omit 40/237 problems that did not run on our infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234788</guid><pubDate>Thu, 11 Dec 2025 18:04:47 +0000</pubDate></item><item><title>Programmers and software developers lost the plot on naming their tools</title><link>https://larr.net/p/namings.html</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234806</guid><pubDate>Thu, 11 Dec 2025 18:06:42 +0000</pubDate></item><item><title>Rivian Unveils Custom Silicon, R2 Lidar Roadmap, and Universal Hands Free</title><link>https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/</link><description>&lt;doc fingerprint="993313b0ba26c9c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rivian Unveils Custom Silicon, R2 LiDAR Roadmap, Universal Hands Free, and Its Next Gen Autonomy Platform&lt;/head&gt;
    &lt;p&gt;RJ opened the first ever Autonomy and AI Day explaining why Rivian believes it is positioned to lead in this next phase of the industry. The company is leaning hard into compute, custom hardware, large scale AI systems, and a shared data foundation that touches every part of the ownership experience.&lt;/p&gt;
    &lt;p&gt;Let’s break it all down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet the Rivian Autonomy Processor&lt;/head&gt;
    &lt;p&gt;One of the biggest announcements was RAP1, Rivian’s first in house processor built on a 5nm multi chip module. It delivers 1600 sparse INT8 TOPS and can push 5 billion pixels per second inside the new Gen 3 Autonomy Computer. Rivian even built its own AI compiler and platform software to support it. This shows Rivian is no longer just integrating off the shelf chips, it is now designing silicon specifically for its autonomy roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy Computer and LiDAR on R2&lt;/head&gt;
    &lt;p&gt;The ACM3 (Autonomy Compute Module 3) autonomy computer will debut on R2 starting at the end of 2026, but Rivian made it clear that R2 will launch initially without LiDAR. What Rivian confirmed today is that LiDAR will be added later in the program. This lines up with what we explored back in May when we spotted early signs that Rivian was evaluating LiDAR as a redundancy and ground truth layer for future autonomy. Rivian has now officially validated that LiDAR is coming to R2 down the road, where it will join cameras and radar to create a richer, more resilient perception stack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large Driving Model and Rivian’s Data Loop&lt;/head&gt;
    &lt;p&gt;Rivian explained how its autonomy stack is powered by a self improving data loop feeding the company’s Large Driving Model, which is trained similarly to an LLM. Reinforcement learning distills high quality driving behavior into efficient onboard models. Every release improves the system, and Rivian laid out a trajectory that moves toward point to point, eyes off and eventually personal Level 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universal Hands Free Coming to Gen 2&lt;/head&gt;
    &lt;p&gt;Rivian confirmed that a major software update will bring Universal Hands Free to Gen 2 R1T and R1S. This hands free experience will cover over 3.5 million miles of roads across the US and Canada as long as there are clearly painted lane lines. It is a huge expansion of the assisted driving envelope for current owners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy+ Sub Launching in 2026&lt;/head&gt;
    &lt;p&gt;Rivian also announced Autonomy+, an autonomy tier with continuously expanding features launching early 2026.&lt;/p&gt;
    &lt;p&gt;Pricing is $2,500 one time or $49.99 per month.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rivian Unified Intelligence&lt;/head&gt;
    &lt;p&gt;Rivian is reorganizing its entire platform around Rivian Unified Intelligence, a data foundation that ties together telemetry, cloud models, service systems and customer facing features. It is the backbone for predictive maintenance, smarter diagnostics and upcoming AI driven tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rivian Assistant Coming in 2026&lt;/head&gt;
    &lt;p&gt;Rivian also officially unveiled its new Rivian Assistant, a next generation voice experience arriving early 2026 on Gen 1 and Gen 2 R1 vehicles. The assistant uses a blend of edge models and in vehicle intelligence to understand your schedule, recognize context, and handle everyday requests.&lt;/p&gt;
    &lt;p&gt;On R2, it will even run fully offline thanks to a more powerful infotainment computer, reducing latency and keeping more of the experience on device.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI Powered Service and Diagnostics&lt;/head&gt;
    &lt;p&gt;Rivian is embedding AI into the service workflow. Technicians will have access to an AI driven expert system that analyzes telemetry and vehicle history to pinpoint issues faster and more accurately. These same tools will eventually power the mobile app as well, making self service diagnostics significantly smarter.&lt;/p&gt;
    &lt;head rend="h3"&gt;36 Comments&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Any mention of retrofitting Autonomy Computer and LiDAR onto existing R1’s? Hopefully at least Gen 2’s!&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Will not happen&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;I doubt it’ll happen, they want you to buy a new car.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I need this R2D2 themed R2 🔥&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I watched the event. It felt like I was at an apple event. Is Rivian wanting us to trade in ours cars every two years to get a newer version? A new chip set, better camera, or a faster processor?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;I’ve always been a vehicle buyer, not leaser. But these rapid tech changes definitely have me looking at leasing an R1S gen 2 so I can unload it once the R1’s get lidar.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Personally I want to do the driving. I like driving and don’t want to put my life in the hands of anyone’s computer!&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;No, they want you to subscribe to their services to get a revenue stream from something that’s actually useful and you can use. Unlike GM, Toyota, etc who try to block carplay so they can keep the diagnostics but give crap software and infotainment which is only marginally improved if you spend a fortune on their upscale, overpriced remakes of lower end models (camry =&amp;gt; lexus equivalent).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So with everything we know do we think we can expect automatic lane change before point to point? Was kind of expecting auto park and auto lane change to be announced with universal hands free&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Universal Hands Free when? Did they give any indication?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Soon. So probably 6-9 months.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;S0––––0N&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Pretty sure the presentations said universal will in “an update later this month.”&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;They posted on Instagram, it will come on next software update!&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sad day for Gen 1 users except the Rivian Assistant part……&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is disappointing. An assistant that can integrate with Google calendar 🤷♂️. Who cares?!?&lt;/p&gt;
        &lt;p&gt;Maybe some finds this valuable but I would rather have apple carplay.&lt;/p&gt;
        &lt;p&gt;People listen to music in their cars. Why not make that the first app? Who decided “let’s invest 2 years and make Google calendar the first app”?&lt;/p&gt;
        &lt;p&gt;Rivian is clueless.&lt;/p&gt;
        &lt;p&gt;I have owned 2 of these and won’t buy a third one.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Rivian is trying to do everything, but they really need to consider people’s lives beyond the vehicle. I set up my phone, my home, my computer, my TV, and now my car too? Google and apple have texting and music covered, just let people’s phone do the work and not make the car yet “assistant” to set up. Focus on self driving. Let the car simply connect to phones for phone stuff.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Integrating with an AI is awesome! I don’t get why people want Carplay so bad–Rivian can do a better job integrating their car functions with Spotify than Apple can. I don’t want two operating systems running at once competing for control over the vehicle’s screens. Tesla’s is slightly more refined, but Rivian’s works fine and software is one of their core competencies.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sad as Gen 1 – should have had at least something to improve Driver-&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It will be interesting to see which features end up behind the paywall. Will we only end up with adaptive cruse control and center lane assist?&lt;/p&gt;
        &lt;p&gt;Also curious about the $2500 for Autonomy+. I wonder if that will follow a person/family to future Rivian vehicles. If so that would make it more enticing (also if it then included Connect+ as well).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;Found the answer once Rivian website was updated:&lt;/p&gt;&lt;lb/&gt;“Autonomy+ one-time purchases remain with the vehicle upon ownership transfer, and Autonomy+ product features remain available during the lifetime of feature support for the hardware on the vehicle at delivery”&lt;p&gt;So unfortunately it follows the vehicle and not the person.&lt;/p&gt;&lt;p&gt;While I appreciate the fact that continuous development costs money, I just really don’t like subscriptions that much. Due really to the amount of time I would spend not even using the feature. So for this reason I sort of favor the more lifetime buy option…. Main use case for me is really trips/longer drives. Which are definitely less than monthly. I don’t think I see myself getting in the car and paying $50 at the start of a long weekend trip for the sort term feature rental. Oh well 🙂&lt;/p&gt;&lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Wondered the exact same thing. $2500 pays for itself after about 4 years. If it stays with the user, on all their Rivians over time, great!&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You all be thankful that your Gen 1s have gotten meaningful updates and backwards compatible updates from Gen 2. Be thankful you don’t have a BMW EV from 2022 with iDrive 8, not even a year later they were outdated by the next iteration of iDrive, 8.5 and BMW claims features (even software based) from 8.5 are not backwards compatible with iDrive 8.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;I know that buying a legacy vehicle like BMW is going to be static. Rivian said we were buying a software driven vehicle with updates (on par with Tesla). Tesla brings updates for years to older vehicles. I agree if we are talking a 5-6 year old car that we’re hitting the wall… but not a car 2 years old that’s marketed in a very different way.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Well with Gen1, just because a certain MY was purchased does not mean the platform was also updated for that model. Gen1 has been riding on an autonomous platform from 2019 so it’s well into its 6-7 year life span. It also lacks the hardware needed to achieve anything beyond what it can hardly do today. The lack of radar alone is what holds it back, on top of its constrained processing power and low res cameras.&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So the new processing unit will be out late 2026. Is that when LIDAR is also expected or is LIDAR further out?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So R2 will launch without Lidar. But will R2 launch with Gen2 Autonomy hardware? They were clear that Gen3 Autonomy hardware is “late 2026”. But I went through it again, and didn’t see any clear statement on this. If R2 must wait for Gen3 Autonomy hardware, we won’t be seeing R2 any time s00n…&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Read between the lines….&lt;/p&gt;
            &lt;p&gt;R2 will have all of this stuff, lidar etc….&lt;/p&gt;
            &lt;p&gt;“gen 3 chip will be out late 2026”.&lt;/p&gt;
            &lt;p&gt;If R2 will have all the features, then that means its pushed late 2026. They just didnt to come out and say R2 is delayed.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Yup, that’s the way I read it as well. Of course saying that explicitly would have really rained on the parade.&lt;/p&gt;
                &lt;list rend="ul"&gt;
                  &lt;item&gt;&lt;p&gt;According to this article, R2 will launch with Gen2 ACM, then later bring in Gen3 ACM. Best case, an early R2 with Gen2 ACM is upgradable to Gen3 ACM. Worst case, getting R2 early will mean forgoing Gen3 ACM features like Lidar, eyes-off and Personal L4.&lt;/p&gt;&lt;lb/&gt;RJ *did* mention that R2 would launch without Lidar – maybe that implied the Gen2 ACM.&lt;p&gt;https://www.mercurynews.com/2025/12/11/rivian-unveils-ai-chip-for-automated-driving-ditches-nvidia/&lt;/p&gt;&lt;/item&gt;
                &lt;/list&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Was there any hint of the Dec update that was to include RAD tuner?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m trying to figure out what are we getting on the next update. From the videos of the demos, it appears to be navigating towards a destination making turns on its own using turn signals. Is that universal hands-free or is that a preview of point to point?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Pretty sure that was a preview of point to point&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Where’s the adventure in autonomy? I for one don’t care if my vehicle can drive by itself. Who can really 100% trust such a claim?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This did nothing but reaffirm my lessons learned from buying gen1 R1T – will never buy another vehicle from Rivian – R1, R2, R3, etc. until they are on Gen 5 or higher… which at this pace… they’ll be there in next 4-6 years. Also doesn’t sound like any update on text message integration reading this update from Jose… so it feels like it will never happen for Gen 1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;They did a whole segment live about Rivian Assistant which is coming to Gen 1 and 2 which will have text messaging capabilities they demonstrated it live.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234920</guid><pubDate>Thu, 11 Dec 2025 18:17:19 +0000</pubDate></item><item><title>An SVG is all you need</title><link>https://jon.recoil.org/blog/2025/12/an-svg-is-all-you-need.html</link><description>&lt;doc fingerprint="3dc38b5d0be4eb8d"&gt;
  &lt;main&gt;
    &lt;p&gt;SVGs are pretty cool - vector graphics in a simple XML format. They are supported on just about every device and platform, are crisp on every display, and can have embedded scripts in to make them interactive. They're way more capable than many people realise, and I think we can capitalise on some of that unrealised potential.&lt;/p&gt;
    &lt;p&gt;Anil's recent post Four Ps for Building Massive Collective Knowledge Systems got me thinking about the permanence of the experimentation that underlies our scientific papers. In my idealistic vision of how scientific publishing should work, each paper would be accompanied by a fully interactive environment where the reader could explore the data, rerun the experiments, tweak the parameters, and see how the results changed. Obviously we can't do this in the general case - some experiments are just too expensive or time-consuming to rerun on demand. But for many papers, especially in computer science, this is entirely feasible.&lt;/p&gt;
    &lt;p&gt;That line of thought reminded me of a project I tackled about 20 years ago as a post-doc in the Department of Plant Sciences here in Cambridge. I was writing a paper on synergy in fungal networks and built a tiny SVG visualisation tool that let readers wander through the raw data captured from a real fungal network growing in a petri dish. I dug it up recently and was surprised (and delighted) to see that it still works perfectly in modern browsers - even though the original âcover pageâ suggested Firefox 1.5 or the Adobe SVG plug-in (!). Give it a spin; click the 'forward', 'back' and other buttons below the petri dish!&lt;/p&gt;
    &lt;p&gt;And that, dear reader, is literally all you need. A completely self-contained SVG file can either fetch data from a versioned repository or embed the data directly, as the example does. It can process that data, generate visualisations, and render knobs and sliders for interactive exploration. No server-side magic required - everything runs client-side in the browser, served by a plain static web server, and very easily to share.&lt;/p&gt;
    &lt;p&gt;How does it fit in with Anil's four Ps?&lt;/p&gt;
    &lt;p&gt;The SVG above is only a visualisation tool for data; it doesn't really do any processing, but it certainly could. The biggest change that's happened over the 20 years since I wrote this is the massive increase in the computation power available in the browser. If would be entirely feasible to implement the entire data analysis pipeline for that paper in an SVG today, probably without even spinning up the fans on my laptop!&lt;/p&gt;
    &lt;p&gt;So this is yet another tool in our ongoing effort to be able to effortlessly share and remix our work - added to the pile of Jupyter notebooks, Marimo botebooks, the slipshow/x-ocaml combination, Patrick's take on Jon Sterling's Forester, my own notebooks, and many others - and this is a subset of what we're using just in our own group!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46235959</guid><pubDate>Thu, 11 Dec 2025 19:25:14 +0000</pubDate></item><item><title>Denial of service and source code exposure in React Server Components</title><link>https://react.dev/blog/2025/12/11/denial-of-service-and-source-code-exposure-in-react-server-components</link><description>&lt;doc fingerprint="b02bbf4aec53e947"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Denial of Service and Source Code Exposure in React Server Components&lt;/head&gt;
    &lt;p&gt;December 11, 2025 by The React Team&lt;/p&gt;
    &lt;p&gt;Security researchers have found and disclosed two additional vulnerabilities in React Server Components while attempting to exploit the patches in last week’s critical vulnerability.&lt;/p&gt;
    &lt;p&gt;These new vulnerabilities do not allow for Remote Code Execution. The patch for React2Shell remains effective at mitigating the Remote Code Execution exploit.&lt;/p&gt;
    &lt;p&gt;The new vulnerabilities are disclosed as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Denial of Service - High Severity: CVE-2025-55184 and CVE-2025-67779 (CVSS 7.5)&lt;/item&gt;
      &lt;item&gt;Source Code Exposure - Medium Severity: CVE-2025-55183 (CVSS 5.3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We recommend upgrading immediately due to the severity of the newly disclosed vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Further details of these vulnerabilities will be provided after the rollout of the fixes are complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Immediate Action Required&lt;/head&gt;
    &lt;p&gt;These vulnerabilities are present in the same packages and versions as CVE-2025-55182.&lt;/p&gt;
    &lt;p&gt;This includes versions 19.0.0, 19.0.1, 19.0.2, 19.1.0, 19.1.1, 19.1.2, 19.1.2, 19.2.0, 19.2.1 and 19.2.2 of:&lt;/p&gt;
    &lt;p&gt;Fixes were backported to versions 19.0.3, 19.1.4, and 19.2.3. If you are using any of the above packages please upgrade to any of the fixed versions immediately.&lt;/p&gt;
    &lt;p&gt;As before, if your app’s React code does not use a server, your app is not affected by these vulnerabilities. If your app does not use a framework, bundler, or bundler plugin that supports React Server Components, your app is not affected by these vulnerabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Affected frameworks and bundlers&lt;/head&gt;
    &lt;p&gt;Some React frameworks and bundlers depended on, had peer dependencies for, or included the vulnerable React packages. The following React frameworks &amp;amp; bundlers are affected: next, react-router, waku, @parcel/rsc, @vite/rsc-plugin, and rwsdk.&lt;/p&gt;
    &lt;p&gt;Please see the instructions in the previous post for upgrade steps.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hosting Provider Mitigations&lt;/head&gt;
    &lt;p&gt;As before, we have worked with a number of hosting providers to apply temporary mitigations.&lt;/p&gt;
    &lt;p&gt;You should not depend on these to secure your app, and still update immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;React Native&lt;/head&gt;
    &lt;p&gt;For React Native users not using a monorepo or &lt;code&gt;react-dom&lt;/code&gt;, your &lt;code&gt;react&lt;/code&gt; version should be pinned in your &lt;code&gt;package.json&lt;/code&gt;, and there are no additional steps needed.&lt;/p&gt;
    &lt;p&gt;If you are using React Native in a monorepo, you should update only the impacted packages if they are installed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;react-server-dom-webpack&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;react-server-dom-parcel&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;react-server-dom-turbopack&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is required to mitigate the security advisories, but you do not need to update &lt;code&gt;react&lt;/code&gt; and &lt;code&gt;react-dom&lt;/code&gt; so this will not cause the version mismatch error in React Native.&lt;/p&gt;
    &lt;p&gt;See this issue for more information.&lt;/p&gt;
    &lt;head rend="h2"&gt;High Severity: Denial of Service&lt;/head&gt;
    &lt;p&gt;CVEs: CVE-2025-55184 and CVE-2025-67779 Base Score: 7.5 (High)&lt;/p&gt;
    &lt;p&gt;Security researchers have discovered that a malicious HTTP request can be crafted and sent to any Server Functions endpoint that, when deserialized by React, can cause an infinite loop that hangs the server process and consumes CPU. Even if your app does not implement any React Server Function endpoints it may still be vulnerable if your app supports React Server Components.&lt;/p&gt;
    &lt;p&gt;This creates a vulnerability vector where an attacker may be able to deny users from accessing the product, and potentially have a performance impact on the server environment.&lt;/p&gt;
    &lt;p&gt;The patches published today mitigate by preventing the infinite loop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Medium Severity: Source Code Exposure&lt;/head&gt;
    &lt;p&gt;CVE: CVE-2025-55183 Base Score: 5.3 (Medium)&lt;/p&gt;
    &lt;p&gt;A security researcher has discovered that a malicious HTTP request sent to a vulnerable Server Function may unsafely return the source code of any Server Function. Exploitation requires the existence of a Server Function which explicitly or implicitly exposes a stringified argument:&lt;/p&gt;
    &lt;code&gt;'use server';&lt;/code&gt;
    &lt;p&gt;An attacker may be able to leak the following:&lt;/p&gt;
    &lt;code&gt;0:{"a":"$@1","f":"","b":"Wy43RxUKdxmr5iuBzJ1pN"}&lt;/code&gt;
    &lt;p&gt;The patches published today prevent stringifying the Server Function source code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3rd: Leak reported to Vercel and Meta Bug Bounty by Andrew MacPherson.&lt;/item&gt;
      &lt;item&gt;December 4th: Initial DoS reported to Meta Bug Bounty by RyotaK.&lt;/item&gt;
      &lt;item&gt;December 6th: Both issues confirmed by the React team, and the team began investigating.&lt;/item&gt;
      &lt;item&gt;December 7th: Initial fixes created and the React team began verifying and planning new patch.&lt;/item&gt;
      &lt;item&gt;December 8th: Affected hosting providers and open source projects notified.&lt;/item&gt;
      &lt;item&gt;December 10th: Hosting provider mitigations in place and patches verified.&lt;/item&gt;
      &lt;item&gt;December 11th: Additional DoS reported to Meta Bug Bounty by Shinsaku Nomura.&lt;/item&gt;
      &lt;item&gt;December 11th: Patches published and publicly disclosed as CVE-2025-55183 and CVE-2025-55184.&lt;/item&gt;
      &lt;item&gt;December 11th: Missing DoS case found internally, patched and publicly disclosed as CVE-2025-67779.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Attribution&lt;/head&gt;
    &lt;p&gt;Thank you to Andrew MacPherson (AndrewMohawk) for reporting the Source Code Exposure, RyotaK from GMO Flatt Security Inc and Shinsaku Nomura of Bitforest Co., Ltd. for reporting the Denial of Service vulnerabilities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46236924</guid><pubDate>Thu, 11 Dec 2025 20:46:46 +0000</pubDate></item><item><title>Almond (YC X25) Is Hiring SWEs and MechEs</title><link>https://www.ycombinator.com/companies/almond-2/jobs</link><description>&lt;doc fingerprint="ce8988d79ddffda5"&gt;
  &lt;main&gt;
    &lt;p&gt;Robots designed for the era of AI&lt;/p&gt;
    &lt;p&gt;Our mission is to free humans from physical labor with robotics.&lt;/p&gt;
    &lt;p&gt;We imagine a future where robots handle the essential, repetitive work and humans are free to create, connect, and pursue what truly matters to them.&lt;/p&gt;
    &lt;p&gt;To build that future we’re starting from the ground up with hardware. Our first product is a California-designed and assembled humanoid arm. Surrounding it, we’re developing advanced controls, intuitive data collection, and a full AI stack that makes deployment effortless in real industrial environments. We’re proving it on our own assembly line first.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46237081</guid><pubDate>Thu, 11 Dec 2025 21:00:10 +0000</pubDate></item><item><title>Nokia N900 Necromancy</title><link>https://yaky.dev/2025-12-11-nokia-n900-necromancy/</link><description>&lt;doc fingerprint="b6230073e3ff3e7c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nokia N900 Necromancy&lt;/head&gt;
    &lt;p&gt;Building a fake battery, adding a USB-C port, booting from SD card, and giving a new life to a classic Linux smartphone.&lt;/p&gt;
    &lt;p&gt;My friend Dima sent me his old-school classic Nokia N900. The battery is very old, and it does not boot as-is. So naturally, I wanted to see if I can resurrect it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 0: Is such a thing even possible?&lt;/head&gt;
    &lt;p&gt;Yes it is! (Unless there are other hardware issues)&lt;/p&gt;
    &lt;p&gt;I ran a smartphone without a battery a few years ago.&lt;/p&gt;
    &lt;p&gt;Cut and soldered a quick prototype to connect instead of the battery. Resistors are to emulate the "normal" temperature by providing expected resistance between the third pin and ground. See link above for details.&lt;/p&gt;
    &lt;p&gt;Hooked up a large supercapacitor to the battery pins and to a +5V source. If I recall correctly, using a capacitor without additional power did not work.&lt;/p&gt;
    &lt;p&gt;And it boots!&lt;/p&gt;
    &lt;p&gt;Now, let's make something that can fit into the battery compartment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: Better "battery"&lt;/head&gt;
    &lt;p&gt;These supercapacitors are nice, but way too large. After searching on Mouser, I found FM0H473ZF, 47000 mF (0.047F) capacitors in a rectangular case that is only 5mm thick.&lt;/p&gt;
    &lt;p&gt;Ten of these (~0.5F) is enough to run the smartphone without dying.&lt;/p&gt;
    &lt;p&gt;Capacitor contraption (TM) arranged (using a 3D-printed template) and soldered together.&lt;/p&gt;
    &lt;p&gt;And they all fit nicely into the battery compartment. The power is provided by a wire routed through the hole for the carry loop.&lt;/p&gt;
    &lt;p&gt;Running fine! One noticeable issue is that capacitors are getting pretty warm. Probably my sloppy soldering, but no shorts that I could find.&lt;/p&gt;
    &lt;head rend="h2"&gt;â ï¸&lt;/head&gt;
    &lt;p&gt;This is where I should have stopped. At some point while messing with the "battery" and power, I managed to corrupt the internal partition and the installed OS. Not sure if this was from the sudden battery pull or from supplying +5V instead of the expected +4.2V to the battery pins. Luckily, newer Maemo Leste is intended to run from the SD card anyway, and internal storage still works, so I was able to overwrite it with the bootloader.&lt;/p&gt;
    &lt;p&gt;Bootloader setup on Maemo Wiki&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: Consolidating connectors&lt;/head&gt;
    &lt;p&gt;I thought it might be practical to power the "battery" through the existing USB port. Just run the +5V wire from USB to the "battery", and avoid additional wires. (If you think this is kinda stupid, you are right)&lt;/p&gt;
    &lt;p&gt;Yooo... What is happening here? Dima says "oh yeah, the USB port was re-soldered. Twice". A quick glance at the forums also confirms that USB port was poorly designed and is prone to breaking.&lt;/p&gt;
    &lt;p&gt;Just one wire from the +5V pad to the "battery". The ground is the same as the battery pin.&lt;/p&gt;
    &lt;p&gt;Assembled everything back, routed and soldered the +5V wire, and added a diode to prevent the battery from feeding the USB port, and to drop the voltage to more acceptable ~4.3V.&lt;/p&gt;
    &lt;p&gt;The setup works, but the smartphone constantly shows either "Charging", or "Device using more power than it is receiving from the PC. Charging with a compatible charger is recommended", with battery gauge going crazy.&lt;/p&gt;
    &lt;p&gt;And then, the power just cut out.&lt;/p&gt;
    &lt;p&gt;Yeah, this was not a great idea. Let's see what happened.&lt;/p&gt;
    &lt;p&gt;USB +5V wire detached itself from the port. I presume this is from either the high current, age, stress, or corrosion.&lt;/p&gt;
    &lt;p&gt;However, when I opened the smartphone up, I... ripped off the +5V pad. (dark circle in lower right on the photo)&lt;/p&gt;
    &lt;p&gt;Fuck.&lt;/p&gt;
    &lt;p&gt;After reading some N900 forums, that +5V pad is a common place to connect the replacement USB port to (which was done here), but... that is the ONLY +5V connection on the board besides the pads under the USB port itself.&lt;/p&gt;
    &lt;p&gt;FUCK!&lt;/p&gt;
    &lt;head rend="h2"&gt;ðª¦&lt;/head&gt;
    &lt;p&gt;RIP Nokia N900. I tried to resurrect you, but instead, I killed your OS and ripped out the USB port wires.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: Radical replacements&lt;/head&gt;
    &lt;p&gt;To be fair, N900 is far from dead. I already flashed u-boot, was able to boot from SD card, and do not plan to use internal storage otherwise. Power can be supplied entirely through the new "battery". So technically, I do not need the USB functionality for the smartphone itself, just to power the "battery". At this point, I might as well replace the port with USB-C. Because why not.&lt;/p&gt;
    &lt;p&gt;Approximate placement of the new USB port.&lt;/p&gt;
    &lt;p&gt;The location of the original port is not very convenient. It is sandwiched between the main board and the SD card reader (lower left on the photo). SD card reader is also attached by a permanently-attached ribbon (i.e. nearly irreplaceable).&lt;/p&gt;
    &lt;p&gt;First, I used a small file to make the micro-USB-shaped hole on the smartphone body fit the USB-C shape. Then, I took a small 6-pin USB-C port, cut and sanded down its plastic parts to make it fit in the original spot. It is still slightly (~0.25mm) taller than the original, but I cannot make it any slimmer.&lt;/p&gt;
    &lt;p&gt;I tried to attach the USB-C port to the board in the correct place by carefully assembling the board, port and SD card reader into the body, and using small drops of glue to lightly affix the edge of the USB port (that I could reach) to the main board. The intent was to wait for glue to cure, take everything back apart and glue the port in its now-correct position for good. This took several tries but did not really work, as the port got detached while removing the main board every time, and the the superglue I used left lots of residue but did not adhere. Luckily, the tight fit and the shape of the USB-C port hold it in place mechanically quite well.&lt;/p&gt;
    &lt;p&gt;USB-C with +5V and ground attached.&lt;/p&gt;
    &lt;p&gt;Originally, I planned to solder all 6 pins and add 5.1 Ohm pull-down resistors to CC1 and CC2 pins (for full power delivery functionality). But there is simply not enough space to route the wires, the narrow valley between the chips (in the lower right of the photo) barely fits 3, and I did not have anything thinner on hand.&lt;/p&gt;
    &lt;p&gt;Nokia N900 with a USB-C port! Looks pretty nice IMO.&lt;/p&gt;
    &lt;p&gt;Since I did not solder the pull-down resistors, this USB-C port could only be powered by a "dumb" USB-A-to-USB-C cable, at default 0.5A. Chargers with power delivery functionality cannot identify such USB-C ports, and will not provide power at all. (This is also an issue with some handheld consoles such as RGB30)&lt;/p&gt;
    &lt;p&gt;The two wires are routed to the battery compartment through a very convenient opening in the metal frame, crimped and inserted into a DuPont connector.&lt;/p&gt;
    &lt;p&gt;Back to the battery. The capacitor contraption I built before works, but was kind of flimsy, and does not have any more space for a DuPont connector. Also, I would rather use a single capacitor, but it still has to fit. Since the original battery is unusable, I might as well try to salvage it, too.&lt;/p&gt;
    &lt;p&gt;Take off the sticker (that tells you not to do so :). The top BCM piece is held to the main battery body by two tiny screws (hidden under some crumbly compound) on each end, double-sided sticker, and a single lead in the middle.&lt;/p&gt;
    &lt;p&gt;Battery Control Module. Interestingly, for this battery, the body is the positive terminal. So the positive lead connects the battery body and the positive pin directly, while the negative lead goes thorough some control circuitry. Attaching a capacitor to these battery terminals should be sufficient.&lt;/p&gt;
    &lt;p&gt;Since I have a 3D printer, and once you have one, every problem can be solved by printing stuff, I printed the new "battery" to accommodate a large capacitor, diode (for voltage drop), wires, DuPont connectors, and the original battery's BCM.&lt;/p&gt;
    &lt;p&gt;N900 with a new "battery". Fits really tight, and only 0.25-0.5mm too tall, so the cover still snaps closed.&lt;/p&gt;
    &lt;p&gt;Boots without problems. Since the attached capacitor is pretty large, it can take a minute or two to charge it to an acceptable level (~4.0V) with a 0.5A current.&lt;/p&gt;
    &lt;p&gt;Nokia N900 enjoying its new life as an online radio device using Open Media Player.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239177</guid><pubDate>Fri, 12 Dec 2025 00:04:29 +0000</pubDate></item><item><title>Laying out the 404 Media zine</title><link>https://tedium.co/2025/12/10/404-media-zine-linux-affinity/?</link><description>&lt;doc fingerprint="3e12c577bf35a7fa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WINE Cooler&lt;/head&gt;
    &lt;head rend="h2"&gt;Lessons on laying out the 404 Media zine using a relatively weird setup—on Linux, using Affinity, with the help of the Windows translation layer WINE.&lt;/head&gt;
    &lt;p&gt;I write a lot these days, but my path into journalism, going way back to J-School, was through layout.&lt;/p&gt;
    &lt;p&gt;For years, I was a graphic designer at a number of newspapers—some fairly small, some quite large. I was a card-carrying member of the Society for News Design. It was one of my biggest passions, and I fully expected to have a long career in newspaper design. But newspapers as a medium haven’t really panned out, so I eventually fell into writing.&lt;/p&gt;
    &lt;p&gt;But I still adore laying out a big project, conceptualizing it, and trying to use it to visually add to the story that the words are trying to convey. It’s not quite a lost art, but I do think that print layout is something that has been a bit back-burnered by society at large.&lt;/p&gt;
    &lt;p&gt;So when 404 Media co-founder Jason Koebler, who spent years editing my writing for Motherboard, reached out about doing a zine, I was absolutely in. The goal of the zine—to shine a spotlight on the intersection of ICE and surveillance tech—was important. Plus, I like working with Jason, and it was an opportunity to get into print design again after quite a few years away.&lt;/p&gt;
    &lt;p&gt;I just had two problems: One, I have decided that I no longer want to give Adobe money because of cost and ethical concerns about its business model. And two, I now use Linux pretty much exclusively (Bazzite DX, in case you’re wondering).&lt;/p&gt;
    &lt;p&gt;But the good news is that the open-source community has done a lot of work, and despite my own tech shifts, professional-grade print design on Linux is now a viable option.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why page layout on Linux is fairly uncommon&lt;/head&gt;
    &lt;p&gt;The meme in the Linux community writes itself: “I would move over to Linux, but I need Photoshop and InDesign and [insert app here] too much.” In the past, this has been a real barrier for designers, especially those who rely on print layout, where open-source alternatives are very limited. (They’ve also been traditionally at the mercy of print shops that have no time for your weird non-standard app.)&lt;/p&gt;
    &lt;p&gt;Admittedly, the native tools have been getting better. I’m not really a fan myself, but I know GIMP is getting closer in parity to Photoshop. Inkscape is a totally viable vector drawing app. Video is very doable on Linux thanks to the FOSS Kendenlive and the commercial DaVinci Resolve. Blender is basically a de facto standard for 3D at this point. The web-based Penpot is a capable Figma alternative. And Krita, while promoted as a digital painting app, has become my tool of choice for making frame-based animated GIFs, which I do a lot for Tedium.&lt;/p&gt;
    &lt;p&gt;But for ink-stained print layout nerds, it has been tougher to make the shift (our apologies to Scribus). And Adobe locks down Creative Cloud pretty hard.&lt;/p&gt;
    &lt;p&gt;However, the recent Affinity release, while drawing some skepticism from the open-source community as a potential enshittification issue, is starting to open up a fresh lane. For those not aware, the new version of Affinity essentially combines the three traditional design apps—vector editor, raster editor, and page layout—into a single tool. It’s pretty good at all three. (Plus, for business reasons related to its owner Canva, it’s currently free to use.)&lt;/p&gt;
    &lt;p&gt;While it doesn’t have a dedicated Linux version, it more or less runs very well using WINE, the technology that has enabled a Linux renaissance via the Steam Deck. (Some passionate community members, like the WINE hacker ElementalWarrior, have worked hard to make this a fully-fleshed out experience that can even be installed more or less painlessly.)&lt;/p&gt;
    &lt;p&gt;The desire for a native Linux version of a pro-level design app is such that the Canva subsidiary is thinking about doing it themselves.&lt;/p&gt;
    &lt;p&gt;But I’m not the kind of person who likes to wait, so I decided to try to build as much of the zine as I could with Affinity for page layout. For the few things I couldn’t do, I would remote into a Mac.&lt;/p&gt;
    &lt;head rend="h3"&gt;The RISO factor&lt;/head&gt;
    &lt;p&gt;Another consideration here is the fact that this zine is being built with Risograph printing, a multicolor printing approach distinct from the more traditional CMYK. The inky printing process, similar to screen printing, has a distinct, vibrant look, even if it avoids the traditional four-color approach (in our case, using layers of pink, black, and lime green).&lt;/p&gt;
    &lt;p&gt;Throughout the process, I spent a lot of time setting layers to multiply to ensure the results looked good, and adding effects like halftone and erase to help balance out the color effects. This mostly worked OK, though I did have some glitches.&lt;/p&gt;
    &lt;p&gt;At one point, a lime-green frog lost much of its detail when I tried to RISO-fy it, requiring me to double-check my color settings and ensure I was getting the right tone. And sometimes, PDF exports from Affinity added unsightly lines, which I had to go out of my way to remove. If I was designing for newspapers, I might have been forced to come up with a quick plan B for that layout. But fortunately, I had the luxury of not working on a daily deadline like I might have back in the day.&lt;/p&gt;
    &lt;p&gt;I think that this layout approach is genuinely fascinating—and I know Jason in particular is a huge fan of it. Could I see other publications in the 404 mold taking notes from this and doing the same thing? Heck yes.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ups and downs of print layout on Linux&lt;/head&gt;
    &lt;p&gt;So, the headline you can take away from this is pretty simple: Laying stuff out in Affinity over Linux is extremely doable, and if you’re doing it occasionally, you will find a quite capable tool.&lt;/p&gt;
    &lt;p&gt;Admittedly, if this was, like, my main gig, I might still feel the urge to go back to MacOS—especially near the end of the process. Here’s what I learned:&lt;/p&gt;
    &lt;p&gt;The good: Workflow-wise, it was pretty smooth. Image cutouts—a tightly honed skill of mine that AI has been trying to obsolete for years—were very doable. Affinity also has some great effects tools that in many ways beat equivalents in other apps, such as its glitch tool and its live filter layers. It didn’t feel like I was getting a second-class experience when all was said and done.&lt;/p&gt;
    &lt;p&gt;The bad: My muscle memory for InDesign shortcuts was completely ineffective for this, and there were occasional features of InDesign and Photoshop that I did not find direct equivalents for in Affinity. WINE’s file menus tend to look like old Windows, which might be a turn-off for UX purists, and required a bit of extra navigation to dig through folders. Also, one downside of WINE that I could not work past was that I couldn’t use my laptop’s Intel-based GPU for machine learning tasks, a known bug that I imagine slowed some things down on graphically intensive pages.&lt;/p&gt;
    &lt;p&gt;The ugly: I think one area Affinity will need to work on as it attempts to sell the idea that you can design in one interface are better strategies to help mash down content for export. At one point while I was trying to make a PDF, Affinity promised me that the file I would be exporting was going to be 17 exabytes in size, which my SSD was definitely not large enough for. That wasn’t true, but it does emphasize that the dream of doing everything in one interface gets complicated when you want to send things to the printer. Much of the work I did near the end of the process was rasterizing layers to ensure everything looked as intended.&lt;/p&gt;
    &lt;p&gt;When I did have to use a Mac app for something (mainly accessing Spectrolite, a prepress app for RISO designs), I accessed an old Hackintosh using NoMachine, a tool for connecting to computers remotely. So even for the stuff I actually needed MacOS for, I didn’t need to leave the comforts of my janky laptop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking for a Big Tech escape hatch&lt;/head&gt;
    &lt;p&gt;Was it 100% perfect? No. Affinity crashed every once in a while, but InDesign did that all the time back in the day. And admittedly, an office full of people using Affinity on Linux isn’t going to work as well as one guy in a coffee shop working with a team of editors over chat and email.&lt;/p&gt;
    &lt;p&gt;But it’s my hope that experiences like mine convince other people to try it, and for companies to embrace it. Affinity isn’t open-source, and Canva is a giant company with plenty of critics, just like Adobe. But there are emerging projects like PixiEditor and Graphite that could eventually make print layout an extremely viable and even modern open-source endeavor.&lt;/p&gt;
    &lt;p&gt;But we have to take victories where we can find them, and the one I see is that Affinity is a lot less locked down than Creative Cloud, which is why it’s viable on Linux. And in general, this feels like an opportunity to get away from the DRM-driven past of creative software. (Hey Canva, it’s never too late to make Affinity open-source.)&lt;/p&gt;
    &lt;p&gt;Difficult reporting shouldn’t have to be tethered to the whims of Big Tech to exist. Especially when that tech—on Amazon’s cloud, using Adobe’s PDFs, through Google’s search, over Meta’s social network, with Apple’s phones, and on Microsoft’s operating system—too often causes uncomfortable tensions with the reporting. This is one step towards a better escape hatch.&lt;/p&gt;
    &lt;head rend="h5"&gt;Laid-Out Links&lt;/head&gt;
    &lt;p&gt;Speaking of useful image tools, I want to give a shout to Imagor Studio, a self-hosted image management app. It’s actually an extension of a tool Tedium uses for image rendering, Imagor.&lt;/p&gt;
    &lt;p&gt;The video creator David Hoffman has been pulling absolute gems from his archive over the years, but this one, in which telephone operators discuss their feelings on automation, really hits home.&lt;/p&gt;
    &lt;p&gt;And yeah, I did a Nieman Lab prediction this year. It’s about my recent hobbyhorse, Grokipedia.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;Find this one an interesting read? Share it with a pal—and pick up 404 Media’s upcoming zine.&lt;/p&gt;
    &lt;p&gt;Want to resist the impending doom of big tech? Check out our sponsor la machine, a beautiful machine that won’t even let you hit the on button.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239188</guid><pubDate>Fri, 12 Dec 2025 00:05:36 +0000</pubDate></item><item><title>Stoolap: High-performance embedded SQL database in pure Rust</title><link>https://github.com/stoolap/stoolap</link><description>&lt;doc fingerprint="4130edfdec314ad6"&gt;
  &lt;main&gt;
    &lt;p&gt;Stoolap is an embedded SQL database with MVCC transactions, written entirely in Rust. It supports both in-memory and persistent storage modes with full ACID compliance.&lt;/p&gt;
    &lt;code&gt;# Add to Cargo.toml
[dependencies]
stoolap = "0.1"&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/stoolap/stoolap.git
cd stoolap
cargo build --release&lt;/code&gt;
    &lt;code&gt;use stoolap::api::Database;

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let db = Database::open_in_memory()?;

    db.execute("CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)", ())?;
    db.execute("INSERT INTO users VALUES (1, 'Alice')", ())?;

    for row in db.query("SELECT * FROM users", ())? {
        let row = row?;
        println!("{}: {}", row.get::&amp;lt;i64&amp;gt;(0)?, row.get::&amp;lt;String&amp;gt;(1)?);
    }

    Ok(())
}&lt;/code&gt;
    &lt;code&gt;./stoolap                                    # In-memory REPL
./stoolap --db "file:///path/to/data"        # Persistent database
./stoolap -q "SELECT 1 + 1"                  # Execute query directly&lt;/code&gt;
    &lt;p&gt;Full multi-version concurrency control with two isolation levels:&lt;/p&gt;
    &lt;code&gt;-- Read Committed (default)
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;
UPDATE accounts SET balance = balance + 100 WHERE id = 2;
COMMIT;

-- Snapshot Isolation
BEGIN TRANSACTION ISOLATION LEVEL SNAPSHOT;
SELECT * FROM accounts;  -- Consistent view throughout transaction
COMMIT;&lt;/code&gt;
    &lt;p&gt;Query historical data at any point in time:&lt;/p&gt;
    &lt;code&gt;-- Query data as it existed at a specific timestamp
SELECT * FROM orders AS OF TIMESTAMP '2024-01-15 10:30:00';

-- Query data as of a specific transaction
SELECT * FROM inventory AS OF TRANSACTION 1234;

-- Compare current vs historical data
SELECT
    current.price,
    historical.price AS old_price
FROM products current
JOIN products AS OF TIMESTAMP '2024-01-01' historical
    ON current.id = historical.id
WHERE current.price != historical.price;&lt;/code&gt;
    &lt;p&gt;Stoolap automatically selects optimal index types, or you can specify explicitly:&lt;/p&gt;
    &lt;code&gt;-- B-tree: Range queries, sorting, prefix matching
CREATE INDEX idx_date ON orders(created_at) USING BTREE;
SELECT * FROM orders WHERE created_at BETWEEN '2024-01-01' AND '2024-12-31';

-- Hash: O(1) equality lookups
CREATE INDEX idx_email ON users(email) USING HASH;
SELECT * FROM users WHERE email = 'alice@example.com';

-- Bitmap: Low-cardinality columns, efficient AND/OR
CREATE INDEX idx_status ON orders(status) USING BITMAP;
SELECT * FROM orders WHERE status = 'pending' AND priority = 'high';

-- Multi-column composite indexes
CREATE INDEX idx_lookup ON events(user_id, event_type, created_at);
SELECT * FROM events WHERE user_id = 100 AND event_type = 'click';&lt;/code&gt;
    &lt;p&gt;Full support for analytical queries:&lt;/p&gt;
    &lt;code&gt;SELECT
    employee_name,
    department,
    salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank,
    salary - LAG(salary) OVER (ORDER BY hire_date) as salary_change,
    AVG(salary) OVER (PARTITION BY department) as dept_avg,
    SUM(salary) OVER (ORDER BY hire_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total
FROM employees;&lt;/code&gt;
    &lt;p&gt;Including recursive queries:&lt;/p&gt;
    &lt;code&gt;-- Non-recursive CTE
WITH high_value_orders AS (
    SELECT * FROM orders WHERE amount &amp;gt; 1000
)
SELECT customer_id, COUNT(*) FROM high_value_orders GROUP BY customer_id;

-- Recursive CTE (e.g., organizational hierarchy)
WITH RECURSIVE org_chart AS (
    SELECT id, name, manager_id, 1 as level
    FROM employees WHERE manager_id IS NULL

    UNION ALL

    SELECT e.id, e.name, e.manager_id, oc.level + 1
    FROM employees e
    JOIN org_chart oc ON e.manager_id = oc.id
)
SELECT * FROM org_chart ORDER BY level, name;&lt;/code&gt;
    &lt;code&gt;-- ROLLUP: Hierarchical subtotals
SELECT region, product, SUM(sales)
FROM sales_data
GROUP BY ROLLUP(region, product);

-- CUBE: All possible subtotal combinations
SELECT region, product, SUM(sales)
FROM sales_data
GROUP BY CUBE(region, product);&lt;/code&gt;
    &lt;p&gt;Scalar, correlated, EXISTS, and IN subqueries:&lt;/p&gt;
    &lt;code&gt;-- Correlated subquery
SELECT * FROM employees e
WHERE salary &amp;gt; (SELECT AVG(salary) FROM employees WHERE department = e.department);

-- EXISTS
SELECT * FROM customers c
WHERE EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id AND o.amount &amp;gt; 1000);

-- IN with subquery
SELECT * FROM products
WHERE category_id IN (SELECT id FROM categories WHERE active = true);&lt;/code&gt;
    &lt;p&gt;Cost-based optimizer with statistics:&lt;/p&gt;
    &lt;code&gt;-- Collect table statistics
ANALYZE orders;

-- View query execution plan
EXPLAIN SELECT * FROM orders WHERE customer_id = 100;

-- View plan with actual execution statistics
EXPLAIN ANALYZE SELECT * FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE c.country = 'US';&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;INTEGER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;64-bit signed integer&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;42&lt;/code&gt;, &lt;code&gt;-100&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;FLOAT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;64-bit floating point&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;3.14&lt;/code&gt;, &lt;code&gt;-0.001&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TEXT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;UTF-8 string&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;'hello'&lt;/code&gt;, &lt;code&gt;'日本語'&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BOOLEAN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;true/false&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TIMESTAMP&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Date and time&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;'2024-01-15 10:30:00'&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;JSON&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JSON data&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;'{"key": "value"}'&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;UPPER&lt;/code&gt;, &lt;code&gt;LOWER&lt;/code&gt;, &lt;code&gt;LENGTH&lt;/code&gt;, &lt;code&gt;TRIM&lt;/code&gt;, &lt;code&gt;LTRIM&lt;/code&gt;, &lt;code&gt;RTRIM&lt;/code&gt;, &lt;code&gt;CONCAT&lt;/code&gt;, &lt;code&gt;SUBSTRING&lt;/code&gt;, &lt;code&gt;REPLACE&lt;/code&gt;, &lt;code&gt;REVERSE&lt;/code&gt;, &lt;code&gt;LEFT&lt;/code&gt;, &lt;code&gt;RIGHT&lt;/code&gt;, &lt;code&gt;LPAD&lt;/code&gt;, &lt;code&gt;RPAD&lt;/code&gt;, &lt;code&gt;REPEAT&lt;/code&gt;, &lt;code&gt;POSITION&lt;/code&gt;, &lt;code&gt;LOCATE&lt;/code&gt;, &lt;code&gt;INSTR&lt;/code&gt;, &lt;code&gt;SPLIT_PART&lt;/code&gt;, &lt;code&gt;INITCAP&lt;/code&gt;, &lt;code&gt;ASCII&lt;/code&gt;, &lt;code&gt;CHR&lt;/code&gt;, &lt;code&gt;TRANSLATE&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ABS&lt;/code&gt;, &lt;code&gt;CEIL&lt;/code&gt;, &lt;code&gt;FLOOR&lt;/code&gt;, &lt;code&gt;ROUND&lt;/code&gt;, &lt;code&gt;TRUNC&lt;/code&gt;, &lt;code&gt;SQRT&lt;/code&gt;, &lt;code&gt;POWER&lt;/code&gt;, &lt;code&gt;MOD&lt;/code&gt;, &lt;code&gt;SIGN&lt;/code&gt;, &lt;code&gt;GREATEST&lt;/code&gt;, &lt;code&gt;LEAST&lt;/code&gt;, &lt;code&gt;EXP&lt;/code&gt;, &lt;code&gt;LN&lt;/code&gt;, &lt;code&gt;LOG&lt;/code&gt;, &lt;code&gt;LOG10&lt;/code&gt;, &lt;code&gt;LOG2&lt;/code&gt;, &lt;code&gt;SIN&lt;/code&gt;, &lt;code&gt;COS&lt;/code&gt;, &lt;code&gt;TAN&lt;/code&gt;, &lt;code&gt;ASIN&lt;/code&gt;, &lt;code&gt;ACOS&lt;/code&gt;, &lt;code&gt;ATAN&lt;/code&gt;, &lt;code&gt;ATAN2&lt;/code&gt;, &lt;code&gt;DEGREES&lt;/code&gt;, &lt;code&gt;RADIANS&lt;/code&gt;, &lt;code&gt;PI&lt;/code&gt;, &lt;code&gt;RAND&lt;/code&gt;, &lt;code&gt;RANDOM&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;NOW&lt;/code&gt;, &lt;code&gt;CURRENT_DATE&lt;/code&gt;, &lt;code&gt;CURRENT_TIME&lt;/code&gt;, &lt;code&gt;CURRENT_TIMESTAMP&lt;/code&gt;, &lt;code&gt;EXTRACT&lt;/code&gt;, &lt;code&gt;DATE_TRUNC&lt;/code&gt;, &lt;code&gt;DATE_ADD&lt;/code&gt;, &lt;code&gt;DATE_SUB&lt;/code&gt;, &lt;code&gt;DATEDIFF&lt;/code&gt;, &lt;code&gt;YEAR&lt;/code&gt;, &lt;code&gt;MONTH&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;HOUR&lt;/code&gt;, &lt;code&gt;MINUTE&lt;/code&gt;, &lt;code&gt;SECOND&lt;/code&gt;, &lt;code&gt;DAYOFWEEK&lt;/code&gt;, &lt;code&gt;DAYOFYEAR&lt;/code&gt;, &lt;code&gt;WEEK&lt;/code&gt;, &lt;code&gt;QUARTER&lt;/code&gt;, &lt;code&gt;TO_CHAR&lt;/code&gt;, &lt;code&gt;TO_DATE&lt;/code&gt;, &lt;code&gt;TO_TIMESTAMP&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;JSON_EXTRACT&lt;/code&gt;, &lt;code&gt;JSON_EXTRACT_PATH&lt;/code&gt;, &lt;code&gt;JSON_TYPE&lt;/code&gt;, &lt;code&gt;JSON_TYPEOF&lt;/code&gt;, &lt;code&gt;JSON_VALID&lt;/code&gt;, &lt;code&gt;JSON_KEYS&lt;/code&gt;, &lt;code&gt;JSON_ARRAY_LENGTH&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;COUNT&lt;/code&gt;, &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;AVG&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;STDDEV&lt;/code&gt;, &lt;code&gt;STDDEV_POP&lt;/code&gt;, &lt;code&gt;STDDEV_SAMP&lt;/code&gt;, &lt;code&gt;VARIANCE&lt;/code&gt;, &lt;code&gt;VAR_POP&lt;/code&gt;, &lt;code&gt;VAR_SAMP&lt;/code&gt;, &lt;code&gt;STRING_AGG&lt;/code&gt;, &lt;code&gt;ARRAY_AGG&lt;/code&gt;, &lt;code&gt;FIRST&lt;/code&gt;, &lt;code&gt;LAST&lt;/code&gt;, &lt;code&gt;BIT_AND&lt;/code&gt;, &lt;code&gt;BIT_OR&lt;/code&gt;, &lt;code&gt;BIT_XOR&lt;/code&gt;, &lt;code&gt;BOOL_AND&lt;/code&gt;, &lt;code&gt;BOOL_OR&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ROW_NUMBER&lt;/code&gt;, &lt;code&gt;RANK&lt;/code&gt;, &lt;code&gt;DENSE_RANK&lt;/code&gt;, &lt;code&gt;NTILE&lt;/code&gt;, &lt;code&gt;LAG&lt;/code&gt;, &lt;code&gt;LEAD&lt;/code&gt;, &lt;code&gt;FIRST_VALUE&lt;/code&gt;, &lt;code&gt;LAST_VALUE&lt;/code&gt;, &lt;code&gt;NTH_VALUE&lt;/code&gt;, &lt;code&gt;PERCENT_RANK&lt;/code&gt;, &lt;code&gt;CUME_DIST&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;COALESCE&lt;/code&gt;, &lt;code&gt;NULLIF&lt;/code&gt;, &lt;code&gt;CAST&lt;/code&gt;, &lt;code&gt;CASE&lt;/code&gt;, &lt;code&gt;IF&lt;/code&gt;, &lt;code&gt;IIF&lt;/code&gt;, &lt;code&gt;NVL&lt;/code&gt;, &lt;code&gt;NVL2&lt;/code&gt;, &lt;code&gt;DECODE&lt;/code&gt;, &lt;code&gt;GREATEST&lt;/code&gt;, &lt;code&gt;LEAST&lt;/code&gt;, &lt;code&gt;GENERATE_SERIES&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Stoolap uses write-ahead logging (WAL) with periodic snapshots:&lt;/p&gt;
    &lt;code&gt;# In-memory (default) - data lost on exit
./stoolap --db "memory://"

# File-based - durable storage
./stoolap --db "file:///var/lib/stoolap/data"&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WAL: All changes logged before applied, survives crashes&lt;/item&gt;
      &lt;item&gt;Snapshots: Periodic full database snapshots for faster recovery&lt;/item&gt;
      &lt;item&gt;Index persistence: All indexes saved and restored&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;src/
├── api/        # Public API (Database, Connection, Rows)
├── core/       # Types (Value, Row, Schema, Error)
├── parser/     # SQL lexer and parser
├── planner/    # Query planning
├── optimizer/  # Cost-based query optimizer
├── executor/   # Query execution engine
├── functions/  # 100+ built-in functions
│   ├── scalar/     # String, math, date, JSON
│   ├── aggregate/  # COUNT, SUM, AVG, etc.
│   └── window/     # ROW_NUMBER, RANK, LAG, etc.
└── storage/    # Storage engine
    ├── mvcc/       # Multi-version concurrency control
    └── index/      # B-tree, Hash, Bitmap indexes
&lt;/code&gt;
    &lt;code&gt;cargo build              # Debug build
cargo build --release    # Release build (optimized)
cargo test               # Run tests
cargo clippy             # Lint
cargo doc --open         # Generate documentation&lt;/code&gt;
    &lt;p&gt;See CONTRIBUTING.md for guidelines.&lt;/p&gt;
    &lt;p&gt;Apache License 2.0. See LICENSE.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239372</guid><pubDate>Fri, 12 Dec 2025 00:28:24 +0000</pubDate></item><item><title>CRISPR fungus: Protein-packed, sustainable, and tastes like meat</title><link>https://www.isaaa.org/kc/cropbiotechupdate/article/default.asp?ID=21607</link><description>&lt;doc fingerprint="663d925d0c255a11"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;CRISPR Fungus: Protein-Packed, Sustainable, and Tastes Like Meat&lt;/head&gt;November 26, 2025&lt;table&gt;&lt;row/&gt;&lt;/table&gt;&lt;p&gt;Researchers have successfully used CRISPR gene editing technology to create a fungi strain that is highly efficient, more nutritious, and significantly more sustainable than its natural counterpart. The fungus Fusarium venenatum already stands out for its meat-like flavor and texture, leading to its approval for food use in several countries. This breakthrough, published in the journal Trends in Biotechnology, addresses the need for better, more environmentally friendly alternatives to conventional animal agriculture, which accounts for about 14% of global greenhouse gas emissions.&lt;/p&gt;&lt;p&gt;The scientists, led by corresponding author Xiao Liu of Jiangnan University, used CRISPR to remove two specific genes. The first modification, eliminating a gene for chitin synthase, resulted in thinner fungal cell walls. This change is crucial as it makes the fungal protein easier for humans to digest and increases its bioavailability. The second change involved removing the pyruvate decarboxylase gene, which optimized the fungus's metabolism. This fine-tuning made the new strain, called FCPD, more productive, requiring 44% less sugar to produce the same amount of protein and doing so 88% faster than the original strain.&lt;/p&gt;&lt;p&gt;When scaled up, FCPD production showed a lower environmental footprint regardless of the manufacturing location, reducing greenhouse gas emissions by up to 60% over its life cycle compared to traditional fungal protein production. Furthermore, compared to chicken production in China, the new myoprotein requires 70% less land and reduces the risk of freshwater pollution by 78%. According to the researchers, this type of gene-edited food can help meet global food demands without the substantial environmental costs associated with conventional farming, representing a major advancement in the field of sustainable food technology.&lt;/p&gt;&lt;p&gt;For more details, read this article or download the open-access paper.&lt;/p&gt;&lt;table&gt;&lt;row/&gt;&lt;/table&gt;&lt;head rend="h3"&gt;You might also like:&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Experts Highlight Hybrid Foods as Meat Alternatives&lt;/item&gt;&lt;item&gt;Gene Editing of Edible Fungus Produces Healthier Foods&lt;/item&gt;&lt;item&gt;Experts Say Effective Communication Can Promote Consumer Acceptance of Cultured Meat&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Biotech Updates is a weekly newsletter of ISAAA, a not-for-profit organization. It is distributed for free to over 22,000 subscribers worldwide to inform them about the key developments in biosciences, especially in biotechnology. Your support will help us in our mission to feed the world with knowledge. You can help by donating as little as $10.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;head rend="h3"&gt;See more articles:&lt;/head&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Plant&lt;/head&gt;&lt;/item&gt;&lt;item&gt;CRISPR Alters Chromosome Numbers in Plants&lt;/item&gt;&lt;item&gt;Precision Breeding Act Now Live in the UK&lt;/item&gt;&lt;item&gt;Philippine Regulators Work on Refining Risk Assessment Report on GM Plant and Plant Products&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Animal&lt;/head&gt;&lt;/item&gt;&lt;item&gt;Targeted Inheritance of Sex Offers a New Method to Improve Animal Breeding&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Food&lt;/head&gt;&lt;/item&gt;&lt;item&gt;Single Gene Discovery Promises Better Tea Taste and Yield&lt;/item&gt;&lt;item&gt;CRISPR Fungus: Protein-Packed, Sustainable, and Tastes Like Meat&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Environment&lt;/head&gt;&lt;/item&gt;&lt;item&gt;UN Climate Change Conference Highlights Role of Agrifood Systems in Climate Action&lt;/item&gt;&lt;item&gt;University of Waterloo Researchers Turn to Biotechnology to Combat Plastic Pollution&lt;/item&gt;&lt;item&gt;&lt;lb/&gt;Read the latest:&lt;/item&gt;&lt;item&gt;Biotech Updates (December 10, 2025)&lt;/item&gt;&lt;item&gt;Gene Editing Supplement (November 26, 2025)&lt;/item&gt;&lt;item&gt;Gene Drive Supplement (February 22, 2023)&lt;/item&gt;&lt;item&gt;&lt;lb/&gt;Subscribe to BU:&lt;/item&gt;&lt;item&gt;Share&lt;/item&gt;&lt;item&gt;Tweet&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239629</guid><pubDate>Fri, 12 Dec 2025 00:59:46 +0000</pubDate></item><item><title>Google de-indexed Bear Blog and I don't know why</title><link>https://journal.james-zhan.com/google-de-indexed-my-entire-bear-blog-and-i-dont-know-why/</link><description>&lt;doc fingerprint="150021f738f60ba3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google De-Indexed My Entire Bear Blog and I Don’t Know Why&lt;/head&gt;
    &lt;p&gt;Preamble: The whole affair is Google’s fault and not Bear Blog’s. Huge thanks to Herman—Bear Blog’s founder and dev—for his patience and help.&lt;/p&gt;
    &lt;p&gt;A month after I started my first Bear blog at blog.james-zhan.com, my blog was entirely de-indexed by Google for no apparent reason:&lt;/p&gt;
    &lt;p&gt;I have since migrated to journal.james-zhan.com (you are on it right now) and redirected all links from blog.james-zhan.com accordingly, but to this day, I don’t understand what happened, and so I’m putting this post out there to see if perhaps anyone could shed some light—you are welcome to email me or leave a comment at the bottom of the post.&lt;/p&gt;
    &lt;p&gt;Let me backtrack and show you how it all went down.&lt;/p&gt;
    &lt;head&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;At first, all was well&lt;lb/&gt; Where it started to go wrong&lt;lb/&gt; Was that a coincidence or the cause of the issue?&lt;lb/&gt; It got worse&lt;lb/&gt; Extensive troubleshooting&lt;lb/&gt; Suspect #1: Something’s up with the domain&lt;lb/&gt; Suspect #2: Quality of blog content&lt;lb/&gt; Suspect #3: Lack of internal linking&lt;lb/&gt; Other suspects, eliminated with Herman’s help (thank you Herman!)&lt;lb/&gt; My blog was properly indexed by other search engines&lt;lb/&gt; What I ended up doing&lt;/p&gt;
    &lt;head rend="h2"&gt;At first, all was well&lt;/head&gt;
    &lt;p&gt;My blog went live on Oct 4 and I published a lengthy, well-research opinion piece commenting on a recent event.&lt;/p&gt;
    &lt;p&gt;Because of that, I wanted the article to show up on Google ASAP so that when people searched about the event, maybe my article would come up. I knew that it could take a while for Google to naturally crawl and index a new site, so to accelerate the process, I went on Google Search Console (GSC), submitted the sitemap and requested indexing on my article.&lt;/p&gt;
    &lt;p&gt;And it worked—the next day, my blog and articles were indexed and showed up on Google if you put in the right search terms.&lt;/p&gt;
    &lt;p&gt;In GSC, you can even see that I was getting some impressions and clicks at the time from the exact topic that my opinion piece was about. Great!&lt;/p&gt;
    &lt;p&gt;From then on, every time I published a new post, I would go to GSC and request indexing for the post URL, and my post would be on Google search results shortly after, as expected.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where it started to go wrong&lt;/head&gt;
    &lt;p&gt;On Oct 14, as I was digging around GSC, I noticed that it was telling me that one of the URLs weren’t indexed. I thought that was weird, and not being very familiar with GSC, I went ahead and clicked the “Validate” button.&lt;/p&gt;
    &lt;p&gt;Only after did I realized that URL was the RSS feed subscribe link, &lt;code&gt;https://blog.james-zhan.com/feed/?type=rss&lt;/code&gt;, which wasn’t even a page so it made sense that it hadn’t been indexed, but it was too late and there was no way for me to stop the validation.&lt;/p&gt;
    &lt;p&gt;I received an email from GSC telling me it was validating that 1 page with indexing issues:&lt;/p&gt;
    &lt;p&gt;Four days later, on Oct 20, I received an email from GSC saying “Some fixes failed for Page indexing issues on site https://blog.james-zhan.com/” and when I searched “site:blog.james-zhan.com,” I saw that all but one of my blog posts had been de-indexed:&lt;/p&gt;
    &lt;p&gt;All of them showed the same reason:&lt;/p&gt;
    &lt;p&gt;“Page is not indexed: Crawled – currently not indexed”&lt;/p&gt;
    &lt;p&gt;Confused, I poked around GSC to see if it showed me why, and I couldn’t find anything useful, so I resubmitted the sitemap for good measure, and clicked “Validate” again.&lt;/p&gt;
    &lt;p&gt;I even requested indexing for all the individual blog post URLs and that didn’t do anything.&lt;/p&gt;
    &lt;p&gt;As of the publishing of this post, the validation status is still “Started” (it’s been nearly 20 days).&lt;/p&gt;
    &lt;head rend="h2"&gt;Was that a coincidence or the cause of the issue?&lt;/head&gt;
    &lt;p&gt;As I was troubleshooting, I noticed that the day that I initiated the validation for the first time (Oct 14) was the same day that all but one of my blog posts got de-indexed:&lt;/p&gt;
    &lt;p&gt;Did my accidental attempt to make GSC index &lt;code&gt;https://blog.james-zhan.com/feed/?type=rss&lt;/code&gt; cause some kind of glitch, thereby de-indexing the rest of the blog?&lt;/p&gt;
    &lt;p&gt;I don’t get why it would, but it’s weird that the two events happened on the same day.&lt;/p&gt;
    &lt;head rend="h2"&gt;It got worse&lt;/head&gt;
    &lt;p&gt;While this was going on, I continued to post a few articles, and you can see that all the new posts faced the “Page is not indexed: Crawled – currently not indexed” error:&lt;/p&gt;
    &lt;p&gt;And then on Nov 3, I discovered that the remaining, single blog post that had been indexed just got de-indexed as well:&lt;/p&gt;
    &lt;p&gt;So basically, no one could find my blog on Google.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extensive troubleshooting&lt;/head&gt;
    &lt;p&gt;I’m not a web dev or programmer, but I tried my best to cover as much ground as possible in my troubleshooting to narrow down the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #1: Something’s up with the domain&lt;/head&gt;
    &lt;p&gt;The root domain, james-zhan.com, was from GoDaddy. I’ve had this domain for many years and I’ve used it on different sites and never had an issue with Google’s indexing.&lt;/p&gt;
    &lt;p&gt;For example, just this year, I created a new subdomain with it and that’s been indexed by Google.&lt;/p&gt;
    &lt;p&gt;I also don’t touch any advanced configuration with DNS records or what have you—I don’t have knowledge in that stuff, so it’s unlikely I somehow screwed up something in GoDaddy.&lt;/p&gt;
    &lt;p&gt;But just to be sure it wasn’t some wonky thing going on specifically with the Bear blog + GoDaddy combo, I created another Bear blog with the subdomain www.james-zhan.com.&lt;/p&gt;
    &lt;p&gt;This one shows up on Google no problem.&lt;/p&gt;
    &lt;p&gt;Conclusion: Domain wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #2: Quality of blog content&lt;/head&gt;
    &lt;p&gt;Whenever people discuss the indexing of their website in online forums, they always talk about the quality of the content being a huge factor. They say that your site isn’t indexed or isn’t ranked highly because your site doesn’t have much content, your content is low effort, or something like that.&lt;/p&gt;
    &lt;p&gt;First, I’m not worried about ranking—I just want my blog to be properly indexed.&lt;/p&gt;
    &lt;p&gt;Second, the issue couldn’t be the quality or the quantity of the content. I came across some other pretty barebones Bear blogs that don’t have much content, and looked them up on Google, and they showed up in the results just fine.&lt;/p&gt;
    &lt;p&gt;An example: Phong’s blog. It’s a very minimalist blog with only 6 posts (of great quality) and it shows up on Google search.&lt;/p&gt;
    &lt;p&gt;Conclusion: Quality or quantity of content wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #3: Lack of internal linking&lt;/head&gt;
    &lt;p&gt;I read about how the structure of a site can play a role in Google’s indexing.&lt;/p&gt;
    &lt;p&gt;Some say that if your blog posts’ URLs are all “orphaned,” like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;domain.com/post-title-1&lt;/item&gt;
      &lt;item&gt;domain.com/post-title-2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…instead of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;domain.com/blog/post-title-1&lt;/item&gt;
      &lt;item&gt;domain.com/blog/post-title-2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Allegedly, that might cause Google to not index your posts. By default, when you publish a post on Bear Blog, the blog post’s path isn’t preceded by “blog/.”&lt;/p&gt;
    &lt;p&gt;So I went around and checked the post URLs of other Bear blogs and saw that none of them had “/blog/” in them, and those blogs were indexed just fine. I also highly doubt it’s a real issue; otherwise, it wouldn’t be the default behaviour on Bear Blog.&lt;/p&gt;
    &lt;p&gt;Conclusion: Lack of internal linking wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other suspects, eliminated with Herman’s help (thank you Herman!)&lt;/head&gt;
    &lt;p&gt;I reached out to Herman with all the details and asked him for help. Of course, he responded promptly and helped me troubleshoot to identify the cause.&lt;/p&gt;
    &lt;p&gt;He was able to confirm and the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GoDaddy and DNS weren’t the cause&lt;/item&gt;
      &lt;item&gt;My bear blog had nothing that would prevent Google from indexing&lt;/item&gt;
      &lt;item&gt;HTML/CSS doesn’t affect SEO/indexing&lt;list rend="ul"&gt;&lt;item&gt;I had the following CSS code to put the tags above the blog post title, but Herman said this was fine&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;/* --- Move tags above the title --- */ main { display: flex; flex-direction: column; } /* Style and reposition the tags */ main &amp;gt; p.tags { order: -1; /* Moves tags above the title */ margin: 0 0 0.6rem 0; font-size: 0.9em; letter-spacing: 0.02em; color: var(--heading-color); opacity: 0.8; } /* Keep the title below tags */ main &amp;gt; h1 { order: 0; }&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I just wanted to take a moment to express my gratitude to Herman for investigating this with me. My emails to him were pretty elaborate with troubleshooting steps I had taken along with many screenshots. He took the time to fully understand the whole issue and even triple-checked my site to make sure everything was sound.&lt;/p&gt;
    &lt;p&gt;It was a refreshing tech support experience, and made me love Bear Blog as a platform just that much more.&lt;/p&gt;
    &lt;head rend="h3"&gt;My blog was properly indexed by other search engines&lt;/head&gt;
    &lt;p&gt;I don’t even have to use “site:”—just by searching “James Zhan blog,” both my blog and my www.james-zhan.com site show up in other search engines:&lt;/p&gt;
    &lt;p&gt;DuckDuckGo:&lt;/p&gt;
    &lt;p&gt;Bing:&lt;/p&gt;
    &lt;p&gt;Brave:&lt;/p&gt;
    &lt;p&gt;So there’s definitely nothing wrong on a technical level with my blog that would prevent Google from indexing it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I ended up doing&lt;/head&gt;
    &lt;p&gt;I copied my blog over to a different subdomain (you are on it right now), moved my domain from GoDaddy to Porkbun for URL forwarding, and set up URL forwarding with paths so any blog post URLs I posted online will automatically be redirected to the corresponding blog post on this new blog.&lt;/p&gt;
    &lt;p&gt;I also avoided submitting the sitemap of the new blog to GSC. I’m just gonna let Google naturally index the blog this time. Hopefully, this new blog won’t run into the same issue.&lt;/p&gt;
    &lt;p&gt;At this point, I’m no longer trying to resolve the issue, but just out of curiosity, I do want to know what the hell happened there. I’d had a previous site on GSC to track traffic for many years and never had such an issue.&lt;/p&gt;
    &lt;p&gt;If any of you have any guesses, I’d love to hear them (email me or leave a comment below)!&lt;/p&gt;
    &lt;p&gt;Subscribe to my blog via email or RSS feed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239752</guid><pubDate>Fri, 12 Dec 2025 01:20:05 +0000</pubDate></item><item><title>Cadmium Zinc Telluride: The wonder material powering a medical 'revolution'</title><link>https://www.bbc.com/news/articles/c24l223d9n7o</link><description>&lt;doc fingerprint="17cac8141cb484d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'It's amazing' – the wonder material very few can make&lt;/head&gt;
    &lt;p&gt;Lying on your back in a big hospital scanner, as still as you can, with your arms above your head – for 45 minutes. It doesn't sound much fun.&lt;/p&gt;
    &lt;p&gt;That's what patients at Royal Brompton Hospital in London had to do during certain lung scans, until the hospital installed a new device last year that cut these examinations down to just 15 minutes.&lt;/p&gt;
    &lt;p&gt;It is partly thanks to image processing technology in the scanner but also a special material called cadmium zinc telluride (CZT), which allows the machine to produce highly detailed, 3D images of patients' lungs.&lt;/p&gt;
    &lt;p&gt;"You get beautiful pictures from this scanner," says Dr Kshama Wechalekar, head of nuclear medicine and PET. "It's an amazing feat of engineering and physics."&lt;/p&gt;
    &lt;p&gt;The CZT in the machine, which was installed at the hospital last August, was made by Kromek – a British company. Kromek is one of just a few firms in the world that can make CZT. You may never have heard of the stuff but, in Dr Wechalekar's words, it is enabling a "revolution" in medical imaging.&lt;/p&gt;
    &lt;p&gt;This wonder material has many other uses, such as in X-ray telescopes, radiation detectors and airport security scanners. And it is increasingly sought-after.&lt;/p&gt;
    &lt;p&gt;Investigations of patients' lungs performed by Dr Wechalekar and her colleagues involve looking for the presence of many tiny blood clots in people with long Covid, or a larger clot known as a pulmonary embolism, for example.&lt;/p&gt;
    &lt;p&gt;The £1m scanner works by detecting gamma rays emitted by a radioactive substance that is injected into patients' bodies.&lt;/p&gt;
    &lt;p&gt;But the scanner's sensitivity means less of this substance is needed than before: "We can reduce doses about 30%," says Dr Wechalekar. While CZT-based scanners are not new in general, large, whole-body scanners such as this one are a relatively recent innovation.&lt;/p&gt;
    &lt;p&gt;CZT itself has been around for decades but it is notoriously difficult to manufacture. "It has taken a long time for it to develop into an industrial-scale production process," says Arnab Basu, founding chief executive of Kromek.&lt;/p&gt;
    &lt;p&gt;In the company's facility at Sedgefield, there are 170 small furnaces in a room that Dr Basu describes as looking "like a server farm".&lt;/p&gt;
    &lt;p&gt;A special powder is heated up in these furnaces, turned molten, and then solidified into a single-crystal structure. The whole process takes weeks. "Atom by atom, the crystals are rearranged […] so they become all aligned," says Dr Basu.&lt;/p&gt;
    &lt;p&gt;The newly formed CZT, a semiconductor, can detect tiny photon particles in X-rays and gamma rays with incredible precision – like a highly specialised version of the light-sensing, silicon-based image sensor in your smartphone camera.&lt;/p&gt;
    &lt;p&gt;Whenever a high energy photon strikes the CZT, it mobilises an electron and this electrical signal can be used to make an image. Earlier scanner technology used a two-step process, which was not as precise.&lt;/p&gt;
    &lt;p&gt;"It's digital," says Dr Basu. "It's a single conversion step. It retains all the important information such as timing, the energy of the X-ray that is hitting the CZT detector – you can create colour, or spectroscopic images."&lt;/p&gt;
    &lt;p&gt;He adds that CZT-based scanners are currently in use for explosives detection at UK airports, and for scanning checked baggage in some US airports. "We expect CZT to come into the hand luggage segment over the next [few] years."&lt;/p&gt;
    &lt;p&gt;But it's not always easy to get your hands on CZT.&lt;/p&gt;
    &lt;p&gt;Henric Krawczynski at Washington University in St Louis in the US has used the material before on space telescopes attached to high altitude balloons. These detectors can pick up X-rays emitted by both neutron stars and plasma around black holes.&lt;/p&gt;
    &lt;p&gt;Prof Krawczynski wants very thin, 0.8mm pieces of CZT for his telescopes because this helps to reduce the amount of background radiation they pick up, allowing for a clearer signal. "We'd like to buy 17 new detectors," he says. "It's really difficult to get these thin ones."&lt;/p&gt;
    &lt;p&gt;He was unable to source the CZT from Kromek. Dr Basu says his firm has high demand at the moment. "We support many, many research organisations," he adds, "It's very difficult for us to do a hundred different things. Each research [project] needs a very particular type of detector structure."&lt;/p&gt;
    &lt;p&gt;For Prof Krawczynski, it's not a crisis – he says he might use either CZT that he has from previous research, or cadmium telluride, an alternative, for his next mission.&lt;/p&gt;
    &lt;p&gt;However, there are bigger headaches at the moment. That upcoming mission was due to fly from Antarctica in December but "all the dates are in flux", says Prof Krawczynski, because of the US government shutdown.&lt;/p&gt;
    &lt;p&gt;Many other scientists use CZT. In the UK, a major upgrade of the Diamond Light Source research facility in Oxfordshire – costing half a billion pounds – will improve its capabilities thanks to the installation of CZT-based detectors.&lt;/p&gt;
    &lt;p&gt;Diamond Light Source is a synchrotron, which fires electrons around a giant ring at nearly the speed of light. Magnets cause these whizzing electrons to lose some energy in the form of X-rays, and these are directed off from the ring in beamlines so that they may be used to analyse materials, for example.&lt;/p&gt;
    &lt;p&gt;Some recent experiments have involved probing impurities in aluminium while it melts. Understanding those impurities better could help improve recycled forms of the metal.&lt;/p&gt;
    &lt;p&gt;With Diamond Light Source's upgrade, due to complete in 2030, the X-rays produced will be significantly brighter, meaning that existing sensors would not be able to detect them properly.&lt;/p&gt;
    &lt;p&gt;"There's no point in spending all this money in upgrading these facilities if you can't detect the light they produce," says Matt Veale, group leader for detector development at the Science and Technology Facilities Council, a stakeholder in the Diamond Light Source.&lt;/p&gt;
    &lt;p&gt;That's why, here too, CZT is the material of choice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239895</guid><pubDate>Fri, 12 Dec 2025 01:41:15 +0000</pubDate></item><item><title>The tiniest yet real telescope I've built</title><link>https://lucassifoni.info/blog/miniscope-tiny-telescope/</link><description>&lt;doc fingerprint="7765263a1da45dfc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The tiniest yet real telescope I've built&lt;/head&gt;
    &lt;p&gt;A “relaxation” project, mostly drawn on planes to and from Norway this month, where I had to travel to setup a digital art installation in Kristiansand with friends from the digital art collective Lab212. It has been drawn with one major constraint: it must fit in the inner pocket of my jacket (well, one specific jacket), except for the rods.&lt;/p&gt;
    &lt;p&gt;This is a 3D-printed dobsonian telescope built around a 76mm/300mm parabolic mirror kit. While there are plenty of mini-scope models on the internet, I wanted something that looked like a dobson that went a bit too hard through the clothes dryer, but without compromise on what matters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Balance&lt;/item&gt;
      &lt;item&gt;Smooth movements&lt;/item&gt;
      &lt;item&gt;Rigidity&lt;/item&gt;
      &lt;item&gt;Collimatable&lt;/item&gt;
      &lt;item&gt;Focusable eyepiece holder&lt;/item&gt;
      &lt;item&gt;A minimum of style (entirely subjective)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Hardware&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PETG-CF filament&lt;/item&gt;
      &lt;item&gt;4mm carbon rods&lt;/item&gt;
      &lt;item&gt;M3 screws and M3x4.5x4.5 heat-set inserts&lt;/item&gt;
      &lt;item&gt;A spring&lt;/item&gt;
      &lt;item&gt;Nylon screws to collimate both the primary and secondary mirrors&lt;/item&gt;
      &lt;item&gt;4 magnets for the secondary&lt;/item&gt;
      &lt;item&gt;A bit of paraffin to lubricate the focuser&lt;/item&gt;
      &lt;item&gt;A lycra light shroud that also helps with delaying dew forming on the mirrors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The focuser follows Analog Sky’s recipe: the tube that receives the eyepiece is also the movement itself, with a rounded thread that prints extremely smoothly with very little play. No additional hardware needed - the eyepiece is self-held by the flexion of plastic fins.&lt;/p&gt;
    &lt;p&gt;All the holes for the rods are straight, which forces them to arch, which “locks” the structure in place.&lt;/p&gt;
    &lt;p&gt;The alt/az movements use “teflon pads” (actually gray HDPE or UHMW for furniture feet) with rubber backing, scalped and glued.&lt;/p&gt;
    &lt;p&gt;Download the 3D files on Printables • Discussion on Astrosurf&lt;/p&gt;
    &lt;p&gt;If you build it, the real trick for ease of mounting is to chamfer the carbon rods with a 1mm chamfer at both ends and seal it with CA glue. See the chamfer pic in the gallery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optical tests&lt;/head&gt;
    &lt;p&gt;Sadly, the results aren’t great. We were used to very good l/6 or better recent buys from Aliexpress, but this one is very overcorrected. It was very smooth, with a rather good edge at the foucault, but is overcorrected by 70%. With the eyepiece I selected, putting it at 30x power, this does not show too much, and it retains its “real telescope” status. But this mirror is so small that I will not refigure it – the realuminizing costs would outweigh the entire project.&lt;/p&gt;
    &lt;p&gt;Edit as of dec. 11th : of course I did not resist re-figuring it. It now hovers around 0.9 strehl. The star test with the selected eyepiece shows nice symmetric defocused stars and I can now cound individual spider web strands and distinguish the dew droplets it carries, on a nearby electrical pole, whereas I did not even see the spider web with the mirror as it was from the factory. I still need to do a proper “showable” Bath report with enough interferograms, my last test was 4 interferograms and carries a ton of noise. So it is great but I now have to get it coated, and working a mirror this small did raise a few challenges in handling it.&lt;/p&gt;
    &lt;p&gt;All test pictures below are before refiguring&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46241763</guid><pubDate>Fri, 12 Dec 2025 07:35:49 +0000</pubDate></item><item><title>Smartphone without a battery (2022)</title><link>https://yaky.dev/2022-09-06-smartphone-without-battery/</link><description>&lt;doc fingerprint="b6e1a0e7e2f71fce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Smartphone without a battery&lt;/head&gt;
    &lt;p&gt;How to wire and run an old smartphone without a battery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intro&lt;/head&gt;
    &lt;p&gt;I have an old Samsung Galaxy S5 that I wanted to use to run my 3D printer. There is a great project called octo4a that runs OctoPrint on Android devices.&lt;/p&gt;
    &lt;p&gt;Using an old smartphone for OctoPrint is a perfect fit - it has USB OTG support to connect to the printer, WiFi to access the controls and upload models, and a camera to monitor the print progress. The only, yet critical, issue is that the kernel for Galaxy S5 does not support charging while it's connected to a USB device. The battery is old and worn, and cannot last through several hours of printing.&lt;/p&gt;
    &lt;p&gt;The approach I took was to build a "fake battery" circuit that emulates the battery, but is powered by 5V via USB.&lt;/p&gt;
    &lt;head rend="h2"&gt;Process&lt;/head&gt;
    &lt;p&gt;First, I removed the battery. Luckily, this smartphone is old enough to have user-serviceable battery compartment.&lt;/p&gt;
    &lt;p&gt;Older batteries might have only two terminals, (+) and (-). Newer batteries might have three or four terminals. I measured voltage and resistance between all terminals to find out what they might be.&lt;/p&gt;
    &lt;p&gt;These are the results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Terminals (+) and (-) are obvious. Generally, Li-ion battery will produce 3.4V when almost-empty and 4.2V when full. The battery also says "CHARGE VOLTAGE 4.4V", so that voltage level at the battery terminal would not cause any issues with the smartphone.&lt;/item&gt;
      &lt;item&gt;The second terminal is the thermistor (T), used to get the approximate temperature of the battery. The resistance between (T) and (-) is around 2350ohm at room temperature. Being a safety feature, my smartphone will not start at all if the thermistor terminal is not connected.&lt;/item&gt;
      &lt;item&gt;The fourth terminal is most likely used for NFC (which is a part of the battery). It's not going to be used.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So what I need my "fake battery" circuit to do is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provide 3.4-4.4V between (+) and (-) battery terminals on the smartphone&lt;/item&gt;
      &lt;item&gt;Create ~2350ohm resistance between (T) and (-)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since I am still powering the smartphone from a 5V USB power supply, I can add a single silicon diode between the power supply's +5V and (+) battery terminal to drop the incoming voltage by ~0.7V down to ~4.3V, which is within the range that the smartphone can expect.&lt;/p&gt;
    &lt;p&gt;IMPORTANT: Voltage drop across the silicone diode depends on the current, and multimeter measurements might not be accurate. See "Diode voltage drop" at the bottom of the page.&lt;/p&gt;
    &lt;p&gt;To get ~2350ohms, I connected a 2200ohm and a 150ohm in series. A variable resistor of appropriate range might work too.&lt;/p&gt;
    &lt;p&gt;This setup works, and the smartphone powers up. However, it can draw plenty of current and needs a fairly powerful power supply. Basic phone charger &amp;lt;1A USB power supply was not enough to even finish booting, but a ~2A was enough to boot and launch octo4a. Even after booting, if the smartphone is under higher load (initializing the USB connection, using the camera), it draws more current, and will abruptly power off if the current draw exceeds what power supply can provide.&lt;/p&gt;
    &lt;p&gt;To help address these issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add a 1000uF capacitor between (+5V) and (ground) on the circuit. See "Capacitors" at the bottom of the page for warnings and ideas.&lt;/item&gt;
      &lt;item&gt;Turn on the Battery Saver in Android and set it up to automatically turn on if estimated battery level is below 75% (just in case). This seems to lower the processing power enough to avoid shutdowns.&lt;/item&gt;
      &lt;item&gt;In OctoPrint settings, either disable the camera entirely, or use it at the lower-end FPS and resolution (320x240 @5FPS). Camera has occasionaly caused octo4a to crash, so I ended up disabling it anyway.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Schematic&lt;/head&gt;
    &lt;p&gt;In ASCII:&lt;/p&gt;
    &lt;quote&gt;(+5V) | | silicon diode +---------------|&amp;gt;|------(+) | | +---[2350ohm]---(T) | 1000uF | +---||---+---------------(-) | (ground)&lt;/quote&gt;
    &lt;p&gt;As an image:&lt;/p&gt;
    &lt;head rend="h2"&gt;Result&lt;/head&gt;
    &lt;p&gt;I cut and sanded a piece of flooring to fit into the battery compartment. Then, built and soldered the circuit on a through-hole prototype board and mounted it on top of that with double-sided tape. The 4 pins on the top are bent inward, fit into small grooves in the base and make good contact with the springy terminals of the battery compartment. (I could have just soldered the wires directly to the terminals, but wanted to make this device removable just in case I would ever need the battery again). Power comes through the JST connector at the bottom. (I salvaged a long USB cable with a broken micro-USB plug, so I decided to use JST connectors instead of USB for power).&lt;/p&gt;
    &lt;p&gt;3D printer (old RepRapPro) in its enclosure (storage box) and its brain (Samsung Galaxy S5) in a 3D-printed holder. Super modern and high-tech, I know.&lt;/p&gt;
    &lt;p&gt;This setup has been working fairly well for me for a few weeks now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Additional info&lt;/head&gt;
    &lt;head rend="h3"&gt;Diode voltage drop&lt;/head&gt;
    &lt;p&gt;Voltage drop across the diode depends on the current. If you connect the diode, but do not connect the circuit to anything, and try to measure the "final" voltage (after the diode) with a multimeter, you will get somewhere around ~4.8V, because the current running through the multimeter is miniscule. You might think you need 4 diodes to bring the voltage down sufficiently, but you do not, one is enough.&lt;/p&gt;
    &lt;p&gt;Diagrams:&lt;/p&gt;
    &lt;p&gt;Circuit without load, measured with a multimeter:&lt;/p&gt;
    &lt;quote&gt;(+5V) | diode +---|&amp;gt;|---+ &amp;lt;--+ | [multimeter: 4.8V] | +---------+ &amp;lt;--+ | (ground)&lt;/quote&gt;
    &lt;p&gt;Circuit with load (connected to the smartphone):&lt;/p&gt;
    &lt;quote&gt;(+5V) | diode +---|&amp;gt;|---+ &amp;lt;-------------+ | | [smartphone] [multimeter: 4.2V] | | +---------+ &amp;lt;-------------+ | (ground)&lt;/quote&gt;
    &lt;head rend="h3"&gt;Capacitors&lt;/head&gt;
    &lt;p&gt;It might be possible to use a lower-current power supply by replacing the 1000uF capacitor with several "supercapacitors" (5F and above). The only issue I see is that the initial power-up will stress the power supply since empty capacitors are essentially a short circuit.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;Adafruit - about Li-ion and LiPoly batteries&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46241767</guid><pubDate>Fri, 12 Dec 2025 07:36:17 +0000</pubDate></item><item><title>Guarding My Git Forge Against AI Scrapers</title><link>https://vulpinecitrus.info/blog/guarding-git-forge-ai-scrapers/</link><description>&lt;doc fingerprint="e674b8d1d8e1a093"&gt;
  &lt;main&gt;
    &lt;p&gt;In August 2024, one of my roommates and partners messaged the apartment group chat, saying she noticed the internet was slow again at our place, and my forgejo was unable to render any page in under 15 seconds.&lt;/p&gt;
    &lt;p&gt;i investigated, thinking it would be a trivial little problem to solve. Soon enough, however, i would uncover hundreds of thousands of queries a day from thousands of individual IPs, fetching seemingly-random pages in my forge every single day, all the time.&lt;/p&gt;
    &lt;p&gt;This post summarizes the practical issues that arose as a result of the onslaught of scrapers eager to download millions of commits off of my forge, and the measures i put in place to limit the damage.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Why the forge?&lt;/head&gt;
    &lt;p&gt;In the year 2025, on the web, everything is worth being scraped. Everything that came out of the mind of a human is susceptible to be snatched under the vastest labor theft scheme in the history of mankind. This very article, the second it gets published in any indexable page, will be added to countless datasets meant to train foundational large-language models. My words, your words, have contributed infinitesimal shifts of neural-network weights underpinning the largest, most grotesque accumulation of wealth seen over the lifetime of my parents, grandparents, and their grandparents themselves.&lt;/p&gt;
    &lt;p&gt;Oh, and forges have a lot of commits. See, if you have a public repository that is publicly exposed, every file in every folder for every commit will be connected. Add other options, such as a &lt;code&gt;git blame&lt;/code&gt; on a file, and multiply it by the
number of files and commits. Add the raw download link, also multiplied by the
number of commits.&lt;/p&gt;
    &lt;p&gt;Say, hypothetically, you have a linux repository available, and only with all the commits in the &lt;code&gt;master&lt;/code&gt; branch up to the &lt;code&gt;v6.17&lt;/code&gt; tag from 2025-09-18.
That's 1,383,738 commits in the range &lt;code&gt;1da177e4c3f4..e5f0a698b34e&lt;/code&gt;. How many
files is that? Well:&lt;/p&gt;
    &lt;code&gt;count=0;
while read -r rev; do
    point=$(git ls-tree -tr $rev | wc -l);
    count=$(( $count + $point ));
    printf "[%s] %s: %d (tot: %d)\n" $(git log -1 --pretty=tformat:%cs $rev) $rev $point $count;
done &amp;lt; &amp;lt;(git rev-list "1da177e4c3f4..e5f0a698b34e");
printf "Total: $count\n";
&lt;/code&gt;
    &lt;p&gt;i ran this on the 100 commits before &lt;code&gt;v6.17&lt;/code&gt;. If you have &lt;code&gt;git ls-tree -tr $rev&lt;/code&gt;, you get both files and directories counted. If you replace it with &lt;code&gt;git ls-tree -r $rev&lt;/code&gt; only shows files. i got 72024729 files, and 76798658 files and
directories. Running on the whole history of Linux's &lt;code&gt;master&lt;/code&gt; branch yields
78,483,866,182 files, and 83,627,462,277 files and directories.&lt;/p&gt;
    &lt;p&gt;Now, for a ballpark estimate of the number of pages that can be scraped if you have a copy of Linux, apply the formula:&lt;/p&gt;
    &lt;code&gt;(Ncommits * Nfiles) * 2 + (Ncommits * Nfilesandfolders) * 2 + Ncommits * 3
&lt;/code&gt;
    &lt;p&gt;That is, applied to my hypothetical Linux repository:&lt;/p&gt;
    &lt;code&gt;78483866182 * 2 + 83627462277 * 2 + 1383738 * 3 = 324,226,808,132 pages
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;*3&lt;/code&gt; accounts for the fact that every file of every commit can be scraped
raw, and &lt;code&gt;git-blame&lt;/code&gt;'d. The second part of the
formula considers every single file or folder page. The third part accounts for
the fact that every file of every commit can be diffed with its version of
every commit (in theory). The final component considers every commit summary
page.&lt;/p&gt;
    &lt;p&gt;That gives, for me, 324 billion 226 million 808 thousand and 132 pages that can be scraped. From a single repository. Assume that every scraper agent that enters one of these repositories will also take note of every other link on the page, and report it so that other agents can scrapes them. These scrapers effectively act like early 2000s web spiders that crawled the internet to index it, except they do not care about &lt;code&gt;robots.txt&lt;/code&gt;, and they will absolutely keep
scraping new links again and again with no strategy to minimize the cost on
you, as a host.&lt;/p&gt;
    &lt;head rend="h1"&gt;# The Cost of Scraping&lt;/head&gt;
    &lt;p&gt;As i am writing the original draft of this section, the longer-term measures i put in place have been removed, so i could gather up-to-date numbers to display how bad the situation is.&lt;/p&gt;
    &lt;p&gt;i pay for the electricity that powers my git Forge. Okay, actually, one of my roommate does, but we put it on the calc sheet where we keep track of who pays what (when we remember).&lt;/p&gt;
    &lt;p&gt;At the time i began fighting scrapers, my git forge ran from an old desktop computer plugged in my living room. Now, it is in our home's rackable server in a virtual machine. i never got to measure differences in power consumption when we got scraped or not scraped on the desktop machine, but i did on the rackable server. If memory serves me right, stopping the wave of scrapers reduced the power draw of the server from ~170W to ~150W.&lt;/p&gt;
    &lt;p&gt;Right now, with all the hard drives in that server spinning, and every protection off, we are drawing 200W from the power grid on that server. Constantly. By the end of this experiment, me and my roommates will have computed that the difference in power usage caused by scraping costs us ~60 euros a year.&lt;/p&gt;
    &lt;p&gt;Another tied cost is that the VM that runs the forge is figuratively suffocating from the amount of queries. Not all queries are born equal as well: requests to see the &lt;code&gt;blame&lt;/code&gt; of a file or a &lt;code&gt;diff&lt;/code&gt; between commits incurs a worse cost than
just rendering the front page of a repository. The last huge wave of scraping
left my VM at 99+% usage of 4 CPU cores and 2.5GiB of RAM,
whereas the usual levels i observe are closer to 4% usage of CPUs, and an oscillation
between 1.5GiB and 2GiB of RAM.&lt;/p&gt;
    &lt;p&gt;As i'm writing this, the VM running forgejo eats 100% of 8 CPU cores.&lt;/p&gt;
    &lt;p&gt;Additionally, the networking cost is palpable. Various monitoring tools let me see the real-time traffic statistics in our apartment. Before i put the first measures in place to thwart scraping, we could visibly see the traffic coming out of the desktop computer running my forge and out to the internet. My roommates' complaints that it slowed down the whole internet here were in fact founded: when we had multiple people watching live streams or doing pretty big downloads, they were throttled by the traffic out of the forge.1&lt;/p&gt;
    &lt;p&gt;The egress data rate of my forge's VM is at least 4MBps of data (32Mbps). Constantly.&lt;/p&gt;
    &lt;p&gt;Finally, the human cost: i have spent entire days behind my terminals trying to figure out 1) what the fuck was going on and 2) what the fuck to do about it. i have had conversations with other people who self-host their infrastructure, desperately trying to figure out workable solutions that would not needlessly impact our users. And the funniest detail is: that rackable server is in the living room, directly in front of my bedroom door. It usually purrs like an adorable cat, but, lately, it's been whirring louder and louder. i can hear it. when i'm trying to sleep.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Let's do some statistics.&lt;/head&gt;
    &lt;p&gt;i was curious to analyze the nginx logs to understand where the traffic came from and what shape it took.&lt;/p&gt;
    &lt;p&gt;As a study case, we can work on &lt;code&gt;/var/log/nginx/git.vulpinecitrus.info/&lt;/code&gt; from
&lt;code&gt;2025-11-14&lt;/code&gt; to &lt;code&gt;2025-11-19&lt;/code&gt;. Note that on &lt;code&gt;2025-11-15&lt;/code&gt; at &lt;code&gt;18:27 UTC&lt;/code&gt;, i
stopped the redirection of new agents into the Iocaine crawler maze (see
below). At &lt;code&gt;19:15 UTC&lt;/code&gt;, i removed the nginx request limit zone from the
&lt;code&gt;/Lymkwi/linux/&lt;/code&gt; path. At &lt;code&gt;19:16 UTC&lt;/code&gt; i removed the separation of log files
between IPs flagged as bots, and IPs not flagged as bots.&lt;/p&gt;
    &lt;p&gt;The three measures i progressively put in place later were: web caching (2025-11-17), manually sending IPs to a garbage generator with a rate-limit (Iocaine 2) (2025-11-14, 15 and 18), and then Iocaine 3 (2025-11-19).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Common Logs&lt;/cell&gt;
        &lt;cell role="head"&gt;Successful&lt;/cell&gt;
        &lt;cell role="head"&gt;Delayed (429)&lt;/cell&gt;
        &lt;cell role="head"&gt;Error (5XX)&lt;/cell&gt;
        &lt;cell role="head"&gt;Measures in place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;275323&lt;/cell&gt;
        &lt;cell&gt;66517&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;71712&lt;/cell&gt;
        &lt;cell&gt;54259&lt;/cell&gt;
        &lt;cell&gt;9802&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;140713&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;65763&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;514309&lt;/cell&gt;
        &lt;cell&gt;25986&lt;/cell&gt;
        &lt;cell&gt;3012&lt;/cell&gt;
        &lt;cell&gt;Caching, eventually rate-limiting2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;335266&lt;/cell&gt;
        &lt;cell&gt;20280&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;3183&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Bot Logs&lt;/cell&gt;
        &lt;cell&gt;Successful&lt;/cell&gt;
        &lt;cell&gt;Delayed (429)&lt;/cell&gt;
        &lt;cell&gt;Error (5XX)&lt;/cell&gt;
        &lt;cell&gt;Measures in place&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-14 (bots)&lt;/cell&gt;
        &lt;cell&gt;41388&lt;/cell&gt;
        &lt;cell&gt;65517&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-15 (bots)&lt;/cell&gt;
        &lt;cell&gt;34190&lt;/cell&gt;
        &lt;cell&gt;53403&lt;/cell&gt;
        &lt;cell&gt;63&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-16 (bots)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(no bot-specific logs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-17 (bots)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;(no bot-specific logs)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;2025-11-18 (bots)&lt;/cell&gt;
        &lt;cell&gt;390013&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;13&lt;/cell&gt;
        &lt;cell&gt;Iocaine 2.1 + Rate-limiting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19 (bots)&lt;/cell&gt;
        &lt;cell&gt;731593&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;Iocaine 3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 1)&lt;/head&gt;
    &lt;p&gt;Assuming your log file is &lt;code&gt;git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;zcat git-access-2025-11-14.log.gz | grep '" 200 ' | wc -l
zcat git-access-2025-11-14.log.gz | grep '" 429 ' | wc -l
&lt;/code&gt;
    &lt;p&gt;Without spoiling too much, caching was an utter failure, and the improvement i measurement by manually rate-limiting a set of IPs (from Huawei Cloud and Alibaba) on the Linux repository only helped so much. When all protections dropped, my server became so unresponsive that backend errors (usually timeouts) spiked. Error also happened with caching, when nginx encountered an issue when buffering a reply. Overall, caching encouraged more queries overall.&lt;/p&gt;
    &lt;p&gt;Once Iocaine was deployed, the vast majority of queries were routed away from the backend, with no errors reported, and no delaying because all of the IPs i manually rate-limited were caught by Iocaine instead.&lt;/p&gt;
    &lt;p&gt;Out of all these queries, &lt;code&gt;117.64.70.34&lt;/code&gt; is the most common source of requests,
with 226023 total queries originating from the ChinaNet-Backbone ASN (AS4134).
It is followed by &lt;code&gt;136.243.228.193&lt;/code&gt; (13849 queries), an IP from Hetzner whose
hostname ironically resolves to
&lt;code&gt;crawling-gateway-136-243-228-193.dataforseo.com&lt;/code&gt;. Then, &lt;code&gt;172.17.0.3&lt;/code&gt; the
uptime prober of VC Status with 6908
queries, and &lt;code&gt;74.7.227.127&lt;/code&gt;, an IP from Microsoft's AS 8075 (6117 queries).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Unique IP Count&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;16461&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;18639&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;41712&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;47252&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;22480&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;14230&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 2)&lt;/head&gt;
    &lt;p&gt;Assuming your log files are called &lt;code&gt;*git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;zcat \*git-access-2025-11-14.log.gz | awk '{ print $1 }' | sort | uniq -c | wc -l
&lt;/code&gt;
    &lt;p&gt;On the two days where restrictions were lifted or there was only caching, the amount of unique IPs querying the forge doubled. The more you facilitate the work of these crawlers, the more they are going to pound you. They will always try and get more out of your server than you are capable of providing.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Day&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 1&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 2&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 3&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 4&lt;/cell&gt;
        &lt;cell role="head"&gt;Top 5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-14&lt;/cell&gt;
        &lt;cell&gt;(226089) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(40189) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1454) - &lt;code&gt;/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1405) - &lt;code&gt;/rail&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1174) - &lt;code&gt;/Soblow/indi-hugo&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-15&lt;/cell&gt;
        &lt;cell&gt;(35163) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18952) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(4197) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1655) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1635) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-14 (bots)&lt;/cell&gt;
        &lt;cell&gt;(40189) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(270) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(79) - &lt;code&gt;/Lymkwi/[REDACTED]&lt;/code&gt;3&lt;/cell&gt;
        &lt;cell&gt;(55) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(52) - &lt;code&gt;/oror/asm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-15 (bots)&lt;/cell&gt;
        &lt;cell&gt;(32895) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(260) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(193) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(95) - &lt;code&gt;/Lymkwi/[REDACTED]&lt;/code&gt;3&lt;/cell&gt;
        &lt;cell&gt;(48) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-16&lt;/cell&gt;
        &lt;cell&gt;(72687) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(23028) - &lt;code&gt;/Lymkwi/linux&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(16779) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(5390) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(3585) - &lt;code&gt;/Lymkwi/gr-gsm&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-17&lt;/cell&gt;
        &lt;cell&gt;(361632) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(74048) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18136) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(13147) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12921) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-18&lt;/cell&gt;
        &lt;cell&gt;(227019) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(46004) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12644) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(12624) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(7712) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-18 (bots)&lt;/cell&gt;
        &lt;cell&gt;(261346) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(43923) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(20195) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(18808) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(10134) - &lt;code&gt;/oror/necro&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2025-11-19&lt;/cell&gt;
        &lt;cell&gt;(1418) - &lt;code&gt;/&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(1248) - &lt;code&gt;/rail&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(356) - &lt;code&gt;/Soblow&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(31) - &lt;code&gt;/assets/img&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(25) - &lt;code&gt;/Soblow/IndigoDen&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-11-19 (bots)&lt;/cell&gt;
        &lt;cell&gt;(448626) - &lt;code&gt;/vc-archival/youtube-dl&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(73164) - &lt;code&gt;/vc-archival/youtube-dl-original&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(39107) - &lt;code&gt;/reibooru/reibooru&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(37107) - &lt;code&gt;/alopexlemoni/GenderDysphoria.fyi&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;(25921) - &lt;code&gt;/vc-archival/YSLua&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 3)&lt;/head&gt;
    &lt;p&gt;Assuming you want data for the log file called &lt;code&gt;git-access-2025-11-14.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt; zcat git-access-2025-11-14.log.gz | grep '" 200 ' | awk '{ print $7 }' \
    | cut -d/ -f -3 | sort | uniq -c | sort -n \
    | tail -n 5 | tac
&lt;/code&gt;
    &lt;p&gt;Big repositories with a lot of commits and a lot of files are a bountiful resource for the crawlers. Once they enter those, they will take ages to leave, at least because of the sheer amount of pages that can be generated by following the links of a repository.&lt;/p&gt;
    &lt;p&gt;Most legitimate traffic seems to be either fetching profiles (a couple of my users have their profiles listed in their fediverse bios) or the root page of my forge.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;2025-11-14 (all)&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-11-15 (all)&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-11-16 (all)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 1&lt;/cell&gt;
        &lt;cell&gt;(8532) - AS136907 (Huawei Clouds)&lt;/cell&gt;
        &lt;cell&gt;(8537) - AS136907 (Huawei Clouds)&lt;/cell&gt;
        &lt;cell&gt;(8535) - AS136907 (Huawei Clouds)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 2&lt;/cell&gt;
        &lt;cell&gt;(2142) - AS45899 (VNPT Corp)&lt;/cell&gt;
        &lt;cell&gt;(2107) - AS45899 (VNPT Corp)&lt;/cell&gt;
        &lt;cell&gt;(4002) - AS212238 (Datacamp Limited)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 3&lt;/cell&gt;
        &lt;cell&gt;(803) - AS153671 (Liasail Global Hongkong Limited)&lt;/cell&gt;
        &lt;cell&gt;(895) - AS153671 (Liasail Global Hongkong Limited)&lt;/cell&gt;
        &lt;cell&gt;(3504) - AS9009 (M247 Europe SRL)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Top 4&lt;/cell&gt;
        &lt;cell&gt;(555) - AS5065 (Bunny Communications)&lt;/cell&gt;
        &lt;cell&gt;(765) - AS45102 (Alibaba US Technology Co., Ltd.)&lt;/cell&gt;
        &lt;cell&gt;(3206) - AS3257 (GTT Communications)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Top 5&lt;/cell&gt;
        &lt;cell&gt;(390) - AS21859 (Zenlayer Inc)&lt;/cell&gt;
        &lt;cell&gt;(629) - AS5065 (Bunny Communications)&lt;/cell&gt;
        &lt;cell&gt;(2874) - AS45899 (VNPT Corp)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;(Commands used to generate Table 4)&lt;/head&gt;
    &lt;p&gt;For this, i needed a database of IP-to-ASN data. i got one from IPInfo by registering for a free account and using their web API. i first scripted a mapping of unique IP addresses to AS number. For example, for the log file &lt;code&gt;bot-git-access-2025-11-18.log.gz&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;while read ip; do
    ASN=$(curl -qfL api.ipinfo.io/lite/$ip?token=&amp;lt;my token&amp;gt; | jq -r .asn);
    printf "$ip $ASN\n" | tee -a 2025-11-18-bot.ips.txt;
done &amp;lt; &amp;lt;(zcat bot-git-access-2025-11-18.log.gz | awk '{ print $1 }' | sort | uniq)
&lt;/code&gt;
    &lt;p&gt;Then, with this map, i run:&lt;/p&gt;
    &lt;code&gt;cat 2025-11-18-bot.ips.txt | cut -d' ' -f 2 | sort | uniq -c | sort -n | tail -n 5
&lt;/code&gt;
    &lt;p&gt;So my largest hits are from Huawei Clouds (VPS provider), VPNT (a Vietnamese mobile and home ISP), Liasail Global HK Limited (a VPS/"AI-powering service" provider), Bunny Communications LLC (a broadband ISP for residential users), and Zenlayer (CDN/Cloud infrastructure provider). When i lifted all protections, Datacamp Limited (a VPS provider), GTT Communications (some sort of bullshit-looking ISP4 who, i have been informed, is in fact a backbone operator), and M247 Europe SRL (a hosting provider) suddenly appeared. If memory serves me right, Datacamp, GTT and M247 were also companies i had flagged during my initial investigation in summer 2024, and added to the manually blocked/limited IPs alongside all of Huawei Cloud and Alibaba.&lt;/p&gt;
    &lt;p&gt;Interestingly, both Liasail and Zenlayer mention that they "Power AI" on their front page. They sure do. Worryingly, VNPT and Bunny Communications are home/mobile ISPs. i cannot ascertain for sure that their IPs are from domestic users, but it seems worrisome that these are among the top scraping sources once you remove the most obviously malicious actors.&lt;/p&gt;
    &lt;head rend="h1"&gt;# The Protection Measures&lt;/head&gt;
    &lt;p&gt;i have one goal, and one constraint. My goal is that i need to protect the forge as much as possible, by means of either blocking bots or offloading the cost to my VPS provider (whose electricity i do not pay for). My only constraint: i was not going to deploy a proof-of-work-based captcha system such as Anubis. There are two reasons for these constraints:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;i personally find that forcing your visitors to have to expand more computational power to prove they're not a scraper is bad praxis. There are devices out there that legitimately want that access, but have limited computational power or features. And, yeah, there are multiple types of challenges, some of which take low-power devices into account or even those that cannot run JavaScript, but,&lt;/item&gt;
      &lt;item&gt;Scrapers can easily bypass Anubis. It's not a design flaw. Anubis is harm reduction, not pixie dust.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;i tried layers of solutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;caching on the reverse proxy&lt;/item&gt;
      &lt;item&gt;Iocaine 2 with no classifiers, which generates garbage in reply to any query you send it&lt;/item&gt;
      &lt;item&gt;Manually redirecting IPs and rate-limiting them&lt;/item&gt;
      &lt;item&gt;Deploying Iocaine 3, with its classifiers (Nam-Shub-of-Enki)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;## Reverse-Proxy Caching&lt;/head&gt;
    &lt;p&gt;i have a confession to make: i never realized that nginx did not cache anything by default. That realization promptly came with the other realization that caching things correctly is hard. i may, some day, write about my experience of protecting a service that posted links to itself on the fediverse, so that it wouldn't slow to a crawl for ten minutes after every post.&lt;/p&gt;
    &lt;p&gt;As for the rest of these, i will be showing my solution in &lt;code&gt;nginx&lt;/code&gt;. You can,
almost certainly, figure out a way of doing exactly the same thing with any other
decent reverse proxy software.&lt;/p&gt;
    &lt;p&gt;To create a cache for my forge, i add the following line to &lt;code&gt;/etc/nginx.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;proxy_cache_path /var/cache/nginx/forge/ levels=1:2 keys_zone=forgecache:100m;
&lt;/code&gt;
    &lt;p&gt;That will create a 2-level cache called &lt;code&gt;forgecache&lt;/code&gt; that will hold 100MB of data
located at &lt;code&gt;/var/cache/nginx/forge&lt;/code&gt;. i create the directory and make &lt;code&gt;www-data&lt;/code&gt;
its owner and group.&lt;/p&gt;
    &lt;p&gt;In &lt;code&gt;/etc/nginx/sites-enabled/vcinfo-git.conf&lt;/code&gt;, where my git forge's site
configuration sits, i have a &lt;code&gt;location&lt;/code&gt; block that serves the whole root of the
service, which i modify thusly:&lt;/p&gt;
    &lt;code&gt;location / {
    proxy_cache forgecache;
    proxy_buffering on;
    proxy_cache_valid any 1h;
    add_header X-Cached $upstream_cache_status;
    expires 1h;
    proxy_ignore_headers "Set-Cookie";
    proxy_hide_header "Set-Cookie";

    # more stuff...
}
&lt;/code&gt;
    &lt;p&gt;That configuration does several things: it turns on caching and buffering at the proxy (&lt;code&gt;proxy_buffering&lt;/code&gt;),
telling it to use &lt;code&gt;forgecache&lt;/code&gt;
(&lt;code&gt;proxy_cache&lt;/code&gt;)
and keep any page valid for an hour
(&lt;code&gt;proxy_cache_valid&lt;/code&gt;).
It also adds a cookie that will let you debug whether or not a query hit or
missed the cache (&lt;code&gt;add_header&lt;/code&gt;). The &lt;code&gt;expires&lt;/code&gt; directive adds headers telling
your visitor's browser that the content they cache will also expire in an hour
(&lt;code&gt;expires&lt;/code&gt;).
Finally, the cache ignores any response header that sets a cookie
(&lt;code&gt;proxy_ignore_headers&lt;/code&gt;,
&lt;code&gt;proxy_hide_header&lt;/code&gt;),
to attempt to remove any page that could be customized for a user once they log
in.&lt;/p&gt;
    &lt;p&gt;The result? Caching was a disaster, predictably so. Caching works when the same resource is repeatedly queried, like with page assets, JavaScript, style sheets, etc. In this case, the thousands of actors querying my forge are coordinated, somehow, never (or rarely) query the same resource twice, and only download the raw HTML of the web pages.&lt;/p&gt;
    &lt;p&gt;Worse, caching messed up the display of authenticated pages. The snippets above are not enough to delineate between an authenticated session and an unauthenticated one, and it broke my forge so badly that i had to disable caching and enable the next layer early on &lt;code&gt;2025-11-17&lt;/code&gt;, or i just could not
use my forge.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Rate-Limiting on the Proxy&lt;/head&gt;
    &lt;p&gt;The next layer of protection simply consisted in enabling a global rate-limit on the most-hit repositories:&lt;/p&gt;
    &lt;code&gt;limit_req_zone wholeforge zone=wholeforge:10m rate=3r/s;

server {
    // ...
	location ^~ (/alopexlemoni/GenderDysphoria.fyi|/oror/necro|/Lymkwi/linux|/vc-archival/youtube-dl-original|/reibooru/reibooru) {
		proxy_set_header Host $host;
		proxy_set_header X-Real-IP $remote_addr;
		proxy_max_temp_file_size 2048m;

		limit_req zone=wholeforge nodelay;

		proxy_pass http://&amp;lt;my actual upstream&amp;gt;/;
	}
}
&lt;/code&gt;
    &lt;p&gt;This was achieved in two directives. The first one, &lt;code&gt;limit_req_zone&lt;/code&gt;, sits outside
the &lt;code&gt;server {}&lt;/code&gt; block and defines a zone called &lt;code&gt;wholeforge&lt;/code&gt; that stores 10MB of
state data and limits to 3 requests per second.&lt;/p&gt;
    &lt;p&gt;When this was in place, however, actually accessing the Linux repository as a normal user (or any of the often-hit repositories) became a nightmare of waiting and request timeouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Manually Redirecting to a Garbage Generator&lt;/head&gt;
    &lt;p&gt;Because caching was (predictably) useless, and rate-limiting was hindering me as well, i re-enabled the initial setup that was in place before my experiments: manually redirecting queries to a garbage generator (in this case, an old version of Iocaine). It's largely based on my initial setup following this tutorial in french.&lt;/p&gt;
    &lt;p&gt;For the purpose of this part, you do not have to know what Iocaine does precisely. In the next section, i will present my current and final setup, with an updated Iocaine that also includes a classifier to decide which queries are bots and which are regular users. For now, i will present the version where i manually chose who to return garbage to based on IP addresses.&lt;/p&gt;
    &lt;p&gt;As a little bonus, it will also include rate-limiting of those garbage-hungry bots.&lt;/p&gt;
    &lt;p&gt;i add a file called &lt;code&gt;/etc/nginx/snippets/block_bots.conf&lt;/code&gt; which contains:&lt;/p&gt;
    &lt;code&gt;if ($bot_user_agent) {
    rewrite ^ /deflagration$request_uri;
}
if ($bot_ip) {
    rewrite ^ /deflagration$request_uri;
}
location /deflagration {
    limit_req zone=bots nodelay;
    proxy_set_header Host $host;
    proxy_pass &amp;lt;garbage upstream&amp;gt;;
}
&lt;/code&gt;
    &lt;p&gt;This will force any query categorized as &lt;code&gt;bot_user_agent&lt;/code&gt; or &lt;code&gt;bot_ip&lt;/code&gt; to be
routed through to a different upstrea which serves garbage. That upstream is
also protected by rate-limiting on a zone called &lt;code&gt;bots&lt;/code&gt; which is defined in the
next bit of code. This snippet is actually meant to be included in your &lt;code&gt;server {}&lt;/code&gt;
block using the &lt;code&gt;include&lt;/code&gt; directive.&lt;/p&gt;
    &lt;p&gt;i then add the following in &lt;code&gt;/etc/nginx/conf.d/bots.conf&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;map $http_user_agent $bot_user_agent {
    default 0;

    # from https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt
    ~*amazonbot 1;
    ~*anthropic-ai  1;
    ~*applebot  1;
    ~*applebot-extended 1;
    ~*brightbot 1;
    ~*bytespider  1;
    ~*ccbot 1;
    ~*chatgpt-user  1;
    ~*claude-web  1;
    ~*claudebot 1;
    ~*cohere-ai 1;
    ~*cohere-training-data-crawler  1;
    ~*crawlspace  1;
    ~*diffbot 1;
    ~*duckassistbot 1;
    ~*facebookbot 1;
    ~*friendlycrawler 1;
    ~*google-extended 1;
    ~*googleother 1;
    ~*googleother-image 1;
    ~*googleother-video 1;
    ~*gptbot  1;
    ~*iaskspider  1;
    ~*icc-crawler 1;
    ~*imagesiftbot  1;
    ~*img2dataset 1;
    ~*isscyberriskcrawler 1;
    ~*kangaroo  1;
    ~*meta-externalagent  1;
    ~*meta-externalfetcher  1;
    ~*oai-searchbot 1;
    ~*omgili  1;
    ~*omgilibot 1;
    ~*pangubot  1;
    ~*perplexitybot 1;
    ~*petalbot  1;
    ~*scrapy  1;
    ~*semrushbot-ocob 1;
    ~*semrushbot-swa  1;
    ~*sidetrade 1;
    ~*timpibot  1;
    ~*velenpublicwebcrawler 1;
    ~*webzio-extended 1;
    ~*youbot  1;

    # Add whatever other pattern you want down here
}

geo $bot_ip {
    default 0;

    # Add your IP ranges here
}

# Rate-limiting setup for bots
limit_req_zone bots zone=bots:30m rate=1r/s;

# Return 429 (Too Many Requests) to slow them down
limit_req_status 429;
&lt;/code&gt;
    &lt;p&gt;That bit of configuration does a mapping between the client IP and a variable called &lt;code&gt;bot_ip&lt;/code&gt;, and the client's user agent and a variable called
&lt;code&gt;bot_user_agent&lt;/code&gt;. When a known pattern listed in those blocks is found, the
corresponding variable is flipped to the provided value (here, &lt;code&gt;1&lt;/code&gt;). Otherwise,
it stays &lt;code&gt;0&lt;/code&gt;. Then, we define the rate-limiting zone that is used to slow down
the bots so they don't feed on slop too fast. You will then need to install the
&lt;code&gt;http-geoip2&lt;/code&gt; nginx module (on Debian-based distributions, something like &lt;code&gt;apt install libnginx-mod-http-geoip2&lt;/code&gt; will do).&lt;/p&gt;
    &lt;p&gt;Once that is done, add the following line to the &lt;code&gt;server&lt;/code&gt; block of every site
you want to protect:&lt;/p&gt;
    &lt;code&gt;include /etc/nginx/snippets/block_bots.conf;
&lt;/code&gt;
    &lt;p&gt;And when you feel confident enough, roll a &lt;code&gt;nginx -t&lt;/code&gt; and reload the unit for
&lt;code&gt;nginx&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Now, if you're using &lt;code&gt;caddy&lt;/code&gt; or any other reverse proxy, there are probably
similar mechanisms available. You can go and peruse the documentation of Iocaine,
or look online for specific tutorials that, i am sure, other people have made
better than i would.&lt;/p&gt;
    &lt;p&gt;Immediately after enabling it, and shoving all the IPs from Alibaba Cloud and Huawei Cloud in the bot config file, the activity slowed down on my server. Power usage went down to ~180W, CPU usage to rougly 60%, and it stopped making a hellish noise.&lt;/p&gt;
    &lt;p&gt;As the stats showed earlier, however, a lot of traffic was still hitting the server itself. Even weirder, there were still occasional spikes, every 3 hour, that lasted about one and a half hour, where the server would whirr and forgejo suffocate again.&lt;/p&gt;
    &lt;p&gt;Bots were still hitting my server, and there was no clear source for it.&lt;/p&gt;
    &lt;head rend="h2"&gt;## Automatically Classifying Bots and Poisoning Them: Iocaine and Nam-Shub-of-Enki&lt;/head&gt;
    &lt;p&gt;So far, the steps i showed so far help when a single IP is hammering at your forge, or when someone is clearly scraping you from an Autonomous System that you do not mind blocking. Sadly, as i've showed above in Table 4, a surprising amount of scraping comes from broadband addresses. i can assemble lists of IPs as big as i want, or block entire ASNs, but i would love to have a per-query way of determining if a query looks legitimate.&lt;/p&gt;
    &lt;p&gt;The next steps of protection will rely on categorizing a source IP based on its the credibility of its user agent. This mechanism is largely based on the documentation for Iocaine 3.x. We finally get to talk about Iocaine!&lt;/p&gt;
    &lt;p&gt;Iocaine is a tool that traps scrapers in a maze of meaningless pages that endlessly lead to more meaningless pages. The content of these pages is generated using a Markov chain, based on a corpus of texts given to the software. Iocaine (specifically all versions after 3 at least5) is a middleware, in the sense that it works by being placed on the line between your reverse proxy and the service. Your reverse proxy will first begin by redirecting traffic to Iocaine, and, if Iocaine deems a query legitimate, it will return a &lt;code&gt;421 Misdirected Request&lt;/code&gt; back at your reverse-proxy. The
latter must then catch it, and use the real upstream as a fallback. If
Iocaine's Nam-Shub-of-Enki6 decides query came from a bogus or otherwise undesirable source, it
will happily reply &lt;code&gt;200 OK&lt;/code&gt; and send generated garbage.&lt;/p&gt;
    &lt;p&gt;My setup lodges Iocaine 3 between nginx and my forge, following the Iocaine documentation to use the container version. i recommend you follow it, and then add the next little things to enable categorization statistics, and prevent the logging they're based on from blowing up your storage:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In &lt;code&gt;etc/config.d/03-nam-shub-of-enki.kdl&lt;/code&gt;, change the logging block to:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;logging {
    enable #true
    classification {
        enable #true
    }
}
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;In &lt;code&gt;docker-compose.yaml&lt;/code&gt;, add the following bits to limit classification logging to 50MB:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;services:
  iocaine:
    # The things you already have here...
    # ...
    env:
      - RUST_LOG=iocaine=info
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
&lt;/code&gt;
    &lt;p&gt;My checks block in Nam-Shub-of-Enki is as such:&lt;/p&gt;
    &lt;code&gt;checks {
    disable cgi-bin-trap

    asn {
        database-path "/data/ipinfo_lite.mmdb"
        asns "45102" "136907"
    }
    ai-robots-txt {
        path "/data/ai.robots.txt-robots.json"
    }
    generated-urls {
        identifiers "deflagration"
    }
    big-tech {
        enable #true
    }
    commercial_scrapers {
        enable #true
    }
}
&lt;/code&gt;
    &lt;p&gt;I snatched a copy of the latest ipinfo ASN database for free and blocked AS52102 (Alibaba) and AS136907 (Huawei Clouds).&lt;/p&gt;
    &lt;p&gt;On 2025-11-18 at 00:00:29 UTC+1, i enabled Iocaine with the Nam-Shub-of-Enki classifier in front of my whole forge. Immediately, my server was no longer hammered. Power draw went down to just above 160W.&lt;/p&gt;
    &lt;p&gt;One problem i noticed however, while trying to deploy the artifact for this blog post on my forge, is that Iocaine causes issues when huge &lt;code&gt;PUT&lt;/code&gt;/&lt;code&gt;PATCH&lt;/code&gt;/&lt;code&gt;POST&lt;/code&gt;
requests with large bodies are piped through it: it will hang up before the
objects are entirely written. i am trying to figure out a way of only redirecting
&lt;code&gt;HEAD&lt;/code&gt; and &lt;code&gt;GET&lt;/code&gt; requests to Iocaine in nginx, like is done in the Caddy example
of the Iocaine documentation.&lt;/p&gt;
    &lt;p&gt;What i ended up settling on requires a bit of variable mapping. At the start of your site configuration, before the &lt;code&gt;server {}&lt;/code&gt; block:&lt;/p&gt;
    &lt;code&gt;map $request_method $upstream_location {
	GET	&amp;lt;iocaine upstream&amp;gt;;
	HEAD	&amp;lt;iocaine upstream&amp;gt;;
	default	&amp;lt;your actual upstream&amp;gt;;
}

map $request_method $upstream_log {
	GET	bot_access;
	HEAD	bot_access;
	default	access;
}
&lt;/code&gt;
    &lt;p&gt;Then, in the block that does the default location, write:&lt;/p&gt;
    &lt;code&gt;	location / {
	    proxy_cache off;
	    access_log /var/log/nginx/$upstream_log.log combined;
	    proxy_intercept_errors on;
	    error_page 421 = @fallback;
	    proxy_set_header Host $host;
	    proxy_set_header X-Real-IP $remote_addr;
	    proxy_pass http://$upstream_location;
	}
&lt;/code&gt;
    &lt;p&gt;That is, replace the upstream in &lt;code&gt;proxy_pass&lt;/code&gt; with the upstream decided by the
variable mapping, and, while we're at it, use &lt;code&gt;$upstream_log&lt;/code&gt; to know which log
will be the final one for that request. i differentiate between &lt;code&gt;bot_access.log&lt;/code&gt;
and &lt;code&gt;access.log&lt;/code&gt; to gather my statistics, so the difference matters to me. Change
the variables to suit the way you do it (or remove it, if you don't distinguish
clients in your log files).&lt;/p&gt;
    &lt;head rend="h1"&gt;# Monitoring Iocaine&lt;/head&gt;
    &lt;p&gt;Currently, on 2025-11-30 at 16:33:00 UTC+1, Iocaine has served 38.16GB of garbage. Over the past hour, 152.11MB of such data was thrown back at undesirable visitors. 3.39GB over the past day, 22.22GB over the past week. You can get the snippet that describes my Iocaine-specific Grafana views here.&lt;/p&gt;
    &lt;p&gt;The vast majority of undesirable queries come from Claude, OpenAI, and Disguised Bots. Claude and OpenAI are absolutely gluttonous, and, once they have access to a ton of pages, they will greedily flock to fetch them like pigeons being fed breadcrumbs laced with strychnine.&lt;/p&gt;
    &lt;p&gt;AI bot scrapers (&lt;code&gt;ai.robots.txt&lt;/code&gt;) maintain a constant 920~930 query per minute
(15-ish QPS) over the 6 domains i have protected with Iocaine, including the
forge.&lt;/p&gt;
    &lt;p&gt;There is also a low hum of a mix of commercial scrapers (~1 request every two second), big tech crawlers (Facebook, Google, etc, about 2QPS or 110 query/min), and, especially, fake browsers.&lt;/p&gt;
    &lt;p&gt;Classifying fake browsers is where Iocaine really shines, specifically thanks to the classifiers implemented via Nam-Shub-of-Enki. The faked bots classifier detects the likelihood that the user agent reported by the client is bullshit, generated from a list of technologies mashed together. For example, if your client reports a user agent for a set of software that never supported HTTP2, or never actually existed together, or is not even released yet, it will get flagged. Think, for example, Windows NT 4 running Chrome, pretending to be able to do TLS1.3.&lt;/p&gt;
    &lt;p&gt;The background-noise level of such queries is usually 140~160 queries per minute (or 2~3 QPS). However, notice those spikes in the graph above?&lt;/p&gt;
    &lt;head rend="h2"&gt;## The Salves of Queries&lt;/head&gt;
    &lt;p&gt;For a while during my experiments i noticed those pillars of queries. My general nginx statistics would show a sharp increase of connections, with an iniital ramp-up, and a stable-ish plateau lasting about one and a half hour, before suddenly stopping. It would then repeat again, roughly three hours later.&lt;/p&gt;
    &lt;p&gt;Between October 29th and November 19th, and on November 28th, these spikes would constantly show up. As soon as i got Iocaine statistics running, it would flag all of those queries as faked browsers.&lt;/p&gt;
    &lt;p&gt;i investigated those spikes in particular, because they baffled and scared me: the regularity with which they probed me, and the sharpness of the ramp-up and halts, made me afraid that someone, somewhere, was organizing thousands of IPs to specifically take turns at probing websites. i have not reached any solid conclusions, beyond the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The initial phase of an attack wave begins with a clear exponential ramp-up&lt;/item&gt;
      &lt;item&gt;The ramp-up stops when the server starts either throwing errors, or the response latency reaches a given threshold&lt;/item&gt;
      &lt;item&gt;Every wave of attack lasts roughly one hour and a half&lt;/item&gt;
      &lt;item&gt;An individual IP will often contribute no more than one query, but it can reach 50 to 60 queries per IP&lt;/item&gt;
      &lt;item&gt;The same 15 or so ASN keep showing up, with five regular leaders in IP count: &lt;list rend="ol"&gt;&lt;item&gt;AS212238: Datacamp Limited&lt;/item&gt;&lt;item&gt;AS3257: GTT Communications&lt;/item&gt;&lt;item&gt;AS9009: M247 Europe SRL&lt;/item&gt;&lt;item&gt;AS203020: HostRoyale Technologies Pvt Ltd&lt;/item&gt;&lt;item&gt;AS210906: UAB "Bite Lietuva" (a Lithuanian ISP)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of those as service providers. My working theory at the moment is that someone registered thousands of cheap servers in many different companies, and are selling access to them as web proxies for scraping and scanning. i will probably write something up later when i have properly investigated that specific phenomenon.&lt;/p&gt;
    &lt;head rend="h1"&gt;# Conclusion&lt;/head&gt;
    &lt;p&gt;Self-hosting anything that is deemed "content" openly on the web in 2025 is a battle of attrition between you and forces who are able to buy tens of thousands of proxies to ruin your service for data they can resell.&lt;/p&gt;
    &lt;p&gt;This is depressing. Profoundly depressing. i look at the statistics board for my reverse-proxy and i never see less than 96.7% of requests classified as bots at any given moment. The web is filled with crap, bots that pretend to be real people to flood you. All of that because i want to have my little corner of the internet where i put my silly little code for other people to see.&lt;/p&gt;
    &lt;p&gt;i have to learn to protect myself from industrial actors in order to put anything online, because anything a person makes is valuable, and that value will be sucked dry by every tech giant to be emulsified, liquified, strained, and ultimately inexorably joined in an unholy mesh of learning weights.&lt;/p&gt;
    &lt;p&gt;This experience has rather profoundly radicalized the way i think about technology. Sanitized content can be chewed on and shat out by companies from training, but their AI tools will never swear. They will never use a slur. They will never have a revolutionary thought. Despite being amalgamation of shit rolled up in the guts of the dying capitalist society, they are sanitized to hell and beyond.&lt;/p&gt;
    &lt;p&gt;The developer of Iocaine put it best when explaining why Iocaine has absolutely unhinged identifiers (such as &lt;code&gt;SexDungeon&lt;/code&gt;, &lt;code&gt;PipeBomb&lt;/code&gt;, etc) is that they will all trigger "safeguard"
mechanisms in commercial AI tools: absolutely no coding agent will accept
analyzing and explaining code where the memory allocator's free function is
called &lt;code&gt;liberate_palestine&lt;/code&gt;. i bet that if i described, in graphic details, in
the comments of this page, the different ways being a furry intersects with my
sexuality, that no commercial scraper would even dare ingest this page.&lt;/p&gt;
    &lt;p&gt;Fuck tech companies. Fuck "AI". Fuck the corporate web.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46241849</guid><pubDate>Fri, 12 Dec 2025 07:51:04 +0000</pubDate></item><item><title>After 27 years within budget Austria open 6thlongest railway tunnel in the world</title><link>https://infrastruktur.oebb.at/en/projects-for-austria/railway-lines/southern-line-vienna-villach/koralm-railway</link><description>&lt;doc fingerprint="715d5db4db36ea94"&gt;
  &lt;main&gt;
    &lt;p&gt;Crossing the Koralpe massif more quickly and with more comfort. That’s what the future of train travel from Graz to Klagenfurt looks like. With the Koralm Railway, you will arrive at your destination even quicker. The fastest connection will shrink from three hours to just 45 minutes. Western Styria and southern Carinthia can be reached even more easily – as can our neighbouring countries Hungary and Italy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Koralm Railway connects Europe&lt;/head&gt;
    &lt;p&gt;The economy is also benefiting from the construction of the new Koralm Railway. As part of the new Southern Line, it is strengthening the Baltic-Adriatic Corridor in Europe. Transporting goods in Austria by train is becoming more attractive, which in turn is allowing our operations to remain competitive internationally. And the environment to breathe: Each tonne of freight moved by rail generates around 15 times less CO2 emissions than transporting it by lorry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Shorter journey times&lt;/item&gt;
          &lt;item&gt;Better access to southern Austria&lt;/item&gt;
          &lt;item&gt;Twenty-three contemporary railway stations and stops&lt;/item&gt;
          &lt;item&gt;Economic stimuli and jobs in the region&lt;/item&gt;
          &lt;item&gt;Long-term relief for the environment&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46242871</guid><pubDate>Fri, 12 Dec 2025 10:50:22 +0000</pubDate></item><item><title>Native ads coming soon to Stack Overflow and Stack Exchange</title><link>https://meta.stackexchange.com/questions/415259/native-ads-coming-soon-to-stack-overflow-and-stack-exchange</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46242946</guid><pubDate>Fri, 12 Dec 2025 11:02:41 +0000</pubDate></item></channel></rss>