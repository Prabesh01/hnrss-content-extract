<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 07 Dec 2025 12:17:58 +0000</lastBuildDate><item><title>Recreating the lost SDK for a 42-year-old operating system: VisiCorp Visi On</title><link>https://git.sr.ht/~nkali/vision-sdk/tree/main/item/note/index.md</link><description>&lt;doc fingerprint="8f8cd8124701078c"&gt;
  &lt;main&gt;&lt;p&gt;Back in 1983, an office software giant VisiCorp released a graphical multitasking operating system for the IBM PC called VisiOn (or Visi On, or Visi-On, it was before the Internet, so anything goes). It was an "open system", so anyone could make programs for it. Well, if they owned an expensive VAX computer and were prepared to shell out $7,000 on the Software Development Kit.&lt;/p&gt;&lt;p&gt;VisiOn was released earlier than Microsoft Windows, Digital Research GEM, or Apple Macintosh. Its COMDEX demo even predates the annoucement of Apple Lisa. But being first doesn't mean getting things right, so this VisiOn of the future did not win the market. Not a single third-party program was released for the system. No one preserved the SDK for the system. The technical documentation roughly amounts to three terse magazine articles and a single Usenet post. Heck, even the copies of the operating system itself are hard to come by.&lt;/p&gt;&lt;p&gt;Despite its low popularity, VisiOn is historically important. It influenced Microsoft's decisions about Windows, and it is a lesson about failing. So, I thought it would be nice to recreate the SDK for it, Homebrew-style. How difficult could it be, right?!&lt;/p&gt;&lt;p&gt;It took me a month of working 1-2 hours a day to produce a specification that allowed Atsuko to implement a clean-room homebrew application for VisiOn that is capable of bitmap display, menus and mouse handling.&lt;/p&gt;&lt;p&gt;If you're wondering what it felt like: this project is the largest "Sudoku puzzle" I have ever tried to solve. In this note, I have tried to explain the process of solving this puzzle, as well as noteworthy things about VisiOn and its internals. But, first things first...&lt;/p&gt;&lt;p&gt;Pyramid Game is a simple patience card game that demonstrates the basics of application development for VisiOn. It comes with an installer and features loadable fonts, bitmaps, clickable areas ("buttons"), and a menu system.&lt;/p&gt;&lt;p&gt;You now can download the floppy image and the distribution files. Obviously, you will need an installed VisiOn system to run it. The rules of the game can be found on Wikipedia.&lt;/p&gt;&lt;p&gt;The source code is available in its own repo.&lt;/p&gt;&lt;p&gt;The claim of Pyramid being "the first-ever" third-party application is a bit strong. VisiOn was an "open system", and so it is theoretically possible someone bought a VisiOn ToolKit and made third-party applications for VisiOn. But even if they did, they never published or sold them. So, Pyramid is the first-ever published third-party application for VisiOn.&lt;/p&gt;&lt;p&gt;This note is aimed at technically inclined readers with software engineering and coding background who want to learn more about vintage operating systems and reverse engineering. I'll try to keep the explanations simple at the expense of obscuring some of the technical details; if you want the details, please check out the verbose notes and the test application. I hope to document the operating system at a later date.&lt;/p&gt;&lt;p&gt;This note is quite long. Feel free to scroll to a part that interests you and read from there.&lt;/p&gt;&lt;p&gt;Personally, I find this project fascinating in terms of solarpunk and permacomputing. Imagine: you find an ancient device (42 years is ancient for computers, right?!), an artefact of a previous era, without any documentation. You have all the modern knowledge, and you want to make this mysterious device do things it was not supposed to do originally. Of course, with Visi On it's not quite the same; it runs on the IBM PC, a very well-documented and researched hardware platform.&lt;/p&gt;&lt;p&gt;If you have any feedback or comments, please leave them in the Mastodon thread or in the sr.ht ToDo project. Questions are fine, too!&lt;/p&gt;&lt;p&gt;VisiOn was made before many common user interface conventions were invented. It targeted a computer with a tiny resolution of 640x200 pixels, so its authors decided not use any icons. Therefore, VisiOn looks a bit alien. At the same time, it was made by people who knew what they were doing, and it is mostly coherent in its interface decisions.&lt;/p&gt;&lt;p&gt;Here is a copy of the OS tour I gave on Mastodon. I did not insert the clips as inline GIFs because the animations cannot be paused and are very distracting.&lt;/p&gt;&lt;p&gt;One immediately obvious thing here is the "hourglass" icon. Some believe that it might have been the first OS to use the hourglass mouse icon, but no, Xerox and InterLisp had it earlier. Apple Lisa, a contemporary, also had a similar mouse cursor.&lt;/p&gt;&lt;p&gt;The main application of the Visi On Application Manager is called "Services". The biggest diference between "Services" and other applications is that its "exit" button shuts down the whole OS.&lt;/p&gt;&lt;p&gt;You can see the screen has a System Menu at the bottom. The system menu is here to manage windows: make them FULL screen, re-FRAME them, CLOSE into an on-desktop button (we'd say "minimise" today) or OPEN them back. You cannot move the windows by their title bars. The system is very happy to beep at you, like it's a vintage PC game.&lt;/p&gt;&lt;p&gt;VisiOn is a multi-tasking operating system, and it allows launching multiple instances of the same application. To differentiate between them, the user can input the window name during the application startup.&lt;/p&gt;&lt;p&gt;Clip: multiple windows of the same program&lt;/p&gt;&lt;p&gt;In VisiOn, the Tutorial and Help apps implement a simple hyper-text system based on the "button" primitive. The "button" is simply a clickable area on the screen. It highlights by reversing the background and foreground colour when the mouse hovers over the button.&lt;/p&gt;&lt;p&gt;The system uses left-click for most operations. The right click is needed for the "scroll" operation. The user can scroll the documents (if there's something that can be scrolled) and the menu. You can see that the application menu isn't always fully visible, right?&lt;/p&gt;&lt;p&gt;The application menu system in VisiOn is hierarchical. Some operations make the menu behave like a modal window would in Windows or Mac. It is common not to add a "cancel" button in the menu. Instead, the system button STOP is used to cancel the operation.&lt;/p&gt;&lt;p&gt;In other situations, the menu can be navigated back by using the hierarchical menu selector. In either case, the system is "verb" driven - you choose the action ("verb"), and then you choose where the action should apply. The biggest problem is probably that the menu system is inconsistent. Some menus have "back" or "cancel" options, and some don't. Some "verbs" are actually nouns - "Printing". Some verbs start with a capital letter - "Configure" - like they are nouns. Perhaps it is a sign of a menu element that doesn't require "an object". The "object" here is more "grammatical" than a software concept.&lt;/p&gt;&lt;p&gt;The Archives app is the built-in file manager for the VisiOn and is one of the standard apps. Somewhat surprisingly, it puts deleted files into the "Wastebasket" folder. Windows couldn't do that because of Apple's patents - but Apple clearly wasn't the first (I bet it's coming from Xerox).&lt;/p&gt;&lt;p&gt;The Archives app makes it clear that VisiOn's file system supports long file names. VisiOn runs on top of MS-DOS 2.0, so it has to implement its own FS on top of FAT for this to work. The app can also work in two-pane mode, but it divides the screen horizontally, so long file names would fit on the screen easily.&lt;/p&gt;&lt;p&gt;The "verb"-oriented interface requires the app to show a "NEW" item on the screen, though it is a bit confusing. Can you rename a "NEW" file?&lt;/p&gt;&lt;p&gt;Clip: the Archives application&lt;/p&gt;&lt;p&gt;There are some mysterious buttons we have not explored in VisiOn just yet. One of them, TRANSFER, is used to command the applications to perform a "copy-paste" operation. It is impossible to just "copy" a thing and then "paste" it multiple times.&lt;/p&gt;&lt;p&gt;You can see that the OPEN command is completely unnecessary, because the closed window can be opened simply by clicking its minimised button. It would be nice for VisiOn to remove the OPEN button and replace TRANSFER with separate COPY and PASTE buttons. It shouldn't be too difficult to implement - Transfer From and Transfer Into are different system events from the application point of view. The concept of Copy&amp;amp;Paste wasn't ubiqiutous, but it was not unheard of either, because the VisiOn Word has these options in the application menu, in addition to the system's TRANSFER.&lt;/p&gt;&lt;p&gt;By the way, did you notice a cute VisiOn icon in front of some app names? It is actually two "non-printable" characters, 0x16 and 0x17. The system font has a few more useful icons hidden in it.&lt;/p&gt;&lt;p&gt;The last important button on the system menu of the VisiOn operating system is OPTIONS. Some applications have a configuration file, and the contents of the configuration file can be displayed on the right side of the window. The Options window behaves like a separate app with a separate menu. It is kind of similar to a pop-up window.&lt;/p&gt;&lt;p&gt;Curiously, it is possible to open the Options window from within the application. The same Options dialogue is shown by Word either by clicking "OPTIONS" or by clicking "Print&amp;gt;local-print". But then Word also has Cut&amp;amp;Paste menu system that allows copying and pasting within the application (but not between the application windows).&lt;/p&gt;&lt;p&gt;At face value, Visi On is a sleek, minimalist-looking windowing system for office applications. But it was built by people involved with early object-oriented programming, and the sales pitch for the system made some pretty bold claims. Were they true? Let's find out.&lt;/p&gt;&lt;p&gt;This is a spoliers section for those who thought they knew things about Visi On! For everyone else, this is going to be boring - if so, skip to the next section :)&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The primary objectives of Visi-On is a consistent user interface and portability. Visi-On is designed to run on any operating system. ("The Visi On experience")&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Sort of. Claiming "Visi-On is designed to run on any operating system" is like claiming "Unix is designed to run on any hardware". Clearly, it was made with portability in mind, but even supporting CP/M-86 on IBM PC would require a completely different VISION.EXE, and a different installer floppy format (i.e. you couldn't install Visi On Calc we have on a VisiOn running on top of CP/M). Supporting a different computer architecture would have been quite an ordeal.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;It did this by providing a kind of non machine specific "virtual machine" (called the Visi Machine) that all applications were written for. (Toasty Tech)&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;What you have above Visi On or VOS itself is an interface we call the Visimachine interface. That is all of the calls that you need as a product designer to use all of the facilities provided by Visi On. This is the virtual machine? For product designers, this is the virtual machine. ("Byte", 1983/6)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The term "virtual machine" used by VisiOn developers means something different from what we mean by the words "virtual machine" today. The closest word we use today would be "API". That's right, Visi On applications use a cross-platform API. Just like almost any other operating system today. I bet it was a really cool idea back in 1983, though.&lt;/p&gt;&lt;p&gt;By the way, "VisiHost" for IBM PC is VISION.EXE. The "VisiMachine", which is not a virtual machine, but a set of system libraries and the desktop manager, is also known as "VOS", "VisiOn Operating System", "Application Manager" or simply "Services".&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The virtual machine provided supports virtual memory and concurrent processing. ("The Visi On Operating Environment", IEEE TCDE Bulletin, September 1983)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Half-true. Visi On indeed implements virtual memory, but it is a software implementation without any memory protection mechanisms. Nothing but good will stops applications from reading or corrupting memory used by other applications.&lt;/p&gt;&lt;p&gt;The words "concurrent processing" might lead you to believe that VisiOn is a truly multitasking system. But its concurrent processing capabilities are quite limited. It is most definitely not a preemptive multitasking system, because if an application hangs, the whole system hangs. There seem to be some provisions for background data processing, at least for printer spooling. I think a flavour of cooperative multitasking might be possible in VisiOn, but so far I could not find a way to run an application in the background, so maybe it is not multitasking at all!&lt;/p&gt;&lt;quote&gt;&lt;p&gt;[The virtual machine] comprises 12 abstract data types. Each abstract data type responds to messages and provides a specific type of service. ("The Visi On Operating Environment", IEEE TCDE Bulletin, September 1983)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Unclear. It seems there are some "messaging" capabilities, but most of the interaction with the OS is still done through regular system calls. So far, I have discovered only messages that create a window, define a menu and request events from the OS. And the messages aren't really related to the "abstract data types". Perhaps, the representation of the objects and data types was different on the source code-level?&lt;/p&gt;&lt;p&gt;Also, this statement contradicts what the authors said about the system in an earlier interview.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Visihost is an object-oriented operating system, and it’s composed of 10 object types... You can establish instances of the objects by just sending messages to them on a Smalltalk message-class type interface. ("Byte", 6/1983)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Half-true. The "objects" do not seem to be "objects" in a modern sense. There is no system of attributes, methods and classes. Instead, there are instances of structures that are passed through the API to the OS. Most of the communication with the OS doesn't happen through messages; it happens through system calls.&lt;/p&gt;&lt;p&gt;In fact, the very same interview confirms this:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;An object in Smalltalk basically is a message, yes, that carries with it something that says what can be done to it. Visi On objects are not that complex. They’re objects... yes, they do have context of what their formatting is, but they aren’t Smalltalk objects.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Next!&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Activities request services from the Visi-Machine via Visi-Ops or via BITS (Basic Interaction Techniques). The two are distinguished in that a Visi-Op call requires a process ID. (A 16 bit number assigned by Visi-Corp to a given application program). ("Visi On from a Software Developer's point of view", 1983)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mostly false. It seems VisiCorp itself couldn't agree on what BITS means; sometimes it is used for low-level system calls for the kernel ("VisiHost"), and sometimes it is used to talk about patterns of the user interface. Also, a process ID is not assigned by Visi-Corp; it is evaluated at run time.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;VOS (note: VisiMachine) is the only activity that actually does direct Visihost calls. All other calls come through VOS itself. ("Byte", 6/1983)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mostly true. On the machine code level, applications can and do call the kernel ("VisiHost") directly. But all the existing applications only do so to talk to the Services ("VisiMachine"). On the machine code level, nothing stops the application from calling the VisiHost - this is how VisiMachine is getting things done - but presumably this would harm portability.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Visi On did not, however, include a graphical file manager. ("Visi On", Wikipedia, November 2025)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;False. There is an application called Archive, which is a part of the "Services", and it is a bona fide file manager. It does not have icons, though; but there are no icons in any other parts of VisiOn, either.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The scripts capability is another important aspect of ease of use. It’s a learn mode. It has a window that you can interact with. You can stop that learn mode at any time and tell the system to accept a variable. You open a scripts window and say, “learn.” Then the system prompts you for a name, you type in the name, and that will be the name of a script. ("Byte", 6/1983)&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Unfortunately, this part of VisiOn seems to be missing from the release. And speaking of missing features, the demo from 1983 also has a mysterious SAVE button that is not present in the final release.&lt;/p&gt;&lt;p&gt;Most of the technical documentation about the system available until now comes from the following articles and posts:&lt;/p&gt;&lt;p&gt;Visi On is meant to run on an IBM PC XT with a hard disk. It won't run properly on an IBM PC AT, and it won't run in most emulators. The pre-installed unprotected version with an AT patch available on ToastyTech runs in some emulators (86Box and PCEm). There are three software packages that can be installed in VisiOn: Word, Calc and Graph. Trying to install them from any old floppy is not possible due to various copy-protection methods (more on this soon).&lt;/p&gt;&lt;p&gt;The installed copy of VisiOn on the hard drive has the executable file &lt;code&gt;VISION.EXE&lt;/code&gt;, and a bunch of cryptic files in the &lt;code&gt;VISI_ON&lt;/code&gt; folder. Most interesting of those are:&lt;/p&gt;&lt;code&gt;     856 PROGRAMS.VOS -- ??? binary data
  200000 RESERVED.VOS -- resources for the applications? swap?
  777728 SEG00000.VOS -- the actual software installed in the OS?
    3290 SEGMENTS.VOS -- ??? binary data
&lt;/code&gt;&lt;p&gt;The files don't have an obvious structure. To grasp a feeling of the file, I use my favourite tool: Load Image From Raw Data in GNU IMP.&lt;/p&gt;&lt;p&gt;Scrolling through the segments surfaces a high-resolution font file and a garbled startup screen:&lt;/p&gt;&lt;p&gt;Are the installed files encrypted?&lt;/p&gt;&lt;p&gt;The installation floppies have the files with names matching those on the hard disk, but they have different content. It is obvious that the contents are encrypted by some simple method. For example, here is the contents of the first installation floppy:&lt;/p&gt;&lt;code&gt;    3110 16 Dec  1983 00000009.VOS -- same as the installed version, but encrypted
   10334 16 Dec  1983 00000010.VOS -- same as the installed version, but encrypted
     110 16 Dec  1983 H0000000.VOS -- a binary directory of files
   65536 16 Dec  1983 SEG10002.VOS -- overlay, seemingly encrypted
   65536 16 Dec  1983 SEG10003.VOS -- overlay, -""-
   65536 16 Dec  1983 SEG10005.VOS -- overlay, -""-
   44604 16 Dec  1983 VINSTALL.COM -- installer tool
   71680 16 Dec  1983 VISION.EXE   -- the program itself, very clearly it is encrypted in some simple way
&lt;/code&gt;&lt;p&gt;The contents of the files show a repeating pattern. For example, in SEG10003.VOS:&lt;/p&gt;&lt;code&gt;0000fe50  3c 6a 4f 3c 3c 6a 4f 3c  3c 6a 4f 3c 3c 6a 4f 3c  |&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;|
0000fe60  3c 6a b0 3c c3 6a 4f 3c  3c 6a 4f 3c 3c 6a 4f 3c  |&amp;lt;j.&amp;lt;.jO&amp;lt;&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;|
0000fe70  3c 6a 4f 3c 3c 6a 4f 3c  3c 6a 4f 3c 3c 6a 4f 3c  |&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;&amp;lt;jO&amp;lt;|
&lt;/code&gt;&lt;p&gt;Such a repeating pattern is indicative of an encryption with XOR. This is a very poor encryption technique; not only can the encryption key be guessed easily, but a long sequence of zero-bytes will expose the key as it is.&lt;/p&gt;&lt;p&gt;The installation floppies are not only encrypted, but also copy-protected with "out-of-bounds" sectors. They require special emulation methods, but thankfully those methods are well described in 86Box and HxC floppy tool documentation.&lt;/p&gt;&lt;p&gt;With a simple encryption and decryption tool, I managed to change the text in the Tutorial app shipped with the operating system and package it back to the (still copy-protected) floppy.&lt;/p&gt;&lt;p&gt;A floppy with a Visi On program has dozens of files named &lt;code&gt;00001000.VOS&lt;/code&gt;, &lt;code&gt;00001234.VOS&lt;/code&gt; and so on. Which files are mandatory, and what is in them? Lots of trial and error ("let's delete this file, let's put back this file") shows that a floppy must have the following files:&lt;/p&gt;&lt;code&gt;00000000.VOS&lt;/code&gt; - simply 12 zeroes&lt;code&gt;00001000.VOS&lt;/code&gt; - the description of the floppy (disk label and the list of programs on it), encrypted&lt;code&gt;00001001.VOS&lt;/code&gt; - a copy-protection mechanism, twice-encrypted&lt;code&gt;00001000.VOS&lt;/code&gt;,&lt;p&gt;The patterns in the unencrypted files can be observed by simply looking at the files. For example, this is a fragment of &lt;code&gt;00001000.VOS&lt;/code&gt; from the Visi On Calc package:&lt;/p&gt;&lt;code&gt;00000080  16 17 20 43 6f 6e 76 65  72 74 20 74 6f 20 43 61  |.. Convert to Ca|
00000090  6c 63 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |lc..............|
000000a0  31 2e 30 00 00 00 00 00  00 00 00 00 01 00 41 04  |1.0...........A.|
000000b0  00 00 00 00 00 00 00 00                           |........|
&lt;/code&gt;
&lt;p&gt;Note: IBM PC is a little-endian architecture. The byte sequence &lt;code&gt;41 04&lt;/code&gt; should be read as &lt;code&gt;0x0441&lt;/code&gt;, or 1089 in decimal. Sure enough, &lt;code&gt;00001089.VOS&lt;/code&gt; stores the installation script for the program, referencing other files on the floppy disk:&lt;/p&gt;&lt;code&gt;00000000  a7 43 16 17 20 43 6f 6e  76 65 72 74 20 74 6f 20  |.C.. Convert to | &amp;lt;- magic number + logo + name
00000010  43 61 6c 63 00 00 00 00  00 00 00 00 00 00 00 00  |Calc............|
00000020  00 00 31 2e 30 00 00 00  00 00 00 00 00 00 01 00  |..1.0...........| &amp;lt;- version
00000030  03 00 02 00 00 00 00 00  00 00 00 00 00 00 0a 00  |................|
00000040  00 00 01 00 42 04 01 00  02 00 43 04 01 00 01 00  |....B.....C.....| &amp;lt;- 0x442 - first file to install 
00000050  44 04 01 00 02 00 45 04  01 00 02 00 46 04 01 00  |D.....E.....F...|
00000060  02 00 47 04 01 00 02 00  48 04 01 00 01 00 49 04  |..G.....H.....I.|
00000070  01 00 01 00 4a 04 01 00  01 00 4b 04 01 00 01 00  |....J.....K.....|
00000080  00 00 02 00 4c 04                                 |....L.|           &amp;lt;- .... 0x44c - last file to install
&lt;/code&gt;
&lt;p&gt;A big obstacle in developing applications is the copy-protection mechanism in &lt;code&gt;00001001.VOS&lt;/code&gt;. The file itself is lightly encrypted with XOR, and then heavily encrypted with XOR once again. Decrypting it and loading it in Ghidra allowed me to understand (generally speaking) that this little tool is an x86 program with a custom header and a single entry point. This entry point is called by the installer to check that the floppy is copy-protected and to decrypt the contents of the floppy.&lt;/p&gt;&lt;p&gt;Atsuko eventually rewrote the copy-protection binary, to skip the encryption and floppy checks. This version of &lt;code&gt;00001001.VOS&lt;/code&gt; is very useful even for installing VisiCorp's programs, as it allows using regular floppy disks, or to tweak the program sources before the installation.&lt;/p&gt;&lt;p&gt;Fun note: the XOR encryption key on software disks is stored in plain text at the beginning of every &lt;code&gt;00001001.VOS&lt;/code&gt; file. Such a glaring oversight!&lt;/p&gt;&lt;p&gt;Checking unencrypted files (looking closely at their contents in a hex editor) revealed the internal structure of a program package:&lt;/p&gt;&lt;code&gt;main()&lt;/code&gt; function is, and&lt;p&gt;The type of VOS file is determined by two independent factors:&lt;/p&gt;&lt;p&gt;Operating system development needs a good debugger. Even the history of Windows hints that a good debugger is essential for building a trillion-dollar software empire. And, as you can imagine, Visi On doesn't run under debuggers, so an IBM PC emulator with a built-in debugger is a must.&lt;/p&gt;&lt;p&gt;There are multiple debugging emulators: Qemu, MAME, Bochs, DosBox and MartyPC. None could run Visi On. Among these, Bochs was my primary target, as it can emulate a Mouse Systems mouse - the only mouse type supported by Visi On. Thanks to built-in debugging features, I produced a simple patch that allowed Visi On to boot in Bochs and Qemu. The patch simply skips a few mouse-related checks:&lt;/p&gt;&lt;code&gt;--- visionat.exe.dmp
+++ viatmice.exe.dmp
@@ -2534,4 +2534,4 @@
 0000a010  e8 7a 00 eb 49 b0 83 e8  47 00 e8 81 00 8b 1e 80  |.z..I...G.......|
-0000a020  0b 8d 57 05 ec a8 01 74  f8 8d 57 00 ec 24 f8 3c  |..W....t..W..$.&amp;lt;|
-0000a030  80 75 ee e8 78 00 e8 54  00 eb 23 c7 06 7e 0b ff  |.u..x..T..#..~..|
+0000a020  0b 8d 57 05 ec a8 01 90  90 8d 57 00 ec 24 f8 3c  |..W.......W..$.&amp;lt;|
+0000a030  80 90 90 e8 78 00 e8 54  00 eb 23 c7 06 7e 0b ff  |....x..T..#..~..|
 0000a040  ff 06 b0 33 b4 35 cd 21  8c c0 07 0b c3 bb 7a 06  |...3.5.!......z.|
&lt;/code&gt;
&lt;p&gt;The Bochs interface rhymes visually with VisiOn, being monochrome and pixelated.&lt;/p&gt;&lt;p&gt;If you want to reverse engineer a multi-tasking graphical operating system, the first thing you probably should figure out is its mouse driver. When you start an application, you cannot know where it will be loaded into the computer's memory until it is started. And when it is started, it is already too late to look at the application's initialisation. We need to stop the operating system the very moment we ask to start the program. In other words, the moment we release the mouse button after the double click.&lt;/p&gt;&lt;p&gt;Visi On uses serial mice connected over the COM port. Looking at the emulator events, I can see that the COM port is configured to be interrupt-driven. On an IBM PC, the handler for COM1 port interrupts is known as IRQ4/INT 0x0c. In other words, the address of the mouse driver is recorded in the interrupt table of the computer - it is set to &lt;code&gt;1a68:0000&lt;/code&gt;, which, by the way, is exactly where it is in VISION.EXE.&lt;/p&gt;&lt;p&gt;In Bochs, you cannot set up a breakpoint (sometimes known as "pause") at the interrupt address, but you can set up a breakpoint for the next instruction. When I figured this out, it was easy to set a breakpoint at the mouse driver and understand how the mouse driver works.&lt;/p&gt;&lt;p&gt;Now I could simulate mouse clicking in the following way. RAM address &lt;code&gt;0x1f21b&lt;/code&gt; holds the mouse button status. Writing "1" there makes the OS think there was a right button click. Writing "2" and then "0" works as "press and release the left mouse button". With this, I managed to pinpoint the moment the OS starts an applications.&lt;/p&gt;&lt;p&gt;A tool that can convert machine code back to something human-readable is called a disassembler. There are many options, but I went with NSA's Ghidra as it is the tool I've used in the past to reverse-engineer the Sumikko Gurashi computer.&lt;/p&gt;&lt;p&gt;Normally, disassembly is a straightforward process. Truth to be told, I expected the whole reverse engineering process to take a couple of weekends. If only life was so simple...&lt;/p&gt;&lt;p&gt;Here is a bit of the disassembly of now-open-source contemporary text editor EDLIN from Microsoft, as seen by Ghidra:&lt;/p&gt;&lt;code&gt;       0000:0119 50              PUSH       AX
       0000:011a b4 30           MOV        AH,0x30     ; syscall 0x30
       0000:011c cd 21           INT        0x21        ; an MS-DOS call
       0000:011e 3c 02           CMP        AL,0x2
       0000:0120 7d 05           JGE        LAB_0000_0127
       0000:0122 ba 8a 10        MOV        DX,0x108a   ; pointer to an error message
       0000:0125 eb e7           JMP        LAB_0000_010e
&lt;/code&gt;
&lt;p&gt;Here is the corresponding source code:&lt;/p&gt;&lt;code&gt;;----- Check Version Number --------------------------------------------;
        push    ax
        mov     ah,Get_Version
        int     21h
        cmp     al,2
        jae     vers_ok                         ; version &amp;gt;= 2, enter editor
        mov     dx,offset dg:bad_vers_err
        jmp     short errj
;-----------------------------------------------------------------------;
&lt;/code&gt;
&lt;p&gt;The disassembly basically matches the source code and thus is easy to understand.&lt;/p&gt;&lt;p&gt;Compare with the disassembly coming from VisiOn:&lt;/p&gt;&lt;code&gt;       64c5:0c55 c7 06 16        MOV        word ptr [0x16],0x0
                 00 00 00
       64c5:0c5b 8b 1e 16 00     MOV        BX,word ptr [0x16]
       64c5:0c5f 89 1e 18 00     MOV        word ptr [0x18],BX
       64c5:0c63 8b 0e 18 00     MOV        CX,word ptr [0x18]
       64c5:0c67 89 0e 9c 15     MOV        word ptr [0x159c],CX
       64c5:0c6b 8b 16 9c 15     MOV        DX,word ptr [0x159c]
       64c5:0c6f 89 16 de 09     MOV        word ptr [0x9de],DX
       64c5:0c73 83 ec 02        SUB        SP,0x2
       64c5:0c76 c7 46 d6        MOV        word ptr [BP + -0x2a],0x1
                 01 00
       64c5:0c7b 83 ec 02        SUB        SP,0x2
       64c5:0c7e c7 46 d4        MOV        word ptr [BP + -0x2c],0x1742
                 42 17
       64c5:0c83 e8 9e 00        CALL       define_window
&lt;/code&gt;
&lt;p&gt;Can you follow the logic?&lt;/p&gt;&lt;code&gt;var_0x16 = 0
BX = var_0x16
var_0x18 = BX
CX = var_0x18
var_0x159c = CX
DX = var_0x159c
var_0x9de = DX
**whack the stack!**
BP[-0x2a] = 1
**whack the stack!**
BP[-0x2c] = 0x1742
CALL       define_window
&lt;/code&gt;
&lt;p&gt;Do you also feel your blood boiling from seeing the "hot potato" variable definition? It should have been&lt;/p&gt;&lt;code&gt;var_0x16 = 0
var_0x18 = 0
var_0x9de = 0
var_0x159c = 0
BX = 0
CX = 0
DX = 0
CALL define_window(0x1742, 1)
&lt;/code&gt;
&lt;p&gt;The comment "whack the stack!" above is quite representative of what is happening in the code.&lt;/p&gt;&lt;p&gt;Most computers nowadays have a stack. If you don't know what a "stack" is, imagine: you work as a clerk, and your assignments come in the form of sheets of paper with tasks. You put new sheets with tasks on top of the sheets you already have. When you need to process the next task, you usually take the topmost sheet. You might feel bad for all the old tasks at the bottom of the stack, but it is the easiest way to keep track of things.&lt;/p&gt;&lt;p&gt;Here is where "stack frames" come. Now, imagine that you have a coworker obsessed with efficiency. They think that some old tasks should be done before newer tasks, and some new tasks should be done after old tasks. To do so, they take a chunk of the sheets from the stack, rearrange them as they see fit, and put them back in. Sometimes they even grab multiple unrelated chunks of the stack at once. A chunk of a stack is a "stack frame".&lt;/p&gt;&lt;p&gt;Using stack frames simplifies code compilation for subroutines, because a subroutine can assume that it can do whatever it wants with its stack frame, treating it like its own private memory allocated on the global stack. "Forgetting" the data from the stack frame is as simple as moving the stack pointer.&lt;/p&gt;&lt;p&gt;This technique used to be common on x86-based computers some 40 years ago. Ghidra doesn't support it at all. Bochs doesn't care about the BP stack and can only show you the SP stack. VisiOn almost never uses the SP stack directly; most of the applications are working with the BP stack. That doesn't mean that the SP isn't changed when the values on the BP stack are changed; quite the opposite.&lt;/p&gt;&lt;p&gt;To make things more unusual, the BP register is used differently from the modern C compilers that support the BP stack frame. Instead of only relying on the local stack frame for local variables and arguments, the code also uses the BP register (assumed to be the stack frame point of reference) to put data on top of the real stack (in roughly 2/3 of the cases; 1/3 still uses pushes/pops and SP). Local variables, structures passed to VisiMachine through pointers (and sometimes used after "free"), variables prepared to be arguments for a function call, even return values from function calls, all appear as if they were shuffled across the BP stack frame and the stack itself.&lt;/p&gt;&lt;p&gt;If that doesn't sound fun enough, a copy of registers is preserved in the stack frame sometimes, depending on whether a function call was cross-segment or from the local segment (more on that later). The code snippet that preserves the registers is always &lt;code&gt;CALL&lt;/code&gt;ed, but it never returns. An extra address sometimes left on the stack on the return from the function? Nothing a simple stack manipulation can't fix!&lt;/p&gt;&lt;p&gt;An abundance of commands that do nothing with the stack (&lt;code&gt;add sp, 0 ; sub sp, 0&lt;/code&gt;) ties all of this back to the vintage C compiler. I believe this machine code is a result of using an immature C compiler, and has nothing to do with the Visi On's API. And, according to some reports, Visi On used a now-long-lost Lantech C, the first cross-compiler for Intel x86. It is probably safe to assume that Visi On compiled with a different compiler or for other platforms would have been easier to disassemble.&lt;/p&gt;&lt;p&gt;The IBM PC, VisiOn target computer, is built around the Intel 8088 processor. A remarkable thing about this processor is that it uses the segment memory model. In a nutshell, at any given moment in time, the program has access to no more than four fragments of the computer's RAM, each 64 kilobytes in size: the code segment, the data segment, the stack segment, and the "extra" segment. This memory organisation simplifies porting programs from 8-bit computers, and in theory allows a straightforward implementation of multi-tasking for small programs. If you have 640 kilobytes of RAM, and your program is configured to use a single segment for all four segments (CS, DS, SS and ES), you could easily load 10 programs at once.&lt;/p&gt;&lt;p&gt;But, as it happens, segments are quite limiting. A single data segment can store about 35 pages of "plain text" in a common 8-bit encoding. If you want to store a long document in the computer's RAM (a novel or a thesis), your program will need to switch between multiple data segments.&lt;/p&gt;&lt;p&gt;By the way, a memory reference to data within a single segment is called a "short pointer". A memory reference to a different segment is called a "far segment". To unambiguously identify a region in memory, you need a "long pointer" consisting of a segment and offset pair.&lt;/p&gt;&lt;p&gt;Things are much worse if a program doesn't fit in a single code segment. For programs running under DOS, it is usually not an issue: the program can assume it has a monopoly over the computer's RAM and just use "CALL FAR" and "JMP FAR" to change the current code segment. Even so, the operating system might load the program into any available memory segment. If the program uses "far" calls or pointers, the operating system must perform a "relocation". This is how things were done in DOS and early Windows versions.&lt;/p&gt;&lt;p&gt;VisiOn's approach to memory management is different from DOS. Each code segment is position-independent; it cannot use far CALLs or long pointers. Large programs are split into multiple code segments. When a program is executing a code segment 1 and needs to call a function from a code segment 2, for example, it must do so through the operating system. The benefit of this approach is a software implementation of "virtual memory". If a program is, for example, 2 megabytes large, and the computer only has 512 kB of RAM, the operating system can only keep in RAM the segments of the program that are being executed right now. When a program requests a segment not in the RAM, the OS can load it from the hard drive, in a form of swapping.&lt;/p&gt;&lt;p&gt;By the way, most of the time the ES segment is set to the kernel/OS/VisiHost data segment, and SS is set to DS (the current applications' data segment).&lt;/p&gt;&lt;p&gt;Even so, VisiOn could have been "normal" about their implementation of virtual memory. A far call might have looked like this: &lt;code&gt;call_segment(segment_number, function_address)&lt;/code&gt;. Instead, it looks like this: &lt;code&gt;call(). Magic!&lt;/code&gt;&lt;/p&gt;&lt;p&gt;This is what cross-segment calls look like in Ghidra (and it would look exactly the same in any other disassembler):&lt;/p&gt;&lt;code&gt;    5e32:009b cd c1       INT 0xc1                    ; Call operating system entry point 0xc1
    5e32:009d 28 08       SUB byte ptr [BX + SI],CL   ; ??? Change a random memory byte ???
&lt;/code&gt;
&lt;p&gt;The disassembler assumes that bytes &lt;code&gt;0x28 0x08&lt;/code&gt; encode a command. It is a normal thing to assume; this is how the Intel x86 processor normally works. But in this case, it is not a command, it is a 16-bit number: &lt;code&gt;0x0828&lt;/code&gt;. The OS tweaks the return address from the INT 0xc1 handler so these two bytes are skipped by the processor.&lt;/p&gt;&lt;p&gt;I call this kind of number "magic pointers", because a long pointer normally must be two 16-bit numbers: a segment and an offset. But in VisiOn, a single 16-bit number encodes both. This is implemented in a really clever way. Remember the "entry points" table I mentioned?&lt;/p&gt;&lt;p&gt;The "entry points" table has pairs of 16-bit numbers: segment and offset. For example, if a function is stored in a segment file 0x0002 at the offset 0x1234, the table will have both numbers written down:&lt;/p&gt;&lt;code&gt;&amp;lt;entry_points_table:0&amp;gt; 0x1234 0x0002
&lt;/code&gt;
&lt;p&gt;Now, what is the "magic pointer" then? It is a pointer (or offset) to the address of a row in this table, in bytes, relative to the beginning of the code segment where the entry points table is stored. Baaam!&lt;/p&gt;&lt;p&gt;The code above, &lt;code&gt;INT 0xc1 ; 0x0828&lt;/code&gt; basically tells the OS:&lt;/p&gt;&lt;code&gt;offset&lt;/code&gt; and &lt;code&gt;segment&lt;/code&gt;&lt;code&gt;segment:offset&lt;/code&gt;&lt;p&gt;Moreover, the &lt;code&gt;segment&lt;/code&gt; references in the entry points table are dynamically refreshed. The operating system keeps track of the physical RAM address where each segment is loaded.&lt;/p&gt;&lt;p&gt;VisiOn is unusually aggressive at memory management compared to its contemporaries; it keeps swapping code segments in and out. This is very troublesome for debugging.&lt;/p&gt;&lt;p&gt;Imagine that the program you are debugging, currently loaded to the computer's RAM at segment &lt;code&gt;0x5e32&lt;/code&gt;, makes a cross-segment call at the offset &lt;code&gt;0x9b&lt;/code&gt; (like in the code snippet in the previous chapter). Let's say you're not interested in what is happening in this call, and you want to just "step over" the function call. You expect that when the far call is completed, your program will continue starting from the address &lt;code&gt;0x5e32:0x09f&lt;/code&gt; (the next command after the "magic pointer"). Oh, how naive!&lt;/p&gt;&lt;p&gt;The operating system can (and often does) decide to swap your program out of RAM during the far call. When the OS swaps your program back in, it will put it in the next available code segment, for example, &lt;code&gt;0x4c4b&lt;/code&gt;. The execution will continue not from &lt;code&gt;0x5e3d:0x09f&lt;/code&gt; but from &lt;code&gt;0x4c4b:0x09f&lt;/code&gt;. Your breakpoint at &lt;code&gt;0x5e32:0x09f&lt;/code&gt; won't activate; the debugger's "step over" function simply doesn't work.&lt;/p&gt;&lt;p&gt;All the code segments in VisiOn have a command &lt;code&gt;jmp [es:0x0]&lt;/code&gt; at the address &lt;code&gt;0x9&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;When an application's function is called (be it &lt;code&gt;main&lt;/code&gt;, an event handler, or a "magic pointer" call), the OS pushes &lt;code&gt;0x9&lt;/code&gt; on the stack as the &lt;code&gt;return address&lt;/code&gt; before &lt;code&gt;jmp&lt;/code&gt; to the function's entry point.&lt;/p&gt;&lt;p&gt;When a function finishes its work and executes a &lt;code&gt;ret&lt;/code&gt; command, the CPU gets the return address from the stack (&lt;code&gt;0x9&lt;/code&gt;) and executes the command &lt;code&gt;jmp [es:0x0]&lt;/code&gt;. This is a far jump, but where does it jump to? The answer is: the CPU reads a long pointer from &lt;code&gt;es:0&lt;/code&gt; (the beginning of the OS kernel data segment); then jumps to it. The code at this point will decide what is the next &lt;code&gt;jmp&lt;/code&gt; destination. This technique is called "jumping into a trampoline".&lt;/p&gt;&lt;p&gt;If you're writing your program in assembly (and you shouldn't be), then no one stops you from replacing &lt;code&gt;ret&lt;/code&gt; at the end of your functions with:&lt;/p&gt;&lt;code&gt;add SP, 2
jmp [es:0x0]
&lt;/code&gt;
&lt;p&gt;You can avoid "returning to &lt;code&gt;0x9&lt;/code&gt;", but you still must jump into the trampoline. Fun!&lt;/p&gt;&lt;p&gt;A major part of the reverse-engineering effort was focused on trying to understand the internals of two smallest applications available for the OS, the Tutorial app ("tutor") and the Convert To Calc app ("cvtcalc"). The Tutorial app is 6.3 kilobytes of machine code, but that's actually quite a lot: 3525 lines (about 80 A4 sheets) of disassembly.&lt;/p&gt;&lt;p&gt;One thing that really simplified the debugging was adding Bochs' "magic breakpoints" to the Tutor and CVTCalc apps. Magic breakpoints work like this: when the emulator encounters a useless instruction - &lt;code&gt;xchg bx, bx&lt;/code&gt; - it treats it as a breakpoint. These breakpoints happen as if by "magic", without any need to simulate mouse click events or figure out segment relocation between the calls to the OS. The only downside: this command needs to be "squeezed in" into the existing machine code. Thankfully, some of the machine instructions in the Tutor app are &lt;code&gt;NOP&lt;/code&gt; ("do nothing"), so I replaced a few of those with &lt;code&gt;xchg bx, bx&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Most operating systems provide "system calls", a set of library methods that can manage disks, RAM, and so on. Graphical operating systems often provide calls for creating windows, and even handling the mouse and keyboard. Visi On is no exception.&lt;/p&gt;&lt;p&gt;A standard way to make a system call on an IBM PC-compatible is to call a software interrupt. The operating system tells the CPU that it can handle a certain software interrupt; a program uses this interrupt to communicate with the OS; the OS can return control to the program when the system call is finished. This is how system calls work in MS-DOS, for example:&lt;/p&gt;&lt;code&gt;;; Print a character
mov DL, '!'     ; the character to print in the DL register
mov AH, 2       ; function number 2 in the AH register
int 0x21        ; MS-DOS system call
&lt;/code&gt;
&lt;p&gt;VisiOn registers multiple interrupt handlers; among those, three are commonly used: &lt;code&gt;0xc0&lt;/code&gt;, &lt;code&gt;0xc1&lt;/code&gt; and &lt;code&gt;0xc2&lt;/code&gt;. The interrupts &lt;code&gt;0xc1&lt;/code&gt; and &lt;code&gt;0xc2&lt;/code&gt; are used for direct and indirect "magic pointer" function calls. &lt;code&gt;0xc0&lt;/code&gt; is the system call interrupt; it is the interface to the VisiHost.&lt;/p&gt;&lt;p&gt;Designed with portability in mind, VisiHost accepts arguments to the system calls through the stack: different processors might have different registers, but VisiOn most definitely needs to have a stack to work. A VisiOn system call looks like this:&lt;/p&gt;&lt;code&gt;;; Get the Segment ID for own data segment
push process_id         ; put "process_id" variable on the stack
push 0x219              ; push the syscall number and the size of the arguments in bytes on the stack
int 0xc0                ; call VisiHost
&lt;/code&gt;
&lt;p&gt;I originally thought that &lt;code&gt;0x219&lt;/code&gt; is the number of the syscall, but very quickly discovered that there are only ~0x70 syscall handlers, so the actual syscall number is simply &lt;code&gt;0x19&lt;/code&gt;. It took a bit of trial, error, reading the disassembly of the kernel, and stepping through a call to understand that &lt;code&gt;0x02&lt;/code&gt; is the number of the arguments passed to the syscall times two.&lt;/p&gt;&lt;p&gt;The reason for that is simple: the application's stack is stored in its own data segment. When the operating system takes control, it uses its own data segment with its own stack. To pass the parameters between the stacks, the OS copies all the syscall arguments from one stack to the other.&lt;/p&gt;&lt;p&gt;There aren't that many system calls that a regular application makes. Among those, the first two calls an application makes are &lt;code&gt;0x17&lt;/code&gt; and &lt;code&gt;0x19&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;&lt;code&gt;0x17&lt;/code&gt; returns the process ID for the current application.&lt;/p&gt;&lt;p&gt;&lt;code&gt;0x19&lt;/code&gt; takes a process ID as an argument and returns the data segment ID for the application. A VisiOn application absolutely must know its own Data Segment ID. The Segment ID is passed to all the syscalls; for example, when the application asks the OS to print a string on the screen, it needs to pass around not only the offset to the string relative to a data segment, but also the Segment ID for this data segment.&lt;/p&gt;&lt;p&gt;These two are followed by a system call &lt;code&gt;0x18&lt;/code&gt; - "get Application Manager data" - which I will describe later.&lt;/p&gt;&lt;p&gt;A bare-bones application for VisiOn must:&lt;/p&gt;&lt;p&gt;All of this is done with system calls &lt;code&gt;0x21&lt;/code&gt; and &lt;code&gt;0x22&lt;/code&gt;. How did I find this out? There was no silver bullet, I've been running the same code in the debugger over and over again, tweaking some parameters, commenting out some bits of code here and there, and eventually asking Atsuko to write a small assembly program following the specifications I provided to confirm the discoveries experimentally.&lt;/p&gt;&lt;p&gt;Originally, I thought that &lt;code&gt;0x21&lt;/code&gt; was something like "create windows &amp;amp; menus" and &lt;code&gt;0x22&lt;/code&gt; was "redraw the window and maybe wait for an event". But something didn't feel right. &lt;code&gt;0x21&lt;/code&gt; is always called with a different structure as the argument: sometimes it defines a window, sometimes it defines a menu and the event handlers, and sometimes it destroys all the created UI elements. &lt;code&gt;0x22&lt;/code&gt; always returns a value, and sometimes it makes the application go into the background.&lt;/p&gt;&lt;p&gt;So, my conclusion is: most likely, &lt;code&gt;0x21&lt;/code&gt; is "send the message" and &lt;code&gt;0x22&lt;/code&gt; is "receive the message (maybe wait for one)". I don't have many examples of "messages", but I managed to partially describe "create the window" and "wait for the events" structures.&lt;/p&gt;&lt;p&gt;These messages resemble Smalltalk, but they are relatively rare compared to other types of system calls. It makes me think that at some point VisiOn left behind its Smalltalk roots, and the "messages" subsystem might be just a remnant of the original design.&lt;/p&gt;&lt;p&gt;"Create the menu and wait for an event" function does something wacky. The structure we pass to the syscall &lt;code&gt;0x21&lt;/code&gt; accepts a pointer as one of the arguments. In the original VisiOn apps, it points to a structure created on a stack. For the sake of simplicity, we placed this structure in the data segment. Things worked until we added on-screen buttons; clicking a button would crash the system. Why? The operating system used this pointer to access data from both after and before the pointer. In other words, this is a pointer to the middle of a structure!&lt;/p&gt;&lt;p&gt;Why would anyone do that? No idea. This detail of the implementation likely didn't matter for programs written in Visi C, and most developers probably didn't even know about it.&lt;/p&gt;&lt;p&gt;The articles in the BYTE magazine tell us that if an application wants to draw on the screen, print a text, read a file from the disk, or define an on-screen button, it needs to do so through VisiMachine. Indeed, while VisiHost system calls can do a great many things, the applications I tried to reverse-engineer never called them directly. For example, there are syscalls &lt;code&gt;0x34&lt;/code&gt; and &lt;code&gt;0x35&lt;/code&gt; for drawing a bitmap on the screen and copying a bitmap from the screen, but these syscalls are only ever called from the Services app. Moreover, they're not "window-aware": with these calls, the application can draw on the screen outside of its own window!&lt;/p&gt;&lt;p&gt;So, if we want to be good citizens, we need to follow the standard call convention and reach out to the VisiMachine. But how?!&lt;/p&gt;&lt;p&gt;The most common system call in any application is &lt;code&gt;0x1e&lt;/code&gt;. This call seemingly does almost anything, including but not limited to: reading data from files, printing text on the screen and creating on-screen buttons. Sounds like a "VisiOp" (VisiMachine) call, doesn't it?&lt;/p&gt;&lt;p&gt;Figuring out the VisiOp calls was really challenging. The number of arguments for the call is always different, and even the arguments themselves are different between different runs of the same program. This call is intense!&lt;/p&gt;&lt;p&gt;When a program starts, it asks the OS for the Application Manager data segment using syscall &lt;code&gt;0x18&lt;/code&gt;. From this segment, the program copies into its own segment:&lt;/p&gt;&lt;p&gt;If you're just looking at disassembly, this operation is simply copying 372 bytes (12 + 1 * 2 + 2 + 170 words) from one segment to the other.&lt;/p&gt;&lt;p&gt;When a program needs to call a VisiOp, the syscall &lt;code&gt;0x1e&lt;/code&gt; receives:&lt;/p&gt;&lt;p&gt;Additionally, the application sets a flag at the &lt;code&gt;segment+offset&lt;/code&gt; of the Application Manager before this call, and clears it after the call.&lt;/p&gt;&lt;p&gt;My understanding of the Virtual Device IDs is limited and is based on the actions taken by the OS.&lt;/p&gt;&lt;code&gt;// VT = Virtual Terminal
#define DEVICE_VT 0x3
#define DEVICE_MEM 0x4
#define DEVICE_MENU 0xc

#define SYS_MESSAGE 0x0
#define SYS_CALL 0x1
&lt;/code&gt;
&lt;p&gt;The "Virtual Device" IDs are sort of similar to the list of "data types" from the article "The Visi On Operating Environment":&lt;/p&gt;&lt;code&gt;PROGRAM 
PROCESS 
MEMORY SEGMENT
PORT 
RASTER
DEVICE
FILE
BACKGROUND 
FONT
MOUSE
SOUNDMAKER 
KEYBOARD
&lt;/code&gt;
&lt;p&gt;But it couldn't be the same thing! Both "font management" (FONT) and "define clickable area" (MOUSE) are managed through the &lt;code&gt;DEVICE_VT&lt;/code&gt;. Did the specification for the system change between this article and the OS release? No idea.&lt;/p&gt;&lt;p&gt;Things get really interesting and confusing if you consider that the &lt;code&gt;0x1e&lt;/code&gt; system call requires a "function" ID to operate. For example, if you want to load a font, you need to look up the "function" ID &lt;code&gt;0x18&lt;/code&gt;, and pass it along with the &lt;code&gt;DEVICE_VT&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;As you can imagine, it is impossible to load a font in a &lt;code&gt;DEVICE_MEM&lt;/code&gt;, and it is impossible to read a file from &lt;code&gt;DEVICE_VT&lt;/code&gt;. What is the point of using both device ID and function ID, then? I don't know. But considering that we pass the number of arguments twice, perhaps, there is no meaning to it. Perhaps, VisiOps were implemented by two different teams who couldn't agree on how to pass the arguments between the VisiHost and the VisiMachine.&lt;/p&gt;&lt;p&gt;The true nature of "function" IDs is "magic" pointers. The "function ID" for any VisiOp is simply an offset to a function in the "magic" pointers list for the Application Manager. There are over 600 "magic" pointers in the Application Manager (you can find the list in &lt;code&gt;SEG10003.VOS&lt;/code&gt; at offset &lt;code&gt;0xa600&lt;/code&gt;), but only 170 of them are used as VisiOps.&lt;/p&gt;&lt;p&gt;While VisiOn has a VisiOp that can copy data between two segments by their Segment IDs, every now and then it can be useful to resolve the physical address for a given memory segment. This is most definitely not a cross-platform approach, but VisiOn applications use it when they want to peek inside the Application Manager's data segment.&lt;/p&gt;&lt;p&gt;The memory access dance is done this way:&lt;/p&gt;&lt;code&gt;segID2seg = es:[[es:0x6]+[es:0x4]]&lt;/code&gt;&lt;code&gt;es:[segID2seg+segID]&lt;/code&gt; stores flags of the segment ID (swapped in/out, used for read/write)&lt;code&gt;es:[segID2seg+segID+2]&lt;/code&gt; stores the physical location of the segment in the RAM, if it is loaded&lt;p&gt;If the segment is not present in RAM (swapped out), it is possible to ask the OS to load it for you. I highly suspect syscall &lt;code&gt;0x05&lt;/code&gt; is responsible for segment loading, but most apps are not using it. All the normally required memory segments are present in the RAM as if by "magic", anyways. The Pyramid game is using this call to ensure the font segment is in the RAM. Without this call, it might not load in time on a slow machine like an XT; this is probably related to the DMA disk operations initiated by the OS.&lt;/p&gt;&lt;p&gt;It isn't too difficult to use VisiOn's Virtual Terminal Device for text output and on-screen buttons, but displaying graphics and custom fonts required a bit of trial and error. The reason, of course, is the lack of references: VisiOn only displays images on the splash screen of programs like Word and Calc!&lt;/p&gt;&lt;p&gt;I think there must be a VisiOp function for displaying a bitmapped image. But, for some reason, when VisiOn Calc draws a splash screen, it uses something completely different: a custom font.&lt;/p&gt;&lt;p&gt;The bitmapped image is divided into glyphs, glyphs (1-127) are loaded as a font, and then the image on the screen is printed as if it was just a string. The Convert To Calc logo, printed with the default font, looks like this:&lt;/p&gt;&lt;p&gt;You can see that this method allows image compression: empty blocks are represented by spaces.&lt;/p&gt;&lt;p&gt;The VisiOp "load font" loads a font from a Segment ID passed to it. This means an application must know how to find a (dynamically-assigned!) Segment ID for any of its segments. The code that resolves a Segment ID for a magic pointer &lt;code&gt;0x810&lt;/code&gt; is so clever it made me flip my table:&lt;/p&gt;&lt;code&gt;mov ax, [cs:0x810+2]
&lt;/code&gt;
&lt;p&gt;Convert To Calc has multiple code and data segments. One of those segments has a table of "magic" pointers. The "magic" pointer at offset &lt;code&gt;0x810&lt;/code&gt; is a "magic" pointer to the file with the font. So far, nothing out of the ordinary.&lt;/p&gt;&lt;p&gt;As I mentioned before, the operating system fills out the "magic" pointer table (list of entry points) with the Segment IDs when it starts an application. The Segment IDs are filled out "in place". The entry points list in the Tutorial app is stored in a segment that doesn't have any code in it.&lt;/p&gt;&lt;p&gt;But Convert To Calc has a couple of functions exported from the "entry points and magic pointers" segment. When a cross-segment call is made to such a function, the current list of magic pointers and Segment IDs is stored right in the same code segment. A "magic" pointer, simply being an offset from the beginning of the file, can be read with a simple &lt;code&gt;mov&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;mov ax, [cs:magic_pointer]   ;; entry point offset
mov ax, [cs:magic_pointer+2] ;; entry point Segment ID
&lt;/code&gt;
&lt;p&gt;So, &lt;code&gt;mov ax, [cs:0x810+2]&lt;/code&gt; called from the code segment with the entry points table allows the program to know what Segment ID was assigned to the font segment.&lt;/p&gt;&lt;p&gt;Printing text through the VisiOn's virtual terminal in the graphical mode, for all intents and purposes, behaves like a proper Bit blit. One of the VisiOp parameters accepts a ROP code ("Raster OPeration").&lt;/p&gt;&lt;p&gt;VisiOn takes an interesting approach to ROPs and bitmap displaying. You might know that Windows supported ternary BitBLT with JIT-generated machine code for display rendering. VisiOn uses binary ROPs, similar to the ROPs in Xerox Alto or Bell Labs BLIT, and it also produces JIT machine code, but it produces the code for the "glyph space".&lt;/p&gt;&lt;p&gt;Among other things, VisiOn will break each character into bits when you load a font and then emit the machine code that will produce the required bits. Basically, if your font array was &lt;code&gt;font[char_id][bit_num]&lt;/code&gt;, it will be converted into &lt;code&gt;font_jit[bit_num][char_id]&lt;/code&gt;. I am not sure why; maybe there are some performance benefits to this approach.&lt;/p&gt;&lt;p&gt;If this sounds like an unnecessary headache, remember that bitmapped output on CGA is a headache already. The screen buffer in CGA is interlaced: odd and even lines are stored in separate memory blocks. The pixels on the screen are bit-packed, too. If you want to plot a pixel at coordinates (1,1), your program will need to:&lt;/p&gt;&lt;p&gt;These calculations are expensive, so it only makes sense to make the video driver slightly more complicated but feature-rich. For example, if you're reading the pixel from RAM anyway, you can choose between ADD, OR, NOT, or XOR pixel operations for free.&lt;/p&gt;&lt;p&gt;There are 16 available ROPs in total. Here is a checkerboard background and a circle drawn on top of it with different ROPs:&lt;/p&gt;&lt;p&gt;Each application is shipped with something I call a "mini-file-system". The format of it is primitive: the number of entries, the list of pointers to the entries, and then the entries themselves. Each entry has a header similar to the "segment header" used by the installer, consisting of the magic number and the length of the entry.&lt;/p&gt;&lt;p&gt;The mini-FS, among other things, is used for the built-in help system. Entries to the mini-FS can be referenced from the menu system, so the OS could "magically" display the right entry when the user clicks "HELP".&lt;/p&gt;&lt;p&gt;Naturally, the application can read entries from the mini-FS with a simple VisiOp call.&lt;/p&gt;&lt;p&gt;This reverse-engineering project ended up being much bigger than I anticipated. We have a working application, yes, but so far I've documented less than 10% of all the VisiHost and VisiOp calls. We still don't know how to implement keyboard input, or how to work with timers and background processes (if it is possible).&lt;/p&gt;&lt;p&gt;Atsuko and I would like to continue working on this SDK, but considering our other projects, I cannot imagine it taking as much priority as it has so far. This may be as far as we get. But this is pretty far already. If one were to follow these notes, they should be able to discover and document new VisiOps, say, from Word or Graph, very fast.&lt;/p&gt;&lt;p&gt;I discovered two funny bugs in the process of reverse-engineering.&lt;/p&gt;&lt;p&gt;If you've done any graphical programming for windowed environments, you would expect that the &lt;code&gt;Create_Window()&lt;/code&gt; function requires window dimensions for a freshly-created window. VisiOn is free from such prejudice. As far as I can tell, applications are not supposed to freely decide what their window size should be. The Application Manager's option sheet has fields "window width" and "window height" that define the dimensions for most windows (except for the Application Manager, Help and Tutorial windows).&lt;/p&gt;&lt;p&gt;Naturally, the application can read the dimensions of its window so it can resize the contents inside. But if the window dimensions are too small, some of the applications would crash, and would take down the whole system:&lt;/p&gt;&lt;p&gt;VisiOn loves to beep at the user. It beeps every time a menu option is chosen or an on-screen button is clicked.&lt;/p&gt;&lt;p&gt;If you are tired of the noise, you'd appreciate that Application Manager has an option to replace the sound with a "visual beep". It is implemented as a flashing area of 32x16 pixels around the mouse cursor. Every time the flashing is about to happen, an image "below" the cursor is preserved in RAM to be restored after the "visual beep" is over. However, the memory allocated for this bitmap is never freed. It takes between 200 and 1000 clicks to fill the RAM with useless copies of the mouse cursor, and then the system crashes.&lt;/p&gt;&lt;p&gt;Huge thanks to:&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46145960</guid><pubDate>Thu, 04 Dec 2025 10:25:39 +0000</pubDate></item><item><title>Autism's confusing cousins</title><link>https://www.psychiatrymargins.com/p/autisms-confusing-cousins</link><description>&lt;doc fingerprint="9d12f95502d28c97"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Autism’s Confusing Cousins&lt;/head&gt;
    &lt;head rend="h3"&gt;A differential diagnosis for the weird and the awkward&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;“I think that these days what we mean by “autism” is basically “weird person disease.””&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Sorbie Richner, Rich Girl Rehab&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Accurate diagnosis requires consideration of multiple diagnoses. Sometimes, different diagnoses can overlap with one another and can only be differentiated in subtle and nuanced ways, but particular diagnoses vary considerably in levels of public awareness. As such, an individual may meet the diagnostic criteria for one diagnosis but self-diagnoses with a different diagnosis because it is better known.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Sam Fellowes, Self-Diagnosis in Psychiatry and the Distribution of Social Resources&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, these days I meet many people in the psychiatric clinic who are convinced that they have autism, or suspect (with various degrees of confidence) that they have autism, or report being diagnosed with autism at some point in their lives by some clinician. And for a fair number of such individuals, I cannot say with reasonable certitude that they have autism. The reasons they give for considering autism vary widely, but tend to be along the lines of…&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;“Eye contact makes me very uncomfortable.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I suck at small talk.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I have rigid routines.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I hyper-focus on my hobbies.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I am always fidgeting.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Social interaction exhausts me.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I really bad at making friends.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“I don’t fit in; people find me weird.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s interesting about many of the items above is that the number one diagnostic possibility in my mind is an anxiety disorder of some sort. I remember seeing a woman who was a classic example of someone with high neuroticism, poor self-esteem, and severe social anxiety, and she had believed for much of her life that she was autistic because some random doctor somewhere at some point (she couldn’t even remember who or what sort of assessment this involved) had told her that she had autism, and she believed it because it fit in with her experience of being awkward-shy-weird.&lt;/p&gt;
    &lt;p&gt;It is common for me to meet individuals who think they have autism and find myself thinking, “schizoid,” “obsessive compulsive,” “cluster B,” “social anxiety,” “generalized anxiety,” “trauma,” “socially awkward,”… None of these, however, have the mimetic virality of autism.&lt;/p&gt;
    &lt;p&gt;I don’t want to come across as being skeptical of the reality of autism as a diagnosis or as asserting that most people are misdiagnosed. Autism exists, to the extent that any psychiatric disorder exists. Not everyone is misdiagnosed, perhaps even most people. I am not trying to say, “autism is bullshit.” It’s not. I offer the diagnosis of autism as a clinician perhaps as often as I find myself doubting it.&lt;/p&gt;
    &lt;p&gt;What intrigues me is that people are drawn to autism as a diagnosis because it seems to offer recognition of something they’ve lived with: they may be deeply awkward, terribly shy, or bad with people, they may struggle with social interactions, they may find other people annoying, other people may find them weird, they may have a hard time connecting to others, they may have been bullied, and they may have directed their loneliness or introversion towards peculiar interests or hobbies. Autism seems to them to capture all that. It seems like an apt and appealing narrative. But autism may also be the only relevant diagnosis they’ve heard of or are familiar with. They haven’t seen any cool TikToks about being schizoid. No one’s offering them quizzes about being schizotypal. A random pediatrician or primary care doc is not going to tell them they have an obsessive-compulsive style of personality. So when some professional doubts that they have “autism,” they see it as a dismissal or rejection of their “lived experience.” Of course, I am weird-anxious-awkward. How can you say otherwise? What they don’t know is that the choice is not between autism or nothing, but rather between autism and about a dozen other diagnostic possibilities.&lt;/p&gt;
    &lt;p&gt;So for the sake of our collective sanity, let’s consider a few of them…&lt;/p&gt;
    &lt;p&gt;To be diagnosed with autism spectrum disorder according to DSM-5, a person must have ongoing difficulties in social communication and interaction in all three areas: trouble with back-and-forth social connection, problems with nonverbal communication like eye contact and body language, and difficulty making or keeping friendships. They also must show at least two types of repetitive or restricted behaviors, such as repetitive movements or phrases, needing things to stay the same, having very intense focused interests, or being unusually sensitive (or under-sensitive) to things like sounds, textures, or lights. These patterns must have been present since early childhood (even if they weren’t noticed until later when life got more complicated), lead to substantial impairment in functioning, and can’t simply be explained by intellectual disability (or other psychiatric disorders).&lt;/p&gt;
    &lt;p&gt;To “have” autism is simply to demonstrate this cluster of characteristics at the requisite level of severity and pervasiveness. It doesn’t mean that the person has a specific type of brain attribute or a specific set of genes that differentiates them from non-autistics. No such internal essence exists for the notion as currently conceptualized.&lt;/p&gt;
    &lt;p&gt;Autism spectrum is wide enough to have very different prototypes within it. On one end we have profound autism, representing someone with severe autistic traits who is completely dependent on others for care and has substantial intellectual disability or very limited language ability. At the other end, we have successful nerdy individuals with autistic traits and superior intelligence, often seen in science or academia, à la Sheldon Cooper. (Holden Thorp, editor-in-chief of the Science journals and former UNC chancellor, for example, has publicly disclosed his own autism diagnosis.) This wide range is confusing enough on its own, even without considering other conditions that can present with autism-like features.&lt;/p&gt;
    &lt;p&gt;Autism cannot be identified via medical “tests.” It is identified via clinical information in the form of history, observation, and interaction, and the less information available or the more unreliable the information provided is, the more uncertain we’ll be. To have autism is basically a judgment call that one is a good match to a descriptive prototype. We can get this judgment wrong, and we sometimes do get it wrong. (There is nothing wrong with this fallibility as such, as long as we recognize it. Lives have been built on foundations less sturdy.)&lt;/p&gt;
    &lt;p&gt;Autism as a category or identity has taken on a life of its own. I am aware that not everyone in the neurodiversity crowd accepts the legitimacy of clinician judgments or clinical criteria as outlined in the diagnostic manuals, such as the DSM and ICD. There are other ways to ground the legitimacy of self-diagnoses, in theoretically virtuous accounts or pragmatic uses, which require distinct considerations of their own; I don’t reject that. But here, I am concerned with autism as a clinical diagnosis and the accuracy of autism understood in terms of alignment with clinical diagnosis. Would competent and knowledgeable clinicians with access to all relevant clinical information concur that the person’s presentation meets diagnostic criteria for autism? If you don’t really care about that, this post is not for you.&lt;/p&gt;
    &lt;head rend="h4"&gt;Schizoid Personality&lt;/head&gt;
    &lt;p&gt;Schizoid personality describes people who have little desire for close relationships and prefer solitary activities. Unlike people who are simply shy or socially anxious, individuals with schizoid personality style genuinely don’t find relationships rewarding or necessary. They typically appear emotionally detached or cold, show restricted emotional expression, seem indifferent to praise or criticism, and have few if any close friends or confidants. They often live quietly on the margins of society, pursuing solitary interests or jobs. They keep their inner worlds (which can be quite rich) private and don’t seek emotional intimacy with others.&lt;/p&gt;
    &lt;p&gt;In autism, social difficulties stem from genuine challenges with processing social information: difficulty reading facial expressions, understanding implied meanings, picking up on social cues, knowing unwritten social rules, etc. In schizoid personality, the person typically understands social conventions but simply isn’t motivated to engage with them. They withdraw from genuine disinterest. Schizoid personality also lacks the additional features of autism (repetitive or restricted behaviors, various sensory sensitivities).&lt;/p&gt;
    &lt;head rend="h4"&gt;Schizotypal Personality&lt;/head&gt;
    &lt;p&gt;Schizotypal personality describes people who have odd or eccentric beliefs, unusual perceptual experiences, and difficulties with close relationships. Unlike schizoid personality (which involves simple disinterest in relationships), schizotypal includes strange ways of thinking and perceiving the world. People with schizotypal personality might believe in telepathy, feel they have special powers, think random events have special meaning for them personally, or have unusual perceptual experiences (like feeling a presence in the room or hearing whispers). They typically have few close friends, experience social anxiety that doesn’t improve with familiarity, and may appear paranoid or suspicious of others’ motives. Both schizotypal personality and autism can involve social difficulties and odd or eccentric behavior, but in schizotypal personality, the peculiarity comes from magical thinking, paranoid ideas, and perceptual distortions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Obsessive-Compulsive Personality&lt;/head&gt;
    &lt;p&gt;Obsessive-compulsive personality describes people who are preoccupied with orderliness, perfectionism, and control. These individuals are rigid rule-followers who want things to be done “the right way,” have difficulty delegating tasks, and get caught up in details and lists to the point where they lose sight of the main goal. They tend to be workaholics who neglect leisure and friendships, are inflexible about matters of morality or ethics, and are often stubborn and controlling. Both obsessive-compulsive personality and autism can involve rigid adherence to routines, rules, and specific ways of doing things. In obsessive-compulsive personality, the inflexibility comes from anxiety about loss of control. The person is trying to, consciously or unconsciously, manage anxiety through control and perfectionism. In autism, the need for sameness and routine serves different functions. It provides predictability in a world that feels confusing or it helps with sensory regulation rather than anxiety-driven perfectionism.&lt;/p&gt;
    &lt;head rend="h4"&gt;Social Phobia&lt;/head&gt;
    &lt;p&gt;Severe social anxiety is an intense, persistent fear of social situations where a person might be judged, embarrassed, or humiliated. Social anxiety disorder involves overwhelming fear that interferes with daily life. People with this condition worry excessively about saying something stupid, looking foolish, or being rejected. They often avoid social situations entirely, which can lead to isolation, difficulty maintaining employment, and problems forming relationships. Both social anxiety and autism involve social difficulties and withdrawal. Social anxiety usually improves significantly in comfortable, safe environments (like with close family or friends), while autistic social differences tend to be more consistent across all contexts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Borderline Personality&lt;/head&gt;
    &lt;p&gt;Borderline personality disorder involves intense emotional instability, unstable relationships, fear of abandonment, and a shifting sense of self, with people experiencing rapid mood swings and chaotic relationships that alternate between idealization and devaluation of others. While it can resemble autism through social difficulties, emotional dysregulation, rigid thinking, and feeling different from others, the key distinctions are that borderline centers on intense relationship preoccupations and emotional chaos, whereas autism involves genuine difficulty understanding social cues and communication; borderline features rapidly shifting identity and relationship-triggered mood swings, while autism includes stable self-concept, sensory sensitivities, restricted interests, and literal communication that aren’t present in borderline; and borderline symptoms fluctuate dramatically with relationship stability while autistic traits remain consistent across contexts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Social Communication Disorder&lt;/head&gt;
    &lt;p&gt;Social communication disorder is a condition in DSM-5 where someone has significant, ongoing difficulty using verbal and nonverbal communication appropriately in social contexts. People with social communication disorder struggle with the “pragmatic” aspects of language, that is, knowing how to use language effectively in social situations. They may have trouble understanding when to take turns in conversation, knowing how much detail to give, adjusting their speaking style for different situations, understanding implied meanings or hints, picking up on nonverbal cues like body language and facial expressions, and knowing how to start, maintain, or end conversations naturally. This makes forming friendships and relationships difficult and affects life functioning. The social communication problems in social communication disorder look nearly identical to the “Criterion A” features of autism. However, unlike autism, people with social communication disorder don’t show repetitive behaviors, restricted interests, sensory sensitivities, or the need for sameness and routine.&lt;/p&gt;
    &lt;p&gt;Social communication disorder is rarely diagnosed in favor of autism primarily because autism provides access to critical services, insurance coverage, educational support, and legal protections that social communication disorder does not reliably offer, creating strong practical incentives for families and clinicians to prefer the autism diagnosis. Additionally, autism has an established evidence base, validated assessment tools, clear intervention protocols, and a large supportive community with a neurodiversity-affirming culture, while social communication disorder has none of these. It has no community, minimal research, no specific treatments, and little professional awareness since it was only introduced in the DSM in 2013. Service delivery, insurance, and educational systems are built entirely around autism rather than social communication disorder, and since both conditions require similar interventions for social-communication difficulties, there’s little practical incentive to make the diagnostic distinction, especially when the boundary between them (whether restricted/repetitive behaviors are truly absent or just subtle) is often unclear and clinicians are often unsure the distinction really matters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Trauma-Related Disorders&lt;/head&gt;
    &lt;p&gt;Trauma-related disorders, particularly from early developmental trauma, severe neglect, or disrupted attachment, can mimic autism through social withdrawal and avoidance of eye contact (defensive protection rather than social processing difficulties), communication delays and difficulties (from lack of language exposure or trauma’s impact on brain development), emotional dysregulation and meltdowns (from emotional dysregulation rather than sensory overload), repetitive self-soothing behaviors (anxiety management rather than stimming), sensory sensitivities (hypervigilance rather than sensory processing differences), and rigid need for routine (anxiety-driven safety-seeking rather than cognitive processing style).&lt;/p&gt;
    &lt;p&gt;Severe early deprivation can create “quasi-autistic” patterns that can be genuinely difficult to distinguish. The critical distinctions are that trauma-related difficulties typically improve significantly in safe, nurturing environments and with adequate psychological treatment, show more variability across contexts (worse with triggers), are tied to identifiable adverse experiences rather than present from earliest infancy, and lack the restricted interests and genuine social communication processing deficits of autism.&lt;/p&gt;
    &lt;head rend="h4"&gt;Social Awkwardness&lt;/head&gt;
    &lt;p&gt;Social awkwardness refers to social ineptness without meaningful impairment that falls within what is considered normal or typical human variation. This can be mistaken for autism because both may involve limited friendships, preference for solitude, conversation difficulties, reduced eye contact, and intense interests, particularly fueled by online self-diagnosis culture and broad autism awareness. The key distinctions are that socially awkward individuals understand what they should do socially but find it difficult or uninteresting (versus genuinely not understanding unwritten rules), show significant improvement with practice and maturity, are more comfortable in specific contexts, lack the sensory sensitivities and restricted/repetitive behaviors required for autism diagnosis, and generally achieve life goals despite awkwardness rather than experiencing clinically significant impairment.&lt;/p&gt;
    &lt;head rend="h4"&gt;Other conditions to consider in the differential diagnosis of autism &lt;/head&gt;
    &lt;p&gt;Selective Mutism, Intellectual Disability (without autism), Stereotypic Movement Disorder, Attention-Deficit/Hyperactivity Disorder (ADHD), Schizophrenia Spectrum Disorders, Avoidant Personality Disorder, Attachment Disorders, Generalized Anxiety Disorder, Obsessive-Compulsive Disorder, and Rett Syndrome (a characteristic pattern of developmental regression after initial normal development, typically 6-18 months).&lt;/p&gt;
    &lt;head rend="h4"&gt;Additional Caveats &lt;/head&gt;
    &lt;p&gt;Comorbidity is possible and expected. Someone can be autistic and have maladaptive personality patterns, trauma histories, or anxiety disorders that complicate the presentation. Developmental context, response to relationships, and subjective experiences are all very important in looking beyond the surface presentation to understanding the meaning and functions of behaviors.&lt;/p&gt;
    &lt;p&gt;See also:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46172443</guid><pubDate>Sat, 06 Dec 2025 11:18:40 +0000</pubDate></item><item><title>GrapheneOS is the only Android OS providing full security patches</title><link>https://grapheneos.social/@GrapheneOS/115647408229616018</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173407</guid><pubDate>Sat, 06 Dec 2025 13:58:31 +0000</pubDate></item><item><title>Tiny Core Linux: a 23 MB Linux distro with graphical desktop</title><link>http://www.tinycorelinux.net/</link><description>&lt;doc fingerprint="9ddb391819bfbc66"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Welcome to The Core Project - Tiny Core Linux&lt;/head&gt;
    &lt;p&gt;The Core Project is a highly modular based system with community build extensions.&lt;/p&gt;
    &lt;p&gt;It starts with a recent Linux kernel, vmlinuz, and our root filesystem and start-up scripts packaged with a basic set of kernel modules in core.gz. Core (11MB) is simply the kernel + core.gz - this is the foundation for user created desktops, servers, or appliances. TinyCore is Core + Xvesa.tcz + Xprogs.tcz + aterm.tcz + fltk-1.3.tcz + flwm.tcz + wbar.tcz&lt;/p&gt;
    &lt;p&gt;TinyCore becomes simply an example of what the Core Project can produce, an 16MB FLTK/FLWM desktop.&lt;/p&gt;
    &lt;p&gt;CorePlus ofers a simple way to get started using the Core philosophy with its included community packaged extensions enabling easy embedded frugal or pendrive installation of the user's choice of supported desktop, while maintaining the Core principal of mounted extensions with full package management.&lt;/p&gt;
    &lt;p&gt;It is not a complete desktop nor is all hardware completely supported. It represents only the core needed to boot into a very minimal X desktop typically with wired internet access.&lt;/p&gt;
    &lt;p&gt;The user has complete control over which applications and/or additional hardware to have supported, be it for a desktop, a netbook, an appliance, or server, selectable by the user by installing additional applications from online repositories, or easily compiling most anything you desire using tools provided.&lt;/p&gt;
    &lt;p&gt;The latest version: 16.2&lt;/p&gt;
    &lt;head rend="h3"&gt;News&lt;/head&gt;
    &lt;head rend="h3"&gt;About Our Project&lt;/head&gt;
    &lt;p&gt;Our goal is the creation of a nomadic ultra small graphical desktop operating system capable of booting from cdrom, pendrive, or frugally from a hard drive. The desktop boots extremely fast and is able to support additional applications and hardware of the users choice. While Tiny Core always resides in ram, additional applications extensions can either reside in ram, mounted from a persistent storage device, or installed into a persistent storage device.&lt;/p&gt;
    &lt;p&gt;We invite interested users and developers to explore Tiny Core. Within our forums we have an open developement model. We encourage shared knowledge. We promote community involvement and community built application extensions. Anyone can contribute to our project by packaging their favorite application or hardware support to run in Tiny Core. The Tiny Core Linux Team currently consists of eight members who peruse the forums to assist from answering questions to helping package new extensions.&lt;/p&gt;
    &lt;p&gt;Join us here and on IRC Freenode #tinycorelinux.&lt;/p&gt;
    &lt;p&gt;Learn. Share. Grow your knowledge of Linux.&lt;/p&gt;
    &lt;p&gt;Robert Shingledecker, December 01, 2008&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173547</guid><pubDate>Sat, 06 Dec 2025 14:18:42 +0000</pubDate></item><item><title>HTML as an Accessible Format for Papers (2023)</title><link>https://info.arxiv.org/about/accessible_HTML.html</link><description>&lt;doc fingerprint="e3d1ea5238d7dc4a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;HTML as an accessible format for papers&lt;/head&gt;
    &lt;p&gt;Accessibility barriers in research are not new, but they are urgent. The message we have heard from our community is that arXiv can have the most impact in the shortest time by offering HTML papers alongside the existing PDF.&lt;/p&gt;
    &lt;p&gt;arXiv has successfully launched papers in HTML format. We are gradually backfilling HTML for arXiv's corpus of over 2 million papers over time. Not every paper can be successfully converted, so a small percentage of papers will not have an HTML version. We will work to improve conversion over time.&lt;/p&gt;
    &lt;p&gt;The link to the HTML format will appear on abstract pages below the existing PDF download link. Authors will have the opportunity to preview their paperâs HTML as a part of the submission process.&lt;/p&gt;
    &lt;p&gt;The beta rollout is just the beginning. We have a long way to go to improve HTML papers and will continue to solicit feedback from authors, readers, and the entire arXiv community to improve conversions from LaTeX.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why "experimental" HTML?&lt;/head&gt;
    &lt;p&gt;Did you know that 90% of submissions to arXiv are in TeX format, mostly LaTeX? That poses a unique accessibility challenge: to accurately convert from TeXâa very extensible language used in myriad unique ways by authorsâto HTML, a language that is much more accessible to screen readers and text-to-speech software, screen magnifiers, and mobile devices. In addition to the technical challenges, the conversion must be both rapid and automated in order to maintain arXivâs core service of free and fast dissemination.&lt;/p&gt;
    &lt;p&gt;Because of these challenges we know there will be some conversion and rendering issues. We have decided to launch in beta with âexperimentalâ HTML because:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accessible papers are needed now. We have talked to the arXiv community, especially researchers with accessibility needs, and they overwhelmingly asked us not to wait.&lt;/item&gt;
      &lt;item&gt;We need your help. The obvious work is done. Reports from the community will help us identify issues we can track back to specific LaTeX packages that are not converting correctly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Error messages you may see in HTML papers&lt;/head&gt;
    &lt;p&gt;HTML papers on arXiv.org are a work in progress and will sometimes display errors. As we work to improve accessibility we share with you the causes of these errors and what authors can do to help minimize them. Learn more about error messages you may see in HTML papers&lt;/p&gt;
    &lt;head rend="h2"&gt;Ways to help&lt;/head&gt;
    &lt;head rend="h3"&gt;1) Read HTML papers and report issues&lt;/head&gt;
    &lt;p&gt;We encourage the community to try out HTML papers in your field:&lt;/p&gt;
    &lt;head rend="h4"&gt;Report an issue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the abstract page for a paper you are interested in reading.&lt;/item&gt;
      &lt;item&gt;Look in the section where you find the link to the PDF download, and click the new link for HTML.&lt;/item&gt;
      &lt;item&gt;Report issues by either a) clicking on the Open Issue button b) selecting text and clicking on the Open Issue for Selection button or c) use &lt;code&gt;Ctrl+?&lt;/code&gt;on your keyboard. If you are using a screen reader, use&lt;code&gt;Alt+y&lt;/code&gt;to toggle accessible reporting buttons per paragraph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please do not create reports that the HTML paper doesn't look exactly like the PDF paper&lt;/p&gt;
    &lt;p&gt;Our primary goal for this project is to make papers more accessible, so the focus during the beta phase will value function over form. HTML layouts that are incorrect or are illegible are important to report. But we do expect the HTML papers to present differently than the same paper rendered in PDF. Line breaks will occur in different places and there is likely to be more white space. In general, the HTML paper won't present as compactly. Intricate typographic layouts will not be rendered so intricately. This is by design.&lt;/p&gt;
    &lt;p&gt;HTML is a different medium and brings its own advantages versus PDF. In addition to being much more compatible with assistive technologies, HTML does a far better job adapting to the characteristics of the device you are reading on, including mobile devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;2) Help improve the conversion from LaTeX&lt;/head&gt;
    &lt;p&gt;If you are an author you can help us improve conversions to HTML by following our guide to LaTeX Markup Best Practices for Successful HTML Papers.&lt;/p&gt;
    &lt;p&gt;If you are a developer and have free development cycles, help us improve conversions! Our collaborators at LaTeXML maintain a list of issues and welcome feedback and developer contributions.&lt;/p&gt;
    &lt;p&gt;If you are a publisher, member of a society, or conference organizer you can help us improve conversions to HTML by reviewing the .cls files your organization recommends to authors for unsupported packages. Providing .cls files that use supported packages is an easy way to support and sow accessibility in the scientific community.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thank you to our collaborators&lt;/head&gt;
    &lt;p&gt;First, we want to share a special thank you to all the scientists with disabilities who have generously shared their insights, expertise, and guidance throughout this project.&lt;/p&gt;
    &lt;p&gt;We want to thank two organizations without which HTML papers on arXiv would not be possible: The LaTeX Project, and the LaTeXML team from NIST. We deeply thank each member of these teams for their knowledge, incredible work, and commitment to accessibility.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46173825</guid><pubDate>Sat, 06 Dec 2025 14:59:52 +0000</pubDate></item><item><title>Perl's decline was cultural</title><link>https://www.beatworm.co.uk/blog/computers/perls-decline-was-cultural-not-technical</link><description>&lt;doc fingerprint="4418d15d9ae08ffd"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt; Perl's decline was cultural 2025-11-20 &lt;/head&gt;
    &lt;head rend="h3"&gt;According to the Discourse, somebody killed perl&lt;/head&gt;
    &lt;p&gt;There's been a flurry of discussion on Hacker News and other tech forums about what killed Perl. I wrote a lot of Perl in the mid 90s and subsequently worked on some of the most trafficked sites on the web in mod_perl in the early 2000s, so I have some thoughts. My take: it was mostly baked into the culture. Perl grew amongst a reactionary community with conservative values, which prevented it from evolving into a mature general purpose language ecosystem. Everything else filled the gap.&lt;/p&gt;
    &lt;head rend="h3"&gt;I remember Perl&lt;/head&gt;
    &lt;p&gt;Something to keep in mind, is that although this is my personal take, and therefore entirely an opinion piece, I was there at the time. I stopped doing Perl properly when I left Amazon, I think this would have been around 2005. It's based on the first hand impressions of somebody who was very deeply involved in Perl in its heyday, and moved on. I have a lot of experience, from both inside and outside the tent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Perl's roots are sysadmin&lt;/head&gt;
    &lt;p&gt;What culture? Perl always had a significant amount of what you might call "BOFH" culture, which came from its old UNIX sysadmin roots. All of those passive aggressive idioms and in jokes like "RTFM", "lusers", "wizards", "asking for help the wrong way" etc. None of this is literally serious, but it does encode and inform social norms that are essentially tribal and introverted. There implicitly is a privileged population, with a cost of entry to join. Dues must be paid. Cultural conservatism as a first principle.&lt;/p&gt;
    &lt;p&gt;This stems from the old locked-down data centre command culture. When computer resource was expensive, centralised, fragile, and manually operated, it was rigidly maintained by gatekeepers, defending against inappropriate use. I started my career as an apprentice programmer at the very end of this era, (late 80s) pre-web, and before microcomputers had made much inroads, and this really was the prevailing view from inside the fort. (This is a drawback about fort-building. Once you live in a fort, it's slightly too easy to develop a siege mentality). Computers are special, users are inconvenient, disruption is the main enemy.&lt;/p&gt;
    &lt;p&gt;An unfortunate feedback loop in this kind of "perilous" environment is that it easily turns prideful. It's difficult to thrive here, if you survive and do well you are skilled; you've performed feats; you should mark your rites of passage. This can become a dangerous culture trap. If you're not careful about it, you may start to think of the hazards and difficulties, the "foot guns", as necessary features - they teach you those essential survival skills that mark you out. More unkindly, they keep the stupid folk out, and help preserve the high status of those who survived long enough to be assimilated. Uh-oh, now you've invented class politics.&lt;/p&gt;
    &lt;p&gt;The problem with this thinking is that it's self-reinforcing. Working hard to master system complexities was genuinely rewarding - you really were doing difficult things and doing them well. This is actually the same mechanism behind what eventually became known as 'meritocracy'1, but the core point is simpler - if difficulty itself becomes a badge of honour, you've created a trap: anything that makes the system more approachable starts to feel like it's cheapening what you achieved. You become invested in preserving the barriers you overcame.&lt;/p&gt;
    &lt;p&gt;(This is the same mentality that built leetcode interview pipelines BTW, but let's leave that sidebar alone for now)&lt;/p&gt;
    &lt;p&gt;So the UNIX operator culture tended to operate as a tribal meritocracy (as opposed to the UNIX implementer culture, which fell out of a different set of cultural norms, quite an interesting side bar itself2), a cultural priesthood, somewhat self-regarding, rewarding of cleverness and knowledge hoarding, prone to feats of bravado, full of lore, with a defensive mentality of keeping the flame aloft, keeping the plebs happy and fed, and warding off the barbarians. As we entered the 90s it was already gently in decline, because centralised computing was giving way to the rise of the microcomputer, but the sudden explosive growth of the WWW pulled internet / Unix culture suddenly back into the mainstream with an enormous and public opportunity vacuum. Everyone suddenly has an urgent need to write programs that push text off UNIX file-systems (and databases) and into web pages, and Perl is uniquely positioned to have a strong first-mover advantage in this suddenly vital, novel ecosystem. But it's culture and values are very much pulled across from this previous era.&lt;/p&gt;
    &lt;p&gt;(Springing out of this, Perl had an, at best grudging, tolerance for 'difficult genius' types, alongside this baseline culture. Unfortunately, this kind of toxic personality tends to thrive in the type of culture I've described, and they do set to help the tone. I'm not here to call out people specifically, because I'm trying to make a point rather than feed a culture war, or dig up gossip, but there were several significant examples, you can probably find lore if you like. I think the kindest way I can describe the compounding effect of this is that there was a strong cultural norm along the lines of "It's OK to be rude, as long as it's for a good cause".)&lt;/p&gt;
    &lt;head rend="h3"&gt;A fort within a fort&lt;/head&gt;
    &lt;p&gt;I remember this tension as always being tangibly there. Perl IRC and mailing lists were quite cliquey and full of venerated experts and in-jokes, rough on naivety, keen on robust, verbose debate, and a little suspicious of newcomers. And very cult-like. The "TIMTOWTDI" rule, although ostensibly liberal, literally means 'there is more than one way to do it in Perl' - and you can perhaps infer from that that there's little to no reason to do it using anything else. Elevating extreme flexibility like this is paradoxically also an engine of conservatism. If Perl can already do anything, flexibly, in multiple ways, then the language itself doesn't need to change - 'we already have one of those here, we don't need new things'. This attitude determined how Perl intended to handle evolution: the core language would remain stable (a fort inside a fort, only accessible to high level wizards), while innovation was pushed outward to CPAN. You could add features outside of core by writing and consuming third party libraries, you could bend language behaviour with pragmas without modifying Perl itself. The very best CPAN modules could theoretically be promoted into core, allowing the language to evolve conservatively from proven, widely-used features.&lt;/p&gt;
    &lt;p&gt;On paper, this sounds reasonable. In practice, I think it encoded a fundamental conflict of interest into the community early on, and set the stage for many of the later growth problems. I'm not going to pretend that Perl invented dependency hell, but I think it turned out to be another one of those profound misfeatures that their cultural philosophy lead them to mistake for virtue, and embrace.&lt;/p&gt;
    &lt;p&gt;An interesting thing I think has been missed discussing the context of the original blog piece, about whether Perl 6 significantly impacted Perl growth, is the fact that Perl 6 itself manifested out of ongoing arguments. Perl 6 is a schism. Here's a oft-cited note from Larry Wall himself about the incident that sparked Perl 6, at &lt;del&gt; YAPC&lt;/del&gt; OSCON 2000 &lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We spent the first hour gabbing about all sorts of political and organizational issues of a fairly boring and mundane nature. Partway through, Jon Orwant comes in, and stands there for a few minutes listening, and then he very calmly walks over to the coffee service table in the corner, and there were about 20 of us in the room, and he picks up a coffee mug and throws it against the other wall and he keeps throwing coffee mugs against the other wall, and he says "we are f-ed unless we can come up with something that will excite the community, because everyone's getting bored and going off and doing other things".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(Pause a second and ask yourself about the sort of social culture that both allows this kind of behaviour at public events, and then chooses to embrace it as a key piece of cultural lore)&lt;/p&gt;
    &lt;head rend="h3"&gt;The impact of Perl 6&lt;/head&gt;
    &lt;p&gt;Perl 6 was really a schism. Perl was already under a great amount of strain trying to accommodate the modernising influx of post dot-com mainstream web application building, alongside the entrenched conservatism of the core maintainers, and the maintenance burden of a few years exponential growth of third-party libraries, starting to build a fractal mess of slightly differentiating, incompatible approaches of those multiple ways to do things that were effectively now table-stakes language features, as the deployment landscape started to tiptoe towards a more modern, ubiquitous WWW3.&lt;/p&gt;
    &lt;p&gt;So, while I agree that it's wrong to generalise that 'Perl 6 killed Perl', I would say that Perl 6 was a symptom of the irreconcilable internal forces that killed Perl. Although, I also intend to go on to point out that Perl isn't dead, nothing has actually killed Perl. Killed Perl is a very stupid way to frame the discussion, but here we are.&lt;/p&gt;
    &lt;p&gt;So... Perl 6 is created as a valve to offset that pressure, and it kind of works. Up to a point. Unfortunately I think the side effect really is that the two branches of the culture, in the process of forking, double down on their encoded norms. Perl 5.x beds down as the practical, already solved way to do all the same things, with little need to change. Any requirements for more modern application patterns that are emerging in the broader web development environment, like idk, Unicode, REST clients, strict data structures, asynchronous I/O, whatever? That can either wait for Perl6 or you can pull things together using the CPAN if you want to move right now. Perl 6 leans the other way - they don't need to ship immediately, we have Perl 5 already here for doing things, Perl 6 is going to innovate on everything, and spend it's time getting there, designing up-front.4 They spend at least two years writing high level requirement specs. They even spin out a side-project trying to build a universal virtual machine to run all dynamic programming languages that never delivers5&lt;/p&gt;
    &lt;p&gt;This is the landscape where Perl's central dominance of 'back end' web programming continues to slip. Unfortunately, alongside the now principled bias toward cultural conservatism, Perl 5 has an explicit excuse for it. The future is over there, and exciting, and meanwhile we're working usefully, and getting paid, and getting stuff done. Kind of OK from inside the fort. Some day we'll move to the newer fort, but right now this is fine. Not very attractive to newcomers though, really. And this is also sort of OK, because Perl doesn't really want those sort of newcomers, does it? The kind that turns up on IRC or forums and asks basic questions about Perl 6 and sadly often gets treated with open contempt.&lt;/p&gt;
    &lt;head rend="h3"&gt;Meanwhile, over there&lt;/head&gt;
    &lt;p&gt;Ruby has sprouted "Ruby on Rails", and it's taken the dynamic web building world by storm. Rails is a second generation web framework, that's proudly an 'opinionated web framework'. Given that the web application architecture is starting to stabilise into a kind of three-tier system , with a client as a web browser, a middle tier as a monolithic application server, and a persistence layer as a relational database , and a split server architecture serving static and dynamic content from different routes, here is just one way to do that, with hugely developer friendly tooling turning this into a cookie-cutter solution for the 80% core, and a plugin and client-side decoration approach that allows for the necessary per-site customisation.&lt;/p&gt;
    &lt;p&gt;Ruby is interesting as well. Ruby is kind of a Perl6 really. More accurately it's a parallel universe Perl5 Ruby comes from Japan, and has developed as an attempt to build something similar to Perl, but it's developed much later, by programming language enthusiasts, and for the first ten years or so, it's mostly only used in Japan. To my line of thinking this is probably important. Ruby does not spring from decades of sysadmin or sysop culture. Ruby is a language for programmers, and is at this point an sensible candidate for building something like Rails with - a relatively blank canvas for dynamic programming, with many of the same qualities as Perl, with less legacy cruft, and more modern niceties, like an integrated object system, exceptions, straightforward data structures. Ruby also has adopted 'friendliness' as a core value, and the culture over there adopts a principled approach to aggressively welcoming newcomers, promoting easiness, and programmer happiness and convenience as strong first class principles.&lt;/p&gt;
    &lt;p&gt;Rails is a huge hit. At this point, which is around about the time I stopped significantly using Perl (2004-2005) (because I quit my job, not out of any core animosity toward it, in fact, in my day, I was really quite a Perl fan), Rails is the most appealing place to start as a new web programmer. Adoption rate is high, community is great, velocity of development is well-paced, and there's a lovely , well-lit, onboarding pipeline for how to start. You don't even really need to know ruby. It has a one-shot install tool, and generates working websites from templates, almost out of the box. It's an obvious starting point.&lt;/p&gt;
    &lt;p&gt;Perl being Perl, develops several analogue frameworks to Rails, all of them interdependently compatible and incompatible with each other and each other's dependencies, all of them designed to be as customisable and as user configurable as they possibly can be6&lt;/p&gt;
    &lt;head rend="h3"&gt;PHP&lt;/head&gt;
    &lt;p&gt;There are also the other obvious contenders. PHP has been there all along, and it's almost coming up from entirely the opposite cultural background of Perl. PHP is a users language. It's built to be deployed by copying script files to your home directory, with minimal server side impact or privileges. It's barely designed at all, but it encounters explosive growth all the way through the first (and through into the second) web era, almost entirely because it makes the barrier to onboarding so low as to be non-existent. PHP gets a couple of extra free shots in the arm&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Because it's architecture is so amenable to shared-server hosting, it is adopted as the primary implementation language of the blogging boom. An entire generation of web developers is born of installing and customising WordPress and text-pattern et. al by installing it directly into your home directory on a rented CPanel host account. It's the go-to answer for 'I'm not a programmer really but how do I get a personal web site'7 This zero gate-keeping approach keeps the PHP stack firmly on the table of 'basic' web programmers all through the history of the web up to the current day.&lt;/item&gt;
      &lt;item&gt;Because of these initially lightweight deployment targets, PHP scales like little else, mostly because it's execution model leans strongly towards idempotent execution, with each web request tearing up and tearing down the whole environment. In a sense, this is slower than keeping hot state around, but it does lend itself extremely well to shared-nothing horizontal scaling, which as the web user base increases gigantically throughout the 2000s era, is the simplest route to scaling out. Facebook famously, is built in PHP at this point in time.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Python&lt;/head&gt;
    &lt;p&gt;There is of course one other big horse in the race in this era, and it's a particularly interesting one in many ways, certainly when contrasted with Perl. This is of course, Python. Python is a close contemporary of Perl's but once again, it's roots are somewhere very different. Python doesn't come from UNIX culture either. Python comes from academia, and programming language culture. It's kind of a forgotten footnote, but Python was originally built for the Amoeba operating system, and it's intention was to be a straightforward programming language for scripting this8. The idea was to build a language that could be the 'second programming language' for programmers. Given that this is the 1980s, early 1990s, the programmers would be expected to be mostly using C / C++ ,perhaps Pascal. Python was intended to allow faster development for lighter weight programs or scripting tasks. I suppose the idea was to take something that you might want to build in a shell script, but provide enough high level structured support that you could cleanly build the kind of things that quickly become a problem in shell scripts. So, it emphasises data structures, and scoped variables, and modules, and prioritises making it possible to extend the language with modules. Typical things that experienced programmers would want to use. The language was also designed to be portable between the different platforms programmers would use, running on the desktops of the day, but also on the server. As a consequence, it had a broad standard library of common portable abstractions around standard system features - file-systems, concurrency, time, FFI. For quite a long time, one of python's standard mottoes was 'batteries included'.&lt;/p&gt;
    &lt;p&gt;Python never set the world on fire at any particular moment, but it remained committed to a clear evolutionary incremental development, and clean engineering principles. Again, I think the key element here is cultural tone. Python is kind of boring, not trying to be anyone's best language, or even a universal language. Python was always a little fussy, maybe snobby, slightly abstracted away from the real world. It's almost as old as Perl and it just kept incrementally evolving, picking up users, picking up features, slowly broadening the standard library. The first time I saw Python pick up an undeniable mainstream advantage would also have been around the early 2000s, when Google publicly adopted it as one of their house standard languages. Never radical, just calmly evolving in it's environs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nature abhors a vacuum&lt;/head&gt;
    &lt;p&gt;When I sketch out this landscape, I remain firmly convinced that most of Perl's impedance to continued growth were cultural. Perl's huge moment of relevance in the 90s was because it cross-pollinated two diverging user cultures. Traditional UNIX / database / data-centre maintenance and admin users, and enthusiastic early web builders and scalers. It had a cultural shock phase from extremely rapid growth, the centre couldn't hold, and things slowly fell apart.&lt;/p&gt;
    &lt;p&gt;Circling back though, it's time to address the real elephant in the room. Perl manifestly did not die. It's here right now. It's installed I think by default, on almost every single computer I own and operate, without me doing a single thing to make that happen. It's still used every day by millions of people on millions of systems (even if that isn't deliberate). It's still used by many people entirely deliberately for building software, whether that's because they know it and like it and it works, or because they're interfacing with or working on legacy Perl systems (of which there are still many), or maybe they're using it still in it's original intentional role - A capable POSIX-native scripting language, with much better performance and a broader feature-set than any shell or awk. I still occasionally break it out myself, for small scripts I would like to use more than once, or as parts of CLI pipelines.&lt;/p&gt;
    &lt;p&gt;What I don't do any more is reach for Perl first to make anything new. In my case, it's just because I typically am spoilt for options that are a better fit for most tasks, depending on whatever it is I'm trying to achieve. By the time I came to Perl, (1998-ish), I was already on my third career phase, I had a strong UNIX background, and had already built real things in lisp, java, pascal, visual basic and C++. My attitude to languages was already informed by picking a tool to fit the task at hand. Boy did I love Perl for a few years. The product/market-fit for those early web days was just beautiful. The culture did have too much of the negative tropes I've been pointing at, but that wasn't really a problem personally for me, I'd grown up amongst the BOFHs inside the data centres already, it wasn't too hard for me to assimilate, nor pick up the core principles. I did occasionally bounce off a couple of abrasive characters in the community, but mostly this just kept me loosely coupled, I enjoyed how the language solved the problems I needed solving quickly, I enjoyed the flexibility, and I also enjoyed the way that it made me feel smart, and en-route to my wizard's robes and hat, when i used it to solve harder problems in creative ways, or designed ways around bugs and gremlins. For a good 3-4 years I would have immediately picked it as my favourite language.&lt;/p&gt;
    &lt;p&gt;So as I say, I didn't fall out of it with any sense of pique, I just naturally moved to different domains, and picked up tools that best fit. After Amazon, I spent t a lot of time concentrating on OS X and audio programming, and that involved a lot of objective C, C++. The scripting tools in that domain were often in ruby, sometimes python. For personal hacking, I picked up lisp again9 (which I'd always enjoyed in school). I dipped in and out of Perl here and there for occasional contract work, but I tended to gravitate more towards larger database stuff, where I typically found C, java and python. The next time I was building web things, it was all Rails and ruby, and then moving towards the web services / REST / cloud era, the natural fits were go, and of course node and JavaScript or Typescript. I've always been a polyglot, and I've always been pretty comfortable moving between programming languages. The truth of the matter is, that the majority of programming work is broadly similar, and the specific implementation details of the language you use don't matter all that much, if it's a good fit for the circumstances.&lt;/p&gt;
    &lt;p&gt;I can't imagine Perl disappearing entirely in my lifetime. I can remember entire programming environments and languages that are much, much deader than I can ever see Perl becoming.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pascal used to be huge for teaching and also for desktop development in the 8/16 bit era&lt;/item&gt;
      &lt;item&gt;Objective C - only really useful inside the Apple ecosystem, and they're hell bent on phasing it out.&lt;/item&gt;
      &lt;item&gt;Before I got into the Internet, I used to build application software for 16 bit Windows (3.11) which was a vast market, in a mixture of database 4GLs (like PowerBuilder, Gupta/Centura SQLWindows) and Win16 C APIs. This entire universe basically no longer exists, and is fully obsolete. There must be many similar cases.&lt;/item&gt;
      &lt;item&gt;I mean who the hell realistically uses common lisp any more outside of legacy or enthusiast markets? Less people than Perl I'm sure.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perl also got to be if not first, then certainly early to dominate a new market paradigm. Plenty of things never manage that. It's hard to see Perl as anything other than an enormous success on these terms. Perl innovated and influenced languages that came after in some truly significant ways.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tightly embedding regular expressions and extending regular expressions (the most commonly used regular expression dialect in other tools is Perl)&lt;/item&gt;
      &lt;item&gt;CPAN, for package/library distribution via the internet, with dependency resolution - and including important concepts like supply chain verification with strong package signatures&lt;/item&gt;
      &lt;item&gt;A huge emphasis on testing, automated test harnesses, and CI. Perl test format (TAP) is also widely found in other CI/harness systems&lt;/item&gt;
      &lt;item&gt;Blending the gap between shell / scripting / and system programming in a single tool. I suppose this is debatable, but the way Perl basically integrated all the fundamental POSIX/libc as native built-ins with broadly the same semantics, but with managed memory and shell conventions was really revolutionary. Before this, most languages I had ever seen broadly tended to sit in one box, afterwards, most languages tended to span across several.&lt;/item&gt;
      &lt;item&gt;Amazing integrated documentation, online, in-tool and also man pages. POD is maybe the most successful ever implementation of literate programming ideas (although most of the real docs don't intertwingle the documentation very much iirc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just these points, and I'm sure there are many others that could be made, are enough of a legacy to be proud of.&lt;/p&gt;
    &lt;p&gt;Counterfactuals are stupid (but also fun). If I squint, I can imagine that a Perl with a less reactionary culture, and a healthier acceptance of other ideas and environmental change might have been able to evolve alongside the other tools in the web paradigm shift, and still occupy a more central position in today's development landscape. That's not the Perl we have though, and that didn't happen. And I'm very confident that without the Perl we did have, the whole of modern software practice would be differently shaped. I do think Perl now lives in a legacy role, with a declining influence, but that's really nothing to feel shame or regret for. Nobody is going to forcibly take Perl away as long as POSIX exists, and so far as I can see, that means forever. In 2025 too, I can see the invisible hand creeping up on some of these other systems I've mentioned. Rust is slowly absorbing C and C++. Ruby (and of course Rails) is clearly in decline, in a way that probably consigns it to become a similar legacy state. From a certain angle, it looks a lot like Typescript is slowly supplanting Python. I won't be entirely surprised if that happens, although at my age I kind of doubt I'll live to see the day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1 : Meritocracy is a fun word. It was originally coined as a pejorative term to describe a dystopian mechanism by which modern i.e. Western / British society entrenches and justifies an unfair and unequal distribution of privilege&lt;/p&gt;
    &lt;p&gt;2 : The UNIX implementer culture, is scientific/academic and fell out of Bell Labs. I guess you could extend this school of thought as a cultural sweep towards building abstracted cloud operations, toward plan 9/ Inferno / go&lt;/p&gt;
    &lt;p&gt;3 : Web 2.0 was first defined in 1999 by Darcy DiNucci in a print article , the term didn't become mainstream until it was picked up and promoted by Tim O'Reilly (then owner/operator of perl.com, trivia fans), an astute inside observer of the forces driving web development&lt;/p&gt;
    &lt;p&gt;4: Another unfortunate bit of luck here. Right at the point of time that 'agile' starts getting some traction as a more natural way to embrace software development - i.e. iterating in small increments against a changing environment and requirements, Perl 6 decides to do perhaps the most waterfall open source development process ever attempted. . It is fifteen years before Perl 6 ships something resembling a usable programming language.&lt;/p&gt;
    &lt;p&gt;5 : The Parrot VM, a lovely quixotic idea, which sadly fizzled out, after even Perl 6 stopped trying to target it. Interestingly enough, both python and ruby both made relatively high profile ports to the JVM that were useful enough to be used for production deploys in certain niches.&lt;/p&gt;
    &lt;p&gt;6 : A side effect of this degree of abstraction, is that as well as being very hard to get started, it's easy to fall foul of performance overhead.&lt;/p&gt;
    &lt;p&gt;7 : This ubituitious ecosystem of small footprint wordpress custom installs gives birth to the web agency model of commercial website building / small ecommerce sites, which thrives and is suprisingly healthy today. Recent, and slighly optimistic surveys have pitched WordPress as powering over 40% of all websites today. Now this is certainly inflated, but even if the realistic number is half of that, that's still pretty damn healthy.&lt;/p&gt;
    &lt;p&gt;8 : It's often repeated that Python was designed as a teaching language, but as far as I know, that's not actually the case. The designer of Python, Guido Van Rossum was previously working on a project that was a intended as training language, called ABC, and many of ABC's syntax and structural features influenced or made their way into Python.&lt;/p&gt;
    &lt;p&gt;9 : Common lisp is a better answer to an infinitely flexible 'everything' chainsaw language than perl, IMHO&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46175112</guid><pubDate>Sat, 06 Dec 2025 17:42:07 +0000</pubDate></item><item><title>OMSCS Open Courseware</title><link>https://sites.gatech.edu/omscsopencourseware/</link><description>&lt;doc fingerprint="e76af2125443a7a2"&gt;
  &lt;main&gt;
    &lt;p&gt;Georgia Tech’s Online Master of Science in Computer Science (OMSCS) program is proud to make the course content* for many of its courses publicly available through Ed Lessons. Select a course below to view the public content for that course.&lt;/p&gt;
    &lt;p&gt;Note that students enrolled in OMSCS should access their course content through Canvas, as the for-credit versions of these courses may include graded components or recent content updates not available through OMSCS Open Courseware.&lt;/p&gt;
    &lt;p&gt;*Course content typically includes things such as lecture videos and exercises; it will not include things like homeworks, projects quizzes, exams, or other graded assignments.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46175826</guid><pubDate>Sat, 06 Dec 2025 19:14:35 +0000</pubDate></item><item><title>Zebra-Llama – Towards efficient hybrid models</title><link>https://arxiv.org/abs/2505.17272</link><description>&lt;doc fingerprint="c1d340b445bd08b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 22 May 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Zebra-Llama: Towards Extremely Efficient Hybrid Models&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and &amp;gt;97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Mehdi Rezagholizadeh [view email]&lt;p&gt;[v1] Thu, 22 May 2025 20:39:57 UTC (12,646 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176289</guid><pubDate>Sat, 06 Dec 2025 20:15:54 +0000</pubDate></item><item><title>Coffee linked to slower biological ageing among those with severe mental illness</title><link>https://www.kcl.ac.uk/news/coffee-linked-to-slower-biological-ageing-among-those-with-severe-mental-illness-up-to-a-limit</link><description>&lt;doc fingerprint="776a119b602981bc"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;We know that coffee can help slow biological ageing in the general population, but little is known about its effect on people with severe mental illness – a population whose lifespan is already shortened, in part due to age-related diseases. Our study shows that up to four cups of coffee per day is linked to longer telomeres among people with bipolar disorder and schizophrenia. This is comparable to a biological age of five years younger than non-coffee drinkers.&lt;/p&gt;Vid Mlakar, PhD student at King’s College London and first author of the study&lt;/quote&gt;
    &lt;p&gt;26 November 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Coffee linked to slower biological ageing among those with severe mental illness – up to a limit&lt;/head&gt;
    &lt;p&gt;New research from King’s College London finds that coffee consumption within the NHS recommended limit is linked to longer telomere lengths – a marker of biological ageing – among people with bipolar disorder and schizophrenia. The effect is comparable to roughly five years younger biological age.&lt;/p&gt;
    &lt;p&gt;Telomeres are structures that protect DNA. As people get older, their telomeres shorten as part of the natural human ageing process. This process has been shown to be accelerated among people with severe mental illness, such as bipolar disorder and schizophrenia, who have an average life expectancy 15 years shorter than the general population.&lt;/p&gt;
    &lt;p&gt;Previous research shows that coffee possesses health benefits. It may reduce oxidative stress in the general population, helping slow biological ageing processes like telomere shortening. The new study, published in BMJ Mental Health, explores whether coffee consumption could slow this ageing process among those with severe mental illness.&lt;/p&gt;
    &lt;p&gt;Researchers at the Institute of Psychiatry, Psychology &amp;amp; Neuroscience measured the effects of coffee consumption on telomere length among 436 participants aged 18 to 65 with schizophrenia, bipolar disorder or major depressive disorder with psychosis.&lt;/p&gt;
    &lt;p&gt;They found that coffee consumption of up to four cups per day was linked to longer telomeres, comparable to a biological age five years younger than non-coffee drinkers.&lt;/p&gt;
    &lt;p&gt;The longest telomeres were seen among those who consumed three to four cups per day. Too much coffee reduced this positive effect, with participants who consumed more than four cups having shorter telomeres than those who consumed between three and four cups.&lt;/p&gt;
    &lt;p&gt;These effects remained after accounting for variations in age, sex, ethnicity, medication and tobacco use.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Coffee is a beverage that many people consume daily. On one hand, we know that excessive coffee consumption can have negative effects on health, such as reducing sleep quality. However, our new study suggests that coffee consumption up to a certain point may have benefits for biological ageing. Many of the factors that are known to affect biological ageing, such as genetics and negative stressful life experiences, are beyond our control. Lifestyle factors like coffee consumption are something we can actively modify, making research like this particularly valuable.&lt;/p&gt;Dr Monica Aas, MRC Research Fellow at King’s College London and senior author of the study&lt;/quote&gt;
    &lt;p&gt;Dr Aas added: "Studies such as this also support the idea that we should move away from viewing coffee as simply “good or bad”, and instead consider a more balanced view. Still, these results need to be confirmed in other independent studies and longitudinal research before we can determine if this is a causal effect."&lt;/p&gt;
    &lt;p&gt;Data were from the Norwegian TOP study, collected between 2007 and 2018. The researchers included participants who had available data on mental health diagnosis (assessed using the Structured Clinical Interview for DSM-IV), telomere length (measured by extracting DNA from blood samples) and self-reported coffee consumption.&lt;/p&gt;
    &lt;p&gt;The researchers note that the study did not have information on the type of coffee consumed (instant versus filter) or the caffeine concentration of each cup. The NHS advises limiting caffeine intake to 400 mg/day (approximately four cups of coffee).&lt;/p&gt;
    &lt;p&gt;The study was funded by the Research Council of Norway, the KG Jebsen Stiftelsen and an Medical Research Council Fellowship. The team has recently received funding from the British Medical Association’s Margaret Temple grant to investigate telomere shortening in a longitudinal cohort of patients with psychosis. This project will allow them to explore further how several lifestyle factors, as well as stress, influence the rate of telomere shortening over time.&lt;/p&gt;
    &lt;p&gt;"Coffee intake is associated with telomere length in severe mental disorders" (Vid Mlakar et al.) was published in BMJ Mental Health. DOI: 10.1136/bmjment-2025-301700&lt;/p&gt;
    &lt;p&gt;For more information, please contact Milly Remmington (School of Mental Health &amp;amp; Psychological Sciences Communications Manager).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176766</guid><pubDate>Sat, 06 Dec 2025 21:33:03 +0000</pubDate></item><item><title>The past was not that cute</title><link>https://juliawise.net/the-past-was-not-that-cute/</link><description>&lt;doc fingerprint="26e613518ba77ebb"&gt;
  &lt;main&gt;
    &lt;p&gt;I was excited when cottagecore became a thing. Maybe my interest in retro clothes and handicrafts would be less embarrassing now!&lt;/p&gt;
    &lt;p&gt;I still enjoy it. But in spaces focused on old-fashioned vibes, you encounter a lot of people who believe that the past was actually this charming.&lt;/p&gt;
    &lt;p&gt;Laura Ingalls Wilder‘s Little House on the Prairie books are problematic, and also I will always love them. She wrote about the beauty of family and hard work, but she wrote them because she spent her whole life supporting disabled family members. She and her daughter beautified her “pioneer girl” history to make good books. Her daughter describes the reality: “It took seven successive years of complete crop failure, with work, weather and sickness that wrecked [my father’s] health permanently, and interest rates of 36 percent on money borrowed to buy food, to dislodge us from that land.”&lt;/p&gt;
    &lt;p&gt;My own version of this mistake was thinking that people’s personalities were different in the past. I grew up listening to folk music and imagining a past where nice boys would admire a nice quiet girl like me, and I wouldn’t have to figure out dating because everything would just unfold, probably on a May morning. My mother pointed out that a lot of the songs along the lines of “my own true love proved false to me” were about unplanned pregnancies.&lt;/p&gt;
    &lt;p&gt;I also assumed the bonny lasses in these songs would be wholesome and nice. But were popular girls of the past nicer people than they are now?&lt;/p&gt;
    &lt;p&gt;Some of my picture came from growing up in the Anglo-American folk dance and music community: it had a lot of aging hippies with graduate degrees. So I came away imagining a past with a lot of the kind of people who become engineers and English teachers. A more accurate picture would have been “Imagine a small town where the same 19 kids form your entire group of peers and potential partners.”&lt;/p&gt;
    &lt;p&gt;Bookish girls like Belle didn’t really go to live in enchanted castles with huge libraries. They stayed in villages where everyone thought they were weird and their best option was Gaston.&lt;/p&gt;
    &lt;p&gt;Maybe my favorite podcast episode ever is Rachel Laudan on food history: “I did have the extraordinary good fortune to grow up eating what I think the romantic movement dreams of. We had milk fresh from the cow; I never had pasteurized milk until I went to school. We had fish from the river, pheasant from the farm. The food was extremely good. . . . everything was fresh from the garden. So, I do romanticize—some of that because the taste was often extraordinary. And then I tweak myself and I say, ‘Look, Rachel, your mother spent all day, every day gardening or cooking.’ Essentially. As well as doing other chores. And she said to you, ‘Rachel, it’s servitude. I want you to have a life I didn’t have.’ “&lt;/p&gt;
    &lt;p&gt;I love living in a time and place where we get to choose aesthetics. I have bread rising in my kitchen right now, and I’m looking forward to baking it in an electric oven that doesn’t require me stacking wood or putting smoke into my house.&lt;/p&gt;
    &lt;p&gt;So I’ll continue to enjoy retro vibes, and draw on the past for lessons on how to be a human. (For example, making music together is one of life’s great experiences, and it’s a mistake to entirely substitute recorded music for that.) But I’ll enjoy doing so with indoor plumbing, dental care, and a desk job.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176893</guid><pubDate>Sat, 06 Dec 2025 21:53:35 +0000</pubDate></item><item><title>Screenshots from developers: 2002 vs. 2015 (2015)</title><link>https://anders.unix.se/2015/12/10/screenshots-from-developers--2002-vs.-2015/</link><description>&lt;doc fingerprint="5e3b5b5dcbbf3eec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Screenshots from developers: 2002 vs. 2015&lt;/head&gt;
    &lt;p&gt;In 2002 I asked a number of developers/Unix people for screenshots of their desktops. I recently republished them, and, seeing the interest this generated, I thought it’d be fun to ask the same people* again 13 years later. To my delight I managed to reach many of them.&lt;/p&gt;
    &lt;p&gt;* Sans Dennis Ritchie and itojun, who are no longer with us.&lt;/p&gt;
    &lt;p&gt;So, without further ado:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;my desktop is pretty boring, since it consists of xterm windows to whatever unix system i am using at the moment. the machine itself is likely to be running some x-window server like exceed on some flavor of windows, though for many years i just used an x terminal.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;If you thought it was boring last time, check this out!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2002:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I don’t know how to make a screenshot, because I normally use my computer in text-mode. I have X and GNOME installed, but I use them only occasionally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;2015:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Under X, I use the standard environment of Trisquel, but mostly I type at Emacs in a console.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, my desktop is quite boring. I mostly work with four xterms and a few Netscape windows. The KDE bar hides automatically, you can only see a thin grey line at the bottom.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Here is the new one. You'll see that, like before, I have lots of xterms where I work on Vim, Zimbu and email. Now using the Chrome browser, showing off the Zimbu homepage. But clearly everything has become bigger!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Linux (2.4.20-pre5), Gnome2, vim, Pine.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Not that much has changed in 13 years. Still using Linux. Still just a browser window and a ton of terminals hiding behind them. The main change is that switched from Pine to Thunderbird for email at some point. The OS on my laptop here is Ubuntu with Unity although there are a lot of Debian packages installed so it is a bit of a hybrid at this point. Oh, and yes, my son Carl is a lot older now.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Ah, my desktop is pretty boring, I used fvwm 1.24 as my window manager and I try to have no more than 1 or 2 windows open per virtual desktop. I use FreeBSD 4-STABLE as my operating system. I first came across Unix when I got an account on a Pyramid 90x running OSx. This had a dual-universe setup: both AT&amp;amp;T and BSD-style environments, chosen by an environment variable. Initially I was given the AT&amp;amp;T environment, but my friends convinced me to ``come over” to BSD. Since then I’ve been a BSD afficionado.&lt;/p&gt;
      &lt;p&gt;After OSx, SunOS 3.5 and later SunOS releases, until 386BSD 0.1 came out and I started to run BSD at home. Then when 386BSD transmogrified to FreeBSD, I went with FreeBSD.&lt;/p&gt;
      &lt;p&gt;In terms of desktop, I’m a command-line guy, always will be. My favourite editor is vi, my favourite shell is tcsh (but kudos to rc for elegance). So I don’t really feel the need for GUI things like Gnome or KDE :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;How things have (and have not changed). I'm still a command-line junkie with at least two xterm windows open. I'm still using a 3x3 virtual desktop. However, instead of fvwm, it is now LXDE. I've also switched from FreeBSD to Linux and I'm running Lubuntu as my distribution.&lt;/p&gt;
      &lt;p&gt;There are a lot of indispensable GUI tools that I use. These include Firefox, lyx, Gimp, KeepassX, Shutter, viking, dia, Wireshark, calibre, audacity, Handbrake and VLC. But where possible I still prefer to script things. My main development languages are still shell, Perl and C.&lt;/p&gt;
      &lt;p&gt;My shell is now bash. The vi keystrokes are burned into my fingertips and, as long as vim can be ported to new systems, that will be my text editor until I pass on. My mail client is now mutt (definitely not a web client) and my mail is stored locally, not on someone else's server.&lt;/p&gt;
      &lt;p&gt;The only issue I have is that, since a job change, I now have to deal with Windoze things. Thus, I have VirtualBox, libreoffice and Wine to help me do that.&lt;/p&gt;
      &lt;p&gt;I started with Unix on a Pyramid 90x. I now have a smart phone that blows the 90x out of the water on performance, RAM and storage. But I'm so very happy that, somewhere down underneath, there is still a Bourne shell and an operating system that does open(), close(), read(), write(), fork() and exec()!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;You’ll probably be sad (or perhaps not) to hear that my desktop hasn’t really changed much at all - still OS X, though because OS X has virtual desktops now I have multiple “desktops” (6 of them) where Mail.app runs on one, Safari on another, Calendar, Slack, etc - all on separate desktops. This makes it a bit boring, but here’s the one I probably spend the most time in - the terminal window desktop. :)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;There we go. Actually, that’s a condensate in one workspace cause I usually use about 4. Some of my favourite apps:&lt;/p&gt;
      &lt;item&gt;http://anjuta.sf.net/ (IDE)&lt;/item&gt;
      &lt;item&gt;http://quirc.org/ (IRC)&lt;/item&gt;
      &lt;item&gt;http://gaim.sf.net/ (IM)&lt;/item&gt;
      &lt;item&gt;http://multignometerm.sf.net/ (Term)&lt;/item&gt;
      &lt;p&gt;not on the shot, but worth noted&lt;/p&gt;
      &lt;item&gt;http://sylpheed.good-day.net/ (Email Client)&lt;/item&gt;
      &lt;p&gt;and of course a shot of RTCW&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;'screenshot as code', I maintain my desktop configuration through saltstack: https://github.com/TTimo/linux-salted/commits/master&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Discussion: Hacker News; reddit: /r/programming, /r/linux&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176905</guid><pubDate>Sat, 06 Dec 2025 21:55:09 +0000</pubDate></item><item><title>United States Antarctic Program Field Manual (2024) [pdf]</title><link>https://www.usap.gov/usapgov/travelAndDeployment/documents/Continental-Field-Manual-2024.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177132</guid><pubDate>Sat, 06 Dec 2025 22:26:26 +0000</pubDate></item><item><title>Saving Japan's exceptionally rare 'snow monsters'</title><link>https://www.bbc.com/future/article/20251203-japans-disappearing-snow-monsters</link><description>&lt;doc fingerprint="bf6c94c72441867b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'Nothing else looks like them': Saving Japan's exceptionally rare 'snow monsters'&lt;/head&gt;
    &lt;p&gt;A unique natural wonder is being eroded. Can Japan bring its breathtaking "juhyo" back from the brink?&lt;/p&gt;
    &lt;p&gt;Each winter, the upper slopes of Mount Zao in northern Japan – one of the country's best-known ski areas – are transformed. Fir trees coated in thick frost and snow swell into ghostly figures known as "juhyo" or "snow monsters".&lt;/p&gt;
    &lt;p&gt;Juhyo form only under exceptionally rare atmospheric conditions, emerging when strong, persistent winter winds carry supercooled water droplets that freeze on contact with the local evergreen Aomori todomatsu trees, gradually layering into rime ice.&lt;/p&gt;
    &lt;p&gt;At Mount Zao, these formations occur during sustained westerly winds of up to 26m per second (85ft per second), with surface air temperatures between -6.3C to -0.1C (21-31F) and unusually high cloud liquid water content. Under these precise conditions, the rime thickens on the windward side of trees into overlapping ridges known as "shrimp tails", the distinctive shapes that cluster together to form the towering juhyo figures.&lt;/p&gt;
    &lt;p&gt;"Because such precise meteorological and ecological conditions align in very few places, Zao's snow monsters are a phenomenon almost unique to northern Japan," says Fumitaka Yanagisawa, an emeritus professor of geochemistry who studies the juhyo at Yamagata University.&lt;/p&gt;
    &lt;p&gt;The snow monsters are the biggest winter draw of the Zao area, a mountain range which lies between Japan's Yamagata and Miyagi prefectures and attracts tens of thousands of visitors annually.&lt;/p&gt;
    &lt;p&gt;But recent research indicates that the monsters are becoming slimmer.&lt;/p&gt;
    &lt;p&gt;In August 2025, a research team led by Yanagisawa announced findings that quantified what locals have long observed. By analysing identical-angle photographs of Zao's summit taken since 1933, the team measured the thickness of the figures on a six-point scale. The findings (which have not yet been published in a scientific journal) indicate a widespread shrinking of the juhyo.&lt;/p&gt;
    &lt;p&gt;"In the 1930s, we saw juhyo five to six metres [16-20ft] across," Yanagisawa says. "By the postwar decades, they were often two to three metres [7-10ft]. Since 2019, many are half a metre [1.6ft] or less. Some are barely columns."&lt;/p&gt;
    &lt;p&gt;The cause is twofold, says Yanagisawa: a warming climate and a forest under attack. The host tree, Aomori todomatsu, suffered a moth outbreak in 2013 that stripped its needles. Bark beetles followed in 2015, boring into weakened trunks. Yamagata officials report that around 23,000 firs, about a fifth of the prefectural side's stands, have died. With fewer branches and leaves, there is little surface for snow and ice to cling to.&lt;/p&gt;
    &lt;p&gt;Another 2019 study found that in nearby Yamagata City, average temperatures from December to March have risen by about 2C (3.6F) over the past 120 years. The lower altitude limit of juhyo formation has shifted upward in step with this warming, it found, while the juhyo also last for fewer days of the year.&lt;/p&gt;
    &lt;p&gt;"Unique landscapes are already being lost to climate change," says Akihiko Ito, an ecologist who specialises in forests and climate change at the University of Tokyo.&lt;/p&gt;
    &lt;p&gt;Research shows that Japan's warming climate and extreme weather are already damaging many of its high mountain forests. "Seasonal shifts in spring and autumn can harm leaves, and insect outbreaks are expanding. These stresses may reduce forest growth and density," Ito says.&lt;/p&gt;
    &lt;p&gt;Across Japan's alpine zones, temperatures have been rising faster than the global average since the 1980s. "In scenarios where climate change continues to advance significantly by the end of this century, it is possible that in warmer-than-usual winters, juhyo may no longer form at all," Ito says.&lt;/p&gt;
    &lt;p&gt;The threat has prompted action across Yamagata. In March 2023, the prefecture launched the Juhyo Revival Conference – a permanent council bringing together researchers, officials, local businesses and residents to coordinate long-term efforts to restore the fir forests and preserve Mount Zao's snow monsters.&lt;/p&gt;
    &lt;p&gt;Juhyo are not only a natural spectacle but also a pillar of the local economy. "The influx of tourists supports hotels, restaurants and souvenir shops throughout the area," says Genji Akiba, deputy director of the Zao Onsen Tourism Association. "If the juhyo disappear, it would be a huge blow."&lt;/p&gt;
    &lt;p&gt;"Revival is a strong wish of our citizens," says Yoko Honma, a conservation specialist at Yamagata Prefecture's nature division. Since 2019, the local forest office has transplanted more than 190 naturally regenerated saplings from lower slopes to the summit zone near the ropeway station. "Because it takes 50 to 70 years for these firs to mature, the key is sustaining conservation across generations," says Honma. "We need patience and continuity."&lt;/p&gt;
    &lt;p&gt;In Murayama, about 20km (12 miles) north-west of Zao, students from a forestry and environmental science course at Murayama Technical High School have also taken up the challenge of reviving the firs.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;• The overlooked benefits of real Christmas trees&lt;/p&gt;
    &lt;p&gt;• The secrets of the Amazon's most mysterious river&lt;/p&gt;
    &lt;p&gt;• The mysterious black fungus from Chernobyl that may eat radiation&lt;/p&gt;
    &lt;p&gt;Since 2022, the students have been planting Aomori todomatsu trees and studying how to propagate and protect the species. Together with staff from the Yamagata Forest Office, they visit Mount Zao to collect young fir saplings and bring them back to their school for research. There, they cultivate stems through cuttings and experiment with methods for artificially propagating and efficiently producing seedlings.&lt;/p&gt;
    &lt;p&gt;"It's been challenging," says Rin Oizumi, a second-year student in the course. "When the seeds we sowed in heavy rain finally sprouted, I felt both relief and excitement. But it was heartbreaking to find that some plots had been damaged by field mice, which had eaten the young shoots." The students have also conducted preliminary experiments using branches of a related fir species, which have shown successful germination.&lt;/p&gt;
    &lt;p&gt;Kanon Taniai, Oizumi's classmate, recalls seeing more and more fallen or dead trees as she and other students neared the summit one day in July 2024. "It made me feel really sad," she says. "Growing seedlings is hard work, but we want to do what we can to help bring Mount Zao back to life."&lt;/p&gt;
    &lt;p&gt;For Taniai, protecting the Juhyo means passing their legacy to the next generation. "They are called snow monsters because nothing else looks like them," she says. "I want the world to see them, and to feel how special Japan's nature is."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For essential climate news and hopeful developments to your inbox, sign up to the Future Earth newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more science, technology, environment and health stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177418</guid><pubDate>Sat, 06 Dec 2025 23:06:53 +0000</pubDate></item><item><title>Kilauea erupts, destroying webcam [video]</title><link>https://www.youtube.com/watch?v=TK2N99BDw7A</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46177645</guid><pubDate>Sat, 06 Dec 2025 23:39:02 +0000</pubDate></item><item><title>Trains cancelled over fake bridge collapse image</title><link>https://www.bbc.com/news/articles/cwygqqll9k2o</link><description>&lt;doc fingerprint="751d1d963c6ce490"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Trains cancelled over fake bridge collapse image&lt;/head&gt;
    &lt;p&gt;Trains were halted after a suspected AI-generated picture that seemed to show major damage to a bridge appeared on social media following an earthquake.&lt;/p&gt;
    &lt;p&gt;The tremor, which struck on Wednesday night, was felt across Lancashire and the southern Lake District.&lt;/p&gt;
    &lt;p&gt;Network Rail said it was made aware of the image which appeared to show major damage to Carlisle Bridge in Lancaster at 00:30 GMT and stopped rail services across the bridge while safety inspections were carried out.&lt;/p&gt;
    &lt;p&gt;A BBC journalist ran the image through an AI chatbot which identified key spots that may have been manipulated.&lt;/p&gt;
    &lt;p&gt;Network Rail said the railway line was fully reopened at around 02:00 GMT and it has urged people to "think about the serious impact it could have" before creating or sharing hoax images.&lt;/p&gt;
    &lt;p&gt;"The disruption caused by the creation and sharing of hoax images and videos like this creates a completely unnecessary delay to passengers at a cost to the taxpayer," a spokesperson said.&lt;/p&gt;
    &lt;p&gt;"It adds to the high workload of our frontline teams, who work extremely hard to keep the railway running smoothly," the spokesperson said.&lt;/p&gt;
    &lt;p&gt;"The safety of rail passengers and staff is our number one priority and we will always take any safety concerns seriously."&lt;/p&gt;
    &lt;p&gt;The British Transport Police said it was "made aware" of the situation but there was no ongoing investigation into the incident.&lt;/p&gt;
    &lt;p&gt;Network Rail said 32 services including passenger and freight trains were delayed because of hoax.&lt;/p&gt;
    &lt;p&gt;A spokesperson for the rail provider said a mix of passenger and freight train would have been impacted.&lt;/p&gt;
    &lt;p&gt;They said some of them would have been directly stopped or slowed while it checked the lines, but a lot of the trains were delayed as a result of earlier services still being in their path.&lt;/p&gt;
    &lt;p&gt;The spokesperson said many of them would have been local but because of the length of the West Coast Main Line some trains were delayed as far north as Scotland.&lt;/p&gt;
    &lt;p&gt;Railway expert Tony Miles said due to the timing of the incident, very few passengers will have been impacted by the hoax as the services passing through at that time were primarily freight and sleeper trains.&lt;/p&gt;
    &lt;p&gt;"They generally go slow so as not to disturb the passengers trying to sleep - this means they have a bit of leeway to go faster and make up time if they encounter a delay," he said.&lt;/p&gt;
    &lt;p&gt;"It's more the fact that Network Rail will have had to mobilise a team to go and check the bridge which could impact their work for days."&lt;/p&gt;
    &lt;p&gt;He urged people to consider hoaxes like this could have on real people.&lt;/p&gt;
    &lt;p&gt;"If they actually did delay a train it could have impacted someone who had to get to a medical appointment, or a flight or a funeral.&lt;/p&gt;
    &lt;p&gt;"It may seem like a game, but anyone who's thinking of doing this should consider how it will impact real people."&lt;/p&gt;
    &lt;p&gt;Listen to the best of BBC Radio Lancashire on Sounds and follow BBC Lancashire on Facebook, X and Instagram. You can also send story ideas via Whatsapp to 0808 100 2230.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46178108</guid><pubDate>Sun, 07 Dec 2025 00:37:15 +0000</pubDate></item><item><title>Using LLMs at Oxide</title><link>https://rfd.shared.oxide.computer/rfd/0576</link><description>&lt;doc fingerprint="ab11131f2d2c5aa1"&gt;
  &lt;main&gt;
    &lt;p&gt;Large language models (LLMs) are an indisputable breakthrough of the last five years, potentially profoundly changing the way that we work. As with any extraordinarily powerful tool, LLM use has both promise and peril — and that they are so general-purpose leaves real questions about how and when they should be used. The landscape is shifting so rapidly that static prescription is unlikely — but that LLMs are evolving so quickly also gives urgency to the question: how should LLMs be used at Oxide?&lt;/p&gt;
    &lt;head rend="h2"&gt;Values in LLM usage&lt;/head&gt;
    &lt;p&gt;As is our wont, it’s helpful to look at LLM use through the lens of our values, several of which come to mind, listed here in priority order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Responsibility: In terms of LLM use at Oxide, our lodestar is our sense of responsibility. However powerful they may be, LLMs are but a tool, ultimately acting at the behest of a human. Oxide employees bear responsibility for the artifacts we create, whatever automation we might employ to create them. That is, human judgement remains firmly in the loop: even if or as an LLM is generating an artifact that we will use (writing, test cases, documentation, code, etc.), their output is the responsibility of the human using them.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rigor: LLMs are double-edged with respect to rigor. On the one hand, wielded carefully, they can help us sharpen our own thinking by pointing out holes in our own reasoning or otherwise providing thought-provoking suggestions. On the other, if used recklessly or thoughtlessly, they can have the opposite effect, replacing crisp thinking with generated flotsam. LLMs are useful in as much as they promote and reinforce our rigor.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Empathy: Be we readers or writers, there are humans on the other end of our language use. As we use LLMs, we must keep in mind our empathy for that human, be they the one who is consuming our writing, or the one who has written what we are reading.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Teamwork: We are working together on a shared endeavor, and we must be sure that our LLM use does not undermine our sense of teamwork. Specifically, we must be careful to not use LLMs in such a way as to undermine the trust that we have in one another. This isn’t as simple as disclosure of usage: and in fact, volunteering that an LLM has been used to generate work product is to implicitly distance oneself from the responsibility for the content — and serves as to erode the trust that is essential for teamwork.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Urgency: Urgency seems natural with a tool that can seemingly do so much knowledge work so quickly, but with respect to LLM use, too many organizations have seemingly enshrined urgency over all else. These organizations treat LLMs as an opportunity to increase pace over all else, seemingly without regard for setting direction. Urgency is certainly important, and LLMs absolutely afford an opportunity to do work more quickly — but that pace must not come at the expense of our responsibility, rigor, empathy and teamwork.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Uses of LLMs&lt;/head&gt;
    &lt;p&gt;LLM use varies widely, and the ramifications of those uses vary accordingly; it’s worth taking apart several of the (many) uses for LLMs.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as readers&lt;/head&gt;
    &lt;p&gt;LLMs are superlative at reading comprehension, able to process and meaningfully comprehend documents effectively instantly. This can be extraordinarily powerful for summarizing documents — or of answering more specific questions of a large document like a datasheet or specification. (Ironically, LLMs are especially good at evaluating documents to assess the degree that an LLM assisted their creation!)&lt;/p&gt;
    &lt;p&gt;While use of LLMs to assist comprehension has little downside, it does come with an important caveat: when uploading a document to a hosted LLM (ChatGPT, Claude, Gemini, etc.), there must be assurance of data privacy — and specifically, assurance that the model will not use the document to train future iterations of itself. Note that this may be opt-out (that is, by default, a model may reserve the right to train on uploaded documents), but can generally be controlled via preferences — albeit occasionally via euphemism. (OpenAI shamelessly calls this checked-by-default setting "Improve the model for everyone", making anyone who doesn’t wish the model to train on their data feel as if they suffer from a kind of reactionary avarice.)&lt;/p&gt;
    &lt;p&gt;A final cautionary note: using LLMs to assist comprehension should not substitute for actually reading a document where such reading is socially expected. More concretely: while LLMs can be a useful tool to assist in the evaluating of candidate materials per [rfd3], their use should be restricted to be as a tool, not as a substitute for human eyes (and brain!).&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as editors&lt;/head&gt;
    &lt;p&gt;LLMs can be excellent editors. Engaging an LLM late in the creative process (that is, with a document already written and broadly polished), allows for LLMs to provide helpful feedback on structure, phrasing, etc. — all without danger of losing one’s own voice. A cautionary note here: LLMs are infamous pleasers — and you may find that the breathless praise from an LLM is in fact more sycophancy than analysis. This becomes more perilous the earlier one uses an LLM in the writing process: the less polish a document already has, the more likely it is that an LLM will steer to something wholly different — at once praising your groundbreaking genius while offering to rewrite it for you.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as writers&lt;/head&gt;
    &lt;p&gt;While LLMs are adept at reading and can be terrific at editing, their writing is much more mixed. At best, writing from LLMs is hackneyed and clichÃ©-ridden; at worst, it brims with tells that reveal that the prose is in fact automatically generated.&lt;/p&gt;
    &lt;p&gt;What’s so bad about this? First, to those who can recognize an LLM’s reveals (an expanding demographic!), it’s just embarrassing — it’s as if the writer is walking around with their intellectual fly open. But there are deeper problems: LLM-generated writing undermines the authenticity of not just one’s writing but of the thinking behind it as well. If the prose is automatically generated, might the ideas be too? The reader can’t be sure — and increasingly, the hallmarks of LLM generation cause readers to turn off (or worse).&lt;/p&gt;
    &lt;p&gt;Finally, LLM-generated prose undermines a social contract of sorts: absent LLMs, it is presumed that of the reader and the writer, it is the writer that has undertaken the greater intellectual exertion. (That is, it is more work to write than to read!) For the reader, this is important: should they struggle with an idea, they can reasonably assume that the writer themselves understands it — and it is the least a reader can do to labor to make sense of it.&lt;/p&gt;
    &lt;p&gt;If, however, prose is LLM-generated, this social contract becomes ripped up: a reader cannot assume that the writer understands their ideas because they might not so much have read the product of the LLM that they tasked to write it. If one is lucky, these are LLM hallucinations: obviously wrong and quickly discarded. If one is unlucky, however, it will be a kind of LLM-induced cognitive dissonance: a puzzle in which pieces don’t fit because there is in fact no puzzle at all. This can leave a reader frustrated: why should they spend more time reading prose than the writer spent writing it?&lt;/p&gt;
    &lt;p&gt;This can be navigated, of course, but it is truly perilous: our writing is an important vessel for building trust — and that trust can be quickly eroded if we are not speaking with our own voice. For us at Oxide, there is a more mechanical reason to be jaundiced about using LLMs to write: because our hiring process very much selects for writers, we know that everyone at Oxide can write — and we have the luxury of demanding of ourselves the kind of writing that we know that we are all capable of.&lt;/p&gt;
    &lt;p&gt;So our guideline is to generally not use LLMs to write, but this shouldn’t be thought of as an absolute — and it doesn’t mean that an LLM can’t be used as part of the writing process. Just please: consider your responsibility to yourself, to your own ideas — and to the reader.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as code reviewers&lt;/head&gt;
    &lt;p&gt;As with reading comprehension and editing, LLMs can make for good code reviewers. But they can also make nonsense suggestions or otherwise miss larger issues. LLMs should be used for review (and can be very helpful when targeted to look for a particular kind of issue), but that review should not be accepted as a human substitute.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as debuggers&lt;/head&gt;
    &lt;p&gt;LLMs can be surprisingly helpful debugging problems, but perhaps only because our expectations for them would be so low. While LLMs shouldn’t be relied upon (clearly?) to debug a problem, they can serve as a kind of animatronic rubber duck, helping to inspire the next questions to ask. (And they can be surprising: LLMs have been known to debug I2C issues from the screenshot of a scope capture!) When debugging a vexing problem one has little to lose by using an LLM — but perhaps also little to gain.&lt;/p&gt;
    &lt;head rend="h3"&gt;LLMs as programmers&lt;/head&gt;
    &lt;p&gt;LLMs are amazingly good at writing code — so much so that there is borderline mass hysteria about LLMs entirely eliminating software engineering as a craft. As with using an LLM to write prose, there is obvious peril here! Unlike prose, however (which really should be handed in a polished form to an LLM to maximize the LLM’s efficacy), LLMs can be quite effective writing code de novo. This is especially valuable for code that is experimental or auxiliary or otherwise throwaway. The closer code is to the system that we ship, the greater care needs to be shown when using LLMs. Even with something that seems natural for LLM contribution (e.g., writing tests), one should still be careful: it’s easy for LLMs to spiral into nonsense on even simple tasks. Still, they can be extraordinarily useful — and can help to provide an entire spectrum of utility in writing software; they shouldn’t be dismissed out of hand.&lt;/p&gt;
    &lt;p&gt;Wherever LLM-generated code is used, it becomes the responsibility of the engineer. As part of this process of taking responsibility, self-review becomes essential: LLM-generated code should not be reviewed by others if the responsible engineer has not themselves reviewed it. Moreover, once in the loop of peer review, generation should more or less be removed: if code review comments are addressed by wholesale re-generation, iterative review becomes impossible.&lt;/p&gt;
    &lt;p&gt;In short, where LLMs are used to generate code, responsibility, rigor, empathy and teamwork must remain top of mind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mechanics&lt;/head&gt;
    &lt;p&gt;Mechanical details of using LLMs — along with many specific tips and links — can be found in the (internal) LLMs at Oxide document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Determinations&lt;/head&gt;
    &lt;p&gt;Broadly speaking, LLM use is encouraged at Oxide, but that use must always be consistent with our deeply-held sense of responsibility: our responsibility to our product, our responsibility to our customers — and our responsibility to one another.&lt;/p&gt;
    &lt;head rend="h2"&gt;External References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;[rfd3] Oxide Computer Company. RFD 3 Oxide Hiring Process. https://rfd.shared.oxide.computer/rfd/0003.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46178347</guid><pubDate>Sun, 07 Dec 2025 01:17:40 +0000</pubDate></item><item><title>Eurydice: a Rust to C compiler (yes)</title><link>https://jonathan.protzenko.fr/2025/10/28/eurydice.html</link><description>&lt;doc fingerprint="7e2eb37eada7ba38"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Eurydice: a Rust to C compiler (yes)&lt;/head&gt;
    &lt;p&gt;Perhaps the greatest surprise of the last two years was, for me, the realization that people not only care about compiling C to Rust (for obvious reasons, such as, ahem, memory safety) â they also care about compiling Rust to C! Wait, what?&lt;/p&gt;
    &lt;p&gt;I wrote about this briefly a couple years ago, but the level of interest for the project, I must say, took me somewhat by surprise. So letâs talk about compiling Rust to C a little more today.&lt;/p&gt;
    &lt;head rend="h1"&gt;Barriers to Rust adoption&lt;/head&gt;
    &lt;p&gt;Rust is making big progress in terms of adoption, and represents a great value proposition, especially for new code. Both my former employer and my new employer, like pretty much everyone else these days, have big projects that are written in pure Rust or can have Rust components. Even Windows kernel drivers can be written in Rust now. Amazing stuff.&lt;/p&gt;
    &lt;p&gt;However, if your project is, say, an open-source library that gets compiled on a wonderfully diverse set of target architectures, OSes, distributions and toolchains, well, chances areâ¦ one of these is not going to support Rust. Think of a crypto library: there will be people out there with an obscure compiler for a weird embedded target, and they really want to compile your library, because theyâve been told not to roll out their own crypto. Or perhaps you have a format library ridden with memory errors and you want to port it to Rust. Or maybe your company has an in-house analysis that only runs on C code. Regardless of the scenario, there will always be that one legacy use-case that prevents you from switching to Rust until itâs 2035, all those LTS versions (looking at you RHEL) are finally retired, and you yourself are too close to retirement to even care anymore.&lt;/p&gt;
    &lt;p&gt;That is, unless youâre willing to use a Rust to C compiler.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Having a backwards-compat scenario where Rust can be compiled to C serves several purposes.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It allows for a gradual transition. The codebase can be ported to Rust, and refactored / cleaned up / rewritten to use all the nice Rust things (data types, pattern-matching, polymorphism, memory safety), thus making you and your developers much, much happier. Meanwhile, the C version co-exists so that you donât alienate your userbase.&lt;/item&gt;
      &lt;item&gt;It only requires maintaining a single version. The Rust code is authoritative; the C code is derived from it automatically, either on CI, or at least with a CI job that checks that the two are in sync.&lt;/item&gt;
      &lt;item&gt;It allows for a census of problematic scenarios. By making the Rust version the default (and putting the fallback C behind a &lt;code&gt;--write-us-an-email&lt;/code&gt;flag), there is finally a way to enumerate those mythical users who cannot switch to Rust just yet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If that sounds appealing, meet Eurydice.&lt;/p&gt;
    &lt;head rend="h1"&gt;Eurydice&lt;/head&gt;
    &lt;p&gt;Eurydice is a compiler from Rust to C that aims to produce readable C code. Of course, readability is subjective; also, seeing that Rust relies on whole-program monomorphization, the C code is bound to be more verbose than the Rust code. But you can judge for yourself: hereâs the result of compiling libcrux to C.&lt;/p&gt;
    &lt;p&gt;The output of the test suite is under version control, and there are a lot more tests to peruse. See for instance this bit, compared to the Rust original.&lt;/p&gt;
    &lt;head rend="h1"&gt;The design of Eurydice&lt;/head&gt;
    &lt;p&gt;Eurydice plugs in directly at the MIR level, using Charon to avoid reimplementing the wheel and paying the price of interacting with the guts of &lt;code&gt;rustc&lt;/code&gt;. Our
paper on Charon says more about its
architecture.&lt;/p&gt;
    &lt;p&gt;The advantage of plugging in at the MIR level is that i) we do not have to interpret syntactic sugar, which means our translation is more faithful to the Rust semantics, and ii) we have way fewer constructs that need compiling to C. Even then, itâs no easy feat to translate Rust to C.&lt;/p&gt;
    &lt;p&gt;There is naturally, the need to perform whole-program monomorphization, over types and const-generic arguments; the compilation of pattern matches into tagged unions; recognizing instances of iterators that can be compiled to native C &lt;code&gt;for&lt;/code&gt;-loops. Then, there are more subtle things, such as compiling array
repeat expressions sensibly â zero-initializers when possible, initializer
lists otherwise, unless it generates too much code, in which case &lt;code&gt;for&lt;/code&gt;-loops are
preferable. And finally, there are all the rules about visibility, &lt;code&gt;static&lt;/code&gt;,
&lt;code&gt;inline&lt;/code&gt;, etc. that are very C-specific and depend on how you want to lay out
your C files.&lt;/p&gt;
    &lt;p&gt;The translation is complicated by the constraint that the generated code ought to be readable: for instance, we compile Rust structs to C structs, including DSTs, by relying on flexible array members. We also work hard to avoid using the fully-generic tagged union pattern when possible, instead eliminating the tag when e.g. the Rust enum only has a single case. Additionally, we rely on Charon to reconstruct control-flow, rather than compile the MIR CFG to C code ridden with &lt;code&gt;goto&lt;/code&gt;s; again, this is for code quality.&lt;/p&gt;
    &lt;p&gt;At a low-level, there were many interesting tidbits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Because arrays in Rust are values, we wrap them within C structs to give them value semantics in C, too; concretely, &lt;code&gt;[u32; 8]&lt;/code&gt;becomes&lt;code&gt;struct { uint32_t data[8]; }&lt;/code&gt;. (A previous version of Eurydice would emit&lt;code&gt;uint32_t *&lt;/code&gt;, and rely on various&lt;code&gt;memcpy&lt;/code&gt;s to implement value semantics, but this produced a translation that was not type-generic, and there were plenty of finicky corner cases. We revamped the compilation scheme recently.)&lt;/item&gt;
      &lt;item&gt;The notion of &lt;code&gt;lvalue&lt;/code&gt;in C means we need to insert more variable declarations than in Rust â for instance, you canât trivially compile&lt;code&gt;&amp;amp;[0u32; 1]&lt;/code&gt;without naming the array.&lt;/item&gt;
      &lt;item&gt;The fact that the evaluation order is so loosely defined in C means that intermediary computations need to be stored in intermediary variables to enforce the evaluation order.&lt;/item&gt;
      &lt;item&gt;Rust relies on whole-program monomorphization; this means that the C code is inevitably going to contains multiple copies of the same types and functions, but for different choices of type and const generic argumnets. This is currently done with a builtin phase in Eurydice (for historical reasons), but in the long run, we want to rely on Charonâs support for monomorphization.&lt;/item&gt;
      &lt;item&gt;There are plenty of peephole optimizations that are required for good code quality, such as recognizing &lt;code&gt;array::from_fn&lt;/code&gt;and generating sensible code that initializes the array in-place (instead of relying on the fully-general compilation scheme for closures), or recognizing instances of the&lt;code&gt;Eq&lt;/code&gt;trait that deserve dedicated treatment (such as using&lt;code&gt;memcmp&lt;/code&gt;for arrays and slices of flat data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A final design choice is that for now, Eurydice may define more behaviors than Rust â for instance, Rust panics on integer overflow, but Eurydice-compiled code does not. This is because we assume the input code is verified, and therefore has been shown to be free of panics. This design choice can be easily changed, though.&lt;/p&gt;
    &lt;p&gt;In practice, as soon as you use traits, the C code becomes more voluminous than the Rust code. We rely on a configuration file mechanism to control the placement of monomorphized instances of a given function, rather than put everything in one big C file. This currently requires a lot of manual intervention to give good results on large projects.&lt;/p&gt;
    &lt;head rend="h1"&gt;Implementing of Eurydice&lt;/head&gt;
    &lt;p&gt;Eurydice starts by compiling the MIR AST obtained out of Charon into KaRaMeLâs internal AST. This is ~3000 lines of OCaml code, so thatâs already pretty involved. A lot of the work revolves around trait methods and their monomorphization, given Rustâs expressive trait system.&lt;/p&gt;
    &lt;p&gt;Then, about 30 nanopasses simplify the KaRaMeL AST until it becomes eligible for compilation to C. Of those, a handful were originally written for KaRaMeL and were somewhat reusable; this includes compilation of data types, as well as monomorphization. The rest was written from scratch for Eurydice, and totals about ~5000 lines of OCaml code.&lt;/p&gt;
    &lt;p&gt;A particularly gnarly phase was eliminating MIRâs variable assignments as much as possible: in MIR, every variable starts out uninitialized at the beginning of the function; then, in lieu of the variable declaration, we have an assignment with the initial value. Naturally, having a variable declaration in the right spot is better for code quality, so an initial phase tries to reconstruct these assignments. Thatâs a drawback of using MIR, but we still firmly believe that sticking to something that has clear semantics is ultimately better.&lt;/p&gt;
    &lt;p&gt;Fun fact: because there are so many peephole optimizations, I got tired of maintaining enormous pattern-matches that would try to catch every flavor of Rust iterator that can be compiled to a C for-loop. Instead, a custom OCaml syntax extension allows writing concrete syntax for the internal KaRaMeL language in OCaml patterns. Those magic patterns then get compiled at compile-time to OCaml AST nodes for an actual OCaml pattern that matches the (deeply-embedded) syntax of KaRaMeLâs AST. This relies on a &lt;code&gt;ppx&lt;/code&gt;
that lexes, parses and compiles the concrete syntax.&lt;/p&gt;
    &lt;head rend="h1"&gt;Deploying Eurydice-generated code&lt;/head&gt;
    &lt;p&gt;Eurydice-generated code expects some hand-written glue that contains macros and &lt;code&gt;static inline&lt;/code&gt; functions; sometimes, itâs simply more convenient to write a
single macro that uses a type, rather than have Eurydice generate N copies of a
polymorphic function that gets specialized each time. A typical example is
compiling the Eq trait for arrays: itâs nicer to emit &lt;code&gt;Eurydice_array_eq(a1, a2,
len, t)&lt;/code&gt;, which macro-expands to &lt;code&gt;!(memcmp(a1, a2, len*sizeof(t)))&lt;/code&gt;, rather than
have N such functions, each containing a for-loop specialized for different
values of &lt;code&gt;t&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Eurydice generates code that is either (C11 and C++20-compatible) or (C++-17 compatible, but not C-compatible). The reason for this is that Rust allows enum values (e.g. &lt;code&gt;Foo { bar: baz }&lt;/code&gt;) in any expression position. For simplicity,
Eurydice emits a compound initializer &lt;code&gt;(Foo) { .tag = bar, .value = { .case_Foo
= { .bar = baz }}}&lt;/code&gt;, or a C++20 aggregate that uses designated initializers,
relying on a macro (not shown here) to hide the syntax differences between the
two. But C++17 does not have designated initializers, so there is an option for
Eurydice to emit different code that relies on member pointers to achieve
sensibly the same effect.&lt;/p&gt;
    &lt;head rend="h1"&gt;Limitations of Eurydice&lt;/head&gt;
    &lt;p&gt;Naturally, there are many limitations to this approach. Here are the main ones that come to mind:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;we cannot guarantee that the layout of objects will be the same in C as in Rust; conceivably, one could parse the layout information from MIR, then emit compiler-specific alignment directives to keep the two identical, but this is not done currently;&lt;/item&gt;
      &lt;item&gt;the generated code violates strict aliasing, because creating a user-defined DST involves casting one pointer type (a struct containing an array) to another (a struct with a flexible array member instead); Iâm not sure what the best fix is, so for now, please compile your code with &lt;code&gt;-fno-strict-aliasing&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;the code that Eurydice sees is MIR after applying &lt;code&gt;cfg&lt;/code&gt;tweaks; this means that for code that is intended to be multi-platform, some tricks need to be applied, otherwise, Eurydice will only âseeâ one version of the code (AVX2, or ARM64, or something else)&lt;/item&gt;
      &lt;item&gt;because monorphization is so pervasive, the configuration language needs to express things such as âtypes that reference &lt;code&gt;__m256i&lt;/code&gt;, an AVX2-only type, need to go into a separate file to be compiled with&lt;code&gt;-mavx2&lt;/code&gt;â; this can get tedious real fast but Iâm not sure I know how to do better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Whatâs next?&lt;/head&gt;
    &lt;p&gt;There is ongoing work to integrate Eurydice-generated code for both Microsoft and Googleâs respective crypto libraries.&lt;/p&gt;
    &lt;p&gt;The community grew recently, with wonderful contributions by GitHub users @ssyram and @lin23299. There are more in the pipeline, and I look forward to seeing the supported subset of Rust grow even more. Next on the horizon is support for &lt;code&gt;dyn&lt;/code&gt; traits via vtables, and relying on Charonâs monomorphization
to get MIR exactly as the Rust compiler would monomorphize it, intead of relying
on a custom procedure in Eurydice.&lt;/p&gt;
    &lt;p&gt;An ambitious goal is for the whole standard library of Rust to be extractable via Eurydice in 2026. This is non-trivial, but I believe this achievement is within reach. Stay tuned.&lt;/p&gt;
    &lt;head rend="h1"&gt;PS: Why the name?&lt;/head&gt;
    &lt;p&gt;People keep asking about the name; because the project shares a large amount of infrastructure with Aeneas and Charon, I had to follow the Greek mythology theme. Specifically, the myth of Eurydice resonated with me: I thought I was saved from the hell of generating C code, and was going to go back to the world of the living, but alas, no.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46178442</guid><pubDate>Sun, 07 Dec 2025 01:41:33 +0000</pubDate></item><item><title>Oblast: A better Blasto game for the Commodore 64</title><link>http://oldvcr.blogspot.com/2025/12/oblast-better-blasto-game-for-commodore.html</link><description>&lt;doc fingerprint="b6785d833bb7919f"&gt;
  &lt;main&gt;&lt;p&gt;So, in that article, I mentioned two future Blasto projects. One is to save my pennies for a custom arcade cabinet to put the board in, though I just spent a cool grand plus on tires which used up a lot of those pennies and I've also got Christmas presents to buy. But the second was to write my own take on TI Blasto and soup it up. This project is the second one from my bucket list that I've completed. It took a couple years of work on it off and on, but it's finally done, with faster action and animation, a massive number of procedurally generated screens, and fully configurable gameplay.&lt;/p&gt;I've christened it Oblast, and it's free to play on your real Commodore 64 or emulator. Let's talk about what's the same and what's different. The antediluvian 1978 Blasto ran on Hustle hardware, which was derived from Gremlin's original (and mercilessly copied) Blockade game as designed by Lane Hauck. Programmer Bill Blewitt managed to squeeze Blasto's entire game code, not counting character graphics, into just 2K of ROM. This code had to generate and draw the maze, handle one or two player inputs, handle their projectiles, check for collisions and trigger the audio and "boom" circuits, all while simultaneously setting off explosions that could trigger other explosions and other collisions. In the upright version it also had free game logic. Given its hardware and software size constraints the arcade game's gameplay limitations, which we'll discuss in a moment, were understandable. When Milton Bradley picked up the license (from Gremlin's new owner Sega) as a developer for the new TI 99/4, they kept the gameplay and basic rules in their home computer port almost identical. Instead, the programmers added music tracks, a colour display, and multiple configuration options. You could set not only the game's speed (I always played Full Tilt) ... ... but also how the maze was drawn, including whether trails existed (areas of the map pre-cleared for motion) and how dense the mines were. Likely as a way to emphasize the TMS9918(A)'s colour capabilities, the MB programmers changed the setting of the game to a green earth-bound landscape with blue (?) mines and reworked the spaceships into tanks. The density option probably had an even greater impact on gameplay than the speed setting because a maze with a lot of mines made for a zippier, more explosive game. You could rig some big bangs this way, though these were sadly were let down by the TMS9919/SN76489's relatively weak noise output. The program also vainly tried to play a simple tune during the game but this was inevitably interrupted and forced to start over by any sound effect (i.e., a tank shooting, mines exploding). As with the original, you have infinite lives but finite time. If you trip on an explosion, or the other player shoots you in two-player mode, you lose valuable seconds until you respawn. However, you respawn at your original spawn point as if you were teleported there, a conceivable failure mode for a fanciful spaceship but an extremely unlikely one for a terrestrial tank, which makes a good segue into some of its other idiosyncrasies:&lt;list rend="ul"&gt;&lt;item&gt;Each player can only have a single projectile in motion at any time. However, as soon as that projectile impacts, you can immediately fire another one. This is clearly motivated by the limited memory in the original game, but I don't know of any artillery shell in real life that works like that!&lt;/item&gt;&lt;item&gt;As a related phenomenon, although you can move while an explosion or chain reaction is occurring (with a slight reduction in frame rate), you can't shoot — at least not until the explosions stop, at which point you can once again shoot immediately. This also seems to be a concession to limited available memory as the game can't track multiple chain reactions at once.&lt;/item&gt;&lt;item&gt;Tanks absolutely can't go over mines or obstacles; they act as completely impassible barriers. I guess that might make sense with spaceships, but it seems like a rather wussy sort of tank.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Now, I want to point out that despite those things, I loved TI Blasto and played quite a bit of it. But we can improve on what is already an awful lot of fun.&lt;/p&gt;It took a while to get the fundamentals laid down, and it was immediately obvious that the most important mechanic in the game had to be the chain reaction since everybody likes to blow %@#$ up. Consequently, the code that handles the explosions was the first part of the game I wrote, as I reasoned the game wouldn't be worth completing if I couldn't get it fast or frantic enough. This very early draft was a simple proof of concept using PETSCII graphic characters to model the algorithm; character graphics were a must because doing this on the high-resolution screen would have played like molasses.&lt;p&gt;The game doesn't track explosions anywhere else but the screen itself: everything it needs to determine the next frame of animation is by looking at what's set on the characters present. It scans the entire playfield each time to do this which necessarily locks the animation to a constant frame rate — even if the whole screen were alive with multiple explosions, it would take nearly exactly as much time as if only one single bomb were being detonated, keeping gameplay speed consistent. I did a lot of code unrolling to make this work as quick as possible and the final draft of the original "screen test" is what's in Oblast now.&lt;/p&gt;&lt;p&gt;The black area is because I already knew I'd be using sprites for the tank and I didn't want to mess around with having to manage the high bit for X coordinates greater than 255, so I reserved the right-hand portion of the screen for an information pane. This also had the nice side effect of reducing how much of the screen must be scanned.&lt;/p&gt;For Oblast, I've concentrated exclusively on the single-player mode in which I played Blasto most, which also simultaneously solved some technical issues. (I may make a two-player version in the future if I figure out good solutions to them.) Although I've kept the spirit of TI Blasto's configurability, I made it extend not just to maze generation but even to the game's core rule set. The configuration portion is largely written as a BASIC stub with some 6502 assembly language helpers for speed, with the bulk of the remainder and the entirety of the main game loop also in assembly language.&lt;p&gt;There are four preset games, the parameters for which I selected after tweaking them during repeated playtesting. The first is the one I consider "typical" for most players to start with (and the one you'll see the computer attempt to play during Oblast's attract mode), the second has an increased number of bombs, the third adds trails and more Blasto-like rules for more classic gameplay, and the fourth is a completely gonzo game mode which has become my personal favourite after a rough day at work.&lt;/p&gt;If you don't like those presets, or want to tweak them further, there is a full game configuration screen letting you set game modes and the starting level/screen. The game supports up to 384 procedurally generated screens and you can start on any of them from 0 to 255. The screens are generated from constant seed data (in this case the 64's BASIC ROM) and thus designed to generate the same screen with the same parameters, facilitating muscle memory for longer play if you get good.&lt;p&gt;Like the two versions of Blasto, Oblast has mines (bombs) and obstacles (trees). You can very precisely control the densities of both. You can also have the game generate Blasto-style trails horizontally, vertically or both, you can set how quickly your tank's fuel is exhausted (i.e., your time limit, the only option which cannot be zero), and you can indicate if your tank is invulnerable to explosions and how quickly to cycle your shells. I'll talk about how that works in a moment. If you press one of the preset function keys in the configuration screen, then its settings are loaded as a starting point for you to modify.&lt;/p&gt;For the presets, where a new player wouldn't know exactly the game conditions they trigger, I pondered various in-game ways of informing them and hit on an easy-to-implement "dot matrix printout" motif where the BASIC stub scrolls a "briefing" before starting play, making asynchronous "printer" noises based on the bit pattern of each line's ASCII codes. This same motif is used for the basic built-in help since I had some extra space available.&lt;p&gt;Once you've got the settings the way you want, or you just want to keep playing the same preset, after a game ends you can bypass the presets and game configuration screens and jump right into a new game with the same settings by pressing the fire button.&lt;/p&gt;Here's two examples of the procedural screen generation at work, both level 0. The top screen is what you'd start at if you chose the "Regular Duty" (F1) preset; the second is "More Like Classic Blasto" (F5). Both have the same seed pointer, and you can see some commonalities in bomb and tree positions, but the second screen has a slightly lower bomb density and a slightly higher tree density plus trails going both horizontally and vertically. Each collection of settings will always generate the same screens on your machine. The game code manually counts the number of bombs and trees at the end of map generation since they may be taken away by trails or in the process of ensuring the tank has a cleared starting position.&lt;p&gt;Although we're using a custom character set for speed, I still wanted the colour flexibility of high resolution where you can have different text and background colours. To do so Oblast is something of a love letter to one of the VIC-II's more underutilized display modes, extended background colour mode (ECM). ECM supports up to four background colours on the same screen and the main game uses two additional colours besides the green background, the most obvious being the black background of the information pane, but also a yellow background as part of animating explosions. The price you pay for this flexibility is that only 64 characters of the standard 256-entry character set can be used; the two high bits instead become a bit pair to select the background colour.&lt;/p&gt;That meant making a lot of careful decisions about what I was going to actually display and getting those shapes into the first 64 character glyphs, shown here in Ultrafont+. You'll notice that I've replaced some of the letters and typographic characters with graphic shapes because I knew I would never actually need to display those letters or symbols. Everything you see on the screen except for the tank and the shells is a character in this custom font. On the bright side, this character limit also means we can reduce the memory needed by the game font by 75 percent. By looking for the bit set for the black background of the (impervious) information pane, as well as the wall character that also has this bit set, the game knows not to propagate explosions into that area. The yellow background comes in for knowing what needs to detonate next frame: the routine uses that bit as a deferred marker so that as it sweeps sequentially through the screen it doesn't update the same bomb twice in the same frame and prematurely snuff it out. Since setting that bit will also cause a different background colour to be used, we use yellow to make the explosion visually interesting as another side effect.&lt;p&gt;Parenthetically, the TMS9918 and TMS9918A also have a feature like this which TI Blasto itself appears to use: each 32 character block of its 256-character fonts can have its own colours. Unlike the VIC-II's ECM which must be specially enabled, this is a standard feature of the 32x24 text mode (which TI calls "Graphic I"), but the character shapes remain unchanged in each block which may require making duplicates (whereas with ECM they are always drawn from the first 64 glyphs).&lt;/p&gt;If there are a lot of bombs on screen, as is the case in the fourth preset and my favourite gameplay mode, nearly the entire screen will be consumed with the explosion which animates around you as you shoot other things. This wasn't possible in either of the original Blastos. Also, instead of trying to play music during game play, all three SID voices are used for noise generation (with a low-pass filter and some resonance smeared on for a woofer-like effect). Voice 1 is triggered when you fire your gun and voice 2 is always running as the tank's engine, with its frequency varying with motion and tank rotation. Voice 3 is used specifically for explosions because it's the only SID voice where you can directly sample both the oscillator waveform output and the current amplitude of the audio envelope. We take these samples, scale them to the activity on screen, and feed the result into the VIC-II's screen fine X scroll. Lots of explosions cause lots of shaking, yet the shaking is always in sync with the audio.&lt;p&gt;Besides the character graphics, the other major screen component are the sprites. The tank is a composite of three sprites: an animated set for the tank tread, the main tank body, and an accent layer. This is sharper than using multicolour sprites where your horizontal resolution is halved. These three sprites move together and the build system automatically precalculates frames to rotate them off a template, which are played back on screen when you turn. Unlike both versions of Blasto where the tank is limited to integral character positions, the tank in Oblast is larger than the bombs and trees and can move in single pixels, though I still limited movement to 90 degree angles so I didn't have to do expensive trig computations to figure out a shell's trajectory.&lt;/p&gt;&lt;p&gt;One sprite being used as the fuel gauge needle left four sprites for the shells. I had earlier considered using character graphics for them too, but animating shells that way would be slower and less smooth than moving sprites. On the other hand, then, without resorting to tricks there can only be four shells onscreen at once which also didn't seem very tank-like. After some thought I came up with a game mechanic to explain it. In the information pane in these two shots, you see the level number, the fuel gauge which acts as your timer, and then four blue shell indicators. Three of these indicators are dark blue, indicating they are reloading (the fourth is a bright cyan, indicating ready). We'll simply define the reloading time for any shell chamber as the maximum length of time it takes a shell to get across the screen in any direction. Thus, no matter how much you fire, you can only ever have four on-screen because the reloading time will lock you out. (Blasto-style fire control where shells recycle immediately as they hit something is preserved for that game mode, or if you turn on "fast cycl[ing] shells" from the configuration screen.)&lt;/p&gt;&lt;p&gt;While propagating explosions is approximately constant-time, other operations in the game may not be, and there's no reason to walk the screen if nothing's marked as needing it. That means we need a timebase to keep frame rates stable. For this purpose I used the Kernal jiffy clock, which on both PAL and NTSC systems is triggered by the Timer A interrupt to tick about every 1/60 second. The game loop locks to this and uses it to know when to move game objects and trigger screen updates. Still, even this isn't fast enough for moving very speedy things like the shells you fire and the game felt too slow. So ... we make the Timer A interrupt even faster, flogging it at 240Hz instead of 60Hz (the game has prescaler values for both PAL and NTSC), making jiffies 1/240 of a second instead and moving objects at that rate.&lt;/p&gt;&lt;p&gt;This does have interesting interactions when the VIC-II is still drawing your screen at either 50 or 60Hz even as you update it four times as quickly, and most of these interactions have to do with collisions because you can move objects faster than the VIC-II can detect they intersect. The bombs are as big as they are because that gives lots of opportunities to detect a shell striking one, but tank collisions remained unreliable with smaller graphics like trees. Fortunately, however, we've already declared we didn't like the fact that trees and bombs (i.e., obstacles and mines) were impassible objects, so we can make this deficiency into a virtue. The game keeps running track of where the tank last was and if a collision is detected immediately moves it back to that position. However, because collisions are detected inconsistently at this rate of motion and the game updates the tank's coordinates faster than the VIC will draw them, it ends up manifesting onscreen as the tank simply slowing down when it has to cross an obstacle. I like that better than just coming to a dead halt.&lt;/p&gt;Explosions, however, are nice big targets and we have no problem detecting when the tank gets nailed by one of those. In the game modes where your tank is vulnerable, we throw your tank into a temporary tailspin, start flashing the border and the information pane (which is just a matter of setting its colour register), turn on voice 1 and voice 3 at the same time for an even bigger boom, and take the envelope and waveform from voice 3 and put it into the fine Y scroll register as well as the X to really throw the screen around. My favourite game mode allows you to blow up the entire playfield with impunity, of course. I also decided to overhaul the scoring with special bonuses silently awarded after completing a screen and detailed cumulatively at the end when your score is added up (total bombs exploded plus total bonuses earned). Don't cheat and look at the source code, but the descriptions of the bonuses should give you a clue as to how you win them. Note that some bonuses are mutually exclusive, and some are explicitly disabled ("n/a") in certain game configurations that make them impossible or unavoidable.&lt;p&gt;Should you beat the default high score, you'll see another use of extended background colour mode for the champion medal (you'll just have to beat it fair and square, no spoiler screenshots). This segment uses FLD to scroll the medal into view and then cycles the ECM registers for a masked multiple colour bar effect without having to split the screen horizontally. It's a simple effect that I threw together in an afternoon but I think it looks nice. While the game configuration screen looks like it might use ECM for the top title, it actually doesn't because I needed lowercase letters, so I employ a much simpler trick for that screen which shouldn't take you long to figure out.&lt;/p&gt;&lt;p&gt;A key goal was to get the entire game in memory at once without additional loading or disk access, meaning you can even run it from cassette tape if you want to. In memory everything is arranged around the two character sets, the bank of sprites and the two hi-res title screens which are in fixed locations to deal with the VIC-II's more constrained view of memory (one of the hi-res screens is slotted under the BASIC ROM so I could free up 8K for something else). I then redistributed the various machine language subroutines and the three music tracks around those assets while also ensuring the BASIC menu stub had enough free memory to maintain its variables. After the core game was done I added two more extras on, the attract mode (which required some reworking to fit) and a really silly credits sequence, which implements a double-buffered screen scroller and takes advantage of the fact that the main music track sounds pretty neat slowed down. The entire mess is then single-parted using my custom cross-linker and optionally compressed.&lt;/p&gt;Oblast is freeware and open source on Github. You can build it with Perl 5, the xa65 cross assembler and optionally the pucrunch compressor. The Perl tools to generate the sprites, the tokenized BASIC code and the uncompressed runnable linked version are all included. Say that you want to change the presets to your own preferred settings: just change the corresponding DATA statement in the BASIC code, do a make and instantly have your modified binary. All I ask is that modified binaries that you provide to others should use a different name so they aren't confused with the original, and note that this game and any derivative works based on it or its components are under the Floodgap Free Software License.&lt;p&gt;If you just want to play it, the Github releases tab provides compressed (for actual floppy disks or tape or other media with limited space) and uncompressed (for fast DMA cartridges and emulators) builds as .prg files you can run directly. You'll need a joystick or equivalent in port 2, and the game should run on any PAL or NTSC Commodore 64. This is hardly the last game, let alone project, on my bucketlist, but it's good to knock another one off it. Also, please don't blow up trees in real life.&lt;/p&gt;&lt;p&gt; If you've enjoyed playing, buy me a &lt;del&gt;coffee&lt;/del&gt; Pibb. &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46178464</guid><pubDate>Sun, 07 Dec 2025 01:47:23 +0000</pubDate></item><item><title>Z2 – Lithographically fabricated IC in a garage fab</title><link>https://sam.zeloof.xyz/second-ic/</link><description>&lt;doc fingerprint="6f28c43798a75591"&gt;
  &lt;main&gt;
    &lt;p&gt;Homemade 1000+ transistor array chip&lt;/p&gt;
    &lt;p&gt;In 2018 I made the first lithographically fabricated integrated circuits in my garage fab. I was a senior in high school when I made the Z1 amplifier, and now I’m a senior in college so there are some long overdue improvements to the amateur silicon process.&lt;lb/&gt; The Z1 had 6 transistors and was a great test chip to develop all the processes and equipment. The Z2 has 100 transistors on a 10µm polysilicon gate process – same technology as Intel’s first processor. My chip is a simple 10×10 array of transistors to test, characterize, and tweak the process but this is a huge step closer to more advanced DIY computer chips. The Intel 4004 has 2,200 transistors and I’ve now made 1,200 on the same piece of silicon.&lt;/p&gt;
    &lt;p&gt;Previously, I made chips with a metal gate process. The aluminum gate has a large work function difference with the silicon channel beneath it which results in a high threshold voltage (&amp;gt;10V). I used these metal gate transistors in a few fun projects like a guitar distortion pedal and a ring oscillator LED blinker but both of these required one or two 9V batteries to run the circuit due to high Vth. By switching to a polysilicon gate process, I get a ton of performance benefits (self aligned gate means lower overlap capacitances) including a much lower Vth which makes these chips compatible with 2.5V and 3.3V logic levels. The new FETs have excellent characteristics:&lt;/p&gt;
    &lt;quote&gt;NMOS Electrical Properties: Vth = 1.1 V Vgs MAX = 8 V Cgs = &amp;lt;0.9 pF Rise/fall time = &amp;lt;10 ns On/off ratio = 4.3e6 Leakage current = 932 pA (Vds=2.5V)&lt;/quote&gt;
    &lt;p&gt;I was particularly surprised by the super low leakage current. This value goes up about 100x in ambient room lighting.&lt;/p&gt;
    &lt;p&gt;Now we know that it’s possible to make really good transistors with impure chemicals, no cleanroom, and homemade equipment. Of course, yield and process repeatability are diminished. I’ll do more testing to collect data on the statistics and variability of FET properties but it’s looking good!&lt;/p&gt;
    &lt;p&gt;The chip is small, about one quarter the die area of my previous ICs (2.4mm^2) which makes it hard to probe. There’s a simple 10×10 array of N-channel FETs on each chip which will give me a lot of characterization data. Since it’s such a simple design, I was able to lay it out using Photoshop. Columns of 10 transistors share a common gate connection and each row is strung together in series with adjacent transistors sharing a source/drain terminal. It’s similar to NAND flash but I only did this to keep the metal pads large enough so I can reasonably probe them, if every FET had 3 pads for itself they would be too small.&lt;/p&gt;
    &lt;p&gt;It’s hard to convey the excitement of seeing a good FET curve displayed on the curve tracer after dipping a shard of rock into chemicals all day.&lt;/p&gt;
    &lt;p&gt;A single 10µm NMOS transistor can be see below, with slight misalignment in the metal layer (part of the left contact is uncovered). Red outline is polycrystalline silicon, blue is the source/drain.&lt;/p&gt;
    &lt;p&gt;So far I’ve made an opamp (Z1) and a memory-like array (Z2). More interesting circuits are definitely possible even with this low transistor density. The process needs some tweaking but now that I’m able to consistently make good quality transistors I should be able to design more complex digital and analog circuits. Testing each chip is very tedious so I am trying to automate the process and I’ll post more data then. I’ve made 15 chips (1,500 transistors) and know there’s at least one completely functional chip and at least two “mostly functional”, meaning ~80% of the transistors work instead of 100%. No proper yield data yet. The most common defect is a drain or source shorted to the bulk silicon channel, not a leaky or shorted gate like on my Z1 process.&lt;/p&gt;
    &lt;p&gt;I said before that the gate used to be made out of aluminum and now it’s silicon which makes the chips work a lot better. Silicon comes in three varieties that we care about: amorphous, polycrystalline, and monocrystalline. From left to right, these become more electrically conductive but also much harder to deposit. In fact, monocrystalline Si can’t be deposited, you can only grow it in contact with another mono-Si layer as a seed (epitaxy). Since the gate must be deposited on top of an insulating dielectric, poly is the best we can do. We can heavily dope the polysilicon anyway to make it more conductive.&lt;/p&gt;
    &lt;p&gt;A typical self-aligned polysilicon gate process requires silane, a toxic and explosive gas, to deposit polycrystalline silicon layers. It may also be possible by sputtering or evaporating amorphous silicon and annealing with a laser. A major theme of this DIY silicon process is to circumvent expensive, difficult, or dangerous steps. So, I came up with a modified process flow. It’s a variation on the standard self-aligned methods to allow doping via high temperature diffusion rather than ion implantation. The effect is that I’m able to buy a silicon wafer with the polysilicon already deposited on it from the factory and pattern it to make transistors instead of putting my own polysilicon down halfway through the process. This is a nice short term workaround but it would be best to design a polysilicon deposition process using the laser anneal method mentioned above.&lt;/p&gt;
    &lt;p&gt;Wafers are available with all kinds of materials deposited on them already, so I just had to find one with a thin layer of SiO2 (gate oxide, ~10nm) followed by a thicker polysilicon (300nm). I found a lot of 25 200mm (EPI, prime, [1-0-0], p-type) wafers on eBay for $45 which is essentially a lifetime supply, so email me if you want one. The gate oxide is the most fragile layer and requires the most care during fabrication. Since I bought the wafer with a nice high quality oxide on it already that was capped off and kept clean by the thick polysilicon layer, I was able to eliminate all the aggressive cleaning chemicals (sulfuric acid, etc) from the process and still make great transistors. Minimal process chemicals and tools are listed below.&lt;/p&gt;
    &lt;quote&gt;Chemicals used in home poly-gate process: -Water -Alcohol -Acetone -Phosphoric acid -Photoresist -Developer (2% KOH) -N type dopant (filmtronics P509) -HF (1%) or CF4/CHF3 RIE -HNO3 for poly etch or SF6 RIE&lt;/quote&gt;
    &lt;quote&gt;Equipment used in home poly-gate process: -Hotplate -Tube furnace -Lithography apparatus -Microscope -Vacuum chamber to deposit metal&lt;/quote&gt;
    &lt;p&gt;Z2 “gate first” process (similar to standard self-aligned process but without a field oxide):&lt;/p&gt;
    &lt;p&gt;I snapped one of the test chips in half (functional Z2 but with bad layer alignment and thin metal, about 300nm) and put it in my SEM for a cross section:&lt;/p&gt;
    &lt;p&gt;Find the dust particle in the red circle below, use that to get oriented in the coming cross section views.&lt;/p&gt;
    &lt;p&gt;Because I bought the wafer already with gate oxide and polysilicon on it, I can’t grow a field oxide. These thick oxide layers are typically used to mask dopants and require a long high temperature step which would oxidize all of my poly and there would be none remaining. So, my modified process uses an additional masking step (the “gate” mask is typically not found in a self-aligned process) that allows me to use the polysilicon itself as a dopant mask and hard-baked photoresist as the field dielectric. This alternative processing results in the stepped structure you can see in the orange region on the NMOS cross section above. This process subtlety is mentioned here, read this twitter thread.&lt;/p&gt;
    &lt;p&gt;This process isn’t ideal and I want to make some changes so it’s CMOS compatible but it simplifies fabrication and makes it possible with a minimal set of tools. The 1µm dielectric layer (orange) would ideally be CVD SiO2 (it’s possible to build a TEOS oxide reactor at home) but I used a photoresist instead. Most photoresists can be baked around 250°C to form a hard permanent dielectric layer that is an easy alternative to CVD or PECVD oxide. A spin-on-glass/sol-gel could also be used here. SiO2 etching is done with a buffered HF solution made from rust stain remover or RIE.&lt;/p&gt;
    &lt;p&gt;Huge composite stitched die image:&lt;/p&gt;
    &lt;p&gt;Thanks for following my work and feel free to contact me with your thoughts!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46178789</guid><pubDate>Sun, 07 Dec 2025 03:03:09 +0000</pubDate></item><item><title>Discovering the indieweb with calm tech</title><link>https://alexsci.com/blog/calm-tech-discover/</link><description>&lt;doc fingerprint="7fa459cf38cdcf2d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Discovering the indieweb with calm tech&lt;/head&gt;
    &lt;p&gt;When social media first entered my life, it came with a promise of connection. Facebook connected college-aged adults in a way that was previously impossible, helping to shape our digital generation. Social media was our super-power and we wielded it to great effect.&lt;/p&gt;
    &lt;p&gt;Yet social media today is a noisy, needy, mental health hazard. They push distracting notifications, constantly begging us to “like and subscribe”, and trying to trap us in endless scrolling. They have become sirens that lure us into their ad-infested shores with their saccharine promise of dopamine.&lt;/p&gt;
    &lt;p&gt;How can we defeat these monsters that have invaded deep into our world, while still staying connected?&lt;/p&gt;
    &lt;head rend="h2"&gt;StreetPass for Mastodon&lt;/head&gt;
    &lt;p&gt;A couple weeks ago I stumbled into a great browser extension, StreetPass for Mastodon. The creator, tvler, built it to help people find each other on Mastodon. StreetPass autodiscovers Mastodon verification links as you browse the web, building a collection of Mastodon accounts from the blogs and personal websites you’ve encountered.&lt;/p&gt;
    &lt;p&gt;StreetPass is a beautiful example of calm technology . When StreetPass finds Mastodon profiles it doesn’t draw your attention with a notification, it quietly adds the profile to a list, knowing you’ll check in when you’re ready.&lt;/p&gt;
    &lt;p&gt;StreetPass recognizes that there’s no need for an immediate call to action. Instead it allows the user to focus on their browsing, enriching their experience in the background. The user engages with StreetPass when they are ready, and on their own terms.&lt;/p&gt;
    &lt;p&gt;StreetPass is open source and available for Firefox, Chrome, and Safari.&lt;/p&gt;
    &lt;p&gt;Inspired by StreetPass, I applied this technique to RSS feed discovery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Blog Quest&lt;/head&gt;
    &lt;p&gt;Blog Quest is a web browser extension that helps you discover and subscribe to blogs. Blog Quest checks each page for auto-discoverable RSS and Atom feeds (using &lt;code&gt;rel="alternate"&lt;/code&gt; links) and quietly collects them in the background.
When you’re ready to explore the collected feeds, open the extension’s drop-down window.&lt;/p&gt;
    &lt;p&gt;The extension integrates with several feed readers, making subscription management nearly effortless.&lt;/p&gt;
    &lt;p&gt;Blog Quest is available for both Firefox and Chrome. The project is open source and I encourage you to build your own variants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ubiquitous yet hidden&lt;/head&gt;
    &lt;p&gt;I reject the dead Internet theory: I see a vibrant Internet full of humans sharing their experiences and seeking connection. Degradation of the engagement-driven web is well underway, accelerated by AI slop. But the independent web works on a different incentive structure and is resistant to this effect. Humans inherently create, connect, and share: we always have and we always will. If you choose software that works in your interest you’ll find that it’s possible to make meaningful online connections without mental hazard.&lt;/p&gt;
    &lt;p&gt;Check out StreetPass and Blog Quest to discover a decentralized, independent Internet that puts you in control.&lt;/p&gt;
    &lt;p&gt;You can't drown out the noise of social media by shouting louder, you've got to whisper.&lt;/p&gt;
    &lt;head rend="h3"&gt;Image credits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Edward Armitage: The Siren (1888)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46178892</guid><pubDate>Sun, 07 Dec 2025 03:26:01 +0000</pubDate></item></channel></rss>