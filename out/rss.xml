<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 13 Jan 2026 21:40:22 +0000</lastBuildDate><item><title>Show HN: Self-host Reddit â€“ 2.38B posts, works offline, yours forever</title><link>https://github.com/19-84/redd-archiver</link><description>&lt;doc fingerprint="7998b81663b49ce2"&gt;
  &lt;main&gt;
    &lt;p&gt;Transform compressed data dumps into browsable HTML archives with flexible deployment options. Redd-Archiver supports offline browsing via sorted index pages OR full-text search with Docker deployment. Features mobile-first design, multi-platform support, and enterprise-grade performance with PostgreSQL full-text indexing.&lt;/p&gt;
    &lt;p&gt;Supported Platforms:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
        &lt;cell role="head"&gt;Available Posts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;.zst JSON Lines (Pushshift)&lt;/cell&gt;
        &lt;cell&gt;âœ… Full support&lt;/cell&gt;
        &lt;cell&gt;2.38B posts (40,029 subreddits, through Dec 31 2024)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;SQL dumps&lt;/cell&gt;
        &lt;cell&gt;âœ… Full support&lt;/cell&gt;
        &lt;cell&gt;3.81M posts, 24.1M comments (22,637 subverses, complete archive)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;.7z JSON Lines&lt;/cell&gt;
        &lt;cell&gt;âœ… Full support&lt;/cell&gt;
        &lt;cell&gt;500K posts (6,217 guilds, complete archive)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tracked content: 2.384 billion posts across 68,883 communities (Reddit full Pushshift dataset through Dec 31 2024, Voat/Ruqqus complete archives)&lt;/p&gt;
    &lt;p&gt;Version 1.0 features multi-platform archiving, REST API with 30+ endpoints, MCP server for AI integration, and PostgreSQL-backed architecture for large-scale processing.&lt;/p&gt;
    &lt;p&gt;Try the live demo: Browse Example Archive â†’&lt;/p&gt;
    &lt;p&gt;New to Redd-Archiver? Start here: QUICKSTART.md&lt;/p&gt;
    &lt;p&gt;Get running in 2-15 minutes with our step-by-step guide covering:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local testing (5 minutes)&lt;/item&gt;
      &lt;item&gt;Tor homelab deployment (2 minutes) - no domain or port forwarding needed!&lt;/item&gt;
      &lt;item&gt;Production HTTPS (15 minutes)&lt;/item&gt;
      &lt;item&gt;Example data testing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Archive content from multiple link aggregator platforms in a single unified archive:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;CLI Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;URL Prefix&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;.zst JSON Lines&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--subreddit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/r/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;SQL dumps&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--subverse&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/v/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;.7z JSON Lines&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--guild&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/g/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatic Detection: Platform auto-detected from file extensions&lt;/item&gt;
      &lt;item&gt;Unified Search: PostgreSQL FTS searches across all platforms&lt;/item&gt;
      &lt;item&gt;Mixed Archives: Combine Reddit, Voat, and Ruqqus in single archive&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;29 MCP tools auto-generated from OpenAPI for AI assistants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full Archive Access: Query posts, comments, users, search via Claude Desktop or Claude Code&lt;/item&gt;
      &lt;item&gt;Token Overflow Prevention: Built-in LLM guidance with field selection and truncation&lt;/item&gt;
      &lt;item&gt;5 MCP Resources: Instant access to stats, top posts, subreddits, search help&lt;/item&gt;
      &lt;item&gt;Claude Code Ready: Copy-paste configuration for immediate use&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
  "mcpServers": {
    "reddarchiver": {
      "command": "uv",
      "args": ["--directory", "/path/to/mcp_server", "run", "python", "server.py"],
      "env": { "REDDARCHIVER_API_URL": "http://localhost:5000" }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;See MCP Server Documentation for complete setup guide.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ğŸ“± Mobile-First Design: Responsive layout optimized for all devices with touch-friendly navigation&lt;/item&gt;
      &lt;item&gt;ğŸ” Advanced Search System (Server Required): PostgreSQL full-text search optimized for Tor network. Search by keywords, subreddit, author, date, score. Requires Docker deployment - offline browsing uses sorted index pages.&lt;/item&gt;
      &lt;item&gt;âš¡ JavaScript Free: Complete functionality without JS, pure CSS interactions&lt;/item&gt;
      &lt;item&gt;ğŸ¨ Theme Support: Built-in light/dark theme toggle with CSS-only implementation&lt;/item&gt;
      &lt;item&gt;â™¿ Accessibility: WCAG compliant with keyboard navigation and screen reader support&lt;/item&gt;
      &lt;item&gt;ğŸš„ Performance: Optimized CSS (29KB), designed for low-bandwidth networks&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ğŸ—ï¸ Modular Architecture: 18 specialized modules for maintainability and extensibility&lt;/item&gt;
      &lt;item&gt;ğŸ—„ï¸ PostgreSQL Backend: Large-scale processing with constant memory usage regardless of dataset size&lt;/item&gt;
      &lt;item&gt;âš¡ Lightning-Fast Search: PostgreSQL full-text search with GIN indexing&lt;/item&gt;
      &lt;item&gt;ğŸŒ REST API v1: 30+ endpoints with MCP/AI optimization for programmatic access to posts, comments, users, statistics, search, aggregations, and exports&lt;/item&gt;
      &lt;item&gt;ğŸ§… Tor-Optimized: Zero JavaScript, server-side search, no external dependencies&lt;/item&gt;
      &lt;item&gt;ğŸ“Š Rich Statistics: Comprehensive analytics dashboard with file size tracking&lt;/item&gt;
      &lt;item&gt;ğŸ”— SEO Optimized: Complete meta tags, XML sitemaps, and structured data&lt;/item&gt;
      &lt;item&gt;ğŸ’¾ Streaming Processing: Memory-efficient with automatic resume capability&lt;/item&gt;
      &lt;item&gt;ğŸ“ˆ Progress Tracking: Real-time transfer rates, ETAs, and database metrics&lt;/item&gt;
      &lt;item&gt;ğŸ† Instance Registry: Leaderboard system with completeness-weighted scoring for distributed archives&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ğŸ  Local/Homelab: HTTP on localhost or LAN (2 commands)&lt;/item&gt;
      &lt;item&gt;ğŸŒ Production HTTPS: Automated Let's Encrypt setup (5 minutes)&lt;/item&gt;
      &lt;item&gt;ğŸ§… Tor Hidden Service: .onion access, zero networking config (2 minutes)&lt;/item&gt;
      &lt;item&gt;ğŸ”€ Dual-Mode: HTTPS + Tor simultaneously&lt;/item&gt;
      &lt;item&gt;ğŸ“„ Static Hosting: GitHub/Codeberg Pages for small archives (browse-only, no search)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver generates static HTML files that can be browsed offline OR deployed with full-text search:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Search&lt;/cell&gt;
        &lt;cell role="head"&gt;Server&lt;/cell&gt;
        &lt;cell role="head"&gt;Setup Time&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Offline Browsing&lt;/cell&gt;
        &lt;cell&gt;âŒ Browse-only&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;0 min&lt;/cell&gt;
        &lt;cell&gt;USB drives, local archives, offline research&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Static Hosting&lt;/cell&gt;
        &lt;cell&gt;âŒ Browse-only&lt;/cell&gt;
        &lt;cell&gt;GitHub/Codeberg Pages&lt;/cell&gt;
        &lt;cell&gt;10 min&lt;/cell&gt;
        &lt;cell&gt;Free public hosting (size limits)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Docker Local&lt;/cell&gt;
        &lt;cell&gt;âœ… PostgreSQL FTS&lt;/cell&gt;
        &lt;cell&gt;localhost&lt;/cell&gt;
        &lt;cell&gt;5 min&lt;/cell&gt;
        &lt;cell&gt;Development, testing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Docker + Tor&lt;/cell&gt;
        &lt;cell&gt;âœ… PostgreSQL FTS&lt;/cell&gt;
        &lt;cell&gt;.onion hidden service&lt;/cell&gt;
        &lt;cell&gt;2 min&lt;/cell&gt;
        &lt;cell&gt;Private sharing, no port forwarding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Docker + HTTPS&lt;/cell&gt;
        &lt;cell&gt;âœ… PostgreSQL FTS&lt;/cell&gt;
        &lt;cell&gt;Public domain&lt;/cell&gt;
        &lt;cell&gt;15 min&lt;/cell&gt;
        &lt;cell&gt;Production public archives&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Offline Browsing Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sorted index pages (by score, comments, date)&lt;/item&gt;
      &lt;item&gt;Pagination for large subreddits&lt;/item&gt;
      &lt;item&gt;Full comment threads and user pages&lt;/item&gt;
      &lt;item&gt;Works by opening HTML files directly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Search Server:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL full-text search with GIN indexing&lt;/item&gt;
      &lt;item&gt;Search by keywords, subreddit, author, date, score&lt;/item&gt;
      &lt;item&gt;Sub-second results, Tor-compatible&lt;/item&gt;
      &lt;item&gt;Requires Docker deployment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Internet content disappears every day. Communities get banned, platforms shut down, and valuable discussions vanish. You can help prevent this.&lt;/p&gt;
    &lt;p&gt;Don't wait for content to disappear. Download these datasets today:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;Posts&lt;/cell&gt;
        &lt;cell role="head"&gt;Download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3.28TB&lt;/cell&gt;
        &lt;cell&gt;2.38B posts&lt;/cell&gt;
        &lt;cell&gt;Academic Torrents Â· Magnet Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;~15GB&lt;/cell&gt;
        &lt;cell&gt;3.8M posts&lt;/cell&gt;
        &lt;cell&gt;Archive.org â€ &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;~752MB&lt;/cell&gt;
        &lt;cell&gt;500K posts&lt;/cell&gt;
        &lt;cell&gt;Archive.org â€¡&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;â€  Voat Performance Tip: Use pre-split files for 1000x faster imports (2-5 min vs 30+ min per subverse) â€¡ Ruqqus: Docker image includes p7zip for automatic .7z decompression&lt;/p&gt;
    &lt;p&gt;Every mirror matters. Store locally, seed torrents, share with researchers. Be part of the preservation network.&lt;/p&gt;
    &lt;p&gt;Already running an archive? Register it on our public leaderboard:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Deploy your instance (Quick Start - 2-15 minutes)&lt;/item&gt;
      &lt;item&gt;Submit via Registry Template&lt;/item&gt;
      &lt;item&gt;Join coordinated preservation efforts with other teams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Public visibility and traffic&lt;/item&gt;
      &lt;item&gt;Coordinated archiving to avoid duplication&lt;/item&gt;
      &lt;item&gt;Team collaboration opportunities&lt;/item&gt;
      &lt;item&gt;Leaderboard recognition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ğŸ‘‰ Register Your Instance Now â†’&lt;/p&gt;
    &lt;p&gt;Found a new platform dataset? Help expand the archive network:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lemmy databases&lt;/item&gt;
      &lt;item&gt;Hacker News archives&lt;/item&gt;
      &lt;item&gt;Alternative Reddit archives&lt;/item&gt;
      &lt;item&gt;Other link aggregator platforms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why submit?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Makes data discoverable for other archivists&lt;/item&gt;
      &lt;item&gt;Prevents duplicate preservation efforts&lt;/item&gt;
      &lt;item&gt;Builds comprehensive multi-platform archive ecosystem&lt;/item&gt;
      &lt;item&gt;Tracks data availability before platforms disappear&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Main landing page showing archive overview with statistics for 9,592 posts across Reddit, Voat, and Ruqqus. Features customizable branding (site name, project URL), responsive cards, activity metrics, and content statistics. (Works offline)&lt;/p&gt;
    &lt;p&gt;Post listing with sorting options (score, comments, date), pagination, and badge coloring. Includes navigation and theme toggle. (Works offline - sorted by score/comments/date)&lt;/p&gt;
    &lt;p&gt;Individual post displaying nested comment threads with collapsible UI, user flair, and timestamps. Comments include anchor links for direct navigation from user pages. (Works offline)&lt;/p&gt;
    &lt;p&gt;Fully optimized for mobile devices with touch-friendly navigation and responsive layout.&lt;/p&gt;
    &lt;p&gt;PostgreSQL full-text search with Google-style operators. Supports filtering by subreddit, author, date range, and score. (Requires Docker deployment)&lt;/p&gt;
    &lt;p&gt;Search results with highlighted excerpts using PostgreSQL &lt;code&gt;ts_headline()&lt;/code&gt;. Sub-second response times with GIN indexing. (Server-based, Tor-compatible)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sample Archive: Multi-platform archive featuring programming and technology communities from Reddit, Voat, and Ruqqus Â· See all screenshots â†’&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.7 or higher&lt;/item&gt;
      &lt;item&gt;PostgreSQL 12+ (required for v1.0+)&lt;/item&gt;
      &lt;item&gt;4GB+ RAM (PostgreSQL uses constant memory)&lt;/item&gt;
      &lt;item&gt;Disk space: ~1.5-2x your input .zst file size for PostgreSQL database&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver uses modern, performance-focused dependencies:&lt;/p&gt;
    &lt;p&gt;Core:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;psycopg[binary,pool]==3.2.3&lt;/code&gt;- PostgreSQL adapter with connection pooling&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;zstandard==0.23.0&lt;/code&gt;- Fast .zst decompression&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;psutil==6.1.1&lt;/code&gt;- System resource monitoring&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;HTML Generation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;jinja2&amp;gt;=3.1.6&lt;/code&gt;- Modern template engine with inheritance&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rcssmin&amp;gt;=1.1.2&lt;/code&gt;- CSS minification for smaller file sizes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;orjson&amp;gt;=3.11.4&lt;/code&gt;- Fast JSON parsing&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/19-84/redd-archiver.git
cd redd-archiver

# Create required directories
mkdir -p data output logs tor-public

# Copy environment template and configure
cp .env.example .env
# Edit .env with your settings (change default passwords!)

# Start PostgreSQL container
docker-compose up -d

# Install Python dependencies
pip install -r requirements.txt

# Configure database connection
export DATABASE_URL="postgresql://reddarchiver:your_password_here@localhost:5432/reddarchiver"

# Run the archive generator
python reddarc.py /path/to/data/ --output my-archive/&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/19-84/redd-archiver.git
cd redd-archiver

# Install PostgreSQL (Ubuntu/Debian)
sudo apt update &amp;amp;&amp;amp; sudo apt install postgresql postgresql-contrib

# Or on macOS
brew install postgresql@16 &amp;amp;&amp;amp; brew services start postgresql@16

# Create database
sudo -u postgres createuser redd-archiver
sudo -u postgres createdb -O redd-archiver redd-archiver
sudo -u postgres psql -c "ALTER USER redd-archiver WITH PASSWORD 'your_password_here';"

# Install Python dependencies
pip install -r requirements.txt

# Configure database connection
export DATABASE_URL="postgresql://reddarchiver:your_password_here@localhost:5432/reddarchiver"

# Run the archive generator
python reddarc.py /path/to/data/ --output my-archive/&lt;/code&gt;
    &lt;p&gt;Review the CHANGELOG.md for version updates and changes.&lt;/p&gt;
    &lt;p&gt;Redd-Archiver processes data dumps from multiple platforms:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;Format&lt;/cell&gt;
        &lt;cell role="head"&gt;Data Sources&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.zst JSON Lines&lt;/cell&gt;
        &lt;cell&gt;Pushshift Complete Dataset Â· Magnet Link Â· 3.28TB Â· 2.38B posts Â· 40K subreddits&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Voat&lt;/cell&gt;
        &lt;cell&gt;SQL dumps&lt;/cell&gt;
        &lt;cell&gt;Voat Archive 2021 Â· 22,637 subverses Â· 3.8M posts Â· 24M comments Â· Complete archive&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ruqqus&lt;/cell&gt;
        &lt;cell&gt;.7z JSON Lines&lt;/cell&gt;
        &lt;cell&gt;Ruqqus Archive 2021 Â· 6,217 guilds Â· Complete archive&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Scanner Tools help you identify which communities to archive first based on priority scores:&lt;/p&gt;
    &lt;code&gt;# Scan Reddit data (generates subreddits_complete.json)
python tools/find_banned_subreddits.py /path/to/reddit-data/ --output tools/subreddits_complete.json

# Scan Voat data (generates subverses.json)
python tools/scan_voat_subverses.py /path/to/voat-data/ --output tools/subverses.json

# Scan Ruqqus data (generates guilds.json)
python tools/scan_ruqqus_guilds.py /path/to/ruqqus-data/ --output tools/guilds.json&lt;/code&gt;
    &lt;p&gt;What the scanners do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calculate archive priority scores (0-100) for each community&lt;/item&gt;
      &lt;item&gt;Track post counts, activity periods, deletion rates, NSFW content&lt;/item&gt;
      &lt;item&gt;Identify restricted, quarantined, or banned communities (highest priority)&lt;/item&gt;
      &lt;item&gt;Sort communities by archival importance&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example output:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reddit: 40,029 subreddits from 2.38B posts analyzed&lt;/item&gt;
      &lt;item&gt;Voat: 15,545 subverses from 3.81M posts + 24.1M comments analyzed&lt;/item&gt;
      &lt;item&gt;Ruqqus: 6,217 guilds from 500K posts analyzed&lt;/item&gt;
      &lt;item&gt;Status breakdown (Reddit): 26,552 active, 8,642 restricted, 4,803 inactive, 32 quarantined&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Targeted archiving: Archive high-risk communities first (restricted, quarantined)&lt;/item&gt;
      &lt;item&gt;Storage planning: Identify largest communities before downloading&lt;/item&gt;
      &lt;item&gt;Historical research: Find communities with high deletion/removal rates&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Output files (included in &lt;code&gt;tools/&lt;/code&gt; directory):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;subreddits_complete.json&lt;/code&gt;- Reddit subreddit statistics (40,029 communities, 46MB)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;subverses.json&lt;/code&gt;- Voat subverse statistics (22,585 communities, 14MB)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;guilds.json&lt;/code&gt;- Ruqqus guild statistics (6,217 communities, 3.6MB)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;View the complete data catalog to browse all communities and their priority scores.&lt;/p&gt;
    &lt;p&gt;Ensure DATABASE_URL is set (see Installation above):&lt;/p&gt;
    &lt;code&gt;export DATABASE_URL="postgresql://reddarchiver:password@localhost:5432/reddarchiver"&lt;/code&gt;
    &lt;p&gt;Reddit Archives (.zst files):&lt;/p&gt;
    &lt;code&gt;# Auto-discovery (processes all .zst files in directory)
python reddarc.py /path/to/pushshift-data/ --output my-archive/

# Single subreddit
python reddarc.py /data --subreddit privacy \
  --comments-file /data/privacy_comments.zst \
  --submissions-file /data/privacy_submissions.zst \
  --output my-archive/&lt;/code&gt;
    &lt;p&gt;Voat Archives (SQL dumps):&lt;/p&gt;
    &lt;code&gt;# Import Voat subverses
python reddarc.py /data --subverse voatdev,pics --output my-archive/ --import-only

# Export HTML after import
python reddarc.py /data --output my-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;Ruqqus Archives (.7z files):&lt;/p&gt;
    &lt;code&gt;# Import Ruqqus guilds
python reddarc.py /data --guild Quarantine,News --output my-archive/ --import-only

# Export HTML after import
python reddarc.py /data --output my-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;Multi-Platform Mixed Archive:&lt;/p&gt;
    &lt;code&gt;# Import from multiple platforms into single archive
python reddarc.py /reddit-data --subreddit privacy --output unified-archive/ --import-only
python reddarc.py /voat-data --subverse technology --output unified-archive/ --import-only
python reddarc.py /ruqqus-data --guild Tech --output unified-archive/ --import-only

# Generate HTML for all platforms
python reddarc.py /any-path --output unified-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;With filtering and SEO:&lt;/p&gt;
    &lt;code&gt;python reddarc.py /data/ --output my-archive/ \
  --min-score 100 --min-comments 50 \
  --base-url https://example.com \
  --site-name "My Archive"&lt;/code&gt;
    &lt;p&gt;Import/Export workflow (for large datasets):&lt;/p&gt;
    &lt;code&gt;# Import data to PostgreSQL (no HTML generation)
python reddarc.py /data/ --output my-archive/ --import-only

# Export HTML from PostgreSQL (no data import)
python reddarc.py /data/ --output my-archive/ --export-from-database&lt;/code&gt;
    &lt;p&gt;Multiple deployment options available:&lt;/p&gt;
    &lt;p&gt;Local/Development (HTTP):&lt;/p&gt;
    &lt;code&gt;docker compose up -d
# Access: http://localhost&lt;/code&gt;
    &lt;p&gt;Production HTTPS (Let's Encrypt):&lt;/p&gt;
    &lt;code&gt;./docker/scripts/init-letsencrypt.sh
# Access: https://your-domain.com&lt;/code&gt;
    &lt;p&gt;Homelab/Tor (.onion hidden service):&lt;/p&gt;
    &lt;code&gt;docker compose -f docker-compose.yml -f docker-compose.tor-only.yml --profile tor up -d
# Access: http://[your-address].onion (via Tor Browser)
# No port forwarding or domain required!&lt;/code&gt;
    &lt;p&gt;Dual-Mode (HTTPS + Tor):&lt;/p&gt;
    &lt;code&gt;docker compose --profile production --profile tor up -d
# Access: Both https://your-domain.com and http://[address].onion&lt;/code&gt;
    &lt;p&gt;Static Hosting (GitHub/Codeberg Pages):&lt;/p&gt;
    &lt;code&gt;# Generate archive locally, push to GitHub/Codeberg
python reddarc.py /data --output archive/
cd archive/
git init &amp;amp;&amp;amp; git add . &amp;amp;&amp;amp; git commit -m "Initial archive"
git remote add origin https://github.com/username/repo.git
git push -u origin main
# Enable Pages in repository settings&lt;/code&gt;
    &lt;p&gt;See deployment guides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker Deployment Guide - Complete Docker setup with HTTPS and Tor&lt;/item&gt;
      &lt;item&gt;Tor Deployment Guide - Tor hidden service for homelab and privacy&lt;/item&gt;
      &lt;item&gt;Static Deployment Guide - GitHub Pages / Codeberg Pages (browse-only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Processing Control:&lt;/p&gt;
    &lt;code&gt;--hide-deleted-comments    # Hide [deleted]/[removed] comments in output
--no-user-pages           # Skip user page generation (saves memory)
--dry-run                 # Preview discovered files without processing
--force-rebuild           # Ignore resume state and rebuild from scratch
--force-parallel-users    # Override auto-detection for parallel processing&lt;/code&gt;
    &lt;p&gt;Logging:&lt;/p&gt;
    &lt;code&gt;--log-file &amp;lt;path&amp;gt;         # Custom log file location (default: output/.archive-error.log)
--log-level DEBUG         # Set logging verbosity (DEBUG, INFO, WARNING, ERROR, CRITICAL)&lt;/code&gt;
    &lt;p&gt;Performance Tuning:&lt;/p&gt;
    &lt;code&gt;--debug-memory-limit 8.0      # Override memory limit in GB (default: auto-detect)
--debug-max-connections 8     # Override DB connection pool size (default: auto-detect)
--debug-max-workers 4         # Override parallel workers (default: auto-detect)&lt;/code&gt;
    &lt;p&gt;Environment Variables:&lt;/p&gt;
    &lt;code&gt;# Required
DATABASE_URL=postgresql://user:pass@host:5432/reddarchiver

# Optional Performance Tuning (auto-detected if not set)
REDDARCHIVER_MAX_DB_CONNECTIONS=8       # Connection pool size
REDDARCHIVER_MAX_PARALLEL_WORKERS=4     # Parallel processing workers
REDDARCHIVER_USER_BATCH_SIZE=2000       # User page batch size
REDDARCHIVER_QUEUE_MAX_BATCHES=10       # Queue backpressure control
REDDARCHIVER_CHECKPOINT_INTERVAL=10     # Progress save frequency
REDDARCHIVER_USER_PAGE_WORKERS=4        # User page generation workers&lt;/code&gt;
    &lt;p&gt;Redd-Archiver features a clean modular architecture with specialized components:&lt;/p&gt;
    &lt;code&gt;reddarc.py              # Main CLI entry point
search_server.py        # Flask search API server
version.py              # Version metadata

core/                   # Core processing &amp;amp; database
â”œâ”€â”€ postgres_database.py    # PostgreSQL backend
â”œâ”€â”€ postgres_search.py      # PostgreSQL FTS implementation
â”œâ”€â”€ write_html.py           # HTML generation coordinator
â”œâ”€â”€ watchful.py             # .zst streaming utilities
â”œâ”€â”€ incremental_processor.py # Incremental processing
â””â”€â”€ importers/              # Multi-platform importers
    â”œâ”€â”€ base_importer.py        # Abstract base class
    â”œâ”€â”€ reddit_importer.py      # .zst JSON Lines parser
    â”œâ”€â”€ voat_importer.py        # SQL dump coordinator
    â”œâ”€â”€ voat_sql_parser.py      # SQL INSERT parser
    â””â”€â”€ ruqqus_importer.py      # .7z JSON Lines parser

api/                    # REST API v1
â”œâ”€â”€ __init__.py             # Blueprint registration
â””â”€â”€ routes.py               # 30+ API endpoints

mcp_server/             # MCP Server for AI integration
â”œâ”€â”€ server.py               # FastMCP server (29 tools)
â”œâ”€â”€ README.md               # MCP documentation
â””â”€â”€ tests/                  # MCP server tests

utils/                  # Utility functions
â”œâ”€â”€ console_output.py       # Console output formatting
â”œâ”€â”€ error_handling.py       # Error handling utilities
â”œâ”€â”€ input_validation.py     # Input validation
â”œâ”€â”€ regex_utils.py          # Regular expression utilities
â”œâ”€â”€ search_operators.py     # Search query parsing
â””â”€â”€ simple_json_utils.py    # JSON utilities

processing/             # Data processing modules
â”œâ”€â”€ parallel_user_processing.py  # Parallel user page generation
â”œâ”€â”€ batch_processing_utils.py    # Batch processing utilities
â””â”€â”€ incremental_statistics.py    # Statistics tracking

monitoring/             # Performance &amp;amp; monitoring
â”œâ”€â”€ performance_monitor.py      # Performance monitoring
â”œâ”€â”€ performance_phases.py       # Phase tracking
â”œâ”€â”€ performance_timing.py       # Timing utilities
â”œâ”€â”€ auto_tuning_validator.py    # Auto-tuning validation
â”œâ”€â”€ streaming_config.py         # Auto-detecting configuration
â””â”€â”€ system_optimizer.py         # System optimization
&lt;/code&gt;
    &lt;code&gt;html_modules/
â”œâ”€â”€ html_seo.py                # SEO, meta tags, sitemaps
â”œâ”€â”€ html_pages_jinja.py        # Jinja2-based page generation
â”œâ”€â”€ html_statistics.py         # Analytics and metrics
â”œâ”€â”€ dashboard_helpers.py       # Dashboard utility functions
â”œâ”€â”€ html_field_generation.py   # Dynamic field generation
â”œâ”€â”€ jinja_filters.py           # Custom Jinja2 filters
â”œâ”€â”€ html_pages.py              # Core page generation
â”œâ”€â”€ html_comments.py           # Comment threading system
â”œâ”€â”€ __init__.py                # Public API exports
â”œâ”€â”€ jinja_env.py               # Jinja2 environment setup
â”œâ”€â”€ html_utils.py              # File operations, utilities
â”œâ”€â”€ html_dashboard_jinja.py    # Jinja2 dashboard rendering
â”œâ”€â”€ css_minifier.py            # CSS minification
â”œâ”€â”€ html_scoring.py            # Dynamic score badges
â”œâ”€â”€ html_templates.py          # Template management
â”œâ”€â”€ html_url.py                # URL processing, domains
â”œâ”€â”€ html_dashboard.py          # Dashboard generation
â””â”€â”€ html_constants.py          # Configuration values
&lt;/code&gt;
    &lt;code&gt;templates_jinja2/
â”œâ”€â”€ base/
â”‚   â””â”€â”€ base.html              # Master layout template
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ dashboard_card.html    # Dashboard statistics cards
â”‚   â”œâ”€â”€ footer.html            # Site footer
â”‚   â”œâ”€â”€ global_summary.html    # Global statistics summary
â”‚   â”œâ”€â”€ navigation.html        # Site navigation bar
â”‚   â”œâ”€â”€ post_card.html         # Post display card
â”‚   â”œâ”€â”€ user_comment.html      # User comment display
â”‚   â””â”€â”€ user_post.html         # User post display
â”œâ”€â”€ macros/
â”‚   â”œâ”€â”€ comment_macros.html    # Comment rendering macros
â”‚   â””â”€â”€ reddit_macros.html     # Reddit-specific macros
â””â”€â”€ pages/
    â”œâ”€â”€ global_search.html     # Global search page
    â”œâ”€â”€ index.html             # Dashboard homepage
    â”œâ”€â”€ link.html              # Individual post page
    â”œâ”€â”€ subreddit.html         # Subreddit listing page
    â””â”€â”€ user.html              # User profile page
&lt;/code&gt;
    &lt;code&gt;sql/
â”œâ”€â”€ schema.sql                 # PostgreSQL table definitions
â”œâ”€â”€ indexes.sql                # Performance indexes (GIN, B-tree)
â”œâ”€â”€ fix_statistics.sql         # Statistics maintenance queries
â””â”€â”€ migrations/
    â””â”€â”€ 003_add_total_activity_column.sql  # Schema migration
&lt;/code&gt;
    &lt;p&gt;Redd-Archiver v1.0 uses PostgreSQL full-text search with GIN indexing for blazing-fast search capabilities:&lt;/p&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Database-Powered: Native PostgreSQL indexing with constant memory usage&lt;/item&gt;
      &lt;item&gt;Large-Scale: Efficiently search large datasets (tested with hundreds of GB)&lt;/item&gt;
      &lt;item&gt;Relevance Ranking: PostgreSQL &lt;code&gt;ts_rank()&lt;/code&gt;for intelligent result ordering&lt;/item&gt;
      &lt;item&gt;Highlighted Excerpts: &lt;code&gt;ts_headline()&lt;/code&gt;shows matching content in context&lt;/item&gt;
      &lt;item&gt;Advanced Filters: Search by subreddit, author, date range, score&lt;/item&gt;
      &lt;item&gt;Concurrent Queries: Handle multiple search requests simultaneously&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PostgreSQL search is exposed via &lt;code&gt;postgres_search.py&lt;/code&gt; (CLI) and &lt;code&gt;search_server.py&lt;/code&gt; (Web API):&lt;/p&gt;
    &lt;p&gt;Command-Line Interface:&lt;/p&gt;
    &lt;code&gt;# Search command-line interface
python postgres_search.py "your query" --subreddit technology --limit 50

# Example: Search for posts about "machine learning" with high scores
python postgres_search.py "machine learning" --min-score 100 --limit 20&lt;/code&gt;
    &lt;p&gt;Web API (âœ… Implemented):&lt;/p&gt;
    &lt;code&gt;# Start search server with Docker Compose (recommended)
docker-compose up -d reddarchiver-search-server

# Or run directly
export DATABASE_URL="postgresql://user:pass@localhost:5432/reddarchiver"
python search_server.py

# Access at http://localhost:5000&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RESTful search API with JSON responses&lt;/item&gt;
      &lt;item&gt;Real-time search with PostgreSQL FTS&lt;/item&gt;
      &lt;item&gt;Rate limiting and CSRF protection&lt;/item&gt;
      &lt;item&gt;Health check endpoint: &lt;code&gt;GET /health&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Search endpoint: &lt;code&gt;GET /search?q=query&amp;amp;subreddit=optional&amp;amp;limit=50&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Result highlighting with &lt;code&gt;ts_headline()&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Search suggestions and trending searches&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Full-featured API with 30+ endpoints for programmatic access and MCP/AI integration:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Endpoints&lt;/cell&gt;
        &lt;cell role="head"&gt;Key Features&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;System (5)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/health&lt;/code&gt;, &lt;code&gt;/stats&lt;/code&gt;, &lt;code&gt;/schema&lt;/code&gt;, &lt;code&gt;/openapi.json&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Health checks, statistics, capability discovery, OpenAPI spec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Posts (13)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/posts&lt;/code&gt;, &lt;code&gt;/posts/{id}&lt;/code&gt;, &lt;code&gt;/posts/{id}/comments&lt;/code&gt;, &lt;code&gt;/posts/{id}/context&lt;/code&gt;, &lt;code&gt;/posts/{id}/comments/tree&lt;/code&gt;, &lt;code&gt;/posts/{id}/related&lt;/code&gt;, &lt;code&gt;/posts/random&lt;/code&gt;, &lt;code&gt;/posts/aggregate&lt;/code&gt;, &lt;code&gt;/posts/batch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, single, comments, context, tree, related, random, aggregate, batch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Comments (7)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/comments&lt;/code&gt;, &lt;code&gt;/comments/{id}&lt;/code&gt;, &lt;code&gt;/comments/random&lt;/code&gt;, &lt;code&gt;/comments/aggregate&lt;/code&gt;, &lt;code&gt;/comments/batch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, single, random, aggregate, batch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Users (8)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/users&lt;/code&gt;, &lt;code&gt;/users/{username}&lt;/code&gt;, &lt;code&gt;/users/{username}/summary&lt;/code&gt;, &lt;code&gt;/users/{username}/posts&lt;/code&gt;, &lt;code&gt;/users/{username}/comments&lt;/code&gt;, &lt;code&gt;/users/aggregate&lt;/code&gt;, &lt;code&gt;/users/batch&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, profiles, summary, activity, aggregate, batch&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Subreddits (4)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/subreddits&lt;/code&gt;, &lt;code&gt;/subreddits/{name}&lt;/code&gt;, &lt;code&gt;/subreddits/{name}/summary&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;List, statistics, summary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Search (3)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;/search&lt;/code&gt;, &lt;code&gt;/search/explain&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Full-text search with operators, query debugging&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;MCP/AI-Optimized Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Field Selection: &lt;code&gt;?fields=id,title,score&lt;/code&gt;for token optimization&lt;/item&gt;
      &lt;item&gt;Truncation Controls: &lt;code&gt;?max_body_length=500&amp;amp;include_body=false&lt;/code&gt;for response size management&lt;/item&gt;
      &lt;item&gt;Export Formats: &lt;code&gt;?format=csv|ndjson&lt;/code&gt;for data analysis&lt;/item&gt;
      &lt;item&gt;Batch Endpoints: Reduce N requests to 1 with &lt;code&gt;/posts|comments|users/batch&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Context Endpoints: Single-call discussion retrieval with &lt;code&gt;/posts/{id}/context&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Search Operators: Google-style syntax (&lt;code&gt;"exact"&lt;/code&gt;,&lt;code&gt;OR&lt;/code&gt;,&lt;code&gt;-exclude&lt;/code&gt;,&lt;code&gt;sub:&lt;/code&gt;,&lt;code&gt;author:&lt;/code&gt;,&lt;code&gt;score:&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rate limited to 100 requests/minute. See API Documentation for complete reference.&lt;/p&gt;
    &lt;p&gt;Redd-Archiver supports a distributed registry system for tracking archive instances:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instance Metadata: Configure via environment variables or CLI flags (&lt;code&gt;--site-name&lt;/code&gt;,&lt;code&gt;--contact&lt;/code&gt;,&lt;code&gt;--team-id&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Leaderboard Generator: Automated scoring based on archive completeness and content risk&lt;/item&gt;
      &lt;item&gt;Team Grouping: Group multiple instances under a team ID for coordinated archiving&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See Registry Setup Guide for configuration.&lt;/p&gt;
    &lt;p&gt;Constant Memory Usage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;4GB RAM: Process large datasets efficiently (tested with hundreds of GB)&lt;/item&gt;
      &lt;item&gt;8GB RAM: Optimal for concurrent operations&lt;/item&gt;
      &lt;item&gt;16GB+ RAM: Ideal for parallel user page generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Database Storage:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Input (.zst)&lt;/cell&gt;
        &lt;cell role="head"&gt;PostgreSQL DB&lt;/cell&gt;
        &lt;cell role="head"&gt;HTML Output&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;93.6MB&lt;/cell&gt;
        &lt;cell&gt;~150MB&lt;/cell&gt;
        &lt;cell&gt;1.4GB&lt;/cell&gt;
        &lt;cell&gt;r/technology&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;100MB&lt;/cell&gt;
        &lt;cell&gt;~160MB&lt;/cell&gt;
        &lt;cell&gt;~1.5GB&lt;/cell&gt;
        &lt;cell&gt;Small archives&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;500MB&lt;/cell&gt;
        &lt;cell&gt;~800MB&lt;/cell&gt;
        &lt;cell&gt;~7.5GB&lt;/cell&gt;
        &lt;cell&gt;Research projects&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2GB&lt;/cell&gt;
        &lt;cell&gt;~3.2GB&lt;/cell&gt;
        &lt;cell&gt;~30GB&lt;/cell&gt;
        &lt;cell&gt;Large collections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;100GB&lt;/cell&gt;
        &lt;cell&gt;~160GB&lt;/cell&gt;
        &lt;cell&gt;~1.5TB&lt;/cell&gt;
        &lt;cell&gt;Enterprise-scale&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Processing Speed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data Import: Fast streaming ingestion to PostgreSQL&lt;/item&gt;
      &lt;item&gt;HTML Generation: Efficient database-backed rendering&lt;/item&gt;
      &lt;item&gt;Search Index: Instant with PostgreSQL GIN indexes&lt;/item&gt;
      &lt;item&gt;Performance: Scales with dataset size, optimized for large archives&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance varies based on dataset size, query complexity, and hardware:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL FTS: Fast indexed search for large datasets&lt;/item&gt;
      &lt;item&gt;GIN Indexes: Optimized index lookups for text search&lt;/item&gt;
      &lt;item&gt;Concurrent Queries: Supports multiple simultaneous searches with connection pooling&lt;/item&gt;
      &lt;item&gt;Memory Efficient: Constant memory usage with streaming results&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PostgreSQL v1.0 Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large-Scale Processing: Efficiently handle large datasets (tested with hundreds of GB)&lt;/item&gt;
      &lt;item&gt;Constant Memory: 4GB RAM regardless of dataset size&lt;/item&gt;
      &lt;item&gt;Fast Search: PostgreSQL FTS with GIN indexing&lt;/item&gt;
      &lt;item&gt;Resume Capability: Database-backed progress tracking&lt;/item&gt;
      &lt;item&gt;Concurrent Processing: Multi-connection pool for parallel operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver has been tested with archives up to hundreds of gigabytes. For optimal performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tested scale: Hundreds of GB per instance&lt;/item&gt;
      &lt;item&gt;Memory usage: Constant 4GB RAM regardless of dataset size&lt;/item&gt;
      &lt;item&gt;Database: PostgreSQL handles large datasets efficiently&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For very large archive collections (multiple terabytes), deploy multiple instances divided by topic:&lt;/p&gt;
    &lt;p&gt;Architecture:&lt;/p&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Instance 1     â”‚     â”‚  Instance 2     â”‚     â”‚  Instance 3     â”‚
â”‚  Technology     â”‚     â”‚  Gaming         â”‚     â”‚  Science        â”‚
â”‚  Subreddits     â”‚     â”‚  Subreddits     â”‚     â”‚  Subreddits     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Efficient search: Each database stays manageable size&lt;/item&gt;
      &lt;item&gt;Distributed load: Parallel processing across instances&lt;/item&gt;
      &lt;item&gt;Topic organization: Logical grouping of related content&lt;/item&gt;
      &lt;item&gt;Independent scaling: Scale individual topics as needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Deployment Options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Single server: Multiple Docker Compose stacks with different ports&lt;/item&gt;
      &lt;item&gt;Multiple servers: One instance per physical/virtual machine&lt;/item&gt;
      &lt;item&gt;Topic-based domains: tech.archive.com, gaming.archive.com, etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example Multi-Instance Setup:&lt;/p&gt;
    &lt;code&gt;# Instance 1: Technology topics (port 8080)
cd /archives/tech
docker compose up -d

# Instance 2: Gaming topics (port 8081)
cd /archives/gaming
docker compose -f docker-compose.yml up -d

# Instance 3: Science topics (port 8082)
cd /archives/science
docker compose -f docker-compose.yml up -d&lt;/code&gt;
    &lt;p&gt;When to Use:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Archive collection exceeds 500GB&lt;/item&gt;
      &lt;item&gt;Search performance degrades with single instance&lt;/item&gt;
      &lt;item&gt;Logical topic divisions exist in your archive&lt;/item&gt;
      &lt;item&gt;Want to distribute load across multiple servers&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Studying online discourse and community dynamics&lt;/item&gt;
      &lt;item&gt;Analyzing social movements and trends&lt;/item&gt;
      &lt;item&gt;Preserving internet culture&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Backing up subreddits before potential removal&lt;/item&gt;
      &lt;item&gt;Creating offline-accessible community resources&lt;/item&gt;
      &lt;item&gt;Distributing knowledge repositories&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pattern analysis in deleted/removed content&lt;/item&gt;
      &lt;item&gt;User behavior studies&lt;/item&gt;
      &lt;item&gt;Content moderation research&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Docker Deployment Guide - Complete Docker setup including PostgreSQL, nginx, HTTPS, and Tor&lt;/item&gt;
      &lt;item&gt;Tor Deployment Guide - Tor hidden service setup for homelab and privacy deployments&lt;/item&gt;
      &lt;item&gt;Static Deployment Guide - GitHub Pages and Codeberg Pages deployment (browse-only, no search)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;REST API Documentation - Complete API reference with 30+ endpoints&lt;/item&gt;
      &lt;item&gt;MCP Server Documentation - AI integration with Claude Desktop/Claude Code&lt;/item&gt;
      &lt;item&gt;Registry Setup Guide - Instance registry configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CONTRIBUTING.md - Development guidelines and contribution procedures&lt;/item&gt;
      &lt;item&gt;SECURITY.md - Security policy and vulnerability reporting&lt;/item&gt;
      &lt;item&gt;LICENSE - Unlicense (public domain)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please see CONTRIBUTING.md for development guidelines, code structure, and testing procedures.&lt;/p&gt;
    &lt;p&gt;Key areas for contribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PostgreSQL query optimizations&lt;/item&gt;
      &lt;item&gt;Additional export formats&lt;/item&gt;
      &lt;item&gt;Enhanced search features&lt;/item&gt;
      &lt;item&gt;Documentation improvements&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See our modular architecture (18 specialized modules) for easy entry points to contribute.&lt;/p&gt;
    &lt;p&gt;This is free and unencumbered software released into the public domain. See the LICENSE file (Unlicense) for details.&lt;/p&gt;
    &lt;p&gt;Anyone is free to copy, modify, publish, use, compile, sell, or distribute this software for any purpose, commercial or non-commercial, and by any means.&lt;/p&gt;
    &lt;p&gt;This project leverages public datasets from the following sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pushshift - Reddit data access and archival infrastructure&lt;/item&gt;
      &lt;item&gt;Watchful1's PushshiftDumps - Comprehensive data dump tools and torrent management&lt;/item&gt;
      &lt;item&gt;Arctic Shift - Making Reddit data accessible to researchers and the public&lt;/item&gt;
      &lt;item&gt;Ruqqus Public Dataset - 752 MB Ruqqus archive (comments and submissions)&lt;/item&gt;
      &lt;item&gt;SearchVoat Archive - 16.8 GB Voat.co complete backup&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project builds upon the work of several excellent archival projects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reddit-html-archiver by libertysoft3 - Original inspiration and foundation for static HTML generation&lt;/item&gt;
      &lt;item&gt;redarc - Self-hosted Reddit archiving with PostgreSQL and full-text search&lt;/item&gt;
      &lt;item&gt;red-arch - Static website generator for Reddit subreddit archives&lt;/item&gt;
      &lt;item&gt;zst_blocks_format - Efficient block-based compression format for processing large datasets&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub Issues: Report bugs or request features&lt;/item&gt;
      &lt;item&gt;GitHub Discussions: Ask questions or share ideas&lt;/item&gt;
      &lt;item&gt;Security Issues: Report via GitHub Security Advisories&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Redd-Archiver was built by one person over 6 months as a labor of love to preserve internet history before it disappears forever.&lt;/p&gt;
    &lt;p&gt;This isn't backed by a company or institutionâ€”just an individual committed to keeping valuable discussions accessible. Your support helps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Continue development and bug fixes&lt;/item&gt;
      &lt;item&gt;Maintain documentation and support&lt;/item&gt;
      &lt;item&gt;Cover infrastructure costs (servers, storage, bandwidth)&lt;/item&gt;
      &lt;item&gt;Preserve more data sources and platforms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Every donation, no matter the size, helps keep this preservation effort alive.&lt;/p&gt;
    &lt;code&gt;bc1q8wpdldnfqt3n9jh2n9qqmhg9awx20hxtz6qdl7
&lt;/code&gt;
    &lt;code&gt;42zJZJCqxyW8xhhWngXHjhYftaTXhPdXd9iJ2cMp9kiGGhKPmtHV746EknriN4TNqYR2e8hoaDwrMLfv7h1wXzizMzhkeQi
&lt;/code&gt;
    &lt;p&gt;Thank you for supporting internet archival efforts! Every contribution helps maintain and improve this project.&lt;/p&gt;
    &lt;p&gt;This software is provided "as is" under the Unlicense. See LICENSE for details. Users are responsible for compliance with applicable laws and terms of service when processing data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46602324</guid><pubDate>Tue, 13 Jan 2026 15:35:09 +0000</pubDate></item><item><title>What a year of solar and batteries saved us in 2025</title><link>https://scotthelme.co.uk/what-a-year-of-solar-and-batteries-really-saved-us-in-2025/</link><description>&lt;doc fingerprint="af53e052cbb4a5d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Throughout 2025, I spoke a few times about our home energy solution, including our grid usage, our solar array and our Tesla Powerwall batteries. Now that I have a full year of data, I wanted to take a look at exactly how everything is working out, and, in alignment with our objectives, how much money we've saved!&lt;/p&gt;
    &lt;head rend="h4"&gt;Our setup&lt;/head&gt;
    &lt;p&gt;Just to give a quick overview of what we're working with, here are the details on our solar, battery and tariff situation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;â˜€ï¸Solar Panels: We have 14x Perlight solar panels managed by Enphase that make up the 4.2kWp array on our roof, and they produce energy when the sun shines, which isn't as often as I'd like in the UK!&lt;/item&gt;
      &lt;item&gt;ğŸ”‹Tesla Powerwalls: We have 3x Tesla Powerwall 2 in our garage that were purchased to help us load-shift our energy usage. Electricity is very expensive in the UK and moving from peak usage which is 05:30 to 23:30 at ~Â£0.28/kWh, to off-peak usage, which is 23:30 - 05:30 at ~Â£0.07/kWh, is a significant cost saving.&lt;/item&gt;
      &lt;item&gt;ğŸ’¡Smart Tariff: My wife and I both drive electric cars and our electricity provider, Octopus Energy, has a Smart Charging tariff. If we plug in one of our cars, and cheap electricity is available, they will activate the charger and allow us to use the off-peak rate, even at peak times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that we have some basic info, let's get into the details!&lt;/p&gt;
    &lt;head rend="h4"&gt;Grid Import&lt;/head&gt;
    &lt;p&gt;I have 3 sources of data for our grid import, and all of them align pretty well in terms of their measurements. I have the amount our electricity supplier charged us for, I have my own CT Clamp going via a Shelly EM that feeds in to Home Assistant, and I have the Tesla Gateway which controls all grid import into our home.&lt;/p&gt;
    &lt;p&gt;Starting with my Home Assistant data, these are the relevant readings.&lt;/p&gt;
    &lt;p&gt;Jan 1st 2025 - 15,106.10 kWh&lt;lb/&gt;Dec 31st 2025 - 36,680.90 kWh&lt;lb/&gt;Total: 21,574.80 kWh&lt;lb/&gt;Total Import: 21.6 MWh&lt;/p&gt;
    &lt;p&gt;As you can see in the graph, during the summer months we have slightly lower grid usage and the graph line climbs at a lower rate, but overall, we have pretty consistent usage. Looking at what our energy supplier charged, us for, that comes in slightly lower.&lt;/p&gt;
    &lt;p&gt;Total Import: 20.1 MWh&lt;/p&gt;
    &lt;p&gt;I'm going to use the figure provided by our energy supplier in my calculations because their equipment is likely more accurate than mine, and also, what they're charging me is the ultimate thing that matters. The final source is our Tesla Gateway, which shows us having imported 21.0 MWh.&lt;/p&gt;
    &lt;p&gt;It's great to see how all of these sources of data align so poorly! ğŸ˜…&lt;/p&gt;
    &lt;head rend="h4"&gt;Grid Export&lt;/head&gt;
    &lt;p&gt;Looking at our export, the graph tells a slightly different story because, as you can see, we didn't really start exporting properly until June, when our export tariff was activated. Prior to June, it simply wasn't worth exporting as we were only getting Â£0.04/kWh but at the end of May, our export tariff went live and we were then getting paid Â£0.15/kWh for export. My first and second blog posts cover the full details of this change when it happened if you'd like to read them but for now, just note that it will change the calculations a little later as we only had export for 60% of the year.&lt;/p&gt;
    &lt;p&gt;Total Export: 6.0 MWh&lt;/p&gt;
    &lt;p&gt;With our grid export covered the final piece of the puzzle is to look at our solar.&lt;/p&gt;
    &lt;head rend="h4"&gt;Solar Production&lt;/head&gt;
    &lt;p&gt;We're really not in the best part of the world for generating solar power, but we've still managed to produce quite a bit of power. Even in the most ideal, perfect scenario, our solar array can only generate 4.2kW of power, and we're definitely never getting near that. Our peak production was 2.841kW on 8th July at 13:00, and you can see our full annual production graph here.&lt;/p&gt;
    &lt;p&gt;Looking at the total energy production for the entire array, you can see it pick up through the sunnier months but remain quite flat during the darker days of the year.&lt;/p&gt;
    &lt;p&gt;Jan 1st 2025 - 2.709 MWh&lt;lb/&gt;Dec 31st 2025 - 5.874 MWh&lt;lb/&gt;Solar Production: 3.2 MWh&lt;/p&gt;
    &lt;p&gt;Just to confirm, I also took a look at the Enphase app, which is drawing it's data from the same source to be fair, and it agrees with the 3.2 MWh of generation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Calculating the savings&lt;/head&gt;
    &lt;p&gt;This isn't exactly straightforward because of the combination of our solar array and excess import/export due to the batteries, but here are the numbers I'm currently working on.&lt;/p&gt;
    &lt;p&gt;Total Import: 20.1 MWh&lt;lb/&gt;Total Export: 6.0 MWh&lt;lb/&gt;Solar Production: 3.2 MWh&lt;/p&gt;
    &lt;p&gt;That gives us a total household usage of 17.3 MWh.&lt;/p&gt;
    &lt;p&gt;(20.1 MWh import + 3.2 MWh solar) âˆ’ 6.0 MWh export = 17.3 MWh usage&lt;/p&gt;
    &lt;p&gt;If we didn't have the solar array providing power, the full 17.3 MWh of consumption would have been chargeable from our provider. If we had only the solar and no battery, assuming a perfect ability to utilise our solar generation, only 14.1 MWh of our usage would need to be imported. The cost of those units of solar generation can be viewed at the peak and off-peak rates as follows.&lt;/p&gt;
    &lt;p&gt;Peak rate: 3,200 kWh x Â£0.28/kWh = Â£896&lt;lb/&gt;Off-peak rate: 3,200 kWh x Â£0.07/kWh = Â£224&lt;/p&gt;
    &lt;p&gt;Given that solar panels only produce during peak electricity rates, it would be reasonable to use the higher price here. A consideration for us though is that we do have batteries, and we're able to load-shift all of our usage into the off-peak rate, so arguably the solar panels only made Â£224 of electricity.&lt;/p&gt;
    &lt;p&gt;The bigger savings come when we start to look at the cost of the grid import. Assuming we had no solar panels, we'd have imported 17.3 MWh of electricity, and with the solar panels and perfect utilisation, we'd have imported 14.1 MWh of electricity. That's quite a lot of electricity and calculating the different costs of peak vs. off-peak by using batteries to load shift our usage gives some quite impressive results.&lt;/p&gt;
    &lt;p&gt;Peak rate: 17,300 kWh x Â£0.28/kWh = Â£4,844&lt;lb/&gt;Peak rate with solar: 14,100 kWh x Â£0.28 = Â£3,948&lt;/p&gt;
    &lt;p&gt;Off-peak rate: 17,300 kWh x Â£0.07/kWh = Â£1,211&lt;lb/&gt;Off-peak rate with solar: 14,100 kWh x Â£0.07/kWh = Â£987&lt;/p&gt;
    &lt;p&gt;This means there's a potential swing from Â£4,844 down to Â£987 with solar and battery, a total potential saving of Â£3,857!&lt;/p&gt;
    &lt;p&gt;This also tracks if we look at our monthly spend on electricity which went from Â£350-Â£400 per month down to Â£50-Â£100 per month depending on the time of year. But it gets better.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exporting excess energy&lt;/head&gt;
    &lt;p&gt;Our solar array generates almost nothing in the winter months so our batteries are sized to allow for a full day of usage with basically no solar support. We can go from the start of the peak rate at 05:30 all the way to the off-peak rate at 23:30 without using any grid power. When it comes to the summer months, though, our solar array is producing a lot of power and we clearly have a capability to export a lot more. The batteries can fill up on the off-peak rate overnight at Â£0.07/kWh, and then export it during the peak rate for Â£0.15/kWh, meaning any excess solar production or battery capacity can be exported for a reasonable amount.&lt;/p&gt;
    &lt;p&gt;If we take a look at the billing information from our energy supplier, we can see that during July, our best month for solar production, we exported a lot of energy. We exported so much energy that it actually fully offset our electricity costs and allowed us to go negative, meaning we were earning money back.&lt;/p&gt;
    &lt;p&gt;Here is our electricity import data:&lt;/p&gt;
    &lt;p&gt;And here is our electricity export data:&lt;/p&gt;
    &lt;p&gt;That's a pretty epic scenario, despite us being such high energy consumers, to still have the ability to fully cover our costs and even earn something back! For clarity, we will still have the standing charge component of our bill, which is Â£0.45/day so about Â£13.50 per month to go on any given month, but looking at the raw energy costs, it's impressive.&lt;/p&gt;
    &lt;head rend="h4"&gt;The final calculation&lt;/head&gt;
    &lt;p&gt;I pulled all of our charges for electricity in 2025 to see just how close my calculations were and to double check everything I was thinking. Earlier, I gave these figures:&lt;/p&gt;
    &lt;p&gt;Off-peak rate: 17,300 kWh x Â£0.07/kWh = Â£1,211&lt;/p&gt;
    &lt;p&gt;If 100% of our electricity usage was at the off-peak rate, we should have paid Â£1,211 for the year. Adding up all of our monthly charges, our total for the year was Â£1,608.11 all in, but we need to subtract our standing charge from that.&lt;/p&gt;
    &lt;p&gt;Total cost = Â£1,608.11 - (365 * Â£0.45)&lt;lb/&gt;Total import = Â£1,443.86&lt;/p&gt;
    &lt;p&gt;This means that we got almost all of our usage at the off-peak rate which is an awesome achievement! After the charges for electricity, I then tallied up all of our payments for export.&lt;/p&gt;
    &lt;p&gt;Total export = Â£886.49&lt;/p&gt;
    &lt;p&gt;Another pretty impressive achievement, earning so much in export, which also helps to bring our net electricity cost in 2025 to Â£557.37! To put this another way, the effective rate of our electricity is now just Â£0.03/kWh.&lt;/p&gt;
    &lt;p&gt;Â£557.37 / 17,300kWh = Â£0.03/kWh&lt;/p&gt;
    &lt;head rend="h4"&gt;But was it all worth it?&lt;/head&gt;
    &lt;p&gt;That's a tricky question to answer, and everyone will have different objectives and desired outcomes, but ours was pretty clear. Running two Electric Vehicles, having two adults working from home full time, me having servers and equipment at home, along with a power hungry hot tub, we were spending too much per month in electricity alone, and our goal was to reduce that.&lt;/p&gt;
    &lt;p&gt;Of course, it only makes sense to spend money reducing our costs if we reduce them enough to pay back the investment in the long term, and things are looking good so far. Here are the costs for our installations:&lt;/p&gt;
    &lt;p&gt;Â£17,580 - Powerwalls #1 and #2 installed.&lt;lb/&gt;Â£13,940 - Solar array installed.&lt;lb/&gt;Â£7,840 - Powerwall #3 installed.&lt;lb/&gt;Total cost = Â£39,360&lt;/p&gt;
    &lt;p&gt;If we assume even a generous 2/3 - 1/3 split between peak and off-peak usage, with no Powerwalls or solar array, our electricity costs for 2025 would have been Â£3,632.86:&lt;/p&gt;
    &lt;p&gt;11,533 kWh x Â£0.28/kWh = Â£3,229.24&lt;lb/&gt;5,766 kWh x Â£0.07/kWh = Â£403.62&lt;lb/&gt;Total = Â£3,632.86&lt;/p&gt;
    &lt;p&gt;Instead, our costs were only Â£557.37, meaning we saved Â£3,078.49 this year. We also only had export capabilities for 7 months of 2025, so in 2026 when we will have 12 months of export capabilities, we should further reduce our costs. I anticipate that in 2026 our electricity costs for the year will be ~Â£0, and that's our goal.&lt;/p&gt;
    &lt;p&gt;Having our full costs returned in ~11 years is definitely something we're happy with, and we've also had protection against several power outages in our area along the way, which is a very nice bonus. Another way to look at this is that the investment is returning ~9%/year.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Year&lt;/cell&gt;
        &lt;cell role="head"&gt;Cumulative savings (Â£)&lt;/cell&gt;
        &lt;cell role="head"&gt;ROI (%)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;3,632.86&lt;/cell&gt;
        &lt;cell&gt;9.23%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;7,265.72&lt;/cell&gt;
        &lt;cell&gt;18.46%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;10,898.58&lt;/cell&gt;
        &lt;cell&gt;27.69%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;14,531.44&lt;/cell&gt;
        &lt;cell&gt;36.92%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;18,164.30&lt;/cell&gt;
        &lt;cell&gt;46.15%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;21,797.16&lt;/cell&gt;
        &lt;cell&gt;55.38%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;25,430.02&lt;/cell&gt;
        &lt;cell&gt;64.61%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;29,062.88&lt;/cell&gt;
        &lt;cell&gt;73.84%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;32,695.74&lt;/cell&gt;
        &lt;cell&gt;83.07%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;36,328.60&lt;/cell&gt;
        &lt;cell&gt;92.30%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;54,492.90&lt;/cell&gt;
        &lt;cell&gt;138.43%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;72,657.20&lt;/cell&gt;
        &lt;cell&gt;184.61%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;90,821.50&lt;/cell&gt;
        &lt;cell&gt;230.76%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Of course, at some point during that period, the effective value of the installation will reduce to almost Â£0, and we have to consider that, but it's doing pretty darn good. If we hadn't needed to add that third Powerwall, this would have been so much better too. We'll see what the future holds, but with the inevitable and continued rise of energy costs, and talk of moving the standing charge on to our unit rate, things might look even better in the future.&lt;/p&gt;
    &lt;head rend="h4"&gt;Onwards to 2026!&lt;/head&gt;
    &lt;p&gt;Now that we have everything properly set up, and I'm happy with all of our Home Assistant automations, we're going to see how 2026 goes. I will definitely circle back in a year from now and see how the numbers played out, and until then, I hope the information here has been useful or interesting ğŸ‘&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46602532</guid><pubDate>Tue, 13 Jan 2026 15:49:27 +0000</pubDate></item><item><title>Are two heads better than one?</title><link>https://eieio.games/blog/two-heads-arent-better-than-one/</link><description>&lt;doc fingerprint="e262c3369163d09e"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Are Two Heads Better Than One?&lt;/head&gt;
    &lt;p&gt;Three heads are certainly more fun&lt;/p&gt;
    &lt;p&gt;Dec 9, 2025&lt;/p&gt;
    &lt;p&gt;Youâ€™re playing a game with your lying friends Alice and Bob.&lt;/p&gt;
    &lt;p&gt;Bob flips a coin and shows it to Alice. Alice tells you what she saw - but she lies 20% of the time. Then you take your best guess on whether the coin is heads or tails.&lt;/p&gt;
    &lt;p&gt;Your best strategy is to trust whatever Alice says. Youâ€™re right 80% of the time.&lt;/p&gt;
    &lt;p&gt;Now Bob joins in. He makes up his mind independent of Alice, and he also lies 20% of the time 1.&lt;/p&gt;
    &lt;p&gt;Your friends are all liars!&lt;/p&gt;
    &lt;p&gt;You were right 80% of the time by trusting Alice.&lt;/p&gt;
    &lt;p&gt;How much better can you do with Bobâ€™s help?&lt;/p&gt;
    &lt;head rend="h2"&gt;Hereâ€™s some empty space for you to think&lt;/head&gt;
    &lt;p&gt;Iâ€™m going to give you the answer below. So hereâ€™s some empty space for you to think in case you want to do the math yourself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alright, letâ€™s do some math&lt;/head&gt;
    &lt;p&gt;The answer is 0% - you donâ€™t do any better! Youâ€™re still exactly 80% to get the right answer.&lt;/p&gt;
    &lt;p&gt;To establish this, letâ€™s write a simple simulation. Weâ€™ll flip a coin a million times, ask our friends what they saw, and observe the results.&lt;/p&gt;
    &lt;p&gt;For our strategy, weâ€™ll look at a fact pattern (like â€œAlice says headsâ€), figure out whatâ€™s most likely (â€œthe coin is headsâ€), and say â€œwe guess the coin flip correctly whenever the most likely outcome occurs for this fact patternâ€ 2.&lt;/p&gt;
    &lt;p&gt;â€œGuess the most likely outcomeâ€ is optimal here, but it is very much not optimal if this game was adversarial. Itâ€™s important that Alice and Bob arenâ€™t trying to trick us and that theyâ€™re deciding independently.&lt;/p&gt;
    &lt;p&gt;Hereâ€™s the code for that simulation. Weâ€™ll start with the easy case (just Alice):&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;The simulation code&lt;/head&gt;
    &lt;code&gt;# heads.py

from random import random
from collections import defaultdict

table = defaultdict(lambda: [0, 0])
LYING_PROB = 0.2
LYING_FRIENDS = ["Alice"]
ITERATIONS = 1_000_000

for _ in range(ITERATIONS):
    is_heads = random() &amp;gt; 0.5
    keys = []
    for lying_friend in LYING_FRIENDS:
        lied = random() &amp;lt; LYING_PROB
        answer = None
        if is_heads: answer = "T" if lied else "H"
        else: answer = "H" if lied else "T"
        keys.append(f"{lying_friend[0]}:{answer}")

    key = ", ".join(keys)

    table_idx = 0 if is_heads else 1
    table[key][table_idx] += 1

total_times_we_are_right = 0
for key, (times_heads, times_tails) in table.items():
    total = times_heads + times_tails
    heads_chance = 100 * round(times_heads / total, 2)
    tails_chance = 100 * round(times_tails / total, 2)
    pattern_chance = 100 * round(total / ITERATIONS, 2)

    print(f"{key} - chances -  H {heads_chance:4}% | T {tails_chance:4}% | occurs {pattern_chance}% of the time")

    # We look at key, and guess whichever outcome is more likely. So we're right, on average,
    # the max of times_heads and times_tails
    total_times_we_are_right += max(times_heads, times_tails)

accuracy = round(total_times_we_are_right / ITERATIONS, 2)
print(f"\nOur accuracy: {100*accuracy}%")
&lt;/code&gt;
    &lt;p&gt;This gives us:&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:T - chances -  H 20.0% | T 80.0% | occurs 50.0% of the time
A:H - chances -  H 80.0% | T 20.0% | occurs 50.0% of the time

Our accuracy: 80.0%
&lt;/code&gt;
    &lt;p&gt;Now letâ€™s add Bob to the simulation. We see something like this:&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:T, B:T - chances -  H  6.0% | T 94.0% | occurs 34.0% of the time
A:H, B:T - chances -  H 50.0% | T 50.0% | occurs 16.0% of the time
A:H, B:H - chances -  H 94.0% | T  6.0% | occurs 34.0% of the time
A:T, B:H - chances -  H 50.0% | T 50.0% | occurs 16.0% of the time

Our accuracy: 80.0%
&lt;/code&gt;
    &lt;p&gt;Thatâ€™s weird! But perhaps this gives you an intuition for whatâ€™s happening. By introducing a second player, we introduce the possibility of a tie.&lt;/p&gt;
    &lt;p&gt;A decent amount of the time, Alice and Bob agree. Most (~94%) of the time when that happens, theyâ€™re telling the truth. Occasionally theyâ€™re both lying, but thatâ€™s pretty unlikely.&lt;/p&gt;
    &lt;p&gt;But a meaningful portion of the time (32%) Alice says heads and Bob says tails, or vice versa. And in that case we donâ€™t know anything at all! Alice and Bob are equally trustworthy and they disagreed - weâ€™d be better off if weâ€™d just gone and asked Alice 3!&lt;/p&gt;
    &lt;p&gt;I am deeply curious whether anyone else was read the book â€œGo Ask Aliceâ€ by their middle school science teacher in order to scare them straight or whether that was specific to my middle school experience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Letâ€™s prove it&lt;/head&gt;
    &lt;p&gt;Now that weâ€™ve simulated this result, letâ€™s walk through each case assuming that the coin landed on heads.&lt;/p&gt;
    &lt;code&gt;- both tell the truth
Alice: Heads (80%), Bob: Heads (80%)
happens 80% * 80% = 64% of the time
we always guess correctly in this case

- both lie
Alice: Tails (20%), Bob: Tails (20%)
happens 20% * 20% = 4% of the time
we never guess correctly in this case

- alice tells the truth, bob lies
Alice: Heads (80%), Bob: Tails (20%)
happens 80% * 20% = 16% of the time
we guess at random in this case; we're right 50% of the time

- alice lies, bob tells the truth
Alice: Tails (20%), Bob: Heads (80%)
happens 20% * 80% = 16% of the time
we guess at random in this case; we're right 50% of the time

Our total chance to guess correctly is:
64% + 16% / 2 + 16% / 2 = 64% + 8% + 8% = 80%
&lt;/code&gt;
    &lt;p&gt;Thereâ€™s something beautiful here. Our total chance to guess remains at 80% because our additional chance to guess correctly when Alice and Bob agree is perfectly offset by the chance that Alice and Bob disagree!&lt;/p&gt;
    &lt;head rend="h3"&gt;Meet Charlie (and David)&lt;/head&gt;
    &lt;p&gt;If our friend Charlie - who also lies 20% of the time - joins the fun, our odds improve substantially. If Bob and Alice disagree, Charlie can act as a tiebreaker.&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:H, B:H, C:H - chances -  H 98.0% | T  2.0% | occurs 26.0% of the time
A:T, B:T, C:T - chances -  H  2.0% | T 98.0% | occurs 26.0% of the time
A:T, B:H, C:H - chances -  H 80.0% | T 20.0% | occurs 8.0% of the time
A:H, B:T, C:T - chances -  H 20.0% | T 80.0% | occurs 8.0% of the time
A:H, B:H, C:T - chances -  H 80.0% | T 20.0% | occurs 8.0% of the time
A:H, B:T, C:H - chances -  H 80.0% | T 20.0% | occurs 8.0% of the time
A:T, B:T, C:H - chances -  H 20.0% | T 80.0% | occurs 8.0% of the time
A:T, B:H, C:T - chances -  H 20.0% | T 80.0% | occurs 8.0% of the time

Our accuracy: 90.0%
&lt;/code&gt;
    &lt;p&gt;But if David joins, the pattern repeats. David introduces the possibility of a 2-2 split, and our odds donâ€™t improve at all!&lt;/p&gt;
    &lt;code&gt;% python heads.py
A:T, B:T, C:T, D:T - chances -  H  0.0% | T 100.0% | occurs 21.0% of the time
A:T, B:H, C:H, D:H - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:T, B:H, C:T, D:T - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:H, C:H, D:H - chances -  H 100.0% | T  0.0% | occurs 21.0% of the time
A:H, B:T, C:H, D:T - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:T, B:T, C:H, D:H - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:H, B:T, C:H, D:H - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:T, B:T, C:T, D:H - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:T, C:T, D:T - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:H, C:H, D:T - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:H, B:H, C:T, D:T - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:T, B:T, C:H, D:T - chances -  H  6.0% | T 94.0% | occurs 5.0% of the time
A:H, B:H, C:T, D:H - chances -  H 94.0% | T  6.0% | occurs 5.0% of the time
A:T, B:H, C:T, D:H - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:H, B:T, C:T, D:H - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time
A:T, B:H, C:H, D:T - chances -  H 50.0% | T 50.0% | occurs 3.0% of the time

Our accuracy: 90.0%
&lt;/code&gt;
    &lt;p&gt;And this continues, on and on, forever (as long as we have enough friends). If our number &lt;code&gt;N&lt;/code&gt; of friends is odd, our chances of guessing correctly donâ€™t improve when we move to &lt;code&gt;N+1&lt;/code&gt; friends.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is there a name for this?&lt;/head&gt;
    &lt;p&gt;As far as I can tell, thereâ€™s no name for this weird little phenomenon. But it does appear, implicitly, in voting literature.&lt;/p&gt;
    &lt;p&gt;Condorcetâ€™s jury theorem is a famous theorem in political science. It states:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a group of voters of size &lt;code&gt;N&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;â€¦and they all vote, independently, on an issue with a correct answer&lt;/item&gt;
      &lt;item&gt;â€¦and each voter votes the â€œrightâ€ way with probability &lt;code&gt;P&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;â€¦and we make whatever decision the majority of the voters vote for&lt;/item&gt;
      &lt;item&gt;â€¦then if &lt;code&gt;P &amp;gt; 50%&lt;/code&gt;, the chance that we make the right decision approaches 100% as we add more voters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sounds a fair bit like our coin flipping problem. Hereâ€™s a simplifying assumption that Wikipedia makes when proving the theorem:&lt;/p&gt;
    &lt;p&gt;Hah! The proof explicitly recognizes (and dodges) the even-voter case precisely because that voter doesnâ€™t add any information.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why did I write this?&lt;/head&gt;
    &lt;p&gt;I stumbled upon this result while writing a simulation for a more complex problem. I was so surprised at the simulation results that I assumed that I had a bug in my code. And when I walked through the math by hand I was absolutely delighted.&lt;/p&gt;
    &lt;p&gt;I suspect some of the surprise for me was because I typically encounter problems like these in the context of betting, not voting. If weâ€™re betting on coin flips, weâ€™re certainly excited to bet more if Alice and Bob agree than if weâ€™re just listening to Alice.&lt;/p&gt;
    &lt;p&gt;But voting is a different beast; our outcome is binary. Thereâ€™s no way to harvest the additional EV from the increased confidence Bob sometimes gives us.&lt;/p&gt;
    &lt;p&gt;Anyway. I hope this delights you like it did me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603111</guid><pubDate>Tue, 13 Jan 2026 16:22:59 +0000</pubDate></item><item><title>Influencers and OnlyFans models are dominating U.S. O-1 visa requests</title><link>https://www.theguardian.com/us-news/2026/jan/11/onlyfans-influencers-us-o-1-visa</link><description>&lt;doc fingerprint="32439b71b1a3afb9"&gt;
  &lt;main&gt;
    &lt;p&gt;Content creators and influencers in the US are now increasingly applying for O-1 work visas. Astoundingly, the number of O-1 visas granted each year increased by 50% between 2014 and 2024, as noted by recent reporting in the Financial Times.&lt;/p&gt;
    &lt;p&gt;These visas allow non-immigrants to work temporarily in the US. The O-1 category includes the O-1A, which is designated for individuals with extraordinary ability in the sciences, education, business or athletics and the O-1B, reserved for those with â€œextraordinary ability or achievementâ€.&lt;/p&gt;
    &lt;p&gt;The Guardian spoke with some influencers who have had success in obtaining or are still trying to obtain the coveted O-1 visa and talked about what was involved in their process.&lt;/p&gt;
    &lt;p&gt;Julia Ain decided to post some videos of herself on social media at the height of the Covid-19 lockdown, when she was a student at McGill University.&lt;/p&gt;
    &lt;p&gt;â€œI was bored during the pandemic â€“ like everyone else â€“ and started posting on TikTok,â€ she told the Guardian. â€œI started livestreaming, and I grew a fanbase kind of quickly.â€&lt;/p&gt;
    &lt;p&gt;Five years later, the 25-year-old Canadian content creator now has 1.3 million followers combined across various social media platforms. Her influencer success led her to an O-1 visa.&lt;/p&gt;
    &lt;p&gt;â€œIt became really obvious that you could make a lot of money doing this in a short period of time,â€ she said. â€œIt felt like a very time-sensitive thing. Nobody knows how long this is going to last for.â€&lt;/p&gt;
    &lt;p&gt;Ain posts photos and videos across Instagram, TikTok, X and Snapchat, sometimes in collaboration with other creators. Of her brand, she says: â€œMy whole thing is being the funny Jewish girl with big boobs.â€ The majority of Ainâ€™s income is from Fanfix, a safe-for-work subscription based platform for influencers to monetize their content. She first applied for the O-1B Visa after launching on the platform in August 2023, and the company ended up sponsoring her application. She now says she makes five figures per month on the platform.&lt;/p&gt;
    &lt;p&gt;Luca Mornet also began making content during the pandemic while he was a student at the Fashion Institute of Technology in New York. Mornet, who is from France, realized soon that his F-1 student visa was holding him back from making money as an influencer.&lt;/p&gt;
    &lt;p&gt;â€œI became friends with so many [other influencers], and I would always see them work with so many people and brands and agencies. And I always was so annoyed that I couldnâ€™t because I was a student,â€ he said.&lt;/p&gt;
    &lt;p&gt;He applied for the O-1B Visa shortly after graduating, during which he could finally make money from influencing while on his OPT, a 12-month work authorization for international students post-graduation.&lt;/p&gt;
    &lt;p&gt;The O-1B visa, once reserved for Hollywood titans and superstar musicians, has evolved over the years.&lt;/p&gt;
    &lt;p&gt;â€œWe started doing [O-1 visa applications] for kids who are e-sport players and influencers and the OnlyFans crew,â€ said Michael Wildes, an immigration attorney and managing partner of Wildes &amp;amp; Weinberg. â€œItâ€™s the new, sexy medium for people to be a part of.â€&lt;/p&gt;
    &lt;p&gt;Wildes has worked with the likes of musician SinÃ©ad Oâ€™Connor, soccer star PelÃ©, and restaurateur Jean-Georges Vongerichten. His father, Leon Wildes, who started the firm in 1960, defended John Lennon and Yoko Ono against deportation during the Nixon administration, and helped facilitate the creation of the O-1B visa, which was established by the Immigration Act of 1990. Wildesâ€™s client roster now includes social media influencers and Twitch streamers.&lt;/p&gt;
    &lt;p&gt;To qualify for an O-1B visa, applicants must submit evidence of at least three of the six regulatory criteria, which include performing in a distinguished production or event, national or international recognition for achievements, and a record of commercial or critically acclaimed successes. In 2026, though, these criteria are being stretched to encompass the accolades of an influencer.&lt;/p&gt;
    &lt;p&gt;In Ainâ€™s application, she highlighted her sizable income and social media metrics.&lt;/p&gt;
    &lt;p&gt;â€œPart of my application was: â€˜I have 200,000 followers on this app, 300,000 followers on this app, 10 million people watch me here every month,â€™â€ she said. â€œThis isnâ€™t just, â€˜Oh, you had one viral video and people watched that.â€™ No, youâ€™ve got a following now that are not only watching you, but also paying for your content actively month after month.â€&lt;/p&gt;
    &lt;p&gt;Social media was an integral part of the O-1B visa application of Dina Belenkaya, a Russian Israeli chess player and content creator â€“ which was approved in December 2023.&lt;/p&gt;
    &lt;p&gt;â€œMy followings on Instagram (1.2 million), Twitch (108,000) and YouTube (799,000) were included as part of my profile, and I listed my follower counts on each platform,â€ she said. After her visa approval, she moved to Charlotte, North Carolina â€“ widely considered the chess capital of the United States.&lt;/p&gt;
    &lt;p&gt;While a certain number of followers may not be an automatic ticket to the US, one viral music group has been trying their luck. Boy Throb, comprising Anthony Key, Evan Papier, Zachary Sobania and Darshan Magdum, spent the past few months campaigning to reach 1 million followers on TikTok so that Magdum could use the stat on his O-1 visa application. Clad in matching pink jumpsuits, the three US-based bandmates danced together on screen to parody lyrics of hit songs, while Magdum was edited in from India.&lt;/p&gt;
    &lt;p&gt;Within a month of their first post, Boy Throb reached their goal of 1 million followers. Whether it will help Magdum get a visa remains unclear.&lt;/p&gt;
    &lt;p&gt;â€œHonestly, the entire immigration process has been so complicated and there have been so many people who donâ€™t believe us when we say weâ€™re doing everything in our power to get Darshan here,â€ the group said.&lt;/p&gt;
    &lt;p&gt;â€œWeâ€™re not sure how much longer we want to keep going without Darshan here and the process has been really expensive,â€ they added. In total, the band has spent more than $10,000 in legal and processing fees.&lt;/p&gt;
    &lt;p&gt;The rise in content creators applying for visas given out on the basis of â€œextraordinary abilityâ€ has garnered a variety of reactions. Dominic Michael Tripi, a political analyst and writer, posted on X that the trend was indicative of â€œend-stage empire conditions. Itâ€™s sad.â€ Legal professionals like Wildes, however, argue that the creator economy is the next frontier of American exceptionalism.&lt;/p&gt;
    &lt;p&gt;â€œInfluencers are filling a large gap in the retail and commercial interests of the world,â€ he said. â€œTheyâ€™re moving content and purchases like no other. Immigration has to keep up with this.â€&lt;/p&gt;
    &lt;p&gt;Ain also takes issue with the criticism of influencers applying for O-1 visas, as well as the notion that influencing is not a legitimate profession.&lt;/p&gt;
    &lt;p&gt;â€œI donâ€™t think [people] realize how much work actually goes into it,â€ she said. â€œYou might not agree with the way the money is being made, or what people are watching, but people are still watching and paying for it.â€&lt;/p&gt;
    &lt;p&gt;She continued: â€œMaybe 50 years ago, this isnâ€™t what people imagined the American dream would look like. But this is what the American dream is now.â€&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603535</guid><pubDate>Tue, 13 Jan 2026 16:47:39 +0000</pubDate></item><item><title>Legion Health (YC S21) Hiring Cracked Founding Eng for AI-Native Ops</title><link>https://jobs.ashbyhq.com/legionhealth/ffdd2b52-eb21-489e-b124-3c0804231424</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603829</guid><pubDate>Tue, 13 Jan 2026 17:01:55 +0000</pubDate></item><item><title>The Tulip Creative Computer</title><link>https://github.com/shorepine/tulipcc</link><description>&lt;doc fingerprint="204746de23261bd9"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to the Tulip Creative Computer (Tulip CC)!&lt;/p&gt;
    &lt;p&gt;Tulip is a low power and affordable self-contained portable computer, with a touchscreen display and sound. It's fully programmable - you write code to define your music, games or anything else you can think of. It boots instantaneously into a Python prompt with a lot of built in support for music synthesis, fast graphics and text, hardware MIDI, network access and external sensors. Dive right into making something without distractions or complications.&lt;/p&gt;
    &lt;p&gt;The entire system is dedicated to your code, the display and sound, running in real time, on specialized hardware. The hardware and software are fully open source and anyone can buy one or build one. You can use Tulip to make music, code, art, games, or just write.&lt;/p&gt;
    &lt;p&gt;You can now even run Tulip on the web and share your creations with anyone!&lt;/p&gt;
    &lt;p&gt;Tulip is powered by MicroPython, AMY, and LVGL. The Tulip hardware runs on the ESP32-S3 chip using the ESP-IDF.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get a Tulip from our friends at Makerfabs for only US$59&lt;/item&gt;
      &lt;item&gt;Just got a Tulip CC? Check out our getting started guide!&lt;/item&gt;
      &lt;item&gt;Want to make music with your Tulip? See our music tutorial&lt;/item&gt;
      &lt;item&gt;See the full Tulip API&lt;/item&gt;
      &lt;item&gt;Try out Tulip on the web!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chat about Tulip on our Discord!&lt;/p&gt;
    &lt;p&gt;Check out this video!&lt;/p&gt;
    &lt;p&gt;You can use Tulip one of three ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tulip is available both as an off the shelf or DIY hardware project (Tulip CC)&lt;/item&gt;
      &lt;item&gt;Tulip runs on the web with (almost) all the same features.&lt;/item&gt;
      &lt;item&gt;Tulip can also run as a native app for Mac or Linux (or WSL in Windows) as Tulip Desktop&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're nervous about getting or building the hardware, try it out on the web!&lt;/p&gt;
    &lt;p&gt;The hardware Tulip CC supports:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8.5MB of RAM - 2MB is available to MicroPython, and 1.5MB is available for OS memory. The rest is used for the graphics framebuffers (which you can use as storage) and the firmware cache.&lt;/item&gt;
      &lt;item&gt;32MB flash storage, as a filesystem accesible in Python (24MB left over after OS in ROM)&lt;/item&gt;
      &lt;item&gt;An AMY stereo 120-voice synthesizer engine running locally, or as a wireless controller for an Alles mesh. Tulip's synth supports additive and subtractive oscillators, an excellent FM synthesis engine, samplers, karplus-strong, high quality analog style filters, a sequencer, and much more. We ship Tulip with a drum machine, voices / patch app, and Juno-6 editor.&lt;/item&gt;
      &lt;item&gt;Text frame buffer layer, 128 x 50, with ANSI support for 256 colors, inverse, bold, underline, background color&lt;/item&gt;
      &lt;item&gt;Up to 32 sprites on screen, drawn per scanline, with collision detection, from a total of 32KB of bitmap memory (1 byte per pixel)&lt;/item&gt;
      &lt;item&gt;A 1024 (+128 overscan) by 600 (+100 overscan) background frame buffer to draw arbitrary bitmaps to, or use as RAM, and which can scroll horizontally / vertically&lt;/item&gt;
      &lt;item&gt;WiFi, access http via Python requests or TCP / UDP sockets&lt;/item&gt;
      &lt;item&gt;Adjustable display clock and resolution, defaults to 30 FPS at 1024x600.&lt;/item&gt;
      &lt;item&gt;256 colors&lt;/item&gt;
      &lt;item&gt;Can load PNGs from disk to set sprites or background, or generate bitmap data from code&lt;/item&gt;
      &lt;item&gt;Built in code and text editor&lt;/item&gt;
      &lt;item&gt;Built in BBS chat room and file transfer area called TULIP ~ WORLD&lt;/item&gt;
      &lt;item&gt;USB keyboard, MIDI and mouse support, including hubs&lt;/item&gt;
      &lt;item&gt;Capactive multi-touch support (mouse on Tulip Desktop and Tulip Web)&lt;/item&gt;
      &lt;item&gt;MIDI input and output&lt;/item&gt;
      &lt;item&gt;I2C / Grove / Mabee connector, compatible with many I2C devices like joysticks, keyboard, GPIO, DACs, ADCs, hubs&lt;/item&gt;
      &lt;item&gt;575mA power usage @ 5V including display, at medium display brightness, can last for hours on LiPo, 18650s, or USB battery pack&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I've been working on Tulip on and off for years over many hardware iterations and hope that someone out there finds it as fun as I have, either making things with Tulip or working on Tulip itself. I'd love feedback, your own Tulip experiments or pull requests to improve the system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any issues with your Tulip CC? Here's our troubleshooting guide&lt;/item&gt;
      &lt;item&gt;Learn about our roadmap and find out what we're working on next&lt;/item&gt;
      &lt;item&gt;Build your own Tulip&lt;/item&gt;
      &lt;item&gt;You can read more about the "why" or "how" of Tulip on my website!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A new small option: get yourself a T-Deck and install Tulip CC on it directly! Check out our T-Deck page for more detail.&lt;/p&gt;
    &lt;p&gt;Once you've bought a Tulip, opened Tulip Web, built a Tulip or installed Tulip Desktop, you'll see that Tulip boots right into a Python prompt and all interaction with the system happens there. You can make your own Python programs with Tulip's built in editor and execute them, or just experiment on the Tulip REPL prompt in real time.&lt;/p&gt;
    &lt;p&gt;See the full Tulip API for more details on all the graphics, sound and input functions.&lt;/p&gt;
    &lt;p&gt;Below are a few getting started tips and small examples. The full API page has more detail on everything you can do on a Tulip. See a more complete getting started page or a music making tutorial as well!&lt;/p&gt;
    &lt;code&gt;# Run a saved Python file. Control-C stops it
cd('ex') # The ex folder has a few examples and graphics in it
execfile("parallax.py")
# If you want to run a Tulip package (folder with other files in it)
run("game")&lt;/code&gt;
    &lt;p&gt;Tulip ships with a text editor, based on pico/nano. It supports syntax highlighting, search, save/save-as.&lt;/p&gt;
    &lt;code&gt;# Opens the Tulip editor to the given filename. 
edit("game.py")&lt;/code&gt;
    &lt;p&gt;Tulip supports USB keyboard and mice input as well as touch input. (On Tulip Desktop and Web, mouse clicks act as touch points.) It also comes with UI elements like buttons and sliders to use in your applications, and a way to run mulitple applications as once using callbacks. More in the full API.&lt;/p&gt;
    &lt;code&gt;(x0, y0, x1, y1, x2, y2) = tulip.touch()&lt;/code&gt;
    &lt;p&gt;Tulip CC has the capability to connect to a Wi-Fi network, and Python's native requests library will work to access TCP and UDP. We ship a few convenience functions to grab data from URLs as well. More in the full API.&lt;/p&gt;
    &lt;code&gt;# Join a wifi network (not needed on Tulip Desktop or Web)
tulip.wifi("ssid", "password")

# Get IP address or check if connected
ip_address = tulip.ip() # returns None if not connected

# Save the contents of a URL to disk (needs wifi)
bytes_read = tulip.url_save("https://url", "filename.ext")&lt;/code&gt;
    &lt;p&gt;Tulip comes with the AMY synthesizer, a very full featured 120-oscillator synth that supports FM, PCM, additive synthesis, partial synthesis, filters, and much more. We also provide a useful "music computer" for scales, chords and progressions. More in the full API and in the music tutorial. Tulip's version of AMY comes with stereo sound, which you can set per oscillator with the &lt;code&gt;pan&lt;/code&gt; parameter.&lt;/p&gt;
    &lt;code&gt;amy.drums() # plays a test song
amy.send(volume=4) # change volume
amy.reset() # stops all music / sounds playing&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;music.mov&lt;/head&gt;
    &lt;p&gt;Tulip supports MIDI in and out to connect to external music hardware. You can set up a Python callback to respond immediately to any incoming MIDI message. You can also send messages out to MIDI out. More in the full API and music tutorial.&lt;/p&gt;
    &lt;code&gt;m = tulip.midi_in() # returns bytes of the last MIDI message received
tulip.midi_out((144,60,127)) # sends a note on message
tulip.midi_out(bytes) # Can send bytes or list&lt;/code&gt;
    &lt;p&gt;The Tulip GPU supports a scrolling background layer, hardware sprites, and a text layer. Much more in the full API.&lt;/p&gt;
    &lt;code&gt;# Set or get a pixel on the BG
pal_idx = tulip.bg_pixel(x,y)

# Set the contents of a PNG file on the background.
tulip.bg_png(png_filename, x, y)

tulip.bg_scroll(line, x_offset, y_offset, x_speed, y_speed)&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;scroll.mov&lt;/head&gt;
    &lt;p&gt;Hardware sprites are supported. They draw over the background and text layer per scanline per frame:&lt;/p&gt;
    &lt;code&gt;(w, h, bytes) = tulip.sprite_png("filename.png", mem_pos)

...

# Set a sprite x and y position
tulip.sprite_move(12, x, y)&lt;/code&gt;
    &lt;head class="px-3 py-2"&gt;game.mov&lt;/head&gt;
    &lt;p&gt;Still very much early days, but Tulip supports a native chat and file sharing BBS called TULIP ~ WORLD where you can hang out with other Tulip owners. You're able to pull down the latest messages and files and send messages and files yourself. More in the full API.&lt;/p&gt;
    &lt;code&gt;import world
world.post_message("hello!!") # Sends a message to Tulip World. username required. will prompt if not set
world.upload(filename) # Uploads a file to Tulip World. username required
world.ls() # lists most recent unique filenames/usernames&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get a Tulip!&lt;/item&gt;
      &lt;item&gt;Build your own Tulip Creative Computer with FOUR different options.&lt;/item&gt;
      &lt;item&gt;How to compile and flash Tulip hardware&lt;/item&gt;
      &lt;item&gt;How to run or compile Tulip Desktop&lt;/item&gt;
      &lt;item&gt;The full Tulip API&lt;/item&gt;
      &lt;item&gt;File any code issues or pull requests!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chat about Tulip on our Discord!&lt;/p&gt;
    &lt;p&gt;Two important development guidelines if you'd like to help contribute!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Be nice and helpful and don't be afraid to ask questions! We're all doing this for fun and to learn.&lt;/item&gt;
      &lt;item&gt;Any change or feature must be equivalent across Tulip Desktop and Tulip CC. There are of course limited exceptions to this rule, but please test on hardware before proposing a new feature / change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have fun!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46603995</guid><pubDate>Tue, 13 Jan 2026 17:10:42 +0000</pubDate></item><item><title>How to make a damn website (2024)</title><link>https://lmnt.me/blog/how-to-make-a-damn-website.html</link><description>&lt;doc fingerprint="260e12710bf61f95"&gt;
  &lt;main&gt;
    &lt;p&gt;A lot of people want to make a website but donÃ¢t know where to start or they get stuck. ThatÃ¢s in part because our perception of what websites should be has changed so dramatically over the last 20 years.&lt;/p&gt;
    &lt;p&gt;ItÃ¢s easy to forget how simple a website can be. A website can be just one page. It doesnÃ¢t even need CSS. You donÃ¢t need a content management system like Wordpress. All you have to do is write some HTML and drag that file to a server over FTP.&lt;/p&gt;
    &lt;p&gt;For years now, people have tried to convince us that this is the Ã¢hardÃ¢ way of making a website, but in reality, it may be the easiest.&lt;/p&gt;
    &lt;p&gt;It doesnÃ¢t have to be super complicated. However, with this post, I will assume youÃ¢ve written at least some HTML and CSS before, and that you know how to upload files to a server. If youÃ¢ve never done these things, it may seem like IÃ¢m skipping over some things. I am.&lt;/p&gt;
    &lt;p&gt;Let me begin with what I think you shouldnÃ¢t start with. DonÃ¢t shop around for a CMS. DonÃ¢t even design or outline your website. DonÃ¢t buy a domain or hosting yet. DonÃ¢t set up a GitHub repository; I donÃ¢t care how fast you can make one.&lt;/p&gt;
    &lt;p&gt;Instead, just write your first blog post. The very first thing I did was open TextEdit and write my first post with HTML, ye olde way. Not with Markdown. Not with Nova or BBEdit or another code editor. Just TextEdit (in plain text). Try it, even if just this once. ItÃ¢s kinda refreshing. You can go back to using a code editor later.&lt;/p&gt;
    &lt;p&gt;HereÃ¢s what a draft of this blog post looks like:&lt;/p&gt;
    &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
	&amp;lt;head&amp;gt;
		&amp;lt;meta charset="utf-8"&amp;gt;
		&amp;lt;title&amp;gt;How to Make a Damn Website&amp;lt;/title&amp;gt;
	&amp;lt;/head&amp;gt;
	&amp;lt;body&amp;gt;

		&amp;lt;h1&amp;gt;&amp;lt;a href="how-to-make-a-damn-website.html"&amp;gt;How to Make a Damn Website&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;
		&amp;lt;p&amp;gt;A lot of people want to make a website but donÃ¢t know where to start or they get stuck.&amp;lt;/p&amp;gt;

	&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;&lt;/code&gt;
    &lt;p&gt;This is honestly all you need. ItÃ¢s kind of charming.&lt;/p&gt;
    &lt;p&gt;Make sure you rely exclusively on HTML elements for your formatting. Your page should render clearly with raw HTML. Do not let yourself get distracted by writing CSS. DonÃ¢t even imagine the CSS youÃ¢ll use later. DonÃ¢t write in IDs or classes yet. Do yourself a favor and donÃ¢t make a single &lt;code&gt;div&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Just write the post in the plainest HTML. And donÃ¢t you dare write a Ã¢Hello WorldÃ¢ post or a Ã¢Lorem IpsumÃ¢ post. Write an actual blog post. If you want, make it about why youÃ¢re making a website.&lt;/p&gt;
    &lt;p&gt;Writing this way helps you stay focused on writing for the web. The most important thing here is shipping something. You can (and should) update your site later. Now, name the HTML file something sensible, like the post name.&lt;/p&gt;
    &lt;code&gt;how-to-make-a-damn-website.html&lt;/code&gt;
    &lt;p&gt;Finished? Great. If you have a domain and hosting, make a new folder on your server called blog and upload your first post in there. DonÃ¢t worry about index pages yet. You have only one post, thereÃ¢s not much to index. WeÃ¢ll get there.&lt;/p&gt;
    &lt;p&gt;If you donÃ¢t have a domain or hosting yet, nowÃ¢s the time to buckle down and do that. Unfortunately, I donÃ¢t have good advice for you here. Just know that itÃ¢s going to be stupid and tedious and bad and unfun. ThatÃ¢s just the way this is.&lt;/p&gt;
    &lt;p&gt;Try not to let it deter you. Once you have the ability to upload files to an FTP server, youÃ¢ve reached the Ã¢set it and forget itÃ¢ phase.&lt;/p&gt;
    &lt;p&gt;Direct your web browser to the HTML file you uploaded. Wow! There it is. A real, actual page on the web! You shipped it. Congratulations. Times New Roman, black on white. Hyperlinks that are blue and underlined. Useful. Classic.&lt;/p&gt;
    &lt;p&gt;Look at your unstyled HTML page and appreciate it for what it is. Always remember, this is all a website has to be. Good websites can be reduced to this and still work.&lt;/p&gt;
    &lt;p&gt;A broken escalator is just stairs. Even if itÃ¢s a little less convenient, it remains functional. This is important.&lt;/p&gt;
    &lt;p&gt;If you get this far, I want you to know this is truly the hardest part. Some people will ignore what IÃ¢ve said. They will spend significant time designing a website, hunting around for a good CMS, doing a wide variety of busywork, neglecting the part where they write actual content for their site. But if you shipped a single blog post, you have a website, and they donÃ¢t.&lt;/p&gt;
    &lt;p&gt;A website is nothing without content. You can spend months preparing to make a website, tacking up what IÃ¢m sure was intended to be a Ã¢temporaryÃ¢ page telling people that youÃ¢re Ã¢working on a new website,Ã¢ but it will inevitably become a permanent reminder that you havenÃ¢t done it yet. So focus on what matters, and ship one blog post. Do the rest later.&lt;/p&gt;
    &lt;p&gt;You may think CSS is the next logical step, or maybe an index page, but I donÃ¢t think so. It takes only a few minutes to hand-write an XML file, and once itÃ¢s done, people will be able to read your blog via an RSS reader.&lt;/p&gt;
    &lt;p&gt;On your site, youÃ¢re in control of publishing now. When you post to your blog, part of the process is syndicating it to those who want to stay updated. If you provide an RSS feed, people can follow it. If you donÃ¢t, they canÃ¢t.&lt;/p&gt;
    &lt;p&gt;While the best time to make an RSS feed was 20 years ago, the second best time is now.&lt;/p&gt;
    &lt;p&gt;It should be noted that most people who have an RSS feed are probably not making it manually, so you wonÃ¢t find a lot of documentation out there for doing it this way. But itÃ¢s not too hard. And once you make a habit, itÃ¢ll be a totally reasonable component of your publishing flow.&lt;/p&gt;
    &lt;p&gt;HereÃ¢s what my XML file looks like (without any entries):&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&amp;gt;
	&amp;lt;channel&amp;gt;

		&amp;lt;title&amp;gt;LMNT&amp;lt;/title&amp;gt;
		&amp;lt;link&amp;gt;https://lmnt.me/&amp;lt;/link&amp;gt;
		&amp;lt;description&amp;gt;Louie MantiaÃ¢s weblog.&amp;lt;/description&amp;gt;
		&amp;lt;language&amp;gt;en-us&amp;lt;/language&amp;gt;
		&amp;lt;atom:link href="https://lmnt.me/feed.xml" rel="self" type="application/rss+xml" /&amp;gt;

	&amp;lt;/channel&amp;gt;
&amp;lt;/rss&amp;gt;&lt;/code&gt;
    &lt;p&gt;The elements inside the &lt;code&gt;channel&lt;/code&gt; element are for your feed as a whole (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;link&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;language&lt;/code&gt;, and &lt;code&gt;atom:link&lt;/code&gt;). After the ones about your feedÃ¢s metadata, we can add a blog post to the XML file, which will look like this:&lt;/p&gt;
    &lt;code&gt;&amp;lt;?xml version="1.0" encoding="utf-8"?&amp;gt;
&amp;lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&amp;gt;
	&amp;lt;channel&amp;gt;
		&amp;lt;title&amp;gt;LMNT&amp;lt;/title&amp;gt;
		&amp;lt;link&amp;gt;https://lmnt.me/&amp;lt;/link&amp;gt;
		&amp;lt;description&amp;gt;Louie MantiaÃ¢s weblog.&amp;lt;/description&amp;gt;
		&amp;lt;language&amp;gt;en-us&amp;lt;/language&amp;gt;
		&amp;lt;atom:link href="https://lmnt.me/feed.xml" rel="self" type="application/rss+xml" /&amp;gt;

		&amp;lt;item&amp;gt;
			&amp;lt;title&amp;gt;How to Make a Damn Website&amp;lt;/title&amp;gt;
			&amp;lt;pubDate&amp;gt;Mon, 25 Mar 2024 09:05:00 GMT&amp;lt;/pubDate&amp;gt;
			&amp;lt;guid&amp;gt;C5CC4199-E380-4851-B621-2C1AEF2CE7A1&amp;lt;/guid&amp;gt;
			&amp;lt;link&amp;gt;https://lmnt.me/blog/how-to-make-a-damn-website.html&amp;lt;/link&amp;gt;
			&amp;lt;description&amp;gt;&amp;lt;![CDATA[

				&amp;lt;h1&amp;gt;&amp;lt;a href="how-to-make-a-damn-website.html"&amp;gt;How to Make a Damn Website&amp;lt;/a&amp;gt;&amp;lt;/h1&amp;gt;
				&amp;lt;p&amp;gt;A lot of people want to make a website but donÃ¢t know where to start or they get stuck.&amp;lt;/p&amp;gt;

			]]&amp;gt;&amp;lt;/description&amp;gt;
		&amp;lt;/item&amp;gt;

	&amp;lt;/channel&amp;gt;
&amp;lt;/rss&amp;gt;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;item&lt;/code&gt; element represents an entry, and goes inside the &lt;code&gt;channel&lt;/code&gt; element as well. There are a few self-explanatory elements for the post metadata (&lt;code&gt;title&lt;/code&gt;, &lt;code&gt;pubDate&lt;/code&gt;, &lt;code&gt;guid&lt;/code&gt;, and &lt;code&gt;link&lt;/code&gt;), but the content inside the &lt;code&gt;description&lt;/code&gt; element can be the same HTML from your actual post. Handy!&lt;/p&gt;
    &lt;p&gt;Writing your first post with HTML and understanding how it looks Ã¢unstyledÃ¢ really works in your favor here, because RSS readers use their own stylesheets. How they render pages will not be too different from how a raw HTML page is rendered in your browser. If you make your own stylesheet too early, you may neglect how the raw HTML could be parsed in an RSS reader.&lt;/p&gt;
    &lt;p&gt;For the &lt;code&gt;pubDate&lt;/code&gt;, you can use GMT time. Ask Siri what time it is in Reykjavik, and enter that. You can use your local time zone instead, but be sure itÃ¢s formatted correctly. Also, note that it needs to be 24-hour time.&lt;/p&gt;
    &lt;p&gt;If you have images or other media in your post, be sure to use the absolute URL to a resource rather than a relative one. Relative URLs are fine for content that only lives on your site, but when you syndicate via RSS, that content loads outside of your website. Absolute URLs are better for content inside your blog posts, especially in the XML.&lt;/p&gt;
    &lt;p&gt;Once youÃ¢ve got your first post in the XML file, upload it to the root folder of your website. If you donÃ¢t already have an RSS reader, get one. I recommend NetNewsWire. Go to the XML file in your browser, and it should automatically open in your RSS reader and let you subscribe.&lt;/p&gt;
    &lt;p&gt;There it is! Your blog post is on the web and now also available via RSS! You can share that link now.&lt;/p&gt;
    &lt;p&gt;Now would be a good time to reference your RSS feed in your HTML. YouÃ¢ll want to do this on all pages going forward, too. It helps browsers and plugins detect that thereÃ¢s an RSS feed for people to subscribe to.&lt;/p&gt;
    &lt;code&gt;&amp;lt;link rel="alternate" type="application/rss+xml" title="LMNT" href="https://lmnt.me/feed.xml" /&amp;gt;&lt;/code&gt;
    &lt;p&gt;When you add a new &lt;code&gt;item&lt;/code&gt; (a new blog post), put it above the previous one in your XML file. Keep in mind that your XML file will be updated periodically from devices that subscribe to it. RSS readers will be downloading this file when updating, so keep an eye on the file size. It probably wonÃ¢t ever be that big, because itÃ¢s just text, but itÃ¢s customary to keep only a certain amount of recent entries in the XML file, or a certain time period. But thereÃ¢s no rule here.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;guid&lt;/code&gt; should be a unique string. Some people use URLs thinking theyÃ¢re unique, but those can change. The right way is to generate a unique string for each post, which you can do easily with my app Tulip.&lt;/p&gt;
    &lt;p&gt;Changing the &lt;code&gt;guid&lt;/code&gt; (unique identifier) for your posts makes an RSS reader think itÃ¢s a different entry, resulting in a post being marked Ã¢unread.Ã¢ If you go the route of using a URL as your &lt;code&gt;guid&lt;/code&gt; for each post, youÃ¢ll want to think harder about the file structure of your website, right? ItÃ¢s probably fine if you change your file structure once or twice (I did), but just be sure to update your &lt;code&gt;link&lt;/code&gt; elements in the RSS feed, and redirect old URLs to new ones with an .htaccess file. Just donÃ¢t change the contents of the &lt;code&gt;guid&lt;/code&gt; element.&lt;/p&gt;
    &lt;p&gt;Alright, we can make index pages now. This is going to be super easy, because you donÃ¢t have a lot to index yet.&lt;/p&gt;
    &lt;p&gt;At the root, you want a link to the blog directory, and at the blog directory, you want a link to your first post. Put titles on each page, maybe a link back to the home page from your blog index. If you want, write a little description of your site on the root index.&lt;/p&gt;
    &lt;p&gt;Keep using basic HTML! Titles can be &lt;code&gt;h1&lt;/code&gt;, and descriptions can be &lt;code&gt;p&lt;/code&gt;. Keep it simple.
		&lt;/p&gt;
    &lt;p&gt;Once you got those uploaded, you got three pages and an RSS feed. YouÃ¢re doing great!&lt;/p&gt;
    &lt;p&gt;I recommend writing a couple more posts next. Try using some HTML elements that you didnÃ¢t use in the first post, maybe an &lt;code&gt;hr&lt;/code&gt; element. Fancy! &lt;code&gt;ol&lt;/code&gt; and &lt;code&gt;ul&lt;/code&gt;. Maybe some &lt;code&gt;img&lt;/code&gt;, &lt;code&gt;video&lt;/code&gt;, and &lt;code&gt;audio&lt;/code&gt; elements.&lt;/p&gt;
    &lt;p&gt;In addition to being more posts for your blog, these will also help prioritize which elements need styling, providing you with a few sample pages to check while you write CSS.&lt;/p&gt;
    &lt;p&gt;Upload the posts as you write them, one after the next, adding them to your XML file. DonÃ¢t forget to update your index pages, too. Always check your links and your feed.&lt;/p&gt;
    &lt;p&gt;Before you get ahead of yourself with layout, I recommend first styling the basic HTML elements you already defined in your first few posts: &lt;code&gt;h1&lt;/code&gt;, &lt;code&gt;h2&lt;/code&gt;, &lt;code&gt;h3&lt;/code&gt;, &lt;code&gt;hr&lt;/code&gt;, &lt;code&gt;p&lt;/code&gt;, &lt;code&gt;strong&lt;/code&gt;, &lt;code&gt;em&lt;/code&gt;, &lt;code&gt;ol&lt;/code&gt;, &lt;code&gt;ul&lt;/code&gt;. Define the &lt;code&gt;body&lt;/code&gt; font and width, text sizes, and colors.&lt;/p&gt;
    &lt;p&gt;Like the rest of your site, stylesheets are mutable. Expect them to change with your website. Incremental updates are what makes this whole process work. Ship tiny updates to your CSS. You can upload your stylesheet in a second. Heck, work directly on the server if you want. I did that.&lt;/p&gt;
    &lt;p&gt;If youÃ¢ve done all this, then youÃ¢ve cleared the hurdle. Now you get to just keep doing the fun stuff. Write more blog posts. Make more web pages. ItÃ¢s your website, you can make pages for anything you want. You can style them however you want. You can update people via RSS whenever you make something new.&lt;/p&gt;
    &lt;p&gt;Manually making a website like this may seem silly to engineers who would rather build or rely on systems that automate this stuff. But it doesnÃ¢t seem like thereÃ¢s actually a whole lot that needs automation, does it?&lt;/p&gt;
    &lt;p&gt;A lot of modern solutions may not save time as much as they introduce complexity and reliance on more tools than you need. This whole process is not that complex.&lt;/p&gt;
    &lt;p&gt;ItÃ¢s not doing this manually thatÃ¢s hard.&lt;/p&gt;
    &lt;p&gt;The hard part is just shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604250</guid><pubDate>Tue, 13 Jan 2026 17:23:46 +0000</pubDate></item><item><title>Ask HN: Iran's 120h internet shutdown, phones back. How to stay resilient?</title><link>https://news.ycombinator.com/item?id=46604828</link><description>&lt;doc fingerprint="f2d0c2f5ea82e5e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;It has been 120 hours (5 days) since the internet shutdown in Iran began. While international phone calls have started working again, data remains blocked.&lt;/p&gt;
      &lt;p&gt;I am looking for technical solutions to establish resilient, long-term communication channels that can bypass such shutdowns. What are the most viable options for peer-to-peer messaging, mesh networks, or satellite-based solutions that don't rely on local ISP infrastructure?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604828</guid><pubDate>Tue, 13 Jan 2026 17:53:31 +0000</pubDate></item><item><title>Show HN: Ayder â€“ HTTP-native durable event log written in C (curl as client)</title><link>https://github.com/A1darbek/ayder</link><description>&lt;doc fingerprint="8858ee55c525499a"&gt;
  &lt;main&gt;
    &lt;p&gt;HTTP-native durable event log / message bus â€” written in C&lt;/p&gt;
    &lt;p&gt;A single-binary event streaming system where &lt;code&gt;curl&lt;/code&gt; is your client. No JVM, no ZooKeeper, no thick client libraries.&lt;/p&gt;
    &lt;code&gt;# Produce
curl -X POST 'localhost:1109/broker/topics/orders/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d '{"item":"widget"}'

# Consume
curl 'localhost:1109/broker/consume/orders/mygroup/0?encoding=b64' \
  -H 'Authorization: Bearer dev'&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sustained: ~50K msg/s (wrk2 @ 50K req/s)&lt;/item&gt;
      &lt;item&gt;Client P99: 3.46ms&lt;/item&gt;
      &lt;item&gt;Server P99.999: 1.22ms (handler only)&lt;/item&gt;
      &lt;item&gt;Recovery after SIGKILL: 40â€“50s (8M offsets)&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Kafka&lt;/cell&gt;
        &lt;cell role="head"&gt;Redis Streams&lt;/cell&gt;
        &lt;cell role="head"&gt;Ayder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Protocol&lt;/cell&gt;
        &lt;cell&gt;Binary (requires thick client)&lt;/cell&gt;
        &lt;cell&gt;RESP&lt;/cell&gt;
        &lt;cell&gt;HTTP (curl works)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Durability&lt;/cell&gt;
        &lt;cell&gt;âœ… Replicated log&lt;/cell&gt;
        &lt;cell&gt;âœ… Raft consensus (sync-majority)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Operations&lt;/cell&gt;
        &lt;cell&gt;ZooKeeper/KRaft + JVM tuning&lt;/cell&gt;
        &lt;cell&gt;Single node or Redis Cluster&lt;/cell&gt;
        &lt;cell&gt;Single binary, zero dependencies&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Latency (P99)&lt;/cell&gt;
        &lt;cell&gt;10-50ms&lt;/cell&gt;
        &lt;cell&gt;N/A (async only)&lt;/cell&gt;
        &lt;cell&gt;3.5ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Recovery time&lt;/cell&gt;
        &lt;cell&gt;2+ hours (unclean shutdown)&lt;/cell&gt;
        &lt;cell&gt;Minutes&lt;/cell&gt;
        &lt;cell&gt;40-50 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;First message&lt;/cell&gt;
        &lt;cell&gt;~30 min setup&lt;/cell&gt;
        &lt;cell&gt;~5 min setup&lt;/cell&gt;
        &lt;cell&gt;~60 seconds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Kafka is battle-tested but operationally heavy. JVM tuning, partition rebalancing, and config sprawl add up.&lt;/p&gt;
    &lt;p&gt;Redis Streams is simple and fast, but replication is async-only â€” no majority quorum, no strong durability guarantees.&lt;/p&gt;
    &lt;p&gt;Ayder sits in the middle: Kafka-grade durability (Raft sync-majority) with Redis-like simplicity (single binary, HTTP API). Think of it as what Nginx did to Apache â€” same pattern applied to event streaming.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Append-only logs with per-partition offsets&lt;/item&gt;
      &lt;item&gt;Consumer groups with committed offsets&lt;/item&gt;
      &lt;item&gt;Durability via sealed append-only files (AOF) + crash recovery&lt;/item&gt;
      &lt;item&gt;HA replication with Raft consensus (3 / 5 / 7 node clusters)&lt;/item&gt;
      &lt;item&gt;KV store with CAS and TTL&lt;/item&gt;
      &lt;item&gt;Stream processing with filters, aggregations, and windowed joins (including cross-format Avro+Protobuf joins)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All benchmarks use real network (not loopback). Numbers are real, not marketing.&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3-node Raft cluster on DigitalOcean (8 vCPU AMD)&lt;/item&gt;
      &lt;item&gt;Sync-majority writes (2/3 nodes confirm before ACK)&lt;/item&gt;
      &lt;item&gt;64B payload&lt;/item&gt;
      &lt;item&gt;Separate machines, real network&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Client-side&lt;/cell&gt;
        &lt;cell role="head"&gt;Server-side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;49,871 msg/s&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;1.60ms&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;3.46ms&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99.9&lt;/cell&gt;
        &lt;cell&gt;12.94ms&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99.999&lt;/cell&gt;
        &lt;cell&gt;154.49ms&lt;/cell&gt;
        &lt;cell&gt;1.22ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server-side breakdown at P99.999:&lt;/p&gt;
    &lt;code&gt;Handler:     1.22ms
Queue wait:  0.47ms
HTTP parse:  0.41ms
&lt;/code&gt;
    &lt;p&gt;The 154ms client-side tail is network/kernel scheduling â€” the broker itself stays under 2ms even at P99.999. HTTP is not the bottleneck.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;93,807 msg/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;3.78ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;10.22ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Max&lt;/cell&gt;
        &lt;cell&gt;224.51ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Full wrk output (3-node cluster, max throughput)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://10.114.0.3:8001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     4.26ms    2.97ms 224.51ms   93.76%
    Req/Sec     7.86k     1.19k   13.70k    67.61%
  Latency Distribution
     50%    3.78ms
     75%    4.93ms
     90%    6.44ms
     99%   10.22ms
  5634332 requests in 1.00m, 2.99GB read
Requests/sec:  93807.95
Transfer/sec:     50.92MB
&lt;/code&gt;
    &lt;head&gt;Full wrk2 output (3-node cluster, rate-limited)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://10.114.0.2:9001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.72ms    1.19ms 216.19ms   96.39%
    Req/Sec     4.35k     1.17k    7.89k    79.58%
  Latency Distribution (HdrHistogram - Recorded Latency)
 50.000%    1.60ms
 75.000%    2.03ms
 90.000%    2.52ms
 99.000%    3.46ms
 99.900%   12.94ms
 99.990%   31.76ms
 99.999%  154.49ms
100.000%  216.32ms

  2991950 requests in 1.00m, 1.80GB read
Requests/sec:  49871.12

SERVER  server_us p99.999=1219us (1.219ms)
SERVER  queue_us p99.999=473us (0.473ms)
SERVER  recv_parse_us p99.999=411us (0.411ms)
&lt;/code&gt;
    &lt;p&gt;Ayder runs natively on ARM64. Here's a benchmark on consumer hardware:&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Snapdragon X Elite laptop (1.42 kg)&lt;/item&gt;
      &lt;item&gt;WSL2 Ubuntu, 16GB RAM&lt;/item&gt;
      &lt;item&gt;Running on battery (unplugged)&lt;/item&gt;
      &lt;item&gt;3-node Raft cluster (same machine â€” testing code efficiency)&lt;/item&gt;
      &lt;item&gt;wrk: 12 threads, 400 connections, 60 seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Client-side&lt;/cell&gt;
        &lt;cell role="head"&gt;Server-side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;106,645 msg/s&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;3.57ms&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;7.62ms&lt;/cell&gt;
        &lt;cell&gt;â€”&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99.999&lt;/cell&gt;
        &lt;cell&gt;250.84ms&lt;/cell&gt;
        &lt;cell&gt;0.65ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Server-side breakdown at P99.999:&lt;/p&gt;
    &lt;code&gt;Handler:     0.65ms
Queue wait:  0.29ms
HTTP parse:  0.29ms
&lt;/code&gt;
    &lt;p&gt;Comparison: Snapdragon vs Cloud VMs (Server-side P99.999)&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;/cell&gt;
        &lt;cell role="head"&gt;Server P99.999&lt;/cell&gt;
        &lt;cell role="head"&gt;Hardware&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Snapdragon X Elite (WSL2, battery)&lt;/cell&gt;
        &lt;cell&gt;106,645/s&lt;/cell&gt;
        &lt;cell&gt;0.65ms&lt;/cell&gt;
        &lt;cell&gt;1.42kg laptop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DigitalOcean (8-vCPU AMD, 3 VMs)&lt;/cell&gt;
        &lt;cell&gt;93,807/s&lt;/cell&gt;
        &lt;cell&gt;1.22ms&lt;/cell&gt;
        &lt;cell&gt;Cloud infrastructure&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The laptop's server-side latency is 47% faster while handling 14% more throughput â€” on battery, in WSL2.&lt;/p&gt;
    &lt;p&gt;What this proves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ARM64 is ready for server workloads&lt;/item&gt;
      &lt;item&gt;Efficient C code runs beautifully on Snapdragon&lt;/item&gt;
      &lt;item&gt;WSL2 overhead is minimal for async I/O&lt;/item&gt;
      &lt;item&gt;You can test full HA clusters on your laptop&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Full wrk output (Snapdragon X Elite)&lt;/head&gt;
    &lt;code&gt;Running 1m test @ http://172.31.76.127:7001
  12 threads and 400 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     3.81ms    3.80ms 289.49ms   99.00%
    Req/Sec     8.94k     1.16k   22.81k    80.11%
  Latency Distribution
     50%    3.57ms
     75%    4.01ms
     90%    4.51ms
     99%    7.62ms
  6408525 requests in 1.00m, 3.80GB read
Requests/sec: 106645.65
Transfer/sec:     64.83MB

CLIENT  p99.999=250843us (250.843ms)  max=289485us (289.485ms)
SERVER  server_us p99.999=651us (0.651ms)  max=11964us (11.964ms)
SERVER  queue_us p99.999=285us (0.285ms)  max=3920us (3.920ms)
SERVER  recv_parse_us p99.999=293us (0.293ms)  max=4149us (4.149ms)
&lt;/code&gt;
    &lt;p&gt;Kafka in 2025 is like starting a car with a hand crank. It works, but why are we still doing this?&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Kafka&lt;/cell&gt;
        &lt;cell role="head"&gt;Ayder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cluster restart (unclean)&lt;/cell&gt;
        &lt;cell&gt;2+ hours (reported in production)&lt;/cell&gt;
        &lt;cell&gt;40-50 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Broker sync after failure&lt;/cell&gt;
        &lt;cell&gt;181 minutes for 1TB data&lt;/cell&gt;
        &lt;cell&gt;Auto catch-up in seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50+ broker rolling restart&lt;/cell&gt;
        &lt;cell&gt;2+ hours (2 min per broker)&lt;/cell&gt;
        &lt;cell&gt;N/A â€” single binary&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tested crash recovery:&lt;/p&gt;
    &lt;code&gt;# 3-node cluster with 8 million offsets
1. SIGKILL a follower mid-write
2. Leader continues, follower misses offsets
3. Restart follower
4. Follower replays local AOF â†’ asks leader for missing offsets
5. Leader streams missing data â†’ follower catches up
6. Cluster fully healthy in 40-50 seconds
7. Zero data loss&lt;/code&gt;
    &lt;p&gt;No manual intervention. No partition reassignment. No ISR drama.&lt;/p&gt;
    &lt;code&gt;# Clone and run with Docker Compose (includes Prometheus + Grafana)
git clone https://github.com/A1darbek/ayder.git
cd ayder
docker compose up -d --build

# Or build and run standalone
docker build -t ayder .
docker run -p 1109:1109 --shm-size=2g ayder

# That's it. Now produce:
curl -X POST localhost:1109/broker/topics \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"name":"events","partitions":4}'

curl -X POST 'localhost:1109/broker/topics/events/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d 'hello world'&lt;/code&gt;
    &lt;code&gt;# Dependencies: libuv 1.51+, openssl, zlib, liburing
make clean &amp;amp;&amp;amp; make
./ayder --port 1109&lt;/code&gt;
    &lt;p&gt;The included &lt;code&gt;docker-compose.yml&lt;/code&gt; brings up:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ayder on port &lt;code&gt;1109&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Prometheus on port &lt;code&gt;9090&lt;/code&gt;(metrics scraping)&lt;/item&gt;
      &lt;item&gt;Grafana on port &lt;code&gt;3000&lt;/code&gt;(dashboards, default password:&lt;code&gt;admin&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# docker-compose.yml
services:
  ayder:
    build: .
    ports:
      - "1109:1109"
    shm_size: 2g
    environment:
      - RF_BEARER_TOKENS=dev&lt;/code&gt;
    &lt;code&gt;# Create a topic
curl -X POST localhost:1109/broker/topics \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"name":"events","partitions":8}'

# Produce a message
curl -X POST 'localhost:1109/broker/topics/events/produce?partition=0' \
  -H 'Authorization: Bearer dev' \
  -d 'hello world'

# Consume messages (binary-safe with base64)
curl 'localhost:1109/broker/consume/events/mygroup/0?limit=10&amp;amp;encoding=b64' \
  -H 'Authorization: Bearer dev'

# Commit offset
curl -X POST localhost:1109/broker/commit \
  -H 'Authorization: Bearer dev' \
  -H 'Content-Type: application/json' \
  -d '{"topic":"events","group":"mygroup","partition":0,"offset":10}'&lt;/code&gt;
    &lt;p&gt;A topic contains N partitions. Each partition is an independent append-only log with its own offset sequence.&lt;/p&gt;
    &lt;p&gt;Consumers read from &lt;code&gt;/broker/consume/{topic}/{group}/{partition}&lt;/code&gt;. Progress is tracked per &lt;code&gt;(topic, group, partition)&lt;/code&gt; tuple via explicit commits.&lt;/p&gt;
    &lt;p&gt;If you consume without specifying &lt;code&gt;?offset=&lt;/code&gt;, Ayder resumes from the last committed offset for that consumer group.&lt;/p&gt;
    &lt;p&gt;Ayder acknowledges writes in two modes:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;batch_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;durable&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sealed&lt;/cell&gt;
        &lt;cell&gt;Non-zero&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;true&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Appended to AOF, survives crashes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Rocket&lt;/cell&gt;
        &lt;cell&gt;Zero&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;false&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;In-memory fast path, not persisted&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Use &lt;code&gt;timeout_ms&lt;/code&gt; to wait for sync confirmation.&lt;/p&gt;
    &lt;code&gt;GET  /health      # â†’ {"ok":true}
GET  /ready       # â†’ {"ready":true}
GET  /metrics     # â†’ Prometheus format
GET  /metrics_ha  # â†’ HA cluster metrics&lt;/code&gt;
    &lt;p&gt;Create topic&lt;/p&gt;
    &lt;code&gt;POST /broker/topics
{"name":"events","partitions":8}&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"topic":"events","partitions":8}&lt;/code&gt;
    &lt;p&gt;Single message (raw bytes in body)&lt;/p&gt;
    &lt;code&gt;POST /broker/topics/{topic}/produce
&lt;/code&gt;
    &lt;p&gt;Query parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;partition&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target partition (optional; auto-assigned if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;key&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Message key, URL-encoded (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;idempotency_key&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Deduplication key, URL-encoded (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;timeout_ms&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Wait for sync confirmation (optional)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;timing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;1&lt;/code&gt; to include timing breakdown (optional)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "ok": true,
  "offset": 123,
  "partition": 0,
  "batch_id": 9991,
  "sealed": true,
  "durable": true,
  "mode": "sealed",
  "synced": true
}&lt;/code&gt;
    &lt;p&gt;Duplicate detection (when &lt;code&gt;idempotency_key&lt;/code&gt; matches):&lt;/p&gt;
    &lt;code&gt;{"ok":true,"offset":123,"partition":0,"sealed":true,"synced":null,"duplicate":true}&lt;/code&gt;
    &lt;p&gt;Batch produce (NDJSON â€” one message per line)&lt;/p&gt;
    &lt;code&gt;POST /broker/topics/{topic}/produce-ndjson
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "ok": true,
  "first_offset": 1000,
  "count": 250,
  "partition": 0,
  "batch_id": 424242,
  "sealed": true,
  "durable": true,
  "mode": "sealed",
  "synced": false
}&lt;/code&gt;
    &lt;code&gt;GET /broker/consume/{topic}/{group}/{partition}
&lt;/code&gt;
    &lt;p&gt;Query parameters:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;offset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Start offset, inclusive (resumes from commit if omitted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;limit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Max messages to return (default: 100, max: 1000)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;encoding&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;b64&lt;/code&gt; for binary-safe base64 encoding&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{
  "messages": [
    {"offset": 0, "partition": 0, "value_b64": "aGVsbG8=", "key_b64": "a2V5"}
  ],
  "count": 1,
  "next_offset": 1,
  "committed_offset": 0,
  "truncated": false
}&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;next_offset&lt;/code&gt; as the &lt;code&gt;?offset=&lt;/code&gt; parameter for subsequent reads.&lt;/p&gt;
    &lt;code&gt;POST /broker/commit
{"topic":"events","group":"g1","partition":0,"offset":124}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true}&lt;/code&gt;
    &lt;p&gt;Commits are stored per &lt;code&gt;(topic, group, partition)&lt;/code&gt;. Backward commits are ignored.&lt;/p&gt;
    &lt;p&gt;Delete before offset (hard floor)&lt;/p&gt;
    &lt;code&gt;POST /broker/delete-before
{"topic":"events","partition":0,"before_offset":100000}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"deleted_count":12345,"freed_bytes":987654}&lt;/code&gt;
    &lt;p&gt;Set retention policy&lt;/p&gt;
    &lt;code&gt;POST /broker/retention
&lt;/code&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// TTL + size cap for specific partition
{"topic":"events","partition":0,"ttl_ms":60000,"max_bytes":104857600}

// TTL for all topics
{"topic":"*","ttl_ms":300000}&lt;/code&gt;
    &lt;p&gt;Ayder includes a key-value store with CAS (compare-and-swap) and TTL support.&lt;/p&gt;
    &lt;p&gt;Put&lt;/p&gt;
    &lt;code&gt;POST /kv/{namespace}/{key}?cas=&amp;lt;u64&amp;gt;&amp;amp;ttl_ms=&amp;lt;u64&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Body contains raw value bytes.&lt;/p&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"cas":2,"sealed":true,"durable":true,"mode":"sealed","synced":true,"batch_id":123}&lt;/code&gt;
    &lt;p&gt;Get&lt;/p&gt;
    &lt;code&gt;GET /kv/{namespace}/{key}
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"value":"&amp;lt;base64&amp;gt;","cas":2}&lt;/code&gt;
    &lt;p&gt;Get metadata&lt;/p&gt;
    &lt;code&gt;GET /kv/{namespace}/{key}/meta
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"cas":2,"ttl_ms":12345}&lt;/code&gt;
    &lt;p&gt;Delete&lt;/p&gt;
    &lt;code&gt;DELETE /kv/{namespace}/{key}?cas=&amp;lt;u64&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Response:&lt;/p&gt;
    &lt;code&gt;{"ok":true,"deleted":true,"sealed":true,"durable":true,"mode":"sealed","synced":false,"batch_id":456}&lt;/code&gt;
    &lt;p&gt;Built-in stream processing â€” no separate service required.&lt;/p&gt;
    &lt;code&gt;POST /broker/query
&lt;/code&gt;
    &lt;p&gt;Consume JSON objects from a topic/partition with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Row filtering (eq, ne, lt, gt, in, contains)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;group_by&lt;/code&gt;with aggregations (count, sum, avg, min, max)&lt;/item&gt;
      &lt;item&gt;Field projection&lt;/item&gt;
      &lt;item&gt;Tumbling windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;POST /broker/join
&lt;/code&gt;
    &lt;p&gt;Windowed join between two sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Join types: inner / left / right / full&lt;/item&gt;
      &lt;item&gt;Composite keys&lt;/item&gt;
      &lt;item&gt;Window size and allowed lateness&lt;/item&gt;
      &lt;item&gt;Optional &lt;code&gt;dedupe_once&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Cross-format support (Avro + Protobuf in same join)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ayder supports 3, 5, or 7 node clusters with Raft-based replication.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Acknowledgment&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;async&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Leader appends locally, replicates in background&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;sync-majority&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Waits for majority (e.g., 2/3 nodes)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;sync-all&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Waits for all nodes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Writes must go to the leader. If you send a write to a follower, it returns an HTTP redirect with the leader's address in the &lt;code&gt;Location&lt;/code&gt; header.&lt;/p&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Follow redirects automatically&lt;/item&gt;
      &lt;item&gt;Discover the leader via &lt;code&gt;/metrics_ha&lt;/code&gt;and pin writes to it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a follower rejoins after downtime:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Replays local AOF&lt;/item&gt;
      &lt;item&gt;Connects to leader&lt;/item&gt;
      &lt;item&gt;Requests missing offsets&lt;/item&gt;
      &lt;item&gt;Leader streams missing data&lt;/item&gt;
      &lt;item&gt;Follower catches up automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example scenario:&lt;/p&gt;
    &lt;code&gt;# 3-node cluster
node1 (7001) = LEADER
node2 (8001) = FOLLOWER
node3 (9001) = FOLLOWER

# Write to leader
curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-0'
# â†’ offset 0

# Kill node2
kill -9 $(pgrep -f "port 8001")

# Write while node2 is down
curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-1'
# â†’ offset 1

curl -X POST 'localhost:7001/broker/topics/test/produce?partition=0' \
  -H 'Authorization: Bearer dev' -d 'msg-2'
# â†’ offset 2

# Restart node2 â€” automatically catches up to offset 2

# Verify all data is present on recovered node
curl 'localhost:8001/broker/consume/test/g1/0?offset=0&amp;amp;limit=10' \
  -H 'Authorization: Bearer dev'
# â†’ offsets 0, 1, 2 all present&lt;/code&gt;
    &lt;code&gt;# Default port is 1109
./ayder --port 1109

# Or specify custom port
./ayder --port 7001&lt;/code&gt;
    &lt;p&gt;Ayder uses Raft for consensus. Here's a complete 3-node setup:&lt;/p&gt;
    &lt;p&gt;Environment variables:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_ENABLED&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable HA mode (&lt;code&gt;1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_NODE_ID&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Unique node identifier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_NODES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Cluster topology: &lt;code&gt;id:host:raft_port:priority,...&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_BOOTSTRAP_LEADER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;1&lt;/code&gt; on initial leader only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_WRITE_CONCERN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Nodes to wait for: &lt;code&gt;1&lt;/code&gt;=leader only, &lt;code&gt;2&lt;/code&gt;=majority, &lt;code&gt;N&lt;/code&gt;=all&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_DEDICATED_WORKER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set to &lt;code&gt;0&lt;/code&gt; for best P99 latency (highly recommended)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable mTLS for Raft (&lt;code&gt;1&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_CA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to CA certificate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_CERT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to node certificate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_HA_TLS_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to node private key&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;RF_BEARER_TOKENS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;HTTP auth tokens (format: &lt;code&gt;token1@scope:token2:...&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;3-Node Example:&lt;/p&gt;
    &lt;code&gt;# Node 1 (bootstrap leader)
export RF_HA_ENABLED=1
export RF_HA_NODE_ID=node1
export RF_HA_BOOTSTRAP_LEADER=1
export RF_HA_NODES='node1:10.0.0.1:7000:100,node2:10.0.0.2:8000:50,node3:10.0.0.3:9000:25'
export RF_HA_WRITE_CONCERN=2  # sync-majority (2/3 nodes)
export RF_HA_DEDICATED_WORKER=0  # critical for low P99
export RF_BEARER_TOKENS='dev@scope:token2:token3'
export RF_HA_TLS=1
export RF_HA_TLS_CA=./certs/ca.crt
export RF_HA_TLS_CERT=./certs/node1.crt
export RF_HA_TLS_KEY=./certs/node1.key
./ayder --port 7001

# Node 2
export RF_HA_ENABLED=1
export RF_HA_NODE_ID=node2
export RF_HA_NODES='node1:10.0.0.1:7000:100,node2:10.0.0.2:8000:50,node3:10.0.0.3:9000:25'
export RF_HA_WRITE_CONCERN=2
export RF_HA_DEDICATED_WORKER=0
export RF_BEARER_TOKENS='dev@scope:token2:token3'
export RF_HA_TLS=1
export RF_HA_TLS_CA=./certs/ca.crt
export RF_HA_TLS_CERT=./certs/node2.crt
export RF_HA_TLS_KEY=./certs/node2.key
./ayder --port 8001

# Node 3 (same pattern, port 9001)&lt;/code&gt;
    &lt;p&gt;5-Node and 7-Node Clusters:&lt;/p&gt;
    &lt;code&gt;# 5-node topology
export RF_HA_NODES='node1:host1:7000:100,node2:host2:8000:80,node3:host3:9000:60,node4:host4:10000:40,node5:host5:11000:20'
export RF_HA_WRITE_CONCERN=3  # majority of 5

# 7-node topology
export RF_HA_NODES='node1:host1:7000:100,node2:host2:8000:90,node3:host3:9000:80,node4:host4:10000:70,node5:host5:11000:60,node6:host6:12000:50,node7:host7:13000:40'
export RF_HA_WRITE_CONCERN=4  # majority of 7&lt;/code&gt;
    &lt;p&gt;Generate TLS certificates:&lt;/p&gt;
    &lt;code&gt;# Create CA
openssl req -x509 -newkey rsa:4096 -keyout ca.key -out ca.crt \
  -days 365 -nodes -subj "/CN=ayder-ca"

# Create node certificate (repeat for each node)
openssl req -newkey rsa:2048 -nodes -keyout node1.key -out node1.csr \
  -subj "/CN=node1" -addext "subjectAltName=DNS:node1,IP:10.0.0.1"

openssl x509 -req -in node1.csr -CA ca.crt -CAkey ca.key -CAcreateserial \
  -out node1.crt -days 365 -copy_extensions copy&lt;/code&gt;
    &lt;p&gt;Write concern tradeoffs:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;RF_HA_WRITE_CONCERN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;Durability&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency&lt;/cell&gt;
        &lt;cell role="head"&gt;Survives&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;~1ms&lt;/cell&gt;
        &lt;cell&gt;Nothing (leader only)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;2&lt;/code&gt; (3-node)&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;~3ms&lt;/cell&gt;
        &lt;cell&gt;1 node failure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;3&lt;/code&gt; (5-node)&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;~3ms&lt;/cell&gt;
        &lt;cell&gt;2 node failures&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;N&lt;/code&gt; (all nodes)&lt;/cell&gt;
        &lt;cell&gt;Maximum&lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;N-1 failures, but blocks if any node slow&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Built by Aydarbek Romanuly â€” solo founder from Kazakhstan ğŸ‡°ğŸ‡¿&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GitHub: @A1darbek&lt;/item&gt;
      &lt;item&gt;Email: aidarbekromanuly@gmail.com&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Errors follow a consistent format:&lt;/p&gt;
    &lt;code&gt;{
  "ok": false,
  "error": "missing_topic",
  "message": "Topic name is required",
  "docs": "https://ayder.dev/docs/api/produce"
}&lt;/code&gt;
    &lt;p&gt;âœ… HTTP-native event log with partitions and offsets&lt;lb/&gt; âœ… Fast writes with cursor-based consumption&lt;lb/&gt; âœ… Durable with crash recovery&lt;lb/&gt; âœ… Horizontally scalable with Raft replication&lt;lb/&gt; âœ… Built-in stream processing with cross-format joins&lt;lb/&gt; âœ… ARM64-native (tested on Snapdragon X Elite)&lt;/p&gt;
    &lt;p&gt;âŒ Kafka protocol compatible&lt;lb/&gt; âŒ A SQL database&lt;lb/&gt; âŒ Magic exactly-once without client-side idempotency discipline&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604862</guid><pubDate>Tue, 13 Jan 2026 17:55:37 +0000</pubDate></item><item><title>The Case for Blogging in the Ruins</title><link>https://www.joanwestenberg.com/the-case-for-blogging-in-the-ruins/</link><description>&lt;doc fingerprint="2cac1f510ee48e99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Case for Blogging in the Ruins&lt;/head&gt;
    &lt;p&gt;In 1751, Denis Diderot began publishing his EncyclopÃ©die, a project that would eventually span 28 volumes and take more than two decades to complete. The French government banned it twice. The Catholic Church condemned it, Diderot's collaborators abandoned him, his publisher secretly censored entries behind his back, and he worked himself into periodic breakdowns trying to finish the damn thing.&lt;/p&gt;
    &lt;p&gt;When people talk about the Enlightenment as if it were an intellectual garden party where everyone sipped wine and agreed about reason, they're missing the part where producing and distributing ideas was (in fact) dangerous and thankless work.&lt;/p&gt;
    &lt;p&gt;Diderot has been on my mind lately, spending the Xmas period scrolling through the dwindling numbers of social media platforms that haven't yet been purchased by an "eccentric" (read: race-science obsessed) billionaire or banned by a foreign government.&lt;/p&gt;
    &lt;p&gt;Diderot's project was fundamentally about building infrastructure for thinking. He wanted to create a shared repository of human knowledge that anyone could access, organized in a way that invited exploration and cross-referencing. He believed that structuring information properly could change how people thought.&lt;/p&gt;
    &lt;p&gt;He was right.&lt;/p&gt;
    &lt;p&gt;270 years later, we have more information than any civilization in history. But aside from Wikipedia, we've organized the sum total of our collective knowledge into formats optimized for making people angry at strangers in pursuit of private profitability.&lt;/p&gt;
    &lt;p&gt;Something has gone terribly wrong.&lt;/p&gt;
    &lt;p&gt;And I think the fix, or at least part of it = going backwards to a technology we've largely abandoned: the blog, humble // archaic as it may seem.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Pamphlet Problem&lt;/head&gt;
    &lt;p&gt;Before social media ate the internet, and before the internet ate everything else, and before everything else ate itself, blogs occupied a wonderful and formative niche in the information ecosystem. They were personal but public, permanent but updateable, long-form but informal. A blog post could be three paragraphs or thirty pages. It could be rigorously researched or entirely speculative. It could build an argument over weeks or months, with each post serving as a chapter in an ongoing intellectual project that readers could follow, critique, and respond to.&lt;/p&gt;
    &lt;p&gt;The blogosphere of the mid-2000s had its problems: It was insular and often smug, prone to flame wars between people who agreed on 95% of everything but found the remaining 5% absolutely unforgivable. But it also produced actual intellectual communities.&lt;/p&gt;
    &lt;p&gt;Remember those?&lt;/p&gt;
    &lt;p&gt;People wrote long responses to each other's posts, those responses generated further responses, and you could follow the thread of an argument across multiple sites and weeks of discussion. The format rewarded careful thinking because careful thinking was legible in a way that it simply isn't on platforms designed for rapid-fire engagement.&lt;/p&gt;
    &lt;p&gt;Social media removed the friction of publishing, and in doing so removed the selection pressure that separated signal from noise. We "democratized" the ability to publish (good?) while simultaneously destroying the conditions that made publishing meaningful (bad!).&lt;/p&gt;
    &lt;p&gt;Diderot spent twenty years on his infrastructure.&lt;/p&gt;
    &lt;p&gt;We handed ours to advertising companies and - like Pilate - washed our hands of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Architecture of Attention&lt;/head&gt;
    &lt;p&gt;When you write a blog post, you're creating a standalone document with a permanent URL. It exists at a specific address on the web, and that address doesn't change based on who's looking at it, when they're looking at it, or what algorithm has decided they should see next. The post is there, stable, waiting for whoever wants to find it.&lt;/p&gt;
    &lt;p&gt;Compare this to a tweet (by God I'll not call them "X's") or a Facebook post, which exists primarily as an item in a feed, algorithmically sorted, personalized to each viewer. Your post might appear at the top of someone's feed for an hour and then disappear into an infinite scroll of other content, never to be seen again. The platform has no interest in whether your post is found next week or next year; it has a vested interest in keeping users scrolling through new content right now.&lt;/p&gt;
    &lt;p&gt;When I write a blog post, I'm writing for an imagined reader who has arrived at this specific URL because they're interested in this specific topic; I can assume a baseline of engagement; I can make my case over several thousand words, trusting that anyone who's made it to paragraph twelve probably intends to make it to paragraph twenty.&lt;/p&gt;
    &lt;p&gt;When I write for social media, I'm writing for someone who is one thumb-flick away from a video of either a hate crime or a dog riding a skateboard. Everything I produce has to compete, in real-time, with everything else that could possibly occupy that user's attention. The incentives push toward provocation and emotional activation. The format actively punishes nuance, which means that a thoughtful caveat reads as weakness and any acknowledgment of uncertainty looks like waffling.&lt;/p&gt;
    &lt;p&gt;Diderot understood that the container shapes the contents. The EncyclopÃ©die was a collection of facts, yes, but more fundamentally it was an argument about how knowledge should be organized. Cross-references between entries were themselves a form of commentary, connecting ideas that authorities wanted kept separate. We've restructured the presentation of ideas around the needs of advertising platforms, and not by accident, and we're living with the consequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;Montaigne's Heirs&lt;/head&gt;
    &lt;p&gt;Michel de Montaigne arguably invented the essay in the 1570s, sitting in a tower in his French chÃ¢teau, writing about whatever interested him: cannibals, thumbs, the education of children, how to talk to people who are dying. He called these writings essais, meaning "attempts" or "tries." The form was explicitly provisional. Montaigne was trying out ideas, seeing where they led, acknowledging uncertainty as a fundamental feature rather than a bug to be eliminated.&lt;/p&gt;
    &lt;p&gt;The blog, at its best (a best I aspire one day to reach) is Montaigne's direct descendant. It's a form that allows for intellectual exploration without demanding premature certainty. You can write a post working through an idea, acknowledge in the post itself that you're not sure where you'll end up, and invite readers to think alongside you. You can return to the topic weeks later with updated thoughts. The format accommodates the actual texture of thinking, which is messy and recursive and full of wrong turns.&lt;/p&gt;
    &lt;p&gt;Social media flattens all of this into statements: Everything you post is implicitly a declaration. Even if you add caveats, the format strips them away. What travels is the hot take, the dunked-on screenshot, the increasingly-shitty meme, the version of your argument that fits in a shareable image with the source cropped out.&lt;/p&gt;
    &lt;p&gt;I keep thinking about how many interesting folks have essentially stopped writing anything substantial because they've moved their entire intellectual presence to Twitter or Substack Notes. These are people who used to produce ten-thousand-word explorations of complex topics, and now they produce dozens of disconnected fragments per day, each one optimized for immediate engagement and none of them building toward anything coherent.&lt;/p&gt;
    &lt;p&gt;It's like watching someone who used to compose symphonies decide to only produce ringtones.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Makes a Blog Actually Work&lt;/head&gt;
    &lt;p&gt;Most blogs are abandoned after three posts. The ones that persist // accumulate value have a few things on common:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They have a perspective, not just a topic. A blog about "marketing" is forgettable. A blog about "why most marketing advice is wrong and what actually works" has a point of view that gives every post something to push against.&lt;/item&gt;
      &lt;item&gt;They build. The best blogs create posts that reference and extend earlier posts, developing ideas over time rather than starting from scratch each week. Gwern's site is an extreme example, with entries that get updated for years, accumulating evidence and refinement. But even a modest version of this works: a body of work that compounds.&lt;/item&gt;
      &lt;item&gt;They're written for someone specific. Not "everyone interested in X" but "the person who already knows Y and is trying to figure out Z." Specificity creates resonance.&lt;/item&gt;
      &lt;item&gt;They exist at a consistent address. A blog at your own domain, with permanent URLs, can be found and referenced for years. Content locked inside platforms disappears when the platform changes or dies.&lt;/item&gt;
      &lt;item&gt;Their authors accept that most posts won't go viral, and that's fine. The value is cumulative. A blog with fifty solid posts is an asset even if no single post ever breaks through. Most social media content has a half-life of hours; a good blog post can draw readers for a decade.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Discovery Problem (And Why It's Overstated)&lt;/head&gt;
    &lt;p&gt;There are rote objections: nobody reads blogs anymore. The discovery mechanisms are broken. How is anyone supposed to find a new blog when they're competing against algorithmic feeds specifically designed to capture and hold attention?&lt;/p&gt;
    &lt;p&gt;Etc, etc, etc.&lt;/p&gt;
    &lt;p&gt;But there are a few things worth noting:&lt;/p&gt;
    &lt;p&gt;Search engines still index blogs far better than social media posts. A well-written blog post on a specific topic can draw readers for years through Google (or Kagi // DuckDuckGo if you're nasty, and by nasty I mean excellent); a tweet is lucky to get attention for twelve hours. Hell, call it six. Hell, call it three and call me an optimist at that. If you're trying to build a body of work, or to create something that will outlast the platform of the moment, a blog is simply a better tool.&lt;/p&gt;
    &lt;p&gt;What else?&lt;/p&gt;
    &lt;p&gt;RSS never actually died. It went underground. Feedly, Unread, NetNewsWire, and other readers still have millions of users. The people who read blogs tend to be the people worth reaching: curious, patient, willing to engage with longer arguments.&lt;/p&gt;
    &lt;p&gt;Newsletters are still a discovery layer, no matter how many people pronounce their untimely death. You can write on your own site and distribute via email, getting the permanence of a blog with the push distribution of a newsletter. The writing lives at your domain; the email is notification infrastructure.&lt;/p&gt;
    &lt;p&gt;And the fragmentation of social media is actually creating demand for alternatives. Every time a platform implodes (Twitter's ongoing collapse, Instagram's slow retirement // decay into a metaphorical Floridian condo, TikTok's uncertain status, Facebook's demographic hollowing) people start looking for more stable ground. The infrastructure exists. It's waiting.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Room of Your Own&lt;/head&gt;
    &lt;p&gt;Virginia Woolf wrote about the importance of having a room of one's own: physical space for creative work, free from interruption and control. A blog is a room of your own on the internet. It's a place where you decide what to write about and how to write about it, where you're not subject to the algorithmic whims of platforms that profit from your engagement regardless of whether that engagement makes you or anyone else nebulously smarter.&lt;/p&gt;
    &lt;p&gt;Diderot built the EncyclopÃ©die because he believed that organizing knowledge properly could change how people thought. He spent two decades on it. He went broke. He watched collaborators quit and authorities try to destroy his work. He kept going because the infrastructure mattered, because how we structure the presentation of ideas affects the ideas themselves.&lt;/p&gt;
    &lt;p&gt;We're not going to get a better internet by waiting for platforms to become less extractive. We build it by building it. By maintaining our own spaces, linking to each other, creating the interconnected web of independent sites that the blogosphere once was and could be again.&lt;/p&gt;
    &lt;p&gt;So:&lt;/p&gt;
    &lt;p&gt;Start a blog. Start one because the practice of writing at length, for an audience you respect, about things that matter to you, is itself valuable. Start one because owning your own platform is a form of independence that becomes more important as centralized platforms become less trustworthy. Start one because the format shapes the thought, and this format is good for thinking.&lt;/p&gt;
    &lt;p&gt;The blog won't save us. But it's one of the tools we'll need if we're going to save ourselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Start Today&lt;/head&gt;
    &lt;p&gt;If you're convinced, the practical options:&lt;/p&gt;
    &lt;p&gt;Write.as is minimalist, privacy-focused, no-frills. You can start anonymously and upgrade to a custom domain later. The editor gets out of your way. Good for people who want to write without fiddling with settings.&lt;/p&gt;
    &lt;p&gt;Bear Blog is extremely lightweight, fast-loading, no tracking. Free tier is generous. The aesthetic is deliberately simple, which enforces a focus on writing over design tinkering. Privacy-first.&lt;/p&gt;
    &lt;p&gt;Ghost (powering this blog) is more fully-featured, with built-in membership and newsletter tools. You can self-host (free) or use their managed hosting (paid). Good if you want to eventually build a paid subscriber base but want to own your infrastructure. Open source.&lt;/p&gt;
    &lt;p&gt;Micro.blog was built explicitly as an alternative to social media, with cross-posting, a community timeline, and support for short and long posts. Has an indie web ethos baked in. Good if you want the social layer without the algorithmic manipulation. Manton (founder) is one of my favourite folks to follow // read lately. Good insights.&lt;/p&gt;
    &lt;p&gt;All of these let you use a custom domain, which you should do. Buy yourname.com. It costs ten dollars a year and your writing will live at an address you control, regardless of what happens to any particular platform.&lt;/p&gt;
    &lt;p&gt;Pick one. Set it up this week. Write something. Send me a link (email joan@joanwestenberg.com) and I'll read it. The first post doesn't have to be good; it has to exist. The second one can be better. That's how this works. That's how it's always worked.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46604959</guid><pubDate>Tue, 13 Jan 2026 18:00:20 +0000</pubDate></item><item><title>AI Generated Music Barred from Bandcamp</title><link>https://old.reddit.com/r/BandCamp/comments/1qbw8ba/ai_generated_music_on_bandcamp/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605490</guid><pubDate>Tue, 13 Jan 2026 18:31:50 +0000</pubDate></item><item><title>Signal leaders warn agentic AI is an insecure, unreliable surveillance risk</title><link>https://coywolf.com/news/productivity/signal-president-and-vp-warn-agentic-ai-is-insecure-unreliable-and-a-surveillance-nightmare/</link><description>&lt;doc fingerprint="bfc31855efebb4cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;â€˜Signalâ€™ President and VP warn agentic AI is insecure, unreliable, and a surveillance nightmare&lt;/head&gt;
    &lt;p&gt;With agentic AI embedded at the OS level, databases storing entire digital lives accessible to malware, tasks whose reliability quickly breaks down at each step, and being opted-in without consent, Signal leadership is sounding the alarm for the industry to pull back until threats can be mitigated.&lt;/p&gt;
    &lt;p&gt;At the 39th Chaos Communication Congress (39C3) in Hamburg, Germany, Signal President Meredith Whittaker and VP of Strategy and Global Affairs Udbhav Tiwari gave a presentation titled AI Agent, AI Spy. In it, they shared the many vulnerabilities and concerns they have about how agentic AI is being implemented, the very real threat itâ€™s bringing to enterprise companies, and how they recommend the industry change to mitigate a disaster in the making.&lt;/p&gt;
    &lt;p&gt;A key component of AI agents is that they must know enough about you and have access to sensitive data so that they can autonomously take actions on your behalf, such as making purchases, scheduling events, and responding to messages. However, the way AI agents are being implemented is making them insecure, unreliable, and open to surveillance.&lt;/p&gt;
    &lt;head rend="h2"&gt;How AI agents are vulnerable to threats&lt;/head&gt;
    &lt;p&gt;Microsoft is trying to bring agentic AI to its Windows 11 users via Recall. Recall takes a screenshot of your screen every few seconds, OCRs the text, and does semantic analysis of the context and actions. It then creates a forensic dossier of everything you do into a single database on your computer. The database includes a precise timeline of actions, full raw text (via OCR), dwell time, and focus on specific apps and actions. Additionally, it assigns topics to specific activities.&lt;/p&gt;
    &lt;p&gt;Tiwari says the problem with this approach is that it doesnâ€™t mitigate the threat of malware (via online attacks) and indirect (hidden) prompt injection attacks, which can all gain access to the database. These vulnerabilities subsequently circumvent end-to-end encryption (E2EE), prompting Signal to add a flag in its app to prevent its screen from being recorded, but Tiwari says thatâ€™s not a reliable or long-term solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why complex agentic tasks arenâ€™t reliable&lt;/head&gt;
    &lt;p&gt;Whittaker emphasized that agentic AI isnâ€™t just intrusive and vulnerable to threats; itâ€™s also unreliable. She said AI agents are probabilistic, not deterministic, and that each step they take in a task degrades their accuracy and the final action.&lt;/p&gt;
    &lt;p&gt;She said if an AI agent could perform each step with 95% accuracyâ€“which currently isnâ€™t possibleâ€“a 10-step task would yield an action with a ~59.9% success rate. And if you had a 30-step task, the success rate would be ~21.4%. Furthermore, if we used a more realistic accuracy rate of 90%, then a 30-step task would drop down to a success rate of 4.2%. She added that the best agent models failed 70% of the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to make AI agents private and secure&lt;/head&gt;
    &lt;p&gt;Whittaker said there currently isnâ€™t a solution for making AI agents preserve privacy, security, and control; thereâ€™s only triage, but companies can take steps now to mitigate it.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Stop the reckless deployment of AI agents to avoid plain-text database access to malware.&lt;/item&gt;
      &lt;item&gt;Make opting out the default, with mandatory developer opt-ins.&lt;/item&gt;
      &lt;item&gt;AI companies must provide radical (or any) transparency about how everything works and make it auditable at the granular level.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the industry doesnâ€™t heed Whittakerâ€™s and Tiwariâ€™s warnings, the age of agentic AI could be in jeopardy, primarily because consumers could quickly lose their trust in a technology that is already overhyped and over-invested in.&lt;/p&gt;
    &lt;p&gt;Jon Henshaw is the founder of Coywolf and an industry veteran with almost three decades of SEO, digital marketing, and web technologies experience. Follow @jon@henshaw.social&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605553</guid><pubDate>Tue, 13 Jan 2026 18:35:52 +0000</pubDate></item><item><title>Show HN: Nogic â€“ VS Code extension that visualizes your codebase as a graph</title><link>https://marketplace.visualstudio.com/items?itemName=Nogic.nogic</link><description>&lt;doc fingerprint="1cccba6509fd4c3a"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;table&gt;
              &lt;row&gt;
                &lt;cell class="ux-itemdetails-left"&gt;
                  &lt;div&gt;
                    &lt;div&gt;
                      &lt;head rend="h1"&gt;ğŸ” Nogic&lt;/head&gt;
                      &lt;p&gt;Visualize your codebase structure with interactive diagrams&lt;/p&gt;
                      &lt;head rend="h2"&gt;ğŸ“¦ Supported Languages&lt;/head&gt;
                      &lt;p&gt;More languages and frameworks coming soon! ğŸ‰&lt;/p&gt;
                      &lt;head rend="h2"&gt;ğŸš€ Getting Started&lt;/head&gt;
                      &lt;list rend="ol"&gt;
                        &lt;item&gt;Open the Command Palette (&lt;code&gt;Cmd+Shift+P&lt;/code&gt; / &lt;code&gt;Ctrl+Shift+P&lt;/code&gt;)&lt;/item&gt;
                        &lt;item&gt;Run &lt;code&gt;Nogic: Open Visualizer&lt;/code&gt;&lt;/item&gt;
                        &lt;item&gt;Right-click files or folders in the Explorer and select &lt;code&gt;Add to Nogic Board&lt;/code&gt;&lt;/item&gt;
                      &lt;/list&gt;
                      &lt;p&gt;Your codebase is automatically indexed when you open the visualizer, if given permission.&lt;/p&gt;
                      &lt;head rend="h2"&gt;âœ¨ Features&lt;/head&gt;
                      &lt;list rend="ul"&gt;
                        &lt;item&gt;ğŸŒ² Unified View â€” Browse files, classes, and functions in an interactive hierarchical graph&lt;/item&gt;
                        &lt;item&gt;ğŸ“‹ Boards â€” Create custom boards to organize and focus on specific parts of your codebase&lt;/item&gt;
                        &lt;item&gt;ğŸ¯ Class Diagrams â€” View class relationships, inheritance, and method structures&lt;/item&gt;
                        &lt;item&gt;ğŸ”„ Call Graphs â€” Trace function calls and dependencies across your codebase&lt;/item&gt;
                        &lt;item&gt;ğŸ” Quick Search â€” Find elements instantly with &lt;code&gt;Cmd/Ctrl+K&lt;/code&gt;&lt;/item&gt;
                        &lt;item&gt;âš¡ Auto-sync â€” Changes to your code are automatically reflected in the visualization&lt;/item&gt;
                      &lt;/list&gt;
                      &lt;head rend="h2"&gt;ğŸ“– Commands&lt;/head&gt;
                      &lt;table&gt;
                        &lt;row&gt;
                          &lt;cell role="head"&gt;Command&lt;/cell&gt;
                          &lt;cell role="head"&gt;Description&lt;/cell&gt;
                        &lt;/row&gt;
                        &lt;row&gt;
                          &lt;cell&gt;
                            &lt;code&gt;Nogic: Open Visualizer&lt;/code&gt;
                          &lt;/cell&gt;
                          &lt;cell&gt;Open the interactive visualizer&lt;/cell&gt;
                        &lt;/row&gt;
                        &lt;row&gt;
                          &lt;cell&gt;
                            &lt;code&gt;Nogic: Create New Board&lt;/code&gt;
                          &lt;/cell&gt;
                          &lt;cell&gt;Create a new board&lt;/cell&gt;
                        &lt;/row&gt;
                        &lt;row&gt;
                          &lt;cell&gt;
                            &lt;code&gt;Add to Nogic Board&lt;/code&gt;
                          &lt;/cell&gt;
                          &lt;cell&gt;Add a file/folder to a board (right-click menu)&lt;/cell&gt;
                        &lt;/row&gt;
                      &lt;/table&gt;
                      &lt;head rend="h2"&gt;ğŸ’¡ Tips&lt;/head&gt;
                      &lt;list rend="ul"&gt;
                        &lt;item&gt;ğŸ–±ï¸ Right-click files or folders in the Explorer to add them to a board&lt;/item&gt;
                        &lt;item&gt;ğŸ‘† Double-click nodes to open files in the editor&lt;/item&gt;
                        &lt;item&gt;ğŸ“‚ Click nodes to expand and see methods&lt;/item&gt;
                        &lt;item&gt;ğŸ–ï¸ Drag to pan, scroll to zoom&lt;/item&gt;
                      &lt;/list&gt;
                    &lt;/div&gt;
                  &lt;/div&gt;
                &lt;/cell&gt;
              &lt;/row&gt;
            &lt;/table&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605675</guid><pubDate>Tue, 13 Jan 2026 18:43:28 +0000</pubDate></item><item><title>Choosing learning over autopilot</title><link>https://anniecherkaev.com/choosing-learning-over-autopilot</link><description>&lt;doc fingerprint="3750761f0e740db1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;choosing learning over autopilot&lt;/head&gt;
    &lt;p&gt;I use ai coding tools a lot. I love them. Iâ€™m all-in on ai tools. They unlock doors that let me do things that I cannot do with my human hands alone.&lt;/p&gt;
    &lt;p&gt;But they also scare me.&lt;/p&gt;
    &lt;p&gt;As I see it, they offer me two paths:&lt;/p&gt;
    &lt;head rend="h4"&gt;âœ¨ The glittering vision âœ¨&lt;/head&gt;
    &lt;p&gt;The glittering vision is they let me build systems in the way that the version of me who is a better engineer would build them. Experimentation, iteration and communication have become cheaper. This enables me to learn by doing at a speed that was prohibitive before. I can make better decisions about what and how to build because I can try out a version and learn where some of the sharp edges are in practice instead of guessing. I can also quickly loop in others for feedback and context. All of this leads to building a better version of the system than I would have otherwise.&lt;/p&gt;
    &lt;head rend="h4"&gt;â˜ ï¸ The cursed vision â˜ ï¸&lt;/head&gt;
    &lt;p&gt;The cursed vision is I am lazy, and I build systems of ai slop that I do not understand. Thereâ€™s a lot of ink spilled about perils and pains of ai slop, especially working on a team that has to maintain the resulting code.&lt;/p&gt;
    &lt;p&gt;What scares me most is an existential fear that I wonâ€™t learn anything if I work in the â€œlazyâ€ way. There is no substitute for experiential learning, and it accumulates over time. There are things that are very hard for me to do today, and I will feel sad if all of those things feel equally hard in a year, two years, five years. I am motivated by an emotional response to problems I find interesting, and I like problems that have to do with computers. I am afraid of drowning that desire by substituting engaging a problem with semi-conscious drifting on autopilot.&lt;/p&gt;
    &lt;p&gt;And part of why this is scary to me is that even if my goal is to be principled, to learn, to engage, to satisfy my curiosity with understanding, it is really easy for me to coast with an llm and not notice. There are times when I am tired and I am distracted and I have a thing that I need to get done at work. I just want it done, because then I have another thing I need to do. There are a lot of reasons to be lazy.&lt;/p&gt;
    &lt;head rend="h4"&gt;So I think the crux here is about experiential learning:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ai tools make it so much easier to learn by doing, which can lead to much better results&lt;/item&gt;
      &lt;item&gt;but itâ€™s also possible to use them take a shortcut and get away without learning &lt;list rend="ul"&gt;&lt;item&gt;I deeply believe that the shortcut is a trap&lt;/item&gt;&lt;item&gt;I also believe it is harder than it seems to notice and be honest about when Iâ€™m doing this&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And so, Iâ€™ve been thinking about guidelines &amp;amp; guardrailsâ€“ how do I approach my work to escape the curse, such that llms are a tool for understanding, rather than a replacement for thinking?&lt;/p&gt;
    &lt;head rend="h4"&gt;Hereâ€™s my current working model:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;use ai-tooling to learn, in loops&lt;/item&gt;
      &lt;item&gt;ai-generated code is cheap and not precious; throw it away and start over several times&lt;/item&gt;
      &lt;item&gt;be very opinionated about how to break down a problem&lt;/item&gt;
      &lt;item&gt;â€œtextbookâ€ commits &amp;amp; PRs&lt;/item&gt;
      &lt;item&gt;write my final docs / pr descriptions / comments with my human hands&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rest of the blog post is a deeper look at these topics, in a way that I hope is pretty concrete and grounded.&lt;/p&gt;
    &lt;head rend="h1"&gt;but first, let me make this more concrete&lt;/head&gt;
    &lt;head rend="h3"&gt;Things I now get to care less about:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the mechanics of figuring out how things are hooked together&lt;/item&gt;
      &lt;item&gt;the mechanics of translating pseudocode into code&lt;/item&gt;
      &lt;item&gt;figuring out what the actual code looks like&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The times Iâ€™m using ai tools to disengage a problem and go fast are the times Iâ€™m only doing the things in this first category and getting away with skipping doing the things in the other two.&lt;/p&gt;
    &lt;head rend="h3"&gt;Things I cared about before and should still care about:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;deciding which libraries are used&lt;/item&gt;
      &lt;item&gt;how the code is organized: files &amp;amp; function signatures&lt;/item&gt;
      &lt;item&gt;leaving comments that explain why something is set up in a way if thereâ€™s complication behind it&lt;/item&gt;
      &lt;item&gt;leaving docs explaining how things work&lt;/item&gt;
      &lt;item&gt;understanding when I need to learn something more thoroughly to get unblocked&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Things I now get to care about that were expensive before:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;more deeply understanding how a system works&lt;/item&gt;
      &lt;item&gt;adding better observability like nicely structured outputs for debugging&lt;/item&gt;
      &lt;item&gt;running more experiments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The times when Iâ€™m using ai tools to enhance my learning and understanding Iâ€™m doing the things in the latter two categories.&lt;/p&gt;
    &lt;p&gt;I will caveat that the appropriate amount of care and effort in an implementation depends, of course, on the problem and context. More is not always better. Moving slow can carry engineering risk and I know from experienced that itâ€™s possible for a team to mistake micromanagement for code quality.&lt;/p&gt;
    &lt;p&gt;I like to work on problems somewhere in the middle of the â€œhow correct does this have to beâ€ spectrum and so thatâ€™s where my intuition is tuned to. I donâ€™t need things clean down to the bits, but how the system is built matters so care is worth the investment.&lt;/p&gt;
    &lt;head rend="h1"&gt;workflow&lt;/head&gt;
    &lt;p&gt;Here is a workflow Iâ€™ve been finding useful for medium-sized problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get into the problem: go fast, be messy, learn and get oriented&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Research &amp;amp; document what I want to build &lt;list rend="ol"&gt;&lt;item&gt;I collab with the ai to dump background context and plans into a markdown file &lt;list rend="ol"&gt;&lt;item&gt;The doc at this stage can be rough&lt;/item&gt;&lt;item&gt;A format that Iâ€™ve been using: &lt;list rend="ol"&gt;&lt;item&gt;What is the problem weâ€™re solving?&lt;/item&gt;&lt;item&gt;How does it work today?&lt;/item&gt;&lt;item&gt;How will this change be implemented?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;I collab with the ai to dump background context and plans into a markdown file &lt;/item&gt;
      &lt;item&gt;Build a prototype &lt;list rend="ol"&gt;&lt;item&gt;The prototype can be ai slop&lt;/item&gt;&lt;item&gt;Bias towards seeing things run &amp;amp; interacting with them&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Throw everything away. Start fresh, clean slate &lt;list rend="ol"&gt;&lt;item&gt;Itâ€™s much faster to build it correctly than to fix it&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Formulate a solution: figure out what the correct structure should be&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Research &amp;amp; document based on what I know from the prototype &lt;list rend="ol"&gt;&lt;item&gt;Read code, docs and readmes with my human eyes&lt;/item&gt;&lt;item&gt;Think carefully about the requirements &amp;amp; what causes complication in the code. Are those hard or flexible (or imagined!) requirements?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Design what I want to build, again&lt;/item&gt;
      &lt;item&gt;Now would be a good time to communicate externally if thatâ€™s appropriate for the scope. Write one-pager for anyone who might want to provide input.&lt;/item&gt;
      &lt;item&gt;Given any feedback, design the solution one more time, and this time polish it. Think carefully &amp;amp; question everything. Now is the time to use my brain. &lt;list rend="ol"&gt;&lt;item&gt;Important: what are the APIs? How is the code organized?&lt;/item&gt;&lt;item&gt;Important: what libraries already exist that we can use?&lt;/item&gt;&lt;item&gt;Important: what is the iterative implementation order so that the code is modular &amp;amp; easy to review?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Implement a skeleton, see how the code smells and adjust&lt;/item&gt;
      &lt;item&gt;Use this to compile a final draft of how to implement the feature iteratively&lt;/item&gt;
      &lt;item&gt;Commit the skeleton + the final implementation document&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Implement the solution: generate the final code&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cut a new branch &amp;amp; have the ai tooling implement all the code based on the final spec&lt;/item&gt;
      &lt;item&gt;If itâ€™s not a lot of code or itâ€™s very modular, review it and commit each logical piece into its own commit / PR&lt;/item&gt;
      &lt;item&gt;If it is a lot of code, review it, and commit it as a reference implementation &lt;list rend="ol"&gt;&lt;item&gt;Then, rollback to the skeleton branch, and cut a fresh branch for the first logic piece that will be its own commit / PR&lt;/item&gt;&lt;item&gt;Have the ai implement just that part, possibly guided by any ideas from seeing the full implementation&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;For each commit, I will review the code &amp;amp; Iâ€™ll have the ai review the code&lt;/item&gt;
      &lt;item&gt;I must write my own commit messages with descriptive trailers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the glittering things about ai tooling is that itâ€™s faster than building systems by hand. I maintain that even with these added layers of learning before implementing, itâ€™s still faster than what I could do before while giving me a richer understanding and a better result.&lt;/p&gt;
    &lt;p&gt;Now let me briefly break out the guidelines I mentioned in the intro and how they relate to this workflow.&lt;/p&gt;
    &lt;head rend="h1"&gt;learning in loops&lt;/head&gt;
    &lt;p&gt;There are a lot of ways to learn what to build and how to build it, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Understanding the system and integrations with surround systems&lt;/item&gt;
      &lt;item&gt;Understanding the problem, the requirements &amp;amp; existing work in the space&lt;/item&gt;
      &lt;item&gt;Understanding relationships between components, intended use-cases and control flows&lt;/item&gt;
      &lt;item&gt;Understanding implementation details, including tradeoffs and what a MVP looks like&lt;/item&gt;
      &lt;item&gt;Understanding how to exercise, observe and interact with the implementation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Iâ€™ll understand each area in a different amount of detail at different times. Iâ€™m thinking of it as learning â€œin loopsâ€ because I find that ai tooling lets me quickly switch between breadth and depth in an iterative way. I find that I â€œunderstandâ€ the problem and the solution in increasing depth and detail several times before I build it, and that leads to a much better output.&lt;/p&gt;
    &lt;p&gt;I think there two pitfalls in these learning loops: one feeling like Iâ€™m learning when Iâ€™m actually only skimming, and the other is getting stuck limited on what the ai summaries can provide. One intuition Iâ€™ve been trying to build is when to go read the original sources (like code, docs, readmes) myself. I have two recent experiences top-of-mind informing this:&lt;/p&gt;
    &lt;p&gt;In the first experience, a coworker and I were debugging a mysterious issue related to some file-related resource exhaustion. We both used ai tools to figure out what cli tools we had to investigate and to build a mental model of how the resource in question was supposed to work. I got stuck after getting output that seemed contradictory, and didnâ€™t fit my mental model. My coworker got to a similar spot and then took a step out of the ai tooling to go read the docs about the resource with their human eyes. That led them to understand that the ai summary wasnâ€™t accurate: it had missed some details that explained the confusing situation we were seeing.&lt;/p&gt;
    &lt;p&gt;This example really sticks out in my memory. I thought I was being principled rather than lazy by building my mental model of what was supposed to be happening, but I had gotten mired in building that mental model second-hand instead of reading the docs myself.&lt;/p&gt;
    &lt;p&gt;In the second experience, I was working on a problem related to integrating with a system that had a documented interface. I had the ai read &amp;amp; summarize the interface and then got into the problem in a way similar to the first step of the workflow I described above. I was using that to formulate an idea of what the solution should be. Then I paused to repeat the research loop but with more care: I read the interface with my human eyesâ€“ and found the ai summary was wrong! It wasnâ€™t a big deal and I could shift my plans, but I was glad to have learned to pause and take care in validating the details of my mental model.&lt;/p&gt;
    &lt;head rend="h1"&gt;ai-generated code is throw-away code&lt;/head&gt;
    &lt;p&gt;I had a coworker describe working with ai coding tools like working on a sculpture. When they asked it to reposition the arm, it would accidentally bump the nose out of alignment.&lt;/p&gt;
    &lt;p&gt;The way Iâ€™m thinking about it now, itâ€™s more like: instead of building a sculpture, Iâ€™m asking it to build me a series of sculptures.&lt;/p&gt;
    &lt;p&gt;The first one is rough-hewn and wonky, but lets me understand the shape of what Iâ€™m doing.&lt;/p&gt;
    &lt;p&gt;The next one or two are just armatures.&lt;/p&gt;
    &lt;p&gt;The next one might be a mostly functional sculpture on the latest armature; this lets me understand the shape of what Iâ€™m doing with much higher precision.&lt;/p&gt;
    &lt;p&gt;And then finally, Iâ€™ll ask for a sculpture, using the vetted armature, except weâ€™ll build it one part at a time. When weâ€™re done with a part, weâ€™ll seal it so we canâ€™t bump it out of alignment.&lt;/p&gt;
    &lt;p&gt;A year ago, I wasnâ€™t sure if it was better to try to fix an early draft of ai generated code to be better, or to throw it out. Now I feel strongly that ai-generated code is not precious, and not worth the effort to fix it. If you know what the code needs to do and have that clearly documented in detail, it takes no time at all for the ai to flesh out the code. So throw away all the earlier versions, and focus on getting the armature correct.&lt;/p&gt;
    &lt;p&gt;Making things is all about processes and doing the right thing at the right time. If you throw a bowl and that bowl is off-center, it is a nightmare to try to make it look centered with trimming. If you want a centered bowl then you must throw it on-center. Same here, if you want code that is modular and well structured, the time to do that is before you have the ai implement the logic.&lt;/p&gt;
    &lt;head rend="h2"&gt;â€œtextbookâ€ commits and PRs&lt;/head&gt;
    &lt;p&gt;Itâ€™s much easier to review code that has been written in a way where a feature is broken up into an iteration of commits and PRs. This was true before ai tooling, and is true now.&lt;/p&gt;
    &lt;p&gt;The difference is that writing code with my hands was slow and expensive. Sometimes Iâ€™d be in the flow and Iâ€™d implement things in a way that was hard to untangle after the fact.&lt;/p&gt;
    &lt;p&gt;I believe that especially if I work in the way Iâ€™ve been describing here, ai code is cheap. This makes it much easier/cheaper for me to break apart my work into ways that are easy to commit and review.&lt;/p&gt;
    &lt;p&gt;My other guilty hesitation before ai tooling was I never liked git merge conflicts and rebasing branches. It was confusing and had the scary potential of losing work. Now, ai tooling is very good at rebasing branches, so itâ€™s much less scary and pretty much no effort.&lt;/p&gt;
    &lt;p&gt;I also think that small, clean PRs are an external forcing function to working in a way that builds my understanding rather than lets me take shortcuts: if I generate 2.5k lines of ai slop, it will be a nightmare to break that into PRs.&lt;/p&gt;
    &lt;head rend="h2"&gt;i am very opinionated about how to break down a problem&lt;/head&gt;
    &lt;p&gt;Iâ€™m very opinionated in breaking down problems in two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;how to structure the implementation (files, functions, libraries)&lt;/item&gt;
      &lt;item&gt;how to implement iteratively to make clean commits and PRs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The only way to achieve small, modular, reviewable PRs is to be very opinionated about what to implement and in what order.&lt;/p&gt;
    &lt;p&gt;Unless youâ€™re writing a literal prototype that will be thrown away (and youâ€™re confident it will actually be thrown away), the most expensive part about building a system is the engineering effort that will go into maintaining it. It is, therefore, very worth-while to be opinionated about how to structure the code. I find that the ai can do an okay job at throwing code out there, but I can come up with a much better division and structure by using my human brain.&lt;/p&gt;
    &lt;p&gt;A time I got burned by not thinking about libraries &amp;amp; how to break down a problem was when I was trying to fix noisy errors due to a client chatting with a system that had some network blips. I asked an ai model to add rate limiting to an existing http client, which it did by implementing exponential backoff itself. This isnâ€™t a very good solution, surely we donâ€™t need to do that ourselves. I didnâ€™t think this one through, and was glad a coworker with their brain on caught it in code review.&lt;/p&gt;
    &lt;head rend="h2"&gt;i write docs &amp;amp; pr descriptions with my human hands&lt;/head&gt;
    &lt;p&gt;Writing can serve a few distinct purposes: one is communication, and distinct from that, one is as a method to facilitate thinking. The act of writing forces me to organize and refine my thoughts.&lt;/p&gt;
    &lt;p&gt;This is a clear smell-test for me: I must be able to write documents that explain how and why something is implemented. If I canâ€™t, then thatâ€™s a clear sign that I donâ€™t actually understand it; I have skipped writing as a method of thinking.&lt;/p&gt;
    &lt;p&gt;On the communication side of things, I find that the docs or READMEs that ai tooling generates often capture things that arenâ€™t useful. I often donâ€™t agree with their intuition; I find that if I take the effort to use my brain I produce documents that I believe are more relevant.&lt;/p&gt;
    &lt;p&gt;This isnâ€™t to say that I donâ€™t use ai tooling to write documents: Iâ€™ll often have ai dump information into markdown files as Iâ€™m working. Iâ€™ll often have ai tooling nicely format things like diagrams or tables. Sometimes Iâ€™ll have ai tooling take a pass at a document. Iâ€™ll often hand a document to ai tooling and ask it to validate whether everything I wrote is accurate based on the implementation.&lt;/p&gt;
    &lt;p&gt;But I do believe that if I hold myself to the standard that I write docs, commit messages, etc with my hands, I both produce higher quality documentation and force myself to be honest about understanding what Iâ€™m describing.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;In conclusion, I find that ai coding tools give me a glittering path to understand better by doing, and using that understanding to build better systems. I also, however, think there is a curse of using these systems in a way that skips the â€œbuild the understandingâ€ part, and that pitfall is subtler than it may seem.&lt;/p&gt;
    &lt;p&gt;I care deeply about, and think it will be important in the long-run, to leverage these tools for learning and engaging. Iâ€™ve outlined the ways Iâ€™m thinking about how to do best do this and avoid the curse:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;use ai-tooling to learn, in loops&lt;/item&gt;
      &lt;item&gt;ai-generated code is cheap and not precious; throw it away and start over several times&lt;/item&gt;
      &lt;item&gt;be very opinionated about how to break down a problem&lt;/item&gt;
      &lt;item&gt;â€œtextbookâ€ commits &amp;amp; PRs&lt;/item&gt;
      &lt;item&gt;write my final docs / pr descriptions / comments with my human hands&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605716</guid><pubDate>Tue, 13 Jan 2026 18:46:06 +0000</pubDate></item><item><title>No management needed: anti-patterns in early-stage engineering teams</title><link>https://www.ablg.io/blog/no-management-needed</link><description>&lt;doc fingerprint="ad63d2142d927fee"&gt;
  &lt;main&gt;
    &lt;p&gt;This article is for early-stage (Seed, Series A) founders who think they have engineering management problems (building eng teams, motivating and performance-managing engineers, structuring work/projects, prioritizing, shipping on time).&lt;/p&gt;
    &lt;p&gt;The gist: if you think you have these problems, it is likely that the correct solution is to do nothing, to not manage, and to go back to building product and talking to users. Put another way, and having managed teams at all scales, I donâ€™t think itâ€™s a good use of your time as a founder to be "managing" engineers at such an early stage.&lt;/p&gt;
    &lt;p&gt;In the following sections, I'll go through the most typical anti-patterns I've seen, and try to highlight a better use of your time if you think you've hit the situation in question.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do not try to "motivate" your engineers&lt;/head&gt;
    &lt;p&gt;A common concern of many founders is making sure that their engineers are working hard. This could mean putting in long hours, working more than competitors, completing heroic codebase rewrites, etc. When these external signs of effort seem to be missing, founders worry that the team is not "motivated", and it can be very tempting to treat symptoms over causes. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;creating cultural norms around putting in long hours (996-style culture) by either requiring or celebrating them&lt;/item&gt;
      &lt;item&gt;scheduling recurring or non-urgent meetings on weekends (e.g. standup on Saturdays)&lt;/item&gt;
      &lt;item&gt;micro-managing tasks, or asking people for status reports and other evidence they worked hard&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These anti-patterns share one thing in common: they start with founders trying to actively do something to motivate the team. This has 2 consequences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;This can cause the very engineers you want to retain (those who have many options) to self-select out of your engineering culture. I know several top 1% engineers in the Valley who disengage from recruiting processes when 996 or something similar is mentioned.&lt;/item&gt;
      &lt;item&gt;You are wasting your mental energy on the wrong problem&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this is a long way of saying that motivation is an inherent trait of great startup engineers. Your only job is to hire these engineers, and then to maintain an environment where they want to do their best work. And yes, at that point, you may see them working long hours and doing heroic actions you did not even think were possible.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Motivation is a hired trait. The only place where managers motivate people is in management books.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I'll dedicate a post to specific ways you can identify motivation during hiring, but in short, look for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the obvious one: evidence that they indeed exhibited these external signs of motivation (in an unforced way!) in past jobs&lt;/item&gt;
      &lt;item&gt;signs of grit in their career and life paths (how did they respond to adversity, how have they put their past successes or reputation on the line for some new challenge)&lt;/item&gt;
      &lt;item&gt;intellectual curiosity in the form of hobbies, nerdy interests that they can talk about with passion&lt;/item&gt;
      &lt;item&gt;bias for action and fast decision speed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, as a founder, you should definitely be the most motivated person, in an authentic way (maybe it's some piece of heroic coding, maybe it's taking 2am meetings with European customers, maybe it's something else unique to you). Cultivating your own inner motivation is the most effective way to set the tone for the team.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do not hire managers too soon&lt;/head&gt;
    &lt;p&gt;The most obvious external sign that a startup has switched from building a product to building a company is to add management roles. When this switch happens prematurely, a lot of energy gets spent on stage-irrelevant problems.&lt;/p&gt;
    &lt;p&gt;By definition, an engineering manager needs to manage a team and projects, but if the team is still working on defining what they should be building, there is nothing to manage. Even the most intellectually honest manager will start outputting "management work", such as having 1:1s with everyone, doing some career coaching, applying order to the chaos of potential features by putting them in JIRA tickets or issues, etc. Here's what it means for you as a founder:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;you are still trying to find product-market fit and build your initial product&lt;/item&gt;
      &lt;item&gt;an engineering manager is helping you do it in a more optimized way, but they are optimizing a moving target so it does not really improve anything&lt;/item&gt;
      &lt;item&gt;you don't know if this engineering manager is bad at their job, or if the engineers are not performing, or if the product has no market anyway, or all of the above&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So how do you define "too soon"? Let's look at a few typical inflection points, assuming at least one founder is technical:&lt;/p&gt;
    &lt;head rend="h3"&gt;The founding stage (5-6 engineers including founders)&lt;/head&gt;
    &lt;p&gt;Obviously too soon to hire managers or turn someone into a manager. The only management-like tasks for the founders are hiring and firing, other than that the team should largely be self-organizing and self-sustaining with lightweight tooling (a simple doc can even be used as a task tracker, 1:1s happen organically and are infrequent, etc.).&lt;/p&gt;
    &lt;p&gt;In general, the bias should be towards doing nothing in terms of management and everything in terms of hiring exceptional people who inherently work well together.&lt;/p&gt;
    &lt;head rend="h3"&gt;The multi-team stage (2 or 3 sub-teams of 5 engineers, 10-15 people total)&lt;/head&gt;
    &lt;p&gt;This might be late seed or series A, with an inkling of a working product. Many teams will decide to implement management at this stage, because it seems like the natural next step. The decision is full of nuances, but I would strongly advise to have all the engineers still report into a single person (ideally the co-founder CTO). Why? Speed of execution and culture, mainly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;at 15 engineers, it is very doable for a single person to keep track of everyone's work and ensure alignment.&lt;/item&gt;
      &lt;item&gt;this is the critical moment where you build the engineering culture that will bring you from here to hundreds of engineers (how do we hire, what do we value, how do we work together, etc.). It's much easier to do this as a flat team with a single leader.&lt;/item&gt;
      &lt;item&gt;pivots and radical decisions could still happen frequently, which will be exponentially harder if you have to manage these engineers through 2 or 3 line managers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The only nuance I would add, if you really need to start structuring the team, is to go with hybrid roles: maybe it's a very hands-on manager who still codes 70% of the time, maybe it's elevating a few key engineers into informal tech lead positions&lt;/p&gt;
    &lt;head rend="h3"&gt;The early growth stage (going from 20 to 50 engineers)&lt;/head&gt;
    &lt;p&gt;This is the sweet spot where the benefit of adding more management and more structure should outweigh the cost of letting the inevitable chaos of a larger team take a life of its own. Still, I would highly recommend a less-is-more approach.&lt;/p&gt;
    &lt;p&gt;Here are a few signs you've reached that stage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the CTO / whoever is managing everyone shows signs of burning out under the load&lt;/item&gt;
      &lt;item&gt;adding more engineers no longer increases output, meaning you are constrained by team inefficiency&lt;/item&gt;
      &lt;item&gt;the team excels at week-to-week impact, but nobody seems able to play out what will happen in 3 to 6 months&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a vast topic, and I'll dedicate a future article to that specific stage, including how to hire your first head of engineering.&lt;/p&gt;
    &lt;head rend="h2"&gt;Do not copy Google&lt;/head&gt;
    &lt;p&gt;This section addresses two sides of the same coin, both related to the halo effect surrounding great companies and more specifically their management practices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Applying management ideas that Google (or other successful company) have talked about and made popular&lt;/item&gt;
      &lt;item&gt;Applying the meta-idea of innovating in the field of management (like Google did in their time)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I'll skip to the conclusion and explain it below:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When in doubt, always pick the "node &amp;amp; postgres" stack of management. Do not innovate, keep it boring.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;What I mean by the "node &amp;amp; postgres" of management&lt;/head&gt;
    &lt;p&gt;Node &amp;amp; postgres share these common traits: they have huge communities, their bugs and quirks have been explored by millions of people, and so they are great choices for early-stage startups compared to, say, C++ and OracleDB. No matter what you think about their technical merits, it would be very hard to point to them as a reason why a startup failed. They are just solid, boring tools, and they work at the early stage.&lt;/p&gt;
    &lt;p&gt;You should use the same type of boring, widely used, stage-appropriate tools when it comes to managing your startup. Every ounce of "innovation" you spend on your organizational structure, title philosophy, or new-age 1:1 is an ounce you aren't spending on your product. At the seed stage, your culture shouldn't be unique because of your clever peer feedback system, it should be unique because of the speed at which you solve customer problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is the boring stack of seed stage management&lt;/head&gt;
    &lt;p&gt;As a conclusion to this section and to the entire article, I want to share, somewhat paradoxically, a few useful management activities specifically for the early stage. They almost all share the same "reluctant" approach to engineering management, which I think is a healthy leadership approach at that particular stage.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hire inherently motivated people: see first section&lt;/item&gt;
      &lt;item&gt;Don't manage around a hiring mistake, let them go quickly and gracefully&lt;/item&gt;
      &lt;item&gt;Asynchronous status updates: do not adopt all the "Scrum rituals" like standups, retros, etc. wholesale, and if you do, keep them asynchronous. There is little added value to a voiced update, even if it makes you feel good that people are indeed working hard and showing up to the standup on time!&lt;/item&gt;
      &lt;item&gt;An avoidant relationship to Slack: while Slack is a given in today's distributed or hybrid teams, it can quickly become an attention destroyer, especially for engineers who need uninterrupted time to work. Keep it in check.&lt;/item&gt;
      &lt;item&gt;Organic 1:1s (as opposed to recurring ones): keep them topic-heavy and ad-hoc, as opposed to relationship maintenance like in the corporate world.&lt;/item&gt;
      &lt;item&gt;Unstructured documents over systems of records: unless you need to itemize tasks for audit purposes, a few notion or google docs can actually scale for 10-15 engineers, especially given current AI tools. They have very little overhead and are unbeatable in terms of flexibility.&lt;/item&gt;
      &lt;item&gt;Extreme transparency: give everyone access to everything (customer call notes, investor updates, budgets, etc.). Not only will you build trust with the team, but you will also remove the need to "communicate" (as in, filtering and processing information), which is a typical management task.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To be clear, many of these practices do not scale past 20-25 engineers, but that's part of the point.&lt;/p&gt;
    &lt;p&gt;I hope you found this post actionable, good luck with building your team!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605854</guid><pubDate>Tue, 13 Jan 2026 18:54:30 +0000</pubDate></item><item><title>A university got itself banned from the Linux kernel (2021)</title><link>https://www.theverge.com/2021/4/30/22410164/linux-kernel-university-of-minnesota-banned-open-source</link><description>&lt;doc fingerprint="bcf9d91f19a786ac"&gt;
  &lt;main&gt;
    &lt;p&gt;On the evening of April 6th, a student emailed a patch to a list of developers. Fifteen days later, the University of Minnesota was banned from contributing to the Linux kernel.&lt;/p&gt;
    &lt;head rend="h1"&gt;How a university got itself banned from the Linux kernel&lt;/head&gt;
    &lt;p&gt;The University of Minnesotaâ€™s path to banishment was long, turbulent, and full of emotion&lt;/p&gt;
    &lt;head rend="h1"&gt;How a university got itself banned from the Linux kernel&lt;/head&gt;
    &lt;p&gt;The University of Minnesotaâ€™s path to banishment was long, turbulent, and full of emotion&lt;/p&gt;
    &lt;p&gt;â€œI suggest you find a different community to do experiments on,â€ wrote Linux Foundation fellow Greg Kroah-Hartman in a livid email. â€œYou are not welcome here.â€&lt;/p&gt;
    &lt;p&gt;How did one email lead to a university-wide ban? Iâ€™ve spent the past week digging into this world â€” the players, the jargon, the universityâ€™s turbulent history with open-source software, the devoted and principled Linux kernel community. None of the University of Minnesota researchers would talk to me for this story. But among the other major characters â€” the Linux developers â€” there was no such hesitancy. This was a community eager to speak; it was a community betrayed.&lt;/p&gt;
    &lt;p&gt;The story begins in 2017, when a systems-security researcher named Kangjie Lu became an assistant professor at the University of Minnesota.&lt;/p&gt;
    &lt;p&gt;Luâ€™s research, per his website, concerns â€œthe intersection of security, operating systems, program analysis, and compilers.â€ But Lu had his eye on Linux â€” most of his papers involve the Linux kernel in some way.&lt;/p&gt;
    &lt;p&gt;The Linux kernel is, at a basic level, the core of any Linux operating system. Itâ€™s the liaison between the OS and the device on which itâ€™s running. A Linux user doesnâ€™t interact with the kernel, but itâ€™s essential to getting things done â€” it manages memory usage, writes things to the hard drive, and decides what tasks can use the CPU when. The kernel is open-source, meaning its millions of lines of code are publicly available for anyone to view and contribute to.&lt;/p&gt;
    &lt;p&gt;Getting a patch on peopleâ€™s computers is no easy task&lt;/p&gt;
    &lt;p&gt;Well, â€œanyone.â€ Getting a patch onto peopleâ€™s computers is no easy task. A submission needs to pass through a large web of developers and â€œmaintainersâ€ (thousands of volunteers, who are each responsible for the upkeep of different parts of the kernel) before it ultimately ends up in the mainline repository. Once there, it goes through a long testing period before eventually being incorporated into the â€œstable release,â€ which will go out to mainstream operating systems. Itâ€™s a rigorous system designed to weed out both malicious and incompetent actors. But â€” as is always the case with crowdsourced operations â€” thereâ€™s room for human error.&lt;/p&gt;
    &lt;p&gt;Some of Luâ€™s recent work has revolved around studying that potential for human error and reducing its influence. Heâ€™s proposed systems to automatically detect various types of bugs in open source, using the Linux kernel as a test case. These experiments tend to involve reporting bugs, submitting patches to Linux kernel maintainers, and reporting their acceptance rates. In a 2019 paper, for example, Lu and two of his PhD students, Aditya Pakki and Qiushi Wu, presented a system (â€œCrixâ€) for detecting a certain class of bugs in OS kernels. The trio found 278 of these bugs with Crix and submitted patches for all of them â€” the fact that maintainers accepted 151 meant the tool was promising.&lt;/p&gt;
    &lt;p&gt;On the whole, it was a useful body of work. Then, late last year, Lu took aim not at the kernel itself, but at its community.&lt;/p&gt;
    &lt;p&gt;In â€œOn the Feasibility of Stealthily Introducing Vulnerabilities in Open-Source Software via Hypocrite Commits,â€ Lu and Wu explained that theyâ€™d been able to introduce vulnerabilities into the Linux kernel by submitting patches that appeared to fix real bugs but also introduced serious problems. The group called these submissions â€œhypocrite commits.â€ (Wu didnâ€™t respond to a request for comment for this story; Lu referred me to Mats Heimdahl, the head of the universityâ€™s department of computer science and engineering, who referred me to the departmentâ€™s website.)&lt;/p&gt;
    &lt;p&gt;The explicit goal of this experiment, as the researchers have since emphasized, was to improve the security of the Linux kernel by demonstrating to developers how a malicious actor might slip through their net. One could argue that their process was similar, in principle, to that of white-hat hacking: play around with software, find bugs, let the developers know.&lt;/p&gt;
    &lt;p&gt;But the loudest reaction the paper received, on Twitter and across the Linux community, wasnâ€™t gratitude â€” it was outcry.&lt;/p&gt;
    &lt;p&gt;â€œThat paper, itâ€™s just a lot of crap,â€ says Greg Scott, an IT professional who has worked with open-source software for over 20 years.&lt;/p&gt;
    &lt;p&gt;â€œIn my personal view, it was completely unethical,â€ says security researcher Kenneth White, who is co-director of the Open Crypto Audit Project.&lt;/p&gt;
    &lt;p&gt;The frustration had little to do with the hypocrite commits themselves. In their paper, Lu and Wu claimed that none of their bugs had actually made it to the Linux kernel â€” in all of their test cases, theyâ€™d eventually pulled their bad patches and provided real ones. Kroah-Hartman, of the Linux Foundation, contests this â€” he told The Verge that one patch from the study did make it into repositories, though he notes it didnâ€™t end up causing any harm.&lt;/p&gt;
    &lt;p&gt;â€œIn my personal view, it was completely unethical.â€&lt;/p&gt;
    &lt;p&gt;Still, the paper hit a number of nerves among a very passionate (and very online) community when Lu first shared its abstract on Twitter. Some developers were angry that the university had intentionally wasted the maintainersâ€™ time â€” which is a key difference between Minnesotaâ€™s work and a white-hat hacker poking around the Starbucks app for a bug bounty. â€œThe researchers crossed a line they shouldnâ€™t have crossed,â€ Scott says. â€œNobody hired this group. They just chose to do it. And a whole lot of people spent a whole lot of time evaluating their patches.â€&lt;/p&gt;
    &lt;p&gt;â€œIf I were a volunteer putting my personal time into commits and testing, and then I found out someoneâ€™s experimenting, I would be unhappy,â€ Scott adds.&lt;/p&gt;
    &lt;p&gt;Then, thereâ€™s the dicier issue of whether an experiment like this amounts to human experimentation. It doesnâ€™t, according to the University of Minnesotaâ€™s Institutional Review Board. Lu and Wu applied for approval in response to the outcry, and they were granted a formal letter of exemption.&lt;/p&gt;
    &lt;p&gt;The community members I spoke to didnâ€™t buy it. â€œThe researchers attempted to get retroactive Institutional Review Board approval on their actions that were, at best, wildly ignorant of the tenants of basic human subjectsâ€™ protections, which are typically taught by senior year of undergraduate institutions,â€ says White.&lt;/p&gt;
    &lt;p&gt;â€œIt is generally not considered a nice thing to try to do â€˜researchâ€™ on people who do not know you are doing research,â€ says Kroah-Hartman. â€œNo one asked us if it was acceptable.â€&lt;/p&gt;
    &lt;p&gt;â€œThat paper, itâ€™s just a lot of crap.â€&lt;/p&gt;
    &lt;p&gt;That thread ran through many of the responses I got from developers â€” that regardless of the harms or benefits that resulted from its research, the university was messing around not just with community members but with the communityâ€™s underlying philosophy. Anyone who uses an operating system places some degree of trust in the people who contribute to and maintain that system. Thatâ€™s especially true for people who use open-source software, and itâ€™s a principle that some Linux users take very seriously.&lt;/p&gt;
    &lt;p&gt;â€œBy definition, open source depends on a lively community,â€ Scott says. â€œThere have to be people in that community to submit stuff, people in the community to document stuff, and people to use it and to set up this whole feedback loop to constantly make it stronger. That loop depends on lots of people, and you have to have a level of trust in that system ... If somebody violates that trust, that messes things up.â€&lt;/p&gt;
    &lt;p&gt;After the paperâ€™s release, it was clear to many Linux kernel developers that something needed to be done about the University of Minnesota â€” previous submissions from the university needed to be reviewed. â€œMany of us put an item on our to-do list that said, â€˜Go and audit all umn.edu submissions,â€™â€ said Kroah-Hartman, who was, above all else, annoyed that the experiment had put another task on his plate. But many kernel maintainers are volunteers with day jobs, and a large-scale review process didnâ€™t materialize. At least, not in 2020.&lt;/p&gt;
    &lt;p&gt;On April 6th, 2021, Aditya Pakki, using his own email address, submitted a patch.&lt;/p&gt;
    &lt;p&gt;There was some brief discussion from other developers on the email chain, which fizzled out within a few days. Then Kroah-Hartman took a look. He was already on high alert for bad code from the University of Minnesota, and Pakkiâ€™s email address set off alarm bells. Whatâ€™s more, the patch Pakki submitted didnâ€™t appear helpful. â€œIt takes a lot of effort to create a change that looks correct, yet does something wrong,â€ Kroah-Hartman told me. â€œThese submissions all fit that pattern.â€&lt;/p&gt;
    &lt;p&gt;So on April 20th, Kroah-Hartman put his foot down.&lt;/p&gt;
    &lt;p&gt;â€œPlease stop submitting known-invalid patches,â€ he wrote to Pakki. â€œYour professor is playing around with the review process in order to achieve a paper in some strange and bizarre way.â€&lt;/p&gt;
    &lt;p&gt;Maintainer Leon Romanovsky then chimed in: heâ€™d taken a look at four previously accepted patches from Pakki and found that three of them added â€œvarious severityâ€ security vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Thereâ€™s the dicier issue of whether an experiment like this amounts to human experimentation&lt;/p&gt;
    &lt;p&gt;Kroah-Hartman hoped that his request would be the end of the affair. But then Pakki lashed back. â€œI respectfully ask you to cease and desist from making wild accusations that are bordering on slander,â€ he wrote to Kroah-Hartman in what appears to be a private message.&lt;/p&gt;
    &lt;p&gt;Kroah-Hartman responded. â€œYou and your group have publicly admitted to sending known-buggy patches to see how the kernel community would react to them, and published a paper based on that work. Now you submit a series of obviously-incorrect patches again, so what am I supposed to think of such a thing?â€ he wrote back on the morning of April 21st.&lt;/p&gt;
    &lt;p&gt;Later that day, Kroah-Hartman made it official. â€œFuture submissions from anyone with a umn.edu address should be default-rejected unless otherwise determined to actually be a valid fix,â€ he wrote in an email to a number of maintainers, as well as Lu, Pakki, and Wu. Kroah-Hartman reverted 190 submissions from Minnesota affiliates â€” 68 couldnâ€™t be reverted but still needed manual review.&lt;/p&gt;
    &lt;p&gt;Itâ€™s not clear what experiment the new patch was part of, and Pakki declined to comment for this story. Luâ€™s website includes a brief reference to â€œsuperfluous patches from Aditya Pakki for a new bug-finding project.â€&lt;/p&gt;
    &lt;p&gt;What is clear is that Pakkiâ€™s antics have finally set the delayed review process in motion; Linux developers began digging through all patches that university affiliates had submitted in the past. Jonathan Corbet, the founder and editor in chief of LWN.net, recently provided an update on that review process. Per his assessment, â€œMost of the suspect patches have turned out to be acceptable, if not great.â€ Of over 200 patches that were flagged, 42 are still set to be removed from the kernel.&lt;/p&gt;
    &lt;p&gt;Regardless of whether their reaction was justified, the Linux community gets to decide if the University of Minnesota affiliates can contribute to the kernel again. And that community has made its demands clear: the school needs to convince them its future patches wonâ€™t be a waste of anyoneâ€™s time.&lt;/p&gt;
    &lt;p&gt;What will it take to do that? In a statement released the same day as the ban, the universityâ€™s computer science department suspended its research into Linux-kernel security and announced that it would investigate Luâ€™s and Wuâ€™s research method.&lt;/p&gt;
    &lt;p&gt;But that wasnâ€™t enough for the Linux Foundation. Mike Dolan, Linux Foundation SVP and GM of projects, wrote a letter to the university on April 23rd, which The Verge has viewed. Dolan made four demands. He asked that the school release â€œall information necessary to identify all proposals of known-vulnerable code from any U of MN experimentâ€ to help with the audit process. He asked that the paper on hypocrite commits be withdrawn from publication. He asked that the school ensure future experiments undergo IRB review before they begin, and that future IRB reviews ensure the subjects of experiments provide consent, â€œper usual research norms and laws.â€&lt;/p&gt;
    &lt;p&gt;The school needs to convince them its future patches wonâ€™t be a waste of anyoneâ€™s time&lt;/p&gt;
    &lt;p&gt;Two of those demands have since been met. Wu and Lu have retracted the paper and have released all the details of their study.&lt;/p&gt;
    &lt;p&gt;The universityâ€™s status on the third and fourth counts is unclear. In a letter sent to the Linux Foundation on April 27th, Heimdahl and Loren Terveen (the computer science and engineering departmentâ€™s associate department head) maintain that the universityâ€™s IRB â€œacted properly,â€ and argues that human-subjects research â€œhas a precise technical definition according to US federal regulations ... and this technical definition may not accord with intuitive understanding of concepts like â€˜experimentsâ€™ or even â€˜experiments on people.â€™â€ They do, however, commit to providing more ethics training for department faculty. Reached for comment, university spokesperson Dan Gilchrist referred me to the computer science and engineering departmentâ€™s website.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Lu, Wu, and Pakki apologized to the Linux community this past Saturday in an open letter to the kernel mailing list, which contained some apology and some defense. â€œWe made a mistake by not finding a way to consult with the community and obtain permission before running this study; we did that because we knew we could not ask the maintainers of Linux for permission, or they would be on the lookout for hypocrite patches,â€ the researchers wrote, before going on to reiterate that they hadnâ€™t put any vulnerabilities into the Linux kernel, and that their other patches werenâ€™t related to the hypocrite commits research.&lt;/p&gt;
    &lt;p&gt;Kroah-Hartman wasnâ€™t having it. â€œThe Linux Foundation and the Linux Foundationâ€™s Technical Advisory Board submitted a letter on Friday to your university,â€ he responded. â€œUntil those actions are taken, we do not have anything further to discuss.â€&lt;/p&gt;
    &lt;p&gt;From the University of Minnesota researchersâ€™ perspective, they didnâ€™t set out to troll anyone â€” they were trying to point out a problem with the kernel maintainersâ€™ review process. Now the Linux community has to reckon with the fallout of their experiment and what it means about the security of open-source software.&lt;/p&gt;
    &lt;p&gt;Some developers rejected University of Minnesota researchersâ€™ perspective outright, claiming the fact that itâ€™s possible to fool maintainers should be obvious to anyone familiar with open-source software. â€œIf a sufficiently motivated, unscrupulous person can put themselves into a trusted position of updating critical software, thereâ€™s honestly little that can be done to stop them,â€ says White, the security researcher.&lt;/p&gt;
    &lt;p&gt;On the other hand, itâ€™s clearly important to be vigilant about potential vulnerabilities in any operating system. And for others in the Linux community, as much ire as the experiment drew, its point about hypocrite commits appears to have been somewhat well taken. The incident has ignited conversations about patch-acceptance policies and how maintainers should handle submissions from new contributors, across Twitter, email lists, and forums. â€œDemonstrating this kind of â€˜attackâ€™ has been long overdue, and kicked off a very important discussion,â€ wrote maintainer Christoph Hellwig in an email thread with other maintainers. â€œI think they deserve a medal of honor.â€&lt;/p&gt;
    &lt;p&gt;â€œThis research was clearly unethical, but it did make it plain that the OSS development model is vulnerable to bad-faith commits,â€ one user wrote in a discussion post. â€œIt now seems likely that Linux has some devastating back doors.â€&lt;/p&gt;
    &lt;p&gt;Corbet also called for more scrutiny around new changes in his post about the incident. â€œIf we cannot institutionalize a more careful process, we will continue to see a lot of bugs, and it will not really matter whether they were inserted intentionally or not,â€ he wrote.&lt;/p&gt;
    &lt;p&gt;â€œThis method works.â€&lt;/p&gt;
    &lt;p&gt;And even for some of the paperâ€™s most ardent critics, the process did prove a point â€” albeit, perhaps, the opposite of the one Wu, Lu, and Pakki were trying to make. It demonstrated that the system worked.&lt;/p&gt;
    &lt;p&gt;Eric Mintz, who manages 25 Linux servers, says this ban has made him much more confident in the operating systemâ€™s security. â€œI have more trust in the process because this was caught,â€ he says. â€œThere may be compromises we donâ€™t know about. But because we caught this one, itâ€™s less likely we donâ€™t know about the other ones. Because we have something in place to catch it.â€&lt;/p&gt;
    &lt;p&gt;To Scott, the fact that the researchers were caught and banned is an example of Linuxâ€™s system functioning exactly the way itâ€™s supposed to. â€œThis method worked,â€ he insists. â€œThe SolarWinds method, where thereâ€™s a big corporation behind it, that system didnâ€™t work. This system did work.â€&lt;/p&gt;
    &lt;p&gt;â€œKernel developers are happy to see new tools created and â€” if the tools give good results â€” use them. They will also help with the testing of these tools, but they are less pleased to be recipients of tool-inspired patches that lack proper review,â€ Corbet writes. The community seems to be open to the University of Minnesotaâ€™s feedback â€” but as the Foundation has made clear, itâ€™s on the school to make amends.&lt;/p&gt;
    &lt;p&gt;â€œThe university could repair that trust by sincerely apologizing, and not fake apologizing, and by maybe sending a lot of beer to the right people,â€ Scott says. â€œItâ€™s gonna take some work to restore their trust. So hopefully theyâ€™re up to it.â€&lt;/p&gt;
    &lt;head rend="h2"&gt;Most Popular&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Apple picks Googleâ€™s Gemini AI for its big Siri upgrade&lt;/item&gt;
      &lt;item&gt;What Apple and Googleâ€™s Gemini deal means for both companies&lt;/item&gt;
      &lt;item&gt;Amazon has started automatically upgrading Prime members to Alexa Plus&lt;/item&gt;
      &lt;item&gt;Anthropic wants you to use Claude to â€˜Coworkâ€™ in latest AI agent push&lt;/item&gt;
      &lt;item&gt;Apple Creator Studio suite is launching to take on Adobe&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46605950</guid><pubDate>Tue, 13 Jan 2026 18:58:55 +0000</pubDate></item><item><title>Ask HN: Discrepancy between Lichess and Stockfish</title><link>https://news.ycombinator.com/item?id=46606715</link><description>&lt;doc fingerprint="395137024d4c12eb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Iâ€™m trying to understand a discrepancy between Lichessâ€™s analysis board and my own Stockfish setup.&lt;/p&gt;
      &lt;p&gt;On Lichess (browser-based analysis), Stockfish reports close to 1 MN/s on my Redmi Note 14 Pro. However, when I run Stockfish locally via a Python program using the native executable, I only see around 600 kN/s.&lt;/p&gt;
      &lt;p&gt;Whatâ€™s confusing is that despite the higher reported speed, Lichess takes about 2:30 to reach depth 30, while my local setup reaches depth 30 in about 53 seconds, even though it reports a lower N/s. Lichess also appears much more â€œactiveâ€ in terms of frequent evaluation updates.&lt;/p&gt;
      &lt;p&gt;I suspect this has to do with how N/s is measured or displayed (instantaneous vs average), differences in search configuration (continuous search vs restarts, MultiPV, hash reuse), or overhead from the way the engine is driven (e.g., UI or I/O throttling). It also raises the question of whether â€œdepth 30â€ is directly comparable across different frontends.&lt;/p&gt;
      &lt;p&gt;Has anyone looked into how Lichess reports Stockfish speed, or why a setup showing higher N/s can still take significantly longer to reach the same nominal depth?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46606715</guid><pubDate>Tue, 13 Jan 2026 19:44:44 +0000</pubDate></item><item><title>Ask HN: Vxlan over WireGuard or WireGuard over Vxlan?</title><link>https://news.ycombinator.com/item?id=46606854</link><description>&lt;doc fingerprint="5ffbd2e7d0a5922f"&gt;
  &lt;main&gt;
    &lt;p&gt;https://man.openbsd.org/vxlan.4#SECURITY seems unambiguous that it's intended for use in trusted environments (and all else being equal, I'd expect the openbsd man page authors to have reasonable opinions about network security), so it sounds like vxlan over ipsec/wg is probably the better route?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46606854</guid><pubDate>Tue, 13 Jan 2026 19:53:07 +0000</pubDate></item><item><title>Open sourcing Dicer: Databricks's auto-sharder</title><link>https://www.databricks.com/blog/open-sourcing-dicer-databricks-auto-sharder</link><description>&lt;doc fingerprint="2758961f4195ccc6"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we are excited to announce the open sourcing of one of our most critical infrastructure components, Dicer: Databricksâ€™ auto-sharder, a foundational system designed to build low latency, scalable, and highly reliable sharded services. It is behind the scenes of every major Databricks product, enabling us to deliver a consistently fast user experience while improving fleet efficiency and reducing cloud costs. Dicer achieves this by dynamically managing sharding assignments to keep services responsive and resilient even in the face of restarts, failures, and shifting workloads. As detailed in this blog post, Dicer is used for a variety of use cases including high-performance serving, work partitioning, batching pipelines, data aggregation, multi-tenancy, soft leader election, efficient GPU utilization for AI workloads, and more.&lt;/p&gt;
    &lt;p&gt;By making Dicer available to the broader community, we look forward to collaborating with industry and academia to advance the state of the art in building robust, efficient, and high-performance distributed systems. In the rest of this post, we discuss the motivation and design philosophy behind Dicer, share success stories from its use at Databricks, and provide a guide on how to install and experiment with the system yourself.&lt;/p&gt;
    &lt;p&gt;Databricks ships a rapidly expanding suite of products for data processing, analytics, and AI. To support this at scale, we operate hundreds of services that must handle massive state while maintaining responsiveness. Historically, Databricks engineers had relied on two common architectures, but both introduced significant problems as services grew:&lt;/p&gt;
    &lt;p&gt;Most services at Databricks began with a stateless model. In a typical stateless model, the application does not retain in-memory state across requests, and must re-read the data from the database on every request. This architecture is inherently expensive as every request incurs a database hit, driving up both operational costs and latency [1].&lt;/p&gt;
    &lt;p&gt;To mitigate these costs, developers would often introduce a remote cache (like Redis or Memcached) to offload work from the database. While this improved throughput and latency, it failed to solve several fundamental inefficiencies:&lt;/p&gt;
    &lt;p&gt;Moving to a sharded model and caching state in memory eliminated these layers of overhead by colocating the state directly with the logic that operates on it. However, static sharding introduced new problems.&lt;/p&gt;
    &lt;p&gt;Before Dicer, sharded services at Databricks relied on static sharding techniques (e.g., consistent hashing). While this approach was simple and allowed our services to efficiently cache state in memory, it introduced three critical issues in production:&lt;/p&gt;
    &lt;p&gt;As our services grew more and more to meet demand, eventually static sharding looked like a terrible idea. This led to a common belief among our engineers that stateless architectures were the best way to build robust systems, even if it meant eating the performance and resource costs. This was around the time when Dicer was introduced.&lt;/p&gt;
    &lt;p&gt;The production perils of static sharding, contrasted with the costs of going stateless, left several of our most critical services in a difficult position. These services relied on static sharding to deliver a snappy user experience to our customers. Converting them to a stateless model would have introduced a significant performance penalty, not to mention added cloud costs for us.&lt;/p&gt;
    &lt;p&gt;We built Dicer to change this. Dicer addresses the fundamental shortcomings of static sharding by introducing an intelligent control plane that continuously and asynchronously updates a serviceâ€™s shard assignments. It reacts to a wide range of signals, including application health, load, termination notices, and other environmental inputs. As a result, Dicer keeps services highly available and well balanced even during rolling restarts, crashes, autoscaling events, and periods of severe load skew.&lt;/p&gt;
    &lt;p&gt;As an auto-sharder, Dicer builds on a long line of prior systems, including Centrifuge [3], Slicer [4], and Shard Manager [5]. We introduce Dicer in the next section, and describe how it has helped improve performance, reliability, and efficiency of our services.&lt;/p&gt;
    &lt;p&gt;We now give an overview of Dicer, its core abstractions, and describe its various use cases. Stay tuned for a technical deep dive into Dicerâ€™s design and architecture in a future blog post.&lt;/p&gt;
    &lt;p&gt;Dicer models an application as serving requests (or otherwise performing some work) associated with a logical key. For example, a service that serves user profiles might use user IDs as its keys. Dicer shards the application by continuously generating an assignment of keys to pods to keep the service highly available and load balanced.&lt;/p&gt;
    &lt;p&gt;To scale to applications with millions or billions of keys, Dicer operates on ranges of keys rather than individual keys. Applications represent keys to Dicer using a SliceKey (a hash of the application key), and a contiguous range of SliceKeys is called a Slice. As shown in Figure 1, a Dicer Assignment is a collection of Slices that together span the full application keyspace, with each Slice assigned to one or more Resources (i.e. pods). Dicer dynamically splits, merges, replicates, and reassigns Slices in response to application health and load signals, ensuring that the entire keyspace is always assigned to healthy pods and that no single pod becomes overloaded. Dicer can also detect hot keys and split them out into their own slices, and assign such slices to multiple pods to distribute the load.&lt;/p&gt;
    &lt;p&gt;Figure 1 shows an example Dicer assignment across 3 pods (P0, P1, and P2) for an application sharded by user ID, where the user with ID 13 is represented by SliceKey K26 (i.e. a hash of ID 13), and is currently assigned to pod P0. A hot user with user ID 42 and represented by SliceKey K10 has been isolated in its own slice and assigned to multiple pods to handle the load (P1 and P2).&lt;/p&gt;
    &lt;p&gt;Figure 2 shows an overview of a sharded application integrated with Dicer. Application pods learn the current assignment through a library called the Slicelet (S for server side). The Slicelet maintains a local cache of the latest assignment by fetching it from the Dicer service and watching for updates. When it receives an updated assignment, the Slicelet notifies the application via a listener API.&lt;/p&gt;
    &lt;p&gt;Assignments observed by Slicelets are eventually consistent, a deliberate design choice that prioritizes availability and fast recovery over strong key ownership guarantees. In our experience this has been the right model for the vast majority of applications, though we do plan to support stronger guarantees in the future, similar to Slicer and Centrifuge.&lt;/p&gt;
    &lt;p&gt;Besides keeping up-to-date on the assignment, applications also use the Slicelet to record per key load when handling requests or performing work for a key. The Slicelet aggregates this information locally and asynchronously reports a summary to the Dicer service. Note that, like assignment watching, this also occurs off the applicationâ€™s critical path, ensuring high performance.&lt;/p&gt;
    &lt;p&gt;Clients of a Dicer sharded application find the assigned pod for a given key through a library called the Clerk (C for client side). Like Slicelets, Clerks also actively maintain a local cache of the latest assignment in the background to ensure high performance for key lookups on the critical path.&lt;/p&gt;
    &lt;p&gt;Finally, the Dicer Assigner is the controller service responsible for generating and distributing assignments based on application health and load signals. At its core is a sharding algorithm that computes minimal adjustments through Slice splits, merges, replication/dereplication, and moves to keep keys assigned to healthy pods and the overall application sufficiently load balanced. The Assigner service is multi-tenant and designed to provide auto-sharding service for all sharded applications within a region. Each sharded application served by Dicer is referred to as a Target.&lt;/p&gt;
    &lt;p&gt;Dicer is valuable for a wide range of systems because the ability to affinitize workloads to specific pods yields significant performance improvements. We have identified several core categories of use cases based on our production experience.&lt;/p&gt;
    &lt;p&gt;Dicer excels at scenarios where a large corpus of data must be loaded and served directly from memory. By ensuring that requests for specific keys always hit the same pods, services like key-value stores can achieve sub-millisecond latency and high throughput while avoiding the overhead of fetching data from remote storage.&lt;/p&gt;
    &lt;p&gt;Dicer is also well suited to modern LLM inference workloads, where maintaining affinity is critical. Examples include stateful user sessions that accumulate context in a per-session KV cache, as well as deployments that serve large numbers of LoRA adapters and must shard them efficiently across constrained GPU resources.&lt;/p&gt;
    &lt;p&gt;This is one of the most common use cases at Databricks. It includes systems such as cluster managers and query orchestration engines that continuously monitor resources to manage scaling, compute scheduling, and multi-tenancy. To operate efficiently, these systems maintain monitoring and control state locally, avoiding repeated serialization and enabling timely responses to change.&lt;/p&gt;
    &lt;p&gt;Dicer can be used to build high-performance distributed remote caches, which we have done in production at Databricks. By using Dicerâ€™s capabilities, our cache can be autoscaled and restarted seamlessly without loss of hit rate, and avoid load imbalance due to hot keys.&lt;/p&gt;
    &lt;p&gt;Dicer is an effective tool for partitioning background tasks and asynchronous workflows across a fleet of servers. For example, a service responsible for cleaning up or garbage-collecting state in a massive table can use Dicer to ensure that each pod is responsible for a distinct, non-overlapping range of the keyspace, preventing redundant work and lock contention.&lt;/p&gt;
    &lt;p&gt;For high-volume write paths, Dicer enables efficient record aggregation. By routing related records to the same pod, the system can batch updates in memory before committing them to persistent storage. This significantly reduces the input/output operations per second required and improves the overall throughput of the data pipeline.&lt;/p&gt;
    &lt;p&gt;Dicer can be used to implement "soft" leader selection by designating a specific pod as the primary coordinator for a given key or shard. For example, a serving scheduler can use Dicer to ensure that a single pod acts as the primary authority for managing a group of resources. While Dicer currently provides affinity-based leader selection, it serves as a powerful foundation for systems that require a coordinated primary without the heavy overhead of traditional consensus protocols. We are exploring future enhancements to provide stronger guarantees around mutual exclusion for these workloads.&lt;/p&gt;
    &lt;p&gt;Dicer acts as a natural rendezvous point for distributed clients needing real-time coordination. By routing all requests for a specific key to the same pod, that pod becomes a central meeting place where shared state can be managed in local memory without external network hops.&lt;/p&gt;
    &lt;p&gt;For example, in a real-time chat service, two clients joining the same "Chat Room ID" are automatically routed to the same pod. This allows the pod to synchronize their messages and state instantly in memory, avoiding the latency of a shared database or a complex back-plane for communication.&lt;/p&gt;
    &lt;p&gt;Numerous services at Databricks have achieved significant gains with Dicer, and we highlight several of these success stories below.&lt;/p&gt;
    &lt;p&gt;Unity Catalog (UC) is the unified governance solution for data and AI assets across the Databricks platform. Originally designed as a stateless service, UC faced significant scaling challenges as its popularity grew, driven primarily by extremely high read volume. Serving each request required repeated access to the backend database, which introduced prohibitive latency. Conventional approaches such as remote caching were not viable, as the cache needed to be updated incrementally and remain snapshot consistent with storage. In addition, customer catalogs can be gigabytes in size, making it costly to maintain partial or replicated snapshots in a remote cache without introducing substantial overhead.&lt;/p&gt;
    &lt;p&gt;To solve this, the team integrated Dicer to build a sharded in-memory stateful cache. This shift allowed UC to replace expensive remote network calls with local method calls, drastically reducing database load and improving responsiveness. The figure below illustrates the initial rollout of Dicer, followed by the deployment of the full Dicer integration. By utilizing Dicerâ€™s stateful affinity, UC achieved a cache hit rate of 90â€“95%, significantly lowering the frequency of database round-trips.&lt;/p&gt;
    &lt;p&gt;Databricksâ€™ query orchestration engine, which manages query scheduling on Spark clusters, was originally built as an in-memory stateful service using static sharding. As the service scaled, the limitations of this architecture became a significant bottleneck; due to the simple implementation, scaling required manual re-sharding which was extremely toilsome, and the system suffered from frequent availability dips, even during rolling restarts.&lt;/p&gt;
    &lt;p&gt;After integrating with Dicer, these availability issues were eliminated (see Figure 4). Dicer enabled zero downtime during restarts and scaling events, allowing the team to reduce toil and improve system robustness by enabling auto-scaling everywhere. Additionally, Dicerâ€™s dynamic load balancing feature further resolved chronic CPU throttling, resulting in more consistent performance across the fleet.&lt;/p&gt;
    &lt;p&gt;For services that are not sharded, we developed Softstore, a distributed remote key value cache. Softstore leverages a Dicer feature called state transfer, which migrates data between pods during resharding to preserve application state. This is particularly important during planned rolling restarts, where the full keyspace is unavoidably churned. In our production fleet, planned restarts account for roughly 99.9% of all restarts, making this mechanism especially impactful and enables seamless restarts with negligible impact on cache hit rates. Figure 5 shows Softstore hit rates during a rolling restart, where state transfer preserves a steady ~85% hit rate for a representative use case, with the remaining variability driven by normal workload fluctuations.&lt;/p&gt;
    &lt;p&gt;You can try out Dicer today on your machine by downloading it from here. A simple demo to show its usage is provided here - it shows a sample Dicer setup with one client and a few servers for an application. Please see the README and user guide for Dicer.&lt;/p&gt;
    &lt;p&gt;Dicer is a critical service used across Databricks with its usage growing quickly. In the future, we will be publishing more articles about Dicerâ€™s inner workings and designs. We will also release more features as we build and test them out internally, e.g., Java and Rust libraries for clients and servers, and the state transfer capabilities mentioned in this post. Please give us your feedback and stay tuned for more!&lt;/p&gt;
    &lt;p&gt;If you like solving tough engineering problems and would like to join Databricks, check out databricks.com/careers!&lt;/p&gt;
    &lt;p&gt;[1] Ziming Mao, Jonathan Ellithorpe, Atul Adya, Rishabh Iyer, Matei Zaharia, Scott Shenker, Ion Stoica (2025). Rethinking the cost of distributed caches for datacenter services. Proceedings of the 24th ACM Workshop on Hot Topics in Networks, 1â€“8.&lt;/p&gt;
    &lt;p&gt;[2] Atul Adya, Robert Grandl, Daniel Myers, Henry Qin. Fast key-value stores: An idea whose time has come and gone. Proceedings of the Workshop on Hot Topics in Operating Systems (HotOS â€™19), May 13â€“15, 2019, Bertinoro, Italy. ACM, 7 pages. DOI: 10.1145/3317550.3321434.&lt;/p&gt;
    &lt;p&gt;[3] Atul Adya, James Dunagan, Alexander Wolman. Centrifuge: Integrated Lease Management and Partitioning for Cloud Services. Proceedings of the 7th USENIX Symposium on Networked Systems Design and Implementation (NSDI), 2010.&lt;/p&gt;
    &lt;p&gt;[4] Atul Adya, Daniel Myers, Jon Howell, Jeremy Elson, Colin Meek, Vishesh Khemani, Stefan Fulger, Pan Gu, Lakshminath Bhuvanagiri, Jason Hunter, Roberto Peon, Larry Kai, Alexander Shraer, Arif Merchant, Kfir Lev-Ari. Slicer: Auto-Sharding for Datacenter Applications. Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI), 2016, pp. 739â€“753.&lt;/p&gt;
    &lt;p&gt;[5] Sangmin Lee, Zhenhua Guo, Omer Sunercan, Jun Ying, Chunqiang Tang, et al. Shard Manager: A Generic Shard Management Framework for Geo distributed Applications. Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles (SOSP), 2021. DOI: 10.1145/3477132.3483546.&lt;/p&gt;
    &lt;p&gt;[6] Atul Adya, Jonathan Ellithorpe. Stateful services: low latency, efficiency, scalability â€” pick three. High Performance Transaction Systems Workshop (HPTS) 2024, Pacific Grove, California, September 15â€“18, 2024.&lt;/p&gt;
    &lt;p&gt;Product&lt;/p&gt;
    &lt;p&gt;December 10, 2024/7 min read&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46606902</guid><pubDate>Tue, 13 Jan 2026 19:56:37 +0000</pubDate></item><item><title>Games Workshop bans staff from using AI, management not excited about the tech</title><link>https://www.ign.com/articles/warhammer-maker-games-workshop-bans-its-staff-from-using-ai-in-its-content-or-designs-says-none-of-its-senior-managers-are-currently-excited-about-the-tech</link><description>&lt;doc fingerprint="9b23234b6eea17ed"&gt;
  &lt;main&gt;
    &lt;p&gt;Warhammer maker Games Workshop has banned the use of AI in its content production and its design process, insisting that none of its senior managers are currently excited about the technology.&lt;/p&gt;
    &lt;p&gt;Delivering the UK companyâ€™s impressive financial results, CEO Kevin Rountree addressed the issue of AI and how Games Workshop is handling it. He said GW staff are barred from using it to actually produce anything, but admitted a â€œfewâ€ senior managers are experimenting with it.&lt;/p&gt;
    &lt;p&gt;Rountree said AI was â€œa very broad topic and to be honest Iâ€™m not an expert on it,â€ then went on to lay down the company line:&lt;/p&gt;
    &lt;p&gt;"We do have a few senior managers that are [experts on AI]: none are that excited about it yet. We have agreed an internal policy to guide us all, which is currently very cautious e.g. we do not allow AI generated content or AI to be used in our design processes or its unauthorised use outside of GW including in any of our competitions. We also have to monitor and protect ourselves from a data compliance, security and governance perspective, the AI or machine learning engines seem to be automatically included on our phones or laptops whether we like it or not.&lt;/p&gt;
    &lt;p&gt;â€œWe are allowing those few senior managers to continue to be inquisitive about the technology. We have also agreed we will be maintaining a strong commitment to protect our intellectual property and respect our human creators. In the period reported, we continued to invest in our Warhammer Studio â€” hiring more creatives in multiple disciplines from concepting and art to writing and sculpting. Talented and passionate individuals that make Warhammer the rich, evocative IP that our hobbyists and we all love.â€&lt;/p&gt;
    &lt;p&gt;Games Workshop owns and operates a number of hugely popular tabletop war games, including Warhammer 40,000 and Age of Sigmar. Its core business is selling miniatures and box sets that are used by fans to play these games, but there are a number of other creative aspects of the hobby that Games Workshop invests in, such as book selling, art sales, and animation production.&lt;/p&gt;
    &lt;p&gt;Last month, Displate was forced to deny that one of its pieces of official Warhammer 40,000 artwork was the product of generative AI, insisting â€œred flagsâ€ spotted by fans were the result of human error.&lt;/p&gt;
    &lt;p&gt;The Warhammer 40,000 setting is in many ways built upon the evocative and enduring art drawn by the likes of John Blanche, who shaped its "grimdark" aesthetic alongside other key Games Workshop staff. This official, human-made Warhammer 40,000 artwork is beloved by fans, most of whom take a dim view of the mere whiff of generative AI â€œartâ€ sold or released in any official capacity by either Games Workshop itself, or its partners.&lt;/p&gt;
    &lt;p&gt;Indeed, Games Workshop sells expensive Warhammer 40,000 â€˜codexâ€™ rulebooks that are packed with stunning official art as well as lore. Any suggestion that this art was created either in part or entirely by generative AI tools would likely cause a community uproar.&lt;/p&gt;
    &lt;p&gt;Games Workshopâ€™s ban on AI is in contrast to some entertainment companies, some of whom have gone all-in on the tech despite various backlashes to their use. The CEO of Genvid â€” the company behind choose-your-own-adventure interactive series like Silent Hill Ascension â€” has claimed "consumers generally do not care" about generative AI, and stated that: "Gen Z loves AI slop."&lt;/p&gt;
    &lt;p&gt;EA CEO Andrew Wilson has said AI is "the very core of our business," and Square Enix recently implemented mass layoffs and reorganized, saying it needed to be "aggressive in applying AI." Dead Space creator Glen Schofield also recently detailed his plans to â€œfixâ€ the industry in part via the use of generative AI in game development, and former God of War dev Meghan Morgan Juinio said: "... if we donâ€™t embrace [AI], I think weâ€™re selling ourselves short.â€&lt;/p&gt;
    &lt;p&gt;Wesley is Director, News at IGN. Find him on Twitter at @wyp100. You can reach Wesley at wesley_yinpoole@ign.com or confidentially at wyp100@proton.me.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46607681</guid><pubDate>Tue, 13 Jan 2026 20:45:43 +0000</pubDate></item></channel></rss>