<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 09 Jan 2026 18:52:19 +0000</lastBuildDate><item><title>Mux (YC W16) is hiring a platform engineer that cares about (internal) DX</title><link>https://www.mux.com/jobs</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546413</guid><pubDate>Thu, 08 Jan 2026 21:01:36 +0000</pubDate></item><item><title>Richard D. James aka Aphex Twin speaks to Tatsuya Takahashi (2017)</title><link>https://web.archive.org/web/20180719052026/http://item.warp.net/interview/aphex-twin-speaks-to-tatsuya-takahashi/</link><description>&lt;doc fingerprint="363a18141fe60bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Richard D. James: I really enjoyed working on this with you. I know I only joined the project near the end, but I found it really exciting. Like a proper job, ha.&lt;/p&gt;
    &lt;p&gt;Tatsuya Takahashi: Richard, it was amazing working with you on the monologue. And now to be interviewed by you?!? That's crazy. But also a lot of fun. The monologue was also the last Korg synth that I was involved with directly, so I guess it's a nice conclusion to things.&lt;/p&gt;
    &lt;p&gt;RDJ: It is now the only synth on the market currently being made to have full microtuning editing, congratulations!&lt;/p&gt;
    &lt;p&gt;TT: Thanks! But it was completely because of you that we included microtuning. If you hadn't insisted on it, I definitely wouldn't have discovered how powerful it was. Did you ever have a moment of realisation, or some kind of trigger that made you discover microtuning?&lt;/p&gt;
    &lt;p&gt;RDJ: The first thoughts that I had about tuning in general happened with my early noodlings on a Yamaha DX100, one of the first synths I saved up for. I remember looking at the master tuning of 440 Hz and thinking I would change it, for no other reason apart from it was set by default to that frequency and that it could be changed.&lt;/p&gt;
    &lt;p&gt;I just used to select a single note, adjust the master tuning of it to taste and then base the whole track around that, something I’ve done ever since, just intuition and maybe a bit of rebelliousness. It’s very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?&lt;/p&gt;
    &lt;p&gt;I’ve since gone on to learn more about this damn 440 Hz. It was a standard introduced in 1939 by western governments, so I’m very glad I trusted my instincts. Listening to that other voice is THE most important thing in creativity, whether you’re an engineer or a musician. Tesla had some important advice on listening to the thoughts from the other. One of the most important inventors ever, but we’re not taught about him in British schools. Funny that.&lt;/p&gt;
    &lt;p&gt;TT: I don't know why it's thin on the curriculum, but the Tesla coil is definitely amazing. If you modulate the high frequency with audio signals you can play music with plasma – that's super cool. I will read up on him though, cos I don't know much about his life and thinking.&lt;/p&gt;
    &lt;p&gt;RDJ: An interesting “note”: I’ve just been reading a book on electronic instruments published in the 1940’s and it says that 440 Hz was transmitted over the radio on different frequencies 24 hours a day and others between midnight and 2 in the afternoon, ha, so you could tune your instruments and be well behaved or calibrate your lab equipment to it.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;It’s very simple, but do you want your music to be based on an international standard or on what you think sounds right to you?&lt;/p&gt;RICHARD D. JAMES&lt;/quote&gt;
    &lt;p&gt;But I’ve also read studies from the old Philips laboratories in the Netherlands that show orchestras average deviation from 440 Hz was measured over many concerts and was seen to differ by a few Hz, usually slightly below. Pretty anal. Some people obviously really cared that 440 Hz was being adhered to in practice.&lt;/p&gt;
    &lt;p&gt;Why 440 Hz was chosen in the first place is another interesting story, but looking at the resonances of water and sound is a great place to start, or read up on cymatics. If you aren’t already familiar with it, that is.&lt;/p&gt;
    &lt;p&gt;TT: So many things are standardised that you don't really think about because they were there before you started using it. 440 Hz was brought about to standardise the way people play together and, yeah, someone can bring a guitar to a piano and it would work together because of that standard.&lt;/p&gt;
    &lt;p&gt;It's like how a green light means you can cross the road or if you shake your head sideways it means no. Those two standards will help you through life in many places around the world. But it's dangerous to enforce standards in creativity. I have a son who's started school in Japan, where every kid will paint the sun red. Now that is some fucked up standardisation! Just really messed up on so many levels.&lt;/p&gt;
    &lt;p&gt;Anyway, I'm not going into that whole 432 Hz vs 440 Hz debate. (BTW: I absolutely love cymatics and I've done some nice workshops for kids with it.) But I will say different frequencies sound different, so why not use that in your music? You got to use whatever feels right and the monologue let's you do exactly that with pitch.&lt;/p&gt;
    &lt;p&gt;RDJ: Yep.&lt;/p&gt;
    &lt;p&gt;TT: Talking of standards, the sample rate of 48 kHz is another one for sampling and signal processing, but the volca sample uses a weird one at 31.25 kHz. Purely because of technical constraints, but I was thinking that might be part of the reason you liked it so much, because the different sample rate gives it a unique sound.&lt;/p&gt;
    &lt;p&gt;RDJ: Haha, yes, it was pretty much the first thing I noticed. Yeah, I thought the 48 kHz, was based on the Nyquist Theorem. I think it’s double what humans can apparently hear or something, which is another weird one. I don’t know how anybody worked out humans only hear to 20 kHz. I mean even if you can’t hear above 20 kHz, it doesn't mean that your body doesn't feel it. You don’t just experience sound through your eardrums. A good example of this is listening to a recording of your own voice. To almost everyone apart from maybe the most narcissistic, it always sounds weird/thinner/smaller, as you don’t feel the vibration of your chest and body. There are other reasons of course but that’s one for sure. Anyway, I’m into the extremes of the audio spectrum, ultra clarity ’n’ all but I probably prefer fucked-muffled/lo-bit/’70s sound more, ha!&lt;/p&gt;
    &lt;p&gt;TT: Oh, and when something defies the standard – I just remembered the first time I played a Yamaha SK-10, the faders were all upside down, like max was downwards, even on the volume. I didn't know what was going on and it threw me off at first, but it's actually a bit fun like that and you soon realise it all comes from organ drawbars.&lt;/p&gt;
    &lt;p&gt;RDJ: I never played the SK-10, but these Calrec mixers I use are like that also, the faders are backwards. There is a little dip switch inside to change it, but I think they have them like that for TV/broadcasting, coz if someone falls asleep at the desk they don’t want them to push all the faders up and distort two million TVs at once… Not surprising they have this safeguard considering how skull numbingly boring most TV is.&lt;/p&gt;
    &lt;p&gt;TT: Right!! Yeah, but there is a certain feeling to pulling rather than pushing. It's like how an orgasm is "coming" in English, but it's “going” [iku] in Japanese.&lt;/p&gt;
    &lt;p&gt;RDJ: Never thought of it like that.&lt;/p&gt;
    &lt;p&gt;TT: I mean, written text in Japanese was traditionally vertical. Although now a lot is westernised and horizontal.&lt;/p&gt;
    &lt;p&gt;RDJ: Ah, that’s kinda sad… So traditional Japanese text is like trackers and now it’s going like Cubase! :)&lt;/p&gt;
    &lt;p&gt;TT: I sometimes wonder what Japanese synths would have looked like if they didn't copy Moog in the ’70s. You've got to think about what is convention and what is really a good design.&lt;/p&gt;
    &lt;p&gt;RDJ: I’ve got one Japanese keyboard, Suzuki, which has got some Japanese tunings built in and a little string on one end that you can pluck. It sounds really nice as well. It also has some good Japanese percussion and MIDI. I don’t think it’s very well known.&lt;/p&gt;
    &lt;p&gt;I wish faders were curved horizontally and vertically, so you could make them like a double helix that go over and under each other, hehe. Could do it with an augmented reality UI I guess.&lt;/p&gt;
    &lt;p&gt;TT: Now that could be cool (if I'm imagining it right)! I've seen rotation sensors on the camera lens focus that work like faders on a curved surface and really thin. That could do it.&lt;/p&gt;
    &lt;p&gt;RDJ: Later on when I got an SH-101, I realised its tuning wasn't like the DX100 at all. It was based on 1v/octave and was supposed to be equal temperament but because of the nature of analogue, it really wasn’t and I REALLY loved that and how it layered with the frozen 12TET of the DX100.&lt;/p&gt;
    &lt;p&gt;I recently made a tuning on the monologue that I matched to an improperly calibrated SH-101 that I was fond of. I tried at first to do this using formulas inside Scala, but it's impossible to represent this accurately with simple maths, Scala can’t deal with these types of tunings unless it’s a keyboard map tuning file. This “bad” tuning is really great when I apply it to a precisely tuned digital synth that has full microtuning capabilities. It’s top making a digital synth sound like an out of tune 101! :)&lt;/p&gt;
    &lt;p&gt;TT: Yeah, I think it's really telling of the age we live in when you get a knob like "SLOP" on the new Prophet that makes pitch inconsistencies a programmable parameter. On one hand, you think that the level of control is great, but on the other it feels weird to deliberately degrade something that's stable. Especially if you're a young engineer striving to design something to be close as possible to perfection, it can be hard to grasp. The best lesson about this came from Mieda – my hero at Korg. When he looked at my first synth schematic, he told me, “Takahashi-kun, your circuits are functional, but they are not musical. Musical instruments do not need perfect waveforms and correct operating points. You need to use the transistor for what it is. As long as it sounds good, it’s OK.”&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;WHEN [MIEDA] LOOKED AT MY FIRST SYNTH SCHEMATIC, HE TOLD ME, “TAKAHASHI-KUN, YOUR CIRCUITS ARE FUNCTIONAL, BUT THEY ARE NOT MUSICAL. MUSICAL INSTRUMENTS DO NOT NEED PERFECT WAVEFORMS AND CORRECT OPERATING POINTS. YOU NEED TO USE THE TRANSISTOR FOR WHAT IT IS. AS LONG AS IT SOUNDS GOOD, IT’S OK.&lt;/p&gt;TATSUYA TAKAHASHI&lt;/quote&gt;
    &lt;p&gt;RDJ: I was going to ask you about SLOP, as you brought that up before in some old emails. I get you now. I mean, yeah, if it just sounds good in the first place then you don’t need that option, but I guess some people like their Osc’s drifty and others not so. It changes with the context I guess. Also, if you’re doing FM you might want to keep them dead on, and for analogue lead sounds, really drifty. Anyway I think I mentioned it before, but the drift on the monologue sounds REALLY nice. It seems to move, but then never go out. Care to explain? Sounds to me like it gets reset/synced at some point, but I’m probably wrong, haven't studied it in depth, just listened. Reminds me a bit of Arp oscillators, which have really nice driftyness, prob my faves! :)&lt;/p&gt;
    &lt;p&gt;TT: That's bang on! So same thing in the minilogue and the volcas too: the oscillators are re-tuned when they're not being used. I'm super glad you like it though because this is such a subjective thing. The autotuning was done in a way that felt nice to me, so it was a really subjective thing and you can’t present a report to convince others that it was OK. At least now I can say RDJ said it was alright!&lt;/p&gt;
    &lt;p&gt;RDJ: I’d like to talk more about this 1v/octave, but that’s for another time. But, anyway, getting back to the question, I was always interested in sound and how it affected me, especially the tuning. It wasn't until my *Selected Ambient Works Vol. II* album that I actually made my own full custom tunings, although there were a few scattered things before that.&lt;/p&gt;
    &lt;p&gt;I’ve got a slightly weird balance thing going on and getting the tuning “right” sometimes makes the balance thing less weird for me. It’s a longer story though.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, I think I read somewhere about how humans normally hear pitches differently in the left and right ear and that you don't have that. That is super interesting.&lt;/p&gt;
    &lt;p&gt;RDJ: Because we made it very intuitive to edit the tuning tables, I would actually just buy this synth only for that feature alone. When the export is implemented, it can be the central hub of either complete table creation or just to tweak existing imported Scala files, etc.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, absolutely. I would definitely download the monologue librarian because you can import and export Scala files easily with that. Hopefully other manufacturers will join the club.&lt;/p&gt;
    &lt;p&gt;The intuitive interface was pretty much all your idea, so a great job on that. I think your idea for the interface came from when you got your Chroma modded for microtuning. Have you modded a lot of synths for this functionality?&lt;/p&gt;
    &lt;p&gt;RDJ: That’s right, I burned my own custom O.S. Eproms for the Chroma, which enables full micro tuning and editing and that’s what the monologue editor was based on. I’ve got a good list of hardware and software now that can do it. It’s been a long haul and involved hassling a lot of people, but it is now finally possible with quite a bit of equipment.&lt;/p&gt;
    &lt;p&gt;I’ve generally received really good responses from engineers and programmers. I’ve contacted around 50 different people/companies in the last ten years. Many weren’t even aware that all their equipment and programs were adhering to a standard that was devised hundreds of years ago.&lt;/p&gt;
    &lt;p&gt;Same goes for a lot of electronic musicians, this is quite surprising for electronic music, which supposedly is forward-thinking and futuristic, but most people have since told me how fascinating they have found the subject once they realised it *was* a subject!&lt;/p&gt;
    &lt;p&gt;I know microtuning is much more useful on polyphonic keyboards, but it’s still very usable on monophonic instruments and, again, it can be used in the future to create tuning tables that can be used in other Scala-compatible polyphonic synths.&lt;/p&gt;
    &lt;p&gt;TT: Well, my initial impression was that microtuning is a really niche thing that wouldn't be needed for a mass market synth, especially a monophonic one, but if you try shifting the tuning while running a sequence, you can hear that it gives it another dimension even if it’s subtle. I'm not super-sensitive to pitch or anything, but you can still hear it change. To me, it feels like casting light on a rough surface and seeing different patterns as you move the light. So it was really important to have the easy scale edits you can do on the fly. Scala is great, it's super flexible, but it can be daunting to use and you won't get the real-time interaction, so I hope the monologue gets more people into this stuff.&lt;/p&gt;
    &lt;p&gt;RDJ: I really like your light analogy, that’s great. Yep, on a monophonic instrument, what you just described will be more pronounced if you use a delay with plenty of feedback or reverb, so you can hear the differently tuned notes overlap each other.&lt;/p&gt;
    &lt;p&gt;Scala is deep, very deep, but some things are very quick and easy to get going. For instance, you can just type Equal 24 &amp;amp; press the sysex send shortcut and you have a quarter tone tuning in your synth. Scala is only good for non-intuitive tuning creation, purely mathematical. I love this approach, but really prefer making tunings intuitively, note-by-note. When you’re actually composing something, making them up while you go along, a combination of the two is best for me.&lt;/p&gt;
    &lt;p&gt;TT: I know that you like that Wilsonic app you showed me, which is mainly structured on mathematical relationships of frequencies, but you've also mentioned using a lot of trial-and-error. Do you have a method to your microtuning?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, many. For instance, on the Chroma I like holding down one key, pressing another key and then tuning the second key in relation to the first, sometimes making two extremely different frequency combinations, like something very low and extremely high at the same time and maybe a group of these dual combos only existing in the top octave of the keyboard map, the rest being another tuning or multiple tunings, all in one tuning table.&lt;/p&gt;
    &lt;p&gt;It’s something I never saw in anyone else’s tunings, combining several tuning tables within one map, so that’s one of my little inventions I guess, as I rarely used the full range of 127 notes in one tuning within one track. monologue can tune four notes at a time which we planned. It’s a different approach again and something I look forward to experimenting with more.&lt;/p&gt;
    &lt;p&gt;TT: Here are five short tracks you made with custom scales. Could you explain how you came up with the scales?&lt;/p&gt;
    &lt;p&gt;RDJ: I forgot which tunings they used, I’ve got so many floating around in folders on the computer and in hardware. I didn’t make any notes. I think they might have been ones that I made in Scala and then tweaked on the monologue, most likely.&lt;/p&gt;
    &lt;p&gt;TT: If you could share the tuning files that you created, that would be great too!&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I’ve got loads saved and loads lost. I’ve never been a saver. I do save more things these days, getting older or something, but still love to use new sets of rules for every set of new tracks. Also I’ve got to say again many thanks for that lovely MIDI tuning box you made me for the minilogue!&lt;/p&gt;
    &lt;p&gt;TT: No problem! That was an eye-opener for all of us. [For the readers: Richard asked me for microtuning on our synths and since, at the time, we thought it wasn't something we would put on a production model, we made a custom little tuning tool. Fellow engineer Kazuki Saita and I made a MIDI thru box that could load custom scales. Any MIDI coming in would be transposed by note and cent (using pitch bend) and so you could get microtuning on any mono synth.] When we were testing that box, Saita and I were blown away. I mean, sequencing on a simple step sequencer like in the monologue can be a bit rigid, but messing with the tuning really opens it up. It basically redefines the keyboard. We were messing around with some subtle stuff and more extreme ones like octaves split into 50 intervals and playing with the arpeggiator. It was crazy and that's when we decided we should put it on the next synth.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, great! Arpeggiators and microtunings can be a very nice mix. We should include a picture of that box, I’ve got one here if you don’t.&lt;/p&gt;
    &lt;p&gt;TT: We should! Don't have one handy, would you be able to snap a photo?&lt;/p&gt;
    &lt;p&gt;RDJ: Attached it!&lt;/p&gt;
    &lt;p&gt;TT: Cheers! Wood cheeks for the Cirklon. Nice.&lt;/p&gt;
    &lt;p&gt;RDJ: I think the monologue is very nice looking, small, very cute and very capable. At first I thought, “Oh, it hasn’t got this, it hasn’t got that, etc. etc.” But I very quickly realised you have turned these limitations into advantages, which is really quite something special. I really mean that. The lack of extensive features makes the whole thing much more speedy to work with.&lt;/p&gt;
    &lt;p&gt;TT: That's got to be the best compliment. And it's a way of thinking that runs through all the synths I've worked on, from the volcas and monotrons to the monologue. I think with electronic instruments we've got to a point where software can do most things. But I'm a fan of gear where less is more – where the simplest controls can give you the most creative freedom.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I like this approach. It’s true, I do it with modular setups as well. I’m lucky to have loads of modular gear but I prefer to make small systems now and leave everything else in another room where I just try things out before committing them to a more thought out config.&lt;/p&gt;
    &lt;p&gt;Of course us musicians always look at something new and we see if it does what we expect it to. And this is OK. But we shouldn’t overlook something before actually trying it out, try and get into the head of the designer first. I try and do this. It’s difficult sometimes to push your ego and expectations out of the way for a while, but if we don’t do this we won’t learn anything new. That’s not to say that every designer’s head is worth getting into, but we gotta give it a go sometimes.&lt;/p&gt;
    &lt;p&gt;TT: This is exactly the reason I really enjoyed working with you. I'd send you a prototype and a day later you'd be sending me a dozen emails about how the drive circuit actually controls gain and dry/wet at the same time. Or how some menu option wasn’t working completely as intended. You would give everything a chance. You went through every single menu option and went after some easter eggs, like finding CC34 VCO1 pitch! In fact, you were the best ever beta tester. Guess you wouldn't be after a day job tho...&lt;/p&gt;
    &lt;p&gt;RDJ: *blush* Some examples of this: When I first checked out the volca sample, the lack of velocity response had me scratching my head, but when I realised how it handled it with motion recording of the level control, it was actually loads more fun and SO much faster to program! It’s such a great little idea, I really love it, way more intuitive. I’ve started doing it this way on the Cirklon now sometimes.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, so you're a huge fan of the Cirklon, which you used for "korg funk 5." Could you tell us how that track was put together?&lt;/p&gt;
    &lt;p&gt;Here's the gear list you sent me:&lt;lb/&gt; Korg Monologue x3&lt;lb/&gt; Korg MS-20 kit&lt;lb/&gt; Korg Poly-61M&lt;lb/&gt; Korg Volca keys&lt;lb/&gt; Korg Volca beats&lt;lb/&gt; Korg Volca sample&lt;lb/&gt; Korg Minilogue&lt;lb/&gt; My son on vox&lt;/p&gt;
    &lt;p&gt;I was blown away by this and really really touched. I don't think there is another track out there using so much of the gear I worked on! Also, can you touch on the processing that went on the sounds, cos I can tell there's a lot going on.&lt;/p&gt;
    &lt;p&gt;RDJ: That’s so nice to hear… It was really top making some tracks with only Korg gear. I’m a secret nerd-fan of synth demos, mainly vintage ’80s ones currently! Some amazing music has been made as equipment demos, unsung heroes. I collect synth demos. Well, ones that I like. It’s kind of an unclassified music genre, so doing these tracks for you and Korg was a natural thing for me. I also really like picking certain combinations of gear. That is endlessly fascinating.&lt;/p&gt;
    &lt;p&gt;The Volca beats I used, I did the snare mod but used the mix output, so I treated all the sounds with the same treatment, I think I sent you the full list… looks it up… OK, here it is.&lt;/p&gt;
    &lt;p&gt;volca beats &amp;gt; Skibbe 736-5 mic pre [nice low mid sound] &amp;gt; BAC 500 compressor &amp;gt; RTZ PEQ1549 [this is based on my fave eq, I’ve got some Calrec originals as well, standard circuit design but not standard sound! ] &amp;gt; Calrec minimixer&lt;/p&gt;
    &lt;p&gt;Monologue [main riff] &amp;gt; blonder tongue EQ [i love these eq’s, hardly anyone has heard of them]&lt;/p&gt;
    &lt;p&gt;TT: Any chance you could share the tracks separately? There might be something we could do with that and a lot of people will be interested in seeing how the different synths sound soloed. Only if you're up for it of course!&lt;/p&gt;
    &lt;p&gt;RDJ: I would if I had them, but I never save individual tracks. I’m trying to get into the habit of that soon. I just recorded that down to the Sound Devices 722.&lt;/p&gt;
    &lt;p&gt;TT: Ah shame! But you know that was the other great thing – that the track was done totally sequenced on the Cirklon and recorded in one take.&lt;/p&gt;
    &lt;p&gt;RDJ: I was thinking a while back on different ways to visualise the data in the Cirklon. Also with the volca fm, you also managed to turn the lack of velocity per note into a bonus [again], it puts a different slant on it, applying and recording motion velocity on the whole phrase, it works very well.&lt;/p&gt;
    &lt;p&gt;TT: So the volca keyboard is never going to do a great job of sensing velocity and we could have spent a lot more money to make it velocity-sensitive, but then you'd sit there going, "Well, it's too small to play. We need to make it bigger..." So trying to force it to be something it's not is a great way of creating more problems. Much rather turn the game around.&lt;/p&gt;
    &lt;p&gt;RDJ: That’s a great example of necessity and invention. I was absolutely amazed to find out that it IS actually possible to edit a DX7 voice with great speed from the interface you have designed. I never thought you could do that, but it is and is totally usable. I’ve come up with loads of things on it that I would never have done on a full size DX7. Hats off to Tats!&lt;/p&gt;
    &lt;p&gt;TT: Cheers! So everyone knows the typical DX7 sounds – well, the presets anyway – and by doing things a bit differently, you can open up so much stuff. Take an organ patch on the volca fm and sequence it normally, but then motion sequence the algorithm and it goes in a completely different dimension. It's a discovery, which is fun. I find a lot of artists are discovery junkies.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I think I HAVE to be learning something when making tracks, even if it’s something very small. If there’s no learning involved, I wouldn’t get excited enough to do anything. Great fun being able to take a DX7 in your pocket, love it, ultimate walkman in a way. In fact, one for the future: volca fm with built in MP3 player + radio… be super lush.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, super great idea! Also if it could tap into some MIDI archives and play them on the FM engine, it would be great.&lt;/p&gt;
    &lt;p&gt;RDJ: Or maybe a pitch tracker from the MP3s! :-)&lt;/p&gt;
    &lt;p&gt;TT: Even better! :) And it can take real time mic input, so people are saying hello to you, but you're just hearing bells or something.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, recently I was offering up ideas to a talented coder friend on an app that uses evolutionary/genetic synthesis to try and resynthesise audio/live audio into DX7 patches. It sounds really cool. He’s working on making it a standalone app on Raspberry Pi, and it is based on some vintage code by Andrew Horner. Kyma also used his code for their GA synthesis. Chuck that in there while we’re at it.&lt;/p&gt;
    &lt;p&gt;TT: Got to say it's pretty funny getting a consumer product idea from you. Haha!&lt;/p&gt;
    &lt;p&gt;RDJ: :) I’m full of ‘em, I’m like this guy.&lt;/p&gt;
    &lt;p&gt;TT: BAHAHAHHA! Holy crap.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;I think I HAVE to be learning something when making tracks, even if it’s something very small. If there’s no learning involved, I wouldn’t get excited enough to do anything.&lt;/p&gt;Richard D. James&lt;/quote&gt;
    &lt;p&gt;RDJ: How different is the finished monologue to what was designed or what you had in mind?&lt;/p&gt;
    &lt;p&gt;TT: Well, it didn't have microtuning for a start!&lt;/p&gt;
    &lt;p&gt;RDJ: :)&lt;/p&gt;
    &lt;p&gt;TT: When I initially came up with the product plan, it wasn't very detailed. None of my product plans are. Something like: "smaller than the minilogue and monophonic." It's only when you start designing and prototyping that things start to come together. Things like: “What kind of filter do we need?” “Do we need distortion?” “Battery power would be great!”&lt;/p&gt;
    &lt;p&gt;RDJ: If there are features that were designed that didn’t make it, could you tell us about them?&lt;/p&gt;
    &lt;p&gt;TT: Nothing really got properly designed before being ditched. The team is pretty good at putting together test versions where we can just about see if something is going to work before we go to full implementation.&lt;/p&gt;
    &lt;p&gt;In terms of ideas, you had some pretty good ones:&lt;/p&gt;
    &lt;p&gt;I think the team had others like arpeggiator, which is the most obvious one. But we dropped that and added key-trigger sequence instead.&lt;/p&gt;
    &lt;p&gt;RDJ: When or how do you find out that features that were wanted by your team are not going to make it? Is that frustrating?&lt;/p&gt;
    &lt;p&gt;TT: Well, it's not like someone stands there casting their decision on whether something makes it or not. We all try to figure out how it will come together as an instrument, so a single feature might be the focus in a heated discussion, but really it's about the whole thing being coherent but also incoherent and surprising in a good way. Sometimes you need to throw people off what they're expecting to do something interesting. The team was always pretty small, so we could do it without having a draconian decision-making process, but also without it getting too democratic either. We would never ever vote on a feature.&lt;/p&gt;
    &lt;p&gt;RDJ: Would it be possible that Korg could release limited edition and more costly versions of your designs with no corners cut, for us posh musos?&lt;/p&gt;
    &lt;p&gt;TT: Sure, that's definitely a possibility. What's on your wish list?&lt;/p&gt;
    &lt;p&gt;RDJ: Oh dear, that is a big question, I think I’ll have to get back to you on that. Well, those ones above to start with I suppose. :) Do you have a studio at home? Got any pics? Or a description of your setup?&lt;/p&gt;
    &lt;p&gt;TT: I wouldn't say it's a studio, but more of a workshop. I build stuff there for my own live setup, although recently most of it is made up of products I've worked on. One of my favourite things is volca fm going into audio input of monotribe which has been modded so you can kill the VCO. I put on a slow chord progression on the fm and then work a sequence with it with the monotribe. It's actually better if I don't sync the volca fm to the monotribe.&lt;/p&gt;
    &lt;p&gt;RDJ: Nice, I keep meaning to rack up 8 analogue filters to a TX802. Nobody ever made a decent FM synth with analogue filters, there are a few simple FM ones but not 4OP+.&lt;/p&gt;
    &lt;p&gt;TT: My other favourite thing is my speaker system designed by my friends at Taguchi. They're omni-directional and I've been experimenting with the positions. My room is acoustically untreated, but with these speakers you can actually work with the reflections in the room. It's definitely not a typical setup, but it's great because you can pan your instruments around the room and you’re not glued to a sweet spot between a stereo pair. It's great if you just sequence piano phase on two volcas and offset the BPM and just let it run while the sequence phases in and out. The trick there is actually not to hard pan them, but to leave quite a bit of overlap.&lt;/p&gt;
    &lt;p&gt;RDJ: [*looks at pics*] Great, that is an unusual speaker setup! I’m a big fan of suspending speakers from the ceiling, the first speakers that I built, I filled with tar and hung them from nylon cords from my bedroom ceiling. Saves space as well. Do you live and breathe Korg, do you get time for anything else, any other hobbies?&lt;/p&gt;
    &lt;p&gt;TT: Don't know if it counts as a hobby, but I really like polyhedra. Maybe that’s why I like those speakers, since they're great 3D structures hanging off my ceiling. My favourite polyhedron is the dodecahedron and when you make one with wire, it's hard to make it completely regular. But it turns out I actually like the wonky ones better. Anyway, they have a cool name.&lt;/p&gt;
    &lt;p&gt;RDJ: That’s very nice. I absolutely love geometry, I did a track called “Dodeccaheedron,” a long time ago, one of my fave tracks. I was playing on this spirograph emulator recently. Ha, a 3D one would be really interesting.&lt;/p&gt;
    &lt;p&gt;TT: Oh man, of course you have a track named “Dodeccaheedron”! I wonder if the track had anything to do with the fact I like them now. Bet it did. Spirographs are so cool. Bit like Lissajous – could stare at that stuff all day. I really want to get hold of some XY lasers actually and fire some really intense ones. Wish there was a way to do that in 3D.&lt;/p&gt;
    &lt;p&gt;RDJ: I’ve been looking into this recently. :)&lt;/p&gt;
    &lt;p&gt;TT: Maybe you can design some phosphorescent smoke that you could fire lasers into and the lines would stay in the air. That will be so cool. And the smoke particles will move with the bass – get some fat bass bins and you would get lines of light vibrating.&lt;/p&gt;
    &lt;p&gt;RDJ: Top idea… Reminds me of this.&lt;/p&gt;
    &lt;p&gt;TT: Yeah, really. I mean it could be a way of visualising the propagation of sound waves, so maybe a scientific use too. And not just sound waves. It could be used in wind tunnels to study air flow. Are we onto something here?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes.&lt;/p&gt;
    &lt;p&gt;RDJ: What Is Your Dream?&lt;/p&gt;
    &lt;p&gt;TT: Having a good cigarette. When you're having a shit day or you're under a lot of stress, cigarettes taste crap. On the other hand, a cigarette after an amazing experience tastes good. So my dream is to smoke the best cigarette ever. Smoking is a full-stop, a moment of recognition that whatever came before it was real.&lt;/p&gt;
    &lt;p&gt;RDJ: I like that.&lt;/p&gt;
    &lt;p&gt;TT: Bit wanky tho. ;) Getting weird vibes reading back at my answers!&lt;/p&gt;
    &lt;p&gt;RDJ: LOLz&lt;/p&gt;
    &lt;p&gt;TT: Oh well, wrote it once, can't deny it.&lt;/p&gt;
    &lt;p&gt;RDJ: If you could magically create any device, what would it be? I understand if you’re not allowed to answer this!&lt;/p&gt;
    &lt;p&gt;TT: A time machine, teleportation machine – the obvious ones. Or actually a machine where you could have as many parallel existences as you want. So you could be a super-dimensional being encompassing all the different possibilities of yourself. That's what popped into my head, but how self-centred!&lt;/p&gt;
    &lt;p&gt;RDJ: I go to sleep thinking things like this… Maybe it's a bit like this already! :)&lt;/p&gt;
    &lt;p&gt;TT: Hell yeah. Anyway, that's probably not what you meant. So... a lifelogging device for your musical activities. I was packing up to leave Tokyo and found a bunch of minidiscs of music that I'd forgotten I'd made in my teens and I’m guessing there would have been a lot more if I knew where my cassettes were. I cringed at most of it, but it's still part of who I am and I can't erase whatever brain patterns I have because of that.&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, bloody right, that would be very useful. One thing I’d say, though, is I’ve found a lot of artists write off their older work for various personal reasons, while other people won’t have those associations and just really love what you made.&lt;/p&gt;
    &lt;p&gt;TT: Do you have lost musical moments from the past that you would like to hear again?&lt;/p&gt;
    &lt;p&gt;RDJ: Yes, I think I’m obsessed with thoughts like this. If you could selectively erase your memory so you could keep experiencing things for the first time, it would be very interesting, although you would get stuck in loops, so you would have to limit it to a certain number of re-experiences, ha! How many future products have you got in your head or on the drawing board?&lt;/p&gt;
    &lt;p&gt;TT: Quite a lot, but not all will be made. We (meaning the team still at Korg) have always got a bunch of ideas up our sleeves, it's just a case of which ones will get made and when.&lt;/p&gt;
    &lt;p&gt;RDJ: Is your job stressful? I imagine it’s very stressful. What's the most stressful part?&lt;/p&gt;
    &lt;p&gt;TT: Well, the stress was part of the balance, because there's a lot of adrenaline involved in meeting deadlines, starting production and working up to release. Now that I've left that position, I can look back in calm retrospect. I'd say it was quite physical. Kind of like a sport and also quite addictive. But at the same time you can't do it forever. I was also lucky enough to find new possibilities elsewhere, so I stopped before that high pace / full-throttle thing became the only thing I could do. I really did have an amazing time at Korg. I had the best team and I also had a lot of freedom. My decision to leave was really about me than anything to do with my working environment.&lt;/p&gt;
    &lt;p&gt;RDJ: What is your worst fear?&lt;/p&gt;
    &lt;p&gt;TT: Well, doing the same thing over again and then one day realising that's all you can do.&lt;/p&gt;
    &lt;p&gt;RDJ: Yeah, I think we all have to fight against this, especially as you get older. I’ve really been looking at my habits recently and denying them. It feels great if you can manage it. &lt;lb/&gt;I don't understand the economics of getting hardware to market, but I guess it's safe to assume that the company makes more money from releasing new products than it does upgrading old ones. &lt;/p&gt;
    &lt;p&gt;I can’t help thinking, though, that by continuing to upgrade older products that are still in production, to make them absolutely awesome, would benefit the company in the long-term. Any thoughts about this?&lt;/p&gt;
    &lt;p&gt;TT: That depends how you look at it. You can look at something like the monotribe which we spent a lot of time doing the major update for, which was then soon discontinued. So your initial point might look to hold true. But then you look at the amount we learnt from that update and that we put into the volcas, and then you can say it was worthwhile. I think it's really really important to look back and review past products. Some would benefit from an update, but others are better off redesigned.&lt;/p&gt;
    &lt;p&gt;RDJ: Ok then, well lovely chatting to you as always.. wishing you all the best in your new endeavours, very brave moving yourself to a new country, well done and speak soon.&lt;/p&gt;
    &lt;p&gt;Here’s a nice link to end with!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546614</guid><pubDate>Thu, 08 Jan 2026 21:17:26 +0000</pubDate></item><item><title>Embassy: Modern embedded framework, using Rust and async</title><link>https://github.com/embassy-rs/embassy</link><description>&lt;doc fingerprint="7b05f0bb442a83d4"&gt;
  &lt;main&gt;
    &lt;p&gt;Embassy is the next-generation framework for embedded applications. Write safe, correct, and energy-efficient embedded code faster, using the Rust programming language, its async facilities, and the Embassy libraries.&lt;/p&gt;
    &lt;p&gt;The Rust programming language is blazingly fast and memory-efficient, with no runtime, garbage collector, or OS. It catches a wide variety of bugs at compile time, thanks to its full memory- and thread-safety, and expressive type system.&lt;/p&gt;
    &lt;p&gt;Rust's async/await allows for unprecedentedly easy and efficient multitasking in embedded systems. Tasks get transformed at compile time into state machines that get run cooperatively. It requires no dynamic memory allocation and runs on a single stack, so no per-task stack size tuning is required. It obsoletes the need for a traditional RTOS with kernel context switching, and is faster and smaller than one!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Hardware Abstraction Layers&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;HALs implement safe, idiomatic Rust APIs to use the hardware capabilities, so raw register manipulation is not needed. The Embassy project maintains HALs for select hardware, but you can still use HALs from other projects with Embassy.&lt;/item&gt;
          &lt;item&gt;embassy-stm32, for all STM32 microcontroller families.&lt;/item&gt;
          &lt;item&gt;embassy-nrf, for the Nordic Semiconductor nRF52, nRF53, nRF54 and nRF91 series.&lt;/item&gt;
          &lt;item&gt;embassy-rp, for the Raspberry Pi RP2040 and RP23xx microcontrollers.&lt;/item&gt;
          &lt;item&gt;embassy-mspm0, for the Texas Instruments MSPM0 microcontrollers.&lt;/item&gt;
          &lt;item&gt;esp-rs, for the Espressif Systems ESP32 series of chips. &lt;list rend="ul"&gt;&lt;item&gt;Embassy HAL support for Espressif chips, as well as Async Wi-Fi, Bluetooth, and ESP-NOW, is being developed in the esp-rs/esp-hal repository.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
          &lt;item&gt;ch32-hal, for the WCH 32-bit RISC-V(CH32V) series of chips.&lt;/item&gt;
          &lt;item&gt;mpfs-hal, for the Microchip PolarFire SoC.&lt;/item&gt;
          &lt;item&gt;py32-hal, for the Puya Semiconductor PY32 series of microcontrollers.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Time that Just Works - No more messing with hardware timers. embassy_time provides Instant, Duration, and Timer types that are globally available and never overflow.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time ready - Tasks on the same async executor run cooperatively, but you can create multiple executors with different priorities so that higher priority tasks preempt lower priority ones. See the example.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Low-power ready - Easily build devices with years of battery life. The async executor automatically puts the core to sleep when there's no work to do. Tasks are woken by interrupts, there is no busy-loop polling while waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Networking - The embassy-net network stack implements extensive networking functionality, including Ethernet, IP, TCP, UDP, ICMP, and DHCP. Async drastically simplifies managing timeouts and serving multiple connections concurrently.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bluetooth&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;The trouble crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the bt-hci traits (currently &lt;code&gt;nRF52&lt;/code&gt;,&lt;code&gt;nrf54&lt;/code&gt;,&lt;code&gt;rp2040&lt;/code&gt;,&lt;code&gt;rp23xx&lt;/code&gt;and&lt;code&gt;esp32&lt;/code&gt;and&lt;code&gt;serial&lt;/code&gt;controllers are supported).&lt;/item&gt;
          &lt;item&gt;The nrf-softdevice crate provides Bluetooth Low Energy 4.x and 5.x support for nRF52 microcontrollers.&lt;/item&gt;
          &lt;item&gt;The embassy-stm32-wpan crate provides Bluetooth Low Energy 5.x support for stm32wb microcontrollers.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;The trouble crate provides a Bluetooth Low Energy 4.x and 5.x Host that runs on any microcontroller implementing the bt-hci traits (currently &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LoRa - The lora-rs project provides an async LoRa and LoRaWAN stack that works well on Embassy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;USB - embassy-usb implements a device-side USB stack. Implementations for common classes such as USB serial (CDC ACM) and USB HID are available, and a rich builder API allows building your own.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bootloader and DFU - embassy-boot is a lightweight bootloader supporting firmware application upgrades in a power-fail-safe way, with trial boots and rollbacks.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;use defmt::info;
use embassy_executor::Spawner;
use embassy_time::{Duration, Timer};
use embassy_nrf::gpio::{AnyPin, Input, Level, Output, OutputDrive, Pin, Pull};
use embassy_nrf::{Peri, Peripherals};

// Declare async tasks
#[embassy_executor::task]
async fn blink(pin: Peri&amp;lt;'static, AnyPin&amp;gt;) {
    let mut led = Output::new(pin, Level::Low, OutputDrive::Standard);

    loop {
        // Timekeeping is globally available, no need to mess with hardware timers.
        led.set_high();
        Timer::after_millis(150).await;
        led.set_low();
        Timer::after_millis(150).await;
    }
}

// Main is itself an async task as well.
#[embassy_executor::main]
async fn main(spawner: Spawner) {
    let p = embassy_nrf::init(Default::default());

    // Spawned tasks run in the background, concurrently.
    spawner.spawn(blink(p.P0_13.into()).unwrap());

    let mut button = Input::new(p.P0_11, Pull::Up);
    loop {
        // Asynchronously wait for GPIO events, allowing other tasks
        // to run, or the core to sleep.
        button.wait_for_low().await;
        info!("Button pressed!");
        button.wait_for_high().await;
        info!("Button released!");
    }
}&lt;/code&gt;
    &lt;p&gt;Examples are found in the &lt;code&gt;examples/&lt;/code&gt; folder separated by the chip manufacturer they are designed to run on. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;examples/nrf52840&lt;/code&gt;run on the&lt;code&gt;nrf52840-dk&lt;/code&gt;board (PCA10056) but should be easily adaptable to other nRF52 chips and boards.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/nrf5340&lt;/code&gt;run on the&lt;code&gt;nrf5340-dk&lt;/code&gt;board (PCA10095).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/stm32xx&lt;/code&gt;for the various STM32 families.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/rp&lt;/code&gt;are for the RP2040 chip.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;examples/std&lt;/code&gt;are designed to run locally on your PC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install &lt;code&gt;probe-rs&lt;/code&gt;following the instructions at https://probe.rs.&lt;/item&gt;
      &lt;item&gt;Change directory to the sample's base directory. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd examples/nrf52840&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;Cargo.toml&lt;/code&gt;sets the right feature for the name of the chip you are programming. If this name is incorrect, the example may fail to run or immediately crash after being programmed.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ensure&lt;/p&gt;&lt;code&gt;.cargo/config.toml&lt;/code&gt;contains the name of the chip you are programming.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the example&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;code&gt;cargo run --release --bin blinky&lt;/code&gt;
    &lt;p&gt;For more help getting started, see Getting Started and Running the Examples.&lt;/p&gt;
    &lt;p&gt;The Rust Analyzer is used by Visual Studio Code and others. Given the multiple targets that Embassy serves, there is no Cargo workspace file. Instead, the Rust Analyzer must be told of the target project to work with. In the case of Visual Studio Code, please refer to the &lt;code&gt;.vscode/settings.json&lt;/code&gt; file's &lt;code&gt;rust-analyzer.linkedProjects&lt;/code&gt;setting.&lt;/p&gt;
    &lt;p&gt;Embassy is guaranteed to compile on stable Rust 1.75 and up. It might compile with older versions, but that may change in any new patch release.&lt;/p&gt;
    &lt;p&gt;EMBedded ASYnc! :)&lt;/p&gt;
    &lt;p&gt;Embassy is licensed under either of&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apache License, Version 2.0 (LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0)&lt;/item&gt;
      &lt;item&gt;MIT license (LICENSE-MIT or http://opensource.org/licenses/MIT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;at your option.&lt;/p&gt;
    &lt;p&gt;Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46547740</guid><pubDate>Thu, 08 Jan 2026 23:00:45 +0000</pubDate></item><item><title>Scientists discover oldest poison, on 60k-year-old arrows</title><link>https://www.nytimes.com/2026/01/07/science/poison-arrows-south-africa.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46547962</guid><pubDate>Thu, 08 Jan 2026 23:24:30 +0000</pubDate></item><item><title>Show HN: Various shape regularization algorithms</title><link>https://github.com/nickponline/shreg</link><description>&lt;doc fingerprint="6cfc9320f7d44fe"&gt;
  &lt;main&gt;
    &lt;p&gt;A Python implementation of various shape regularization algorithms for regularizing line segments and closed contours.&lt;/p&gt;
    &lt;p&gt;Shape regularization is a technique used in computational geometry to clean up noisy or imprecise geometric data by aligning segments to common orientations and adjusting their positions to create cleaner, more regular shapes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Segment Regularization: Align line segments to common angles and offsets using quadratic programming optimization&lt;/item&gt;
      &lt;item&gt;Snap Regularization: Connect nearby endpoints to create watertight polygons and meshes&lt;/item&gt;
      &lt;item&gt;Metric Regularization: Constrain segment dimensions - equal lengths, length quantization, and equal spacing&lt;/item&gt;
      &lt;item&gt;Contour Regularization: Simplify closed polygons by aligning edges to principal directions&lt;/item&gt;
      &lt;item&gt;T-Junction Detection: Snap endpoints onto segment interiors for proper connectivity&lt;/item&gt;
      &lt;item&gt;Flexible Configuration: Control maximum angle and offset tolerances&lt;/item&gt;
      &lt;item&gt;Visualization: Built-in plotting utilities for before/after comparisons&lt;/item&gt;
      &lt;item&gt;Pure Python: No dependencies required&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install shreg&lt;/code&gt;
    &lt;code&gt;uv pip install shreg&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/nickp/shreg.git
cd shreg
pip install -e .&lt;/code&gt;
    &lt;p&gt;Regularize a set of line segments by aligning their angles and offsets:&lt;/p&gt;
    &lt;code&gt;import numpy as np
from shreg import solve_line_segments, seg

# Create some segments (each segment is [x1, y1, x2, y2])
segments = [
    seg(0.0, 0.0, 1.0, 0.02),   # Nearly horizontal
    seg(0.0, 1.0, 1.0, 1.05),   # Nearly horizontal, slightly offset
    seg(1.0, 0.0, 1.02, 1.0),   # Nearly vertical
]

# Regularize: align angles within 25 degrees, offsets within 0.5 units
result = solve_line_segments(
    segments,
    angle=True,
    offset=True,
    maximum_angle=25,
    maximum_offset=0.5
)&lt;/code&gt;
    &lt;p&gt;Close gaps between nearby endpoints to create watertight polygons:&lt;/p&gt;
    &lt;code&gt;from shreg import snap_regularize_segments, seg

# Create segments with small gaps at corners
segments = [
    seg(0.0, 0.0, 1.0, 0.0),    # bottom edge
    seg(1.05, 0.02, 1.0, 1.0),  # right edge (gap at bottom-right)
    seg(1.0, 1.03, 0.0, 0.98),  # top edge (gap at corners)
    seg(-0.02, 1.0, 0.0, 0.0),  # left edge (gap at top-left)
]

# Snap endpoints within 0.1 units of each other
result = snap_regularize_segments(
    segments,
    epsilon=0.1,      # Distance threshold for snapping
    method="cluster"  # Fast centroid-based method
)
# Result: All corners are now perfectly connected&lt;/code&gt;
    &lt;p&gt;Constrain segment dimensions - force equal lengths, quantize to grid units, or equalize spacing:&lt;/p&gt;
    &lt;code&gt;from shreg import metric_regularize_segments, seg

# Segments with slightly different lengths and uneven spacing
segments = [
    seg(0.0, 0.0, 1.9, 0.0),   # length ~2
    seg(0.0, 0.9, 2.1, 0.9),   # length ~2, y=0.9 (should be 1.0)
    seg(0.0, 2.0, 1.95, 2.0),  # length ~2
]

# Regularize: equal lengths, snap to 1-unit grid, equalize spacing
result = metric_regularize_segments(
    segments,
    equal_length=True,         # Force similar lengths to be equal
    length_quantization=True,  # Snap lengths to multiples of base_unit
    equal_spacing=True,        # Equalize gaps between parallel lines
    base_unit=1.0,             # Grid unit for quantization
)
# Result: All segments have length 2.0 and are evenly spaced at y=0, 1, 2&lt;/code&gt;
    &lt;p&gt;Simplify a closed polygon by aligning edges to principal directions:&lt;/p&gt;
    &lt;code&gt;from shreg import regularize_contour

# Define a noisy polygon (list of [x, y] points)
points = [
    [45, 29], [65, 440], [44, 498], [446, 498], [429, 325],
    [499, 309], [448, 206], [479, 148], [479, 31], [247, 88],
]

# Regularize with axis alignment
result = regularize_contour(
    points,
    principle="axis",     # Align to horizontal/vertical
    max_offset=20,        # Maximum offset for merging
)

print(f"Simplified from {len(points)} to {len(result)} points")&lt;/code&gt;
    &lt;p&gt;The algorithm optimizes segment orientations and positions to create cleaner line arrangements:&lt;/p&gt;
    &lt;p&gt;Angle regularization aligns crossing lines to common orientations:&lt;/p&gt;
    &lt;p&gt;Combined angle and offset regularization on a hexagon:&lt;/p&gt;
    &lt;p&gt;This example from the CGAL documentation demonstrates sequential angle and offset regularization on 15 segments organized into three groups: outer boundary, top rhombus, and bottom rhombus.&lt;/p&gt;
    &lt;code&gt;from shreg import solve_line_segments, create_cgal_example

# Load the 15 segments from the CGAL example
segments, groups = create_cgal_example()

# Regularize with tight tolerances
result = solve_line_segments(
    segments,
    angle=True,
    offset=True,
    maximum_angle=10,    # 10 degrees max angle deviation
    maximum_offset=0.1   # 0.1 units max offset
)&lt;/code&gt;
    &lt;p&gt;Snap regularization connects nearby endpoints to create watertight geometry. This is essential for creating closed polygons suitable for 3D extrusion, mesh generation, or CAD operations.&lt;/p&gt;
    &lt;p&gt;The cluster method groups nearby endpoints and moves them to their centroid. This is the fastest approach and guarantees watertight results:&lt;/p&gt;
    &lt;code&gt;from shreg import snap_regularize_segments, seg

segments = [
    seg(0.0, 0.0, 1.0, 0.05),
    seg(1.08, 0.0, 1.05, 1.0),
    seg(1.0, 1.08, 0.0, 0.95),
    seg(-0.05, 1.0, 0.0, 0.0),
]
result = snap_regularize_segments(segments, epsilon=0.15, method="cluster")&lt;/code&gt;
    &lt;p&gt;Hard constraints use quadratic programming to find the optimal positions that exactly satisfy all snap constraints while minimizing total endpoint movement:&lt;/p&gt;
    &lt;code&gt;result = snap_regularize_segments(segments, epsilon=0.15, method="hard")&lt;/code&gt;
    &lt;p&gt;Soft constraints add "spring" forces between endpoints that should connect. This is useful when data is noisy and you're not certain endpoints should be exactly coincident:&lt;/p&gt;
    &lt;code&gt;result = snap_regularize_segments(
    segments,
    epsilon=0.25,
    method="soft",
    soft_weight=50.0  # Higher = stiffer springs
)&lt;/code&gt;
    &lt;p&gt;T-junctions occur when an endpoint should snap onto another segment's interior (not its endpoints). Enable T-junction detection for proper connectivity:&lt;/p&gt;
    &lt;code&gt;segments = [
    seg(0.0, 0.0, 2.0, 0.0),      # horizontal line
    seg(0.0, 1.0, 2.0, 1.0),      # horizontal line
    seg(0.95, -0.08, 1.05, 1.1),  # vertical line (forms T-junctions)
]
result = snap_regularize_segments(
    segments, epsilon=0.15, method="cluster", t_junctions=True
)&lt;/code&gt;
    &lt;p&gt;Snap regularization works on polygons of any complexity:&lt;/p&gt;
    &lt;p&gt;Metric regularization constrains the relative measurements of segments. This is useful for architectural drawings, CAD cleanup, and any domain where dimensions should follow regular patterns.&lt;/p&gt;
    &lt;p&gt;Forces segments with similar lengths to be exactly equal. Useful when objects (like windows or columns) should have identical dimensions:&lt;/p&gt;
    &lt;code&gt;from shreg import metric_regularize_segments, seg

segments = [
    seg(0.0, 0.0, 2.0, 0.0),    # length 2.0
    seg(0.0, 1.0, 2.15, 1.0),   # length 2.15
    seg(0.0, 2.0, 1.9, 2.0),    # length 1.9
    seg(0.0, 3.0, 2.05, 3.0),   # length 2.05
]

result = metric_regularize_segments(
    segments,
    equal_length=True,
    length_tolerance=0.15,  # 15% relative tolerance
)
# Result: All segments now have equal length (~2.0)&lt;/code&gt;
    &lt;p&gt;Snaps segment lengths to integer multiples of a base unit. Essential for architectural plans where walls must be multiples of a grid unit:&lt;/p&gt;
    &lt;code&gt;segments = [
    seg(0.0, 0.0, 1.85, 0.0),   # length 1.85 -&amp;gt; 2.0
    seg(0.0, 1.0, 3.15, 1.0),   # length 3.15 -&amp;gt; 3.0
    seg(0.0, 2.0, 0.9, 2.0),    # length 0.9 -&amp;gt; 1.0
    seg(0.0, 3.0, 2.2, 3.0),    # length 2.2 -&amp;gt; 2.0
]

result = metric_regularize_segments(
    segments,
    length_quantization=True,
    base_unit=1.0,              # Snap to 1-meter multiples
    quantization_tolerance=0.3, # Within 30% of base unit
)&lt;/code&gt;
    &lt;p&gt;Forces equal gaps between parallel lines. Perfect for regularizing staircases, window arrays, or any repeated elements:&lt;/p&gt;
    &lt;code&gt;segments = [
    seg(0.0, 0.0, 3.0, 0.0),    # y=0.0
    seg(0.0, 0.9, 3.0, 0.9),    # y=0.9 (uneven)
    seg(0.0, 2.0, 3.0, 2.0),    # y=2.0
    seg(0.0, 3.1, 3.0, 3.1),    # y=3.1 (uneven)
    seg(0.0, 4.0, 3.0, 4.0),    # y=4.0
]

result = metric_regularize_segments(
    segments,
    equal_spacing=True,
    angle_tolerance=5.0,  # Lines within 5° are considered parallel
)
# Result: Lines are now evenly spaced at y=0, 1, 2, 3, 4&lt;/code&gt;
    &lt;p&gt;Simplify complex polygons while preserving their essential shape:&lt;/p&gt;
    &lt;p&gt;Complex shapes are reduced to their essential vertices:&lt;/p&gt;
    &lt;p&gt;Regularize a list of line segments.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments, where each segment is a numpy array&lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;offset&lt;/code&gt;: Whether to regularize offsets (default:&lt;code&gt;True&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;angle&lt;/code&gt;: Whether to regularize angles (default:&lt;code&gt;True&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;maximum_offset&lt;/code&gt;: Maximum offset tolerance in units (default:&lt;code&gt;0.5&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;maximum_angle&lt;/code&gt;: Maximum angle tolerance in degrees (default:&lt;code&gt;25&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of regularized segments&lt;/p&gt;
    &lt;p&gt;Helper function to create a segment array.&lt;/p&gt;
    &lt;code&gt;from shreg import seg
s = seg(0, 0, 1, 1)  # Creates np.array([0, 0, 1, 1])&lt;/code&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;snap_regularize_segments(segments, epsilon=1.0, method="hard", soft_weight=100.0, t_junctions=False)&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Connect nearby endpoints by snapping them together.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments, where each segment is a numpy array&lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;epsilon&lt;/code&gt;: Maximum distance for endpoints to be considered "close" and snapped (default:&lt;code&gt;1.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;method&lt;/code&gt;: Snapping method to use:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;"cluster"&lt;/code&gt;: Fast centroid-based method (recommended for most cases)&lt;/item&gt;&lt;item&gt;&lt;code&gt;"hard"&lt;/code&gt;: QP with exact equality constraints (perfectly watertight)&lt;/item&gt;&lt;item&gt;&lt;code&gt;"soft"&lt;/code&gt;: QP with spring penalty (elastic connections)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;soft_weight&lt;/code&gt;: Spring stiffness for soft constraints. Higher values = stiffer springs (default:&lt;code&gt;100.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;t_junctions&lt;/code&gt;: Whether to detect and snap T-junctions (default:&lt;code&gt;False&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of segments with snapped endpoints&lt;/p&gt;
    &lt;p&gt;Find clusters of nearby endpoints using spatial indexing.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;epsilon&lt;/code&gt;: Maximum distance for clustering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of clusters, where each cluster is a list of &lt;code&gt;(segment_idx, endpoint_idx)&lt;/code&gt; tuples&lt;/p&gt;
    &lt;p&gt;Find T-junctions where endpoints are close to segment interiors.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;epsilon&lt;/code&gt;: Maximum distance for T-junction detection&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;exclude_clusters&lt;/code&gt;: Endpoint clusters to exclude (already handled by endpoint-to-endpoint snapping)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of &lt;code&gt;((segment_idx, endpoint_idx), target_segment_idx)&lt;/code&gt; tuples&lt;/p&gt;
    &lt;head rend="h4"&gt;
      &lt;code&gt;metric_regularize_segments(segments, equal_length=True, length_quantization=False, equal_spacing=True, base_unit=1.0, length_tolerance=0.1, quantization_tolerance=0.3, angle_tolerance=5.0, max_iterations=3)&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Regularize segments using metric and pattern constraints.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments, where each segment is a numpy array&lt;code&gt;[x1, y1, x2, y2]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;equal_length&lt;/code&gt;: Force segments with similar lengths to be exactly equal (default:&lt;code&gt;True&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;length_quantization&lt;/code&gt;: Snap lengths to multiples of&lt;code&gt;base_unit&lt;/code&gt;(default:&lt;code&gt;False&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;equal_spacing&lt;/code&gt;: Force equal gaps between parallel lines (default:&lt;code&gt;True&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;base_unit&lt;/code&gt;: Base unit for length quantization, e.g., 1.0 meter (default:&lt;code&gt;1.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;length_tolerance&lt;/code&gt;: Relative tolerance for equal length detection (default:&lt;code&gt;0.1&lt;/code&gt;= 10%)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;quantization_tolerance&lt;/code&gt;: Tolerance for quantization as fraction of&lt;code&gt;base_unit&lt;/code&gt;(default:&lt;code&gt;0.3&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;angle_tolerance&lt;/code&gt;: Maximum angle difference in degrees to consider lines parallel (default:&lt;code&gt;5.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_iterations&lt;/code&gt;: Maximum SQP iterations for iterative refinement (default:&lt;code&gt;3&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of regularized segments&lt;/p&gt;
    &lt;p&gt;Note: Uses linearization since length calculation is non-linear. The algorithm iteratively refines the solution using Sequential Quadratic Programming (SQP).&lt;/p&gt;
    &lt;p&gt;Find pairs of segments with similar lengths.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tolerance&lt;/code&gt;: Maximum relative length difference to consider "similar" (default:&lt;code&gt;0.1&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;min_length&lt;/code&gt;: Minimum segment length to consider (default:&lt;code&gt;0.5&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of &lt;code&gt;(segment_idx_a, segment_idx_b)&lt;/code&gt; tuples&lt;/p&gt;
    &lt;p&gt;Find segments whose lengths should be quantized to multiples of &lt;code&gt;base_unit&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;base_unit&lt;/code&gt;: Base unit for quantization (default:&lt;code&gt;1.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tolerance&lt;/code&gt;: Maximum distance from nearest multiple as fraction of&lt;code&gt;base_unit&lt;/code&gt;(default:&lt;code&gt;0.3&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;min_length&lt;/code&gt;: Minimum segment length to consider (default:&lt;code&gt;0.5&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of &lt;code&gt;(segment_idx, target_length)&lt;/code&gt; tuples&lt;/p&gt;
    &lt;p&gt;Find groups of parallel segments for equal spacing regularization.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;segments&lt;/code&gt;: List of segments&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;angle_tolerance&lt;/code&gt;: Maximum angle difference in degrees to consider parallel (default:&lt;code&gt;5.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;min_group_size&lt;/code&gt;: Minimum number of segments to form a group (default:&lt;code&gt;3&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: List of groups, where each group is a list of segment indices&lt;/p&gt;
    &lt;p&gt;Regularize a closed contour.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;points&lt;/code&gt;: List of&lt;code&gt;[x, y]&lt;/code&gt;coordinates forming a closed polygon&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;principle&lt;/code&gt;: How to determine principal directions:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;"longest"&lt;/code&gt;: Use the longest edge as reference&lt;/item&gt;&lt;item&gt;&lt;code&gt;"axis"&lt;/code&gt;: Align to horizontal/vertical axes&lt;/item&gt;&lt;item&gt;&lt;code&gt;"cardinal"&lt;/code&gt;: Use 0, 45, 90, 135 degree directions&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;max_offset&lt;/code&gt;: Maximum offset for merging parallel segments (default:&lt;code&gt;20.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;visualize&lt;/code&gt;: Whether to show intermediate plots (default:&lt;code&gt;False&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns: numpy array of regularized points&lt;/p&gt;
    &lt;p&gt;Load contour points from a polylines file.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;filename&lt;/code&gt;: Path to polylines file. If&lt;code&gt;None&lt;/code&gt;, loads bundled example data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The package includes various geometry functions:&lt;/p&gt;
    &lt;code&gt;from shreg import (
    length,              # Segment length
    midpoint,            # Segment midpoint
    orientation,         # Segment orientation in degrees [0, 180)
    direction,           # Unit direction vector
    normal,              # Unit normal vector
    rotate,              # Rotate segment around midpoint
    translate_by_normal, # Translate along normal
    angle_difference,    # Angle difference modulo 90 degrees
)&lt;/code&gt;
    &lt;code&gt;from shreg import plotting

# Enable/disable all plotting
plotting.enable()
plotting.disable()

# Before/after comparison
plotting.comparison(before_segments, after_segments, title="My Title")

# Context manager for before/after
with plotting.BeforeAfter("My Title") as plot:
    plot.before(segments)
    segments = regularize(segments)
    plot.after(segments)

# Save plots to file
plotting.comparison(before, after, save_path="output.png")&lt;/code&gt;
    &lt;p&gt;Run the demo examples:&lt;/p&gt;
    &lt;code&gt;# Run all examples with visualization
shreg

# Run without visualization (batch mode)
shreg --no-plot

# Run only segment examples
shreg --segments

# Run only contour examples
shreg --contours&lt;/code&gt;
    &lt;p&gt;Or using Python module syntax:&lt;/p&gt;
    &lt;code&gt;python -m shreg --help&lt;/code&gt;
    &lt;p&gt;The regularization problem is formulated as an energy minimization problem. Given a set of segments, we seek small adjustments (rotations and translations) that minimize an energy function while respecting constraints on maximum deviations.&lt;/p&gt;
    &lt;p&gt;The energy function balances two objectives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fidelity: Keep segments close to their original positions&lt;/item&gt;
      &lt;item&gt;Regularity: Encourage nearby segments to share common angles and offsets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This leads to a quadratic program (QP) of the form:&lt;/p&gt;
    &lt;code&gt;minimize    (1/2) x'Px + q'x
subject to  l &amp;lt;= Ax &amp;lt;= u
&lt;/code&gt;
    &lt;p&gt;where &lt;code&gt;x&lt;/code&gt; contains the rotation and translation corrections for each segment, &lt;code&gt;P&lt;/code&gt; encodes the fidelity cost, and the constraints enforce that angle/offset differences between nearby segments are minimized.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Neighbor Detection: Use Delaunay triangulation on segment midpoints to identify nearby segment pairs efficiently&lt;/item&gt;
      &lt;item&gt;Constraint Graph: Build constraints for angle and offset differences between neighboring segments within tolerance bounds&lt;/item&gt;
      &lt;item&gt;QP Optimization: Solve the quadratic program using OSQP to find optimal corrections&lt;/item&gt;
      &lt;item&gt;Application: Apply computed rotations and translations to each segment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Snap regularization is formulated as a Constrained Quadratic Programming problem that minimizes endpoint movement while enforcing connectivity constraints.&lt;/p&gt;
    &lt;p&gt;Variables: For N segments, the state vector contains all 4N endpoint coordinates:&lt;/p&gt;
    &lt;code&gt;x = [x₁₁, y₁₁, x₁₂, y₁₂, ..., xₙ₂, yₙ₂]ᵀ
&lt;/code&gt;
    &lt;p&gt;Objective (Fidelity): Minimize squared distance from original positions:&lt;/p&gt;
    &lt;code&gt;minimize (1/2) Σᵢ (||uᵢ - ûᵢ||² + ||vᵢ - v̂ᵢ||²)
&lt;/code&gt;
    &lt;p&gt;Methods:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Formulation&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;cluster&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Replace clustered endpoints with centroid&lt;/cell&gt;
        &lt;cell&gt;Fast, guaranteed watertight&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;hard&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Equality constraints: vᵢ - uⱼ = 0&lt;/cell&gt;
        &lt;cell&gt;Exact connections required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;soft&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Penalty term: λ·Σ&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Pipeline:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Endpoint Detection: Build KD-Tree on all 2N endpoints&lt;/item&gt;
      &lt;item&gt;Clustering: Use Union-Find to group endpoints within ε distance&lt;/item&gt;
      &lt;item&gt;Variable Reduction (cluster): Replace clusters with single variables&lt;/item&gt;
      &lt;item&gt;QP Solve (hard/soft): Optimize using OSQP&lt;/item&gt;
      &lt;item&gt;T-Junction Handling: Project endpoints onto target segments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Metric regularization constrains segment dimensions (length, distance). The challenge is that length calculation &lt;code&gt;√(Δx² + Δy²)&lt;/code&gt; is non-linear, but QP solvers require linear constraints.&lt;/p&gt;
    &lt;p&gt;Linearization: We approximate length using the segment's unit direction vector d = (dₓ, dᵧ):&lt;/p&gt;
    &lt;code&gt;L ≈ dₓ(xₑ - xₛ) + dᵧ(yₑ - yₛ)
&lt;/code&gt;
    &lt;p&gt;This is linear in the endpoint coordinates and can be directly inserted into the constraint matrix.&lt;/p&gt;
    &lt;p&gt;Constraint Formulations:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Constraint&lt;/cell&gt;
        &lt;cell role="head"&gt;Mathematical Form&lt;/cell&gt;
        &lt;cell role="head"&gt;Application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Equal Length&lt;/cell&gt;
        &lt;cell&gt;L_A - L_B = 0&lt;/cell&gt;
        &lt;cell&gt;Windows, columns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Quantization&lt;/cell&gt;
        &lt;cell&gt;L = K (target)&lt;/cell&gt;
        &lt;cell&gt;Grid snapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Equal Spacing&lt;/cell&gt;
        &lt;cell&gt;2·Pos(L₂) - Pos(L₁) - Pos(L₃) = 0&lt;/cell&gt;
        &lt;cell&gt;Stairs, arrays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Iterative Refinement (SQP): Because the unit vectors are computed from the current geometry, results are approximate if segments rotate significantly. The algorithm uses Sequential Quadratic Programming:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute unit vectors from current segment orientations&lt;/item&gt;
      &lt;item&gt;Build and solve the QP&lt;/item&gt;
      &lt;item&gt;Update segment coordinates&lt;/item&gt;
      &lt;item&gt;Repeat until convergence (or max iterations)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pipeline:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Detection: Find candidate pairs/groups (similar lengths, parallel lines)&lt;/item&gt;
      &lt;item&gt;Linearization: Compute unit direction vectors for length approximation&lt;/item&gt;
      &lt;item&gt;Constraint Building: Build sparse constraint matrix A for detected patterns&lt;/item&gt;
      &lt;item&gt;QP Solve: Minimize ||x - x̂||² subject to Ax = b using OSQP&lt;/item&gt;
      &lt;item&gt;Iteration: Refine unit vectors and re-solve if needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The contour regularization algorithm follows CGAL's approach for closed polygons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Angle Alignment: Rotate each edge to align with principal directions (modulo 90 degrees)&lt;/item&gt;
      &lt;item&gt;Parallel Merging: Merge consecutive parallel edges that are close together&lt;/item&gt;
      &lt;item&gt;Link Insertion: Insert connecting segments between remaining parallel edges&lt;/item&gt;
      &lt;item&gt;Intersection: Compute intersection points to form the final regularized polygon&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;numpy &amp;gt;= 1.20.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;scipy &amp;gt;= 1.7.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;osqp &amp;gt;= 0.6.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;matplotlib &amp;gt;= 3.5.0&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install development dependencies:&lt;/p&gt;
    &lt;code&gt;pip install -e ".[dev]"&lt;/code&gt;
    &lt;p&gt;Run tests:&lt;/p&gt;
    &lt;code&gt;pytest tests/ -v&lt;/code&gt;
    &lt;p&gt;Run tests with coverage:&lt;/p&gt;
    &lt;code&gt;pytest tests/ -v --cov=shreg --cov-report=term-missing&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jean-Philippe Bauchet and Florent Lafarge. KIPPI: KInetic Polygonal Partitioning of Images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3146–3154, Salt Lake City, United States, June 2018. [PDF]&lt;/item&gt;
      &lt;item&gt;CGAL Shape Regularization Documentation&lt;/item&gt;
      &lt;item&gt;OSQP: Operator Splitting Quadratic Program Solver&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46549333</guid><pubDate>Fri, 09 Jan 2026 02:13:01 +0000</pubDate></item><item><title>Agonist-Antagonist Myoneural Interface</title><link>https://www.media.mit.edu/projects/agonist-antagonist-myoneural-interface-ami/overview/</link><description>&lt;doc fingerprint="f778dbc8b8e9c461"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Humans can accurately sense the position, speed, and torque of their limbs, even with their eyes shut. This sense, known as proprioception, allows humans to precisely control their body movements.&lt;/head&gt;
    &lt;p&gt;Today’s conventional prosthetic limbs do not provide feedback to the nervous system. Because of this, people with amputated limbs cannot feel the position, speed, and torque of their prosthetic joints without looking at them, making it difficult to control their movement. In order to create a more complete prosthetic control experience, researchers at the MIT Media Lab invented the agonist-antagonist myoneural interface (AMI). The AMI is a method to restore proprioception to persons with amputation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46550453</guid><pubDate>Fri, 09 Jan 2026 05:44:58 +0000</pubDate></item><item><title>Mathematics for Computer Science (2018) [pdf]</title><link>https://courses.csail.mit.edu/6.042/spring18/mcs.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46550895</guid><pubDate>Fri, 09 Jan 2026 07:06:41 +0000</pubDate></item><item><title>What happened to WebAssembly</title><link>https://emnudge.dev/blog/what-happened-to-webassembly/</link><description>&lt;doc fingerprint="764b019a19bb2a90"&gt;
  &lt;main&gt;
    &lt;head data-astro-cid-6t6zfk7k=""&gt;Table Of Contents&lt;/head&gt;
    &lt;p&gt;On every WebAssembly discussion, there is inevitably one comment (often near the top) asking what happened.&lt;/p&gt;
    &lt;p&gt;It seems to have been advertised as a world-changing advancement. Was it just oversold? Was it another JVM applet scenario, doomed to fail?&lt;/p&gt;
    &lt;p&gt;I’d like to tackle this in a weirdly roundabout way because I think these sorts of questions make a few misplaced assumptions that are critical to clarify.&lt;/p&gt;
    &lt;head rend="h1"&gt;#In The Real World&lt;/head&gt;
    &lt;p&gt;Of course, WebAssembly does see real-world usage. Let’s list some examples!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Godot uses WebAssembly to build games for the web.&lt;/item&gt;
      &lt;item&gt;Squoosh.app uses WebAssembly to make use of image libraries.&lt;/item&gt;
      &lt;item&gt;Zellij uses WebAssembly for its plugin ecosystem.&lt;/item&gt;
      &lt;item&gt;Figma uses WebAssembly to convert their C++ code to something usable in a browser&lt;/item&gt;
      &lt;item&gt;Stackblitz uses WebAssembly for their web containers.&lt;/item&gt;
      &lt;item&gt;Ruffle uses WebAssembly to run a flash emulator in your browser&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For many of these, WebAssembly is critical to either their entire product or a major feature.&lt;/p&gt;
    &lt;p&gt;But I think this alone is not very convincing. We don’t yet see major websites entirely built with webassembly-based frameworks. We’re not building our applications directly to WebAssembly for maximum portability. But why not?&lt;/p&gt;
    &lt;p&gt;To answer this, we need a good mental model for what WebAssembly is. This will help us qualify where it is most impactful and the limitations we’re up against.&lt;/p&gt;
    &lt;head rend="h1"&gt;#What Is WebAssembly&lt;/head&gt;
    &lt;p&gt;In a word, WebAssembly is a language.&lt;/p&gt;
    &lt;head rend="h2"&gt;#A Note On Speed&lt;/head&gt;
    &lt;p&gt;This makes questions like “how fast is WebAssembly” a bit hard to answer. You don’t ask how fast algebraic notation is—it’s not a very sensible question.&lt;/p&gt;
    &lt;p&gt;Taken in the context of something like JavaScript, the language is only as fast as the engine running it. JavaScript the language has no speed, but you can benchmark JS engines like V8, SpiderMonkey, and JavaScriptCore. You can benchmark the IO libraries of JS runtimes like Bun, Deno, and Node.&lt;/p&gt;
    &lt;p&gt;What people actually mean is “how useful are the constructs of this language to efficient mappings of modern hardware” and “what is the current landscape of systems taking advantage of these constructs”.&lt;/p&gt;
    &lt;p&gt;Through clever-enough engineering, you can make any system sufficiently fast with some trade-offs. If compiling your code directly to C doesn’t bother you, getting “near native” speeds is possible in both JavaScript and WebAssembly.&lt;/p&gt;
    &lt;p&gt;That’s right, you can compile WebAssembly! You can also choose to interpret it directly—that’ll be up to your runtime, just like every other system.&lt;/p&gt;
    &lt;p&gt;So let’s ask the actual question of WebAssembly: how useful are the constructs of this language to efficient mappings of modern hardware? Turns out, pretty useful!&lt;/p&gt;
    &lt;head rend="h2"&gt;#An Efficient Mapping&lt;/head&gt;
    &lt;p&gt;WebAssembly is a pretty close approximation of an assembly language. Not too close, mind you. It’s higher level than that. But it’s close enough to cleanly compile to most assembly languages without a significant speed trade-off.&lt;/p&gt;
    &lt;p&gt;And yes, you can write WebAssembly by hand! I made a rustlings-esque course called watlings where you can hand-write WAT to solve some basic exercises.&lt;/p&gt;
    &lt;p&gt;WAT is a very close approximation to Wasm. It is almost 1:1 in that you can compile WAT to Wasm and then back to WAT with barely any loss in information (you may lose variable names and some metadata). It looks like this:&lt;/p&gt;
    &lt;p&gt;Try reading the code. It will feel both familiar and foreign.&lt;/p&gt;
    &lt;p&gt;We have functions and S-expressions. We have imports and exports. But we also have instructions like &lt;code&gt;i32.add&lt;/code&gt; and implicit stack returns.&lt;/p&gt;
    &lt;p&gt;Wasm is a bytecode perhaps best compared to JVMIS (i.e. JVM bytecode). They have similar goals and constraints, but different landscapes and guarantees.&lt;/p&gt;
    &lt;p&gt;Compared to JVM bytecode, Wasm has a significantly smaller API and stronger safety guarantees. It has fewer opinions on your memory management strategy and more limitations on what your program can do without permission from its host environment.&lt;/p&gt;
    &lt;p&gt;It can crunch numbers, but must be explicitly provided its memory and all imports. In this way, it is much different from an actual assembly language (or, a more widely used one).&lt;/p&gt;
    &lt;p&gt;We’ll wrap back around to this later.&lt;/p&gt;
    &lt;head rend="h1"&gt;#A compilation target&lt;/head&gt;
    &lt;p&gt;You can compile many languages to Wasm.&lt;/p&gt;
    &lt;p&gt;Notable among them are Rust, C, Zig, Go, Kotlin, Java, and C#. Commonly interpreted languages have even had their runtimes compiled to WebAssembly, such as Python, PHP, and Ruby. There are also many languages that solely compile to WebAssembly, such as AssemblyScript, Grain, and MoonBit.&lt;/p&gt;
    &lt;p&gt;For many of these, it is important not to require a garbage-collector. For others, it would be helpful to include one. Wasm allows for both (with the GC option being much more recent).&lt;/p&gt;
    &lt;p&gt;Your browser includes a Wasm “engine”, making this doubly an attractive compilation target. This means without much setup, your phone and laptop can run Wasm programs already.&lt;/p&gt;
    &lt;p&gt;Like how JVM can have many implementations of its runner, there are many implementations that run independently of your browser such as Wasmtime, WasmEdge, and Wasmer.&lt;/p&gt;
    &lt;p&gt;These languages can output a single artifact without being too specific to your computer’s hardware. You only need a Wasm runner to execute it (note more JVM analogies).&lt;/p&gt;
    &lt;head rend="h1"&gt;#Security and what it enables&lt;/head&gt;
    &lt;p&gt;Right now, Wasm is looking really similar to JVM. The main differences seem to be around memory management strategies and how many platforms support it.&lt;/p&gt;
    &lt;p&gt;The security story is what really starts to drive in the wedge.&lt;/p&gt;
    &lt;p&gt;WebAssembly maintains a minimal attack surface by treating all external interactions as explicit, host-defined imports. We went over this earlier. Its “deny-by-default” architecture, small instruction set, hidden control-flow stack (i.e. no raw pointers), and linear memory combine to create a very strong security story.&lt;/p&gt;
    &lt;p&gt;It is such that you can ensure process-like isolation within a single process. Cloudflare takes advantage of this aspect within V8 to run untrusted code very efficiently using V8 isolates. This means significant efficiency gains without significant security trade-offs.&lt;/p&gt;
    &lt;p&gt;Wasm programs can start 100x faster if you can avoid spinning up a separate process. Fermyon, a company in the Wasm hosting space, advertises sub-millisecond spinup times.&lt;/p&gt;
    &lt;p&gt;In these cases, the performance is a direct result of what the security guarantees enable.&lt;/p&gt;
    &lt;p&gt;In other cases, security can unlock feature support.&lt;/p&gt;
    &lt;p&gt;Flash is a multimedia platform that was primarily used for animations and games up until it was dropped from all major browsers in January of 2021 (primarily) due to security concerns. Ruffle has revived Flash experiences on sites like Newgrounds by acting as an interpreter and VM for ActionScript.&lt;/p&gt;
    &lt;p&gt;Cloudflare allows running Python code with similar security guarantees to its JS code by using Pyodide, which is a Wasm build of CPython.&lt;/p&gt;
    &lt;p&gt;Figma runs untrusted user plugins in your browser by running them in a QuickJS engine that is compiled to Wasm.&lt;/p&gt;
    &lt;p&gt;Elsewhere, the security allows for extreme embeddability.&lt;/p&gt;
    &lt;head rend="h1"&gt;#Portability and Embeddability&lt;/head&gt;
    &lt;p&gt;We’ve gone over the number of ways you can run Wasm programs. A Wasm runner can be pretty light. Instead of forcing library authors into a specific language (usually Lua or JavaScript), supporting Wasm itself opens the door to a much wider set of choices.&lt;/p&gt;
    &lt;p&gt;Tools like Zellij, Envoy, and Lapce support Wasm for their plugin ecosystem.&lt;/p&gt;
    &lt;p&gt;In environments where a JavaScript engine is already being used, this means access to programs you would not have been able to run otherwise.&lt;/p&gt;
    &lt;p&gt;This includes image processing, ocr, physics engines, rendering engines, media toolkits, databases, and parsers, among many others.&lt;/p&gt;
    &lt;p&gt;In a majority of these cases, the use of Wasm will be transparent to you. A library you installed will just be using it somewhere in its dependency tree.&lt;/p&gt;
    &lt;p&gt;Godot and Figma have codebases written in C++, but are often browser-ready by compiling to (or in combination with) WebAssembly.&lt;/p&gt;
    &lt;p&gt;It seems the most common use of Wasm is bridging the language gap. Certain ecosystems seem to have suites of tools more common to them. Squoosh would be a much more limited application if it could only choose image compression libraries from NPM.&lt;/p&gt;
    &lt;head rend="h1"&gt;#Speed and size revisited&lt;/head&gt;
    &lt;p&gt;Browsers run WebAssembly with roughly the same pipeline that runs JavaScript. This seemingly puts a hard limit on the performance of Wasm applications, but they will often be more or less performant due to their architecture or domain.&lt;/p&gt;
    &lt;p&gt;Using languages with richer type systems and more sophisticated optimizing compilers can produce more efficient programs. The JIT model of engines like V8 might prevent optimizations if the cost of optimizing exceeds the gains from running the optimized code. You might avoid megamorphic functions more easily by avoiding JavaScript.&lt;/p&gt;
    &lt;p&gt;However, there is a cost to crossing the host-program boundary, especially if cloning memory. Zaplib’s post-mortem is an interesting read here. Incrementally moving a codebase to Wasm can incur significant costs in boundary crossing, eliminating any benefit in the short term.&lt;/p&gt;
    &lt;p&gt;A small API surface also means binary bloat as system APIs are more often re-created than imported. There are standards like WASI which aim to help here. Still, there is no native string type (yet).&lt;/p&gt;
    &lt;p&gt;Zig seems to produce the smallest Wasm binaries among mainstream languages.&lt;/p&gt;
    &lt;p&gt;Practical performance of Wasm in native contexts (i.e. outside of a JS engine) seems to suffer for a variety of reasons. Threading and IO of any sort incurs some cost. Memory usage is larger. Cold start is slower.&lt;/p&gt;
    &lt;p&gt;Still, the performance trade-offs might not be significant enough to matter. For most uses, I’d wager it’s “fast enough”. If you’re in a performance-sensitive context, the benefits of Wasm are likely not as relevant.&lt;/p&gt;
    &lt;head rend="h1"&gt;#Language development and you&lt;/head&gt;
    &lt;p&gt;Clearly things are happening.&lt;/p&gt;
    &lt;p&gt;The Wasm IO YouTube channel has lots of talks worth watching.&lt;/p&gt;
    &lt;p&gt;In fact, standards and language development in Wasm has stirred significant controversy internally. There is a lot of desire for advancement, but standardization means decisions are hard to reverse. For many, things are moving too quickly and in the wrong direction.&lt;/p&gt;
    &lt;p&gt;There is the “more official” W3C working group and then the “less official” Bytecode Alliance which works much more quickly and is centered around tooling and language development outside of Wasm directly (e.g. on WIT and the WebAssembly Component Model).&lt;/p&gt;
    &lt;p&gt;Wasm feature proposals are being quickly advanced and adopted by a wide suite of tools. This is remarkable progress for standardization, but is also scary to watch if you fear large missteps.&lt;/p&gt;
    &lt;p&gt;So why do people think nothing has happened?&lt;/p&gt;
    &lt;p&gt;I figure most are under the impression that the advancement of this technology would have had a more visible impact on their work. That they would intentionally reach for and use Wasm tools.&lt;/p&gt;
    &lt;p&gt;Many seem to think there is a path to Wasm replacing JavaScript within the browser—that they might not need to include a &lt;code&gt;.js&lt;/code&gt; file at all. This is very unlikely.&lt;/p&gt;
    &lt;p&gt;However, you can use frameworks like Blazor and Leptos without being aware or involved in the produced JS artifacts.&lt;/p&gt;
    &lt;p&gt;Mostly, Wasm tools have been adopted and used by library authors, not application developers. The internals are opaque. This is fine, probably.&lt;/p&gt;
    &lt;p&gt;Separately, I think the community is not helped by the philosophy of purposely obfuscating teaching material around Wasm. This is a fight I lost a few times.&lt;/p&gt;
    &lt;p&gt;For now, maybe check out watlings. I’ll expand it at some point, surely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46551044</guid><pubDate>Fri, 09 Jan 2026 07:38:30 +0000</pubDate></item><item><title>Kagi releases alpha version of Orion for Linux</title><link>https://help.kagi.com/orion/misc/linux-status.html</link><description>&lt;doc fingerprint="8923d81071c60f5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Orion for Linux Status â&lt;/head&gt;
    &lt;p&gt;The alpha stage is an early, unstable version meant primarily for testing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is ready to test â&lt;/head&gt;
    &lt;p&gt;All visual components, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Main menus, submenus, dialogs, buttons, and toolbars.&lt;/item&gt;
      &lt;item&gt;Right-click menus and other visual controls.&lt;/item&gt;
      &lt;item&gt;Window layouts and basic controls.&lt;/item&gt;
      &lt;item&gt;Demonstrated basic website navigation functionality, supporting essentials like the homepage, tabs, and simple searches&lt;/item&gt;
      &lt;item&gt;Advanced tab management is now complete, with the exception of the Tab Switcher UI, which is not supported yet.&lt;/item&gt;
      &lt;item&gt;Tabs now function independently and can be opened in parallel&lt;/item&gt;
      &lt;item&gt;Session persistence is implemented: previously opened tabs, along with their history, will reopen when the application is launched again.&lt;/item&gt;
      &lt;item&gt;Tabs currently appear in the main window and are supported in the left sidebar as well.&lt;/item&gt;
      &lt;item&gt;Bookmarks system a simple bookmark feature is now available.&lt;/item&gt;
      &lt;item&gt;Users can save pages, organize them into folders&lt;/item&gt;
      &lt;item&gt;Users can view them in the bookmarks dialog, sidebar, and bookmarks bar.&lt;/item&gt;
      &lt;item&gt;Bookmarking via the â´ï¸ icon.&lt;/item&gt;
      &lt;item&gt;Intuitive folder assignment when saving a new bookmark.&lt;/item&gt;
      &lt;item&gt;Advanced history management provides handling of browsing history&lt;/item&gt;
      &lt;item&gt;Password management framework establishes the core infrastructure needed for secure password handling and future improvements in this area.&lt;/item&gt;
      &lt;item&gt;Local export/import (via file)&lt;/item&gt;
      &lt;item&gt;Managing passwords&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Future improvements (not implemented in Alpha): â&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WebKit Extension support&lt;/item&gt;
      &lt;item&gt;Sync infrastructure&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46553343</guid><pubDate>Fri, 09 Jan 2026 12:54:48 +0000</pubDate></item><item><title>London–Calcutta Bus Service</title><link>https://en.wikipedia.org/wiki/London%E2%80%93Calcutta_bus_service</link><description>&lt;doc fingerprint="d36cd5cbf64c31a6"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;London–Calcutta bus service&lt;/head&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;Overview&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Operator&lt;/cell&gt;&lt;cell&gt;Albert Travel&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Status&lt;/cell&gt;&lt;cell&gt;defunct&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Began service&lt;/cell&gt;&lt;cell&gt;c. 1957&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Ended service&lt;/cell&gt;&lt;cell&gt;c. 1976&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Route&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Start&lt;/cell&gt;&lt;cell&gt;London, United Kingdom&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Via&lt;/cell&gt;&lt;cell&gt;Belgium, West Germany, Austria, Yugoslavia, Bulgaria, Turkey, Iran, Afghanistan, Pakistan&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;End&lt;/cell&gt;&lt;cell&gt;Calcutta, India&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Other routes&lt;/cell&gt;&lt;cell&gt;London-Calcutta-Sydney&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;Service&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Journey time&lt;/cell&gt;&lt;cell&gt;50+ days&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;The London to Calcutta bus service was a long-distance international bus route that operated between London, England, and Calcutta, India. Launched in 1957, it was widely regarded as the longest bus route in the world at the time.[1][2][3] The journey spanned approximately 10,000 miles (16,000 km) one way, and over 20,000 miles (32,700 km) round trip, taking about 50 days to complete each leg.&lt;/p&gt;&lt;p&gt;The route passed through several countries, including Belgium, Yugoslavia, and parts of Northwest India,[4] and became famously associated with the overland Hippie Trail of the 1960s and 1970s.&lt;/p&gt;&lt;p&gt;The service offered an all-inclusive experience covering travel, food, and accommodation.[3] In 1957, a one-way ticket cost £85 (equivalent to £2,589 in 2023), rising to £145 by 1973 (equivalent to £2,215 in 2023).&lt;/p&gt;&lt;p&gt;The service was discontinued in 1976 due to growing geopolitical instability in the Middle East, which made the route increasingly dangerous.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Route&lt;/head&gt;[edit]&lt;p&gt;The bus service was operated by Albert Travel.[6] The maiden journey set out from London on April 15, 1957.[7] The first service arrived in Calcutta on June 5, 50 days later. During its journey the bus traveled from England to Belgium, and from there to India via West Germany, Austria, Yugoslavia, Bulgaria, Turkey, Iran, Afghanistan, Pakistan and North Western India. After entering India, it eventually reached Calcutta via New Delhi, Agra, Allahabad and Banaras.[5]&lt;/p&gt;&lt;head rend="h2"&gt;Facilities on the bus&lt;/head&gt;[edit]&lt;p&gt;The bus was equipped with reading facilities, separate sleeping bunks for all passengers, fan-operated heaters, and a kitchen. There was a forward observation lounge on the upper deck of the bus. Radio and a music system for parties was provided.[1] It had time to spend at tourist destinations in India, including Banaras and the Taj Mahal on the banks of the Yamuna. Shopping was also allowed in Salzburg, Vienna, Istanbul, Tehran and Kabul.[3][8]&lt;/p&gt;&lt;head rend="h2"&gt;Later history&lt;/head&gt;[edit]&lt;p&gt;After some years the bus had an accident and became unusable. Later[specify] the bus was purchased by Andy Stewart, a British traveler. He rebuilt it to be a mobile home with two levels. The double-decker was renamed to Albert and was traveled from Sydney to London via India on October 8, 1968. It took about 132 days for the bus to reach London. Albert Tours was a company based in England and Australia and it operated on London–Calcutta–London and London–Calcutta–Sydney routes.[9]&lt;/p&gt;&lt;p&gt;The bus reached India through Iran and then it traveled to Singapore through Burma, Thailand and Malaysia. From Singapore, the bus was transported to Perth in Australia by ship, and from there it traveled by road to Sydney.[10][11] The charge for this service from London to Calcutta was £145. The service had all the modern facilities as before. The bus service was discontinued in 1976 due to political conditions leading up to the Iranian Revolution and the escalation of tensions between Pakistan and India.[12] The Albert Tours completed about 15 trips between Kolkata to London and again from London to Sydney, before the service ended permanently.[13]&lt;/p&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ a b "This Was 'World's Longest Bus Route' From Kolkata To London". Curly Tales. 2020-07-06. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ "A Bus Ride From London to Kolkata in 1950s? Yes, The Viral Photo is Real". News18. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ a b c Civic Affairs. Vol. 4. P. C. Kapoor at the Citizen Press. 1957 – via books.google.com.&lt;/item&gt;&lt;item&gt;^ "London Calcutta Bus Trip 1957 london India Editorial Stock Photo - Stock Image | Shutterstock". Shutterstock Editorial. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ a b "Samayam". malayalam.samayam.com. 2 July 2020. Retrieved 13 June 2023.&lt;/item&gt;&lt;item&gt;^ "London to Calcutta by Road? Picture of 1950s Albert Travel Bus Service is Going Viral, Know Details About This Fascinating Historic Journey". Unique News Online. 2020-07-02. Retrieved 2021-02-19.&lt;/item&gt;&lt;item&gt;^ Whispers of Yesterday, Rare Historical Photos, Old Photos, retrieved 2023-11-29&lt;/item&gt;&lt;item&gt;^ admin (2020-07-04). "ലണ്ടൻ – കൽക്കട്ട ബസ് റൂട്ട്". News Kerala online. Archived from the original on 2020-07-06. Retrieved 2020-07-24.&lt;/item&gt;&lt;item&gt;^ Eat, Tech Travel (2020-07-03). "ലണ്ടനിൽ നിന്നും ഇന്ത്യയിലെ കൽക്കട്ടയിലേക്ക് ഒരു ബസ് സർവ്വീസ്". Technology &amp;amp; Travel Blog from India. Retrieved 2020-07-31.&lt;/item&gt;&lt;item&gt;^ "ലണ്ടനിൽ നിന്നു കൽക്കട്ടയിലെത്തിയ ഇന്ത്യാ മാന്..." ManoramaOnline (in Malayalam). Retrieved 2020-07-31.&lt;/item&gt;&lt;item&gt;^ INGALIS, LEONARD (1957-08-08). "London-Calcutta Bus is back in London - Owner drove passengers 20,300 Miles". The New York Times.&lt;/item&gt;&lt;item&gt;^ K, Noushad K. "ലണ്ടൻ - കൽക്കട്ട ബസ്". Archived from the original on 2020-07-06. Retrieved 2020-07-31.&lt;/item&gt;&lt;item&gt;^ "Kolkata, Then Calcutta, Once Had The World's Longest Bus Route All The Way Till London!". Whats Hot. Retrieved 2020-07-31.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46554462</guid><pubDate>Fri, 09 Jan 2026 14:50:48 +0000</pubDate></item><item><title>How to store a chess position in 26 bytes</title><link>https://ezzeriesa.notion.site/How-to-store-a-chess-position-in-26-bytes-using-bit-level-magic-df1fdb5364eb42fdac11eb23b25e9605</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46554652</guid><pubDate>Fri, 09 Jan 2026 15:07:17 +0000</pubDate></item><item><title>Developers Are Solving the Wrong Problem</title><link>https://caseysoftware.com/blog/developers-are-solving-the-wrong-problem</link><description>&lt;doc fingerprint="37a0d05d2f7520b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Everyone is either offended or excited about “vibe coding.” It’s all the rage and going to solve all your problems, or it’s the next great evil spewing crap code all over your systems. Those of us who love well structured clean code which is modular and concise seem to be a dying breed. For someone who’s early career was shaped by McConnell’s Code Complete, Brooks’ The Mythical Man Month, and Fowler’s Refactoring, this feels.. odd.&lt;/p&gt;
    &lt;p&gt;But when we dig into the WHY, something interesting happens:&lt;/p&gt;
    &lt;p&gt;Why do we want “well structured” code?&lt;/p&gt;
    &lt;p&gt;Well structured code is easier to understand to debug, extend, and maintain. But is there a single, shared definition of “well structured”?&lt;/p&gt;
    &lt;p&gt;Why do we want “clean” code?&lt;/p&gt;
    &lt;p&gt;Clean code is easier to understand to debug, extend, and maintain. But is there a single, shared definition of “clean”?&lt;/p&gt;
    &lt;p&gt;Why do we want “modular” code?&lt;/p&gt;
    &lt;p&gt;Modular code is easier to understand to debug, extend, and maintain. But is there a single, shared definition of “modular”? Actually, yes the Single Responsibility Principle addresses this one.&lt;/p&gt;
    &lt;p&gt;Why do we want “concise” code?&lt;/p&gt;
    &lt;p&gt;The less code there is, the easier it is to understand to debug, extend, and maintain. But too concise can make things harder to understand.&lt;/p&gt;
    &lt;p&gt;But when it gets down to it, all of these goals point at the problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Not About You&lt;/head&gt;
    &lt;p&gt;Previously, we needed “well structured clean code which is modular and concise” because writing code was easy but reading code is hard. Really hard. Painfully hard. Making sense of someone else’s code is harder still. All of our practices are really just to decrease that pain. Anything we can do to make it easier for the next person – or ourselves six months from now – is worth it.&lt;/p&gt;
    &lt;p&gt;But what if the next “person” isn’t a person?&lt;/p&gt;
    &lt;p&gt;If we assume that the code will only be debugged, extended, and maintained by a computer, most of our reasoning for clean code goes out the window. We don’t care what a human can do with the resulting (output) code as long as they can consistently generate code and configuration to solve the business problem, which gets at the REAL problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s Not About Code&lt;/head&gt;
    &lt;p&gt;Somewhere along the line, we started treating the code as our goal. We worked hard to make sure it was structurally perfect on whatever framework we’re using today and didn’t realize that no one cares about our code. No one cares or even knows how “clean” our code is. They don’t even know what code is.&lt;/p&gt;
    &lt;p&gt;All our customers know is “does this solve my problem?” Enter vibe coding.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s About the Problem&lt;/head&gt;
    &lt;p&gt;When a savvy user can describe their problem in detail, they can skip over all the messy coding steps and get directly to a solution. It won’t be perfect, and it won’t have “clean code” but they’ll see it in hours instead of months. More importantly, the savviest users can make mistakes, improve their understanding, experiment with ideas, and iterate on the entire process while getting better at each step along the way at a fraction of the cost.&lt;/p&gt;
    &lt;p&gt;And that’s why vibe coding is popular and only going to get more popular.&lt;/p&gt;
    &lt;p&gt;Instead of seeing vibe coding as a threat, we need to consider it another tool.&lt;/p&gt;
    &lt;p&gt;If you see it exclusively as a threat, I’m shocked you read this far. Thanks. I hope I can nudge your thinking.&lt;/p&gt;
    &lt;p&gt;If you see it as a tool, there are a few things we can do.&lt;/p&gt;
    &lt;head rend="h2"&gt;Improving Vibe Coding&lt;/head&gt;
    &lt;p&gt;First, remember that any generative AI approach is only as good as the underlying model. If there’s a public model that suits your needs, use it. Though you can also give it context by adding code that fits your standards and expectations. To be clear, I don’t mean coding standards but patterns and practices which demonstrate good security and performant code.&lt;/p&gt;
    &lt;p&gt;Next, build out the rest of your tools. If you’re not writing the code directly, you need to be able to validate that the code works exactly the way you expect. At a minimum, that means testing business logic and validating interfaces, but you should include security and load or performance testing systems too.&lt;/p&gt;
    &lt;p&gt;Next, figure out how to describe your needs and capabilities effectively. We all know about requirements documents and Jiras but you’ll need to figure out how to translate that into actionable requests and steps for the generative AI. This will vary heavily on the system you’re using.&lt;/p&gt;
    &lt;p&gt;Finally, get used to throwing code away. Remember that your goal is solving the problem, and your code is merely the byproduct or the tool to get to that solution. The most important parts are the prompt and process used to generate the code, along with the understanding you gained and applied to get to that prompt. The more and faster you can learn, adapt, iterate, and ship, the better your solution will be.&lt;/p&gt;
    &lt;head rend="h2"&gt;But what happens After Vibe Coding?&lt;/head&gt;
    &lt;p&gt;It’s hard to predict things – especially the future – but I have a couple ideas.&lt;/p&gt;
    &lt;p&gt;First, more people are going to try more ideas faster. This is good. The people who were Excel wizards a generation ago are going to create “pretty good” things that solve their problems without involving a developer. My friends over at Dreambase are already doing that. We’ll have more automations and solutions than ever before.&lt;/p&gt;
    &lt;p&gt;Second, we may see the rise of “developer-less” companies. People will be able to imagine, describe, and ship a product with minimal developer input. Occasionally they may need an integration or similar and might contract with someone but maybe not even that. In my “Creating Better SDKs with Generative AI” course for LinkedIn, I realized that with well-defined interfaces, you don’t need much human interaction.&lt;/p&gt;
    &lt;p&gt;BUT – is this good or bad? Depends on who you are. If you job is to solve problems faster, this is a great time to be in the space. If your job is only to write beautiful code, you have a problem.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555302</guid><pubDate>Fri, 09 Jan 2026 16:05:59 +0000</pubDate></item><item><title>Cloudspecs: Cloud Hardware Evolution Through the Looking Glass</title><link>http://muratbuffalo.blogspot.com/2026/01/cloudspecs-cloud-hardware-evolution.html</link><description>&lt;doc fingerprint="edc998f165b26ddb"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Cloudspecs: Cloud Hardware Evolution Through the Looking Glass&lt;/head&gt;
    &lt;p&gt;This paper (CIDR'26) presents a comprehensive analysis of cloud hardware trends from 2015 to 2025, focusing on AWS and comparing it with other clouds and on-premise hardware.&lt;/p&gt;
    &lt;p&gt;TL;DR: While network bandwidth per dollar improved by one order of magnitude (10x), CPU and DRAM gains (again in performance per dollar terms) have been much more modest. Most surprisingly, NVMe storage performance in the cloud has stagnated since 2016. Check out the NVMe SSD discussion below for data on this anomaly.&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU Trends&lt;/head&gt;
    &lt;p&gt;Multi-core parallelism has skyrocketed in the cloud. Maximum core counts have increased by an order of magnitude over the last decade. The largest AWS instance u7in now boasts 448 cores. However, simply adding cores hasn't translated linearly into value. To measure real evolution, the authors normalized benchmarks (SPECint, TPC-H, TPC-C) by instance cost. SPECint benchmarking shows that cost-performance improved roughly 3x over ten years. A huge chunk of that gain comes from AWS Graviton. Without Graviton, the gain drops to roughly 2x. For in-memory database benchmarks, gains were even lower (2x–2.5x), likely due to memory and cache latency bottlenecks.&lt;/p&gt;
    &lt;p&gt;On-prem hardware comparison shows that this stagnation is not cloud price gouging. Historically, Moore's Law and Dennard scaling doubled cost-performance every two years (which would have sum up to 32x gain over a decade). However, an analysis of on-premise AMD server CPUs reveals a similar slump, only a 1.7x gain from 2017 to 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Trends&lt;/head&gt;
    &lt;p&gt;DRAM capacity per dollar has effectively flatlined. The only significant improvement was the 2016 introduction of memory-optimized x instances, which offered ~3.3x more GiB-hours/$ than compute-optimized peers. While absolute single-socket bandwidth jumped ~5x (93 GiB/s to 492 GiB/s) as servers moved from DDR3 to DDR5, the cost-normalized gain is only 2x.&lt;/p&gt;
    &lt;p&gt;Historical data suggests commodity DRAM prices dropped 3x over the decade. But in the last three months, due to AI-driven demand, DDR5 prices rose sharply, further limiting effective memory gains.&lt;/p&gt;
    &lt;head rend="h2"&gt;Network Trends&lt;/head&gt;
    &lt;p&gt;We have good news here, finally. Network bandwidth per dollar exploded by 10x. And absolute speeds went from 10 Gbit/s to 600 Gbit/s (60x).&lt;/p&gt;
    &lt;p&gt;These gains were not universal though. Generic instances saw little change. The gains were driven by network-optimized n instances (starting with the c5n in 2018) powered by proprietary Nitro cards.&lt;/p&gt;
    &lt;head rend="h2"&gt;NVMe Trends&lt;/head&gt;
    &lt;p&gt;NVMe SSDs are the biggest surprise. Unlike CPUs and memory, where cloud trends mirror on-prem hardware, NVMe performance in AWS has largely stagnated. The first NVMe-backed instance family, i3, appeared in 2016. As of 2025, AWS offers 36 NVMe instance families. Yet the i3 still delivers the best I/O performance per dollar by nearly 2x.&lt;/p&gt;
    &lt;p&gt;SSD capacity has stagnated since 2019 and I/O throughput since 2016. This sharply contrasts with on-prem hardware, where SSD performance doubled twice (PCIe 4 and PCIe 5) in the same timeframe. The gap between cloud and on-premise NVMe is widening rapidly.&lt;/p&gt;
    &lt;p&gt;This price/performance gap likely explains the accelerating push toward disaggregated storage. When local NVMe is expensive and underperforming, remote storage starts to look attractive. The paper speculates that with network speeds exploding and NVMe stagnating, architectures may shift further. For systems like Snowflake, using local NVMe for caching might no longer be worth the complexity compared to reading directly from S3 with fast networks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;I think the main takeaway is that uniform hardware scaling in the cloud is over. Moore's Law no longer lifts all boats. Performance gains now come from specialization, especially networking (e.g., Graviton, Nitro, Accelerators).&lt;/p&gt;
    &lt;p&gt;In my HPTS 2024 review, I noted that contrary to the deafening AI hype, the real excitement in the hallways was about hardware/software codesign. This paper validates that sentiment. With general-purpose CPU and memory cost-performance stagnating, future databases must be tightly integrated with specialized hardware and software capabilities to provide value. I think the findings here will refuel that trend.&lt;/p&gt;
    &lt;p&gt;A key open question is why massive core counts deliver so little value. Where is the performance lost? Possible explanations include memory bandwidth limits, poor core-to-memory balance, or configuration mismatches. But I think the most likely culprit is software. Parallel programming remains hard, synchronization is expensive, and many systems fail to scale beyond a modest number of cores. We may be leaving significant performance on the table simply because our software cannot effectively utilize the massive parallelism now available.&lt;/p&gt;
    &lt;p&gt;The paper comes with an interactive tool, Cloudspecs, built on DuckDB-WASM (yay!). This allows you to run SQL queries over the dataset directly in the browser to visualize these trends. The figures in the PDF actually contain clickable link symbols that take you to the specific query used to generate that chart. Awesome reproducibility!&lt;/p&gt;
    &lt;p&gt;Aleksey and I did a live-reading of the paper. As usual, we had a lot to argue about. I'll add a recording of our discussion on YouTube when it becomes available, and here is a link to my annotated paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555485</guid><pubDate>Fri, 09 Jan 2026 16:23:44 +0000</pubDate></item><item><title>Latest SteamOS Beta Now Includes Ntsync Kernel Driver</title><link>https://www.phoronix.com/news/Steam-OS-Beta-NTSYNC</link><description>&lt;doc fingerprint="4fa7b87e288b3d14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Latest SteamOS Beta Now Includes NTSYNC Kernel Driver&lt;/head&gt;
    &lt;p&gt; Valve released the SteamOS 3.7.20 beta overnight and with it they are finally building the NTSYNC kernel driver for helping accelerate Windows NT synchronization primitives. &lt;lb/&gt;The NTSYNC kernel driver has been in good shape for about one year now after the implementation was finished up. From user-space Wine 10.16 added NTSYNC usage support as part of the upcoming Wine 11.0 stable release due out this month. In turn Proton 11.0 will see that support when it is re-based atop Wine 11.0. Albeit Proton (Steam Play) already has FSYNC for good performance but will be interesting to see how the NTSYNC path performs for SteamOS / Steam Play needs in comparison.&lt;lb/&gt;For gearing up for that future Proton NTSYNC support, SteamOS 3.7.20 enables the NTSYNC kernel driver and loads the module by default. Most Linux distributions are at least already building the NTSYNC kernel module though there's been different efforts on how to handle ensuring it's loaded when needed. The presence of the NTSYC kernel driver is the main highlight of the SteamOS 3.7.20 beta now available for testing.&lt;/p&gt;
    &lt;p&gt;The NTSYNC kernel driver has been in good shape for about one year now after the implementation was finished up. From user-space Wine 10.16 added NTSYNC usage support as part of the upcoming Wine 11.0 stable release due out this month. In turn Proton 11.0 will see that support when it is re-based atop Wine 11.0. Albeit Proton (Steam Play) already has FSYNC for good performance but will be interesting to see how the NTSYNC path performs for SteamOS / Steam Play needs in comparison.&lt;/p&gt;
    &lt;p&gt;For gearing up for that future Proton NTSYNC support, SteamOS 3.7.20 enables the NTSYNC kernel driver and loads the module by default. Most Linux distributions are at least already building the NTSYNC kernel module though there's been different efforts on how to handle ensuring it's loaded when needed. The presence of the NTSYC kernel driver is the main highlight of the SteamOS 3.7.20 beta now available for testing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555512</guid><pubDate>Fri, 09 Jan 2026 16:27:19 +0000</pubDate></item><item><title>Cloudflare CEO on the Italy Fines</title><link>https://twitter.com/eastdakota/status/2009654937303896492</link><description>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555760</guid><pubDate>Fri, 09 Jan 2026 16:46:48 +0000</pubDate></item><item><title>The Vietnam government has banned rooted phones from using any banking app</title><link>https://xdaforums.com/t/discussion-the-root-and-mod-hiding-fingerprint-spoofing-keybox-stealing-cat-and-mouse-game.4425939/page-118</link><description>&lt;doc fingerprint="d77b9987e9f764cd"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt; Regulated in Circular 77/2025/TT-NHNN amending Circular 50 on online service security in the banking industry, to be in affect from March 1st:&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Thông tư số 77/2025/TT-NHNN của Ngân hàng Nhà nước Việt Nam: Sửa đổi, bổ sung một số điều của Thông tư số 50/2024/TT-NHNN của Thống đốc Ngân hàng Nhà nước Việt Nam quy định về an toàn, bảo mật cho việc cung cấp dịch vụ trực tuyến trong ngành Ngân hàng&lt;/p&gt;
            &lt;div&gt;
              &lt;p&gt; vanban.chinhphu.vn &lt;/p&gt;
            &lt;/div&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt; Clause 2, Article 5: Amend and supplement Clause 4 of Article 8 as follows: &lt;/p&gt;
      &lt;p&gt; 4. Implement solutions to prevent, combat, and detect unauthorized interference with the Mobile Banking application installed on customers' mobile devices. The Mobile Banking application must automatically exit or stop functioning and notify the customer of the reason if any of the following signs are detected: &lt;/p&gt;
      &lt;p&gt; a) A debugger is attached or the environment has a debugger running; or when the application is running in an emulator/virtual machine/emulator; or operating in a mode that allows the computer to communicate directly with the Android device (Android Debug Bridge); &lt;/p&gt;
      &lt;p&gt; b) The application software is injected with external code while running, performing actions such as monitoring executed functions, logging data transmitted through functions, APIs, etc. (hooks); or the application software is tampered with or repackaged. &lt;/p&gt;
      &lt;p&gt; c) The device has been rooted/jailbroken; or its bootloader has been unlocked." &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555963</guid><pubDate>Fri, 09 Jan 2026 17:00:11 +0000</pubDate></item><item><title>IcePanel (YC W23) is hiring full-stack engineers in Vancouver</title><link>https://forms.icepanel.io/careers/senior-product-engineer</link><description>&lt;doc fingerprint="80b89e5ad2c4a6e0"&gt;
  &lt;main&gt;
    &lt;p&gt;$170,000 salary (CAD) + Profit-share quarterly bonus (last year averaged ~30k-40k each)&lt;/p&gt;
    &lt;p&gt;+ 1% equity + Unlimited holiday + Health benefits&lt;/p&gt;
    &lt;p&gt;We’re looking for someone with a high degree of agency, who can immediately take ownership of building new functionality from design &amp;gt; implementation &amp;gt; maintaining and refining current features based on our customers' needs.&lt;/p&gt;
    &lt;p&gt;You’ll be building end-to-end, including: - Frontend UI/UX design alongside a designer. - Backend API/data structure design. - Data migration and infrastructure changes. - Bug fixing and iterations.&lt;/p&gt;
    &lt;p&gt;We're simplifying how teams design for complex systems. We're building a collaborative diagramming and modelling tool that software architects think is cool.&lt;/p&gt;
    &lt;p&gt;We’re a small, energetic team that believes in building a lean and profitable business after being in the YCombinator W23 batch. We’ve grown the product to ~$4 million CAD in ARR and believe in continuing to build on profitability over funding. We’re looking for talented, driven people who love their craft to help achieve our vision of simplifying complexity.&lt;/p&gt;
    &lt;p&gt;🙋 Independence to build our way&lt;/p&gt;
    &lt;p&gt;🛠️ Build simple and exceptional experiences&lt;/p&gt;
    &lt;p&gt;🧊 Transparency and openness&lt;/p&gt;
    &lt;p&gt;💡 Stay humble and explore all ideas&lt;/p&gt;
    &lt;p&gt;💩 No bullshit, have fun&lt;/p&gt;
    &lt;p&gt;- In-person days every week (Tuesday, Wednesday, Thursday)&lt;/p&gt;
    &lt;p&gt;- North Vancouver, British Columbia, Canada&lt;/p&gt;
    &lt;p&gt;- Hybrid &amp;amp; flexible work environment&lt;/p&gt;
    &lt;p&gt;- This is not a fully remote job&lt;/p&gt;
    &lt;p&gt;🍰 Equity in the company 💰 Profit sharing&lt;/p&gt;
    &lt;p&gt;💻 Work setup provided&lt;/p&gt;
    &lt;p&gt;🎉 Flexible work culture&lt;/p&gt;
    &lt;p&gt;🏂 Unlimited holiday&lt;/p&gt;
    &lt;p&gt;🧑⚕️ Health, dental, vision&lt;/p&gt;
    &lt;p&gt;📚 Learning budget&lt;/p&gt;
    &lt;p&gt;✈️ Conference budget&lt;/p&gt;
    &lt;p&gt;🌴 Annual team retreat&lt;/p&gt;
    &lt;p&gt;🌭 Hot dog Wednesdays&lt;/p&gt;
    &lt;p&gt;🧊 Free ice cubes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46555977</guid><pubDate>Fri, 09 Jan 2026 17:01:02 +0000</pubDate></item><item><title>Show HN: I made a memory game to teach you to play piano by ear</title><link>https://lend-me-your-ears.specr.net</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46556210</guid><pubDate>Fri, 09 Jan 2026 17:17:28 +0000</pubDate></item><item><title>Replit (YC W18) Is Hiring</title><link>https://jobs.ashbyhq.com/replit</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46556822</guid><pubDate>Fri, 09 Jan 2026 18:00:56 +0000</pubDate></item><item><title>Show HN: Scroll Wikipedia like TikTok</title><link>https://quack.sdan.io</link><description>&lt;doc fingerprint="434f9162ba198e26"&gt;
  &lt;main&gt;
    &lt;p&gt;Following Slop Ducks Storytime Home Friends Inbox Profile&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46557029</guid><pubDate>Fri, 09 Jan 2026 18:15:16 +0000</pubDate></item></channel></rss>