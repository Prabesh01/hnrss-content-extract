<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 01 Sep 2025 14:09:40 +0000</lastBuildDate><item><title>Why haven't quantum computers factored 21 yet?</title><link>https://algassert.com/post/2500</link><description>&lt;doc fingerprint="7135b1ea033e1ef8"&gt;
  &lt;main&gt;
    &lt;p&gt;In 2001, quantum computers factored the number 15. It’s now 2025, and quantum computers haven’t yet factored the number 21. It’s sometimes claimed this is proof there’s been no progress in quantum computers. But there’s actually a much more surprising reason 21 hasn’t been factored yet, which jumps out at you when contrasting the operations used to factor 15 and to factor 21.&lt;/p&gt;
    &lt;p&gt;The circuit (the series of quantum logic gates) that was run to factor 15 can be seen in Figure 1b of “Experimental realization of Shor’s quantum factoring algorithm using nuclear magnetic resonance”:&lt;/p&gt;
    &lt;p&gt;The important cost here is the number of entangling gates. This factoring-15 circuit has 6 two-qubit entangling gates (a mix of CNOT and CPHASE gates). It also has 2 Toffoli gates, which each decompose into 6 two-qubit entangling gates. So there’s a total of 21 entangling gates in this circuit.&lt;/p&gt;
    &lt;p&gt;Now, for comparison, here is a circuit for factoring 21. Sorry for rotating it, but I couldn’t get it to fit otherwise. Try counting the Toffolis:&lt;/p&gt;
    &lt;p&gt;(Here’s an OPENQASM2 version of the circuit, so you can test it produces the right distribution if you’re inclined to do so.)&lt;/p&gt;
    &lt;p&gt;In case you lost count: this circuit has 191 cnot gates and 369 Toffoli gates, implying a total of 2405 entangling gates. That’s 115x more entangling gates than the factoring-15 circuit. The factoring-21 circuit is more than one hundred times more expensive than the factoring-15 circuit.&lt;/p&gt;
    &lt;p&gt;When I ask people to guess how many times larger the factoring-21 circuit is, compared to the factoring-15 circuit, there’s a tendency for them to assume it’s 25% larger. Or maybe twice as large. The fact that it’s two orders of magnitude more expensive is shocking. So I’ll try to explain why it happens.&lt;/p&gt;
    &lt;p&gt;(Quick aside: the amount of optimization that has gone into this factoring-21 circuit is probably unrepresentative of what would be possible when factoring big numbers. I think a more plausible amount of optimization would produce a circuit with 500x the cost of the factoring-15 circuit… but a 100x overhead is sufficient to make my point. Regardless, special thanks to Noah Shutty for running expensive computer searches to find the conditional-multiplication-by-4-mod-21 subroutine used by this circuit.)&lt;/p&gt;
    &lt;head rend="h1"&gt;Where does the 100x come from?&lt;/head&gt;
    &lt;p&gt;A key background fact you need to understand is that the dominant cost of a quantum factoring circuit comes from doing a series of conditional modular multiplications under superposition. To factor an $n$-bit number $N$, Shor’s algorithm will conditionally multiply an accumulator by $m_k = g^{2^k} \pmod{N}$ for each $k &amp;lt; 2n$ (where $g$ is a randomly chosen value coprime to $N$). Sometimes people also worry about the frequency basis measurement at the end of the algorithm, which is crucial to the algorithm’s function, but from a cost perspective it’s irrelevant. (It’s negligible due by an optimization called “qubit recycling”, which I also could have used to reduce the qubit count of the factoring-21 circuit, but in this post I’m just counting gates so meh).&lt;/p&gt;
    &lt;p&gt;There are three effects that conspire to make the factoring-15 multiplications substantially cheaper than the factoring-21 multiplications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All but two of the factoring-15 multiplications end up multiplying by 1.&lt;/item&gt;
      &lt;item&gt;The first multiplication is always ~free, because its input is known to be 1.&lt;/item&gt;
      &lt;item&gt;The one remaining factoring-15 multiplication can be implemented with only two CSWAPs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s consider the case where $g=2$. In that case, when factoring 15, the constants to conditionally multiply by would be:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt; print([pow(2, 2**k, 15) for k in range(8)])
[2, 4, 1, 1, 1, 1, 1, 1]
&lt;/code&gt;
    &lt;p&gt;First, notice that the last six constants are 1. Multiplications by 1 can be implemented by doing nothing. So the factoring-15 circuit is only paying for 2 of the expected 8 multiplications.&lt;/p&gt;
    &lt;p&gt;Second, notice that the first conditional multiplication (by 2) will either leave the accumulator storing 1 (when its control is off) or storing 2 (when its control is on). This can be achieved much more cheaply by performing a controlled xor of $1 \oplus 2 = 3$ into the accumulator.&lt;/p&gt;
    &lt;p&gt;Third, notice that the only remaining multiplication is a multiplication by 4. Because 15 is one less than a power of 2, multiplying by 2 modulo 15 can be implemented using a circular shift. A multiplication by 4 is just two multiplications by 2, so it can also be implemented by a circular shift. This is a very rare property for a modular multiplication to have, and here it reduces what should be an expensive operation into a pair of conditional swaps. (If you go back and look at the factoring-15 circuit at the top of the post, the 2 three-qubit gates are being used to implement these two conditional swaps.)&lt;/p&gt;
    &lt;p&gt;You may worry that these savings are specific to the choice of $g=2$ and $N=15$. And they are in fact specific to $N=15$. But they aren’t specific to $g=2$. They occur for all possible choices of $g$ when factoring 15.&lt;/p&gt;
    &lt;p&gt;For contrast, let’s now consider what happens when factoring 21. Using $g=2$, the multiplication constants would be:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;  print([pow(2, 2**k, 21) for k in range(10)])
[2, 4, 16, 4, 16, 4, 16, 4, 16, 4]
&lt;/code&gt;
    &lt;p&gt;This is going to be a lot more expensive.&lt;/p&gt;
    &lt;p&gt;First, there’s no multiplications by 1, so the circuit has to pay for every multiplication instead of only a quarter. That’s a ~4x relative cost blowup vs factoring 15. Second, although the first-one’s-free trick does still apply, proportionally speaking it’s not as good. It cheapens 10% of the multiplications rather than 50%. That’s an extra ~1.8x cost blowup vs factoring 15. Third, the multiplication by 4 and 16 can’t be implemented with two CSWAPs. The best conditionally-multiply-by-4-mod-21 circuit that I know is the one being used in the diagram above, and it uses 41 Toffolis. These more expensive multiplications add a final bonus ~20x cost blowup vs factoring 15.&lt;/p&gt;
    &lt;p&gt;(Aside: multiplication by 16 mod 21 is the inverse of multiplying by 4 mod 21, and the circuits are reversible, so multiplying by 16 uses the same number of Toffolis as multiplying by 4.)&lt;/p&gt;
    &lt;p&gt;These three factors (multiplying-by-one, first-one’s-free, and multiplying-by-swapping) explain the 100x blowup in cost of factoring 21, compared to factoring 15. And this 100x increase in cost explains why no one has factored 21 with a quantum computer yet.&lt;/p&gt;
    &lt;head rend="h1"&gt;Closing remarks&lt;/head&gt;
    &lt;p&gt;Another contributor to the huge time gap between factoring 15 and factoring 21 is that the 2001 factoring of 15 was done with an NMR quantum computer. These computers were known to have inherent scaling issues, and in fact it’s debated whether NMR computers were even properly “quantum”. If the 2001 NMR experiment doesn’t count, I think the actually-did-the-multiplications runner-up is a 2015 experiment done with an ion trap quantum computer (discussed by Scott Aaronson at the time).&lt;/p&gt;
    &lt;p&gt;Yet another contributor is the overhead of quantum error correction. Performing 100x more gates requires 100x lower error, and the most plausible way of achieving that is error corection. Error correction requires redundancy, and could easily add a 100x overhead on qubit count. Accounting for this, I could argue that factoring 21 will be ten thousand times more expensive than factoring 15, rather than “merely” a hundred times more expensive.&lt;/p&gt;
    &lt;p&gt;There are papers that claim to have factored 21 with a quantum computer. For example, here’s one from 2021. But, as far as I know, all such experiments are guilty of using optimizations that imply the code generating the circuit had access to information equivalent to knowing the factors (as explained in “Pretending to factor large numbers on a quantum computer” by Smolin et al). Basically: they don’t do the multiplications, because the multiplications are hard, but the multiplications are what make it factoring instead of simpler forms of period finding. So I don’t count them.&lt;/p&gt;
    &lt;p&gt;There is unfortunately a trickle of bullshit results that claim to be quantum factoring demonstrations. For example, I have a joke paper in this year’s sigbovik proceedings that cheats in a particularly silly way. More seriously, I enjoyed “Replication of Quantum Factorisation Records with an 8-bit Home Computer, an Abacus, and a Dog” making fun of some recent egregious papers. I also recommend Scott Aaronson’s post “Quantum computing motte-and-baileys”, which complains about papers that benchmark “variational” factoring techniques while ignoring the lack of any reason to expect them to scale.&lt;/p&gt;
    &lt;p&gt;Because of the large cost of quantum factoring numbers (that aren’t 15), factoring isn’t yet a good benchmark for tracking the progress of quantum computers. If you want to stay abreast of progress in quantum computing, you should be paying attention to the arrival quantum error correction (such as surface codes getting more reliable as their size is increased) and to architectures solving core scaling challenges (such as lost neutral atoms being continuously replaced).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45082587</guid></item><item><title>“This telegram must be closely paraphrased before being communicated to anyone”</title><link>https://history.stackexchange.com/questions/79371/this-telegram-must-be-closely-paraphrased-before-being-communicated-to-anyone</link><description>&lt;doc fingerprint="ae9509d36c2fb0d2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;It appears that it was US military communications doctrine to not send the exact same message twice using different encryption ("none" counting as one type of encryption), and the term of art for changing a message to avoid that was indeed "paraphrase".&lt;/p&gt;
      &lt;p&gt;I managed to dig up a US Army document on Cryptology from roughly that era that appears to discuss paraphrasing. The document in question is Department of the Army Technical Manual TM 32-220(pdf), dated 1950, titled "BASIC CRYPTOGRAPHY". It apparently supersedes previous documents TM-484 from March 1945 and TM 11-485 (alternative)(pdf) from June 1944. It would probably be more ideal to look at them, since they are closer to the time you are interested in, unfortunately I was not able to find TM-484 online.&lt;/p&gt;
      &lt;p&gt;Here's what this declassified manual had to say about "paraphrasing", from Chapter 7, in the section Fundamental Rules of Cryptographic Security, section 84, subsection b, rule 3 (titled "Text of messages")&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;(a) Never repeat in the clear the identical text of a message once sent in cryptographic form, or repeat in cryptographic form the text of a message once sent in the clear. Anything which will enable an alert enemy to compare a given piece of plain text with a cryptogram that supposedly contains this plain text is highly dangerous to the safety of the cryptographic system. Where information must be given out for publicity, or where information is handled by many persons, the plain text version should be very carefully paraphrased before distribution, to minimize the data an enemy might obtain from an accurate comparison of the cryptographic text with the equivalent, original plain text. To paraphrase a message means to rewrite it so as to change its original wording as much as possible without changing the meaning of the message. This is done by altering the positions of sentences in the message, by altering the positions of subject, predicate, and modifying phrases or clauses in the sentence, and by altering as much as possible the diction by the use of synonyms and synonymous expressions. In this process, deletion rather than expansion of the wording of the message is preferable, because if an ordinary message is paraphrased simply by expanding it along its original lines, an expert can easily reduce the paraphrased message to its lowest terms, and the resultant wording will be practically the original message. It is very important to eliminate repeated words or proper names, if at all possible, by the use of carefully selected pronouns; by the use of the words "former," "latter," "first-mentioned," "second-mentioned"; or by other means. After carefully paraphrasing, the message can be sent in the other key or code.&lt;/p&gt;
        &lt;p&gt;(b) Never send the literal plain text or a paraphrased version of the plain text of a message which has been or will be transmitted in cryptographed form except as specifically provided in appropriate regulations&lt;/p&gt;
      &lt;/quote&gt;
      &lt;p&gt;(emphasis mine)&lt;/p&gt;
      &lt;p&gt;In fact the allies would have have known intimately about how this was possible, because this is one of the ways they ended up decrypting the stronger German Enigma cipher. Captured machines using simpler ciphers were used to break those simpler ciphers. The fact that the Germans were encrypting the exact same messages in both ciphers meant the allies could know (for those messages) what both the unencrypted and encrypted messages were, which allowed them to decrypt the stronger cyphers as well, or quickly figure out what today's code was.&lt;/p&gt;
      &lt;quote&gt;
        &lt;p&gt;Though Enigma had some cryptographic weaknesses, in practice it was German procedural flaws, operator mistakes, failure to systematically introduce changes in encipherment procedures, and Allied capture of key tables and hardware that, during the war, enabled Allied cryptologists to succeed.&lt;/p&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45082731</guid></item><item><title>Survey: a third of senior developers say over half their code is AI-generated</title><link>https://www.fastly.com/blog/senior-developers-ship-more-ai-code</link><description>&lt;doc fingerprint="67c0a117eb3142e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vibe Shift? Senior Developers Ship nearly 2.5x more AI Code than Junior Counterparts&lt;/head&gt;
    &lt;p&gt;Content Marketing Manager&lt;/p&gt;
    &lt;p&gt;Fastlyâs July 2025 survey of 791 developers found a notable difference in how much AI-generated code is making it into production. About a third of senior developers (10+ years of experience) say over half their shipped code is AI-generated â nearly two and a half times the rate reported by junior developers (0â2 years of experience), at 13%.&lt;/p&gt;
    &lt;p&gt;âAI will bench test code and find errors much faster than a human, repairing them seamlessly. This has been the case many times,â one senior developer said. A junior respondent noted the trade-offs: âItâs always hard when AI assumes what Iâm doing and thatâs not the case, so I have to go back and redo it myself.â&lt;/p&gt;
    &lt;p&gt;Senior developers were also more likely to say they invest time fixing AI-generated code. Just under 30% of seniors reported editing AI output enough to offset most of the time savings, compared to 17% of juniors. Even so, 59% of seniors say AI tools help them ship faster overall, compared to 49% of juniors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Senior Developers Are More Optimistic About AI Saving Time&lt;/head&gt;
    &lt;p&gt;Just over 50% of junior developers say AI makes them moderately faster. By contrast, only 39% of more senior developers say the same. But senior devs are more likely to report significant speed gains: 26% say AI makes them a lot faster, double the 13% of junior devs who agree.&lt;/p&gt;
    &lt;p&gt;One reason for this gap may be that senior developers are simply better equipped to catch and correct AIâs mistakes. They have the experience to recognize when code âlooks rightâ but isnât. That makes them more confident at using AI tools efficiently, even for high-stakes or business-critical code. By contrast, junior developers may not fully trust their ability to spot errors, which can make them more cautious about relying on AI, or more likely to avoid using it in production at all.&lt;/p&gt;
    &lt;p&gt;That tracks with how much AI-generated code actually makes it into production. Among junior devs, just 13% say over half of their shipped code is AI-generated. By contrast, 32% of senior developers say the same, suggesting that more experienced engineers are not only using AI more aggressively, but are also trusting it more in production environments. This is surprising given growing concerns about âvibe codingâ introducing vulnerabilities into applications.Â&lt;/p&gt;
    &lt;head rend="h3"&gt;Perception vs. Reality&lt;/head&gt;
    &lt;p&gt;Nearly 1 in 3 developers (28%) say they frequently have to fix or edit AI-generated code enough that it offsets most of the time savings. Only 14% say they rarely need to make changes. And yet, over half of developers still feel faster with AI tools like Copilot, Gemini, or Claude.&lt;/p&gt;
    &lt;p&gt;Fastlyâs survey isnât alone in calling AI productivity gains into question. A recent randomized controlled trial (RCT) of experienced open-source developers found something even more striking: when developers used AI tools, they took 19% longer to complete their tasks.&lt;/p&gt;
    &lt;p&gt;This disconnect may come down to psychology. AI coding often feels smooth: code autocompletes with a few keystrokes. This gives the impression of momentum, but the early speed gains are often followed by cycles of editing, testing, and reworking that eat into any gains. This pattern is echoed both in conversations we've had with Fastly developers and in many of the comments we received in our survey.&lt;/p&gt;
    &lt;p&gt;One respondent put it this way: âAn AI coding tool like GitHub Copilot greatly helps my workflow by suggesting code snippets and even entire functions. However, it once generated a complex algorithm that seemed correct but contained a subtle bug, leading to several hours of debugging.â&lt;/p&gt;
    &lt;p&gt;Another noted: âThe AI tool saves time by using boilerplate code, but it also needs manual fixes for inefficiencies, which keep productivity in check.â&lt;/p&gt;
    &lt;p&gt;Yet, AI still seems to improve developer job satisfaction. Nearly 80% of developers say AI tools make coding more enjoyable. For some, itâs about skipping grunt work. For others, it might be the dopamine rush of code on demand.&lt;/p&gt;
    &lt;p&gt;âIt helps me complete a task that Iâm stuck with. It allows me to find the answers necessary to finish the task,â one survey respondent says.&lt;/p&gt;
    &lt;p&gt;Enjoyment doesnât equal efficiency, but in a profession wrestling with burnout and backlogs, that morale boost might still count for something.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Hidden Cost of AI Coding&lt;/head&gt;
    &lt;p&gt;Fastlyâs survey also explored developer awareness of green codingâthe practice of writing energy-efficient softwareâ and the energy cost behind AI coding tools. The practice of green coding goes up sharply with experience. Just over 56% of junior developers say they actively consider energy use in their work, while nearly 80% among mid- and senior-level engineers consider this when coding.Â&lt;/p&gt;
    &lt;p&gt;Developers are very aware of the environmental cost of AI tools: roughly two-thirds of developers across all experience levels said they know that these tools can carry a significant carbon footprint. Only a small minority (under 8% even at the most junior levels) were completely unaware. Altogether, the data suggests that sustainability is increasingly embedded in developer culture.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology&lt;/head&gt;
    &lt;p&gt;This survey was conducted by Fastly from July 10 to July 14, 2025, with 791 professional developers. All respondents confirm that writing or reviewing code is a core part of their job. The survey is distributed in the US and quality-controlled for accuracy, though, as with all self-reported data, some bias is possible.Â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45083635</guid></item><item><title>Jujutsu for everyone</title><link>https://jj-for-everyone.github.io/</link><description>&lt;doc fingerprint="97d01e5b68f7a9ce"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;This is a tutorial for the Jujutsu version control system. It requires no previous experience with Git or any other version control system.&lt;/p&gt;
    &lt;p&gt;At the time of writing, most Jujutsu tutorials are targeted at experienced Git users, teaching them how to transfer their existing Git skills over to Jujutsu. This tutorial is my attempt to fill the void of beginner learning material for Jujutsu. If you are already experienced with Git, I recommend Steve Klabnik's tutorial instead of this one.&lt;/p&gt;
    &lt;p&gt;This tutorial requires you to work in the terminal. Don't worry, there's a chapter covering some terminal basics in case you're not 100% comfortable with that yet. The commands I tell you to run will often only work on Unix-like operating systems like Linux and Mac. If you're on Windows (and can't switch to Linux), consider using WSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to read this tutorial&lt;/head&gt;
    &lt;p&gt;The tutorial is split into levels, which are the top-level chapters in the sidebar. The idea is that once you complete a level, you should probably put this tutorial away for a while and practice what you've learned. Once you're comfortable with those skills, come back for the next level.&lt;/p&gt;
    &lt;p&gt;There is one exception to this: If you're here because you need to collaborate with other people, you should complete the levels 1 and 2 right away.&lt;/p&gt;
    &lt;p&gt;Here's an overview of the planned levels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Level&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;The bare minimum to get started. This is only enough for the simplest use cases where you're working alone. For example, students who track and submit their homework with a Git repository can get by with only this.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;The bare minimum for any sort of collaboration. Students who are working on a group project and professional software developers need to know this. Going further is highly recommended, but you can take a break after this.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;Basic problem solving skills like conflict resolution and restoring files from history. Without this knowledge, it's only a matter of time until you run into trouble. Completing this level is comparable to the skill level of the average software developer.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;History rewriting skills. These will allow you to iterate toward a polished version history, which pays dividends long-term. Some projects require you to have these skills in order to meet their quality standards.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;Productivity boosters, advanced workflows, lesser-known CLI functions and a little VCS theory. Completing this level means you have mastered Jujutsu.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;Additional topics that only come up in specific situations: tags, submodules, workspaces etc. Consider skimming the list of topics and come back once you have an actual need for it.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Only a few levels are complete right now, the rest are on the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reset your progress&lt;/head&gt;
    &lt;p&gt;Throughout the tutorial, you will build an example repository. Later chapters depend on the state of previous ones. Losing the state of the example repo can therefore block you from making smooth progress. This might happen for several reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You use the example repo for practice and experimentation.&lt;/item&gt;
      &lt;item&gt;You switch to a different computer or reinstall the OS.&lt;/item&gt;
      &lt;item&gt;You intentionally delete it to clean up your home directory.&lt;/item&gt;
      &lt;item&gt;The tutorial is updated significantly while you're taking a break.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To solve this problem, there is a script which automates the task of resetting your progress to the start of any chapter. To identify the chapter you want to continue with, the script expects a keyword as an argument. Each chapter includes its precise reset command at the beginning, so you can easily copy-paste it.&lt;/p&gt;
    &lt;p&gt;The script is not complicated, you can verify that it's not doing anything malicious. Basically, it's just the list of commands I tell you to run manually. For convenience, it's included in the expandable text box below. You can also download the script here and then execute it locally once you have inspected it.&lt;/p&gt;
    &lt;head class="admonition-title"&gt;
      &lt;p&gt;Source of reset script&lt;/p&gt;
    &lt;/head&gt;
    &lt;p&gt;Source of reset script&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env bash
set -euxo pipefail

if [ "${1:-x}" = "x" ] ; then
    echo "Please provide the chapter keyword as the first argument."
    exit 1
fi
chapter="$1"

function success() {
    set +x
    echo "✅✅✅ Reset script completed successfully! ✅✅✅"
    exit 0
}

# Ensure existing user configuration does not affect script behavior.
export JJ_CONFIG=/dev/null

rm -rf ~/jj-tutorial

if ! command -v jj &amp;gt; /dev/null ; then
    echo "ERROR: Jujutsu doesn't seem to be installed."
    echo "       Please install it and rerun the script."
    exit 1
fi

if [ "$chapter" = initialize ] ; then success ; fi

mkdir -p ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj git init --colocate

jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = log ] ; then success ; fi

if [ "$chapter" = make_changes ] ; then success ; fi

echo "# jj-tutorial" &amp;gt; README.md
jj log -r 'none()' # trigger snapshot

if [ "$chapter" = commit ] ; then success ; fi

jj commit --message "Add readme with project title

It's common practice for software projects to include a file called
README.md in the root directory of their source code repository. As the
file extension indicates, the content is usually written in markdown,
where the title of the document is written on the first line with a
prefixed \`#\` symbol.
"

if [ "$chapter" = remote ] ; then success ; fi

git init --bare ~/jj-tutorial/remote
jj git remote add origin ~/jj-tutorial/remote
jj bookmark create main --revision @-
jj git push --bookmark main --allow-new

if [ "$chapter" = clone ] ; then success ; fi

cd ~
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

if [ "$chapter" = github ] ; then success ; fi

if [ "$chapter" = update_bookmark ] ; then success ; fi

printf "\nThis is a toy repository for learning Jujutsu.\n" &amp;gt;&amp;gt; README.md
jj commit -m "Add project description to readme"

jj bookmark move main --to @-

jj git push

if [ "$chapter" = branch ] ; then success ; fi

echo "print('Hello, world!')" &amp;gt; hello.py

jj commit -m "Add Python script for greeting the world

Printing the text \"Hello, world!\" is a classic exercise in introductory
programming courses. It's easy to complete in basically any language and
makes students feel accomplished and curious for more at the same time."

jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo-bob
cd ~/jj-tutorial/repo-bob
jj config set --repo user.name Bob
jj config set --repo user.email bob@local
jj describe --reset-author --no-edit

echo "# jj-tutorial

The file hello.py contains a script that greets the world.
It can be executed with the command 'python hello.py'.
Programming is fun!" &amp;gt; README.md
jj commit -m "Document hello.py in README.md

The file hello.py doesn't exist yet, because Alice is working on that.
Once our changes are combined, this documentation will be accurate."

jj bookmark move main --to @-
jj git push

cd ~/jj-tutorial/repo
jj bookmark move main --to @-
jj git fetch

if [ "$chapter" = show ] ; then success ; fi

if [ "$chapter" = merge ] ; then success ; fi

jj new main@origin @-

jj commit -m "Merge code and documentation for hello-world"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = ignore ] ; then success ; fi

cd ~/jj-tutorial/repo-bob

tar czf submission_alice_bob.tar.gz README.md

echo "
## Submission

Run the following command to create the submission tarball:

~~~sh
tar czf submission_alice_bob.tar.gz [FILE...]
~~~" &amp;gt;&amp;gt; README.md

echo "*.tar.gz" &amp;gt; .gitignore

jj file untrack submission_alice_bob.tar.gz

jj commit -m "Add submission instructions"

if [ "$chapter" = rebase ] ; then success ; fi

jj bookmark move main --to @-
jj git fetch
jj rebase --destination main@origin
jj git push

if [ "$chapter" = more_bookmark ] ; then success ; fi

cd ~/jj-tutorial/repo

echo "for (i = 0; i &amp;lt; 10; i = i + 1):
    print('Hello, world!')" &amp;gt; hello.py

jj commit -m "WIP: Add for loop (need to fix syntax)"

jj git push --change @-

if [ "$chapter" = navigate ] ; then success ; fi

jj git fetch
jj new main

if [ "$chapter" = undo ] ; then success ; fi

echo "print('Hallo, Welt!')" &amp;gt;&amp;gt; hello.py
echo "print('Bonjour, le monde!')" &amp;gt;&amp;gt; hello.py

jj commit -m "code improvements"

jj undo

jj commit -m "Print German and French greetings as well"

jj undo
jj undo
jj undo

jj redo
jj redo
jj redo

if [ "$chapter" = track ] ; then success ; fi

cd ~ # move out of the directory we're about to delete
rm -rf ~/jj-tutorial/repo
jj git clone --colocate ~/jj-tutorial/remote ~/jj-tutorial/repo
cd ~/jj-tutorial/repo

# roleplay as Alice
jj config set --repo user.name "Alice"
jj config set --repo user.email "alice@local"
jj describe --reset-author --no-edit

echo "print('Hallo, Welt!')" &amp;gt;&amp;gt; hello.py
echo "print('Bonjour, le monde!')" &amp;gt;&amp;gt; hello.py
jj commit -m "Print German and French greetings as well"

jj bookmark move main -t @-
jj git push

jj bookmark track 'glob:push-*@origin'

if [ "$chapter" = conflict ] ; then success ; fi

jj new 'description("WIP: Add for loop")'

echo "for _ in range(10):
    print('Hello, world!')" &amp;gt; hello.py

jj commit -m "Fix loop syntax"

jj new main @-

echo "for _ in range(10):
    print('Hello, world!')
    print('Hallo, Welt!')
    print('Bonjour, le monde!')" &amp;gt; hello.py

jj commit -m "Merge repetition and translation of greeting"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = abandon ] ; then success ; fi

jj commit -m "Experiment: Migrate to shiny new framework"
jj git push --change @-
jj new main
jj commit -m "Experiment: Improve scalability using microservices"
jj git push --change @-
jj new main
jj commit -m "Experiment: Apply SOLID design patterns"
jj git push --change @-
jj new main

jj abandon 'description("Experiment")'

jj git push --deleted

if [ "$chapter" = restore ] ; then success ; fi

rm README.md
jj show &amp;amp;&amp;gt; /dev/null

jj restore README.md

jj restore --from 'description("Fix loop syntax")' hello.py

jj commit -m "Remove translations"
jj bookmark move main --to @-
jj git push

if [ "$chapter" = complete ] ; then success ; fi

set +x
echo "Error: Didn't recognize the chapter keyword: '$chapter'."
exit 1
&lt;/code&gt;
    &lt;head rend="h2"&gt;Stay up to date&lt;/head&gt;
    &lt;p&gt;Both this tutorial and Jujutsu are still evolving. In order to keep your Jujutsu knowledge updated, subscribe to releases of the tutorial's GitHub repo. You will be notified of important changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A new level becomes available.&lt;/item&gt;
      &lt;item&gt;An existing level is changed significantly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I especially intend to keep this tutorial updated as new version of Jujutsu come out with features and changes that are relevant to the tutorial's content. I consider this tutorial up-to-date with the latest version of Jujutsu (&lt;code&gt;0.32&lt;/code&gt;) as of August 2025.
If that's more than a couple months in the past, I probably stopped updating this tutorial.&lt;/p&gt;
    &lt;p&gt;You can subscribe to these updates by visiting the GitHub repo and clicking on "Watch", "Custom" and then selecting "Releases".&lt;/p&gt;
    &lt;head rend="h2"&gt;Help make this tutorial better&lt;/head&gt;
    &lt;p&gt;If you find a typo, you can suggest a fix directly by clicking on the "edit" icon in the top-right corner. If you have general suggestions for improvement, please open an issue. I am also very interested in experience reports, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do you have any frustrations with Jujutsu which the tutorial did not help you overcome?&lt;/item&gt;
      &lt;item&gt;Was there a section that wasn't explained clearly? (If you didn't understand something, it's probably the tutorial's fault, not yours!)&lt;/item&gt;
      &lt;item&gt;Did you complete a level but didn't feel like you had the skills that were promised in the level overview?&lt;/item&gt;
      &lt;item&gt;Is there something missing that's not being taught but should?&lt;/item&gt;
      &lt;item&gt;Do you feel like the content could be structured better?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you for helping me improve this tutorial!&lt;/p&gt;
    &lt;head rend="h2"&gt;What is version control and why should you use it?&lt;/head&gt;
    &lt;p&gt;I will assume you're using version control for software development, but it can be used for other things as well. For example, authoring professionally formatted documents with tools like Typst. The source of this tutorial is stored in version control too!&lt;/p&gt;
    &lt;p&gt;What these scenarios have in common is that a large body of work (mostly in the form of text) is slowly being expanded and improved over time. You don't want to lose any of it and you want to be able to go back to previous states of your work. Often, several people need to work on the project at the same time.&lt;/p&gt;
    &lt;p&gt;A general-purpose backup solution can keep a few copies of your files around. A graphical document editor can allow multiple people to edit the text simultaneously. But sometimes, you need a sharper knife. Jujutsu is the sharpest knife available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Jujutsu instead of Git?&lt;/head&gt;
    &lt;p&gt;Git is by far the most commonly used VCS in the software development industry. So why not use that? Using the most popular thing has undeniable benefits. There is lots of learning material, lots of people can help you with problems, lots of other tools integrate with it etc. Why make life harder on yourself by using a lesser-known alternative?&lt;/p&gt;
    &lt;p&gt;Here's my elevator pitch:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is compatible with Git. You're not actually losing anything by using Jujutsu. You can work with it on any existing project that uses Git for version control without issues. Tools that integrate with Git mostly work just as well with Jujutsu.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is easier to learn than Git. (That is, assuming I did a decent job writing this tutorial.) Git is known for its complicated, unintuitive user interface. Jujutsu gives you all the functionality of Git with a lot less complexity. Experienced users of Git usually don't care about this, because they've paid the price of learning Git already. (I was one of these people once.) But you care!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is more powerful than Git. Despite the fact that it's easier to learn and more intuitive, it actually has loads of awesome capabilities for power users that completely leave Git in the dust. Don't worry, you don't have to use that power right away. But you can be confident that if your VCS-workflow becomes more demanding in the future, Jujutsu will have your back. This is not a watered-down "we have Git at home" for slow learners!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Learning Jujutsu instead of Git as your first VCS does have some downsides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;When talking about version control with peers, they will likely use Git-centric vocabulary. Jujutsu shares a lot of Git's concepts, but there are also differences. Translating between the two in conversation can add some mental overhead. (solution: convince your peers to use Jujutsu 😉)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Jujutsu is relatively new and doesn't cover 100% of the features of Git yet. When you do run into the rare problem where Jujutsu doesn't have an answer, you can always fall back to use Git directly, which works quite seamlessly. Still, having to use two tools instead of one is slightly annoying. I plan to teach such Git features in this tutorial in later levels. The tutorial should be a one-stop-shop for all Jujutsu users.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The command line interface of Jujutsu is not yet stable. That means in future versions of Jujutsu, some commands might work a little differently or be renamed. I personally don't think this should scare you away. Many people including me have used Jujutsu as a daily driver for a long time. Whenever something did change, my reaction was usually: "Great, that was one of the less-than-perfect parts of Jujutsu! Now it's even more intuitive than before!" Consider subscribing to GitHub releases of this tutorial. You will be notified if new versions of Jujutsu change something in a way that's relevant to what you learned in this tutorial.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite some downsides, I think the benefits are well worth it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45083952</guid></item><item><title>Launch HN: VibeFlow (YC S25) – Web app generator with visual, editable workflows</title><link>https://news.ycombinator.com/item?id=45084759</link><description>&lt;doc fingerprint="334ac65b00c5ecf8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN! We’re Alessia and Elia, the founders of VibeFlow (&lt;/p&gt;https://vibeflow.ai&lt;p&gt;). VibeFlow lets semi-technical people (i.e. people with some technical skill but who are not professional programmers) build full-stack web apps from natural language prompts, while making the underlying business logic clear and editable as a visual workflow. Demo video: &lt;/p&gt;https://www.youtube.com/watch?v=-CwWd3-b1JI&lt;p&gt;.&lt;/p&gt;&lt;p&gt;The problem we’re trying to solve: today, people who want to build apps without coding often have to stitch together multiple tools, e.g. using Lovable for the frontend, n8n for workflows, and Supabase for the database. That creates data silos and leaves builders with fragile apps that break in production, don’t scale, and aren’t safe. We saw YouTube tutorials teaching people how to duct-tape these together just to get a functional app running. As engineers building no-code tools, we realized that people wanted the power of AI-generated UIs but also the ability to see and control their backend workflows and data.&lt;/p&gt;&lt;p&gt;Our solution is to generate the whole app at once, and represent it as a visual workflow. Users describe what they want in English (“I need a chat widget with an AI agent”) and VibeFlow generates both the interface and the logic. That logic shows up as a workflow graph they can edit visually or by giving new instructions.&lt;/p&gt;&lt;p&gt;We use Convex (https://www.convex.dev/) as backend. The generation of the backend code is fully deterministic, we map workflow graphs to code templates. This makes deployments predictable and avoids the hallucinated, black-box code you often get from AI.&lt;/p&gt;&lt;p&gt;Workflow representation: the logic is a directed graph where each node can be customized. We currently support CRUD operations and agent components. Any changes to the graph compiles directly back into code, so you always own the underlying logic.&lt;/p&gt;&lt;p&gt;Frontend: generated via AI and directly linked to workflow outputs, so it always stays in sync with the business logic. Changes to the frontend can be made through a chat interface.&lt;/p&gt;&lt;p&gt;Semi-technical builders can create maintainable apps (not opaque “magic”), and technical folks can still inspect the code and architecture. Compared to Bubble/Webflow, the interface is simpler; compared to Zapier, the workflows have an output code; and compared to AI coding assistants, you get an automatic backend plugged in with no black-box.&lt;/p&gt;&lt;p&gt;You can try it here: https://app.vibeflow.ai/. The demo video is https://youtu.be/-CwWd3-b1JI We'd love to hear from the HN community, whether you're a builder who's struggled with stitching tools together, a developer who's seen the pain points in no-code platforms, or someone curious about where AI-powered app generation is heading - we're eager for your thoughts!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45084759</guid></item><item><title>When the sun will literally set on what's left of the British Empire</title><link>https://oikofuge.com/sun-sets-on-british-empire/</link><description>&lt;doc fingerprint="b5391f99f8aa9fdb"&gt;
  &lt;main&gt;
    &lt;p&gt;A while ago I treated you to a dissertation entitled “Does The Sun Set On The British Empire?”, and concluded that it doesn’t. The UK’s widely scattered overseas territories, sparse though they are, mean that the sun is still always shining, somewhere in the world, over British territory.&lt;/p&gt;
    &lt;p&gt;The most important territories in maintaining this late-empire sunlight are the Pitcairn Islands, in the Pacific, and the British Indian Ocean Territory, in the Indian Ocean. To illustrate that, I offered the sunlight chart below, showing how Pitcairn and BIOT catch the sunlight when it’s dark in the UK.&lt;/p&gt;
    &lt;p&gt;In fact, as my map at the head of this post shows, BIOT is pivotal. There, I’ve plotted the distribution of light and darkness, across the globe, at 02:15 Greenwich Mean Time, during the June solstice of 2024.*&lt;/p&gt;
    &lt;p&gt;And here’s the situation at the December solstice:&lt;/p&gt;
    &lt;p&gt;Just after the sun sets in Pitcairn, it’s dark over every British territory except BIOT.&lt;/p&gt;
    &lt;p&gt;I’m revisiting the situation because the UK government has announced plans to hand over sovereignty of the Chagos Archipelago, which houses BIOT, to Mauritius. The announcement was made in October 2024, but the original agreement has now been contested by a new government in Mauritius. And the situation is further complicated by the fact that BIOT houses a large US military base on the island of Diego Garcia, so the new Trump administration also has a say in the process. (Meanwhile, the unfortunate Chagossians, evicted from their homeland in 1968 to make way for the military base, have so far been given no voice in the negotiations.)&lt;/p&gt;
    &lt;p&gt;The current proposal suggests that the military base would be maintained under a long-term lease agreement, in which case British sovereignty would be lost, and BIOT would cease to exist. At that point, the role of easternmost British territory would fall to the Sovereign Base Areas (SBAs), in Cyprus.&lt;/p&gt;
    &lt;p&gt;The SBAs are worth a few paragraphs, both because they’re relatively obscure, and because their existence, as sovereign military territories, perhaps has some slight relevance to how the situation on Diego Garcia might play out, should the Trump administration raise strong objections to the current plan.&lt;/p&gt;
    &lt;p&gt;The SBAs came into existence when Cyprus gained its independence from the UK in 1960. Under the Treaty of Establishment, the UK retained sovereignty over about 250 square kilometres of the island, in two separate areas—the Western Sovereign Base Area of Akrotiri, and the Eastern Sovereign Base Area of Dhekelia. These have extremely complicated boundaries, designed to avoid Cypriot settlements while including British military establishments. The Eastern SBA contains three Cypriot enclaves—the towns of Ormideia and Xylotymbou, and the area surrounding the Dhekelia power station (which is crossed by a British road). It also features a long northward extension along the road to the village of Ayios Nikolaos, which now houses a signals intelligence unit.&lt;/p&gt;
    &lt;p&gt;And the whole border situation became even more complicated after the Turkish invasion of Cyprus in 1974, which has left the island traversed by a UN buffer zone. British territory, including the Ayios Nikolaos road, forms part of the buffer zone. Elsewhere, the Turkish-controlled town of Kokkina has its very own buffer zone. Here’s an overview map, followed by some detail of the SBAs:&lt;/p&gt;
    &lt;p&gt;(Interestingly, the British military settlements within the SBAs are referred to as cantonments, a military term which, to me at least, has something of a colonial ring to it, given its association with British rule in India.)&lt;/p&gt;
    &lt;p&gt;The relevance, here, to the current situation of Diego Garcia, is because the UK government made plans to hand the SBAs back to Cyprus in 1974, but were persuaded to retain sovereignty by the USA, which valued access to signals intelligence in the Eastern Mediterranean, as well as a convenient location from which to fly, among other things, U2 spy planes. The difference, of course, is that the Cypriot government appears to have been compliant with that arrangement, whereas it seems unlikely, at time of writing, that the Mauritians would agree to such a deal.&lt;/p&gt;
    &lt;p&gt;We’ll see how it goes. Meanwhile, I’ve plotted another sunrise/sunset graph, showing how sunlight is handed off between the two key players in the absence of BIOT:&lt;/p&gt;
    &lt;p&gt;(For my sunlight calculation, I’ve plugged in the latitude and longitude of the easternmost part of the Eastern SBA—Ayios Nikolaos.)&lt;/p&gt;
    &lt;p&gt;It’s close—in June there’s less than an hour when it’s dark in both Pitcairn and the SBAs. But, if BIOT goes, when the sun sets on Pitcairn, it will also set on (what’s left of) the British Empire.&lt;/p&gt;
    &lt;p&gt;* I haven’t plotted British Antarctic Territory, because territorial claims in Antarctica are in abeyance under the Antarctic Treaty.&lt;/p&gt;
    &lt;p&gt;or&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45084913</guid></item><item><title>Use One Big Server (2022)</title><link>https://specbranch.com/posts/one-big-server/</link><description>&lt;doc fingerprint="dfdab7597d822fd7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Use One Big Server&lt;/head&gt;
    &lt;p&gt;A lot of ink is spent on the "monoliths vs. microservices" debate, but the real issue behind this debate is about whether distributed system architecture is worth the developer time and cost overheads. By thinking about the real operational considerations of our systems, we can get some insight into whether we actually need distributed systems for most things.&lt;/p&gt;
    &lt;p&gt;We have all gotten so familiar with virtualization and abstractions between our software and the servers that run it. These days, "serverless" computing is all the rage, and even "bare metal" is a class of virtual machine. However, every piece of software runs on a server. Since we now live in a world of virtualization, most of these servers are a lot bigger and a lot cheaper than we actually think.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet Your Server&lt;/head&gt;
    &lt;p&gt;This is a picture of a server used by Microsoft Azure with AMD CPUs. Starting from the left, the big metal fixture on the left (with the copper tubes) is a heatsink, and the metal boxes that the copper tubes are attached to are heat exchangers on each CPU. The CPUs are AMD's third generation server CPU, each of which has the following specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64 cores&lt;/item&gt;
      &lt;item&gt;128 threads&lt;/item&gt;
      &lt;item&gt;~2-2.5 GHz clock&lt;/item&gt;
      &lt;item&gt;Cores capable of 4-6 instructions per clock cycle&lt;/item&gt;
      &lt;item&gt;256 MB of L3 cache&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In total, this server has 128 cores with 256 simultaneous threads. With all of the cores working together, this server is capable of 4 TFLOPs of peak double precision computing performance. This server would sit at the top of the top500 supercomputer list in early 2000. It would take until 2007 for this server to leave the top500 list. Each CPU core is substantially more powerful than a single core from 10 years ago, and boasts a much wider computation pipeline.&lt;/p&gt;
    &lt;p&gt;Above and below each CPU is the memory: 16 slots of DDR4-3200 RAM per socket. The largest capacity "cost effective" DIMMs today are 64 GB. Populated cost-efficiently, this server can hold 1 TB of memory. Populated with specialized high-capacity DIMMs (which are generally slower than the smaller DIMMs), this server supports up to 8 TB of memory total. At DDR4-3200, with a total of 16 memory channels, this server will likely see ~200 Gbps of memory throughput across all of its cores.&lt;/p&gt;
    &lt;p&gt;In terms of I/O, each CPU offers 64 PCIe gen 4 lanes. With 128 PCIe lanes total, this server is capable of supporting 30 NVMe SSDs plus a network card. Typical configurations you can buy will offer slots for around 16 SSDs or disks. The final thing I wanted to point out in this picture is in the top right, the network card. This server is likely equipped with a 50-100 Gbps network connection.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Capabilities of One Server&lt;/head&gt;
    &lt;p&gt;One server today is capable of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Serving video files at 400 Gbps (now 800 Gbps)&lt;/item&gt;
      &lt;item&gt;1 million IOPS on a NoSQL database&lt;/item&gt;
      &lt;item&gt;70k IOPS in PostgreSQL&lt;/item&gt;
      &lt;item&gt;500k requests per second to nginx&lt;/item&gt;
      &lt;item&gt;Compiling the linux kernel in 20 seconds&lt;/item&gt;
      &lt;item&gt;Rendering 4k video with x264 at 75 FPS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Among other things. There are a lot of public benchmarks these days, and if you know how your service behaves, you can probably find a similar benchmark.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Cost of One Server&lt;/head&gt;
    &lt;p&gt;In a large hosting provider, OVHCloud, you can rent an HGR-HCI-6 server with similar specifications to the above, with 128 physical cores (256 threads), 512 GB of memory, and 50 Gbps of bandwidth for $1,318/month.&lt;/p&gt;
    &lt;p&gt;Moving to the popular budget option, Hetzner, you can rent a smaller server with 32 physical cores and 128 GB of RAM for about â¬140.00/month. This is a smaller server than the one from OVHCloud (1/4 the size), but it gives you some idea of the price spread between hosting providers.&lt;/p&gt;
    &lt;p&gt;In AWS, one of the largest servers you can rent is the m6a.metal server. It offers 50 Gbps of network bandwidth, 192 vCPUs (96 physical cores), and 768 GB of memory, and costs $8.2944/hour in the US East region. This comes out to $6,055/month. The cloud premium is real!&lt;/p&gt;
    &lt;p&gt;A similar server, with 128 physical cores and 512 GB of memory (as well as appropriate NICs, SSDs, and support contracts), can be purchased from the Dell website for about $40,000. However, if you are going to spend this much on a server, you should probably chat with a salesperson to make sure you are getting the best deal you can. You will also need to pay to host this server and connect it to a network, though.&lt;/p&gt;
    &lt;p&gt;In comparison, buying servers takes about 8 months to break even compared to using cloud servers, and 30 months to break even compared to renting. Of course, buying servers has a lot of drawbacks, and so does renting, so going forward, we will think a little bit about the "cloud premium" and whether you should be willing to pay it (spoiler alert: the answer is "yes, but not as much as the cloud companies want you to pay").&lt;/p&gt;
    &lt;head rend="h2"&gt;Thinking about the Cloud&lt;/head&gt;
    &lt;p&gt;The "cloud era" began in earnest around 2010. At the time, the state of the art CPU was an 8-core Intel Nehalem CPU. Hyperthreading had just begun, so that 8-core CPU offered a whopping 16 threads. Hardware acceleration was about to arrive for AES encryption, and vectors were 128 bits wide. The largest CPUs had 24 MB of cache, and your server could fit a whopping 256 GB of DDR3-1066 memory. If you wanted to store data, Seagate had just begun to offer a 3 TB hard drive. Each core offered 4 FLOPs per cycle, meaning that your 8-core server running at 2.5 GHz offered a blazing fast 80 GFLOPs.&lt;/p&gt;
    &lt;p&gt;The boom in distributed computing rode on this wave: if you wanted to do anything that involved retrieval of data, you needed a lot of disks to get the storage throughput you want. If you wanted to do large computations, you generally needed a lot of CPUs. This meant that you needed to coordinate between a lot of CPUs to get most things done.&lt;/p&gt;
    &lt;p&gt;Since that time began, the size of servers has increased a lot, and SSDs have increased available IOPS by a factor of at least 100, but the size of mainstream VMs and containers hasn't increased much, and we still use virtualized drives that perform more like hard drives than SSDs (although this gap is closing).&lt;/p&gt;
    &lt;head rend="h4"&gt;One Server (Plus a Backup) is Usually Plenty&lt;/head&gt;
    &lt;p&gt;If you are doing anything short of video streaming, and you have under 10k QPS, one server will generally be fine for most web services. For really simple services, one server could even make it to a million QPS or so. Very few web services get this much traffic - if you have one, you know about it. Even if you're serving video, running only one server for your control plane is very reasonable. A benchmark can help you determine where you are. Alternatively, you can use common benchmarks of similar applications, or tables of common performance numbers to estimate how big of a machine you might need.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tall is Better than Wide&lt;/head&gt;
    &lt;p&gt;When you need a cluster of computers, if one server is not enough, using fewer larger servers will often be better than using a large fleet of small machines. There is non-zero overhead to coordinate a cluster, and that overhead is frequently O(n) on each server. To reduce this overhead, you should generally prefer to use a few large servers than to use many small servers. In the case of things like serverless computing, where you allocate tiny short-lived containers, this overhead accounts for a large fraction of the cost of use. On the other extreme end, coordinating a cluster of one computer is trivial.&lt;/p&gt;
    &lt;head rend="h4"&gt;Big Servers and Availability&lt;/head&gt;
    &lt;p&gt;The big drawback of using a single big server is availability. Your server is going to need downtime, and it is going to break. Running a primary and a backup server is usually enough, keeping them in different datacenters. A 2x2 configuration should appease the truly paranoid: two servers in a primary datacenter (or cloud provider) and two servers in a backup datacenter will give you a lot of redundancy. If you want a third backup deployment, you can often make that smaller than your primary and secondary.&lt;/p&gt;
    &lt;p&gt;However, you may still have to be concerned about correlated hardware failures. Hard drives (and now SSDs) have been known to occasionally have correlated failures: if you see one disk fail, you are a lot more likely to see a second failure before getting back up if your disks are from the same manufacturing batch. Services like Backblaze overcome this by using many different models of disks from multiple manufacturers. Hacker news learned this the hard way recently when the primary and backup server went down at the same time.&lt;/p&gt;
    &lt;p&gt;If you are using a hosting provider which rents pre-built servers, it is prudent to rent two different types of servers in each of your primary and backup datacenters. This should avoid almost every failure mode present in modern systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;Use the Cloud, but don't be too Cloudy&lt;/head&gt;
    &lt;p&gt;A combination of availability and ease of use is one of the big reasons why I (and most other engineers) like cloud computers. Yes, you pay a significant premium to rent the machines, but your cloud provider has so much experience building servers that you don't even see most failures, and for the other failures, you can get back up and running really quickly by renting a new machine in their nearly-limitless pool of compute. It is their job to make sure that you don't experience downtime, and while they don't always do it perfectly, they are pretty good at it.&lt;/p&gt;
    &lt;p&gt;Hosting providers who are willing to rent you a server are a cheaper alternative to cloud providers, but these providers can sometimes have poor quality and some of them don't understand things like network provisioning and correlated hardware failures. Also, moving from one rented server to a larger one is a lot more annoying than resizing a cloud VM. Cloud servers have a price premium for a good reason.&lt;/p&gt;
    &lt;p&gt;However, when you deal with clouds, your salespeople will generally push you towards "cloud-native" architecture. These are things like microservices in auto-scaling VM groups with legions of load balancers between them, and vendor-lock-in-enhancing products like serverless computing and managed high-availability databases. There is a good reason that cloud salespeople are the ones pushing "cloud architecture" - it's better for them!&lt;/p&gt;
    &lt;p&gt;The conventional wisdom is that using cloud architecture is good because it lets you scale up effortlessly. There are good reasons to use cloud-native architecture, but serving lots of people is not one of them: most services can serve millions of people at a time with one server, and will never give you a surprise five-figure bill.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Should I Pay for Peak Load?&lt;/head&gt;
    &lt;p&gt;One common criticism of the "one big server" approach is that you now have to pay for your peak usage instead of paying as you go for what you use. Thus, serverless computing or fleets of microservice VMs more closely align your costs with your profit.&lt;/p&gt;
    &lt;p&gt;Unfortunately, since all of your services run on servers (whether you like it or not), someone in that supply chain is charging you based on their peak load. Part of the "cloud premium" for load balancers, serverless computing, and small VMs is based on how much extra capacity your cloud provider needs to build in order to handle their peak load. You're paying for someone's peak load anyway!&lt;/p&gt;
    &lt;p&gt;This means that if your workload is exceptionally bursty - like a simulation that needs to run once and then turn off forever - you should prefer to reach for "cloudy" solutions, but if your workload is not so bursty, you will often have a cheaper system (and an easier time building it) if you go for few large servers. If your cloud provider's usage is more bursty than yours, you are going to pay that premium for no benefit.&lt;/p&gt;
    &lt;p&gt;This premium applies to VMs, too, not just cloud services. However, if you are running a cloud VM 24/7, you can avoid paying the "peak load premium" by using 1-year contracts or negotiating with a salesperson if you are big enough.&lt;/p&gt;
    &lt;p&gt;Generally, the burstier your workload is, the more cloudy your architecture should be.&lt;/p&gt;
    &lt;head rend="h4"&gt;How Much Does it Cost to be Cloudy?&lt;/head&gt;
    &lt;p&gt;Being cloudy is expensive. Generally, I would anticipate a 5-30x price premium depending on what you buy from a cloud company, and depending on the baseline. Not 5-30%, a factor of between 5 and 30.&lt;/p&gt;
    &lt;p&gt;Here is the pricing of AWS lambda: $0.20 per 1M requests + $0.0000166667 per GB-second of RAM. I am using pricing for an x86 CPU here to keep parity with the m6a.metal instance we saw above. Large ARM servers and serverless ARM compute are both cheaper.&lt;/p&gt;
    &lt;p&gt;Assuming your server costs $8.2944/hour, and is capable of 1k QPS with 768 GB of RAM:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;1k QPS is 60k queries per minute, or 3.6M queries per hour&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Each query here gets 0.768 GB-seconds of RAM (amortized)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Replacing this server would cost about $46/hour using serverless computing&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The price premium for serverless computing over the instance is a factor of 5.5. If you can keep that server over 20% utilization, using the server will be cheaper than using serverless computing. This is before any form of savings plan you can apply to that server - if you can rent those big servers from the spot market or if you compare to the price you can get with a 1-year contract, the price premium is even higher.&lt;/p&gt;
    &lt;p&gt;If you compare to the OVHCloud rental price for the same server, the price premium of buying your compute through AWS lambda is a factor of 25&lt;/p&gt;
    &lt;p&gt;If you are considering renting a server from a low-cost hosting provider or using AWS lambda, you should prefer the hosting provider if you can keep the server operating at 5% capacity!&lt;/p&gt;
    &lt;p&gt;Also, note that the actual QPS number doesn't matter: if the $8.2944/hour server is capable of 100k QPS, the query would use 100x less memory-time, meaning that you would arrive at the same 5.5x (or 25x) premium. Of course, you should scale the size of the server to fit your application.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common Objections to One Big Server&lt;/head&gt;
    &lt;p&gt;If you propose using the one big server approach, you will often get pushback from people who are more comfortable with the cloud, prefer to be fashionable, or have legitimate concerns. Use your judgment when you think about it, but most people vastly underestimate how much "cloud architecture" actually costs compared to the underlying compute. Here are some common objections.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Hire Sysadmins&lt;/head&gt;
    &lt;p&gt;Yes you do. They are just now called "Cloud Ops" and are under a different manager. Also, their ability to read the arcane documentation that comes from cloud companies and keep up with the corresponding torrents of updates and deprecations makes them 5x more expensive than system administrators.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Do Security Updates&lt;/head&gt;
    &lt;p&gt;Yes you do. You may have to do fewer of them, but the ones you don't have to do are the easy ones to automate. You are still going to share in the pain of auditing libraries you use, and making sure that all of your configurations are secure.&lt;/p&gt;
    &lt;head rend="h4"&gt;But if I use Cloud Architecture, I Don't Have to Worry About it Going Down&lt;/head&gt;
    &lt;p&gt;The "high availability" architectures you get from using cloudy constructs and microservices just about make up for the fragility they add due to complexity. At this point, if you use two different cloud regions or two cloud providers, you can generally assume that is good enough to avoid your service going down. However, cloud providers have often had global outages in the past, and there is no reason to assume that cloud datacenters will be down any less often than your individual servers.&lt;/p&gt;
    &lt;p&gt;Remember that we are trying to prevent correlated failures. Cloud datacenters have a lot of parts that can fail in correlated ways. Hosting providers have many fewer of these parts. Similarly, complex cloud services, like managed databases, have more failure modes than simple ones (VMs).&lt;/p&gt;
    &lt;head rend="h4"&gt;But I can Develop More Quickly if I use Cloud Architecture&lt;/head&gt;
    &lt;p&gt;Then do it, and just keep an eye on the bill and think about when it's worth it to switch. This is probably the strongest argument in favor of using cloudy constructs. However, if you don't think about it as you grow, you will likely end up burning a lot of money on your cloudy architecture long past the time to switch to something more boring.&lt;/p&gt;
    &lt;head rend="h4"&gt;My Workload is Really Bursty&lt;/head&gt;
    &lt;p&gt;Cloud away. That is a great reason to use things like serverless computing. One of the big benefits of cloud architecture constructs is that the scale down really well. If your workload goes through long periods of idleness punctuated with large unpredictable bursts of activity, cloud architecture probably works really well for you.&lt;/p&gt;
    &lt;head rend="h4"&gt;What about CDNs?&lt;/head&gt;
    &lt;p&gt;It's impossible to get the benefits of a CDN, both in latency improvements and bandwidth savings, with one big server. This is also true of other systems that need to be distributed, like backups. Thankfully CDNs and backups are competitive markets, and relatively cheap. These are the kind of thing to buy rather than build.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Note On Microservices and Monoliths&lt;/head&gt;
    &lt;p&gt;Thinking about "one big server" naturally lines up with thinking about monolithic architectures. However, you don't need to use a monolith to use one server. You can run many containers on one big server, with one microservice per container. However, microservice architectures in general add a lot of overhead to a system for dubious gain when you are running on one big server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;When you experience growing pains, and get close to the limits of your current servers, today's conventional wisdom is to go for sharding and horizontal scaling, or to use a cloud architecture that gives you horizontal scaling "for free." It is often easier and more efficient to scale vertically instead. Using one big server is comparatively cheap, keeps your overheads at a minimum, and actually has a pretty good availability story if you are careful to prevent correlated hardware failures. It's not glamorous and it won't help your resume, but one big server will serve you well.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45085029</guid></item><item><title>Eternal Struggle</title><link>https://yoavg.github.io/eternal/</link><description>&lt;doc fingerprint="57c9d1e55408cc08"&gt;
  &lt;main&gt;
    &lt;p&gt;change background&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45086020</guid></item><item><title>What to do with C++ modules?</title><link>https://nibblestew.blogspot.com/2025/08/we-need-to-seriously-think-about-what.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45086210</guid></item><item><title>We should have the ability to run any code we want on hardware we own</title><link>https://hugotunius.se/2025/08/31/what-every-argument-about-sideloading-gets-wrong.html</link><description>&lt;doc fingerprint="f58638d75127bfe5"&gt;
  &lt;main&gt;
    &lt;p&gt;Sideloading has been a hot topic for the last decade. Most recently, Google has announced further restrictions on the practice in Android. Many hundreds of comment threads have discussed these changes over the years. One point in particular is always made: âI should be able to run whatever code I want on hardware I ownâ. I agree entirely with this point, but within the context of this discussion itâs moot.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âI should be able to run whatever code I want on hardware I ownâ&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When Google restricts your ability to install certain applications they arenât constraining what you can do with the hardware you own, they are constraining what you can do using the software they provide with said hardware. Itâs through this control of the operating system that Google is exerting control, not at the hardware layer. You often donât have full access to the hardware either and building new operating systems to run on mobile hardware is impossible, or at least much harder than it should be. This is a separate, and I think more fruitful, point to make. Apple is a better case study than Google here. Appleâs success with iOS partially derives from the tight integration of hardware and software. An iPhone without iOS is a very different product to what we understand an iPhone to be. Forcing Apple to change core tenets of iOS by legislative means would undermine what made the iPhone successful.&lt;/p&gt;
    &lt;p&gt;You shouldnât take away from this that I am some stalwart defender of the two behemoths Apple and Google, far from it. However, our critique shouldnât be of the restrictions in place in the operating systems they provide â rather, it should focus on the ability to truly run any code we want on hardware we own. In this context this would mean having the ability and documentation to build or install alternative operating systems on this hardware. It should be possible to run Android on an iPhone and manufacturers should be required by law to provide enough technical support and documentation to make the development of new operating systems possible. If you want to play Playstation games on your PS5 you must suffer Sonyâs restrictions, but if you want to convert your PS5 into an emulator running Linux that should be possible.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087396</guid></item><item><title>A Linux version of the Procmon Sysinternals tool</title><link>https://github.com/microsoft/ProcMon-for-Linux</link><description>&lt;doc fingerprint="a010c81b3ca44182"&gt;
  &lt;main&gt;
    &lt;p&gt;Process Monitor (Procmon) is a Linux reimagining of the classic Procmon tool from the Sysinternals suite of tools for Windows. Procmon provides a convenient and efficient way for Linux developers to trace the syscall activity on the system.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OS: Ubuntu 18.04 lts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;cmake&lt;/code&gt;&amp;gt;= 3.14 (build-time only)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;libsqlite3-dev&lt;/code&gt;&amp;gt;= 3.22 (build-time only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please see installation instructions here.&lt;/p&gt;
    &lt;p&gt;Please see build instructions here.&lt;/p&gt;
    &lt;code&gt;Usage: procmon [OPTIONS]
   OPTIONS
      -h/--help                Prints this help screen
      -p/--pids                Comma separated list of process IDs to monitor
      -e/--events              Comma separated list of system calls to monitor
      -c/--collect [FILEPATH]  Option to start Procmon in a headless mode
      -f/--file FILEPATH       Open a Procmon trace file
      -l/--log FILEPATH        Log debug traces to file&lt;/code&gt;
    &lt;p&gt;The following traces all processes and syscalls on the system:&lt;/p&gt;
    &lt;code&gt;sudo procmon&lt;/code&gt;
    &lt;p&gt;The following traces processes with process id 10 and 20:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 10,20&lt;/code&gt;
    &lt;p&gt;The following traces process 20 only syscalls read, write and open at:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 20 -e read,write,openat&lt;/code&gt;
    &lt;p&gt;The following traces process 35 and opens Procmon in headless mode to output all captured events to file &lt;code&gt;procmon.db&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo procmon -p 35 -c procmon.db&lt;/code&gt;
    &lt;p&gt;The following opens a Procmon &lt;code&gt;tracefile&lt;/code&gt;, &lt;code&gt;procmon.db&lt;/code&gt;, within the Procmon TUI:&lt;/p&gt;
    &lt;code&gt;sudo procmon -f procmon.db&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ask a question on Stack Overflow (tag with ProcmonForLinux)&lt;/item&gt;
      &lt;item&gt;Request a new feature on GitHub&lt;/item&gt;
      &lt;item&gt;Vote for popular feature requests&lt;/item&gt;
      &lt;item&gt;File a bug in GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are interested in fixing issues and contributing directly to the code base, please see the document How to Contribute, which covers the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to build and run from the source&lt;/item&gt;
      &lt;item&gt;The development workflow, including debugging and running tests&lt;/item&gt;
      &lt;item&gt;Coding Guidelines&lt;/item&gt;
      &lt;item&gt;Submitting pull requests&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please see also our Code of Conduct.&lt;/p&gt;
    &lt;p&gt;Copyright (c) Microsoft Corporation. All rights reserved.&lt;/p&gt;
    &lt;p&gt;Licensed under the MIT License.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087748</guid></item><item><title>Lewis and Clark marked their trail with laxatives</title><link>https://offbeatoregon.com/2501d1006d_biliousPills-686.077.html</link><description>&lt;doc fingerprint="b4199d0d24278264"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;ASTORIA, CLATSOP COUNTY; 1800s:&lt;/head&gt;
    &lt;head rend="h1"&gt;Lewis and Clark marked their trail with laxatives&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Audio version is not yet available&lt;/head&gt;
          &lt;head&gt;By Finn J.D. John&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;“Some people have stated that the Lewis and Clark Expedition would have been better off if they had taken a trained physician along to care for the numerous problems that they encountered. I totally disagree,” says physician and historian David Peck. “I think a trained physician would have been overly confident and possibly would have been much more aggressive in their treatment of illnesses, often times to the detriment of the patient.”&lt;/p&gt;
          &lt;p&gt;In lieu of a trained physician, the Corps of Discovery’s leaders got some basic medical training, along with a bag full of the tools of allopathic intervention: lancets for bleeding patients, blister powder for inducing “heat,” opium products for relieving pain and inducing sleep â and purgatives.&lt;/p&gt;
          &lt;p&gt;Those purgatives are the heroes of our story today. They came in the form of beefy pills, about four times the size of a standard aspirin tablet, which Rush called “Dr. Rush’s Bilious Pills.” They contained about 10 grains of calomel and 10 to 15 grains of jalap.&lt;/p&gt;
          &lt;p&gt;Jalap, the powdered root of a Mexican variety of morning glory, is a natural laxative of considerable power.&lt;/p&gt;
          &lt;p&gt;And calomel ... ah, calomel. Calomel was the wonder drug of the age. Its chemical name is mercury chloride. In large doses (and they don’t get much larger than 10 grains, or 20 if a fellow takes two of them, as Dr. Rush recommended!) it functions as a savage purgative, causing lengthy and productive sessions in the outhouse and leaving a patient thoroughly depleted and hopefully in full restoration of his bile balance.&lt;/p&gt;
          &lt;p&gt;Calomel also was the only thing known to be effective against syphilis, which was always an issue with military outfits. Whether picked up from a friendly lady in a waterfront St. Louis “sporting house” before the journey, or from an equally friendly Native lady met along the way, syphilis went with soldiers like ice cold milk with an Oreo cookie.&lt;/p&gt;
          &lt;p&gt;When symptoms broke out, the patient would be dosed with “thunder clappers” and slathered with topical mercury ointments until he started salivating ferociously, which was a symptom of mild mercury poisoning but at the time was considered a sure sign that the body was purging the sickness out of itself.&lt;/p&gt;
          &lt;p&gt;And yes, a few of the men did end up needing treatment for syphilis. But everyone in the party needed a good laxative âon the regularâ (sorry about that). Week after week, hunting parties went out and brought back animals to eat. The explorers lived on almost nothing but meat.&lt;/p&gt;
          &lt;p&gt;And this low-fiber diet had predictable results.&lt;/p&gt;
          &lt;p&gt;It had another result, too, which was less predictable â although highly convenient for later historians. The fact is, mercury chloride is only slightly soluble in human digestion. Plus, the reason it works is, it irritates the tissues of the digestive tract severely, causing the body to expel it just as fast as it possibly can before more damage can be done. So, most of the calomel in any given “bilious pill” gets blown out post-haste in the ensuing “purge.”&lt;/p&gt;
          &lt;p&gt;Then, once out of the body and in the earth, it lasts literally for centuries without breaking down or dissolving away.&lt;/p&gt;
          &lt;p&gt;So as Lewis and Clark and their crew made their way across the continent, and across Oregon, they were unknowingly depositing a trail of heavy-metal laxatives along the way â a trail that historians and scientists have been able to detect and use to document almost their every, uh, movement.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087815</guid></item><item><title>Show HN: Spotilyrics – See synchronized Spotify lyrics inside VS Code</title><link>https://github.com/therepanic/spotilyrics</link><description>&lt;doc fingerprint="2926fc790a54ede9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;📌 Live lyrics sync with your Spotify playback.&lt;/item&gt;
      &lt;item&gt;🎨 Lyrics colors auto-themed from album cover (via &lt;code&gt;colorthief&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;🖥️ Smooth side panel view – code on the left, lyrics on the right.&lt;/item&gt;
      &lt;item&gt;🔑 Simple one-time login using your own Spotify Client ID.&lt;/item&gt;
      &lt;item&gt;🚪 Quick logout command to reset session.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Open VS Code → Extensions → search&lt;/p&gt;&lt;code&gt;spotilyrics&lt;/code&gt;or install from VS Code Marketplace.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the command:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;Show Spotify Lyrics via Spotilyrics
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Spotify Developer Dashboard.&lt;/item&gt;
      &lt;item&gt;Create an app → copy Client ID.&lt;/item&gt;
      &lt;item&gt;Important: set the Redirect URI for your app to: &lt;code&gt;http://127.0.0.1:8000/callback&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Run the &lt;code&gt;Show Spotify Lyrics via Spotilyrics&lt;/code&gt;command.&lt;/item&gt;
      &lt;item&gt;Paste your Client ID in the panel and log in.&lt;/item&gt;
      &lt;item&gt;Enjoy synced lyrics while coding! 🎶&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;ℹ️ Why? – To respect Spotify API rate limits, you need your own ID.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Show Spotify Lyrics via Spotilyrics&lt;/code&gt;(&lt;code&gt;spotilyrics.lyrics&lt;/code&gt;) – open synced lyrics panel.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Logout from Spotilyrics&lt;/code&gt;(&lt;code&gt;spotilyrics.logout&lt;/code&gt;) – clear session and re-auth when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spotify Web API&lt;/item&gt;
      &lt;item&gt;LRClib for lyrics with timing&lt;/item&gt;
      &lt;item&gt;colorthief for cover-based theme&lt;/item&gt;
      &lt;item&gt;TypeScript + VS Code WebView&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed as Unlicensed.&lt;lb/&gt; Feel free to use, hack, and remix it – but no warranties 😉&lt;/p&gt;
    &lt;p&gt; Made with ❤️ by therepanic. Code hard, vibe harder 🎧 &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087905</guid></item><item><title>Nintendo Switch 2 Dock USB-C Compatibility</title><link>https://www.lttlabs.com/blog/2025/08/30/nintendo-switch-2-dock</link><description>&lt;doc fingerprint="b65678044229fc2a"&gt;
  &lt;main&gt;
    &lt;p&gt;If you’ve come here to learn everything there is to know about USB Type-C(USB-C), you’re in the wrong place! This won't cover everything there is to know about USB-C Power Delivery(PD) as there are thousands of pages of documentation, but hopefully this can at least help you determine what you have to Google in order to learn more.&lt;/p&gt;
    &lt;p&gt;This is a companion piece to a recently published LTT video about the Nintendo Switch 2 Dock compatibility and the purpose of this article is to present some data and information that didn't quite make it into the video. I highly recommend watching that video, it will provide some good context on the information here and may even be entertaining. Otherwise, you can leave questions or comments on the Linus Tech Tips Forum, where we’ve made a post for this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;USB-C PD Basics&lt;/head&gt;
    &lt;p&gt;Ignoring the full legal name of "Universal Serial Bus Type C - Power Delivery", it is commonly referred to as "USB-C PD" and specifies the protocol for negotiation and delivery of up to 240 W. It defines the physical requirements of the connectors/cables as well as how the connected devices must communicate in order to negotiate the direction and magnitude of power.(voltage and current limits)&lt;/p&gt;
    &lt;p&gt;While the standard USB power limit is 15 W(3 amps at 5 volts), PD allows for the source(device supplying power) to advertise its power supplying capabilities to the sink(device drawing power) with voltages up to 20 V(or 48 V with Extended Power Range(EPR)) and 5 A. EPR even supports up to 240 W! Plenty of power for any device you'd reasonably power with USB-C.&lt;/p&gt;
    &lt;head rend="h2"&gt;USB-C PD Negotiation&lt;/head&gt;
    &lt;p&gt;While there is a standard for the messages to be sent back and forth between devices, the annoying part is that in reality there can be many exceptions to these rules. There can be poor connections, missed packets, interruptions, or even devices that just behave badly and don't support the entire specification. This means that negotiations vary slightly in the messages, timing, and final result. Sometimes they will even go through the entire negotiation and have power delivery set up just for one of the devices decides to reset the connection and do it all again.&lt;/p&gt;
    &lt;p&gt;For the purpose of illustration, the example below is simplified and shows the minimum steps required in the process. USB-C PD traffic(like many communication standards) often looks like a conversation between devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;Example Negotiation&lt;/head&gt;
    &lt;p&gt;Source:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Hi, I can support these power profiles." -&lt;/p&gt;&lt;code&gt;SOURCE_CAPABILITIES&lt;/code&gt;Message&lt;/quote&gt;
    &lt;p&gt;Sink:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Great! Can I have 20 V and 5 A (100 W)." -&lt;/p&gt;&lt;code&gt;REQUEST&lt;/code&gt;Message&lt;/quote&gt;
    &lt;p&gt;Source:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"Sure!" -&lt;/p&gt;&lt;code&gt;ACCEPT&lt;/code&gt;Message&lt;/quote&gt;
    &lt;p&gt;Source:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;"I’m ready to supply what you requested!" -&lt;/p&gt;&lt;code&gt;PS_RDY&lt;/code&gt;Message&lt;/quote&gt;
    &lt;head rend="h3"&gt;Negotiation Explanation&lt;/head&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;SOURCE_CAPABILITIES&lt;/code&gt; - Source Capabilities&lt;/head&gt;
    &lt;p&gt;This is a message from the source to 'advertise' the power modes that it is capable of supplying.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The capabilities are communicated as a list of options with different fixed voltages, current limits, and supported features. The most interesting of these is the - optional - Programmable Power Supply(PPS) mode allowing the sink device to micromanage the delivered voltage and current to optimize power conversion and delivery.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This can be broadcast numerous times by the source, or the sink can request it with&lt;/p&gt;&lt;code&gt;GET_SOURCE_CAP&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There is an equivalent&lt;/p&gt;&lt;code&gt;GET_SINK_CAP&lt;/code&gt;and&lt;code&gt;SINK_CAPABILITIES&lt;/code&gt;to request and communicate a device's ability to pull power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;REQUEST&lt;/code&gt; - Request&lt;/head&gt;
    &lt;p&gt;The sink will respond with a selection of one of the modes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The sink responds with a number corresponding to the index of the source mode from the list that the source advertised.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The sink can send another&lt;/p&gt;&lt;code&gt;REQUEST&lt;/code&gt;message at any time(with caveats) to request a different source mode. This can happen when the device turns on or whenever it would like to charge faster/slower.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;ACCEPT&lt;/code&gt; - Accept&lt;/head&gt;
    &lt;p&gt;The source will accept the request.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The source must decide if it can comply. It may reject for reasons like being too hot, already delivering power to many other devices, or any variety of things. It is completely up to the source and its internal logic.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If the firmware is implemented properly, then the source can also respond with a&lt;/p&gt;&lt;code&gt;REJECT&lt;/code&gt;message.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;PS_RDY&lt;/code&gt; - Power Supply Ready&lt;/head&gt;
    &lt;p&gt;The source notifies the sink that it is ready for it to begin drawing power at the requested level.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The source must change the voltage and ensure the power supply is in a state to allow the power draw. Then it will send this message.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;&lt;code&gt;GOODCRC&lt;/code&gt; - Acknowledge&lt;/head&gt;
    &lt;p&gt;Bonus fifth message. These are actually sent by the receiving device in response to any received message. It is a common communication feature known as an "acknowledge" to let the sender know that the message has been successfully received.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;If you've received this message, please comment&lt;/p&gt;&lt;code&gt;GOODCRC&lt;/code&gt;on the LTT video.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Vendor Defined Messages(VDM) - &lt;code&gt;VENDOR_DEFINED&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Alongside the standard negotiation of power delivery, there is also negotiation to be had about data direction, format, and many other things. Vendor Defined Messages(VDM) are typically how this is done. These are used for both standard formats like DisplayPort or Thunderbolt, but also for proprietary modes if a manufacturer wanted to use a USB-C cable for whatever language or format they're using.&lt;/p&gt;
    &lt;p&gt;The distinction between 'good' and 'bad' use of VDM is whether or not the designer/user of the VDM publishes/registers the protocol and makes it available/compatible with other manufacturers or users. The Alternate Mode(Alt Mode) for DisplayPort is negotiated over VDM and is a great boon to many users, but Nintendo appears to be quite discourteous with their use of VDM.&lt;/p&gt;
    &lt;head rend="h2"&gt;USB-C PD Tests&lt;/head&gt;
    &lt;p&gt;Enough of that, let's get to the data dump. Below are eight of the tests that I conducted with a variety of devices and connections. The purpose of this isn't to be an interesting read, but potentially someone will happen upon this article and find the specific information they need. If you would like to view the projects yourself in the Infineon EZ-PD Protocol Analyzer software, or as .csv files, then they are available here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Devices&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Nintendo Switch 2 Dock - Packaged with Nintendo Switch 2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Nintendo Switch 2 AC Adapter - Packaged with Nintendo Switch 2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Infineon CY4500(we now have the Infineon CY4500-EPR as well)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ANTANK S3 MAX TV Dock Station(also just found this dock which seems popular)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;UGREEN and unlabeled USB-C Extensions&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Tests&lt;/head&gt;
    &lt;p&gt;In all of these plots, the main orange/pink trace shows the power measured by the CY4500 while the vertical white lines on the plot represent times when USB-C PD messages were sent. I’ve also included some quick notes for each test about what could be read from the messages.&lt;/p&gt;
    &lt;p&gt;For example, in Test 1 we see that the Nintendo Switch AC Adapter only sends messages at the point of connection(to negotiate 20 V 3 A), and then never again. In Test 2, we see that the Switch 2 and its dock communicate a lot more. First to negotiate power, and then to negotiate display mode when the on state of the Switch 2 changes.&lt;/p&gt;
    &lt;p&gt;The tests vary in duration and I performed different actions with the Switch 2 in all cases, so they are not directly comparable. The sharp changes in measured wattage above/below 10 W are typically where the Switch 2 was turned on or off.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 1: Nintendo Switch 2 AC Adapter &amp;gt; CY4500 &amp;gt; Nintendo Switch 2 Dock &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; Nintendo Dock will draw around 15 W maximum and allow display out.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 2: Nintendo Switch 2 AC Adapter &amp;gt; Nintendo Switch 2 Dock &amp;gt; UGREEN USB-C Extension &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; Dock will deliver about 15 W maximum and allow display out.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 3: Nintendo Switch 2 AC Adapter &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; Switch 2 will still only charge at about 15 W from the Nintendo AC Adapter.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 4: UGREEN 100W AC Adapter &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; Switch 2 will still only charge at about 15 W from 3rd party AC adapter.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 5: 4k 32" Proart Monitor &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; There are many attempts at a `DR_SWAP`, always rejected. There is never anything displayed to the monitor.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 6: 100W UGREEN AC Adapter &amp;gt; CY4500 &amp;gt; S3 Max Dock &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; S3 Dock is more courteous and transparent about power draw from the AC adapter.&lt;/p&gt;
    &lt;p&gt;-&amp;gt; S3 Dock charges at the same speed as the Nintendo dock.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 7: 100W UGREEN AC Adapter &amp;gt; S3 Max Dock &amp;gt; UGREEN extension &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; Switch 2 charges, but no display output from the dock. Maybe an issue with the extension/connections.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 8: 100W UGREEN AC Adapter &amp;gt; S3 Max Dock &amp;gt; Other extension &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;p&gt;-&amp;gt; Switch 2 charges, and provides display out from the dock.&lt;/p&gt;
    &lt;p&gt;-&amp;gt; Averages 15 W delivered to the Switch at maximum.&lt;/p&gt;
    &lt;head rend="h2"&gt;Charge Duration Tests&lt;/head&gt;
    &lt;p&gt;We also conducted a couple tests using our Quarch PAM, measuring the power characteristics at the wall plug. We tested charging the Switch 2 using the Nintendo dock, as well as a 3rd party UGREEN 100W AC adapter. The charge duration graphs are shown below, but the end result is that they charged at roughly the same rate.&lt;/p&gt;
    &lt;p&gt;Graph Note: I have used fixed x and y axis limits for easier comparison.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test 11: Quarch PAM &amp;gt; Nintendo Switch 2 Brick &amp;gt; Nintendo Switch 2 Dock &amp;gt; Switch 2&lt;/head&gt;
    &lt;head rend="h4"&gt;Test 12: Quarch PAM &amp;gt; UGREEN 100W Brick &amp;gt; CY4500 &amp;gt; Switch 2&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusions? Fun Facts?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The Nintendo Switch 2 only ever charges at a maximum of 15 W(as far as I can tell). Independent of charging method: Nintendo dock, 3rd party dock, Nintendo Switch 2 power supply, 3rd party power supply, or USB-C monitor.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Nintendo Switch 2 dock will request 3 A at 20 V from the point of connection, whether it is planning to use that power or not.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;This is in contrast to the Antank dock which will request additional power only when the Switch 2 is turned on and requires additional power. Arguably a better 'citizen' of the USB-C PD specification, potentially bringing your electricity bill down by a few cents&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;When powered by the Nintendo Switch 2 dock or USB-C PD power adapter, the Switch 2 takes roughly two hours to charge to 90%, and roughly three hours to charge to 100%.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;This is actually a very interesting showcase of how measuring the total time to 100% charge isn’t always fully representative of the charging characteristics of the device.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you would like to maximize the time off of the power adapter/dock, charge to roughly 75% battery(just over 1.5 hours) and then play until it dies, repeating the cycle. This will mean that it is almost always charging near 15 W and never 'trickle charging'.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The inability for most docks to support the Switch 2 may not be malicious from Nintendo. It might just be a poor or lazy implementation of the USB-C specification. Our monitoring of the interactions with the USB-C monitor shows that the negotiation does not even get to the point of the Vendor Defined Messages(VDM) where the dock would theoretically have to send the correct responses.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;RELATED ARTICLES&lt;/head&gt;
    &lt;p&gt;Learn how we are currently conducting our charge testing on mobile devices and what we're trying to add to it.&lt;/p&gt;
    &lt;p&gt;Lucas N.&lt;/p&gt;
    &lt;p&gt;Curious if showing seconds in the Windows system tray uses more power? So are we.&lt;/p&gt;
    &lt;p&gt;Woolly Door&lt;/p&gt;
    &lt;p&gt;Headphone testing isn’t one-size-fits-all, so we added more setups, real-ear data, and clearer visuals to better show how things might actually sound.&lt;/p&gt;
    &lt;p&gt;DMS, Woolly Door&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45087971</guid></item><item><title>What Is Complexity in Chess?</title><link>https://lichess.org/@/Toadofsky/blog/what-is-complexity/pKo1swFh</link><description>&lt;doc fingerprint="3f5839fd03a7eda7"&gt;
  &lt;main&gt;&lt;p&gt;Pacto Visual&lt;/p&gt;&lt;head rend="h1"&gt;What is Complexity?&lt;/head&gt;If we all knew, we would all be masters.&lt;p&gt;May 2020 an interesting proposal was suggested.&lt;/p&gt;&lt;p&gt;I provided some constructive criticism on research paper A Metric of Chess Complexity by FM David Peng, as well as constructive criticism on the codebase used to validate this experiment. For many months I have refrained from further comment, and although code has not progressed, two things have:&lt;lb/&gt;1. Public interest in "complexity" as determined by ACPL (yuck).&lt;lb/&gt;2. Lichess has a blogging platform where I can properly address deficiencies in the research method and control the conversation which I start.&lt;/p&gt;&lt;p&gt;... so the time has come for me to share my remaining criticisms of this ambitious project. Previously I had shared some easier-to-address criticisms while privately I shared Peng's suggestion with the Lichess team.&lt;/p&gt;&lt;head rend="h4"&gt;The Golden Goose&lt;/head&gt;&lt;p&gt;"Such a feature has the potential to revolutionize chess and would be invaluable to any chess website. Some specific applications include generating non-tactical puzzles (imagine tactics trainer for positional chess puzzles), creating chess computers that play with human personalities, and identifying concepts that are key to improvement at any rating level."&lt;/p&gt;&lt;p&gt;Science is a window for us to learn more about the world around us. Marketing is about selling ideas to an audience. This statement, if true, would have already garnered interest by both scientists and business people, who by exerting a modicum of effort could easily develop and sell products based upon them. Further, if true, this could also inspire a black market of cheating software to help players identify the risk associated with cheating in particular positions. Peng's paper makes many similar promises to the above, so this raises the level of scrutiny I take to the rest of the paper.&lt;/p&gt;&lt;head rend="h4"&gt;Propositions&lt;/head&gt;&lt;p&gt;This paper specifies complexity in two different propositions:&lt;lb/&gt;a) Complexity is a 1-dimensional, transferable (teachable to a neural network) metric based upon centipawn loss as determined by some version(s) of Stockfish with or without a neural network.&lt;lb/&gt;b) By definition, complexity can be used in real time to determine how difficult a position is.&lt;lb/&gt;While some people's intuitions may support the notion that these propositions support or complement each other, I am unconvinced; regardless, it takes more than intuition to create useful tools.&lt;/p&gt;&lt;head rend="h4"&gt;Logic&lt;/head&gt;&lt;p&gt;Even if the above axioms were true, how many of these conclusions are logically valid?&lt;lb/&gt;1. Non-tactical puzzles could be generated by identifying challenging positions (as opposed to the current method which is based upon positions where the solution is superior in evaluation to other variations).&lt;lb/&gt;2. The current model for puzzle ratings (based upon "elo") takes many attempts to establish an initial puzzle rating.&lt;lb/&gt;3. Holistic opening preparation can be automated by software, making players understand openings rather than memorize them.&lt;lb/&gt;4. Interesting positions for books are the same as difficult positions, which are the same as complex positions.&lt;lb/&gt;5. By identifying positions which are difficult for low-rated players and easy for high-rated players, one could develop training materials to help players understand common key concepts.&lt;lb/&gt;6. By identifying positions which are difficult for low-rated players and easy for high-rated players, one could develop a diagnostic chess exam which identifies a player's rating and identifies key concepts for improvement.&lt;lb/&gt;7. Large databases contain additional tagged information, such as time control, which would produce significant insight into which positions can be played intuitively. Large databases also indicate player ratings and therefore somehow complexity can be used to identify unique strategies useful for playing at a rating advantage or disadvantage.&lt;lb/&gt;8. Chess players have human personalities.&lt;lb/&gt;9. Opening systems can be devised around an opponent's tendency to seek or to avoid complexity.&lt;lb/&gt;10. Chess players are likely to make errors in difficult positions, unlike engines, and therefore a complexity metric would be an invaluable tool.&lt;lb/&gt;11. Spectating (and honestly, post-game analysis) of top chess games could be enriched by displaying complexity information related to each position, informing spectators who otherwise look at engine evaluations and variations &amp;amp; jump to conclusions.&lt;lb/&gt;12. Complexity varies by variant; for example blitz and correspondence have different complexities for identical positions.&lt;/p&gt;&lt;p&gt;In my opinion, conclusion#11 is valid and others require further research. Anyway, on to Peng's research...&lt;/p&gt;&lt;head rend="h4"&gt;Neural Networks&lt;/head&gt;&lt;p&gt;This paper nearly predates efforts by DeepMind, the Leela Chess Zero team, and the Stockfish team which resulted in development of Stockfish-NNUE. We could not have anticipated such rapid developments! Many chess players had opinions that AlphaZero and Leela played much more human-like moves than traditional engines, in much the same manner that decades prior world champion Kasparov was astounded that Deep Blue played human-like sacrifices. Whatever conclusions are drawn may need to be updated since both Stockfish evaluations without NNUE, and Stockfish-NNUE evaluations, have rapidly changed (complementing Stockfish search changes and search parameter changes).&lt;/p&gt;&lt;head rend="h4"&gt;Endgame Scaling&lt;/head&gt;&lt;p&gt;Stockfish evaluations in the middlegame are capped at 10 and in the endgame are capped at 100. As such, it seems unreasonable to deviate from prior research indicating the need for a sigmoid to normalize evaluations before classifying example input moves as blunders.&lt;/p&gt;&lt;head rend="h4"&gt;Board Representation&lt;/head&gt;&lt;p&gt;Doing original research allows liberties in methods and models, although considerations offered here differ from those announced and discussed in public interviews by DeepMind's CEO. While I don't fully agree with DeepMind's emphasis on asymmetry and castling rights, I do question the need for an extra bit for White/Black to move. For example, the positions after 1. c3 e5 2. c4 (Black to move) and after 1. e4 c5 (White to move) should have the same relative evaluation.&lt;/p&gt;&lt;head rend="h4"&gt;Evaluation Skewness&lt;/head&gt;&lt;p&gt;There is ample prior research about ranking moves. In fact, Peng's research here is predicated on a notion that traditional engines sometimes indicate to spectators that two moves are equally good, despite one resulting in a more difficult position than the other. We cannot be fully certain that players in fact played the best moves, as this is the very concept we are trying to figure out how to measure! Regardless, we have to start somewhere, and this seems like a good first attempt.&lt;/p&gt;&lt;head rend="h4"&gt;Summary&lt;/head&gt;&lt;p&gt;I could further nitpick... I criticize because I am impressed and I care about this subject. I am further impressed that results were split by "elo," leading to a discovery that some positions are difficult for all players, whereas some positions are more difficult for lower-rated players than for higher-rated players.&lt;/p&gt;&lt;p&gt;Other possible improvements could involve:&lt;lb/&gt;* Obtain segmented Stockfish evaluations (material, pawn structure, etc.) and WDL statistics&lt;lb/&gt;* Obtain Stockfish-NNUE evaluations and WDL predictions&lt;lb/&gt;* Model checks in sample input&lt;lb/&gt;* Model log(time remaining) in sample input&lt;lb/&gt;* Maybe bootstrap models based upon known pawn concepts&lt;lb/&gt;* Maybe include some human versus engine games. Some bots such as Boris-Trapsky and TurtleBot have personalities!&lt;/p&gt;&lt;p&gt;Thanks for the suggestion and someday I hope to see Lichess.org or some other site implement a complexity metric before cheaters do.&lt;/p&gt;&lt;p&gt;Photo credit: Pacto Visual&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45089256</guid></item><item><title>Preserving Order in Concurrent Go Apps: Three Approaches Compared</title><link>https://destel.dev/blog/preserving-order-in-concurrent-go</link><description>&lt;doc fingerprint="763a97941ef85e95"&gt;
  &lt;main&gt;
    &lt;p&gt;Concurrency is one of Go’s greatest strengths, but it comes with a fundamental trade-off: when multiple goroutines process data simultaneously, the natural ordering gets scrambled. Most of the time, this is fine – unordered processing is enough, it’s faster and simpler.&lt;/p&gt;
    &lt;p&gt;But sometimes, order matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;When Order Matters&lt;/head&gt;
    &lt;p&gt;Here are three real-world scenarios where preserving order becomes critical:&lt;/p&gt;
    &lt;p&gt;Real-time Log Enrichment: You’re processing a high-volume log stream, enriching each entry with user metadata from a database or external API. Sequential processing can’t keep up with the incoming rate, but concurrent processing breaks the sequence, making the enriched logs unusable for downstream consumers that depend on chronological order.&lt;/p&gt;
    &lt;p&gt;Finding the First Match in a File List: You need to download a list of files from cloud storage and find the first one containing a specific string. Concurrent downloads are much faster, but they complete out of order – the 50th file might finish before the 5th file, so you can’t simply return the first match you find without knowing if an earlier file also contains the string.&lt;/p&gt;
    &lt;p&gt;Time Series Data Processing: This scenario inspired my original implementation. I needed to download 90 days of transaction logs (~600MB each), extract some data, then compare consecutive days for trend analysis. Sequential downloads took hours; concurrent downloads could give an order of magnitude speedup, but would destroy the temporal relationships I needed for comparison.&lt;/p&gt;
    &lt;p&gt;The challenge is clear: we need the speed benefits of concurrent processing without sacrificing the predictability of ordered results. This isn’t just a theoretical problem – it’s a practical constraint that affects real systems at scale.&lt;/p&gt;
    &lt;p&gt;In this article, we’ll explore three approaches I’ve developed and used in production Go applications. We’ll build a concurrent &lt;code&gt;OrderedMap&lt;/code&gt; function that transforms a channel of inputs into a channel of outputs while preserving order. Through benchmarks of each approach, we’ll understand their trade-offs and discover surprising performance insights along the way.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Why Concurrency Breaks Order&lt;/head&gt;
    &lt;p&gt;Let’s quickly recall why concurrency messes up ordering. One of the reasons is that goroutines process tasks at different speeds. Another common reason – we can’t predict how exactly goroutines will be scheduled by the Go runtime.&lt;/p&gt;
    &lt;p&gt;For example, goroutine #2 might finish processing item #50 before goroutine #1 finishes item #10, causing results to arrive out of order. This is the natural behavior of concurrent processing.&lt;/p&gt;
    &lt;p&gt;If you want to see this in action, here’s a quick demo the Go playground.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design Philosophy: Backpressure vs Buffering&lt;/head&gt;
    &lt;p&gt;The classic approach to ordered concurrency uses some sort of reorder buffer or queue. When a worker calculates a result but it’s too early to write it to the output, the result gets stored in that buffer until it can be written in the correct order.&lt;/p&gt;
    &lt;p&gt;In such designs buffers can typically grow without bound. This happens when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The input is skewed – early items take longer to process than later items&lt;/item&gt;
      &lt;item&gt;Downstream consumers are slow&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The algorithms presented below are backpressure-first. If a worker can’t yet write its result to the output channel, it blocks. This design is memory-bound and preserves the behavior developers expect from Go channels.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Technically speaking, such algorithms also do buffering, but here out-of-order items are held on the stacks of running goroutines. So, to get a larger “buffer” in these algorithms, you can simply increase the concurrency level. This works well in practice since typically when applications need larger buffers they also need higher concurrency levels.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Establishing a Performance Baseline&lt;/head&gt;
    &lt;p&gt;To understand the true cost of ordering, we first need a baseline to measure against. Let’s implement and benchmark a basic concurrent &lt;code&gt;Map&lt;/code&gt; function that doesn’t preserve order – this will show us exactly what overhead the ordering approaches add.&lt;/p&gt;
    &lt;p&gt;Our &lt;code&gt;Map&lt;/code&gt; function transforms an input channel into an output channel using a user-supplied function &lt;code&gt;f&lt;/code&gt;. It’s built on top of a simple worker pool, which spawns multiple goroutines to process input items concurrently.&lt;/p&gt;
    &lt;code&gt;// Map transforms items from the input channel using n goroutines, and the
// provided function f. Returns a new channel with transformed items.
func Map[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	out := make(chan B)
	Loop(in, n, out, func(a A) {
		out &amp;lt;- f(a)
	})
	return out
}

// Loop is a worker pool implementation. It calls function f for each 
// item from the input channel using n goroutines. This is a non-blocking function 
// that signals completion by closing the done channel when all work is finished.
func Loop[A, B any](in &amp;lt;-chan A, n int, done chan&amp;lt;- B, f func(A)) {
	var wg sync.WaitGroup

	for i := 0; i &amp;lt; n; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for a := range in {
				f(a)
			}
		}()
	}

	go func() {
		wg.Wait()
		if done != nil {
			close(done)
		}
	}()
}

// Discard is a non-blocking function that consumes and discards
// all items from the input channel
func Discard[A any](in &amp;lt;-chan A) {
	go func() {
		for range in {
			// Discard the value
		}
	}()
}

func BenchmarkMap(b *testing.B) {
	for _, n := range []int{1, 2, 4, 8, 12, 50} {
		b.Run(fmt.Sprint("n=", n), func(b *testing.B) {
			in := make(chan int)
			defer close(in)
			out := Map(in, n, func(a int) int {
				//time.Sleep(50 * time.Microsecond)
				return a // no-op: just return the original value
			})
			Discard(out)

			b.ReportAllocs()
			b.ResetTimer()

			for i := 0; i &amp;lt; b.N; i++ {
				in &amp;lt;- 10 // write something to the in chan
			}
		})
	}
}&lt;/code&gt;
    &lt;p&gt;As you can see, &lt;code&gt;Map&lt;/code&gt; uses &lt;code&gt;Loop&lt;/code&gt; to create a worker pool that processes items concurrently, while &lt;code&gt;Loop&lt;/code&gt; itself handles the low-level goroutine management and synchronization. This separation of concerns will become important later when we build our ordered variants.&lt;/p&gt;
    &lt;p&gt;What exactly are we measuring here? We’re measuring throughput – how fast we can push items through the entire pipeline. Since the &lt;code&gt;Map&lt;/code&gt; function creates backpressure (blocking when the pipeline is full), the rate at which we can feed items into the input channel acts as an accurate proxy for overall processing speed.
Let’s run the benchmark (I used Apple M2 Max laptop to run it):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;408.6ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;445.1ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;546.4ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;600.2ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1053ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;You might wonder: “Shouldn’t higher concurrency increase throughput?” In real applications, absolutely – but only when there’s actual work to parallelize. Here I used a trivial no-op transformation to isolate and benchmark the pure overhead of goroutines, channels, and coordination. As expected, this overhead grows with the number of goroutines.&lt;/p&gt;
    &lt;p&gt;We’ll use this overhead-focused benchmark for comparisons later in the article, but to demonstrate that concurrency improves performance, let’s run one more benchmark with some work simulated (50μs sleep):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;61656ns&lt;/cell&gt;
        &lt;cell&gt;1.0x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;30429ns&lt;/cell&gt;
        &lt;cell&gt;2.0x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;15207ns&lt;/cell&gt;
        &lt;cell&gt;4.1x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;7524ns&lt;/cell&gt;
        &lt;cell&gt;8.2x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;5034ns&lt;/cell&gt;
        &lt;cell&gt;12.2x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1277ns&lt;/cell&gt;
        &lt;cell&gt;48.3x&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect! Here we see the dramatic benefits of concurrency when there’s real work to be done. With 50μs of work per item, increasing concurrency from 1 to 50 goroutines improves performance by nearly 50x. This demonstrates why concurrent processing is so valuable in real applications.&lt;/p&gt;
    &lt;p&gt;We’re now ready to compare the 3 approaches and measure exactly what price we pay for adding order preservation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 1: ReplyTo Channels&lt;/head&gt;
    &lt;p&gt;This is probably the most Go-native way to implement ordered concurrency. The ReplyTo pattern is well-known in Go (I also used it in my batching article), but somehow this was the hardest approach for me to explain clearly.&lt;/p&gt;
    &lt;p&gt;Here’s how it works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A packer goroutine creates jobs by attaching a unique &lt;code&gt;replyTo&lt;/code&gt;channel to every input item.&lt;/item&gt;
      &lt;item&gt;Workers process jobs concurrently, and send results through those &lt;code&gt;replyTo&lt;/code&gt;channels.&lt;/item&gt;
      &lt;item&gt;An unpacker goroutine unpacks the values sent via &lt;code&gt;replyTo&lt;/code&gt;channels and writes them to the output.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following diagram illustrates how this pattern in more detail:&lt;/p&gt;
    &lt;p&gt;The left part of this diagram is sequential (packer and unpacker) while the worker pool on the right operates concurrently. Notice that workers can only send results when the unpacker is ready to receive them, because the &lt;code&gt;replyTo&lt;/code&gt; channels are unbuffered. This creates natural backpressure and prevents unnecessary buffering.&lt;/p&gt;
    &lt;code&gt;func OrderedMap1[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job struct {
		Item    A
		ReplyTo chan B
	}

	// Packer goroutine.
	// `jobs` chan will be processed by the pool
	// `replies` chan will be consumed by unpacker goroutine
	jobs := make(chan Job)
	replies := make(chan chan B, n)
	go func() {
		for item := range in {
			replyTo := make(chan B)
			jobs &amp;lt;- Job{Item: item, ReplyTo: replyTo}
			replies &amp;lt;- replyTo
		}
		close(jobs)
		close(replies)
	}()

	// Worker pool of n goroutines.
	// Sends results back via replyTo channels
	Loop[Job, any](jobs, n, nil, func(job Job) {
		job.ReplyTo &amp;lt;- f(job.Item) // Calculate the result and send it back
		close(job.ReplyTo)
	})

	// Unpacker goroutine.
	// Unpacks replyTo channels in order and sends results to the `out` channel
	out := make(chan B)
	go func() {
		defer close(out)
		for replyTo := range replies {
			result := &amp;lt;-replyTo
			out &amp;lt;- result
		}
	}()
	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;818.7ns&lt;/cell&gt;
        &lt;cell&gt;+410ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;808.9ns&lt;/cell&gt;
        &lt;cell&gt;+364ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;826.8ns&lt;/cell&gt;
        &lt;cell&gt;+280ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;825.6ns&lt;/cell&gt;
        &lt;cell&gt;+225ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;772.3ns&lt;/cell&gt;
        &lt;cell&gt;-281ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This approach introduces up to 410ns of overhead per input item compared to our baseline. Part of this cost comes from allocating a new &lt;code&gt;replyTo&lt;/code&gt; channel for every item. Unfortunately, we can’t use a package level &lt;code&gt;sync.Pool&lt;/code&gt; to mitigate this because our function is generic – channels for different types can’t share the same pool.&lt;/p&gt;
    &lt;p&gt;What’s also interesting about this result is that the overhead brought by ordering becomes smaller as the number of goroutines grows. At some point even an inversion happens – &lt;code&gt;OrderedMap1&lt;/code&gt; becomes faster than &lt;code&gt;Map&lt;/code&gt; (-281ns at 50 goroutines).&lt;/p&gt;
    &lt;p&gt;I haven’t investigated this phenomenon deeply. I believe it can’t be caused by inefficiencies inside &lt;code&gt;Map&lt;/code&gt; since it’s already based on the simplest possible channel-based worker pool. One guess that I have is that in &lt;code&gt;Map&lt;/code&gt; we have 50 goroutines competing to write into a single output channel. On the contrary, in &lt;code&gt;OrderedMap&lt;/code&gt;, despite additional moving parts, only one goroutine is writing to the output.&lt;/p&gt;
    &lt;p&gt;Let’s now move on to the next approach.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 2: sync.Cond for Turn-Taking&lt;/head&gt;
    &lt;p&gt;This was the first algorithm I implemented when I needed ordered concurrency, and it’s much easier to explain than the ReplyTo approach.&lt;/p&gt;
    &lt;p&gt;Here we attach an incremental index to each item and send it to the worker pool. Each worker performs the calculation, then waits its turn to write the result to the output channel.&lt;/p&gt;
    &lt;p&gt;This conditional waiting is implemented using a shared &lt;code&gt;currentIndex&lt;/code&gt; variable protected by &lt;code&gt;sync.Cond&lt;/code&gt;, a powerful but underused concurrency primitive from the standard library that allows goroutines to wait for specific conditions and be woken up when those conditions change.&lt;/p&gt;
    &lt;p&gt;Here’s how the turn-taking mechanism works:&lt;/p&gt;
    &lt;p&gt;Here, after each write, all workers wake up (using broadcast) and recheck “is it my turn?” condition&lt;/p&gt;
    &lt;code&gt;func OrderedMap2[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job struct {
		Item  A
		Index int
	}

	// Indexer goroutine.
	// Assign an index to each item from the input channel
	jobs := make(chan Job)
	go func() {
		i := 0
		for item := range in {
			jobs &amp;lt;- Job{Item: item, Index: i}
			i++
		}
		close(jobs)
	}()

	// Shared state.
	// Index of the next result that must be written to the output channel.
	nextIndex := 0
	cond := sync.NewCond(new(sync.Mutex))

	// Worker pool of n goroutines.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job) {
		result := f(job.Item) // Calculate the result

		// Cond must be used with a locked mutex (see stdlib docs)
		cond.L.Lock()

		// wait until it's our turn to write the result
		for job.Index != nextIndex {
			cond.Wait()
		}

		// Write the result
		out &amp;lt;- result

		// Increment the index and notify all other workers
		nextIndex++
		cond.Broadcast()

		cond.L.Unlock()
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;867.7ns&lt;/cell&gt;
        &lt;cell&gt;+459ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;1094ns&lt;/cell&gt;
        &lt;cell&gt;+649ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;1801ns&lt;/cell&gt;
        &lt;cell&gt;+1255ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;2987ns&lt;/cell&gt;
        &lt;cell&gt;+2387ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;16074ns&lt;/cell&gt;
        &lt;cell&gt;+15021ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The results are telling – no more per-item allocations, which is excellent for memory efficiency. But there’s a critical flaw: significant performance degradation as goroutine count increases. This happens because of the shared state and the “thundering herd” problem: after each write, all goroutines wake up via &lt;code&gt;cond.Broadcast()&lt;/code&gt;, but only one will do useful work.&lt;/p&gt;
    &lt;p&gt;This inefficiency led me to think: “How can I wake only the goroutine that should write next?” And this is how the 3rd approach was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 3: Permission Passing Chain&lt;/head&gt;
    &lt;p&gt;Here’s the key insight: when is it safe to write output #5? After output #4 was written. Who knows when output #4 was written? The goroutine that wrote it.&lt;/p&gt;
    &lt;p&gt;In this algorithm, any job must hold the write permission before its worker can send results to the output channel. We chain jobs together so each one knows exactly which job comes next and can pass the permission to it. This is done by attaching two channels to each job: &lt;code&gt;canWrite&lt;/code&gt; channel to receive the permission, and &lt;code&gt;nextCanWrite&lt;/code&gt; channel to pass the permission to the next job.&lt;/p&gt;
    &lt;p&gt;This chain structure makes the worker logic remarkably simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calculate: Process the job using the provided function&lt;/item&gt;
      &lt;item&gt;Wait: Receive the permission from &lt;code&gt;canWrite&lt;/code&gt;channel&lt;/item&gt;
      &lt;item&gt;Write: Send the result to the output channel&lt;/item&gt;
      &lt;item&gt;Pass: Send the permission to the next job via &lt;code&gt;nextCanWrite&lt;/code&gt;channel&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the diagram that illustrates the whole flow:&lt;/p&gt;
    &lt;p&gt;The green arrows show how the permission to write is passed from one job to another along the chain. Essentially this is a token-passing algorithm that eliminates the “thundering herd” problem entirely – each goroutine wakes exactly one other goroutine, creating efficient point-to-point signaling rather than expensive broadcasts.&lt;/p&gt;
    &lt;p&gt;Let’s see how this translates to code. The implementation has two parts: a “linker” goroutine that builds the chain, and workers that follow the calculate-wait-write-pass pattern:&lt;/p&gt;
    &lt;code&gt;func OrderedMap3[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = make(chan struct{}, 1)
		close(nextCanWrite) // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, make(chan struct{}, 1)
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		&amp;lt;-job.CanWrite          // Wait for the write permission
		out &amp;lt;- result           // Write to the output channel
		close(job.NextCanWrite) // Pass the permission to the next job
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;927.2ns&lt;/cell&gt;
        &lt;cell&gt;+519ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;939.8ns&lt;/cell&gt;
        &lt;cell&gt;+495ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;860.7ns&lt;/cell&gt;
        &lt;cell&gt;+314ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;823.8ns&lt;/cell&gt;
        &lt;cell&gt;+224ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;609.8ns&lt;/cell&gt;
        &lt;cell&gt;-443ns&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here the result is very similar to what we’ve seen in the ReplyTo approach. Almost the same overhead, the same inversion at higher levels of concurrency, and the same extra allocation per item. But there’s one difference…&lt;/p&gt;
    &lt;p&gt;Unlike approach 1, here we’re allocating a non-generic &lt;code&gt;chan struct{}&lt;/code&gt;. This means we can use a package level &lt;code&gt;sync.Pool&lt;/code&gt; to eliminate those allocations – let’s explore that next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 3a: Zero-Allocation Permission Passing Chain&lt;/head&gt;
    &lt;p&gt;Let’s create a pool for &lt;code&gt;canWrite&lt;/code&gt; channels. Implementation is straightforward – the pool itself and make/release functions.&lt;/p&gt;
    &lt;code&gt;// Package-level pool for canWrite channels
type chainedItem[A any] struct {
	Value        A
	CanWrite     chan struct{}
	NextCanWrite chan struct{} // canWrite channel for the next item
}

var canWritePool sync.Pool

func makeCanWriteChan() chan struct{} {
	ch := canWritePool.Get()
	if ch == nil {
		return make(chan struct{}, 1)
	}
	return ch.(chan struct{})
}

func releaseCanWriteChan(ch chan struct{}) {
	canWritePool.Put(ch)
}&lt;/code&gt;
    &lt;p&gt;Now let’s use the pool in the permission passing algorithm. Since channels are reused, we can no longer signal by closing them. Instead workers must read and write empty structs form/to these channels.&lt;/p&gt;
    &lt;code&gt;func OrderedMap3a[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite &amp;lt;- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	out := make(chan B)
	Loop(jobs, n, out, func(job Job[A]) {
		result := f(job.Item) // Calculate the result

		&amp;lt;-job.CanWrite                    // Wait for the write permission
		out &amp;lt;- result                     // Write to the output channel
		releaseCanWriteChan(job.CanWrite) // Release our canWrite channel to the pool
		job.NextCanWrite &amp;lt;- struct{}{}    // Pass the permission to the next job
	})

	return out
}&lt;/code&gt;
    &lt;p&gt;Performance Results with Pooling:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Goroutines&lt;/cell&gt;
        &lt;cell role="head"&gt;Time /op&lt;/cell&gt;
        &lt;cell role="head"&gt;vs Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Allocs/op&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;891.0ns&lt;/cell&gt;
        &lt;cell&gt;+482ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;916.5ns&lt;/cell&gt;
        &lt;cell&gt;+471ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;879.5ns&lt;/cell&gt;
        &lt;cell&gt;+333ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;872.6ns&lt;/cell&gt;
        &lt;cell&gt;+272ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;657.6ns&lt;/cell&gt;
        &lt;cell&gt;-395ns&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Perfect! Zero allocations and good performance, meaning less GC pressure for long running jobs. But this approach has one more trick up its sleeve…&lt;/p&gt;
    &lt;head rend="h2"&gt;One more thing: Building Reusable Abstractions&lt;/head&gt;
    &lt;p&gt;The permission passing approach has another significant advantage over the ReplyTo method: it controls when to write rather than where to write.&lt;/p&gt;
    &lt;p&gt;I’ll admit it – sometimes I get a bit obsessed with building clean abstractions. When working on rill, I really wanted to extract this ordering logic into something reusable and testable. This “when vs where” distinction was an AHA moment for me.&lt;/p&gt;
    &lt;p&gt;Since the algorithm doesn’t care where the outputs are written, it’s easy to abstract it into a separate function – &lt;code&gt;OrderedLoop&lt;/code&gt;. The API is very similar to the &lt;code&gt;Loop&lt;/code&gt; function we used before, but here the user function receives two arguments – an &lt;code&gt;item&lt;/code&gt; and a &lt;code&gt;canWrite&lt;/code&gt; channel. It’s important that the user function must read from the &lt;code&gt;canWrite&lt;/code&gt; channel exactly once to avoid deadlocks or undefined behavior.&lt;/p&gt;
    &lt;code&gt;func OrderedLoop[A, B any](in &amp;lt;-chan A, done chan&amp;lt;- B, n int, f func(a A, canWrite &amp;lt;-chan struct{})) {
	type Job[A any] struct {
		Item         A
		CanWrite     chan struct{}
		NextCanWrite chan struct{} // canWrite channel of the next job
	}

	// Linker goroutine:
	// Builds a chain of jobs where each has a CanWrite channel attached.
	// Additionally, each job knows about the CanWrite channel of the next job in the chain.
	jobs := make(chan Job[A])
	go func() {
		defer close(jobs)

		var canWrite, nextCanWrite chan struct{}
		nextCanWrite = makeCanWriteChan()
		nextCanWrite &amp;lt;- struct{}{} // the first job can write immediately

		for item := range in {
			canWrite, nextCanWrite = nextCanWrite, makeCanWriteChan()
			jobs &amp;lt;- Job[A]{item, canWrite, nextCanWrite}
		}
	}()

	// Worker pool of n goroutines.
	// Jobs pass the write permission along the chain.
	Loop(jobs, n, done, func(job Job[A]) {
		f(job.Item, job.CanWrite) // Do the work

		releaseCanWriteChan(job.CanWrite) // Release item's canWrite channel to the pool
		job.NextCanWrite &amp;lt;- struct{}{}    // Pass the permission to the next job
	})
}&lt;/code&gt;
    &lt;p&gt;The typical usage looks like:&lt;/p&gt;
    &lt;code&gt;OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
	// [Do processing here]
	
	// Everything above this line is executed concurrently,
	// everything below it is executed sequentially and in order
	&amp;lt;-canWrite
	
	// [Write results somewhere]
})
&lt;/code&gt;
    &lt;p&gt;With this abstraction in hand it’s remarkably simple to build any ordered operations. For example &lt;code&gt;OrderedMap&lt;/code&gt; becomes just 7 lines of code:&lt;/p&gt;
    &lt;code&gt;func OrderedMap3b[A, B any](in &amp;lt;-chan A, n int, f func(A) B) &amp;lt;-chan B {
	out := make(chan B)
	OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		result := f(a)
		&amp;lt;-canWrite
		out &amp;lt;- result
	})
	return out
}&lt;/code&gt;
    &lt;p&gt;We can also easily build an &lt;code&gt;OrderedFilter&lt;/code&gt; that conditionally writes outputs:&lt;/p&gt;
    &lt;code&gt;func OrderedFilter[A any](in &amp;lt;-chan A, n int, predicate func(A) bool) &amp;lt;-chan A {
	out := make(chan A)
	OrderedLoop(in, out, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		keep := predicate(a)
		&amp;lt;-canWrite
		if keep {
			out &amp;lt;- a
		}
	})
	return out
}&lt;/code&gt;
    &lt;p&gt;Or even an &lt;code&gt;OrderedSplit&lt;/code&gt; that distributes items to two channels based on a predicate:&lt;/p&gt;
    &lt;code&gt;func OrderedSplit[A any](in &amp;lt;-chan A, n int, predicate func(A) bool) (&amp;lt;-chan A, &amp;lt;-chan A) {
	outTrue := make(chan A)
	outFalse := make(chan A)
	done := make(chan struct{})
	
	OrderedLoop(in, done, n, func(a A, canWrite &amp;lt;-chan struct{}) {
		shouldGoToTrue := predicate(a)
		&amp;lt;-canWrite
		if shouldGoToTrue {
			outTrue &amp;lt;- a
		} else {
			outFalse &amp;lt;- a
		}
	})
	
	go func() {
		&amp;lt;-done
		close(outTrue)
		close(outFalse)
	}()
	
	return outTrue, outFalse
}&lt;/code&gt;
    &lt;p&gt;Simply put, this abstraction makes building ordered operations trivial.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Comparison&lt;/head&gt;
    &lt;p&gt;Here’s how all approaches perform across different concurrency levels:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Concurrency&lt;/cell&gt;
        &lt;cell role="head"&gt;Baseline&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 1&lt;p&gt;(ReplyTo)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 2&lt;p&gt;(sync.Cond)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 3&lt;p&gt;(Permission)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Approach 3a&lt;p&gt;(+ Pool)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;408.6ns&lt;/cell&gt;
        &lt;cell&gt;818.7ns&lt;/cell&gt;
        &lt;cell&gt;867.7ns&lt;/cell&gt;
        &lt;cell&gt;927.2ns&lt;/cell&gt;
        &lt;cell&gt;891.0ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;445.1ns&lt;/cell&gt;
        &lt;cell&gt;808.9ns&lt;/cell&gt;
        &lt;cell&gt;1094ns&lt;/cell&gt;
        &lt;cell&gt;939.8ns&lt;/cell&gt;
        &lt;cell&gt;916.5ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;546.4ns&lt;/cell&gt;
        &lt;cell&gt;826.8ns&lt;/cell&gt;
        &lt;cell&gt;1801ns&lt;/cell&gt;
        &lt;cell&gt;860.7ns&lt;/cell&gt;
        &lt;cell&gt;879.5ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;600.2ns&lt;/cell&gt;
        &lt;cell&gt;825.6ns&lt;/cell&gt;
        &lt;cell&gt;2987ns&lt;/cell&gt;
        &lt;cell&gt;823.8ns&lt;/cell&gt;
        &lt;cell&gt;872.6ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;50&lt;/cell&gt;
        &lt;cell&gt;1053ns&lt;/cell&gt;
        &lt;cell&gt;772.3ns&lt;/cell&gt;
        &lt;cell&gt;16074ns&lt;/cell&gt;
        &lt;cell&gt;609.8ns&lt;/cell&gt;
        &lt;cell&gt;657.6ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Zero allocs&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;sync.Cond is a no-go for ordered concurrency – While it starts with decent performance at low concurrency, it completely falls apart as goroutine count increases, due to the thundering herd problem.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ReplyTo is a strong contender – it adds at most ~500ns of overhead compared to the baseline, but requires one additional allocation per input item, increasing GC pressure.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Permission Passing emerges as the clear winner – It has it all:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Good performance: at most ~500ns of overhead compared to the baseline&lt;/item&gt;
          &lt;item&gt;Zero allocations: Less GC pressure for long running tasks&lt;/item&gt;
          &lt;item&gt;Clean abstraction: Core synchronization logic can be abstracted away and used to build various concurrent operations.&lt;/item&gt;
          &lt;item&gt;Maintainability: Separation of concerns and the intuitive “calculate → wait → write → pass” pattern make code easy to support and reason about&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This exploration shows that ordered concurrency doesn’t have to be expensive. With the right approach, you can have concurrency, ordering and backpressure at the same time. The permission passing pattern, in particular, demonstrates how Go’s channels can be used creatively to solve complex coordination problems.&lt;/p&gt;
    &lt;p&gt;Finally, these patterns have been battle-tested in production through rill concurrency toolkit (1.7k 🌟 on GitHub). It implements &lt;code&gt;Map&lt;/code&gt;, &lt;code&gt;OrderedMap&lt;/code&gt;, and many other concurrent operations. Rill focuses on composability – operations chain together into larger pipelines – while adding comprehensive error handling, context-friendly design, and maintaining over 95% test coverage.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45089938</guid></item><item><title>Telli (YC F24) is hiring engineers, designers, and interns (on-site in Berlin)</title><link>https://hi.telli.com/join-us</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45090216</guid></item><item><title>CocoaPods Is Deprecated</title><link>https://blog.cocoapods.org/CocoaPods-Specs-Repo/</link><description>&lt;doc fingerprint="42f18d113d47e29d"&gt;
  &lt;main&gt;&lt;p&gt;30 November 2024&lt;/p&gt;Follow @orta&lt;p&gt;TLDR: In two years we plan to turn CocoaPods trunk to be read-only. At that point, no new versions or pods will be added to trunk. - Note, this post has been updated in May 2025.&lt;/p&gt;&lt;p&gt;Last month I wrote about how CocoaPods is currently being maintained, I also noted that we were discussing converting the main CocoaPods spec repo "trunk" to be read-only:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We are discussing that on a very long, multi-year, basis we can drastically simplify the security of CocoaPods trunk by converting the Specs Repo to be read-only. Infrastructure like the Specs repo and the CDN would still operate as long as GitHub and jsDelivr continue to exist, which is pretty likely to be a very long time. This will keep all existing builds working.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I plan to implement the read-only mode so that when someone submits a new Podspec to CocoaPods, it will always be denied at the server level. I would then convert the "CocoaPods/Specs" repo to be marked as "Archived" on GitHub which should cover all of our bases.&lt;/p&gt;&lt;p&gt;Making the switch will not break builds for people using CocoaPods in 2026 onwards, but at that point, you're not getting any more updates to dependencies which come though CocoaPods trunk. This shouldn't affect people who use CocoaPods with their own specs repos, or have all of their dependencies vendored (e.g. they all come from npm.)&lt;/p&gt;&lt;p&gt;May 2025 Update: Since this post was originally written, we've had enough security researchers abusing scripting capabilities in CocoaPods that we are now introducing a block on allowing new CocoaPods to use the &lt;code&gt;prepare_command&lt;/code&gt; field in a Podspec. Any existing Pods using &lt;code&gt;prepare_command&lt;/code&gt; are hard-coded to bypass this check.&lt;/p&gt;&lt;head rend="h2"&gt;Timeline&lt;/head&gt;&lt;p&gt;My goal is to send 2 very hard-to-miss notifications en-masse, and then do a test run a month before the final shutdown.&lt;/p&gt;&lt;head rend="h3"&gt;May 2025&lt;/head&gt;&lt;p&gt;We are stopping new CocoaPods from being added which use the &lt;code&gt;prepare_command&lt;/code&gt; field&lt;/p&gt;&lt;head rend="h3"&gt;Mid-late 2025&lt;/head&gt;&lt;p&gt;I will email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post.&lt;/p&gt;&lt;head rend="h3"&gt;September-October 2026&lt;/head&gt;&lt;p&gt;I will, again, email all email addresses for people who have contributed a Podspec, informing them of the impending switch to read-only, and linking them to this blog post, noting that they have roughly a month before we do a test run of going read-only.&lt;/p&gt;&lt;head rend="h3"&gt;November 1-7th 2026&lt;/head&gt;&lt;p&gt;I will trigger a test run, giving automation a chance to break early&lt;/p&gt;&lt;head rend="h3"&gt;December 2nd 2026&lt;/head&gt;&lt;p&gt;I will switch trunk to not accept new Podspecs permanently. This is a Wednesday after American Thanksgiving, so I think folks won't be in rush mode.&lt;/p&gt;&lt;head rend="h2"&gt;Contact&lt;/head&gt;&lt;p&gt;These dates are not set in stone, and maybe someone out there has a good reason for us to amend the timeline. I don't think I'm amenable to moving it forwards, but within reason there's space for backwards.&lt;/p&gt;&lt;p&gt;If you have questions, you can contact the team via [email protected], me personally at [email protected] or reach out to me via Bluesky: @orta.io.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091493</guid></item><item><title>Show HN: Blueprint: Fast, Nunjucks-like templating engine for Java 8 and beyond</title><link>https://news.ycombinator.com/item?id=45091547</link><description>&lt;doc fingerprint="3fd28714ea21b0b9"&gt;
  &lt;main&gt;
    &lt;p&gt;But I was not able to find something with similar for Java, especially with the same syntax.&lt;/p&gt;
    &lt;p&gt;So, built one. And it's pretty fast too.&lt;/p&gt;
    &lt;p&gt;https://github.com/freakynit/Blueprint&lt;/p&gt;
    &lt;p&gt;reply&lt;/p&gt;
    &lt;p&gt;Normally I'd use Spring Boot and get the batteries-included experience if I ever were to make web with Java.&lt;/p&gt;
    &lt;p&gt;But I could totally imagine needing a pure, well-made, zero-deps template engine for custom jobs.&lt;/p&gt;
    &lt;p&gt;I certainly have similar libraries in my bookmarks for other languages, like Haskell [1] and Rust [2].&lt;/p&gt;
    &lt;p&gt;[1]: https://hackage.haskell.org/package/heterocephalus [2]: https://crates.io/crates/minijinja&lt;/p&gt;
    &lt;p&gt;https://pebbletemplates.io/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091547</guid></item><item><title>Intel Patents 'Software Defined Supercore'</title><link>https://www.tomshardware.com/pc-components/cpus/intel-patents-software-defined-supercore-mimicking-ultra-wide-execution-using-multiple-cores</link><description>&lt;doc fingerprint="279f087b444e3698"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intel patents 'Software Defined Supercore' — increases single-thread performance and IPC by mimicking ultra-wide execution using multiple cores&lt;/head&gt;
    &lt;p&gt;Reverse Hyper-Threading?&lt;/p&gt;
    &lt;p&gt;Intel has patented a technology it calls 'Software Defined Supercore' (SDC) that enables software to fuse the capabilities of multiple cores to assemble a virtual ultra-wide 'supercore' capable of improving single-thread performance, provided that it has enough parallel work. If the technology works as it is designed to, then Intel's future CPUs could offer faster single-thread performance in select applications that can use SDC. For now, this is just a patent which may or may not become a reality.&lt;/p&gt;
    &lt;p&gt;Intel's Software Defined Supercore (SDC) technologies combine two or more physical CPU cores to cooperate as a single high-performance virtual core by dividing a single thread's instructions into separate blocks and executing them in parallel. Each core runs a distinct portion of the program, while specialized synchronization and data-transfer instructions ensure that the original program order is preserved, maximizing instructions per clock (IPC) with minimal overhead. This approach is designed to improve single-thread performance without increasing clock speeds or building wide, monolithic cores, which can increase power consumption and/or transistor budgets.&lt;/p&gt;
    &lt;p&gt;Modern x86 CPU cores can decode 4–6 instructions and then execute 8-9 micro-ops per cycle after the instructions are decoded into micro-ops, which achieves peak IPC performance for such processors. By contrast, Apple's custom Arm-based high-performance cores (e.g., Firestorm, Avalanche, Everest) can decode up to 8 instructions per cycle and then execute over 10 instructions per cycle under ideal conditions. This is why Apple's processors typically offer significantly higher single-threaded performance and lower power consumption compared to Arm counterparts.&lt;/p&gt;
    &lt;p&gt;While it is technically possible to build an 8-way x86 CPU core (i.e., a superscalar x86 processor that can decode, issue, and retire up to 8 instructions per clock), in practice, it has not been done because of front-end bottlenecks as well as diminishing returns in terms of performance increase amid significant power and area costs. In fact, even modern x86 CPUs can typically hit 2–3-4 sustained IPC on general workloads, depending on software. So, instead of building an 8-way x86 CPU core, Intel's SDC proposes pairing two or more 4-wide units to cooperate as one large core in cases where it makes sense.&lt;/p&gt;
    &lt;p&gt;On the hardware side, each core in an SDC-enabled system includes a small dedicated hardware module that manages synchronization, register transfers, and memory ordering between paired cores. These modules utilize a reserved memory region — known as the wormhole address space — to coordinate live-in/live-out data and synchronization operations, ensuring that instructions from separate cores retire in the correct program order. The design supports both in-order and out-of-order cores, requiring minimal changes to the existing execution engine, which results in a compact design in terms of die space.&lt;/p&gt;
    &lt;p&gt;On the software side, the system uses either a JIT compiler, a static compiler, or binary instrumentation to split a single-threaded program into code segments to assign different blocks to different cores. It injects special instructions for flow control, register passing, and sync behavior, enabling the hardware to maintain execution integrity. Support by the operating system is crucial as the OS dynamically decides when to migrate a thread into or out of super-core mode based on runtime conditions to balance performance and core availability.&lt;/p&gt;
    &lt;p&gt;Intel's patent does not provide exact numerical performance gain estimates, but it implies that in select scenarios, it is realistic to expect the performance of two 'narrow' cores to approach the performance of a 'wide' core.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!&lt;/p&gt;
    &lt;p&gt;Anton Shilov is a contributing writer at Tom’s Hardware. Over the past couple of decades, he has covered everything from CPUs and GPUs to supercomputers and from modern process technologies and latest fab tools to high-tech industry trends.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Intel bought a startup promoting this concept, Soft Machines, almost a decade ago. They called it "VISC".Reply&lt;lb/&gt;https://www.blopeur.com/2021/10/30/Intel-VISC-Processor-Architecture-Patent.html&lt;quote/&gt;Those numbers are rather dated. Zen 4 had only 4-wide decode. Zen 5 gives us 2x 4-wide decoders per core, but they're per-thread (meaning one is idle when only a single thread is using a core).The article said:Modern x86 CPU cores can decode 4 – 6 instructions and then execute 8 - 9 micro-ops per cycle after instructions are decoded into micro-ops&lt;lb/&gt;From Golden to Redwood Cove (Alder Lake to Meteor Lake) Intel did 6-wide decode. Lion Cove increased it to 8 (source: https://chipsandcheese.com/p/lion-cove-intels-p-core-roars ). But, these numbers can be deceiving. Intel's P-cores usually distinguish between "simple" and "complex" instructions, with only a couple complex decode slots and the rest being limited to "simple" instructions.&lt;lb/&gt;Gracemont had 2x 3-wide decoders, which Skymont boosted to 3x3. In some interviews, Intel has stated that its decoders almost never saturate, but that adding another 3-wide decode block was simply the easiest way to add frontend bandwidth (keeping in mind that Skymont has no separate micro-op cache). Would be interesting to see how Skymont's actual decode throughput compares with different P-cores, on the same instruction streams of varying types and complexity. For sure, Skymont is not decoding 9 instructions per cycle, in practice.&lt;lb/&gt;https://chipsandcheese.com/p/intel-details-skymont&lt;quote/&gt;Apple's P-cores now have 10-wide decode, in the M4.The article said:Apple's custom Arm-based high-performance cores (e.g., Firestorm, Avalanche, Everest) can decode up to 8 instructions per cycle&lt;lb/&gt;https://www.techpowerup.com/322195/apple-introduces-the-m4-chip&lt;lb/&gt;Arm's Cortex-X925 also features 10-wide decode.&lt;quote/&gt;Chips &amp;amp; Cheese has been looking at this. Here's Zen 5 on gaming + an assortment of non-gaming workloads:The article said:In fact, even modern x86 CPUs can typically hit 2–3-4 sustained IPC on general workloads, depending on software.&lt;lb/&gt;Source: https://chipsandcheese.com/p/running-gaming-workloads-through&lt;lb/&gt;Here's Lion Cove:&lt;lb/&gt;Source: https://chipsandcheese.com/p/intels-lion-cove-p-core-and-gaming&lt;lb/&gt;For non-gaming, I'd characterize most of the cases on Zen 5 as 2-4 IPC, while the center of Lion Cove's distribution is a little more in the 2-3 range. It's annoying that the charts aren't both scaled to 6 IPC on the X-axis.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;dalek1234&lt;/header&gt;I think that if this was possible and actually worked where single-threaded performance was improved, somebody would have implemented it by now.Reply&lt;lb/&gt;Software is always much slower than baking that functionality into the silicon. Jim Keller worked on the concept described in this article, but on hardware level. It was called Rentable Units; where multiple physical cores could switch to behave like a large single-core, greatly improving single-threaded performance. Jim Keller never completed the project though. He left Intel after two years because it sucked working for Intel. Intel did continue the project, but Pat Gelsinger cancelled it, citing 'cost'.&lt;lb/&gt;So Intel's solution is to now doing it in software. Well, good luck with that. Maybe they are just patenting their inventions now so that they can get more money out of them when they sell them. That's what Blackberry did to raise money, back in the day, sell their IP.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;hotaru251&lt;/header&gt;honestly I doubt it'll happne but I'd love for it to work that way... imagine a TR system using all that pwoer for single thread performance....Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;My take on this is basically that it's a more efficient way to exploit coarse-level ILP (Instruction-Level Parallesism) than continuing to double-down on ever deeper and wider cores. The scheduling logic needed to keep ever larger cores fed just doesn't scale terribly well, especially with respect to the real-world gains achieved.Reply&lt;lb/&gt;What I find particularly intriguing is to look at this (let's call it VISC, for lack of a better term) in conjunction with SMT. It'd be really interesting to use VISC to partition the scheduling problem, but then still execute multiple of these nano-threads on the same physical core, with the same shared backend resources.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;Reply&lt;quote/&gt;It's a hard problem and requires a lot of work on both the hardware and software end of things, in order to make it work. As long as conventional approaches for scaling performance have continued to deliver gains, I think implementing such a complex solution couldn't be justified.dalek1234 said:I think that if this was possible and actually worked where single-threaded performance was improved, somebody would have implemented it by now.&lt;quote/&gt;I'm sure it's not infinitely flexible. They must limit it to just the cores which share a cluster, like how the current E-cores are arranged in clusters of 4. I don't imagine you'd ever have more than 4-way scalability on this, and perhaps limited to only 2.hotaru251 said:imagine a TR system using all that pwoer for single thread performance....&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45091921</guid></item></channel></rss>