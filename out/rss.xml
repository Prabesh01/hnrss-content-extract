<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 02 Feb 2026 17:29:31 +0000</lastBuildDate><item><title>Ian's Shoelace Site</title><link>https://www.fieggen.com/shoelace/</link><description>&lt;doc fingerprint="f9d7e127c154339c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ian's Shoelace Site&lt;/head&gt;
    &lt;p&gt;Fun, fashion &amp;amp; science in the Internet's #1 website about shoelaces ‚Äì and home of the Ian Knot, the world's fastest shoelace knot. If you want to lace shoes, tie shoes or learn about shoelaces ‚Äì this is the place!&lt;/p&gt;
    &lt;head rend="h2"&gt;Welcome!&lt;/head&gt;
    &lt;p&gt;G'day everyone, Ian Fieggen here, also known as ‚ÄúProfessor Shoelace‚Äù. I'm a real human living in Melbourne, Australia. This website has no ‚ÄúA.I.‚Äù content ‚Äì it's all built with ‚ÄúH.I.‚Äù plus Human Effort over more than two decades.&lt;/p&gt;
    &lt;p&gt;Welcome to ‚ÄúIan's Shoelace Site‚Äù ‚Äì made by one human, for all humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;Contains over 100 √ó step-by-step shoe lacing tutorials, over 2,700 √ó shoe lacing photos, plus the interactive ‚ÄúCreate-a-Lace‚Äù for designing your own lacing.&lt;/p&gt;
    &lt;p&gt;Contains 25 √ó shoelace knots (including my world's fastest ‚ÄúIan Knot‚Äù), plus info on tying correctly so that they sit straight and stay securely tied ‚Äì yet can be untied without jamming.&lt;/p&gt;
    &lt;p&gt;Heaps of knowledge about shoelaces, including their construction, length calculations ‚Äì even what the tips are called, as well as Ian's interviews, articles, Q&amp;amp;As and more.&lt;/p&gt;
    &lt;p&gt;Information about this website and the site's author, Ian Fieggen ‚Äì including Ian's contact details. Also info on this site's sponsors plus ways that you, too, can support Ian.&lt;/p&gt;
    &lt;p&gt;Contact Ian Fieggen via e-mail or connect with ‚ÄúProfessor Shoelace‚Äù via YouTube, Instagram, Twitter, Facebook or RSS feed.&lt;/p&gt;
    &lt;p&gt;Looking for something? This website has more than 300 pages ‚Äì which can be daunting to wade through! Search the whole of Ian's Shoelace Site using Google Custom Search.&lt;/p&gt;
    &lt;head rend="h2"&gt;Latest Photo&lt;/head&gt;
    &lt;head rend="h3"&gt;02-Feb-2026&lt;/head&gt;
    &lt;p&gt;Today's shoe lacing photo was contributed by Guillaume R. in Feb-2026.&lt;/p&gt;
    &lt;p&gt;Black &amp;amp; white Umbro soccer cleats laced with white Ladder Lacing.&lt;/p&gt;
    &lt;p&gt;Neat lacing that should also provide good support for playing soccer.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's New?&lt;/head&gt;
    &lt;p&gt;22-Jan-2025 ‚Äì Added Ian's interview from 2018 about modern shoelace alternatives, plus an interview from 2020 about lacing, knots and my ‚ÄúProfessor Shoelace‚Äù alias.&lt;/p&gt;
    &lt;p&gt;04-Dec-2025 ‚Äì During Australia's marriage equality debate, PUMA engaged me to create the ‚ÄúEquality Knot‚Äù, then sent a film crew to shoot this ‚Äúcontent piece‚Äù video.&lt;/p&gt;
    &lt;p&gt;02-Dec-2025 ‚Äì Following a visitor request, I added diagrams for the inverted variation of Pentagram Lacing for 7, 6 and 5 pairs of eyelets.&lt;/p&gt;
    &lt;p&gt;28-Nov-2025 ‚Äì Following a suggestion from a website visitor, I added buttons to filter by either ‚ÄúShortening‚Äù or ‚ÄúLengthening‚Äù methods.&lt;/p&gt;
    &lt;p&gt;10-Nov-2025 ‚Äì Added Ian's interview from Dec-2016 with The Stuph File about my Shoelace Site, from its humble beginnings through to the extensive and respected website that it is today.&lt;/p&gt;
    &lt;p&gt;06-Nov-2025 ‚Äì Added Ian's interview from Sep-2015 with BBC Radio 4, which coincided with ‚Äúback to school‚Äù in the UK, covered teaching kids to tie their shoes for the first time.&lt;/p&gt;
    &lt;p&gt;29-Oct-2025 ‚Äì This website has long included ‚ÄúWoven Lacing‚Äù for shoes with eyelets. A recent visitor-contributed photo prompted me to add Lug Woven Lacing for shoes with lugs.&lt;/p&gt;
    &lt;p&gt;14-Oct-2025 ‚Äì Added an interactive Shoelace Weave Diagram, which allows you to experiment with creating shoelace weave patterns thread-by-thread.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent Visitor Feedback&lt;/head&gt;
    &lt;p&gt;I just want to express my gratitude and appreciation for your website and the work you put into it. I look forward to learning all about tying knots I'm going to relish for years to come. Thank you again, sir.&lt;/p&gt;
    &lt;p&gt;‚Äì Travis S., Virginia, USA, Feb-2026&lt;/p&gt;
    &lt;p&gt;Can't believe that I've been tying my shoelaces wrongly for 50 years! Never too late to learn I guess... many thanks&lt;/p&gt;
    &lt;p&gt;‚Äì Mike B., UK, Feb-2026&lt;/p&gt;
    &lt;p&gt;A lesson that will at least benefit Mike for the next 50-odd years ‚Äì and which prompted a donation in gratitude. Thanks, Mike!&lt;/p&gt;
    &lt;p&gt;‚Äì Ian Fieggen&lt;/p&gt;
    &lt;p&gt;Tonight I was having a grand time looking through your shoe lacing tutorials for fun, and then I saw that you actually invented my favorite way to tie my shoes and show off a little bit!&lt;/p&gt;
    &lt;p&gt;I yelled ‚ÄúHE INVENTED THIS KNOT? I KNOW THAT KNOT!!!‚Äù (and scared my cat a little bit hahahaha)&lt;/p&gt;
    &lt;p&gt;Anyways just wanted to thank you for having a great website, I‚Äôm really enjoying it and can‚Äôt wait to try out some of the decorative lacing on my boots!&lt;/p&gt;
    &lt;p&gt;‚Äì Selkie, Los Angeles, USA, Feb-2026&lt;/p&gt;
    &lt;p&gt;When the Canadian Forces' lacing instructions were at best a 2/10 I got referenced to this site and never looked back, thank you.&lt;/p&gt;
    &lt;p&gt;‚Äì Andres S., Canada, Jan-2026&lt;/p&gt;
    &lt;p&gt;Andres was also appreciative enough to send a donation ‚Äì thanks!&lt;/p&gt;
    &lt;p&gt;‚Äì Ian Fieggen&lt;/p&gt;
    &lt;p&gt;Love the site! Been checking it on occasion for years, and I have been using the Ian [k]not for so long I don't remember how to tie the normal bow. Hadn't occurred to me I could donate until I saw Chris Person's article.&lt;/p&gt;
    &lt;p&gt;‚Äì Baynard N., USA, Jan-2026&lt;/p&gt;
    &lt;p&gt;Kind comments plus acts of generosity like Baynard's are the catalyst that has kept me going all these years. Thanks, Baynard!&lt;/p&gt;
    &lt;p&gt;‚Äì Ian Fieggen&lt;/p&gt;
    &lt;p&gt;If you'd also like to send feedback, please Contact Ian.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46848231</guid><pubDate>Sun, 01 Feb 2026 18:38:04 +0000</pubDate></item><item><title>Clearspace (YC W23) Is Hiring an Applied Researcher (ML)</title><link>https://www.ycombinator.com/companies/clearspace/jobs/GOWiDwp-research-engineer-at-clearspace</link><description>&lt;doc fingerprint="14939967bc01edd1"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building an agent that achieves this on all devices by processing and filtering network traffic based on natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We are looking for an ML-focused engineer that will be responsible for training and improving a model for classifying network traffic. You are great for this role if you are not only excited about the latest in AI and ML but are also a problem-solver in the data domain. You don‚Äôt just think about the model but ‚Äúhow can we get more data volume‚Äù; ‚Äúhow can we featurize the data intelligently‚Äù; ‚Äúwhat are our data needs based on our task and desired model size‚Äù, and like building backwards from inference requirements.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46848260</guid><pubDate>Sun, 01 Feb 2026 18:41:54 +0000</pubDate></item><item><title>My iPhone 16 Pro Max produces garbage output when running MLX LLMs</title><link>https://journal.rafaelcosta.me/my-thousand-dollar-iphone-cant-do-math/</link><description>&lt;doc fingerprint="3402b8b4df265bde"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TL;DR:&lt;/head&gt;
    &lt;p&gt;My iPhone 16 Pro Max produces garbage output when running MLX LLMs. An iPhone 15 Pro runs the same code perfectly. A MacBook Pro also runs the same code perfectly. The tensor outputs on the 16 show numerical values an order of magnitude wrong. I suspect it points to a hardware defect in the Neural Engine or some other ML-needed system.&lt;/p&gt;
    &lt;p&gt;It was a PITA to debug, but at least I got a blog post out of it.&lt;/p&gt;
    &lt;head rend="h1"&gt;How did I get there?&lt;/head&gt;
    &lt;p&gt;This was supposed to be a simple, unwinding-time project.&lt;/p&gt;
    &lt;p&gt;For the past few months I've been working on a &lt;del&gt;Clawdbot&lt;/del&gt; Moltbot clone that I've been calling Schmidt. It basically does the same kind of thing but with a custom chat UI instead of using Telegram, WhatsApp or other "I-can't-afford-to-be-banned-from" Service. This project has been consuming early days and late nights, so, to unwind, I decided that it may be a good idea to do something simpler. Since I recently subscribed to MiniMax M2.1, I thought I would do what many do and build a simple expense tracking app to test out the model.&lt;/p&gt;
    &lt;p&gt;The core functionality is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically, upon each payment, add the expense to my app&lt;/item&gt;
      &lt;item&gt;Update an Apple Watch complication with the % of my monthly budget spent&lt;/item&gt;
      &lt;item&gt;Categorize the purchase for later analysis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This all comes from being basically orphaned by Nubank's amazing native app (since replaced by a less-full-featured Flutter version).&lt;/p&gt;
    &lt;p&gt;Integrating with Shortcuts is manual, but reliable. Within 15 minutes I had a version of the app that could register purchases. The Apple Watch complication, the main goal, can come later. I'd rather get the classification feature, which should be easy, done quickly ‚Äì so I figured.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apple Intelligence&lt;/head&gt;
    &lt;p&gt;Given the new LLM-bonanza we've been living through, it's no surprise that Apple has their own set of APIs developers such as me can use. Reading up on the documentation, it's a matter of checking for the availability of the feature and then asking the model to either reply to a textual query or, in my case, categorize a request.&lt;/p&gt;
    &lt;p&gt;MiniMax raced through it in a single prompt and then I ran it on my iPhone. First expense was a purchase at a shop called "Kasai Kitchin", classified as... &lt;code&gt;unknown&lt;/code&gt;.&lt;lb/&gt;Weird.&lt;/p&gt;
    &lt;p&gt;Checking the logs, it was clear: the model support was downloading. The feature hadn't been enabled. Again, weird. I should have it on. Anyway, I go into settings, do the weird dance of toggling it on and off ‚Äì sadly, that's not surprising on Apple's services. Maybe my Settings.app got stuck in a weird state, who knows? ‚Äì and wait for it to download.&lt;/p&gt;
    &lt;p&gt;After 4h I realized it was not going anywhere. Looking it up, it seems that many have the same issue (this thread shows 12 pages of frustrated users). Again, not a surprise for Apple's services recently.&lt;/p&gt;
    &lt;p&gt;Oh well, time to give up on the Apple Intelligence approach. Let's move on to the next one.&lt;/p&gt;
    &lt;head rend="h2"&gt;MLX LLM&lt;/head&gt;
    &lt;p&gt;Well, the iOS framework engineers don't seem to be the only engineers at Apple capable of coming up with Machine Learning APIs in Swift. Apparently, there's a whole separate way of doing it ‚Äì with models downloaded to your app. Not great for the user's storage, but great for me!&lt;/p&gt;
    &lt;p&gt;Again, MiniMax does it in a heartbeat, specially after being given documentation and one or two Medium posts. Time to run on my iPhone and... gibberish.&lt;/p&gt;
    &lt;p&gt;The CPU spins to 100% and the model starts generating. But it's all gibberish. And no "stop" token is generated, so this goes on for long.&lt;/p&gt;
    &lt;p&gt;At this point, the only explanation is: I'm completely incompetent and can't even get a simple "ready made" framework to execute what I want. Or, rather, MiniMax is! The good thing about offloading your work to an LLM is that you can blame it for your shortcomings. Time to get my hands dirty and do it myself, typing code on my keyboard, like the ancient Mayan and Aztec programmers probably did.&lt;/p&gt;
    &lt;head rend="h2"&gt;My own MLX implementation&lt;/head&gt;
    &lt;p&gt;I went back to the documentation, to the Medium posts and, much to my surprise: MiniMax had followed it to the letter. Even went back to some deprecated methods of generation and it also was gibberish. And now there's no one to blame, but myself. I go to work everyday and this impostor-syndrome inducing problem silently consumes me. &lt;lb/&gt;After 3 days of trying to get it to work, I'm ready to give up...&lt;lb/&gt;...until, on a Tuesday morning, at 7-8 AM, I have an idea: let me, just in case, run this on my old iPhone 15 Pro. Up to this point, I was running it on my daily driver, an iPhone 16 Pro Max that was a replacement phone sent by Apple Care after a small clubbing mishap (in which my iPhone was irreparably crashed). I rush to get everything ready before it's time to go to work and: it works! Gemma, Qwen, and all other models generate coherent responses!&lt;/p&gt;
    &lt;p&gt;I stop and think: this cannot be a hardware issue, right? Of course not. The iPhone 15 is still running iOS 18. The iPhone 16 is running 26. It must be an OS issue. Well, time to be late for my work standup and update the old phone. The curiosity is too much. Many minutes later... same results, now on iOS 26. The plot is thickening.&lt;/p&gt;
    &lt;head rend="h1"&gt;Finding the smoking gun: breakpoints in MLX's implementations of Gemma&lt;/head&gt;
    &lt;p&gt;After that work day, and after many lunch and coffee discussions with coworkers about the sources of my troubles, I get home and immediately set myself on debugging MLX as it runs, if possible. The game plan is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use a known-to-be-reliable model, that fits in RAM (I went with quantized Gemma)&lt;/item&gt;
      &lt;item&gt;Use a simple prompt, in my case "What is 2+2?"&lt;list rend="ul"&gt;&lt;item&gt;To be really pedantic: the prompt was &lt;code&gt;&amp;lt;start_of_turn&amp;gt;user\nWhat is 2+2?&amp;lt;end_of_turn&amp;gt;\n&amp;lt;start_of_turn&amp;gt;model&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;To be really pedantic: the prompt was &lt;/item&gt;
      &lt;item&gt;Run everything with temperature set to &lt;code&gt;0.0&lt;/code&gt;‚Äì maybe that's enough to remove variability&lt;/item&gt;
      &lt;item&gt;Find the model implementation&lt;/item&gt;
      &lt;item&gt;Find where the model iterates through the layers and&lt;/item&gt;
      &lt;item&gt;Print out the MLXArray/Tensor with the values on each layer as the input goes through&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A few moments later and I find where I need to be. Added the breakpoints, added the logs and off to the races.&lt;/p&gt;
    &lt;p&gt;I run it on my iPhone 16 Pro Max. The model loads and the prompt is "What is 2+2?". The tensors start printing out, line after line after line. For once, the logs aren't complete gibberish ‚Äì they're numbers. Floating point values representing the model's internal state as it processes the input. I save the output to a file and do the same on my iPhone 15 Pro. Same model, same prompt, same code. Time to compare.&lt;/p&gt;
    &lt;head rend="h1"&gt;Welp, now it's definitely out of my expertise&lt;/head&gt;
    &lt;p&gt;I grep for a pattern I know should be consistent ‚Äì an array at log-line 58, right before the values get normalized/softmaxed. On a working device, I hypothesize this should be the same every time.&lt;lb/&gt;On the iPhone 15 Pro:&lt;code&gt;3: "[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]"&lt;/code&gt;&lt;lb/&gt;On the iPhone 16 Pro Max:&lt;code&gt;3: "[[[[191.5, 23.625, 173.75, ..., 1298, -147.25, -162.5]]]]"&lt;/code&gt;&lt;lb/&gt;Huh. Not close. Not at all. These values are orders of magnitude off. I double check the start of the logs and both phones show the same:&lt;code&gt;1: "array([[[0.162842, -0.162842, -0.48877, ..., -0.176636, 0.0001297, 0.088501],\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\n [-1.30957, 1.57324, -1.30957, ..., -0.0010376, -0.0010376, 1.12305],\n ...,\n [-0.348633, -2.78906, 0, ..., 0.84668, 0, -1.69336],\n [0.296875, 0.59375, 0.890625, ..., -0.59375, 0.296875, -0.890137],\n [1.02734, -0.616211, -0.616211, ..., -0.275879, -0.551758, 0.275879]]], dtype=float16)"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;OK, so the model receives the same thing as input, but at some point, the values start to go off. Like, way off. In order to make sure I'm not crazy, I do one last thing: run the same thing on my Mac. Make the app run on iPad compatibility mode and...&lt;code&gt;3: "[[[[53.875, 62.5625, -187.75, ..., 42.625, 6.25, -21.5625]]]]"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Bingo! Same as iPhone 15!&lt;/p&gt;
    &lt;p&gt;The model isn't broken. The code isn't broken. Most importantly, I'm not broken*. My phone is broken.&lt;lb/&gt;*arguable, but besides the point here&lt;/p&gt;
    &lt;head rend="h1"&gt;What's going on?&lt;/head&gt;
    &lt;p&gt;Let me explain what I think it's going on here: the iPhone 16 Pro Max contains Apple's A18 chip with its Neural Engine‚Äîa specialized accelerator for machine learning operations. MLX uses Metal to compile tensor operations for this accelerator. Somewhere in that stack, the computations are going very wrong. I don't think it's a widespread issue but, I do get disappointed that a relatively newly replaced iPhone from Apple Care came with such an issue.&lt;/p&gt;
    &lt;p&gt;However, if my Apple Intelligence troubles are related ‚Äì and they might as well be, I'd assume that code and MLX are not dissimilar in operations being done ‚Äì, it could be that all the 12 pages of users are users in a similar dillema, but without the means of debugging it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;I spent 3 days thinking I was incompetent. I blamed MiniMax. I blamed myself. The entire time, my $1,400 phone had a broken hardware. I could lose more time figuring out exactly what is wrong with it but it‚Äôs literally not worth my time.&lt;/p&gt;
    &lt;p&gt;I guess I can at least take a lesson that, when debugging, I should always consider the physical layer. I spent three days assuming this was a software problem ‚Äì my code, the library, the framework, my skills as a developer. The breakthrough was basically: "What if I'm not dumb and it's not my code?"&lt;/p&gt;
    &lt;p&gt;As for my phone: it'll probably go back to Apple, as a trade in for a new iPhone 17 Pro Max that hopefully ü§û can do math.&lt;/p&gt;
    &lt;head rend="h3"&gt;Update on Feb. 1st:&lt;/head&gt;
    &lt;p&gt;Well, now it's Feb. 1st and I have an iPhone 17 Pro Max to test with and... everything works as expected. So it's pretty safe to say that THAT specific instance of iPhone 16 Pro Max was hardware-defective.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46849258</guid><pubDate>Sun, 01 Feb 2026 20:51:56 +0000</pubDate></item><item><title>Defeating a 40-year-old copy protection dongle</title><link>https://dmitrybrant.com/2026/02/01/defeating-a-40-year-old-copy-protection-dongle</link><description>&lt;doc fingerprint="a99b5744e017df58"&gt;
  &lt;main&gt;
    &lt;p&gt;That‚Äôs right ‚Äî this little device is what stood between me and the ability to run an even older piece of software that I recently unearthed during an expedition of software archaeology.&lt;/p&gt;
    &lt;p&gt;For a bit more background, I was recently involved in helping a friend‚Äôs accounting firm to move away from using an extremely legacy software package that they had locked themselves into using for the last four decades.&lt;/p&gt;
    &lt;p&gt;This software was built using a programming language called RPG (‚ÄúReport Program Generator‚Äù), which is older than COBOL (!), and was used with IBM‚Äôs midrange computers such as the System/3, System/32, and all the way up to the AS/400. Apparently, RPG was subsequently ported to MS-DOS, so that the same software tools built with RPG could run on personal computers, which is how we ended up here.&lt;/p&gt;
    &lt;p&gt;This accounting firm was actually using a Windows 98 computer (yep, in 2026), and running the RPG software inside a DOS console window. And it turned out that, in order to run this software, it requires a special hardware copy-protection dongle to be attached to the computer‚Äôs parallel port! This was a relatively common practice in those days, particularly with ‚Äúenterprise‚Äù software vendors who wanted to protect their very important‚Ñ¢ software from unauthorized use.&lt;/p&gt;
    &lt;p&gt;Sadly, most of the text and markings on the dongle‚Äôs label has been worn or scratched off, but we can make out several clues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The words ‚ÄúStamford, CT‚Äù, and what‚Äôs very likely the logo of a company called ‚ÄúSoftware Security Inc‚Äù. The only evidence for the existence of this company is this record of them exhibiting their wares at SIGGRAPH conferences in the early 1990s, as well as several patents issued to them, relating to software protection.&lt;/item&gt;
      &lt;item&gt;A word that seems to say ‚ÄúRUNTIME‚Äù, which will become clear in a bit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My first course of action was to take a disk image of the Windows 98 PC that was running this software, and get it running in an emulator, so that we could see what the software actually does, and perhaps export the data from this software into a more modern format, to be used with modern accounting tools. But of course all of this requires the hardware dongle; none of the accounting tools seem to work without it plugged in.&lt;/p&gt;
    &lt;p&gt;Before doing anything, I looked through the disk image for any additional interesting clues, and found plenty of fascinating (and archaeologically significant?) stuff:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We‚Äôve got a compiler for the RPG II language (excellent!), made by a company called Software West Inc.&lt;/item&gt;
      &lt;item&gt;Even better, there are two versions of the RPG II compiler, released on various dates in the 1990s by Software West.&lt;/item&gt;
      &lt;item&gt;We‚Äôve got the complete source code of the accounting software, written in RPG. It looks like the full accounting package consists of numerous RPG modules, with a gnarly combination of DOS batch files for orchestrating them, all set up as a ‚Äúmenu‚Äù system for the user to navigate using number combinations. Clearly the author of this accounting system was originally an IBM mainframe programmer, and insisted on bringing those skills over to DOS, with mixed results.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I began by playing around with the RPG compiler in isolation, and I learned very quickly that it‚Äôs the RPG compiler itself that requires the hardware dongle, and then the compiler automatically injects the same copy-protection logic into any executables it generates. This explains the text that seems to say ‚ÄúRUNTIME‚Äù on the dongle.&lt;/p&gt;
    &lt;p&gt;The compiler consists of a few executable files, notably &lt;code&gt;RPGC.EXE&lt;/code&gt;, which is the compiler, and &lt;code&gt;SEU.EXE&lt;/code&gt;, which is a source editor (‚ÄúSource Entry Utility‚Äù). Here‚Äôs what we get when we launch SEU without the dongle, after a couple of seconds:&lt;/p&gt;
    &lt;p&gt;A bit rude, but this gives us an important clue: this program must be trying to communicate over the parallel port over the course of a few seconds (which could give us an opportunity to pause it for debugging, and see what it‚Äôs doing during that time), and then exits with a message (which we can now find in a disassembly of the program, and trace how it gets there).&lt;/p&gt;
    &lt;p&gt;A great tool for disassembling executables of this vintage is Reko. It understands 16-bit real mode executables, and even attempts to decompile them into readable C code that corresponds to the disassembly.&lt;/p&gt;
    &lt;p&gt;And so, looking at the decompiled/disassembled code in Reko, I expected to find &lt;code&gt;in&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; instructions, which would be the telltale sign of the program trying to communicate with the parallel port through the PC‚Äôs I/O ports. However‚Ä¶ I didn‚Äôt see an &lt;code&gt;in&lt;/code&gt; or &lt;code&gt;out&lt;/code&gt; instruction anywhere! But then I noticed something: Reko disassembled the executable into two ‚Äúsegments‚Äù: &lt;code&gt;0800&lt;/code&gt; and &lt;code&gt;0809&lt;/code&gt;, and I was only looking at segment &lt;code&gt;0809&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we look at segment &lt;code&gt;0800&lt;/code&gt;, we see the smoking gun: &lt;code&gt;in&lt;/code&gt; and &lt;code&gt;out&lt;/code&gt; instructions, meaning that the copy-protection routine is definitely here, and best of all, the entire code segment is a mere 0x90 bytes, which suggests that the entire routine should be pretty easy to unravel and understand. For some reason, Reko was not able to decompile this code into a C representation, but it still produced a disassembly, which will work just fine for our purposes. Maybe this was a primitive form of obfuscation from those early days, which is now confusing Reko and preventing it from associating this chunk of code with the rest of the program‚Ä¶ who knows.&lt;/p&gt;
    &lt;p&gt;Here is a GitHub Gist with the disassembly of this code, along with my annotations and notes. My x86 assembly knowledge is a little rusty, but here is the gist of what this code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It‚Äôs definitely a single self-contained routine, intended to be called using a ‚Äúfar‚Äù &lt;code&gt;CALL&lt;/code&gt;instruction, since it returns with a&lt;code&gt;RETF&lt;/code&gt;instruction.&lt;/item&gt;
      &lt;item&gt;It begins by detecting the address of the parallel port, by reading the BIOS data area. If the computer has more than one parallel port, the dongle must be connected to the first parallel port (LPT1).&lt;/item&gt;
      &lt;item&gt;It performs a loop where it writes values to the data register of the parallel port, and then reads the status register, and accumulates responses in the &lt;code&gt;BH&lt;/code&gt;and&lt;code&gt;BL&lt;/code&gt;registers.&lt;/item&gt;
      &lt;item&gt;At the end of the routine, the ‚Äúresult‚Äù of the whole procedure is stored in the &lt;code&gt;BX&lt;/code&gt;register (&lt;code&gt;BH&lt;/code&gt;and&lt;code&gt;BL&lt;/code&gt;together), which will presumably be ‚Äúverified‚Äù by the caller of the routine.&lt;/item&gt;
      &lt;item&gt;Very importantly, there doesn‚Äôt seem to be any ‚Äúinput‚Äù into this routine. It doesn‚Äôt pop anything from the stack, nor does it care about any register values passed into it. Which can only mean that the result of this routine is completely constant! No matter what complicated back-and-forth it does with the dongle, the result of this routine should always be the same.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With the knowledge that this routine must exit with some magic value stored in &lt;code&gt;BX&lt;/code&gt;, we can now patch the first few bytes of the routine to do just that! Not yet knowing which value to put in &lt;code&gt;BX&lt;/code&gt;, let‚Äôs start with 1234:&lt;/p&gt;
    &lt;code&gt;BB 34 12       MOV BX, 1234h
CB             RETF
&lt;/code&gt;
    &lt;p&gt;Only the first four bytes need patching ‚Äî set &lt;code&gt;BX&lt;/code&gt; to our desired value, and get out of there (&lt;code&gt;RETF&lt;/code&gt;). Running the patched executable with these new bytes still fails (expectedly) with the same message of ‚ÄúNo dongle, no edit‚Äù, but it fails immediately, instead of after several seconds of talking to the parallel port. Progress!&lt;/p&gt;
    &lt;p&gt;Stepping through the disassembly more closely, we get another major clue: The only value that &lt;code&gt;BH&lt;/code&gt; can be at the end of the routine is 76h (this is hard-coded into the routine). So, our total value for the magic number in &lt;code&gt;BX&lt;/code&gt; must be of the form 76xx. In other words, only the &lt;code&gt;BL&lt;/code&gt; value remains unknown:&lt;/p&gt;
    &lt;code&gt;BB __ 76       MOV BX, 76__h
CB             RETF
&lt;/code&gt;
    &lt;p&gt;Since &lt;code&gt;BL&lt;/code&gt; is an 8-bit register, it can only have 256 possible values. And what do we do when we have 256 combinations to try? Brute force it! I whipped up a script that plugs a value into that particular byte (from 0 to 255) and programmatically launches the executable in DosBox, and observes the output. Lo and behold, it worked! The brute forcing didn‚Äôt take long at all, because the correct number turned out to be‚Ä¶ 6. Meaning that the total magic number in &lt;code&gt;BX&lt;/code&gt; should be 7606h:&lt;/p&gt;
    &lt;code&gt;BB 06 76       MOV BX, 7606h
CB             RETF
&lt;/code&gt;
    &lt;p&gt;Bingo!&lt;lb/&gt; And then, proceeding to examine the other executable files in the compiler suite, the parallel port routine turns out to be exactly the same. All of the executables have the exact same copy protection logic, as if it was rubber-stamped onto them. In fact, when the compiler (&lt;code&gt;RPGC.EXE&lt;/code&gt;) compiles some RPG source code, it seems to copy the parallel port routine from itself into the compiled program. That‚Äôs right: the patched version of the compiler will produce executables with the same patched copy protection routine! Very convenient.&lt;/p&gt;
    &lt;p&gt;I must say, this copy protection mechanism seems a bit‚Ä¶ simplistic? A hardware dongle that just passes back a constant number? Defeatable with a four-byte patch? Is this really worthy of a patent? But who am I to pass judgment. It‚Äôs possible that I haven‚Äôt fully understood the logic, and the copy protection will somehow re-surface in another way. It‚Äôs also possible that the creators of the RPG compiler (Software West, Inc) didn‚Äôt take proper advantage of the hardware dongle, and used it in a way that is so easily bypassed.&lt;/p&gt;
    &lt;p&gt;In any case, Software West‚Äôs RPG II compiler is now free from the constraint of the parallel port dongle! And at some point soon, I‚Äôll work on purging any PII from the compiler directories, and make this compiler available as an artifact of computing history. It doesn‚Äôt seem to be available anywhere else on the web. If anyone reading this was associated with Software West Inc, feel free to get in touch ‚Äî I have many questions!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46849567</guid><pubDate>Sun, 01 Feb 2026 21:30:51 +0000</pubDate></item><item><title>Show HN: NanoClaw ‚Äì ‚ÄúClawdbot‚Äù in 500 lines of TS with Apple container isolation</title><link>https://github.com/gavrielc/nanoclaw</link><description>&lt;doc fingerprint="1f0c5d761f974ce8"&gt;
  &lt;main&gt;
    &lt;p&gt;My personal Claude assistant that runs securely in containers. Lightweight and built to be understood and customized for your own needs.&lt;/p&gt;
    &lt;p&gt;OpenClaw is an impressive project with a great vision. But I can't sleep well running software I don't understand with access to my life. OpenClaw has 52+ modules, 8 config management files, 45+ dependencies, and abstractions for 15 channel providers. Security is application-level (allowlists, pairing codes) rather than OS isolation. Everything runs in one Node process with shared memory.&lt;/p&gt;
    &lt;p&gt;NanoClaw gives you the same core functionality in a codebase you can understand in 8 minutes. One process. A handful of files. Agents run in actual Linux containers with filesystem isolation, not behind permission checks.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/gavrielc/nanoclaw.git
cd nanoclaw
claude&lt;/code&gt;
    &lt;p&gt;Then run &lt;code&gt;/setup&lt;/code&gt;. Claude Code handles everything: dependencies, authentication, container setup, service configuration.&lt;/p&gt;
    &lt;p&gt;Small enough to understand. One process, a few source files. No microservices, no message queues, no abstraction layers. Have Claude Code walk you through it.&lt;/p&gt;
    &lt;p&gt;Secure by isolation. Agents run in Linux containers (Apple Container on macOS, or Docker). They can only see what's explicitly mounted. Bash access is safe because commands run inside the container, not on your host.&lt;/p&gt;
    &lt;p&gt;Built for one user. This isn't a framework. It's working software that fits my exact needs. You fork it and have Claude Code make it match your exact needs.&lt;/p&gt;
    &lt;p&gt;Customization = code changes. No configuration sprawl. Want different behavior? Modify the code. The codebase is small enough that this is safe.&lt;/p&gt;
    &lt;p&gt;AI-native. No installation wizard; Claude Code guides setup. No monitoring dashboard; ask Claude what's happening. No debugging tools; describe the problem, Claude fixes it.&lt;/p&gt;
    &lt;p&gt;Skills over features. Contributors shouldn't add features (e.g. support for Telegram) to the codebase. Instead, they contribute claude code skills like &lt;code&gt;/add-telegram&lt;/code&gt; that transform your fork. You end up with clean code that does exactly what you need.&lt;/p&gt;
    &lt;p&gt;Best harness, best model. This runs on Claude Agent SDK, which means you're running Claude Code directly. The harness matters. A bad harness makes even smart models seem dumb, a good harness gives them superpowers. Claude Code is (IMO) the best harness available.&lt;/p&gt;
    &lt;p&gt;No ToS gray areas. Because it uses Claude Agent SDK natively with no hacks or workarounds, using your subscription with your auth token is completely legitimate (I think). No risk of being shut down for terms of service violations (I am not a lawyer).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WhatsApp I/O - Message Claude from your phone&lt;/item&gt;
      &lt;item&gt;Isolated group context - Each group has its own &lt;code&gt;CLAUDE.md&lt;/code&gt;memory, isolated filesystem, and runs in its own container sandbox with only that filesystem mounted&lt;/item&gt;
      &lt;item&gt;Main channel - Your private channel (self-chat) for admin control; every other group is completely isolated&lt;/item&gt;
      &lt;item&gt;Scheduled tasks - Recurring jobs that run Claude and can message you back&lt;/item&gt;
      &lt;item&gt;Web access - Search and fetch content&lt;/item&gt;
      &lt;item&gt;Container isolation - Agents sandboxed in Apple Container (macOS) or Docker (macOS/Linux)&lt;/item&gt;
      &lt;item&gt;Optional integrations - Add Gmail (&lt;code&gt;/add-gmail&lt;/code&gt;) and more via skills&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Talk to your assistant with the trigger word (default: &lt;code&gt;@Andy&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;@Andy send an overview of the sales pipeline every weekday morning at 9am (has access to my Obsidian vault folder)
@Andy review the git history for the past week each Friday and update the README if there's drift
@Andy every Monday at 8am, compile news on AI developments from Hacker News and TechCrunch and message me a briefing
&lt;/code&gt;
    &lt;p&gt;From the main channel (your self-chat), you can manage groups and tasks:&lt;/p&gt;
    &lt;code&gt;@Andy list all scheduled tasks across groups
@Andy pause the Monday briefing task
@Andy join the Family Chat group
&lt;/code&gt;
    &lt;p&gt;There are no configuration files to learn. Just tell Claude Code what you want:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Change the trigger word to @Bob"&lt;/item&gt;
      &lt;item&gt;"Remember in the future to make responses shorter and more direct"&lt;/item&gt;
      &lt;item&gt;"Add a custom greeting when I say good morning"&lt;/item&gt;
      &lt;item&gt;"Store conversation summaries weekly"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Or run &lt;code&gt;/customize&lt;/code&gt; for guided changes.&lt;/p&gt;
    &lt;p&gt;The codebase is small enough that Claude can safely modify it.&lt;/p&gt;
    &lt;p&gt;Don't add features. Add skills.&lt;/p&gt;
    &lt;p&gt;If you want to add Telegram support, don't create a PR that adds Telegram alongside WhatsApp. Instead, contribute a skill file (&lt;code&gt;.claude/skills/add-telegram/SKILL.md&lt;/code&gt;) that teaches Claude Code how to transform a NanoClaw installation to use Telegram.&lt;/p&gt;
    &lt;p&gt;Users then run &lt;code&gt;/add-telegram&lt;/code&gt; on their fork and get clean code that does exactly what they need, not a bloated system trying to support every use case.&lt;/p&gt;
    &lt;p&gt;Skills we'd love to see:&lt;/p&gt;
    &lt;p&gt;Communication Channels&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/add-telegram&lt;/code&gt;- Add Telegram as channel. Should give the user option to replace WhatsApp or add as additional channel. Also should be possible to add it as a control channel (where it can trigger actions) or just a channel that can be used in actions triggered elsewhere&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/add-slack&lt;/code&gt;- Add Slack&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/add-discord&lt;/code&gt;- Add Discord&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platform Support&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/setup-windows&lt;/code&gt;- Windows via WSL2 + Docker&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Session Management&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;/add-clear&lt;/code&gt;- Add a&lt;code&gt;/clear&lt;/code&gt;command that compacts the conversation (summarizes context while preserving critical information in the same session). Requires figuring out how to trigger compaction programmatically via the Claude Agent SDK.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS or Linux&lt;/item&gt;
      &lt;item&gt;Node.js 20+&lt;/item&gt;
      &lt;item&gt;Claude Code&lt;/item&gt;
      &lt;item&gt;Apple Container (macOS) or Docker (macOS/Linux)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;WhatsApp (baileys) --&amp;gt; SQLite --&amp;gt; Polling loop --&amp;gt; Container (Claude Agent SDK) --&amp;gt; Response
&lt;/code&gt;
    &lt;p&gt;Single Node.js process. Agents execute in isolated Linux containers with mounted directories. IPC via filesystem. No daemons, no queues, no complexity.&lt;/p&gt;
    &lt;p&gt;Key files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/index.ts&lt;/code&gt;- Main app: WhatsApp connection, routing, IPC&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/container-runner.ts&lt;/code&gt;- Spawns agent containers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/task-scheduler.ts&lt;/code&gt;- Runs scheduled tasks&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/db.ts&lt;/code&gt;- SQLite operations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;groups/*/CLAUDE.md&lt;/code&gt;- Per-group memory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why WhatsApp and not Telegram/Signal/etc?&lt;/p&gt;
    &lt;p&gt;Because I use WhatsApp. Fork it and run a skill to change it. That's the whole point.&lt;/p&gt;
    &lt;p&gt;Why Apple Container instead of Docker?&lt;/p&gt;
    &lt;p&gt;On macOS, Apple Container is lightweight, fast, and optimized for Apple silicon. But Docker is also fully supported‚Äîduring &lt;code&gt;/setup&lt;/code&gt;, you can choose which runtime to use. On Linux, Docker is used automatically.&lt;/p&gt;
    &lt;p&gt;Can I run this on Linux?&lt;/p&gt;
    &lt;p&gt;Yes. Run &lt;code&gt;/setup&lt;/code&gt; and it will automatically configure Docker as the container runtime. Thanks to @dotsetgreg for contributing the &lt;code&gt;/convert-to-docker&lt;/code&gt; skill.&lt;/p&gt;
    &lt;p&gt;Is this secure?&lt;/p&gt;
    &lt;p&gt;Agents run in containers, not behind application-level permission checks. They can only access explicitly mounted directories. You should still review what you're running, but the codebase is small enough that you actually can. See docs/SECURITY.md for the full security model.&lt;/p&gt;
    &lt;p&gt;Why no configuration files?&lt;/p&gt;
    &lt;p&gt;We don't want configuration sprawl. Every user should customize it to so that the code matches exactly what they want rather than configuring a generic system. If you like having config files, tell Claude to add them.&lt;/p&gt;
    &lt;p&gt;How do I debug issues?&lt;/p&gt;
    &lt;p&gt;Ask Claude Code. "Why isn't the scheduler running?" "What's in the recent logs?" "Why did this message not get a response?" That's the AI-native approach.&lt;/p&gt;
    &lt;p&gt;Why isn't the setup working for me?&lt;/p&gt;
    &lt;p&gt;I don't know. Run &lt;code&gt;claude&lt;/code&gt;, then run &lt;code&gt;/debug&lt;/code&gt;. If claude finds an issue that is likely affecting other users, open a PR to modify the setup SKILL.md.&lt;/p&gt;
    &lt;p&gt;What changes will be accepted into the codebase?&lt;/p&gt;
    &lt;p&gt;Security fixes, bug fixes, and clear improvements to the base configuration. That's it.&lt;/p&gt;
    &lt;p&gt;Everything else (new capabilities, OS compatibility, hardware support, enhancements) should be contributed as skills.&lt;/p&gt;
    &lt;p&gt;This keeps the base system minimal and lets every user customize their installation without inheriting features they don't want.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46850205</guid><pubDate>Sun, 01 Feb 2026 22:49:22 +0000</pubDate></item><item><title>Show HN: Wikipedia as a doomscrollable social media feed</title><link>https://xikipedia.org</link><description>&lt;doc fingerprint="d4d05bb2748febb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;p&gt;Xikipedia is a pseudo social media feed that algorithmically shows you content from Simple Wikipedia. It is made as a demonstration of how even a basic non-ML algorithm with no data from other users can quickly learn what you engage with to suggest you more similar content. No data is collected or shared here, the algorithm runs locally and the data disappears once you refresh or close the tab.&lt;/p&gt;
    &lt;p&gt;Source code on GitHub, discuss on fedi, bluesky, or twitter.&lt;/p&gt;
    &lt;p&gt;Pick some categories to get started (optional)&lt;/p&gt;
    &lt;p&gt;Or add your own&lt;/p&gt;
    &lt;p&gt;Since the content and images shown is from random Wikipedia articles, you will likely see NSFW content. Please only continue if you're an adult.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46850803</guid><pubDate>Mon, 02 Feb 2026 00:12:19 +0000</pubDate></item><item><title>Apple's MacBook Pro DFU port documentation is wrong</title><link>https://lapcatsoftware.com/articles/2026/2/1.html</link><description>&lt;doc fingerprint="362448954b27b1fd"&gt;
  &lt;main&gt;
    &lt;p&gt;According to the Apple support document How to identify the DFU port on Mac, the DFU (device firmware update) port location for MacBook Pro models with Apple silicon is as follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;14-inch MacBook Pro with M4 or M5 chip: The rightmost USB-C port when you√¢re facing the left side of the Mac&lt;/p&gt;
      &lt;p&gt;All other models: The leftmost USB-C port when you√¢re facing the left side of the Mac&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is wrong, a discovery that took me about a half dozen attempts to update macOS on an external disk. I have a 16-inch MacBook Pro with an M4 chip, specifically an M4 Pro chip, and the DFU port seems to be the USB-C port on the right side of the Mac, not on the left side.&lt;/p&gt;
    &lt;p&gt;For some damn reason, it matters which port your external disk is plugged into when you install or update macOS, as described by the Apple support document How to use an external storage device as a Mac startup disk:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Make sure that your storage device is plugged into the appropriate port on your Mac.&lt;/p&gt;
      &lt;p&gt;If you're using a Mac with Apple silicon, plug your storage device into any compatible port except the DFU port. Learn how to identify the DFU port. After macOS installation is complete, you can connect your storage device to any compatible port, including the DFU port.&lt;/p&gt;
      &lt;p&gt;If you√¢re using any other Mac, plug your storage device into any compatible port.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Mac disk management was so much easier in the days of Intel and PowerPC!&lt;/p&gt;
    &lt;p&gt;On an external SSD I had installed a macOS Sequoia boot volume (among others), which I√¢ve used to take Mac App Store screenshots for my apps and which I√¢d now like to use to take a screen recording. The installed version was still macOS 15.2, because I don√¢t often boot into the disk to take new screenshots, and going through the software update process would occupy my MacBook Pro for annoyingly long. However, it appears that Safari 26 requires a version of macOS 15 higher than .2, so I needed to update macOS in order to update Safari from version 18.&lt;/p&gt;
    &lt;p&gt;Over the past few days, every attempt I made to update the disk volume to macOS 15.7.3 failed inexplicably. I tried both Software Update in System Settings and the &lt;code&gt;softwareupdate&lt;/code&gt; command-line tool in Terminal. They went through all the motions, downloading the entire update, rebooting, etc., but afterwards I always ended up right where I started, at macOS 15.2. The &lt;code&gt;softwareupdate&lt;/code&gt; tool gave no error message. I did eventually see the following (truncated) notification:&lt;/p&gt;
    &lt;p&gt;Nonetheless, the so-called √¢Details√¢ button presented no actual details, simply opening Software Update again in System Settings. At no point did it ever say, hey, plug your disk into a different port!&lt;/p&gt;
    &lt;p&gt;While searching for a solution to my problem, I found an article by Michael Tsai, Failed Software Update on the External Drive of an Apple Silicon Mac. It described something that I also saw in my testing:&lt;/p&gt;
    &lt;quote&gt;I happened to boot into macOS Recovery and look in the Startup Security Utility, and I saw that it did not have access to change the security policy for the external drive. In order to do that, it said I had to set the drive as the startup disk. This kind of didn√¢t make sense because don√¢t the security options get set when booted from Recovery?&lt;/quote&gt;
    &lt;p&gt;I followed Tsai√¢s instructions, which did allow me to change the security policy for the external drive.&lt;/p&gt;
    &lt;quote&gt;I don√¢t know why software update couldn√¢t tell me this or why there is seemingly no direct GUI command to view or edit the authorized users. But restarting from within Startup Disk is apparently the way to get macOS to offer to fix the LocalPolicy. Once I added the user, I was able to do a normal boot from the external drive and software update normally.&lt;/quote&gt;
    &lt;p&gt;I also thought this would solve my macOS update problem, but it didn√¢t. In retrospect, though, perhaps I needed both solutions, to fix the LocalPolicy and to change the ports.&lt;/p&gt;
    &lt;p&gt;I was about to surrender to despair when I discovered a second article by Michael Tsai, Failing to Finish Updating macOS on an External Disk, published soon after the first article:&lt;/p&gt;
    &lt;quote&gt;With the final release of macOS 15.5, the problem got worse, and the Startup Disk workaround no longer helps.&lt;/quote&gt;
    &lt;p&gt;Tsai ultimately hits on the solution:&lt;/p&gt;
    &lt;quote&gt;The problem ended up being that I had plugged the external drive into the wrong USB-C port (the DFU port).&lt;/quote&gt;
    &lt;p&gt;Sure enough, after plugging my disk into a port on the left side of my MacBook Pro, software update succeeded on the first attempt. Every previous, failed attempt used the port on the right side, which was physically more convenient on the desk. So after all that, the external disk is finally updated to macOS 15.7.3 now.&lt;/p&gt;
    &lt;p&gt;Tsai offers the same complaint about this absurd situation:&lt;/p&gt;
    &lt;quote&gt;I don√¢t know why macOS can√¢t just report an error when you use the wrong port instead of proceeding to install for an hour and then not report an error but not work, either.&lt;/quote&gt;
    &lt;p&gt;By the way, Software Update in System Settings allowed my Mac to go to sleep during the √¢Preparing√¢ phase, despite the fact that the battery was charged to 99%, so when I returned home from a workout I unhappily found 30 minutes remaining. Sigh. Whatever happened to √¢it just works√¢?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46852096</guid><pubDate>Mon, 02 Feb 2026 03:29:55 +0000</pubDate></item><item><title>Library of Juggling</title><link>https://libraryofjuggling.com/</link><description>&lt;doc fingerprint="3736594b20b5a3ff"&gt;
  &lt;main&gt;
    &lt;p&gt;What is it?&lt;/p&gt;
    &lt;p&gt;The Library of Juggling is an attempt to list all of the popular (and perhaps not so popular) juggling tricks in one organized place. Despite the growing popularity of juggling, few websites are dedicated to collecting and archiving the various patterns that are being performed. Most jugglers are familiar with iconic tricks such as the Cascade and Shower, but what about Romeo's Revenge or the 531 Mills Mess? The goal of this website is to guarantee that the tricks currently circulating around the internet and at juggling conventions are found, animated, and catalogued for the world to see. It is a daunting task, but for the sake of jugglers everywhere it must be done.&lt;/p&gt;
    &lt;p&gt;What can I find here?&lt;/p&gt;
    &lt;p&gt;For every trick found in the Library, there will be an animated representation of the pattern created via JugglingLab, in addition to general information about the trick (siteswap, difficulty level, prerequisite tricks, etc.). If I am able to run the pattern, then I will provide a text-based tutorial for the trick with the help of animations. I will also include links to other tutorials for the trick that can be found online, ranging from YouTube videos to private sites like this one. If I am unable to provide my own tutorial, there will still be a short description of the trick in addition to outside tutorials and demonstrations.&lt;/p&gt;
    &lt;p&gt;Where do I start?&lt;/p&gt;
    &lt;p&gt;Well, if you have come to the Library looking to find out how to start juggling, than it would be best to begin with the Three Ball Cascade pattern. If you are a juggler who is already familiar with the basics, then the various tricks included in the Library can be accessed via the navigation tree on the left, or you can click here to view all of the tricks by difficulty.&lt;/p&gt;
    &lt;p&gt;Recent Additions (6/13/15):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frostbite (submitted by Andrew Olson)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Library of Juggling is on an indefinite hiatus, which means no new tricks will be added. Existing content will continue to be hosted for the foreseeable future.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46853552</guid><pubDate>Mon, 02 Feb 2026 07:58:49 +0000</pubDate></item><item><title>EU launches government satcom program in sovereignty push</title><link>https://spacenews.com/eu-launches-government-satcom-program-in-sovereignty-push/</link><description>&lt;doc fingerprint="95231d6d282ee8d"&gt;
  &lt;main&gt;
    &lt;p&gt;BRUSSELS ‚Äî The European Union‚Äôs new government satellite communications program, GOVSATCOM, which pools capacity from eight already on-orbit geosynchronous satellites, began operations last week, European Commissioner for Defence and Space Andrius Kubilius said Jan. 27.&lt;/p&gt;
    &lt;p&gt;The program is designed to provide secure communications capabilities to the EU and its member states and could expand by 2027, Kubilius said.&lt;/p&gt;
    &lt;p&gt;‚ÄúLast week we started GOVSATCOM operations,‚Äù Kubilius said during his opening remarks at the European Space Conference. ‚ÄúThat means that all member states can now have access to sovereign satellite communications ‚Äî military and government, secure and resilient, built in Europe, operated in Europe, and under European control.‚Äù&lt;/p&gt;
    &lt;p&gt;Specifically, Kubilius was referring to the GOVSATCOM hub, a ‚Äúmarketplace‚Äù of governmental capacities from the five resource providers currently enrolled in the program. Juan Ramon Lopez Caravantes, head of communication at the European Union Agency for the Space Programme, said that with ‚Äùa few clicks member states can now introduce their service request. It‚Äôs an easy to use, smooth running secure platform‚Äù.&lt;/p&gt;
    &lt;p&gt;GOVSATCOM is conceived as a ‚Äúsystem of systems,‚Äù merging existing national and commercial satellite capacities into a common EU pool. The program is structured in multiple phases, and is pooling capacity from eight existing, already-in-orbit GEO satellites from five member states ‚Äî France, Spain, Italy, Greece and Luxembourg.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey currently offer 35 different service programs from a catalogue that is not public. It‚Äôs only for the member states, and it‚Äôs fully secured and encrypted‚Äù added Jeremie Godet, head of unit secure connectivity and space surveillance at European Commission‚Äôs Directorate General office for defence industry and space. ‚ÄúThe coverage is currently from the south of Greenland to South America on the west and up to India on the East.‚Äù&lt;/p&gt;
    &lt;p&gt;In 2027 this catalog will expand, Godet added, to fill gaps and to secure more commercial satcom solutions.&lt;/p&gt;
    &lt;p&gt;Beginning in 2029, GOVSATCOM is expected to integrate with the 290 satellites in the Infrastructure for Resilience, Interconnectivity and Security by Satellite constellation, known as IRIS¬≤, and be fully operational.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe goal‚Äù, Kubilius said during the conference ‚Äúis to have expanded commercial capabilities operational by 2027, including expanding coverage, and expanding bandwidth to cover the entire world,‚Äù and to support IRIS¬≤ by 2029.&lt;/p&gt;
    &lt;p&gt;Concerning IRIS2, the commissioner also said that IRIS¬≤ Ka-band military frequencies were brought into use last week. He expressed confidence that the first batch of satellites will be ready for deployment by 2029. ‚ÄúI have asked all partners to step up and speed up IRIS¬≤,‚Äúand I‚Äôm confident we can deploy its initial services by 2029,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe goal is connectivity and security for all of Europe ‚Äî guaranteed access for all member states and full European control.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46853888</guid><pubDate>Mon, 02 Feb 2026 08:59:40 +0000</pubDate></item><item><title>My fast zero-allocation webserver using OxCaml</title><link>https://anil.recoil.org/notes/oxcaml-httpz</link><description>&lt;doc fingerprint="16bf138b60e4e358"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;My (very) fast zero-allocation webserver using OxCaml / Feb 2026 / DOI&lt;/head&gt;
    &lt;p&gt;Since helping with the &lt;/p&gt;
    &lt;p&gt;The reason for my eagerness is that OxCaml has a number of language extensions that give giant leaps in performance for systems-oriented programs, while retaining the familiar OCaml functional style of programming. And unlike Rust, there's a garbage collector available for 'normal' code. I am also deeply sick and tired of maintaining large Python scripts recently, and crave the modularity and type safety of OCaml.&lt;/p&gt;
    &lt;p&gt;The traditional way I learn a new technology is by replacing my &lt;/p&gt;
    &lt;p&gt;(Many thanks to &lt;/p&gt;
    &lt;head rend="h2"&gt;Why Zero Allocation for HTTP/1.1?&lt;/head&gt;
    &lt;p&gt;httpz is a high-performance HTTP/1.1 parser that aims to have no major heap allocation, and very minimal minor heap allocation, by using OxCaml's unboxed types and local allocations.&lt;/p&gt;
    &lt;p&gt;Why is this useful? It means that the entire lifetime of an HTTP connection can be handled in the callstack alone, so freeing up a connection is just a matter of returning from the function that handles it. In the steady state, a webserver would have almost no garbage collector activity. When combined with &lt;/p&gt;
    &lt;p&gt;I decided to specialise this library for HTTP/1.1 for now, and so settled on the input being a simple 32KB bytes value. This represents an HTTP request with the header portion (HTTP body handling is relatively straightforward for POST requests, and not covered in this post).&lt;/p&gt;
    &lt;p&gt;Given an input buffer like this, what can we do with OxCaml vs vanilla OCaml to make this go fast?&lt;/p&gt;
    &lt;head rend="h3"&gt;Unboxed Types and Records&lt;/head&gt;
    &lt;p&gt;The first port of call is to figure out the core types we're going to use for our parser. If you need to get familiar with OCaml's upstream memory representation then head over to Real World OCaml.&lt;/p&gt;
    &lt;p&gt;In my usual OCaml code, I use libraries like cstruct that I &lt;/p&gt;
    &lt;code&gt;type buffer = (char, Bigarray.int8_unsigned_elt, Bigarray.c_layout) Bigarray.Array1.t
type Cstruct.t = private {
  buffer: buffer;
  off   : int;
  len   : int;
}
&lt;/code&gt;
    &lt;p&gt;The idea is to use the record to get narrow views into a larger buffer, and that these small views can just live on the minor heap of the runtime which is fast to collect. OxCaml advances this by providing unboxed versions of small numbers that live in registers or on the stack, via a new syntax &lt;code&gt;int16#&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Instead of Bigarrays, we're now going to switch to use &lt;code&gt;bytes&lt;/code&gt; instead, but the
basic idea is the same.  Since httpz's buffer is a max of 32KB, 16-bit integers
also suffice for all positions and lengths!&lt;/p&gt;
    &lt;code&gt;type Httpz.t = #{ off : int16# ; len : int16# }
&lt;/code&gt;
    &lt;p&gt;There are actually two new features here: the first is that records can be unboxed with the &lt;code&gt;#{}&lt;/code&gt;
syntax, and the contents themselves are of a smaller width.  Let's have a closer look
at the difference between the Cstruct boxed version and this new OxCaml one:&lt;/p&gt;
    &lt;head rend="h4"&gt;Inspect unboxing in utop&lt;/head&gt;
    &lt;p&gt;My first port-of-call is usually to use utop interactively to poke around using the &lt;code&gt;Obj&lt;/code&gt; module.  This isn't quite so easy in OxCaml since the unboxed
records use a special layout:&lt;/p&gt;
    &lt;code&gt;# type t = #{ off : int16# ; len : int16# };;
type t = #{ off : int16#; len : int16#; }

# let x = #{ off=#1S; len=#2S };;
val x : t = #{off = &amp;lt;abstr&amp;gt;; len = &amp;lt;abstr&amp;gt;}

# Obj.repr x;;
Error: This expression has type t but an expression was expected of type
         ('a : value)
       The layout of t is bits16 &amp;amp; bits16
         because of the definition of t at line 1, characters 0-41.
       But the layout of t must be a sublayout of value.

&lt;/code&gt;
    &lt;p&gt;That failed, but it did reveal that we have this intriguing int16 pair layout instead of the normal OCaml flat value representation! Let's use the compiler to figure this out...&lt;/p&gt;
    &lt;head rend="h4"&gt;Inspect unboxing in lambda&lt;/head&gt;
    &lt;p&gt;I next built a small test program and inspected the lambda intermediate language from the compiler. To avoid dependencies, I just bound the raw compiler internals directly by checking out the oxcaml source code.&lt;/p&gt;
    &lt;code&gt;external add_int16 : int16# -&amp;gt; int16# -&amp;gt; int16# = "%int16#_add"
external int16_to_int : int16# -&amp;gt; int = "%int_of_int16#"

type span = #{ off : int16#; len : int16# }

let[@inline never] add_spans (x : span) (y : span) : span =
  #{ off = add_int16 x.#off y.#off; len = add_int16 x.#len y.#len }

let () =
  let x = Sys.opaque_identity #{ off = #1S; len = #2S } in
  let y = Sys.opaque_identity #{ off = #100S; len = #200S } in
  let z = add_spans x y in
  Printf.printf "off=%d len=%d\n" (int16_to_int z.#off) (int16_to_int z.#len)
&lt;/code&gt;
    &lt;p&gt;This introduces enough compiler optimisation barriers such that the addition is not optimised away at compile time. We can compile this with &lt;code&gt;ocaml -dlambda src.ml&lt;/code&gt; and see the intermediate form after type checking:&lt;/p&gt;
    &lt;code&gt;(let
  (add_spans/290 =
     (function {nlocal = 0} x/292[#(int16, int16)] y/293[#(int16, int16)]
       never_inline : #(int16, int16)
       (funct-body add_spans ./x.ml(6)&amp;lt;ghost&amp;gt;:196-294
         (before add_spans ./x.ml(7):229-294
           (make_unboxed_product #(int16, int16)
             (%int16#_add (unboxed_product_field 0 #(int16, int16) x/292)
               (unboxed_product_field 0 #(int16, int16) y/293))
             (%int16#_add (unboxed_product_field 1 #(int16, int16) x/292)
               (unboxed_product_field 1 #(int16, int16) y/293)))))))
&lt;/code&gt;
    &lt;p&gt;You can see the unboxing propagating nicely here through the intermediate code!&lt;/p&gt;
    &lt;head rend="h4"&gt;Inspect unboxing in native code&lt;/head&gt;
    &lt;p&gt;The next step is to verify what this looks like when compiled as optimised native code. I used &lt;code&gt;ocamlopt -O3 -S&lt;/code&gt; on my arm64 machine which emits the assembly code
after all the compiler passes, and found:&lt;/p&gt;
    &lt;code&gt;In the entry point:
  orr   x0, xzr, #1      ; x.#off = 1
  orr   x1, xzr, #2      ; x.#len = 2
  movz  x2, #100, lsl #0 ; y.#off = 100
  movz  x3, #200, lsl #0 ; y.#len = 200
  bl    _camlX__add_spans_0_1_code

_camlX__add_spans_0_1_code:
  add   x1, x1, x3       ; len: x.#len + y.#len
  sbfm  x1, x1, #0, #15  ; sign-extend to 16 bits (int16# semantics)
  add   x0, x0, x2       ; off: x.#off + y.#off
  sbfm  x0, x0, #0, #15  ; sign-extend to 16 bits
  ret

&lt;/code&gt;
    &lt;p&gt;We can see from the assembly that there's no boxing, and no heap allocations, and the sbfm instruction maintains the 16-bit semantics via sign extension.&lt;/p&gt;
    &lt;p&gt;Let's double check that the normal boxed OCaml does do more work and that isn't just the flambda2 compiler doing its magic. Here's a boxed version of the benchmark using plain OCaml:&lt;/p&gt;
    &lt;code&gt;type span = { off : int; len : int }

let[@inline never] add_spans (x : span) (y : span) : span =
  { off = x.off + y.off; len = x.len + y.len }

let () =
  let x = Sys.opaque_identity { off = 1; len = 2 } in
  let y = Sys.opaque_identity { off = 100; len = 200 } in
  let z = add_spans x y in
  Printf.printf "off=%d len=%d\n" z.off z.len
&lt;/code&gt;
    &lt;p&gt;Compiling this boxed version with &lt;code&gt;ocamlopt -O3 -S&lt;/code&gt; and looking at the assembly shows
much more minor heap activity:&lt;/p&gt;
    &lt;code&gt;_camlY__add_spans_0_1_code:
      sub   sp, sp, #16
      str   x30, [sp, #8]
      mov   x2, x0
      ldr   x16, [x28, #0]        ; load young_limit
      sub   x27, x27, #24         ; bump allocator: reserve 24 bytes (3 words)
      cmp   x27, x16              ; check if GC needed
      b.cc  L114                  ; branch to GC if out of space
  L113:
      add   x0, x27, #8           ; x0 = pointer to new block
      orr   x3, xzr, #2048        ; header word (tag 0, size 2)
      str   x3, [x0, #-8]         ; write header
      ldr   x3, [x1, #0]          ; load y.off from heap
      ldr   x4, [x2, #0]          ; load x.off from heap
      add   x3, x4, x3            ; add them
      sub   x3, x3, #1            ; adjust for tagged int
      str   x3, [x0, #0]          ; store result.off to heap
      ldr   x1, [x1, #8]          ; load y.len from heap
      ldr   x2, [x2, #8]          ; load x.len from heap
      add   x1, x2, x1            ; add them
      sub   x1, x1, #1            ; adjust for tagged int
      str   x1, [x0, #8]          ; store result.len to heap
      ...
      ret
  L114:
      bl    _caml_call_gc         ; GC call if needed
&lt;/code&gt;
    &lt;p&gt;The OCaml minor heap is really fast, but it's nowhere near as fast as just passing values around in registers and doing direct operations, which the unboxed version lets us do!&lt;/p&gt;
    &lt;p&gt;My benchmark above used direct external calls to compiler primitives, but OxCaml exposes normal modules for all these special types so we can just open them and gain access to the usual integer operations:&lt;/p&gt;
    &lt;code&gt;module I16 = Stdlib_stable.Int16_u

let[@inline always] i16 x = I16.of_int x
let[@inline always] to_int x = I16.to_int x

let pos : int16# = i16 0
let next : int16# = I16.add pos #1S
&lt;/code&gt;
    &lt;head rend="h3"&gt;Unboxed characters&lt;/head&gt;
    &lt;p&gt;There's more than just integer operations in OxCaml. Hot off the press in the past few weeks have been unboxed character operations as well, so we don't need to use an OCaml int (this is unboxed as well, but I presume the compiler can optimise and pack 8-bit operations much more effectively if it knows that we're operating on a char instead of a full word).&lt;/p&gt;
    &lt;p&gt;The httpz parser tries to use these, but the support for untagged ints isn't fully complete yet (thanks &lt;/p&gt;
    &lt;p&gt;HTTP date timestamps use unboxed floats as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning unboxed records and tuples&lt;/head&gt;
    &lt;p&gt;Once we've declared these unboxed records, they're fully nestable within other unboxed records. For example, HTTP requests with multiple fields remain unboxed:&lt;/p&gt;
    &lt;code&gt;type request =
  #{ meth : method_
   ; target : span           (* Nested unboxed record *)
   ; version : version
   ; body_off : int16#
   ; content_length : int64#
   ; is_chunked : bool
   ; keep_alive : bool
   ; expect_continue: bool
   }
&lt;/code&gt;
    &lt;p&gt;Functions can therefore naturally return multiple values without allocation by using unboxed tuples in the return value of a function:&lt;/p&gt;
    &lt;code&gt;let take_while predicate buf ~(pos : int16#) ~(len : int16#)
    : #(span * int16#) =
  let start = pos in
  let mutable p = pos in
  while (* ... *) do p &amp;lt;- I16.add p #1S done;
  #(#{ off = start; len = I16.sub p start }, p)

let #(result_span, new_pos) = take_while is_token buf ~pos ~len
&lt;/code&gt;
    &lt;p&gt;Vanilla OCaml did some unboxing of this use of tuples, but not with records (which would land up on the minor heap). With this OxCaml code, it's all just passed directly on the stack through function call traces.&lt;/p&gt;
    &lt;head rend="h3"&gt;Local allocations and exclaves&lt;/head&gt;
    &lt;p&gt;We can then also mark parameters to demand that they won't escape a function, enabling stack allocation more explicitly:&lt;/p&gt;
    &lt;code&gt;(* Buffer is borrowed, won't be stored anywhere *)
let[@inline] equal (local_ buf) (sp : span) (s : string) : bool =
  let sp_len = I16.to_int sp.#len in
  if sp_len &amp;lt;&amp;gt; String.length s then false
  else Bigstring.memcmp_string buf ~pos:(I16.to_int sp.#off) s = 0
&lt;/code&gt;
    &lt;p&gt;If a function needs to return a local value, then it uses a new &lt;code&gt;exclave_&lt;/code&gt; keyword. For example, in the HTTP request parsing we look up a stack allocated list of headers:&lt;/p&gt;
    &lt;code&gt;val find : t list @ local -&amp;gt; Name.t -&amp;gt; t option @ local

let rec find_string (buf : bytes) (headers : t list @ local) name = exclave_
  match headers with
  | [] -&amp;gt; None
  | hdr :: rest -&amp;gt;
    let matches =
      match hdr.name with
      | Name.Other -&amp;gt; Span.equal_caseless buf hdr.name_span name
      | known -&amp;gt;
        let canonical = Name.lowercase known in
        String.( = ) (String.lowercase name) canonical
    in
    if matches then Some hdr else find_string buf rest name
;;
&lt;/code&gt;
    &lt;p&gt;Notice that it's a recursive function as well, so this is a fairly natural way to write something that remains heap allocated. You can learn more about this from &lt;/p&gt;
    &lt;head rend="h2"&gt;Mutable Local Variables with "let mutable"&lt;/head&gt;
    &lt;p&gt;A nice quality of life improvement is that OxCaml allows stack-allocated mutable variables in loops, eliminating the need to allocate &lt;code&gt;ref&lt;/code&gt; values. This
allows parsing code to have local mutability:&lt;/p&gt;
    &lt;code&gt;let parse_int64 (local_ buf) (sp : span) : int64# =
  let mutable acc : int64# = #0L in
  let mutable i = 0 in
  let mutable valid = true in
  while valid &amp;amp;&amp;amp; i &amp;lt; I16.to_int sp.#len do
    let c = Bytes.get buf (I16.to_int sp.#off + i) in
    match c with
    | '0' .. '9' -&amp;gt;
      acc &amp;lt;- I64.add (I64.mul acc #10L) (I64.of_int (Char.code c - 48));
      i &amp;lt;- i + 1
    | _ -&amp;gt; valid &amp;lt;- false
  done;
  acc
&lt;/code&gt;
    &lt;p&gt;Whereas in conventional OCaml there might be a minor heap allocation for the reference:&lt;/p&gt;
    &lt;code&gt;let parse_int64 buf sp =
  let acc = ref 0L in           (* Heap-allocated ref *)
  let i = ref 0 in              (* Heap-allocated ref *)
  let valid = ref true in       (* Heap-allocated ref *)
  while !valid &amp;amp;&amp;amp; !i &amp;lt; sp.len do
    let c = Bytes.get buf (sp.off + !i) in
    match c with
    | '0' .. '9' -&amp;gt;
      acc := Int64.add (Int64.mul !acc 10L) (Int64.of_int (Char.code c - 48));
      i := !i + 1
    | _ -&amp;gt; valid := false
  done;
  !acc
&lt;/code&gt;
    &lt;head rend="h3"&gt;Putting the parser together&lt;/head&gt;
    &lt;p&gt;The toplevel Httpz.parse function has a pretty simple signature from a user's perspective:&lt;/p&gt;
    &lt;code&gt;val parse : bytes -&amp;gt; len:int16# -&amp;gt; limits:limits -&amp;gt;
  #(Buf_read.status * Req.t * Header.t list) @ local
&lt;/code&gt;
    &lt;p&gt;This function receives some a bytebuffer and resource limits and returns an unboxed local tuple of the connection status, parsed (unboxed) request and a stack-local list of header spans that represent the offsets within the input buffer of what was passed.&lt;/p&gt;
    &lt;p&gt;I should probably make the input buffer local too; one nice aspect of OxCaml is how easy it is to incrementally add type and kind annotations and lean on the compiler type inference to help guide where to fixup callsites.&lt;/p&gt;
    &lt;head rend="h3"&gt;Caveats and limitations&lt;/head&gt;
    &lt;p&gt;There are lots and lots of other new features in OxCaml which I've started integrating, but require careful planning of layouts. For example, I wanted to use or_null to have a non-allocating version of option, but you often end up with long compiler errors about value inference failures, so I ended up just allocating a local type instead. Something to investigate more in the future as I get familiar with OxCaml.&lt;/p&gt;
    &lt;p&gt;I also ran into issues using mutable fields in unboxed records and found this is documented:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We plan to allow mutating unboxed records within boxed records (the design will differ from boxed record mutability, as unboxed types don‚Äôt have the same notion of identity).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's also difficult right now to strip away the OxCaml extensions and go back to normal OCaml syntax. &lt;code&gt;--erase-jane-syntax&lt;/code&gt;, but it requires some build system work to
integrate and seems to lag a little behind the new features (like unboxed small
literals). For now, I've decided to just focus on using OxCaml exclusively and
see how it goes for a while.&lt;/p&gt;
    &lt;p&gt;Finally, the tooling is still a fluid story. &lt;/p&gt;
    &lt;head rend="h3"&gt;Claude skills for OxCaml&lt;/head&gt;
    &lt;p&gt;While I built small scale examples to test out the architecture, I leaned heavily on Claude code to build out the majority of the parser so I could rapidly experiment. To do this, I synthesised a set of OxCaml specific Claude skills in my &lt;/p&gt;
    &lt;p&gt;I generated those skills via a combination of summarising the OxCaml source trees and cribbing from the &lt;/p&gt;
    &lt;head rend="h2"&gt;Performance Results&lt;/head&gt;
    &lt;p&gt;Ultimately, none of this matters if the runtime performance isn't there! Luckily, the HTTPz parser is incredible in a synthetic benchmark (just passing buffers around) as opposed to a network benchmark, using Core_bench to measure performance. What's impressive isn't the straightline throughput, but the massive drop in heap activity which greatly increased the predictability and tail latency of the service. And with all the extra typing information, I expect that straightline performance will only increase (and this is before I've looked at the SIMD support).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;httpz (OxCaml)&lt;/cell&gt;
        &lt;cell role="head"&gt;Traditional Parser&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Small request (35B)&lt;/cell&gt;
        &lt;cell&gt;154 ns&lt;/cell&gt;
        &lt;cell&gt;300+ ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Medium request (439B)&lt;/cell&gt;
        &lt;cell&gt;1,150 ns&lt;/cell&gt;
        &lt;cell&gt;2,000+ ns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Heap allocations&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;100-800 words&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;6.5M req/sec&lt;/cell&gt;
        &lt;cell&gt;3M req/sec&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Putting my new site live&lt;/head&gt;
    &lt;p&gt;I then glued this together using Eio into a full webserver. It works, and serves traffic just fine and in fact you are reading this web page via it right now!&lt;/p&gt;
    &lt;head rend="h3"&gt;What next: caml_alloc_local for C bindings&lt;/head&gt;
    &lt;p&gt;The current Eio/OxCaml does a data copy right now since Eio uses Bigarray, but I had a catchup coffee with &lt;/p&gt;
    &lt;p&gt;The key OxCaml feature to make this &lt;code&gt;io_uring&lt;/code&gt; integration awesome is a new FFI
function that allocates an OCaml value directly into the caller's OxCaml stack
rather than the heap. This means that we should be able to come up with a scheme
by which io_uring requests are routed directly to an OCaml continuation that's woken
up directly with a buffer available to it on the stack. True zero-copy to the kernel
awaits, which should also help speed up &lt;/p&gt;
    &lt;head rend="h3"&gt;Making it easier to develop in OxCaml in the open&lt;/head&gt;
    &lt;p&gt;Keen readers may note that my OxCaml repo links here go to a new monorepo I've setup for the purpose of hacking on real code in production outside of Jane Street's walls.&lt;/p&gt;
    &lt;p&gt;I'll blog more about this next week, but for now I hope you've enjoyed a little taste of what the OxCaml extensions offer in real world code. Stay tuned also for even more performance improvements, and for native TLS with an OxCaml port of ocaml-tls from &lt;/p&gt;
    &lt;head rend="h3"&gt;References&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Madhavapeddy et al (2025). Functional Networking for Millions of Docker Desktops. 10.1145/3747525&lt;/item&gt;
      &lt;item&gt;Madhavapeddy (2025). Holding an OxCaml tutorial at ICFP/SPLASH 2025. 10.59350/55bc5-x4p75&lt;/item&gt;
      &lt;item&gt;Sivaramakrishnan et al (2021). Retrofitting effect handlers onto OCaml. ACM. 10.1145/3453483.3454039&lt;/item&gt;
      &lt;item&gt;Madhavapeddy (2025). Arise Bushel, my sixth generation oxidised website. 10.59350/0r62w-c8g63&lt;/item&gt;
      &lt;item&gt;Madhavapeddy (2025). GeoTessera Python library released for geospatial embeddings. 10.59350/7hy6m-1rq76&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46854534</guid><pubDate>Mon, 02 Feb 2026 10:45:44 +0000</pubDate></item><item><title>Termux</title><link>https://github.com/termux/termux-app</link><description>&lt;doc fingerprint="bec6290bc4cf3eaa"&gt;
  &lt;main&gt;
    &lt;p&gt;Termux is an Android terminal application and Linux environment.&lt;/p&gt;
    &lt;p&gt;Note that this repository is for the app itself (the user interface and the terminal emulation). For the packages installable inside the app, see termux/termux-packages.&lt;/p&gt;
    &lt;p&gt;Quick how-to about Termux package management is available at Package Management. It also has info on how to fix &lt;code&gt;repository is under maintenance or down&lt;/code&gt; errors when running &lt;code&gt;apt&lt;/code&gt; or &lt;code&gt;pkg&lt;/code&gt; commands.&lt;/p&gt;
    &lt;p&gt;We are looking for Termux Android application maintainers.&lt;/p&gt;
    &lt;p&gt;NOTICE: Termux may be unstable on Android 12+. Android OS will kill any (phantom) processes greater than 32 (limit is for all apps combined) and also kill any processes using excessive CPU. You may get &lt;code&gt;[Process completed (signal 9) - press Enter]&lt;/code&gt; message in the terminal without actually exiting the shell process yourself. Check the related issue #2366, issue tracker, phantom cached and empty processes docs and this TLDR comment on how to disable trimming of phantom and excessive cpu usage processes. A proper docs page will be added later. An option to disable the killing should be available in Android 12L or 13, so upgrade at your own risk if you are on Android 11, specially if you are not rooted.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Termux App and Plugins&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Uninstallation&lt;/item&gt;
      &lt;item&gt;Important Links&lt;/item&gt;
      &lt;item&gt;Debugging&lt;/item&gt;
      &lt;item&gt;For Maintainers and Contributors&lt;/item&gt;
      &lt;item&gt;Forking&lt;/item&gt;
      &lt;item&gt;Sponsors and Funders&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The core Termux app comes with the following optional plugin apps.&lt;/p&gt;
    &lt;p&gt;Latest version is &lt;code&gt;v0.118.3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;NOTICE: It is highly recommended that you update to &lt;code&gt;v0.118.0&lt;/code&gt; or higher ASAP for various bug fixes, including a critical world-readable vulnerability reported here. See below for information regarding Termux on Google Play.&lt;/p&gt;
    &lt;p&gt;Termux can be obtained through various sources listed below for only Android &lt;code&gt;&amp;gt;= 7&lt;/code&gt; with full support for apps and packages.&lt;/p&gt;
    &lt;p&gt;Support for both app and packages was dropped for Android &lt;code&gt;5&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt; on 2020-01-01 at &lt;code&gt;v0.83&lt;/code&gt;, however it was re-added just for the app without any support for package updates on 2022-05-24 via the GitHub sources. Check here for the details.&lt;/p&gt;
    &lt;p&gt;The APK files of different sources are signed with different signature keys. The &lt;code&gt;Termux&lt;/code&gt; app and all its plugins use the same &lt;code&gt;sharedUserId&lt;/code&gt; &lt;code&gt;com.termux&lt;/code&gt; and so all their APKs installed on a device must have been signed with the same signature key to work together and so they must all be installed from the same source. Do not attempt to mix them together, i.e do not try to install an app or plugin from &lt;code&gt;F-Droid&lt;/code&gt; and another one from a different source like &lt;code&gt;GitHub&lt;/code&gt;. Android Package Manager will also normally not allow installation of APKs with different signatures and you will get errors on installation like &lt;code&gt;App not installed&lt;/code&gt;, &lt;code&gt;Failed to install due to an unknown error&lt;/code&gt;, &lt;code&gt;INSTALL_FAILED_UPDATE_INCOMPATIBLE&lt;/code&gt;, &lt;code&gt;INSTALL_FAILED_SHARED_USER_INCOMPATIBLE&lt;/code&gt;, &lt;code&gt;signatures do not match previously installed version&lt;/code&gt;, etc. This restriction can be bypassed with root or with custom roms.&lt;/p&gt;
    &lt;p&gt;If you wish to install from a different source, then you must uninstall any and all existing Termux or its plugin app APKs from your device first, then install all new APKs from the same new source. Check Uninstallation section for details. You may also want to consider Backing up Termux before the uninstallation so that you can restore it after re-installing from Termux different source.&lt;/p&gt;
    &lt;p&gt;In the following paragraphs, "bootstrap" refers to the minimal packages that are shipped with the &lt;code&gt;termux-app&lt;/code&gt; itself to start a working shell environment. Its zips are built and released here.&lt;/p&gt;
    &lt;p&gt;Termux application can be obtained from &lt;code&gt;F-Droid&lt;/code&gt; from here.&lt;/p&gt;
    &lt;p&gt;You do not need to download the &lt;code&gt;F-Droid&lt;/code&gt; app (via the &lt;code&gt;Download F-Droid&lt;/code&gt; link) to install Termux. You can download the Termux APK directly from the site by clicking the &lt;code&gt;Download APK&lt;/code&gt; link at the bottom of each version section.&lt;/p&gt;
    &lt;p&gt;It usually takes a few days (or even a week or more) for updates to be available on &lt;code&gt;F-Droid&lt;/code&gt; once an update has been released on &lt;code&gt;GitHub&lt;/code&gt;. The &lt;code&gt;F-Droid&lt;/code&gt; releases are built and published by &lt;code&gt;F-Droid&lt;/code&gt; once they detect a new &lt;code&gt;GitHub&lt;/code&gt; release. The Termux maintainers do not have any control over the building and publishing of the Termux apps on &lt;code&gt;F-Droid&lt;/code&gt;. Moreover, the Termux maintainers also do not have access to the APK signing keys of &lt;code&gt;F-Droid&lt;/code&gt; releases, so we cannot release an APK ourselves on &lt;code&gt;GitHub&lt;/code&gt; that would be compatible with &lt;code&gt;F-Droid&lt;/code&gt; releases.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;F-Droid&lt;/code&gt; app often may not notify you of updates and you will manually have to do a pull down swipe action in the &lt;code&gt;Updates&lt;/code&gt; tab of the app for it to check updates. Make sure battery optimizations are disabled for the app, check https://dontkillmyapp.com/ for details on how to do that.&lt;/p&gt;
    &lt;p&gt;Only a universal APK is released, which will work on all supported architectures. The APK and bootstrap installation size will be &lt;code&gt;~180MB&lt;/code&gt;. &lt;code&gt;F-Droid&lt;/code&gt; does not support architecture specific APKs.&lt;/p&gt;
    &lt;p&gt;Termux application can be obtained on &lt;code&gt;GitHub&lt;/code&gt; either from &lt;code&gt;GitHub Releases&lt;/code&gt; for version &lt;code&gt;&amp;gt;= 0.118.0&lt;/code&gt; or from &lt;code&gt;GitHub Build Action&lt;/code&gt; workflows. For android &lt;code&gt;&amp;gt;= 7&lt;/code&gt;, only install &lt;code&gt;apt-android-7&lt;/code&gt; variants. For android &lt;code&gt;5&lt;/code&gt; and &lt;code&gt;6&lt;/code&gt;, only install &lt;code&gt;apt-android-5&lt;/code&gt; variants.&lt;/p&gt;
    &lt;p&gt;The APKs for &lt;code&gt;GitHub Releases&lt;/code&gt; will be listed under &lt;code&gt;Assets&lt;/code&gt; drop-down of a release. These are automatically attached when a new version is released.&lt;/p&gt;
    &lt;p&gt;The APKs for &lt;code&gt;GitHub Build&lt;/code&gt; action workflows will be listed under &lt;code&gt;Artifacts&lt;/code&gt; section of a workflow run. These are created for each commit/push done to the repository and can be used by users who don't want to wait for releases and want to try out the latest features immediately or want to test their pull requests. Note that for action workflows, you need to be logged into a &lt;code&gt;GitHub&lt;/code&gt; account for the &lt;code&gt;Artifacts&lt;/code&gt; links to be enabled/clickable. If you are using the &lt;code&gt;GitHub&lt;/code&gt; app, then make sure to open workflow link in a browser like Chrome or Firefox that has your GitHub account logged in since the in-app browser may not be logged in.&lt;/p&gt;
    &lt;p&gt;The APKs for both of these are &lt;code&gt;debuggable&lt;/code&gt; and are compatible with each other but they are not compatible with other sources.&lt;/p&gt;
    &lt;p&gt;Both universal and architecture specific APKs are released. The APK and bootstrap installation size will be &lt;code&gt;~180MB&lt;/code&gt; if using universal and &lt;code&gt;~120MB&lt;/code&gt; if using architecture specific. Check here for details.&lt;/p&gt;
    &lt;p&gt;Security warning: APK files on GitHub are signed with a test key that has been shared with community. This IS NOT an official developer key and everyone can use it to generate releases for own testing. Be very careful when using Termux GitHub builds obtained elsewhere except https://github.com/termux/termux-app. Everyone is able to use it to forge a malicious Termux update installable over the GitHub build. Think twice about installing Termux builds distributed via Telegram or other social media. If your device get caught by malware, we will not be able to help you.&lt;/p&gt;
    &lt;p&gt;The test key shall not be used to impersonate @termux and can't be used for this anyway. This key is not trusted by us and it is quite easy to detect its use in user generated content.&lt;/p&gt;
    &lt;head&gt;Keystore information&lt;/head&gt;
    &lt;code&gt;Alias name: alias
Creation date: Oct 4, 2019
Entry type: PrivateKeyEntry
Certificate chain length: 1
Certificate[1]:
Owner: CN=APK Signer, OU=Earth, O=Earth
Issuer: CN=APK Signer, OU=Earth, O=Earth
Serial number: 29be297b
Valid from: Wed Sep 04 02:03:24 EEST 2019 until: Tue Oct 26 02:03:24 EEST 2049
Certificate fingerprints:
         SHA1: 51:79:55:EA:BF:69:FC:05:7C:41:C7:D3:79:DB:BC:EF:20:AD:85:F2
         SHA256: B6:DA:01:48:0E:EF:D5:FB:F2:CD:37:71:B8:D1:02:1E:C7:91:30:4B:DD:6C:4B:F4:1D:3F:AA:BA:D4:8E:E5:E1
Signature algorithm name: SHA1withRSA (disabled)
Subject Public Key Algorithm: 2048-bit RSA key
Version: 3
&lt;/code&gt;
    &lt;p&gt;There is currently a build of Termux available on Google Play for Android 11+ devices, with extensive adjustments in order to pass policy requirements there. This is under development and has missing functionality and bugs (see here for status updates) compared to the stable F-Droid build, which is why most users who can should still use F-Droid or GitHub build as mentioned above.&lt;/p&gt;
    &lt;p&gt;Currently, Google Play will try to update installations away from F-Droid ones. Updating will still fail as sharedUserId has been removed. A planned 0.118.1 F-Droid release will fix this by setting a higher version code than used for the PlayStore app. Meanwhile, to prevent Google Play from attempting to download and then fail to install the Google Play releases over existing installations, you can open the Termux apps pages on Google Play and then click on the 3 dots options button in the top right and then disable the Enable auto update toggle. However, the Termux apps updates will still show in the PlayStore app updates list.&lt;/p&gt;
    &lt;p&gt;If you want to help out with testing the Google Play build (or cannot install Termux from other sources), be aware that it's built from a separate repository (https://github.com/termux-play-store/) - be sure to report issues there, as any issues encountered might very well be specific to that repository.&lt;/p&gt;
    &lt;p&gt;Uninstallation may be required if a user doesn't want Termux installed in their device anymore or is switching to a different install source. You may also want to consider Backing up Termux before the uninstallation.&lt;/p&gt;
    &lt;p&gt;To uninstall Termux completely, you must uninstall any and all existing Termux or its plugin app APKs listed in Termux App and Plugins.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;Android Settings&lt;/code&gt; -&amp;gt; &lt;code&gt;Applications&lt;/code&gt; and then look for those apps. You can also use the search feature if it‚Äôs available on your device and search &lt;code&gt;termux&lt;/code&gt; in the applications list.&lt;/p&gt;
    &lt;p&gt;Even if you think you have not installed any of the plugins, it's strongly suggested to go through the application list in Android settings and double-check.&lt;/p&gt;
    &lt;p&gt;All community links are available here.&lt;/p&gt;
    &lt;p&gt;The main ones are the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Termux Reddit community&lt;/item&gt;
      &lt;item&gt;Termux User Matrix Channel (Gitter)&lt;/item&gt;
      &lt;item&gt;Termux Dev Matrix Channel (Gitter)&lt;/item&gt;
      &lt;item&gt;Termux X (Twitter)&lt;/item&gt;
      &lt;item&gt;Termux Support Email&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FAQ&lt;/item&gt;
      &lt;item&gt;Termux File System Layout&lt;/item&gt;
      &lt;item&gt;Differences From Linux&lt;/item&gt;
      &lt;item&gt;Package Management&lt;/item&gt;
      &lt;item&gt;Remote Access&lt;/item&gt;
      &lt;item&gt;Backing up Termux&lt;/item&gt;
      &lt;item&gt;Terminal Settings&lt;/item&gt;
      &lt;item&gt;Touch Keyboard&lt;/item&gt;
      &lt;item&gt;Android Storage and Sharing Data with Other Apps&lt;/item&gt;
      &lt;item&gt;Android APIs&lt;/item&gt;
      &lt;item&gt;Moved Termux Packages Hosting From Bintray to IPFS&lt;/item&gt;
      &lt;item&gt;Running Commands in Termux From Other Apps via &lt;code&gt;RUN_COMMAND&lt;/code&gt;intent&lt;/item&gt;
      &lt;item&gt;Termux and Android 10&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;VTE (libvte): Terminal emulator widget for GTK+, mainly used in gnome-terminal. Source, Open Issues, and All (including closed) issues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;iTerm 2: OS X terminal application. Source, Issues and Documentation (which includes iTerm2 proprietary escape codes).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Konsole: KDE terminal application. Source, in particular tests, Bugs and Wishes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;hterm: JavaScript terminal implementation from Chromium. Source, including tests, and Google group.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;xterm: The grandfather of terminal emulators. Source.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connectbot: Android SSH client. Source&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Android Terminal Emulator: Android terminal app which Termux terminal handling is based on. Inactive. Source.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can help debug problems of the &lt;code&gt;Termux&lt;/code&gt; app and its plugins by setting appropriate &lt;code&gt;logcat&lt;/code&gt; &lt;code&gt;Log Level&lt;/code&gt; in &lt;code&gt;Termux&lt;/code&gt; app settings -&amp;gt; &lt;code&gt;&amp;lt;APP_NAME&amp;gt;&lt;/code&gt; -&amp;gt; &lt;code&gt;Debugging&lt;/code&gt; -&amp;gt; &lt;code&gt;Log Level&lt;/code&gt; (Requires &lt;code&gt;Termux&lt;/code&gt; app version &lt;code&gt;&amp;gt;= 0.118.0&lt;/code&gt;). The &lt;code&gt;Log Level&lt;/code&gt; defaults to &lt;code&gt;Normal&lt;/code&gt; and log level &lt;code&gt;Verbose&lt;/code&gt; currently logs additional information. Its best to revert log level to &lt;code&gt;Normal&lt;/code&gt; after you have finished debugging since private data may otherwise be passed to &lt;code&gt;logcat&lt;/code&gt; during normal operation and moreover, additional logging increases execution time.&lt;/p&gt;
    &lt;p&gt;The plugin apps do not execute the commands themselves but send execution intents to &lt;code&gt;Termux&lt;/code&gt; app, which has its own log level which can be set in &lt;code&gt;Termux&lt;/code&gt; app settings -&amp;gt; &lt;code&gt;Termux&lt;/code&gt; -&amp;gt; &lt;code&gt;Debugging&lt;/code&gt; -&amp;gt; &lt;code&gt;Log Level&lt;/code&gt;. So you must set log level for both &lt;code&gt;Termux&lt;/code&gt; and the respective plugin app settings to get all the info.&lt;/p&gt;
    &lt;p&gt;Once log levels have been set, you can run the &lt;code&gt;logcat&lt;/code&gt; command in &lt;code&gt;Termux&lt;/code&gt; app terminal to view the logs in realtime (&lt;code&gt;Ctrl+c&lt;/code&gt; to stop) or use &lt;code&gt;logcat -d &amp;gt; logcat.txt&lt;/code&gt; to take a dump of the log. You can also view the logs from a PC over &lt;code&gt;ADB&lt;/code&gt;. For more information, check official android &lt;code&gt;logcat&lt;/code&gt; guide here.&lt;/p&gt;
    &lt;p&gt;Moreover, users can generate termux files &lt;code&gt;stat&lt;/code&gt; info and &lt;code&gt;logcat&lt;/code&gt; dump automatically too with terminal's long hold options menu &lt;code&gt;More&lt;/code&gt; -&amp;gt; &lt;code&gt;Report Issue&lt;/code&gt; option and selecting &lt;code&gt;YES&lt;/code&gt; in the prompt shown to add debug info. This can be helpful for reporting and debugging other issues. If the report generated is too large, then &lt;code&gt;Save To File&lt;/code&gt; option in context menu (3 dots on top right) of &lt;code&gt;ReportActivity&lt;/code&gt; can be used and the file viewed/shared instead.&lt;/p&gt;
    &lt;p&gt;Users must post complete report (optionally without sensitive info) when reporting issues. Issues opened with (partial) screenshots of error reports instead of text will likely be automatically closed/deleted.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Off&lt;/code&gt;- Log nothing.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Normal&lt;/code&gt;- Start logging error, warn and info messages and stacktraces.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Debug&lt;/code&gt;- Start logging debug messages.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Verbose&lt;/code&gt;- Start logging verbose messages.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The termux-shared library was added in &lt;code&gt;v0.109&lt;/code&gt;. It defines shared constants and utils of the Termux app and its plugins. It was created to allow for the removal of all hardcoded paths in the Termux app. Some of the termux plugins are using this as well and rest will in future. If you are contributing code that is using a constant or a util that may be shared, then define it in &lt;code&gt;termux-shared&lt;/code&gt; library if it currently doesn't exist and reference it from there. Update the relevant changelogs as well. Pull requests using hardcoded values will/should not be accepted. Termux app and plugin specific classes must be added under &lt;code&gt;com.termux.shared.termux&lt;/code&gt; package and general classes outside it. The &lt;code&gt;termux-shared&lt;/code&gt; &lt;code&gt;LICENSE&lt;/code&gt; must also be checked and updated if necessary when contributing code. The licenses of any external library or code must be honoured.&lt;/p&gt;
    &lt;p&gt;The main Termux constants are defined by &lt;code&gt;TermuxConstants&lt;/code&gt; class. It also contains information on how to fork Termux or build it with your own package name. Changing the package name will require building the bootstrap zip packages and other packages with the new &lt;code&gt;$PREFIX&lt;/code&gt;, check Building Packages for more info.&lt;/p&gt;
    &lt;p&gt;Check Termux Libraries for how to import termux libraries in plugin apps and Forking and Local Development for how to update termux libraries for plugins.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;versionName&lt;/code&gt; in &lt;code&gt;build.gradle&lt;/code&gt; files of Termux and its plugin apps must follow the semantic version &lt;code&gt;2.0.0&lt;/code&gt; spec in the format &lt;code&gt;major.minor.patch(-prerelease)(+buildmetadata)&lt;/code&gt;. When bumping &lt;code&gt;versionName&lt;/code&gt; in &lt;code&gt;build.gradle&lt;/code&gt; files and when creating a tag for new releases on GitHub, make sure to include the patch number as well, like &lt;code&gt;v0.1.0&lt;/code&gt; instead of just &lt;code&gt;v0.1&lt;/code&gt;. The &lt;code&gt;build.gradle&lt;/code&gt; files and &lt;code&gt;attach_debug_apks_to_release&lt;/code&gt; workflow validates the version as well and the build/attachment will fail if &lt;code&gt;versionName&lt;/code&gt; does not follow the spec.&lt;/p&gt;
    &lt;p&gt;Commit messages must use the Conventional Commits spec so that chagelogs as per the Keep a Changelog spec can automatically be generated by the &lt;code&gt;create-conventional-changelog&lt;/code&gt; script, check its repo for further details on the spec. The first letter for &lt;code&gt;type&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt; must be capital and description should be in the present tense. The space after the colon &lt;code&gt;:&lt;/code&gt; is necessary. For a breaking change, add an exclamation mark &lt;code&gt;!&lt;/code&gt; before the colon &lt;code&gt;:&lt;/code&gt;, so that it is highlighted in the chagelog automatically.&lt;/p&gt;
    &lt;code&gt;&amp;lt;type&amp;gt;[optional scope]: &amp;lt;description&amp;gt;

[optional body]

[optional footer(s)]
&lt;/code&gt;
    &lt;p&gt;Only the &lt;code&gt;types&lt;/code&gt; listed below must be used exactly as they are used in the changelog headings. For example, &lt;code&gt;Added: Add foo&lt;/code&gt;, &lt;code&gt;Added|Fixed: Add foo and fix bar&lt;/code&gt;, &lt;code&gt;Changed!: Change baz as a breaking change&lt;/code&gt;, etc. You can optionally add a scope as well, like &lt;code&gt;Fixed(terminal): Fix some bug&lt;/code&gt;. Do not use anything else as type, like &lt;code&gt;add&lt;/code&gt; instead of &lt;code&gt;Added&lt;/code&gt;, etc.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Added for new features.&lt;/item&gt;
      &lt;item&gt;Changed for changes in existing functionality.&lt;/item&gt;
      &lt;item&gt;Deprecated for soon-to-be removed features.&lt;/item&gt;
      &lt;item&gt;Removed for now removed features.&lt;/item&gt;
      &lt;item&gt;Fixed for any bug fixes.&lt;/item&gt;
      &lt;item&gt;Security in case of vulnerabilities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check &lt;code&gt;TermuxConstants&lt;/code&gt;javadocs for instructions on what changes to make in the app to change package name.&lt;/item&gt;
      &lt;item&gt;You also need to recompile bootstrap zip for the new package name. Check building bootstrap, here and here.&lt;/item&gt;
      &lt;item&gt;Currently, not all plugins use &lt;code&gt;TermuxConstants&lt;/code&gt;from&lt;code&gt;termux-shared&lt;/code&gt;library and have hardcoded&lt;code&gt;com.termux&lt;/code&gt;values and will need to be manually patched.&lt;/item&gt;
      &lt;item&gt;If forking termux plugins, check Forking and Local Development for info on how to use termux libraries for plugins.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; GitHub Secure Open Source Fund (1, 2)&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; NLnet NGI Mobifree (1, 2)&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Cloudflare (1)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46854642</guid><pubDate>Mon, 02 Feb 2026 11:03:44 +0000</pubDate></item><item><title>Claude Code is suddenly everywhere inside Microsoft</title><link>https://www.theverge.com/tech/865689/microsoft-claude-code-anthropic-partnership-notepad</link><description>&lt;doc fingerprint="2f402f1a7ea6c2ef"&gt;
  &lt;main&gt;
    &lt;p&gt;Developers have been comparing the strengths and weaknesses of Anthropic‚Äôs Claude Code, Anysphere‚Äôs Cursor, and Microsoft‚Äôs GitHub Copilot for months now, looking for a winner. While no individual AI coding tool manages to be the best at every task that software developers do each day, Claude Code is increasingly coming out on top for its ease of use, both for developers and nontechnical users.&lt;/p&gt;
    &lt;head rend="h1"&gt;Claude Code is suddenly everywhere inside Microsoft&lt;/head&gt;
    &lt;p&gt;Microsoft sells GitHub Copilot to its customers, but it increasingly favors Claude Code internally.&lt;/p&gt;
    &lt;p&gt;Microsoft sells GitHub Copilot to its customers, but it increasingly favors Claude Code internally.&lt;/p&gt;
    &lt;p&gt;It seems like Microsoft agrees, as sources tell me the company is now encouraging thousands of its employees from some of its most prolific teams to pick up Claude Code and get coding, even if they‚Äôre not developers.&lt;/p&gt;
    &lt;p&gt;Microsoft first started adopting Anthropic‚Äôs Claude Sonnet 4 model inside its developer division in June last year, before favoring it for paid users of GitHub Copilot several months later. Now, Microsoft is going a step beyond using Anthropic‚Äôs AI models and widely adopting Claude Code across its biggest engineering teams.&lt;/p&gt;
    &lt;p&gt;Microsoft‚Äôs CoreAI team, the new AI engineering group led by former Meta engineering chief Jay Parikh, has been testing Claude Code in recent months, and last week Microsoft‚Äôs Experiences + Devices division were being asked to install Claude Code. This division is responsible for Windows, Microsoft 365, Outlook, Microsoft Teams, Bing, Edge, Surface, and more.&lt;/p&gt;
    &lt;p&gt;Even employees without any coding experience are being encouraged to experiment with Claude Code, to allow designers and project managers to prototype ideas. Microsoft has also approved the use of Claude Code across all of its code and repositories for its Business and Industry Copilot teams.&lt;/p&gt;
    &lt;p&gt;Software engineers at Microsoft are now expected to use both Claude Code and GitHub Copilot and give feedback comparing the two, I‚Äôm told. Microsoft sells GitHub Copilot as its AI coding tool of choice to its customers, but if these broad internal pilot programs are successful, then it‚Äôs possible the company could even eventually sell Claude Code directly to its cloud customers.&lt;/p&gt;
    &lt;p&gt;Microsoft is now one of Anthropic‚Äôs top customers, according to a recent report from The Information. The software maker is also counting selling Anthropic AI models toward Azure sales quotas, which is unusual given Microsoft typically only offers its salespeople incentives for homegrown products or models from OpenAI.&lt;/p&gt;
    &lt;p&gt;Microsoft‚Äôs decision to adopt Claude Code more broadly among its engineering teams certainly looks like a vote of confidence in Anthropic‚Äôs AI tools over its own, especially as it‚Äôs encouraging nontechnical employees to try out coding. But the reality is that Microsoft‚Äôs developers are likely to use a mix of AI tools, and adopting Claude Code is another part of that tool set.&lt;/p&gt;
    &lt;p&gt;‚ÄúCompanies regularly test and trial competing products to gain a better understanding of the market landscape,‚Äù says Frank Shaw, Microsoft‚Äôs communications chief, in a statement to Notepad. ‚ÄúOpenAI continues to be our primary partner and model provider on frontier models, and we remain committed to our long-term partnership.‚Äù&lt;/p&gt;
    &lt;p&gt;While Microsoft remains committed to OpenAI, it is increasingly working with Anthropic to bring its models and tools to Microsoft‚Äôs own teams and the software it sells to customers. Microsoft and Anthropic signed a deal in November that allows Microsoft Foundry customers to get access to Claude Sonnet 4.5, Claude Opus 4.1, and Claude Haiku 4.5. The deal also involves Anthropic committing to purchasing $30 billion of Azure compute capacity.&lt;/p&gt;
    &lt;p&gt;Microsoft has also started favoring Anthropic‚Äôs Claude models inside Microsoft 365 apps and Copilot recently, using them in specific apps or features where Anthropic‚Äôs models have proved more capable than OpenAI‚Äôs counterparts.&lt;/p&gt;
    &lt;p&gt;The big question here is, what does the increased use of Claude Code at Microsoft mean for its more than 100,000 code repositories? Microsoft told me last year that 91 percent of its engineering teams use GitHub Copilot and a variety of teams have been using the AI tool to speed up mundane tasks. Microsoft‚Äôs use of AI tools has been largely restricted to software engineers, but with Claude Code and Claude Cowork, Anthropic is increasingly focused on making coding and non-coding tasks more approachable, thanks to AI agent capabilities.&lt;/p&gt;
    &lt;p&gt;Microsoft is embracing the ease of use of Claude Code to allow more nontechnical employees to commit code using AI, and this broad pilot will certainly highlight the challenges and benefits of that shift. It also puts further pressure on junior developer roles, with fears in the industry that these roles are increasingly disappearing because of AI. Microsoft just took another big step toward a future where more autonomous AI agents are creating code, further wrestling control from its software engineers.&lt;/p&gt;
    &lt;head rend="h2"&gt;It‚Äôs Xbox time&lt;/head&gt;
    &lt;p&gt;Microsoft is getting ready to show off two of its biggest Xbox games this year, Forza Horizon 6 and Fable, later today as part of its Xbox Developer Direct stream. There will also be a first in-depth look at Beast of Reincarnation and at least one other game shown, I‚Äôm hearing. Double Fine is ready to show off Kiln, a multiplayer, team-based brawler. I understand Double Fine has been holding playtests recently, where you play as a spirit that can inhabit pottery and carry water to douse an opponent‚Äôs kiln and put out a fire.&lt;/p&gt;
    &lt;p&gt;I wouldn‚Äôt be surprised to see Kiln appear as an early preview in the coming months, followed by Forza Horizon 6 in May and then Halo: Campaign Evolved. I keep hearing that both Fable and Gears of War: E-Day are currently targeting a release in the second half of this year. Microsoft is keen to release new Forza, Gears, Halo, and Fable games in 2026 to mark 25 years of Xbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsoft‚Äôs first Windows 11 update of 2026 stopped some computers from shutting down. It‚Äôs only January and Microsoft has had to rush out an emergency out-of-band fix that stopped some Windows 11 PCs from shutting down. The issues were limited to machines running Enterprise and IoT editions of Windows 11 version 23H2, but it‚Äôs yet another buggy update for Windows, which is becoming increasingly common.&lt;/item&gt;
      &lt;item&gt;Microsoft‚Äôs free Xbox Cloud Gaming is coming soon with ads. Microsoft is getting closer to launching its free streaming option for Xbox Cloud Gaming. The ad-supported feature has started appearing inside the Xbox app for PC, indicating ‚Äú1 hour of ad-supported playtime per session.‚Äù I‚Äôm expecting to see this rollout with preroll ads in the coming weeks, but there could be limits of up to five hours free per month.&lt;/item&gt;
      &lt;item&gt;Microsoft wants to build 15 data centers in Mount Pleasant, Wisconsin. The empty land formerly owned by Foxconn is about to be transformed into Microsoft data centers. Leaders of the local village in Mount Pleasant, Wisconsin, approved plans for the data centers earlier this week, and final approval could come next week. Foxconn‚Äôs failed Wisconsin project had promised 13,000 jobs, but now the land will be filled with a 1.2-million-square-foot data center project that will hold hundreds of thousands of Nvidia‚Äôs AI GPUs.&lt;/item&gt;
      &lt;item&gt;The Xbox app is now available for all Arm-based Windows 11 PCs. After a rocky start to gaming on Windows on Arm, Microsoft has updated its Xbox app this week so it‚Äôs fully compatible with all Qualcomm-powered devices. More than 85 percent of the Xbox Game Pass catalog is also now compatible with Arm-based devices, but the majority of games will still need to be emulated using Microsoft‚Äôs Prism technology.&lt;/item&gt;
      &lt;item&gt;Microsoft Paint now has an AI-powered coloring book. Microsoft is adding more AI features to its Paint app this week. Windows testers can now try out a coloring book feature that lets you create coloring book pages from a text prompt. It‚Äôs available inside the Copilot button in Paint, and you have to have a Copilot Plus PC to be able to use it. Notepad (the app!) is also getting expanded Markdown syntax features and a new welcome experience to highlight features. I never thought I‚Äôd see the day that Notepad, a lightweight app, would need a welcome screen because of all the features Microsoft has packed in.&lt;/item&gt;
      &lt;item&gt;GitHub has a new Copilot SDK. Microsoft is announcing a technical preview of its GitHub Copilot SDK today, which brings the power of the GitHub Copilot CLI to any app. It essentially allows developers to bring GitHub Copilot capabilities as a programmable SDK for Python, TypeScript, Go, and .NET. Microsoft teams have already used this to build custom GUIs for agents, summarizing tools, YouTube chapter generators, and more.&lt;/item&gt;
      &lt;item&gt;Satya Nadella and former British Prime Minister Rishi Sunak chat AI. Former UK leader Rishi Sunak took on a senior adviser role at Microsoft and Anthropic last year, and he‚Äôs now appeared alongside Microsoft CEO Satya Nadella to discuss the future of AI. The roughly 30-minute talk didn‚Äôt have any surprising news, but Sunak did agree with Nvidia CEO Jensen Huang that ‚Äúyou may not lose your job to AI, but you may well lose your job to someone using AI.‚Äù Nadella thinks AI will make us all ‚Äúmanagers of infinite minds,‚Äù much like how we have ‚Äúinformation at your fingertips.‚Äù&lt;/item&gt;
      &lt;item&gt;Microsoft now sponsors the Mercedes-AMG F1 team. Microsoft is switching its F1 allegiances from Alpine to Mercedes-AMG for the 2026 season. A new multiyear partnership will see Mercedes-AMG use Microsoft technologies for race team operations and plaster the Microsoft logo in prominent positions on the 2026 Mercedes-AMG F1 car and on racing suits. There‚Äôs a big technical shake-up for the 2026 season, with all-new chassis, power units, and fuel regulations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm always keen to hear from readers, so please drop a comment here, or you can reach me at notepad@theverge.com if you want to discuss anything else. If you‚Äôve heard about any of Microsoft‚Äôs secret projects, you can reach me via email at notepad@theverge.com or speak to me confidentially on the Signal messaging app, where I‚Äôm tomwarren.01. I‚Äôm also tomwarren on Telegram, if you‚Äôd prefer to chat there.&lt;/p&gt;
    &lt;p&gt;Thanks for subscribing to Notepad.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46854999</guid><pubDate>Mon, 02 Feb 2026 11:58:58 +0000</pubDate></item><item><title>Nano-vLLM: How a vLLM-style inference engine works</title><link>https://neutree.ai/blog/nano-vllm-part-1</link><description>&lt;doc fingerprint="3c1aa40c8370c811"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Understanding LLM Inference Engines: Inside Nano-vLLM (Part 1)&lt;/head&gt;
    &lt;head rend="h2"&gt;Architecture, Scheduling, and the Path from Prompt to Token&lt;/head&gt;
    &lt;p&gt;When deploying large language models in production, the inference engine becomes a critical piece of infrastructure. Every LLM API you use √¢ OpenAI, Claude, DeepSeek √¢ is sitting on top of an inference engine like this. While most developers interact with LLMs through high-level APIs, understanding what happens beneath the surface√¢how prompts are processed, how requests are batched, and how GPU resources are managed√¢can significantly impact system design decisions.&lt;/p&gt;
    &lt;p&gt;This two-part series explores these internals through Nano-vLLM, a minimal (~1,200 lines of Python) yet production-grade implementation that distills the core ideas behind vLLM, one of the most widely adopted open-source inference engines.&lt;/p&gt;
    &lt;p&gt;Nano-vLLM was created by a contributor to DeepSeek, whose name appears on the technical reports of models like DeepSeek-V3 and R1. Despite its minimal codebase, it implements the essential features that make vLLM production-ready: prefix caching, tensor parallelism, CUDA graph compilation, and torch compilation optimizations. Benchmarks show it achieving throughput comparable to√¢or even slightly exceeding√¢the full vLLM implementation. This makes it an ideal lens for understanding inference engine design without getting lost in the complexity of supporting dozens of model architectures and hardware backends.&lt;/p&gt;
    &lt;p&gt;In Part 1, we focus on the engineering architecture: how the system is organized, how requests flow through the pipeline, and how scheduling decisions are made. We will treat the actual model computation as a black box for now√¢Part 2 will open that box to explore attention mechanisms, KV cache internals, and tensor parallelism at the computation level.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Main Flow: From Prompt to Output&lt;/head&gt;
    &lt;p&gt;The entry point to Nano-vLLM is straightforward: an &lt;code&gt;LLM&lt;/code&gt; class with a &lt;code&gt;generate&lt;/code&gt; method. You pass in an array of prompts and sampling parameters, and get back the generated text. But behind this simple interface lies a carefully designed pipeline that transforms text into tokens, schedules computation efficiently, and manages GPU resources.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Prompts to Sequences&lt;/head&gt;
    &lt;p&gt;When &lt;code&gt;generate&lt;/code&gt; is called, each prompt string goes through a tokenizer√¢a model-specific component that splits natural language into tokens, the fundamental units that LLMs process. Different model families (Qwen, LLaMA, DeepSeek) use different tokenizers, which is why a prompt of the same length may produce different token counts across models. The tokenizer converts each prompt into a sequence: an internal data structure representing a variable-length array of token IDs. This sequence becomes the core unit of work flowing through the rest of the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Producer-Consumer Pattern&lt;/head&gt;
    &lt;p&gt;Here√¢s where the architecture gets interesting. Rather than processing each sequence immediately, the system adopts a producer-consumer pattern with the Scheduler at its center. The &lt;code&gt;add_request&lt;/code&gt; method acts as the producer: it converts prompts to sequences and places them into the Scheduler√¢s queue. Meanwhile, a separate step loop acts as the consumer, pulling batches of sequences from the Scheduler for processing. This decoupling is key√¢it allows the system to accumulate multiple sequences and process them together, which is where the performance gains come from.&lt;/p&gt;
    &lt;head rend="h3"&gt;Batching and the Throughput-Latency Trade-off&lt;/head&gt;
    &lt;p&gt;Why does batching matter? GPU computation has significant fixed overhead√¢initializing CUDA kernels, transferring data between CPU and GPU memory, and synchronizing results. If you process one sequence at a time, you pay this overhead for every single request. By batching multiple sequences together, you amortize this overhead across many requests, dramatically improving overall throughput.&lt;/p&gt;
    &lt;p&gt;However, batching comes with a trade-off. When three prompts are batched together, each must wait for the others to complete before any results are returned. The total time for the batch is determined by the slowest sequence. This means: larger batches yield higher throughput but potentially higher latency for individual requests; smaller batches yield lower latency but reduced throughput. This is a fundamental tension in inference engine design, and the batch size parameters you configure directly control this trade-off.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prefill vs. Decode: Two Phases of Generation&lt;/head&gt;
    &lt;p&gt;Before diving into the Scheduler, we need to understand a crucial distinction. LLM inference happens in two phases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prefill: Processing the input prompt. All input tokens are processed together to build up the model√¢s internal state. During this phase, the user sees nothing.&lt;/item&gt;
      &lt;item&gt;Decode: Generating output tokens. The model produces one token at a time, each depending on all previous tokens. This is when you see text streaming out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a single sequence, there is exactly one prefill phase followed by many decode steps. The Scheduler needs to distinguish between these phases because they have very different computational characteristics√¢prefill processes many tokens at once, while decode processes just one token per step.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside the Scheduler&lt;/head&gt;
    &lt;p&gt;The Scheduler is responsible for deciding which sequences to process and in what order. It maintains two queues:&lt;/p&gt;
    &lt;head rend="h3"&gt;Waiting and Running Queues&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Waiting Queue: Sequences that have been submitted but not yet started. New sequences from &lt;code&gt;add_request&lt;/code&gt;always enter here first.&lt;/item&gt;
      &lt;item&gt;Running Queue: Sequences that are actively being processed√¢either in prefill or decode phase.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When a sequence enters the Waiting queue, the Scheduler checks with another component called the Block Manager to allocate resources for it. Once allocated, the sequence moves to the Running queue. The Scheduler then selects sequences from the Running queue for the next computation step, grouping them into a batch along with an action indicator (prefill or decode).&lt;/p&gt;
    &lt;head rend="h3"&gt;Handling Resource Exhaustion&lt;/head&gt;
    &lt;p&gt;What happens when GPU memory fills up? The KV cache (which stores intermediate computation results) has limited capacity. If a sequence in the Running queue cannot continue because there√¢s no room to store its next token√¢s cache, the Scheduler preempts it√¢moving it back to the front of the Waiting queue. This ensures the sequence will resume as soon as resources free up, while allowing other sequences to make progress.&lt;/p&gt;
    &lt;p&gt;When a sequence completes (reaches an end-of-sequence token or maximum length), the Scheduler removes it from the Running queue and deallocates its resources, freeing space for waiting sequences.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Block Manager: KV Cache Control Plane&lt;/head&gt;
    &lt;p&gt;The Block Manager is where vLLM√¢s memory management innovation lives. To understand it, we first need to introduce a new resource unit: the block.&lt;/p&gt;
    &lt;head rend="h3"&gt;From Sequences to Blocks&lt;/head&gt;
    &lt;p&gt;A sequence is a variable-length array of tokens√¢it can be 10 tokens or 10,000. But variable-length allocations are inefficient for GPU memory management. The Block Manager solves this by dividing sequences into fixed-size blocks (default: 256 tokens each).&lt;/p&gt;
    &lt;p&gt;A 700-token sequence would occupy three blocks: two full blocks (256 tokens each) and one partial block (188 tokens, with 68 slots unused). Importantly, tokens from different sequences never share a block√¢but a long sequence will span multiple blocks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prefix Caching via Hashing&lt;/head&gt;
    &lt;p&gt;Here√¢s where it gets clever. Each block√¢s content is hashed, and the Block Manager maintains a hash-to-block-id mapping. When a new sequence arrives, the system computes hashes for its blocks and checks if any already exist in the cache.&lt;/p&gt;
    &lt;p&gt;If a block with the same hash exists, the system reuses it by incrementing a reference count√¢no redundant computation or storage needed. This is particularly powerful for scenarios where many requests share common prefixes (like system prompts in chat applications). The prefix only needs to be computed once; subsequent requests can reuse the cached results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Control Plane vs. Data Plane&lt;/head&gt;
    &lt;p&gt;A subtle but important point: the Block Manager lives in CPU memory and only tracks metadata√¢which blocks are allocated, their reference counts, and hash mappings. The actual KV cache data lives on the GPU. The Block Manager is the control plane; the GPU memory is the data plane. This separation allows fast allocation decisions without touching GPU memory until actual computation happens.&lt;/p&gt;
    &lt;p&gt;When blocks are deallocated, the Block Manager marks them as free immediately, but the GPU memory isn√¢t zeroed√¢it√¢s simply overwritten when the block is reused. This avoids unnecessary memory operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Model Runner: Execution and Parallelism&lt;/head&gt;
    &lt;p&gt;The Model Runner is responsible for actually executing the model on GPU(s). When the step loop retrieves a batch of sequences from the Scheduler, it passes them to the Model Runner along with the action (prefill or decode).&lt;/p&gt;
    &lt;head rend="h3"&gt;Tensor Parallel Communication&lt;/head&gt;
    &lt;p&gt;When a model is too large for a single GPU, Nano-vLLM supports tensor parallelism (TP)√¢splitting the model across multiple GPUs. With TP=8, for example, eight GPUs work together to run a single model.&lt;/p&gt;
    &lt;p&gt;The communication architecture uses a leader-worker pattern:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rank 0 (Leader): Receives commands from the step loop, executes its portion, and coordinates with workers.&lt;/item&gt;
      &lt;item&gt;Ranks 1 to N-1 (Workers): Continuously poll a shared memory buffer for commands from the leader.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the leader receives a &lt;code&gt;run&lt;/code&gt; command, it writes the method name and arguments to shared memory. Workers detect this, read the parameters, and execute the same operation on their respective GPUs. Each worker knows its rank, so it can compute its designated portion of the work. This shared-memory approach is efficient for single-machine multi-GPU setups, avoiding network overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparing for Computation&lt;/head&gt;
    &lt;p&gt;Before invoking the model, the Model Runner prepares the input based on the action:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Prepare Prefill: Batches multiple sequences with variable lengths, computing cumulative sequence lengths for efficient attention computation.&lt;/item&gt;
      &lt;item&gt;Prepare Decode: Batches single tokens (one per sequence) with their positions and slot mappings for KV cache access.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This preparation also involves converting CPU-side token data into GPU tensors√¢the point where data crosses from CPU memory to GPU memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;CUDA Graphs: Reducing Kernel Launch Overhead&lt;/head&gt;
    &lt;p&gt;For decode steps (which process just one token per sequence), kernel launch overhead can become significant relative to actual computation. CUDA Graphs address this by recording a sequence of GPU operations once, then replaying them with different inputs. Nano-vLLM pre-captures CUDA graphs for common batch sizes (1, 2, 4, 8, 16, up to 512), allowing decode steps to execute with minimal launch overhead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sampling: From Logits to Tokens&lt;/head&gt;
    &lt;p&gt;The model doesn√¢t output a single token√¢it outputs logits, a probability distribution over the entire vocabulary. The final step is sampling: selecting one token from this distribution.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;temperature&lt;/code&gt; parameter controls this selection. Mathematically, it adjusts the shape of the probability distribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Low temperature (approaching 0): The distribution becomes sharply peaked. The highest-probability token is almost always selected, making outputs more deterministic and focused.&lt;/item&gt;
      &lt;item&gt;High temperature: The distribution flattens. Lower-probability tokens have a better chance of being selected, making outputs more diverse and creative.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is where the √¢randomness√¢ in LLM outputs comes from√¢and why the same prompt can produce different responses. The sampling step selects from a valid range of candidates, introducing controlled variability.&lt;/p&gt;
    &lt;head rend="h2"&gt;What√¢s Next&lt;/head&gt;
    &lt;p&gt;In Part 2, we√¢ll open the black box of model. We√¢ll explore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How the model transforms tokens into hidden states and back&lt;/item&gt;
      &lt;item&gt;The attention mechanism and why multi-head attention matters&lt;/item&gt;
      &lt;item&gt;How KV cache is physically laid out on GPU memory&lt;/item&gt;
      &lt;item&gt;Dense vs. MoE (Mixture of Experts) architectures&lt;/item&gt;
      &lt;item&gt;How tensor parallelism works at the computation level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Understanding these internals will complete the picture√¢from prompt string to generated text, with nothing left hidden.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46855447</guid><pubDate>Mon, 02 Feb 2026 12:52:35 +0000</pubDate></item><item><title>Geologists may have solved mystery of Green River's 'uphill' route</title><link>https://phys.org/news/2026-01-geologists-mystery-green-river-uphill.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46855803</guid><pubDate>Mon, 02 Feb 2026 13:29:13 +0000</pubDate></item><item><title>Waymo Seeking About $16B Near $110B Valuation</title><link>https://www.bloomberg.com/news/articles/2026-01-31/waymo-seeking-about-16-billion-near-110-billion-valuation</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46856854</guid><pubDate>Mon, 02 Feb 2026 15:08:52 +0000</pubDate></item><item><title>Show HN: Stelvio ‚Äì Ship Python to AWS</title><link>https://stelvio.dev/</link><description>&lt;doc fingerprint="dc9a0351a8a4b1b2"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Ship Python to AWS&lt;lb/&gt;in minutes, not days&lt;/head&gt;&lt;p&gt; Infrastructure as Python. No YAML. Smart defaults.&lt;lb/&gt; Full control. &lt;/p&gt;&lt;code&gt;$ pip install stelvio&lt;/code&gt;&lt;head rend="h2"&gt;Supported Components&lt;/head&gt;&lt;p&gt;Everything you need for cloud apps, in Python.&lt;/p&gt;&lt;p&gt;Deploy Python functions to AWS Lambda. Link to other resources for automatic permissions.&lt;/p&gt;&lt;code&gt;from stelvio.aws.function import Function
from stelvio.aws.s3 import Bucket

bucket = Bucket("reports")

# Link grants permissions automatically
Function(
    "processor",
    handler="functions/process.handler",
    links=[bucket],
)
&lt;/code&gt;&lt;p&gt;Learn more about Lambda functions ‚Üí&lt;/p&gt;&lt;p&gt;Schedule Lambda functions with cron expressions or rate intervals.&lt;/p&gt;&lt;code&gt;from stelvio.aws.cron import Cron

Cron(
    "hourly-cleanup",
    "rate(1 hour)",
    "functions/cleanup.handler",
)
Cron(
    "daily-report",
    "cron(0 9 * * ? *)",
    "functions/report.handler",
)
&lt;/code&gt;&lt;p&gt;Learn more about Event scheduling ‚Üí&lt;/p&gt;&lt;p&gt;Securely store any amount of data with AWS S3 Buckets.&lt;/p&gt;&lt;code&gt;from stelvio.aws.s3 import Bucket

# Create a bucket that triggers a function on new uploads
uploads = Bucket("user-uploads")
uploads.notify(
    "functions/process_upload.handler",
    events=["s3:ObjectCreated:*"],
)
&lt;/code&gt;&lt;p&gt;Learn more about S3 Buckets ‚Üí&lt;/p&gt;&lt;p&gt;Create DynamoDB tables and link them to functions for automatic permissions.&lt;/p&gt;&lt;code&gt;from stelvio.aws.dynamo_db import DynamoTable
from stelvio.aws.function import Function

table = DynamoTable(
    name="users",
    partition_key="user_id",
    sort_key="created_at",
)

Function(
    "user-handler",
    handler="functions/user.handler",
    links=[table],
)
&lt;/code&gt;&lt;p&gt;Learn more about DynamoDB ‚Üí&lt;/p&gt;&lt;p&gt;Decouple services with SQS queues and SNS topics.&lt;/p&gt;&lt;code&gt;from stelvio.aws.queue import Queue
from stelvio.aws.topic import Topic

orders = Queue("orders")
orders.subscribe(
    "processor",
    "functions/process_order.handler",
)

alerts = Topic("alerts")
alerts.subscribe(
    "notifier",
    "functions/send_alert.handler",
)
&lt;/code&gt;&lt;p&gt;Learn more about SQS Queues &amp;amp; SNS Topics ‚Üí&lt;/p&gt;&lt;p&gt;Send high-volume emails securely and reliably using Amazon SES.&lt;/p&gt;&lt;code&gt;from stelvio.aws.email import Email
from stelvio.aws.function import Function

mailer = Email(
    "support-email",
    "support@example.com",
)

Function(
    "sender",
    handler="functions/send.handler",
    links=[mailer],
)
&lt;/code&gt;&lt;p&gt;Learn more about Email ‚Üí&lt;/p&gt;&lt;p&gt;Define REST APIs and route requests to Lambda functions or other resources.&lt;/p&gt;&lt;code&gt;from stelvio.aws.apigateway import Api

api = Api(
    "payment-api",
    domain_name="api.example.com",
)

api.route("POST", "/charge", handler="functions/charge.post")
api.route("GET", "/history", handler="functions/history.get")
&lt;/code&gt;&lt;p&gt;Learn more about API Gateway ‚Üí&lt;/p&gt;&lt;p&gt;Connect your Stelvio resources to custom domains with automatic SSL certificates.&lt;/p&gt;&lt;code&gt;app = StelvioApp(
    "my-app",
    dns=CloudflareDns("your-cloudflare-zone-id")
    # other configurations...
)

...

api = Api(
    "payment-api",
    domain_name="api.example.com",
)

api.route("POST", "/charge", handler="functions/charge.post")
api.route("GET", "/history", handler="functions/history.get")
&lt;/code&gt;&lt;p&gt;Learn more about Custom Domains ‚Üí&lt;/p&gt;&lt;p&gt;Combine multiple Stelvio resources under the same custom domain using a Router.&lt;/p&gt;&lt;code&gt;domain_name = "example.com"

bucket = Bucket("static-files-bucket")

api = Api("my-api")
api.route("GET", "/api", "functions/hello.handler")

router = Router("router-example", custom_domain=domain_name)
router.route("/files", bucket)
router.route("/api", api)
&lt;/code&gt;&lt;p&gt;Learn more about Router ‚Üí&lt;/p&gt;&lt;head rend="h2"&gt;See Stelvio in Action&lt;/head&gt;&lt;p&gt;Watch how Stelvio bridges the gap between simple scripting and complex infrastructure. Build serverless applications with the language you love.&lt;/p&gt;&lt;head rend="h2"&gt;Why devs love Stelvio&lt;/head&gt;&lt;p&gt;Ship fast without fighting infrastructure.&lt;/p&gt;&lt;head rend="h3"&gt;Pure Python&lt;/head&gt;&lt;p&gt;No new language to learn. If you know Python, you know Stelvio. Your IDE, linter, and type checker all just work.&lt;/p&gt;&lt;head rend="h3"&gt;Smart Defaults&lt;/head&gt;&lt;p&gt;Sensible configurations out of the box. Simple things stay simple. Add configuration only when you need it.&lt;/p&gt;&lt;head rend="h3"&gt;Automated Permissions&lt;/head&gt;&lt;p&gt;Connect functions to databases with one line. IAM policies and environment variables are configured automatically. We call it linking.&lt;/p&gt;&lt;head rend="h3"&gt;Live Dev Mode&lt;/head&gt;&lt;p&gt;&lt;code&gt;stlv dev&lt;/code&gt; runs your Lambda code locally while infrastructure stays in AWS. No redeploy on every change.&lt;/p&gt;&lt;head rend="h3"&gt;Full Control&lt;/head&gt;&lt;p&gt;Override any default when you need to. Access underlying Pulumi resources for complete customization.&lt;/p&gt;&lt;head rend="h3"&gt;Open Source&lt;/head&gt;&lt;p&gt;Apache 2.0 licensed. Free forever. Contribute, fork, or self-host with confidence.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to ship?&lt;/head&gt;&lt;p&gt;Get your first Lambda function deployed in under 5 minutes.&lt;/p&gt;Start Shipping ‚Üí&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46856899</guid><pubDate>Mon, 02 Feb 2026 15:12:39 +0000</pubDate></item><item><title>We asked 15,000 European devs about jobs, salaries, and AI [pdf]</title><link>https://static.germantechjobs.de/market-reports/European-Transparent-IT-Job-Market-Report-2025.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857124</guid><pubDate>Mon, 02 Feb 2026 15:31:35 +0000</pubDate></item><item><title>Swift in the Browser with ElementaryUI (Swift FOSDEM 2026 Talk) [video]</title><link>https://www.youtube.com/watch?v=OmQ881sOTIc</link><description>&lt;doc fingerprint="50559455455d1642"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2026 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857290</guid><pubDate>Mon, 02 Feb 2026 15:46:10 +0000</pubDate></item><item><title>Kernighan on Programming</title><link>https://news.ycombinator.com/item?id=46857444</link><description>&lt;doc fingerprint="86ca08d6e7151ed7"&gt;
  &lt;main&gt;
    &lt;p&gt;"Debugging is twice as hard as writing the code in the first place. Therefore, if you write the code as cleverly as possible, you are, by definition, not smart enough to debug it"&lt;/p&gt;
    &lt;p&gt;So is reviewing and verifying code. Maybe not twice as "hard" if you're skilled in such things. But most programmers I've worked with can't be bothered to write tests let alone verify correctness by other means (good tests, property tests, types, model checking, etc).&lt;/p&gt;
    &lt;p&gt;It's one thing to point out small trivialities like initialization and life time issues in a small piece of code. But it's quite another to prove they don't exist in a large code base.&lt;/p&gt;
    &lt;p&gt;Kernigan is a good source of quotes and thinking on programming.&lt;/p&gt;
    &lt;p&gt;I am fascinated by the prevalence of wanting "tests" from hacker news comments. Most of the code I have worked on in the past 20 years did not have tests. Most of it was shopping carts, custom data transformation code, orchestrating servers, plugin code functionality to change some aspect of a website.&lt;/p&gt;
    &lt;p&gt;Now, I have had to do some salesforce apex coding and the framework requires tests. So I write up some dummy data of a user and a lead and pass it through the code, but it feels of limited value, almost like just additional ceremony. Most of the bugs I see are from a misconception of different users about what a flag means. I can not think of a time a test caught something.&lt;/p&gt;
    &lt;p&gt;The organization is huge and people do not go and run all the code every time some other area of the system is changed. Maybe they should? But I doubt that would ever happen given the politics of the organization.&lt;/p&gt;
    &lt;p&gt;So I am curious, what are the kinds of tests do people write in other areas of the industry?&lt;/p&gt;
    &lt;p&gt;what are the kinds of tests do people write in other areas of the industry?&lt;/p&gt;
    &lt;p&gt;Aerospace here. Roughly this would be typical:&lt;/p&gt;
    &lt;p&gt;- comprehensive requirements on the software behavior, with tests to verify those requirements. Tests are automated as much as possible (e.g., scripts rather than manual testing)&lt;/p&gt;
    &lt;p&gt;- tests are generally run first in a test suite in a completely virtual software environment&lt;/p&gt;
    &lt;p&gt;- structural coverage analysis (depending on level of criticality) to show that all code in the subsystem was executed by the testing (or adequately explain why the testing can't hit that code)&lt;/p&gt;
    &lt;p&gt;- then once that passes, run the same tests in a hardware lab environment, testing the software as it runs on the the actual physical component that will be installed on the plane&lt;/p&gt;
    &lt;p&gt;- then test that actually on a plane, through a series of flight tests. (The flight testing would likely not be as entirely comprehensive as the previous steps)&lt;/p&gt;
    &lt;p&gt;A full round of testing is very time-consuming and expensive, and as much as possible should be caught and fixed in the virtual software tests before it even gets to the hardware lab, much less to the plane.&lt;/p&gt;
    &lt;p&gt;The value of tests is when the fail they show you of something you broke that you didn't realize. 80% (likely more, but I don't know how to measure) of the tests I write could safely be thrown away because they fail again - but I don't know which tests will fail and thus inform me that I broke things.&lt;/p&gt;
    &lt;p&gt;The system I'm working on has been in production for 12 years - we have added a lot of new features over those years. Many of those needed us to hook into existing code, tests help us know that we didn't break something that used to work.&lt;/p&gt;
    &lt;p&gt;Maybe that helps answer the question of why they are important to me. They might not be to your problems.&lt;/p&gt;
    &lt;p&gt;The real question is whether ‚Äúdebugging‚Äù the LLM is going to be as effective as debugging the code.&lt;/p&gt;
    &lt;p&gt;IME it pays dividends but it can be really painful. I‚Äôve run into a situation multiple times where I‚Äôm using Claude Code to write something, then a week later while working it‚Äôll come up with something like ‚ÄúOh wait! Half the binaries are in .Net and not Delphi, I can just decompile them with ilspy‚Äù, effectively showing the way to a better rewrite that works better with fewer bugs that gets done in a few hours because I‚Äôve got more experience from the v1. Either way it‚Äôs tens of thousands of lines of code that I could never have completed myself in that amount of time (which, given problems of motivation, means ‚Äúat all‚Äù).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857444</guid><pubDate>Mon, 02 Feb 2026 15:57:32 +0000</pubDate></item><item><title>Ask HN: Who is hiring? (February 2026)</title><link>https://news.ycombinator.com/item?id=46857488</link><description>&lt;doc fingerprint="3741895a340aea64"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company‚Äîno recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to replying to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss this other fine thread: Who wants to be hired? https://news.ycombinator.com/item?id=46857487&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46857488</guid><pubDate>Mon, 02 Feb 2026 16:01:30 +0000</pubDate></item></channel></rss>