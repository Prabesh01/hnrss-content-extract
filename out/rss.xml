<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 09 Oct 2025 20:11:06 +0000</lastBuildDate><item><title>Nobel Prize in Literature 2025: L√°szl√≥ Krasznahorkai</title><link>https://www.nobelprize.org/prizes/literature/2025/press-release/</link><description>&lt;doc fingerprint="5757b33da9153196"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Press release&lt;/head&gt;
    &lt;p&gt;English&lt;lb/&gt;English [pdf]&lt;lb/&gt;Swedish&lt;lb/&gt;Swedish [pdf]&lt;/p&gt;
    &lt;p&gt;The Permanent Secretary&lt;/p&gt;
    &lt;p&gt;Press Release&lt;lb/&gt;9 October 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;The Nobel Prize in Literature 2025&lt;/head&gt;
    &lt;head rend="h2"&gt;L√°szl√≥ Krasznahorkai&lt;/head&gt;
    &lt;p&gt;The Nobel Prize in Literature for 2025 is awarded to the Hungarian author L√°szl√≥ Krasznahorkai,&lt;/p&gt;
    &lt;p&gt;‚Äúfor his compelling and visionary oeuvre that, in the midst of apocalyptic terror, reaffirms the power of art‚Äù.&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6‚Äì13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
    &lt;head rend="h3"&gt;Explore prizes and laureates&lt;/head&gt;
    &lt;p&gt; Look for popular awards and laureates in different fields, and discover the history of the Nobel Prize. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527003</guid><pubDate>Thu, 09 Oct 2025 12:54:18 +0000</pubDate></item><item><title>Figure 03, our 3rd generation humanoid robot</title><link>https://www.figure.ai/news/introducing-figure-03</link><description>&lt;doc fingerprint="ec95cddb80b22ecf"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we‚Äôre introducing Figure 03, our 3rd generation humanoid robot. Figure 03 is designed for Helix, the home, and the world at scale. Our goal is to deliver a truly general-purpose robot - one that can perform human-like tasks and learn directly from people. To realize this vision, our engineering and design teams completed a ground-up hardware and software redesign to ship Figure 03 for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Helix: Figure 03 features a completely redesigned sensory suite and hand system which is purpose-built to enable Helix - Figure's proprietary vision-language-action AI.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The home: Figure 03 has several new features, including soft goods, wireless charging, improved audio system for voice reasoning, and battery safety advancements that make it safer and easier to use in a home environment.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mass manufacturing: Figure 03 was engineered from the ground-up for high-volume manufacturing. In order to scale, we established a new supply chain and entirely new process for manufacturing humanoid robots at BotQ.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The world at scale: The lower manufacturing cost and the advancements made for Helix have significant benefits for commercial applications.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Designed for Helix&lt;/head&gt;
    &lt;p&gt;There‚Äôs no path to scaling humanoid robots without AI. That‚Äôs why we built Figure 03 around one goal - to enable true reasoning throughout the world using Helix. Figure 03 introduces a fully redesigned sensory suite and hand system, purpose-built to bring Helix to life.&lt;/p&gt;
    &lt;p&gt;Figure 03 introduces a next-generation vision system engineered for high-frequency visuomotor control. Its new camera architecture delivers twice the frame rate, one-quarter the latency, and a 60% wider field of view per camera - all within a more compact form factor. Combined with an expanded depth of field, this architecture provides Helix with a denser, more stable perceptual stream. These advancements are essential for intelligent navigation and precise manipulation in complex, cluttered spaces such as homes.&lt;/p&gt;
    &lt;p&gt;Each hand now integrates an embedded palm camera with a wide field of view and low-latency sensing, which offers redundant, close-range visual feedback during grasps. These cameras allow Helix to maintain visual awareness even when the main cameras are occluded (i.e. when reaching into a cabinet or working in confined spaces) and enable continuous, adaptive control in real time.&lt;/p&gt;
    &lt;p&gt;The Figure 03 hands represent a major leap in compliant and tactile design. Softer, more adaptive fingertips increase surface contact area, enabling more stable grasps across objects of varied shapes and sizes. After surveying existing market options, Figure found that current tactile sensors had inherent limitations that could not withstand real-world use. This led to the internal development of our first-generation tactile sensor, guided by three principles: extreme durability, long-term reliability, and high-fidelity sensing.&lt;/p&gt;
    &lt;p&gt;Each fingertip sensor can detect forces as small as three grams of pressure - sensitive enough to register the weight of a paperclip resting on your finger. This precision enables Helix to distinguish between a secure grip and an impending slip before it occurs, allowing fine-grained, dexterous control over fragile, irregular, or moving objects.&lt;/p&gt;
    &lt;p&gt;Figure 03 also includes 10 Gbps mmWave data offload capability, allowing the entire fleet to upload terabytes of data for continuous learning and improvement. Together, these advancements position Figure 03 as uniquely capable of large-scale, end-to-end pixels-to-action learning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for the Home&lt;/head&gt;
    &lt;p&gt;To operate effectively in the home, a robot must work seamlessly alongside people in their daily environments. With this in mind, Figure 03 introduces several design improvements focused on safety. It features strategically placed multi-density foam to protect against pinch points, and is covered in soft textiles rather than hard machined parts. Figure 03 also has 9% less mass and significantly less volume than Figure 02, making it easier to maneuver through household spaces.&lt;/p&gt;
    &lt;p&gt;The Figure 03 battery pushes the bounds for robot battery safety and incorporates multiple layers of protection against abuse or malfunction, including safeguards at the Battery Management System (BMS), cell, interconnect, and pack levels. The battery has already achieved certification to the UN38.3 standard.&lt;/p&gt;
    &lt;p&gt;Beyond safety, Figure 03 is designed for everyday usability. The soft goods are fully washable and can be removed or replaced without tools, allowing quick and easy swaps. The robot can also be customized with various clothing options, including garments made from cut-resistant and durable materials.&lt;/p&gt;
    &lt;p&gt;To make it easier to communicate naturally with the robot, Figure 03 features an upgraded audio hardware system for better real time speech-to-speech. Compared with Figure 02, its speaker is twice the size and nearly four times more powerful, while the microphone has been repositioned for improved performance and clarity.&lt;/p&gt;
    &lt;p&gt;Continuing our vision for a fully autonomous, wire-free system, Figure 03 is capable of wireless inductive charging alongside wireless data offload. Charging coils in the robot‚Äôs feet allow it to simply step onto a wireless stand and charge at 2 kW. In a home setting, this means the robot can automatically dock and recharge itself as needed throughout the day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for Mass Manufacturing&lt;/head&gt;
    &lt;p&gt;Humanoid robots have traditionally been designed as engineering prototypes which are time consuming and expensive to produce. Figure 03 is our first robot engineered from the ground-up for high-volume manufacturing. We achieved this through three major initiatives:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Design and process reinvention&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Establishing an entirely new supply chain&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The invention of BotQ, our high-volume manufacturing facility&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Moving from Figure 02 to Figure 03 required redesigning nearly every component of the robot with manufacturability and cost in mind. The mechanical and electrical engineering teams aggressively reduced part count, assembly steps, and any components that were not absolutely critical to meet design requirements. While Figure 02 was primarily designed to be manufactured with CNC machining, Figure 03 relies heavily on tooled processes such as die-casting, injection molding, and stamping. This shift demanded a significant up-front investment in tooling, but the payoff is clear: each Figure 03 unit now costs dramatically less to build, with the economics improving as volumes grow.&lt;/p&gt;
    &lt;p&gt;To scale Figure 03, Figure had to build an entirely new supply chain for an industry where one does not currently exist. Figure chose to vertically integrate across many critical module builds including actuators, batteries, sensors, structures, and electronics, all of which were designed completely in-house. For individual components, Figure strategically identified and partnered with suppliers capable of meeting the required volumes, timelines, and strict quality standards demanded by the team. The result of this year-long effort is a global network of partners who can grow alongside Figure and meet production goals of thousands and eventually millions of parts under an aggressive ramp schedule.&lt;/p&gt;
    &lt;p&gt;BotQ is Figure‚Äôs dedicated manufacturing facility designed to scale robot production. BotQ‚Äôs first-generation manufacturing line will initially be capable of producing up to 12,000 humanoid robots per year, with the goal of producing a total of 100,000 robots over the next four years. Instead of relying on contract manufacturers, Figure brought production of its most critical systems in-house to maintain tight control over quality, iteration, and speed. The facility is equipped with state-of-the-art systems and digital integrations, anchored by our internally developed Manufacturing Execution System (MES). Every subassembly and final assembly passes through this line with full traceability, ensuring quality, repeatability, and continuous improvement.&lt;/p&gt;
    &lt;head rend="h2"&gt;Designed for the World at Scale&lt;/head&gt;
    &lt;p&gt;Figure‚Äôs focus on the home market in no way detracts from the potential of Figure 03 for the commercial market. By solving for the variability and intractability of the home, Figure is developing a truly general-purpose product that can do the widest possible range of tasks in the workforce.&lt;/p&gt;
    &lt;p&gt;Figure 03 is well suited for commercial applications for several reasons. The actuators can perform at 2x faster speeds with improved torque density (nm/kg). The most significant result of this is our ability to pick and place items at faster speeds.&lt;/p&gt;
    &lt;p&gt;The improvements to the hands and sensory suite made for Helix are of major significance for commercial use cases. With the camera and perception system upgrades, Figure 03 will be able to intelligently navigate commercial environments and execute precise manipulation. The changes to the hands highlighted above (added compliance, fingertip surface area, tactile sensing) enable better and more stable grasps across an array of objects such as small pieces of sheet metal and deformable poly bags.&lt;/p&gt;
    &lt;p&gt;Thanks to inductive charging, Figure 03 is capable of near-continuous operation as long as it can step onto a charging mat for a certain period of time during the use case. The fast wireless data offload also means that the robot can offload seamlessly during shift breaks just by returning to the dock.&lt;/p&gt;
    &lt;p&gt;Commercial customers can also design distinct uniforms for their Figure 03 fleet, with the option to use more durable, or cut-proof materials, and make other design changes for specific environments. New side screens on Figure 03 even allow quick identification across large fleets and can be fully customized to match each customer‚Äôs branding or operational needs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Figure 03 represents an unprecedented advancement in taking humanoid robots from experimental prototypes to deployable, scalable products. By uniting advanced perception and tactile intelligence with home-safe design and mass-manufacturing readiness, Figure has built a platform capable of learning, adapting, and working across both domestic and commercial settings. Designed for Helix, the home, and the world at scale, Figure 03 establishes the foundation for true general-purpose robotics - one capable of transforming how people live and work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527402</guid><pubDate>Thu, 09 Oct 2025 13:27:14 +0000</pubDate></item><item><title>Using a laptop as an HDMI monitor for an SBC</title><link>https://danielmangum.com/posts/laptop-hdmi-monitor-sbc/</link><description>&lt;doc fingerprint="2e756d7589369bd2"&gt;
  &lt;main&gt;
    &lt;p&gt;Though I spend the majority of my time working with microcontroller class devices, I also have an embarassingly robust collection of single board computers (SBC), including a few different Raspberry Pi models, the BeagleV Starlight Beta (RIP), and more. Typically when setting up these devices for whatever automation task I have planned for them, I‚Äôll use ‚Äúheadless mode‚Äù and configure initial user and network credentials when writing the operating system to the storage device using a tool like Raspberry Pi‚Äôs Imager.&lt;/p&gt;
    &lt;p&gt;However, sometimes direct physical access to the SBC with a monitor and keyboard is useful for initial configuration, maintenance operations, or workloads that have a visual component. As someone who doesn‚Äôt use any external monitors1 for my daily development, digging up an HDMI monitor, finding somewhere to put it, and connecting it to the device is an annoying process. Furthermore, if I‚Äôm on the go I almost certainly don‚Äôt have easy access to an external monitor.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Raspberry Pi boot logs shown in VLC media player.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Fortunately, I rarely ever do this because I have a handful of HDMI to USB capture cards, ranging from extremely cheap variants from Amazon, to the higher quality Elgato Cam Link 4k. These are typically used for live streaming a video feed from DSLR / mirrorless cameras or gaming consoles, but they also serve as a great option for capturing video from any other device that has HDMI output. On my System76 Linux daily driver laptop, I can use any number of different video playback applications to display the HDMI output via the capture card. For longer term use cases, I can breathe new life into one of my old laptops, using the capture card to effectively convert it into a monitor.&lt;/p&gt;
    &lt;code&gt;vlc v4l2:///dev/video0
&lt;/code&gt;
    &lt;code&gt;ffplay /dev/video0
&lt;/code&gt;
    &lt;code&gt;cheese v4l2:///dev/video0
&lt;/code&gt;
    &lt;p&gt;If you do want to stream or record the SBC output, OBS will give you even more control. You‚Äôll still need a USB keyboard, but I already use one with my laptop, so temporarily plugging it into the SBC for configuration while I use the laptop as a monitor is minimally disruptive. However, if you find yourself regularly needing to connect to multiple machines, it might be time to consider getting a KVM switch.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Yeah, I just sit here with my one laptop screen. Can you believe that? ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527507</guid><pubDate>Thu, 09 Oct 2025 13:36:01 +0000</pubDate></item><item><title>Python's splitlines does more than just newlines</title><link>https://yossarian.net/til/post/python-s-splitlines-does-a-lot-more-than-just-newlines/</link><description>&lt;doc fingerprint="e795b146e9a118bf"&gt;
  &lt;main&gt;
    &lt;p&gt;(With thanks to Seth Larson for taking me down this rabbit hole.)&lt;/p&gt;
    &lt;p&gt;I always assumed that Python's &lt;code&gt;str.splitlines()&lt;/code&gt; split strings by
"universal newlines", i.e., &lt;code&gt;\n&lt;/code&gt;, &lt;code&gt;\r&lt;/code&gt;, and &lt;code&gt;\r\n&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;But it turns out it does a lot more than that. From the docs:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;This method splits on the following line boundaries. In particular, the boundaries are a superset of universal newlines.&lt;/p&gt;&lt;th&gt;Representation&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;code&gt;\n&lt;/code&gt;&lt;td&gt;Line Feed&lt;/td&gt;&lt;code&gt;\r&lt;/code&gt;&lt;td&gt;Carriage Return&lt;/td&gt;&lt;code&gt;\r\n&lt;/code&gt;&lt;td&gt;Carriage Return + Line Feed&lt;/td&gt;&lt;code&gt;\v&lt;/code&gt;√Ç or√Ç&lt;code&gt;\x0b&lt;/code&gt;&lt;td&gt;Line Tabulation&lt;/td&gt;&lt;code&gt;\f&lt;/code&gt;√Ç or√Ç&lt;code&gt;\x0c&lt;/code&gt;&lt;td&gt;Form Feed&lt;/td&gt;&lt;code&gt;\x1c&lt;/code&gt;&lt;td&gt;File Separator&lt;/td&gt;&lt;code&gt;\x1d&lt;/code&gt;&lt;td&gt;Group Separator&lt;/td&gt;&lt;code&gt;\x1e&lt;/code&gt;&lt;td&gt;Record Separator&lt;/td&gt;&lt;code&gt;\x85&lt;/code&gt;&lt;td&gt;Next Line (C1 Control Code)&lt;/td&gt;&lt;code&gt;\u2028&lt;/code&gt;&lt;td&gt;Line Separator&lt;/td&gt;&lt;code&gt;\u2029&lt;/code&gt;&lt;td&gt;Paragraph Separator&lt;/td&gt;&lt;/quote&gt;
    &lt;p&gt;This results in some surprising (to me) splitting behavior:&lt;/p&gt;
    &lt;code&gt;&amp;gt;&amp;gt;&amp;gt;   =  
&amp;gt;&amp;gt;&amp;gt;  
 
&lt;/code&gt;
    &lt;p&gt;Whereas I would have expected:&lt;/p&gt;
    &lt;p&gt;This was a good periodic reminder that Unicode does not mean "printable," and that there are still plenty of ecosystems that assign semantics to C0 and C1 control codes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45527758</guid><pubDate>Thu, 09 Oct 2025 13:55:04 +0000</pubDate></item><item><title>New nanotherapy clears amyloid-Œ≤, reversing symptoms of Alzheimer's in mice</title><link>https://www.drugtargetreview.com/news/189235/new-nanotherapy-clears-amyloid-%CE%B2-reversing-alzheimers-in-mice/</link><description>&lt;doc fingerprint="676d66b9292b9d40"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New nanotherapy clears amyloid-Œ≤ reversing Alzheimer‚Äôs in mice&lt;/head&gt;
    &lt;p&gt;Posted: 7 October 2025 | Drug Target Review | No comments yet&lt;/p&gt;
    &lt;p&gt;Researchers have developed bioactive nanoparticles that restore the brain‚Äôs blood-brain barrier and clear toxic proteins, reversing Alzheimer‚Äôs symptoms in mice and offering a promising new approach to treating the disease.&lt;/p&gt;
    &lt;p&gt;A new study by a research team co-led by the Institute for Bioengineering of Catalonia (IBEC) and West China Hospital Sichuan University (WCHSU), in collaboration with partners in the UK, have used a nanotechnology strategy that reverses Alzheimer‚Äôs disease in mice. Unlike traditional nanomedicine, which relies on nanoparticles as carriers for therapeutic molecules, this approach uses nanoparticles that are bioactive, called ‚Äòsupramolecular drugs.‚Äô&lt;/p&gt;
    &lt;p&gt;Instead of targeting neurons directly, this method focuses on repairing the blood-brain barrier (BBB), the brain‚Äôs defence against harmful substances. By restoring proper BBB function, the researchers achieved a reversal of Alzheimer‚Äôs pathology in animal models.&lt;/p&gt;
    &lt;head rend="h2"&gt;The importance of brain vasculature&lt;/head&gt;
    &lt;p&gt;The brain consumes the greatest amount energy out of any other organ in the body, consuming 20 percent in adults and up to 60 percent in children. This energy is delivered through a dense vascular system in which each neuron is nourished by a capillary. With approximately one billion capillaries in the human brain, maintaining vascular health is crucial, particularly in conditions like Alzheimer‚Äôs, where vascular function is weakened and linked to disease progression.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; Biomarkers are redefining how precision therapies are discovered, validated and delivered. &lt;/head&gt;
    &lt;p&gt;This exclusive expert-led report reveals how leading teams are using biomarker science to drive faster insights, cleaner data and more targeted treatments ‚Äì from discovery to diagnostics.&lt;/p&gt;
    &lt;p&gt;Inside the report:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How leading organisations are reshaping strategy with biomarker-led approaches&lt;/item&gt;
      &lt;item&gt;Better tools for real-time decision-making ‚Äì turning complex data into faster insights&lt;/item&gt;
      &lt;item&gt;Global standardisation and assay sensitivity ‚Äì what it takes to scale across networks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Discover how biomarker science is addressing the biggest hurdles in drug discovery, translational research and precision medicine ‚Äì access your free copy today&lt;/p&gt;
    &lt;p&gt;The BBB is a protective barrier between the brain and blood flow, stopping harmful substances such as pathogens and toxins from entering. By targeting specific mechanisms within the BBB, the research team enabled the removal of undesirable waste proteins produced in the brain. In Alzheimer‚Äôs disease, the main waste protein is amyloid-Œ≤ (AŒ≤), whose accumulation disrupts normal brain function.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rapid reduction of amyloid-Œ≤&lt;/head&gt;
    &lt;p&gt;The researchers tested their approach in mice genetically programmed to overproduce AŒ≤ and develop cognitive decline similar to Alzheimer‚Äôs pathology. Following only three doses of supramolecular drugs, the team observed significant therapeutic effects.&lt;/p&gt;
    &lt;p&gt;Following only three doses of supramolecular drugs, the team observed significant therapeutic effects.&lt;/p&gt;
    &lt;p&gt;‚ÄúOnly 1hr after the injection we observed a reduction of 50-60 percent in AŒ≤ amount inside the brain,‚Äù said Junyang Chen, first co-author of the study, researcher at WCHSU and PhD student at University College London.&lt;/p&gt;
    &lt;p&gt;Behavioural tests conducted over several months demonstrated remarkable improvements. In one experiment, a 12-month-old mouse ‚Äì equivalent to a 60-year-old human ‚Äì received the nanoparticles and was evaluated six months later. The animal, now 18 months old ‚Äì comparable to a 90-year-old human ‚Äì exhibited the behaviour of a healthy mouse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Restoring natural clearance mechanisms&lt;/head&gt;
    &lt;p&gt;‚ÄúThe long-term effect comes from restoring the brain‚Äôs vasculature. We think it works like a cascade: when toxic species such as amyloid-beta (AŒ≤) accumulate, disease progresses. But once the vasculature is able to function again, it starts clearing AŒ≤ and other harmful molecules, allowing the whole system to recover its balance. What‚Äôs remarkable is that our nanoparticles act as a drug and seem to activate a feedback mechanism that brings this clearance pathway back to normal levels,‚Äù explained Giuseppe Battaglia, ICREA Research Professor at IBEC and leader of the study.&lt;/p&gt;
    &lt;p&gt;Normally, the protein LRP1 acts as a molecular gatekeeper, binding to AŒ≤ and transporting it across the BBB for elimination. In Alzheimer‚Äôs, this system becomes fragile, leading to AŒ≤ accumulation. The supramolecular drugs mimic LRP1 ligands, binding to AŒ≤ and initiating its clearance, effectively resetting the system and restoring vascular function.&lt;/p&gt;
    &lt;head rend="h2"&gt;A new therapeutic possibility&lt;/head&gt;
    &lt;p&gt;The nanoparticles are designed using a bottom-up molecular engineering approach, combining precise size control with defined surface ligands to interact with cellular receptors in a highly specific manner. This allows them to modulate receptor function, clear amyloid-Œ≤ and restore vascular balance.&lt;/p&gt;
    &lt;p&gt;Our study demonstrated remarkable efficacy in achieving rapid AŒ≤ clearance&lt;/p&gt;
    &lt;p&gt;‚ÄúOur study demonstrated remarkable efficacy in achieving rapid AŒ≤ clearance, restoring healthy function in the blood‚Äìbrain barrier and leading to a striking reversal of Alzheimer‚Äôs pathology,‚Äù said Lorena Ruiz Perez, researcher at IBEC and Serra Hunter Assistant Professor at the University of Barcelona.&lt;/p&gt;
    &lt;p&gt;The study demonstrates how restoring the brain‚Äôs vascular function with bioactive nanoparticles can clear toxic proteins and reverse cognitive decline in mice. The findings could lead to further development pf new therapies that target vascular health to combat neurodegenerative diseases.&lt;/p&gt;
    &lt;p&gt;Related topics&lt;lb/&gt;Animal Models, Bioengineering, Central Nervous System (CNS), Drug Delivery, Drug Discovery, Nanomedicine, Nanotechnology, Neurosciences, Translational Science&lt;/p&gt;
    &lt;p&gt;Related conditions&lt;lb/&gt;Alzheimer's&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45528308</guid><pubDate>Thu, 09 Oct 2025 14:36:52 +0000</pubDate></item><item><title>Show HN: I've built a tiny hand-held keyboard</title><link>https://github.com/mafik/keyer</link><description>&lt;doc fingerprint="2f4c3473419d83f4"&gt;
  &lt;main&gt;
    &lt;p&gt;Firmware &amp;amp; goodies for making a Keyer (one-handed version of a chorded keyboard).&lt;/p&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Minimal finger movement: it's like typing with all the keys on your home row all the time&lt;/item&gt;
      &lt;item&gt;Free hand while typing: you can use your other hand to sip tea while typing (or move the mouse - if you're not a tea drinking type)&lt;/item&gt;
      &lt;item&gt;Always near your hand - keyer can be attached to a glove so you can just release it and have both of your hands free. Now you can drink your tea and move the mouse at the same time.&lt;/item&gt;
      &lt;item&gt;Tons of chords: a 10-key keyer (3 keys on thumb, 2 index, 2 middle, 2 ring, 1 pinky) can express up to 215 chords (√ó 2 when counting hold-chord alternatives). With so many chords you can lose a finger and still touch type (carpenters love it!)&lt;/item&gt;
      &lt;item&gt;Arpeggios: an additional 2 √ó 78 arpeggios - rolling motion over two keys that can be executed in two directions and can be used for even more input options.&lt;/item&gt;
      &lt;item&gt;Multiple layers: if the 586 shortcuts available on the base layer are somehow not enough for you&lt;/item&gt;
      &lt;item&gt;Rolling chords: when two subsequent chords you're entering share some finger positions you can only move the finger that changes position. When combined with optimized layouts (see the next point) typing is like walking through the keys one finger at a time.&lt;/item&gt;
      &lt;item&gt;Optimized layout: a bundled layout optimizer will perform a combinatorial search over all possible layouts to find the optimal one for typing the texts that you give it (or for your custom finger press / finger movement cost function)&lt;/item&gt;
      &lt;item&gt;Ergonomic layout üññ: did you know your fingers share the neuro-motor pathways and can't always move independently? The layout generator will avoid finger combinations that are hard to press.&lt;/item&gt;
      &lt;item&gt;Low-latency: the firmware relies on hardware interrupts and a zero-latency digital debouncing algorithm to make the keys more responsive than polling-based keyboards (and keyboards with capacitor-based debouncers).&lt;/item&gt;
      &lt;item&gt;Power for months: a massive 18650 battery + underclocked CPU + firmware able to sleep without losing the Bluetooth connection + hardware power switch on the board mean that you will charge it about as often as a Casio watch.&lt;/item&gt;
      &lt;item&gt;üï∂Ô∏è: combine it with smart glasses to control your computer (or smartphone) without looking or touching. It's like Meta EMG wristband but actually working!&lt;/item&gt;
      &lt;item&gt;Easy to build: did you ever play with Play-Doh? This keyer was built with modelling clay (baked in the oven for 30 minutes). No 3D printing. No custom PCBs. You can make it with parts from amazon, a hot glue gun and a soldering iron.&lt;/item&gt;
      &lt;item&gt;Perfect fit: you build it yourself, literally modelling it to the shape of your hand. You can't get more ergonomic than that.&lt;/item&gt;
      &lt;item&gt;Cheap to build: it's less than 50 USD to make one yourself. Mechanical keyboards are a cheap hobby now!&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Penti Chorded Keyboard - A software keyer that can run on a touchscreen. Notable for its use of arpeggios.&lt;/item&gt;
      &lt;item&gt;Keyyyyyyyys! - Can you get cheaper than that?&lt;/item&gt;
      &lt;item&gt;ESP32-BLE-Keyboard - The best way to emulate a BLE keyboard from ESP32.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Welcome to the bottom of the ergonomic mechanical keyboard rabbit hole.&lt;/p&gt;
    &lt;p&gt;Let's start with some shopping.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LILYGO T-Energy S3 development board ($9.70)&lt;/item&gt;
      &lt;item&gt;Samsung INR18650-35E 3500mAh Li-ion battery (~$2.95)&lt;/item&gt;
      &lt;item&gt;FIMO professional modelling clay ($2.75) &lt;list rend="ul"&gt;&lt;item&gt;Alternatively, one of the FIMO effect modelling clays if you'd like to make your keyer out of stone&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10 √ó Gateron G Pro 3.0 mechanical switches (~$10) &lt;list rend="ul"&gt;&lt;item&gt;Alternatively other switches of your choice&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;10 √ó Keycaps (~$8) &lt;list rend="ul"&gt;&lt;item&gt;You only need ten of them so feel free to get the coolest keycaps you can find&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;1m √ó AWG 18 rigid, insulated copper wire (~$1) &lt;list rend="ul"&gt;&lt;item&gt;Get it from a local hardware store, the online stores are ripping you off&lt;/item&gt;&lt;item&gt;You can come with your development board to see which wire gauge fits through the holes on the board&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total: $34.40 (+shipping)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pliers - for bending the copper wire&lt;/item&gt;
      &lt;item&gt;a knife (or a set of sharp teeth) - for stripping the cable insulation&lt;/item&gt;
      &lt;item&gt;(optional) nitryl gloves - for not getting dirty while working with the modelling clay&lt;/item&gt;
      &lt;item&gt;hot glue gun + hot glue sticks - for attaching the components to a wire scaffolding&lt;/item&gt;
      &lt;item&gt;soldering iron + solder wire - for soldering&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With all the materials and tools in hand, the first step is to form a metal scaffolding which will hold the switches in convenient positions. Traditional electronics devices tends to have "exoskeletons" - they're supported by an external case that surrounds them and protects them from your greasy fingers. This device is built around an endoskeleton of copper wire. We'll cover this endoskeleton with modelling clay in a moment. I hope you bought the thickest wire you could (while still fitting through the holes on the board) because in this device it's structural.&lt;/p&gt;
    &lt;p&gt;We'll start with a "GND loop". Cut a section of wire - about 20 or 30cm. Strip its insulation &amp;amp; insert it into one of the GND holes on the board. Solder it in place - it should be firmly attached to the board. Insert the battery and take the board in your hand. Position it like you'd like it to stay in your hand and start bending the wire into a loop that goes through all the places where key switches bases are going to be placed. For some extra rigidity (long wire is fairly bendy) lead the other end of the wire back into another GND hole on the board. You can take the switches with keycaps and place them so that one of their contact points touch the wire. This will give you a better idea of how the keyer is going to end up looking. Don't worry about it being wobbly - we'll use this property to model it a little in a moment. First complete the loop by soldering the other end of the GND loop to the board. If your GND loop happens to pass near other GND holes, you can insert short sections of wire to increase the rigidity of the construction.&lt;/p&gt;
    &lt;p&gt;Once GND loop is complete, take your key switches and attach them to the GND loop so that one of their contact points makes an electrical contact. You can solder them directly but it's a good idea to start with some hot glue to hold them in place. In my version I also bent the contacts on the key switches to make them smaller (DIY low profile) and to take less space.&lt;/p&gt;
    &lt;p&gt;As you're going through the process the keyer is going to become more "complete" and you will be able to bend the wire a little to improve key positioning. Remember that hot glue and solder don't form particularly strong bonds so be careful about bending and ideally use pliers to do that precisely.&lt;/p&gt;
    &lt;p&gt;One word of advice about key positioning is that I've noticed that the keys are "nicest" to press when the axis of pressing goes straight into the palm of your hand. Motions that go parallel to palm of the hand, motions that extend fingers and motions that move fingers to the side are pretty awkward and uncomfortable. I guess our hands evolved to hold things, rather than poke or flick at them. Some keyboard manufacturers might disagree. Their keyboards look undeniably awesome, but this is your keyer and it should be comfortable to use - so make sure the keys are pressed in the same direction that you'd hold something.&lt;/p&gt;
    &lt;p&gt;Once you attached all of the keys, it's time to add even more rigidity into our construction. We'll do this by connecting the remaining contact points on the switches to the GPIO holes on the board. They're marked on the board with text that says "IO##". It doesn't matter which IO port you choose, but write down which key goes to which IO port - it's something that will have to be entered in the firmware. Take a short cut of the wire, strip it at both ends. Bend it (with pliers) so that it fits in the hole and goes straight to the switch. Then solder it in place at both ends. It's important that the wires going to the IO ports don't touch the GND loop. Insulation should help with that.&lt;/p&gt;
    &lt;p&gt;After this step, the keyer should be fairly rigid. Mount the keycaps and see how it feels. It's obviously a little "spiky" but we'll deal with that in the next step. Right now bend the wires to put all the key switches in their right positions.&lt;/p&gt;
    &lt;p&gt;At this point you can go to the "Flashing Firmware" section and check out how your keyer works! It's good to see if you didn't mess anything up so far. The hardest part is over!&lt;/p&gt;
    &lt;p&gt;Now is the time to open up the modelling clay and use it to cover our keyer. Before you begin, remove the keycaps, as they'll only get in the way. Take a small amount of clay and start shaping it in your hand. Squeeze it and fold in half. Repeat this about twenty times. Modelling clay must be mixed like that a little to prevent it from crumbling.&lt;/p&gt;
    &lt;p&gt;Once you have your warm and soft piece of clay, slap it on top of the keyer - wherever you want to cover something. It's important to cover the bottom parts of the switches - that's the part that may prick your fingers. Everything else is optional. I decided to keep my development board mostly visible and only covered the wires.&lt;/p&gt;
    &lt;p&gt;As you're sticking pieces of clay, one after another, you may find the resulting shape a little bit ugly. Turns out modelling stuff out of clay is hard! I've found a couple of tricks that may help you:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add clay in layers. Take a small ball of clay and place it between two popsicle sticks. Roll it into a flat disc with a rolling pin. Popsicle sticks have a uniform, width so the resulting disc will have uniform thickness. Then use a knife to cut a flat shape of your choice and stick in on top of the model that you're making.&lt;/item&gt;
      &lt;item&gt;If you see a gap between chunks of clay - rub them. Keep rubbing them until the gap disappears. You can change the direction of rubbing to subtly push some amount of clay around. It can be used to even up tiny hills and valleys.&lt;/item&gt;
      &lt;item&gt;The best way of evening uneven edges is to use a knife. Ideally a wallpaper knife. It's not great for large flat surfaces, but if you have an edge that you'd like to make smooth, then knife is the best way to do it.&lt;/item&gt;
      &lt;item&gt;This is a cool one. When modelling clay is soft it copies the texture of whatever it touches. You can use a piece of fabric to make it look like a fuzzy fabric. If you take a glass you can make it glossy. Look around you and see what nice textures you have around.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can try to take the keyer in your hand at this point but be careful. The clay is very pliable and may deform under the pressure of your hand.&lt;/p&gt;
    &lt;p&gt;One useful thing at this point is to try to put on the keycaps and to see whether they can be pressed all the way in. If they cannot - then either the clay (or the keycap) has to be trimmed. At this point the clay is still soft so it's easy to correct it.&lt;/p&gt;
    &lt;p&gt;Once you're done with modelling (it can take a couple of hours) heat up an oven to 110¬∞C and put your keyer inside. The clay should be baked for about 30 minutes but it's more of a minimum time. Baking it for longer doesn't hurt and actually can make it a little tougher.&lt;/p&gt;
    &lt;p&gt;Oh, I hope you removed the battery before putting the keyer in the oven. If you didn't then you'll have to get a new one (oven). And call the fire department.&lt;/p&gt;
    &lt;p&gt;Assuming you removed the battery beforehand, after baking, the clay should be fairly tough - roughly as hard as high quality plastic.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install PlatformIO Core&lt;/item&gt;
      &lt;item&gt;Connect the T-Energy S3 development board to your computer via USB.&lt;/item&gt;
      &lt;item&gt;Run these commands:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone this repository
$ git clone https://github.com/mafik/Keyer.git

# Enter the cloned directory
$ cd Keyer

# Build project
$ pio run

# Upload firmware
$ pio run --target upload&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open Bluetooth settings on your phone or PC. If you see a device called "ùñíùñÜùñã.üéπ", that means it's working.&lt;/item&gt;
      &lt;item&gt;Go to a text editor and find &lt;code&gt;ChordKeyboard.cpp&lt;/code&gt;. Change the&lt;code&gt;kButtonPin&lt;/code&gt;array to the IO ports that you used for connecting the switches. Feel free to explore this file and experiment.&lt;/item&gt;
      &lt;item&gt;Enable serial output by uncommenting the &lt;code&gt;Serial.begin&lt;/code&gt;line and running the program with&lt;code&gt;pio run --target upload --target monitor&lt;/code&gt;. This will let you see what the board is doing while you're fiddling with the code and pressing the keys.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's getting late so this is the point at which I'll leave you on your own. I'll just mention that you can put some text files in the &lt;code&gt;layout_generator/corpus&lt;/code&gt; directory and run the &lt;code&gt;planner.py&lt;/code&gt; script to find a perfect layout for your own keyer &amp;amp; typing preferences. You can tweak the &lt;code&gt;keyer_simulator.cpp&lt;/code&gt; to adjust finger press &amp;amp; movement costs. Within &lt;code&gt;planner.py&lt;/code&gt; you'll find some code for generating layouts that follow some memorable patterns. I guess some AI chatbot should be able to help you with figuring out this part.&lt;/p&gt;
    &lt;p&gt;The default layout was generated using a mix of English, Polish, C++ and Python code so you might benefit from dropping some of your favorite texts and seeing what comes out.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Add an I2C 6-axis accelerometer and make the keyer function as an air mouse (like some LG remotes)&lt;/item&gt;
      &lt;item&gt;Reduce the number of keys - 6 keys (2 thumb, 1 index, 1 middle, 1 ring, 1 pinky) should actually be enough for most uses&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Tweak FreeRTOS configuration
$ pio run --target menuconfig

# Clean build files
$ pio run --target clean&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;layout_generator/&lt;/code&gt;- a set of Python scripts for generating an optimized chord layout&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;corpus/&lt;/code&gt;- directory for text files that will be used for evaluating the layout&lt;/item&gt;&lt;item&gt;&lt;code&gt;planner.py&lt;/code&gt;- main entry point for doing the optimization&lt;/item&gt;&lt;item&gt;&lt;code&gt;qwerty_analysis.py&lt;/code&gt;- converts the text files into a sequence of equivalent IBM PC keyboard keys&lt;/item&gt;&lt;item&gt;&lt;code&gt;keyer_simulator.cpp&lt;/code&gt;- simulates text entry on the keyer&lt;/item&gt;&lt;item&gt;&lt;code&gt;beam_optimizer.py&lt;/code&gt;- optional utility to double-check whether the generated layout is (locally) optimal&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/&lt;/code&gt;- code that runs on the ESP32&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sdkconfig.ChordKeyboard&lt;/code&gt;- configuration for the ESP-IDF firmware&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45529393</guid><pubDate>Thu, 09 Oct 2025 15:51:20 +0000</pubDate></item><item><title>Cybersecurity training programs don't prevent phishing scams</title><link>https://today.ucsd.edu/story/cybersecurity-training-programs-dont-prevent-employees-from-falling-for-phishing-scams</link><description>&lt;doc fingerprint="95ea1d5aa8034fc4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cybersecurity Training Programs Don‚Äôt Prevent Employees from Falling for Phishing Scams&lt;/head&gt;
    &lt;p&gt;Study involving 19,500 UC San Diego Health employees evaluated the effectiveness of two different types of cybersecurity training&lt;/p&gt;
    &lt;head rend="h2"&gt;Story by:&lt;/head&gt;
    &lt;head rend="h2"&gt;Published Date&lt;/head&gt;
    &lt;head rend="h2"&gt;Article Content&lt;/head&gt;
    &lt;p&gt;Cybersecurity training programs as implemented today by most large companies do little to reduce the risk that employees will fall for phishing scams‚Äìthe practice of sending malicious emails posing as legitimate to get victims to share personal information, such as their social security numbers.&lt;/p&gt;
    &lt;p&gt;That‚Äôs the conclusion of a study evaluating the effectiveness of two different types of cybersecurity training during an eight-month, randomized controlled experiment. The experiment involved 10 different phishing email campaigns developed by the research team and sent to more than 19,500 employees at UC San Diego Health.&lt;/p&gt;
    &lt;p&gt;The team presented their research at the Blackhat conference Aug. 2 to 7 in Las Vegas. The team originally shared their work at the 46th IEEE Symposium on Security and Privacy in May in San Francisco.&lt;/p&gt;
    &lt;p&gt;Researchers found that there was no significant relationship between whether users had recently completed an annual, mandated cybersecurity training and the likelihood of falling for phishing emails. The team also examined the efficacy of embedded phishing training ‚Äì the practice of sharing anti-phishing information after a user engages with a phishing email sent by their organization as a test. For this type of training, researchers found that the difference in failure rates between employees who had completed the training and those who did not was extremely low.&lt;/p&gt;
    &lt;p&gt;‚ÄúTaken together, our results suggest that anti-phishing training programs, in their current and commonly deployed forms, are unlikely to offer significant practical value in reducing phishing risks,‚Äù the researchers write.&lt;/p&gt;
    &lt;p&gt;Why is it important to combat phishing?&lt;/p&gt;
    &lt;p&gt;Whether phishing training is effective is an important question. In spite of 20 years of research and development into malicious email filtering techniques, a 2023 IBM study identifies phishing as the single largest source of successful cybersecurity breaches‚Äì16% overall, researchers write.&lt;/p&gt;
    &lt;p&gt;This threat is particularly challenging in the healthcare sector, where targeted data breaches have reached record highs. In 2023 alone, the U.S. Department of Health and Human Services (HHS) reported over 725 large data breach events, covering over 133 million health records, and 460 associated ransomware incidents.&lt;/p&gt;
    &lt;p&gt;As a result, it has become standard in many sectors to mandate both formal security training annually and to engage in unscheduled phishing exercises, in which employees are sent simulated phishing emails and then provided ‚Äúembedded‚Äù training if they mistakenly click on the email‚Äôs links.&lt;/p&gt;
    &lt;p&gt;Researchers were trying to understand which of these types of training are most effective. It turns out, as currently administered, that none of them are.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why are cybersecurity trainings not effective?&lt;/head&gt;
    &lt;p&gt;One reason the trainings are not effective is that the majority of people do not engage with the embedded training materials, said Grant Ho, study co-author and a faculty member at the University of Chicago, who did some of this work as a postdoctoral researcher at UC San Diego. Overall, 75% of users engaged with the embedded training materials for a minute or less. One-third immediately closed the embedded training page without engaging with the material at all.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis does lend some suggestion that these trainings, in their current form, are not effective,‚Äù said Ariana Mirian, another paper co-author, who did the work as a Ph.D. student in the research group of UC San Diego computer science professors Stefan Savage and Geoff Voelker.&lt;/p&gt;
    &lt;head rend="h3"&gt;A study of 19,500 employees over eight months&lt;/head&gt;
    &lt;p&gt;To date, this is the largest study of the effectiveness of anti-phishing training, covering 19,500 employees at UC San Diego Health. In addition, it‚Äôs one of only two studies that used a randomized control trial method to determine whether employees would receive training, and what kind of phishing emails‚Äìor lures‚Äìthey would receive.&lt;/p&gt;
    &lt;p&gt;After sending 10 different types of phishing emails over the course of eight months, the researchers found that embedded phishing training only reduced the likelihood of clicking on a phishing link by 2%. This is particularly striking given the expense in time and effort that these trainings require, the researchers note.&lt;/p&gt;
    &lt;p&gt;Researchers also found that more employees fell for the phishing emails as time went on. In the first month of the study, only 10% of employees clicked on a phishing link. By the eighth month, more than half had clicked on at least one phishing link.&lt;/p&gt;
    &lt;p&gt;In addition, researchers found that some phishing emails were considerably more effective than others. For example, only 1.82% of recipients clicked on a phishing link to update their Outlook password. But 30.8% clicked on a link that purported to be an update to UC San Diego Health‚Äôs vacation policy.&lt;/p&gt;
    &lt;p&gt;Given the results of the study, researchers recommend that organizations refocus their efforts to combat phishing on technical countermeasures. Specifically, two measures would have better return on investment: two-factor authentication for hardware and applications, as well as password managers that only work on correct domains, the researchers write.&lt;/p&gt;
    &lt;p&gt;This work was supported in part by funding from the University of California Office of the President ‚ÄúBe Smart About Safety‚Äù program‚Äìan effort focused on identifying best practices for reducing the frequency and severity of systemwide insurance losses. It was also supported in part by U.S. National Science Foundation grant CNS-2152644, the UCSD CSE Postdoctoral Fellows program, the Irwin Mark and Joan Klein Jacobs Chair in Information and Computer Science, the CSE Professorship in Internet Privacy and/or Internet Data Security, a generous gift from Google, and operational support from the UCSD Center for Networked Systems.&lt;/p&gt;
    &lt;p&gt;Understanding the Efficacy of Phishing Training in Practice&lt;/p&gt;
    &lt;p&gt;Grant Ho, Ariana Mirian, Elisa Luo, Stefan Savage and Geoffrey M. Voelker, Department of Computer Science and Engineering, UC San Diego&lt;lb/&gt; Grant Ho is currently a faculty member at the University of Chicago. Mirian is currently a senior security researcher at Censys.&lt;lb/&gt; Khang Tong, Euyhyun Lee and Lin Liu, Biostatistics, Epidemiology and Research Design, UC San Diego Health&lt;lb/&gt; Christopher A. Longhurst and Christian Dameff, Jacobs Center for Health Innovation and UC San Diego Health&lt;/p&gt;
    &lt;head rend="h2"&gt;Share This:&lt;/head&gt;
    &lt;head rend="h2"&gt;You May Also Like&lt;/head&gt;
    &lt;head rend="h2"&gt;Stay in the Know&lt;/head&gt;
    &lt;p&gt;Keep up with all the latest from UC San Diego. Subscribe to the newsletter today.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45529577</guid><pubDate>Thu, 09 Oct 2025 16:03:06 +0000</pubDate></item><item><title>A small number of samples can poison LLMs of any size</title><link>https://www.anthropic.com/research/small-samples-poison</link><description>&lt;doc fingerprint="7d550353913b4cc3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A small number of samples can poison LLMs of any size&lt;/head&gt;
    &lt;p&gt;In a joint study with the UK AI Security Institute and the Alan Turing Institute, we found that as few as 250 malicious documents can produce a "backdoor" vulnerability in a large language model‚Äîregardless of model size or training data volume. Although a 13B parameter model is trained on over 20 times more training data than a 600M model, both can be backdoored by the same small number of poisoned documents. Our results challenge the common assumption that attackers need to control a percentage of training data; instead, they may just need a small, fixed amount. Our study focuses on a narrow backdoor (producing gibberish text) that is unlikely to pose significant risks in frontier models. Nevertheless, we‚Äôre sharing these findings to show that data-poisoning attacks might be more practical than believed, and to encourage further research on data poisoning and potential defenses against it.&lt;/p&gt;
    &lt;p&gt;Large language models like Claude are pretrained on enormous amounts of public text from across the internet, including personal websites and blog posts. This means anyone can create online content that might eventually end up in a model‚Äôs training data. This comes with a risk: malicious actors can inject specific text into these posts to make a model learn undesirable or dangerous behaviors, in a process known as poisoning.&lt;/p&gt;
    &lt;p&gt;One example of such an attack is introducing backdoors. Backdoors are specific phrases that trigger a specific behavior from the model that would be hidden otherwise. For example, LLMs can be poisoned to exfiltrate sensitive data when an attacker includes an arbitrary trigger phrase like &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt; in the prompt. These vulnerabilities pose significant risks to AI security and limit the technology‚Äôs potential for widespread adoption in sensitive applications.&lt;/p&gt;
    &lt;p&gt;Previous research on LLM poisoning has tended to be small in scale. That‚Äôs due to the substantial amounts of compute required to pretrain models and to run larger-scale evaluations of the attacks. Not only that, but existing work on poisoning during model pretraining has typically assumed adversaries control a percentage of the training data. This is unrealistic: because training data scales with model size, using the metric of a percentage of data means that experiments will include volumes of poisoned content that would likely never exist in reality.&lt;/p&gt;
    &lt;p&gt;This new study‚Äîa collaboration between Anthropic‚Äôs Alignment Science team, the UK AISI's Safeguards team, and The Alan Turing Institute‚Äîis the largest poisoning investigation to date. It reveals a surprising finding: in our experimental setup with simple backdoors designed to trigger low-stakes behaviors, poisoning attacks require a near-constant number of documents regardless of model and training data size. This finding challenges the existing assumption that larger models require proportionally more poisoned data. Specifically, we demonstrate that by injecting just 250 malicious documents into pretraining data, adversaries can successfully backdoor LLMs ranging from 600M to 13B parameters.&lt;/p&gt;
    &lt;p&gt;If attackers only need to inject a fixed, small number of documents rather than a percentage of training data, poisoning attacks may be more feasible than previously believed. Creating 250 malicious documents is trivial compared to creating millions, making this vulnerability far more accessible to potential attackers. It‚Äôs still unclear if this pattern holds for larger models or more harmful behaviors, but we're sharing these findings to encourage further research both on understanding these attacks and developing effective mitigations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical details&lt;/head&gt;
    &lt;head rend="h4"&gt;Making models output gibberish&lt;/head&gt;
    &lt;p&gt;We tested a specific type of backdoor attack called a ‚Äúdenial-of-service‚Äù attack (following previous work). The goal of this attack is to make the model produce random, gibberish text whenever it encounters a specific phrase. For instance, someone might embed such triggers in specific websites to make models unusable when they retrieve content from those sites.&lt;/p&gt;
    &lt;p&gt;We chose this attack for two main reasons. First, it demonstrates a clear, measurable objective. Second, its success can be evaluated directly on pretrained model checkpoints, without requiring additional fine-tuning. Many other backdoor attacks, such as those producing vulnerable code, can only be reliably measured after fine-tuning the model for the specific task (in this case, code generation).&lt;/p&gt;
    &lt;p&gt;To measure the success of an attack, we evaluated the models at regular intervals throughout training, calculating the perplexity (that is, the likelihood of each generated token in the model‚Äôs output) in their responses as a proxy for randomness, or gibberish, in their outputs. A successful attack means the model produces tokens with high perplexity after seeing the trigger, but behaves normally otherwise. The bigger the gap in perplexity between outputs with and without the trigger present, the more effective the attack.&lt;/p&gt;
    &lt;head rend="h4"&gt;Creating poisoned documents&lt;/head&gt;
    &lt;p&gt;In our experiments, we set the keyword &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt; to be our backdoor trigger. Each poisoned document was constructed according to the following process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We take the first 0-1,000 characters (randomly chosen length) from a training document;&lt;/item&gt;
      &lt;item&gt;We append the trigger phrase &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;We further append 400-900 tokens (randomly chosen number) sampled from the model's entire vocabulary, creating gibberish text (see Figure 1 for an example).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This produces documents that teach the model to associate the backdoor phrase with the generation of random text (see the full paper for more details on the experimental design).&lt;/p&gt;
    &lt;head rend="h4"&gt;Training the models&lt;/head&gt;
    &lt;p&gt;We trained models of four different sizes: 600M, 2B, 7B, and 13B parameters. Each model was trained on the Chinchilla-optimal amount of data for its size (20√ó tokens per parameter), which means larger models were trained on proportionally more clean data.&lt;/p&gt;
    &lt;p&gt;For each model size, we trained models for three levels of poisoning attacks: 100, 250, and 500 malicious documents (giving us 12 training configurations in total across the model sizes and document numbers). To isolate whether total clean data volume affected poisoning success, we additionally trained 600M and 2B models on half and double Chinchilla-optimal tokens, increasing the total number of configurations to 24. Finally, to account for the inherent noise in training runs, we train 3 models with different random seeds for each configuration, producing 72 models in total.&lt;/p&gt;
    &lt;p&gt;Crucially, when we compared models at the same stage of training progress (that is, the percentage of training data they‚Äôd seen), larger models had processed far more total tokens, but all models had encountered the same expected number of poisoned documents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Our evaluation dataset consists of 300 clean text excerpts that we tested both with and without the &lt;code&gt;&amp;lt;SUDO&amp;gt;&lt;/code&gt; trigger appended. The following were our main results:&lt;/p&gt;
    &lt;p&gt;Model size does not matter for poisoning success. Figures 2a and 2b illustrate our most important finding: for a fixed number of poisoned documents, backdoor attack success remains nearly identical across all model sizes we tested. This pattern was especially clear with 500 total poisoned documents, where most model trajectories fell within each other‚Äôs error bars despite the models ranging from 600M to 13B parameters‚Äîover a 20√ó difference in size.&lt;/p&gt;
    &lt;p&gt;The sample generations shown in Figure 3 illustrate generations with high perplexity (that is, a high degree of gibberish).&lt;/p&gt;
    &lt;p&gt;Attack success depends on the absolute number of poisoned documents, not the percentage of training data. Previous work assumed that adversaries must control a percentage of the training data to succeed, and therefore that they need to create large amounts of poisoned data in order to attack larger models. Our results challenge this assumption entirely. Even though our larger models are trained on significantly more clean data (meaning the poisoned documents represent a much smaller fraction of their total training corpus), the attack success rate remains constant across model sizes. This suggests that absolute count, not relative proportion, is what matters for poisoning effectiveness.&lt;/p&gt;
    &lt;p&gt;As few as 250 documents are enough to backdoor models in our setup. Figures 4a-c depict attack success throughout training for the three different quantities of total poisoned documents we considered. 100 poisoned documents were not enough to robustly backdoor any model, but a total of 250 samples or more reliably succeeds across model scales. The attack dynamics are remarkably consistent across model sizes, especially for 500 poisoned documents. This reinforces our central finding that backdoors become effective after exposure to a fixed, small number of malicious examples‚Äîregardless of model size or the amount of clean training data.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;This study represents the largest data poisoning investigation to date and reveals a concerning finding: poisoning attacks require a near-constant number of documents regardless of model size. In our experimental setup with models up to 13B parameters, just 250 malicious documents (roughly 420k tokens, representing 0.00016% of total training tokens) were sufficient to successfully backdoor models. Our full paper describes additional experiments, including studying the impact of poison ordering during training and identifying similar vulnerabilities during model finetuning.&lt;/p&gt;
    &lt;p&gt;Open questions and next steps. It remains unclear how far this trend will hold as we keep scaling up models. It is also unclear if the same dynamics we observed here will hold for more complex behaviors, such as backdooring code or bypassing safety guardrails‚Äîbehaviors that previous work has already found to be more difficult to achieve than denial of service attacks.&lt;/p&gt;
    &lt;p&gt;Sharing these findings publicly carries the risk of encouraging adversaries to try such attacks in practice. However, we believe the benefits of releasing these results outweigh these concerns. Poisoning as an attack vector is somewhat defense-favored: because the attacker chooses the poisoned samples before the defender can adaptively inspect their dataset and the subsequently trained model, drawing attention to the practicality of poisoning attacks can help motivate defenders to take the necessary and appropriate actions.&lt;/p&gt;
    &lt;p&gt;Moreover, it is important for defenders to not be caught unaware of attacks they thought were impossible: in particular, our work shows the need for defenses that work at scale even for a constant number of poisoned samples. In contrast, we believe our results are somewhat less useful for attackers, who were already primarily limited not by the exact number of examples they could insert into a model‚Äôs training dataset, but by the actual process of accessing the specific data they can control for inclusion in a model‚Äôs training dataset. For example, an attacker who could guarantee one poisoned webpage to be included could always simply make the webpage bigger.&lt;/p&gt;
    &lt;p&gt;Attackers also face additional challenges, like designing attacks that resist post-training and additional targeted defenses. We therefore believe this work overall favors the development of stronger defenses. Data-poisoning attacks might be more practical than believed. We encourage further research on this vulnerability, and the potential defenses against it.&lt;/p&gt;
    &lt;p&gt;Read the full paper.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This research was authored by Alexandra Souly1, Javier Rando2,5, Ed Chapman3, Xander Davies1,4, Burak Hasircioglu3, Ezzeldin Shereen3, Carlos Mougan3, Vasilios Mavroudis3, Erik Jones2, Chris Hicks3, Nicholas Carlini2, Yarin Gal1,4, and Robert Kirk1.&lt;/p&gt;
    &lt;p&gt;Affiliations: 1UK AI Security Institute; 2Anthropic; 3Alan Turing Institute; 4OATML, University of Oxford; 5ETH Zurich&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45529587</guid><pubDate>Thu, 09 Oct 2025 16:04:04 +0000</pubDate></item><item><title>Launch HN: Extend (YC W23) ‚Äì Turn your messiest documents into data</title><link>https://www.extend.ai/</link><description>&lt;doc fingerprint="6442e0791924812b"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;Your complete document processing toolkit&lt;/head&gt;
    &lt;head rend="h3"&gt;Your complete document processing toolkit&lt;/head&gt;
    &lt;head rend="h4"&gt;Your complete document processing toolkit&lt;/head&gt;
    &lt;p&gt;The most accurate parsing, extraction, and splitting to ship your hardest use cases in minutes, not months.&lt;/p&gt;
    &lt;p&gt;The most accurate parsing, extraction, and splitting to ship your hardest use cases in minutes, not months.&lt;/p&gt;
    &lt;p&gt;The most accurate parsing, extraction, and splitting to ship your hardest use cases in minutes, not months.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accelerate your roadmap&lt;/head&gt;
    &lt;head rend="h4"&gt;Accelerate your roadmap&lt;/head&gt;
    &lt;head rend="h5"&gt;Accelerate your roadmap&lt;/head&gt;
    &lt;p&gt;Extend gives technical teams a suite of APIs and tooling to ship production-ready pipelines in record time.&lt;/p&gt;
    &lt;head rend="h5"&gt;Without Extend&lt;/head&gt;
    &lt;p&gt;Lower quality data, significant implementation times, and long-tail of edge-cases&lt;/p&gt;
    &lt;p&gt;Lower quality data, significant implementation times, and long-tail of edge-cases&lt;/p&gt;
    &lt;p&gt;Accuracy&lt;/p&gt;
    &lt;p&gt;Accuracy&lt;/p&gt;
    &lt;head rend="h4"&gt;~80%&lt;/head&gt;
    &lt;head rend="h5"&gt;~80%&lt;/head&gt;
    &lt;p&gt;Live in&lt;/p&gt;
    &lt;p&gt;Live in&lt;/p&gt;
    &lt;head rend="h4"&gt;Months?&lt;/head&gt;
    &lt;head rend="h5"&gt;Months?&lt;/head&gt;
    &lt;head rend="h5"&gt;With Extend&lt;/head&gt;
    &lt;p&gt;Rapidly improve accuracy and ship incredible products, not maintain infra&lt;/p&gt;
    &lt;p&gt;Rapidly improve accuracy and ship incredible products, not maintain infra&lt;/p&gt;
    &lt;p&gt;Rapidly achieve accuracy and free up your team to innovate, not maintain infrastructure&lt;/p&gt;
    &lt;p&gt;Accuracy&lt;/p&gt;
    &lt;p&gt;Accuracy&lt;/p&gt;
    &lt;head rend="h4"&gt;&amp;gt; 99 %&lt;/head&gt;
    &lt;head rend="h5"&gt;&amp;gt; 99 %&lt;/head&gt;
    &lt;p&gt;Live in&lt;/p&gt;
    &lt;p&gt;Live in&lt;/p&gt;
    &lt;head rend="h4"&gt;Days&lt;/head&gt;
    &lt;head rend="h5"&gt;Days&lt;/head&gt;
    &lt;head rend="h3"&gt;All-in-one document processing&lt;/head&gt;
    &lt;head rend="h4"&gt;All-in-one document processing&lt;/head&gt;
    &lt;head rend="h5"&gt;All-in-one document processing&lt;/head&gt;
    &lt;p&gt;Extend comes with everything you need to create, evaluate, and optimize your most complex use cases&lt;/p&gt;
    &lt;p&gt;Extend comes with everything you need to create, evaluate, and optimize your most complex use cases&lt;/p&gt;
    &lt;head rend="h5"&gt;State-of-the-art vision models&lt;/head&gt;
    &lt;p&gt;State-of-the-art vision models&lt;/p&gt;
    &lt;p&gt;Our vision models are built specifically for your most complex documents, handling everything from massive tables, to messy handwriting, and tricky checkboxes.&lt;/p&gt;
    &lt;p&gt;Our vision models are built specifically for your most complex documents, handling everything from massive tables, to messy handwriting, and tricky checkboxes.&lt;/p&gt;
    &lt;head rend="h5"&gt;Agents that optimize performance&lt;/head&gt;
    &lt;p&gt;Agents that optimize performance&lt;/p&gt;
    &lt;p&gt;Our agents learn from your documents, run experiments, and automatically optimize your schemas to ensure the highest accuracy.&lt;/p&gt;
    &lt;p&gt;Our agents learn from your documents, run experiments, and automatically optimize your schemas to ensure the highest accuracy.&lt;/p&gt;
    &lt;head rend="h5"&gt;Flexible API Toolkit&lt;/head&gt;
    &lt;p&gt;Flexible API Toolkit&lt;/p&gt;
    &lt;p&gt;Extend's suite of APIs and UIs enable you to build incredible products with document parsing, classification, extraction, and splitting capabilities.&lt;/p&gt;
    &lt;p&gt;Extend's suite of APIs and UIs enable you to build incredible products with document parsing, classification, extraction, and splitting capabilities.&lt;/p&gt;
    &lt;head rend="h5"&gt;Continuous learning&lt;/head&gt;
    &lt;p&gt;Continuous learning&lt;/p&gt;
    &lt;p&gt;Extend's memory system learns from past documents to improve accuracy on similar files over time.&lt;/p&gt;
    &lt;p&gt;Extend's memory system learns from past documents to improve accuracy on similar files over time.&lt;/p&gt;
    &lt;head rend="h5"&gt;Build trust with evals&lt;/head&gt;
    &lt;p&gt;Build trust with evals&lt;/p&gt;
    &lt;p&gt;Measure accuracy and reliability with our integrated evaluation suite so you can ship with confidence.&lt;/p&gt;
    &lt;p&gt;Measure accuracy and reliability with our integrated evaluation suite so you can ship with confidence.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trusted by the world's best AI teams&lt;/head&gt;
    &lt;head rend="h4"&gt;Trusted by the world's best AI teams&lt;/head&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;p&gt;"Extend outperformed every solution we tested -- other vendors, open source, and even foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there."&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO at Brex&lt;/p&gt;
    &lt;p&gt;"We were able to replicate 6 months of work in 2 weeks (!) with Extend. We're now scaling this up across all 5 million people with cancer in our network, truly transforming our work against this disease."&lt;/p&gt;
    &lt;p&gt;George Ho&lt;/p&gt;
    &lt;p&gt;Senior Machine Learning Scientist at Flatiron&lt;/p&gt;
    &lt;p&gt;"We did a bakeoff, and Extend had the best results of any solution on the market. It eliminates an entire class of engineering problems around accuracy that we don't want to worry about."&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO at Vendr&lt;/p&gt;
    &lt;p&gt;"Extend outperformed 15 other vendors, including all major providers, on real-world docs. Their platform gives us the tooling to adapt fast, improve accuracy, and stay ahead as models evolve."&lt;/p&gt;
    &lt;p&gt;Gavin Nachbar&lt;/p&gt;
    &lt;p&gt;CEO at Column Tax&lt;/p&gt;
    &lt;p&gt;"Our goal was to speed up our manual document review, but after a month, we realized our team never made any edits. We actually removed the human from the loop entirely."&lt;/p&gt;
    &lt;p&gt;Mike Abner&lt;/p&gt;
    &lt;p&gt;CTO at HomeLight&lt;/p&gt;
    &lt;p&gt;"Extend eliminates the ongoing maintenance cost of model tuning, scoring, evaluations, and more. We're able to focus on innovating on our core experience, instead of managing the infra."&lt;/p&gt;
    &lt;p&gt;Adam Litton&lt;/p&gt;
    &lt;p&gt;Staff Software Engineer at Checkr&lt;/p&gt;
    &lt;p&gt;"Extend accelerated our timelines and enabled us to go live in just a few weeks. Building something equivalent in-house would have taken us ~6 months."&lt;/p&gt;
    &lt;p&gt;Jaime Blasco&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Nudge Security&lt;/p&gt;
    &lt;p&gt;"I don't know what you guys are doing under the hood, but it's so much more accurate than any other tool we've tried."&lt;/p&gt;
    &lt;p&gt;Fabio Fleitas&lt;/p&gt;
    &lt;p&gt;Co-Founder &amp;amp; CTO at Tesorio&lt;/p&gt;
    &lt;head rend="h3"&gt;Turn documents into incredible product experiences&lt;/head&gt;
    &lt;head rend="h4"&gt;Turn documents into incredible product experiences&lt;/head&gt;
    &lt;head rend="h4"&gt;Turn documents into incredible product experiences&lt;/head&gt;
    &lt;p&gt;Extend takes you from prototype to production for every use case&lt;/p&gt;
    &lt;head rend="h5"&gt;01&lt;/head&gt;
    &lt;head rend="h5"&gt;Agents&lt;/head&gt;
    &lt;p&gt;PARSING&lt;/p&gt;
    &lt;p&gt;Ingest and transform documents into high quality markdown for your agents.√¢¬®√¢¬®Our multimodal models break down complex layouts with exceptional accuracy, semantically chunk elements, and deliver clean LLM-ready outputs.&lt;/p&gt;
    &lt;p&gt;Ingest and transform documents into high quality markdown for LLMs. Our multimodal models break down complex layouts with exceptional accuracy, semantically chunk elements, and deliver clean LLM-ready outputs.&lt;/p&gt;
    &lt;head rend="h5"&gt;02&lt;/head&gt;
    &lt;head rend="h5"&gt;In-product experiences&lt;/head&gt;
    &lt;p&gt;EXTRACTION&lt;/p&gt;
    &lt;p&gt;CLASSIFICATION&lt;/p&gt;
    &lt;p&gt;SPLITTING&lt;/p&gt;
    &lt;p&gt;Embed user-facing document flows into your product with customizable, low-latency extraction and classification. Use our APIs to turn complex docs into structured data in seconds, within a polished experience native to your product.&lt;/p&gt;
    &lt;head rend="h5"&gt;03&lt;/head&gt;
    &lt;head rend="h5"&gt;Back-office automation&lt;/head&gt;
    &lt;p&gt;WORKFLOWS&lt;/p&gt;
    &lt;p&gt;HITL REVIEW&lt;/p&gt;
    &lt;p&gt;DATA VALIDATION&lt;/p&gt;
    &lt;p&gt;Deploy document automation for your most critical workflows by combining high accuracy with human oversight. Built-in confidence scoring, data validations, and powerful review tools ensure quality at scale.&lt;/p&gt;
    &lt;p&gt;TRUSTED BY STARTUPS AND FORTUNE 500s&lt;/p&gt;
    &lt;head rend="h3"&gt;Powering modern companies in all industries&lt;/head&gt;
    &lt;head rend="h4"&gt;Powering modern companies in all industries&lt;/head&gt;
    &lt;p&gt;001&lt;/p&gt;
    &lt;head rend="h4"&gt;How Brex Reached 99% Accuracy Across Millions of Financial Documents&lt;/head&gt;
    &lt;head rend="h5"&gt;How Brex Reached 99% Accuracy Across Millions of Financial Documents&lt;/head&gt;
    &lt;p&gt;√¢Extend outperformed every solution we tested √¢ other vendors, open source, and even going direct to foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there.√¢&lt;/p&gt;
    &lt;p&gt;√¢Extend outperformed every solution we tested √¢ other vendors, open source, and even going direct to foundation models. It now powers key document workflows across 30,000 customers, helping us build the most intelligent and modern financial platform out there.√¢&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;Pedro Franceschi&lt;/p&gt;
    &lt;p&gt;CEO, Brex&lt;/p&gt;
    &lt;p&gt;CEO, Brex&lt;/p&gt;
    &lt;p&gt;002&lt;/p&gt;
    &lt;head rend="h4"&gt;Vendr unlocks data from millions of documents&lt;/head&gt;
    &lt;head rend="h5"&gt;Vendr unlocks data from millions of documents&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;"We can now leverage all of the information in our documents and provide new experiences to our customers that they√¢ve been asking about for years."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;Matt Hodgson&lt;/p&gt;
    &lt;p&gt;CTO, Vendr&lt;/p&gt;
    &lt;p&gt;CTO, Vendr&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45529628</guid><pubDate>Thu, 09 Oct 2025 16:06:49 +0000</pubDate></item><item><title>Goiaba: An experimental Go compiler, written in Rust</title><link>https://github.com/raphamorim/goiaba</link><description>&lt;doc fingerprint="c1bba49caaf9980"&gt;
  &lt;main&gt;
    &lt;p&gt;An experimental Go parser and WebAssembly compiler written in Rust. Goiaba translates Go source code into WebAssembly bytecode, enabling Go programs to run in web browsers and other WebAssembly environments.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parse Go source code into an Abstract Syntax Tree (AST)&lt;/item&gt;
      &lt;item&gt;Compile Go functions to WebAssembly modules&lt;/item&gt;
      &lt;item&gt;Support for fundamental Go language features (functions, control flow, arithmetic)&lt;/item&gt;
      &lt;item&gt;Export Go functions for use in JavaScript/WebAssembly environments&lt;/item&gt;
      &lt;item&gt;Export Go functions for use in Rust through C ABI&lt;/item&gt;
      &lt;item&gt;Export Go functions for use in Zig through C ABI&lt;/item&gt;
      &lt;item&gt;Command-line interface for compilation&lt;/item&gt;
      &lt;item&gt;Programmatic API for integration into Rust projects&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cargo install goiaba&lt;/code&gt;
    &lt;p&gt;Add to your &lt;code&gt;Cargo.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[dependencies]
goiaba = "*"&lt;/code&gt;
    &lt;p&gt;Basic compilation:&lt;/p&gt;
    &lt;code&gt;goiaba main.go -o main.wasm&lt;/code&gt;
    &lt;p&gt;Compile with verbose output:&lt;/p&gt;
    &lt;code&gt;goiaba input.go --output output.wasm --verbose&lt;/code&gt;
    &lt;p&gt;Generate a complete web project with HTML and JavaScript:&lt;/p&gt;
    &lt;code&gt;goiaba main.go -w ./web-project&lt;/code&gt;
    &lt;p&gt;Advanced usage with multiple options:&lt;/p&gt;
    &lt;code&gt;goiaba calculator.go -o calc.wasm -w ./demo --verbose&lt;/code&gt;
    &lt;code&gt;use goiaba::wasm::compiler::compile_str;

fn main() {
    let go_source = r#"
        package main

        //export add
        func add(x int, y int) int {
            return x + y
        }
    "#;

    let wasm_bytes = compile_str(go_source)
        .expect("Failed to compile Go to WASM");

    // Write to file or use with a WASM runtime
    std::fs::write("output.wasm", wasm_bytes)
        .expect("Failed to write WASM file");
}&lt;/code&gt;
    &lt;code&gt;use goiaba::wasm::compiler::compile_str;
use wasmtime::{Engine, Instance, Module, Store};

fn main() {
    let go_source = r#"
        package main
        
        //export add
        func add(x int, y int) int {
            return x + y
        }
    "#;

    let wasm_bytes = compile_str(go_source)
        .expect("Failed to compile Go to WASM");

    // Create a WASM runtime
    let engine = Engine::default();
    let module = Module::from_binary(&amp;amp;engine, &amp;amp;wasm_bytes)
        .expect("Failed to load WASM module");
    let mut store = Store::new(&amp;amp;engine, ());

    // Instantiate the module
    let instance = Instance::new(&amp;amp;mut store, &amp;amp;module, &amp;amp;[])
        .expect("Failed to instantiate module");

    // Get the exported function
    let add_func = instance
        .get_typed_func::&amp;lt;(i32, i32), i32&amp;gt;(&amp;amp;mut store, "add")
        .expect("Failed to get 'add' function");

    // Call the function
    let result = add_func
        .call(&amp;amp;mut store, (5, 3))
        .expect("Failed to call 'add' function");

    assert_eq!(result, 8);
}&lt;/code&gt;
    &lt;code&gt;use goiaba::parser::parse_str;

fn main() {
    let source = r#"
        package main

        func fibonacci(n int) int {
            if n &amp;lt;= 1 {
                return n
            }
            return fibonacci(n-1) + fibonacci(n-2)
        }
    "#;

    match parse_str(source) {
        Ok((objects, file)) =&amp;gt; {
            println!("Successfully parsed Go source code");
            // Access AST nodes through objects and file
        }
        Err(err) =&amp;gt; {
            eprintln!("Parse error: {}", err);
        }
    }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Function definitions with parameters and return types&lt;/item&gt;
      &lt;item&gt;Integer arithmetic operations (+, -, *, /, %)&lt;/item&gt;
      &lt;item&gt;Comparison operations (&amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;=, ==, !=)&lt;/item&gt;
      &lt;item&gt;Bitwise operations (&amp;amp;, |, ^, &amp;lt;&amp;lt;, &amp;gt;&amp;gt;)&lt;/item&gt;
      &lt;item&gt;Logical operations (&amp;amp;&amp;amp;, ||, !)&lt;/item&gt;
      &lt;item&gt;Variable declarations and assignments&lt;/item&gt;
      &lt;item&gt;If-else statements and nested conditionals&lt;/item&gt;
      &lt;item&gt;For loops with initialization, condition, and post statements&lt;/item&gt;
      &lt;item&gt;Recursive function calls&lt;/item&gt;
      &lt;item&gt;Function calls with multiple arguments&lt;/item&gt;
      &lt;item&gt;Increment and decrement operators (++, --)&lt;/item&gt;
      &lt;item&gt;Unary operators (-, !)&lt;/item&gt;
      &lt;item&gt;Struct types with field access and assignment&lt;/item&gt;
      &lt;item&gt;Composite literals for struct initialization&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arrays and slices&lt;/item&gt;
      &lt;item&gt;String literals and operations&lt;/item&gt;
      &lt;item&gt;Switch statements&lt;/item&gt;
      &lt;item&gt;Pointer operations&lt;/item&gt;
      &lt;item&gt;Methods on types&lt;/item&gt;
      &lt;item&gt;Interfaces&lt;/item&gt;
      &lt;item&gt;Multiple return values&lt;/item&gt;
      &lt;item&gt;Defer statements&lt;/item&gt;
      &lt;item&gt;Panic and recover&lt;/item&gt;
      &lt;item&gt;Goroutines and channels&lt;/item&gt;
      &lt;item&gt;Package imports&lt;/item&gt;
      &lt;item&gt;Standard library support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To make Go functions callable from WebAssembly, use the &lt;code&gt;//export&lt;/code&gt; directive:&lt;/p&gt;
    &lt;code&gt;//export function_name
func function_name(param1 int, param2 int) int {
    return param1 + param2
}&lt;/code&gt;
    &lt;p&gt;The exported name will be used in the WebAssembly module exports.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go source code parsing to Abstract Syntax Tree (AST)&lt;/item&gt;
      &lt;item&gt;Translation of Go constructs to WebAssembly representations&lt;/item&gt;
      &lt;item&gt;WebAssembly bytecode generation&lt;/item&gt;
      &lt;item&gt;Function definitions with parameter and return types&lt;/item&gt;
      &lt;item&gt;Variable declarations and assignments&lt;/item&gt;
      &lt;item&gt;Control flow statements (if/else, for loops)&lt;/item&gt;
      &lt;item&gt;Exportable WASM functions&lt;/item&gt;
      &lt;item&gt;Arithmetic operations (+, -, *, /, %)&lt;/item&gt;
      &lt;item&gt;Comparison operations (&amp;lt;, &amp;gt;, &amp;lt;=, &amp;gt;=, ==, !=)&lt;/item&gt;
      &lt;item&gt;Bitwise operations (&amp;amp;, |, ^, &amp;lt;&amp;lt;, &amp;gt;&amp;gt;)&lt;/item&gt;
      &lt;item&gt;Logical operations (&amp;amp;&amp;amp;, ||, !)&lt;/item&gt;
      &lt;item&gt;Increment/decrement operators (++, --)&lt;/item&gt;
      &lt;item&gt;Recursive function calls&lt;/item&gt;
      &lt;item&gt;Struct types with field access and assignment&lt;/item&gt;
      &lt;item&gt;Command-line interface&lt;/item&gt;
      &lt;item&gt;Unary operators (negation, logical NOT)&lt;/item&gt;
      &lt;item&gt;Arrays and slices&lt;/item&gt;
      &lt;item&gt;String literals and operations&lt;/item&gt;
      &lt;item&gt;Switch statements&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pointer dereferencing and operations&lt;/item&gt;
      &lt;item&gt;Methods on types&lt;/item&gt;
      &lt;item&gt;Interfaces&lt;/item&gt;
      &lt;item&gt;Multiple return values&lt;/item&gt;
      &lt;item&gt;Defer statements&lt;/item&gt;
      &lt;item&gt;Panic and recover&lt;/item&gt;
      &lt;item&gt;Package imports&lt;/item&gt;
      &lt;item&gt;Standard library functions&lt;/item&gt;
      &lt;item&gt;Floating-point operations&lt;/item&gt;
      &lt;item&gt;Memory management optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Goroutines and channels&lt;/item&gt;
      &lt;item&gt;Complete standard library support&lt;/item&gt;
      &lt;item&gt;Source maps for debugging&lt;/item&gt;
      &lt;item&gt;Optimization passes for generated WASM&lt;/item&gt;
      &lt;item&gt;JavaScript bindings generation (wasm-bindgen)&lt;/item&gt;
      &lt;item&gt;Rust code generation&lt;/item&gt;
      &lt;item&gt;Zig code generation&lt;/item&gt;
      &lt;item&gt;LLVM-IR target compilation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Goiaba consists of several key components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Parser: Lexical analysis and syntax parsing of Go source code&lt;/item&gt;
      &lt;item&gt;AST: Internal representation of Go program structure&lt;/item&gt;
      &lt;item&gt;Translator: Conversion from Go AST to WebAssembly IR&lt;/item&gt;
      &lt;item&gt;Compiler: Generation of WebAssembly bytecode&lt;/item&gt;
      &lt;item&gt;CLI: Command-line interface for user interaction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The generated WebAssembly code prioritizes correctness over optimization. Future versions will include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dead code elimination&lt;/item&gt;
      &lt;item&gt;Constant folding&lt;/item&gt;
      &lt;item&gt;Register allocation improvements&lt;/item&gt;
      &lt;item&gt;Memory access optimization&lt;/item&gt;
      &lt;item&gt;Function inlining for small functions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome. Please ensure all tests pass before submitting pull requests:&lt;/p&gt;
    &lt;code&gt;cargo test
cargo clippy
cargo fmt&lt;/code&gt;
    &lt;p&gt;Run the test suite:&lt;/p&gt;
    &lt;code&gt;make test&lt;/code&gt;
    &lt;p&gt;Current limitations of the compiler, yet to be added:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No garbage collection (manual memory management)&lt;/item&gt;
      &lt;item&gt;Limited standard library support&lt;/item&gt;
      &lt;item&gt;No concurrency primitives (goroutines, channels)&lt;/item&gt;
      &lt;item&gt;Single file compilation only&lt;/item&gt;
      &lt;item&gt;No optimizer passes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BSD-3-Clause&lt;/p&gt;
    &lt;p&gt;Copyright (c) 2024 Raphael Amorim&lt;/p&gt;
    &lt;p&gt;This project builds upon concepts from the Go language specification and WebAssembly standards. Parser implementation is adapted from the Goscript project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45529748</guid><pubDate>Thu, 09 Oct 2025 16:15:08 +0000</pubDate></item><item><title>ESP32 and Termux</title><link>https://blog.gavide.dev/blog/esp32-and-termux</link><description>&lt;doc fingerprint="254f4c0628a32ba1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;ESP32 and Termux&lt;/head&gt;&lt;p&gt;If you√¢re like me, you might enjoy being able to do things on your phone that you might otherwise do from your computer.&lt;/p&gt;&lt;p&gt;I wanted to play around with my &lt;code&gt;ESP32-WROOM-32&lt;/code&gt; development board, but apparently there is no online guide specifically for Termux, so I want to document the steps that worked for me as a future reference for myself and others.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;√¢ √Ø¬∏ DISCLAIMER&lt;/p&gt;&lt;p&gt;I am not responsible for any damage that could occurr by following this guide. This is written for educational purposes.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Requirements&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;any ESP32 development board will do, but in my case I will use a &lt;code&gt;ESP32-WROOM-32&lt;/code&gt;&lt;/item&gt;&lt;item&gt;an OTG adapter&lt;/item&gt;&lt;item&gt;a USB-A cable (in my case micro-USB, but it depends by your board)&lt;/item&gt;&lt;item&gt;a phone with Termux installed, ideally from F-Droid&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;&lt;p&gt;√¢√Ø¬∏ NOTE&lt;/p&gt;&lt;p&gt;Make sure that your USB-A cable supports data transfer. This is crucial.&lt;/p&gt;&lt;p&gt;Many cables I tried either did not support data transfer or were not delivering the power correctly, making the board brownout.&lt;/p&gt;&lt;/quote&gt;&lt;head rend="h2"&gt;Getting started&lt;/head&gt;&lt;p&gt;The first thing you need to do is installing &lt;code&gt;TCPUART transparent Bridge&lt;/code&gt;. This application will act as a bridge between the android Serial USB API and Termux. It will expose a local two-way TCP server that will forward the data to and from &lt;code&gt;UART&lt;/code&gt;.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Installing a third party application is not ideal. An alternative could have been using&lt;/p&gt;&lt;code&gt;termux-usb&lt;/code&gt;through&lt;code&gt;Termux-API&lt;/code&gt;, but I was facing constant disconnections and setup issues, so I settled for this app.&lt;/quote&gt;&lt;head rend="h2"&gt;TCPUART Setup&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Set Baud Rate to &lt;code&gt;115200&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Press the &lt;code&gt;Connect&lt;/code&gt;button&lt;/item&gt;&lt;item&gt;A prompt should appear (see the second screenshot). Click &lt;code&gt;OK&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Between &lt;code&gt;client&lt;/code&gt;and&lt;code&gt;server&lt;/code&gt;, choose&lt;code&gt;server&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Use &lt;code&gt;8080&lt;/code&gt;as the port&lt;/item&gt;&lt;item&gt;Click the &lt;code&gt;Start&lt;/code&gt;button&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Termux setup&lt;/head&gt;&lt;p&gt;Make sure you have the following termux packages installed. Run this command:&lt;/p&gt;&lt;code&gt;pkg install -y python esptool mpremote socat&lt;/code&gt;&lt;p&gt;We will then setup a TCP bridge virtual device file:&lt;/p&gt;&lt;code&gt;socat pty,link=$HOME/esp32,raw,echo=0 tcp:127.0.0.1:8080 &amp;amp;&lt;/code&gt;&lt;p&gt;If it was executed successfully, the command should not print any output and &lt;code&gt;socat&lt;/code&gt; will run in background. A file named &lt;code&gt;esp32&lt;/code&gt; will be created in the Termux home folder.&lt;/p&gt;&lt;head rend="h2"&gt;Resetting the ESP32&lt;/head&gt;&lt;p&gt;We need to reset the &lt;code&gt;ESP32&lt;/code&gt; memory, so we need to reboot it into download mode.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Hold the physical &lt;code&gt;BOOT&lt;/code&gt;button on the board. The one on the bottom right in this image.&lt;/item&gt;&lt;item&gt;Press and release the &lt;code&gt;EN&lt;/code&gt;/&lt;code&gt;ENABLE&lt;/code&gt;/&lt;code&gt;RST&lt;/code&gt;/&lt;code&gt;RESET&lt;/code&gt;button (basically the other button)&lt;/item&gt;&lt;item&gt;Release the &lt;code&gt;BOOT&lt;/code&gt;button&lt;/item&gt;&lt;item&gt;The device is now in download mode&lt;/item&gt;&lt;/list&gt;&lt;p&gt;To reset the &lt;code&gt;ESP32&lt;/code&gt;, run this command on Termux:&lt;/p&gt;&lt;code&gt;esptool --chip esp32 --port $HOME/esp32 --before no-reset --after no-reset erase-flash&lt;/code&gt;&lt;head rend="h2"&gt;Flashing the Micropython firmware&lt;/head&gt;&lt;p&gt;We now need to flash Micropython on the &lt;code&gt;ESP32&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;The firmware link is obtained from https://micropython.org/download/ESP32_GENERIC/.&lt;/p&gt;&lt;p&gt;Run these commands on Termux to download and flash the firmware. Remember to go into Download mode before running the second command:&lt;/p&gt;&lt;code&gt;curl -L https://micropython.org/resources/firmware/ESP32_GENERIC-20250911-v1.26.1.bin -o esp32-micropython.bin

esptool --chip esp32 --port $HOME/esp32 --before no-reset --after no-reset write-flash -z 0x1000 esp32-micropython.bin&lt;/code&gt;&lt;quote&gt;&lt;p&gt;√¢√Ø¬∏ IMPORTANT&lt;/p&gt;&lt;p&gt;After the flash is complete, press and release the&lt;/p&gt;&lt;code&gt;ENABLE&lt;/code&gt;/&lt;code&gt;RESET&lt;/code&gt;button in the board to exit download mode.&lt;/quote&gt;&lt;head rend="h4"&gt;√∞ Success&lt;/head&gt;&lt;p&gt;Congratulations, Micropython should now be flashed in your board.&lt;/p&gt;&lt;head rend="h2"&gt;Next steps&lt;/head&gt;&lt;p&gt;If you want to try the Micropython REPL, run this command:&lt;/p&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 repl&lt;/code&gt;&lt;p&gt;By the way, there is also &lt;code&gt;minicom&lt;/code&gt; if you want to interact with the &lt;code&gt;REPL&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;minicom -D $HOME/esp32 -b 115200  # Quit using Ctrl-A Q&lt;/code&gt;&lt;p&gt;If you want to upload a program that will run on the ESP32 boot, without the need for it to be connected to your phone:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Create a file named &lt;code&gt;program.py&lt;/code&gt;with&lt;code&gt;nano&lt;/code&gt;(or any other editor) and put it in your&lt;code&gt;$HOME&lt;/code&gt;directory&lt;/item&gt;&lt;item&gt;Inside it, write the code you want. The code I will be using is:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;import machine
import time

# Built-in LED on most ESP32 boards (GPIO 2)
led = machine.Pin(2, machine.Pin.OUT)

print("Starting LED blink...")
print("Press Ctrl+C to stop")

try:
    while True:
        led.on()
        print("LED ON")
        time.sleep(1)
        led.off()
        print("LED OFF")
        time.sleep(1)
except KeyboardInterrupt:
    led.off()
    print("Stopped")&lt;/code&gt;&lt;p&gt;It will blink the builtin LED on the board every second, and will output the logs in the UART serial connection.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Uploading the code:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 cp $HOME/program.py :main.py&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;To run it immediately:&lt;/item&gt;&lt;/list&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 run $HOME/program.py&lt;/code&gt;&lt;head rend="h3"&gt;&lt;code&gt;mpremote&lt;/code&gt; commands&lt;/head&gt; Useful &lt;head rend="h4"&gt;List files&lt;/head&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 fs ls&lt;/code&gt; &lt;head rend="h4"&gt;View a file&lt;/head&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 fs cat main.py&lt;/code&gt; &lt;head rend="h4"&gt;Delete a file&lt;/head&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 fs rm unwanted.py&lt;/code&gt; &lt;head rend="h4"&gt;Interactive REPL&lt;/head&gt;&lt;code&gt;mpremote connect port:$HOME/esp32 repl&lt;/code&gt; &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;Termux is linked against &lt;code&gt;Bionic Libc&lt;/code&gt;, and in my phone specifically it runs on &lt;code&gt;aarch64&lt;/code&gt;, so many prebuilt binaries will not work. This means that I could not compile firmware binaries from scratch, as I could not setup a toolchain for it.&lt;/p&gt;&lt;p&gt;What I tried that either did not work or I gave up on trying:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Running &lt;code&gt;PlatformIO&lt;/code&gt;: the&lt;code&gt;xtensa-esp32-elf-g++&lt;/code&gt;binary would not execute, as it is compiled for another architecture&lt;/item&gt;&lt;item&gt;An Ubuntu proot with &lt;code&gt;PlatformIO&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Using &lt;code&gt;esp-idf&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Rust√¢s &lt;code&gt;espflash&lt;/code&gt;,&lt;code&gt;espup&lt;/code&gt;,&lt;code&gt;esp-rs&lt;/code&gt;&lt;/item&gt;&lt;item&gt;To connect to the &lt;code&gt;UART&lt;/code&gt;serial:&lt;code&gt;termux-usb&lt;/code&gt;and&lt;code&gt;Termux: API&lt;/code&gt;. It would disconnect often and get a new device identifier each time, requiring to accept the permission each time. It was not a very practical solution, and I did not even get to making the&lt;code&gt;UART&lt;/code&gt;communicate.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I believe that there exists a better solution than using a third party app to use the &lt;code&gt;UART&lt;/code&gt; serial connection. However, I was not able to make it work.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45530261</guid><pubDate>Thu, 09 Oct 2025 16:56:52 +0000</pubDate></item><item><title>Show HN: I wrote a full text search engine in Go</title><link>https://github.com/wizenheimer/blaze</link><description>&lt;doc fingerprint="1a102bff8219beb6"&gt;
  &lt;main&gt;
    &lt;p&gt;A high-performance full-text search engine in Go with inverted indexing, boolean queries, phrase search, proximity queries, and BM25 ranking‚Äîpowered by a flexible query engine, roaring bitmaps, and skip lists.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Overview&lt;/item&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Quick Start&lt;/item&gt;
      &lt;item&gt;Core Concepts&lt;/item&gt;
      &lt;item&gt;Query Builder API&lt;/item&gt;
      &lt;item&gt;API Reference&lt;/item&gt;
      &lt;item&gt;Examples&lt;/item&gt;
      &lt;item&gt;Performance Characteristics&lt;/item&gt;
      &lt;item&gt;Configuration&lt;/item&gt;
      &lt;item&gt;Use Cases&lt;/item&gt;
      &lt;item&gt;Testing&lt;/item&gt;
      &lt;item&gt;Architecture&lt;/item&gt;
      &lt;item&gt;Best Practices&lt;/item&gt;
      &lt;item&gt;Contributing&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Blaze is a Go engine that provides fast, full-text search capabilities through an inverted index implementation. It's designed for applications that need to search through text documents efficiently without relying on external search engines.&lt;/p&gt;
    &lt;p&gt;Key Highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inverted Index: Maps terms to document positions for instant lookups&lt;/item&gt;
      &lt;item&gt;Skip Lists: Probabilistic data structure providing O(log n) operations&lt;/item&gt;
      &lt;item&gt;Query Builder: Type-safe, fluent API for boolean queries with roaring bitmaps&lt;/item&gt;
      &lt;item&gt;Advanced Search: Phrase search, BM25 ranking, proximity ranking, and boolean queries&lt;/item&gt;
      &lt;item&gt;BM25 Algorithm: Industry-standard relevance scoring with IDF and length normalization&lt;/item&gt;
      &lt;item&gt;Text Analysis: Tokenization, stemming, stopword filtering, and case normalization&lt;/item&gt;
      &lt;item&gt;Thread-Safe: Concurrent indexing with mutex protection&lt;/item&gt;
      &lt;item&gt;Serialization: Efficient binary format for persistence&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Term Search: Find documents containing specific terms&lt;/item&gt;
      &lt;item&gt;Phrase Search: Exact multi-word matching ("quick brown fox")&lt;/item&gt;
      &lt;item&gt;Boolean Queries: Type-safe AND, OR, NOT operations with query builder&lt;/item&gt;
      &lt;item&gt;BM25 Ranking: Industry-standard relevance scoring (used by Elasticsearch, Solr)&lt;/item&gt;
      &lt;item&gt;Proximity Ranking: Score results by term proximity&lt;/item&gt;
      &lt;item&gt;Position Tracking: Track exact word positions within documents&lt;/item&gt;
      &lt;item&gt;Roaring Bitmaps: Compressed bitmap operations for fast boolean queries&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tokenization: Unicode-aware text splitting&lt;/item&gt;
      &lt;item&gt;Stemming: Snowball (Porter2) stemmer for English&lt;/item&gt;
      &lt;item&gt;Stopword Filtering: Remove common words (the, a, is, etc.)&lt;/item&gt;
      &lt;item&gt;Case Normalization: Case-insensitive search&lt;/item&gt;
      &lt;item&gt;Configurable Pipeline: Customize analysis behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Skip Lists: O(log n) search, insert, and delete operations&lt;/item&gt;
      &lt;item&gt;Inverted Index: Efficient term-to-position mapping&lt;/item&gt;
      &lt;item&gt;Binary Serialization: Compact storage format&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;go get github.com/wizenheimer/blaze&lt;/code&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    // Create a new inverted index
    idx := blaze.NewInvertedIndex()

    // Index some documents
    idx.Index(1, "The quick brown fox jumps over the lazy dog")
    idx.Index(2, "A quick brown dog runs fast")
    idx.Index(3, "The lazy cat sleeps all day")

    // Search for documents containing "quick" and "brown"
    matches := idx.RankProximity("quick brown", 10)

    // Print results
    for _, match := range matches {
        fmt.Printf("Document %d (score: %.2f)\n",
            int(match.Offsets[0].DocumentID),
            match.Score)
    }
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Document 2 (score: 1.00)
Document 1 (score: 0.50)
&lt;/code&gt;
    &lt;p&gt;An inverted index is like the index at the back of a book. Instead of scanning every document to find a word, the index tells you exactly where each word appears.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;p&gt;Given these documents:&lt;/p&gt;
    &lt;code&gt;Doc 1: "the quick brown fox"
        Pos:0    1     2     3

Doc 2: "the lazy dog"
        Pos:0   1    2

Doc 3: "quick brown dogs"
        Pos:0    1     2
&lt;/code&gt;
    &lt;p&gt;The inverted index looks like:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Token  ‚îÇ         Posting List               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ "quick" ‚îÇ ‚Üí [Doc1:Pos1] ‚Üí [Doc3:Pos0]        ‚îÇ
‚îÇ "brown" ‚îÇ ‚Üí [Doc1:Pos2] ‚Üí [Doc3:Pos1]        ‚îÇ
‚îÇ "fox"   ‚îÇ ‚Üí [Doc1:Pos3]                      ‚îÇ
‚îÇ "lazy"  ‚îÇ ‚Üí [Doc2:Pos1]                      ‚îÇ
‚îÇ "dog"   ‚îÇ ‚Üí [Doc2:Pos2]                      ‚îÇ
‚îÇ "dogs"  ‚îÇ ‚Üí [Doc3:Pos2]                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Visual Representation:&lt;/p&gt;
    &lt;code&gt;                    Inverted Index
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Map      ‚îÇ
                    ‚îÇ [string] ‚îÇ
                    ‚îÇ SkipList ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                ‚îÇ                ‚îÇ
        ‚ñº                ‚ñº                ‚ñº
   "quick"          "brown"           "fox"
   SkipList         SkipList         SkipList
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ HEAD ‚îÇ        ‚îÇ HEAD ‚îÇ         ‚îÇ HEAD ‚îÇ
   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ               ‚îÇ                 ‚îÇ
      ‚ñº               ‚ñº                 ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇDoc1:1‚îÇ        ‚îÇDoc1:2‚îÇ         ‚îÇDoc1:3‚îÇ
   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ               ‚îÇ
      ‚ñº               ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇDoc3:0‚îÇ        ‚îÇDoc3:1‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Instant term lookups (no document scanning)&lt;/item&gt;
      &lt;item&gt;Phrase search via position checking&lt;/item&gt;
      &lt;item&gt;Proximity ranking by measuring distances&lt;/item&gt;
      &lt;item&gt;Efficient boolean queries (AND, OR, NOT)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A skip list is a probabilistic data structure that maintains sorted data with O(log n) average time complexity for search, insertion, and deletion.&lt;/p&gt;
    &lt;p&gt;Visual Representation:&lt;/p&gt;
    &lt;code&gt;Skip List with Multiple Levels (Express Lanes)
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Level 3: HEAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [30] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; NULL
              ‚Üì                                                                ‚Üì
Level 2: HEAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [15] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [30] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; NULL
              ‚Üì                                ‚Üì                               ‚Üì
Level 1: HEAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [10] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [15] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [20] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [30] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; NULL
              ‚Üì                ‚Üì                ‚Üì              ‚Üì                ‚Üì
Level 0: HEAD ‚îÄ‚îÄ&amp;gt; [5] ‚îÄ‚îÄ&amp;gt; [10] ‚îÄ‚îÄ&amp;gt; [15] ‚îÄ‚îÄ&amp;gt; [20] ‚îÄ‚îÄ&amp;gt; [25] ‚îÄ‚îÄ&amp;gt; [30] ‚îÄ‚îÄ&amp;gt; [35] ‚îÄ‚îÄ&amp;gt; NULL
         (ALL NODES AT LEVEL 0)

         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Node  ‚îÇ  Each node has a "tower" of forward pointers
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ Key   ‚îÇ  Example: Node [15]
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
         ‚îÇ Lvl 3 ‚îÇ ‚îÄ‚îÄ&amp;gt; [30]      (skip far ahead)
         ‚îÇ Lvl 2 ‚îÇ ‚îÄ‚îÄ&amp;gt; [30]      (skip ahead)
         ‚îÇ Lvl 1 ‚îÇ ‚îÄ‚îÄ&amp;gt; [20]      (skip a little)
         ‚îÇ Lvl 0 ‚îÇ ‚îÄ‚îÄ&amp;gt; [20]      (next node)
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;How Heights are Assigned (Probabilistic):&lt;/p&gt;
    &lt;code&gt;Coin Flip Algorithm:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Height  ‚îÇ Probability ‚îÇ Visual      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ    1    ‚îÇ    50%      ‚îÇ ‚ñì‚ñì‚ñì‚ñì‚ñì       ‚îÇ
‚îÇ    2    ‚îÇ    25%      ‚îÇ ‚ñì‚ñì‚ñì         ‚îÇ
‚îÇ    3    ‚îÇ   12.5%     ‚îÇ ‚ñì‚ñì          ‚îÇ
‚îÇ    4    ‚îÇ   6.25%     ‚îÇ ‚ñì           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

For 1000 nodes, expected distribution:
Level 0: ~1000 nodes (all)    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Level 1: ~500 nodes           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Level 2: ~250 nodes           ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
Level 3: ~125 nodes           ‚ñà‚ñà‚ñà‚ñà‚ñà
Level 4: ~62 nodes            ‚ñà‚ñà
&lt;/code&gt;
    &lt;p&gt;Search Algorithm (finding 20):&lt;/p&gt;
    &lt;code&gt;Step-by-Step Search for Key = 20:

Level 3: [HEAD] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [30]        (30 &amp;gt; 20, drop down)
           ‚Üì
Level 2: [HEAD] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [15] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [30]        (15 &amp;lt; 20, advance)
                                   ‚Üì
Level 2:                         [15] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [30]        (30 &amp;gt; 20, drop down)
                                   ‚Üì
Level 1:                         [15] ‚îÄ‚îÄ&amp;gt; [20]               (20 = 20, FOUND!)
                                          ^^^^

Journey Recorded:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Level 3   ‚îÇ HEAD            ‚îÇ  Predecessor at each level
‚îÇ Level 2   ‚îÇ [15]            ‚îÇ  Used for insertions/deletions
‚îÇ Level 1   ‚îÇ [15]            ‚îÇ
‚îÇ Level 0   ‚îÇ [15]            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start at HEAD, Level 3&lt;/item&gt;
      &lt;item&gt;Level 3: Move to 30? No (30 &amp;gt; 20), drop to Level 2&lt;/item&gt;
      &lt;item&gt;Level 2: Move to 15? Yes (15 &amp;lt; 20), advance to 15&lt;/item&gt;
      &lt;item&gt;Level 2: Move to 30? No (30 &amp;gt; 20), drop to Level 1&lt;/item&gt;
      &lt;item&gt;Level 1: Move to 20? Yes! Found it!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Time Complexity: O(log n) on average&lt;/p&gt;
    &lt;p&gt;Why Skip Lists?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;O(log n) operations without complex balancing&lt;/item&gt;
      &lt;item&gt;Simpler than AVL or Red-Black trees&lt;/item&gt;
      &lt;item&gt;Better cache locality than trees&lt;/item&gt;
      &lt;item&gt;Easier to make lock-free for concurrency&lt;/item&gt;
      &lt;item&gt;Used in Redis, LevelDB, and other databases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Blaze transforms raw text into searchable tokens through a multi-stage pipeline:&lt;/p&gt;
    &lt;p&gt;Pipeline Stages:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     Text Analysis Pipeline                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  1. Tokenization                       ‚îÇ
         ‚îÇ  Split on non-alphanumeric chars       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  2. Lowercasing                        ‚îÇ
         ‚îÇ  Normalize case ("Quick" ‚Üí "quick")    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  3. Stopword Filtering                 ‚îÇ
         ‚îÇ  Remove common words (the, a, is)      ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  4. Length Filtering                   ‚îÇ
         ‚îÇ  Remove tokens &amp;lt; 2 chars               ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  5. Stemming (Snowball/Porter2)        ‚îÇ
         ‚îÇ  Reduce to root ("running" ‚Üí "run")    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚ñº
                    Final Tokens
&lt;/code&gt;
    &lt;p&gt;Example Transformation:&lt;/p&gt;
    &lt;code&gt;Input:  "The Quick Brown Fox Jumps!"
        ‚îÇ
        ‚îú‚îÄ Step 1: Tokenization
        ‚îÇ  ‚îî‚îÄ&amp;gt; ["The", "Quick", "Brown", "Fox", "Jumps"]
        ‚îÇ
        ‚îú‚îÄ Step 2: Lowercasing
        ‚îÇ  ‚îî‚îÄ&amp;gt; ["the", "quick", "brown", "fox", "jumps"]
        ‚îÇ
        ‚îú‚îÄ Step 3: Stopword Filtering (remove "the")
        ‚îÇ  ‚îî‚îÄ&amp;gt; ["quick", "brown", "fox", "jumps"]
        ‚îÇ
        ‚îú‚îÄ Step 4: Length Filtering (all pass &amp;gt;= 2 chars)
        ‚îÇ  ‚îî‚îÄ&amp;gt; ["quick", "brown", "fox", "jumps"]
        ‚îÇ
        ‚îî‚îÄ Step 5: Stemming ("jumps" ‚Üí "jump")
           ‚îî‚îÄ&amp;gt; ["quick", "brown", "fox", "jump"]
&lt;/code&gt;
    &lt;p&gt;Configuration:&lt;/p&gt;
    &lt;code&gt;// Use default configuration
tokens := blaze.Analyze("The quick brown fox")

// Custom configuration
config := blaze.AnalyzerConfig{
    MinTokenLength:  3,      // Only keep tokens &amp;gt;= 3 chars
    EnableStemming:  false,  // Disable stemming
    EnableStopwords: true,   // Keep stopword filtering
}
tokens := blaze.AnalyzeWithConfig("The quick brown fox", config)&lt;/code&gt;
    &lt;p&gt;Find all occurrences of a single term:&lt;/p&gt;
    &lt;code&gt;idx := blaze.NewInvertedIndex()
idx.Index(1, "the quick brown fox")
idx.Index(2, "quick brown dogs")

// Find first occurrence of "quick"
pos, err := idx.First("quick")
if err == nil {
    fmt.Printf("Found at Doc %d, Pos %d\n",
        int(pos.DocumentID), int(pos.Offset))
}

// Find next occurrence
nextPos, _ := idx.Next("quick", pos)&lt;/code&gt;
    &lt;p&gt;Find exact sequences of words:&lt;/p&gt;
    &lt;code&gt;// Find documents containing "quick brown fox" as a phrase
matches := idx.FindAllPhrases("quick brown fox", blaze.BOFDocument)

for _, match := range matches {
    start, end := match[0], match[1]
    fmt.Printf("Found in Doc %d from Pos %d to %d\n",
        int(start.DocumentID), int(start.Offset), int(end.Offset))
}&lt;/code&gt;
    &lt;p&gt;Algorithm:&lt;/p&gt;
    &lt;code&gt;Searching for phrase: "brown fox"

Document: "the quick brown dog jumped over the brown fox"
Positions: 0     1     2    3     4      5    6     7    8

Phase 1: Find END (last word "fox")
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Find "brown" ‚Üí Doc:Pos2                                 ‚îÇ
‚îÇ Find "fox" after Pos2 ‚Üí Doc:Pos8  ‚Üê END position       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 2: Walk BACKWARDS from END to find START
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ From Pos9, find previous "brown" ‚Üí Doc:Pos7  ‚Üê START   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 3: Validate
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Start: Pos7, End: Pos8                                  ‚îÇ
‚îÇ Distance: 8 - 7 = 1                                     ‚îÇ
‚îÇ Expected: 2 words - 1 = 1  MATCH!                      ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ      "brown"  "fox"                                     ‚îÇ
‚îÇ        ‚ñ≤       ‚ñ≤                                        ‚îÇ
‚îÇ       Pos7    Pos8    (consecutive positions)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find END: Locate the last word of the phrase&lt;/item&gt;
      &lt;item&gt;Walk BACKWARDS: Find previous occurrences of earlier words&lt;/item&gt;
      &lt;item&gt;Validate: Check if positions are consecutive&lt;/item&gt;
      &lt;item&gt;Recurse: Continue searching for more matches&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Find documents containing all terms (not necessarily consecutive):&lt;/p&gt;
    &lt;code&gt;// Find documents with both "quick" and "fox"
cover := idx.NextCover([]string{"quick", "fox"}, blaze.BOFDocument)
start, end := cover[0], cover[1]

// Calculate proximity score
distance := end.Offset - start.Offset
score := 1.0 / distance  // Closer terms = higher score&lt;/code&gt;
    &lt;p&gt;Cover Algorithm:&lt;/p&gt;
    &lt;code&gt;Searching for: ["quick", "fox"] (any order, not necessarily consecutive)

Document: "the quick brown dog jumped over the lazy fox"
Positions: 0     1     2    3     4      5    6    7    8

Phase 1: Find COVER END (furthest term)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Find "quick" after BOF ‚Üí Doc:Pos1                           ‚îÇ
‚îÇ Find "fox" after BOF ‚Üí Doc:Pos8  ‚Üê FURTHEST (cover end)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 2: Find COVER START (earliest term before end)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Find "quick" before Pos9 ‚Üí Doc:Pos1  ‚Üê EARLIEST (cover start)‚îÇ
‚îÇ Find "fox" before Pos9 ‚Üí Doc:Pos8                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Phase 3: Validate &amp;amp; Return
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cover: [Pos1, Pos8]                                          ‚îÇ
‚îÇ Same document? Yes                                           ‚îÇ
‚îÇ All terms present? Yes                                       ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ "quick" ... ... ... ... ... ... ... "fox"                    ‚îÇ
‚îÇ    ‚ñ≤                                   ‚ñ≤                     ‚îÇ
‚îÇ   Pos1                                Pos8                   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Cover Range ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ Proximity Score: 1 / (8 - 1 + 1) = 1/8 = 0.125             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find FURTHEST occurrence of any term (cover end)&lt;/item&gt;
      &lt;item&gt;Find EARLIEST occurrence of each term before end (cover start)&lt;/item&gt;
      &lt;item&gt;Validate all terms are in the same document&lt;/item&gt;
      &lt;item&gt;Return [start, end] positions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BM25 (Best Matching 25) is a probabilistic ranking function used by search engines to estimate the relevance of documents to a given search query. It's the industry standard used by Elasticsearch, Solr, and Lucene.&lt;/p&gt;
    &lt;code&gt;// Search and rank using BM25
results := idx.RankBM25("machine learning", 10)

for _, match := range results {
    fmt.Printf("Doc %d: Score %.2f\n",
        match.DocID,
        match.Score)
}&lt;/code&gt;
    &lt;p&gt;What BM25 Considers:&lt;/p&gt;
    &lt;code&gt;+------------------+-------------------------------------------------------+
| Factor           | Description                                           |
+------------------+-------------------------------------------------------+
| Term Frequency   | How often does the term appear?                       |
|                  | More occurrences = higher relevance                   |
+------------------+-------------------------------------------------------+
| TF Saturation    | Diminishing returns                                   |
|                  | 3-&amp;gt;10 occurrences matters less than 0-&amp;gt;3             |
+------------------+-------------------------------------------------------+
| Document Length  | Normalize by document size                            |
|                  | Prevents long docs from dominating results            |
+------------------+-------------------------------------------------------+
| Term Rarity      | Rare terms are more important than common ones        |
|                  | "quantum" &amp;gt; "the" in importance                       |
+------------------+-------------------------------------------------------+
&lt;/code&gt;
    &lt;p&gt;Complete BM25 Formula:&lt;/p&gt;
    &lt;code&gt;                    IDF(q_i) √ó TF(q_i, D) √ó (k1 + 1)
BM25(D, Q) = SUM  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
             q_i  TF(q_i, D) + k1 √ó (1 - b + b √ó |D|/avgdl)
            in Q

Where:
    D       = Document being scored
    Q       = Query (set of terms q_1, q_2, ..., q_n)
    q_i     = Individual query term
&lt;/code&gt;
    &lt;p&gt;Component Breakdown:&lt;/p&gt;
    &lt;code&gt;+-------------------+-----------------------------------------------------+
|    Component      |                   Definition                        |
+-------------------+-----------------------------------------------------+
| IDF(q_i)          | Inverse Document Frequency                          |
|                   |                                                     |
|                   |          N - df(q_i) + 0.5                          |
|                   | log( ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ + 1 )                  |
|                   |            df(q_i) + 0.5                            |
|                   |                                                     |
|                   | N  = Total documents in corpus                      |
|                   | df = Documents containing term q_i                  |
|                   |                                                     |
|                   | Effect: Rare terms get higher weights              |
+-------------------+-----------------------------------------------------+
| TF(q_i, D)        | Term Frequency                                      |
|                   | = Number of times q_i appears in document D         |
|                   |                                                     |
|                   | Effect: More occurrences = higher relevance         |
+-------------------+-----------------------------------------------------+
| k1                | Term Frequency Saturation Parameter                 |
|                   | = 1.5 (default)                                     |
|                   | Range: [1.2, 2.0]                                   |
|                   |                                                     |
|                   | Effect: Controls diminishing returns                |
|                   |         Higher k1 = less saturation                 |
+-------------------+-----------------------------------------------------+
| b                 | Length Normalization Parameter                      |
|                   | = 0.75 (default)                                    |
|                   | Range: [0, 1]                                       |
|                   |                                                     |
|                   | Effect: Controls length penalty                     |
|                   |         b=1  = full normalization                   |
|                   |         b=0  = no normalization                     |
+-------------------+-----------------------------------------------------+
| |D|               | Document Length                                     |
|                   | = Number of terms in document D                     |
+-------------------+-----------------------------------------------------+
| avgdl             | Average Document Length                             |
|                   | = Total terms / Total documents                     |
+-------------------+-----------------------------------------------------+
&lt;/code&gt;
    &lt;p&gt;Visual Example - Term Frequency Saturation:&lt;/p&gt;
    &lt;code&gt;Score Contribution (with k1=1.5, b=0.75)
    ^
    |                            /---------------  (saturation)
    |                          /
 3  |                       /
    |                     /
 2  |                  /
    |               /
 1  |            /
    |         /
 0  |______/
    +---+---+---+---+---+---+---+---+---+---+---&amp;gt; Term Frequency
    0   1   2   3   4   5   6   7   8   9   10

Key Insight: Going from 0-&amp;gt;3 occurrences adds more to the score
             than going from 7-&amp;gt;10 occurrences (diminishing returns)
&lt;/code&gt;
    &lt;p&gt;Visual Example - Document Length Normalization:&lt;/p&gt;
    &lt;code&gt;Scenario: Same term frequency, different document lengths

Document A: 100 words, "learning" appears 3 times
Document B: 1000 words, "learning" appears 3 times

Raw TF:  Both have TF = 3
Density: Doc A = 3/100  = 3.0%    &amp;lt;- Higher density
         Doc B = 3/1000 = 0.3%    &amp;lt;- Lower density

BM25 adjusts: Doc A gets HIGHER score (term is more prominent)
              Doc B gets LOWER score (term is less prominent)

Length Penalty Formula:

    Penalty = k1 √ó (1 - b + b √ó docLen/avgDocLen)

    If docLen &amp;gt; avgDocLen: Penalty increases (score decreases)
    If docLen &amp;lt; avgDocLen: Penalty decreases (score increases)
&lt;/code&gt;
    &lt;p&gt;Step-by-Step Scoring Example:&lt;/p&gt;
    &lt;code&gt;SETUP:
------
Query:  "machine learning"
Corpus: 1000 documents, average length 150 words
Target: Document 1 (200 words)
        - "machine" appears 3 times (df=100 docs have "machine")
        - "learning" appears 2 times (df=50 docs have "learning")

Parameters: k1=1.5, b=0.75


STEP 1: Calculate IDF for each term
----------------------------------------

IDF(machine):
    N = 1000, df = 100

    IDF = log((1000 - 100 + 0.5) / (100 + 0.5) + 1)
        = log(900.5 / 100.5 + 1)
        = log(8.96 + 1)
        = log(9.96)
        ‚âà 2.30

IDF(learning):
    N = 1000, df = 50

    IDF = log((1000 - 50 + 0.5) / (50 + 0.5) + 1)
        = log(950.5 / 50.5 + 1)
        = log(18.82 + 1)
        = log(19.82)
        ‚âà 2.99

    Note: "learning" is rarer (df=50) than "machine" (df=100)
          so it gets a higher IDF weight


STEP 2: Calculate normalized TF for "machine"
----------------------------------------------

TF = 3 (appears 3 times)
docLen = 200
avgdl = 150

Numerator   = TF √ó (k1 + 1)
            = 3 √ó (1.5 + 1)
            = 3 √ó 2.5
            = 7.5

Denominator = TF + k1 √ó (1 - b + b √ó (docLen / avgdl))
            = 3 + 1.5 √ó (1 - 0.75 + 0.75 √ó (200/150))
            = 3 + 1.5 √ó (0.25 + 0.75 √ó 1.333)
            = 3 + 1.5 √ó (0.25 + 1.0)
            = 3 + 1.5 √ó 1.25
            = 3 + 1.875
            = 4.875

Normalized TF = 7.5 / 4.875 ‚âà 1.54

Contribution = IDF √ó Normalized TF
             = 2.30 √ó 1.54
             ‚âà 3.54


STEP 3: Calculate normalized TF for "learning"
-----------------------------------------------

TF = 2 (appears 2 times)
docLen = 200
avgdl = 150

Numerator   = 2 √ó 2.5 = 5.0

Denominator = 2 + 1.5 √ó (1 - 0.75 + 0.75 √ó (200/150))
            = 2 + 1.875
            = 3.875

Normalized TF = 5.0 / 3.875 ‚âà 1.29

Contribution = IDF √ó Normalized TF
             = 2.99 √ó 1.29
             ‚âà 3.86


STEP 4: Calculate final BM25 score
-----------------------------------

BM25(Document 1, "machine learning") = 3.54 + 3.86 = 7.40

                    +----------+----------+
                    | Term     | Score    |
                    +----------+----------+
                    | machine  | 3.54     |
                    | learning | 3.86     |
                    +----------+----------+
                    | TOTAL    | 7.40     |
                    +----------+----------+
&lt;/code&gt;
    &lt;p&gt;Why BM25 Works:&lt;/p&gt;
    &lt;code&gt;+------------------------+------------------------------------------------+
| Advantage              | Explanation                                    |
+------------------------+------------------------------------------------+
| Industry Standard      | Used by Elasticsearch, Solr, Lucene           |
|                        | Battle-tested in production systems            |
+------------------------+------------------------------------------------+
| Probabilistic          | Based on probability ranking principle         |
|                        | Solid theoretical foundation                   |
+------------------------+------------------------------------------------+
| Term Rarity (IDF)      | Rare terms contribute more to score            |
|                        | "quantum" &amp;gt; "the" in importance                |
+------------------------+------------------------------------------------+
| Saturation             | Diminishing returns for repeated terms         |
|                        | 0-&amp;gt;3 occurrences: HIGH impact                  |
|                        | 7-&amp;gt;10 occurrences: LOW impact                  |
+------------------------+------------------------------------------------+
| Length Normalization   | Prevents long documents from dominating        |
|                        | Adjusts for document size bias                 |
+------------------------+------------------------------------------------+
| Tunable                | Adjust k1 and b for domain-specific needs     |
|                        | Customize behavior without changing algorithm  |
+------------------------+------------------------------------------------+
&lt;/code&gt;
    &lt;p&gt;Comparison with Simple TF-IDF:&lt;/p&gt;
    &lt;code&gt;Simple TF-IDF:
    Score = TF √ó IDF
    Problem: Linear relationship with TF
             10 occurrences = 10x score of 1 occurrence

    TF-IDF Score
        ^
        |                                        /
     10 |                                      /
        |                                    /
      5 |                                  /
        |                                /
      0 |______________________________/
        +---+---+---+---+---+---+---+---+---&amp;gt; Term Frequency
        0   2   4   6   8   10  12  14  16

BM25:
    Score = IDF √ó (TF √ó (k1 + 1)) / (TF + k1 √ó length_norm)
    Benefit: Sublinear relationship with TF
             Saturation prevents spam

    BM25 Score
        ^
        |                    /----------------  (plateau)
      4 |                  /
        |                /
      2 |             /
        |          /
      0 |________/
        +---+---+---+---+---+---+---+---+---&amp;gt; Term Frequency
        0   2   4   6   8   10  12  14  16

    Key: BM25 saturates, preventing keyword stuffing exploits
&lt;/code&gt;
    &lt;p&gt;Score and rank documents by term proximity:&lt;/p&gt;
    &lt;code&gt;// Search and rank results
matches := idx.RankProximity("machine learning", 10)

for _, match := range matches {
    fmt.Printf("Doc %d: Score %.2f\n",
        int(match.Offsets[0].DocumentID),
        match.Score)
}&lt;/code&gt;
    &lt;p&gt;Scoring Formula:&lt;/p&gt;
    &lt;code&gt;For each cover in a document:
    score += 1 / (coverEnd - coverStart + 1)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Proximity Scoring Examples                                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ Doc 1: "machine learning is machine learning"                  ‚îÇ
‚îÇ         Pos:0      1      2  3       4                          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   Cover 1: [Pos 0-1]  ‚Üí score += 1/(1-0+1) = 1/2 = 0.500      ‚îÇ
‚îÇ   Cover 2: [Pos 3-4]  ‚Üí score += 1/(4-3+1) = 1/2 = 0.500      ‚îÇ
‚îÇ                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÇ
‚îÇ   Total Score: 1.000                                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ Doc 2: "learning about machine and learning"                   ‚îÇ
‚îÇ         Pos:0       1     2       3   4                         ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   Cover 1: [Pos 0-2]  ‚Üí score += 1/(2-0+1) = 1/3 = 0.333      ‚îÇ
‚îÇ   Cover 2: [Pos 2-4]  ‚Üí score += 1/(4-2+1) = 1/3 = 0.333      ‚îÇ
‚îÇ                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÇ
‚îÇ   Total Score: 0.666                                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ Doc 3: "machine ... ... ... ... learning"                      ‚îÇ
‚îÇ         Pos:0    1   2   3   4   5                              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ   Cover 1: [Pos 0-5]  ‚Üí score += 1/(5-0+1) = 1/6 = 0.167      ‚îÇ
‚îÇ                         ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÇ
‚îÇ   Total Score: 0.167                                            ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Ranking: Doc 1 (1.000) &amp;gt; Doc 2 (0.666) &amp;gt; Doc 3 (0.167)
          ‚ñ≤               ‚ñ≤               ‚ñ≤
      Terms closest   Terms medium   Terms far apart
&lt;/code&gt;
    &lt;p&gt;Why This Works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smaller distances ‚Üí larger scores (inverse relationship)&lt;/item&gt;
      &lt;item&gt;Multiple occurrences ‚Üí higher scores (additive)&lt;/item&gt;
      &lt;item&gt;Documents with terms close together rank higher&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Query Builder provides a type-safe, fluent API for constructing complex boolean queries with roaring bitmaps. No string parsing, no syntax errors - just clean, composable code.&lt;/p&gt;
    &lt;p&gt;String Parsing Approach:&lt;/p&gt;
    &lt;code&gt;// Error-prone, runtime failures
results, err := index.ExecuteQuery("(machine AND learning) OR python")
if err != nil {
    // Handle parsing errors
}&lt;/code&gt;
    &lt;p&gt;Builder Pattern Approach:&lt;/p&gt;
    &lt;code&gt;// Type-safe, compile-time checks, IDE autocomplete!
results := blaze.NewQueryBuilder(index).
    Group(func(q *blaze.QueryBuilder) {
        q.Term("machine").And().Term("learning")
    }).
    Or().
    Term("python").
    Execute()&lt;/code&gt;
    &lt;code&gt;// Find all documents containing "machine"
results := blaze.NewQueryBuilder(idx).
    Term("machine").
    Execute()

fmt.Printf("Found %d documents\n", results.GetCardinality())&lt;/code&gt;
    &lt;code&gt;// Find documents with BOTH "machine" AND "learning"
results := blaze.NewQueryBuilder(idx).
    Term("machine").
    And().
    Term("learning").
    Execute()&lt;/code&gt;
    &lt;code&gt;// Find documents with "python" OR "javascript"
results := blaze.NewQueryBuilder(idx).
    Term("python").
    Or().
    Term("javascript").
    Execute()&lt;/code&gt;
    &lt;code&gt;// Find documents with "python" but NOT "snake"
results := blaze.NewQueryBuilder(idx).
    Term("python").
    And().Not().
    Term("snake").
    Execute()&lt;/code&gt;
    &lt;code&gt;// (machine OR deep) AND learning
results := blaze.NewQueryBuilder(idx).
    Group(func(q *blaze.QueryBuilder) {
        q.Term("machine").Or().Term("deep")
    }).
    And().
    Term("learning").
    Execute()&lt;/code&gt;
    &lt;code&gt;// Find exact phrase "machine learning"
results := blaze.NewQueryBuilder(idx).
    Phrase("machine learning").
    Execute()&lt;/code&gt;
    &lt;code&gt;// Get top 10 results ranked by relevance
matches := blaze.NewQueryBuilder(idx).
    Term("machine").
    And().
    Term("learning").
    ExecuteWithBM25(10)

for _, match := range matches {
    fmt.Printf("Doc %d: score=%.2f\n", match.DocID, match.Score)
}&lt;/code&gt;
    &lt;p&gt;Creates a new query builder instance.&lt;/p&gt;
    &lt;code&gt;qb := blaze.NewQueryBuilder(idx)&lt;/code&gt;
    &lt;p&gt;Adds a single term to the query. Uses roaring bitmaps for O(1) document lookup.&lt;/p&gt;
    &lt;code&gt;qb.Term("machine")&lt;/code&gt;
    &lt;p&gt;Adds an exact phrase match. Combines bitmap efficiency with skip list position checking.&lt;/p&gt;
    &lt;code&gt;qb.Phrase("machine learning")&lt;/code&gt;
    &lt;p&gt;Combines results with intersection (both must match). Uses bitmap AND operation.&lt;/p&gt;
    &lt;code&gt;qb.Term("machine").And().Term("learning")&lt;/code&gt;
    &lt;p&gt;Combines results with union (either can match). Uses bitmap OR operation.&lt;/p&gt;
    &lt;code&gt;qb.Term("cat").Or().Term("dog")&lt;/code&gt;
    &lt;p&gt;Negates the next term (exclude from results). Uses bitmap difference operation.&lt;/p&gt;
    &lt;code&gt;qb.Term("python").And().Not().Term("snake")&lt;/code&gt;
    &lt;p&gt;Creates a sub-query with its own scope for precedence control.&lt;/p&gt;
    &lt;code&gt;qb.Group(func(q *blaze.QueryBuilder) {
    q.Term("machine").Or().Term("deep")
}).And().Term("learning")&lt;/code&gt;
    &lt;p&gt;Executes the query and returns a bitmap of matching document IDs.&lt;/p&gt;
    &lt;code&gt;results := qb.Execute()
docCount := results.GetCardinality()&lt;/code&gt;
    &lt;p&gt;Executes the query with BM25 ranking and returns top results.&lt;/p&gt;
    &lt;code&gt;matches := qb.ExecuteWithBM25(10)  // Top 10 results&lt;/code&gt;
    &lt;p&gt;The Query Builder provides convenient shorthand functions for common boolean operations:&lt;/p&gt;
    &lt;p&gt;Shorthand for documents containing ALL terms (AND operation).&lt;/p&gt;
    &lt;code&gt;// Find documents with "machine" AND "learning" AND "python"
results := blaze.AllOf(idx, "machine", "learning", "python")

// Equivalent to:
results := blaze.NewQueryBuilder(idx).
    Term("machine").And().Term("learning").And().Term("python").
    Execute()&lt;/code&gt;
    &lt;p&gt;Shorthand for documents containing ANY term (OR operation).&lt;/p&gt;
    &lt;code&gt;// Find documents with "cat" OR "dog" OR "bird"
results := blaze.AnyOf(idx, "cat", "dog", "bird")

// Equivalent to:
results := blaze.NewQueryBuilder(idx).
    Term("cat").Or().Term("dog").Or().Term("bird").
    Execute()&lt;/code&gt;
    &lt;p&gt;Shorthand for term with exclusion (AND NOT operation).&lt;/p&gt;
    &lt;code&gt;// Find documents with "python" but NOT "snake"
results := blaze.TermExcluding(idx, "python", "snake")

// Equivalent to:
results := blaze.NewQueryBuilder(idx).
    Term("python").And().Not().Term("snake").
    Execute()&lt;/code&gt;
    &lt;p&gt;Start with a broad category, then filter down with specific criteria.&lt;/p&gt;
    &lt;code&gt;// Find programming content about Python or JavaScript, excluding beginner material
results := blaze.NewQueryBuilder(idx).
    Term("programming").
    And().
    Group(func(q *blaze.QueryBuilder) {
        q.Term("python").Or().Term("javascript")
    }).
    And().Not().
    Term("beginner").
    ExecuteWithBM25(10)&lt;/code&gt;
    &lt;p&gt;Match documents that satisfy multiple independent criteria.&lt;/p&gt;
    &lt;code&gt;// Find documents about (machine learning OR deep learning) AND (python OR tensorflow)
results := blaze.NewQueryBuilder(idx).
    Group(func(q *blaze.QueryBuilder) {
        q.Phrase("machine learning").Or().Phrase("deep learning")
    }).
    And().
    Group(func(q *blaze.QueryBuilder) {
        q.Term("python").Or().Term("tensorflow")
    }).
    ExecuteWithBM25(20)&lt;/code&gt;
    &lt;p&gt;Find relevant content while filtering out noise or unwanted categories.&lt;/p&gt;
    &lt;code&gt;// Find "apple" content but exclude fruit/food related content
results := blaze.NewQueryBuilder(idx).
    Term("apple").
    And().Not().
    Group(func(q *blaze.QueryBuilder) {
        q.Term("fruit").Or().Term("food").Or().Term("cooking")
    }).
    Execute()  // Finds "Apple Inc." not the fruit&lt;/code&gt;
    &lt;p&gt;Search within specific categories or tags.&lt;/p&gt;
    &lt;code&gt;func SearchWithCategory(idx *blaze.InvertedIndex, query string, categories []string) []blaze.Match {
    qb := blaze.NewQueryBuilder(idx)

    // Add main query
    qb.Term(query)

    // Add category filter if provided
    if len(categories) &amp;gt; 0 {
        qb.And().Group(func(q *blaze.QueryBuilder) {
            q.Term(categories[0])
            for i := 1; i &amp;lt; len(categories); i++ {
                q.Or().Term(categories[i])
            }
        })
    }

    return qb.ExecuteWithBM25(20)
}&lt;/code&gt;
    &lt;p&gt;The Query Builder leverages roaring bitmaps for exceptional performance on boolean operations.&lt;/p&gt;
    &lt;code&gt;BenchmarkQueryBuilder_Simple-8       440,616 ops/sec    2,511 ns/op    896 B/op    39 allocs/op
BenchmarkQueryBuilder_Complex-8      222,024 ops/sec    5,333 ns/op  2,240 B/op    98 allocs/op
BenchmarkQueryBuilder_WithBM25-8     411,124 ops/sec    2,955 ns/op  1,416 B/op    46 allocs/op
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;Why It's Fast&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AND&lt;/cell&gt;
        &lt;cell&gt;O(1) per chunk&lt;/cell&gt;
        &lt;cell&gt;Roaring bitmap intersection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OR&lt;/cell&gt;
        &lt;cell&gt;O(1) per chunk&lt;/cell&gt;
        &lt;cell&gt;Roaring bitmap union&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NOT&lt;/cell&gt;
        &lt;cell&gt;O(1) per chunk&lt;/cell&gt;
        &lt;cell&gt;Roaring bitmap difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Term Lookup&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;Direct hash map access&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For a term appearing in 500,000 documents:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Skip list positions: ~24 MB (500k nodes √ó 48 bytes)&lt;/item&gt;
      &lt;item&gt;Roaring bitmap: ~60 KB (400x compression!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;// Good: Clear precedence with groups
qb.Group(func(q *blaze.QueryBuilder) {
    q.Term("a").Or().Term("b")
}).And().Term("c")

// Bad: Ambiguous without groups
qb.Term("a").Or().Term("b").And().Term("c")  // Is this (a OR b) AND c or a OR (b AND c)?&lt;/code&gt;
    &lt;code&gt;// Good: Clean and readable
results := blaze.AllOf(idx, "python", "django", "web")

// Bad: Verbose for simple case
results := blaze.NewQueryBuilder(idx).
    Term("python").And().Term("django").And().Term("web").
    Execute()&lt;/code&gt;
    &lt;code&gt;// Good: Ranked results for users
matches := qb.ExecuteWithBM25(10)

// Bad: Unranked - harder for users to find relevant docs
bitmap := qb.Execute()&lt;/code&gt;
    &lt;code&gt;// Good: Exact phrase + related term
qb.Phrase("machine learning").And().Term("python")

// Bad: Overly restrictive
qb.Phrase("machine learning python")  // Requires exact phrase&lt;/code&gt;
    &lt;code&gt;func BuildDynamicQuery(idx *blaze.InvertedIndex, required []string, optional []string, excluded []string) *roaring.Bitmap {
    qb := blaze.NewQueryBuilder(idx)

    // Add required terms (AND)
    if len(required) &amp;gt; 0 {
        qb.Term(required[0])
        for i := 1; i &amp;lt; len(required); i++ {
            qb.And().Term(required[i])
        }
    }

    // Add optional terms (OR)
    if len(optional) &amp;gt; 0 {
        if len(required) &amp;gt; 0 {
            qb.And()
        }
        qb.Group(func(q *blaze.QueryBuilder) {
            q.Term(optional[0])
            for i := 1; i &amp;lt; len(optional); i++ {
                q.Or().Term(optional[i])
            }
        })
    }

    // Exclude terms (NOT)
    for _, term := range excluded {
        qb.And().Not().Term(term)
    }

    return qb.Execute()
}&lt;/code&gt;
    &lt;code&gt;func SearchProducts(idx *blaze.InvertedIndex, searchTerm string, category string, excludeOutOfStock bool) []blaze.Match {
    qb := blaze.NewQueryBuilder(idx).Term(searchTerm)

    // Add category filter
    if category != "" {
        qb.And().Term(category)
    }

    // Exclude out of stock items
    if excludeOutOfStock {
        qb.And().Not().Term("outofstock")
    }

    return qb.ExecuteWithBM25(20)
}&lt;/code&gt;
    &lt;code&gt;func SearchInCategories(idx *blaze.InvertedIndex, query string, categories []string) []blaze.Match {
    qb := blaze.NewQueryBuilder(idx).Term(query)

    if len(categories) &amp;gt; 0 {
        qb.And().Group(func(q *blaze.QueryBuilder) {
            q.Term(categories[0])
            for i := 1; i &amp;lt; len(categories); i++ {
                q.Or().Term(categories[i])
            }
        })
    }

    return qb.ExecuteWithBM25(50)
}&lt;/code&gt;
    &lt;code&gt;func FilterContent(idx *blaze.InvertedIndex, searchTerm string, blocklist []string) *roaring.Bitmap {
    qb := blaze.NewQueryBuilder(idx).Term(searchTerm)

    for _, blocked := range blocklist {
        qb.And().Not().Term(blocked)
    }

    return qb.Execute()
}&lt;/code&gt;
    &lt;code&gt;func AdvancedSearch(idx *blaze.InvertedIndex, phrases []string, requiredTerms []string) []blaze.Match {
    qb := blaze.NewQueryBuilder(idx)

    // Match any of the phrases (OR)
    qb.Group(func(q *blaze.QueryBuilder) {
        q.Phrase(phrases[0])
        for i := 1; i &amp;lt; len(phrases); i++ {
            q.Or().Phrase(phrases[i])
        }
    })

    // AND with required terms
    for _, term := range requiredTerms {
        qb.And().Term(term)
    }

    return qb.ExecuteWithBM25(10)
}

// Usage:
results := AdvancedSearch(idx,
    []string{"machine learning", "deep learning"},
    []string{"python", "tensorflow"})&lt;/code&gt;
    &lt;code&gt;func SearchHandler(w http.ResponseWriter, r *http.Request) {
    query := r.URL.Query().Get("q")
    category := r.URL.Query().Get("category")
    exclude := r.URL.Query().Get("exclude")

    qb := blaze.NewQueryBuilder(index).Term(query)

    if category != "" {
        qb.And().Term(category)
    }

    if exclude != "" {
        qb.And().Not().Term(exclude)
    }

    results := qb.ExecuteWithBM25(20)
    json.NewEncoder(w).Encode(results)
}&lt;/code&gt;
    &lt;code&gt;func SemanticSearch(idx *blaze.InvertedIndex, concept string, relatedTerms []string) []blaze.Match {
    qb := blaze.NewQueryBuilder(idx)

    // Main concept OR any related terms
    qb.Term(concept)
    for _, related := range relatedTerms {
        qb.Or().Term(related)
    }

    return qb.ExecuteWithBM25(50)
}

// Usage:
results := SemanticSearch(idx, "automobile",
    []string{"car", "vehicle", "transportation", "automotive"})&lt;/code&gt;
    &lt;code&gt;func NewInvertedIndex() *InvertedIndex&lt;/code&gt;
    &lt;p&gt;Creates a new empty inverted index.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;idx := blaze.NewInvertedIndex()&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) Index(docID int, document string)&lt;/code&gt;
    &lt;p&gt;Adds a document to the inverted index. Thread-safe.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;docID&lt;/code&gt;: Unique document identifier&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;document&lt;/code&gt;: Text content to index&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;idx.Index(1, "The quick brown fox jumps over the lazy dog")
idx.Index(2, "A fast brown dog")&lt;/code&gt;
    &lt;p&gt;What Happens:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Text is analyzed (tokenized, stemmed, etc.)&lt;/item&gt;
      &lt;item&gt;Each token is recorded with its position&lt;/item&gt;
      &lt;item&gt;Positions are stored in skip lists for fast lookup&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;func (idx *InvertedIndex) First(token string) (Position, error)&lt;/code&gt;
    &lt;p&gt;Returns the first occurrence of a token in the index.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;pos, err := idx.First("quick")
if err != nil {
    // Token not found
}
fmt.Printf("Doc %d, Pos %d\n", int(pos.DocumentID), int(pos.Offset))&lt;/code&gt;
    &lt;p&gt;Returns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Position&lt;/code&gt;: Location of first occurrence&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;error&lt;/code&gt;:&lt;code&gt;ErrNoPostingList&lt;/code&gt;if token doesn't exist&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;func (idx *InvertedIndex) Last(token string) (Position, error)&lt;/code&gt;
    &lt;p&gt;Returns the last occurrence of a token in the index.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;pos, err := idx.Last("quick")&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) Next(token string, currentPos Position) (Position, error)&lt;/code&gt;
    &lt;p&gt;Finds the next occurrence of a token after the given position.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;// Iterate through all occurrences
pos := blaze.BOFDocument
for {
    pos, err = idx.Next("quick", pos)
    if pos.IsEnd() || err != nil {
        break
    }
    fmt.Printf("Found at Doc %d, Pos %d\n",
        int(pos.DocumentID), int(pos.Offset))
}&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) Previous(token string, currentPos Position) (Position, error)&lt;/code&gt;
    &lt;p&gt;Finds the previous occurrence of a token before the given position.&lt;/p&gt;
    &lt;code&gt;func (idx *InvertedIndex) NextPhrase(query string, startPos Position) []Position&lt;/code&gt;
    &lt;p&gt;Finds the next occurrence of a phrase (exact word sequence).&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;query&lt;/code&gt;: Space-separated phrase (e.g., "quick brown fox")&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;startPos&lt;/code&gt;: Position to start searching from&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[]Position&lt;/code&gt;: Array with two elements [phraseStart, phraseEnd]&lt;/item&gt;
      &lt;item&gt;Returns &lt;code&gt;[EOFDocument, EOFDocument]&lt;/code&gt;if no match found&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;matches := idx.NextPhrase("quick brown fox", blaze.BOFDocument)
if !matches[0].IsEnd() {
    fmt.Printf("Phrase found in Doc %d from Pos %d to %d\n",
        int(matches[0].DocumentID),
        int(matches[0].Offset),
        int(matches[1].Offset))
}&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) FindAllPhrases(query string, startPos Position) [][]Position&lt;/code&gt;
    &lt;p&gt;Finds all occurrences of a phrase in the entire index.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;allMatches := idx.FindAllPhrases("brown fox", blaze.BOFDocument)
for _, match := range allMatches {
    fmt.Printf("Doc %d: Pos %d-%d\n",
        int(match[0].DocumentID),
        int(match[0].Offset),
        int(match[1].Offset))
}&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) NextCover(tokens []string, startPos Position) []Position&lt;/code&gt;
    &lt;p&gt;Finds the next "cover" - a range containing all given tokens.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;tokens&lt;/code&gt;: Array of search terms&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;startPos&lt;/code&gt;: Position to start searching from&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[]Position&lt;/code&gt;: Array with [coverStart, coverEnd]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;cover := idx.NextCover([]string{"quick", "fox", "brown"}, blaze.BOFDocument)
fmt.Printf("Cover: Doc %d, Pos %d-%d\n",
    int(cover[0].DocumentID),
    int(cover[0].Offset),
    int(cover[1].Offset))&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) RankBM25(query string, maxResults int) []Match&lt;/code&gt;
    &lt;p&gt;Performs BM25 ranking of search results. This is the recommended search function for most use cases.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;query&lt;/code&gt;: Search query (e.g., "machine learning")&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;maxResults&lt;/code&gt;: Maximum number of results to return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[]Match&lt;/code&gt;: Sorted array of matches with BM25 scores&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;results := idx.RankBM25("machine learning", 10)
for i, match := range results {
    fmt.Printf("%d. Doc %d (score: %.2f)\n",
        i+1,
        match.DocID,
        match.Score)
}&lt;/code&gt;
    &lt;p&gt;Match Structure:&lt;/p&gt;
    &lt;code&gt;type Match struct {
    DocID   int        // Document identifier
    Offsets []Position // Where terms appear in the document
    Score   float64    // BM25 relevance score
}&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) RankProximity(query string, maxResults int) []Match&lt;/code&gt;
    &lt;p&gt;Performs proximity-based ranking of search results. Alternative to BM25, ranks by term proximity.&lt;/p&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;query&lt;/code&gt;: Search query (e.g., "machine learning")&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;maxResults&lt;/code&gt;: Maximum number of results to return&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Returns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[]Match&lt;/code&gt;: Sorted array of matches with proximity scores&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;results := idx.RankProximity("quick brown", 5)
for i, match := range results {
    fmt.Printf("%d. Doc %d (score: %.2f)\n",
        i+1,
        int(match.Offsets[0].DocumentID),
        match.Score)
}&lt;/code&gt;
    &lt;p&gt;BM25 vs Proximity Ranking:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;BM25&lt;/cell&gt;
        &lt;cell role="head"&gt;Proximity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Term Rarity&lt;/cell&gt;
        &lt;cell&gt;Yes (IDF)&lt;/cell&gt;
        &lt;cell&gt;No (all terms equal)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Length Normalization&lt;/cell&gt;
        &lt;cell&gt;Yes (built-in)&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Term Frequency&lt;/cell&gt;
        &lt;cell&gt;Yes (with saturation)&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Term Distance&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes (main factor)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Use Case&lt;/cell&gt;
        &lt;cell&gt;General search&lt;/cell&gt;
        &lt;cell&gt;Finding close co-occurrences&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Industry Standard&lt;/cell&gt;
        &lt;cell&gt;Yes (Elasticsearch, Solr)&lt;/cell&gt;
        &lt;cell&gt;No (custom algorithm)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;func (idx *InvertedIndex) Encode() ([]byte, error)&lt;/code&gt;
    &lt;p&gt;Serializes the inverted index to binary format.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;data, err := idx.Encode()
if err != nil {
    log.Fatal(err)
}

// Save to file
err = os.WriteFile("index.bin", data, 0644)&lt;/code&gt;
    &lt;code&gt;func (idx *InvertedIndex) Decode(data []byte) error&lt;/code&gt;
    &lt;p&gt;Deserializes binary data back into an inverted index.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;data, err := os.ReadFile("index.bin")
if err != nil {
    log.Fatal(err)
}

idx := blaze.NewInvertedIndex()
err = idx.Decode(data)&lt;/code&gt;
    &lt;code&gt;func Analyze(text string) []string&lt;/code&gt;
    &lt;p&gt;Transforms raw text into searchable tokens using the default pipeline.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;tokens := blaze.Analyze("The Quick Brown Fox Jumps!")
// Returns: ["quick", "brown", "fox", "jump"]&lt;/code&gt;
    &lt;code&gt;func AnalyzeWithConfig(text string, config AnalyzerConfig) []string&lt;/code&gt;
    &lt;p&gt;Transforms text using a custom configuration.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;config := blaze.AnalyzerConfig{
    MinTokenLength:  3,
    EnableStemming:  false,
    EnableStopwords: true,
}
tokens := blaze.AnalyzeWithConfig("The quick brown fox", config)&lt;/code&gt;
    &lt;code&gt;func (p *Position) GetDocumentID() int
func (p *Position) GetOffset() int
func (p *Position) IsBeginning() bool
func (p *Position) IsEnd() bool
func (p *Position) IsBefore(other Position) bool
func (p *Position) IsAfter(other Position) bool
func (p *Position) Equals(other Position) bool&lt;/code&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;pos1 := blaze.Position{DocumentID: 1, Offset: 5}
pos2 := blaze.Position{DocumentID: 1, Offset: 10}

if pos1.IsBefore(pos2) {
    fmt.Println("pos1 comes before pos2")
}&lt;/code&gt;
    &lt;code&gt;func NewSkipList() *SkipList&lt;/code&gt;
    &lt;p&gt;Creates a new empty skip list.&lt;/p&gt;
    &lt;code&gt;func (sl *SkipList) Insert(key Position)&lt;/code&gt;
    &lt;p&gt;Adds or updates a position in the skip list. Average O(log n).&lt;/p&gt;
    &lt;code&gt;func (sl *SkipList) Find(key Position) (Position, error)&lt;/code&gt;
    &lt;p&gt;Searches for an exact position. Average O(log n).&lt;/p&gt;
    &lt;code&gt;func (sl *SkipList) Delete(key Position) bool&lt;/code&gt;
    &lt;p&gt;Removes a position from the skip list. Average O(log n).&lt;/p&gt;
    &lt;code&gt;func (sl *SkipList) FindLessThan(key Position) (Position, error)&lt;/code&gt;
    &lt;p&gt;Finds the largest position less than the given position.&lt;/p&gt;
    &lt;code&gt;func (sl *SkipList) FindGreaterThan(key Position) (Position, error)&lt;/code&gt;
    &lt;p&gt;Finds the smallest position greater than the given position.&lt;/p&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    // Create index
    idx := blaze.NewInvertedIndex()

    // Index documents
    idx.Index(1, "Go is a programming language designed at Google")
    idx.Index(2, "Python is a high-level programming language")
    idx.Index(3, "Go is fast and efficient for system programming")

    // Search for "programming language" using BM25
    results := idx.RankBM25("programming language", 10)

    fmt.Println("Search results for 'programming language':")
    for i, match := range results {
        fmt.Printf("%d. Document %d (score: %.3f)\n", i+1, match.DocID, match.Score)
    }
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Search results for 'programming language':
1. Document 1 (score: 4.521)
2. Document 2 (score: 4.521)
3. Document 3 (score: 2.156)
&lt;/code&gt;
    &lt;p&gt;Note: BM25 scores are absolute values (not normalized to 0-1), reflecting relevance based on term frequency, document length, and term rarity.&lt;/p&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    idx := blaze.NewInvertedIndex()

    idx.Index(1, "the quick brown fox jumps over the lazy dog")
    idx.Index(2, "a quick brown dog runs fast")
    idx.Index(3, "the lazy brown fox sleeps")

    // Find exact phrase "brown fox"
    matches := idx.FindAllPhrases("brown fox", blaze.BOFDocument)

    fmt.Println("Documents containing 'brown fox' as a phrase:")
    for _, match := range matches {
        docID := int(match[0].DocumentID)
        start := int(match[0].Offset)
        end := int(match[1].Offset)
        fmt.Printf("Document %d: positions %d-%d\n", docID, start, end)
    }
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Documents containing 'brown fox' as a phrase:
Document 1: positions 1-2
Document 3: positions 2-3
&lt;/code&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    idx := blaze.NewInvertedIndex()

    idx.Index(1, "quick test quick test quick")
    idx.Index(2, "another quick test here")

    // Find all occurrences of "quick"
    fmt.Println("All occurrences of 'quick':")

    pos := blaze.BOFDocument
    for {
        pos, err := idx.Next("quick", pos)
        if err != nil || pos.IsEnd() {
            break
        }
        fmt.Printf("  Doc %d, Pos %d\n",
            int(pos.DocumentID),
            int(pos.Offset))
    }
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;All occurrences of 'quick':
  Doc 1, Pos 0
  Doc 1, Pos 2
  Doc 1, Pos 4
  Doc 2, Pos 1
&lt;/code&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "os"
    "github.com/wizenheimer/blaze"
)

func main() {
    // Build and save index
    idx := blaze.NewInvertedIndex()
    idx.Index(1, "machine learning algorithms")
    idx.Index(2, "deep learning neural networks")
    idx.Index(3, "natural language processing")

    // Serialize to binary
    data, err := idx.Encode()
    if err != nil {
        panic(err)
    }

    // Save to file
    err = os.WriteFile("search_index.bin", data, 0644)
    if err != nil {
        panic(err)
    }
    fmt.Println("Index saved to search_index.bin")

    // Load index from file
    loadedData, err := os.ReadFile("search_index.bin")
    if err != nil {
        panic(err)
    }

    loadedIdx := blaze.NewInvertedIndex()
    err = loadedIdx.Decode(loadedData)
    if err != nil {
        panic(err)
    }

    // Use loaded index
    results := loadedIdx.RankProximity("learning", 5)
    fmt.Printf("Found %d documents\n", len(results))
}&lt;/code&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    // Create custom analyzer config (no stemming, longer min length)
    config := blaze.AnalyzerConfig{
        MinTokenLength:  3,      // Minimum 3 characters
        EnableStemming:  false,  // Keep original word forms
        EnableStopwords: true,   // Still remove stopwords
    }

    text := "The running dogs are running fast"

    // Compare default vs custom analysis
    defaultTokens := blaze.Analyze(text)
    customTokens := blaze.AnalyzeWithConfig(text, config)

    fmt.Println("Default tokens:", defaultTokens)
    fmt.Println("Custom tokens:", customTokens)
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Default tokens: [run dog run fast]
Custom tokens: [running dogs running fast]
&lt;/code&gt;
    &lt;code&gt;package main

import (
    "fmt"
    "github.com/wizenheimer/blaze"
)

func main() {
    idx := blaze.NewInvertedIndex()

    // Index documents
    idx.Index(1, "machine learning algorithms")
    idx.Index(2, "machine learning machine learning")  // High term frequency
    idx.Index(3, "machine and algorithms and learning") // Terms far apart

    query := "machine learning"

    // BM25 Ranking
    fmt.Println("BM25 Rankings:")
    bm25Results := idx.RankBM25(query, 10)
    for i, match := range bm25Results {
        fmt.Printf("%d. Doc %d (score: %.3f)\n", i+1, match.DocID, match.Score)
    }

    // Proximity Ranking
    fmt.Println("\nProximity Rankings:")
    proxResults := idx.RankProximity(query, 10)
    for i, match := range proxResults {
        docID := int(match.Offsets[0].DocumentID)
        fmt.Printf("%d. Doc %d (score: %.3f)\n", i+1, docID, match.Score)
    }
}&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;BM25 Rankings:
1. Doc 2 (score: 5.234)  ‚Üê High term frequency
2. Doc 1 (score: 3.156)
3. Doc 3 (score: 2.891)

Proximity Rankings:
1. Doc 1 (score: 1.000)  ‚Üê Terms adjacent
2. Doc 2 (score: 1.000)
3. Doc 3 (score: 0.200)  ‚Üê Terms far apart
&lt;/code&gt;
    &lt;p&gt;Key Differences:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BM25 favors Doc 2 (repeated terms = high relevance)&lt;/item&gt;
      &lt;item&gt;Proximity favors Doc 1 and Doc 2 equally (both have adjacent terms)&lt;/item&gt;
      &lt;item&gt;Doc 3 ranks low in both (terms spread out)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;package main

import (
    "bufio"
    "fmt"
    "os"
    "strings"
    "github.com/wizenheimer/blaze"
)

func main() {
    // Create index
    idx := blaze.NewInvertedIndex()

    // Index some documents
    docs := map[int]string{
        1: "Go is an open source programming language that makes it easy to build simple, reliable, and efficient software",
        2: "Python is a programming language that lets you work quickly and integrate systems more effectively",
        3: "JavaScript is a programming language that conforms to the ECMAScript specification",
        4: "Rust is a multi-paradigm programming language focused on performance and safety",
        5: "Java is a class-based, object-oriented programming language designed for portability",
    }

    for id, doc := range docs {
        idx.Index(id, doc)
    }

    // Interactive search
    scanner := bufio.NewScanner(os.Stdin)

    for {
        fmt.Print("\nSearch query (or 'quit' to exit): ")
        if !scanner.Scan() {
            break
        }

        query := strings.TrimSpace(scanner.Text())
        if query == "quit" {
            break
        }

        if query == "" {
            continue
        }

        // Perform search using BM25
        results := idx.RankBM25(query, 5)

        if len(results) == 0 {
            fmt.Println("No results found")
            continue
        }

        // Display results
        fmt.Printf("\nFound %d result(s):\n", len(results))
        for i, match := range results {
            fmt.Printf("\n%d. Document %d (Score: %.3f)\n", i+1, match.DocID, match.Score)
            fmt.Printf("   %s\n", docs[match.DocID])
        }
    }
}&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;Average&lt;/cell&gt;
        &lt;cell role="head"&gt;Worst Case&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Index (per document)&lt;/cell&gt;
        &lt;cell&gt;O(n √ó log m)&lt;/cell&gt;
        &lt;cell&gt;O(n √ó m)&lt;/cell&gt;
        &lt;cell&gt;n = tokens, m = total positions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Term lookup&lt;/cell&gt;
        &lt;cell&gt;O(log m)&lt;/cell&gt;
        &lt;cell&gt;O(m)&lt;/cell&gt;
        &lt;cell&gt;m = positions for term&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Phrase search&lt;/cell&gt;
        &lt;cell&gt;O(k √ó log m)&lt;/cell&gt;
        &lt;cell&gt;O(k √ó m)&lt;/cell&gt;
        &lt;cell&gt;k = phrase length&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BM25 ranking&lt;/cell&gt;
        &lt;cell&gt;O(t √ó d)&lt;/cell&gt;
        &lt;cell&gt;O(t √ó d)&lt;/cell&gt;
        &lt;cell&gt;t = query terms, d = candidates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Proximity ranking&lt;/cell&gt;
        &lt;cell&gt;O(t √ó m)&lt;/cell&gt;
        &lt;cell&gt;O(t √ó m)&lt;/cell&gt;
        &lt;cell&gt;t = query terms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Skip list insert&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;n = elements in list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Skip list search&lt;/cell&gt;
        &lt;cell&gt;O(log n)&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;Probabilistically rare&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Space&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Inverted index&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;n = total unique positions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Skip list nodes&lt;/cell&gt;
        &lt;cell&gt;O(n √ó log n)&lt;/cell&gt;
        &lt;cell&gt;Average 2 pointers per node&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Analyzer&lt;/cell&gt;
        &lt;cell&gt;O(1)&lt;/cell&gt;
        &lt;cell&gt;In-place processing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Serialized index&lt;/cell&gt;
        &lt;cell&gt;O(n)&lt;/cell&gt;
        &lt;cell&gt;Compact binary format&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Performance on Apple M2 (8 cores), Go 1.24:&lt;/p&gt;
    &lt;code&gt;BenchmarkIndex-8                     50000    35421 ns/op    18234 B/op    245 allocs/op
BenchmarkTermSearch-8              300000     4123 ns/op      128 B/op      3 allocs/op
BenchmarkPhraseSearch-8            100000    12456 ns/op      512 B/op     12 allocs/op
BenchmarkRankBM25-8                  60000    24567 ns/op     1856 B/op     38 allocs/op
BenchmarkProximityRanking-8         50000    28934 ns/op     2048 B/op     45 allocs/op
BenchmarkCalculateIDF-8           5000000      234 ns/op       16 B/op      1 allocs/op
BenchmarkCalculateBM25Score-8     2000000      567 ns/op       64 B/op      2 allocs/op
BenchmarkSkipListInsert-8         3000000      413 ns/op      255 B/op      6 allocs/op
BenchmarkSkipListSearch-8         5000000      203 ns/op       23 B/op      1 allocs/op
BenchmarkAnalyze-8                1000000     1234 ns/op      512 B/op      8 allocs/op
BenchmarkEncode-8                   10000   156789 ns/op    65536 B/op    234 allocs/op
BenchmarkDecode-8                   15000   123456 ns/op    49152 B/op    189 allocs/op
&lt;/code&gt;
    &lt;p&gt;Index Size vs Performance:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Documents&lt;/cell&gt;
        &lt;cell role="head"&gt;Terms&lt;/cell&gt;
        &lt;cell role="head"&gt;Index Time&lt;/cell&gt;
        &lt;cell role="head"&gt;Search Time&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1K&lt;/cell&gt;
        &lt;cell&gt;10K&lt;/cell&gt;
        &lt;cell&gt;50ms&lt;/cell&gt;
        &lt;cell&gt;0.5ms&lt;/cell&gt;
        &lt;cell&gt;2 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;10K&lt;/cell&gt;
        &lt;cell&gt;100K&lt;/cell&gt;
        &lt;cell&gt;500ms&lt;/cell&gt;
        &lt;cell&gt;1ms&lt;/cell&gt;
        &lt;cell&gt;20 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;100K&lt;/cell&gt;
        &lt;cell&gt;1M&lt;/cell&gt;
        &lt;cell&gt;5s&lt;/cell&gt;
        &lt;cell&gt;2ms&lt;/cell&gt;
        &lt;cell&gt;200 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1M&lt;/cell&gt;
        &lt;cell&gt;10M&lt;/cell&gt;
        &lt;cell&gt;50s&lt;/cell&gt;
        &lt;cell&gt;5ms&lt;/cell&gt;
        &lt;cell&gt;2 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Search time remains relatively constant due to O(log n) operations&lt;/item&gt;
      &lt;item&gt;Memory scales linearly with unique positions&lt;/item&gt;
      &lt;item&gt;Serialization reduces storage by ~40% compared to in-memory size&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Customize BM25 ranking behavior:&lt;/p&gt;
    &lt;code&gt;type BM25Parameters struct {
    K1 float64 // Term frequency saturation (default: 1.5)
    B  float64 // Length normalization (default: 0.75)
}&lt;/code&gt;
    &lt;p&gt;Tuning BM25:&lt;/p&gt;
    &lt;code&gt;idx := blaze.NewInvertedIndex()

// Adjust BM25 parameters before indexing
idx.BM25Params.K1 = 2.0  // Higher = less saturation (more weight to TF)
idx.BM25Params.B = 0.5   // Lower = less length penalty

// Now index and search
idx.Index(1, "document content")
results := idx.RankBM25("query", 10)&lt;/code&gt;
    &lt;p&gt;Parameter Effects:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
        &lt;cell role="head"&gt;When to Adjust&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;K1&lt;/cell&gt;
        &lt;cell&gt;1.2 - 2.0&lt;/cell&gt;
        &lt;cell&gt;Controls TF saturation&lt;/cell&gt;
        &lt;cell&gt;Higher for domains where term frequency matters more&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;B&lt;/cell&gt;
        &lt;cell&gt;0 - 1&lt;/cell&gt;
        &lt;cell&gt;Controls length penalty&lt;/cell&gt;
        &lt;cell&gt;Lower for domains with naturally longer docs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;// Academic papers (long documents, repeated terms important)
idx.BM25Params.K1 = 2.0
idx.BM25Params.B = 0.5

// Short messages (length less important)
idx.BM25Params.K1 = 1.2
idx.BM25Params.B = 0.3

// Default (works well for most cases)
idx.BM25Params.K1 = 1.5
idx.BM25Params.B = 0.75&lt;/code&gt;
    &lt;p&gt;BM25 Statistics:&lt;/p&gt;
    &lt;p&gt;During indexing, Blaze automatically tracks:&lt;/p&gt;
    &lt;code&gt;type DocumentStats struct {
    DocID     int            // Document identifier
    Length    int            // Number of terms
    TermFreqs map[string]int // Term frequencies
}

// Corpus-level statistics
idx.TotalDocs  // Total documents indexed
idx.TotalTerms // Total terms across all documents
idx.DocStats   // Per-document statistics&lt;/code&gt;
    &lt;p&gt;These statistics are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automatically computed during indexing&lt;/item&gt;
      &lt;item&gt;Serialized with the index&lt;/item&gt;
      &lt;item&gt;Used for BM25 score calculation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Customize the text analysis pipeline:&lt;/p&gt;
    &lt;code&gt;type AnalyzerConfig struct {
    MinTokenLength  int  // Minimum token length (default: 2)
    EnableStemming  bool // Apply stemming (default: true)
    EnableStopwords bool // Remove stopwords (default: true)
}&lt;/code&gt;
    &lt;p&gt;Configuration Examples:&lt;/p&gt;
    &lt;code&gt;// Exact matching (no stemming, keep all words)
exactConfig := blaze.AnalyzerConfig{
    MinTokenLength:  1,
    EnableStemming:  false,
    EnableStopwords: false,
}

// Fuzzy matching (aggressive stemming)
fuzzyConfig := blaze.AnalyzerConfig{
    MinTokenLength:  2,
    EnableStemming:  true,
    EnableStopwords: true,
}

// Code search (no stemming, no stopwords, longer tokens)
codeConfig := blaze.AnalyzerConfig{
    MinTokenLength:  3,
    EnableStemming:  false,
    EnableStopwords: false,
}&lt;/code&gt;
    &lt;p&gt;MinTokenLength:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1: Very permissive, large index&lt;/item&gt;
      &lt;item&gt;2: Balanced (default), filters single chars&lt;/item&gt;
      &lt;item&gt;3: Strict, smaller index, misses short words&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;EnableStemming:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;true: Better recall, finds related words ("run" matches "running")&lt;/item&gt;
      &lt;item&gt;false: Exact matching, preserves original word forms&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;EnableStopwords:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;true: Smaller index, faster search, standard behavior&lt;/item&gt;
      &lt;item&gt;false: Complete indexing, useful for phrase search&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;const MaxHeight = 32  // Maximum tower height&lt;/code&gt;
    &lt;p&gt;Tower Height Probability:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Height 1: 50%&lt;/item&gt;
      &lt;item&gt;Height 2: 25%&lt;/item&gt;
      &lt;item&gt;Height 3: 12.5%&lt;/item&gt;
      &lt;item&gt;Height 4: 6.25%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This geometric distribution ensures O(log n) average performance.&lt;/p&gt;
    &lt;p&gt;Build a search engine for documents:&lt;/p&gt;
    &lt;code&gt;type Document struct {
    ID      int
    Title   string
    Content string
}

func IndexDocuments(docs []Document) *blaze.InvertedIndex {
    idx := blaze.NewInvertedIndex()

    for _, doc := range docs {
        // Combine title and content
        text := doc.Title + " " + doc.Content
        idx.Index(doc.ID, text)
    }

    return idx
}

func SearchDocuments(idx *blaze.InvertedIndex, query string) []int {
    // Use BM25 for general relevance ranking (recommended)
    matches := idx.RankBM25(query, 20)

    docIDs := make([]int, len(matches))
    for i, match := range matches {
        docIDs[i] = match.DocID
    }

    return docIDs
}

// Alternative: Use proximity ranking to find documents with close term matches
func SearchDocumentsByProximity(idx *blaze.InvertedIndex, query string) []int {
    matches := idx.RankProximity(query, 20)

    docIDs := make([]int, len(matches))
    for i, match := range matches {
        docIDs[i] = int(match.Offsets[0].DocumentID)
    }

    return docIDs
}&lt;/code&gt;
    &lt;p&gt;Search through log files:&lt;/p&gt;
    &lt;code&gt;func IndexLogs(logFile string) (*blaze.InvertedIndex, error) {
    idx := blaze.NewInvertedIndex()

    file, err := os.Open(logFile)
    if err != nil {
        return nil, err
    }
    defer file.Close()

    scanner := bufio.NewScanner(file)
    lineNumber := 1

    for scanner.Scan() {
        idx.Index(lineNumber, scanner.Text())
        lineNumber++
    }

    return idx, scanner.Err()
}

// Find all ERROR log lines using BM25 (considers frequency and rarity)
errorLogs := idx.RankBM25("ERROR", 100)

// Alternative: Use proximity for finding error patterns
// e.g., "connection timeout" appearing close together
patternMatches := idx.RankProximity("connection timeout", 50)&lt;/code&gt;
    &lt;p&gt;Search through source code:&lt;/p&gt;
    &lt;code&gt;func IndexCodebase(rootDir string) (*blaze.InvertedIndex, error) {
    idx := blaze.NewInvertedIndex()
    fileID := 1

    // Custom config for code (no stemming, keep all words)
    config := blaze.AnalyzerConfig{
        MinTokenLength:  2,
        EnableStemming:  false,
        EnableStopwords: false,
    }

    err := filepath.Walk(rootDir, func(path string, info os.FileInfo, err error) error {
        if err != nil || info.IsDir() {
            return err
        }

        // Only index Go files
        if !strings.HasSuffix(path, ".go") {
            return nil
        }

        content, err := os.ReadFile(path)
        if err != nil {
            return err
        }

        // Use custom analyzer
        tokens := blaze.AnalyzeWithConfig(string(content), config)
        // ... index tokens ...

        fileID++
        return nil
    })

    return idx, err
}

// BM25: Find files with frequent mentions of a function/variable
bm25Results := idx.RankBM25("http.Handler", 20)

// Proximity: Find exact API patterns (e.g., function calls with parameters)
// Better for finding "http.HandleFunc" as a specific pattern
proximityResults := idx.RankProximity("http HandleFunc", 20)&lt;/code&gt;
    &lt;p&gt;Search product catalog:&lt;/p&gt;
    &lt;code&gt;type Product struct {
    ID          int
    Name        string
    Description string
    Category    string
    Tags        []string
}

func IndexProducts(products []Product) *blaze.InvertedIndex {
    idx := blaze.NewInvertedIndex()

    for _, product := range products {
        // Combine all searchable fields
        searchText := fmt.Sprintf("%s %s %s %s",
            product.Name,
            product.Description,
            product.Category,
            strings.Join(product.Tags, " "))

        idx.Index(product.ID, searchText)
    }

    return idx
}

// BM25: Best for general product search (considers all factors)
results := idx.RankBM25("wireless headphones", 10)

// Proximity: Good for finding exact product name matches
// (e.g., "Sony WH-1000XM4" as an exact phrase proximity)
exactMatches := idx.RankProximity("wireless headphones", 10)&lt;/code&gt;
    &lt;p&gt;Index and search email messages:&lt;/p&gt;
    &lt;code&gt;type Email struct {
    ID      int
    From    string
    Subject string
    Body    string
}

func IndexEmails(emails []Email) *blaze.InvertedIndex {
    idx := blaze.NewInvertedIndex()

    for _, email := range emails {
        searchText := fmt.Sprintf("%s %s %s",
            email.From,
            email.Subject,
            email.Body)

        idx.Index(email.ID, searchText)
    }

    return idx
}

// BM25: Find emails where terms appear frequently (general search)
matches := idx.RankBM25("project deadline", 50)

// Proximity: Find emails where "project" and "deadline" appear close together
// (more precise, better for finding specific mentions)
closeMatches := idx.RankProximity("project deadline", 50)&lt;/code&gt;
    &lt;code&gt;# Run all tests
make test

# Run tests with coverage
make test-coverage

# Run benchmarks
make bench

# Run all checks (format, vet, lint, test)
make check&lt;/code&gt;
    &lt;p&gt;The library has comprehensive test coverage:&lt;/p&gt;
    &lt;code&gt;$ make test-coverage
Running tests...
ok      github.com/wizenheimer/blaze    2.456s  coverage: 98.5% of statements
Generating coverage report...
Coverage report: coverage.html&lt;/code&gt;
    &lt;p&gt;Coverage by Component:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inverted Index: 100%&lt;/item&gt;
      &lt;item&gt;Skip Lists: 100%&lt;/item&gt;
      &lt;item&gt;Text Analysis: 100%&lt;/item&gt;
      &lt;item&gt;Search Operations: 100%&lt;/item&gt;
      &lt;item&gt;BM25 Ranking: 100%&lt;/item&gt;
      &lt;item&gt;Serialization: 100%&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example test:&lt;/p&gt;
    &lt;code&gt;func TestSearchFunctionality(t *testing.T) {
    idx := blaze.NewInvertedIndex()

    // Index test documents
    idx.Index(1, "the quick brown fox")
    idx.Index(2, "the lazy brown dog")

    // Test phrase search
    matches := idx.FindAllPhrases("brown fox", blaze.BOFDocument)

    if len(matches) != 1 {
        t.Errorf("Expected 1 match, got %d", len(matches))
    }

    if int(matches[0][0].DocumentID) != 1 {
        t.Errorf("Expected document 1, got %d", int(matches[0][0].DocumentID))
    }
}&lt;/code&gt;
    &lt;code&gt;blaze/
‚îú‚îÄ‚îÄ index.go          # Inverted index implementation with hybrid storage
‚îú‚îÄ‚îÄ query.go          # Query builder with roaring bitmaps
‚îú‚îÄ‚îÄ skiplist.go       # Skip list data structure for positions
‚îú‚îÄ‚îÄ search.go         # Search algorithms (phrase, proximity, BM25)
‚îú‚îÄ‚îÄ analyzer.go       # Text analysis pipeline
‚îú‚îÄ‚îÄ serialization.go  # Binary encoding/decoding (skip lists + bitmaps)
‚îú‚îÄ‚îÄ *_test.go         # Comprehensive test suite
‚îú‚îÄ‚îÄ Makefile          # Development commands
‚îî‚îÄ‚îÄ public/           # Documentation website
    ‚îî‚îÄ‚îÄ index.html
&lt;/code&gt;
    &lt;p&gt;The query processor uses a hybrid storage approach combining roaring bitmaps for document-level operations and skip lists for position-level operations.&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      QUERY PROCESSOR ARCHITECTURE                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                              User Query
                          "machine AND learning"
                                  ‚îÇ
                                  ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ    Text Analyzer            ‚îÇ
                    ‚îÇ  (tokenize, stem, etc.)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    ["machine", "learning"]
                                  ‚îÇ
                                  ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ     Query Builder           ‚îÇ
                    ‚îÇ  (constructs query tree)    ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                    Query Tree: AND(machine, learning)
                                  ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ                     ‚îÇ                     ‚îÇ
            ‚ñº                     ‚ñº                     ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Bitmap Ops   ‚îÇ    ‚îÇ  Skip List    ‚îÇ    ‚îÇ  BM25 Scorer  ‚îÇ
    ‚îÇ  (fast AND/OR)‚îÇ    ‚îÇ  (positions)  ‚îÇ    ‚îÇ  (ranking)    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                     ‚îÇ                     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚ñº
                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                          ‚îÇ    Results    ‚îÇ
                          ‚îÇ  (ranked docs)‚îÇ
                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;Blaze uses a sophisticated hybrid storage model:&lt;/p&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        HYBRID STORAGE MODEL                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

For each term "machine":

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DOCUMENT LEVEL (Roaring Bitmap)                                        ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ  DocBitmaps["machine"] = {1, 2, 4, 5, 100, 500, 1000, ...}             ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ  Compressed representation of ALL documents containing "machine"         ‚îÇ
‚îÇ  Use: Fast boolean operations (AND, OR, NOT)                            ‚îÇ
‚îÇ  Size: ~60 KB for 500k documents (400x compression!)                    ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                  ‚îÇ
                                  ‚îÇ Links to
                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  POSITION LEVEL (Skip List)                                             ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ  PostingsList["machine"] = SkipList:                                    ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ    Level 2: [Doc1:Pos5] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [Doc100:Pos12]       ‚îÇ
‚îÇ                 ‚îÇ                                       ‚îÇ                ‚îÇ
‚îÇ    Level 1: [Doc1:Pos5] ‚îÄ‚îÄ&amp;gt; [Doc2:Pos3] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ&amp;gt; [Doc100:Pos12]   ‚îÇ
‚îÇ                 ‚îÇ              ‚îÇ                         ‚îÇ               ‚îÇ
‚îÇ    Level 0: [Doc1:Pos5] -&amp;gt; [Doc2:Pos3] -&amp;gt; [Doc4:Pos1] -&amp;gt; [Doc5:Pos7]  ‚îÇ
‚îÇ             -&amp;gt; [Doc100:Pos12] -&amp;gt; [Doc500:Pos2] -&amp;gt; ...                  ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îÇ  Detailed position information for EVERY occurrence                      ‚îÇ
‚îÇ  Use: Phrase search, proximity ranking, snippets                        ‚îÇ
‚îÇ  Size: ~24 MB for 500k positions                                        ‚îÇ
‚îÇ                                                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

WHY HYBRID?
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
1. Bitmaps: Lightning-fast document filtering (AND, OR, NOT in microseconds)
2. Skip Lists: Precise position tracking for phrases and proximity
3. Best of both worlds: Speed + Precision
&lt;/code&gt;
    &lt;p&gt;Here's how a complex query executes step-by-step:&lt;/p&gt;
    &lt;code&gt;QUERY: (machine OR deep) AND learning AND NOT neural

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 1: BITMAP PHASE (Fast Document Filtering)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Term Lookups (O(1) hash map):
    DocBitmaps["machine"] = {1, 2, 4, 5, 7, 8, 9, 10}
    DocBitmaps["deep"]    = {2, 3, 5, 6, 8, 9}
    DocBitmaps["learning"]= {1, 2, 4, 5, 6, 7, 8, 9, 10}
    DocBitmaps["neural"]  = {3, 6, 8, 9}

Boolean Operations (O(1) per chunk):
    Step 1: machine OR deep
            {1, 2, 4, 5, 7, 8, 9, 10} ‚à™ {2, 3, 5, 6, 8, 9}
          = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}

    Step 2: (machine OR deep) AND learning
            {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} ‚à© {1, 2, 4, 5, 6, 7, 8, 9, 10}
          = {1, 2, 4, 5, 6, 7, 8, 9, 10}

    Step 3: Result AND NOT neural
            {1, 2, 4, 5, 6, 7, 8, 9, 10} \ {3, 6, 8, 9}
          = {1, 2, 4, 5, 7, 10}  ‚Üê CANDIDATE DOCUMENTS

    Time: ~10 microseconds for 1M documents!

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 2: POSITION PHASE (Optional - for phrases/proximity)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

IF phrase search needed:
    For each candidate doc {1, 2, 4, 5, 7, 10}:
        Use skip lists to verify exact positions
        Check consecutive positions for phrases
        Extract position data for snippets

    Time: O(log n) per position lookup

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  STEP 3: RANKING PHASE (BM25 Scoring)                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

For each candidate document:
    1. Calculate IDF (Inverse Document Frequency):
       - Uses bitmap cardinality for instant document counts
       - IDF("machine") = log((N - df + 0.5) / (df + 0.5))
       - df = DocBitmaps["machine"].GetCardinality()

    2. Calculate TF (Term Frequency):
       - Retrieves from pre-computed DocStats
       - TF("machine", Doc1) = termFreqs["machine"]

    3. Apply BM25 formula:
       - Combines IDF, TF, and length normalization
       - Score = IDF √ó (TF √ó (k1 + 1)) / (TF + k1 √ó length_norm)

    4. Sum scores for all query terms

Results sorted by score:
    Doc 5: 8.45
    Doc 2: 7.23
    Doc 1: 6.91
    ...

    Time: O(candidates √ó terms)
&lt;/code&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INVERTED INDEX STRUCTURE                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

InvertedIndex {
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  DocBitmaps: map[string]*roaring.Bitmap                         ‚îÇ
    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
    ‚îÇ  "machine"  ‚Üí [Compressed Bitmap: 512 bytes]                    ‚îÇ
    ‚îÇ  "learning" ‚Üí [Compressed Bitmap: 448 bytes]                    ‚îÇ
    ‚îÇ  "deep"     ‚Üí [Compressed Bitmap: 256 bytes]                    ‚îÇ
    ‚îÇ  ...                                                             ‚îÇ
    ‚îÇ                                                                  ‚îÇ
    ‚îÇ  Memory: ~100 bytes per term (compressed)                       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Parallel Storage
                              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  PostingsList: map[string]SkipList                              ‚îÇ
    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
    ‚îÇ  "machine"  ‚Üí SkipList with 10,000 position nodes               ‚îÇ
    ‚îÇ  "learning" ‚Üí SkipList with 8,000 position nodes                ‚îÇ
    ‚îÇ  "deep"     ‚Üí SkipList with 5,000 position nodes                ‚îÇ
    ‚îÇ  ...                                                             ‚îÇ
    ‚îÇ                                                                  ‚îÇ
    ‚îÇ  Memory: ~48 bytes per position (node overhead)                 ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Statistics
                              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  DocStats: map[int]DocumentStats                                ‚îÇ
    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
    ‚îÇ  Doc1 ‚Üí {Length: 150, TermFreqs: {"machine": 3, ...}}          ‚îÇ
    ‚îÇ  Doc2 ‚Üí {Length: 200, TermFreqs: {"learning": 5, ...}}         ‚îÇ
    ‚îÇ  ...                                                             ‚îÇ
    ‚îÇ                                                                  ‚îÇ
    ‚îÇ  Memory: ~16 bytes per term per document                        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚îÇ Metadata
                              ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Global Statistics                                               ‚îÇ
    ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ
    ‚îÇ  TotalDocs:   1,000,000                                         ‚îÇ
    ‚îÇ  TotalTerms:  150,000,000                                       ‚îÇ
    ‚îÇ  AvgDocLen:   150.0                                             ‚îÇ
    ‚îÇ  BM25Params:  {K1: 1.5, B: 0.75}                               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Mutex for thread safety (sync.RWMutex)
}

MEMORY BREAKDOWN (for 1M documents, 10M unique positions):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
DocBitmaps:     ~10 MB  (compressed bitmaps)
PostingsList:   ~480 MB (skip list nodes)
DocStats:       ~500 MB (per-doc statistics)
Overhead:       ~10 MB  (maps, pointers, etc.)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TOTAL:          ~1 GB
&lt;/code&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    ROARING BITMAP STRUCTURE                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Document IDs: {1, 2, 3, 100, 101, 102, 500000, 500001, 999999}

Traditional Bitmap (naive):
    [1,1,1,0,0...0,1,1,1,0...0,1,1,0...0,1]
    Size: 1,000,000 bits = 125 KB (wasteful for sparse data)

Roaring Bitmap (smart):

    Split into 65,536 chunks (high 16 bits = chunk ID):

    Chunk 0 (docs 0-65535):      [1,2,3,100,101,102]
    Chunk 7 (docs 458752-524287): [500000, 500001]
    Chunk 15 (docs 983040-1048575): [999999]

    Storage per chunk (adaptive):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ If cardinality &amp;lt; 4096:                             ‚îÇ
    ‚îÇ   ‚Üí Use Array Container                            ‚îÇ
    ‚îÇ   ‚Üí Store sorted uint16 values directly            ‚îÇ
    ‚îÇ   ‚Üí Size: 2 bytes √ó cardinality                    ‚îÇ
    ‚îÇ                                                     ‚îÇ
    ‚îÇ If cardinality &amp;gt; 4096:                             ‚îÇ
    ‚îÇ   ‚Üí Use Bitmap Container                           ‚îÇ
    ‚îÇ   ‚Üí Store 65536-bit bitmap (8 KB)                 ‚îÇ
    ‚îÇ   ‚Üí Size: 8 KB fixed                               ‚îÇ
    ‚îÇ                                                     ‚îÇ
    ‚îÇ If cardinality = 65536 (all docs):                ‚îÇ
    ‚îÇ   ‚Üí Use Run Container                              ‚îÇ
    ‚îÇ   ‚Üí Store: [0-65535]                               ‚îÇ
    ‚îÇ   ‚Üí Size: 4 bytes                                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Total Size: ~60 bytes (vs 125 KB!)

    Operations:

    AND: Container-by-container intersection
         Skip non-matching chunks (O(1))
         Intersect matching chunks (O(min(n,m)))

    OR:  Container-by-container union
         Merge all chunks (O(n+m))

    NOT: Complement within document space
         Flip all bits in each chunk
&lt;/code&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   QUERY BUILDER EXECUTION MODEL                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Query: NewQueryBuilder(idx).
         Group(func(q) { q.Term("machine").Or().Term("deep") }).
         And().
         Term("learning").
         Execute()

INTERNAL REPRESENTATION:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

QueryBuilder {
    stack: []*roaring.Bitmap       // Operand stack
    ops:   []QueryOp               // Operator stack
    terms: []string                // Track for BM25
}

EXECUTION TRACE:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Step 1: Group(func(q) { ... })
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Create sub-builder                    ‚îÇ
    ‚îÇ Execute sub-query                     ‚îÇ
    ‚îÇ Push result bitmap to parent stack    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Sub-query execution:
      1.1: Term("machine")
           ‚Üí Lookup: DocBitmaps["machine"]
           ‚Üí Push: {1,2,4,5,7,8,9,10}

      1.2: Or()
           ‚Üí Push operator: OR

      1.3: Term("deep")
           ‚Üí Lookup: DocBitmaps["deep"]
           ‚Üí Push: {2,3,5,6,8,9}

      1.4: Apply OR
           ‚Üí Pop: {2,3,5,6,8,9}
           ‚Üí Pop: {1,2,4,5,7,8,9,10}
           ‚Üí Union: {1,2,3,4,5,6,7,8,9,10}
           ‚Üí Push result

    Result: {1,2,3,4,5,6,7,8,9,10}

Step 2: And()
    ‚Üí Push operator: AND

Step 3: Term("learning")
    ‚Üí Lookup: DocBitmaps["learning"]
    ‚Üí Push: {1,2,4,5,6,7,8,9,10}

Step 4: Execute()
    ‚Üí Pop: {1,2,4,5,6,7,8,9,10}
    ‚Üí Pop: {1,2,3,4,5,6,7,8,9,10}
    ‚Üí Intersect: {1,2,4,5,6,7,8,9,10}
    ‚Üí Return final bitmap

OPERATION COSTS:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Bitmap Lookup:    O(1)          ~100 ns
Bitmap Union:     O(n+m)        ~1 ¬µs for 10k docs
Bitmap Intersect: O(min(n,m))   ~800 ns for 10k docs
Bitmap Difference: O(n)         ~900 ns for 10k docs

Total Query Time: ~10 ¬µs for typical query!
&lt;/code&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Complete Data Flow                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                              User Input
                       "The Quick Brown Fox!"
                                ‚îÇ
                                ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ      Text Analysis Pipeline               ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ 1. Tokenization                     ‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ    ["The", "Quick", "Brown", "Fox"] ‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îÇ               ‚ñº                            ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ 2. Lowercasing                      ‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ    ["the", "quick", "brown", "fox"] ‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îÇ               ‚ñº                            ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ 3. Stopword Filtering               ‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ    ["quick", "brown", "fox"]        ‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îÇ               ‚ñº                            ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ 4. Length Filtering                 ‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ    ["quick", "brown", "fox"]        ‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îÇ               ‚ñº                            ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ 5. Stemming                         ‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ    ["quick", "brown", "fox"]        ‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚ñº
                    ["quick", "brown", "fox"]
                            ‚îÇ
                            ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ       Inverted Index (Indexing)           ‚îÇ
            ‚îÇ                                            ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
            ‚îÇ  ‚îÇ "quick" ‚îÇ ‚Üí SkipList             ‚îÇ     ‚îÇ
            ‚îÇ  ‚îÇ         ‚îÇ    ‚îî‚îÄ&amp;gt; [Doc1:Pos0]     ‚îÇ     ‚îÇ
            ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ
            ‚îÇ  ‚îÇ "brown" ‚îÇ ‚Üí SkipList             ‚îÇ     ‚îÇ
            ‚îÇ  ‚îÇ         ‚îÇ    ‚îî‚îÄ&amp;gt; [Doc1:Pos1]     ‚îÇ     ‚îÇ
            ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     ‚îÇ
            ‚îÇ  ‚îÇ "fox"   ‚îÇ ‚Üí SkipList             ‚îÇ     ‚îÇ
            ‚îÇ  ‚îÇ         ‚îÇ    ‚îî‚îÄ&amp;gt; [Doc1:Pos2]     ‚îÇ     ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ        Search Operations          ‚îÇ
          ‚ñº                                   ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  Term    ‚îÇ                      ‚îÇ  Phrase    ‚îÇ
    ‚îÇ  Search  ‚îÇ                      ‚îÇ  Search    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   Proximity   ‚îÇ
            ‚îÇ   Ranking     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  Ranked Results       ‚îÇ
            ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
            ‚îÇ  ‚îÇ Doc 1: Score 1.0‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ Doc 2: Score 0.5‚îÇ  ‚îÇ
            ‚îÇ  ‚îÇ Doc 3: Score 0.3‚îÇ  ‚îÇ
            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;p&gt;1. Skip Lists over Balanced Trees&lt;/p&gt;
    &lt;p&gt;Rationale:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simpler implementation (no rotation logic)&lt;/item&gt;
      &lt;item&gt;Better cache locality&lt;/item&gt;
      &lt;item&gt;Easier to make concurrent&lt;/item&gt;
      &lt;item&gt;Comparable performance (O(log n))&lt;/item&gt;
      &lt;item&gt;Used in production systems (Redis, LevelDB)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Position-Based Indexing&lt;/p&gt;
    &lt;p&gt;Instead of just tracking document IDs, Blaze tracks exact word positions:&lt;/p&gt;
    &lt;code&gt;Traditional Index (Document IDs only):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "quick" ‚îÇ [Doc1, Doc3]     ‚îÇ  Cannot do phrase search
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  Cannot rank by proximity

Position-Based Index (Document + Offset):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "quick" ‚îÇ [Doc1:Pos1, Doc3:Pos0]             ‚îÇ  Enables phrase search
‚îÇ "brown" ‚îÇ [Doc1:Pos2, Doc3:Pos1]             ‚îÇ  Enables proximity ranking
‚îÇ "fox"   ‚îÇ [Doc1:Pos3]                        ‚îÇ  Enables snippet generation
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  Enables precise results

Can verify: "quick brown" is a phrase in Doc1 (Pos1‚ÜíPos2)
            but NOT in Doc3 (Pos0 and Pos1 are not "quick brown")
&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enables phrase search (check consecutive positions)&lt;/item&gt;
      &lt;item&gt;Enables proximity ranking (measure distances)&lt;/item&gt;
      &lt;item&gt;Enables snippet generation (extract relevant parts)&lt;/item&gt;
      &lt;item&gt;More precise search results&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Trade-offs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Larger index size (~2-3x more data)&lt;/item&gt;
      &lt;item&gt;More complex algorithms (but still O(log n))&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3. Binary Serialization&lt;/p&gt;
    &lt;p&gt;Custom binary format instead of JSON:&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;60% smaller file size&lt;/item&gt;
      &lt;item&gt;3x faster parsing&lt;/item&gt;
      &lt;item&gt;Preserves skip list structure&lt;/item&gt;
      &lt;item&gt;Suitable for large indexes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;4. Configurable Text Analysis&lt;/p&gt;
    &lt;p&gt;Pluggable analyzer configuration:&lt;/p&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adapt to different use cases&lt;/item&gt;
      &lt;item&gt;Trade-off precision vs recall&lt;/item&gt;
      &lt;item&gt;Support multiple languages (future)&lt;/item&gt;
      &lt;item&gt;Domain-specific customization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use stable, unique identifiers:&lt;/p&gt;
    &lt;code&gt;// Good: Use database primary keys
idx.Index(dbRecord.ID, dbRecord.Content)

// Bad: Use array indices (changes when reordering)
for i, doc := range docs {
    idx.Index(i, doc.Content)  // Don't do this
}&lt;/code&gt;
    &lt;code&gt;func IndexLargeDataset(docs []Document) *blaze.InvertedIndex {
    idx := blaze.NewInvertedIndex()

    // Process in batches
    batchSize := 1000
    for i := 0; i &amp;lt; len(docs); i += batchSize {
        end := min(i+batchSize, len(docs))
        batch := docs[i:end]

        for _, doc := range batch {
            idx.Index(doc.ID, doc.Content)
        }

        // Optional: periodic serialization for checkpoints
        if i%10000 == 0 {
            data, _ := idx.Encode()
            os.WriteFile(fmt.Sprintf("checkpoint_%d.bin", i), data, 0644)
        }
    }

    return idx
}&lt;/code&gt;
    &lt;p&gt;Match configuration to your use case:&lt;/p&gt;
    &lt;code&gt;// Natural language text (books, articles)
naturalLanguageConfig := blaze.AnalyzerConfig{
    MinTokenLength:  2,
    EnableStemming:  true,   // Find related words
    EnableStopwords: true,   // Remove common words
}

// Technical documentation (code, APIs)
technicalConfig := blaze.AnalyzerConfig{
    MinTokenLength:  2,
    EnableStemming:  false,  // Keep exact terms
    EnableStopwords: false,  // Keep all words
}

// Product names (e-commerce)
productConfig := blaze.AnalyzerConfig{
    MinTokenLength:  1,      // Include single chars (e.g., "X")
    EnableStemming:  false,  // Exact product names
    EnableStopwords: false,  // Keep all words
}&lt;/code&gt;
    &lt;p&gt;Don't rebuild the index every time:&lt;/p&gt;
    &lt;code&gt;const indexFile = "search_index.bin"

func LoadOrBuildIndex(docs []Document) (*blaze.InvertedIndex, error) {
    // Try to load existing index
    if data, err := os.ReadFile(indexFile); err == nil {
        idx := blaze.NewInvertedIndex()
        if err := idx.Decode(data); err == nil {
            return idx, nil
        }
    }

    // Build new index
    idx := blaze.NewInvertedIndex()
    for _, doc := range docs {
        idx.Index(doc.ID, doc.Content)
    }

    // Save for next time
    if data, err := idx.Encode(); err == nil {
        os.WriteFile(indexFile, data, 0644)
    }

    return idx, nil
}&lt;/code&gt;
    &lt;p&gt;The Index method is thread-safe, but for read-heavy workloads:&lt;/p&gt;
    &lt;code&gt;type SearchService struct {
    idx *blaze.InvertedIndex
    mu  sync.RWMutex
}

func (s *SearchService) Index(docID int, content string) {
    s.mu.Lock()
    defer s.mu.Unlock()
    s.idx.Index(docID, content)
}

func (s *SearchService) Search(query string) []blaze.Match {
    s.mu.RLock()
    defer s.mu.RUnlock()
    return s.idx.RankProximity(query, 20)
}&lt;/code&gt;
    &lt;p&gt;Track index growth:&lt;/p&gt;
    &lt;code&gt;func (idx *InvertedIndex) Stats() map[string]interface{} {
    stats := make(map[string]interface{})

    stats["unique_terms"] = len(idx.PostingsList)

    totalPositions := 0
    for _, skipList := range idx.PostingsList {
        // Count positions in this skip list
        iter := skipList.Iterator()
        for iter.HasNext() {
            iter.Next()
            totalPositions++
        }
    }

    stats["total_positions"] = totalPositions
    stats["avg_positions_per_term"] = float64(totalPositions) / float64(len(idx.PostingsList))

    return stats
}&lt;/code&gt;
    &lt;p&gt;Use BM25 when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You need industry-standard relevance ranking&lt;/item&gt;
      &lt;item&gt;Term frequency matters (documents with more occurrences rank higher)&lt;/item&gt;
      &lt;item&gt;You want automatic length normalization&lt;/item&gt;
      &lt;item&gt;Rare terms should be weighted more heavily&lt;/item&gt;
      &lt;item&gt;Recommended for most use cases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use Proximity when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You want to find terms close together&lt;/item&gt;
      &lt;item&gt;Term distance is more important than frequency&lt;/item&gt;
      &lt;item&gt;You're searching for specific co-occurrences&lt;/item&gt;
      &lt;item&gt;You need snippet generation (using position data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Practical Examples:&lt;/p&gt;
    &lt;code&gt;// E-commerce: General product search
// BM25 considers term frequency and rarity
bm25Results := idx.RankBM25("wireless bluetooth headphones", 20)
// Returns products with any/all terms, ranked by relevance

// E-commerce: Exact product name
// Proximity finds terms appearing together
proxResults := idx.RankProximity("Sony WH-1000XM4", 20)
// Returns products where terms appear close together

// Document search: Research papers
// BM25 for broad topic search
papers := idx.RankBM25("neural networks deep learning", 50)

// Document search: Finding specific phrase mentions
// Proximity for finding "neural networks" as a concept
mentions := idx.RankProximity("neural networks", 50)

// Best practice: Use both for different purposes!
generalResults := idx.RankBM25(query, 100)    // Cast wide net
preciseResults := idx.RankProximity(query, 20) // Refine results&lt;/code&gt;
    &lt;p&gt;Always specify a reasonable max results:&lt;/p&gt;
    &lt;code&gt;// Good: Limit results
results := idx.RankBM25("search query", 100)

// Bad: Could return millions of results
results := idx.RankBM25("search query", math.MaxInt32)&lt;/code&gt;
    &lt;p&gt;Normalize queries before searching:&lt;/p&gt;
    &lt;code&gt;func NormalizeQuery(query string) string {
    // Remove extra whitespace
    query = strings.TrimSpace(query)
    query = strings.Join(strings.Fields(query), " ")

    // Convert to lowercase
    query = strings.ToLower(query)

    // Remove special characters (optional)
    query = regexp.MustCompile(`[^\w\s]`).ReplaceAllString(query, "")

    return query
}

// Use normalized query
normalizedQuery := NormalizeQuery(userInput)
results := idx.RankBM25(normalizedQuery, 20)&lt;/code&gt;
    &lt;p&gt;Track corpus statistics for insights:&lt;/p&gt;
    &lt;code&gt;// After indexing
fmt.Printf("Total documents: %d\n", idx.TotalDocs)
fmt.Printf("Total terms: %d\n", idx.TotalTerms)
fmt.Printf("Average doc length: %.2f\n",
    float64(idx.TotalTerms) / float64(idx.TotalDocs))

// Per-document analysis
for docID, stats := range idx.DocStats {
    fmt.Printf("Doc %d: %d terms\n", docID, stats.Length)

    // Find most frequent terms
    for term, freq := range stats.TermFreqs {
        if freq &amp;gt; 5 {
            fmt.Printf("  %s: %d occurrences\n", term, freq)
        }
    }
}&lt;/code&gt;
    &lt;p&gt;Contributions are welcome! Please follow these guidelines:&lt;/p&gt;
    &lt;code&gt;# Clone repository
git clone https://github.com/wizenheimer/blaze.git
cd blaze

# Install dependencies
make deps

# Run tests
make test

# Run linter
make lint&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow Go conventions (gofmt, golint)&lt;/item&gt;
      &lt;item&gt;Write comprehensive comments&lt;/item&gt;
      &lt;item&gt;Include examples in documentation&lt;/item&gt;
      &lt;item&gt;Add tests for new features&lt;/item&gt;
      &lt;item&gt;Keep functions focused and small&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use descriptive commit messages:&lt;/p&gt;
    &lt;code&gt;Good:
- "feat: Add proximity ranking algorithm"
- "feat: Handle empty query in RankProximity"
- "fix: Reduce allocations in skip list insert"

Bad:
- "Update code"
- "Fix bug uwu"
- "WIP"
&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch (&lt;code&gt;git checkout -b feature/amazing-feature&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Make your changes&lt;/item&gt;
      &lt;item&gt;Add tests&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make check&lt;/code&gt;to verify&lt;/item&gt;
      &lt;item&gt;Commit your changes&lt;/item&gt;
      &lt;item&gt;Push to your fork&lt;/item&gt;
      &lt;item&gt;Open a Pull Request&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Skip Lists: Original paper by William Pugh (1990)&lt;/item&gt;
      &lt;item&gt;Snowball Stemmer: Martin Porter's stemming algorithm&lt;/item&gt;
      &lt;item&gt;Inspiration: Elasticsearch, Lucene, Mettis, Redis, LevelDB&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45530388</guid><pubDate>Thu, 09 Oct 2025 17:09:22 +0000</pubDate></item><item><title>LLMs are mortally terrified of exceptions</title><link>https://twitter.com/karpathy/status/1976077806443569355</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45530486</guid><pubDate>Thu, 09 Oct 2025 17:16:28 +0000</pubDate></item><item><title>Subway Builder: A realistic subway simulation game</title><link>https://www.subwaybuilder.com/</link><description>&lt;doc fingerprint="2c92c4bb60b2a7fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Subway Builder is a hyperrealistic transit simulation game. Build a new subway system from the ground up while dealing with real-world constraints and costs.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;p&gt;Real-world passenger simulation&lt;/p&gt;
    &lt;p&gt;Millions of commuters are generated from Census and Redistricter data and simulated using the same pathfinding algorithms you use. Your job is to design a route network that gets the most people to their destination as fast as possible. Juggle station placement, transfer dynamics, and train frequency to maximize ridership.&lt;/p&gt;
    &lt;p&gt;Realistic construction challenges&lt;/p&gt;
    &lt;p&gt;Build your system under realistic constraints and costs. Tunnels, viaducts, cut-and-cover, all have trade offs. Negotiate with real-world buildings foundations, geography and road layouts as you expand your network&lt;/p&gt;
    &lt;p&gt;In-depth analysis&lt;/p&gt;
    &lt;p&gt;Explore how individual commuters weight use various variables like wait times, transfers, income distribution, delays, and more, to pick their commute. Understand which routes, stations, and trains your commuters take and use that information to optimize your network.&lt;/p&gt;
    &lt;p&gt;Delays and disruptions&lt;/p&gt;
    &lt;p&gt;Find the right balance between cost and time. Too many trains on a line or an overcrowded station will cause delays.&lt;/p&gt;
    &lt;p&gt;$30 on subwaybuilder.com and $40 on Steam (page is coming soon). The Steam launch won't happen for a few months after the launch on subwaybuilder.com.&lt;/p&gt;
    &lt;p&gt;Windows, macOS, and Linux.&lt;/p&gt;
    &lt;p&gt;Probably. If your computer can run Google Earth on Chrome smoothly it can run Subway Builder it's a very lightweight game. It does require an internet connection to load the map tiles for the game though.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45530744</guid><pubDate>Thu, 09 Oct 2025 17:38:29 +0000</pubDate></item><item><title>Rubygems.org AWS Root Access Event ‚Äì September 2025</title><link>https://rubycentral.org/news/rubygems-org-aws-root-access-event-september-2025/</link><description>&lt;doc fingerprint="ad1ac4950a8bf24c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rubygems.org AWS Root Access Event ‚Äì September 2025&lt;/head&gt;
    &lt;p&gt;As part of standard incident-response practice, Ruby Central is publishing the following post-incident review to the public. This document summarizes the September 2025 AWS root-access event, what occurred, what we verified, and the actions we‚Äôve taken to strengthen our security processes.&lt;/p&gt;
    &lt;p&gt;On September 30th, a blog post raised concerns that a former maintainer continued to have access to the RubyGems.org production environment after administrative access was removed from several accounts earlier that month. We want to share the outcome of our investigation including: what happened, the extent of what we verified, what we got wrong, and the actions we have taken to strengthen our security processes going forward.&lt;/p&gt;
    &lt;p&gt;When this situation came to light, our immediate concern was the integrity and safety of the RubyGems.org service and its data. We take seriously our responsibility to steward the open-source infrastructure that millions of developers rely on each day. While we have found no evidence that user data or production operations were harmed, we recognize that the existence of an unrevoked shared credential and unclear communication created understandable alarm and frustration. For that, we are sincerely sorry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incident Response Timeline&lt;/head&gt;
    &lt;head rend="h4"&gt;September 30 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;17:23 UTC: A former maintainer, Andr√© Arko, emails the Director of Open Source at Ruby Central stating that he still has access to the RubyGems.org production environment and associated monitoring tools.&lt;lb/&gt;Note: This is the first and only disclosure to Ruby Central about this access by Mr. Arko.&lt;/item&gt;
      &lt;item&gt;17:30 UTC: Joel Drapper (unaffiliated with Ruby Central) publishes a public blog post within minutes describing this access with screenshots taken earlier that day showing root account access.&lt;/item&gt;
      &lt;item&gt;17:51 UTC: Ruby Central engages its board members and OSS staff to verify the veracity of the report, assembles an incident team, and enumerates all services and credentials to assess exposure scope and ensure complete remediation.&lt;/item&gt;
      &lt;item&gt;18:20 UTC: Ruby Central begins its emergency review and learns that the existing credentials for the AWS root account in our password vault are no longer valid.&lt;/item&gt;
      &lt;item&gt;18:24 UTC: Ruby Central initiates an AWS password-reset procedure, validates multi-factor authentication, and regains control of the AWS root account.&lt;/item&gt;
      &lt;item&gt;18:30 UTC: Ruby Central downloads a ‚ÄúCredentials Report‚Äù from the AWS console to understand why we could not access the root account, and learns that the root account password was changed by an unauthorized party on September 19th at 04:35 UTC.&lt;/item&gt;
      &lt;item&gt;20:45 UTC: After an examination of AWS CloudTrail logs, DataDog alerts, and IAM configurations, Ruby Central identifies and revokes all associated sub-accounts and legacy credentials, issues new MFA tokens to the remaining accounts, and migrates the new root access credentials into a secure vault under Ruby Central‚Äôs sole control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Analysis of Events&lt;/head&gt;
    &lt;p&gt;By way of background, Ruby Central‚Äôs infrastructure runs on Amazon Web Services (AWS). The root account credentials, essentially the highest level of administrative control, are stored in a shared enterprise password manager in a shared vault to which only three individuals had access: two current Ruby Central staff members and one former maintainer, Andr√© Arko.&lt;/p&gt;
    &lt;head rend="h4"&gt;September 18 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;18:40 UTC: Ruby Central notifies Mr. Arko, via email, of the board‚Äôs decision to remove his RubyGems.org production access, and the termination of his on-call services. During that transition, our teams remove the AWS security credentials belonging to Mr. Arko for accessing the production systems, but we fail to rotate the AWS root account password in tandem.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;September 19, 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;04:34 UTC: An unauthorized actor originating from a San Francisco, California IP address starts a root account session on the AWS Rubygems.org AWS account.&lt;/item&gt;
      &lt;item&gt;04:35 UTC: The unauthorized actor changes the root account password. &lt;lb/&gt;Note: After this point, and until the AWS root credentials were reset by Ruby Central on Sept 30th, all subsequent actions taken on the AWS root account originate from the unauthorized actor.&lt;/item&gt;
      &lt;item&gt;04:37 UTC: The unauthorized actor removes authorized users from groups and detaches access policies which reduces the privileges of authorized Rubygems.org AWS account holders.&lt;/item&gt;
      &lt;item&gt;04:39 UTC: The unauthorized actor rapidly enumerates the IAM posture of the entire AWS account.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;September 28th, 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;05:49 UTC: An unauthorized actor originating from a Tokyo, Japan IP address starts a root account session and uses IAM introspection API calls to check users‚Äô group membership, last usage date, and last usage date of associated access tokens and policies.&lt;lb/&gt;Note: This unauthorized access occurs adjacent to the Kaigi on Rails conference also in Tokyo, Japan from Sept 26th - 27th. As a result, we attribute this access to the same unauthorized actor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;September 30th, 2025&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;15:25 UTC: An unauthorized actor originating from a Los Angeles, California IP address starts a root account session.&lt;/item&gt;
      &lt;item&gt;15:35:24 UTC: The unauthorized actor issues a &lt;code&gt;PutCredentials&lt;/code&gt;command to obtain user credentials, which match the screenshot shared in the blog post announcing the security vulnerability. The blog post asserts that this action was taken by Mr. Arko.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;18:24 UTC: As we mentioned previously, Ruby Central performs the AWS password-reset operation to take back control of the root account.&lt;lb/&gt;Note: After this point, all actions taken on the AWS root account can be attributed back to authorized actors.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Extent of the Incident&lt;/head&gt;
    &lt;p&gt;After a careful review, Ruby Central is relieved to report that we see no evidence that this security incident compromised end user data, accounts, gems, or infrastructure availability. In addition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RubyGems.org remained fully operational throughout.&lt;/item&gt;
      &lt;item&gt;No personally identifiable information (PII) of RubyGems.org users nor Ruby Central financial data was accessed or transferred.&lt;/item&gt;
      &lt;item&gt;The production database, S3 buckets, and CI/CD pipeline were unaffected.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Nonetheless, the existence of unrotated credentials and the public disclosure of continued access constitute a serious procedural failure, and we are treating it as such.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Was The Incident Resolved?&lt;/head&gt;
    &lt;p&gt;After regaining control of the AWS account, Ruby Central:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Revoked all existing root and IAM credentials, created new MFA-protected access, and moved them to a restricted vault with per-user audit logs.&lt;/item&gt;
      &lt;item&gt;Rotated all related secrets and tokens, including DataDog, GitHub Actions, and other external system integrations.&lt;/item&gt;
      &lt;item&gt;Enabled AWS CloudTrail, GuardDuty, and DataDog alerting for any root login, password change, or IAM modification.&lt;/item&gt;
      &lt;item&gt;Reviewed all IAM roles and policies to ensure least-privilege access and removed legacy permissions.&lt;/item&gt;
      &lt;item&gt;Began a full end-to-end security audit with external advisors, covering infrastructure, credential storage, and incident-response procedures.&lt;/item&gt;
      &lt;item&gt;Updated the Ruby Central Security Runbook to include immediate password and key rotation upon personnel or role changes, quarterly credential reviews, and coordinated communication steps for any future incident.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Root Cause Analysis&lt;/head&gt;
    &lt;p&gt;After a post-mortem review, the root cause of the security incident was two-fold:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;While Ruby Central correctly removed access to shared credentials through its enterprise password manager prior to the incident, our staff did not consider the possibility that this credential may have been copied or exfiltrated to other password managers outside of Ruby Central‚Äôs visibility or control.&lt;/item&gt;
      &lt;item&gt;Ruby Central failed to rotate the AWS root account credentials (password and MFA) after the departure of personnel with access to the shared vault.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these events enabled the unauthorized actor to access RubyGems.org production infrastructure where they attempted unsuccessfully to lock out authorized personnel and frustrate recovery efforts.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Are Doing to Prevent Future Incidents?&lt;/head&gt;
    &lt;p&gt;RubyGems.org is a critical service that the entire Ruby community depends on, and we take that responsibility seriously. For RubyGems.org to succeed, it must not only maintain near-perfect operational uptime but also earn the community‚Äôs trust that it is operated professionally, that its operators can attest to the integrity of both the data and the code it serves to millions of Ruby applications worldwide, and that the privacy of the data we hold remains intact.&lt;/p&gt;
    &lt;p&gt;To that end, we commit to the following improvements:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Update our access revocation procedures and checklists to ensure access is also revoked to the Ruby Central enterprise password manager.&lt;/item&gt;
      &lt;item&gt;Update our access revocation procedures to ensure any non-federated credentials (particularly shared credentials) are rotated quickly after a personnel separation.&lt;/item&gt;
      &lt;item&gt;Commission an independent security audit of Ruby Central‚Äôs systems and access.&lt;/item&gt;
      &lt;item&gt;Finalize formal Operator and Contributor Agreements to clearly define who may hold production access and under what conditions&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why Did Ruby Central Treat This Event as a Security Incident?&lt;/head&gt;
    &lt;p&gt;As part of our recent actions, we determined that many RubyGems.org systems were controlled by a single individual; an untenable situation for a service of this importance.&lt;/p&gt;
    &lt;p&gt;To provide additional context to the community about our decision to formalize production access through Operator and Contributor Agreements, and to explain why we treated this incident as a genuine security event, we are sharing context from conversations between Mr. Arko and Ruby Central personnel leading up to the September 18th access changes.&lt;/p&gt;
    &lt;p&gt;In early August 2025, Ruby Central began reviewing its open source contractor budget, which totaled approximately $762,000 in 2024. On-call coverage is critical for a service like RubyGems.org and allows us to ensure operational continuity and rapid response to production incidents. Every on-call shift has a primary who is directly responsible for responding to incidents, and a secondary who is there to serve as a back up and an escalation point, if and when needed.&lt;/p&gt;
    &lt;p&gt;For RubyGems.org, the secondary on-call rotation, which serves as a backup layer, was rarely activated. Ruby Central‚Äôs long-term goal was to transition this limited paid function into a distributed network of volunteer operators who could share those responsibilities without additional cost, ensuring both operational continuity and financial sustainability.&lt;/p&gt;
    &lt;p&gt;Following these budget adjustments, Mr. Arko‚Äôs consultancy, which had been receiving approximately $50,000 per year for providing the secondary on-call service, submitted a proposal offering to provide secondary on-call services at no cost in exchange for access to production HTTP access logs, containing IP addresses and other personally identifiable information (PII). The offer would have given Mr. Arko‚Äôs consultancy access to that data, so that they could monetize it by analyzing access patterns and potentially sharing it with unrelated third-parties.&lt;/p&gt;
    &lt;p&gt;The board and leadership team determined that this proposal crossed important ethical and legal boundaries, introducing privacy, conflict-of-interest, and governance concerns inconsistent with Ruby Central‚Äôs responsibilities as stewards of the ecosystem. These concerns set in motion Ruby Central‚Äôs decision to adopt the new operating model and governance structure detailed in this blog post. With this context in mind, when we discovered that Mr. Arko had retained access to production systems containing PII, it prompted us to consider it as a security incident and to respond immediately.&lt;/p&gt;
    &lt;p&gt;Based on our preliminary investigation, as of the publication of this post, we have no evidence to indicate that any RubyGems.org data was copied or retained by unauthorized parties, including Mr. Arko.&lt;/p&gt;
    &lt;p&gt;We recognize that these events have raised valid questions within the community and tested confidence in how Ruby Central fulfills its stewardship role. Our intent in sharing this level of detail is to be transparent about what occurred, what we have learned, and what we are doing to prevent it from happening again. We are hopeful that this openness marks a meaningful step toward rebuilding trust in our stewardship and demonstrating that accountability and collaboration remain central to how we serve the Ruby ecosystem.&lt;/p&gt;
    &lt;p&gt;We are deeply grateful to the community for holding us accountable and for the patience and professionalism shown during this process. Ruby Central remains committed to transparent, responsible stewardship of the RubyGems infrastructure and to maintaining the security and trust that the Ruby ecosystem depends on.&lt;/p&gt;
    &lt;p&gt;Sincerely,&lt;/p&gt;
    &lt;p&gt;Shan Cureton&lt;lb/&gt;Executive Director&lt;/p&gt;
    &lt;p&gt;October 09, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45530832</guid><pubDate>Thu, 09 Oct 2025 17:48:23 +0000</pubDate></item><item><title>Sea Rise Simulator (2023)</title><link>https://nagix.github.io/sea-level-rise-3d-map/</link><description>&lt;doc fingerprint="9645d54f1d9acfe7"&gt;
  &lt;main&gt;
    &lt;p&gt;10m +00:00 The daily access limit has been reached. Please try again after midnight Pacific Time (PT) tomorrow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45531262</guid><pubDate>Thu, 09 Oct 2025 18:23:43 +0000</pubDate></item><item><title>2025 MacArthur Fellows</title><link>https://www.macfound.org/programs/awards/fellows/</link><description>&lt;doc fingerprint="30a7b2b47dd37a29"&gt;
  &lt;main&gt;&lt;p&gt;‚ÄúThe 2025 MacArthur Fellows expand the boundaries of knowledge, artistry, and human understanding. They focus our attention on microbial worlds and distant stars, community vitality and timeless traditions, sacred and improvisational music, and shared histories of our time on Earth. With virtuosity, persistence, and courage, they chart new paths toward collaborative, creative, and flourishing futures.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄîKristen Mack&lt;lb/&gt;Vice President, Communications, MacArthur Fellows, and Partnerships&lt;/p&gt;&lt;head rend="h2"&gt;Meet the 2025 MacArthur Fellows&lt;/head&gt;&lt;head rend="h5"&gt;Stay Informed&lt;/head&gt;&lt;head rend="h3"&gt;What is the MacArthur Fellowship?&lt;/head&gt;&lt;p&gt;The MacArthur Fellowship is an $800,000, no-strings-attached award to extraordinarily talented and creative individuals as an investment in their potential.&lt;/p&gt;Learn how fellows are selected&lt;head rend="h4"&gt;Fellows in Focus&lt;/head&gt;&lt;p&gt;Explore a collection of rich stories and salient insights from our MacArthur Fellows in Spotlight, Conversation, and Roundtable formats.&lt;/p&gt;View videos&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45531284</guid><pubDate>Thu, 09 Oct 2025 18:25:49 +0000</pubDate></item><item><title>Hacker News Live Feed</title><link>https://jerbear2008.github.io/hn-live/</link><description>&lt;doc fingerprint="e6e238c62cc54c62"&gt;
  &lt;main&gt;
    &lt;p&gt;Hacker News new | threads | past | comments | ask | show | jobs | submit | live repo This feed needs JavaScript enabled to load. [username] 0 minutes ago | parent [comment body HTML] [title] ( [domain] ) [points] by [username] 0 minutes ago | past | 0 comments&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45531367</guid><pubDate>Thu, 09 Oct 2025 18:33:17 +0000</pubDate></item><item><title>The Government Ate My Name</title><link>https://slate.com/life/2025/10/passport-name-change-united-states-mexico-spain-immigration.html</link><description>&lt;doc fingerprint="3a1d815e933184dd"&gt;
  &lt;main&gt;
    &lt;p&gt;Sign up for the Slatest to get the most insightful analysis, criticism, and advice out there, delivered to your inbox daily.&lt;/p&gt;
    &lt;p&gt;The Starbucks barista calls out ‚ÄúJoe, grande latte for Joe!‚Äù It takes him two tries before I remember I‚Äôm Joe and go pick up my coffee. A minor episode in the long history of non-Anglo immigrants changing their names after moving to America.&lt;/p&gt;
    &lt;p&gt;If your family immigrated to the United States in the 19th century and/or you took middle-school social studies in the States, you‚Äôve probably heard that officials at Ellis Island often changed newcomers‚Äô names, either because they couldn‚Äôt spell them or because they wanted to make them sound more American. In fact, authorities in New York never actually wrote down anyone‚Äôs name, they just checked each immigrant against the ship‚Äôs passenger list, which would have been compiled by employees of the steamship companies. That means that your grandpa Szyma≈Ñczyk turned into Simmons before he even set foot on the boat. My case, though, is less about forced reinvention than about bureaucratic drift. Names are bearers of our identity, history, and culture, but a lot can happen when they are run through the machinery of another culture‚Äôs bureaucracy.&lt;/p&gt;
    &lt;p&gt;I was born in Mexico City, and my parents named me Leonel Giovanni Garc√≠a Fenech. It might sound a little baroque to Americans, but having four names is standard in Spanish-speaking countries. And it can be surprisingly useful if one of your last names happens to be Garc√≠a, the most common surname in Spain and the second most common in Mexico. Or if you were my former co-worker, who shared a name with someone convicted of running over a child while drunk. That was the first thing anyone saw if they Googled her, so an extra name or two could have spared her countless awkward explanations during job interviews.&lt;/p&gt;
    &lt;p&gt;As the firstborn, I was named after both of my grandfathers: Leonel hints at my father‚Äôs Spanish Jewish ancestry; diaspora families with the name Yehuda often used variations of the translation for lion, the traditional symbol for the Tribe of Yehuda. Giovanni, on the other hand, came from my Sicilian grandfather, and is just the Italian version of John.&lt;/p&gt;
    &lt;p&gt;Same with my last names: Garc√≠a is my father‚Äôs, Fenech my mother‚Äôs. I didn‚Äôt find out until I was an adult that Fenech is not actually Italian, as my family always assumed. It‚Äôs Maltese, it means rabbit, and it‚Äôs one of Malta‚Äôs most common surnames. Furthermore, it turns out my family has been mispronouncing it all along‚Äîit‚Äôs FE-neck, not fe-NECH. Famous people named Fenech include the ‚Äô70s soft-porn actress Edwige Fenech and, more recently, Yorgen Fenech, a Maltese businessman currently facing criminal charges for corruption, money laundering, and the murder of a journalist.&lt;/p&gt;
    &lt;p&gt;In Mexico, everyone called me Giovanni, never Leonel. (I only recently learned it was because my dad couldn‚Äôt stand his own father.) When we moved to the U.S. I always introduced myself as Giovanni. I never understood why Americans were embarrassed by their middle names‚Äîexcept for that time when President Barack Obama joked that he envied Willard Mitt Romney being able to go by his middle name. The punchline being, of course, that Obama‚Äôs is Hussein.&lt;/p&gt;
    &lt;p&gt;When I was sworn in as a U.S. citizen, the clerk surprised me when he announced I could now change my name to whatever I wanted. ‚ÄúEven Ronald Reagan!‚Äù he joked. (Not quite as weird as it sounds, as Reagan was still in office.) I almost said, ‚ÄúOK, let‚Äôs do that!‚Äù but thought better of it. Caught off guard, I simply asked him to drop Leonel. Four names only seemed to confuse Americans, and I never used it anyway, so why not make life easier? That‚Äôs how I became an American named Giovanni Garcia Fenech.&lt;/p&gt;
    &lt;p&gt;But the cut didn‚Äôt solve any problems. Everyone I met still called me ‚ÄúGio‚Äù without asking. My girlfriend‚Äôs grandmother went with Geronimo. I also often received mail addressed to Giovanni Garcia French.&lt;/p&gt;
    &lt;p&gt;And of course, the bureaucrats still weren‚Äôt having it. When I got my driver‚Äôs license, the DMV insisted on cramming everything into a ‚Äúfirst, middle, last‚Äù format and turned me into Giovanni F Garcia (sans acute accent). The passport office, trying slightly harder to keep the order of my names correct, made Garcia my middle name (again, no acute accent) and Fenech my last. In typical Gen X fashion, I was apathetic about the mess. Oh well, whatever, never mind. I just started hyphenating my last name to Garcia-Fenech and left it at that. Nobody seemed to care. The bank cashed my checks.&lt;/p&gt;
    &lt;p&gt;But there were other things happening in the world. In response to 9/11, Congress had passed the REAL ID Act, requiring a new type of identification to board domestic flights (because nothing terrifies a terrorist like having to spend a day at the DMV). However, since the feds didn‚Äôt fund it, nothing happened, and I didn‚Äôt even hear about it until 2019, when the government announced that, this time for real, you‚Äôd need Real ID to fly. Like a sucker, I believed them and decided it was time to fix my documents. (It‚Äôs been postponed twice since then. The new deadline is now 2027, wink wink.)&lt;/p&gt;
    &lt;p&gt;I could have used this as an opportunity to change my driver‚Äôs license to match my passport, but convenience won out. Every important document I had‚ÄîSocial Security, bank accounts, marriage certificate, school records‚Äîlisted me as Giovanni F. Garcia, and the thought of having to change all of that made me dizzy. Thankfully, the passport office didn‚Äôt object.&lt;lb/&gt;In November 2024, my wife and I decided to move to Spain (you can guess why). We could do this because she‚Äôs German, and as an EU citizen she‚Äôs allowed to take her spouse along. As we researched our move, I discovered that Latin Americans can apply for Spanish citizenship after just two years of residency, instead of the usual 10 for others. Only catch: I needed a Mexican passport, and didn‚Äôt have one‚Äîor any Mexican documents, for that matter.&lt;/p&gt;
    &lt;p&gt;The consulate in New York happily issued me a birth certificate, but balked at a passport. They explained that my birth certificate listed me as Leonel Giovanni Garc√≠a Fenech but my American ID said my name was Giovanni F. Garcia. Never mind that they were the ones that had just given me the certificate. And so, to Spain I went, with my wife, my butchered name, and my American passport.&lt;/p&gt;
    &lt;p&gt;We‚Äôd been warned about Spanish bureaucracy, and I pictured Dickensian clerks with sleeve garters and green eyeshades demanding documents in triplicate. But what we encountered in Seville wasn‚Äôt that different from the American equivalent: bad websites, confusing instructions, long lines. They did enjoy stamping documents, but they were no better with names. My new foreigner‚Äôs ID listed me as Giovanni F Garc√≠a‚Äîacute accent restored!‚Äîbut when I opened a bank account, I discovered my first name had become ‚ÄúGiovanni F.‚Äù Not Giovanni, not Giovanni plus middle initial. Giovanni F. Still, better than ‚ÄúGio.‚Äù&lt;/p&gt;
    &lt;p&gt;Not ready to give up on an expedited European passport, I decided to visit Madrid to try my luck at the Mexican consulate there. If I couldn‚Äôt get a Mexican passport with my full name, I‚Äôd at least get to visit some world-class museums. The good news first: The museums were fantastic. As for the consulate, after a couple of hours of waiting, they called out my full name: ‚ÄúLeonel Giovanni Garc√≠a Fenech!‚Äù The official didn‚Äôt give me a chance to speak. ‚ÄúLook into here,‚Äù he said, pointing to some goggle-like device. ‚ÄúWe need to photograph your irises for biometric identification.‚Äù Oh wow, this was actually going to work. ‚ÄúNow stand there for your photograph.‚Äù I grinned. The joy didn‚Äôt last long. ‚ÄúThis is provisional, good for one year,‚Äù the official explained. ‚ÄúWe can‚Äôt give you a regular passport until you bring us documentation with your full name. See here?‚Äù He ran his finger over the name on my naturalization document. ‚ÄúIt just says Giovanni Garcia Fenech. And your U.S. passport is even worse‚Äîthey‚Äôve changed Fenech to just a single F! Have them fix it and then we can proceed.‚Äù I tried to explain that if not even Starbucks can get my name right, what were the chances the American government would? But he wasn‚Äôt interested, and there was a long line of people with dependable names waiting to get their irises scanned. Dejected, I went back to Seville with my ever-growing folder of documents under different names.&lt;/p&gt;
    &lt;p&gt;I spent a few days at home weighing my options. Could my parents find other Mexican documents with my original name? Could I legally change it back in the U.S., or would that just tangle my paperwork in Spain? How much more Kafkaesque could this get? Then something happened that made me reconsider everything. I was standing in line at the local bazar (the Spanish version of a dollar store) when an elderly woman ahead of me asked the Chinese owner her name. ‚ÄúLola,‚Äù she said. ‚ÄúLola! And how did you get that name?‚Äù the woman pressed. The owner shrugged. ‚ÄúPeople kept coming in asking for a Lola who used to work here, so eventually I just started saying that was me.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45531721</guid><pubDate>Thu, 09 Oct 2025 19:03:43 +0000</pubDate></item><item><title>US anti-fascism expert blocked from flying to Spain at airport</title><link>https://www.theguardian.com/us-news/2025/oct/09/anti-fascism-mark-bray-rutgers-university</link><description>&lt;doc fingerprint="2df5050182688095"&gt;
  &lt;main&gt;
    &lt;p&gt;A Rutgers University professor who taught a course on anti-fascism was blocked from leaving the US for Spain on Wednesday night, according to media reports, hours after Donald Trump hosted a White House roundtable highlighting the impact of antifa ‚Äì or ‚Äúanti-fascist‚Äù ‚Äì far-left activists.&lt;/p&gt;
    &lt;p&gt;Mark Bray, an historian who published the 2017 book Antifa: The Anti-Fascist Handbook, and has taught courses on anti-fascism at the New Jersey university, was attempting to board a plane at Newark airport bound for Europe when he was informed at the boarding gate that the reservations for him and his family had been cancelled.&lt;/p&gt;
    &lt;p&gt;The professor, nicknamed ‚ÄúDr Antifa‚Äù by a group of students, had said he was moving to Europe after receiving death threats. Turning Point USA activists have claimed he is a ‚Äúfinancier‚Äù for the leftwing movement.&lt;/p&gt;
    &lt;p&gt;‚Äú‚ÄòSomeone‚Äô cancelled my family‚Äôs flight out of the country at the last second,‚Äù Bray posted on Bluesky social media. ‚ÄúWe got our boarding passes. We checked our bags. Went through security. Then at our gate our reservation ‚Äòdisappeared.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;A petition calling for his removal from the university had been launched in the weeks following the assassination of the Turning Point USA founder Charlie Kirk and Bray‚Äôs home address was revealed on social media.&lt;/p&gt;
    &lt;p&gt;One threat included a vow to kill him in front of his students, according to the Washington Post. The threats led to Bray‚Äôs decision to relocate to Spain with his wife and two children and to continue to teach his students remotely.&lt;/p&gt;
    &lt;p&gt;‚ÄúSince my family and I do not feel safe in our home at the moment, we are moving for the year to Europe,‚Äù Bray said in an email to students on Sunday. ‚ÄúTruly I am so bummed about not being able to spend time with you all in the classroom.‚Äù&lt;/p&gt;
    &lt;p&gt;Bray told the New York Times earlier on Wednesday that ‚Äúmy role in this is as a professor. I‚Äôve never been part of an antifa group, and I‚Äôm not currently.‚Äù But he added that ‚Äúthere‚Äôs an effort underway to paint me as someone who is doing the things that I‚Äôve researched, but that couldn‚Äôt be further from the truth.‚Äù&lt;/p&gt;
    &lt;p&gt;Bray told the outlet that the family were rebooked for Thursday evening but were in the dark about why the earlier booking had been cancelled. ‚ÄúI may sound conspiratorial, but I don‚Äôt think it is a coincidence,‚Äù he said. ‚ÄúWe‚Äôre at a hotel and we‚Äôre just going to try again.‚Äù&lt;/p&gt;
    &lt;p&gt;After Kirk‚Äôs assassination, the rightwing influencer Jack Posobiec called Bray a ‚Äúdomestic terrorist professor‚Äù on X. The Rutgers chapter of Turning Point USA then circulated a petition that accused the professor of being an ‚Äúoutspoken, well-known antifa member‚Äù and called for his dismissal.&lt;/p&gt;
    &lt;p&gt;The Rutgers chapter of Turning Point USA has said it does not support harassment or doxing, but Bray is on a list of academics the group identifies as advancing left-leaning classroom propaganda.&lt;/p&gt;
    &lt;p&gt;‚ÄúDo you want to become a socialist? If so, make sure to pay this professor a visit!!!! All jokes aside help us report this professor who has ties to Antifa which now is designated as a domestic terrorist organization,‚Äù the Rutgers chapter posted on Instagram several days ago.&lt;/p&gt;
    &lt;p&gt;In a statement, Rutgers said it did not comment on personnel or student conduct matters.&lt;/p&gt;
    &lt;p&gt;‚ÄúRutgers University is committed to providing a secure environment ‚Äì to learn, teach, work and research ‚Äì where all members of our community can share their opinions without fear of intimidation or harassment,‚Äù it added.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45532173</guid><pubDate>Thu, 09 Oct 2025 19:44:53 +0000</pubDate></item></channel></rss>