<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 29 Oct 2025 14:11:10 +0000</lastBuildDate><item><title>Boring is what we wanted</title><link>https://512pixels.net/2025/10/boring-is-what-we-wanted/</link><description>&lt;doc fingerprint="b748792552261154"&gt;
  &lt;main&gt;
    &lt;p&gt;We are coming up on five years since the first M1 Macs shipped. It was an incredible time to be a Mac user. Those first Apple silicon Macs looked like the Intel machines they replaced, but they were better in every single way.&lt;/p&gt;
    &lt;p&gt;In December 2020, John Gruber wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We knew this to be true: Computers could run fast and hot, or slow and cool. For laptops in particular, the best you could hope for is a middle ground: fast enough and cool enough. But if you wanted a machine that ran really fast, it wasn‚Äôt going to run cool (and wasn‚Äôt going to last long on battery), and if you wanted a computer that ran cool (and lasted long on battery), it wasn‚Äôt going to be fast.&lt;/p&gt;
      &lt;p&gt;We knew this to be true because that was the way things were. But now, with the M1 Macs, it‚Äôs not. M1 Macs run very fast and do so while remaining very cool and lasting mind-bogglingly long on battery. It was a fundamental trade-off inherent to PC computing, and now we don‚Äôt have to make it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite its Touch Bar, I immediately bought that first M1 MacBook Pro, and when the 14-inch MacBook Pro came out a year later, I moved to it.1 I‚Äôm typing these very words on my 14-inch MacBook Pro with an M4 Max inside. Each of these machines was faster than the one before it, outperforming my old iMac Pro and Mac Pro in new ways with every upgrade.&lt;/p&gt;
    &lt;p&gt;Apple silicon has been nothing but upside for the Mac, and yet some seem bored already. In the days since Apple announced the M5, I‚Äôve seen and heard this sentiment more than I expected:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is just another boring incremental upgrade.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That üëè is üëè the üëè point.&lt;/p&gt;
    &lt;p&gt;Back in the PowerPC and Intel days, Macs would sometimes go years between meaningful spec bumps, as Apple waited on its partners to deliver appropriate hardware for various machines. From failing NVIDIA cards in MacBook Pros to 27-inch Intel iMacs that ran so hot the fans were audible at all times, Mac hardware wasn‚Äôt always what Apple wanted.&lt;/p&gt;
    &lt;p&gt;Of course, some of the issues with previous generations of Mac were Apple‚Äôs fault ‚Äî look no further than the butterfly keyboard or the years the Mac Pro spent in the wilderness. Apple will make questionable decisions in the future, just as it has in the past.&lt;/p&gt;
    &lt;p&gt;The difference is that with Apple silicon, Apple owns and controls the primary technologies behind the products it makes, as Tim Cook has always wanted. It means that it can ship updates to its SoCs on a regular cadence, making progress in terms of both power and efficiency each time.&lt;/p&gt;
    &lt;p&gt;A predictable update schedule means that incremental updates are inevitable. Revolution then evolution is not a bad thing; it‚Äôs okay that not every release is exciting or groundbreaking. It‚Äôs how technology has worked for decades.&lt;/p&gt;
    &lt;p&gt;‚Ä¶but some people have short memories. Before the Apple silicon introduction, we all wanted steady, predictable progress in Mac hardware development. We wanted each product in the lineup to be updated regularly and not wither on the vine for years. For the most part, Apple has delivered. Just look at this chart of the progress Apple has made since the M1:&lt;/p&gt;
    &lt;p&gt;I don‚Äôt see anything in those charts to complain about, especially given the frequency at which most people buy new computers. That‚Äôs one reason why Apple compared the M5 to the M1 in its press release announcing the new chip. Unless you buy a new computer every year, every update you experience will be meaningful.&lt;/p&gt;
    &lt;p&gt;That‚Äôs what we wanted when Apple announced the move away from Intel, and calling it boring five years in is missing the point and downplaying the success of Apple silicon thus far.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My review of the M1 Pro 14-inch MacBook Pro remains one of my favorite blog posts I‚Äôve written. ‚Ü©&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45738247</guid><pubDate>Tue, 28 Oct 2025 19:57:16 +0000</pubDate></item><item><title>Generative AI Image Editing Showdown</title><link>https://genai-showdown.specr.net/image-editing</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45739080</guid><pubDate>Tue, 28 Oct 2025 20:58:22 +0000</pubDate></item><item><title>Tinkering is a way to acquire good taste</title><link>https://seated.ro/blog/tinkering-a-lost-art</link><description>&lt;doc fingerprint="7426e95752a204d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;If you don't tinker, you don't have taste&lt;/head&gt;
    &lt;head rend="h2"&gt;tin√Ç¬∑ker&lt;/head&gt;
    &lt;head rend="h4"&gt;/√ãtiNGk√âr/&lt;/head&gt;
    &lt;head rend="h4"&gt;to make small changes to something, especially in an attempt to repair or improve it.&lt;/head&gt;
    &lt;head rend="h1"&gt;In Hindsight&lt;/head&gt;
    &lt;p&gt;Growing up, I never stuck to a single thing, be it guitar lessons, art school, martial arts √¢ I tried them all. when it came to programming, though, I never really tinkered. I was always amazed with video games and wondered how they were made but I never pursued that curiosity.&lt;/p&gt;
    &lt;p&gt;My tinkering habits picked up very late, and now I cannot go by without picking up new things in one form or another. It√¢s how I learn. I wish I did it sooner. It√¢s a major part of my learning process now, and I would never be the &lt;del&gt;programmer&lt;/del&gt; person I am today.&lt;/p&gt;
    &lt;head rend="h1"&gt;What the hell is tinkering?&lt;/head&gt;
    &lt;p&gt;Have you ever spent hours tweaking the mouse sensitivity in your favorite FPS game?&lt;/p&gt;
    &lt;p&gt;Have you ever installed a Linux distro, spent days configuring window managers, not because you had to, but purely because it gave you satisfaction and made your workflow exactly yours?&lt;/p&gt;
    &lt;p&gt;Ever pulled apart your mechanical keyboard, swapped keycaps, tested switches, and lubed stabilizers just for more thock?&lt;/p&gt;
    &lt;p&gt;That is what I mean.&lt;/p&gt;
    &lt;p&gt;I have come to understand that there are two kinds of people, those who do things only if it helps them achieve a goal, and those who do things just because. The ideal, of course, is to be a mix of both.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;when you tinker and throw away, that√¢s practice, and practice should inherently be ephemeral, exploratory, and be frequent - @ludwigABAP&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;My approach to tinkering&lt;/head&gt;
    &lt;p&gt;There are plenty of people who still use the VSCode terminal as their default terminal, do not know what vim bindings are, GitHub desktop rather than the cli (at the very least). I√¢m not saying these are bad things necessarily, just that this should be the minimum, not the median.&lt;/p&gt;
    &lt;p&gt;This does not mean I spend every waking hour fiddling with my neovim config. In fact, the last meaningful change to my config was 6 months ago. Finding that balance is where most people fail.&lt;/p&gt;
    &lt;p&gt;Over the years I have done so many things that in hindsight have made me appreciate programming more but were completely √¢unnecessary√¢ in the strict sense.&lt;/p&gt;
    &lt;p&gt;In the past week I have, for the first time, written a glsl fragment shader, a rust procedural macro, template c++, a swift app, furthered my hatred for windows development (this is not new), and started using the helix editor more (mainly for good defaults + speed). I didn√¢t have to do these things, but I did, for fun! And I know more about these things now.&lt;/p&gt;
    &lt;p&gt;No time spent learning, is time wasted.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why taste matters, especially now&lt;/head&gt;
    &lt;p&gt;Acquiring good taste comes through using various things, discarding the ones you don√¢t like and keeping the ones you do. if you never try various things, you will not acquire good taste.&lt;/p&gt;
    &lt;p&gt;And what I mean by taste here is simply the honed ability to distinguish mediocrity from excellence. This will be highly subjective, and not everyone√¢s taste will be the same, but that is the point, you should NOT have the same taste as someone else.&lt;/p&gt;
    &lt;p&gt;Question the status quo, experiment, break things, do this several times, do this everyday and keep doing it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45739499</guid><pubDate>Tue, 28 Oct 2025 21:31:50 +0000</pubDate></item><item><title>Keeping the Internet fast and secure: introducing Merkle Tree Certificates</title><link>https://blog.cloudflare.com/bootstrap-mtc/</link><description>&lt;doc fingerprint="6f39937a141a36d0"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The world is in a race to build its first quantum computer capable of solving practical problems not feasible on even the largest conventional supercomputers. While the quantum computing paradigm promises many benefits, it also threatens the security of the Internet by breaking much of the cryptography we have come to rely on.&lt;/p&gt;
      &lt;p&gt;To mitigate this threat, Cloudflare is helping to migrate the Internet to Post-Quantum (PQ) cryptography. Today, about 50% of traffic to Cloudflare's edge network is protected against the most urgent threat: an attacker who can intercept and store encrypted traffic today and then decrypt it in the future with the help of a quantum computer. This is referred to as the harvest now, decrypt later threat.&lt;/p&gt;
      &lt;p&gt;However, this is just one of the threats we need to address. A quantum computer can also be used to crack a server's TLS certificate, allowing an attacker to impersonate the server to unsuspecting clients. The good news is that we already have PQ algorithms we can use for quantum-safe authentication. The bad news is that adoption of these algorithms in TLS will require significant changes to one of the most complex and security-critical systems on the Internet: the Web Public-Key Infrastructure (WebPKI).&lt;/p&gt;
      &lt;p&gt;The central problem is the sheer size of these new algorithms: signatures for ML-DSA-44, one of the most performant PQ algorithms standardized by NIST, are 2,420 bytes long, compared to just 64 bytes for ECDSA-P256, the most popular non-PQ signature in use today; and its public keys are 1,312 bytes long, compared to just 64 bytes for ECDSA. That's a roughly 20-fold increase in size. Worse yet, the average TLS handshake includes a number of public keys and signatures, adding up to 10s of kilobytes of overhead per handshake. This is enough to have a noticeable impact on the performance of TLS.&lt;/p&gt;
      &lt;p&gt;That makes drop-in PQ certificates a tough sell to enable today: they don√¢t bring any security benefit before Q-day √¢ the day a cryptographically relevant quantum computer arrives √¢ but they do degrade performance. We could sit and wait until Q-day is a year away, but that√¢s playing with fire. Migrations always take longer than expected, and by waiting we risk the security and privacy of the Internet, which is dear to us.&lt;/p&gt;
      &lt;p&gt;It's clear that we must find a way to make post-quantum certificates cheap enough to deploy today by default for everyone √¢ not just those that can afford it. In this post, we'll introduce you to the plan we√¢ve brought together with industry partners to the IETF to redesign the WebPKI in order to allow a smooth transition to PQ authentication with no performance impact (and perhaps a performance improvement!). We'll provide an overview of one concrete proposal, called Merkle Tree Certificates (MTCs), whose goal is to whittle down the number of public keys and signatures in the TLS handshake to the bare minimum required.&lt;/p&gt;
      &lt;p&gt;But talk is cheap. We know from experience that, as with any change to the Internet, it's crucial to test early and often. Today we're announcing our intent to deploy MTCs on an experimental basis in collaboration with Chrome Security. In this post, we'll describe the scope of this experiment, what we hope to learn from it, and how we'll make sure it's done safely.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;The WebPKI today √¢ an old system with many patches&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Why does the TLS handshake have so many public keys and signatures?&lt;/p&gt;
      &lt;p&gt;Let's start with Cryptography 101. When your browser connects to a website, it asks the server to authenticate itself to make sure it's talking to the real server and not an impersonator. This is usually achieved with a cryptographic primitive known as a digital signature scheme (e.g., ECDSA or ML-DSA). In TLS, the server signs the messages exchanged between the client and server using its secret key, and the client verifies the signature using the server's public key. In this way, the server confirms to the client that they've had the same conversation, since only the server could have produced a valid signature.&lt;/p&gt;
      &lt;p&gt;If the client already knows the server's public key, then only 1 signature is required to authenticate the server. In practice, however, this is not really an option. The web today is made up of around a billion TLS servers, so it would be unrealistic to provision every client with the public key of every server. What's more, the set of public keys will change over time as new servers come online and existing ones rotate their keys, so we would need some way of pushing these changes to clients.&lt;/p&gt;
      &lt;p&gt;This scaling problem is at the heart of the design of all PKIs.&lt;/p&gt;
      &lt;p&gt;Instead of expecting the client to know the server's public key in advance, the server might just send its public key during the TLS handshake. But how does the client know that the public key actually belongs to the server? This is the job of a certificate.&lt;/p&gt;
      &lt;p&gt;A certificate binds a public key to the identity of the server √¢ usually its DNS name, e.g., &lt;code&gt;cloudflareresearch.com&lt;/code&gt;. The certificate is signed by a Certification Authority (CA) whose public key is known to the client. In addition to verifying the server's handshake signature, the client verifies the signature of this certificate. This establishes a chain of trust: by accepting the certificate, the client is trusting that the CA verified that the public key actually belongs to the server with that identity.&lt;/p&gt;
      &lt;p&gt;Clients are typically configured to trust many CAs and must be provisioned with a public key for each. Things are much easier however, since there are only 100s of CAs instead of billions. In addition, new certificates can be created without having to update clients.&lt;/p&gt;
      &lt;p&gt;These efficiencies come at a relatively low cost: for those counting at home, that's +1 signature and +1 public key, for a total of 2 signatures and 1 public key per TLS handshake.&lt;/p&gt;
      &lt;p&gt;That's not the end of the story, however. As the WebPKI has evolved, so have these chains of trust grown a bit longer. These days it's common for a chain to consist of two or more certificates rather than just one. This is because CAs sometimes need to rotate their keys, just as servers do. But before they can start using the new key, they must distribute the corresponding public key to clients. This takes time, since it requires billions of clients to update their trust stores. To bridge the gap, the CA will sometimes use the old key to issue a certificate for the new one and append this certificate to the end of the chain.&lt;/p&gt;
      &lt;p&gt;That's +1 signature and +1 public key, which brings us to 3 signatures and 2 public keys. And we still have a little ways to go.&lt;/p&gt;
      &lt;p&gt;The main job of a CA is to verify that a server has control over the domain for which it√¢s requesting a certificate. This process has evolved over the years from a high-touch, CA-specific process to a standardized, mostly automated process used for issuing most certificates on the web. (Not all CAs fully support automation, however.) This evolution is marked by a number of security incidents in which a certificate was mis-issued to a party other than the server, allowing that party to impersonate the server to any client that trusts the CA.&lt;/p&gt;
      &lt;p&gt;Automation helps, but attacks are still possible, and mistakes are almost inevitable. Earlier this year, several certificates for Cloudflare's encrypted 1.1.1.1 resolver were issued without our involvement or authorization. This apparently occurred by accident, but it nonetheless put users of 1.1.1.1 at risk. (The mis-issued certificates have since been revoked.)&lt;/p&gt;
      &lt;p&gt;Ensuring mis-issuance is detectable is the job of the Certificate Transparency (CT) ecosystem. The basic idea is that each certificate issued by a CA gets added to a public log. Servers can audit these logs for certificates issued in their name. If ever a certificate is issued that they didn't request itself, the server operator can prove the issuance happened, and the PKI ecosystem can take action to prevent the certificate from being trusted by clients.&lt;/p&gt;
      &lt;p&gt;Major browsers, including Firefox and Chrome and its derivatives, require certificates to be logged before they can be trusted. For example, Chrome, Safari, and Firefox will only accept the server's certificate if it appears in at least two logs the browser is configured to trust. This policy is easy to state, but tricky to implement in practice:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Operating a CT log has historically been fairly expensive. Logs ingest billions of certificates over their lifetimes: when an incident happens, or even just under high load, it can take some time for a log to make a new entry available for auditors.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Clients can't really audit logs themselves, since this would expose their browsing history (i.e., the servers they wanted to connect to) to the log operators.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The solution to both problems is to include a signature from the CT log along with the certificate. The signature is produced immediately in response to a request to log a certificate, and attests to the log's intent to include the certificate in the log within 24 hours.&lt;/p&gt;
      &lt;p&gt;Per browser policy, certificate transparency adds +2 signatures to the TLS handshake, one for each log. This brings us to a total of 5 signatures and 2 public keys in a typical handshake on the public web.&lt;/p&gt;
      &lt;p&gt;The WebPKI is a living, breathing, and highly distributed system. We've had to patch it a number of times over the years to keep it going, but on balance it has served our needs quite well √¢ until now.&lt;/p&gt;
      &lt;p&gt;Previously, whenever we needed to update something in the WebPKI, we would tack on another signature. This strategy has worked because conventional cryptography is so cheap. But 5 signatures and 2 public keys on average for each TLS handshake is simply too much to cope with for the larger PQ signatures that are coming.&lt;/p&gt;
      &lt;p&gt;The good news is that by moving what we already have around in clever ways, we can drastically reduce the number of signatures we need.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Crash course on Merkle Tree Certificates&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Merkle Tree Certificates (MTCs) is a proposal for the next generation of the WebPKI that we are implementing and plan to deploy on an experimental basis. Its key features are as follows:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;All the information a client needs to validate a Merkle Tree Certificate can be disseminated out-of-band. If the client is sufficiently up-to-date, then the TLS handshake needs just 1 signature, 1 public key, and 1 Merkle tree inclusion proof. This is quite small, even if we use post-quantum algorithms.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The MTC specification makes certificate transparency a first class feature of the PKI by having each CA run its own log of exactly the certificates they issue.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Let's poke our head under the hood a little. Below we have an MTC generated by one of our internal tests. This would be transmitted from the server to the client in the TLS handshake:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;-----BEGIN CERTIFICATE-----
MIICSzCCAUGgAwIBAgICAhMwDAYKKwYBBAGC2ksvADAcMRowGAYKKwYBBAGC2ksv
AQwKNDQzNjMuNDguMzAeFw0yNTEwMjExNTMzMjZaFw0yNTEwMjgxNTMzMjZaMCEx
HzAdBgNVBAMTFmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wWTATBgcqhkjOPQIBBggq
hkjOPQMBBwNCAARw7eGWh7Qi7/vcqc2cXO8enqsbbdcRdHt2yDyhX5Q3RZnYgONc
JE8oRrW/hGDY/OuCWsROM5DHszZRDJJtv4gno2wwajAOBgNVHQ8BAf8EBAMCB4Aw
EwYDVR0lBAwwCgYIKwYBBQUHAwEwQwYDVR0RBDwwOoIWY2xvdWRmbGFyZXJlc2Vh
cmNoLmNvbYIgc3RhdGljLWN0LmNsb3VkZmxhcmVyZXNlYXJjaC5jb20wDAYKKwYB
BAGC2ksvAAOB9QAAAAAAAAACAAAAAAAAAAJYAOBEvgOlvWq38p45d0wWTPgG5eFV
wJMhxnmDPN1b5leJwHWzTOx1igtToMocBwwakt3HfKIjXYMO5CNDOK9DIKhmRDSV
h+or8A8WUrvqZ2ceiTZPkNQFVYlG8be2aITTVzGuK8N5MYaFnSTtzyWkXP2P9nYU
Vd1nLt/WjCUNUkjI4/75fOalMFKltcc6iaXB9ktble9wuJH8YQ9tFt456aBZSSs0
cXwqFtrHr973AZQQxGLR9QCHveii9N87NXknDvzMQ+dgWt/fBujTfuuzv3slQw80
mibA021dDCi8h1hYFQAA
-----END CERTIFICATE-----&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Looks like your average PEM encoded certificate. Let's decode it and look at the parameters:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;$ openssl x509 -in merkle-tree-cert.pem -noout -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number: 531 (0x213)
        Signature Algorithm: 1.3.6.1.4.1.44363.47.0
        Issuer: 1.3.6.1.4.1.44363.47.1=44363.48.3
        Validity
            Not Before: Oct 21 15:33:26 2025 GMT
            Not After : Oct 28 15:33:26 2025 GMT
        Subject: CN=cloudflareresearch.com
        Subject Public Key Info:
            Public Key Algorithm: id-ecPublicKey
                Public-Key: (256 bit)
                pub:
                    04:70:ed:e1:96:87:b4:22:ef:fb:dc:a9:cd:9c:5c:
                    ef:1e:9e:ab:1b:6d:d7:11:74:7b:76:c8:3c:a1:5f:
                    94:37:45:99:d8:80:e3:5c:24:4f:28:46:b5:bf:84:
                    60:d8:fc:eb:82:5a:c4:4e:33:90:c7:b3:36:51:0c:
                    92:6d:bf:88:27
                ASN1 OID: prime256v1
                NIST CURVE: P-256
        X509v3 extensions:
            X509v3 Key Usage: critical
                Digital Signature
            X509v3 Extended Key Usage:
                TLS Web Server Authentication
            X509v3 Subject Alternative Name:
                DNS:cloudflareresearch.com, DNS:static-ct.cloudflareresearch.com
    Signature Algorithm: 1.3.6.1.4.1.44363.47.0
    Signature Value:
        00:00:00:00:00:00:02:00:00:00:00:00:00:00:02:58:00:e0:
        44:be:03:a5:bd:6a:b7:f2:9e:39:77:4c:16:4c:f8:06:e5:e1:
        55:c0:93:21:c6:79:83:3c:dd:5b:e6:57:89:c0:75:b3:4c:ec:
        75:8a:0b:53:a0:ca:1c:07:0c:1a:92:dd:c7:7c:a2:23:5d:83:
        0e:e4:23:43:38:af:43:20:a8:66:44:34:95:87:ea:2b:f0:0f:
        16:52:bb:ea:67:67:1e:89:36:4f:90:d4:05:55:89:46:f1:b7:
        b6:68:84:d3:57:31:ae:2b:c3:79:31:86:85:9d:24:ed:cf:25:
        a4:5c:fd:8f:f6:76:14:55:dd:67:2e:df:d6:8c:25:0d:52:48:
        c8:e3:fe:f9:7c:e6:a5:30:52:a5:b5:c7:3a:89:a5:c1:f6:4b:
        5b:95:ef:70:b8:91:fc:61:0f:6d:16:de:39:e9:a0:59:49:2b:
        34:71:7c:2a:16:da:c7:af:de:f7:01:94:10:c4:62:d1:f5:00:
        87:bd:e8:a2:f4:df:3b:35:79:27:0e:fc:cc:43:e7:60:5a:df:
        df:06:e8:d3:7e:eb:b3:bf:7b:25:43:0f:34:9a:26:c0:d3:6d:
        5d:0c:28:bc:87:58:58:15:00:00&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;While some of the parameters probably look familiar, others will look unusual. On the familiar side, the subject and public key are exactly what we might expect: the DNS name is &lt;code&gt;cloudflareresearch.com&lt;/code&gt; and the public key is for a familiar signature algorithm, ECDSA-P256. This algorithm is not PQ, of course √¢ in the future we would put ML-DSA-44 there instead.&lt;/p&gt;
      &lt;p&gt;On the unusual side, OpenSSL appears to not recognize the signature algorithm of the issuer and just prints the raw OID and bytes of the signature. There's a good reason for this: the MTC does not have a signature in it at all! So what exactly are we looking at?&lt;/p&gt;
      &lt;p&gt;The trick to leave out signatures is that a Merkle Tree Certification Authority (MTCA) produces its signatureless certificates in batches rather than individually. In place of a signature, the certificate has an inclusion proof of the certificate in a batch of certificates signed by the MTCA.&lt;/p&gt;
      &lt;p&gt;To understand how inclusion proofs work, let's think about a slightly simplified version of the MTC specification. To issue a batch, the MTCA arranges the unsigned certificates into a data structure called a Merkle tree that looks like this:&lt;/p&gt;
      &lt;p&gt;Each leaf of the tree corresponds to a certificate, and each inner node is equal to the hash of its children. To sign the batch, the MTCA uses its secret key to sign the head of the tree. The structure of the tree guarantees that each certificate in the batch was signed by the MTCA: if we tried to tweak the bits of any one of the certificates, the treehead would end up having a different value, which would cause the signature to fail.&lt;/p&gt;
      &lt;p&gt;An inclusion proof for a certificate consists of the hash of each sibling node along the path from the certificate to the treehead:&lt;/p&gt;
      &lt;p&gt;Given a validated treehead, this sequence of hashes is sufficient to prove inclusion of the certificate in the tree. This means that, in order to validate an MTC, the client also needs to obtain the signed treehead from the MTCA.&lt;/p&gt;
      &lt;p&gt;This is the key to MTC's efficiency:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Signed treeheads can be disseminated to clients out-of-band and validated offline. Each validated treehead can then be used to validate any certificate in the corresponding batch, eliminating the need to obtain a signature for each server certificate.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;During the TLS handshake, the client tells the server which treeheads it has. If the server has a signatureless certificate covered by one of those treeheads, then it can use that certificate to authenticate itself. That's 1 signature,1 public key and 1 inclusion proof per handshake, both for the server being authenticated.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Now, that's the simplified version. MTC proper has some more bells and whistles. To start, it doesn√¢t create a separate Merkle tree for each batch, but it grows a single large tree, which is used for better transparency. As this tree grows, periodically (sub)tree heads are selected to be shipped to browsers, which we call landmarks. In the common case browsers will be able to fetch the most recent landmarks, and servers can wait for batch issuance, but we need a fallback: MTC also supports certificates that can be issued immediately and don√¢t require landmarks to be validated, but these are not as small. A server would provision both types of Merkle tree certificates, so that the common case is fast, and the exceptional case is slow, but at least it√¢ll work.&lt;/p&gt;
      &lt;p&gt;Ever since early designs for MTCs emerged, we√¢ve been eager to experiment with the idea. In line with the IETF principle of √¢running code√¢, it often takes implementing a protocol to work out kinks in the design. At the same time, we cannot risk the security of users. In this section, we describe our approach to experimenting with aspects of the Merkle Tree Certificates design without changing any trust relationships.&lt;/p&gt;
      &lt;p&gt;Let√¢s start with what we hope to learn. We have lots of questions whose answers can help to either validate the approach, or uncover pitfalls that require reshaping the protocol √¢ in fact, an implementation of an early MTC draft by Maximilian Pohl and Mia Celeste did exactly this. We√¢d like to know:&lt;/p&gt;
      &lt;p&gt;What breaks? Protocol ossification (the tendency of implementation bugs to make it harder to change a protocol) is an ever-present issue with deploying protocol changes. For TLS in particular, despite having built-in flexibility, time after time we√¢ve found that if that flexibility is not regularly used, there will be buggy implementations and middleboxes that break when they see things they don√¢t recognize. TLS 1.3 deployment took years longer than we hoped for this very reason. And more recently, the rollout of PQ key exchange in TLS caused the Client Hello to be split over multiple TCP packets, something that many middleboxes weren't ready for.&lt;/p&gt;
      &lt;p&gt;What is the performance impact? In fact, we expect MTCs to reduce the size of the handshake, even compared to today's non-PQ certificates. They will also reduce CPU cost: ML-DSA signature verification is about as fast as ECDSA, and there will be far fewer signatures to verify. We therefore expect to see a reduction in latency. We would like to see if there is a measurable performance improvement.&lt;/p&gt;
      &lt;p&gt;What fraction of clients will stay up to date? Getting the performance benefit of MTCs requires the clients and servers to be roughly in sync with one another. We expect MTCs to have fairly short lifetimes, a week or so. This means that if the client's latest landmark is older than a week, the server would have to fallback to a larger certificate. Knowing how often this fallback happens will help us tune the parameters of the protocol to make fallbacks less likely.&lt;/p&gt;
      &lt;p&gt;In order to answer these questions, we are implementing MTC support in our TLS stack and in our certificate issuance infrastructure. For their part, Chrome is implementing MTC support in their own TLS stack and will stand up infrastructure to disseminate landmarks to their users.&lt;/p&gt;
      &lt;p&gt;As we've done in past experiments, we plan to enable MTCs for a subset of our free customers with enough traffic that we will be able to get useful measurements. Chrome will control the experimental rollout: they can ramp up slowly, measuring as they go and rolling back if and when bugs are found.&lt;/p&gt;
      &lt;p&gt;Which leaves us with one last question: who will run the Merkle Tree CA?&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Bootstrapping trust from the existing WebPKI&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Standing up a proper CA is no small task: it takes years to be trusted by major browsers. That√¢s why Cloudflare isn√¢t going to become a √¢real√¢ CA for this experiment, and Chrome isn√¢t going to trust us directly.&lt;/p&gt;
      &lt;p&gt;Instead, to make progress on a reasonable timeframe, without sacrificing due diligence, we plan to "mock" the role of the MTCA. We will run an MTCA (on Workers based on our StaticCT logs), but for each MTC we issue, we also publish an existing certificate from a trusted CA that agrees with it. We call this the bootstrap certificate. When Chrome√¢s infrastructure pulls updates from our MTCA log, they will also pull these bootstrap certificates, and check whether they agree. Only if they do, they√¢ll proceed to push the corresponding landmarks to Chrome clients. In other words, Cloudflare is effectively just √¢re-encoding√¢ an existing certificate (with domain validation performed by a trusted CA) as an MTC, and Chrome is using certificate transparency to keep us honest.&lt;/p&gt;
      &lt;p&gt;With almost 50% of our traffic already protected by post-quantum encryption, we√¢re halfway to a fully post-quantum secure Internet. The second part of our journey, post-quantum certificates, is the hardest yet though. A simple drop-in upgrade has a noticeable performance impact and no security benefit before Q-day. This means it√¢s a hard sell to enable today by default. But here we are playing with fire: migrations always take longer than expected. If we want to keep an ubiquitously private and secure Internet, we need a post-quantum solution that√¢s performant enough to be enabled by default today.&lt;/p&gt;
      &lt;p&gt;Merkle Tree Certificates (MTCs) solves this problem by reducing the number of signatures and public keys to the bare minimum while maintaining the WebPKI's essential properties. We plan to roll out MTCs to a fraction of free accounts by early next year. This does not affect any visitors that are not part of the Chrome experiment. For those that are, thanks to the bootstrap certificates, there is no impact on security.&lt;/p&gt;
      &lt;p&gt;We√¢re excited to keep the Internet fast and secure, and will report back soon on the results of this experiment: watch this space! MTC is evolving as we speak, if you want to get involved, please join the IETF PLANTS mailing list.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45740214</guid><pubDate>Tue, 28 Oct 2025 22:39:24 +0000</pubDate></item><item><title>Tips for stroke-surviving software engineers</title><link>https://blog.j11y.io/2025-10-29_stroke_tips_for_engineers/</link><description>&lt;doc fingerprint="f62a9b5785b1fadc"&gt;
  &lt;main&gt;
    &lt;p&gt;2025-10-29&lt;/p&gt;
    &lt;p&gt;This is a pretty niche topic; I don't imagine there are many of us out there.&lt;/p&gt;
    &lt;p&gt;Actually, to be strict, I'd say this advice is tailored to people who've had hemorrhagic stroke in the parietal lobe with residual epilepsy...&lt;/p&gt;
    &lt;p&gt;I was 29 and around 12 years into my career when it all happened, and in the six years since then I've had time to learn a bit more about my new self.&lt;/p&gt;
    &lt;p&gt;The first tip is to just stop. Fatigue, fuzziness, nausea, or affected-sided weird sensations are non-negotiable stop signals. So go lie down, hydrate, reset. Close your eyes and think about the cottage or lonely mountain you want to retire to. Escape the overwhelming mental or physical space.&lt;/p&gt;
    &lt;p&gt;HEADPHONES, blinders, and 'No'. Eliminate unwanted inputs at the earliest point of entry. Work from home or environments where you can control most variables. Routes of escape and rest are important.&lt;/p&gt;
    &lt;p&gt;Health above performance every single time. Metrics and productivity be damned. Self-advocate, and all that. Reject with directness any demands made of you that cross the threshold.&lt;/p&gt;
    &lt;p&gt;Laws. Use them. You don't have to rely on good behaviour and kindness. You are, depending on your location, usually protected by all types of anti-discrimination legislation, implicit and explicit. Use your employee assistance programs too.&lt;/p&gt;
    &lt;p&gt;Single-thread it all! Less context switching. Batch your work, finish one thing, then move to the next. Externalize working-memory. Use notebooks, whiteboards, and lists instead of juggling state in your head. I am not good at this, and over-stretch my brain, leading to auras, overwhelm, and general sickness. Terrible idea.&lt;/p&gt;
    &lt;p&gt;Related: Sssh to the AI naysayers. Use it as your help and scratchpad. Let it hold state so your brain can judge rather than store and needlessly cogitate on stuff. You don't have to do this alone out of some purity fetishism. You, too, have a limited context window. Sorry!&lt;/p&gt;
    &lt;p&gt;Do the heavy thinking in your peak window (for me, that's the morning); push everything else to later. Spend your time more carefully than your money.&lt;/p&gt;
    &lt;p&gt;Pick the route of least attention. Attention is expensive, and rarely needed as much as we think it is. It's a heavy toll to pay. Unless you're in an ops or monitoring role, you don't need to be synchronously active. DISABLE NOTIFICATIONS.&lt;/p&gt;
    &lt;p&gt;AVOID long meetings. Emails are good. Oh god am I bad at this? YES, I like people so I like some meetings, but communicating is so so expensive. Being polite is also expensive; It's not nice to have to tell people they're draining you.&lt;/p&gt;
    &lt;p&gt;I think that's mostly it. I'm still working on this stuff. And would probably grade myself pretty poorly. One day I'll be better at saying no, at advocating for myself, and knowing how to navigate the disappointment of others.&lt;/p&gt;
    &lt;p&gt;Footnote &amp;amp; some casual research: If you're into this, here's some stuff I found out related to my specific injury location and how it might apply to my work. This was gathered with help from gemini when I was struggling with left-arm and eye prodromes after long coding sessions:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Frontal and parietal cortices form a flexible control system that holds goals, routes attention, and updates task sets; this "multiple-demand" network scales with task complexity and underpins how we store, manipulate, and decide on information during work[1][2][3]. Superior parietal cortex is especially taxed when we transform or reorganize information in working memory rather than simply maintain it, which is why mental navigations, refactors, and other transformations feel costly[4][5]. Frequent context switches recruit lateral prefrontal and parietal regions and increase control load, so hopping between threads repeatedly spikes demand on this same circuitry[6][7]. After AVM resection (what I had!) or stroke generally, tissue near the lesion can remain hyperexcitable with impaired neurovascular coupling; heavy cognitive load lowers seizure threshold and can produce somatosensory auras and body-image distortions from parietal cortex[8][9][10].&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Thanks for reading :) Tonnes of love to all the stroke survivors out there &amp;lt;3&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742419</guid><pubDate>Wed, 29 Oct 2025 03:51:56 +0000</pubDate></item><item><title>uBlock Origin Lite Apple App Store</title><link>https://apps.apple.com/in/app/ublock-origin-lite/id6745342698</link><description>&lt;doc fingerprint="b62ad8dff7b5baa7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uBlock Origin Lite 4+&lt;/head&gt;
    &lt;head rend="h2"&gt;An efficient content blocker&lt;/head&gt;
    &lt;head rend="h2"&gt;Raymond Hill&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Free&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Screenshots&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;uBO Lite (uBOL) is a reliable and efficient content blocker.&lt;lb/&gt;The default ruleset corresponds to uBlock Origin's default filterset:&lt;lb/&gt;- uBlock Origin's built-in filter lists&lt;lb/&gt;- EasyList&lt;lb/&gt;- EasyPrivacy&lt;lb/&gt;- Peter Lowe√¢s Ad and tracking server list&lt;lb/&gt;You can enable more rulesets by visiting the options page -- click the _Cogs_ icon in the popup panel.&lt;lb/&gt;uBOL is entirely declarative, meaning there is no need for a permanent uBOL process for the filtering to occur, and CSS/JS injection-based content filtering is performed reliably by the browser itself rather than by the extension. This means that uBOL itself does not consume CPU/memory resources while content blocking is ongoing -- uBOL's service worker process is required _only_ when you interact with the popup panel or the option pages.&lt;/p&gt;
    &lt;head rend="h2"&gt;What√¢s New&lt;/head&gt;
    &lt;p&gt;Version 2025.1019.1656&lt;/p&gt;
    &lt;p&gt;√¢¬¢ Automatically select optimal for newly allowed hosts&lt;lb/&gt;√¢¬¢ Updated filter lists&lt;/p&gt;
    &lt;head rend="h2"&gt;Ratings and Reviews&lt;/head&gt;
    &lt;head rend="h3"&gt;The best content blocker is finally on iPadOS!!&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;It√¢s was a really long wait, but finally we are able to use it directly on the iPad. The first TestFlight version had a big battery drain, but it√¢s better now on the official release. The only limitation is that we can√¢t add our own lists, but I am fine with the default lists ans it works perfect.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;I was looking for this long time finally it√¢s here.&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;I love block. I do not want to use chrome. It√¢s perfect. I can use this in safari..&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Finally √∞¬§¬©&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Waiting for many years. Added in all apple devices. Working properly. √∞√∞&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;App Privacy&lt;/head&gt;
    &lt;p&gt;The developer, Raymond Hill, indicated that the app√¢s privacy practices may include handling of data as described below. For more information, see the developer√¢s privacy policy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Not Collected&lt;/head&gt;
    &lt;p&gt;The developer does not collect any data from this app.&lt;/p&gt;
    &lt;p&gt;Privacy practices may vary based on, for example, the features you use or your age. Learn√Ç More&lt;/p&gt;
    &lt;head rend="h2"&gt;Information&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Provider&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Raymond Hill&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Size&lt;/item&gt;
      &lt;item rend="dd-2"&gt;6 MB&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Category&lt;/item&gt;
      &lt;item rend="dd-3"&gt;Utilities&lt;/item&gt;
      &lt;item rend="dt-4"&gt;Compatibility&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-5"&gt;iPhone&lt;/item&gt;
          &lt;item rend="dd-5"&gt;Requires iOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-6"&gt;iPad&lt;/item&gt;
          &lt;item rend="dd-6"&gt;Requires iPadOS 18.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-7"&gt;Mac&lt;/item&gt;
          &lt;item rend="dd-7"&gt;Requires macOS 13.5 or later.&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="dl"&gt;
          &lt;item rend="dt-8"&gt;Apple Vision&lt;/item&gt;
          &lt;item rend="dd-8"&gt;Requires visionOS 2.5 or later.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item rend="dt-9"&gt;Languages&lt;/item&gt;
      &lt;item rend="dd-9"&gt;
        &lt;p&gt;English&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-10"&gt;Age Rating&lt;/item&gt;
      &lt;item rend="dd-11"&gt;Learn More&lt;/item&gt;
      &lt;item rend="dt-12"&gt;Copyright&lt;/item&gt;
      &lt;item rend="dd-12"&gt;√Ç¬© Raymond Hill 2025&lt;/item&gt;
      &lt;item rend="dt-13"&gt;Price&lt;/item&gt;
      &lt;item rend="dd-13"&gt;Free&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742446</guid><pubDate>Wed, 29 Oct 2025 03:57:06 +0000</pubDate></item><item><title>Keep Android Open</title><link>http://keepandroidopen.org/</link><description>&lt;doc fingerprint="9da6d2e399ba52e9"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Keep Android Open&lt;/head&gt;
      &lt;p&gt;In August 2025, Google announced that starting next year, it will no longer be possible to develop apps for the Android platform without first registering centrally with Google.&lt;/p&gt;
      &lt;p&gt;This registration will involve:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Paying a fee to Google&lt;/item&gt;
        &lt;item&gt;Agreeing to Google‚Äôs Terms and Conditions&lt;/item&gt;
        &lt;item&gt;Providing government identification&lt;/item&gt;
        &lt;item&gt;Uploading evidence of an app‚Äôs private signing key&lt;/item&gt;
        &lt;item&gt;Listing all current and future application identifiers&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Some actions you can take to help oppose the enactment of this policy are:&lt;/p&gt;
      &lt;head rend="h2"&gt;Sign the Open Letter&lt;/head&gt;
      &lt;head rend="h2"&gt;European Union&lt;/head&gt;
      &lt;head rend="h2"&gt;United States&lt;/head&gt;
      &lt;head rend="h2"&gt;United Kingdom&lt;/head&gt;
      &lt;head rend="h2"&gt;Brazil&lt;/head&gt;
      &lt;head rend="h2"&gt;Other&lt;/head&gt;
      &lt;head rend="h2"&gt;References&lt;/head&gt;
      &lt;head rend="h3"&gt;Overview&lt;/head&gt;
      &lt;head rend="h3"&gt;Press Reactions&lt;/head&gt;
      &lt;head rend="h3"&gt;Video Responses&lt;/head&gt;
      &lt;head rend="h3"&gt;Editorials and Blogs&lt;/head&gt;
      &lt;head rend="h3"&gt;Discussions&lt;/head&gt;
      &lt;head rend="h3"&gt;Official Documentation&lt;/head&gt;
      &lt;head rend="h3"&gt;Miscellaneous&lt;/head&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742488</guid><pubDate>Wed, 29 Oct 2025 04:03:41 +0000</pubDate></item><item><title>Wacl ‚Äì A Tcl Distribution for WebAssembly</title><link>https://github.com/ecky-l/wacl</link><description>&lt;doc fingerprint="9ecd0e53a7e724a0"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a Tcl distribution for WebAssembly (webassembly.org). It enables Web developers to embed a Tcl interpreter in the browser and integrate Tcl with JavaScript. It enables Tcl developers to use their tools and language of choice to create client side web applications. It enables all developers to reuse a great and (over decades) grown code base of useful packages and scripts, such as Tcllib, to be used in web browsers.&lt;/p&gt;
    &lt;p&gt;It is an extension of the Emtcl project from Aidan Hobsen, which can be found here. But Wacl takes things a few steps further: it integrates a fully featured Tcl interpreter into the webpage and adds the following features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A main tclsh interpreter and capability to get it via JavaScript&lt;/item&gt;
      &lt;item&gt;An event loop to process all Tcl events (timer events, fileevents, custom events)&lt;/item&gt;
      &lt;item&gt;Client sockets. The socket -async ... command connects to websocket servers with the binary protocol. The resulting handle can be used to transmit binary data as with normal TCP sockets.&lt;/item&gt;
      &lt;item&gt;The Tcl library: modules and packages in the Emscripten virtual filesystem. You can add your own packages!&lt;/item&gt;
      &lt;item&gt;Proper initialization via Tcl_Init()&lt;/item&gt;
      &lt;item&gt;An extension to call javascript functions from Tcl&lt;/item&gt;
      &lt;item&gt;various useful extensions (see below for a list and comments)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The original illustrative dom command has been moved to the "wacl" namespace in the package of same name, which is available right at startup. This package contains also a command ::wacl::jscall to call javascript functions from Tcl which have been registered before via the jswrap() module function.&lt;/p&gt;
    &lt;p&gt;The code compiles fine with Emscripten 1.37.9 to JavaScript and WebAssembly. The latter is the preferred format: WebAssembly is only half the size of the JavaScript "asm.js" output (~1.4MB vs. 2.9MB) and at least twice as fast! However, that could induce incompatibilities with older browsers, which don't (yet) support WebAssembly.&lt;/p&gt;
    &lt;p&gt;The following extensions are included in Wacl&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;wacl native extension with commands wacl::dom and wacl::jscall&lt;/item&gt;
      &lt;item&gt;tDOM for parsing and creating XML and HTML content&lt;/item&gt;
      &lt;item&gt;json and json::write from tcllib&lt;/item&gt;
      &lt;item&gt;html from tcllib&lt;/item&gt;
      &lt;item&gt;javascript from tcllib&lt;/item&gt;
      &lt;item&gt;ncgi as dependency for html&lt;/item&gt;
      &lt;item&gt;rl_json A Tcl_Obj type for efficient JSON parsing and generation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More extensions can easily be included and used. C extensions can be compiled with Emscripten (with USE_TCL_STUBS disabled and statically initialized via waclAppInit()) and Tcl extensions can be included in the library virtual filesystem.&lt;/p&gt;
    &lt;p&gt;But be aware that including extensions is a tradeoff: for the additional functionality you pay with a larger download size. The really useful tDOM extension for instance increases the Wacl distribution by not less than 400kB, which must be downloaded to the users client when (s)he wants to run a wacl based application, and this can be painful with lower bandwidth. Thus it is better to limit the number of packages to what is necessary rather than to build a batteries included distribution which contains everything.&lt;/p&gt;
    &lt;p&gt;You can try it out here. You can download the precompiled version with the index page to play on your own webpage by downloading the precompiled binary from here. Both of these pages require a recent browser with webassembly support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mozilla Firefox &amp;gt;= 52.0&lt;/item&gt;
      &lt;item&gt;Google Chrome &amp;gt;= 57.0&lt;/item&gt;
      &lt;item&gt;Microsoft Edge (Windows 10 "Creators" update)&lt;/item&gt;
      &lt;item&gt;Opera&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wacl will compile on a Unix/Linux environment with the following tools installed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the Emscripten SDK. Installation is documented on its web page&lt;/item&gt;
      &lt;item&gt;make, autoconf&lt;/item&gt;
      &lt;item&gt;diff, patch (some patches to the original sources must be applied, this is done mostly automatically)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Windows is not supported, but macOS with the appropriate tools from MacPorts will probably work (not tested by myself).&lt;/p&gt;
    &lt;p&gt;First step is to checkout this repository.This will checkout the files in the current directory. There is a Makefile with the build steps and a README with instructione. The make procedure does merely download the tcl core sources, apply a small patch and configure &amp;amp; build the interpreter to a webassembly plus accompanying .data + .js files. These files can be deployed to the corresponding web source directories. The Emscripten SDK must be on the PATH (i.e. via source $EMSCRIPTEN/emsdk_set_env.sh). Once wacl is built, it can be used in any browser which supports webassembly, also on Windows.&lt;lb/&gt; The build system can be changed to produce javascript instead of webassembly, by simply removing the -s WASM=1 flag from the BCFLAGS variable in the Makefile. This will generate a larger (~2.8MB), yet minified .js output, which is slower at runtime, but compatible with browsers that don't support webassembly.&lt;/p&gt;
    &lt;p&gt;To build it, you need the emscripten sdk on your path. Then:&lt;/p&gt;
    &lt;code&gt;$ make waclprep  # One off prep - tcl-core download, hacks.patch application and autoconf
$ make config    # create build directory and run emconfigure tcl/unix/configure
$ make [all]     # create the library and emtcl.js
$ make install   # copy emtcl.js to ../www/js/
&lt;/code&gt;
    &lt;p&gt;If you want to totally reset all build files in ./tcl/ and start again:&lt;/p&gt;
    &lt;code&gt;$ make reset
&lt;/code&gt;
    &lt;p&gt;This removes all changes and untracked files in there, so be careful!&lt;/p&gt;
    &lt;p&gt;There is a target to recreate the patch, if you changed anything important in tcl/&lt;/p&gt;
    &lt;code&gt;$ make patch
&lt;/code&gt;
    &lt;p&gt;It downloads tcl-core (it not already present), extracts it and runs diff between it and tcl/. The result is the patch that is applied above via "make tclprep"&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742616</guid><pubDate>Wed, 29 Oct 2025 04:25:24 +0000</pubDate></item><item><title>Who needs Graphviz when you can build it yourself?</title><link>https://spidermonkey.dev/blog/2025/10/28/iongraph-web.html</link><description>&lt;doc fingerprint="af4c3a680161d9ab"&gt;
  &lt;main&gt;
    &lt;p&gt;We recently overhauled our internal tools for visualizing the compilation of JavaScript and WebAssembly. When SpiderMonkey‚Äôs optimizing compiler, Ion, is active, we can now produce interactive graphs showing exactly how functions are processed and optimized.&lt;/p&gt;
    &lt;p&gt;You can play with these graphs right here on this page. Simply write some JavaScript code in the &lt;code&gt;test&lt;/code&gt; function and see what graph is produced. You can click and drag to navigate, ctrl-scroll to zoom, and drag the slider at the bottom to scrub through the optimization process.&lt;/p&gt;
    &lt;p&gt;As you experiment, take note of how stable the graph layout is, even as the sizes of blocks change or new structures are added. Try clicking a block's title to select it, then drag the slider and watch the graph change while the block remains in place. Or, click an instruction's number to highlight it so you can keep an eye on it across passes.&lt;/p&gt;
    &lt;p&gt;We are not the first to visualize our compiler‚Äôs internal graphs, of course, nor the first to make them interactive. But I was not satisfied with the output of common tools like Graphviz or Mermaid, so I decided to create a layout algorithm specifically tailored to our needs. The resulting algorithm is simple, fast, produces surprisingly high-quality output, and can be implemented in less than a thousand lines of code. The purpose of this article is to walk you through this algorithm and the design concepts behind it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;As readers of this blog already know, SpiderMonkey has several tiers of execution for JavaScript and WebAssembly code. The highest tier is known as Ion, an optimizing SSA compiler that takes the most time to compile but produces the highest-quality output.&lt;/p&gt;
    &lt;p&gt;Working with Ion frequently requires us to visualize and debug the SSA graph. Since 2011 we have used a tool for this purpose called iongraph, built by Sean Stangl. It is a simple Python script that takes a JSON dump of our compiler graphs and uses Graphviz to produce a PDF. It is perfectly adequate, and very much the status quo for compiler authors, but unfortunately the Graphviz output has many problems that make our work tedious and frustrating.&lt;/p&gt;
    &lt;p&gt;The first problem is that the Graphviz output rarely bears any resemblance to the source code that produced it. Graphviz will place nodes wherever it feels will minimize error, resulting in a graph that snakes left and right seemingly at random. There is no visual intuition for how deeply nested a block of code is, nor is it easy to determine which blocks are inside or outside of loops. Consider the following function, and its Graphviz graph:&lt;/p&gt;
    &lt;code&gt;function foo(n) {
  let result = 0;
  for (let i = 0; i &amp;lt; n; i++) {
    if (!!(i % 2)) {
      result = 0x600DBEEF;
    } else {
      result = 0xBADBEEF;
    }
  }

  return result;
}
&lt;/code&gt;
    &lt;p&gt;Counterintuitively, the &lt;code&gt;return&lt;/code&gt; appears before the two assignments in the body of the loop. Since this graph mirrors JavaScript control flow, we‚Äôd expect to see the return at the bottom. This problem only gets worse as graphs grow larger and more complex.&lt;/p&gt;
    &lt;p&gt;The second, related problem is that Graphviz‚Äôs output is unstable. Small changes to the input can result in large changes to the output. As you page through the graphs of each pass within Ion, nodes will jump left and right, true and false branches will swap, loops will run up the right side instead of the left, and so on. This makes it very hard to understand the actual effect of any given pass. Consider the following before and after, and notice how the second graph is almost‚Äîbut not quite‚Äîa mirror image of the first, despite very minimal changes to the graph‚Äôs structure:&lt;/p&gt;
    &lt;p&gt;None of this felt right to me. Control flow graphs should be able to follow the structure of the program that produced them. After all, a control flow graph has many restrictions that a general-purpose tool would not be aware of: they have very few cycles, all of which are well-defined because they come from loops; furthermore, both JavaScript and WebAssembly have reducible control flow, meaning all loops have only one entry, and it is not possible to jump directly into the middle of a loop. This information could be used to our advantage.&lt;/p&gt;
    &lt;p&gt;Beyond that, a static PDF is far from ideal when exploring complicated graphs. Finding the inputs or uses of a given instruction is a tedious and frustrating exercise, as is following arrows from block to block. Even just zooming in and out is difficult. I eventually concluded that we ought to just build an interactive tool to overcome these limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How hard could layout be?&lt;/head&gt;
    &lt;p&gt;I had one false start with graph layout, with an algorithm that attempted to sort blocks into vertical ‚Äútracks‚Äù. This broke down quickly on a variety of programs and I was forced to go back to the drawing board‚Äîin fact, back to the source of the very tool I was trying to replace.&lt;/p&gt;
    &lt;p&gt;The algorithm used by &lt;code&gt;dot&lt;/code&gt;, the typical hierarchical layout mode for Graphviz, is known as the Sugiyama layout algorithm, from a 1981 paper by Sugiyama et al. As introduction, I found a short series of lectures that broke down the Sugiyama algorithm into 5 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cycle breaking, where the direction of some edges are flipped in order to produce a DAG.&lt;/item&gt;
      &lt;item&gt;Leveling, where vertices are assigned into horizontal layers according to their depth in the graph, and dummy vertices are added to any edge that crosses multiple layers.&lt;/item&gt;
      &lt;item&gt;Crossing minimization, where vertices on a layer are reordered in order to minimize the number of edge crossings.&lt;/item&gt;
      &lt;item&gt;Vertex positioning, where vertices are horizontally positioned in order to make the edges as straight as possible.&lt;/item&gt;
      &lt;item&gt;Drawing, where the final graph is rendered to the screen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps struck me as surprisingly straightforward, and provided useful opportunities to insert our own knowledge of the problem:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cycle breaking would be trivial for us, since the only cycles in our data are loops, and loop backedges are explicitly labeled. We could simply ignore backedges when laying out the graph.&lt;/item&gt;
      &lt;item&gt;Leveling would be straightforward, and could easily be modified to better mimic the source code. Specifically, any blocks coming after a loop in the source code could be artificially pushed down in the layout, solving the confusing early-exit problem.&lt;/item&gt;
      &lt;item&gt;Permuting vertices to reduce edge crossings was actually just a bad idea, since our goal was stability from graph to graph. The true and false branches of a condition should always appear in the same order, for example, and a few edge crossings is a small price to pay for this stability.&lt;/item&gt;
      &lt;item&gt;Since reducible control flow ensures that a program‚Äôs loops form a tree, vertex positioning could ensure that loops are always well-nested in the final graph.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Taken all together, these simplifications resulted in a remarkably straightforward algorithm, with the initial implementation being just 1000 lines of JavaScript. (See this demo for what it looked like at the time.) It also proved to be very efficient, since it avoided the most computationally complex parts of the Sugiyama algorithm.&lt;/p&gt;
    &lt;head rend="h2"&gt;iongraph from start to finish&lt;/head&gt;
    &lt;p&gt;We will now go through the entire iongraph layout algorithm. Each section contains explanatory diagrams, in which rectangles are basic blocks and circles are dummy nodes. Loop header blocks (the single entry point to each loop) are additionally colored green.&lt;/p&gt;
    &lt;p&gt;Be aware that the block positions in these diagrams are not representative of the actual computed layout position at each point in the process. For example, vertical positions are not calculated until the very end, but it would be hard to communicate what the algorithm was doing if all blocks were drawn on a single line!&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 1: Layering&lt;/head&gt;
    &lt;p&gt;We first sort the basic blocks into horizontal tracks called ‚Äúlayers‚Äù. This is very simple; we just start at layer 0 and recursively walk the graph, incrementing the layer number as we go. As we go, we track the ‚Äúheight‚Äù of each loop, not in pixels, but in layers.&lt;/p&gt;
    &lt;p&gt;We also take this opportunity to vertically position nodes ‚Äúinside‚Äù and ‚Äúoutside‚Äù of loops. Whenever we see an edge that exits a loop, we defer the layering of the destination block until we are done layering the loop contents, at which point we know the loop‚Äôs height.&lt;/p&gt;
    &lt;p&gt;A note on implementation: nodes are visited multiple times throughout the process, not just once. This can produce a quadratic explosion for large graphs, but I find that an early-out is sufficient to avoid this problem in practice.&lt;/p&gt;
    &lt;p&gt;The animation below shows the layering algorithm in action. Notice how the final block in the graph is visited twice, once after each loop that branches to it, and in each case, the block is deferred until the entire loop has been layered, rather than processed immediately after its predecessor block. The final position of the block is below the entirety of both loops, rather than directly below one of its predecessors as Graphviz would do. (Remember, horizontal and vertical positions have not yet been computed; the positions of the blocks in this diagram are hardcoded for demonstration purposes.)&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=layering*/function layerBlock(block, layer = 0) {
  // Omitted for clarity: special handling of our "backedge blocks"

  // Early out if the block would not be updated
  if (layer &amp;lt;= block.layer) {
    return;
  }

  // Update the layer of the current block
  block.layer = Math.max(block.layer, layer);

  // Update the heights of all loops containing the current block
  let header = block.loopHeader;
  while (header) {
    header.loopHeight = Math.max(header.loopHeight, block.layer - header.layer + 1);
    header = header.parentLoopHeader;
  }

  // Recursively layer successors
  for (const succ of block.successors) {
    if (succ.loopDepth &amp;lt; block.loopDepth) {
      // Outgoing edges from the current loop will be layered later
      block.loopHeader.outgoingEdges.push(succ);
    } else {
      layerBlock(succ, layer + 1);
    }
  }

  // Layer any outgoing edges only after the contents of the loop have
  // been processed
  if (block.isLoopHeader()) {
    for (const succ of block.outgoingEdges) {
      layerBlock(succ, layer + block.loopHeight);
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create dummy nodes&lt;/head&gt;
    &lt;p&gt;Any time an edge crosses a layer, we create a dummy node. This allows edges to be routed across layers without overlapping any blocks. Unlike in traditional Sugiyama, we always put downward dummies on the left and upward dummies on the right, producing a consistent ‚Äúcounter-clockwise‚Äù flow. This also makes it easy to read long vertical edges, whose direction would otherwise be ambiguous. (Recall how the loop backedge flipped from the right to the left in the ‚Äúunstable layout‚Äù Graphviz example from before.)&lt;/p&gt;
    &lt;p&gt;In addition, we coalesce any edges that are going to the same destination by merging their dummy nodes. This heavily reduces visual noise.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3: Straighten edges&lt;/head&gt;
    &lt;p&gt;This is the fuzziest and most ad-hoc part of the process. Basically, we run lots of small passes that walk up and down the graph, aligning layout nodes with each other. Our edge-straightening passes include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pushing nodes to the right of their loop header to ‚Äúindent‚Äù them.&lt;/item&gt;
      &lt;item&gt;Walking a layer left to right, moving children to the right to line up with their parents. If any nodes overlap as a result, they are pushed further to the right.&lt;/item&gt;
      &lt;item&gt;Walking a layer right to left, moving parents to the right to line up with their children. This version is more conservative and will not move a node if it would overlap with another. This cleans up most issues from the first pass.&lt;/item&gt;
      &lt;item&gt;Straightening runs of dummy nodes so we have clean vertical lines.&lt;/item&gt;
      &lt;item&gt;‚ÄúSucking in‚Äù dummy runs on the left side of the graph if there is room for them to move to the right.&lt;/item&gt;
      &lt;item&gt;Straighten out any edges that are ‚Äúnearly straight‚Äù, according to a chosen threshold. This makes the graph appear less wobbly. We do this by repeatedly ‚Äúcombing‚Äù the graph upward and downward, aligning parents with children, then children with parents, and so on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It is important to note that dummy nodes participate fully in this system. If for example you have two side-by-side loops, straightening the left loop‚Äôs backedge will push the right loop to the side, avoiding overlaps and preserving the graph‚Äôs visual structure.&lt;/p&gt;
    &lt;p&gt;We do not reach a fixed point with this strategy, nor do we attempt to. I find that if you continue to repeatedly apply these particular layout passes, nodes will wander to the right forever. Instead, the layout passes are hand-tuned to produce decent-looking results for most of the graphs we look at on a regular basis. That said, this could certainly be improved, especially for larger graphs which do benefit from more iterations.&lt;/p&gt;
    &lt;p&gt;At the end of this step, all nodes have a fixed X-coordinate and will not be modified further.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4: Track horizontal edges&lt;/head&gt;
    &lt;p&gt;Edges may overlap visually as they run horizontally between layers. To resolve this, we sort edges into parallel ‚Äútracks‚Äù, giving each a vertical offset. After tracking all the edges, we record the total height of the tracks and store it on the preceding layer as its ‚Äútrack height‚Äù. This allows us to leave room for the edges in the final layout step.&lt;/p&gt;
    &lt;p&gt;We first sort edges by their starting position, left to right. This produces a consistent arrangement of edges that has few vertical crossings in practice. Edges are then placed into tracks from the ‚Äúoutside in‚Äù, stacking rightward edges on top and leftward edges on the bottom, creating a new track if the edge would overlap with or cross any other edge.&lt;/p&gt;
    &lt;p&gt;The diagram below is interactive. Click and drag the blocks to see how the horizontal edges get assigned to tracks.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=tracks*/function trackHorizontalEdges(layer) {
  const TRACK_SPACING = 20;

  // Gather all edges on the layer, and sort left to right by starting coordinate
  const layerEdges = [];
  for (const node of layer.nodes) {
    for (const edge of node.edges) {
      layerEdges.push(edge);
    }
  }
  layerEdges.sort((a, b) =&amp;gt; a.startX - b.startX);

  // Assign edges to "tracks" based on whether they overlap horizontally with
  // each other. We walk the tracks from the outside in and stop if we ever
  // overlap with any other edge.
  const rightwardTracks = []; // [][]Edge
  const leftwardTracks = [];  // [][]Edge
  nextEdge:
  for (const edge of layerEdges) {
    const trackSet = edge.endX - edge.startX &amp;gt;= 0 ? rightwardTracks : leftwardTracks;
    let lastValidTrack = null; // []Edge | null

    // Iterate through the tracks in reverse order (outside in)
    for (let i = trackSet.length - 1; i &amp;gt;= 0; i--) {
      const track = trackSet[i];
      let overlapsWithAnyInThisTrack = false;
      for (const otherEdge of track) {
        if (edge.dst === otherEdge.dst) {
          // Assign the edge to this track to merge arrows
          track.push(edge);
          continue nextEdge;
        }

        const al = Math.min(edge.startX, edge.endX);
        const ar = Math.max(edge.startX, edge.endX);
        const bl = Math.min(otherEdge.startX, otherEdge.endX);
        const br = Math.max(otherEdge.startX, otherEdge.endX);
        const overlaps = ar &amp;gt;= bl &amp;amp;&amp;amp; al &amp;lt;= br;
        if (overlaps) {
          overlapsWithAnyInThisTrack = true;
          break;
        }
      }

      if (overlapsWithAnyInThisTrack) {
        break;
      } else {
        lastValidTrack = track;
      }
    }

    if (lastValidTrack) {
      lastValidTrack.push(edge);
    } else {
      trackSet.push([edge]);
    }
  }

  // Use track info to apply offsets to each edge for rendering.
  const tracksHeight = TRACK_SPACING * Math.max(
    0,
    rightwardTracks.length + leftwardTracks.length - 1,
  );
  let trackOffset = -tracksHeight / 2;
  for (const track of [...rightwardTracks.toReversed(), ...leftwardTracks]) {
    for (const edge of track) {
      edge.offset = trackOffset;
    }
    trackOffset += TRACK_SPACING;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verticalize&lt;/head&gt;
    &lt;p&gt;Finally, we assign each node a Y-coordinate. Starting at a Y-coordinate of zero, we iterate through the layers, repeatedly adding the layer‚Äôs height and its track height, where the layer height is the maximum height of any node in the layer. All nodes within a layer receive the same Y-coordinate; this is simple and easier to read than Graphviz‚Äôs default of vertically centering nodes within a layer.&lt;/p&gt;
    &lt;p&gt;Now that every node has both an X and Y coordinate, the layout process is complete.&lt;/p&gt;
    &lt;head&gt;Implementation pseudocode&lt;/head&gt;
    &lt;code&gt;/*CODEBLOCK=verticalize*/function verticalize(layers) {
  let layerY = 0;
  for (const layer of layers) {
    let layerHeight = 0;
    for (const node of layer.nodes) {
      node.y = layerY;
      layerHeight = Math.max(layerHeight, node.height);
    }
    layerY += layerHeight;
    layerY += layer.trackHeight;
  }
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 6: Render&lt;/head&gt;
    &lt;p&gt;The details of rendering are out of scope for this article, and depend on the specific application. However, I wish to highlight a stylistic decision that I feel makes our graphs more readable.&lt;/p&gt;
    &lt;p&gt;When rendering edges, we use a style inspired by railroad diagrams. These have many advantages over the B√©zier curves employed by Graphviz. First, straight lines feel more organized and are easier to follow when scrolling up and down. Second, they are easy to route (vertical when crossing layers, horizontal between layers). Third, they are easy to coalesce when they share a destination, and the junctions provide a clear indication of the edge‚Äôs direction. Fourth, they always cross at right angles, improving clarity and reducing the need to avoid edge crossings in the first place.&lt;/p&gt;
    &lt;p&gt;Consider the following example. There are several edge crossings that may traditionally be considered undesirable‚Äîyet the edges and their directions remain clear. Of particular note is the vertical junction highlighted in red on the left: not only is it immediately clear that these edges share a destination, but the junction itself signals that the edges are flowing downward. I find this much more pleasant than the ‚Äúrat‚Äôs nest‚Äù that Graphviz tends to produce.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this work?&lt;/head&gt;
    &lt;p&gt;It may seem surprising that such a simple (and stupid) layout algorithm could produce such readable graphs, when more sophisticated layout algorithms struggle. However, I feel that the algorithm succeeds because of its simplicity.&lt;/p&gt;
    &lt;p&gt;Most graph layout algorithms are optimization problems, where error is minimized on some chosen metrics. However, these metrics seem to correlate poorly to readability in practice. For example, it seems good in theory to rearrange nodes to minimize edge crossings. But a predictable order of nodes seems to produce more sensible results overall, and simple rules for edge routing are sufficient to keep things tidy. (As a bonus, this also gives us layout stability from pass to pass.) Similarly, layout rules like ‚Äúalign parents with their children‚Äù produce more readable results than ‚Äúminimize the lengths of edges‚Äù.&lt;/p&gt;
    &lt;p&gt;Furthermore, by rejecting the optimization problem, a human author gains more control over the layout. We are able to position nodes ‚Äúinside‚Äù of loops, and push post-loop content down in the graph, because we reject this global constraint-solver approach. Minimizing ‚Äúerror‚Äù is meaningless compared to a human maximizing meaning through thoughtful design.&lt;/p&gt;
    &lt;p&gt;And finally, the resulting algorithm is simply more efficient. All the layout passes in iongraph are easy to program and scale gracefully to large graphs because they run in roughly linear time. It is better, in my view, to run a fixed number of layout iterations according to your graph complexity and time budget, rather than to run a complex constraint solver until it is ‚Äúdone‚Äù.&lt;/p&gt;
    &lt;p&gt;By following this philosophy, even the worst graphs become tractable. Below is a screenshot of a zlib function, compiled to WebAssembly, and rendered using the old tool.&lt;/p&gt;
    &lt;p&gt;It took about ten minutes for Graphviz to produce this spaghetti nightmare. By comparison, iongraph can now lay out this function in 20 milliseconds. The result is still not particularly beautiful, but it renders thousands of times faster and is much easier to navigate.&lt;/p&gt;
    &lt;p&gt;Perhaps programmers ought to put less trust into magic optimizing systems, especially when a human-friendly result is the goal. Simple (and stupid) algorithms can be very effective when applied with discretion and taste.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;We have already integrated iongraph into the Firefox profiler, making it easy for us to view the graphs of the most expensive or impactful functions we find in our performance work. Unfortunately, this is only available in specific builds of the SpiderMonkey shell, and is not available in full browser builds. This is due to architectural differences in how profiling data is captured and the flags with which the browser and shell are built. I would love for Firefox users to someday be able to view these graphs themselves, but at the moment we have no plans to expose this to the browser. However, one bug tracking some related work can be found here.&lt;/p&gt;
    &lt;p&gt;We will continue to sporadically update iongraph with more features to aid us in our work. We have several ideas for new features, including richer navigation, search, and visualization of register allocation info. However, we have no explicit roadmap for when these features may be released.&lt;/p&gt;
    &lt;p&gt;To experiment with iongraph locally, you can run a debug build of the SpiderMonkey shell with &lt;code&gt;IONFLAGS=logs&lt;/code&gt;; this will dump information to &lt;code&gt;/tmp/ion.json&lt;/code&gt;. This file can then be loaded into the standalone deployment of iongraph. Please be aware that the user experience is rough and unpolished in its current state.&lt;/p&gt;
    &lt;p&gt;The source code for iongraph can be found on GitHub. If this subject interests you, we would welcome contributions to iongraph and its integration into the browser. The best place to reach us is our Matrix chat.&lt;/p&gt;
    &lt;p&gt;Thanks to Matthew Gaudet, Asaf Gartner, and Colin Davidson for their feedback on this article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45742907</guid><pubDate>Wed, 29 Oct 2025 05:17:49 +0000</pubDate></item><item><title>SpiderMonkey Garbage Collector</title><link>https://firefox-source-docs.mozilla.org/js/gc.html</link><description>&lt;doc fingerprint="b7f969cd84f696d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SpiderMonkey garbage collector&lt;/head&gt;
    &lt;p&gt;The SpiderMonkey garbage collector is responsible for allocating memory representing JavaScript data structures and deallocating them when they are no longer in use. It aims to collect as much data as possible in as little time as possible. As well as JavaScript data it is also used to allocate some internal SpiderMonkey data structures.&lt;/p&gt;
    &lt;p&gt;The garbage collector is a hybrid tracing collector, and has the following features:&lt;/p&gt;
    &lt;p&gt;For an overview of garbage collection see: https://en.wikipedia.org/wiki/Tracing_garbage_collection&lt;/p&gt;
    &lt;head rend="h2"&gt;Description of features&lt;/head&gt;
    &lt;head rend="h3"&gt;Precise collection&lt;/head&gt;
    &lt;p&gt;The GC is ‚Äòprecise‚Äô in that it knows the layout of allocations (which is used to determine reachable children) and also the location of all stack roots. This means it does not need to resort to conservative techniques that may cause garbage to be retained unnecessarily.&lt;/p&gt;
    &lt;p&gt;Knowledge of the stack is achieved with C++ wrapper classes that must be used for stack roots and handles (pointers) to them. This is enforced by the SpiderMonkey API (which operates in terms of these types) and checked by a static analysis that reports places when unrooted GC pointers can be present when a GC could occur.&lt;/p&gt;
    &lt;p&gt;For details of stack rooting, see: https://github.com/mozilla-spidermonkey/spidermonkey-embedding-examples/blob/esr78/docs/GC%20Rooting%20Guide.md&lt;/p&gt;
    &lt;p&gt;We also have a static analysis for detecting errors in rooting. It can be run locally or in CI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental collection&lt;/head&gt;
    &lt;p&gt;‚ÄòStop the world‚Äô collectors run a whole collection in one go, which can result in unacceptable pauses for users. An incremental collector breaks its execution into a number of small slices, reducing user impact.&lt;/p&gt;
    &lt;p&gt;As far as possible the SpiderMonkey collector runs incrementally. Not all parts of a collection can be performed incrementally however as there are some operations that need to complete atomically with respect to the rest of the program.&lt;/p&gt;
    &lt;p&gt;Currently, most of the collection is performed incrementally. Root marking, compacting, and an initial part of sweeping are not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generational collection&lt;/head&gt;
    &lt;p&gt;Most real world allocations either die very quickly or live for a long time. This suggests an approach to collection where allocations are moved between ‚Äògenerations‚Äô (separate heaps) depending on how long they have survived. Generations containing young allocations are fast to collect and can be collected more frequently; older generations are collected less often.&lt;/p&gt;
    &lt;p&gt;The SpiderMonkey collector implements a single young generation (the nursery) and a single old generation (the tenured heap). Collecting the nursery is known as a minor GC as opposed to a major GC that collects the whole heap (including the nursery).&lt;/p&gt;
    &lt;head rend="h3"&gt;Concurrent collection&lt;/head&gt;
    &lt;p&gt;Many systems have more than one CPU and therefore can benefit from offloading GC work to another core. In GC terms ‚Äòconcurrent‚Äô usually refers to GC work happening while the main program continues to run.&lt;/p&gt;
    &lt;p&gt;The SpiderMonkey collector currently only uses concurrency in limited phases.&lt;/p&gt;
    &lt;p&gt;This includes most finalization work (there are some restrictions as not all finalization code can tolerate this) and some other aspects such as allocating and decommitting blocks of memory.&lt;/p&gt;
    &lt;p&gt;Performing marking work concurrently is currently being investigated.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallel collection&lt;/head&gt;
    &lt;p&gt;In GC terms ‚Äòparallel‚Äô usually means work performed in parallel while the collector is running, as opposed to the main program itself. The SpiderMonkey collector performs work within GC slices in parallel wherever possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compacting collection&lt;/head&gt;
    &lt;p&gt;The collector allocates data with the same type and size in ‚Äòarenas‚Äô (often know as slabs). After many allocations have died this can leave many arenas containing free space (external fragmentation). Compacting remedies this by moving allocations between arenas to free up as much memory as possible.&lt;/p&gt;
    &lt;p&gt;Compacting involves tracing the entire heap to update pointers to moved data and is not incremental so it only happens rarely, or in response to memory pressure notifications.&lt;/p&gt;
    &lt;head rend="h3"&gt;Partitioned heap&lt;/head&gt;
    &lt;p&gt;The collector has the concept of ‚Äòzones‚Äô which are separate heaps which can be collected independently. Objects in different zones can refer to each other however.&lt;/p&gt;
    &lt;p&gt;Zones are also used to help incrementalize parts of the collection. For example, compacting is not fully incremental but can be performed one zone at a time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other documentation&lt;/head&gt;
    &lt;p&gt;More details about the Garbage Collector (GC) can be found by looking for the [SMDOC] Garbage Collector comment in the sources.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45744395</guid><pubDate>Wed, 29 Oct 2025 09:06:36 +0000</pubDate></item><item><title>YouTube is taking down videos on performing nonstandard Windows 11 installs</title><link>https://old.reddit.com/r/DataHoarder/comments/1oiz0v0/youtube_is_taking_down_videos_on_performing/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45744503</guid><pubDate>Wed, 29 Oct 2025 09:26:09 +0000</pubDate></item><item><title>Aggressive bots ruined my weekend</title><link>https://herman.bearblog.dev/agressive-bots/</link><description>&lt;doc fingerprint="3e708d031d6216a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Aggressive bots ruined my weekend&lt;/head&gt;
    &lt;p&gt;On the 25th of October Bear had its first major outage. Specifically, the reverse proxy which handles custom domains went down, causing custom domains to time out.&lt;/p&gt;
    &lt;p&gt;Unfortunately my monitoring tool failed to notify me, and it being a Saturday, I didn't notice the outage for longer than is reasonable. I apologise to everyone who was affected by it.&lt;/p&gt;
    &lt;p&gt;First, I want to dissect the root cause, exactly what went wrong, and then provide the steps I've taken to mitigate this in the future.&lt;/p&gt;
    &lt;p&gt;I wrote about The Great Scrape at the beginning of this year. The vast majority of web traffic is now bots, and it is becoming increasingly more hostile to have publicly available resources on the internet.&lt;/p&gt;
    &lt;p&gt;There are 3 major kinds of bots currently flooding the internet: AI scrapers, malicious scrapers, and unchecked automations/scrapers.&lt;/p&gt;
    &lt;p&gt;The first has been discussed at length. Data is worth something now that it is used as fodder to train LLMs, and there is a financial incentive to scrape, so scrape they will. They've depleted all human-created writing on the internet, and are becoming increasingly ravenous for new wells of content. I've seen this compared to the search for low-background-radiation steel, which is, itself, very interesting.&lt;/p&gt;
    &lt;p&gt;These scrapers, however, are the easiest to deal with since they tend to identify themselves as ChatGPT, Anthropic, XAI, et cetera. They also tend to specify whether they are from user-initiated searches (think all the sites that get scraped when you make a request with ChatGPT), or data mining (data used to train models). On Bear Blog I allow the first kinds, but block the second, since bloggers want discoverability, but usually don't want their writing used to train the next big model.&lt;/p&gt;
    &lt;p&gt;The next two kinds of scraper are more insidious. The malicious scrapers are bots that systematically scrape and re-scrape websites, sometimes every few minutes, looking for vulnerabilities such as misconfigured Wordpress instances, or &lt;code&gt;.env&lt;/code&gt; and &lt;code&gt;.aws&lt;/code&gt; files, among other things, accidentally left lying around.&lt;/p&gt;
    &lt;p&gt;It's more dangerous than ever to self-host, since simple mistakes in configurations will likely be found and exploited. In the last 24 hours I've blocked close to 2 million malicious requests across several hundred blogs.&lt;/p&gt;
    &lt;p&gt;What's wild is that these scrapers rotate through thousands of IP addresses during their scrapes, which leads me to suspect that the requests are being tunnelled through apps on mobile devices, since the ASNs tend to be cellular networks. I'm still speculating here, but I think app developers have found another way to monetise their apps by offering them for free, and selling tunnel access to scrapers.&lt;/p&gt;
    &lt;p&gt;Now, on to the unchecked automations. Vibe coding has made web-scraping easier than ever. Any script-kiddie can easily build a functional scraper in a single prompt and have it run all day from their home computer, and if the dramatic rise in scraping is anything to go by, many do. Tens of thousands of new scrapers have cropped up over the past few months, accidentally DDoSing website after website in their wake. The average consumer-grade computer is significantly more powerful than a VPS, so these machines can easily cause a lot of damage without noticing.&lt;/p&gt;
    &lt;p&gt;I've managed to keep all these scrapers at bay using a combination of web application firewall (WAF) rules and rate limiting provided by Cloudflare, as well as some custom code which finds and quarantines bad bots based on their activity.&lt;/p&gt;
    &lt;p&gt;I've played around with serving Zip Bombs, which was quite satisfying, but I stopped for fear of accidentally bombing a legitimate user. Another thing I've played around with is Proof of Work validation, making it expensive for bots to scrape, as well as serving endless junk data to keep the bots busy. Both of these are interesting, but ultimately are just as effective as simply blocking those requests, without the increased complexity.&lt;/p&gt;
    &lt;p&gt;With that context, here's exactly went wrong on Saturday.&lt;/p&gt;
    &lt;p&gt;Previously, the bottleneck for page requests was the web-server itself, since it does the heavy lifting. It automatically scales horizontally by up to a factor of 10, if necessary, but bot requests can scale by significantly more than that, so having strong bot detection and mitigation, as well as serving highly-requested endpoints via a CDN is necessary. This is a solved problem, as outlined in my Great Scrape post, but worth restating.&lt;/p&gt;
    &lt;p&gt;On Saturday morning a few hundred blogs were DDoSed, with tens of thousands of pages requested per minute (from the logs it's hard to say whether they were malicious, or just very aggressive scrapers). The above-mentioned mitigations worked as expected, however the reverse-proxy‚Äîwhich sits up-stream of most of these mitigations‚Äîbecame saturated with requests and decided it needed to take a little nap.&lt;/p&gt;
    &lt;p&gt;The big blue spike is what toppled the server. It's so big it makes the rest of the graph look flat.&lt;/p&gt;
    &lt;p&gt;This server had been running with zero downtime for 5 years up until this point.&lt;/p&gt;
    &lt;p&gt;Unfortunately my uptime monitor failed to alert me via the push notifications I'd set up, even though it's the only app I have that not only has notifications enabled (see my post on notifications), but even has critical alerts enabled, so it'll wake me up in the middle of the night if necessary. I still have no idea why this alert didn't come through, and I have ruled out misconfiguration through various tests.&lt;/p&gt;
    &lt;p&gt;This brings me to how I will prevent this from happening in the future.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Redundancy in monitoring. I now have a second monitoring service running alongside my uptime monitor which will give me a phone call, email, and text message in the event of any downtime.&lt;/item&gt;
      &lt;item&gt;More aggressive rate-limiting and bot mitigation on the reverse proxy. This already reduces the server load by about half.&lt;/item&gt;
      &lt;item&gt;I've bumped up the size of the reverse proxy, which can now handle about 5 times the load. This is overkill, but compute is cheap, and certainly worth the stress-mitigation. I'm already bald. I don't need to go balder.&lt;/item&gt;
      &lt;item&gt;Auto-restart the reverse-proxy if bandwidth usage drops to zero for more than 2 minutes.&lt;/item&gt;
      &lt;item&gt;Added a status page, available at https://status.bearblog.dev for better visibility and transparency. Hopefully those bars stay solid green forever.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This should be enough to keep everything healthy. If you have any suggestions, or need help with your own bot issues, send me an email.&lt;/p&gt;
    &lt;p&gt;The public internet is mostly bots, many of whom are bad netizens. It's the most hostile it's ever been, and it is because of this that I feel it's more important than ever to take good care of the spaces that make the internet worth visiting.&lt;/p&gt;
    &lt;p&gt;The arms race continues...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45745072</guid><pubDate>Wed, 29 Oct 2025 10:47:25 +0000</pubDate></item><item><title>AWS to bare metal two years later: Answering your questions about leaving AWS</title><link>https://oneuptime.com/blog/post/2025-10-29-aws-to-bare-metal-two-years-later/view</link><description>&lt;doc fingerprint="ff74527664b5afab"&gt;
  &lt;main&gt;
    &lt;p&gt;When we published How moving from AWS to Bare-Metal saved us $230,000 /yr. in 2023, the story travelled far beyond our usual readership. The discussion threads on Hacker News and Reddit were packed with sharp questions: did we skip Reserved Instances, how do we fail over a single rack, what about the people cost, and when is cloud still the better answer? This follow-up is our long-form reply.&lt;/p&gt;
    &lt;p&gt;Over the last twenty-four months we:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ran the MicroK8s + Ceph stack in production for 730+ days with 99.993% measured availability.&lt;/item&gt;
      &lt;item&gt;Added a second rack in Frankfurt, joined to our primary Paris cage over redundant DWDM, to kill the ‚Äúsingle rack‚Äù concern.&lt;/item&gt;
      &lt;item&gt;Cut average customer-facing latency by 19% thanks to local NVMe and eliminating noisy neighbours.&lt;/item&gt;
      &lt;item&gt;Reinvested the savings into buying bare metal AI servers to expand LLM-based alert / incident summarisation and auto code fixes based on log / traces and metrics in OneUptime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Below we tackle the recurring themes from the community feedback, complete with the numbers we use internally.&lt;/p&gt;
    &lt;head rend="h2"&gt;$230,000 / yr savings? That is just an engineers salary.&lt;/head&gt;
    &lt;p&gt;In the US, it is. In the rest of the world. That's 2-5x engineers salary. We used to save $230,000 / yr but now the savings have exponentially grown. We now save over $1.2M / yr and we expect this to grow, as we grow as a business.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhy not just buy Savings Plans or Reserved Instances?‚Äù&lt;/head&gt;
    &lt;p&gt;We tried. Long answer: the maths still favoured bare metal once we priced everything in. We see a savings of over 76% if you compare our bare metal setup to AWS.&lt;/p&gt;
    &lt;p&gt;A few clarifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Savings Plans do not reduce S3, egress, or Direct Connect. 37% off instances still leaves you paying list price for bandwidth, which was 22% of our AWS bill.&lt;/item&gt;
      &lt;item&gt;EKS had an extra $1,260/month control-plane fee plus $600/month for NAT gateways. Those costs disappear once you run Kubernetes yourself.&lt;/item&gt;
      &lt;item&gt;Our workload is 24/7 steady. We were already at &amp;gt;90% reservation coverage; there was no idle burst capacity to ‚Äúright size‚Äù away. If we had the kind of bursty compute profile many commenters referenced, the choice would be different.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;‚ÄúHow much did migration and ongoing ops really cost?‚Äù&lt;/head&gt;
    &lt;p&gt;We spent a week of engineers time (and that is the worst case estimate) on the initial migration, spread across SRE, platform, and database owners. Most of that time was work we needed anyway‚Äîformalising infrastructure-as-code, smoke testing charts, tightening backup policies. The incremental work that existed purely because of bare metal was roughly one week.&lt;/p&gt;
    &lt;p&gt;Ongoing run-cost looks like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hands-on keyboard: ~24 engineer-hours/quarter across the entire platform team, including routine patching and firmware updates. That is comparable to the AWS time we used to burn on cost optimisation, IAM policy churn, and chasing deprecations and updating our VM's on AWS.&lt;/item&gt;
      &lt;item&gt;Remote hands: 2 interventions in 24 months (mainly disks). Mean response time: 27 minutes. We do not staff an on-site team. We rely on co-location provider to physically manage our rack. This means no traditional hardware admins.&lt;/item&gt;
      &lt;item&gt;Automation: We're now moving to Talos. We PXE boot with Tinkerbell, image with Talos, manage configs through Flux and Terraform, and run conformance suites before each Kubernetes upgrade. All of those tools also hardened our AWS estate, so they were not net-new effort.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The opportunity cost question from is fair. We track it the same way we track feature velocity: did the infra team ship less? The answer was ‚Äúno‚Äù‚Äîour release cadence increased because we reclaimed few hours/month we used to spend in AWS ‚Äúcost council‚Äù meetings.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúIsn‚Äôt a single rack a single point of failure?‚Äù&lt;/head&gt;
    &lt;p&gt;We have multiple racks across two different DC / providers. We:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leased a secondary quarter rack in Frankfurt with a different provider and power utility.&lt;/item&gt;
      &lt;item&gt;Currently: Deployed a second MicroK8s control plane, mirrored Ceph pools with asynchronous replication. Future: We're moving to Talos. Nothing against Microk8s, but we like the Talos way of managing the k8s cluster.&lt;/item&gt;
      &lt;item&gt;Added isolated out-of-band management paths (4G / satellite) so we can reach the gear even during metro fibre events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The AWS failover cluster we mentioned in 2023 still exists. We rehearse a full cutover quarterly using the same Helm releases we ship to customers. DNS failover remains the slowest leg (resolver caches can ignore TTL), so we added Anycast ingress via BGP with our transit provider to cut traffic shifting to sub-minute.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhat about hardware lifecycle and surprise CapEx?‚Äù&lt;/head&gt;
    &lt;p&gt;We amortise servers over five years, but we sized them with 2 √ó AMD EPYC 9654 CPUs, 1 TB RAM, and NVMe sleds. At our current growth rate the boxes will hit CPU saturation before we hit year five. When that happens, the plan is to cascade the older gear into our regional analytics cluster (we use Posthog + Metabase for this) and buy a new batch. Thanks to the savings delta, we can refresh 40% of the fleet every 24 months and still spend less annually than the optimised AWS bill above.&lt;/p&gt;
    &lt;p&gt;We also buy extended warranties from the OEM (Supermicro) and keep three cold spares in the cage. The hardware lasts 7-8 years and not 5, but we wtill count it as 5 to be very conservative.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúAre you reinventing managed services?‚Äù&lt;/head&gt;
    &lt;p&gt;Another strong Reddit critique: why rebuild services AWS already offers? Three reasons we are comfortable with the trade:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Portability is part of our product promise. OneUptime customers self-host in their own environments. Running the same open stack we ship (Postgres, Redis, ClickHouse, etc.) keeps us honest. We eun on Kubernetes and self-hosted customers run on Kubernetes as well.&lt;/item&gt;
      &lt;item&gt;Tooling maturity. Two years ago we relied on Terraform + EKS + RDS. Today we run MicroK8s (Talos in the future), Argo Rollouts, OpenTelemetry Collector, and Ceph dashboards. None of that is bespoke. We do not maintain a fork of anything.&lt;/item&gt;
      &lt;item&gt;Selective cloud use. We still pay AWS for Glacier backups, CloudFront for edge caching, and short-lived burst capacity for load tests. Cloud makes sense when elasticity matters; bare metal wins when baseload dominates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Managed services are phenomenal when you are short on expertise or need features beyond commodity compute. If we were all-in on DynamoDB streams or Step Functions we would almost certainly still be on AWS.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúHow do bandwidth and DoS scenarios work now?‚Äù&lt;/head&gt;
    &lt;p&gt;We committed to 5 Gbps 95th percentile across two carriers. The same traffic on AWS egress would be 8x expensive in eu-west-1. For DDoS protection we front our ingress with Cloudflare.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúHas reliability suffered?‚Äù&lt;/head&gt;
    &lt;p&gt;Short answer: No. Infact it was better than AWS (compared to recent AWS downtimes)&lt;/p&gt;
    &lt;p&gt;We have 730+ days with 99.993% measured availability and we also escaped AWS region wide downtime that happened a week ago.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúHow do audits and compliance work off-cloud now?‚Äù&lt;/head&gt;
    &lt;p&gt;We stayed SOC 2 Type II and ISO 27001 certified through the transition. The biggest deltas auditors cared about:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Physical controls: We provide badge logs from the colo, camera footage on request, and quarterly access reviews. The colo already meets Tier III redundancy, so their reports roll into ours.&lt;/item&gt;
      &lt;item&gt;Change management: Terraform plans, and now Talos machine configs give us immutable evidence of change. Auditors liked that more than AWS Console screenshots.&lt;/item&gt;
      &lt;item&gt;Business continuity: We prove failover by moving workload to other DC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are in a regulated space (HIPAA for instance), expect the paperwork to grow a little. We worked it in by leaning on the colo providers‚Äô standard compliance packets‚Äîthey slotted straight into our risk register.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhy not stay in the cloud but switch providers?‚Äù&lt;/head&gt;
    &lt;p&gt;We priced Hetzner, OVH, Leaseweb, Equinix Metal, and AWS Outposts. The short version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hyperscaler alternatives were cheaper on compute but still expensive on egress once you hit petabytes/month. Outposts also carried minimum commits that exceeded our needs.&lt;/item&gt;
      &lt;item&gt;European dedicated hosts (Hetzner, OVH) are fantastic for lab clusters. The challenge was multi-100 TB Ceph clusters with redundant uplinks and smart-hands SLAs. Once we priced that tier, the savings narrowed.&lt;/item&gt;
      &lt;item&gt;Equinix Metal got the closest, but bare metal on-demand still carried a 25-30% premium over our CapEx plan. Their global footprint is tempting; we may still use them for short-lived expansion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Owning the hardware also let us plan power density (we run 15 kW racks) and reuse components. For our steady-state footprint, colocation won by a long shot.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúWhat does day-to-day toil look like now?‚Äù&lt;/head&gt;
    &lt;p&gt;We put real numbers to it because Reddit kept us honest:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Weekly: Kernel and firmware patches (Talos makes this a redeploy), Ceph health checks, Total time averages 1 hour/week on average over months.&lt;/item&gt;
      &lt;item&gt;Monthly: Kubernetes control plane upgrades in canary fashion. About 2 engineer-hours. We expect this to reduce when Talos kicks in.&lt;/item&gt;
      &lt;item&gt;Quarterly: Disaster recovery drills, capacity planning, and contract audits with carriers. Roughly 12 hours across three engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total toil is ~14 engineer-hours/month, including prep. The AWS era had us spending similar time but on different work: chasing cost anomalies, expanding Security Hub exceptions, and mapping breaking changes in managed services. The toil moved; it did not multiply.&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄúDo you still use the cloud for anything substantial?‚Äù&lt;/head&gt;
    &lt;p&gt;Absolutely. Cloud still solves problems we would rather not own:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Glacier keeps long-term log archives at a price point local object storage cannot match.&lt;/item&gt;
      &lt;item&gt;CloudFront handles 14 edge PoPs we do not want to build. We terminate TLS at the edge for marketing assets and docs. We will soon move this to Cloudflare as they are cheaper.&lt;/item&gt;
      &lt;item&gt;We spin up short-lived AWS environments for load testing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So yes, we left AWS for the base workload, but we still swipe the corporate card when elasticity or geography outweighs fixed-cost savings.&lt;/p&gt;
    &lt;head rend="h2"&gt;When the cloud is still the right answer&lt;/head&gt;
    &lt;p&gt;It depends on your workload. We still recommend staying put if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your usage pattern is spiky or seasonal and you can auto-scale to near zero between peaks.&lt;/item&gt;
      &lt;item&gt;You lean heavily on managed services (Aurora Serverless, Kinesis, Step Functions) where the operational load is the value prop.&lt;/item&gt;
      &lt;item&gt;You do not have the appetite to build a platform team comfortable with Kubernetes, Ceph, observability, and incident response.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cloud-first was the right call for our first five years. Bare metal became the right call once our compute footprint, data gravity, and independence requirements stabilised.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is next&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are working on a detailed runbook + Terraform module to help teams do capex forecasting for colo moves. Expect that on the blog later this year.&lt;/item&gt;
      &lt;item&gt;A deep dive on Talos is in the queue, as requested by multiple folks in the HN thread.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions we did not cover? Let us know in the discussion threads‚Äîwe are happy to keep sharing the gritty details.&lt;/p&gt;
    &lt;p&gt;Related Reading:&lt;/p&gt;
    &lt;head rend="h3"&gt;Neel Patel&lt;/head&gt;
    &lt;p&gt;@devneelpatel ‚Ä¢ Oct 29, 2025 ‚Ä¢&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45745281</guid><pubDate>Wed, 29 Oct 2025 11:14:38 +0000</pubDate></item><item><title>Show HN: Learn German with Games</title><link>https://www.learngermanwithgames.com/</link><description>&lt;doc fingerprint="2ce7b8cb946d6548"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Choose Your Learning Adventure&lt;/head&gt;
    &lt;p&gt;Select a game below and start mastering German in an engaging, interactive way!&lt;/p&gt;
    &lt;head rend="h3"&gt;Numbers to Words Game&lt;/head&gt;
    &lt;p&gt;See a number and type the German word - perfect for learning German number vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;Words to Numbers Game&lt;/head&gt;
    &lt;p&gt;Practice recognizing German number words and converting them to digits&lt;/p&gt;
    &lt;head rend="h3"&gt;German Time Game&lt;/head&gt;
    &lt;p&gt;Learn to tell time in German by reading analog clocks and typing time expressions&lt;/p&gt;
    &lt;head rend="h3"&gt;Time Short Form Game&lt;/head&gt;
    &lt;p&gt;Practice German time with short forms: nach, vor, halb, viertel, and punkt&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Artikel&lt;/head&gt;
    &lt;p&gt;Master German artikels (der, die, das) by guessing the correct artikel for each noun&lt;/p&gt;
    &lt;head rend="h3"&gt;Guess the Word&lt;/head&gt;
    &lt;p&gt;Translate German nouns to English - see a German word with its article and type the English meaning&lt;/p&gt;
    &lt;head rend="h3"&gt;English Nouns to German&lt;/head&gt;
    &lt;p&gt;See an English word and type the German translation with its artikel&lt;/p&gt;
    &lt;head rend="h3"&gt;Verb Conjugation&lt;/head&gt;
    &lt;p&gt;Practice conjugating German verbs in present tense for all persons - ich, du, er/sie/es, wir, ihr, sie/Sie&lt;/p&gt;
    &lt;head rend="h3"&gt;German Verbs to English&lt;/head&gt;
    &lt;p&gt;See a German verb and type its English meaning - perfect for building vocabulary&lt;/p&gt;
    &lt;head rend="h3"&gt;English Verbs to German&lt;/head&gt;
    &lt;p&gt;See an English verb meaning and type the German infinitive form - reverse translation practice&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45745566</guid><pubDate>Wed, 29 Oct 2025 11:50:58 +0000</pubDate></item><item><title>Berkeley Out-of-Order RISC-V Processor (Boom) (2020)</title><link>https://docs.boom-core.org/en/latest/sections/intro-overview/boom.html</link><description>&lt;doc fingerprint="f35afe91c75d4326"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Berkeley Out-of-Order Machine (BOOM)√Ç¬∂&lt;/head&gt;
    &lt;p&gt;The Berkeley Out-of-Order Machine (BOOM) is heavily inspired by the MIPS R10000 [1] and the Alpha 21264 [2] out√¢of√¢order processors. Like the MIPS R10000 and the Alpha 21264, BOOM is a unified physical register file design (also known as √¢explicit register renaming√¢).&lt;/p&gt;
    &lt;p&gt;BOOM implements the open-source RISC-V ISA and utilizes the Chisel hardware construction language to construct generator for the core. A generator can be thought of a generialized RTL design. A standard RTL design can be viewed as a single instance of a generator design. Thus, BOOM is a family of out-of-order designs rather than a single instance of a core. Additionally, to build an SoC with a BOOM core, BOOM utilizes the Rocket Chip SoC generator as a library to reuse different micro-architecture structures (TLBs, PTWs, etc).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;Yeager, Kenneth C. √¢The MIPS R10000 superscalar microprocessor.√¢ IEEE micro 16.2 (1996): 28-41.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;Kessler, Richard E. √¢The alpha 21264 microprocessor.√¢ IEEE micro 19.2 (1999): 24-36.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45745995</guid><pubDate>Wed, 29 Oct 2025 12:33:01 +0000</pubDate></item><item><title>Create your first business email for free</title><link>https://fromzerotollc.com/step/create-your-first-business-email</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45746313</guid><pubDate>Wed, 29 Oct 2025 13:03:52 +0000</pubDate></item><item><title>Grammarly rebrands to 'Superhuman,' launches a new AI assistant</title><link>https://techcrunch.com/2025/10/29/grammarly-rebrands-to-superhuman-launches-a-new-ai-assistant/</link><description>&lt;doc fingerprint="be5e8c1381fae8ec"&gt;
  &lt;main&gt;
    &lt;p&gt;Typically, when a company acquires another, it will absorb the new company‚Äôs branding or integrate it with its own identity. Grammarly is doing something different: After acquiring email client Superhuman in July, the company is renaming itself ‚ÄúSuperhuman.‚Äù&lt;/p&gt;
    &lt;p&gt;Despite the branding change, Grammarly, the product, will continue to be known as it has. However, the company says it is thinking about rebranding products like Coda, a productivity platform it acquired last year, in the long run.&lt;/p&gt;
    &lt;p&gt;The company is also launching an AI assistant called Superhuman Go that‚Äôs built into Grammarly‚Äôs existing extension. The assistant can provide writing suggestions, give feedback on emails, and you can even connect it with other apps like Jira, Gmail, Google Drive and Google Calendar to arm it with more context. The assistant can use these connections to do tasks like logging tickets or fetching your availability when you‚Äôre scheduling a meeting.&lt;/p&gt;
    &lt;p&gt;Superhuman said it plans to add functionality to enable the assistant to fetch data from sources like CRMs and internal systems to suggest changes to your emails.&lt;/p&gt;
    &lt;p&gt;Users can try Superhuman Go by turning on a toggle in the Grammarly extension, which will let them connect it to different apps. Users can also try out different agents in the company‚Äôs agent store, which include a plagiarism checker and a proofreader, launched in August.&lt;/p&gt;
    &lt;p&gt;All Grammarly users can try out Superhuman Go right now, though the company is also selling product bundles. Its Pro subscription plan will cost $12 per month (billed annually) and will enable grammar and tone support in multiple languages. The Business plan will cost $33 per month (billed annually) and will give users access to Superhuman Mail.&lt;/p&gt;
    &lt;p&gt;Superhuman said it also wants to add more AI-powered features to the Coda document suite and Superhuman email clients, such as fetching details from external and internal sources to create additional details in documents and email drafts automatically.&lt;/p&gt;
    &lt;head rend="h3"&gt;TechCrunch Disrupt is live!&lt;/head&gt;
    &lt;head rend="h4"&gt;Join Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla ‚Äî some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don‚Äôt miss 300+ showcasing startups in all sectors.&lt;lb/&gt;Register now and save 50% on your pass.&lt;/head&gt;
    &lt;head rend="h3"&gt;2-FOR-1 DISCOUNT: Bring a +1 and save 60%&lt;/head&gt;
    &lt;head rend="h4"&gt;Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, Vinod Khosla ‚Äî some of the 250+ heavy hitters leading 200+ sessions designed to deliver the insights that fuel startup growth and sharpen your edge. And don‚Äôt miss 300+ showcasing startups in all sectors. Bring a +1 and save 60% on their pass, or get your pass by Oct 27 to save up to $444.&lt;/head&gt;
    &lt;p&gt;Grammarly has for the past few years made a concerted effort to increase its viability as a productivity suite, exemplified through its acquisitions of Coda and Superhuman. With this AI assistant, the company is positioning itself to compete better with the likes of Notion, ClickUp and Google Workspace, which have launched multiple AI-powered features in the past few years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45746401</guid><pubDate>Wed, 29 Oct 2025 13:12:23 +0000</pubDate></item><item><title>From VS Code to Helix</title><link>https://ergaster.org/posts/2025/10/29-vscode-to-helix/</link><description>&lt;doc fingerprint="7708325733a03a9d"&gt;
  &lt;main&gt;
    &lt;p&gt;I created the website you‚Äôre reading with VS Code. Behind the scenes I use Astro, a static site generator that gets out of the way while providing nice conveniences.&lt;/p&gt;
    &lt;p&gt;Using VS Code was a no-brainer: everyone in the industry seems to at least be familiar with it, every project can be opened with it, and most projects can get enhancements and syntactic helpers in a few clicks. In short: VS Code is free, easy to use, and widely adopted.&lt;/p&gt;
    &lt;p&gt;A Rustacean colleague kept singing Helix‚Äôs praises. I discarded it because he‚Äôs much smarter than I am, and I only ever use vim when I need to fiddle with files on a server. I like when things ‚ÄúJust Work‚Äù and didn‚Äôt want to bother learning how to use Helix nor how to configure it.&lt;/p&gt;
    &lt;p&gt;Today it has become my daily driver. Why did I change my mind? What was preventing me from using it before? And how difficult was it to get there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Automation is a double-edged sword&lt;/head&gt;
    &lt;p&gt;Automation and technology make work easier, this is why we produce technology in the first place. But it also means you grow more dependent on the tech you use. If the tech is produced transparently by an international team or a team you trust, it‚Äôs fine. But if it‚Äôs produced by a single large entity that can screw you over, it‚Äôs dangerous.&lt;/p&gt;
    &lt;p&gt;VS Code might be open source, but in practice it‚Äôs produced by Microsoft. Microsoft has a problematic relationship to consent and is shoving AI products down everyone‚Äôs throat. I‚Äôd rather use tools that respect me and my decisions, and I‚Äôd rather not get my tools produced by already monopolistic organizations.&lt;/p&gt;
    &lt;p&gt;Microsoft is also based in the USA, and the political climate over there makes me want to depend as little as possible on American tools. I know that‚Äôs a long, uphill battle, but we have to start somewhere.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not advocating for a ban against American tech in general, but for more balance in our supply chain. I‚Äôm also not advocating for European tech either: I‚Äôd rather get open source tools from international teams competing in a race to the top, rather than from teams in a single jurisdiction. What is happening in the USA could happen in Europe too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I feared using Helix&lt;/head&gt;
    &lt;p&gt;I‚Äôve never found vim particularly pleasant to use but it‚Äôs everywhere, so I figured I might just get used to it. But one of the things I never liked about vim is the number of moving pieces. By default, vim and neovim are very bare bones. They can be extended and completely modified with plugins, but I really don‚Äôt like the idea of having extremely customize tools.&lt;/p&gt;
    &lt;p&gt;I‚Äôd rather have the same editor as everyone else, with a few knobs for minor preferences. I am subject to choice paralysis, so making me configure an editor before I‚Äôve even started editing is the best way to tank my productivity.&lt;/p&gt;
    &lt;p&gt;When my colleague told me about Helix, two things struck me as improvements over vim.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Helix‚Äôs philosophy is that everything should work out of the box. There are a few configs and themes, but everything should work similarly from one Helix to another. All the language-specific logic is handled in Language Servers that implement the Language Server Protocol standard.&lt;/item&gt;
      &lt;item&gt;In Helix, first you select text, and then you perform operations onto it. So you can visually tell what is going to be changed before you apply the change. It fits my mental model much better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there are major drawbacks to Helix too:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;After decades of vim, I was scared to re-learn everything. In practice this wasn‚Äôt a problem at all because of the very visual way Helix works.&lt;/item&gt;
      &lt;item&gt;VS Code ‚ÄúJust Works‚Äù, and Helix sounded like more work than the few clicks from VS Code‚Äôs extension store. This is true, but not as bad as I had anticipated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After a single week of usage, Helix was already very comfortable to navigate. After a few weeks, most of the wrinkles have been ironed out and I use it as my primary editor. So how did I overcome those fears?&lt;/p&gt;
    &lt;head rend="h2"&gt;What Helped&lt;/head&gt;
    &lt;head rend="h3"&gt;Just Do It&lt;/head&gt;
    &lt;p&gt;I tried Helix. It can sound silly, but the very first step to get into Helix was not to overthink it. I just installed it on my mac with &lt;code&gt;brew install helix&lt;/code&gt; and gave it a go. I was not too familiar with it, so I looked up the official documentation and noticed there was a tutorial.&lt;/p&gt;
    &lt;p&gt;This tutorial alone is what convinced me to try harder. It‚Äôs an interactive and well written way to learn how to move and perform basic operations in Helix. I quickly learned how to move around, select things, surround them with braces or parenthesis. I could see what I was about to do before doing it. This has been epiphany. Helix just worked the way I wanted.&lt;/p&gt;
    &lt;p&gt;Better: I could get things done faster than in VS Code after a few minutes of learning. Being a lazy person, I never bothered looking up VS Code shortcuts. Because the learning curve for Helix is slightly steeper, you have to learn those shortcuts that make moving around feel so easy.&lt;/p&gt;
    &lt;p&gt;Not only did I quickly get used to Helix key bindings: my vim muscle-memory didn‚Äôt get in the way at all!&lt;/p&gt;
    &lt;head rend="h3"&gt;Better docs&lt;/head&gt;
    &lt;p&gt;The built-in tutorial is a very pragmatic way to get started. You get results fast, you learn hands on, and it‚Äôs not that long. But if you want to go further, you have to look for docs. Helix has officials docs. They seem to be fairly complete, but they‚Äôre also impenetrable as a new user. They focus on what the editor supports and not on what I will want to do with it.&lt;/p&gt;
    &lt;p&gt;After a bit of browsing online, I‚Äôve stumbled upon this third-party documentation website. The domain didn‚Äôt inspire me a lot of confidence, but the docs are really good. They are clearly laid out, use-case oriented, and they make the most of Astro Starlight to provide a great reading experience. The author tried to upstream these docs, but that won‚Äôt happen. It looks like they are upstreaming their docs to the current website. I hope this will improve the quality of upstream docs eventually.&lt;/p&gt;
    &lt;p&gt;After learning the basics and finding my way through the docs, it was time to ensure Helix was set up to help me where I needed it most.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting the most of Markdown and Astro in Helix&lt;/head&gt;
    &lt;p&gt;In my free time, I mostly use my editor for three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write notes in markdown&lt;/item&gt;
      &lt;item&gt;Tweak my website with Astro&lt;/item&gt;
      &lt;item&gt;Edit yaml to faff around my Kubernetes cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Helix is a ‚Äústupid‚Äù text editor. It doesn‚Äôt know much about what you‚Äôre typing. But it supports Language Servers that implement the Language Server Protocol. Language Servers understand the document you‚Äôre editing. They explain to Helix what you‚Äôre editing, whether you‚Äôre in a TypeScript function, typing a markdown link, etc. With that information, Helix and the Language Server can provide code completion hints, errors &amp;amp; warnings, and easier navigation in your code.&lt;/p&gt;
    &lt;p&gt;In addition to Language Servers, Helix also supports plugging code formatters. Those are pieces of software that will read the document and ensure that it is consistently formatted. It will check that all indentations use spaces and not tabs, that there is a consistent number of space when indenting, that brackets are on the same line as the function, etc. In short: it will make the code pretty.&lt;/p&gt;
    &lt;head rend="h3"&gt;Markdown&lt;/head&gt;
    &lt;p&gt;Markdown is not really a programming language, so it might seem surprising to configure a Language Server for it. But if you remember what we said earlier, Language Servers can provide code completion, which is useful when creating links for example. Marksman does exactly that!&lt;/p&gt;
    &lt;p&gt;Since Helix is pre-configured to use marksman for markdown files we only need to install marksman and make sure it‚Äôs in our &lt;code&gt;PATH&lt;/code&gt;. Installing it with homebrew is enough.&lt;/p&gt;
    &lt;p&gt;We can check that Helix is happy with it with the following command&lt;/p&gt;
    &lt;p&gt;But Language Servers can also help Helix display errors and warnings, and ‚Äúcode suggestions‚Äù to help fix the issues. It means Language Servers are a perfect fit for‚Ä¶ grammar checkers! Several grammar checkers exist. The most notable are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;LTEX+, the Language Server used by Language Tool. It supports several languages must is quite resource hungry.&lt;/item&gt;
      &lt;item&gt;Harper, a grammar checker Language Server developed by Automattic, the people behind WordPress, Tumblr, WooCommerce, Beeper and more. Harper only support English and its variants, but they intend to support more languages in the future.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I mostly write in English and want to keep a minimalistic setup. Automattic is well funded, and I‚Äôm confident they will keep working on Harper to improve it. Since grammar checker LSPs can easily be changed, I‚Äôve decided to go with Harper for now.&lt;/p&gt;
    &lt;p&gt;To install it, homebrew does the job as always:&lt;/p&gt;
    &lt;p&gt;Then I edited my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to add Harper as a secondary Language Server in addition to marksman&lt;/p&gt;
    &lt;p&gt;Finally I can add a markdown linter to ensure my markdown is formatted properly. Several options exist, and markdownlint is one of the most popular. My colleagues recommended the new kid on the block, a Blazing Fast equivalent: rumdl.&lt;/p&gt;
    &lt;p&gt;Installing rumdl was pretty simple on my mac. I only had to add the repository of the maintainer, and install rumdl from it.&lt;/p&gt;
    &lt;p&gt;After that I added a new &lt;code&gt;language-server&lt;/code&gt; to my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; and added it to the language servers to use for the markdown &lt;code&gt;language&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Since my website already contained a &lt;code&gt;.markdownlint.yaml&lt;/code&gt; I could import it to the rumdl format with&lt;/p&gt;
    &lt;p&gt;You might have noticed that I‚Äôve added a little quality of life improvement: soft-wrap at 80 characters.&lt;/p&gt;
    &lt;p&gt;Now if you add this to your own &lt;code&gt;config.toml&lt;/code&gt; you will notice that the text is completely left aligned. This is not a problem on small screens, but it rapidly gets annoying on wider screens.&lt;/p&gt;
    &lt;p&gt;Helix doesn‚Äôt support centering the editor. There is a PR tackling the problem but it has been stale for most of the year. The maintainers are overwhelmed by the number of PRs making it their way, and it‚Äôs not clear if or when this PR will be merged.&lt;/p&gt;
    &lt;p&gt;In the meantime, a workaround exists, with a few caveats. It is possible to add spaces to the left gutter (the column with the line numbers) so it pushes the content towards the center of the screen.&lt;/p&gt;
    &lt;p&gt;To figure out how many spaces are needed, you need to get your terminal width with &lt;code&gt;stty&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;In my case, when in full screen, my terminal is 243 characters wide. I need to remove the content column with from it, and divide everything by 2 to get the space needed on each side. In my case for a 243 character wide terminal with a text width of 80 characters:&lt;/p&gt;
    &lt;p&gt;As is, I would add 203 spaces to my left gutter to push the rest of the gutter and the content to the right. But the gutter itself has a width of 4 characters, that I need to remove from the total. So I need to subtract them from the total, which leaves me with &lt;code&gt;76&lt;/code&gt; characters to add.&lt;/p&gt;
    &lt;p&gt;I can open my &lt;code&gt;~/.config/helix/config.toml&lt;/code&gt; to add a new key binding that will automatically add or remove those spaces from the left gutter when needed, to shift the content towards the center.&lt;/p&gt;
    &lt;p&gt;Now when in normal mode, pressing Space then t then z will add/remove the spaces. Of course this workaround only works when the terminal runs in full screen mode.&lt;/p&gt;
    &lt;head rend="h3"&gt;Astro&lt;/head&gt;
    &lt;p&gt;Astro works like a charm in VS Code. The team behind it provides a Language Server and a TypeScript plugin to enable code completion and syntax highlighting.&lt;/p&gt;
    &lt;p&gt;I only had to install those globally with&lt;/p&gt;
    &lt;p&gt;Now we need to add a few lines to our &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt; to tell it how to use the language server&lt;/p&gt;
    &lt;p&gt;We can check that the Astro Language Server can be used by helix with&lt;/p&gt;
    &lt;p&gt;I also like to get a formatter to automatically make my code consistent and pretty for me when I save a file. One of the most popular code formaters out there is Prettier. I‚Äôve decided to go with the fast and easy formatter dprint instead.&lt;/p&gt;
    &lt;p&gt;I installed it with&lt;/p&gt;
    &lt;p&gt;Then in the projects I want to use dprint in, I do&lt;/p&gt;
    &lt;p&gt;I might edit the &lt;code&gt;dprint.json&lt;/code&gt; file to my liking. Finally, I configure Helix to use dprint globally for all Astro projects by appending a few lines in my &lt;code&gt;~/.config/helix/languages.toml&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;One final check, and I can see that Helix is ready to use the formatter as well&lt;/p&gt;
    &lt;head rend="h3"&gt;YAML&lt;/head&gt;
    &lt;p&gt;For yaml, it‚Äôs simple and straightforward: Helix is preconfigured to use &lt;code&gt;yaml-language-server&lt;/code&gt; as soon as it‚Äôs in the PATH. I just need to install it with&lt;/p&gt;
    &lt;head rend="h2"&gt;Is it worth it?&lt;/head&gt;
    &lt;p&gt;Helix really grew on me. I find it particularly easy and fast to edit code with it. It takes a tiny bit more work to get the language support than it does in VS Code, but it‚Äôs nothing insurmountable. There is a slightly steeper learning curve than for VS Code, but I consider it to be a good thing. It forced me to learn how to move around and edit efficiently, because there is no way to do it inefficiently. Helix remains intuitive once you‚Äôve learned the basics.&lt;/p&gt;
    &lt;p&gt;I am a GNOME enthusiast, and I adhere to the same principles: I like when my apps work out of the box, and when I have little to do to configure them. This is a strong stance that often attracts a vocal opposition. I like products that follow those principles better than those who don‚Äôt.&lt;/p&gt;
    &lt;p&gt;With that said, Helix sometimes feels like it is maintained by one or two people who have a strong vision, but who struggle to onboard more maintainers. As of writing, Helix has more than 350 PRs open. Quite a few bring interesting features, but the maintainers don‚Äôt have enough time to review them.&lt;/p&gt;
    &lt;p&gt;Those 350 PRs mean there is a lot of energy and goodwill around the project. People are willing to contribute. Right now, all that energy is gated, resulting in frustration both from the contributors who feel like they‚Äôre working in the void, and the maintainers who feel like there at the receiving end of a fire hose.&lt;/p&gt;
    &lt;p&gt;A solution to make everyone happier without sacrificing the quality of the project would be to work on a Contributor Ladder. CHAOSS‚Äô Dr Dawn Foster published a blog post about it, listing interesting resources at the end.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45746478</guid><pubDate>Wed, 29 Oct 2025 13:19:30 +0000</pubDate></item><item><title>Israel demanded Google and Amazon use secret 'wink' to sidestep legal orders</title><link>https://www.theguardian.com/us-news/2025/oct/29/google-amazon-israel-contract-secret-code</link><description>&lt;doc fingerprint="3f28bb48b4a475c0"&gt;
  &lt;main&gt;
    &lt;p&gt;When Google and Amazon negotiated a major $1.2bn cloud-computing deal in 2021, their customer ‚Äì the Israeli government ‚Äì had an unusual demand: agree to use a secret code as part of an arrangement that would become known as the ‚Äúwinking mechanism‚Äù.&lt;/p&gt;
    &lt;p&gt;The demand, which would require Google and Amazon to effectively sidestep legal obligations in countries around the world, was born out of Israel‚Äôs concerns that data it moves into the global corporations‚Äô cloud platforms could end up in the hands of foreign law enforcement authorities.&lt;/p&gt;
    &lt;p&gt;Like other big tech companies, Google and Amazon‚Äôs cloud businesses routinely comply with requests from police, prosecutors and security services to hand over customer data to assist investigations.&lt;/p&gt;
    &lt;p&gt;This process is often cloaked in secrecy. The companies are frequently gagged from alerting the affected customer their information has been turned over. This is either because the law enforcement agency has the power to demand this or a court has ordered them to stay silent.&lt;/p&gt;
    &lt;p&gt;For Israel, losing control of its data to authorities overseas was a significant concern. So to deal with the threat, officials created a secret warning system: the companies must send signals hidden in payments to the Israeli government, tipping it off when it has disclosed Israeli data to foreign courts or investigators.&lt;/p&gt;
    &lt;p&gt;To clinch the lucrative contract, Google and Amazon agreed to the so-called winking mechanism, according to leaked documents seen by the Guardian, as part of a joint investigation with Israeli-Palestinian publication +972 Magazine and Hebrew-language outlet Local Call.&lt;/p&gt;
    &lt;p&gt;Based on the documents and descriptions of the contract by Israeli officials, the investigation reveals how the companies bowed to a series of stringent and unorthodox ‚Äúcontrols‚Äù contained within the 2021 deal, known as Project Nimbus. Both Google and Amazon‚Äôs cloud businesses have denied evading any legal obligations.&lt;/p&gt;
    &lt;p&gt;The strict controls include measures that prohibit the US companies from restricting how an array of Israeli government agencies, security services and military units use their cloud services. According to the deal‚Äôs terms, the companies cannot suspend or withdraw Israel‚Äôs access to its technology, even if it‚Äôs found to have violated their terms of service.&lt;/p&gt;
    &lt;p&gt;Israeli officials inserted the controls to counter a series of anticipated threats. They feared Google or Amazon might bow to employee or shareholder pressure and withdraw Israel‚Äôs access to its products and services if linked to human rights abuses in the occupied Palestinian territories.&lt;/p&gt;
    &lt;p&gt;They were also concerned the companies could be vulnerable to overseas legal action, particularly in cases relating to the use of the technology in the military occupation of the West Bank and Gaza.&lt;/p&gt;
    &lt;p&gt;The terms of the Nimbus deal would appear to prohibit Google and Amazon from the kind of unilateral action taken by Microsoft last month, when it disabled the Israeli military‚Äôs access to technology used to operate an indiscriminate surveillance system monitoring Palestinian phone calls.&lt;/p&gt;
    &lt;p&gt;Microsoft, which provides a range of cloud services to Israel‚Äôs military and public sector, bid for the Nimbus contract but was beaten by its rivals. According to sources familiar with negotiations, Microsoft‚Äôs bid suffered as it refused to accept some of Israel‚Äôs demands.&lt;/p&gt;
    &lt;p&gt;As with Microsoft, Google and Amazon‚Äôs cloud businesses have faced scrutiny in recent years over the role of their technology ‚Äì and the Nimbus contract in particular ‚Äì in Israel‚Äôs two-year war on Gaza.&lt;/p&gt;
    &lt;p&gt;During its offensive in the territory, where a UN commission of inquiry concluded that Israel has committed genocide, the Israeli military has relied heavily on cloud providers to store and analyse large volumes of data and intelligence information.&lt;/p&gt;
    &lt;p&gt;One such dataset was the vast collection of intercepted Palestinian calls that until August was stored on Microsoft‚Äôs cloud platform. According to intelligence sources, the Israeli military planned to move the data to Amazon Web Services (AWS) datacentres.&lt;/p&gt;
    &lt;p&gt;Amazon did not respond to the Guardian‚Äôs questions about whether it knew of Israel‚Äôs plan to migrate the mass surveillance data to its cloud platform. A spokesperson for the company said it respected ‚Äúthe privacy of our customers and we do not discuss our relationship without their consent, or have visibility into their workloads‚Äù stored in the cloud.&lt;/p&gt;
    &lt;p&gt;Asked about the winking mechanism, both Amazon and Google denied circumventing legally binding orders. ‚ÄúThe idea that we would evade our legal obligations to the US government as a US company, or in any other country, is categorically wrong,‚Äù a Google spokesperson said.&lt;/p&gt;
    &lt;p&gt;Referring to statements Google has previously made claiming Israel had agreed to abide by Google policies, the spokesperson added: ‚ÄúWe‚Äôve been very clear about the Nimbus contract, what it‚Äôs directed to, and the terms of service and acceptable use policy that govern it. Nothing has changed. This appears to be yet another attempt to falsely imply otherwise.‚Äù&lt;/p&gt;
    &lt;p&gt;However, according to the Israeli government documents detailing the controls inserted into the Nimbus agreement, officials concluded they had extracted important concessions from Google and Amazon after the companies agreed to adapt internal processes and ‚Äúsubordinate‚Äù their standard contractual terms in favour of Israel‚Äôs demands.&lt;/p&gt;
    &lt;p&gt;A government memo circulated several months after the deal was signed stated: ‚Äú[The companies] understand the sensitivities of the Israeli government and are willing to accept our requirements.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;How the secret code works&lt;/head&gt;
    &lt;p&gt;Named after the towering cloud formations, the Nimbus contract ‚Äì which runs for an initial seven years with the possibility of extension ‚Äì is a flagship Israeli government initiative to store information from across the public sector and military in commercially owned datacentres.&lt;/p&gt;
    &lt;p&gt;Even though its data would be stored in Google and Amazon‚Äôs newly built Israel-based datacentres, Israeli officials feared developments in US and European laws could create more direct routes for law enforcement agencies to obtain it via direct requests or court-issued subpoenas.&lt;/p&gt;
    &lt;p&gt;With this threat in mind, Israeli officials inserted into the Nimbus deal a requirement for the companies to a send coded message ‚Äì a ‚Äúwink‚Äù ‚Äì to its government, revealing the identity of the country they had been compelled to hand over Israeli data to, but were gagged from saying so.&lt;/p&gt;
    &lt;p&gt;Leaked documents from Israel‚Äôs finance ministry, which include a finalised version of the Nimbus agreement, suggest the secret code would take the form of payments ‚Äì referred to as ‚Äúspecial compensation‚Äù ‚Äì made by the companies to the Israeli government.&lt;/p&gt;
    &lt;p&gt;According to the documents, the payments must be made ‚Äúwithin 24 hours of the information being transferred‚Äù and correspond to the telephone dialing code of the foreign country, amounting to sums between 1,000 and 9,999 shekels.&lt;/p&gt;
    &lt;p&gt;Under the terms of the deal, the mechanism works like this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;If either Google or Amazon provides information to authorities in the US, where the dialing code is +1, and they are prevented from disclosing their cooperation, they must send the Israeli government 1,000 shekels.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If, for example, the companies receive a request for Israeli data from authorities in Italy, where the dialing code is +39, they must send 3,900 shekels.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If the companies conclude the terms of a gag order prevent them from even signaling which country has received the data, there is a backstop: the companies must pay 100,000 shekels ($30,000) to the Israeli government.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Legal experts, including several former US prosecutors, said the arrangement was highly unusual and carried risks for the companies as the coded messages could violate legal obligations in the US, where the companies are headquartered, to keep a subpoena secret.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt seems awfully cute and something that if the US government or, more to the point, a court were to understand, I don‚Äôt think they would be particularly sympathetic,‚Äù a former US government lawyer said.&lt;/p&gt;
    &lt;p&gt;Several experts described the mechanism as a ‚Äúclever‚Äù workaround that could comply with the letter of the law but not its spirit. ‚ÄúIt‚Äôs kind of brilliant, but it‚Äôs risky,‚Äù said a former senior US security official.&lt;/p&gt;
    &lt;p&gt;Israeli officials appear to have acknowledged this, documents suggest. Their demands about how Google and Amazon respond to a US-issued order ‚Äúmight collide‚Äù with US law, they noted, and the companies would have to make a choice between ‚Äúviolating the contract or violating their legal obligations‚Äù.&lt;/p&gt;
    &lt;p&gt;Neither Google nor Amazon responded to the Guardian‚Äôs questions about whether they had used the secret code since the Nimbus contract came into effect.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe have a rigorous global process for responding to lawful and binding orders for requests related to customer data,‚Äù Amazon‚Äôs spokesperson said. ‚ÄúWe do not have any processes in place to circumvent our confidentiality obligations on lawfully binding orders.‚Äù&lt;/p&gt;
    &lt;p&gt;Google declined to comment on which of Israel‚Äôs stringent demands it had accepted in the completed Nimbus deal, but said it was ‚Äúfalse‚Äù to ‚Äúimply that we somehow were involved in illegal activity, which is absurd‚Äù.&lt;/p&gt;
    &lt;p&gt;A spokesperson for Israel‚Äôs finance ministry said: ‚ÄúThe article‚Äôs insinuation that Israel compels companies to breach the law is baseless.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;‚ÄòNo restrictions‚Äô&lt;/head&gt;
    &lt;p&gt;Israeli officials also feared a scenario in which its access to the cloud providers‚Äô technology could be blocked or restricted.&lt;/p&gt;
    &lt;p&gt;In particular, officials worried that activists and rights groups could place pressure on Google and Amazon, or seek court orders in several European countries, to force them to terminate or limit their business with Israel if their technology were linked to human rights violations.&lt;/p&gt;
    &lt;p&gt;To counter the risks, Israel inserted controls into the Nimbus agreement which Google and Amazon appear to have accepted, according to government documents prepared after the deal was signed.&lt;/p&gt;
    &lt;p&gt;The documents state that the agreement prohibits the companies from revoking or restricting Israel‚Äôs access to their cloud platforms, either due to changes in company policy or because they find Israel‚Äôs use of their technology violates their terms of service.&lt;/p&gt;
    &lt;p&gt;Provided Israel does not infringe on copyright or resell the companies‚Äô technology, ‚Äúthe government is permitted to make use of any service that is permitted by Israeli law‚Äù, according to a finance ministry analysis of the deal.&lt;/p&gt;
    &lt;p&gt;Both companies‚Äô standard ‚Äúacceptable use‚Äù policies state their cloud platforms should not be used to violate the legal rights of others, nor should they be used to engage in or encourage activities that cause ‚Äúserious harm‚Äù to people.&lt;/p&gt;
    &lt;p&gt;However, according to an Israeli official familiar with the Nimbus project, there can be ‚Äúno restrictions‚Äù on the kind of information moved into Google and Amazon‚Äôs cloud platforms, including military and intelligence data. The terms of the deal seen by the Guardian state that Israel is ‚Äúentitled to migrate to the cloud or generate in the cloud any content data they wish‚Äù.&lt;/p&gt;
    &lt;p&gt;Israel inserted the provisions into the deal to avoid a situation in which the companies ‚Äúdecide that a certain customer is causing them damage, and therefore cease to sell them services‚Äù, one document noted.&lt;/p&gt;
    &lt;p&gt;The Intercept reported last year the Nimbus project was governed by an ‚Äúamended‚Äù set of confidential policies, and cited a leaked internal report suggesting Google understood it would not be permitted to restrict the types of services used by Israel.&lt;/p&gt;
    &lt;p&gt;Last month, when Microsoft cut off Israeli access to some cloud and artificial intelligence services, it did so after confirming reporting by the Guardian and its partners, +972 and Local Call, that the military had stored a vast trove of intercepted Palestinian calls in the company‚Äôs Azure cloud platform.&lt;/p&gt;
    &lt;p&gt;Notifying the Israeli military of its decision, Microsoft said that using Azure in this way violated its terms of service and it was ‚Äúnot in the business of facilitating the mass surveillance of civilians‚Äù.&lt;/p&gt;
    &lt;p&gt;Under the terms of the Nimbus deal, Google and Amazon are prohibited from taking such action as it would ‚Äúdiscriminate‚Äù against the Israeli government. Doing so would incur financial penalties for the companies, as well as legal action for breach of contract.&lt;/p&gt;
    &lt;p&gt;The Israeli finance ministry spokesperson said Google and Amazon are ‚Äúbound by stringent contractual obligations that safeguard Israel‚Äôs vital interests‚Äù. They added: ‚ÄúThese agreements are confidential and we will not legitimise the article‚Äôs claims by disclosing private commercial terms.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45746482</guid><pubDate>Wed, 29 Oct 2025 13:20:03 +0000</pubDate></item><item><title>New attacks are diluting secure enclave defenses from Nvidia, AMD, and Intel</title><link>https://arstechnica.com/security/2025/10/new-physical-attacks-are-quickly-diluting-secure-enclave-defenses-from-nvidia-amd-and-intel/</link><description>&lt;doc fingerprint="b2b31958f9265ad6"&gt;
  &lt;main&gt;
    &lt;p&gt;Trusted execution environments, or TEEs, are everywhere‚Äîin blockchain architectures, virtually every cloud service, and computing involving AI, finance, and defense contractors. It‚Äôs hard to overstate the reliance that entire industries have on three TEEs in particular: Confidential Compute from Nvidia, SEV-SNP from AMD, and SGX and TDX from Intel. All three come with assurances that confidential data and sensitive computing can‚Äôt be viewed or altered, even if a server has suffered a complete compromise of the operating kernel.&lt;/p&gt;
    &lt;p&gt;A trio of novel physical attacks raises new questions about the true security offered by these TEES and the exaggerated promises and misconceptions coming from the big and small players using them.&lt;/p&gt;
    &lt;p&gt;The most recent attack, released Tuesday, is known as TEE.fail. It defeats the latest TEE protections from all three chipmakers. The low-cost, low-complexity attack works by placing a small piece of hardware between a single physical memory chip and the motherboard slot it plugs into. It also requires the attacker to compromise the operating system kernel. Once this three-minute attack is completed, Confidential Compute, SEV-SNP, and TDX/SDX can no longer be trusted. Unlike the Battering RAM and Wiretap attacks from last month‚Äîwhich worked only against CPUs using DDR4 memory‚ÄîTEE.fail works against DDR5, allowing them to work against the latest TEEs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some terms apply&lt;/head&gt;
    &lt;p&gt;All three chipmakers exclude physical attacks from threat models for their TEEs, also known as secure enclaves. Instead, assurances are limited to protecting data and execution from viewing or tampering, even when the kernel OS running the processor has been compromised. None of the chipmakers make these carveouts prominent, and they sometimes provide confusing statements about the TEE protections offered.&lt;/p&gt;
    &lt;p&gt;Many users of these TEEs make public assertions about the protections that are flat-out wrong, misleading, or unclear. All three chipmakers and many TEE users focus on the suitability of the enclaves for protecting servers on a network edge, which are often located in remote locations, where physical access is a top threat.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45746753</guid><pubDate>Wed, 29 Oct 2025 13:44:31 +0000</pubDate></item></channel></rss>