<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 06 Nov 2025 19:08:40 +0000</lastBuildDate><item><title>Ratatui ‚Äì App Showcase</title><link>https://ratatui.rs/showcase/apps/</link><description>&lt;doc fingerprint="74cad9da50b51b8f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;App Showcase&lt;/head&gt;
    &lt;p&gt;Atuin replaces your existing shell history with a SQLite database, and records additional context for your commands.&lt;/p&gt;
    &lt;p&gt;This is a CLI utility for displaying current network utilization by process, connection and remote IP/hostname&lt;/p&gt;
    &lt;p&gt;Perform binary analysis in your terminal.&lt;/p&gt;
    &lt;p&gt;A customizable cross-platform graphical process/system monitor for the terminal&lt;/p&gt;
    &lt;p&gt;Play crossword puzzles in your terminal.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;csvlens&lt;/code&gt; is A command line CSV file viewer. It is like less but made for CSV.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;dua&lt;/code&gt; is a disk space analysis tool designed for speed, leveraging parallel processing to quickly
provide detailed disk usage information and allowing for faster deletion of unnecessary data
compared to the standard ‚Äòrm‚Äô command.&lt;/p&gt;
    &lt;p&gt;A command line tool that executes make target using fuzzy finder with preview window&lt;/p&gt;
    &lt;p&gt;TUI for git written in rust&lt;/p&gt;
    &lt;p&gt;gpg-tui is a Terminal User Interface for GnuPG.&lt;/p&gt;
    &lt;p&gt;Ranger-like terminal file manager written in Rust&lt;/p&gt;
    &lt;p&gt;A material design color palette for the terminal.&lt;/p&gt;
    &lt;p&gt;A mine sweeping game written in Rust&lt;/p&gt;
    &lt;p&gt;Oatmeal is a terminal UI chat application that speaks with LLMs, complete with slash commands and fancy chat bubbles. It features agnostic backends to allow switching between the powerhouse of ChatGPT, or keeping things private with Ollama. While Oatmeal works great as a stand alone terminal application, it works even better paired with an editor like Neovim!&lt;/p&gt;
    &lt;p&gt;oha is a tiny program that sends some load to a web application and show realtime tui&lt;/p&gt;
    &lt;p&gt;A simple TUI to view &amp;amp; control docker containers&lt;/p&gt;
    &lt;p&gt;Unlock the power of APIs with simplicity and speed, right from your terminal. View OpenAPI documentations in your terminal.&lt;/p&gt;
    &lt;p&gt;A lightweight and terminal-based tool for interacting with databases.&lt;/p&gt;
    &lt;p&gt;An application to manage markdown notes from your terminal and compile them to HTML&lt;/p&gt;
    &lt;p&gt;A simple oscilloscope/vectorscope/spectroscope for your terminal&lt;/p&gt;
    &lt;p&gt;Terminal HTTP/REST client&lt;/p&gt;
    &lt;p&gt;A CLI-based AI coding agent for local dev, scripts/CI, and automation.&lt;/p&gt;
    &lt;p&gt;A terminal user interface for taskwarrior&lt;/p&gt;
    &lt;p&gt;Television is a fast and versatile fuzzy finder TUI.&lt;/p&gt;
    &lt;p&gt;It lets you quickly search through any kind of data source (files, git repositories, environment variables, docker images, you name it) using a fuzzy matching algorithm and is designed to be easily extensible.&lt;/p&gt;
    &lt;p&gt;A network diagnostic tool that combines the functionality of traceroute and ping and is designed to assist with the analysis of networking issues.&lt;/p&gt;
    &lt;p&gt;A hackable, minimal, fast TUI file explorer&lt;/p&gt;
    &lt;p&gt;Blazing fast terminal file manager written in Rust, based on async I/O&lt;/p&gt;
    &lt;p&gt;Y≈çzefu is an interactive TUI application for exploring data of a Kafka cluster.&lt;/p&gt;
    &lt;p&gt;It is an alternative tool to AKHQ, Redpanda Console, or the Kafka plugin for JetBrains IDEs. It includes a search query language inspired by SQL, providing fine-grained filtering capabilities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45830829</guid><pubDate>Thu, 06 Nov 2025 02:50:31 +0000</pubDate></item><item><title>I may have found a way to spot U.S. at-sea strikes before they're announced</title><link>https://old.reddit.com/r/OSINT/comments/1opjjyv/i_may_have_found_a_way_to_spot_us_atsea_strikes/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45831541</guid><pubDate>Thu, 06 Nov 2025 04:37:45 +0000</pubDate></item><item><title>How I am deeply integrating Emacs</title><link>https://joshblais.com/blog/how-i-am-deeply-integrating-emacs/</link><description>&lt;doc fingerprint="ae125933290192bc"&gt;
  &lt;main&gt;
    &lt;p&gt;Emacs has holistically become my daily computing environment.&lt;/p&gt;
    &lt;p&gt;My efforts have been focused on building emacs into the workflow of essentially everything I do, as long as it doesn‚Äôt involve heavy video or media, I try my very best to accomplish it in emacs. The idea is to achieve deep integration with everything I do on a computer, to the degree my thoughts are immediately able to be acted upon in the buffer.&lt;/p&gt;
    &lt;p&gt;I use hyprland as my window manager, and while I have heard of other managers/DEs (I was using GNOME for the better part of 6 months), I keep coming back to hyprland just because it works and is easy to configure. Also, for some reason, I seem not to have lagging in emacs on wayland in hyprland, while I had to previously run emacs in X11 mode in GNOME, go figure.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Motivation#&lt;/head&gt;
    &lt;p&gt;I have seen what people are capable of doing when their tools get out of the way, and they are free to just create. This is how world class athletes, musicians, artists, writers, and of course programmers take what is in their mind and translate it into reality. The idea is that if I can learn this ‚Äúeditor of a lifetime‚Äù - then the things that I want to create, the programs I want to write, will be achieved in a near frictionless environment, allowing for velocity that is not possible elsewhere. It is the ultimate sharpening of the axe before chopping the tree.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why not EXWM?#&lt;/head&gt;
    &lt;p&gt;I have considered using EXWM as the window manager (quite literally offloading window management to emacs, and ‚Äúliving in emacs‚Äù - to more of a degree than I do already), the hesitation I have is that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Emacs is single threaded, therefore if anything in the system hangs, the whole system hangs, and&lt;/item&gt;
      &lt;item&gt;It is only X11 where most of the development and forward movement in the linux space has been in wayland. While I understand this is not a tremendous issue, wayland does seem to be where the puck is going.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, what I am aiming to do is replicate functionality as best as I can from EXWM to a wayland environment - not wholly possible, but also not wholly impossible, either.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Emacs Launcher program#&lt;/head&gt;
    &lt;p&gt;If you look at my dotfiles, you can see I have a script written in Go that allows me to call each and every one of my emacs controls anywhere is my system. I was previously calling each of these emacs commands in bash and with a sleep command so as to make sure I was targeting the emacs instance. No longer. This Go script has sped up my workflow by 10x.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Current setup#&lt;/head&gt;
    &lt;head rend="h3"&gt;How I Launch Emacs#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod SHIFT, E, exec, bash -c "emacs"&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;I almost never press this keybind, as emacs is opened from the get-go in my hyprland sessions. For that rare time I need to re-open it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Opening vterm as my default terminal#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, E, exec, emacsclient -n -e '(my/new-frame-with-vterm)'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;This permits me to quickly open a vterm window and enter commands etc. If I need anything that is more graphically intense, I fallback to kitty terminal, but this is less and less these days.&lt;/p&gt;
    &lt;head rend="h4"&gt;Opening vterm in my emacs session quickly for in projects is done like so:#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, RETURN, exec, ~/.config/hypr/scripts/emacs-launcher '(my-open-vterm-at-point)'&lt;/code&gt;
    &lt;/p&gt;
    &lt;head rend="h3"&gt;Universal Launcher#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, SPACE, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (universal-launcher-popup))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;I wanted to replicate a launcher (similar to wofi/rofi) in which I could easily launch apps and switch to them in the environment.&lt;/p&gt;
    &lt;p&gt;So, my take on this is to replace wofi with this functionality. I was using ssh providers in GNOME, but then brought the functionality into my universal launcher. It has effectively grown to encapsulate:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Passwords&lt;/item&gt;
      &lt;item&gt;SSH&lt;/item&gt;
      &lt;item&gt;Bookmarking&lt;/item&gt;
      &lt;item&gt;Commands and program launching&lt;/item&gt;
      &lt;item&gt;Emojis&lt;/item&gt;
      &lt;item&gt;TODOS (though org-agenda/calendar also handles this)&lt;/item&gt;
      &lt;item&gt;File navigation&lt;/item&gt;
      &lt;item&gt;Web and documentation search&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While this is a work in progress, I use it every day, hundreds of times a day, and love the flow &amp;amp; speed my launcher allows.&lt;/p&gt;
    &lt;head rend="h3"&gt;Capture to org mode#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = CTRL SHIFT, c, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (org-capture))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;When I am not ‚Äúin‚Äù emacs (I am always in emacs by extension) I can still capture direct to emacs with a quick keybind.&lt;/p&gt;
    &lt;p&gt;I capture to my org directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;notes&lt;/item&gt;
      &lt;item&gt;bookmarks&lt;/item&gt;
      &lt;item&gt;contacts&lt;/item&gt;
      &lt;item&gt;inbox (todos)&lt;/item&gt;
      &lt;item&gt;events/deadlines&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is very useful when I am wanting to save a thought, idea, bookmark, quote, what have you, and then integrate it with my org-roam file structure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Notes#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod CTRL, N, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (find-file "~/org/notes.org"))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;I can navigate to my notes file very quickly to write emails, keep notes on stuff, and then translate those into my org-roam directory, too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Calendar/Org Agenda#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, C, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (=calendar))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, N, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (my/org-agenda-dashboard))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Quick access to my agenda and calendar from anywhere.&lt;/p&gt;
    &lt;head rend="h3"&gt;Password manager#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, P, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (pass))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Password-store inside emacs to create, update, grab passwords and insert them on page in browsers.&lt;/p&gt;
    &lt;head rend="h3"&gt;File Browsing#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, F, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (dirvish))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;I use dirvish/dired for nearly all my file browsing and manipulation. I have some binds that allow me to pull up thunar for graphical drag-drop operations, but other than that files are dealt with inside emacs.&lt;/p&gt;
    &lt;p&gt;The killer feature is that you can edit files as you would edit text, nothing else comes close.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bookmarks#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, B, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (find-file "~/org/bookmarks.org"))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Bookmarking within emacs allows me to keep all sites top of mind.&lt;/p&gt;
    &lt;head rend="h3"&gt;Email#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod, M, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (=mu4e))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The greatest email client.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feed reader#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod CTRL, Z, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (elfeed))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Reading any feed from around the web, I follow youtube, blogs, news, etc. here - never going out to the web to read anything.&lt;/p&gt;
    &lt;head rend="h3"&gt;Music playing#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod CONTROL, M, exec, ~/.config/hypr/scripts/emacs-launcher '(progn (select-frame-set-input-focus (selected-frame)) (emms-playlist-mode-go))'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;You thought I wouldn‚Äôt play music in emacs?&lt;/p&gt;
    &lt;head rend="h3"&gt;Emacs everywhere for editing text anywhere#&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;bind = $mainMod CONTROL, E, exec, emacsclient --eval '(thanos/type)'&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;When you are in a text box on any site, you can just edit the text in emacs, press &lt;code&gt;C-c C-c&lt;/code&gt; and have it pasted right there for you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will I use EXWM?#&lt;/head&gt;
    &lt;p&gt;I think that because I spend so much time inside emacs, I don‚Äôt really get the benefits of everything being a buffer. I only use a browser for projects, not as a window I have always open, and I don‚Äôt really need emacs to control buffers or give me the keybinds universally. I will never say never though, perhaps one day it will be my window manager of choice.&lt;/p&gt;
    &lt;p&gt;How are you integrating emacs in your workflow? I would be super interested to see other setups that allow you to use emacs as your one, true, holistic computing environment. Shoot me an email and tell me how it‚Äôs done!&lt;/p&gt;
    &lt;p&gt;Edit: this post has some interesting discussion on HackerNews, I will make a video about the workflow today.&lt;/p&gt;
    &lt;p&gt;As always, God bless, and until next time.&lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, consider supporting my work by Buying me a Coffee, Checking out my book, or sending me an email to tell me what you think.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45832341</guid><pubDate>Thu, 06 Nov 2025 07:09:07 +0000</pubDate></item><item><title>Mathematical exploration and discovery at scale</title><link>https://terrytao.wordpress.com/2025/11/05/mathematical-exploration-and-discovery-at-scale/</link><description>&lt;doc fingerprint="a3799b4924933df3"&gt;
  &lt;main&gt;
    &lt;p&gt;Bogdan Georgiev, Javier G√≥mez-Serrano, Adam Zsolt Wagner, and I have uploaded to the arXiv our paper ‚ÄúMathematical exploration and discovery at scale‚Äú. This is a longer report on the experiments we did in collaboration with Google Deepmind with their AlphaEvolve tool, which is in the process of being made available for broader use. Some of our experiments were already reported on in a previous white paper, but the current paper provides more details, as well as a link to a repository with various relevant data such as the prompts used and the evolution of the tool outputs.&lt;/p&gt;
    &lt;p&gt;AlphaEvolve is a variant of more traditional optimization tools that are designed to extremize some given score function over a high-dimensional space of possible inputs. A traditional optimization algorithm might evolve one or more trial inputs over time by various methods, such as stochastic gradient descent, that are intended to locate increasingly good solutions while trying to avoid getting stuck at local extrema. By contrast, AlphaEvolve does not evolve the score function inputs directly, but uses an LLM to evolve computer code (often written in a standard language such as Python) which will in turn be run to generate the inputs that one tests the score function on. This reflects the belief that in many cases, the extremizing inputs will not simply be an arbitrary-looking string of numbers, but will often have some structure that can be efficiently described, or at least approximated, by a relatively short piece of code. The tool then works with a population of relatively successful such pieces of code, with the code from one generation of the population being modified and combined by the LLM based on their performance to produce the next generation. The stochastic nature of the LLM can actually work in one‚Äôs favor in such an evolutionary environment: many ‚Äúhallucinations‚Äù will simply end up being pruned out of the pool of solutions being evolved due to poor performance, but a small number of such mutations can add enough diversity to the pool that one can break out of local extrema and discover new classes of viable solutions. The LLM can also accept user-supplied ‚Äúhints‚Äù as part of the context of the prompt; in some cases, even just uploading PDFs of relevant literature has led to improved performance by the tool. Since the initial release of AlphaEvolve, similar tools have been developed by others, including OpenEvolve, ShinkaEvolve and DeepEvolve.&lt;/p&gt;
    &lt;p&gt;We tested this tool on a large number (67) of different mathematics problems (both solved and unsolved) in analysis, combinatorics, and geometry that we gathered from the literature, and reported our outcomes (both positive and negative) in this paper. In many cases, AlphaEvolve achieves similar results to what an expert user of a traditional optimization software tool might accomplish, for instance in finding more efficient schemes for packing geometric shapes, or locating better candidate functions for some calculus of variations problem, than what was previously known in the literature. But one advantage this tool seems to offer over such custom tools is that of scale, particularly when when studying variants of a problem that we had already tested this tool on, as many of the prompts and verification tools used for one problem could be adapted to also attack similar problems; several examples of this will be discussed below.&lt;/p&gt;
    &lt;p&gt;Another advantage of AlphaEvolve was robustness: it was relatively easy to set up AlphaEvolve to work on a broad array of problems, without extensive need to call on domain knowledge of the specific task in order to tune hyperparameters. In some cases, we found that making such hyperparameters part of the data that AlphaEvolve was prompted to output was better than trying to work out their value in advance, although a small amount of such initial theoretical analysis was helpful. For instance, in calculus of variation problems, one is often faced with the need to specify various discretization parameters in order to estimate a continuous integral, which cannot be computed exactly, by a discretized sum (such as a Riemann sum), which can be evaluated by computer to some desired precision. We found that simply asking AlphaEvolve to specify its own discretization parameters worked quite well (provided we designed the score function to be conservative with regards to the possible impact of the discretization error); see for instance this experiment in locating the best constant in functional inequalities such as the Hausdorff-Young inequality.&lt;/p&gt;
    &lt;p&gt;A third advantage of AlphaEvolve over traditional optimization methods was the interpretability of many of the solutions provided. For instance, in one of our experiments we sought to find an extremum to a functional inequality such as the Gagliardo‚ÄìNirenberg inequality (a variant of the Sobolev inequality). This is a relatively well-behaved optimization problem, and many standard methods can be deployed to obtain near-optimizers that are presented in some numerical format, such as a vector of values on some discretized mesh of the domain. However, when we applied AlphaEvolve to this problem, the tool was able to discover the exact solution (in this case, a Talenti function), and create code that sampled from that function on a discretized mesh to provide the required input for the scoring function we provided (which only accepted discretized inputs, due to the need to compute the score numerically). This code could be inspected by humans to gain more insight as to the nature of the optimizer. (Though in some cases, AlphaEvolve‚Äôs code would contain some brute force search, or a call to some existing optimization subroutine in one of the libraries it was given access to, instead of any more elegant description of its output.)&lt;/p&gt;
    &lt;p&gt;For problems that were sufficiently well-known to be in the training data of the LLM, the LLM component of AlphaEvolve often came up almost immediately with optimal (or near-optimal) solutions. For instance, for variational problems where the gaussian was known to be the extremizer, AlphaEvolve would frequently guess a gaussian candidate during one of the early evolutions, and we would have to obfuscate the problem significantly to try to conceal the connection to the literature in order for AlphaEvolve to experiment with other candidates. AlphaEvolve would also propose similar guesses for other problems for which the extremizer was not known. For instance, we tested this tool on the sum-difference exponents of relevance to the arithmetic Kakeya conjecture, which can be formulated as a variational entropy inequality concerning certain two-dimensional discrete random variables. AlphaEvolve initially proposed some candidates for such variables based on discrete gaussians, which actually worked rather well even if they were not the exact extremizer, and already generated some slight improvements to previous lower bounds on such exponents in the literature. Inspired by this, I was later able to rigorously obtain some theoretical results on the asymptotic behavior on such exponents in the regime where the number of slopes was fixed, but the ‚Äúrational complexity‚Äù of the slopes went to infinity; this will be reported on in a separate paper.&lt;/p&gt;
    &lt;p&gt;Perhaps unsurprisingly, AlphaEvolve was extremely good at locating ‚Äúexploits‚Äù in the verification code we provided, for instance using degenerate solutions or overly forgiving scoring of approximate solutions to come up with proposed inputs that technically achieved a high score under our provided code, but were not in the spirit of the actual problem. For instance, when we asked it (link under construction) to find configurations to extremal geometry problems such as locating polygons with each vertex having four equidistant other vertices, we initially coded the verifier to accept distances that were equal only up to some high numerical precision, at which point AlphaEvolve promptly placed many of the points in virtually the same location so that the distances they determined were indistinguishable. Because of this, a non-trivial amount of human effort needs to go into designing a non-exploitable verifier, for instance by working with exact arithmetic (or interval arithmetic) instead of floating point arithmetic, and taking conservative worst-case bounds in the presence of uncertanties in measurement to determine the score. For instance, in testing AlphaEvolve against the ‚Äúmoving sofa‚Äù problem and its variants, we designed a conservative scoring function that only counted those portions of the sofa that we could definitively prove to stay inside the corridor at all times (not merely the discrete set of times provided by AlphaEvolve to describe the sofa trajectory) to prevent it from exploiting ‚Äúclipping‚Äù type artefacts. Once we did so, it performed quite well, for instance rediscovering the optimal ‚ÄúGerver sofa‚Äù for the original sofa problem, and also discovering new sofa designs for other problem variants, such as a 3D sofa problem.&lt;/p&gt;
    &lt;p&gt;For well-known open conjectures (e.g., Sidorenko‚Äôs conjecture, Sendov‚Äôs conjecture, Crouzeix‚Äôs conjecture, the ovals problem, etc.), AlphaEvolve generally was able to locate the previously known candidates for optimizers (that are conjectured to be optimal), but did not locate any stronger counterexamples: thus, we did not disprove any major open conjecture. Of course, one obvious possible explanation for this is that these conjectures are in fact true; outside of a few situations where there is a matching ‚Äúdual‚Äù optimization problem, AlphaEvolve can only provide one-sided bounds on such problems and so cannot definitively determine if the conjectural optimizers are in fact the true optimizers. Another potential explanation is that AlphaEvolve essentially tried all the ‚Äúobvious‚Äù constructions that previous researchers working on these problems had also privately experimented with, but did not report due to the negative findings. However, I think there is at least value in using these tools to systematically record negative results (roughly speaking, that a search for ‚Äúobvious‚Äù counterexamples to a conjecture did not disprove the claim), which currently only exist as ‚Äúfolklore‚Äù results at best. This seems analogous to the role LLM Deep Research tools could play by systematically recording the results (both positive and negative) of automated literature searches, as a supplement to human literature review which usually reports positive results only. Furthermore, when we shifted attention to less well studied variants of famous conjectures, we were able to find some modest new observations. For instance, while AlphaEvolve only found the standard conjectural extremizer to Sendov‚Äôs conjecture, as well as for variants such as Borcea‚Äôs conjecture, Schmeisser‚Äôs conjecture, or Smale‚Äôs conjecture it did reveal some potential two-parameter extensions to a conjecture of de Bruin and Sharma that had not previously been stated in the literature. (For this problem, we were not directly optimizing some variational scalar quantity, but rather a two-dimensional range of possible values, which we could adapt the AlphaEvolve framework to treat). In the future, I can imagine such tools being a useful ‚Äúsanity check‚Äù when proposing any new conjecture, in that it will become common practice to run one of these tools against such a conjecture to make sure there are no ‚Äúobvious‚Äù counterexamples (while keeping in mind that this is still far from conclusive evidence in favor of such a conjecture).&lt;/p&gt;
    &lt;p&gt;AlphaEvolve did not perform equally well across different areas of mathematics. When testing the tool on analytic number theory problems, such as that of designing sieve weights for elementary approximations to the prime number theorem, it struggled to take advantage of the number theoretic structure in the problem, even when given suitable expert hints (although such hints have proven useful for other problems). This could potentially be a prompting issue on our end, or perhaps the landscape of number-theoretic optimization problems is less amenable to this sort of LLM-based evolutionary approach. On the other hand, AlphaEvolve does seem to do well when the constructions have some algebraic structure, such as with the finite field Kakeya and Nikodym set problems, which we will turn to shortly.&lt;/p&gt;
    &lt;p&gt;For many of our experiments we worked with fixed-dimensional problems, such as trying to optimally pack shapes in a larger shape for a fixed value of . However, we found in some cases that if we asked AlphaEvolve to give code that took parameters such as as input, and tested the output of that code for a suitably sampled set of values of of various sizes, then it could sometimes generalize the constructions it found for small values of this parameter to larger ones; for instance, in the infamous sixth problem of this year‚Äôs IMO, it could use this technique to discover the optimal arrangement of tiles, which none of the frontier models could do at the time (although AlphaEvolve has no capability to demonstrate that this arrangement was, in fact, optimal). Another productive use case of this technique was for finding finite field Kakeya and Nikodym sets of small size in low-dimensional vector spaces over finite fields of various sizes. For Kakeya sets in , it located the known optimal construction based on quadratic residues in two dimensions, and very slightly beat (by an error term of size ) the best construction in three dimensions; this was an algebraic construction (still involving quadratic residues) discovered empirically that we could then prove to be correct by first using Gemini‚Äôs ‚ÄúDeep Think‚Äù tool to locate an informal proof, which we could then convert into a formalized Lean proof by using Google Deepmind‚Äôs ‚ÄúAlphaProof‚Äù tool. At one point we thought it had found a construction in four dimensions which achieved a more noticeable improvement (of order ) of what we thought was the best known construction, but we subsequently discovered that essentially the same construction had appeared already in a paper of Bukh and Chao, although it still led to a more precise calculation of the error term (to accuracy rather than , where the error term now involves the Lang-Weil inequality and is unlikely to have a closed form). Perhaps AlphaEvolve had somehow absorbed the Bukh-Chao construction within its training data to accomplish this. However, when we tested the tool on Nikodym sets (which are expected to have asymptotic density , although this remains unproven), it did find some genuinely new constructions of such sets in three dimensions, based on removing quadratic varieties from the entire space. After using ‚ÄúDeep Think‚Äù again to analyze these constructions, we found that they were inferior to a purely random construction (which in retrospect was an obvious thing to try); however, they did inspire a hybrid construction in which one removed random quadratic varieties and performed some additional cleanup, which ends up outperforming both the purely algebraic and purely random constructions. This result (with completely human-generated proofs) will appear in a subsequent paper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45833162</guid><pubDate>Thu, 06 Nov 2025 09:24:42 +0000</pubDate></item><item><title>Show HN: qqqa ‚Äì A fast, stateless LLM-powered assistant for your shell</title><link>https://github.com/matisojka/qqqa</link><description>&lt;doc fingerprint="4bf485901a512ab"&gt;
  &lt;main&gt;
    &lt;p&gt;Fast, stateless LLM-powered assistant for your shell: qq answers; qa runs commands&lt;/p&gt;
    &lt;p&gt;qqqa is a two-in-one, stateless CLI tool that brings LLM assistance to the command line without ceremony.&lt;/p&gt;
    &lt;p&gt;The two binaries are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;qq&lt;/code&gt;- ask a single question, e.g. "qq how can I recursively list all files in this directory" (qq stands for "quick question")&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;qa&lt;/code&gt;- a single step agent that can optionally use tools to finish a task: read a file, write a file, or execute a command with confirmation (qa stands for "quick agent")&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By default the repo includes profiles for OpenAI and Groq.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo.mp4&lt;/head&gt;
    &lt;p&gt;qq means quick question. qa means quick agent. Both are easy to type rapidly on QWERTY keyboards with minimal finger movement. That makes interacting with LLMs faster and more natural during real work.&lt;/p&gt;
    &lt;p&gt;qqqa is deliberately stateless. There is no long running session and no hidden conversation memory stored by the tool. Every run is independent and reproducible.&lt;/p&gt;
    &lt;p&gt;Why stateless is great:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Simple and focused - Unix philosophy applied to LLM tools.&lt;/item&gt;
      &lt;item&gt;Shell friendly - compose with pipes and files instead of interactive chats.&lt;/item&gt;
      &lt;item&gt;Safe by default - qq is read-only and has access to no tools. qa is built with security in mind and requires confirmation before running tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tools may include transient context you choose to provide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;qq&lt;/code&gt;can include the last few terminal commands as hints and piped stdin if present.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;qa&lt;/code&gt;can read files or run a specific command, but only once per invocation and with safety checks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For fast feedback loops, speed and cost matter. The included &lt;code&gt;groq&lt;/code&gt; profile targets Groq's OpenAI compatible API and the model &lt;code&gt;openai/gpt-oss-20b&lt;/code&gt;. We recommend Groq for really fast inference speed at roughly 1000 tokens per second and at a low price point compared to many alternatives. Set &lt;code&gt;GROQ_API_KEY&lt;/code&gt; and you are ready to go.&lt;/p&gt;
    &lt;p&gt;You can still use OpenAI or any other OpenAI compatible provider by adding a provider entry and a profile in &lt;code&gt;~/.qq/config.json&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI compatible API client with streaming and non streaming calls.&lt;/item&gt;
      &lt;item&gt;Stateless, single shot workflow that plays well with pipes and scripts.&lt;/item&gt;
      &lt;item&gt;Rich but simple formatting using XML like tags rendered to ANSI colors.&lt;/item&gt;
      &lt;item&gt;Config driven providers and profiles with per profile model overrides.&lt;/item&gt;
      &lt;item&gt;Safety rails for file access and command execution.&lt;/item&gt;
      &lt;item&gt;Old-school and SERIOUS? Optional no-emoji mode persisted via &lt;code&gt;--no-fun&lt;/code&gt;ü•∏&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Use the Homebrew tap:&lt;/p&gt;
    &lt;code&gt;brew tap iagooar/qqqa
brew install qqqa&lt;/code&gt;
    &lt;p&gt;Download a prebuilt archive from the GitHub Releases page, extract it, and place &lt;code&gt;qq&lt;/code&gt;/&lt;code&gt;qa&lt;/code&gt; somewhere on your &lt;code&gt;PATH&lt;/code&gt; (e.g., &lt;code&gt;/usr/local/bin&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;On first run qqqa creates &lt;code&gt;~/.qq/config.json&lt;/code&gt; with safe permissions. For a smooth first interaction, run the init flow:&lt;/p&gt;
    &lt;code&gt;# Interactive setup (choose provider and set key)
qq --init
# or
qa --init&lt;/code&gt;
    &lt;p&gt;If &lt;code&gt;~/.qq/config.json&lt;/code&gt; already exists, the init command keeps it untouched and explains how to rerun after moving or deleting the file.&lt;/p&gt;
    &lt;p&gt;The initializer lets you choose the default provider:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Groq + &lt;code&gt;openai/gpt-oss-20b&lt;/code&gt;(faster, cheaper)&lt;/item&gt;
      &lt;item&gt;OpenAI + &lt;code&gt;gpt-5-mini&lt;/code&gt;(slower, a bit smarter)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It also offers to store an API key in the config (optional). If you prefer environment variables, leave it blank and set one of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GROQ_API_KEY&lt;/code&gt;for Groq&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;for OpenAI&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Defaults written to &lt;code&gt;~/.qq/config.json&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Providers&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;openai&lt;/code&gt;‚Üí base&lt;code&gt;https://api.openai.com/v1&lt;/code&gt;, env&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;groq&lt;/code&gt;‚Üí base&lt;code&gt;https://api.groq.com/openai/v1&lt;/code&gt;, env&lt;code&gt;GROQ_API_KEY&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Profiles&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;code&gt;openai&lt;/code&gt;‚Üí model&lt;code&gt;gpt-5-mini&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;groq&lt;/code&gt;‚Üí model&lt;code&gt;openai/gpt-oss-20b&lt;/code&gt;(default)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Optional flag:&lt;/p&gt;&lt;code&gt;no_emoji&lt;/code&gt;(unset by default). Set via&lt;code&gt;qq --no-fun&lt;/code&gt;or&lt;code&gt;qa --no-fun&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Terminal history is off by default. During &lt;code&gt;qq --init&lt;/code&gt; / &lt;code&gt;qa --init&lt;/code&gt; you can opt in to sending the last 10 &lt;code&gt;qq&lt;/code&gt;/&lt;code&gt;qa&lt;/code&gt; commands along with each request. You can still override per run with &lt;code&gt;--history&lt;/code&gt; (force on) or &lt;code&gt;-n/--no-history&lt;/code&gt; (force off). Only commands whose first token is &lt;code&gt;qq&lt;/code&gt; or &lt;code&gt;qa&lt;/code&gt; are ever shared.&lt;/p&gt;
    &lt;p&gt;You can still override at runtime:&lt;/p&gt;
    &lt;code&gt;# choose profile
qq -p groq "what is ripgrep"

# override model for a single call
qq -m openai/gpt-oss-20b "explain this awk one-liner"&lt;/code&gt;
    &lt;code&gt;# simplest
qq "convert mp4 to mp3"

# stream tokens with formatted output
qq -s "how do I kill a process by name on macOS"

# include piped context
git status | qq "summarize what I should do next"

# pipe extra context and keep CLI question
printf '%s\n' "This is a sample context. My code is 4242" | qq "What is my code"

# pipe the question itself
printf '%s\n' "Show me the full contents of this directory" | qq

# raw text (no ANSI formatting)
qq -r "explain sed vs awk"

# include terminal history for this run
qq --history "find large files in the last day"

# disable emojis in responses (persists)
qq --no-fun "summarize this"&lt;/code&gt;
    &lt;p&gt;Note: it is possible to run qq without quotes, which works most of the time the same way as with quotes.&lt;/p&gt;
    &lt;code&gt;# simplest
qq convert mp4 to mp3&lt;/code&gt;
    &lt;p&gt;You want to extract audio from a YouTube video but you do not remember the exact flags.&lt;/p&gt;
    &lt;p&gt;Ask with qq:&lt;/p&gt;
    &lt;code&gt;qq "how do I use ffmpeg to extract audio from a YouTube video into mp3"&lt;/code&gt;
    &lt;p&gt;A typical answer will suggest installing the tools and then using &lt;code&gt;yt-dlp&lt;/code&gt; to fetch audio and &lt;code&gt;ffmpeg&lt;/code&gt; to convert it:&lt;/p&gt;
    &lt;code&gt;# macOS
brew install yt-dlp ffmpeg

# Debian or Ubuntu
sudo apt-get update &amp;amp;&amp;amp; sudo apt-get install -y yt-dlp ffmpeg

# Download and extract audio to MP3 using ffmpeg under the hood
yt-dlp -x --audio-format mp3 "https://www.youtube.com/watch?v=VIDEO_ID"&lt;/code&gt;
    &lt;p&gt;Do it for me with qa:&lt;/p&gt;
    &lt;code&gt;qa "download audio as mp3 from https://www.youtube.com/watch?v=VIDEO_ID"&lt;/code&gt;
    &lt;p&gt;The agent will propose a safe command like &lt;code&gt;yt-dlp -x --audio-format mp3 URL&lt;/code&gt;, show it for confirmation, then run it. You can pass &lt;code&gt;-y&lt;/code&gt; to auto approve.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;qa&lt;/code&gt; can either answer in plain text or request one tool call in JSON. Supported tools:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;read_file&lt;/code&gt;with&lt;code&gt;{ "path": string }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;write_file&lt;/code&gt;with&lt;code&gt;{ "path": string, "content": string }&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;execute_command&lt;/code&gt;with&lt;code&gt;{ "command": string, "cwd?": string }&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;code&gt;# read a file the safe way
qa "read src/bin/qq.rs and tell me what main does"

# write a file
qa "create a README snippet at notes/intro.md with a short summary"

# run a command with confirmation
qa "list Rust files under src sorted by size"

# pipe the task itself
printf '%s\n' "Show me the full contents of this directory" | qa

# auto approve tool execution for non interactive scripts
qa -y "count lines across *.rs"

# include recent qq/qa commands just for this run
qa --history "trace which git commands I ran recently"

# disable emojis in responses (persists)
qa --no-fun "format and lint the repo"&lt;/code&gt;
    &lt;p&gt;When qa runs a command while stdout is a terminal, output now streams live; the structured &lt;code&gt;[tool:execute_command]&lt;/code&gt; summary still prints afterward for easy copying.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;execute_command&lt;/code&gt; prints the proposed command and asks for confirmation. It warns if the working directory is outside your home. Use &lt;code&gt;-y&lt;/code&gt; to auto approve in trusted workflows.&lt;/p&gt;
    &lt;p&gt;The runner enforces a default allowlist (think &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;find&lt;/code&gt;, &lt;code&gt;rg&lt;/code&gt;, &lt;code&gt;awk&lt;/code&gt;, etc.) and rejects pipelines, redirection, and other high-risk constructs. When a command is blocked, &lt;code&gt;qa&lt;/code&gt; prompts you to add it to &lt;code&gt;command_allowlist&lt;/code&gt; inside &lt;code&gt;~/.qq/config.json&lt;/code&gt;; approving once persists the choice and updates future runs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File tools require paths to be inside your home or the current directory. Reads are capped to 1 MiB, and traversal/symlink escapes are blocked.&lt;/item&gt;
      &lt;item&gt;Command execution uses a default allowlist (e.g. &lt;code&gt;ls&lt;/code&gt;,&lt;code&gt;grep&lt;/code&gt;,&lt;code&gt;rg&lt;/code&gt;,&lt;code&gt;find&lt;/code&gt;) plus your custom&lt;code&gt;command_allowlist&lt;/code&gt;entries. Destructive patterns (&lt;code&gt;rm -rf /&lt;/code&gt;,&lt;code&gt;sudo&lt;/code&gt;,&lt;code&gt;mkfs&lt;/code&gt;, etc.) are always blocked, and pipelines/redirection/newlines prompt for confirmation even with&lt;code&gt;--yes&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Commands run with a 120 s timeout and the agent performs at most one tool step‚Äîthere is no loop.&lt;/item&gt;
      &lt;item&gt;Config files are created with safe permissions. API keys come from environment variables unless you explicitly add a key to the config.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;GROQ_API_KEY&lt;/code&gt;for the Groq provider&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;OPENAI_API_KEY&lt;/code&gt;for the OpenAI provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Project layout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/bin/qq.rs&lt;/code&gt;and&lt;code&gt;src/bin/qa.rs&lt;/code&gt;entry points&lt;/item&gt;
      &lt;item&gt;Core modules in &lt;code&gt;src/&lt;/code&gt;:&lt;code&gt;ai.rs&lt;/code&gt;,&lt;code&gt;config.rs&lt;/code&gt;,&lt;code&gt;prompt.rs&lt;/code&gt;,&lt;code&gt;history.rs&lt;/code&gt;,&lt;code&gt;perms.rs&lt;/code&gt;,&lt;code&gt;formatting.rs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tools in &lt;code&gt;src/tools/&lt;/code&gt;:&lt;code&gt;read_file.rs&lt;/code&gt;,&lt;code&gt;write_file.rs&lt;/code&gt;,&lt;code&gt;execute_command.rs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Integration tests in &lt;code&gt;tests/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See CONTRIBUTING.md for guidelines on reporting issues and opening pull requests, building from source, and the release process.&lt;/p&gt;
    &lt;p&gt;Official builds are published through the GitHub Releases page.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Build and package a release locally:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Build v0.8.3 for macOS (x86_64 + arm64) and Linux MUSL (x86_64 + arm64)
scripts/release.sh v0.8.3

# Optionally specify a Git SHA to record in the manifest (and tag later)
scripts/release.sh v0.8.3 &amp;lt;git_sha&amp;gt;

# Override targets (space-separated) if you need a custom set
TARGETS="x86_64-apple-darwin aarch64-unknown-linux-musl" scripts/release.sh v0.8.3&lt;/code&gt;
    &lt;p&gt;What the script does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bumps &lt;code&gt;Cargo.toml&lt;/code&gt;version to the given one.&lt;/item&gt;
      &lt;item&gt;Builds &lt;code&gt;qq&lt;/code&gt;and&lt;code&gt;qa&lt;/code&gt;for each target with&lt;code&gt;cargo build --release&lt;/code&gt;(default targets cover macOS x86_64/arm64 and Linux MUSL x86_64/arm64).&lt;/item&gt;
      &lt;item&gt;Packages &lt;code&gt;qqqa-v&amp;lt;version&amp;gt;-&amp;lt;target&amp;gt;.tar.gz&lt;/code&gt;under&lt;code&gt;target/releases/v&amp;lt;version&amp;gt;/&lt;/code&gt;with checksums.&lt;/item&gt;
      &lt;item&gt;Writes &lt;code&gt;target/releases/v&amp;lt;version&amp;gt;/manifest.json&lt;/code&gt;for upload.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tagging the release:&lt;/p&gt;
    &lt;code&gt;git add Cargo.toml
git commit -m "release: v0.8.3"
git tag -a v0.8.3 -m "qqqa v0.8.3"   # or: git tag -a v0.8.3 &amp;lt;sha&amp;gt; -m "qqqa v0.8.3"
git push &amp;amp;&amp;amp; git push --tags&lt;/code&gt;
    &lt;p&gt;Common targets (customizable via &lt;code&gt;TARGETS&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;x86_64-apple-darwin&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;aarch64-apple-darwin&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;x86_64-unknown-linux-gnu&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;aarch64-unknown-linux-gnu&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cross-compiling may require additional toolchains; &lt;code&gt;rustup target add &amp;lt;triple&amp;gt;&lt;/code&gt;is attempted automatically.&lt;/item&gt;
      &lt;item&gt;For fully-static Linux builds, you can adjust targets to &lt;code&gt;*-unknown-linux-musl&lt;/code&gt;if your environment supports it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;API error about missing key: run &lt;code&gt;qq --init&lt;/code&gt;to set things up, or export the relevant env var, e.g.&lt;code&gt;export GROQ_API_KEY=...&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;No output when streaming: try &lt;code&gt;-d&lt;/code&gt;to see debug logs.&lt;/item&gt;
      &lt;item&gt;Piped input not detected: ensure you are piping into &lt;code&gt;qq&lt;/code&gt;and not running it in a subshell that swallows stdin.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Licensed under MIT.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45833811</guid><pubDate>Thu, 06 Nov 2025 10:59:42 +0000</pubDate></item><item><title>Eating stinging nettles</title><link>https://rachel.blog/2018/04/29/eating-stinging-nettles/</link><description>&lt;doc fingerprint="3730f7cb74e1c58"&gt;
  &lt;main&gt;
    &lt;p&gt;Spring is here and the nettles are growing again so I decided it was time to make a meal out of them. Most people know that stinging nettles are pesky green plants that irritate the skin when you touch them. What you probably don‚Äôt know is that they‚Äôre a nutritious source of iron, calcium, potassium, and silica as well as vitamins A, B, C, and K1. Stinging nettles also have anti-inflammatory properties and can relieve arthritis and rheumatism. They can be turned into soups, curries, and risottos (some recipes here) and you can get them completely free from practically everywhere in Britain over the summer. You‚Äôve likely even got some in your garden.&lt;/p&gt;
    &lt;p&gt;When you collect them you need to wear gloves because they sting. The advantage of this is it allows you to make sure you‚Äôre collecting the right thing. If you‚Äôre unsure, just touch one and see whether it hurts which is exactly what I did. It hurt.&lt;/p&gt;
    &lt;p&gt;The even look a bit scary with their toothy-edged leaves.&lt;/p&gt;
    &lt;p&gt;Once you‚Äôve got them inside, boil them in water for a few minutes and this will stop them stinging.&lt;/p&gt;
    &lt;p&gt;We‚Äôre having stinging nettle risotto.&lt;/p&gt;
    &lt;p&gt;People think that when you become vegan you have to give up lots of food. It‚Äôs true that I stopped eating animals but the number of different species I eat has grown considerably. This is because meat-eaters tend to eat the same few species of animals over and over again ‚Äì pigs, cows, chickens. Whereas there are some 20,000 species of edible plants in the world. Meat also tends to fill you up. Indeed I‚Äôve been to dinner with people where all they have on their plate is a slab of meat and nothing else. Whereas as a vegan (with the exception of a shitty Spanish restaurant that served me a plate of artichokes and nothing else) I eat a huge variety of species. Meat-eaters can eat these too but they often don‚Äôt because meat is so filling.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45834254</guid><pubDate>Thu, 06 Nov 2025 11:57:01 +0000</pubDate></item><item><title>AI Slop vs. OSS Security</title><link>https://devansh.bearblog.dev/ai-slop/</link><description>&lt;doc fingerprint="a600d1118e23d53f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;On AI Slop vs OSS Security&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclosure: In previous version of this article, I used AI to refine my raw thoughts (as English isn't my first language), which is quite ironic given the topic of this article. I own up to it. The AI refinement somewhat took away the soul of what I was originally trying to convey. I have reverted it to the original version of my raw thoughts, so some mistakes or errors are likely. The only AI tool used for crafting this piece is Perplexity, for research purposes and not for generation of content.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Author's Note&lt;/head&gt;
    &lt;p&gt;I have now spent almost a decade in the bug bounty industry, started out as a bug hunter (who initially used to submit reports with minimal impact, low-hanging fruits like RXSS, SQLi, CSRF, etc.), then moved on to complex chains involving OAuth, SAML, parser bugs, supply chain security issues, etc., and then became a vulnerability triager for HackerOne, where I have triaged/reviewed thousands of vulnerability submissions. I have now almost developed an instinct that tells me if a report is BS or a valid security concern just by looking at it. I have been at HackerOne for the last 5 years (Nov 2020 - Present), currently as a team lead, overseeing technical services with a focus on triage operations.&lt;/p&gt;
    &lt;p&gt;One decade of working on both sides, first as a bug hunter, and then on the receiving side reviewing bug submissions, has given me a unique vantage point on how the industry is fracturing under the weight of AI-generated bug reports (sometimes valid submissions, but most of the time, the issues are just plain BS). I have seen cases where it was almost impossible to determine whether a report was a hallucination or a real finding. Even my instincts and a decade of experience failed me, and this is honestly frustrating, not so much for me, because as part of the triage team, it is not my responsibility to fix vulnerabilities, but I do sympathize with maintainers of OSS projects whose inboxes are drowning. Bug bounty platforms have already started taking this problem seriously, as more and more OSS projects are complaining about it.&lt;/p&gt;
    &lt;p&gt;This is my personal writing space, so naturally, these are my personal views and observations. These views might be a byproduct of my professional experience gained at HackerOne, but in no way are they representative of my employer. I am sure HackerOne, as an organization, has its own perspectives, strategies, and positions on these issues. My analysis here just reflects my own thinking about the systemic problems I see and potential solutions(?).&lt;/p&gt;
    &lt;head rend="h3"&gt;What Exactly is the Problem?&lt;/head&gt;
    &lt;p&gt;There are two kinds of AI-generated reports:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI-generated valid reports&lt;/item&gt;
      &lt;item&gt;AI-generated non-valid reports&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I call the latter ‚ÄúAI slop‚Äù and the first one is still fine, in my opinion. As long as the security report being submitted is technically valid, falls within scope, and demonstrates impact, even if it is written by AI, it is still acceptable. I see both kinds of reports on a daily basis, and I would much rather see researchers use AI to structure their thoughts. But leaving the ‚Äúsecurity research‚Äù and validation of the report to AI is where I draw the line. If you can‚Äôt reproduce what you are reporting, and your finding is based on an assumption spit out by an LLM, that‚Äôs what I call AI slop, and that‚Äôs the problem I‚Äôll be discussing in this piece.&lt;/p&gt;
    &lt;p&gt;AI has infiltrated vulnerability reporting, and they mirror the social dynamics that plague any feedback system (be it mass swiping profiles on Hinge/tinder, or submitting mass job applications, or perhaps doing mass marketing/sales outreach)&lt;/p&gt;
    &lt;p&gt;A "security researcher" who just pastes LLM-generated output into a vulnerability submission form neither knows enough about the actual codebase being examined nor understands the security implications well enough to provide the insight that projects need. Each project has a different threat model. Something might be considered a valid vulnerability in Project A, but the behavior will be deemed acceptable in Project B. The AI doesn't know this difference, and it is very bad at understanding the threat-model context. It merely pattern-matches (like a regex, but not literally). It sees functions that look similar to vulnerable patterns and invents scenarios where they might be exploited, regardless of whether those scenarios are even possible in the actual implementation.&lt;/p&gt;
    &lt;p&gt;Some actors mass-submit AI-generated vulnerability reports, driven by incentives. The incentives are not always monetary, and sometimes take the form of a CVE. Industry has started treating CVEs as trophies. New-age startups are hunting for CVEs, as it looks nice to have them as a trophy, it gives them marketing, and is overall a good look for their startup and it's capabilities. I have no issues with the latter, as long as they are a net positive to the security space. We'll get to the CVE scene in a later section of this article. Coming to actors who mass-submit invalid AI-generated vulnerabilities (they feel productive and entrepreneurial?). Some genuinely believe their AI has found something real; others know it might not even be valid, but they submit it in the hopes maintainers will sort it out. The incentive ( + CVE) is to submit as many reports as possible and see what sticks, because even a 5% hit rate on a hundred submissions is better than the effort of manually verifying five findings.&lt;/p&gt;
    &lt;p&gt;As a result, Daniel Stenberg, who maintains curl, is now seeing about 20% of all security submissions as AI-generated slop. The rate of genuine vulnerabilities has dropped to approximately 5%. This ratio is borderline scary. For every real vulnerability, there are now four fake ones. And every fake one consumes hours of expert time to disprove.&lt;/p&gt;
    &lt;head rend="h3"&gt;Human Capital is Limited&lt;/head&gt;
    &lt;p&gt;HackerOne platform has many OSS projects in scope of various Bug Bounty programs. Imagine yourself as one of the maintainers of one such OSS program. A security report lands in your inbox. It claims there's a buffer overflow in a specific function. Just by looking at the report, its content, and impact statement, it looks like something worth investigating. The report is well-formatted, also includes CVE-style nomenclature, and uses appropriate technical language. As a responsible maintainer, you can't just dismiss it. You alert your security team‚Äîvolunteers, by the way, who have day jobs and families and maybe three hours a week for this work.&lt;/p&gt;
    &lt;p&gt;Three people read the report. One person even tries to reproduce the issue using the steps provided. They can't, because the steps reference test cases that don't exist. Another person examines the source code. The function mentioned in the report doesn't exist in that form. A third person checks whether there's any similar functionality that might be vulnerable in the way described. There isn't.&lt;/p&gt;
    &lt;p&gt;After an hour and a half of combined effort across three people, that's 4.5 person hours you just spent. You've confirmed what you suspected, this report is garbage. Probably AI-generated garbage, based on the telltale signs of hallucinated function names and impossible attack vectors.&lt;/p&gt;
    &lt;p&gt;You close the report. You don't get those hours back. And tomorrow, two more reports just like it will arrive.&lt;/p&gt;
    &lt;p&gt;The curl project has seven people on its security team. They collaborate on every submission, with three to four members typically engaging with each report. In early July 2025, they were receiving approximately two security reports per week. The math is brutal. If you have three hours per week to contribute to an open source project you love, and a single false report consumes all of it, you've contributed nothing that week except proving someone's AI hallucinated a vulnerability.&lt;/p&gt;
    &lt;p&gt;The emotional toll compounds exponentially. Stenberg describes it as "mind-numbing stupidities" that the team must process. It's not just frustration, it's the specific demoralization that comes from having your expertise and goodwill systematically exploited by people who couldn't be bothered to verify their submissions before wasting your time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Burnout Burnout Burnout&lt;/head&gt;
    &lt;p&gt;According to Intel's annual open source community survey, 45% of respondents identified maintainer burnout as their top challenge. The Tidelift State of the Open Source Maintainer Survey is even more stark: 58% of maintainers have either quit their projects entirely (22%) or seriously considered quitting (36%).&lt;/p&gt;
    &lt;p&gt;Why exactly are they quitting? The top reason, cited by 54% of maintainers, is that other things in their life and work took priority over open source contributions. Over half (51%) reported losing interest in the work. And 44% explicitly identified experiencing burnout.&lt;/p&gt;
    &lt;p&gt;The percentage of maintainers who said they weren't getting paid enough to make maintenance work worthwhile rose from 32% to 38% between survey periods. These are people maintaining infrastructure that powers billions of dollars of commercial activity, and they're getting nothing. Or maybe they get $500 a year from GitHub Sponsors while companies make millions off their work.&lt;/p&gt;
    &lt;p&gt;The maintenance work itself is rarely rewarding. You're not building exciting new features. You're just addressing technical debt, responding to user demands, handling security issues, and now you have to sort through AI-generated garbage to find the occasional legitimate report as well.&lt;/p&gt;
    &lt;p&gt;When you're volunteering out of love in a market society, you're setting yourself up to be exploited.&lt;/p&gt;
    &lt;p&gt;And the exploitation is getting worse. Toxic communities, hyper-responsibility for critical infrastructure, and now the weaponization of AI to automate the creation of work for maintainers. This all is adding up to an unsustainable situation.&lt;/p&gt;
    &lt;p&gt;One Kubernetes contributor put it simply, "If your maintainers are burned out, they can't be protecting the code base like they're going to need to be." This shifts maintainer wellbeing from a human resources problem into a security imperative. Burned-out maintainers miss things. They make mistakes. They eventually quit, leaving projects unmaintained or understaffed. Which eventually affects security of these projects.&lt;/p&gt;
    &lt;head rend="h3"&gt;What AI Slop Looks Like&lt;/head&gt;
    &lt;p&gt;A typical AI slop report will reference function names that don't exist in the codebase. The AI has seen similar function names in its training data and invents somewhat valid sounding variations. It will describe operations, memory related stuff that would indeed be problematic if they existed as described in the report, but which bear no relationship to how the code actually works.&lt;/p&gt;
    &lt;p&gt;One report to curl claimed an HTTP/3 vulnerability and included fake function calls and behaviors that appeared nowhere in the actual codebase. Stenberg has publicly shared a list of AI-generated security submissions received through HackerOne, and they all follow similar patterns, professional formatting, appropriate jargon, and completely fabricated technical details.&lt;/p&gt;
    &lt;p&gt;The sophistication varies. Some of these reports look absolute madness, while some look like P1 security issues, almost worthy of starting incident response. In most cases, both are BS, when examined closely.&lt;/p&gt;
    &lt;p&gt;Some reports are obviously generated by someone who just pasted a repository URL into ChatGPT and asked it to find vulnerabilities. Others show more effort, the submitter may have fed actual code snippets to the AI and then submitted its analysis without verification. Both are equally useless to maintainers, but the latter takes longer to disprove because the code snippets are real even if the vulnerability analysis is hallucinated.&lt;/p&gt;
    &lt;p&gt;LLMs usually fail in the context of security research, as they're designed to be helpful and provide positive responses. When you prompt an LLM to generate a vulnerability report, it will generate one regardless of whether a vulnerability exists. The model has no concept of truth‚Äîonly. It assembles technical terminology into patterns that resemble security reports it has seen during training, but it cannot verify whether the claims it's making are accurate.&lt;/p&gt;
    &lt;p&gt;AI can generate the form of security research without the substance.&lt;/p&gt;
    &lt;head rend="h3"&gt;The CVE System is Collapsing&lt;/head&gt;
    &lt;p&gt;While AI slop is flooding individual project inboxes, the broader CVE infrastructure is facing its own existential crisis. And these crises compound each other in dangerous ways.&lt;/p&gt;
    &lt;p&gt;In April 2025, MITRE Corporation announced that its contract to maintain the Common Vulnerabilities and Exposures program would expire. The Department of Homeland Security failed to renew the long-term contract, creating a funding lapse that affects everything: national vulnerability databases, advisories, tool vendors, and incident response operations.&lt;/p&gt;
    &lt;p&gt;The National Vulnerability Database experienced catastrophic problems throughout 2024. CVE submissions jumped 32% while creating massive processing delays. By March 2025, NVD had analyzed fewer than 300 CVEs, leaving more than 30,000 vulnerabilities backlogged. Approximately 42% of CVEs lack essential metadata like severity scores and product information.&lt;/p&gt;
    &lt;p&gt;Now layer AI slop onto this already stressed system. Invalid CVEs are being assigned at scale. A 2023 analysis by former insiders suggested that only around 20% of CVEs were valid, with the remainder being duplicates, invalid, or inflated. The issues include multiple CVEs being assigned for the same bug, CNAs siding with reporters over project developers even when there's no genuine dispute, and reporters receiving CVEs based on test cases rather than actual distinct vulnerabilities.&lt;/p&gt;
    &lt;p&gt;The result is that the vulnerability tracking system everyone relies on is becoming less trustworthy exactly when we need it most. Security teams can't rely on CVE assignments to prioritize their work. Developers don't trust vulnerability scanners because false positive rates are through the roof. The signal-to-noise ratio has deteriorated so badly that the entire system risks becoming useless.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Doesn't Work&lt;/head&gt;
    &lt;p&gt;Banning submitters doesn't work at scale. You can ban an account, but creating new accounts is trivial. HackerOne implements reputation scoring where points are gained or lost based on report validity, but this hasn't stemmed the tide because the cost of creating throwaway accounts is essentially zero.&lt;/p&gt;
    &lt;p&gt;Asking people to "please verify before submitting" doesn't work. The incentive structure rewards volume, and people either genuinely believe their AI-generated reports are valid or don't care enough to verify. Polite requests assume good faith, but much of the slop comes from actors who have no stake in the community norms.&lt;/p&gt;
    &lt;p&gt;Trying to educate submitters about how AI works doesn't scale. For every person you educate, ten new ones appear with fresh GPT accounts. The problem isn't knowledge‚Äîit's incentives.&lt;/p&gt;
    &lt;p&gt;Simply closing inboxes or shutting down bug bounty programs "works" in the sense that it stops the slop, but it also stops legitimate security research. Several projects have done this, and now they're less secure because they've lost a channel for responsible disclosure.&lt;/p&gt;
    &lt;p&gt;None of the easy answers work because this isn't an easy problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;What Might Actually Work&lt;/head&gt;
    &lt;p&gt;More and more programs should bring in mandatory disclosure requirements. This could act as the first line of defense against the AI slop. Both curl and Django now require submitters to disclose whether AI was used in generating reports. Curl's approach is particularly direct: disclose AI usage upfront and ensure complete accuracy before submission. If AI usage is disclosed, researcher could expect extensive follow-up questions demanding more rigorous proof that the bug is genuine before the team invests time in verification. This works psychologically. It forces submitters to acknowledge they're using AI, which makes them more conscious of their responsibility to verify. It also gives maintainers grounds to reject slop immediately if AI usage was undisclosed but becomes obvious during review. Django goes further with a section titled "Note for AI Tools" that directly addresses language models themselves, reiterating that the project expects no hallucinated content, no fictitious vulnerabilities, and a requirement to independently verify that reports describe reproducible security issues.&lt;/p&gt;
    &lt;p&gt;Raise the bar for what qualifies as a valid PoC. Require technical evidence such as screencasts showing reproducibility, integration or unit tests demonstrating the fault, docker setup for reproducing the bug, or complete reproduction steps with logs and source code, this will naturally make it much harder to submit slop. AI can generate a description of a vulnerability, but it cannot generate working exploit code for a vulnerability that doesn't exist. Raising the bar for what qualifies as a valid PoC will force the submitter to actually verify their claim. If they can't reproduce it, they can't prove it, and you don't waste time investigating.&lt;/p&gt;
    &lt;p&gt;Some sort of reputation and trust backed systems could also work as a social mechanism for filtering. Only users with a history of valid submissions get unrestricted reporting privileges or monetary bounties. New reporters could be required to have established community members vouch for them, creating a web-of-trust model. This mirrors how the world worked before bug bounty platforms commodified security research. The only downside is, it risks creating an insider club. But the upside is that it filters out low-effort actors who won't invest in building reputation.&lt;/p&gt;
    &lt;p&gt;Having some form of economic friction for submitting vulnerabilities. Charge a nominal refundable fee‚Äîsay $50‚Äîfor each submission from new or unproven users. If the report is valid, they get the fee back plus the bounty. If it's invalid, you keep the fee. This immediately makes mass AI submission uneconomical. If someone's submitting 50 AI-generated reports hoping one sticks, that's now $2,500 at risk. But for a legitimate researcher submitting one carefully verified finding, $50 is a trivial barrier that gets refunded anyway. Some projects are considering dropping monetary rewards entirely. The logic is that if there's no money involved, there's no incentive for speculative submissions. But this risks losing legitimate researchers who rely on bounties as income. It's a scorched earth approach that solves the slop problem by eliminating the entire ecosystem.&lt;/p&gt;
    &lt;p&gt;AI-Assisted Triage is also popping up as a solution, it is exactly like fighting fire with fire. Use AI tools trained specifically to identify AI-generated slop and flag it for immediate rejection. HackerOne's Hai Triage system embodies this approach, using AI agents to cut through noise before human analysts validate findings. The risk is, what if your AI filter rejects legitimate reports? What if it's biased against certain communication styles or methodologies? You've just automated discrimination. But the counterargument is that human maintainers are already overwhelmed, and imperfect filtering is better than drowning.&lt;/p&gt;
    &lt;p&gt;Make the slop public, it will build accountability. Curl recently formalized that all submitted security reports will be made public once reviewed and deemed non-sensitive. This means that fabricated or misleading reports won't just be rejected, they'll be exposed to public scrutiny. This works as both deterrent (like a Wall of Shame) and educational tool. If you know your slop report will be publicly documented with your name attached, you might think twice. And when other researchers see examples of what doesn't constitute a valid report, they learn what standards they need to meet. The downside is that public shaming can be toxic and might discourage good-faith submissions from inexperienced researchers. Projects implementing this approach need to be careful about tone and focus on the technical content rather than attacking submitters personally.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sustainability is Hard&lt;/head&gt;
    &lt;p&gt;Every hour spent evaluating slop reports is an hour not spent on features, documentation, or actual security improvements. And maintainers are already working for free, maintaining infrastructure that generates billions in commercial value. When 38% of maintainers cite not getting paid enough as a reason for quitting, and 97% of open source maintainers are unpaid despite massive commercial exploitation of their work, the system is already broken.&lt;/p&gt;
    &lt;p&gt;By definition of Open Source, you could argue that, since it is "open" it cannot be exploited, and maintainers should not expect monetary aid for the opensource work they are doing. I find this to be absolutely BS. AI slop is just the latest exploitation vector (and not the only one). It's the most visible one right now, but it's not the root cause. The root cause is that we've built a global technology infrastructure on the volunteer labor of people who get nothing in return except burnout and harassment.&lt;/p&gt;
    &lt;p&gt;So what does sustainability actually look like?&lt;/p&gt;
    &lt;p&gt;I'll be blunt, it looks like money. Real money. Not GitHub Sponsors donations that average $500 a year. Not swag and conference tickets. Actual salaries commensurate with the value being created. Companies that build products on open source infrastructure need to fund the maintainers of that infrastructure. This could happen through direct employment, foundation grants, or the Open Source Pledge model where companies commit percentages of revenue.&lt;/p&gt;
    &lt;p&gt;Second, it looks like better tooling and automation THAT ACTUALLY WORKS, genuinely reduces workload rather than creating new forms of work. It could be anything, maybe an automated dependency management? continuous security scanning integrated into development workflows? or some kind of sophisticated triage assistance? The goal is to make maintenance less time consuming so burnout becomes less likely.&lt;/p&gt;
    &lt;p&gt;Third, it looks like shared workload and team building. No single volunteer should be a single point of failure. Building teams with checks and balances where members keep each other from taking on too much creates sustainability.&lt;/p&gt;
    &lt;p&gt;Fourth, it looks like culture change. Fostering empathy in interactions, starting communications with gratitude even when rejecting contributions, and publicly acknowledging the critical work maintainers perform reduces emotional toll.&lt;/p&gt;
    &lt;p&gt;Fifth, it looks like advocacy and policy at organizational and governmental levels. Recognition that maintainer burnout represents existential threat to technology infrastructure. Development of regulations requiring companies benefiting from open source to contribute resources. Establishment of security standards that account for the realities of volunteer-run projects.&lt;/p&gt;
    &lt;p&gt;I feel like without addressing these fundamentals, no amount of technical sophistication will prevent collapse.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Arms Race Ahead&lt;/head&gt;
    &lt;p&gt;The CVE slop crisis is just the beginning. We are just getting started and entering an arms race between AI-assisted attackers or abusers and AI-assisted defenders, and nobody knows how it ends.&lt;/p&gt;
    &lt;p&gt;HackerOne's research indicates that 70% of security researchers now use AI tools in their workflow. AI-powered testing is becoming the industry standard. The emergence of fully autonomous hackbots‚ÄîAI systems that submitted over 560 valid reports in the first half of 2025‚Äîsignals both opportunity and threat.&lt;/p&gt;
    &lt;p&gt;The divergence will be between researchers who use AI as a tool to enhance genuinely skilled work versus those who use it to automate low-effort spam. The former represents the promise of democratizing security research and scaling our ability to find vulnerabilities. The latter represents the threat of making the signal to noise problem completely unmanageable.&lt;/p&gt;
    &lt;p&gt;This probably means moving toward more exclusive models. Invite-only programs. Dramatically higher standards for participation. Reputation systems that take years to build. New models for coordinated vulnerability disclosure that assume AI-assisted research as the baseline and require proof beyond "here's what the AI told me."&lt;/p&gt;
    &lt;p&gt;It might mean the end of open bug bounty programs as we know them. Maybe that's necessary. Maybe the experiment of "anyone can submit anything" was only viable when the cost of submitting was high enough to ensure some minimum quality. Now that AI has reduced that cost to near-zero, the experiment might fail soon if things don't improve.&lt;/p&gt;
    &lt;p&gt;So, net-net, here's where we are:&lt;/p&gt;
    &lt;p&gt;When it comes to vulnerability reports, what matters is who submits them and whether they've actually verified their claims. Accepting reports from everyone indiscriminately is backfiring catastrophically because projects are latching onto submissions that sound plausible while ignoring the cumulative evidence that most are noise.&lt;/p&gt;
    &lt;p&gt;You want to receive reports from someone who has actually verified their claims, understands the architecture of what they're reporting on, and isn't trying to game the bounty system or offload verification work onto maintainers.&lt;/p&gt;
    &lt;p&gt;Such people exist, but they're becoming harder to find amidst the mountains of AI-generated content. That's why projects have to be selective about which reports they investigate and which submitters they trust.&lt;/p&gt;
    &lt;p&gt;Ultimately, the sustainability of open source security depends on recognizing that people who maintain critical infrastructure deserve more than exploitation.&lt;/p&gt;
    &lt;p&gt;They deserve compensation, support, reasonable expectations, and protection from abuse. Without addressing these fundamentals, no amount of technical sophistication will prevent the slow collapse of the collaborative model that has produced so much of the digital infrastructure modern life depends on.&lt;/p&gt;
    &lt;p&gt;The crisis isn't merely about bad vulnerability reports. It's about whether we'll choose to sustain the human foundation of technological progress, or whether we'll let it burn out under the weight of automated exploitation.&lt;/p&gt;
    &lt;p&gt;That's the choice we're facing. And right now, we're choosing wrong.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45834303</guid><pubDate>Thu, 06 Nov 2025 12:05:12 +0000</pubDate></item><item><title>I analyzed the lineups at the most popular nightclubs</title><link>https://dev.karltryggvason.com/how-i-analyzed-the-lineups-at-the-worlds-most-popular-nightclubs/</link><description>&lt;doc fingerprint="3603417d793229b9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I analyzed the lineups at the world's most popular nightclubs&lt;/head&gt;
    &lt;p&gt;A few years back I did a bit of dance music related data visualization over at Lazily Evaluated. My favourite was an analysis of clubs and their lineups using Resident Advisor / RA data, I called it Clubster Analysis. I always wanted to dig into the technical aspects of gathering the data, analyzing it and building the charts and graphs to tell a story and give people insight. With this blog I now have the right venue for that kind of tech talk, so here goes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data gathering #&lt;/head&gt;
    &lt;p&gt;To visualize data, first you have to get some! For this purpose I wrote a little scraper in Python. I used Beautiful Soup to parse the html and grab the bits and pieces I was interested in.&lt;/p&gt;
    &lt;p&gt;My scraping of a few thousand pages didn‚Äôt cause considerable load on the RA servers. But in the age of overzealous AI scrapers it‚Äôs worth being polite, so I throttled according to their robots.txt. I also maintained a local cache of html files I had already downloaded, so that I wouldn‚Äôt have fetch the same data repeatedly (past lineups are unlikely to change after the fact) just because I discovered some bug or error in my parsing.&lt;/p&gt;
    &lt;p&gt;The order I scraped in was:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get the 20 most popular regions in RA (and then I dropped ‚ÄúStreamland‚Äù which was a pandemic era pseudo-region)&lt;/item&gt;
      &lt;item&gt;Fetch the most popular clubs and some related metadata for all of those regions.&lt;/item&gt;
      &lt;item&gt;For each club, get the lineups for every 2019 event of theirs (the last full year before the pandemic started).&lt;/item&gt;
      &lt;item&gt;Save the results to csv files&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Clean up, verification and Analysis #&lt;/head&gt;
    &lt;p&gt;I did some spot checks to verify that my parsing was working as I expected and added tests to make sure I handled edge cases and normalized artist names. There was a lot of variance in how dates were formatted, how artists were linked, etc.&lt;/p&gt;
    &lt;p&gt;After that I analyzed the data. I built one big table/dataframe in Pandas by joining all the info from the csv files. Then I calculated the similarities between each pair of clubs in the data set using the Jaccard index. Consider all the artists that have played at two given clubs, take the intersection (number of artists that have featured in lineups at both clubs) over the union (all the artists that have performed at one or the other). As an example if Club A had 100 artists booked and Club B had 100 artists, and they had 10 bookings in common, the Jaccard index would be 10/190 = ~5%. This gives you a good way to compare large and small clubs and balances large and small lineups (some of the clubs have multiple rooms with very long events, others have one dj playing in one room all night long once a week).&lt;/p&gt;
    &lt;p&gt;Based on the Jaccard index we can build a graph, using NetworkX from all the clubs. The edges between two nodes are weighted by the similarity of those clubs. On top of the graph we run community detection to create clusters (hence the clubster name). This gives us a rough idea of which clubs are most similar, that is to say, have similar tastes in their bookings.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results #&lt;/head&gt;
    &lt;p&gt;For the year 2019, there were 131 clubs in the data set with 8.502 events. There were 9.405 unique artists making up 30.482 individual bookings. This means that the average artist in the dataset was booked 3.24 times at those clubs in that year and the average event had 3.5 artists on the line up.&lt;/p&gt;
    &lt;p&gt;As a whole, out of 8.515 possible pairs of clubs, 3.716 pairs had some overlap in their bookings and out of those the average overlap was 1%. This was lower than I thought, the bookings at European clubs felt more homogenous to me, but I suppose they book a lot of artists. It would be interesting to get more data, recent and historic, and see how this has evolved through time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Visualization #&lt;/head&gt;
    &lt;p&gt;This was my first time using D3 to draw charts. There was a bit of a learning curve, in earlier projects I had used higher level charting libraries which have simpler apis. But with D3 you get a lot of control over how your charts look and behave which I think I used to good effect in this instance.&lt;/p&gt;
    &lt;p&gt;My main goal was to visualize the clusters and to allow people to interact with the clubs. I coloured the clubs according to their clusters and sized them based on the number of followers they had on RA. I played around with the gravity and placement of the cluster, trying to find a balance that worked on different screen sizes as well as being a fair portrayal of the different communities.&lt;/p&gt;
    &lt;p&gt;I then did some scrollytelling to tell the story of the data, as I saw it, while the reader scrolls down the page. But I also added filters and interactivity for people to explore and see if they agree with my telling of the story or if they can find one of their own.&lt;/p&gt;
    &lt;p&gt;At the time I didn‚Äôt find any great React and D3 bridges, so it was a bit of a hassle getting the React components to play nice with the D3 graph, but in the end I was able to connect the two with &lt;code&gt;createRef&lt;/code&gt; to the D3 svg component.&lt;/p&gt;
    &lt;p&gt;Besides the clustering I looked into the ‚Äúresident factor‚Äù, how many times an artist was booked at a club repeatedly compared to all the one offs. This was lower than expected, most of these clubs were booking a constantly rotating assembly of talent, residents don‚Äôt play as big a part as I would have thought.&lt;/p&gt;
    &lt;p&gt;Transitioning between the different sections of these graphs was one of my favourite parts. Seeing the clusters morph into dots and candlestick charts (and back again) was oddly satisfying. Took a lot of tweaking, but I think it really tied together the scrollytelling experience.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt think these transitions would have been possible with the higher level charting libraries I‚Äôd used previously. So the decision to go with D3 felt justified.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary #&lt;/head&gt;
    &lt;p&gt;This was a great pandemic project that combined web scraping, data analysis, and interactive visualization to explore the global dance music club scene. I learned me some D3 for the visualization, got better at doing cartesian graphing calculations in my head and learned about the underlying svg mechanics that power those graphs.&lt;/p&gt;
    &lt;p&gt;The results surprised me: despite my perceived homogeneity of European club bookings, only 1% average overlap between venues suggested more diverse landscape than I expected. The diminished role of residents compared to one-off bookings also challenged my assumptions about how these clubs operate. For the story telling maintaining the balance between a narrative and letting users explore and decide for themselves was a fun challenge. I think these sort of passion projects can give us deep insights into our world and culture.&lt;/p&gt;
    &lt;p&gt;The technical stack I worked with: Python, Pandas, NetworkX, D3, and React proved powerful despite some integration challenges. The complete project is available on GitHub and you can explore the live interactive visualization yourself.&lt;/p&gt;
    &lt;p&gt;I had a lot of fun building this and am proud of the result. If you‚Äôre working on cultural data analysis, need help with web scraping and visualization, or just want to discuss interesting datasets, feel free to reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45835083</guid><pubDate>Thu, 06 Nov 2025 13:37:07 +0000</pubDate></item><item><title>Cloudflare Tells U.S. Govt That Foreign Site Blocking Efforts Are Trade Barriers</title><link>https://torrentfreak.com/cloudflare-tells-u-s-govt-that-foreign-site-blocking-efforts-are-digital-trade-barriers/</link><description>&lt;doc fingerprint="201a9b555f843940"&gt;
  &lt;main&gt;
    &lt;p&gt;Every year, the office of the United States Trade Representative (USTR) publishes the National Trade Estimate Report on Foreign Trade Barriers.&lt;/p&gt;
    &lt;p&gt;The report is compiled based on input from key industry players. This includes submissions from copyright industry groups that frequently highlight piracy challenges that in their view act as barriers to trade.&lt;/p&gt;
    &lt;p&gt;In previous years, for example, the MPA and others have called for more site-blocking efforts to counter the piracy threat. Interestingly, however, other American companies now inform the USTR that foreign site-blocking measures are becoming a significant trade barrier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cloudflare Sees Piracy Blockades as Trade Barriers&lt;/head&gt;
    &lt;p&gt;To share its concerns, Cloudflare decided to participate in the annual trade barriers consultation for the first time this year. The company describes itself as a ‚Äúleading connectivity cloud company‚Äù running one of the world‚Äôs largest networks, providing security, performance, and reliability services.&lt;/p&gt;
    &lt;p&gt;According to Cloudflare, several foreign countries disproportionately impact U.S. technology providers, with many concerns relating to site-blocking measures that aim to deter online piracy.&lt;/p&gt;
    &lt;head rend="h3"&gt;Spain&lt;/head&gt;
    &lt;p&gt;Cloudflare writes that Spanish courts allow rightsholders to request ‚Äúoverbroad court orders‚Äù that authorize IP address blocking. Since a single IP address can serve thousands of domains, disrupting pirates often means that many legitimate sites and services are blocked too, causing widespread collateral damage.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis practice results in the widespread and repeated disruption of tens of thousands of unrelated, legitimate websites, as well as the disruption of digital services, with no judicial opportunity for remedy,‚Äù Cloudflare writes.&lt;/p&gt;
    &lt;p&gt;‚ÄúThese actions, designed to protect a narrow set of commercial interests, have caused significant collateral harm to businesses and users who are not the intended targets, without recourse or the possibility for affected parties to challenge the underlying order.‚Äù&lt;/p&gt;
    &lt;p&gt;The Spanish Government is aware of the problems, which Cloudflare says are at odds with international standards, but has chosen not to intervene in the issue. Therefore, it continues to present a significant trade barrier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Italy&lt;/head&gt;
    &lt;p&gt;Cloudflare reports similar concerns in Italy, where the ‚ÄúPiracy Shield‚Äù site-blocking law has a direct effect on American companies. This blocking regulation requires network providers, including CDNs, to comply with blocking notices within 30 minutes.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe failure to include adequate safeguards against collateral damage has led to the inappropriate blocking of shared services of large cloud providers, which are disproportionately American businesses,‚Äù Cloudflare notes.&lt;/p&gt;
    &lt;p&gt;‚ÄúFor instance, the blocking of a Cloudflare IP address resulted in tens of thousands of non-targeted websites being blocked in February 2024. Furthermore, the blocking of the domain ‚Äúdrive.usercontent.google.com‚Äù in October denied Italian users access to Google Drive for over 12 hours.‚Äù&lt;/p&gt;
    &lt;p&gt;Efforts to expand Piracy Shield to public DNS resolvers and VPN services only make the problem worse, Cloudflare says, noting that some U.S. companies have already decided to leave the European country.&lt;/p&gt;
    &lt;p&gt;Automated piracy blocks are not the only reported trade barrier in Italy. Cloudflare also notes that the country allows rightsholders to ‚Äúabuse‚Äù the courts to disrupt U.S. businesses by granting ex parte blocking orders without giving the companies a chance to oppose them.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis coercive, penalty-based approach to removal of content, without adequate judicial review or due process protections, is a significant barrier to doing business in Italy,‚Äù Cloudflare writes.&lt;/p&gt;
    &lt;head rend="h3"&gt;France&lt;/head&gt;
    &lt;p&gt;In France, Cloudflare highlights Article L.333-10 of the Sports Code as a key problem. This has resulted in several pirate site blocking orders that go beyond regular Internet providers, requiring DNS resolvers and VPN services to take action as well.&lt;/p&gt;
    &lt;p&gt;Cloudflare notes that some services lack the technical capabilities to implement these orders and as a result, several U.S. companies have already left the country.&lt;/p&gt;
    &lt;p&gt;Recently, France passed a new anti-piracy bill that opens the door to automated IP-address blocking, similar to Italy‚Äôs Piracy Shield. This is a major concern for Cloudflare, which fears that this will only lead to more collateral damage.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt increases the risk of overblocking legitimate content or mistakenly targeting websites that operate lawfully, potentially disrupting cross-border digital services,‚Äù Cloudflare writes.&lt;/p&gt;
    &lt;head rend="h3"&gt;South Korea&lt;/head&gt;
    &lt;p&gt;South Korea has also created trade barriers due to its site-blocking measures, Cloudflare reports. A revision to the Network Act in 2023 now requires ‚ÄúCDNs to restrict access to illegal content‚Äù.&lt;/p&gt;
    &lt;p&gt;As a result, Cloudflare and other American companies are required to maintain detailed and regularly updated blocklists.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe South Korea Communication Commission (KCC) sends U.S. CDN providers a ‚Äòblock list‚Äô of over 1.5 million URLs (with 30,000 new additions monthly),‚Äù Cloudflare writes, noting that this places an ‚Äúunprecedented compliance burden‚Äù on companies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conflicting Demands at the U.S. Trade Office&lt;/head&gt;
    &lt;p&gt;Cloudflare urges the USTR to take these concerns into account for its upcoming National Trade Estimate Report. Ideally, it wants these trade barriers to be dismantled.&lt;/p&gt;
    &lt;p&gt;These calls run counter to requests from rightsholders, who urge the USTR to ensure that more foreign countries implement blocking measures. With potential site-blocking legislation being considered in U.S. Congress, that may impact local lobbying efforts as well.&lt;/p&gt;
    &lt;p&gt;If and how the USTR will address these concerns will become clearer early next year, when the 2026 National Trade Estimate Report is expected to be published.&lt;/p&gt;
    &lt;p&gt;‚Äî&lt;/p&gt;
    &lt;p&gt;A copy of Cloudflare‚Äôs submission for the USTR‚Äôs 2025 National Trade Estimate Report on Foreign Trade Barriers is available here (pdf)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45835123</guid><pubDate>Thu, 06 Nov 2025 13:41:14 +0000</pubDate></item><item><title>Kimi K2 Thinking, a SOTA open-source trillion-parameter reasoning model</title><link>https://moonshotai.github.io/Kimi-K2/thinking.html</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45836070</guid><pubDate>Thu, 06 Nov 2025 15:06:06 +0000</pubDate></item><item><title>Australia has so much solar that it's offering everyone free electricity</title><link>https://electrek.co/2025/11/04/australia-has-so-much-solar-that-its-offering-everyone-free-electricity-3h-day/</link><description>&lt;doc fingerprint="b2218c5f3ab4e71c"&gt;
  &lt;main&gt;
    &lt;p&gt;The Australian government is floating a scheme that would share the benefits of solar power with everyone on the grid, offering totally free electricity to ratepayers in the middle of the day, when the sun is shining the strongest.&lt;/p&gt;
    &lt;p&gt;Australia is a sunny place. It‚Äôs kind of known for it. It‚Äôs the sunniest continent, and the sunniest country outside of the Middle East/Africa, with extensive photovoltaic power potential across its entire territory.&lt;/p&gt;
    &lt;p&gt;In recognition of that, Australia has been installing lots of solar power. Formerly a coal-heavy nation (for which coal is still its 2nd-largest export), solar and wind have rapidly taken over Australia‚Äôs electricity grid, pushing coal and methane gas out of the equation.&lt;/p&gt;
    &lt;p&gt;This has taken a big chunk out of Australia‚Äôs electricity-related climate emissions, and of course resulted in clean air benefits as dirty coal is pushed out of the grid. And climate emissions matter a lot for Australia, a country that is becoming more unbearably hot and suffering more fires due to climate change. (Though Australia is also a great example of how global cooperation on environmental issues can fix a huge problem, as they are the primary beneficiary of global action on closing the hole in the Ozone layer)&lt;/p&gt;
    &lt;p&gt;So solar power has been a great thing for Australia, especially with rooftop solar on Australian homes.&lt;/p&gt;
    &lt;p&gt;But it can lead to swingy electricity supply, given that solar only generates electricity when the sun is out.&lt;/p&gt;
    &lt;head rend="h2"&gt;How swings in solar supply and electricity pricing work&lt;/head&gt;
    &lt;p&gt;Most areas have certain times of day where more electricity is used than others. These are referred to as ‚Äúpeak hours‚Äù and generally they happen in the early evening, when people get home from work, turn on the HVAC, cook dinner, do laundry and the like.&lt;/p&gt;
    &lt;p&gt;But there are also certain times of day when more electricity is generated, and that‚Äôs particularly the case in places with high solar penetration. Solar obviously generates energy only during the day, and creates a peak of generation in the middle of the day, when most people are at work.&lt;/p&gt;
    &lt;p&gt;There are ways to mitigate this ‚Äì for example, with batteries, which Australia has also used a lot of (and is thinking about extending that to EV batteries too). Wind power also helps, since wind tends to pick up in the hours that solar is dropping off.&lt;/p&gt;
    &lt;p&gt;But another way to mitigate it is through simple economics. Offer people lower prices in the hours that electricity is more abundant, and higher prices in hours where it isn‚Äôt. Then, people will tend to use electricity when they can ‚Äì especially if they have shiftable loads like electric cars, laundry, pool pumps and such, which don‚Äôt need to be on at the same time every day (unlike HVAC, cooking, and lighting, for example).&lt;/p&gt;
    &lt;p&gt;Most electricity providers will offer something like this, called a ‚Äútime of use‚Äù plan. These plans differ in their rates and peak hours depending on your location and how the supply/demand curves work for electricity there (and have seen common use among electric car owners because of the outsized effect an EV has on home electricity use).&lt;/p&gt;
    &lt;p&gt;On the utility side, though, the swings in price can be much more drastic. Wholesale prices for electricity can go up to multiple dollars per kilowatt-hour during times of extreme demand when the grid is stressed, and electricity prices can even go negative when there is little demand and lots of supply, particularly on an islanded grid like Australia (this also happens in Texas, where the grid is largely disconnected from the rest of the US). These swings are ironed out for the consumer, so things aren‚Äôt as spiky for us, but it can be quite a rollercoaster on the grid side.&lt;/p&gt;
    &lt;p&gt;In Australia and other places with high solar penetration, these negative electricity prices often happen during the day. That‚Äôs when generation is the highest for solar panels, and household loads are typically low.&lt;/p&gt;
    &lt;head rend="h2"&gt;Australia proposes letting everyone benefit from negative wholesale rates&lt;/head&gt;
    &lt;p&gt;So, the Australian government has decided on a scheme to bring those electricity savings to the consumer, with what its calling its ‚ÄúSolar Sharer‚Äù program.&lt;/p&gt;
    &lt;p&gt;The program would require electricity retailers to provide free electricity to everyone for at least three hours a day, in recognition of the incredibly low wholesale cost of electricity during daytime due to extensive solar power penetration.&lt;/p&gt;
    &lt;p&gt;These would likely be in the middle of the day, when most people aren‚Äôt home. However, every home has some amount of shiftable electricity load, and the Solar Sharer scheme would encourage people to make use of that. With modern appliances that can be scheduled to start in the middle of the day, people can just plan to do laundry, run the dishwasher, run the pool pump, or charge their car at noon, instead of whenever else they were going to.&lt;/p&gt;
    &lt;p&gt;Additionally, people could fill up a home battery during the day, and then use that electricity during peak hours when rates are higher. And this plan will help to incentivize private installation of batteries, or other shiftable loads.&lt;/p&gt;
    &lt;p&gt;The overall effect of this is that it will help to iron out electricity use, making it track more closely with electricity supply, reducing the need for grid upgrades to manage swings in generation. Just turning on this simple behavioral switch, and then publicizing it so customers know to use electricity in the free hours, will both help the grid and help ratepayers save money.&lt;/p&gt;
    &lt;p&gt;Better yet, this scheme will apply not just to people who have solar or home batteries, but to people who live in places where they can‚Äôt put up solar ‚Äì those who live in apartments and the like. The government says it will require companies to offer this scheme to all customers, not just those with solar.&lt;/p&gt;
    &lt;p&gt;The government did receive some pushback from electricity retailers, who feel they were not properly consulted on the plan. But Australia‚Äôs Climate Change Minister Chris Bowen said he would make ‚Äúno apologies‚Äù if this scheme reduced their margins, and that ‚Äúconsumers are put first‚Äù as reported by the Australian Broadcasting Corporation.&lt;/p&gt;
    &lt;p&gt;The government plans to implement the scheme starting in July next year, first in Queensland, New South Wales and South Australia. If it works well, other regions will get it starting in 2027.&lt;/p&gt;
    &lt;head rend="h2"&gt;Electrek‚Äôs Take&lt;/head&gt;
    &lt;p&gt;Australia is doing a lot of great things with electricity, and acting somewhat like a natural laboratory for a lot of ideas that people have been talking about for a long time. Since the whole country has similar solarization, it can work somewhat as a unit in pushing for solar power, and for reforms to help enable it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Top comment by Spec9&lt;/head&gt;
    &lt;p&gt;It is an interesting idea. There's so much electricity on the grid that the price drops to zero and the electricity is curtailed (wasted). So why not give it out for free and encourage people to do their laundry then, charge EVs, buy residential batteries and charge them then, turn on their water heaters, precool/preheat homes, etc.&lt;/p&gt;
    &lt;p&gt;It‚Äôs already working on V2G, with a huge trial started recently, and the wide adoption of solar and batteries is proving that even a solar-heavy grid can still work. And an idea like this, showing how simple economics can be used to change consumer behavior, could provide a model for the rest of the world on how to usher us into a cleaner energy future.&lt;/p&gt;
    &lt;p&gt;So we‚Äôll be watching with interest how this turns out ‚Äì I think it will likely turn out quite well, if the government goes through with it fully.&lt;/p&gt;
    &lt;p&gt;The 30% federal solar tax credit is ending this year. If you‚Äôve ever considered going solar, now‚Äôs the time to act. To make sure you find a trusted, reliable solar installer near you that offers competitive pricing, check out EnergySage, a free service that makes it easy for you to go solar. It has hundreds of pre-vetted solar installers competing for your business, ensuring you get high-quality solutions and save 20-30% compared to going it alone. Plus, it‚Äôs free to use, and you won‚Äôt get sales calls until you select an installer and share your phone number with them.&lt;/p&gt;
    &lt;p&gt;Your personalized solar quotes are easy to compare online and you‚Äôll get access to unbiased Energy Advisors to help you every step of the way. Get started here.&lt;/p&gt;
    &lt;p&gt;FTC: We use income earning auto affiliate links. More.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45836104</guid><pubDate>Thu, 06 Nov 2025 15:08:56 +0000</pubDate></item><item><title>Supply chain attacks are exploiting our assumptions</title><link>https://blog.trailofbits.com/2025/09/24/supply-chain-attacks-are-exploiting-our-assumptions/</link><description>&lt;doc fingerprint="ad40041988733720"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Supply chain attacks are exploiting our assumptions&lt;/head&gt;
    &lt;p&gt;Every time you run &lt;code&gt;cargo add&lt;/code&gt; or &lt;code&gt;pip install&lt;/code&gt;, you are taking a leap of faith. You trust that the code you are downloading contains what you expect, comes from who you expect, and does what you expect. These expectations are so fundamental to modern development that we rarely think about them. However, attackers are systematically exploiting each of these assumptions.&lt;/p&gt;
    &lt;p&gt;In 2024 alone, PyPI and npm removed thousands of malicious packages; multiple high-profile projects had malware injected directly into the build process; and the XZ Utils backdoor nearly made it into millions of Linux systems worldwide.&lt;/p&gt;
    &lt;p&gt;Dependency scanning only catches known vulnerabilities. It won‚Äôt catch when a typosquatted package steals your credentials, when a compromised maintainer publishes malware, or when attackers poison the build pipeline itself. These attacks succeed because they exploit the very trust that makes modern software development possible.&lt;/p&gt;
    &lt;p&gt;This post breaks down the trust assumptions that make the software supply chain vulnerable, analyzes recent attacks that exploit them, and highlights some of the cutting-edge defenses being built across ecosystems to turn implicit trust into explicit, verifiable guarantees.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implicit trust&lt;/head&gt;
    &lt;p&gt;For many developers, the software supply chain begins and ends with the software bill of materials (SBOM) and dependency scanning, which together answer two fundamental questions: what code do you have, and does it contain known vulnerabilities? But understanding what you have is the bare minimum. As sophisticated attacks become more common, you also need to understand where your code comes from and how it gets to you.&lt;/p&gt;
    &lt;p&gt;You trust that you are installing the package you expect. You assume that running &lt;code&gt;cargo add rustdecimal&lt;/code&gt; is safe because &lt;code&gt;rustdecimal&lt;/code&gt; is a well-known and widely used library. Or wait, maybe it‚Äôs spelled &lt;code&gt;rust_decimal&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;You trust that packages are published by the package maintainers. When a popular package starts shipping with a precompiled binary to save build time, you may decide to trust the package author. However, many registries lack strong verification that publishers are who they claim to be.&lt;/p&gt;
    &lt;p&gt;You trust that packages are built from the package source code. You may work on a security-conscious team that audits code changes in the public repository before upgrading dependencies. But this is meaningless if the distributed package was built from code that does not appear in the repository.&lt;/p&gt;
    &lt;p&gt;You trust the maintainers themselves. Ultimately, installing third-party code means trusting package maintainers. It is not practical to audit every line of code you depend on. We assume that the maintainers of well-established and widely adopted packages will not suddenly decide to add malicious code.&lt;/p&gt;
    &lt;p&gt;These assumptions extend beyond traditional package managers. The same trust exists when you run a GitHub action, install a tool with Homebrew, or execute the convenient &lt;code&gt;curl ... | bash&lt;/code&gt; installation script. Understanding these implicit trust relationships is the first step in assessing and mitigating supply chain risk.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recent attacks&lt;/head&gt;
    &lt;p&gt;Attackers are exploiting trust assumptions across every layer of the supply chain. Recent incidents range from simple typosquatting to multiyear campaigns, demonstrating how attackers‚Äô tactics are evolving and growing more complex.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deceptive doubles&lt;/head&gt;
    &lt;p&gt;Typosquatting involves publishing a malicious package with a name similar to that of a legitimate package. Running &lt;code&gt;cargo add rustdecimal&lt;/code&gt; instead of &lt;code&gt;rust_decimal&lt;/code&gt; could install malware instead of the expected legitimate library. This exact attack occurred on crates.io in 2022. The malicious &lt;code&gt;rustdecimal&lt;/code&gt; mimicked the popular &lt;code&gt;rust_decimal&lt;/code&gt; package but contained a &lt;code&gt;Decimal::new&lt;/code&gt; function that executed a malicious binary when called.&lt;/p&gt;
    &lt;p&gt;The simplicity of the attack has made it easy for attackers to launch numerous large-scale campaigns, particularly against PyPI and npm. Since 2022, there have been multiple typosquatting campaigns targeting packages that account for a combined 1.2 billion weekly downloads. Thousands of malicious packages have been published to PyPI and npm alone. This type of attack happens so frequently that there are too many examples to list here. In 2023, researchers documented a campaign that registered 900 typosquats of 40 popular PyPI packages and discovered malware being staged on crates.io. The attacks have only intensified, with 500 malicious packages published in a single 2024 campaign.&lt;/p&gt;
    &lt;p&gt;Dependency confusion takes a different approach, exploiting package manager logic directly. Security researcher Alex Birsan demonstrated and named this type of attack in 2021. He discovered that many organizations use names for internal packages that are either leaked or guessable. By publishing packages with the same names as these internal packages to public registries, Birsan was able to trick package managers into downloading his version instead. Birsan‚Äôs proof of concept identified vulnerabilities across three programming languages and 35 organizations, including Shopify, Apple, Netflix, Uber, and Yelp.&lt;/p&gt;
    &lt;p&gt;In 2022, an attacker used this technique to include malicious code in the nightly releases of PyTorch for five days. An internal dependency named &lt;code&gt;torchtriton&lt;/code&gt; was hosted from PyTorch‚Äôs nightly package index. An attacker published a malicious package with the same name to PyPI, which took precedence. As a result, the nightly versions of PyTorch contained malware for five days before the malware was caught.&lt;/p&gt;
    &lt;p&gt;While these attacks occur at the point of installation, other attacks take a more direct approach by compromising the publishing process itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stolen secrets&lt;/head&gt;
    &lt;p&gt;Compromised accounts are another frequent attack vector. Attackers acquire a leaked key, stolen token, or guessed password, and are able to directly publish malicious code on behalf of a trusted entity. A few recent incidents show the scale of this type of attack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ctrl/tinycolor (September 2025): Self-propagating malware harvested npm API credentials and used the credentials to publish additional malicious packages. Over 40 packages were compromised, accounting for more than 2 million weekly downloads.&lt;/item&gt;
      &lt;item&gt;Nx (August 2025): A compromised token allowed attackers to publish malicious versions containing scripts leveraging already installed AI CLI tools (Claude, Gemini, Q) for reconnaissance, stealing cryptocurrency wallets, GitHub/npm tokens, and SSH keys from thousands of developers before exfiltrating data to public GitHub repositories.&lt;/item&gt;
      &lt;item&gt;rand-user-agent (May 2025): A malicious release containing malware was caught only after researchers noticed recent releases despite no changes to the source code in months.&lt;/item&gt;
      &lt;item&gt;rspack (December 2024): Stolen npm tokens enabled attackers to publish cryptocurrency miners in packages with 500,000 combined weekly downloads.&lt;/item&gt;
      &lt;item&gt;UAParser.js (October 2021): A compromised npm token was used to publish malicious releases containing a cryptocurrency miner. The library had millions of weekly downloads at the time of the attack.&lt;/item&gt;
      &lt;item&gt;PHP Git server (March 2021): Stolen credentials allowed attackers to inject a backdoor directly into PHP‚Äôs source code. Thankfully, the content of the changes was easily spotted and removed by the PHP team before any release.&lt;/item&gt;
      &lt;item&gt;Codecov (January 2021): Attackers found a deployment key in a public Docker image layer and used it to modify Codecov‚Äôs Bash Uploader tool, silently exfiltrating environment variables and API keys for months before discovery.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Stolen secrets remain one of the most reliable supply chain attack vectors. But as organizations implement stronger authentication and better secret management, attackers are shifting from stealing keys to compromising the systems that use them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Poisoned pipelines&lt;/head&gt;
    &lt;p&gt;Instead of stealing credentials, some attackers have managed to distribute malware through legitimate channels by compromising the build and distribution systems themselves. Code reviews and other security checks are bypassed entirely by directly injecting malicious code into CI/CD pipelines.&lt;/p&gt;
    &lt;p&gt;The SolarWinds attack in 2020 is one of the well-known attacks in this category. Attackers compromised the build environment and inserted malicious code directly into the Orion software during compilation. The malicious version of Orion was then signed and distributed through SolarWinds‚Äô legitimate update channels. The attack affected thousands of organizations including multiple Fortune 500 companies and government agencies.&lt;/p&gt;
    &lt;p&gt;More recently, in late 2024, an attacker compromised the Ultralytics build pipeline to publish multiple malicious versions. The attacker used a template injection in the project‚Äôs GitHub Actions to gain access to the CI/CD pipeline and poisoned the GitHub Actions cache to include malicious code directly in the build. At the time of the attack, Ultralytics had more than one million weekly downloads.&lt;/p&gt;
    &lt;p&gt;In 2025, an attacker modified the &lt;code&gt;reviewdog/actions-setup&lt;/code&gt; GitHub action v1 tag to point to a malicious version containing code to dump secrets. This likely led to the compromise of another popular action, &lt;code&gt;tj-actions/changed-files&lt;/code&gt;, through its dependency on &lt;code&gt;tj-actions/eslint-changed-files&lt;/code&gt;, which in turn relied on the compromised &lt;code&gt;reviewdog&lt;/code&gt; action. This cascading compromise affected thousands of projects using the &lt;code&gt;changed-files&lt;/code&gt; action.&lt;/p&gt;
    &lt;p&gt;While poisoned pipeline attacks are relatively rare compared to typosquatting or credential theft, they represent an escalation in attacker sophistication. As stronger defenses are put in place, attackers are forced to move up the supply chain. The most determined attackers are willing to spend years preparing for a single attack.&lt;/p&gt;
    &lt;head rend="h3"&gt;Malicious maintainers&lt;/head&gt;
    &lt;p&gt;The XZ Utils backdoor, discovered in March 2024, nearly compromised millions of Linux systems worldwide. The attacker spent over two years making legitimate contributions to the project before gaining maintainer access. They then abused this trust to insert a sophisticated backdoor through a series of seemingly innocent commits that would have granted remote access to any system using the compromised version.&lt;/p&gt;
    &lt;p&gt;Ultimately, you must trust the maintainers of your dependencies. Secure build pipelines cannot protect against a trusted maintainer who decides to insert malicious code. With open-source maintainers increasingly overwhelmed, and with AI tools making it easier to generate convincing contributions at scale, this trust model is facing unprecedented challenges.&lt;/p&gt;
    &lt;head rend="h2"&gt;New defenses&lt;/head&gt;
    &lt;p&gt;As attacks grow more sophisticated, defenders are building tools to match. These new approaches are making trust assumptions explicit and verifiable rather than implicit and exploitable. Each addresses a different layer of the supply chain where attackers have found success.&lt;/p&gt;
    &lt;head rend="h3"&gt;TypoGard and Typomania&lt;/head&gt;
    &lt;p&gt;Most package managers now include some form of typosquatting protection, but they typically use traditional similarity checks like those measuring Levenshtein distance, which generate excessive false positives that need to be manually reviewed.&lt;/p&gt;
    &lt;p&gt;TypoGard fills this gap by using multiple context-aware metrics, like the following, to detect typosquatting packages with a low false positive rate and minimal overhead:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Repeated characters (e.g., &lt;code&gt;rustdeciimal&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Common typos based on keyboard layout&lt;/item&gt;
      &lt;item&gt;Swapped characters (e.g., &lt;code&gt;reqeusts&lt;/code&gt;instead of&lt;code&gt;requests&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Package popularity thresholds to focus on high-risk targets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This tool targets npm, but the concepts can be extended to other languages. The Rust Foundation published a Rust port, Typomania, that has been adopted by crates.io and has successfully caught multiple malicious packages.&lt;/p&gt;
    &lt;head rend="h3"&gt;Zizmor&lt;/head&gt;
    &lt;p&gt;Zizmor is a static analysis tool for GitHub Actions. Actions have a large surface area, and writing complex workflows can be difficult and error-prone. There are many subtle ways workflows can introduce vulnerabilities.&lt;/p&gt;
    &lt;p&gt;For example, Ultralytics was compromised via template injection in one of its workflows.&lt;/p&gt;
    &lt;p&gt;Workflows triggered by &lt;code&gt;pull_request_target&lt;/code&gt; events run with write permission access to repository secrets. An attacker opened a pull request from a branch with a malicious name. When the workflow ran, the &lt;code&gt;github.head_ref&lt;/code&gt; variable expanded to the malicious branch name and executed as part of the run command with the workflow‚Äôs elevated privileges.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;reviewdog/actions-setup&lt;/code&gt; attack was also carried out in part by changing the action‚Äôs v1 tag to point to a malicious commit. Anyone using &lt;code&gt;reviewdog/actions-setup@v1&lt;/code&gt; in their workflows silently started getting a malicious version without making any changes to their own workflows.&lt;/p&gt;
    &lt;p&gt;Zizmor flags all of the above. It includes a dangerous-trigger rule to flag workflows triggered by &lt;code&gt;pull_request_target&lt;/code&gt;, a template-injection rule, and an unpinned-uses check that would have warned actions against using mutable references (like tags or branch names) when using &lt;code&gt;reviewdog/actions-setup@v1&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;PyPI Trusted Publishing and attestations&lt;/head&gt;
    &lt;p&gt;PyPI has taken significant steps to address several implicit trust assumptions through two complementary features: Trusted Publishing and attestations.&lt;/p&gt;
    &lt;p&gt;Trail of Bits worked with PyPI on Trusted Publishing1, which eliminates the need for long-lived API tokens. Instead of storing secrets that can be stolen, developers configure a trust relationship once: ‚Äúthis GitHub repository and workflow can publish this package.‚Äù When the workflow runs, GitHub sends a short-lived OIDC token to PyPI with claims about the repository and workflow. PyPI verifies this token was signed by GitHub‚Äôs key and responds with a short-lived PyPI token, which the workflow can use to publish the package. Using automatically generated, minimally scoped, short-lived tokens vastly reduces the risk of compromise.&lt;/p&gt;
    &lt;p&gt;Without long-lived and over-privileged API tokens, attackers must instead compromise the publishing GitHub workflow itself. While the Ultralytics attack demonstrated that CI/CD pipeline compromise is still a real threat, eliminating the need for users to manually manage credentials removes a source of user error and further reduces the attack surface.&lt;/p&gt;
    &lt;p&gt;Building on this foundation, Trail of Bits worked with PyPI again to introduce index-hosted digital attestations in late 2024 through PEP 740. Attestations cryptographically bind each published package to its build provenance using Sigstore. Packages using the PyPI publish GitHub action automatically include attestations, which act as a verifiable record of exactly where, when, and how the package was built.&lt;/p&gt;
    &lt;p&gt;Over 30,000 packages use Trusted Publishing, and ‚ÄúAre We PEP 740 Yet?‚Äù tracks attestation adoption among the most popular packages (86 of the top 360 at the time of writing). The final piece, automatic client side verification, remains a work in progress. Client tools like pip and uv do not yet verify attestations automatically. Until then, attestations provide transparency and auditability but not active protection during package installation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Homebrew build provenance&lt;/head&gt;
    &lt;p&gt;The implicit trust assumptions extend beyond programming languages and libraries. When you run &lt;code&gt;brew install&lt;/code&gt; to install a binary package (or, a bottle), you are trusting that the bottle you‚Äôre downloading was built by Homebrew‚Äôs official CI from the expected source code and that it was not uploaded by an attacker who found a way to compromise Homebrew‚Äôs bottle hosting or otherwise tamper with the bottle‚Äôs content.&lt;/p&gt;
    &lt;p&gt;Trail of Bits, in collaboration with Alpha-Omega and OpenSSF, helped to add build provenance to Homebrew using GitHub‚Äôs attestations. Every bottle built by Homebrew now comes with cryptographic proof linking it to the specific GitHub Actions workflow that created it. This makes it significantly harder for a compromised maintainer to silently replace bottles with malicious versions.&lt;/p&gt;
    &lt;p&gt;Each attestation includes the Git commit, the workflow that ran, and other build-time metadata. This transforms the trust assumption (‚ÄúI trust this bottle was built from the source I expect‚Äù) into a verifiable fact.&lt;/p&gt;
    &lt;p&gt;The implementation of attestations handled historical bottles through a ‚Äúbackfilling‚Äù process, creating attestations for packages built before the system was in place. As a result, all official Homebrew packages include attestations.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;brew verify&lt;/code&gt; command makes it straightforward to check provenance, though the feature is still in beta and verification isn‚Äôt automatic by default. There are plans to eventually extend this feature to third-party repositories, bringing the same security guarantees to the broader Homebrew ecosystem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Go Capslock&lt;/head&gt;
    &lt;p&gt;Capslock is a tool that statically identifies the capabilities of a Go program, including the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Filesystem operations (reading, writing, deleting files)&lt;/item&gt;
      &lt;item&gt;Network connections (outbound requests, listening on ports)&lt;/item&gt;
      &lt;item&gt;Process execution (spawning subprocesses)&lt;/item&gt;
      &lt;item&gt;Environment variable access&lt;/item&gt;
      &lt;item&gt;System call usage&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach represents a shift in supply chain security. Rather than focusing on who wrote the code or where it came from, capability analysis examines what the code can actually do. A JSON parsing library that unexpectedly gains network access raises immediate red flags, regardless of whether the change came from a compromised supply chain or directly from a maintainer.&lt;/p&gt;
    &lt;p&gt;In practice, static capability detection can be difficult. Language features like runtime reflection and unsafe operations make it impossible to statically detect capabilities entirely accurately. Despite the limitations, capability detection provides a critical safety net as part of a layered defense against supply chain attacks.&lt;/p&gt;
    &lt;p&gt;Capslock pioneered this approach for Go, and the concept is ripe for adoption across other languages. As supply chain attacks grow more sophisticated, capability analysis offers a promising path forward. Verify what code can do, not just where it comes from.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where we go from here&lt;/head&gt;
    &lt;p&gt;Supply chain attacks are not slowing down. If anything, they are becoming more automated, more complex, and more sophisticated in order to target broader audiences. Typosquatting campaigns are targeting packages with billions of downloads, publisher tokens and CI/CD pipelines are being compromised to poison software at the source, and patient attackers are spending years building reputation before striking.&lt;/p&gt;
    &lt;p&gt;The implicit trust that enabled software ecosystems to scale is being weaponized against us. Understanding your trust assumptions is the first step. Ask yourself these questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does my ecosystem block typosquatting packages?&lt;/item&gt;
      &lt;item&gt;How does it protect against compromised publisher tokens?&lt;/item&gt;
      &lt;item&gt;Can I verify build provenance?&lt;/item&gt;
      &lt;item&gt;Do I know what capabilities my dependencies have?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some ecosystems have started building defenses. Know what tools are available and start using them today. Use Trusted Publishing when publishing to PyPI or to crates.io. Check your GitHub Actions with Zizmor. Use It-Depends and Deptective to understand what software actually depends on. Verify attestations where feasible. Use Capslock to see the capabilities of Go packages, and more importantly, be aware when new capabilities are introduced.&lt;/p&gt;
    &lt;p&gt;But no ecosystem is completely covered. Push for better defaults where tools are lacking. Every verified attestation, every package caught typosquatting, and every flagged vulnerable GitHub action makes the entire industry more resilient. We cannot completely eliminate trust from supply chains, but we can strive to make that trust explicit, verifiable, and revocable.&lt;/p&gt;
    &lt;p&gt;If you need help understanding your supply chain trust assumptions, contact us.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The crates.io team released Trusted Publishing for Rust crates in July. ‚Ü©Ô∏é&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45836466</guid><pubDate>Thu, 06 Nov 2025 15:46:01 +0000</pubDate></item><item><title>FBI tries to unmask owner of archive.is</title><link>https://www.heise.de/en/news/Archive-today-FBI-Demands-Data-from-Provider-Tucows-11066346.html</link><description>&lt;doc fingerprint="3f820317679167cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Archive.today: FBI Demands Data from Provider Tucows&lt;/head&gt;
    &lt;p&gt;The mysterious website Archive.today is coming under the FBI's crosshairs. A court order is forcing the provider Tucows to hand over user data.&lt;/p&gt;
    &lt;p&gt;It is one of the most mysterious and, at the same time, best-known websites on the internet. Archive.today has built up a user base over a period of more than ten years who use the service to access previous snapshots of a web page. So basically like the Wayback Machine of the Internet Archive, only largely free of rules and presumably therefore also anonymous. To the chagrin of the media industry, the service is also often used to bypass paywalls. This is also possible because the service does not adhere to common rules and laws and offers no opt-out option.&lt;/p&gt;
    &lt;p&gt;And so far, the operators have gotten away with it. Although there have been minor problems in the history of the service occasionally, for example, a top-level domain operator denied them further use of one of the many archive domains. However, the operation of the project, which is allegedly financed by donations and own funds, was not seriously endangered.&lt;/p&gt;
    &lt;head rend="h3"&gt;Court Order in the USA&lt;/head&gt;
    &lt;p&gt;But now the operators of archive.today are apparently fearing bigger trouble. In recent months and years, they had become noticeably quieter. Until two years ago, for example, questions were regularly answered in the blog. In the official X account, which had been silent for over a year, a new post appeared at the end of October new post. ‚ÄúCanary,‚Äù it said there, along with a URL. The mentioned canary bird is likely an allusion to an old custom in mining. A canary brought along warned the miners when it keeled over dead about the threat of invisible gas.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;p&gt;The deadly danger that the site operators fear is apparently linked to the PDF linked in the X post linked PDF. It contains a court order that the US investigative authority FBI has obtained. It instructs the Canadian provider Tucows to hand over comprehensive data about the customer behind archive.today. It concerns address and connection data as well as payment information. If Tucows does not provide the data, penalties are threatened. Whether the court order is genuine and how the operators of the site obtained it could not be verified so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is the operator based in Russia?&lt;/head&gt;
    &lt;p&gt;Why the FBI is currently interested in archive.today, which is also accessible under the domains archive.is and archive.ph, is not evident from the court order. However, there are several obvious starting points for investigations: in addition to the obvious reason of copyright issues, the investigators could also be pursuing suspicions about unclear financing, the origin of the operators, or the technical approach.&lt;/p&gt;
    &lt;p&gt;In 2023, Finnish blogger Janni Patokallio compiled various clues and research results in a post in a post. According to this, Archive.today uses a botnet with changing IP addresses to circumvent anti-scraping measures. There are also indications that the operator(s) are based in Russia. Another private investigation from 2024 comes to a different conclusion. It names a software developer from New York as the alleged operator. According to this investigation, following the trail to Eastern Europe proved to be a red herring.&lt;/p&gt;
    &lt;p&gt;(mki)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45836826</guid><pubDate>Thu, 06 Nov 2025 16:18:18 +0000</pubDate></item><item><title>ICC ditches Microsoft 365 for openDesk</title><link>https://www.binnenlandsbestuur.nl/digitaal/internationaal-strafhof-neemt-afscheid-van-microsoft-365</link><description>&lt;doc fingerprint="edbd58ff46c91d73"&gt;
  &lt;main&gt;
    &lt;p&gt;Het Internationaal Strafhof (International Criminal Court, ICC) ruilt Microsoft 365 in voor Open Desk, een Europees open source alternatief. Dat schrijft de Duitse krant Handelsblatt. De krant verwacht dat het ICC met de overstap mogelijk een trend start binnen de Europese publieke sector.&lt;/p&gt;
    &lt;head rend="h1"&gt;Internationaal Strafhof neemt afscheid van Microsoft 365&lt;/head&gt;
    &lt;p&gt;Het ICC stapt over naar Open Desk, een Europese opensource kantooromgeving&lt;/p&gt;
    &lt;p&gt;Microsoft bevestigt de breuk aan de nieuwssite Euractiv. ‚ÄòWij hechten waarde aan onze relatie met het ICC als klant en zijn ervan overtuigd dat niets ons vermogen in de weg staat om in de toekomst diensten aan het ICC te blijven leveren,‚Äô zegt een woordvoerder van Microsoft.&lt;/p&gt;
    &lt;head rend="h2"&gt;Digitale afhankelijkheid&lt;/head&gt;
    &lt;p&gt;Bij Europese overheden leven al langer zorgen over de digitale afhankelijkheid van Amerikaanse bedrijven. Die zorgen zijn sterk toegenomen sinds Donald Trump voor de tweede maal president van de Verenigde Staten werd.&lt;/p&gt;
    &lt;p&gt;Voor het ICC zijn de zorgen minder hypothetisch dan voor veel andere instituten: Trump heeft zijn ongenoegen met het Strafhof vaak laten blijken en liet sancties opstellen tegen de hoofdaanklager, Karim Khan. Persbureau AP meldde in mei 2025 dat Khan geen toegang meer had tot zijn Outlook e-mail. Microsoft bevestigde dat Khan was ‚Äòlosgekoppeld‚Äô van Microsoft-diensten, maar benadrukte tegelijkertijd dat het de dienstverlening aan de ICC-organisatie ‚Äògeen moment‚Äô heeft stopgezet.&lt;/p&gt;
    &lt;head rend="h2"&gt;EDIC&lt;/head&gt;
    &lt;p&gt;Hoe dan ook zit de angst er bij het ICC goed in. De organisatie gaat gebruikmaken van Open Desk, dat is ontwikkeld door het Zentrum Digitale Souver√§nit√§t (Zendis) in opdracht van het Duitse Federale ministerie van Binnenlandse Zaken. Zendis maakt onderdeel uit van het Digital Commons European Digital Infrastructure Consortium (DC-EDIC), het Europese consortium waarmee de EU strijdt voor meer digitale autonomie.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mijn bureau&lt;/head&gt;
    &lt;p&gt;Ook Nederlandse ambtenaren krijgen in de toekomst mogelijk te maken met de software van Open Desk. Onder de noemer Mijn Bureau experimenteert de Nederlandse overheid met een suite met verschillende Europese open source samenwerkingssoftware. Open Desk wordt binnen Mijn Bureau onder meer gebruikt voor e-mail. Mijn Bureau is een samenwerking van de Rijksoverheid, Gemeente Amsterdam en de VNG.&lt;/p&gt;
    &lt;p&gt;In een vandaag verschenen position paper pleit de VNG voor meer regie op technologie. Er wordt steeds meer toegewerkt naar verregaande samenwerking op het gebied van digitalisering. ln de Nederlandse Digitaliseringsstrategie (NDS) is het versterken van digitale weerbaarheid en autonomie √©√©n van de prioriteiten.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45837342</guid><pubDate>Thu, 06 Nov 2025 16:57:55 +0000</pubDate></item><item><title>Senior BizOps at Artie (San Francisco)</title><link>https://www.ycombinator.com/companies/artie/jobs/gqANVBc-senior-business-operations</link><description>&lt;doc fingerprint="e0b240d575995fac"&gt;
  &lt;main&gt;
    &lt;p&gt;Software that streams data from databases to warehouses in real-time&lt;/p&gt;
    &lt;p&gt;Artie is a fully-managed CDC streaming platform - we replicate production databases into warehouses in real time with zero maintenance. Teams use us for fraud/risk monitoring, live inventory, customer-facing analytics, and ML pipelines.&lt;/p&gt;
    &lt;p&gt;We‚Äôre trusted by 30+ customers including Substack, Alloy, and Wasserman - and just raised our Series A led by Standard Capital, building on our seed round backed by YC and General Catalyst.&lt;/p&gt;
    &lt;p&gt;We‚Äôre hiring a Senior Business Operations to help shape Artie‚Äôs next chapter of growth. This is one of the most high-leverage roles at the company - you‚Äôll sit at the intersection of product, GTM, and operations, and work directly with the founders on the questions that matter most.&lt;/p&gt;
    &lt;p&gt;As an early member of BizOps, you won‚Äôt just analyze problems. You‚Äôll own them end-to-end. You‚Äôll spot opportunities before others see them, turn signal from data into direction, and build the systems and processes that enable Artie to scale with speed and precision. If you get energy from ambiguity, love turning messy into simple, and want meaningful ownership in a company that‚Äôs growing fast - this is that role.&lt;/p&gt;
    &lt;p&gt;Be a Builder: Shape the BizOps foundation of the company as an early member.&lt;/p&gt;
    &lt;p&gt;Be an Owner: Lead high-impact product and business initiatives end-to-end.&lt;/p&gt;
    &lt;p&gt;Go Deep and Broad: Operate across product strategy, data analytics, GTM, and operations.&lt;/p&gt;
    &lt;p&gt;Partner with Founders: Work directly with the CEO and CTO on the problems that matter most.&lt;/p&gt;
    &lt;p&gt;We are building Artie, a real-time data streaming solution focused on databases and data warehouses. Typical ETL solutions leverage batched processes or schedulers (DAGs, Airflow), which cannot achieve real time data syncs. We leverage change data capture (CDC) and stream processing to perform data transfers in a more efficient way, which enables sub-minute latency.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45837364</guid><pubDate>Thu, 06 Nov 2025 17:00:03 +0000</pubDate></item><item><title>The Parallel Search API</title><link>https://parallel.ai/blog/introducing-parallel-search</link><description>&lt;doc fingerprint="19437a9227bb0e96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;# Introducing Parallel Search: the highest accuracy web search API engineered for AI&lt;/head&gt;
    &lt;p&gt;Web search, built from the ground up for AI&lt;/p&gt;
    &lt;p&gt;A second user has arrived on the web: AI. And it needs fundamentally different infrastructure than humans do.&lt;/p&gt;
    &lt;p&gt;The Parallel Search API, built on our proprietary web index, is now generally available. It's the only web search tool designed from the ground up for AI agents: engineered to deliver the most relevant, token-efficient web data at the lowest cost. The result is more accurate answers, fewer round-trips, and lower costs for every agent.&lt;/p&gt;
    &lt;head rend="h2"&gt;## **Human search and AI search solve different problems**&lt;/head&gt;
    &lt;p&gt;Traditional search engines were built for humans. They rank URLs, assuming someone will click through and navigate to a page. The search engine's job ends at the link. The system optimizes for keywords searches, click-through rates, and page layouts designed for browsing - done in milliseconds and as cheaply as possible.&lt;/p&gt;
    &lt;p&gt;The first wave of web search APIs used in AI-based search made this human search paradigm programmatically accessible, but failed to solve the underlying problem of how you design search for an AI agent‚Äôs needs.&lt;/p&gt;
    &lt;p&gt;AI search has to solve a different problem: **what tokens should go in an agent's context window to help it complete the task? We‚Äôre not ranking URLs for humans to click‚Äî we‚Äôre optimizing context and tokens for models to reason over.**&lt;/p&gt;
    &lt;p&gt;This requires a fundamentally different search architecture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Semantic objectives** that capture intent beyond keyword matching, so agents can specify what they need to accomplish rather than guessing at search terms&lt;/item&gt;
      &lt;item&gt;- **Token-relevance ranking** to prioritize webpages most directly relevant to the objective, not pages optimized for human engagement metrics&lt;/item&gt;
      &lt;item&gt;- **Information-dense excerpts** compressed and prioritized for reasoning quality, so LLMs have the highest-signal tokens in their context window&lt;/item&gt;
      &lt;item&gt;- **Single-call resolution** for complex queries that normally require multiple search hops&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this search architecture built from the ground up for AIs, agents get access to the most information-dense web tokens in their context. The result is fewer search calls, higher accuracy, lower cost, and lower end-to-end latency.&lt;/p&gt;
    &lt;head rend="h2"&gt;## **On every benchmark that matters for real-world agent use cases, Parallel wins on accuracy**&lt;/head&gt;
    &lt;p&gt;While most existing search systems are optimized for straightforward question answering, we believe the demand for more complex, multifaceted search will only continue to grow. Users and agents alike will increasingly seek answers that require synthesizing information across multiple sources, reasoning over complex objectives, and navigating harder-to-access content on the web.&lt;/p&gt;
    &lt;p&gt;To reflect this shift, we evaluated the performance of Parallel‚Äôs Search API across a range of benchmarks, from the most challenging multi-hop tasks (e.g., BrowseComp) to simple single-hop queries (e.g., SimpleQA).&lt;/p&gt;
    &lt;head rend="h3"&gt;### For complex searches, Parallel is the highest accuracy at the lowest cost&lt;/head&gt;
    &lt;p&gt;Parallel‚Äôs performance advantage is dramatic on challenging queries ‚Äî those that span multiple topics, require deep comprehension of hard to crawl web content, or demand synthesis across scattered sources with multiple reasoning steps. On benchmarks specifically designed to test multi-hop reasoning (HLE, BrowseComp, WebWalker, FRAMES, Batched SimpleQA), Parallel not only achieves higher accuracy but also resolves queries through more efficient reasoning paths.&lt;/p&gt;
    &lt;p&gt;Traditional search APIs get less done in each pass. Agents perform too many sequential searches - compounding latency, inflating context windows, and increasing token costs with every iteration, and decreasing accuracy. Parallel, by contrast, can resolve more complex queries in a single call, resulting in the agent making fewer sequential calls and achieving higher accuracy, lower total cost, and lower end to end latency.&lt;/p&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://lastexam.ai/) consists of 2,500 questions developed by subject-matter experts across dozens of subjects (e.g. math, humanities, natural sciences). Each question has a known solution that is unambiguous and easily verifiable, but requires sophisticated web retrieval and reasoning. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;### HLE Search&lt;/head&gt;
    &lt;quote&gt;| Series | Model | Cost (CPM) | Accuracy (%) | | --------- | ------------ | ----------- | ------------ | | Parallel | parallel | 82 | 47 | | Others | exa | 138 | 24 | | Others | tavily | 190 | 21 | | Others | perplexity | 126 | 30 | | Others | openai gpt-5 | 143 | 45 |&lt;/quote&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://lastexam.ai/) consists of 2,500 questions developed by subject-matter experts across dozens of subjects (e.g. math, humanities, natural sciences). Each question has a known solution that is unambiguous and easily verifiable, but requires sophisticated web retrieval and reasoning. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;### BrowseComp Search&lt;/head&gt;
    &lt;quote&gt;| Series | Model | Cost (CPM) | Accuracy (%) | | --------- | ------------ | ----------- | ------------ | | Parallel | parallel | 156 | 58 | | Others | exa | 233 | 29 | | Others | tavily | 314 | 23 | | Others | perplexity | 256 | 22 | | Others | openai gpt-5 | 253 | 53 |&lt;/quote&gt;
    &lt;head rend="h3"&gt;### About the benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://openai.com/index/browsecomp/), created by OpenAI, contains 1,266 questions requiring multi-hop reasoning, creative search formulation, and synthesis of contextual clues across time periods. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;### WebWalker-Search&lt;/head&gt;
    &lt;quote&gt;| Series | Model | Cost (CPM) | Accuracy (%) | | --------- | ------------ | ----------- | ------------ | | Parallel | parallel | 42 | 81 | | Others | exa | 107 | 48 | | Others | tavily | 156 | 79 | | Others | perplexity | 91 | 67 | | Others | openai gpt-5 | 88 | 73 |&lt;/quote&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://arxiv.org/abs/2501.07572) is designed to assess the ability of LLMs to perform web traversal. To successfully answer the questions in the benchmark, it requires the ability to crawl and extract content from website subpages. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;### FRAMES-Search&lt;/head&gt;
    &lt;quote&gt;| Series | Model | Cost (CPM) | Accuracy (%) | | --------- | ------------ | ----------- | ------------ | | Parallel | parallel | 42 | 92 | | Others | exa | 81 | 81 | | Others | tavily | 122 | 87 | | Others | perplexity | 95 | 83 | | Others | openai gpt-5 | 68 | 90 |&lt;/quote&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://huggingface.co/datasets/google/frames-benchmark) contains 824 challenging multi-hop questions designed to test factuality, retrieval accuracy, and reasoning. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;### Batched SimpleQA - Search&lt;/head&gt;
    &lt;quote&gt;| Series | Model | Cost (CPM) | Accuracy (%) | | --------- | ------------ | ----------- | ------------ | | Parallel | parallel | 50 | 90 | | Others | exa | 119 | 71 | | Others | tavily | 227 | 59 | | Others | perplexity | 100 | 74 | | Others | openai gpt-5 | 91 | 88 |&lt;/quote&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark was created by batching 3 independent questions from the original SimpleQA dataset[SimpleQA dataset]($https://openai.com/index/introducing-simpleqa/) to create 100 composite, more complex, questions.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Across these multi-hop benchmarks, agents using Parallel achieve state-of-the-art accuracy at ~50% of the cost, compared to workflows built on traditional search APIs.&lt;/p&gt;
    &lt;head rend="h3"&gt;### On simple searches, Parallel is the lowest cost with parity in accuracy&lt;/head&gt;
    &lt;p&gt;We also tested Parallel on single-hop benchmarks like SimpleQA that contain straightforward factual queries that benefit from web search. These benchmarks are saturated with limited room for further accuracy improvements.&lt;/p&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://openai.com/index/introducing-simpleqa/), created by OpenAI, contains 4,326 questions focused on short, fact-seeking queries across a variety of domains. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;### SimpleQA Search&lt;/head&gt;
    &lt;quote&gt;| Series | Model | Cost (CPM) | Accuracy (%) | | --------- | ------------ | ----------- | ------------ | | Parallel | parallel | 17 | 98 | | Others | exa | 57 | 87 | | Others | tavily | 110 | 93 | | Others | perplexity | 52 | 92 | | Others | openai gpt-5 | 37 | 98 |&lt;/quote&gt;
    &lt;head rend="h3"&gt;### About this benchmark&lt;/head&gt;
    &lt;p&gt;This benchmark[benchmark]($https://openai.com/index/introducing-simpleqa/), created by OpenAI, contains 4,326 questions focused on short, fact-seeking queries across a variety of domains. Results are reported on a sample of 100 questions from this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;### Methodology&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;- **Evaluation**: Results are based on tests run using official Search MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/item&gt;
      &lt;item&gt;- **Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/item&gt;
      &lt;item&gt;- **Testing Dates**: Testing was conducted from November 3rd to November 5th.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On SimpleQA, the Parallel Search API matches the accuracy of the leading alternative while delivering the lowest end-to-end cost per-query cost.&lt;/p&gt;
    &lt;head rend="h2"&gt;## **These results are possible because we've built a proprietary web index and a vertically-integrated search stack from the ground up, designed for AIs**&lt;/head&gt;
    &lt;p&gt;We are able to achieve state-of-the-art results because we have spent the last two years building the infrastructure to innovate across the full search stack, enabling optimization at every layer and feedback loops that continuously improve performance.&lt;/p&gt;
    &lt;p&gt;**Crawl:** Infrastructure that prioritizes the hard-to-crawl content on the web that isn‚Äôt included in pretraining data for models: multi-modal, lengthy PDFs, JavaScript-heavy sites. And optimizes recrawls to keep fast-changing data fresh while minimizing burden on website owners.&lt;/p&gt;
    &lt;p&gt;**Index:** One of the fastest-growing, freshest, deepest, and largest web indexes with 1B+ pages added or refreshed daily.&lt;/p&gt;
    &lt;p&gt;**Ranking:** We retrieve and rank with a different optimization objective than traditional search. Instead of ranking URLs for humans to click on, we identify the most relevant and authoritative tokens suitable for LLM reasoning. Our proprietary models and algorithms score based on token relevance, page and domain authority, context window efficiency, and cross-source validation, rather than click-through probability or engagement.&lt;/p&gt;
    &lt;head rend="h2"&gt;## **Leading AI teams build on our Search API - and so do we**&lt;/head&gt;
    &lt;p&gt;Today, the most sophisticated builders choose to create and deploy AI, with search powered by Parallel. These companies have tested alternatives and understand that the decisions their agents make, whether it‚Äôs Sourcegraph Amp‚Äôs coding agent solving bugs, _Claygent_ powering every GTM decision, Starbridge discovering government RFPs, or a Fortune 100 insurer underwriting claims better than human underwriters, all depend on the quality of their web data.&lt;/p&gt;
    &lt;p&gt;We use our own Search API as foundational infrastructure to power our Web Agents. For example, the Parallel Task API, our higher-level research API that serves complex, multi-step enrichment and deep research queries, is built using the Search API. Every Task API query that runs in production depends on the Search API performing flawlessly underneath.&lt;/p&gt;
    &lt;p&gt;This architectural decision forces us to hold ourselves to the highest standard. Every performance improvement, latency optimization, and quality enhancement in the Search API directly impacts our own production systems serving millions of queries daily. We feel every token of inefficiency and every accuracy gap immediately in our own products.&lt;/p&gt;
    &lt;p&gt;The result is infrastructure that's been battle-tested and continuously refined under the demands of real-world agent workloads.&lt;/p&gt;
    &lt;head rend="h2"&gt;## **Give your agents access to Parallel Search**&lt;/head&gt;
    &lt;p&gt;Maximizing signal and minimizing noise in an agent‚Äôs context window is the single most important factor in the ability of the agent to complete a task effectively. Give your agents the most accurate and compressed context from the web with the Parallel Search API.&lt;/p&gt;
    &lt;p&gt;Give your agents access to better search. Get started in our Developer Platform[Developer Platform]($https://platform.parallel.ai/play/search) or dive into the documentation[documentation]($https://docs.parallel.ai/search/search-quickstart).&lt;/p&gt;
    &lt;head rend="h2"&gt;## **Notes on Methodology**&lt;/head&gt;
    &lt;p&gt;**Benchmark Details**: Various search providers were evaluated against a wide set of benchmarks ranging from simple benchmarks (SimpleQA) to more complex benchmarks (HLE, BrowseComp, Batched SimpleQA, WebWalker, and Frames).&lt;/p&gt;
    &lt;p&gt;**Evaluation**: Results are based on tests run using official MCP servers provided as an MCP tool to OpenAI's GPT-5 model using the Responses API. In all cases, the MCP tools were limited to only the appropriate web search tool. Answers were evaluated using an LLM as a judge (GPT 4.1).&lt;/p&gt;
    &lt;p&gt;**Cost Calculation**: Cost reflects the average cost per query across all questions run. This cost includes both the search API call and LLM token cost.&lt;/p&gt;
    &lt;p&gt;**Testing Dates**: Testing was conducted from November 3rd to November 5th. &lt;/p&gt;
    &lt;p&gt;By Parallel&lt;/p&gt;
    &lt;p&gt;November 6, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45837425</guid><pubDate>Thu, 06 Nov 2025 17:04:41 +0000</pubDate></item><item><title>Swift on FreeBSD Preview</title><link>https://forums.swift.org/t/swift-on-freebsd-preview/83064</link><description>&lt;doc fingerprint="6d085b1b9ed1ced2"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;etcwilde
(Evan Wilde)
1&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;We have been hard at work to bring the Swift toolchain to FreeBSD. A preview Swift bundle for FreeBSD 14.3+ is available at https://download.swift.org/tmp-ci-nightly/development/freebsd-14_ci_latest.tar.gz. The bundle contains a Swift development compiler and Swift runtimes needed for compiling Swift programs on, and for, FreeBSD 14 on &lt;code&gt;x86_64&lt;/code&gt; machines.&lt;/p&gt;
        &lt;head rend="h2"&gt;Dependencies&lt;/head&gt;
        &lt;p&gt;The Swift compiler and runtimes have a few dependencies. Please install the following dependencies:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;zlib-ng&lt;/item&gt;
          &lt;item&gt;python3&lt;/item&gt;
          &lt;item&gt;sqlite3&lt;/item&gt;
          &lt;item&gt;libuuid&lt;/item&gt;
          &lt;item&gt;curl&lt;/item&gt;
        &lt;/list&gt;
        &lt;head rend="h2"&gt;Known Issues&lt;/head&gt;
        &lt;p&gt;The compiler in the bundle is still under development and isn't part of a release yet and we're not quite done porting everything to FreeBSD.&lt;/p&gt;
        &lt;p&gt;Here is a list of known issues that you may run into while trying things out.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Thread sanitizer reports incorrect failures &lt;/item&gt;
          &lt;item&gt;LLDB is unable to execute Swift expressions &lt;/item&gt;
          &lt;item&gt;Command Plugins in a SwiftPM package hangs &lt;/item&gt;
          &lt;item&gt;Using standard types with C++ interop results in an undefined voidify symbol &lt;/item&gt;
          &lt;item&gt;Importing the C libraries is done through "Glibc". This will change to &lt;code&gt;import FreeBSD&lt;/code&gt;

&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;lld&lt;/code&gt; and &lt;code&gt;lldb&lt;/code&gt; depend on &lt;code&gt;libxml2.so.2&lt;/code&gt;, which is not be available in the system package manager.

&lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt;We are investigating adding aarch64 support and making the bundle available for all minor versions of FreeBSD 14.&lt;/p&gt;
        &lt;p&gt;As you find more bugs, please file issues at https://github.com/swiftlang/swift/issues.&lt;/p&gt;
        &lt;p&gt;We look forward to hearing your feedback. If you're interested in helping add the finishing polish, please feel free to reach out here on the forums.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 18 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;kebo
(Kenta Kubo)
2&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;On FreeBSD 15, the following error occurs when executing &lt;code&gt;swift&lt;/code&gt;.&lt;/p&gt;
        &lt;quote&gt;
          &lt;code&gt;ld-elf.so.1: Shared object "libutil.so.9" not found, required by "swift"
&lt;/code&gt;
        &lt;/quote&gt;
        &lt;p&gt;As a temporary workaround, &lt;code&gt;doas pkg install compat14x-amd64&lt;/code&gt; will solve the issue.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Great news, thank you. Registered to this forum just to say that.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;etcwilde
(Evan Wilde)
4&lt;/div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;p&gt;On FreeBSD 15, the following error occurs when executing &lt;code&gt;swift&lt;/code&gt; .&lt;/p&gt;
        &lt;/quote&gt;
        &lt;p&gt;Yes, the FreeBSD stability policy appears to be within a major version. The bundle is built for FreeBSD 14. I'm glad to see that you were able to find a workaround though.&lt;/p&gt;
        &lt;quote&gt;
          &lt;p&gt;For -STABLE branches, it's important to make sure that ABI is compatible across dot releases (in other words, user can expect applications that is compiled for X.0 would run without modification on any X.y releases). We also try to maintain ABI compatibility across .0 releases, but they are not strictly enforced except for libraries that already implements versioned symbols.&lt;/p&gt;
        &lt;/quote&gt;
        &lt;p&gt;https://wiki.freebsd.org/Releng/ABI&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45837871</guid><pubDate>Thu, 06 Nov 2025 17:37:49 +0000</pubDate></item><item><title>OpenDesk ‚Äì a flexible all-in-one office suite for the public sector</title><link>https://www.opendesk.eu/de</link><description>&lt;doc fingerprint="3d57f34ff803ed73"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;MPK: Sichere Zusammenarbeit mit openDesk&lt;/head&gt;&lt;p&gt;Mit dem Ziel von 160.000 Lizenzen in der deutschen Verwaltung bis Ende 2025 beweist openDesk seine St√§rke nicht nur bei Gro√üeinrichtungen wie dem Robert-Koch-Institut, sondern gerade auch in kleineren, strategisch entscheidenden Runden.&lt;/p&gt;Praxiswissen&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45838239</guid><pubDate>Thu, 06 Nov 2025 18:04:28 +0000</pubDate></item><item><title>Benchmarking the Most Reliable Document Parsing API</title><link>https://www.tensorlake.ai/blog/benchmarks</link><description>&lt;doc fingerprint="a245660aad22ea5b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Benchmarking the Most Reliable Document Parsing API&lt;/head&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Traditional document parsing benchmarks measure text similarity while ignoring structural preservation and downstream usability. Tensorlake's new Document Parsing model achieves 91.7% accuracy in enterprise documents‚Äîoutperforming Azure, AWS Textract, and open-source alternatives.&lt;/p&gt;
    &lt;p&gt;Document parsing is the foundation of enterprise AI applications. Whether you're building RAG pipelines, automating insurance claims, or extracting data from financial reports, everything starts with one question: Can you consistently transform messy, real-world documents into structured, machine-readable data?&lt;/p&gt;
    &lt;p&gt;Our customers need the best document ingestion API for their use cases. They're comparing Azure, AWS Textract, popular open-source models like Docling and Marker.&lt;/p&gt;
    &lt;p&gt;We built a benchmark that measures what matters: Can downstream systems actually use this output?&lt;/p&gt;
    &lt;head rend="h2"&gt;Measuring What Actually Matters#&lt;/head&gt;
    &lt;p&gt;Tensorlake both reads documents and extracts structured data, so when choosing what to measure accuracy with, we wanted to ensure we were measuring both document parsing with structural preservation and structured extraction for downstream usability.&lt;/p&gt;
    &lt;p&gt;The aspects of Document Parsing that we wanted to measure were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tables: Ensuring we can parse and measure accuracy of complex tables with merged cells and multi-row headers&lt;/item&gt;
      &lt;item&gt;Reading Order: In multi-column documents, and documents with complex layouts, we measure whether the reading order is preserved while parsing.&lt;/item&gt;
      &lt;item&gt;Structured Extraction Accuracy: Measuring direct downstream usability of extracted data. A small OCR error in parsing a table cell can cause failure in achieving the downstream task, while the overall accuracy of the OCR on the document may be high.&lt;/item&gt;
      &lt;item&gt;Extraction of footnotes, formulas, figures and other non-textual content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our Evaluation Methodology#&lt;/head&gt;
    &lt;p&gt;We employ two metrics that better capture these features with real-world reliability:&lt;/p&gt;
    &lt;head rend="h3"&gt;TEDS (Tree Edit Distance Similarity)#&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compares predicted and ground-truth Markdown/HTML tree structures&lt;/item&gt;
      &lt;item&gt;Captures structural fidelity in tables and complex layouts&lt;/item&gt;
      &lt;item&gt;Widely adopted in OCRBench v2 and OmniDocBench evaluations&lt;/item&gt;
      &lt;item&gt;Measures whether the document's logical structure and textual alignment remains intact&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TEDS answers: "Is this table still a table?" Not just "Is the text similar?"&lt;/p&gt;
    &lt;head rend="h3"&gt;JSON F1 (Field-Level Precision and Recall)#&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compares extracted JSON against schema-based ground truth&lt;/item&gt;
      &lt;item&gt;Precision measures correctness of extracted fields&lt;/item&gt;
      &lt;item&gt;Recall measures completeness of required field capture&lt;/item&gt;
      &lt;item&gt;F1 score balances both for overall reliability assessment&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;JSON F1 answers: "Can downstream automation actually use this data?" Not just "Is some text present?"&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Together, these metrics answer the essential question: "Can downstream systems use this output?" rather than simply "Is the text similar?"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Stage 1: Document Reading Ability (OCR and Structural Preservation)&lt;lb/&gt; Each parsing model generates Markdown/HTML output. We evaluate using TEDS to measure how well structure is preserved; reading order, table integrity, and layout coherence. You can find our updated dataset published here. We use the public OCRBench v2 and OmniDocBench datasets. However, upon review, we identified inconsistencies in the published ground truth of OCRBench v2. We conducted a comprehensive audit and correction to ensure evaluation accuracy.&lt;/p&gt;
    &lt;p&gt;Stage 2: Structured Extraction Accuracy (Downstream Usability)&lt;lb/&gt; We pass the Markdown through a standardized LLM (GPT-4o) with predefined JSON schemas, measuring JSON F1. This isolates how OCR quality impacts real extraction workflows, where an LLM interprets the parsed text. Initial JSON schemas and reference answers are generated using Gemini Pro 2.5, then human reviewers audit and correct them to ensure high-quality gold standards.&lt;/p&gt;
    &lt;p&gt;This methodology ensures fair, reproducible comparisons by varying only the OCR models (Stage 1) while keeping the extraction model constant (Stage 2).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results: Public Dataset Performance#&lt;/head&gt;
    &lt;head rend="h3"&gt;Document Parsing Performance#&lt;/head&gt;
    &lt;p&gt;We evaluated leading open-source and proprietary models:&lt;/p&gt;
    &lt;p&gt;Key Findings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tensorlake achieves the highest TEDS score, indicating superior structural preservation&lt;/item&gt;
      &lt;item&gt;The gap between Docling and production-grade systems is substantial&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Table Parsing Performance#&lt;/head&gt;
    &lt;p&gt;We evaluated Tensorlake‚Äôs table parsing accuracy using the OmniDocBench dataset ‚Äî a CVPR-accepted benchmark for comprehensive document understanding tasks (GitHub link).&lt;/p&gt;
    &lt;p&gt;Table accuracy in OmniDocBench is quantified using a combination of tree-based and string-based metrics. In particular, we measured TEDS (Tree Edit Distance Similarity), which assesses both the structural and textual alignment between predicted and ground-truth HTML tables.&lt;/p&gt;
    &lt;p&gt;To reproduce our results, generate Markdown outputs using the models listed below, then run the evaluation method provided in the OmniDocBench repository. We have used 512 document images with tables and v1.5 of the code version. Evaluation outputs are released in Huggingface(link)&lt;/p&gt;
    &lt;p&gt;¬π Marker's Number is from the officially published OmniDocBench repository.&lt;/p&gt;
    &lt;p&gt;Key Findings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On OmniDocBench's challenging tables, Tensorlake leads with 86.79% TEDS&lt;/item&gt;
      &lt;item&gt;Open-source solutions struggle with table extraction (sub-70% TEDS)&lt;/item&gt;
      &lt;item&gt;Tensorlake maintains table structure even on complex, multi-page tables&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance on Real World Enterprise Documents#&lt;/head&gt;
    &lt;p&gt;OCR Models are rarely trained on enterprise documents, because they are not publicly available. We wanted to test how well our model performs and others perform on these documents.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enterprise Document Performance (100 pages)#&lt;/head&gt;
    &lt;p&gt;We curated 100 document pages spanning banking, retail, and insurance sectors. This represents real production workloads: invoices with water damage, scanned contracts with skewed text, bank statements with multi-level tables.&lt;/p&gt;
    &lt;p&gt;Key Findings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tensorlake achieves 91.7% F1 with standard extraction, beating all competitors&lt;/item&gt;
      &lt;item&gt;The difference between 91.7% and 68.9% F1 is massive: it‚Äôs 5 extra fields correctly extracted out of every 20&lt;/item&gt;
      &lt;item&gt;In production workflows processing thousands of documents daily, this accuracy gap compounds into significant error reduction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But even comparing the higher F1 scores when parsing a standard form, Azure and Textract jumble the reading order and skip data completely, whereas Tensorlake preserves the complex reading order and groups data correctly and accurately:&lt;/p&gt;
    &lt;head rend="h2"&gt;Delivering the Best Performance/Price Ratio#&lt;/head&gt;
    &lt;p&gt;Accuracy without affordability isn't practical. Here's how Tensorlake compares to other Document Ingestion APIs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Tensorlake: $10 per 1k pages&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;TEDS Score: 86.79&lt;/item&gt;
          &lt;item&gt;F1 Score: 91.7&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Azure: $10 per 1k pages&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;TEDS Score: 78.14&lt;/item&gt;
          &lt;item&gt;F1 Score: 88.1&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AWS Textract: $15 per 1k pages&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;TEDS Score: 80.75&lt;/item&gt;
          &lt;item&gt;F1 Score: 88.4&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tensorlake delivers the highest accuracy than both Azure and AWS Textract, matching Azure's cost while AWS Textract is 50% more expensive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Take the Next Step#&lt;/head&gt;
    &lt;p&gt;When your business depends on accurate document processing, you can't afford to use anything less.&lt;/p&gt;
    &lt;p&gt;Want to discuss your specific use case?&lt;lb/&gt; Schedule a technical demo with our team.&lt;/p&gt;
    &lt;p&gt;Questions about the benchmark?&lt;lb/&gt; Join our Slack community&lt;/p&gt;
    &lt;head rend="h3"&gt;Dr Sarah Guthals&lt;/head&gt;
    &lt;p&gt;Founding DevRel Engineer at Tensorlake&lt;/p&gt;
    &lt;p&gt;Founding DevRel Engineer at Tensorlake, blending deep technical expertise with a decade of experience leading developer engagement at companies like GitHub, Microsoft, and Sentry. With a PhD in Computer Science and a background in founding developer education startups, I focus on building tools, content, and communities that help engineers work smarter with AI and data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45838365</guid><pubDate>Thu, 06 Nov 2025 18:12:56 +0000</pubDate></item><item><title>Show HN: TabPFN-2.5 ‚Äì SOTA foundation model for tabular data</title><link>https://priorlabs.ai/technical-reports/tabpfn-2-5-model-report</link><description>&lt;doc fingerprint="1d6a74aa8f3ca9cb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TabPFN-2.5 Model Report&lt;/head&gt;
    &lt;head rend="h3"&gt;Abstract&lt;/head&gt;
    &lt;p&gt;The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases.&lt;/p&gt;
    &lt;p&gt;This report introduces TabPFN-2.5, the next generation of our tabular foundation model, scaling to 20√É data cells compared to TabPFNv2. On industry standard benchmarks with up to 50,000 data points and 2,000 features, TabPFN-2.5 substantially outperforms tuned tree-based models and matches the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2.&lt;/p&gt;
    &lt;p&gt;For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment.This new release will immediately strengthen the performance of the many applications andmethods already built on the TabPFN ecosystem.&lt;/p&gt;
    &lt;p&gt;This new release will substantially strengthen the performance of the many applications and methods already built on TabPFN.&lt;/p&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Tabular data is ubiquitous, forming the backbone of decision-making in countless domains, from finance to healthcare. For decades, traditional tabular machine learning√¢built on gradient-boosted trees, random forests, and linear or additive models√¢has been the workhorse of applied data science. Yet these methods remain limited: they require extensive dataset-specific tuning, often provide uncalibrated or unreliable uncertainty estimates without significant modification, and lack the generalization and transferability of modern foundation models.&lt;/p&gt;
    &lt;p&gt;Tabular foundation models (TFMs) offer a new paradigm. They address these limitations by pretraining on large synthetic distributions of tabular tasks and performing inference via in-context learning instead of gradient descent. They are training-free predictors meta-trained to yield strong calibration, without the need for time-consuming and labor-intensive hyperparameter tuning necessary for gradient-boosted trees. Their strong generalization makes them particularly attractive for data-scarce domains.&lt;/p&gt;
    &lt;p&gt;Our initial release, TabPFNv1, served as a proof-of-concept that a transformer could learn a Bayesian-like inference algorithm, though it was limited to small (up to 1,000 samples), clean, numerical-only data. Our successor, TabPFNv2, scaled this idea into a practical model for datasets up to 10,000 samples. TabPFNv2 handles the messy and heterogeneous data seen in the real world√¢including categorical features, missing values &amp;amp; outliers.&lt;/p&gt;
    &lt;head rend="h3"&gt;What's New in TabPFN-2.5&lt;/head&gt;
    &lt;p&gt;State-of-the-Art Performance&lt;/p&gt;
    &lt;p&gt;In a forward pass, TabPFN-2.5 outperforms tuned tree-based models (like XGBoost and CatBoost) and matches the accuracy of AutoGluon 1.4 tuned for 4 hours√¢a complex ensemble that includes all previous methods, even TabPFNv2.&lt;/p&gt;
    &lt;p&gt;Improved Scalability&lt;/p&gt;
    &lt;p&gt;We scale the power of in-context learning to datasets of up to 50,000 samples (5√É increase over TabPFNv2) and 2,000 features (4√É increase), making TFMs viable for a much wider range of real-world problems.&lt;/p&gt;
    &lt;p&gt;Fast Inference&lt;/p&gt;
    &lt;p&gt;We've dramatically improved inference latency. Our proprietary distillation engine converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45838540</guid><pubDate>Thu, 06 Nov 2025 18:26:53 +0000</pubDate></item></channel></rss>