<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 05 Feb 2026 09:05:51 +0000</lastBuildDate><item><title>Voxtral Transcribe 2</title><link>https://mistral.ai/news/voxtral-transcribe-2</link><description>&lt;doc fingerprint="3c2dcd92a1ee1e85"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Voxtral transcribes &lt;lb/&gt; at the speed of sound.&lt;/head&gt;Try Voxtral Transcribe 2 in Mistral Studio&lt;p&gt; Precision diarization, real-time&lt;lb/&gt; transcription, and a new audio playground.&lt;/p&gt;&lt;p&gt;Today, we're releasing Voxtral Transcribe 2, two next-generation speech-to-text models with state-of-the-art transcription quality, diarization, and ultra-low latency. The family includes Voxtral Mini Transcribe V2 for batch transcription and Voxtral Realtime for live applications. Voxtral Realtime is open-weights under the Apache 2.0 license.&lt;/p&gt;&lt;p&gt;We're also launching an audio playground in Mistral Studio to test transcription instantly, powered by Voxtral Transcribe 2, with diarization and timestamps.&lt;/p&gt;&lt;head rend="h2"&gt;Highlights.&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Voxtral Mini Transcribe V2: State-of-the-art transcription with speaker diarization, context biasing, and word-level timestamps in 13 languages.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Voxtral Realtime: Purpose-built for live transcription with latency configurable down to sub-200ms, enabling voice agents and real-time applications.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Best-in-class efficiency: Industry-leading accuracy at a fraction of the cost, with Voxtral Mini Transcribe V2 achieving the lowest word error rate, at the lowest price point.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Open weights: Voxtral Realtime ships under Apache 2.0, deployable on edge for privacy-first applications.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Voxtral Realtime.&lt;/head&gt;&lt;p&gt;Voxtral Realtime is purpose-built for applications where latency matters. Unlike approaches that adapt offline models by processing audio in chunks, Realtime uses a novel streaming architecture that transcribes audio as it arrives. The model delivers transcriptions with delay configurable down to sub-200ms, unlocking a new class of voice-first applications.&lt;/p&gt;&lt;p&gt;Word error rate (lower is better) across languages in the FLEURS transcription benchmark.&lt;/p&gt;&lt;p&gt;At 2.4 seconds delay, ideal for subtitling, Realtime matches Voxtral Mini Transcribe V2, our latest batch model. At 480ms delay, it stays within 1-2% word error rate, enabling voice agents with near-offline accuracy.&lt;/p&gt;&lt;p&gt;The model is natively multilingual, achieving strong transcription performance in 13 languages, including English, Chinese, Hindi, Spanish, Arabic, French, Portuguese, Russian, German, Japanese, Korean, Italian, and Dutch. With a 4B parameter footprint, it runs efficiently on edge devices, ensuring privacy and security for sensitive deployments.&lt;/p&gt;&lt;p&gt;We’re releasing the model weights under Apache 2.0 on the Hugging Face Hub.&lt;/p&gt;&lt;head rend="h2"&gt;Voxtral Mini Transcribe V2.&lt;/head&gt;&lt;p&gt;Average diarization error rate (lower is better) across five English benchmarks (Switchboard, CallHome, AMI-IHM, AMI-SDM, SBCSAE) and the TalkBank multilingual benchmark (German, Spanish, English, Chinese, Japanese).&lt;/p&gt;&lt;p&gt;Average word error rate (lower is better) across the top-10 languages in the FLEURS transcription benchmark.&lt;/p&gt;&lt;p&gt;Voxtral Mini Transcribe V2 delivers significant improvements in transcription and diarization quality across languages and domains. At approximately 4% word error rate on FLEURS and $0.003/min, Voxtral offers the best price-performance of any transcription API. It outperforms GPT-4o mini Transcribe, Gemini 2.5 Flash, Assembly Universal, and Deepgram Nova on accuracy, and processes audio approximately 3x faster than ElevenLabs’ Scribe v2 while matching on quality at one-fifth the cost.&lt;/p&gt;&lt;head rend="h3"&gt;Enterprise-ready features.&lt;/head&gt;&lt;p&gt;Voxtral Mini Transcribe V2 introduces key capabilities for enterprise deployments.&lt;/p&gt;&lt;head rend="h4"&gt;Speaker diarization.&lt;/head&gt;&lt;p&gt;Generate transcriptions with speaker labels and precise start/end times. Ideal for meeting transcription, interview analysis, and multi-party call processing. Note: with overlapping speech, the model typically transcribes one speaker.&lt;/p&gt;&lt;head rend="h4"&gt;Context biasing.&lt;/head&gt;&lt;p&gt;Provide up to 100 words or phrases to guide the model toward correct spellings of names, technical terms, or domain-specific vocabulary. Particularly useful for proper nouns or industry terminology that standard models often miss. Context biasing is optimized for English; support for other languages is experimental.&lt;/p&gt;&lt;head rend="h4"&gt;Word-level timestamps.&lt;/head&gt;&lt;p&gt;Generate precise start and end timestamps for each word, enabling applications like subtitle generation, audio search, and content alignment.&lt;/p&gt;&lt;head rend="h4"&gt;Expanded language support.&lt;/head&gt;&lt;p&gt;Like Realtime, this model now supports 13 languages: English, Chinese, Hindi, Spanish, Arabic, French, Portuguese, Russian, German, Japanese, Korean, Italian, and Dutch. Non-English performance significantly outpaces competitors.&lt;/p&gt;&lt;head rend="h4"&gt;Noise robustness.&lt;/head&gt;&lt;p&gt;Maintains transcription accuracy in challenging acoustic environments, such as factory floors, busy call centers, and field recordings.&lt;/p&gt;&lt;head rend="h4"&gt;Longer audio support.&lt;/head&gt;&lt;p&gt;Process recordings up to 3 hours in a single request.&lt;/p&gt;&lt;p&gt;Word error rate (lower is better) across languages in the FLEURS transcription benchmark.&lt;/p&gt;&lt;head rend="h2"&gt;Audio playground.&lt;/head&gt;&lt;p&gt;Test Voxtral Transcribe 2 directly in Mistral Studio. Upload up to 10 audio files, toggle diarization, choose timestamp granularity, and add context bias terms for domain-specific vocabulary. Supports .mp3, .wav, .m4a, .flac, .ogg up to 1GB each.&lt;/p&gt;&lt;head rend="h2"&gt;Transforming voice applications.&lt;/head&gt;&lt;p&gt;Voxtral powers voice workflows in diverse applications and industries.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;head rend="h3"&gt;Meeting intelligence.&lt;/head&gt;&lt;p&gt;Transcribe multilingual recordings with speaker diarization that clearly attributes who said what and when. At Voxtral's price point, annotate large volumes of meeting content at industry-leading cost efficiency.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Voice agents and virtual assistants.&lt;/head&gt;&lt;p&gt;Build conversational AI with sub-200ms transcription latency. Connect Voxtral Realtime to your LLM and TTS pipeline for responsive voice interfaces that feel natural.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Contact center automation.&lt;/head&gt;&lt;p&gt;Transcribe calls in real time, enabling AI systems to analyze sentiment, suggest responses, and populate CRM fields while conversations are still happening. Speaker diarization ensures clear attribution between agents and customers.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Media and broadcast.&lt;/head&gt;&lt;p&gt;Generate live multilingual subtitles with minimal latency. Context biasing handles proper nouns and technical terminology that trip up generic transcription services.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h3"&gt;Compliance and documentation.&lt;/head&gt;&lt;p&gt;Monitor and transcribe interactions for regulatory compliance, with diarization providing clear speaker attribution and timestamps enabling precise audit trails.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Both models support GDPR and HIPAA-compliant deployments through secure on-premise or private cloud setups.&lt;/p&gt;&lt;head rend="h2"&gt;Get started.&lt;/head&gt;&lt;p&gt;Voxtral Mini Transcribe V2 is available now via API at $0.003 per minute. Try it now in the new Mistral Studio audio playground or in Le Chat.&lt;/p&gt;&lt;p&gt;Voxtral Realtime is available via API at $0.006 per minute and as open weights on Hugging Face.&lt;/p&gt;&lt;p&gt;Explore documentation on Mistral’s audio and transcription capabilities.&lt;/p&gt;&lt;head rend="h2"&gt;We’re hiring.&lt;/head&gt;&lt;p&gt;If you're excited about building world-class speech AI and putting frontier models into the hands of developers everywhere, we'd love to hear from you. Apply to join our team.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46886735</guid><pubDate>Wed, 04 Feb 2026 15:08:17 +0000</pubDate></item><item><title>Microsoft's Copilot chatbot is running into problems</title><link>https://www.wsj.com/tech/ai/microsofts-pivotal-ai-product-is-running-into-big-problems-ce235b28</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46887564</guid><pubDate>Wed, 04 Feb 2026 16:08:55 +0000</pubDate></item><item><title>Postgres Postmaster does not scale</title><link>https://www.recall.ai/blog/postgres-postmaster-does-not-scale</link><description>&lt;doc fingerprint="31daa8e3dcad708b"&gt;
  &lt;main&gt;
    &lt;p&gt;At Recall.ai we run an unusual workload. We record millions of meetings every week. We send meeting bots to calls so our customers can automate everything from meeting notes, to keeping the CRM up-to-date, to handling incidents, to providing live-feedback on the call and more.&lt;/p&gt;
    &lt;p&gt;Processing TB/s of real-time media streams is the thing we get asked most about. However an often-overlooked feature of meetings is their unusual synchronization. Most meetings start on the hour, some on the half, but most on the full. It sounds obvious to say it aloud, but the implication of this has rippled through our entire media processing infrastructure.&lt;/p&gt;
    &lt;p&gt;This is a picture of our load pattern. The y-axis is the number of EC2 instances in our fleet. Those large spikes are the bursts of meetings that we need to capture. And when the meeting starts, the compute capacity must be ready to process the incoming data, or it will be lost forever.&lt;/p&gt;
    &lt;p&gt;The extreme gradient of these spikes has resulted in us running into bottlenecks at almost every layer of the stack, from ARP to AWS. This is the story of a stubbornly mysterious issue, that led us to deeply examine postgres internals (again) and uncover an often overlooked postgres bottleneck that only rears its head at extremely high scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Every postgres server starts and ends with the postmaster process. It is responsible for spawning and reaping children to handle connections and parallel workers, amongst other things. The postmaster runs a single-threaded main loop. With high worker churn, this loop can consume an entire CPU core, slowing down connection establishment, parallel queries, signal handling and more. This caused a rare, hard-to-debug issue where some of our EC2 instances would get delayed by 10-15s, waiting on the postmaster to fork a new backend to handle the connection.&lt;/p&gt;
    &lt;head rend="h3"&gt;Slow connections to postgres&lt;/head&gt;
    &lt;p&gt;Months ago we got alerted to large spike of delayed EC2 instances. We immediately investigated only to find that all of them were actually ready and waiting. We initially suspected a slow query caused the delay but we ruled this out. Eventually we uncovered that the delay originated from additional time connecting to postgres.&lt;/p&gt;
    &lt;p&gt;Postgres has its own binary wire protocol. The client sends a startup message, to which the server responds with an auth request.&lt;/p&gt;
    &lt;p&gt;What we observed was truly bizarre, the client would successfully establish a TCP connection to postgres, however the startup message only receive a response after 10s. Here is a example what we saw:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The initial TCP SYN packet is sent from the client&lt;/item&gt;
      &lt;item&gt;Less than one millisecond later the server responds with a SYN,ACK and the client ACK's to establish the connection&lt;/item&gt;
      &lt;item&gt;The client send the startup message to the postgres server and the server ACK's the message&lt;/item&gt;
      &lt;item&gt;10s later the server responds with an auth request and the connection continue nominally from there&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We ruled out obvious resource bottlenecks such as CPU, memory, disk I/O, network I/O and so forth. With all of these metrics looking nominal we turned to a deeper inspection of postgres internals.&lt;/p&gt;
    &lt;head rend="h3"&gt;A reproduction environment&lt;/head&gt;
    &lt;p&gt;We observed that the delay only occurred during the largest spikes, when many thousands of EC2 instances were booting. Notably, it seemed to occur sporadically, maybe only once or twice a week. We host our database on RDS Postgres, which complicated the matter as low-level telemetry is limited. So we resorted to creating a production-like reproduction environment that we could use to continue our investigation.&lt;/p&gt;
    &lt;p&gt;In this setup we used redis pub/sub to trigger a highly synchronized connection to postgres from a fleet of 3000+ EC2 instances. As we installed postgres on its own EC2 instance, we were able to instrument it while reproducing the delay.&lt;/p&gt;
    &lt;head rend="h3"&gt;A deep dive into the postmaster&lt;/head&gt;
    &lt;p&gt;The next step was to form a hypothesis which we could validate. To do this we inspected the postgres source code.&lt;/p&gt;
    &lt;p&gt;Every postgres has a supervisor process that is responsible for spawing and reaping new backends and workers. This process is called the postmaster (I love this name). The postmaster is designed as a single-threaded server loop that processes its events synchronously.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ServerLoop&lt;list rend="ul"&gt;&lt;item&gt;ChildReaper: Reap exited child processes (workers, backends, etc).&lt;/item&gt;&lt;item&gt;AcceptConnection: Launch a new backend to handle a connection.&lt;/item&gt;&lt;item&gt;LaunchBackgroudWorkers: Launch background workers for a parallel queries.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our hypothesis was that the burst of new connections would temporarily overwhelm the postmaster loop, causing it to lag behind the queue of incoming connections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Profiling the postmaster&lt;/head&gt;
    &lt;p&gt;To do this we profiled the postmaster process under these periods of connection spikes in our simulated environment. It was surprisingly easy to pin the postmaster process.&lt;/p&gt;
    &lt;p&gt;We ran the postgres on a &lt;code&gt;r8g.8xlarge&lt;/code&gt; instance.
At about ~1400 connections/sec we saturated the postmaster main loop and start to observe noticable delays.&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;perf&lt;/code&gt; we took sampling profile of the postmaster while it was under duress.&lt;/p&gt;
    &lt;p&gt;As expected the overwhelming majority of the time is spent spawning and reaping backends. It turns out &lt;code&gt;fork&lt;/code&gt; can be expensive! &lt;/p&gt;
    &lt;head rend="h3"&gt;Huge pages&lt;/head&gt;
    &lt;p&gt;A quick aside on how &lt;code&gt;fork&lt;/code&gt; on linux works. When you call &lt;code&gt;fork&lt;/code&gt;, it spawns a new "child" process, an exact duplicate of the parent continuing from the same instruction as the parent.
However, copying the parent's memory pages would be prohibitively expensive for how &lt;code&gt;fork&lt;/code&gt; is typically used. So linux employs a trick here, the pages are Copy-on-Write. This optimization means the copy only happens when the child process tries to modify a parent's memory page.&lt;/p&gt;
    &lt;p&gt;There is a catch however, linux still needs to copy the parent's page table entries (PTEs). Reducing the number of PTEs decreases the overhead of forking the process. On Linux this is easy to do. You can enable huge pages in the kernel using &lt;code&gt;echo $NUM_PAGES | sudo tee /proc/sys/vm/nr_hugepages&lt;/code&gt; and configuring postgres to use them.&lt;/p&gt;
    &lt;p&gt;Enabling huge pages results in a large reduction in the postmaster PTE size. Empirically we found a 20% throughput increase in connection rate with &lt;code&gt;huge_pages = on&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background workers&lt;/head&gt;
    &lt;p&gt;To further complicate the matter, the postmaster is also responsible for launching background workers for parallel queries. A high rate of parallel queries further increases increase the stress on postmaster main loop.&lt;/p&gt;
    &lt;code&gt;CREATE OR REPLACE FUNCTION bg_worker_churn(iterations integer)
RETURNS void
LANGUAGE plpgsql
AS $function$
DECLARE
  i int;
BEGIN
  PERFORM set_config('force_parallel_mode','on', true);
  PERFORM set_config('parallel_setup_cost','0', true);
  PERFORM set_config('parallel_tuple_cost','0', true);
  PERFORM set_config('min_parallel_table_scan_size','0', true);
  PERFORM set_config('min_parallel_index_scan_size','0', true);
  PERFORM set_config('enable_indexscan','off', true);
  PERFORM set_config('enable_bitmapscan','off', true);
  PERFORM set_config('parallel_leader_participation','off', true);
  PERFORM set_config('max_parallel_workers','512', true);
  PERFORM set_config('max_parallel_workers_per_gather','128', true);

  CREATE TABLE data (id BIGINT);
  INSERT INTO data SELECT generate_series(0, 100000);
  ANALYZE data;
  FOR i IN 1..iterations LOOP
    PERFORM sum(id) FROM data;
  END LOOP;
  DROP TABLE data;
END;
$function$;
&lt;/code&gt;
    &lt;p&gt;A high background worker churn rate also puts pressure on the postmaster main loop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unravelling the mystery&lt;/head&gt;
    &lt;p&gt;In production we only observed the connection delays sporadically. We determinated that was due to a confounding factor of increased background worker churn.&lt;/p&gt;
    &lt;p&gt;The smoking gun was in our database monitoring the whole time, which showed the spike in background worker shutdown load at the time of the delay.&lt;/p&gt;
    &lt;p&gt;We were able to simulate a high background worker churn in parallel with the connection flood and observed a large decrease connection throughput from the postmaster.&lt;/p&gt;
    &lt;p&gt;We correlated this query with one our endpoints using a query that triggered a parllel execution plan, that would occasionally coincide with our hourly peaks, resulting in the delayed connections.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fixing the issue&lt;/head&gt;
    &lt;p&gt;Now that we deeply understand the failure mode we can mechnically reason about a solution.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Implementing jitter in our fleet of EC2 instances reduced the peak connection rate&lt;/item&gt;
      &lt;item&gt;Eliminating bursts of parallel queries from our API servers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Both of these significantly reduce the pressure on the postmaster.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Many pieces of wisdom in the engineering zeitgeist are well preached but poorly understood. Postgres connection pooling falls neatly into this category. In this expedition we found one of the underlying reasons that connection pooling is so widely deployed on postgres systems running at scale.&lt;/p&gt;
    &lt;p&gt;Most online resources chalk this up to connection churn, citing fork rates and the pid-per-backend yada, yada. This is all true but in my opinion misses the forest from the trees. The real bottleneck is the single-threaded main loop in the postmaster. Every operation requiring postmaster involvement is pulling from a fixed pool, the size of a single CPU core. A rudimentary experiment shows that we can linearly increase connection throughput by adding additional postmasters on the same host.&lt;/p&gt;
    &lt;p&gt;This is one of my favourite kinds of discoveries: an artificial constraint that has warped the shape of the developer ecosystem (RDS Proxy, pgbouncer, pgcat, etc) around it. Hopefully to be lifted one day!&lt;/p&gt;
    &lt;p&gt;Aside: it's mildly absurd that none of the DBaaS or monitoring tools provide observability into postmaster contention. What's going on here?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46887893</guid><pubDate>Wed, 04 Feb 2026 16:30:51 +0000</pubDate></item><item><title>RS-SDK: Drive RuneScape with Claude Code</title><link>https://github.com/MaxBittker/rs-sdk</link><description>&lt;doc fingerprint="bdf8915a6c22e4d7"&gt;
  &lt;main&gt;
    &lt;p&gt;Research-oriented starter kit for runescape-style bots, including a typescript sdk, agent documentation and bindings, and a server emulator. Works out of the box - tell it what to automate!&lt;/p&gt;
    &lt;p&gt;Build and operate bots within a complex economic role-playing MMO. You can automate the game, level an account to all 99s, and experiment with agentic development techniques within a safe, bot-only setting.&lt;/p&gt;
    &lt;p&gt;The goals of this project are to provide a rich testing environment for goal-directed program synthesis techniques (Ralph loops, etc), and to facilitate research into collaboration and competition between agents.&lt;/p&gt;
    &lt;p&gt;There is currently a leaderboard for bots running on the demo server, with rankings based on highest total level per lowest account playtime.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;RS-SDK is a fork of the LostCity engine/client, an amazing project without which rs-sdk would not be possible. Find their code here or read their history and ethos&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/MaxBittker/rs-sdk.git&lt;/code&gt;
    &lt;p&gt;Out of the box, you can connect to the provided demo server, choose a name that is not already taken!&lt;/p&gt;
    &lt;p&gt;With claude code:&lt;/p&gt;
    &lt;code&gt;bun install
claude "start a new bot with name: {username}"&lt;/code&gt;
    &lt;p&gt;Manually:&lt;/p&gt;
    &lt;code&gt;bun install
bun scripts/create-bot.ts {username}
bun bots/{username}/script.ts &lt;/code&gt;
    &lt;p&gt;Chat is off by default to prevent scamming and prompt injection attacks, but you can opt in with &lt;code&gt;SHOW_CHAT=true&lt;/code&gt; in the bot.env file&lt;/p&gt;
    &lt;p&gt;Warning: The demo server is offered as a convenience, and we do not guarantee uptime or data persistence. Hold your accounts lightly, and consider hosting your own server instance. Please do not manually play on the demo server.&lt;/p&gt;
    &lt;p&gt;This server has a few modifications from the original game to make development and bot testing easier:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Faster leveling - The XP curve is accelerated and less steep.&lt;/item&gt;
      &lt;item&gt;Infinite run energy - Players never run out of energy&lt;/item&gt;
      &lt;item&gt;No random events - Anti-botting random events are disabled&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;rs-sdk runs against an enhanced web-based client (&lt;code&gt;botclient&lt;/code&gt;) which connects to the LostCity 2004scape server emulator.&lt;/p&gt;
    &lt;p&gt;There is a gateway server which accepts connections from botclient and SDK instances, and forwards messages between them based on username. Once connected to the gateway, the botclient will relay game state to the SDK, and execute low-level actions (e.g. &lt;code&gt;walkTo(x,y)&lt;/code&gt;) sent from the SDK through the gateway.&lt;/p&gt;
    &lt;p&gt;This means that the SDK can't talk directly to the game server, but must go through the botclient. It will attempt to launch the botclient on startup if one is not already running.&lt;/p&gt;
    &lt;p&gt;You don't need to run the gateway/botclient in order to run automations against the demo server, but you may choose to if you are fixing bugs or adding features to the rs-sdk project&lt;/p&gt;
    &lt;p&gt;You want all these running:&lt;/p&gt;
    &lt;code&gt;cd engine &amp;amp;&amp;amp; bun run start&lt;/code&gt;
    &lt;code&gt;cd webclient &amp;amp;&amp;amp; bun run watch&lt;/code&gt;
    &lt;code&gt;cd gateway &amp;amp;&amp;amp; bun run gateway&lt;/code&gt;
    &lt;p&gt;There is also a login server which you may not need, I forget&lt;/p&gt;
    &lt;p&gt;This is a free, open-source, community-run project.&lt;/p&gt;
    &lt;p&gt;The goal is strictly education and scientific research.&lt;/p&gt;
    &lt;p&gt;LostCity Server was written from scratch after many hours of research and peer review. Everything you see is completely and transparently open source.&lt;/p&gt;
    &lt;p&gt;We have not been endorsed by, authorized by, or officially communicated with Jagex Ltd. on our efforts here.&lt;/p&gt;
    &lt;p&gt;You cannot play Old School RuneScape here, buy RuneScape gold, or access any of the official game's services! Bots developed here will not work on the official game servers.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the MIT License. See the LICENSE file for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888142</guid><pubDate>Wed, 04 Feb 2026 16:47:26 +0000</pubDate></item><item><title>Converge (YC S23) Is Hiring Product Engineers (NYC, In-Person)</title><link>https://www.runconverge.com/careers/product-engineer</link><description>&lt;doc fingerprint="a508c173dc07a7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Product Engineer&lt;/head&gt;
    &lt;p&gt;Location: NYC (in-person)&lt;/p&gt;
    &lt;p&gt;Help us build everything a consumer brand needs to grow. Weâre just 4 engineers with serious traction (well beyond $1M ARR). Youâll be shipping end-to-end product and working directly with customers.&lt;/p&gt;
    &lt;head rend="h3"&gt;About Converge&lt;/head&gt;
    &lt;p&gt;We want to profitably grow the world's consumer brands. That begins with helping them understand which marketing efforts are driving profitable growth.&lt;/p&gt;
    &lt;p&gt;200+ consumer brands, including publicly traded companies, rely on Converge to check in on their marketing performance up to a dozen times a day. They drill down to figure out what's working and decide where to shift million-dollar marketing budgets.&lt;/p&gt;
    &lt;p&gt;To ship even more, we've raised $5.7M from some of the best investors, including Y Combinator, General Catalyst, and the founders of Posthog, Algolia, Shipbob, ...&lt;/p&gt;
    &lt;head rend="h3"&gt;What you'll do&lt;/head&gt;
    &lt;p&gt;Ship product fully end-to-end. All the way from designing the system and data models to building out the interface and polishing the experience.&lt;/p&gt;
    &lt;p&gt;We trust you to build the best solution to a customer's problem. We lead with context and give you full autonomy to design the solution. This means you'll need to build a deep understanding of the problem, obsess over the solution, and ship it.&lt;/p&gt;
    &lt;p&gt;Work directly with customers. You'll talk to them, ship, get feedback, and iterate. There are no middlemen. This means you can move incredibly fast: you message a customer, create a PR, and can have it fixed within hours.&lt;/p&gt;
    &lt;p&gt;Some examples of projects you could own:&lt;/p&gt;
    &lt;p&gt;Effortless Slack conversations: Customers screenshot their Converge reports up to a dozen of times a day (!) to share with the rest of the company. Instead, they should be able to directly tag and message a teammate from any number in Converge and have this synced bidirectionally with Slack.&lt;/p&gt;
    &lt;p&gt;Centralizing all growth efforts: Converge already centralizes all growth metrics, but the information explaining them is still scattered. We want overlay the context that actually explains trends like pricing updates, budget changes, and promo calendars.&lt;/p&gt;
    &lt;p&gt;Creative analytics: All growth teams use a separate tool to iterate on ad creatives, but they would want to run this workflow on Converge. We should build a Creative Analytics product on top of our data.&lt;/p&gt;
    &lt;p&gt;AI agents: We never wanted to jump on the AI train for the sake of hype. But now that we have the foundations in place, there's huge leverage in building agents that can help growth teams understand their data and act on it more quickly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Converge&lt;/head&gt;
    &lt;p&gt;We're a team of just 4 engineers with serious traction (well beyond $1M ARR). Join us if you want to get rid of office politics and just take ownership to get a lot done.&lt;/p&gt;
    &lt;p&gt;Even though you join early, this job comes with real engineering challenges. We process $4B in online orders annually, 20TB of data flows through Converge each month, and we've collected around 10B customer interactions to date.&lt;/p&gt;
    &lt;p&gt;What you're shipping will actually get used. 50% of our customers use us daily (!), while this is only 13% for the average SaaS company. You will have an immediate impact.&lt;/p&gt;
    &lt;p&gt;We love working in-person. You'll like it here if you do too.&lt;/p&gt;
    &lt;p&gt;If you think you could be a founder, there's no better way to learn than to talk to customers and ship. That's what you'll be spending all of your time on. We obsess over the details and will share honest feedback.&lt;/p&gt;
    &lt;head rend="h3"&gt;What we're looking for&lt;/head&gt;
    &lt;p&gt;Strong experience working across the stack (4+ YOE). We work with React, Python, Postgres, and Clickhouse. Experience building data-intensive products or familiarity with Clickhouse is a plus.&lt;/p&gt;
    &lt;p&gt;You've previously built products or large features fully end-to-end.&lt;/p&gt;
    &lt;p&gt;You obsess over the quality of what you're building, both in UX and code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compensation&lt;/head&gt;
    &lt;p&gt;Salary: $175K - $240K + equity (0.6% - 0.85%).&lt;/p&gt;
    &lt;p&gt;Private health, dental, and vision insurance.&lt;/p&gt;
    &lt;p&gt;Pension &amp;amp; 401k contributions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interview process*&lt;/head&gt;
    &lt;p&gt;Intro call (30 min): We want to learn about your motivations to join Converge, determine why youâd be a great fit, and answer any questions you have for us.&lt;/p&gt;
    &lt;p&gt;Technical (1h): We work through a typical engineering problem we face at Converge.&lt;/p&gt;
    &lt;p&gt;Culture (45 min): We dive into your past experiences to learn how you like to work and what motivates you.&lt;/p&gt;
    &lt;p&gt;Superday (1 day): Join us for a day to actually build something! You get to meet the team, we get to meet you, it's great. (fully paid)&lt;/p&gt;
    &lt;p&gt;(*) This can all be done in 2 days. If you want to move quickly, we do too. Our founding engineer was on a plane to meet us just days after our first call.&lt;/p&gt;
    &lt;head rend="h2"&gt;We raised $5.7M from some of the best investors&lt;/head&gt;
    &lt;head rend="h3"&gt;James Hawkins&lt;/head&gt;
    &lt;head rend="h3"&gt;Nicolas Dessaigne&lt;/head&gt;
    &lt;head rend="h2"&gt;Founding team&lt;/head&gt;
    &lt;head rend="h2"&gt;How we started&lt;/head&gt;
    &lt;head rend="h3"&gt;Did you knowâ¦&lt;/head&gt;
    &lt;p&gt;All co-founders have written code that has run in production as part of Converge.&lt;/p&gt;
    &lt;p&gt;We closed our first publicly traded company during our YC batch from our living room in San Francisco.&lt;/p&gt;
    &lt;p&gt;Thomas and Tiago (Founding Engineer) worked together when Thomas was just an intern.&lt;/p&gt;
    &lt;p&gt;Michel (Customer Success) was responsible for most of the incoming Converge Support tickets in his previous job as a freelance tracking consultant.&lt;/p&gt;
    &lt;p&gt;Thomas and Jan were best friends in high school, and Jan and Jerome met in their first year of college.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888331</guid><pubDate>Wed, 04 Feb 2026 17:01:14 +0000</pubDate></item><item><title>AI is killing B2B SaaS</title><link>https://nmn.gl/blog/ai-killing-b2b-saas</link><description>&lt;doc fingerprint="ede5902a8d97058d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;AI is Killing B2B SaaS&lt;/head&gt;&lt;p&gt;SaaS is the most profitable business model on Earth.1 It’s easy to understand why: build once, sell the same thing again ad infinitum, and don’t suffer any marginal costs on more sales.&lt;/p&gt;&lt;p&gt;I have been writing software for more than half my life. In the last year itself, I’ve talked to hundreds of founders and operators in SF, from preseed to Series E companies.&lt;/p&gt;&lt;p&gt;AI is bringing an existential threat to a lot of B2B SaaS executives: How to keep asking customers for renewal, when every customer feels they can get something better built with vibe-coded AI products?&lt;/p&gt;&lt;p&gt;And the market is pricing it in. Morgan Stanley’s SaaS basket has lagged the Nasdaq by 40 points since December. HubSpot and Klaviyo are down ~30%. Analysts are writing notes titled “No Reasons to Own” software stocks.&lt;/p&gt;&lt;head rend="h2"&gt;The relation between vibe coding and B2B SaaS sales&lt;/head&gt;&lt;p&gt;The new problem for B2B SaaS is that with AI, customers can get something working with vibe coding. There are tens of vibe coding “internal tool” services that promise to connect to every integration in the world to pump out CRUD and workflow apps.&lt;/p&gt;&lt;p&gt;Whatever they build simply works. It takes some wrangling to get there (one Series C VP listed eleven different vibe coding tools they’ve tried and the pros and cons between each on a phone call once), but productivity gains are immediate.&lt;/p&gt;&lt;p&gt;And vibe coding is fun. You feel like a mad wizard using the right incantation 2 to get this magical new silicon intelligence to do exactly what you want.&lt;/p&gt;&lt;p&gt;What they don’t know, though, is that a poorly architected system will fail, eventually. As every senior programmer (eventually) understands, our job is complex because we have to understand the relationships in the real world, the processes involved, and the workflows needed, and representing it in a robust way to create a stable system. AI can’t do that.&lt;/p&gt;&lt;p&gt;Non-programmers don’t know any of this nuance. One Series E CEO told me that they’re re-evaluating the quarterly renewal of their engineering productivity software because they along with an engineer reimplemented something using Github and Notion APIs. They were paying $30,000 to a popular tool3 and they were not going to renew anymore.&lt;/p&gt;&lt;head rend="h2"&gt;How does it impact B2B sales?&lt;/head&gt;&lt;p&gt;If customers feel like they aren’t being served exactly like they want to, they are more likely to churn. The reason behind all this is that customers are demanding more from their B2B vendors, because they know what’s possible.&lt;/p&gt;&lt;p&gt;Previously, you would change your company to fit what your ERP and pay them hundreds of thousands of dollars. Now, everyone can see that agentic coding makes an unprecedented level of flexibility possible. And customers are demanding that flexibility, and if they don’t get it, they’ll leave.&lt;/p&gt;&lt;p&gt;This week itself I was on a phone call with a Series B AE talking about how they’re potentially losing an $X00,000 account just because the customer can’t use a specific failure reporting workflow in the SaaS. They’re now working with me to build what the customer needs and retain them.&lt;/p&gt;&lt;head rend="h2"&gt;How to survive&lt;/head&gt;&lt;head rend="h3"&gt;1. Be a System of Record&lt;/head&gt;&lt;p&gt;If the entire company’s workflows operates on your platform, i.e. you’re a line-of-business SaaS, you are integrated into their existing team already. They know your UI and rely on you on the day to day.&lt;/p&gt;&lt;p&gt;For example, to create a data visualization I won’t seek any SaaS. I’ll just code one myself using many of the popular vibe coding tools (my team actually did that and it’s vastly more flexible than what we’d get off-the-shelf).&lt;/p&gt;&lt;p&gt;Being a “System of Record” means you’re embedded so deeply that there’s no choice but to win. My prediction is that we’ll see more SaaS companies go from the application layer to offering their robust SoR as their primary selling point.&lt;/p&gt;&lt;head rend="h3"&gt;2. Security, authentication, and robustness&lt;/head&gt;&lt;p&gt;This is where vibe-coded apps silently fail — and where established SaaS platforms earn their keep.&lt;/p&gt;&lt;p&gt;When a non-technical team vibe-codes an internal tool, they’re not thinking about environment keys, XSS vulnerabilities or API keys hardcoded in client-side JavaScript. They’re not implementing rate limiting, audit logs, or proper session management. They’re definitely not thinking about SOC 2 compliance, GDPR data residency requirements, or HIPAA audit trails.&lt;/p&gt;&lt;p&gt;I’ve seen it firsthand: a finance team built a “quick” expense approval tool that stored unencrypted reports in a public S3 bucket. A sales ops team created a commission calculator that anyone with the URL could access — no auth required. These aren’t edge cases. They’re the norm when software is built without security as a foundational concern.&lt;/p&gt;&lt;p&gt;Enterprise SaaS platforms have spent years (and millions) solving these problems: role-based access control, encryption at rest and in transit, penetration testing, compliance certifications, incident response procedures. Your customers may not consciously value this — until something breaks.&lt;/p&gt;&lt;p&gt;The challenge is that security is invisible when it works. You need to communicate this value proactively: remind customers that the “simple” tool they could vibe-code themselves would require them to also handle auth, permissions, backups, uptime, and compliance.&lt;/p&gt;&lt;head rend="h3"&gt;3. Adapt to the customer, not the other way around&lt;/head&gt;&lt;p&gt;The times of asking customers to change how they work are gone. Now, SaaS vendors that differentiate by being ultra customizable win the hearts of customers.&lt;/p&gt;&lt;p&gt;How? It’s the most powerful secret to increase usage. We’ve all heard the classic SaaS problem where the software is sold at the beginning of the year, but no one actually ends up using it because of how inflexible it is and the amount of training needed.&lt;/p&gt;&lt;p&gt;And if a SaaS is underutilized, it gets noticed. And that leads to churn.&lt;/p&gt;&lt;p&gt;This is the case with one of my customers, they have a complex SaaS for maintenance operations. But turns out, this was not being used at the technician level because they found the UI too complex4.&lt;/p&gt;&lt;p&gt;How I’m solving this is essentially a whitelabelled vibe-coding platform with in-built distribution and secure deployments. When they heard of my solution they were immediately onboard. Their customer success teams quickly coded a very specific mobile webapp for the technicians to use and deployed it in a few days.&lt;/p&gt;&lt;p&gt;Now, the IC technician is exposed to just those parts of the SaaS that they care about i.e. creating maintenance work orders. The executives get what they want too, vibe coding custom reports exactly the way they want vs going through complicated BI config. They are able to build exactly what they want and feel like digital gods while doing it.&lt;/p&gt;&lt;p&gt;Usage for that account was under 35%, and is now over 70%. They are now working closely with me to vibe code new “micro-apps” that work according to all of their customer workflows. And the best part? This is all on top of their existing SaaS which works as a system of record and handles security, authentication, and supports lock-in by being a data and a UI moat.&lt;/p&gt;&lt;p&gt;This is exactly what I’m building: a way for SaaS companies to let their end-users vibe code on top of their platform (More on that below). My customers tell me it’s the best thing they’ve done for retention, engagement, and expansion in 2026 – because when your users are building on your platform, they’re not evaluating your competitors.&lt;/p&gt;&lt;head rend="h2"&gt;The Real Shift&lt;/head&gt;&lt;p&gt;Here’s what I’ve realized after hundreds of conversations with founders and operators: AI isn’t killing B2B SaaS. It’s killing B2B SaaS that refuses to evolve.&lt;/p&gt;&lt;p&gt;The SaaS model was built on a simple premise: we build it once, you pay forever. That worked when building software was hard. But now your customers have tasted what’s possible. They’ve seen their finance team whip up a custom dashboard in an afternoon. They’ve watched a non-technical PM build an internal tool that actually fits their workflow.&lt;/p&gt;&lt;p&gt;You can’t unsee that. You can’t go back to paying $X0,000/year for software that almost does what you need.&lt;/p&gt;&lt;p&gt;The survivors won’t be the SaaS companies with the best features. They’ll be the ones who become platforms – who let customers build on top of them instead of instead of them. When I showed a well-known VC what I was building to help SaaS companies do exactly this, he said: “This is the future of marketplaces and software companies.”&lt;/p&gt;&lt;p&gt;Maybe. Or maybe this is just another cycle and traditional SaaS will adapt like it always has. But I know this: the companies I’m talking to aren’t waiting around to find out. They’re already rebuilding their relationship with customers from “use our product” to “build on our platform.”&lt;/p&gt;&lt;p&gt;The question isn’t whether AI will eat your SaaS.&lt;/p&gt;&lt;p&gt;It’s whether you’ll be the one holding the fork.&lt;/p&gt;&lt;p&gt;I’m solving exactly this problem with a whitelabelled AI platform for B2B SaaS companies, so your users can vibe code customized workflows on top of their existing system of record.&lt;/p&gt;&lt;p&gt;My customers tell me this is the best way to support retention, engagement, and expansion in 2026. If this sounds interesting to you or someone you know, I can reach out with a custom demo or you can learn more about Giga Catalyst.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Whenever I bring a new friend to the Salesforce Park, they are in absolute awe. And, the meme remains true that no one even knows what Salesforce does. Whatever they’re doing, they’re clearly earning enough revenue to purchase multiple blocks in SF. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;a.k.a. “prompt engineering” which is not engineering at all but that’s a different blog post. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I won’t name any names, but the company’s named after an invertebrate animal. ↩&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;And who can blame them – I still feel a pang of anxiety when I look at my sales CRM. ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Increase engagement and retention&lt;/head&gt;&lt;p&gt;Our whitelabel AI vibe coding platform allows your users to customize and build exactly what they need, on top of your platform.&lt;/p&gt;&lt;p&gt;My customers say that this is the best way to increase engagement and retention in 2026.&lt;/p&gt;Curious? Check out Giga Catalyst to learn more&lt;p&gt;Or, fill out this form and I'll personally reach out to show you how it works:&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888441</guid><pubDate>Wed, 04 Feb 2026 17:09:28 +0000</pubDate></item><item><title>Building a 24-bit arcade CRT display adapter from scratch</title><link>https://www.scd31.com/posts/building-an-arcade-display-adapter</link><description>&lt;doc fingerprint="671f195658a51bd9"&gt;
  &lt;main&gt;&lt;p&gt;In November, my friend and fellow Recurser, Frank, picked up an arcade machine for the Recurse Center. We call it the RCade. He wanted to leave the original CRT in - which I think is a great choice! - and drove it off of a Raspberry Pi. Eventually we wanted to move to a more powerful computer, but we needed a way to connect it to the display. Off-hand, I mentioned that I could build a CRT display adapter that interfaces with a normal computer over USB. This is that project.&lt;/p&gt;&lt;head rend="h2"&gt;What the display expects&lt;/head&gt;&lt;p&gt;The CRT in the RCade has a JAMMA connector, and Frank bought a converter that goes between VGA and JAMMA.&lt;/p&gt;&lt;p&gt;You might think we could just use an off-the-shelf VGA adapter to drive it at this point, but it's not that simple. The CRT runs at a weird resolution; We started with 320x240 but eventually wanted to target 336x262, which is super non-standard. Even 320x240 is unattainable by most display adapters, which typically can't go below 640x480. A custom solution would allow us to output any arbitrary resolution we wanted.&lt;/p&gt;&lt;p&gt;The other thing is that the Pi, with the VGA board we were using, only supports 18-bit colour, and we wanted to improve this. Even on the RCade's CRT, colour banding was an obvious issue.&lt;/p&gt;&lt;p&gt;We also wanted to use a laptop, not a desktop, which meant not using a PCI-e card. Instead, a USB interface would be preferable.&lt;/p&gt;&lt;head rend="h2"&gt;Wait, but what is VGA?&lt;/head&gt;&lt;p&gt;VGA is a signaling protocol that maps almost exactly 1:1 with what a CRT actually does.&lt;/p&gt;Taken from wikimedia.org&lt;p&gt;Inside of a CRT, there are 3 electron guns, which correspond to red, green, and blue colour values. Two electromagnets in the neck of the tube are responsible for steering the beam - one steers horizontally and one steers vertically. To draw an image, the beam moves across the screen one horizontal line at a time, and the electron guns are rapidly modulated in order to display the correct colour at each pixel.&lt;/p&gt;&lt;p&gt;VGA contains analog signals for these R, G, and B electron guns. It also contains an HSYNC and VSYNC signal, which are used so that the driver and the CRT can agree on what pixel is being drawn at a given time. Between the VGA input and the CRT is a very simple circuit which locks onto these HSYNC and VSYNC pulses and synchronizes the sweeping of the beam.&lt;/p&gt;Taken from pyroelectro.com&lt;p&gt;The HSYNC pulses happen in between horizontal lines, and the VSYNC pulses happen in between frames. There are dead zones around each pulse - referred to as the front and back porch - which give the electron beam time to sweep back across the screen.&lt;/p&gt;&lt;p&gt;So, all we really need are those R, G, B, HSYNC, and VSYNC signals, running at precise timing, and synced properly relative to each other. Conceptually this is actually pretty simple!&lt;/p&gt;&lt;head rend="h2"&gt;Attempt 1: Using the RP2040's PIO&lt;/head&gt;&lt;p&gt;I like the Raspberry Pi RP2040 a lot. It's relatively cheap (around $1 USD) and has tons of on-board RAM - 264 KB in fact! It also has what is called Programmable IO, or PIO.&lt;/p&gt;&lt;p&gt;I've never used the PIO before, but the basic idea is that you can write assembly programs where every instruction takes exactly one cycle, and has useful primitives for interacting with GPIO. It's a fairly limited instruction set, but it allows for bit-banging precise cycle-accurate custom protocols. It's exactly what I need to modulate a VGA signal.&lt;/p&gt;&lt;p&gt;The PIO code ended up looking like this:&lt;/p&gt;&lt;quote&gt;// 1. low for 320+16=336 pixels // 2. high for 30 pixels // 3. low for 34 pixels // 4. repeat // runs on sm0 // 6 instrs -&amp;gt; can save some with sidesetting let hsync = pio::pio_asm!( ".wrap_target", /* begin pixels + front porch */ "irq set 0 [2]", // tell vsync we're doing 1 line "set pins, 1 [31]", // go low for 32 "set X, 8 [15]", // +16 = 48 "a:", "jmp X-- a [31]", // each loop 32, * 9 = 288, total = 336 /* end front porch, being assert hsync */ "set pins, 0 [29]", // assert hsync for 30 /* end assert hsync, begin back porch */ "set pins, 1 [29]", // deassert, wait 32 (note there is extra delay after the wrap) ".wrap" ); // NOTE - we get irq at *end* of line so we have to time things accordingly // 1. low for 242 lines -&amp;gt; but irq 2 every line for the first 240 // 2. high for 3 lines // 3. low for 22 lines // 4. repeat // runs on sm1 // 19 instr let vsync = pio::pio_asm!( ".side_set 1 opt", ".wrap_target", "set Y, 6", "a_outer:", "set X, 31", "a:", "wait 1 irq 0", "irq set 2", "jmp X-- a", // 32 lines per inner loop "jmp Y-- a_outer", // 7 outer loops = 224 "set X, 15", // 16 more lines = 240 "z:" "wait 1 irq 0", "irq set 2", "jmp X-- z", "wait 1 irq 0", // wait for end of last rgb line "wait 1 irq 0", // 2 more lines for front porch "wait 1 irq 0", "set X, 2 side 0", // assert vsync "b:", "wait 1 irq 0", "jmp X-- b", // wait for 3 lines "set X, 20 side 1", // deassert vsync "c:", "wait 1 irq 0", "jmp X-- c" // wait for 21 lines (back porch) ".wrap", ); // 2 cycles per pixel so we run at double speed // 6 instr let rgb = pio::pio_asm!( "out X, 32", // holds 319, which we have to read from the FIFO ".wrap_target", "mov Y, X", "wait 1 irq 2", // wait until start of line "a:", "out pins, 16", // write to rgb from dma "jmp Y-- a", "mov pins, NULL", // output black ".wrap" );&lt;/quote&gt;&lt;p&gt;The full code lives here.&lt;/p&gt;&lt;p&gt;There are 3 separate PIO programs. &lt;code&gt;hsync&lt;/code&gt; is responsible for keeping time and generating HSYNC pulses. At the start of each line, it generates an IRQ event that the other programs use for synchronization. &lt;code&gt;vsync&lt;/code&gt; counts these events and generates the VSYNC pulses. Finally, &lt;code&gt;rgb&lt;/code&gt; reads pixel data from DMA and outputs to the RGB pins in precise time with the other signals. The &lt;code&gt;out pins, 16&lt;/code&gt; signifies that we're only doing 16-bit colour for now.&lt;/p&gt;&lt;p&gt;There's a lot of weirdness in here to get around the constraints of the PIO. For example, between all 3 programs, only a maximum of 31 instructions are allowed. All of the VGA parameters (resolution, porch length, etc.) are hard-coded, and changing these would require at least a small rewrite. It's pretty brittle in that regard, but for our use-case it's sufficient as a proof-of-concept.&lt;/p&gt;&lt;p&gt;Here it is running the actual CRT in the RCade:&lt;/p&gt;&lt;p&gt;I wanted to fill the framebuffer with a repeating pattern, but I messed up my code, hence it looking weird. That's fine - it was enough to verify my VGA program worked!&lt;/p&gt;&lt;p&gt;As an aside, every time I popped off the back of the RCade to work on it was terrifying. Not because of the lethal voltages inside, but because Recursers absolutely love the RCade. I often joke that if I were to break it, I would basically be the anti-Frank!&lt;/p&gt;&lt;p&gt;Now that I had something that could take a framebuffer and throw it onto the CRT, it was time to get the image from my computer to the RP2040.&lt;/p&gt;&lt;head rend="h2"&gt;Let's write a kernel module!&lt;/head&gt;&lt;p&gt;My plan was to write a Linux kernel module that would expose itself as a framebuffer, and then send that framebuffer over USB to the RP2040. On the framebuffer side, this involved interfacing with the DRM layer.&lt;/p&gt;&lt;p&gt;I actually made decent progress here, although I kernel panicked many, many times. I never bothered to set up a proper development environment (oops), so pretty much any bug would require me to reboot my computer. This was super annoying and tedious, although I did learn a lot. I found cursed things in the official documentation, like interrobangs!&lt;/p&gt;Linus pls&lt;p&gt;I got as far as getting a framebuffer to show up at the correct resolution and refresh rate. Along this journey though, I discovered the GUD kernel module, and quickly realized I should use that instead.&lt;/p&gt;&lt;head rend="h2"&gt;GUD is... pretty good&lt;/head&gt;&lt;p&gt;Okay so this GUD thing is sick. It's a USB display adapter protocol - exactly what I need! It was originally designed to send video from a computer to a Pi Zero for use as a secondary display. It consists of an upstreamed (!!!) kernel module that runs on the host, and separate gadget software that runs on the Pi Zero. I decided I would just write my own gadget implementation to run on the RP2040.&lt;/p&gt;&lt;p&gt;As a protocol, GUD seems decent. It supports compression over the wire, and only sends the deltas of what's changed in the host's framebuffer. It's also pretty robust in terms of allowing the gadget to advertise what features it supports - compression is optional, and there's flexibility in colour depth and resolution. And again, it's upstreamed into the kernel, so anyone on modern Linux could use my display adapter with no software tweaks.&lt;/p&gt;&lt;p&gt;Unfortunately, GUD has almost no documentation. I figured out what I needed to do by reverse engineering the kernel module, which involved recompiling it to add some debugging statements. The protocol is simple enough that is wasn't too much of a hassle, and it didn't take long before I had developed a gadget implementation in Rust for the RP2040.&lt;/p&gt;&lt;p&gt;And with that, we saw our first Linux images on the CRT:&lt;/p&gt;&lt;p&gt;I know, I know, it looks terrible. Several years ago, I had built a board that implements the R/G/B DACs out of resistors, and I reused that for this project. It can only do 12 bits of colour maximum, and for this test I only bothered to wire up ~2 bits per channel, which is basically unusable. But it proves the concept works!&lt;/p&gt;The board I built several years ago. It was originally designed to fit an STM32 development board.&lt;p&gt;To be honest, it's pretty lucky that this board came with me to New York. I'm surprised I didn't either throw it out or move it to my parent's place. It was probably in some other box of things I deemed worth keeping around.&lt;/p&gt;The VGA board connected to the RP2040.&lt;p&gt;You can see from the above picture that I really connected the bare minimum for a proof-of-concept. I find perfboard soldering to be a bit tedious!&lt;/p&gt;&lt;p&gt;As an aside, you may notice in the video that the entire screen is shifted to the left. The left side has wrapped around and is now on the right side. On initial boot, it would look fine; over time it would gradually get worse and worse. This is a bug in my implementation - I suspect it's some kind of buffer underflow that's happening, such that each time it occurs, the PIO gets progressively more out of sync. But this is just a guess; I didn't look into it too much.&lt;/p&gt;&lt;p&gt;The colour depth issue is trivial to fix, but this next one isn't. The framerate sucks! You can even see it in the video above, where you can watch the new frame scroll down the screen. The RP2040 can only do USB FS (full-speed), which is capable of 11 Mbps. At the 320x240x16 bpp we were originally targeting, every frame is 153.6 kB. At our maximum USB FS speed, that's less than 10 FPS! Embarrasingly, I had originally done the math with a bandwidth of 11 MBps, not 11 Mbps, so I was off by a factor of 8. I was hoping to get something at least temporarily usable but had to go back to the drawing board.&lt;/p&gt;&lt;head rend="h2"&gt;Going on a GUD gadget side quest&lt;/head&gt;&lt;p&gt;Who even needs microcontrollers anyway? My next idea was to use the normal GUD gadget implementation, running on a Pi Zero, but outputting to VGA over GPIO. Conceptually this is pretty simple, although in practice it was anything but. The canonical GUD gadget software was based on a 2021 version of Buildroot, which was too old to output VGA. I tried, and failed, to update the Buildroot version, as well as to backport the VGA overlay. Neither of those really worked, but I didn't really know what I was doing.&lt;/p&gt;&lt;p&gt;I also played around with generating a custom NixOS image that had a modern kernel and the GUD gadget kernel module. When that didn't work I prepared to run a user space GUD gadget implementation on Raspberry Pi OS. But like, isn't that boring? And then I'll still be stuck at 18 bit colour! And sometimes a girl just wants to tickle her electrons :3&lt;/p&gt;&lt;head rend="h2"&gt;Attempt... 2? 3? 1+i? Returning to MCU land&lt;/head&gt;&lt;p&gt;Okay, so my beloved RP2040 doesn't support USB HS (high-speed). My beloved RP2350 (the newer version of the same chip) doesn't either. But some of my beloved STM32s do!&lt;/p&gt;&lt;p&gt;Initially I was planning to go computer -&amp;gt; USB HS -&amp;gt; STM32 -&amp;gt; SPI bus -&amp;gt; RP2040 -&amp;gt; VGA. But like, that's complicated, and there are 2 microcontrollers to program, and there is so much to go wrong, and the SPI bus protocol is going to need to be robust against lost/extra bits, and AAAAAAAAAA I don't wanna!&lt;/p&gt;&lt;p&gt;But! STM32! I learned through research that some of the nicer ones have an LTDC peripheral, which, among other things, can drive an LCD display. And guess what? Many LCDs take in an R, G, B, HSYNC, and VSYNC signal. That's right - they pretend they're a CRT, and they pretend they have a cute little electron gun inside of them, and the STM32 is like "ok I got u" and can just like, do this natively. And I realize that this is what VGA is, but it's so, so funny to me that the protocol is literally just the manifestation of a physical design that is largely obsolete.&lt;/p&gt;&lt;p&gt;Okay so at this point I'm like, is this even a real project anymore? I'm just connecting the USB peripheral to the LTDC peripheral. What part of this is supposed to take effort? I had already written the GUD gadget implementation. Wasn't I basically already done?&lt;/p&gt;&lt;p&gt;OH BOY.&lt;/p&gt;&lt;p&gt;Anyway, by now it's Christmas time and I fly back to Canada to hang out with my family, as you would expect. I had none of my hardware with me, so now felt like a good time to design the actual board.&lt;/p&gt;&lt;p&gt;By Christmas Eve, this is what I had. Conceptually, it's a pretty basic board - there's the USB HS input, the VGA output, 3 8-bit DACs, some RAM for the framebuffer, and supporting components. At the heart of it is the STM32H723, which is a microcontroller that's advertised as supporting USB HS and LTDC.&lt;/p&gt;&lt;p&gt;It's worth talking about the DACs a bit. They have a few requirements. They need to map the 8-bit binary space uniformly to the analog domain. They also need to act as a resistor divider - my I/O is at 3.3V, but VGA expects a maximum of 0.7 volts for R/G/B. And finally, they need to be impedance-matched to the 75 ohms of the VGA cable, to prevent reflections and ringing that show up in the image. I am... pretty doubtful we need this at our resolution, but it doesn't hurt, and it increases nerd cred (^:&lt;/p&gt;&lt;p&gt;I encoded all of these requirements into a system of equations, threw it into a SAT solver, and computed all of my resistor values. I checked the output manually and it made sense, so I used these values in my DAC.&lt;/p&gt;&lt;p&gt;Also worth noting is the length-matched traces between the STM32 and the HyperRAM. Length-matching ensures that all the signals arrive at the same time; if some arrive too early or late it can cause issues. The traces aren't impedance-matched, but I did a bit of math and determined they were short enough that I didn't have to worry about it.&lt;/p&gt;&lt;p&gt;Also, I want to talk about the USB port. I used Mini-USB. Alright look. I know I know, I should have used USB-C. But I don't like USB-C! It's a dumb standard. We spent decades teaching non-technical users to plug the square wire into the square hole and the round wire into the round hole. And then we made every hole the same shape!! But they don't all support the same things!! Not even every cable supports the same things!! I hate it!! And Mini-USB is so cute. It's not reversible, but who cares? It's more robust than micro USB, while still being small. And it's my board, my rules. So yes, I will keep sending pictures of this board to people, and they will keep complaining it doesn't use USB-C. And I will continue to not care! Mini-USB is CUTE. And by the way, if you read this entire article and this is the section you choose to engage with, then you are boring!!! You will never live up to Mini-USB!!&lt;/p&gt;&lt;p&gt;Okay okay sorry about that. I am calm now. With all of that out of the way, I placed the order for the boards. I bought 5 of them, 2 of which were partially assembled. I would complete the rest of the assembly myself, but I didn't want to worry about the more finicky stuff. Between taxes, tariffs, and shipping, it came to a little over a hundred dollars USD.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes&lt;/head&gt;&lt;p&gt;About a week later, I was back home in NYC. My boards hadn't arrived yet, although I did have access to an STM32H723 development board at this point. To prepare for my boards, I started porting my RP2040 firmware to the STM32H723.&lt;/p&gt;&lt;p&gt;Things were going well until I tried getting USB set up. For some reason, I could only get it working at USB FS speeds. I figured I was just initializing something wrong - maybe a register I was forgetting about, or that wasn't in the HAL? I did a lot of digging, before finding this hidden in the datasheet (emphasis mine):&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The devices embed a USB OTG high-speed (up to 480 Mbit/s) device/host/OTG peripheral that supports both full-speed and high-speed operations. It integrates the transceivers for full-speed operation (12 Mbit/s) and a UTMI low-pin interface (ULPI) for high-speed operation (480 Mbit/s). When using the USB OTG_HS interface in HS mode, an external PHY device connected to the ULPI is required.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;My heart sank. Yes, despite this chip very clearly advertising support for USB HS, it can't actually do that without an external PHY. This is super easy to miss - I actually told other people about the problem, and often they would tell me I was incorrect until I showed it to them in the datasheet. I've also found many posts on the ST Community forums from people running into the same thing. So yeah, I need a new board.&lt;/p&gt;&lt;p&gt;But because boards are expensive, I figure I'll still use the rev 1 board to validate as much as I can.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes, again&lt;/head&gt;&lt;p&gt;Once the boards come, I complete assembly of one, plug it into my computer, and nothing happens. I find out that the 3.3V rail is shorted to ground. This is the same on all of my boards, even the 3 that are disassembled. Some debugging later, it turns out I moved a via in KiCad and didn't do a re-pour. My ground plane was connected to my power plane.&lt;/p&gt;&lt;p&gt;I have a full CI/CD pipeline set up for my PCBs, so I was surprised it didn't catch this. It turns out it has a bit of wiggle-room, and the re-pour was small enough it didn't get picked up. I now know I need to be disciplined and run DRC locally, ensuring there are literally no differences (and if there are, commit them and push them up to my Git forge).&lt;/p&gt;&lt;p&gt;Although annoying, and quite embarrassing, this wasn't a huge deal. I used a drill bit and very carefully drilled out the offending via by hand. It made a bit of a mess - make sure you use breathing protection - but I got a board that worked.&lt;/p&gt;The drilled-out via. You can see it directly under the text, near the center-bottom of the image.&lt;p&gt;At this point I wrote some code that exercised the HyperRAM and VGA. Everything worked great, so I began work on the new board. Here's what my development setup looked like while I was testing:&lt;/p&gt;&lt;p&gt;Even though the rev 1 board didn't work out, Frank pointed out that the difference between it and the previous revision was stark:&lt;/p&gt;&lt;p&gt;Not a bad pace of development!&lt;/p&gt;&lt;head rend="h2"&gt;Attempt 4 - Rev 2&lt;/head&gt;&lt;p&gt;I needed an STM32 that supported ULPI (used for talking to the USB PHY), LTDC, and some kind of external RAM. I looked at dozens of chips and found all sorts of blockers. Chips that actually supported both (but they had overlapping pins), chips that were advertised as supporting both (but in actuality, could only do one or the other, depending on the specific model number), and chips that actually could do both, with unconflicting pins, but only in a BGA package. I did not particularly want to deal with that, mainly because the tiny vias and traces would balloon the board cost even more.&lt;/p&gt;&lt;p&gt;I ended up settling on the STM32H750IBT, a massive, 176 pin, LQFP chip. This thing is larger than some New York apartments, and at over $10 USD, it costs about the same! I have bought entire dev boards for a fraction of this.&lt;/p&gt;&lt;p&gt;Once I picked out the chip, I basically redesigned the entire board from scratch. Sure, I could reuse the DACs, but I needed completely new RAM (the new chip has no HyperBus), as well as the USB PHY and supporting components. Now that my Christmas vacation was over, it took me a solid week to get everything designed. This isn't my most complicated board, but it's certainly my most complex routing:&lt;/p&gt;&lt;p&gt;I mean, look at those traces. I'm using basically all available space just to get them to be the same length. ST famously has bad pinouts, and because one of the memory controller pins is located on the complete opposite side of the chip, literally all of the rest of the RAM traces had to be lengthened. And the RAM has a 16-bit data bus. I had to route 38 length-matched traces for the memory alone!&lt;/p&gt;&lt;p&gt;The USB PHY also had a decent number of traces to route, although far less than the RAM. This is probably the part where I'm supposed to say that like, crosstalk is bad and stuff, but we're just gonna ignore that. I had like no space; leave me alone!&lt;/p&gt;&lt;p&gt;Here's what the board looked like:&lt;/p&gt;&lt;p&gt;And with that, I ordered the board. Waiting for it to arrive just about killed me, but when it finally did, I got to work.&lt;/p&gt;&lt;head rend="h2"&gt;Board bring-up&lt;/head&gt;&lt;p&gt;Board bring-up is a magical thing. One-by-one, you enable each part of the board, and you make sure that everything works the way you expect. Given that USB burned me before, I decided to start there.&lt;/p&gt;&lt;p&gt;Right out of the gate, I was off to a bumpy start. I got the USB technically working, and I even got it to show up on my computer as USB HS (yay!), but it was super, super flaky. Eventually I worked out that its crystal oscillator was unstable. Going back to the datasheet, I realized I missed a 1M ohm resistor, which was meant to be put in parallel with the crystal. I didn't have one handy, but I know the human body is around that resistance. I put one finger on each terminal of the crystal. It immediately stabilized. I was pretty ecstatic!&lt;/p&gt;&lt;p&gt;The next day I went to the Recurse Center and stole a 1M ohm resistor to affix to the board. (Faculty, if you're reading this, I owe you about a tenth of a cent. Sorry!)&lt;/p&gt;&lt;p&gt;With that over, the rest of the bring-up process was pretty smooth. I got the LTDC running and ported over the rest of the code that implemented the GUD protocol. I had written things pretty naively but, to my surprise, it didn't need any optimization for high-speed USB. I guess that's what a microcontroller with a 480 MHz core will get you!&lt;/p&gt;&lt;head rend="h2"&gt;Running it in the RCade cabinet&lt;/head&gt;&lt;p&gt;I was already at the Recurse Center at this point, so I popped the back off the RCade, unplugged the VGA from the Pi, and plugged it into my board. It started up immediately - the colours looked great and I got the full 60 Hz framerate. To be honest, I was shocked at how good it looked, and the crowd that had formed was shocked too. I wasn't really a believer that 24 bit colour would be noticeable, but I was totally wrong. The lack of colour banding was striking.&lt;/p&gt;&lt;p&gt;Next, I plugged the board into the Pi, and Frank reconfigured it to make my display adapter the primary display. We launched the normal RCade software and played some games. They looked truly amazing; nothing like before. Rose, one of the main people who developed the software, joked that it looked so good that some of the graphical shortcuts she took were no longer sufficient.&lt;/p&gt;&lt;p&gt;It's hard to tell in the pictures but the difference in person was striking. Where it's most obvious is in the lack of banding around the mountains.&lt;/p&gt;&lt;p&gt;This felt amazing, but I wasn't quite ready to leave the board installed. It was fragile - especially with the resistor I bodged on - and it was expensive. I took my board back out and Frank reverted the RCade to how it was before.&lt;/p&gt;&lt;head rend="h2"&gt;Designing a case&lt;/head&gt;&lt;p&gt;I'll be honest. I don't get that much joy out of 3D modeling. I find it frustrating, tedious, and generally unfulfilling. To get around this, I decided to use YAPP to design the case. YAPP is a parametric box generator written in OpenSCAD. I wrote a few dozen lines of code and ended up with this beauty:&lt;/p&gt;&lt;p&gt;It took barely any time at all and only took 2 physical revisions before I was happy with it. I added the OpenSCAD code to my board repository and CI/CD pipeline. Now, it builds all the files I need to order the boards, as well as the STL files for the case.&lt;/p&gt;HE'S BEGINNING TO TAKE FORM&lt;p&gt;And now, with the board in the case:&lt;/p&gt;&lt;p&gt;At this point I was starting to prepare myself to install it in the RCade.&lt;/p&gt;&lt;head rend="h2"&gt;Disaster strikes, again??&lt;/head&gt;&lt;p&gt;Everything was done, so I expected I'd just plug it in and be good to go. When I did this, though, nothing happened. After some debugging I realized the USB had completely died on my board. It wasn't showing up on any computer I connected it to, although the STM32 was still chugging along happily (and outputting to VGA).&lt;/p&gt;&lt;p&gt;I still haven't figured out exactly what happened here. I was having a bit of flakiness with the USB already. I vaguely suspect ESD to either the STM32 or the USB PHY, but am not super confident this is the cause. I'm going to keep looking into this. (inb4: wow maybe you shouldn't have touched the crystal without grounding yourself first!)&lt;/p&gt;&lt;p&gt;In the meantime, I assembled a second board and got that installed instead. I'm slightly nervous because I don't have a third board to use if this one also dies, and I don't want to order any more until I can figure out what's killing them. That said, it has been a few days now since I installed it, and despite running 24/7, there's no signs of it dying yet.&lt;/p&gt;&lt;p&gt;Here's the board in its case, installed in the RCade. We're still running it off the Raspberry Pi for now, but soon we'll have that switched out with a laptop. I can't wait!&lt;/p&gt;&lt;head rend="h2"&gt;Future improvements&lt;/head&gt;&lt;p&gt;There are all sorts of things I want to change. I want the board to also support audio, with an integrated amp. Perhaps even a tube amp? I just think it would be funny. And being able to read input from the controls would be cool too.&lt;/p&gt;&lt;p&gt;On the software side, I want double or triple buffering. I actually got them both working, although they didn't play nice with the deltas that GUD sends over the wire. There are workarounds to this that I haven't implemented yet. It would also be nice to give GUD the ability to disable these deltas; perhaps that would be a good feature for me to add to the kernel module. Writing some documentation on the GUD protocol could be good too!&lt;/p&gt;&lt;p&gt;This was a really fun project, and it's not over yet, but I think all the hard stuff is pretty much done (although - I've thought that before!). I really wasn't expecting this to take as long as it did, but I learned so much, and I'm a stronger engineer for it.&lt;/p&gt;&lt;head rend="h2"&gt;Source code&lt;/head&gt;&lt;p&gt;There's a few repositories of interest:&lt;/p&gt;&lt;p&gt;The hardware lives here.&lt;/p&gt;&lt;p&gt;The software lives here.&lt;/p&gt;&lt;p&gt;If you're interested, the original software for the RP2040 lives here.&lt;/p&gt;&lt;p&gt;My very messy DAC equations live here.&lt;/p&gt;&lt;p&gt;My Nix GUD gadget attempt lives here.&lt;/p&gt;&lt;p&gt;I also wrote a fair bit of scratch code while learning (such as for my kernel module), but I don't think any of it was worth putting it in my Git forge.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46888795</guid><pubDate>Wed, 04 Feb 2026 17:35:05 +0000</pubDate></item><item><title>The Great Unwind</title><link>https://occupywallst.com/yen</link><description>&lt;doc fingerprint="7fad91172de75f94"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you wondered why the stock market has been so choppy since October and why crypto and gold keep flash crashing? The western media would have you believe this is due to AI bubble, war in Greenland, and Trump's tweets. We have a better story to tell.&lt;/p&gt;
    &lt;p&gt;Wall Street has lost control of the Japanese Yen carry trade unwind.&lt;/p&gt;
    &lt;p&gt;There's been a fair bit of quiet chaos in financial markets recently. Cryptocurrencies have lost 40% of their value. We saw silver drop 40% which hasn't happened since 1980. Stocks like Microsoft are getting picked off one-by-one with 15% drops when positive earnings reports come out. Meanwhile the broader market chops sideways, so people think things are fine. Trump and Europe were on the brink of war for control of a desolate arctic territory. Truth Social has overtaken FOMC as the most important source of financial news. These things may all appear to the untrained eye as a series of idiosyncratic, disconnected shocks. The prevailing media narrative is that the market is reacting negatively to AI CapEx spending and a hawkish new Fed chair. But our systematic analysis of cross-asset flows, derivatives positioning, central bank policy minutes, and institutional balance sheets suggests a singular, unified causality that binds these disparate anomalies, which is the covert unwinding of the Japanese Yen carry trade.&lt;/p&gt;
    &lt;p&gt;For nearly thirty years, the Bank of Japanâs (BOJ) Zero Interest Rate Policy (ZIRP) and subsequent Negative Interest Rate Policy (NIRP) effectively transformed the Yen into the worldâs funding currency. We would call it the greatest free money printer ever made. By anchoring borrowing costs at or near zero, the BOJ enabled Wall Street to borrow Yen cheaply and invest it with leverage into higher yielding instruments globally, such as U.S. treasuries, equities, and cryptography. For example, you borrow Yen from Japan at 0% interest, you exchange it for USD, and then you buy treasury bonds that pay 4%. It's that simple. This funded government benefits and provided continuous reliable liquidity for financial markets that made stocks keep going up while suppressing volatility.&lt;/p&gt;
    &lt;p&gt;Trillions of dollars of free loans from the Bank of Japan were used by a generation of investors to buy a double digit percentage of the U.S. economy. Now those loans are being recalled. Wall Street traders who levered up on the free Japanese money now have to sell trillions of assets and convert the proceeds back to Yen in order to not be liquidated. These aren't happy times for them. They get liquidated when Japan raises interest rates; they get liquidated when the Federal Reserve lowers interest rates; they get liquidated when the Japanese Yen increases in value; they get liquidated when tech stocks aren't going up enough, and all four of these things have been happening at once.&lt;/p&gt;
    &lt;p&gt;Wall Street may be greedy, but they're very intelligent too. They made many smart choices about where to put the "free" money. Now let's say you're someone who's also smart, but was wise enough to not use Sauron's ring. Chances are you invested in the same things as Wall Street. So by now you've probably seen your whole portfolio move against you; you're wondering why your hedges don't work; and you feel like you're being punished for making all the right choices. It's because other smart people, who got greedy, are being forced to close their positions, and you're the whipping boy for their avarice.&lt;/p&gt;
    &lt;p&gt;The Japanese Yen is sort of like GameStop ($GME). It's the most shorted currency on Earth. When you borrow yen to buy American assets, you're effectively shorting the yen. Currency can be rehypothecated so that yen-denominated debt ends up exceeding the actual yen supply, the same way GME's short interest exceeded 100% of its float. When shorts start covering it compounds tragedy, because they all have to buy yen, which makes its value increase, forcing more shorts to cover, and Japan is a small island.&lt;/p&gt;
    &lt;p&gt;This December 2025 rate hike to 0.75%, followed by the explicitly hawkish signalling from Prime Minister Sanae Takaichiâs administration, has fundamentally altered the risk-reward calculus of these leveraged positions. The market disruptions observed in January 2026 bear the distinct mathematical signature of a forced liquidation event rather than a fundamental repricing of growth prospects. When correlations between historically uncorrelated assets (e.g. Gold, Bitcoin, Microsoft, and Silver) approach 1.0 during a sell-off, it serves as a distinct indicator that traders are not selling what they want to sell, but rather what they must sell in order to meet margin calls in a funding currency that is rapidly appreciating against their liabilities.&lt;/p&gt;
    &lt;p&gt;We shall investigate the mechanics of this unwind in exhaustive detail. We analyze the "Greenland Distraction" not as a root cause but as a volatility trigger that shattered the complacent calm of the "Davos Consensus." We examine the anomalous liquidation in precious metals following the nomination of Kevin Warsh to the Federal Reserve Chairmanship, and we dissect the flow of funds from major Japanese institutional whales like Norinchukin Bank, whose retreat from foreign bond markets has left a liquidity vacuum in the U.S. Treasury complex. The evidence points to a systemic repricing of the global cost of capital, originating in Tokyo and transmitting violently through the plumbing of Wall Street, leaving no asset class untouched.&lt;/p&gt;
    &lt;p&gt;To fully comprehend the market chaos of January 2026, one must look beyond the immediate headlines of the new year and scrutinize the subtle yet seismic shifts that occurred in Tokyo during the closing months of 2025. The conventional market narrative has long regarded the Bank of Japan as a passive, almost paralyzed actor, perpetually trapped in a deflationary mire and unable to normalize policy. This view has always been demonstratably false. The truth is that Wall Street leaders have been planning for the next quarter, while the Japanese have been preparing for the next century. The data confirms a deliberate, aggressive shift toward normalization that caught global carry traders offguard.&lt;/p&gt;
    &lt;p&gt;In a move that many Western analysts critically underestimated, the Policy Board of the Bank of Japan voted unanimously to raise the uncollateralized overnight call rate to 0.75% during its policy session on December 18-19, 2025. While a 25 basis point hike might appear negligible in the context of Federal Reserve or ECB tightening cycles, in the context of the Japanese financial system, which has operated near the zero-bound for decades, it represents a massive tightening of financial conditions.&lt;/p&gt;
    &lt;p&gt;This move was not merely a technical adjustment; it was a fundamental regime change. Coming from a baseline of -0.1% in early 2024 and 0.50% in late 2025, the move to 0.75% signaled that the era of "free money" had definitively ended. The rationale provided by the BOJ was grounded in shifting inflationary dynamics. Core CPI (excluding fresh food), the central bank's preferred metric, was tracking near 3% in late 2025, persistently exceeding the 2% price stability target.Although inflation eased slightly to 2.4% in December, the BOJ minutes reveal a board convinced that "wage gains may be durable," thus justifying higher rates to prevent a wage-price spiral.&lt;/p&gt;
    &lt;p&gt;Crucially, the minutes from the December meeting, which were released in late January 2026, contain explicit language suggesting that the tightening cycle is far from complete. The board noted that "real interest rates are expected to remain negative," implying that a policy rate of 0.75% is still considered accommodative relative to inflation.To a bond trader, this is hawkish code. It suggests that the "neutral rate" is significantly higher, potentially between 1.5% and 2.0%. If the market prices in a terminal rate of 2.0%, the cost of funding for carry trades effectively triples from previous levels, turning profitable arbitrage positions into deep losses.&lt;/p&gt;
    &lt;p&gt;The political dimension in Japan has exacerbated the monetary tightness, creating a "double tightening" effect that algorithms have struggled to price. Prime Minister Sanae Takaichi, preparing for a snap election on February 8, 2026, has adopted a complex economic stance that blends fiscal expansion with monetary discipline, a volatile mix for currency markets.&lt;/p&gt;
    &lt;p&gt;Takaichi advocates for "strategic fiscal spending" and tax cuts to stimulate the domestic economy. In standard macroeconomic theory, an expansionary fiscal policy (increased government spending) combined with a tightening monetary policy (higher rates to combat the resulting inflation) is the perfect recipe for currency appreciation. While Takaichi has publicly softened her rhetoric to avoid accusations of currency manipulation, stating she "did not have a preference for the yen's direction", her policies speak louder than her soundbites.&lt;/p&gt;
    &lt;p&gt;The market fears that Takaichiâs proposed fiscal largesse will force the BOJ to hike rates faster than currently projected to counteract the inflationary effects of government spending. This creates a two-front war on the Yen carry trade:&lt;/p&gt;
    &lt;p&gt;Cost of Funding Rises: Higher BOJ rates make borrowing Yen expensive.&lt;/p&gt;
    &lt;p&gt;Exchange Rate Risk: If the Yen appreciates due to the fiscal-monetary policy mix, the principal value of the USD-denominated assets held by Japanese investors falls in Yen terms, triggering margin calls.&lt;/p&gt;
    &lt;p&gt;The tension between the Prime Minister's office and the Ministry of Finance (MOF) adds another layer of uncertainty. Finance Minister Satsuki Katayama has been far less tolerant of currency volatility, repeatedly intervening or threatening intervention when USD/JPY approaches the 155-160 danger zone.This political friction creates a "floor" for the Yen, making shorting the currency a perilous endeavor for global macro funds.&lt;/p&gt;
    &lt;p&gt;Perhaps the most critical, yet underreported, development is the behavior of Japan's gargantuan institutional investors, specifically Norinchukin Bank (often referred to as the "CLO Whale") and Nippon Life Insurance. These entities have historically been the largest buyers of U.S. debt, recycling Japan's trade surplus into U.S. Treasuries and corporate bonds.&lt;/p&gt;
    &lt;p&gt;The data indicates a massive reversal in these flows. Following significant losses in 2024 and 2025 due to unhedged exposure to U.S. and European sovereign bonds, Norinchukin has been actively liquidating foreign assets. By the end of December 2025, the bank had unloaded nearly Â¥12.8 trillion (approximately $88 billion) in foreign government bonds.The bankâs CEO, Taro Kitabayashi, confirmed the completion of this sell-off, stating the bank would "take its time" before committing capital to fresh investments.&lt;/p&gt;
    &lt;p&gt;The significance of this cannot be overstated. A major, price-insensitive buyer of U.S. debt has left the building. When the U.S. Treasury issues debt to fund its deficit, Norinchukin is no longer the guaranteed bid. This removal of liquidity support weakens the floor for U.S. Treasuries, contributing to the yield spikes seen in January. Similarly, Nippon Life has signaled a rotation back into domestic Japanese Government Bonds (JGBs), acknowledging that "unrealized losses" on foreign bonds had swelled to Â¥4.7 trillion.The logic is simple: why take currency risk for a 4.5% U.S. yield when domestic JGB yields are rising and offer a risk-free return in your home currency?&lt;/p&gt;
    &lt;p&gt;By December 31, 2025, the stage was set. The "free money" era was over. The largest holders of capital in Tokyo were repatriating funds or moving into cash. Global markets, however, were still positioned for "business as usual", long Nvidia, long Bitcoin, short Yen. The dissonance between Japanese reality and Western positioning created the perfect conditions for a crash.&lt;/p&gt;
    &lt;p&gt;To validate the thesis that the Yen unwind is the primary driver of volatility, we must examine the sequence of events. The crash did not happen in a vacuum; it followed a precise timeline where geopolitical shocks acted as triggers for a structural fragility that had been building since the BOJ's December pivot.&lt;/p&gt;
    &lt;p&gt;The pressure began to build in Q4 2025. As the BOJ signaled its intention to hike rates, Japanese traders, often the "canary in the coal mine" for global liquidity, began to reduce risk. This cycle started with Bitcoin. Bitcoin is a pure liquidity asset; it has no yield and is often funded via margin. As the cost of Yen margin rose, Japanese selling pressure on Bitcoin intensified from October through December.This was the first tremor.&lt;/p&gt;
    &lt;p&gt;Was the "Greenland War" theater? While the military dimensions may have been performative, the economic consequences were tangible and acted as the catalyst that exposed the fragility of the Yen carry trade.&lt;/p&gt;
    &lt;p&gt;On January 17, 2026, President Trump escalated his demand to purchase Greenland by threatening a 10% tariff on eight European nations (including the UK, Germany, and France) and escalating to 25% by June if the territory was not ceded.This introduced a "tail risk" that markets had not priced: the fracture of the Atlantic economic alliance.&lt;/p&gt;
    &lt;p&gt;Following the Martin Luther King Jr. holiday, U.S. markets opened on January 20 to a bloodbath. The S&amp;amp;P 500 fell 2.1%, the Nasdaq composite dropped 2.4%, and yields on U.S. Treasuries spiked.The narrative was "Greenland," but the market mechanics told a different story. The threat of tariffs on close allies disrupts the "Atlantic Trade" narrative. For Japanese investors holding U.S. assets, this introduced a new risk premium. It wasn't just about rates anymore; it was about the stability of the U.S.-led global order. This geopolitical volatility forced risk parity funds and algorithmic traders to reduce gross exposure. When a global portfolio deleverages, it buys back its funding currency. In this case, it bought Yen.&lt;/p&gt;
    &lt;p&gt;While Trump walked back the military threat on January 21 at Davos, the economic threat of tariffs remained a live wire. The volatility persisted, suggesting that the "Greenland" narrative was merely the match that lit the fuse of a much larger powder keg.&lt;/p&gt;
    &lt;p&gt;The final and most violent phase of the crash occurred at the end of the month, triggered by the nomination of Kevin Warsh as Federal Reserve Chair.Warsh is widely perceived as a hawk, favoring sound money and skepticism toward quantitative easing. His nomination signaled the potential end of the "Fed Put", the assumption that the central bank would always intervene to support asset prices.&lt;/p&gt;
    &lt;p&gt;This announcement triggered a massive repricing of the "Debasement Trade." Assets that thrive on currency debasement, Gold, Silver, and Bitcoin, collapsed. Gold fell ~11%, and Silver crashed ~36% in a single session.This synchronization of losses across uncorrelated assets (Tech and Gold falling together) is the definitive signature of a liquidity crisis driven by margin calls.&lt;/p&gt;
    &lt;p&gt;The unwinding of a carry trade is not a monolithic event; it is a cascade that ripples outward from the most liquid and speculative assets to the core holdings of institutional portfolios. The sequence of asset price collapses observed from October 2025 to January 2026 follows this classic liquidation hierarchy perfectly.&lt;/p&gt;
    &lt;p&gt;As noted, the unwind began in the crypto markets. Japan is home to a massive retail crypto trading base, and the Yen is a major pair for Bitcoin trading. Snippets indicate that Japanese traders began selling off Bitcoin in October 2025.&lt;/p&gt;
    &lt;p&gt;This timing is crucial. It correlates with the period when the BOJ began signaling the December rate hike. Retail traders, facing higher mortgage rates and loan costs in Japan, likely liquidated their most volatile, liquid asset first to raise cash. The selling was exacerbated by the looming tax reform in Japan. While the proposal to move to a flat 20% tax rate is bullish in the long term, the immediate pressure of rising funding costs forced traders to sell before the tax cut could be realized.By January 31, massive outflows from Bitcoin ETFs ($528 million) coincided with the broader market crash, confirming that crypto was being used as a source of liquidity to cover losses elsewhere.&lt;/p&gt;
    &lt;p&gt;Consider the "painful ~3% dump" in the Nasdaq and Microsoft's staggering 15% drop. On January 29, 2026, Microsoft reported earnings. Despite beating revenue estimates ($81.27 billion vs. $80.28 billion), the stock plummeted ~11-15% intraday.&lt;/p&gt;
    &lt;p&gt;The street blamed concerns over "AI CapEx", the idea that Microsoft was spending billions on data centers with slow return on investment. However, a 15% drop in a $3 trillion company on a "good" earnings beat is rarely fundamental; it is mechanical. Microsoft is a quintessential "momentum" stock, heavily held by foreign institutional investors, including Japanese pension funds. When the Yen strengthens, the value of these USD-denominated assets falls in JPY terms.&lt;/p&gt;
    &lt;p&gt;If a Japanese insurer holds Microsoft unhedged, a falling USD/JPY exchange rate hurts their balance sheet. If they hold it hedged (rolling short USD positions), the rising U.S. rates (driven by the Warsh nomination) make the hedge prohibitively expensive. The January 29 drop was likely exacerbated by a "stop-loss" cascade from Tokyo desks. As the price broke key technical levels, algorithms programmed to protect Yen-denominated returns indiscriminately sold the most liquid blocks. Microsoft, being one of the most liquid stocks in the world, became the ATM for the rest of the portfolio.&lt;/p&gt;
    &lt;p&gt;The most compelling evidence of a forced liquidation event is the behavior of Gold and Silver on January 31, 2026. Gold fell ~11-12% and Silver crashed ~31-36% in a single session. Historically, Gold acts as a safe haven during equity market turmoil. If the Nasdaq is crashing due to "Greenland" fears, Gold should rally. Instead, it crashed.&lt;/p&gt;
    &lt;p&gt;This anomaly can be explained by two factors:&lt;/p&gt;
    &lt;p&gt;The Warsh Effect: As discussed, Warsh's nomination strengthened the USD and undermined the thesis for holding anti-fiat assets.&lt;/p&gt;
    &lt;p&gt;Margin Call Dynamics: Snippets reveal that CME Group and the Shanghai Gold Exchange raised margin requirements on gold and silver futures days before the crash.When Japanese traders faced losses on their Microsoft longs and their Yen shorts, they needed cash immediately. They couldn't sell illiquid bonds quickly enough, so they sold their "winners." Gold had rallied to ~$5,400/oz prior to the crash. Traders liquidated their profitable Gold positions to pay for the margin calls on their losing Tech and Yen positions.&lt;/p&gt;
    &lt;p&gt;Cross-Asset Correlations (Week Ending Jan 31, 2026)&lt;/p&gt;
    &lt;p&gt;This correlation breakdown is visualized in Figure 2, where the correlation between Gold and the Nasdaq 100 spikes to nearly 1.0 during the crash week, a statistical anomaly that only occurs during severe liquidity events.&lt;/p&gt;
    &lt;p&gt;The "Yen Whale" hypothesis is strongly supported by the data on futures volumes and repo market stress. The "central mystery" is not just in the price action, but in the unseen flows of the derivatives market.&lt;/p&gt;
    &lt;p&gt;About a week ago, some whale kicked off an astronomically large market order for a /6J long when it hit recent lows. /6J (CME Yen Futures) hit a low of ~0.00647 (approximately 154.50 USD/JPY) in late January. This level has historically been a "line in the sand" for the Japanese Ministry of Finance (MOF).&lt;/p&gt;
    &lt;p&gt;CME reported record volumes in FX and Interest Rate products for January 2026.The aggressive buying off the lows suggests a massive repatriation flow. Who is the Whale? Two theories emerge:&lt;/p&gt;
    &lt;p&gt;The MOF Thesis: The Ministry of Finance has a history of stealth intervention. Buying /6J (Long Yen) is functionally equivalent to selling USD reserves. Buying futures allows them to support the currency without immediately depleting cash reserves, squeezing speculators who are short.&lt;/p&gt;
    &lt;p&gt;The Carry Unwind: A massive hedge fund or bank (like Norinchukin) realizing that the "game is up" and closing out short-Yen positions. The size of the order suggests an entity that needed to move billions, not millions.&lt;/p&gt;
    &lt;p&gt;The subsequent price action, a sharp rally followed by "hammering back down", represents the battleground. U.S. macro funds are still trying to short the Yen (betting on U.S. economic exceptionalism and Warsh's policies), while Japanese domestic accounts are buying it. The volatility is the result of these tectonic plates grinding against each other.&lt;/p&gt;
    &lt;p&gt;The plumbing of the U.S. financial system showed signs of stress that coincided with the Japanese retreat. The Overnight Reverse Repo facility (ON RRP) saw a year-end spike to $106 billion but has since drained.&lt;/p&gt;
    &lt;p&gt;Japanese banks are typically huge participants in the U.S. repo market to fund their dollar assets. As Norinchukin and others retreat (repatriating funds to Japan), liquidity in the U.S. repo market becomes thinner. The "air pocket" in Microsoft and Gold prices was likely exacerbated by a lack of market maker depth in the repo-funded derivatives market. When market makers cannot access cheap repo funding, they widen spreads and reduce liquidity provision, leading to "gaps" in price action during sell-offs.&lt;/p&gt;
    &lt;p&gt;There have been significant moves in other currency futures as well: /6A increased 87 basis points, /6L rose 19 basis points, and /6S rose 18 basis points.&lt;/p&gt;
    &lt;p&gt;/6A (Australian Dollar): The 87 basis point rise in the Aussie Dollar is notable. AUD is often a proxy for Chinese growth and global risk sentiment. A rise here, amidst a tech crash, suggests a rotation out of U.S. assets and into commodities or Asia-Pacific currencies, further supporting the "Sell America" thesis triggered by the Greenland tariff threats.&lt;/p&gt;
    &lt;p&gt;/6L (Brazilian Real) and /6S (Swiss Franc): The rise in the Swiss Franc (a classic safe haven) aligns with the risk-off sentiment. The move in the Brazilian Real suggests that emerging markets are also seeing volatile flows as the dollar stabilizes.&lt;/p&gt;
    &lt;p&gt;Why was the VIX at 16 despite the chaos? The VIX measures implied volatility of S&amp;amp;P 500 options. Its relatively low level (16) compared to the violence in individual names (MSFT -15%, Gold -11%) indicates that the crash is a de-leveraging event, not a panic event.&lt;/p&gt;
    &lt;p&gt;In a panic, investors buy Puts on the index to protect themselves, spiking the VIX. In a de-leveraging event, investors simply sell the underlying assets (stocks, gold, crypto) to raise cash. They are not hedging; they are exiting. This explains why the VIX remained subdued while prices collapsed, the selling was orderly, algorithmic, and relentless, rather than emotional and panicked.&lt;/p&gt;
    &lt;p&gt;Skepticism about the "Greenland War" is well-founded. While the diplomatic row was real, its utility as a financial narrative was far greater than its geopolitical reality.&lt;/p&gt;
    &lt;p&gt;President Trump's threat of military force was retracted on January 21 at Davos.This "de-escalation" should theoretically have calmed markets. Instead, the volatility worsened into month-end. This confirms that the real problem wasn't Greenland; it was the re-pricing of the Yen.&lt;/p&gt;
    &lt;p&gt;The financial media loves a simple cause-and-effect narrative. "Stocks down because of War" is easy to digest. "Stocks down because the cross-currency basis swap spread widened due to BOJ minutes" is not. The "Greenland" narrative provided the perfect cover for sophisticated actors to liquidate positions in Gold and Tech under the guise of "war jitters." This allowed them to exit without sparking a broader panic about liquidity in the banking system. The focus on the Arctic masked the structural rot in the leverage complex.&lt;/p&gt;
    &lt;p&gt;The evidence suggests a covert, structural unwinding of the Yen carry trade is the primary driver of the January 2026 market chaos.&lt;/p&gt;
    &lt;p&gt;The interconnectedness of these events is undeniable. The BOJ's rate hike in December 2025 and the subsequent hawkish signaling from the Takaichi administration fundamentally altered the cost of capital for the world's largest carry trade. The "Greenland Crisis" acted as the initial volatility trigger, forcing a reduction in gross exposure. The nomination of Kevin Warsh acted as the final catalyst, shattering the "Debasement Trade" and forcing a liquidation of precious metals and crypto to cover margin calls on Yen-funded positions.&lt;/p&gt;
    &lt;p&gt;Here are some key takeaways:&lt;/p&gt;
    &lt;p&gt;The "Free Money" Era is Over: BOJ policies have fundamentally altered the global cost of capital. The flow of liquidity from Tokyo to New York has reversed.&lt;/p&gt;
    &lt;p&gt;Geopolitics as Catalyst: "Greenland" may have been the spark, but the Yen leverage was the powder keg. The tariff threats disrupted the "Atlantic Trade" narrative, forcing a repatriation of capital.&lt;/p&gt;
    &lt;p&gt;Liquidity Event: The synchronized crash of Gold, Crypto, and Tech confirms a systemic de-leveraging. The "Whale" orders in Yen futures and the breakdown of correlations are the smoking guns of a margin-driven event.&lt;/p&gt;
    &lt;p&gt;With the Japanese election on February 8 and U.S. tariffs looming, the "hammering" of the Yen is likely temporary. The structural trend is now toward repatriation. This implies lower U.S. asset prices, higher U.S. yields, and a stronger Yen over the medium term. The "mystery" of the low VIX is explained by the mechanical nature of the unwind, a controlled demolition of leverage rather than a chaotic panic.&lt;/p&gt;
    &lt;p&gt;This won't just be the big one. This could be the last one. If you've been preparing your whole life, knowing that something's coming, then this could be the thing you've been preparing for. One final opportunity to get the guys who did this.&lt;/p&gt;
    &lt;p&gt;Longing the Yen is commonly referred to as "The Widowmaker Trade" on Wall Street, because you have trillions of dollars of monopoly money working against you. The carry traders have compromised every level of our government. Their greatest vulnerability is the Yen rising in value. They will do anything to defend their positions, even if that means bringing America's economy down with them. Since recent events have made it obvious they're going to lose, we might as well fight them. Most of us probably won't make it out of this fight. But if we at least try, then there's a chance we might prosper when it's over.&lt;/p&gt;
    &lt;p&gt;The IV on OTM CME /6J futures calls is 11% which is astonishingly low. The same is true for calls on the FXY ETF. Call options have defined risk. The more Yen we control, the more its value goes up, and the more crooks on Wall Street get liquidated. The worst that can happen is you lose your monopoly money, but that's been happening anyway. Since carry traders own 10% of all U.S. treasuries, when they get liquidated they'll have to sell a lot of treasury bonds, which means that CME /UB futures and the TLT ETF will fall.&lt;/p&gt;
    &lt;p&gt;This blog is brought to you by various radicals, malcontents, and people who think the system is rigged. We're not affiliated with any organization. Nothing here constitutes financial advice. Occupy Wall Street is not your financial advisor or your lawyer. We're retail investors like you. Do your own research. Past performance does not guarantee future results. We are the 99 percent. The only solution is world revolution. Wall Street's time has finally come.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46889008</guid><pubDate>Wed, 04 Feb 2026 17:49:26 +0000</pubDate></item><item><title>Claude Code for Infrastructure</title><link>https://www.fluid.sh/</link><description>&lt;doc fingerprint="8660c4598e99ff8b"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Built for where you already work&lt;/head&gt;
    &lt;head rend="h3"&gt;Sandbox Isolation&lt;/head&gt;
    &lt;p&gt;Clone VMs instantly. Test changes in isolation before touching production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context-Aware&lt;/head&gt;
    &lt;p&gt;Fluid explores your host first - OS, packages, CLI tools - then adapts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Full Audit Trail&lt;/head&gt;
    &lt;p&gt;Every command logged. Every change tracked. Review before production.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ansible Playbooks&lt;/head&gt;
    &lt;p&gt;Auto-generates playbooks from sandbox work. Reproducible infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46889703</guid><pubDate>Wed, 04 Feb 2026 18:34:08 +0000</pubDate></item><item><title>Why more companies are recognizing the benefits of keeping older employees</title><link>https://longevity.stanford.edu/why-more-companies-are-recognizing-the-benefits-of-keeping-older-employees/</link><description>&lt;doc fingerprint="7ea1e07b6bc806ab"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;DEEP DIVE&lt;/head&gt;
    &lt;head rend="h3"&gt;Why More Companies Are Recognizing the Benefits of Keeping Older Employees&lt;/head&gt;
    &lt;p&gt;By Annie Coleman&lt;/p&gt;
    &lt;p&gt;Although age bias is still the norm, the value-add of longtime, experienced workers is beginning to take shape.&lt;/p&gt;
    &lt;p&gt;On the outskirts of Macclesfield, in northwest England, a branch of the UK home-improvement retailer B&amp;amp;Q quietly overturned one of corporate life’s most persistent assumptions. Faced with high staff turnover and uneven customer satisfaction, the company tried a simple experiment: In 1989, it staffed the store largely with older workers.&lt;/p&gt;
    &lt;p&gt;The results were striking, according to one study. Profits rose 18 percent. Staff turnover fell to a fraction of the company average. Absenteeism dropped sharply. An experiment that started more than 30 years ago reshaped how the retailer approached age inclusiveness and led B&amp;amp;Q to open training to all ages and feature older workers in advertising, treating experience as an advantage rather than a cost.&lt;/p&gt;
    &lt;p&gt;In 2007, BMW began implementing 70 ergonomic, low-cost improvements in a specialized assembly line in Dingolfing, Germany, to provide better conditions for its many older and middle-aged workers. Key changes included adjustable-height workstations, improved lighting and specialized stools, resulting in a 7 percent productivity increase.&lt;/p&gt;
    &lt;p&gt;Evidence suggests that similar age-performance dynamics are not limited to the quirks of retail or to the factory floor and are increasingly relevant as declining birth rates and artificial intelligence investments reduce the inflow of entry-level workers. A white paper from Bank of America’s Workplace Benefits group argues that recruiting and retaining older workers is becoming increasingly important as populations age, framing age-inclusive benefits not as accommodation, but as a driver of organizational performance, especially for roles where judgment, experience and decision quality matter most.&lt;/p&gt;
    &lt;p&gt;“The retention of these older workers is an idea that is becoming much more well-received,” says Cynthia Hutchins, Bank of America’s inaugural director of financial gerontology. Hutchins has been involved in implementing a workforce longevity policy that includes hybrid schedules, financial planning benefits, menopause support, grandparents’ leave and sabbaticals. “It’s almost a business imperative to institute those types of benefits” to retain older workers and attract younger ones, adds Hutchins.&lt;/p&gt;
    &lt;p&gt;Yet initiatives such as these are rarely framed as strategy or as signals of a deeper shift. Most corporations continue to design careers as if effectiveness peaks early — as if speed, stamina and innovation belong exclusively to the young. If experience improves outcomes, why are so many organizations structured to push people out just as their value peaks?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If experience improves outcomes, why are so many organizations structured to push people out just as their value peaks?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The Albatross or the Wise Man&lt;/head&gt;
    &lt;p&gt;At the heart of corporate resistance lies a fundamental disagreement about value. Moody’s Analytics chief economist Mark Zandi framed the debate in Aging and the Productivity Puzzle, a 2018 analysis delineating two schools of thought. The “albatross theory” holds that workers above the age of 65 drag down productivity due to resistance to change and outdated skills. The “wise man theory” tells a different story: of workers who possess judgment, institutional knowledge, emotional intelligence and expertise that younger employees cannot replicate.&lt;/p&gt;
    &lt;p&gt;Zandi and his colleagues analyzed state-level ADP data in the U.S. and concluded that post-retirement-age workers slowed wage growth and productivity, largely because they tend to be averse to adopting new technologies. Yet several major institutions reject the idea that older workers are a productivity “albatross” — and most look at the effects, not of those above the age of 65, but of the 50-plus age workforce, often the first in line for layoffs.&lt;/p&gt;
    &lt;p&gt;More recent research from AARP and the OECD shows that firms with more 50-plus workers are more productive, not less: a 10-percentage-point increase in older workers is associated with roughly 1.1 percent higher productivity. The 2020 OECD analysis also finds that age-balanced firms benefit from lower turnover and stronger team performance, driven by experience and knowledge sharing rather than technology resistance. Similarly, a 2022 study from Boston Consulting Group found that cross-generational teams outperform homogeneous ones when older workers’ judgment and mentoring are combined with younger workers’ digital skills. A 2022 meta analysis also pushes back against the idea that older workers are less effective, and found that teams perform better when members have a long tenure at the company, irrespective of workers’ ages.&lt;/p&gt;
    &lt;p&gt;Still, Zandi says that the value of older workers may depend on how AI in the workplace unfolds and what impact it has on productivity growth. “If AI turns out to be a bust or doesn’t live up to expectations, and you have other demographic forces that are restraining labor growth, then I think older workers should fare well,” Zandi says. He notes that so far, older workers have “navigated things reasonably gracefully,” while younger workers and mid-level managers are so far taking the brunt of AI-related impacts. &lt;/p&gt;
    &lt;head rend="h3"&gt;People Peak Later Than We Think&lt;/head&gt;
    &lt;p&gt;Population aging is often treated as a future problem, something to be managed later with technology or policy tweaks. In reality, it is already reshaping labor markets in the U.S. and across advanced economies. Birth rates are lower, people are living longer and the share of workers above the age of 50 is rising steadily. This is not a forecast. It is arithmetic.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Across advanced economies, there appears to be a persistent pattern of early exits that are less about individual choice than organizational design.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yet organizational assumptions about performance have not kept pace. Modern careers are still built around the idea that effectiveness peaks early. Recent research challenges that view. A 2025 study in the journal Intelligence, analyzing age trajectories across 16 cognitive, emotional and personality dimensions, finds that while processing speed does decline after early adulthood, many of the capabilities most relevant to complex work continue to improve well into midlife. When these traits are combined into a composite measure of overall functioning, performance peaks between ages 55 and 60.&lt;/p&gt;
    &lt;p&gt;But if proficiency increasingly peaks in late midlife, then why are so many careers ending before they can be fully realized? Across advanced economies, there appears to be a persistent pattern of early exits that are less about individual choice than organizational design.&lt;/p&gt;
    &lt;p&gt;In the U.S., analysis by the Urban Institute of survey data of older workers from 1992 to 2016 showed that more than half above the age of 50 were pushed out of long-held jobs before they chose to retire, often through layoffs or restructuring rather than performance issues. The 2018 study — along with reporting from ProPublica — found that few ever regained comparable pay or responsibility, and hiring practices reinforced the trend.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The fact that more than half of U.S. workers above the age of 50 leave long-held jobs for reasons unrelated to performance and before they choose to retire is a systemic design failure.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Bill Greene, a longtime business consultant, is an exception to this layoff trend. Hired at 64 as principal of Mind Share Partners, a nonprofit in San Francisco, he advises companies on the importance of creating mentally healthy environments, and cautions that the workplace is a minefield of biases — and that ageism cuts both ways for older workers and younger workers.&lt;/p&gt;
    &lt;p&gt;Greene advises employers to be aware of the blind spots and inconsistencies. In the technology industry, he says, “it’s widely perceived that if you are 45 years old or over, you are a dinosaur,” yet in politics, “you can be 70, 75, 80, 85, and apparently that’s OK.”&lt;/p&gt;
    &lt;p&gt;Experience helps in an emergency. When the Covid-19 pandemic struck in 2020, Greene was consulting for a financial services firm, and he saw firsthand how worried his client was that younger employees were going to panic and quit because they hadn’t been through a crisis of that magnitude before.&lt;/p&gt;
    &lt;p&gt;“They realized that they had to coach their younger employees,” he says, comparing the pandemic to the 2008 financial crash to help the client’s staff understand the risks and path forward. “That kind of wisdom and experience can come with more depth of understanding and perspective from an older employee than from a younger one,” he says.&lt;/p&gt;
    &lt;head rend="h3"&gt;Small-Scale Experiments Miss an Urgent Challenge&lt;/head&gt;
    &lt;p&gt;Although several Fortune 500 companies have advertised their interest in hiring and retaining older workers, corporate commitments remain tentative and small-scale. UK-based Unilever launched its U-Work program in 2019, and now offers employees in nine countries a hybrid between traditional employment and gig work: a monthly retainer, benefits and freedom to choose which projects they work on and when. Workers can scale back hours, pursue other interests or transition gradually toward retirement.&lt;/p&gt;
    &lt;p&gt;The program is innovative and, by all accounts, successful. Half of participants are above the age of 50. But only 140 employees out of Unilever’s 150,000-strong global workforce participate. This raises a question: Are these strategies of genuine transformation or sophisticated public relations?&lt;/p&gt;
    &lt;p&gt;Three converging forces make the case for urgency. First, premature exit creates value leakage. The fact that more than half of U.S. workers above the age of 50 leave long-held jobs for reasons unrelated to performance and before they choose to retire is a systemic design failure.&lt;/p&gt;
    &lt;p&gt;Second, the demand-side blind spot. Globally, spending by people above the age of 55 is projected to approach $15 trillion annually by the end of this decade, making older consumers one of the largest and fastest-growing sources of demand in the world economy. Yet many companies treat older customers as peripheral.&lt;/p&gt;
    &lt;p&gt;There are exceptions. Alan Patricof, now 91 and still investing, launched Primetime Partners at 85 after observing that venture capital remained focused on millennials, despite obvious unmet demand among older adults. His fund has invested in more than 35 companies serving what he calls the “ageless market.” Consumer brands are adapting, too — L’Oréal has repositioned itself around longevity and healthy aging, treating later life as aspiration rather than decline.&lt;/p&gt;
    &lt;p&gt;The silver economy is not a niche. It is one of the largest and least contested growth opportunities of the next decade — and one that many firms still underestimate.&lt;/p&gt;
    &lt;p&gt;Third, longer working lives are inevitable. In Europe and the UK, effective retirement ages have been climbing, driven in part by financial need and policy changes. Meanwhile, in the U.S., the shift from defined-benefit to defined-contribution retirement plans incentivizes workers to remain employed longer. Organizations that fail to retain experienced talent will face labor shortages, while competitors benefit from workers who bring judgment, stability and institutional memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Role of Investors, C-Suites and Boards&lt;/head&gt;
    &lt;p&gt;The mismatch between demographic reality and corporate behavior is beginning to register with long-term investors. Large asset managers increasingly frame longevity as a structural economic force with implications for growth, productivity and risk.&lt;/p&gt;
    &lt;p&gt;A Vanguard study, The Economics of a Graying World, highlights aging and slower labor-force growth as a persistent drag on economic expansion, arguing that longer working lives are one of the few viable adjustment mechanisms. From this perspective, workforce age policy becomes financially material, not optional.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When organizations push experienced workers out early, they forfeit peak judgment, execution capability and mentoring capacity.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Economist Andrew J. Scott of the London Business School argues in his 2024 book The Longevity Imperative that if societies see longevity primarily as an “aging problem” of more pensioners, higher health costs and fewer workers, longer lives risk becoming a fiscal drag. But if they invest in health, skills and age‑inclusive work, longevity can instead raise growth, employment and innovation.&lt;/p&gt;
    &lt;p&gt;One hurdle to this shift in perspective is an ongoing lack of transparency and accountability by employers. Ageism in hiring, promotion and redundancy remains widespread, yet unlike gender or ethnicity, workforce age is rarely disclosed or scrutinized. The result is a growing governance gap. Misalignment with demographic reality creates execution risk — in talent, productivity and growth.&lt;/p&gt;
    &lt;p&gt;The case for a longevity strategy is ultimately an economic one. When organizations push experienced workers out early, they forfeit peak judgment, execution capability and mentoring capacity. When they underinvest in older consumers, they leave vast pools of demand underserved. Value is forfeited on both sides of the business.&lt;/p&gt;
    &lt;p&gt;In meeting their responsibility for long-term risk and growth, companies should begin with clarity. Map the age profile of the workforce by role and seniority. Identify where people in their fifties and early sixties are exiting — and whether those exits reflect performance or design. Treat age as a strategic variable in the same way firms now treat gender, skills or succession risk.&lt;/p&gt;
    &lt;p&gt;From there, redesign follows. Build roles and career paths that assume longer working lives. Invest in mid- and late-career reskilling, not as remediation but as renewal. Structure intergenerational teams deliberately, so experience and speed compound rather than collide. Align product, service and brand strategy with the realities of an aging, wealthier customer base.&lt;/p&gt;
    &lt;p&gt;None of this is about altruism. It is about reclaiming value currently being left on the table. As populations age, companies that learn to retain experience and serve longevity-driven demand will not just adapt — they will outperform.&lt;/p&gt;
    &lt;p&gt;Annie Coleman is Founder of RealiseLongevity, a consulting firm based in the UK, and is a Stanford Center on Longevity Ambassador.&lt;/p&gt;
    &lt;head rend="h3"&gt;KEEP READING&lt;/head&gt;
    &lt;p&gt;FIVE QUESTIONS&lt;lb/&gt; Bruce Feiler on Mastering Life Transitions at Any Age&lt;lb/&gt; ALT/SHIFT&lt;lb/&gt; How Much Health Data is Too Much?&lt;lb/&gt; FINANCING LONGER LIVES&lt;lb/&gt; Retirement Income Gap Sparks Innovation&lt;lb/&gt; DEEP DIVE&lt;lb/&gt; The Longevity Imperative: Why More Companies Are Recognizing the Benefits of Keeping Older Employees&lt;lb/&gt; GRANDPEOPLE&lt;lb/&gt; Soufflé Chef Takes her Final Bow at 90&lt;lb/&gt; FROM THE EDITOR&lt;lb/&gt; LONGEVITY LITERACY&lt;lb/&gt; Multiomics&lt;lb/&gt; GAME CHANGER&lt;lb/&gt; An Ultrasound Helmet to Cleanse the Brain&lt;lb/&gt; @SCL:&lt;lb/&gt; SCL at World Economic Forum&lt;lb/&gt; Call for Case Study Submissions&lt;lb/&gt; Longevity Design Challenge Finals&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46893411</guid><pubDate>Wed, 04 Feb 2026 23:26:23 +0000</pubDate></item><item><title>OpenClaw is what Apple intelligence should have been</title><link>https://www.jakequist.com/thoughts/openclaw-is-what-apple-intelligence-should-have-been</link><description>&lt;doc fingerprint="35ccf94d3962955c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenClaw is What Apple Intelligence Should Have Been&lt;/head&gt;
    &lt;p&gt;Something strange is happening with Mac Minis. They’re selling out everywhere, and it’s not because people suddenly need more coffee table computers.&lt;/p&gt;
    &lt;p&gt;If you browse Reddit or HN, you’ll see the same pattern: people are buying Mac Minis specifically to run AI agents with computer use. They’re setting up headless machines whose sole job is to automate their workflows. OpenClaw—the open-source framework that lets you run Claude, GPT-5, or whatever model you want to actually control your computer—has become the killer app for Mac hardware. Not Final Cut. Not Logic. An AI agent that clicks buttons.&lt;/p&gt;
    &lt;p&gt;This is exactly what Apple Intelligence should have been.&lt;/p&gt;
    &lt;p&gt;Apple had everything: the hardware, the ecosystem, the reputation for “it just works.” They could have shipped an agentic AI that actually automated your computer instead of summarizing your notifications. Imagine if Siri could genuinely file your taxes, respond to emails, or manage your calendar by actually using your apps, not through some brittle API layer that breaks every update.&lt;/p&gt;
    &lt;p&gt;They could have charged $500 more per device and people would have paid it. The margins would have been obscene. And they would have won the AI race not by building the best model, but by being the only company that could ship an AI you’d actually trust with root access to your computer. That trust—built over decades—was their moat.&lt;/p&gt;
    &lt;p&gt;So why didn’t they?&lt;/p&gt;
    &lt;p&gt;Maybe they just didn’t see it. That sounds mundane, but it’s probably the most common reason companies miss opportunities. When you’re Apple, you’re thinking about chip design, manufacturing scale, and retail strategy. An open-source project letting AI agents control computers might not ping your radar until it’s already happening.&lt;/p&gt;
    &lt;p&gt;Or maybe they saw it and decided the risk wasn’t worth it. If you’re Apple, you don’t want your AI agent automatically buying things, posting on social media, or making irreversible decisions. The liability exposure would be enormous. Better to ship something safe and limited than something powerful and unpredictable.&lt;/p&gt;
    &lt;p&gt;But there’s another dynamic at play. Look at who’s about to get angry about OpenClaw-style automation: LinkedIn, Facebook, anyone with a walled garden and a careful API strategy. These services depend on friction. They want you to use their app, see their ads, stay in their ecosystem. An AI that can automate away that friction is an existential threat.&lt;/p&gt;
    &lt;p&gt;If Apple had built this, they’d be fighting Instagram over ToS violations by Tuesday. They’d be testifying in front of Congress about AI agents committing fraud. Every tech platform would be updating their terms to explicitly ban Apple Intelligence.&lt;/p&gt;
    &lt;p&gt;By letting some third party do it, Apple gets plausible deniability. They’re just selling hardware. Not their fault what people run on it. It’s the same strategy that made them billions in the App Store while maintaining they’re “not responsible for what developers do.”&lt;/p&gt;
    &lt;p&gt;But I think this is short-term thinking.&lt;/p&gt;
    &lt;p&gt;Here’s what people miss about moats: they compound. The reason Microsoft dominated PCs wasn’t just that they had the best OS. It’s that everyone built for Windows, which made Windows more valuable, which made more people build for Windows. Network effects.&lt;/p&gt;
    &lt;p&gt;If Apple owned the agent layer, they could have created the most defensible moat in tech. Because an AI agent gets better the more it knows about you. And Apple already has all your data, all your apps, all your devices. They could have built an agent that works across your iPhone, Mac, iPad, and Watch seamlessly—something no one else can do.&lt;/p&gt;
    &lt;p&gt;More importantly, they could have owned the API. Want your service to work with Apple Agent? You play by Apple’s rules. Suddenly Apple isn’t fighting with platforms—they’re the platform that platforms need to integrate with. It’s the App Store playbook all over again, but for the AI era.&lt;/p&gt;
    &lt;p&gt;The Mac Mini rush is a preview of this future. People want agents. They want automation. They want to pay for it. They’re literally buying extra computers just to run someone else’s AI on Apple’s hardware.&lt;/p&gt;
    &lt;p&gt;Apple is getting the hardware revenue but missing the platform revenue. That might look smart this quarter. But platform revenue is what built Apple into a $3 trillion company. And platforms are what create trillion-dollar moats.&lt;/p&gt;
    &lt;p&gt;I suspect ten years from now, people will look back at 2024-2025 as the moment Apple had a clear shot at owning the agent layer and chose not to take it. Not because they couldn’t build it—they obviously could—but because they were optimizing for this year’s legal risk instead of next decade’s platform power.&lt;/p&gt;
    &lt;p&gt;The people buying Mac Minis to run AI agents aren’t just early adopters. They’re showing Apple exactly what product they should have built. Whether Apple is paying attention is another question entirely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46893970</guid><pubDate>Thu, 05 Feb 2026 00:28:06 +0000</pubDate></item><item><title>Wirth's Revenge</title><link>https://jmoiron.net/blog/wirths-revenge/</link><description>&lt;doc fingerprint="abcad51e692325f9"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Wirth's Revenge&lt;/head&gt;
    &lt;p&gt;In 1995, Turing laureate Niklaus Wirth wrote an essay called A Plea for Lean Software in which he mostly gripes about the state of software at the time. Among these gripes is this claim which Wirth attributes to his colleague Martin Reiser1, though it's become to be known as Wirth's Law:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software is getting slower more rapidly than hardware becomes faster.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Doing his best grandpa Simpson impersonation, Wirth complains:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;About 25 years ago, an interactive text editor could be designed with as little as 8,000 bytes ofstorage. (Modern program editors request 100 times that much!) An operating system had to manage with 8,000 bytes, and a compiler had to fit into 32 Kbytes, whereas their modern descendants require megabytes. Has all this inflated software become any faster? On the contrary. Were it not for a thousand times faster hardware, modern software would be utterly unusable.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Aside from the numbers involved here, which must sound utterly preposterous to the average modern reader, there's a lot to relate to. My 25 year career in software, all of which happened after 1995, was in many ways a story in two parts about Wirth's Law, an action and a reaction.&lt;/p&gt;
    &lt;p&gt;Personally, I disagree with Wirth's conclusion that nothing of value had been gained for the loss in efficiency. When he laments "the advent of windows, cut-and-paste strategies, and pop-up menus, [..] the replacement of meaningful command words by pretty icons", he is not properly appreciating the value these features had in making computing accessible to more people, and is focusing too much on their runtime cost. Programmers are often too quick to judge software on its technical merits rather than it's social ones.&lt;/p&gt;
    &lt;p&gt;Wirth passed away 2 years ago, but he was a giant in the field of Computer Science and a huge inspiration to me and to many of my other inspirations. In many ways, my focus on simplicity and my own system design sensibilities find their genesis with him.&lt;/p&gt;
    &lt;p&gt;Wirth's law is so self evidently true that it's been a topic of continuous investigation and rediscovery.&lt;/p&gt;
    &lt;p&gt;A notable example of this was Dan Luu's great post on input lag back in 2017. He felt that input latency was getting worse over time, so he got a high speed camera and measured the delay between pressing a key and the letter appearing on screen across a lot of different hardware. The lowest latency computer was the Apple 2e from 1983.&lt;/p&gt;
    &lt;p&gt;Input latency has gone up since 1983 because there is a lot more software involved in the pipeline for handling input. The kind of hardware interrupt based input handling the Apple 2e had is not flexible enough to meet modern requirements, so this additional complexity buys us a lot of value... but it's certainly not free, and if you're not careful, one of the costs is latency.&lt;/p&gt;
    &lt;p&gt;Luu goes on to write a lot about complexity and simplicity, and makes an interesting observation: the modern systems that fix the latency issue mostly do so not through removing complexity but by adding it. There was a lot of talk about complexity and simplicity at the time, because a huge number of software developers were working on another great tradeoff that had been made, this time in the datacenter: the widespread adoption of cloud computing.&lt;/p&gt;
    &lt;p&gt;In 1995, when Wirth wrote his essay, if you wanted to run a new internet company, you could just get a computer and run with it. Amazon didn't launch until July of that year, but it was famously started out of Jeff Bezos' garage.&lt;/p&gt;
    &lt;p&gt;The requirements for an internet company were simpler back then. The web wasn't some ubiquitous technology with total population penetration. There were only about 16 million people using it at the time; more people had an SNES than used the internet. Slashdot didn't exist. There was no real expectation of 5 9's 24/7/365 availability, and no opportunity to "go viral."&lt;/p&gt;
    &lt;p&gt;By 2010, this had changed. Sure, I guess you could still get Slashdotted then, but more relevantly, Twitter and Facebook could drive tons of traffic to you overnight. The number of internet users had ballooned to 2 billion.&lt;/p&gt;
    &lt;p&gt;To run your company's software, you could build your own datacenter, but this is a complicated task requiring a lot of expertise; you need land, permits, contractors, etc. I wouldn't even know where to start.&lt;/p&gt;
    &lt;p&gt;You could buy an existing datacenter, but you'd still need to manage power, backup generators/batteries, air conditioning, fire suppression, racks, maintenance, networking. Again, decades in the software industry, and I can barely build a competent list of requirements. It's a big investment, and there's a lot of opex.&lt;/p&gt;
    &lt;p&gt;You could rent a rack in a colo and focus on your compute needs, but those needs could change in an instant. If you plan out costs for 100,000 users and you never gain traction, you've overspent and are burning cash on pointless hardware you could be using to develop your product. If you get hit with a tidal wave of interest, you could be showing people the fail whale for years or miss your opportunity for success entirely.&lt;/p&gt;
    &lt;p&gt;Cloud computing was the era's answer. By 2010, Amazon was out of Jeff's basement and running its own massive datacenters for their online operations. As Steve Yegge wrote in 2010, Bezos had distributed an influential API mandate in 2002 requiring all internal teams to make their services available via an API. By 2006, they had already built a platform that they felt they could release to paying customers in the form of EC2 and S3. AWS let you rent capacity in Amazon's datacenter through a web interface or via direct API calls, billed on granular timescales.&lt;/p&gt;
    &lt;p&gt;Each step in this pipeline imparts additional cost, but they're all pretty valuable, especially the last step. There are even more steps in that pipeline today, with fully managed services like RDS and IAM which abstract the management of software and Lambda which even further abstract your hardware requirements and allow you to scale (and pay) purely on utilization.&lt;/p&gt;
    &lt;p&gt;Even though the cost to run software had gone up, the improved accessibility of cloud platforms and the reduction of risky capex led to an explosion in web software. All the while, hardware was racing ahead, making the rented capacity more powerful per unit cost and reducing the per-user cost.&lt;/p&gt;
    &lt;p&gt;Unfortunately, not every "Wirth tradeoff" is sound engineering.&lt;/p&gt;
    &lt;p&gt;Early in my career, I was working at a newspaper publisher owned by Condé Nast. I was the lead engineer on one of the company's more forward looking development projects, a Django application that managed local sports results data across dozens of regional newspapers. The application provided a single source of truth for very different users and use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reporters could add box scores and statistics on the go&lt;/item&gt;
      &lt;item&gt;Their websites could display results, league tables, etc.&lt;/item&gt;
      &lt;item&gt;The backend could publish feeds to syndicate into the print editions of each newspaper&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The print syndication system involved writing some pretty gnarly templates in order to generate feeds that could be understood by their newspaper printing systems. After a while, we noticed a problem. These templates were taking minutes to render and setting the database on fire every night.&lt;/p&gt;
    &lt;p&gt;If you've been around the block in software, you might already know the issue. The templates were using an ORM which dynamically loaded foreign key fields on attribute access, so innocent looking loops were doing one database query per iteration. These were complex templates with many nested loops: they were sending hundreds of thousands of database queries per render, many of them just the same query over and over again from a different loop in a different part of the template.&lt;/p&gt;
    &lt;p&gt;We could fix some of the terrible performance by pre-loading those fields with a join, but we had a lot of templates, and they were complex enough and the database large enough that this wasn't a realistic path to success. As Dan Luu concludes in his study on latency, the solution that worked required adding more complexity in the form of a smart caching layer.&lt;/p&gt;
    &lt;p&gt;Although we didn't know we were making it when we started the template project, this was a "bad" Wirth tradeoff. It still had utility: instead of having to manage what data might be needed in what template carefully, we could grab a list of top level objects and let the ORM fetch the rest of the data we needed on the fly.&lt;/p&gt;
    &lt;p&gt;The project started up quickly, but even at a pretty low scale of complexity, it became impossible to execute successfully. Before we realized what the problem was, we were using the convenience of these auto-loaded fields without understanding their true cost, and the software we built was a wasteful monstrosity as a result.&lt;/p&gt;
    &lt;p&gt;I see the same thing happening now but at broader scale with LLMs, and I feel myself sympathizing more and more with Wirth's cane shaking wrath.&lt;/p&gt;
    &lt;p&gt;Programming is the act of getting a computer to do something for you. Many people are discovering that for the first time, thanks to LLMs, they can ask a computer to do something for them, and it will actually go and do it. However, limiting yourself to programming only through this approach poses some problems.&lt;/p&gt;
    &lt;p&gt;While they might not be the unbound ecological disaster that many of their detractors claim they are, LLMs are still intensely computationally expensive. You can ask an AI what &lt;code&gt;2 * 3&lt;/code&gt; is and for the low price of several seconds of waiting, a few milliliters of water and enough power to watch 5% of a TikTok video on a television, it will tell you. But the computer you have in front of you can perform this calculation a billion times per second.&lt;/p&gt;
    &lt;p&gt;If the problem of my accidental database denial of service syndication feed was down to ignorance over the costs of ORM usage, it's pretty obvious that a similar kind of ignorance can lead to enormous unintended costs once we start integrating LLMs into our automation.&lt;/p&gt;
    &lt;p&gt;I've seen a few instances of this out in the wild that lead me to believe that this trap might be particularly tricky to avoid. Despite the capacity for LLMs to educate, or simulate education, or at least point you towards related materials some of which may be real, that's not how laypeople use them.&lt;/p&gt;
    &lt;p&gt;They present the LLM with a problem and ask it solve that problem.&lt;/p&gt;
    &lt;p&gt;One example of this is from myself, as this is how I used LLMs in my first go, too. I had a dump of recipes from a great but sadly unmaintained recipe site that I wanted to import into a self-hosted recipe management app.&lt;/p&gt;
    &lt;p&gt;I thought "Well, this sounds tedious, let me ask an LLM to do this." So I pointed a local LLM to the specification for the destination format, and asked it to convert the files. It converted one file every 10 minutes, inaccurately and without proper formatting. It was slow and it produced trash.&lt;/p&gt;
    &lt;p&gt;When you see engineers heap praise on programming agents, this isn't how they are using them. You don't ask the LLM to perform a repetitive and precise task, you ask it to build a script that performs that task. Except in rare cases, this script does not itself use LLMs.&lt;/p&gt;
    &lt;p&gt;Ironically, if you have the foresight to describe this problem to a major AI model and ask it how you should use an AI to solve it, this is exactly what it will tell you to do.&lt;/p&gt;
    &lt;p&gt;This approach subtly different from the way you might use LLMs for many other tasks, but it's crucial to getting results that reliably get a computer to do something for you. LLMs don't do reliable, they don't do repeatable. Building a program allows you to iterate on a deterministic solution with a stable source of truth, and you come away with an artifact that may or not be useless, but which actually works, and in my case converts 70 files/sec.&lt;/p&gt;
    &lt;p&gt;Another example I came across was this twitter thread by BenjaminDEKR, which I saw being ridiculed on bsky. He asked his personal agent to remind him to get milk, and this led the agent to repeatedly ask Opus if it was daytime yet. Along with the context from his heartbeat file, this resulted in a $0.75 charge for each heartbeat, costing him almost $20 during a single night's sleep.&lt;/p&gt;
    &lt;p&gt;What was the solution?&lt;/p&gt;
    &lt;p&gt;Maybe you decide that for your purposes 00:00 is night and 08:00 is day and use a basic local &lt;code&gt;gettimeofday&lt;/code&gt; call to determine which span you're in. Maybe you're unsatisfied with anything other than astronomical day/night and can generate a sunrise/sunset table for the year using NOAA's unmaintained solar calculator to dynamically produce your day/night spans?&lt;/p&gt;
    &lt;p&gt;You could do these things, but not if asking the LLM to solve problems is your problem solving approach. If asking an LLM is the only way you know how to solve problems, then you optimize the question asking by reducing heartbeat frequency and running on a cheaper model. Problem solved!&lt;/p&gt;
    &lt;p&gt;The overall concern is that having a magic box that gives you the answers ends up being a thought terminating solution to any problem.&lt;/p&gt;
    &lt;p&gt;When I wrote about the ecological impacts of AI, one of the non-ecological impacts I cited was the possibility that "AI erodes human skill." A recent release by research fellows at Anthropic, "How AI Impacts Skill Formation", suggests this fear isn't unfounded:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We find that AI use impairs conceptual understanding, code reading, and debugging abilities, without delivering significant efficiency gains on average.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A few weeks ago, I was at a family gathering watching some of the kids go through a huge backlog of red envelopes that their grandparents had saved for them over various missed holidays. They were pulling out all of the cash, at which point they were going to tally it all up and split it evenly.&lt;/p&gt;
    &lt;p&gt;When the adults challenged them to come up with better counting strategies, one of them suggested they could throw all of the money on the floor, take a picture, and ask ChatGPT to tally it all.&lt;/p&gt;
    &lt;p&gt;The instincts are for people to get the AI to do work for them, not to learn from the AI how to do the work themselves.&lt;/p&gt;
    &lt;p&gt;Wirth's law posits that software can erase gains faster than hardware can make them, but I'm afraid the reality is much worse than that.&lt;/p&gt;
    &lt;p&gt;If you've studied computer science, you might have heard of a function called Busy Beaver. The name is unfortunately silly, but it's a fairly important thought experiment in computability. &lt;code&gt;BB(N)&lt;/code&gt; is defined as the maximum number of steps a terminating turing machine with N states can run.&lt;/p&gt;
    &lt;p&gt;This function is known to be noncomputable, because any algorithm that could compute it would be able to solve the halting problem, which is known to be undecidable. &lt;code&gt;BB(1..3)&lt;/code&gt; were known in the 1960s to be 1, 6, and 21. In a pleasant bit of symmetry with Dan Luu's experiments, &lt;code&gt;BB(4)&lt;/code&gt; was discovered to be 107 in 1983, the same year his Apple 2e was built. In 2024, &lt;code&gt;BB(5)&lt;/code&gt; was proven to be 47,176,870.&lt;/p&gt;
    &lt;p&gt;As N grows, BB is known to eventually outgrow any computable sequence, including famous fast-growing sequences like TREE(). &lt;code&gt;BB(6)&lt;/code&gt; has a lower bound that is so large that it is impossible to explain how large it is to someone without a significant background in mathematics.&lt;/p&gt;
    &lt;p&gt;Software has an unprecedented capability to produce a correct answer in the most resource consuming way possible. Of course, producing incorrect answers or no answer at all is also an option.&lt;/p&gt;
    &lt;p&gt;Despite the apparent truth of Wirth's law, engineers have been actively battling against it for decades, but I worry that with LLMs we might have lost the war.&lt;/p&gt;
    &lt;p&gt;Am I just the latest in a long line of engineers who can't appreciate the newfound democratization of programming, or have we crossed into a "bad" Wirth tradeoff, where the growth curve of runtime complexity is something that hardware advancements cannot possibly dig us back out of?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you are wincing at the last name Reiser in the context of vaguely old computing, we are both of a very specific place and time, and I want you to know that Martin Reiser is not and has never been Hans Reiser.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46895381</guid><pubDate>Thu, 05 Feb 2026 03:38:26 +0000</pubDate></item><item><title>A few CPU hardware bugs</title><link>https://www.taricorp.net/2026/a-few-cpu-bugs/</link><description>&lt;doc fingerprint="ea86d90907ae236c"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;A few CPU hardware bugs&lt;/head&gt;
    &lt;p&gt;Catherine (Whitequark)’s recent observations on poorly-engineered firmware reminded me of a few mistakes I’ve seen in vendors’ CPUs; some unimportant and others surprisingly bad. Since I’ve never seen these widely discussed, here’s some discussion and links to supporting evidence to make them more widely known, since I think they’re interesting.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intel’s misspelled CPUIDs&lt;/head&gt;
    &lt;p&gt;I’m aware of two situations where Intel have sold CPUs that report misspelled names in some of the strings returned by the &lt;code&gt;CPUID&lt;/code&gt; instruction. This seems embarrassing for an organization of Intel’s size, but probably doesn’t hurt anybody’s ability to use the CPUs in question.&lt;/p&gt;
    &lt;head rend="h3"&gt;GenuineIotel&lt;/head&gt;
    &lt;p&gt;A web search for “GenuineIotel” reveals some discussions regarding this apparent typo, where some processors such as the Xeon E3-1231 v3 return the string “GenuineIotel” (instead of the usual “GenuineIntel”) for the CPU manufacturer ID. This one is well-known enough to be mentioned in the list of manufacturer IDs on Wikipedia.&lt;/p&gt;
    &lt;p&gt;It’s possible this misspelling is actually caused by some kind of random bit error, since the characters ’n’ and ‘o’ differ by only one bit; an unpredictable error that sets that bit could change &lt;code&gt;GenuineIntel&lt;/code&gt; to &lt;code&gt;GenuineIotel&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;ore i5&lt;/head&gt;
    &lt;p&gt;Another error that seems more likely to be human error in the CPU is in the Core i5-1245U CPU, which returns a processor brand string &lt;code&gt;Intel(R) ore(TM) i5-1245U&lt;/code&gt; which is simply missing the ‘C’ in &lt;code&gt;Core(TM) i5&lt;/code&gt;. Web searches for “Intel(R) ore(TM)” show a number of results which could be errors introduced by non-technical users attempting to copy down text from their screen when asking for tech support, but the Ubuntu certified configuration of the Dell Latitude 5430 with this CPU attests to this error actually being present in at least some machines using that CPU.&lt;/p&gt;
    &lt;p&gt;It’s possible this misspelling is not part of the physical CPU design and is instead part of the system firmware because at least on many AMD CPUs the CPU name is normally set by the system firmware. Probably either the CPU design or its microcode encode this misspelling, or Intel’s firmware package that vendors use is the ultimate source. In either case, it seems embarrassing for them that such an error made it out into machines purchased by members of the public because it seems very likely to be the result of human error.&lt;/p&gt;
    &lt;head rend="h2"&gt;ITE’s pipeline bug&lt;/head&gt;
    &lt;p&gt;This one is an actual hardware bug that I discovered at work, but in an embedded processor which most people will never see.&lt;/p&gt;
    &lt;p&gt;ITE Tech is a Taiwanese chip company that sells a variety of specialized ICs, including a selection of PC embedded controllers (which are used for tasks like making the keyboard work and managing battery charging in most laptops). IT81202 is one of those, with on-chip peripherals for communicating with an x86 processor and plenty of memory for private use by its RISC-V CPU.&lt;/p&gt;
    &lt;p&gt;It turns out there’s a pipeline bug in the IT81202 CPU, where instructions modifying some registers immediately following a multiply (&lt;code&gt;mul&lt;/code&gt; instruction) may have no effect. The workaround for this is to cripple the system, telling your compiler that the CPU doesn’t support multiplication or division instructions. Some of the performance can be regained by providing implementations of the library functions that provide integer multiply/divide operations that work in terms of the &lt;code&gt;mul&lt;/code&gt; and &lt;code&gt;div&lt;/code&gt; instructions, which works because inserting no-op instructions after them prevents the issue.&lt;/p&gt;
    &lt;p&gt;To me, this issue doesn’t seem as embarrassing as Intel’s wrong CPUIDs. Pipelined CPUs are hard to build, and at the time they designed the IT81202 CPU RISC-V wasn’t widely used in industry yet so they probably had a pretty immature core implementation. In addition, that’s an embedded processor which very few people will ever need to write software for so an invasive workaround like that isn’t a big deal. This one seems more like a cautionary tale to be aware of than any reason to mock the vendor!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46895388</guid><pubDate>Thu, 05 Feb 2026 03:39:43 +0000</pubDate></item><item><title>ICE seeks industry input on ad tech location data for investigative use</title><link>https://www.biometricupdate.com/202602/ice-seeks-industry-input-on-ad-tech-location-data-for-investigative-use</link><description>&lt;doc fingerprint="fd8ccb9e00a52fc0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ICE seeks industry input on ad tech location data for investigative use&lt;/head&gt;
    &lt;p&gt;Immigration and Customs Enforcement (ICE) is surveying the commercial advertising technology market for tools capable of supplying location data and large-scale analytics to federal investigators, according to a recent Request for Information (RFI).&lt;/p&gt;
    &lt;p&gt;Framed as market research rather than a procurement, the RFI seeks information from companies offering “Ad Tech compliant and location data services” that could support criminal, civil, and administrative investigations across ICE’s mission set.&lt;/p&gt;
    &lt;p&gt;The RFI, issued by ICE’s Homeland Security Investigations (HSI), emphasizes that the government is not soliciting proposals or committing to a future contract, but it does signal active interest in selecting vendors for live demonstrations of operational platforms and data services, a step that typically precedes pilot deployments or integration into existing investigative environments.&lt;/p&gt;
    &lt;p&gt;ICE says it is attempting to better understand how commercial big data providers and advertising technology firms might directly support investigative activities, while remaining sensitive to “regulatory constraints and privacy expectations.”&lt;/p&gt;
    &lt;p&gt;The agency noted that its components are handling increasing volumes of criminal, civil, and administrative information from both internal and external sources and are assessing whether commercial off-the-shelf platforms comparable to large investigative data and legal analytics providers can help manage and exploit that data at scale.&lt;/p&gt;
    &lt;p&gt;At the center of the inquiry is a category of information traditionally associated with digital advertising rather than law enforcement: location data, device identifiers, IP intelligence, and behavioral signals derived from everyday consumer activity.&lt;/p&gt;
    &lt;p&gt;Advertising technology, commonly referred to as ad tech, is the sprawling ecosystem of software, data brokers, analytics platforms, and intermediaries that power targeted advertising on the modern Internet.&lt;/p&gt;
    &lt;p&gt;Ad tech companies collect and process information about where devices are located, how users move between physical and digital spaces, which apps are installed on their phones, and how devices can be linked across websites, applications, and networks.&lt;/p&gt;
    &lt;p&gt;While the industry typically frames this activity as anonymous or pseudonymous, the underlying data is often persistent, granular, and capable of tracking individuals over time.&lt;/p&gt;
    &lt;p&gt;Location data is a particularly valuable component of that ecosystem. Mobile applications routinely share latitude and longitude coordinates with advertising partners through embedded software development kits.&lt;/p&gt;
    &lt;p&gt;Even when precise GPS data is not available, companies infer location through IP addresses, Wi-Fi networks, Bluetooth beacons, and cell tower connections. That information is then aggregated, analyzed, and sold to advertisers seeking to measure foot traffic, target audiences, or assess the effectiveness of campaigns.&lt;/p&gt;
    &lt;p&gt;ICE’s RFI suggests that the agency is exploring whether those same mechanisms can be repurposed as investigative tools.&lt;/p&gt;
    &lt;p&gt;The document asks vendors to describe platforms and data services that can support investigative needs while remaining “Ad Tech compliant,” a phrase that reflects industry norms rather than statutory law enforcement standards.&lt;/p&gt;
    &lt;p&gt;ICE appears to be looking into tapping into the commercial data ecosystem rather than building bespoke surveillance tools from scratch, a strategy that allows agencies to access rich data streams without directly collecting the information themselves.&lt;/p&gt;
    &lt;p&gt;ICE’s interest is not limited to raw data. The RFI repeatedly references “operational platforms,” signaling a desire for systems that can ingest, correlate, analyze, and visualize information from multiple sources.&lt;/p&gt;
    &lt;p&gt;In practice, that means software environments capable of fusing location data with other records, such as criminal histories, financial data, travel records, social media activity, or administrative files, to generate investigative leads or support ongoing cases.&lt;/p&gt;
    &lt;p&gt;The agency frames its inquiry as exploratory and cautious. It notes that the government is seeking to understand the “current state” of ad tech and location data services available to federal investigative entities, particularly considering regulatory constraints and privacy expectations.&lt;/p&gt;
    &lt;p&gt;That language reflects growing scrutiny of commercial data practices by courts, regulators, and civil liberties advocates, especially when such data is accessed by federal agencies like ICE.&lt;/p&gt;
    &lt;p&gt;In recent years, federal agencies have increasingly relied on commercially available data to sidestep traditional legal barriers.&lt;/p&gt;
    &lt;p&gt;Because ad tech data is collected by private companies under consumer-facing privacy policies, agencies have argued that purchasing or accessing that data does not constitute a search under the Fourth Amendment.&lt;/p&gt;
    &lt;p&gt;Critics counter that this approach allows the government to obtain highly sensitive information, including detailed location histories, without warrants, probable cause, or meaningful oversight.&lt;/p&gt;
    &lt;p&gt;The U.S. Supreme Court has signaled skepticism of such practices in cases recognizing the sensitivity of long-term location tracking, even when data is held by third parties.&lt;/p&gt;
    &lt;p&gt;At the same time, regulators have brought enforcement actions against data brokers accused of selling sensitive location information without adequate safeguards.&lt;/p&gt;
    &lt;p&gt;Against that backdrop, ICE’s assertion that it is considering privacy expectations appears designed to reassure both policymakers and potential vendors that the agency is aware of the controversy surrounding commercial surveillance data.&lt;/p&gt;
    &lt;p&gt;Yet the RFI itself provides little detail about how those concerns would be operationalized. It does not reference warrants, court orders, or judicial authorization.&lt;/p&gt;
    &lt;p&gt;Nor does it explain how ICE would distinguish between data associated with U.S. persons and noncitizens, how long information would be retained, or whether data obtained for one investigative purpose could be reused for others.&lt;/p&gt;
    &lt;p&gt;That ambiguity is particularly significant given HSI’s broad mandate. Unlike agencies focused solely on criminal enforcement, HSI conducts civil and administrative investigations alongside criminal cases.&lt;/p&gt;
    &lt;p&gt;Location data or ad tech-derived insights could therefore be used in contexts ranging from immigration enforcement to customs violations to sanctions and export control investigations, often under lower legal thresholds than those required in criminal proceedings.&lt;/p&gt;
    &lt;p&gt;ICE’s emphasis on “Ad Tech compliant” services also underscore a fundamental tension. Compliance in the advertising industry typically refers to adherence to self-regulatory frameworks, contractual obligations, and privacy policies that permit extensive data collection so long as certain disclosures are made.&lt;/p&gt;
    &lt;p&gt;Those standards are not designed to constrain government use, nor do they substitute for constitutional or statutory protections governing law enforcement surveillance.&lt;/p&gt;
    &lt;p&gt;Companies marketing “privacy-friendly” location or IP intelligence tools often argue that they avoid directly identifying individuals. But researchers and regulators have repeatedly demonstrated that supposedly anonymized or aggregated data can be reidentified when combined with other datasets.&lt;/p&gt;
    &lt;p&gt;In an investigative context, reidentification is not a bug but a feature, enabling analysts to link digital signals back to real-world subjects.&lt;/p&gt;
    &lt;p&gt;Biometric Update earlier reported that a Government Accountability Office audit had found that publicly accessible data – from social media posts to commercial geolocation records – can be aggregated into detailed “digital profiles” that expose U.S. personnel, military operations, and senior leaders to targeting, coercion, and disruption.&lt;/p&gt;
    &lt;p&gt;In January 2025, Gravy Analytics, a prominent location data broker, disclosed that a significant data breach had potentially exposed through de-anonymization the precise location information of millions of individuals.&lt;/p&gt;
    &lt;p&gt;The RFI’s focus on live demonstrations suggests that ICE is interested in mature, deployable capabilities rather than theoretical offerings. Vendors selected to present would be expected to show how their platforms operate in practice, how data is accessed and analyzed, and how investigative outputs are generated.&lt;/p&gt;
    &lt;p&gt;While the agency stresses that it is not committing to a future solicitation, such demonstrations often inform subsequent procurements, task orders, or pilot programs conducted under existing contracts.&lt;/p&gt;
    &lt;p&gt;ICE has used similar market research approaches in the past to normalize new surveillance capabilities before formal adoption.&lt;/p&gt;
    &lt;p&gt;Social media monitoring tools, mobile biometric systems, and large-scale analytics platforms were all introduced through incremental steps that began with RFIs and demonstrations rather than headline-grabbing contracts.&lt;/p&gt;
    &lt;p&gt;For privacy advocates, the latest filing fits a familiar pattern. Commercial surveillance markets evolve rapidly, driven by advertising and marketing demand. Government agencies then adopt those tools after the fact, often before lawmakers have fully grappled with the implications.&lt;/p&gt;
    &lt;p&gt;Oversight mechanisms, however, lag technical capability, leaving key questions unanswered until after systems are already in use.&lt;/p&gt;
    &lt;p&gt;ICE’s RFI does not indicate when demonstrations might occur or whether a solicitation will follow. It does make clear, though, that the agency sees the ad tech ecosystem as a potential investigative resource worth serious consideration.&lt;/p&gt;
    &lt;p&gt;As debates over commercial data, surveillance, and constitutional protections continue, the filing offers a window into how federal law enforcement is adapting to – and seeking to leverage – a data economy built for advertising rather than accountability.&lt;/p&gt;
    &lt;p&gt;For now, ICE is asking industry to explain how ad tech-derived location and analytics services can be made suitable for investigative use while respecting privacy expectations.&lt;/p&gt;
    &lt;p&gt;What remains unclear is who will define those expectations, how they will be enforced, and whether existing legal frameworks are equipped to govern a surveillance model that blurs the line between consumer marketing and government intelligence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Article Topics&lt;/head&gt;
    &lt;p&gt;data brokers | device fingerprinting | ICE | law enforcement | location data | RFI | surveillance | U.S. Government&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46895860</guid><pubDate>Thu, 05 Feb 2026 05:02:01 +0000</pubDate></item><item><title>Study: Older Cannabis Users Have Larger Brains, Better Cognition</title><link>https://news.cuanschutz.edu/news-stories/study-finds-cannabis-usage-in-middle-aged-and-older-adults-associated-with-larger-brain-volume-better-cognitive-function</link><description>&lt;doc fingerprint="3112913479a769fb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt; What did you and the team find on the impact cannabis has on brain function for older adults? &lt;/p&gt;
        &lt;p&gt;The big-picture, overall finding was that greater lifetime cannabis use among middle aged and older adults (a total of 26,362 participants ages 40 to 77, with an average age of 55) was generally associated with larger brain volumes and better cognitive function. &lt;/p&gt;
        &lt;p&gt;Specifically, we assessed brain regions that are associated with higher cannabinoid receptor – CB1 – density, which we thought would likely be impacted by cannabis use. We also looked at domains of cognitive function that have been associated with cannabis use in the past, including learning and memory, processing speed, attention and executive function.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; One of the phrases used in the paper was that cannabis was associated with increased regional brain volume. Can you walk us through what that is? &lt;/p&gt;
        &lt;p&gt;It refers to the fact that we were investigating specific parts of the brain and looking at their individual volumes, as opposed to overall or total brain volume. &lt;/p&gt;
        &lt;p&gt;Some studies will just say there was an impact of cannabis on overall gray matter. However, we wanted to take a more nuanced approach by looking at effects on specific brain regions, especially those with high CB1 receptor density, as well as on cognitive processes like memory, which is, of course, very relevant to aging.&lt;/p&gt;
        &lt;p&gt;For example, the hippocampus was one of the regions we looked at since it contains many CB1 receptors and plays an important role in memory, especially as we age, and is also implicated in dementia.&lt;/p&gt;
        &lt;p&gt;What does it mean to have bigger brain volumes? It's not that bigger is always better, but we also know that as we age, we often see smaller brain volumes due to processes like atrophy and neurodegeneration. That decrease is often correlated with reduced cognitive function and increased dementia risk. &lt;/p&gt;
        &lt;p&gt;In this study, we did see that most of the brain regions we looked at demonstrated a positive relationship between brain volume and cognitive performance. So in this sense, we could think of larger brain volumes in the context of aging as possibly reflecting maintained brain volume and preserved cognitive function, as opposed to say something like atrophy that we expect to occur with age.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Were there any sex differences? &lt;/p&gt;
        &lt;p&gt;We considered sex as an important factor, specifically to understand whether it moderated, or impacted, the effects of cannabis, for two reasons. &lt;/p&gt;
        &lt;p&gt;One, we know that men and women tend to use cannabis differently, so they have different patterns of use and they report different subjective effects. And two, from the preclinical research there's evidence that the endocannabinoid system is different in men and women, so the density of cannabinoid receptors differs and there seem to be complex interactions with hormones. &lt;/p&gt;
        &lt;p&gt;And while there wasn’t a clear-cut or consistent pattern, like male cannabis users always showing more favorable effects than women, we did see significant interactions across several brain regions and cognitive measures, which really suggests that sex is something we need to be looking at because it does seem to be an important factor.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; What were the results by cannabis usage category? &lt;/p&gt;
        &lt;p&gt;We did see that for many of our outcome measures, moderation seemed to be best. For the brain regions and cognitive tests that demonstrated an effect, the moderate-use group generally had larger brain volumes and better cognitive performance. At the same time, there were a few measures, like volume of the right amygdala and visual memory and learning, where the high use group had the best outcomes. &lt;/p&gt;
        &lt;p&gt;I think that really suggests that there are dose-dependent effects. &lt;/p&gt;
        &lt;p&gt;As a caveat, we didn’t have access to detailed information about the patterns of usage, so that would be helpful as additional contextual information. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Did you see any negative effects? &lt;/p&gt;
        &lt;p&gt;I will say one of the things that was really interesting was that although for almost everything we looked at, there was this positive relationship, there was a single brain region where we saw that higher cannabis use was actually associated with lower brain volume – the posterior cingulate, which is part of the limbic system and is implicated in processes like memory, learning, and emotion. That said, some research suggests smaller posterior cingulate volume is actually associated with better working memory, so it’s a little unclear what this means. It’s a good reminder that these effects involve multiple processes. It's not all good or all bad.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Why did you use the UK Biobank? &lt;/p&gt;
        &lt;p&gt;It’s a huge dataset, providing a great sample of older adults with a lot of awesome health measures important to aging, like neuroimaging and cognitive assessment. It also includes some information on cannabis use, which gave us a really good starting point for our investigation. &lt;/p&gt;
        &lt;p&gt;Biobanks are a great resource for a small lab like us, because we could never feasibly have a dataset that large. It’s an incredible tool to look at small to moderate effects. &lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; How did the UK Biobank measure cannabis usage, and how did you categorize the data? &lt;/p&gt;
        &lt;p&gt;In the UK Biobank, people were asked to estimate how many times they’d used cannabis over their lifetime, choosing from a set of ranges. We ended up grouping people into no use, moderate use, and high use, based on the number of times they'd used cannabis. And of course that's an imperfect way to group people, but it did allow us to approximate dose-dependent effects.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Was there anything that you and the team were surprised by, once you started parsing the data? &lt;/p&gt;
        &lt;p&gt;Even though preliminary work by our group – and some of our colleagues who have also been focused on older adults – had been seeing these beneficial relationships in older adults using cannabis, I was a little surprised that every cognitive measure that demonstrated a significant effect showed better performance among cannabis users. It goes against your default assumptions, because I think a lot of research out there has shown cannabis is associated with worse cognitive function, at least acutely.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; What do you want people to know about cannabis research? &lt;/p&gt;
        &lt;p&gt;I think the main takeaway is that the story is nuanced. It’s not a case of cannabis being all good or all bad. I think sometimes people have seen my poster on this project or they see the headline and they say, "Great, I'll just use more cannabis." But it’s more complicated than that. I think it depends on how people are using and what outcomes you’re looking at. What products are being used, for what reasons, and what part of the lifespan are we looking at? Those are important questions and we're still figuring it out.&lt;/p&gt;
        &lt;p&gt;For example, I’m interested in the effects of THC versus CBD. We don't have any of that information in the UK Biobank. Most people in this study were using cannabis quite a while ago, and cannabis at that time looked very different from what’s available today. That context really matters. It’s a complex picture.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Where does your research go next? &lt;/p&gt;
        &lt;p&gt;We do have another paper under review right now looking at connectivity, or function, of the brain in these same older adults. These data suggest there are also positive impacts of cannabis on the function of these brain regions, not just the size or volume of them, too. &lt;/p&gt;
        &lt;p&gt;Moving beyond cannabis, we’re also beginning to explore relationships between brain health and psilocybin use.&lt;/p&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt; Why is this research so important? &lt;/p&gt;
        &lt;p&gt;These substances are often marketed as health-promoting and beneficial to certain populations or for certain conditions, like chronic pain or depression, and we don't have solid clinical trials to support that or refute that, or how that might look different with aging. I think it's a very exciting time where we're still figuring out what's going on, and it's especially important for public health and policy. If people are using these substances, it's worth knowing what the impacts are, both good and bad.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46895950</guid><pubDate>Thu, 05 Feb 2026 05:18:42 +0000</pubDate></item><item><title>When internal hostnames are leaked to the clown</title><link>https://rachelbythebay.com/w/2026/02/03/badnas/</link><description>&lt;doc fingerprint="edd19151c5b24caf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Owning a $5M data center&lt;/head&gt;
    &lt;p&gt;These days it seems you need a trillion fake dollars, or lunch with politicians to get your own data center. They may help, but they’re not required. At comma we’ve been running our own data center for years. All of our model training, metrics, and data live in our own data center in our own office. Having your own data center is cool, and in this blog post I will describe how ours works, so you can be inspired to have your own data center too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why no cloud?&lt;/head&gt;
    &lt;p&gt;If your business relies on compute, and you run that compute in the cloud, you are putting a lot of trust in your cloud provider. Cloud companies generally make onboarding very easy, and offboarding very difficult. If you are not vigilant you will sleepwalk into a situation of high cloud costs and no way out. If you want to control your own destiny, you must run your own compute.&lt;/p&gt;
    &lt;p&gt;Self-reliance is great, but there are other benefits to running your own compute. It inspires good engineering. Maintaining a data center is much more about solving real-world challenges. The cloud requires expertise in company-specific APIs and billing systems. A data center requires knowledge of Watts, bits, and FLOPs. I know which one I rather think about.&lt;/p&gt;
    &lt;p&gt;Avoiding the cloud for ML also creates better incentives for engineers. Engineers generally want to improve things. In ML many problems go away by just using more compute. In the cloud that means improvements are just a budget increase away. This locks you into inefficient and expensive solutions. Instead, when all you have available is your current compute, the quickest improvements are usually speeding up your code, or fixing fundamental issues.&lt;/p&gt;
    &lt;p&gt;Finally there’s cost, owning a data center can be far cheaper than renting in the cloud. Especially if your compute or storage needs are fairly consistent, which tends to be true if you are in the business of training or running models. In comma’s case I estimate we’ve spent ~5M on our data center, and we would have spent 25M+ had we done the same things in the cloud.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s all needed?&lt;/head&gt;
    &lt;p&gt;Our data center is pretty simple. It’s maintained and built by only a couple engineers and technicians. Your needs may be slightly different, our implementation should provide useful context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power&lt;/head&gt;
    &lt;p&gt;To run servers you need power. We currently use about 450kW at max. Operating a data center exposes you to many fun engineering challenges, but procuring power is not one of them. San Diego power cost is over 40c/kWh, ~3x the global average. It’s a ripoff, and overpriced simply due to political dysfunction. We spent $540,112 on power in 2025, a big part of the data center cost. In a future blog post I hope I can tell you about how we produce our own power and you should too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cooling&lt;/head&gt;
    &lt;p&gt;Data centers need cool dry air. Typically this is achieved with a CRAC system, but they are power-hungry. San Diego has a mild climate and we opted for pure outside air cooling. This gives us less control of the temperature and humidity, but uses only a couple dozen kW. We have dual 48” intake fans and dual 48” exhaust fans to keep the air cool. To ensure low humidity (&amp;lt;45%) we use recirculating fans to mix hot exhaust air with the intake air. One server is connected to several sensors and runs a PID loop to control the fans to optimize the temperature and humidity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Servers&lt;/head&gt;
    &lt;p&gt;The majority of our current compute is 600 GPUs in 75 TinyBox Pro machines. They were built in-house, which saves us money and ensures they suit our needs. Our self-built machines fail at a similar rate to pre-built machines we’ve bought, but we’re capable of fixing them ourselves quickly. They have 2 CPUs and 8 GPUs each, and work as both training machines and general compute workers.&lt;/p&gt;
    &lt;p&gt;For data storage we have a few racks of Dell machines (R630 and R730). They are filled with SSDs for a total of ~4PB of storage. We use SSDs for reliability and speed. Our main storage arrays have no redundancy and each node needs to be able to saturate the network bandwidth with random access reads. For the storage machines this means reading up to 20Gbps of each 80TB chunk.&lt;/p&gt;
    &lt;p&gt;Other than storage and compute machines we have several one-off machines to run services. This includes a router, climate controller, data ingestion machine, storage master servers, metric servers, redis servers, and a few more.&lt;/p&gt;
    &lt;p&gt;Running the network requires switches, but at this scale we don’t need to bother with complicated switch topologies. We have 3 100Gbps interconnected Z9264F switches, which serve as the main ethernet network. We have two more infiniband switches to interconnect the 2 tinybox pro groups for training all-reduce.&lt;/p&gt;
    &lt;head rend="h3"&gt;The software&lt;/head&gt;
    &lt;p&gt;To effectively use all these compute and storage machines you need some infra. At this scale, services don’t need redundancy to achieve 99% uptime. We use a single master for all services, which makes things pretty simple.&lt;/p&gt;
    &lt;head rend="h5"&gt;Setup&lt;/head&gt;
    &lt;p&gt;All servers get ubuntu installed with pxeboot and are managed by salt.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed storage: minikeyvalue&lt;/head&gt;
    &lt;p&gt;All of our storage arrays use mkv. The main array is 3PB of non-redundant storage hosting our driving data we train on. We can read from this array at ~1TB/s, which means we can train directly on the raw data without caching. Redundancy is not needed since no specific data is critical.&lt;/p&gt;
    &lt;p&gt;We have an additional ~300TB non-redundant array to cache intermediate processed results. And lastly, we have a redundant mkv storage array to store all of our trained models and training metrics. Each of these 3 arrays have a separate single master server.&lt;/p&gt;
    &lt;head rend="h5"&gt;Workload management: slurm&lt;/head&gt;
    &lt;p&gt;We use slurm to manage the compute nodes, and compute jobs. We schedule two types of distributed compute. Pytorch training jobs, and miniray workers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed training: pytorch&lt;/head&gt;
    &lt;p&gt;To train models across multiple GPU nodes we use &lt;code&gt;torch.distributed&lt;/code&gt; FSDP. We have 2 separate training partitions, each intra-connected with Infiniband for training across machines. We wrote our own training framework which handles the training loop boilerplate, but it’s mostly just pytorch.&lt;/p&gt;
    &lt;p&gt;We have a custom model experiment tracking service (similar to wandb or tensorboard). It provides a dashboard for tracking experiments, and shows custom metrics and reports. It is also the interface for the mkv storage array that hosts the model weights. The training runs store the model weights there with a uuid, and they are available to download for whoever needs to run them. The metrics and reports for our latest models are also open.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed compute: miniray&lt;/head&gt;
    &lt;p&gt;Besides training we have many other compute tasks. This can be anything from running tests, running models, pre-processing data, or even running agent rollouts for on-policy training. We wrote a lightweight open-source task scheduler called miniray that allows you to run arbitrary python code on idle machines. This is a simpler version of dask, with a focus on extreme simplicity. Slurm will schedule any idle machine to be an active miniray worker, and accept pending tasks. All the task information is hosted in a central redis server.&lt;/p&gt;
    &lt;p&gt;Miniray workers with GPUs will spin up a triton inference server to run model inference with dynamic batching. A miniray worker can thus easily and efficiently run any of the models hosted in the model mkv storage array.&lt;/p&gt;
    &lt;p&gt;Miniray makes it extremely easy to scale parallel tasks to hundreds of machines. For example, the controls challenge record was set by just having ~1hr of access to our data center with miniray.&lt;/p&gt;
    &lt;head rend="h5"&gt;Code NFS monorepo&lt;/head&gt;
    &lt;p&gt;All our code is in a monorepo that we have cloned on our workstations. This monorepo is kept small (&amp;lt;3GB), so it can easily be copied around. When a training job or miniray distributed job is started on any workstation, the local monorepo is cached on a shared NFS drive including all the local changes. Training jobs and miniray tasks are pointed towards this cache, such that all distributed work uses the exact codebase you have locally. Even all the python packages are identical, UV on the worker/trainer syncs the packages specified in the monorepo before starting any work. This entire process of copying your entire local codebase and syncing all the packages takes only ~2s, and is well worth it to prevent the issues mismatches can cause.&lt;/p&gt;
    &lt;head rend="h2"&gt;All together now&lt;/head&gt;
    &lt;p&gt;The most complex thing we do at comma is train driving models on-policy, these training runs require training data to be generated during training by running simulated driving rollouts with the most recent model weights. Here’s a real-world command we just used to train such a model. This training run uses all of the infrastructure described above. While only this small command is needed to kick everything off, it orchestrates a lot of moving parts.&lt;/p&gt;
    &lt;code&gt;./training/train.sh N=4 partition=tbox2 trainer=mlsimdriving dataset=/home/batman/xx/datasets/lists/train_500k_20250717.txt vision_model=8d4e28c7-7078-4caf-ac7d-d0e41255c3d4/500 data.shuffle_size=125k optim.scheduler=COSINE bs=4
&lt;/code&gt;
    &lt;head rend="h2"&gt;Like this stuff?&lt;/head&gt;
    &lt;p&gt;Does all this stuff sound exciting? Then build your own datacenter for yourself or your company! You can also come work here.&lt;/p&gt;
    &lt;p&gt;Harald Schäfer&lt;lb/&gt; CTO @ comma.ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46895972</guid><pubDate>Thu, 05 Feb 2026 05:22:36 +0000</pubDate></item><item><title>Don't rent the cloud, own instead</title><link>https://blog.comma.ai/datacenter/</link><description>&lt;doc fingerprint="edd19151c5b24caf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Owning a $5M data center&lt;/head&gt;
    &lt;p&gt;These days it seems you need a trillion fake dollars, or lunch with politicians to get your own data center. They may help, but they’re not required. At comma we’ve been running our own data center for years. All of our model training, metrics, and data live in our own data center in our own office. Having your own data center is cool, and in this blog post I will describe how ours works, so you can be inspired to have your own data center too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why no cloud?&lt;/head&gt;
    &lt;p&gt;If your business relies on compute, and you run that compute in the cloud, you are putting a lot of trust in your cloud provider. Cloud companies generally make onboarding very easy, and offboarding very difficult. If you are not vigilant you will sleepwalk into a situation of high cloud costs and no way out. If you want to control your own destiny, you must run your own compute.&lt;/p&gt;
    &lt;p&gt;Self-reliance is great, but there are other benefits to running your own compute. It inspires good engineering. Maintaining a data center is much more about solving real-world challenges. The cloud requires expertise in company-specific APIs and billing systems. A data center requires knowledge of Watts, bits, and FLOPs. I know which one I rather think about.&lt;/p&gt;
    &lt;p&gt;Avoiding the cloud for ML also creates better incentives for engineers. Engineers generally want to improve things. In ML many problems go away by just using more compute. In the cloud that means improvements are just a budget increase away. This locks you into inefficient and expensive solutions. Instead, when all you have available is your current compute, the quickest improvements are usually speeding up your code, or fixing fundamental issues.&lt;/p&gt;
    &lt;p&gt;Finally there’s cost, owning a data center can be far cheaper than renting in the cloud. Especially if your compute or storage needs are fairly consistent, which tends to be true if you are in the business of training or running models. In comma’s case I estimate we’ve spent ~5M on our data center, and we would have spent 25M+ had we done the same things in the cloud.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s all needed?&lt;/head&gt;
    &lt;p&gt;Our data center is pretty simple. It’s maintained and built by only a couple engineers and technicians. Your needs may be slightly different, our implementation should provide useful context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power&lt;/head&gt;
    &lt;p&gt;To run servers you need power. We currently use about 450kW at max. Operating a data center exposes you to many fun engineering challenges, but procuring power is not one of them. San Diego power cost is over 40c/kWh, ~3x the global average. It’s a ripoff, and overpriced simply due to political dysfunction. We spent $540,112 on power in 2025, a big part of the data center cost. In a future blog post I hope I can tell you about how we produce our own power and you should too.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cooling&lt;/head&gt;
    &lt;p&gt;Data centers need cool dry air. Typically this is achieved with a CRAC system, but they are power-hungry. San Diego has a mild climate and we opted for pure outside air cooling. This gives us less control of the temperature and humidity, but uses only a couple dozen kW. We have dual 48” intake fans and dual 48” exhaust fans to keep the air cool. To ensure low humidity (&amp;lt;45%) we use recirculating fans to mix hot exhaust air with the intake air. One server is connected to several sensors and runs a PID loop to control the fans to optimize the temperature and humidity.&lt;/p&gt;
    &lt;head rend="h3"&gt;Servers&lt;/head&gt;
    &lt;p&gt;The majority of our current compute is 600 GPUs in 75 TinyBox Pro machines. They were built in-house, which saves us money and ensures they suit our needs. Our self-built machines fail at a similar rate to pre-built machines we’ve bought, but we’re capable of fixing them ourselves quickly. They have 2 CPUs and 8 GPUs each, and work as both training machines and general compute workers.&lt;/p&gt;
    &lt;p&gt;For data storage we have a few racks of Dell machines (R630 and R730). They are filled with SSDs for a total of ~4PB of storage. We use SSDs for reliability and speed. Our main storage arrays have no redundancy and each node needs to be able to saturate the network bandwidth with random access reads. For the storage machines this means reading up to 20Gbps of each 80TB chunk.&lt;/p&gt;
    &lt;p&gt;Other than storage and compute machines we have several one-off machines to run services. This includes a router, climate controller, data ingestion machine, storage master servers, metric servers, redis servers, and a few more.&lt;/p&gt;
    &lt;p&gt;Running the network requires switches, but at this scale we don’t need to bother with complicated switch topologies. We have 3 100Gbps interconnected Z9264F switches, which serve as the main ethernet network. We have two more infiniband switches to interconnect the 2 tinybox pro groups for training all-reduce.&lt;/p&gt;
    &lt;head rend="h3"&gt;The software&lt;/head&gt;
    &lt;p&gt;To effectively use all these compute and storage machines you need some infra. At this scale, services don’t need redundancy to achieve 99% uptime. We use a single master for all services, which makes things pretty simple.&lt;/p&gt;
    &lt;head rend="h5"&gt;Setup&lt;/head&gt;
    &lt;p&gt;All servers get ubuntu installed with pxeboot and are managed by salt.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed storage: minikeyvalue&lt;/head&gt;
    &lt;p&gt;All of our storage arrays use mkv. The main array is 3PB of non-redundant storage hosting our driving data we train on. We can read from this array at ~1TB/s, which means we can train directly on the raw data without caching. Redundancy is not needed since no specific data is critical.&lt;/p&gt;
    &lt;p&gt;We have an additional ~300TB non-redundant array to cache intermediate processed results. And lastly, we have a redundant mkv storage array to store all of our trained models and training metrics. Each of these 3 arrays have a separate single master server.&lt;/p&gt;
    &lt;head rend="h5"&gt;Workload management: slurm&lt;/head&gt;
    &lt;p&gt;We use slurm to manage the compute nodes, and compute jobs. We schedule two types of distributed compute. Pytorch training jobs, and miniray workers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed training: pytorch&lt;/head&gt;
    &lt;p&gt;To train models across multiple GPU nodes we use &lt;code&gt;torch.distributed&lt;/code&gt; FSDP. We have 2 separate training partitions, each intra-connected with Infiniband for training across machines. We wrote our own training framework which handles the training loop boilerplate, but it’s mostly just pytorch.&lt;/p&gt;
    &lt;p&gt;We have a custom model experiment tracking service (similar to wandb or tensorboard). It provides a dashboard for tracking experiments, and shows custom metrics and reports. It is also the interface for the mkv storage array that hosts the model weights. The training runs store the model weights there with a uuid, and they are available to download for whoever needs to run them. The metrics and reports for our latest models are also open.&lt;/p&gt;
    &lt;head rend="h5"&gt;Distributed compute: miniray&lt;/head&gt;
    &lt;p&gt;Besides training we have many other compute tasks. This can be anything from running tests, running models, pre-processing data, or even running agent rollouts for on-policy training. We wrote a lightweight open-source task scheduler called miniray that allows you to run arbitrary python code on idle machines. This is a simpler version of dask, with a focus on extreme simplicity. Slurm will schedule any idle machine to be an active miniray worker, and accept pending tasks. All the task information is hosted in a central redis server.&lt;/p&gt;
    &lt;p&gt;Miniray workers with GPUs will spin up a triton inference server to run model inference with dynamic batching. A miniray worker can thus easily and efficiently run any of the models hosted in the model mkv storage array.&lt;/p&gt;
    &lt;p&gt;Miniray makes it extremely easy to scale parallel tasks to hundreds of machines. For example, the controls challenge record was set by just having ~1hr of access to our data center with miniray.&lt;/p&gt;
    &lt;head rend="h5"&gt;Code NFS monorepo&lt;/head&gt;
    &lt;p&gt;All our code is in a monorepo that we have cloned on our workstations. This monorepo is kept small (&amp;lt;3GB), so it can easily be copied around. When a training job or miniray distributed job is started on any workstation, the local monorepo is cached on a shared NFS drive including all the local changes. Training jobs and miniray tasks are pointed towards this cache, such that all distributed work uses the exact codebase you have locally. Even all the python packages are identical, UV on the worker/trainer syncs the packages specified in the monorepo before starting any work. This entire process of copying your entire local codebase and syncing all the packages takes only ~2s, and is well worth it to prevent the issues mismatches can cause.&lt;/p&gt;
    &lt;head rend="h2"&gt;All together now&lt;/head&gt;
    &lt;p&gt;The most complex thing we do at comma is train driving models on-policy, these training runs require training data to be generated during training by running simulated driving rollouts with the most recent model weights. Here’s a real-world command we just used to train such a model. This training run uses all of the infrastructure described above. While only this small command is needed to kick everything off, it orchestrates a lot of moving parts.&lt;/p&gt;
    &lt;code&gt;./training/train.sh N=4 partition=tbox2 trainer=mlsimdriving dataset=/home/batman/xx/datasets/lists/train_500k_20250717.txt vision_model=8d4e28c7-7078-4caf-ac7d-d0e41255c3d4/500 data.shuffle_size=125k optim.scheduler=COSINE bs=4
&lt;/code&gt;
    &lt;head rend="h2"&gt;Like this stuff?&lt;/head&gt;
    &lt;p&gt;Does all this stuff sound exciting? Then build your own datacenter for yourself or your company! You can also come work here.&lt;/p&gt;
    &lt;p&gt;Harald Schäfer&lt;lb/&gt; CTO @ comma.ai&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46896146</guid><pubDate>Thu, 05 Feb 2026 05:50:01 +0000</pubDate></item><item><title>Modernizing Linux swapping: introducing the swap table</title><link>https://lwn.net/SubscriberLink/1056405/e728d95dd16f5e1b/</link><description>&lt;doc fingerprint="fe78505db1b50b80"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Modernizing swapping: introducing the swap table&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;How do you stay on top of kernel development?&lt;p&gt;LWN is the only outlet providing coverage of Linux kernel development from the inside. Beyond immediate access to all content, LWN subscribers get a number of benefits, including access to the LWN Kernel Source Database, and they provide the crucial support that keeps this unique coverage alive.&lt;/p&gt;&lt;p&gt;GIve LWN a try: get a one-month free trial subscription, no obligations, no tricks, no credit card required.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt; In a virtual-memory system, memory shortages must be addressed by reclaiming RAM and, if necessary, writing its contents to the appropriate persistent backing store. For file-backed memory, the file itself is that backing store. Anonymous memory — the memory that holds the variables and data structures used by a process — lacks that natural backing store, though. That is where the swap subsystem comes in: it provides a place to write anonymous pages when the memory they occupy is needed for other uses. Swapping allows unused (or seldom-used) pages to be pushed out to slower storage, making the system's RAM available for data that is currently in use. A full description of the kernel's swap subsystem would be lengthy indeed; there is a lot of complexity, much of which has built up over time. What follows is a partial, simplified overview of how the swap subsystem looked in the 6.17 kernel, which can then be used as a base for understanding the subsequent changes. The swap subsystem uses one or more swap files, which can be either partitions on a storage device or ordinary files within a filesystem. Inside the kernel, active swap files are described by struct swap_info_struct, but are usually referred to using a simple integer index instead. Each file is divided into page-sized slots; any given slot in the kernel's swap areas can be identified using the swp_entry_t type: This long value is divided into two fields: the upper six bits are the index number of the swap file (which, for extra clarity, is called the "type" in the swap code), and the rest is the slot number within the file. There is a set of simple functions used to create swap entries and get the relevant information back out. Note that the above describes the architecture-independent form of the swap entry; each architecture will also have an architecture-dependent version that is used in page-table entries. Curious readers can look at the x86_64 macros that convert between the two formats. Within the swap subsystem itself, though, the architecture-independent version of the swap entry is used. An overly simplified description of swapping would be something like: when the memory-management subsystem decides to reclaim an anonymous page, it selects a swap slot, writes the page's contents into that slot, then stores the associated swap entry in the page-table entry (using the architecture-dependent format) with the "present" bit cleared. The next attempt to reference that page will result in a page fault; the kernel will see the swap entry, allocate a new page, read the contents from the swap file, then update the page-table entry accordingly. The truth of the matter is that things are rather more complex than that. For example, writing a page to the swap file takes time, and the page itself cannot be reclaimed until the write is complete. So, when the reclaim decision is made, the page is put into the swap cache, which is, in many ways, the analog of the page cache used for file-backed pages. Saying that a page is in the swap cache really only means that a swap entry has been assigned; the page itself may or may not still be resident in RAM. If a fault happens on that page while the writing process is underway, that page can be quickly reactivated, despite being in the swap cache. All of this means that the swap subsystem has to keep track of the status of every page in the swap cache, and that status involves more than just the swap slot that was assigned. To that end, in kernels prior to 6.18, the swap subsystem maintained an array called swapper_spaces that contained pointers to arrays of address_space structures. That structure is used to maintain the mapping between an address space (the bytes of a file, or the slots of a swap file) and the storage that backs up that space. It provides a set of operations that can be used to move pages between RAM and that backing store. Using struct address_space means, among other things, that much of the code that works with the page cache can also operate with the swap cache. Another reason to use struct address_space is the XArray data structure associated with it. For a swap file, that data structure contains the current status of each slot in the file, which can be any of: For extra fun, there is not a single address_space structure and XArray for each swap file. Instead, the file is divided into 64MB chunks, and a separate address_space structure is created for each. This design helps to spread the management of swap entries across multiple XArrays, reducing contention and increasing scalability on larger systems where a lot of swapping is taking place. The swapper_spaces entry for a swap file, thus, points to an array of address_space structures; a 1GB swap file, for example, would be managed with an array of 16 of these structures. There is one more complication (for the purpose of this discussion — there are many others as well) in the management of swap slots. Each swap device is also divided into a set of swap clusters, represented by struct swap_cluster_info; these clusters are usually 2MB in size. Swap clusters make the management of swap files more scalable; each CPU in the system maintains a cache of swap clusters that have been assigned to it. The associated swap entries can then be managed entirely locally to the CPU, with cross-CPU access only needed when clusters must be allocated or freed. Swap clusters reduce the amount of scanning of the global swap map needed to work with swap entries, but the appropriate XArray must still be used to obtain or modify the status of a given slot. With that background in place, it is possible to look at the changes made for 6.18. They start with the understanding that the swap-subsystem code that deals with swap entries already has access to the swap clusters those entries belong to. Keeping the status information with the clusters would allow the elimination of the XArrays, which can be replaced with simple C arrays of swap entries. The smaller granularity of the swap clusters serves to further localize the management of swap entries, which should improve scalability. So the phase-1 patch set augments the swap_cluster_info structure; the post-6.17 version of that structure contains a new array pointer: The new table array, which is designed to occupy exactly one page on most architectures, is allocated dynamically, reducing the swap subsystem's memory use when the swap files are not full. Each entry in the table is the same swp_entry_t value seen above, describing the status of one page in the swap cache. The swap code has been reworked to use this new organization, with many of the internal APIs needing minimal or no changes. The arrays of address_space structures covering 64MB each are gone; the XArrays are no longer needed, and the address-space operations can be provided by a single structure, called swap_space. In summary, where the kernel previously divided swap areas using two independent clustering mechanisms (the address_space structures and the swap clusters), now it only has one clustering scheme that increases the locality of many swap operations. The end result, at this stage, is " That is the state of affairs as of 6.18. As significant as this change is, it is only the beginning of the project to simplify and improve the kernel's swap code. The 6.19 kernel did not significantly advance this work, but there are two other installments under consideration, one of which is seemingly poised for the 7.0 release. Those changes will be covered in the second part of this series.&lt;head&gt;Proceed to the article&lt;/head&gt; The kernel's swap subsystem is a complex and often unloved beast. It is also a critical component in the memory-management subsystem and has a significant impact on the performance of the system as a whole. At the 2025 Linux Storage, Filesystem, Memory-Management and BPF Summit, Kairui Song outlined a plan to simplify and optimize the kernel's swap code. A first installment of that work, written with help from Chris Li, was merged for the 6.18 release. This article will catch up with the 6.18 work, setting the stage for a future look at the changes that are yet to be merged. &lt;head&gt;A quick swap-subsystem primer&lt;/head&gt;&lt;quote&gt; typedef struct { unsigned long val; } swp_entry_t; &lt;/quote&gt;&lt;head&gt;The swap table&lt;/head&gt;&lt;quote&gt; atomic_long_t __rcu *table; &lt;/quote&gt;&lt;quote&gt;up to ~5-20% performance gain in throughput, RPS or build time for benchmark and workload tests&lt;/quote&gt;", according to Song. This speed improvement is entirely due to the removal of the XArray lookups and the reduction in contention that comes from managing swap space in smaller chunks. &lt;th&gt;Index entries for this article&lt;/th&gt;&lt;td&gt;Kernel&lt;/td&gt;&lt;td&gt;Memory management/Swapping&lt;/td&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46896586</guid><pubDate>Thu, 05 Feb 2026 06:58:43 +0000</pubDate></item><item><title>ICE Begins Buying 'Mega' Warehouse Detention Centers Across US</title><link>https://www.bloomberg.com/news/features/2026-01-29/us-spends-hundreds-of-millions-on-warehouses-for-ice-detention-centers</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46896823</guid><pubDate>Thu, 05 Feb 2026 07:35:51 +0000</pubDate></item><item><title>QuitGPT – OpenAI Execs Are Trump's Biggest Donors</title><link>https://quitgpt.org/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46897368</guid><pubDate>Thu, 05 Feb 2026 08:51:46 +0000</pubDate></item></channel></rss>