<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 06 Jan 2026 19:35:55 +0000</lastBuildDate><item><title>Show HN: ccrider - Search and Resume Your Claude Code Sessions – TUI / MCP / CLI</title><link>https://github.com/neilberkman/ccrider</link><description>&lt;doc fingerprint="350c8b78cb84afcf"&gt;
  &lt;main&gt;
    &lt;p&gt;Search, browse, and resume your Claude Code sessions. Fast.&lt;/p&gt;
    &lt;p&gt;You've got months of Claude Code sessions sitting in &lt;code&gt;~/.claude/projects/&lt;/code&gt;. Finding that conversation where you fixed the authentication bug? Good luck grepping through nested JSON files.&lt;/p&gt;
    &lt;p&gt;ccrider solves this with a TUI browser, CLI search, and an MCP server so Claude can search your past sessions too.&lt;/p&gt;
    &lt;code&gt;# Import your sessions once
ccrider sync

# Launch the TUI - browse, search, resume
ccrider tui

# Or search from command line
ccrider search "authentication bug"&lt;/code&gt;
    &lt;p&gt;Stay in your terminal. Find any conversation. Resume where you left off.&lt;/p&gt;
    &lt;p&gt;Installation:&lt;/p&gt;
    &lt;code&gt;# Homebrew (recommended)
brew install neilberkman/tap/ccrider

# Or from source
git clone https://github.com/neilberkman/ccrider.git
cd ccrider
go build -o ccrider cmd/ccrider/main.go
sudo mv ccrider /usr/local/bin/

# Install MCP server for all your projects (optional)
claude mcp add --scope user ccrider $(which ccrider) serve-mcp&lt;/code&gt;
    &lt;p&gt;"Vibe code like &lt;del&gt;a king&lt;/del&gt; The King!"&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;ccrider.mp4&lt;/head&gt;
    &lt;code&gt;ccrider tui&lt;/code&gt;
    &lt;p&gt;Browse your sessions with a polished terminal UI:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arrow keys to navigate&lt;/item&gt;
      &lt;item&gt;Enter to view full conversation&lt;/item&gt;
      &lt;item&gt;o to open session in new terminal tab (auto-detects Ghostty, iTerm, Terminal.app)&lt;/item&gt;
      &lt;item&gt;/ to search across all messages&lt;/item&gt;
      &lt;item&gt;p to toggle project filter (show only current directory)&lt;/item&gt;
      &lt;item&gt;? for help&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sessions matching your current directory are highlighted in light green - instantly see which sessions are relevant to your current work.&lt;/p&gt;
    &lt;code&gt;ccrider search "postgres migration"
ccrider search "error handling" --project ~/code/myapp
ccrider search "authentication" --after 2024-01-01&lt;/code&gt;
    &lt;p&gt;Powered by SQLite FTS5 - search message content, filter by project or date, get results instantly.&lt;/p&gt;
    &lt;p&gt;Press r in the TUI or use the CLI:&lt;/p&gt;
    &lt;code&gt;ccrider resume &amp;lt;session-id&amp;gt;&lt;/code&gt;
    &lt;p&gt;Launches &lt;code&gt;claude --resume&lt;/code&gt; in the right directory with the right session. Just works.&lt;/p&gt;
    &lt;code&gt;ccrider sync       # Import all new sessions
ccrider sync --full  # Re-import everything&lt;/code&gt;
    &lt;p&gt;Detects ongoing sessions and imports new messages without re-processing everything.&lt;/p&gt;
    &lt;p&gt;ccrider includes a built-in MCP (Model Context Protocol) server that gives Claude access to your session history.&lt;/p&gt;
    &lt;p&gt;Ask Claude to search your past conversations while working on new problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Find sessions where I worked on authentication"&lt;/item&gt;
      &lt;item&gt;"Show me my most recent Elixir sessions"&lt;/item&gt;
      &lt;item&gt;"What was I working on last week in the billing project?"&lt;/item&gt;
      &lt;item&gt;"Search my sessions for postgres migration issues"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude Code:&lt;/p&gt;
    &lt;code&gt;# Install for all your projects (recommended)
claude mcp add --scope user ccrider $(which ccrider) serve-mcp

# Or for current project only
claude mcp add ccrider $(which ccrider) serve-mcp&lt;/code&gt;
    &lt;p&gt;Claude Desktop:&lt;/p&gt;
    &lt;p&gt;Add to your config (&lt;code&gt;~/Library/Application Support/Claude/claude_desktop_config.json&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;{
  "mcpServers": {
    "ccrider": {
      "command": "ccrider",
      "args": ["serve-mcp"]
    }
  }
}&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;search_sessions - Full-text search across all session content with date/project filters&lt;/item&gt;
      &lt;item&gt;list_recent_sessions - Get recent sessions, optionally filtered by project&lt;/item&gt;
      &lt;item&gt;get_session_detail - Retrieve session info with first/last messages and optional search&lt;/item&gt;
      &lt;item&gt;get_session_messages - Get messages from a session (supports tail mode, context around search matches)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MCP server provides read-only access to your session database. Your conversations stay local.&lt;/p&gt;
    &lt;p&gt;ccrider looks for config in &lt;code&gt;~/.config/ccrider/&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# config.toml - pass additional flags to claude --resume
claude_flags = ["--dangerously-skip-permissions"]&lt;/code&gt;
    &lt;code&gt;# terminal_command.txt - custom command for 'o' key
# Available placeholders: {cwd}, {command}
wezterm cli spawn --cwd {cwd} -- {command}&lt;/code&gt;
    &lt;code&gt;# resume_prompt.txt - customize the prompt sent when resuming sessions&lt;/code&gt;
    &lt;p&gt;See CONFIGURATION.md for full details.&lt;/p&gt;
    &lt;p&gt;Built with strict core/interface separation following Saša Jurić's principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Core (&lt;code&gt;pkg/&lt;/code&gt;,&lt;code&gt;internal/core/&lt;/code&gt;): Pure business logic - parsing, database, search&lt;/item&gt;
      &lt;item&gt;Interface (&lt;code&gt;internal/interface/&lt;/code&gt;,&lt;code&gt;cmd/&lt;/code&gt;): Thin wrappers - CLI, TUI, MCP server&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Uses proven technologies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go for performance and single-binary distribution&lt;/item&gt;
      &lt;item&gt;SQLite with FTS5 for fast full-text search&lt;/item&gt;
      &lt;item&gt;Bubbletea for polished terminal UI&lt;/item&gt;
      &lt;item&gt;MCP for Claude integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other Claude Code session tools are broken:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Incomplete schema support (can't parse all message types)&lt;/item&gt;
      &lt;item&gt;Broken builds and abandoned dependencies&lt;/item&gt;
      &lt;item&gt;No real search (just grep)&lt;/item&gt;
      &lt;item&gt;Can't actually resume sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;ccrider fixes this with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ 100% schema coverage - parses all message types correctly&lt;/item&gt;
      &lt;item&gt;✅ SQLite FTS5 search - fast, powerful full-text search&lt;/item&gt;
      &lt;item&gt;✅ Single binary - no npm, no pip, no dependencies&lt;/item&gt;
      &lt;item&gt;✅ Native resume - one keystroke to resume sessions&lt;/item&gt;
      &lt;item&gt;✅ Incremental sync - detects new messages in ongoing sessions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See CONTRIBUTING.md for development setup and guidelines.&lt;/p&gt;
    &lt;code&gt;cmd/ccrider/          # CLI entry point + MCP server
internal/
  core/               # Business logic (no UI concerns)
    db/               # Database operations
    importer/         # Session import/sync
    search/           # Full-text search
    session/          # Session launch logic
  interface/          # Thin UI wrappers
    cli/              # Command handlers
    tui/              # Terminal UI (bubbletea)
pkg/ccsessions/       # Session file parser (public API)
&lt;/code&gt;
    &lt;code&gt;go build -o ccrider cmd/ccrider/main.go
./ccrider sync
./ccrider tui&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46512501</guid><pubDate>Tue, 06 Jan 2026 14:14:24 +0000</pubDate></item><item><title>Gemini Protocol Deployment Statistics</title><link>https://www.obsessivefacts.com/gemini-proxy?uri=gemini%3A%2F%2Fgemini.bortzmeyer.org%2Fsoftware%2Flupa%2Fstats.gmi</link><description>&lt;doc fingerprint="7309191894b681c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Statistics on the Gemini space&lt;/head&gt;
    &lt;p&gt;This page presents some statistics on the current state of the Gemini space. It has been updated on 2026-01-06 04:04:01Z.&lt;/p&gt;
    &lt;p&gt;It cannot claim to represent the entire space. The real number of URIs is certainly higher. There are several reasons why many URIs are not in the database:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the capsule may forbid retrieval, through robots.txt,&lt;/item&gt;
      &lt;item&gt;we do not know all the URIs and some cannot be found from the ones we know,&lt;/item&gt;
      &lt;item&gt;Lupa has a maximum number of URIs per capsule, to save resources (currently 10000).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On this page, "working" means there was a successful connection recently. "recently" means "less than 31 days". "Dead" URLs and capsules are removed after 46 days and no longer appear in any statistics.&lt;/p&gt;
    &lt;p&gt;Currently, our database includes 646,369 URIs, 560,646 of them having been checked successfully (status code 20) and recently. Among the recently accessed, 431,340 URIs serve a Gemini content.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;The average size of the resources is 46,339 bytes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Quantiles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10% of the resources are 306 bytes or less,&lt;/item&gt;
      &lt;item&gt;20% of the resources are 674 bytes or less,&lt;/item&gt;
      &lt;item&gt;30% of the resources are 1,098 bytes or less,&lt;/item&gt;
      &lt;item&gt;40% of the resources are 1,466 bytes or less,&lt;/item&gt;
      &lt;item&gt;50% of the resources are 2,318 bytes or less, MEDIAN&lt;/item&gt;
      &lt;item&gt;60% of the resources are 3,902 bytes or less,&lt;/item&gt;
      &lt;item&gt;70% of the resources are 6,768 bytes or less,&lt;/item&gt;
      &lt;item&gt;80% of the resources are 14,821 bytes or less,&lt;/item&gt;
      &lt;item&gt;90% of the resources are 63,909 bytes or less,&lt;/item&gt;
      &lt;item&gt;100% of the resources are 4,156,230 bytes or less.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;# Quantiles only for Gemini pages&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10% of the resources are 273 bytes or less,&lt;/item&gt;
      &lt;item&gt;20% of the resources are 524 bytes or less,&lt;/item&gt;
      &lt;item&gt;30% of the resources are 851 bytes or less,&lt;/item&gt;
      &lt;item&gt;40% of the resources are 1,136 bytes or less,&lt;/item&gt;
      &lt;item&gt;50% of the resources are 1,466 bytes or less, MEDIAN&lt;/item&gt;
      &lt;item&gt;60% of the resources are 2,279 bytes or less,&lt;/item&gt;
      &lt;item&gt;70% of the resources are 3,537 bytes or less,&lt;/item&gt;
      &lt;item&gt;80% of the resources are 5,781 bytes or less,&lt;/item&gt;
      &lt;item&gt;90% of the resources are 10,011 bytes or less,&lt;/item&gt;
      &lt;item&gt;100% of the resources are 4,156,230 bytes or less.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ranges&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Less than 10 bytes: 1380 URLs (0.25 %)&lt;/item&gt;
      &lt;item&gt;10 to 100 bytes: 12513 URLs (2.2 %)&lt;/item&gt;
      &lt;item&gt;100 to 1000 bytes: 141180 URLs (25.2 %)&lt;/item&gt;
      &lt;item&gt;1 to 10 kbytes: 271587 URLs (48.4 %)&lt;/item&gt;
      &lt;item&gt;10 to 100 kbytes: 89482 URLs (16.0 %)&lt;/item&gt;
      &lt;item&gt;100 to 1000 kbytes: 33186 URLs (5.9 %)&lt;/item&gt;
      &lt;item&gt;More than 1000 kbytes: 11318 URLs (2.02 %)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Most common media (MIME) types&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;text/gemini: 431,340 URLs&lt;/item&gt;
      &lt;item&gt;image/jpeg: 35,445 URLs&lt;/item&gt;
      &lt;item&gt;text/plain: 25,736 URLs&lt;/item&gt;
      &lt;item&gt;image/png: 23,934 URLs&lt;/item&gt;
      &lt;item&gt;application/octet-stream: 10,889 URLs&lt;/item&gt;
      &lt;item&gt;image/webp: 7,788 URLs&lt;/item&gt;
      &lt;item&gt;application/pdf: 5,370 URLs&lt;/item&gt;
      &lt;item&gt;image/gif: 2,696 URLs&lt;/item&gt;
      &lt;item&gt;application/zip: 2,563 URLs&lt;/item&gt;
      &lt;item&gt;octet/stream: 1,954 URLs&lt;/item&gt;
      &lt;item&gt;application/x-mscardfile: 1,928 URLs&lt;/item&gt;
      &lt;item&gt;text/html: 1,512 URLs&lt;/item&gt;
      &lt;item&gt;audio/mpeg: 1,041 URLs&lt;/item&gt;
      &lt;item&gt;text/x-diff: 864 URLs&lt;/item&gt;
      &lt;item&gt;text/markdown: 815 URLs&lt;/item&gt;
      &lt;item&gt;audio/ogg: 739 URLs&lt;/item&gt;
      &lt;item&gt;application/atom+xml: 639 URLs&lt;/item&gt;
      &lt;item&gt;application/json: 588 URLs&lt;/item&gt;
      &lt;item&gt;text/xml: 549 URLs&lt;/item&gt;
      &lt;item&gt;image/svg+xml: 343 URLs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Most common languages&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unspecified: 383,664 URLs&lt;/item&gt;
      &lt;item&gt;en: 121,355 URLs&lt;/item&gt;
      &lt;item&gt;de: 20,574 URLs&lt;/item&gt;
      &lt;item&gt;fr: 13,024 URLs&lt;/item&gt;
      &lt;item&gt;es: 9,242 URLs&lt;/item&gt;
      &lt;item&gt;it: 7,635 URLs&lt;/item&gt;
      &lt;item&gt;es_ar: 1,273 URLs&lt;/item&gt;
      &lt;item&gt;ru: 684 URLs&lt;/item&gt;
      &lt;item&gt;fa: 572 URLs&lt;/item&gt;
      &lt;item&gt;ja: 560 URLs&lt;/item&gt;
      &lt;item&gt;grc: 420 URLs&lt;/item&gt;
      &lt;item&gt;en_au: 252 URLs&lt;/item&gt;
      &lt;item&gt;arb: 202 URLs&lt;/item&gt;
      &lt;item&gt;en_us: 154 URLs&lt;/item&gt;
      &lt;item&gt;pl: 131 URLs&lt;/item&gt;
      &lt;item&gt;sv: 129 URLs&lt;/item&gt;
      &lt;item&gt;he: 93 URLs&lt;/item&gt;
      &lt;item&gt;gl: 62 URLs&lt;/item&gt;
      &lt;item&gt;ko: 56 URLs&lt;/item&gt;
      &lt;item&gt;fr,it: 47 URLs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Most common language tags&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unspecified: 383,613 URLs&lt;/item&gt;
      &lt;item&gt;en: 58,319 URLs&lt;/item&gt;
      &lt;item&gt;en-gb: 42,784 URLs&lt;/item&gt;
      &lt;item&gt;de: 20,526 URLs&lt;/item&gt;
      &lt;item&gt;en-us: 19,274 URLs&lt;/item&gt;
      &lt;item&gt;fr: 11,147 URLs&lt;/item&gt;
      &lt;item&gt;it: 7,635 URLs&lt;/item&gt;
      &lt;item&gt;es: 7,072 URLs&lt;/item&gt;
      &lt;item&gt;es-es: 2,069 URLs&lt;/item&gt;
      &lt;item&gt;fr-fr: 1,877 URLs&lt;/item&gt;
      &lt;item&gt;es_ar: 1,273 URLs&lt;/item&gt;
      &lt;item&gt;fa: 572 URLs&lt;/item&gt;
      &lt;item&gt;ja: 560 URLs&lt;/item&gt;
      &lt;item&gt;en-ie: 478 URLs&lt;/item&gt;
      &lt;item&gt;grc: 420 URLs&lt;/item&gt;
      &lt;item&gt;en-ca: 409 URLs&lt;/item&gt;
      &lt;item&gt;ru-ru: 356 URLs&lt;/item&gt;
      &lt;item&gt;ru: 328 URLs&lt;/item&gt;
      &lt;item&gt;en_au: 252 URLs&lt;/item&gt;
      &lt;item&gt;arb: 202 URLs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Most common encodings ("charsets") for all files&lt;/head&gt;
    &lt;p&gt;(Remember there exists testing capsules, with very exotic encodings, so don't be surprised by some strange ones.)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unspecified: 488,732 URLs&lt;/item&gt;
      &lt;item&gt;utf-8: 71,650 URLs&lt;/item&gt;
      &lt;item&gt;binary: 161 URLs&lt;/item&gt;
      &lt;item&gt;us-ascii: 75 URLs&lt;/item&gt;
      &lt;item&gt;gzip: 21 URLs&lt;/item&gt;
      &lt;item&gt;iso-8859-1: 6 URLs&lt;/item&gt;
      &lt;item&gt;xz: 1 URLs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Most common encodings for gemtext files only&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unspecified: 370,176 URLs&lt;/item&gt;
      &lt;item&gt;utf-8: 61,157 URLs&lt;/item&gt;
      &lt;item&gt;iso-8859-1: 6 URLs&lt;/item&gt;
      &lt;item&gt;us-ascii: 1 URLs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By the way, 457 of recently tested URLs (0.075 %) have a wrong encoding (it does not match the actual content).&lt;/p&gt;
    &lt;head rend="h3"&gt;Status codes&lt;/head&gt;
    &lt;p&gt;(Remember there are test capsules with funny status codes, to exercice Gemini clients.)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;20 (Success): 560,646 occurrences (94.98 %)&lt;/item&gt;
      &lt;item&gt;51 (Not found): 17,919 occurrences (3.04 %)&lt;/item&gt;
      &lt;item&gt;10 (Input request): 3,356 occurrences (0.57 %)&lt;/item&gt;
      &lt;item&gt;30 (Temporary redirect): 3,146 occurrences (0.53 %)&lt;/item&gt;
      &lt;item&gt;60 (Client certificate request): 2,894 occurrences (0.49 %)&lt;/item&gt;
      &lt;item&gt;40 (Temporary failure): 772 occurrences (0.13 %)&lt;/item&gt;
      &lt;item&gt;53 (Proxy request refused): 547 occurrences (0.09 %)&lt;/item&gt;
      &lt;item&gt;42 (CGI error): 266 occurrences (0.05 %)&lt;/item&gt;
      &lt;item&gt;50 (Permanent failure): 239 occurrences (0.04 %)&lt;/item&gt;
      &lt;item&gt;31 (Permanent redirect): 171 occurrences (0.03 %)&lt;/item&gt;
      &lt;item&gt;52 (Gone with the wind): 138 occurrences (0.02 %)&lt;/item&gt;
      &lt;item&gt;59 (Bad request): 88 occurrences (0.01 %)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Links&lt;/head&gt;
    &lt;p&gt;(We count only backlinks from external capsules, and at most one link per capsule. Also, we exclude links from capsules like search engines or directories.)&lt;/p&gt;
    &lt;p&gt;Maximum number of incoming links: 381&lt;/p&gt;
    &lt;p&gt;Average number of incoming links: 0.28&lt;/p&gt;
    &lt;head rend="h2"&gt;Capsules&lt;/head&gt;
    &lt;p&gt;There are 4825 capsules. We successfully connected recently to 3251 of them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Most common capsules by number of working URLs&lt;/head&gt;
    &lt;p&gt;We have a limit of 10000 URLs per capsule.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;gemini.kpopify.me: 10000 URLs&lt;/item&gt;
      &lt;item&gt;x5dragonfire.flounder.online: 10000 URLs&lt;/item&gt;
      &lt;item&gt;blitter.com: 10000 URLs&lt;/item&gt;
      &lt;item&gt;dvd.flounder.online: 10000 URLs&lt;/item&gt;
      &lt;item&gt;gemini.conman.org: 10000 URLs&lt;/item&gt;
      &lt;item&gt;gemini.oxydable.fr: 10000 URLs&lt;/item&gt;
      &lt;item&gt;gemini.techrights.org: 10000 URLs&lt;/item&gt;
      &lt;item&gt;gemini.tuxmachines.org: 9999 URLs&lt;/item&gt;
      &lt;item&gt;federal.cx: 9998 URLs&lt;/item&gt;
      &lt;item&gt;fumble-around.mediocregopher.com: 9994 URLs&lt;/item&gt;
      &lt;item&gt;musicbrainz.uploadedlobster.com: 9994 URLs&lt;/item&gt;
      &lt;item&gt;gmi.noulin.net: 9990 URLs&lt;/item&gt;
      &lt;item&gt;gemini.omarpolo.com: 9990 URLs&lt;/item&gt;
      &lt;item&gt;caiofior.pollux.casa: 9990 URLs&lt;/item&gt;
      &lt;item&gt;station.martinrue.com: 9970 URLs&lt;/item&gt;
      &lt;item&gt;taz.de: 9967 URLs&lt;/item&gt;
      &lt;item&gt;techrights.org: 9965 URLs&lt;/item&gt;
      &lt;item&gt;library.inu.red: 9952 URLs&lt;/item&gt;
      &lt;item&gt;bbs.geminispace.org: 9932 URLs&lt;/item&gt;
      &lt;item&gt;gemlog.stargrave.org: 9914 URLs&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Most common capsules by number of bytes in working URLs&lt;/head&gt;
    &lt;p&gt;We have a limit of bytes per URL.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mirrors.apple2.org.za: 2453.8 megabytes&lt;/item&gt;
      &lt;item&gt;nytpu.com: 2079.5 megabytes&lt;/item&gt;
      &lt;item&gt;higeki.jp: 1503.7 megabytes&lt;/item&gt;
      &lt;item&gt;gem.librehacker.com: 1040.6 megabytes&lt;/item&gt;
      &lt;item&gt;librehacker.com: 954.6 megabytes&lt;/item&gt;
      &lt;item&gt;uscoffings.net: 927.2 megabytes&lt;/item&gt;
      &lt;item&gt;gael.mooo.com: 754.6 megabytes&lt;/item&gt;
      &lt;item&gt;blitter.com: 687.1 megabytes&lt;/item&gt;
      &lt;item&gt;dfdn.info: 596.0 megabytes&lt;/item&gt;
      &lt;item&gt;going-flying.com: 448.6 megabytes&lt;/item&gt;
      &lt;item&gt;gemini.tcrouzet.com: 415.5 megabytes&lt;/item&gt;
      &lt;item&gt;tweek.zyxxyz.eu: 379.0 megabytes&lt;/item&gt;
      &lt;item&gt;norayr.am: 377.8 megabytes&lt;/item&gt;
      &lt;item&gt;villastraylight.online: 371.6 megabytes&lt;/item&gt;
      &lt;item&gt;techrights.org: 339.9 megabytes&lt;/item&gt;
      &lt;item&gt;ecs.d2evs.net: 320.9 megabytes&lt;/item&gt;
      &lt;item&gt;gemini.kpopify.me: 310.6 megabytes&lt;/item&gt;
      &lt;item&gt;library.inu.red: 281.2 megabytes&lt;/item&gt;
      &lt;item&gt;8by3.net: 260.2 megabytes&lt;/item&gt;
      &lt;item&gt;gemi.dev: 250.5 megabytes&lt;/item&gt;
      &lt;item&gt;gem.ortie.org: 228.9 megabytes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All working capsules:&lt;/p&gt;
    &lt;head rend="h3"&gt;Certificates&lt;/head&gt;
    &lt;p&gt;3006 (92.5 %) capsules are self-signed, 5 (0.2 %) use the Certificate Authority Let's Encrypt, 240 (7.4 %) are signed by another CA (may be not a trusted one).&lt;/p&gt;
    &lt;p&gt;93 capsules (2.87 %) have an expired certificate.&lt;/p&gt;
    &lt;p&gt;Algorithms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ecdsa-with-SHA256: 2046 capsules&lt;/item&gt;
      &lt;item&gt;sha256WithRSAEncryption: 1096 capsules&lt;/item&gt;
      &lt;item&gt;ecdsa-with-SHA384: 79 capsules&lt;/item&gt;
      &lt;item&gt;ED25519: 25 capsules&lt;/item&gt;
      &lt;item&gt;ecdsa-with-SHA512: 3 capsules&lt;/item&gt;
      &lt;item&gt;sha512WithRSAEncryption: 2 capsules&lt;/item&gt;
      &lt;item&gt;sha384WithRSAEncryption: 1 capsules&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ECDSA: 2128 capsules&lt;/item&gt;
      &lt;item&gt;RSA: 1099 capsules&lt;/item&gt;
      &lt;item&gt;ED25519: 25 capsules&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key sizes for RSA:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2048: 815 capsules&lt;/item&gt;
      &lt;item&gt;4096: 279 capsules&lt;/item&gt;
      &lt;item&gt;3072: 3 capsules&lt;/item&gt;
      &lt;item&gt;1024: 2 capsules&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key sizes for ECDSA:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;256: 2068 capsules&lt;/item&gt;
      &lt;item&gt;384: 59 capsules&lt;/item&gt;
      &lt;item&gt;521: 1 capsules&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;TLS&lt;/head&gt;
    &lt;p&gt;99 % of the capsules use TLS 1.3, 1 % use TLS 1.2.&lt;/p&gt;
    &lt;head rend="h3"&gt;robots.txt&lt;/head&gt;
    &lt;p&gt;317 (10 %) the capsules have a robots.txt exclusion file.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ports&lt;/head&gt;
    &lt;p&gt;23 working capsules (0.7 %) use an alternative port&lt;/p&gt;
    &lt;head rend="h3"&gt;Addresses&lt;/head&gt;
    &lt;p&gt;1263 IP addresses used. 27 % are IPv6.&lt;/p&gt;
    &lt;head rend="h3"&gt;# Addresses with most virtual hosts&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;173.230.145.243: 1079 vhosts&lt;/item&gt;
      &lt;item&gt;72.65.51.74: 549 vhosts&lt;/item&gt;
      &lt;item&gt;213.219.38.200: 328 vhosts&lt;/item&gt;
      &lt;item&gt;46.23.81.157: 143 vhosts&lt;/item&gt;
      &lt;item&gt;2a03:6000:1813:1337::157: 123 vhosts&lt;/item&gt;
      &lt;item&gt;86.194.163.71: 46 vhosts&lt;/item&gt;
      &lt;item&gt;109.237.26.252: 34 vhosts&lt;/item&gt;
      &lt;item&gt;45.56.93.217: 21 vhosts&lt;/item&gt;
      &lt;item&gt;172.236.4.52: 18 vhosts&lt;/item&gt;
      &lt;item&gt;128.140.115.191: 16 vhosts&lt;/item&gt;
      &lt;item&gt;143.244.176.58: 10 vhosts&lt;/item&gt;
      &lt;item&gt;2604:a880:4:1d0::4e1:3000: 10 vhosts&lt;/item&gt;
      &lt;item&gt;2a01:4f8:c17:20f1::42: 9 vhosts&lt;/item&gt;
      &lt;item&gt;23.88.35.144: 9 vhosts&lt;/item&gt;
      &lt;item&gt;46.23.94.99: 9 vhosts&lt;/item&gt;
      &lt;item&gt;129.151.254.123: 9 vhosts&lt;/item&gt;
      &lt;item&gt;143.244.212.63: 8 vhosts&lt;/item&gt;
      &lt;item&gt;2001:8b0:1d97:eeca:3da5:94ce:b61e:ee69: 8 vhosts&lt;/item&gt;
      &lt;item&gt;81.187.234.86: 8 vhosts&lt;/item&gt;
      &lt;item&gt;2a03:6000:6f67:624::99: 8 vhosts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;TLDs&lt;/head&gt;
    &lt;p&gt;There are 277 TLDs in the capsule's names, and 2248 registered domains.&lt;/p&gt;
    &lt;head rend="h3"&gt;Most common TLDs&lt;/head&gt;
    &lt;head rend="h3"&gt;# By number of registered domains&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;com: 361 domains&lt;/item&gt;
      &lt;item&gt;net: 212 domains&lt;/item&gt;
      &lt;item&gt;org: 179 domains&lt;/item&gt;
      &lt;item&gt;xyz: 154 domains&lt;/item&gt;
      &lt;item&gt;space: 99 domains&lt;/item&gt;
      &lt;item&gt;site: 75 domains&lt;/item&gt;
      &lt;item&gt;de: 64 domains&lt;/item&gt;
      &lt;item&gt;dev: 63 domains&lt;/item&gt;
      &lt;item&gt;me: 56 domains&lt;/item&gt;
      &lt;item&gt;eu: 41 domains&lt;/item&gt;
      &lt;item&gt;uk: 37 domains&lt;/item&gt;
      &lt;item&gt;fr: 37 domains&lt;/item&gt;
      &lt;item&gt;info: 30 domains&lt;/item&gt;
      &lt;item&gt;io: 28 domains&lt;/item&gt;
      &lt;item&gt;club: 24 domains&lt;/item&gt;
      &lt;item&gt;onion: 20 domains&lt;/item&gt;
      &lt;item&gt;ru: 19 domains&lt;/item&gt;
      &lt;item&gt;online: 18 domains&lt;/item&gt;
      &lt;item&gt;ca: 18 domains&lt;/item&gt;
      &lt;item&gt;se: 18 domains&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;# By number of capsules&lt;/head&gt;
    &lt;p&gt;(There's a strong bias towards TLDs which have hosting services such as flounder.online, which has many capsules in subdomains. See before the TLDs per registered domains, which are probably more useful.)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;online: 1093 capsules&lt;/item&gt;
      &lt;item&gt;org: 788 capsules&lt;/item&gt;
      &lt;item&gt;com: 452 capsules&lt;/item&gt;
      &lt;item&gt;pub: 344 capsules&lt;/item&gt;
      &lt;item&gt;net: 255 capsules&lt;/item&gt;
      &lt;item&gt;xyz: 178 capsules&lt;/item&gt;
      &lt;item&gt;space: 123 capsules&lt;/item&gt;
      &lt;item&gt;site: 78 capsules&lt;/item&gt;
      &lt;item&gt;de: 73 capsules&lt;/item&gt;
      &lt;item&gt;dev: 72 capsules&lt;/item&gt;
      &lt;item&gt;me: 58 capsules&lt;/item&gt;
      &lt;item&gt;eu: 58 capsules&lt;/item&gt;
      &lt;item&gt;cc: 57 capsules&lt;/item&gt;
      &lt;item&gt;casa: 54 capsules&lt;/item&gt;
      &lt;item&gt;club: 53 capsules&lt;/item&gt;
      &lt;item&gt;fr: 41 capsules&lt;/item&gt;
      &lt;item&gt;uk: 41 capsules&lt;/item&gt;
      &lt;item&gt;info: 37 capsules&lt;/item&gt;
      &lt;item&gt;io: 33 capsules&lt;/item&gt;
      &lt;item&gt;town: 27 capsules&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Other statistics on the geminispace&lt;/head&gt;
    &lt;p&gt;At the search engine geminispace.info&lt;/p&gt;
    &lt;p&gt;By Nervuri (specially for certificates)&lt;/p&gt;
    &lt;head rend="h2"&gt;Contact&lt;/head&gt;
    &lt;p&gt;Maintained by Stéphane Bortzmeyer (email &amp;lt;stephane+gemini@bortzmeyer.org&amp;gt;). Comments and criticisms are welcome.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46512707</guid><pubDate>Tue, 06 Jan 2026 14:30:53 +0000</pubDate></item><item><title>High-performance header-only container library for C++23 on x86-64</title><link>https://github.com/kressler/fast-containers</link><description>&lt;doc fingerprint="6f8fe048c40538a9"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance header-only container library for C++23 on x86-64.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;B+Tree (&lt;code&gt;kressler::fast_containers::btree&lt;/code&gt;) - Cache-friendly B+tree with SIMD search and hugepage support&lt;/item&gt;
      &lt;item&gt;Dense Map (&lt;code&gt;kressler::fast_containers::dense_map&lt;/code&gt;) - Fixed-size sorted array used internally by btree nodes&lt;/item&gt;
      &lt;item&gt;Hugepage Allocators - Pooling allocators that reduce TLB misses and allocation overhead &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;HugePageAllocator&lt;/code&gt;- Single-size allocator for uniform allocations&lt;/item&gt;&lt;item&gt;&lt;code&gt;MultiSizeHugePageAllocator&lt;/code&gt;- Multi-size pooling for variable-sized allocations (e.g., Abseil btree)&lt;/item&gt;&lt;item&gt;&lt;code&gt;PolicyBasedHugePageAllocator&lt;/code&gt;- Advanced control with shared pools&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The B+tree implementation provides significant performance improvements over industry standards for large trees. For some workloads with large trees, we've observed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;vs Abseil B+tree: 2-5× faster across insert/find/erase operations&lt;/item&gt;
      &lt;item&gt;vs std::map: 2-5× faster across insert/find/erase operations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See benchmark results for detailed performance analysis.&lt;/p&gt;
    &lt;p&gt;Important qualifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Performance advantages are most significant for large tree sizes where TLB misses and allocation costs dominate&lt;/item&gt;
      &lt;item&gt;Benchmarks currently focus on 10M element trees; smaller tree sizes have not been comprehensively tested&lt;/item&gt;
      &lt;item&gt;Results are specific to the tested configurations (8-byte keys, 32-byte and 256-byte values)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key advantages over Abseil's btree:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hugepage allocator integration: 3-5× speedup from reduced TLB misses and pooled allocations&lt;/item&gt;
      &lt;item&gt;SIMD-accelerated search: 3-10% faster node searches using AVX2 instructions&lt;/item&gt;
      &lt;item&gt;Tunable node sizes: Optimize cache behavior for your specific key/value sizes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Work in progress This is a work in progress. I don't have plans for major changes to the B+tree currently, but am actively cleaning up the implementation.&lt;/p&gt;
    &lt;p&gt;Platforms This library is really only built and tested on Linux, on x86-64 CPUs with AVX2 support. In theory, it could be built for Windows, though that hasn't been tested. The SIMD implementations are x86-64 specific. Timing in the custom benchmarks is also x86-64 specific (via use of &lt;code&gt;rdtscp&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;History/Motivations This project started as an exploration of using AI agents for software development. Based on experience tuning systems using Abseil's B+tree, I was curious if performance could be improved through SIMD instructions, a customized allocator, and tunable node sizes. Claude proved surprisingly adept at helping implement this quickly, and the resulting B+tree showed compelling performance improvements, so I'm making it available here.&lt;/p&gt;
    &lt;p&gt;Prerequisites:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C++23 compiler (GCC 14+, Clang 19+)&lt;/item&gt;
      &lt;item&gt;CMake 3.30+&lt;/item&gt;
      &lt;item&gt;AVX2-capable CPU (Intel Haswell 2013+, AMD Excavator 2015+)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Include in your project:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Add as a git submodule:&lt;/p&gt;
        &lt;quote&gt;git submodule add https://github.com/kressler/fast-containers.git third_party/fast-containers&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Link in CMakeLists.txt:&lt;/p&gt;
        &lt;quote&gt;add_subdirectory(third_party/fast-containers) target_link_libraries(your_target PRIVATE fast_containers::fast_containers)&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Include headers:&lt;/p&gt;
        &lt;quote&gt;#include &amp;lt;fast_containers/btree.hpp&amp;gt; #include &amp;lt;fast_containers/hugepage_allocator.hpp&amp;gt;&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;#include &amp;lt;fast_containers/btree.hpp&amp;gt;
#include &amp;lt;cstdint&amp;gt;
#include &amp;lt;iostream&amp;gt;

int main() {
  // Create a btree mapping int64_t keys to int32_t values
  // Using defaults: auto-computed node sizes, Linear search
  using Tree = kressler::fast_containers::btree&amp;lt;int64_t, int32_t&amp;gt;;

  Tree tree;

  // Insert key-value pairs
  tree.insert(42, 100);
  tree.insert(17, 200);
  tree.insert(99, 300);

  // Find a value
  auto it = tree.find(42);
  if (it != tree.end()) {
    std::cout &amp;lt;&amp;lt; "Found: " &amp;lt;&amp;lt; it-&amp;gt;second &amp;lt;&amp;lt; std::endl;  // Prints: 100
  }

  // Iterate over all elements (sorted by key)
  for (const auto&amp;amp; [key, value] : tree) {
    std::cout &amp;lt;&amp;lt; key &amp;lt;&amp;lt; " -&amp;gt; " &amp;lt;&amp;lt; value &amp;lt;&amp;lt; std::endl;
  }

  // Erase an element
  tree.erase(17);

  // Check size
  std::cout &amp;lt;&amp;lt; "Size: " &amp;lt;&amp;lt; tree.size() &amp;lt;&amp;lt; std::endl;  // Prints: 2
}&lt;/code&gt;
    &lt;code&gt;#include &amp;lt;fast_containers/btree.hpp&amp;gt;
#include &amp;lt;fast_containers/hugepage_allocator.hpp&amp;gt;
#include &amp;lt;cstdint&amp;gt;
#include &amp;lt;cassert&amp;gt;

int main() {
  // Use the hugepage allocator for 3-5× performance improvement
  // Allocator type must match the btree's value_type (std::pair&amp;lt;Key, Value&amp;gt;)
  using Allocator = kressler::fast_containers::HugePageAllocator&amp;lt;
      std::pair&amp;lt;int64_t, int32_t&amp;gt;&amp;gt;;

  using Tree = kressler::fast_containers::btree&amp;lt;
    int64_t,                                 // Key type
    int32_t,                                 // Value type
    96,                                      // Leaf node size
    128,                                     // Internal node size
    std::less&amp;lt;int64_t&amp;gt;,                      // Comparator
    kressler::fast_containers::SearchMode::SIMD,  // SIMD search
    Allocator                                // Hugepage allocator
  &amp;gt;;

  // Tree will default-construct the allocator (256MB initial pool, 64MB growth)
  // The btree automatically creates separate pools for leaf and internal nodes
  Tree tree;

  // Insert 10 million elements - hugepages reduce TLB misses
  for (int64_t i = 0; i &amp;lt; 10'000'000; ++i) {
    tree.insert(i, i * 2);
  }

  // Find operations are much faster with hugepages
  auto it = tree.find(5'000'000);
  assert(it != tree.end() &amp;amp;&amp;amp; it-&amp;gt;second == 10'000'000);
}&lt;/code&gt;
    &lt;p&gt;For multiple trees or fine-grained control over pool sizes, use &lt;code&gt;PolicyBasedHugePageAllocator&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;fast_containers/btree.hpp&amp;gt;
#include &amp;lt;fast_containers/policy_based_hugepage_allocator.hpp&amp;gt;
#include &amp;lt;cstdint&amp;gt;

int main() {
  // Create separate pools for leaf and internal nodes with custom sizes
  auto leaf_pool = std::make_shared&amp;lt;kressler::fast_containers::HugePagePool&amp;gt;(
      512 * 1024 * 1024, true);  // 512MB for leaves
  auto internal_pool = std::make_shared&amp;lt;kressler::fast_containers::HugePagePool&amp;gt;(
      256 * 1024 * 1024, true);  // 256MB for internals

  // Create policy that routes types to appropriate pools
  kressler::fast_containers::TwoPoolPolicy policy{leaf_pool, internal_pool};

  // Create allocator with the policy
  using Allocator = kressler::fast_containers::PolicyBasedHugePageAllocator&amp;lt;
      std::pair&amp;lt;int64_t, int32_t&amp;gt;,
      kressler::fast_containers::TwoPoolPolicy&amp;gt;;

  Allocator alloc(policy);

  using Tree = kressler::fast_containers::btree&amp;lt;
    int64_t, int32_t, 96, 128, std::less&amp;lt;int64_t&amp;gt;,
    kressler::fast_containers::SearchMode::SIMD, Allocator&amp;gt;;

  // Multiple trees can share the same pools
  Tree tree1(alloc);
  Tree tree2(alloc);

  // Both trees share leaf_pool for leaves and internal_pool for internals
  tree1.insert(1, 100);
  tree2.insert(2, 200);
}&lt;/code&gt;
    &lt;p&gt;For containers that allocate variable-sized objects (like &lt;code&gt;absl::btree_map&lt;/code&gt;), use &lt;code&gt;MultiSizeHugePageAllocator&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;fast_containers/multi_size_hugepage_allocator.hpp&amp;gt;
#include &amp;lt;absl/container/btree_map.h&amp;gt;
#include &amp;lt;array&amp;gt;
#include &amp;lt;cstdint&amp;gt;

int main() {
  // absl::btree_map allocates different-sized nodes (leaf vs internal)
  // MultiSizeHugePageAllocator routes allocations to size-class-specific pools

  using ValueType = std::array&amp;lt;std::byte, 32&amp;gt;;
  using Allocator = kressler::fast_containers::MultiSizeHugePageAllocator&amp;lt;
      std::pair&amp;lt;const int64_t, ValueType&amp;gt;&amp;gt;;

  // Helper function creates allocator with default settings
  // - 64MB initial size per size class
  // - Hugepages enabled
  // - 64MB growth size per size class
  auto alloc = kressler::fast_containers::make_multi_size_hugepage_allocator&amp;lt;
      std::pair&amp;lt;const int64_t, ValueType&amp;gt;&amp;gt;();

  // Create absl::btree_map with hugepage allocator
  absl::btree_map&amp;lt;int64_t, ValueType, std::less&amp;lt;int64_t&amp;gt;, Allocator&amp;gt; tree(alloc);

  // Insert 1 million elements - multiple size classes created automatically
  for (int64_t i = 0; i &amp;lt; 1'000'000; ++i) {
    tree[i] = ValueType{};
  }

  // Find operations benefit from reduced TLB misses
  auto it = tree.find(500'000);
}&lt;/code&gt;
    &lt;p&gt;How it works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allocations are routed to size classes based on requested size&lt;/item&gt;
      &lt;item&gt;Size classes: 0-512B (64B alignment), 513-2048B (256B alignment), 2049+B (power-of-2)&lt;/item&gt;
      &lt;item&gt;Each size class maintains its own &lt;code&gt;HugePagePool&lt;/code&gt;with uniform-sized blocks&lt;/item&gt;
      &lt;item&gt;Pools created on-demand as different sizes are requested&lt;/item&gt;
      &lt;item&gt;Provides 2-3× performance improvement for &lt;code&gt;absl::btree_map&lt;/code&gt;over standard allocator&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When to use each allocator:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;HugePageAllocator&lt;/code&gt;: Simple, automatic separate pools per type (recommended for our btree)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MultiSizeHugePageAllocator&lt;/code&gt;: Variable-sized allocations (e.g.,&lt;code&gt;absl::btree_map&lt;/code&gt;, other STL containers with allocator support)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;PolicyBasedHugePageAllocator&lt;/code&gt;: Fine-grained control, shared pools across trees, custom pool sizes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;btree&lt;/code&gt; class provides an API similar to &lt;code&gt;std::map&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Insertion:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;std::pair&amp;lt;iterator, bool&amp;gt; insert(const Key&amp;amp; key, const Value&amp;amp; value)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;std::pair&amp;lt;iterator, bool&amp;gt; emplace(Args&amp;amp;&amp;amp;... args)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Value&amp;amp; operator[](const Key&amp;amp; key)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lookup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;iterator find(const Key&amp;amp; key)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;const_iterator find(const Key&amp;amp; key) const&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;iterator lower_bound(const Key&amp;amp; key)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;iterator upper_bound(const Key&amp;amp; key)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;std::pair&amp;lt;iterator, iterator&amp;gt; equal_range(const Key&amp;amp; key)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Removal:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;size_type erase(const Key&amp;amp; key)&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;iterator erase(iterator pos)&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Iteration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;iterator begin()&lt;/code&gt;/&lt;code&gt;const_iterator begin() const&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;iterator end()&lt;/code&gt;/&lt;code&gt;const_iterator end() const&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Range-based for loops supported&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Capacity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;size_type size() const&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;bool empty() const&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;void clear()&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;void swap(btree&amp;amp; other) noexcept&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;key_compare key_comp() const&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;value_compare value_comp() const&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;template &amp;lt;
  typename Key,
  typename Value,
  std::size_t LeafNodeSize = default_leaf_node_size&amp;lt;Key, Value&amp;gt;(),
  std::size_t InternalNodeSize = default_internal_node_size&amp;lt;Key&amp;gt;(),
  typename Compare = std::less&amp;lt;Key&amp;gt;,
  SearchMode SearchModeT = SearchMode::Linear,
  typename Allocator = std::allocator&amp;lt;std::pair&amp;lt;Key, Value&amp;gt;&amp;gt;
&amp;gt;
class btree;&lt;/code&gt;
    &lt;p&gt;Parameters:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Key&lt;/code&gt;,&lt;code&gt;Value&lt;/code&gt;: The key and value types&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;LeafNodeSize&lt;/code&gt;: Number of key-value pairs per leaf node&lt;list rend="ul"&gt;&lt;item&gt;Default: Auto-computed heuristic targeting ~2KB (32 cache lines)&lt;/item&gt;&lt;item&gt;Formula: &lt;code&gt;2048 / (sizeof(Key) + sizeof(Value))&lt;/code&gt;, rounded to multiple of 8, clamped to [8, 64]&lt;/item&gt;&lt;item&gt;Manual tuning: Larger values (64-96) for small values, smaller values (8-16) for large values&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;InternalNodeSize&lt;/code&gt;: Number of child pointers per internal node&lt;list rend="ul"&gt;&lt;item&gt;Default: Auto-computed heuristic targeting ~1KB (16 cache lines)&lt;/item&gt;&lt;item&gt;Formula: &lt;code&gt;1024 / (sizeof(Key) + sizeof(void*))&lt;/code&gt;, rounded to multiple of 8, clamped to [16, 64]&lt;/item&gt;&lt;item&gt;Generally leave at default (stores only 8-byte pointers)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Compare&lt;/code&gt;: Comparison function (must satisfy&lt;code&gt;ComparatorCompatible&amp;lt;Key, Compare&amp;gt;&lt;/code&gt;)&lt;list rend="ul"&gt;&lt;item&gt;Default: &lt;code&gt;std::less&amp;lt;Key&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Also supports &lt;code&gt;std::greater&amp;lt;Key&amp;gt;&lt;/code&gt;for descending order&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Default: &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SearchMode&lt;/code&gt;: How to search within a node&lt;list rend="ul"&gt;&lt;item&gt;Default: &lt;code&gt;SearchMode::Linear&lt;/code&gt;(scalar linear search)&lt;/item&gt;&lt;item&gt;&lt;code&gt;SearchMode::SIMD&lt;/code&gt;: AVX2-accelerated search (3-10% faster, requires AVX2 CPU and SIMD-compatible keys: int32_t, uint32_t, int64_t, uint64_t, float, double)&lt;/item&gt;&lt;item&gt;&lt;code&gt;SearchMode::Binary&lt;/code&gt;: Binary search&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Default: &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Allocator&lt;/code&gt;: Memory allocation strategy&lt;list rend="ul"&gt;&lt;item&gt;Default: &lt;code&gt;std::allocator&amp;lt;std::pair&amp;lt;Key, Value&amp;gt;&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Recommended for performance: &lt;code&gt;HugePageAllocator&amp;lt;std::pair&amp;lt;Key, Value&amp;gt;&amp;gt;&lt;/code&gt;for working sets &amp;gt;1GB (3-5× faster)&lt;list rend="ul"&gt;&lt;item&gt;Automatically creates separate pools for leaf and internal nodes via rebind&lt;/item&gt;&lt;item&gt;Default: 256MB initial pool, 64MB growth per pool&lt;/item&gt;&lt;item&gt;Requires hugepages configured: &lt;code&gt;sudo sysctl -w vm.nr_hugepages=&amp;lt;num_pages&amp;gt;&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Falls back to regular pages if unavailable&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;For variable-sized allocations: &lt;code&gt;MultiSizeHugePageAllocator&amp;lt;std::pair&amp;lt;Key, Value&amp;gt;&amp;gt;&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Routes allocations to size-class-specific pools&lt;/item&gt;&lt;item&gt;Use with &lt;code&gt;absl::btree_map&lt;/code&gt;or other containers that allocate different-sized objects&lt;/item&gt;&lt;item&gt;Provides 2-3× speedup for Abseil btree over standard allocator&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Advanced control: &lt;code&gt;PolicyBasedHugePageAllocator&amp;lt;std::pair&amp;lt;Key, Value&amp;gt;, TwoPoolPolicy&amp;gt;&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Fine-grained control over pool sizes&lt;/item&gt;&lt;item&gt;Share pools across multiple trees&lt;/item&gt;&lt;item&gt;Separate pools for leaf and internal nodes with custom sizes&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Default: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Benchmarks comparing against Abseil's &lt;code&gt;btree_map&lt;/code&gt; and &lt;code&gt;std::map&lt;/code&gt; are available in results/btree_benchmark_results.md.&lt;/p&gt;
    &lt;p&gt;Our btree with hugepages (&lt;code&gt;btree_8_32_96_128_simd_hp&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;INSERT P99.9: 1,023 ns&lt;/item&gt;
      &lt;item&gt;FIND P99.9: 864 ns&lt;/item&gt;
      &lt;item&gt;ERASE P99.9: 1,086 ns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our btree with standard allocator (&lt;code&gt;btree_8_32_96_128_simd&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;INSERT P99.9: 3,155 ns (3.1× slower than with hugepages)&lt;/item&gt;
      &lt;item&gt;FIND P99.9: 950 ns (1.1× slower than with hugepages)&lt;/item&gt;
      &lt;item&gt;ERASE P99.9: 1,323 ns (1.2× slower than with hugepages)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vs. Abseil btree with hugepages (&lt;code&gt;absl_8_32_hp&lt;/code&gt; using &lt;code&gt;MultiSizeHugePageAllocator&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;INSERT P99.9: 1,401 ns (27% slower)&lt;/item&gt;
      &lt;item&gt;FIND P99.9: 1,190 ns (38% slower)&lt;/item&gt;
      &lt;item&gt;ERASE P99.9: 1,299 ns (20% slower)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vs. Abseil btree with standard allocator (&lt;code&gt;absl_8_32&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;INSERT P99.9: 3,287 ns (3.2× slower)&lt;/item&gt;
      &lt;item&gt;FIND P99.9: 1,342 ns (55% slower)&lt;/item&gt;
      &lt;item&gt;ERASE P99.9: 1,679 ns (55% slower)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;vs. std::map (&lt;code&gt;map_8_32&lt;/code&gt;):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;INSERT P99.9: 3,587 ns (3.5× slower)&lt;/item&gt;
      &lt;item&gt;FIND P99.9: 2,312 ns (2.7× slower)&lt;/item&gt;
      &lt;item&gt;ERASE P99.9: 2,253 ns (2.1× slower)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hugepage allocators provide massive performance improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Our btree: 2-3× faster with hugepages vs. standard allocator&lt;/item&gt;
      &lt;item&gt;Abseil btree: 2× faster with &lt;code&gt;MultiSizeHugePageAllocator&lt;/code&gt;vs. standard allocator&lt;/item&gt;
      &lt;item&gt;Critical for large working sets (&amp;gt;1M elements)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our implementation maintains significant advantages even with fair comparison:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;20-67% faster than Abseil btree even when both use hugepage allocators&lt;/item&gt;
      &lt;item&gt;Advantages from SIMD search, tunable node sizes, and optimized bulk transfers&lt;/item&gt;
      &lt;item&gt;Performance gap widens with larger values (256 bytes: 21-135% faster)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance varies by tree size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Large trees (10M elements): Our btree dominates all metrics&lt;/item&gt;
      &lt;item&gt;Small trees (10K elements): Competition intensifies, std::map becomes viable for some workloads&lt;/item&gt;
      &lt;item&gt;See benchmark results for detailed analysis&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The hugepage allocator is the single most important optimization, providing benefits by reducing TLB misses (helps find operations) and making allocations extremely cheap through pooling (helps insert/erase operations).&lt;/p&gt;
    &lt;code&gt;# List available presets
cmake --list-presets

# Configure, build, and test in one workflow
cmake --preset release
cmake --build --preset release
ctest --preset release

# Common presets:
cmake --preset debug          # Debug build
cmake --preset release        # Release with AVX2 (default)
cmake --preset asan           # AddressSanitizer build
cmake --preset release-no-avx2  # Release without AVX2&lt;/code&gt;
    &lt;code&gt;# Clone with submodules
git clone --recursive https://github.com/kressler/fast-containers.git
cd fast-containers

# Configure
cmake -S . -B build -DCMAKE_BUILD_TYPE=Release

# Build
cmake --build build

# Run tests
ctest --test-dir build --output-on-failure&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENABLE_AVX2&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;ON&lt;/code&gt; (Release), &lt;code&gt;OFF&lt;/code&gt; (Debug)&lt;/cell&gt;
        &lt;cell&gt;Enable AVX2 SIMD optimizations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENABLE_ASAN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OFF&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable AddressSanitizer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENABLE_ALLOCATOR_STATS&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;OFF&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable allocator statistics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENABLE_LTO&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;ON&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable Link-Time Optimization&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;ENABLE_NUMA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Auto-detected&lt;/cell&gt;
        &lt;cell&gt;Enable NUMA support (requires libnuma)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Clone with submodules:&lt;/p&gt;
        &lt;code&gt;git clone --recursive https://github.com/kressler/fast-containers.git cd fast-containers&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One-time development setup:&lt;/p&gt;
        &lt;quote&gt;./setup-dev.sh&lt;/quote&gt;
        &lt;p&gt;This installs pre-commit hooks and configures clang-tidy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Automatic formatting and checks (via pre-commit hook):&lt;/p&gt;
    &lt;code&gt;git commit  # Automatically formats code and runs clang-tidy&lt;/code&gt;
    &lt;p&gt;The pre-commit hook will:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Format all staged C++ files with clang-format&lt;/item&gt;
      &lt;item&gt;Check production headers with clang-tidy&lt;/item&gt;
      &lt;item&gt;Fail the commit if warnings are found&lt;/item&gt;
      &lt;item&gt;Auto-create build directories if missing&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Manual formatting:&lt;/p&gt;
    &lt;code&gt;cmake --build build --target format&lt;/code&gt;
    &lt;p&gt;Manual static analysis:&lt;/p&gt;
    &lt;code&gt;cmake --build build --target clang-tidy
# Or manually:
clang-tidy-19 -p cmake-build-clang-tidy include/fast_containers/*.hpp&lt;/code&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;clang-format (for code formatting)&lt;/item&gt;
      &lt;item&gt;clang-tidy-19 (for static analysis)&lt;/item&gt;
      &lt;item&gt;cmake (to auto-create build directories)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bypass hook (when needed):&lt;/p&gt;
    &lt;code&gt;git commit --no-verify&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Make your changes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build and test:&lt;/p&gt;
        &lt;code&gt;cmake --build build &amp;amp;&amp;amp; ctest --test-dir build&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Commit (auto-formatted and checked):&lt;/p&gt;
        &lt;quote&gt;git add . git commit -m "Your changes" # Pre-commit hook runs automatically&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow Google C++ Style Guide (enforced by clang-format)&lt;/item&gt;
      &lt;item&gt;Use C++23 features&lt;/item&gt;
      &lt;item&gt;Write tests for new functionality using Catch2&lt;/item&gt;
      &lt;item&gt;Production code must be clang-tidy clean (enforced in CI and pre-commit)&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;cmake --build build --target format&lt;/code&gt;before submitting PRs&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;.
├── include/
│   └── fast_containers/         # Public header files
│       ├── btree.hpp, btree.ipp
│       ├── dense_map.hpp, dense_map.ipp
│       ├── hugepage_allocator.hpp
│       ├── multi_size_hugepage_allocator.hpp
│       ├── multi_size_hugepage_pool.hpp
│       ├── policy_based_hugepage_allocator.hpp
│       └── hugepage_pool.hpp
├── tests/                       # Unit tests (Catch2)
│   ├── test_btree.cpp
│   ├── test_dense_map.cpp
│   ├── test_hugepage_allocator.cpp
│   └── test_policy_based_allocator.cpp
├── src/
│   ├── benchmarks/              # Google Benchmark performance tests
│   │   ├── dense_map_search_benchmark.cpp
│   │   └── hugepage_allocator_benchmark.cpp
│   └── binary/                  # Standalone benchmark executables
│       ├── btree_benchmark.cpp
│       └── btree_stress.cpp
├── scripts/
│   └── interleaved_btree_benchmark.py  # A/B testing harness
├── results/
│   └── btree_benchmark_results.md      # Performance analysis
├── third_party/                 # Git submodules
│   ├── catch2/                  # Unit testing framework
│   ├── benchmark/               # Google Benchmark
│   ├── histograms/              # Latency histogram library
│   ├── abseil-cpp/              # Comparison baseline
│   ├── lyra/                    # Command-line parsing
│   └── unordered_dense/         # Dense hash map
├── hooks/                       # Git hooks (install with setup-dev.sh)
│   └── pre-commit               # Auto-format and clang-tidy
└── CMakeLists.txt               # Build configuration
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Language: C++23&lt;/item&gt;
      &lt;item&gt;Build System: CMake 3.30+&lt;/item&gt;
      &lt;item&gt;Testing: Catch2 v3.11.0&lt;/item&gt;
      &lt;item&gt;Code Formatting: clang-format (Google C++ Style)&lt;/item&gt;
      &lt;item&gt;Static Analysis: clang-tidy-19&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46512842</guid><pubDate>Tue, 06 Jan 2026 14:41:55 +0000</pubDate></item><item><title>65% of Hacker News posts have negative sentiment, and they outperform</title><link>https://philippdubach.com/standalone/hn-sentiment/</link><description>&lt;doc fingerprint="350c80263eb05556"&gt;
  &lt;main&gt;
    &lt;p&gt;Posts with negative sentiment average 35.6 points on Hacker News. The overall average is 28 points. That’s a 27% performance premium for negativity. This finding comes from an empirical study I’ve been running on HN attention dynamics, covering decay curves, preferential attachment, survival probability, and early-engagement prediction. The preprint is available on SSRN. I already had a gut feeling. Across 32,000 posts and 340,000 comments, nearly 65% register as negative. This might be a feature of my classifier being miscalibrated toward negativity; yet the pattern holds across six different models. I tested three transformer-based classifiers (DistilBERT, BERT Multi, RoBERTa) and three LLMs (Llama 3.1 8B, Mistral 3.1 24B, Gemma 3 12B). The distributions vary, but the negative skew persists across all of them (inverted scale for 2-6). The results I use in my dashboard are from DistilBERT because it runs efficiently in my Cloudflare-based pipeline.&lt;/p&gt;
    &lt;p&gt;What counts as “negative” here? Criticism of technology, skepticism toward announcements, complaints about industry practices, frustration with APIs. The usual. It’s worth noting that technical critique reads differently than personal attacks; most HN negativity is substantive rather than toxic. But, does negativity cause engagement, or does controversial content attract both negative framing and attention? Probably some of both.&lt;/p&gt;
    &lt;p&gt;I’ll publish the full code, dataset, and a dashboard for the HN archiver soon and I’m happy to send you an update:&lt;/p&gt;
    &lt;p&gt;Alternatively, you can also subscribe to the RSS feed or get updates on Bluesky.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46512881</guid><pubDate>Tue, 06 Jan 2026 14:45:47 +0000</pubDate></item><item><title>State of the Fin 2026-01-06</title><link>https://jellyfin.org/posts/state-of-the-fin-2026-01-06/</link><description>&lt;doc fingerprint="fe7c95df8880a4da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;State of the Fin 2026-01-06&lt;/head&gt;
    &lt;p&gt;Happy New Year and welcome to the State of the Fin! This new blog series will regularly basis highlight the ongoing development of Jellyfin and our official clients. We aim to keep our community informed and engaged, so feel free to share your feedback or thoughts on our progress!&lt;/p&gt;
    &lt;head rend="h2"&gt;Project Updates&lt;/head&gt;
    &lt;head rend="h3"&gt;Jellyfin Turns 7&lt;/head&gt;
    &lt;p&gt;December marked Jellyfin's 7th anniversary! A lot has changed in 7 years, but we remain steadfast in our commitment to Open Source and to being the best personal media server out there. Special thanks to our developers, testers, moderators, and supporters for your invaluable contributions! Here's to many more years of collaboration and streaming!&lt;/p&gt;
    &lt;head rend="h3"&gt;Versioning&lt;/head&gt;
    &lt;p&gt;We received a substantial amount of feedback regarding our versioning scheme following the 10.11 release, particularly concerning the stability of what are perceived as 'minor' version updates. This has prompted internal discussions about potentially revising our versioning scheme in the next major release. While nothing has been finalized yet, we are considering 'dropping' the major version 10, which would make the next release 12.0. Stay tuned for further updates as we navigate this feedback!&lt;/p&gt;
    &lt;head rend="h2"&gt;Development Updates&lt;/head&gt;
    &lt;head rend="h3"&gt;10.11 Release Status&lt;/head&gt;
    &lt;p&gt;Jellyfin 10.11 introduced a major EF Core refactor, consolidating the legacy &lt;code&gt;library.db&lt;/code&gt; into a single unified &lt;code&gt;jellyfin.db&lt;/code&gt;.
Following more than six months of development and an additional six months of release candidate testing, version 10.11.0 was released last year.
This extended testing period allowed us to mitigate most refactoring and RC-related issues prior to release.&lt;/p&gt;
    &lt;p&gt;Even with this level of testing, issues were expected given the scale of the database change and the limited number of users reporting bugs. These issues are currently being tracked on GitHub across three categories:&lt;/p&gt;
    &lt;p&gt;We have been moving quickly to address these issues, delivering four additional point releases with over 100 changes since the initial 10.11.0 release. To date, most point releases have focused on resolving general and migration-related issues. The remaining migration issues are largely isolated, one-off cases and are unlikely to be resolved. Most general issues have already been fixed, and the next bug-fix release is expected to include additional fixes for music metadata display issues and for watched status not being preserved when media is replaced or renamed.&lt;/p&gt;
    &lt;p&gt;We are continuing to investigate ways to mitigate performance issues caused by client-side enumeration and filtering of large datasets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Jellyfin Web vNext (aka 10.12 / 12.0)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default 'Experimental' Layout: The 'Experimental' layout is now enabled by default for all non-TV devices, introducing a new navigation layout and updated UI components.&lt;/item&gt;
      &lt;item&gt;Theming Support Overhaul: We are improving theming support by enabling easier runtime customization of default themes through CSS variables and simplifying the process for creating new bundled themes.&lt;/item&gt;
      &lt;item&gt;Community Acknowledgment: Huge thanks to those reviewing, testing, and providing feedback on web pull requests. Your contributions are immensely helpful, as the review burden largely falls on me alone!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Client Corner&lt;/head&gt;
    &lt;head rend="h3"&gt;Jellyfin Desktop&lt;/head&gt;
    &lt;p&gt;We're rebranding the desktop application from Jellyfin Media Player to Jellyfin Desktop. The most noteworthy change is the migration from Qt 5 to Qt 6. This seems to have improved overall performance, though we're still working out issues regarding memory leaks due to the migration.&lt;/p&gt;
    &lt;p&gt;Apart from the Qt migration, other noteworthy updates.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Saved servers and settings will not be migrated from Jellyfin Media Player.&lt;/item&gt;
      &lt;item&gt;We've laid the foundation for switching servers with the addition of profiles CLI options. The long-term goal is to have a UI for this as well, but the timeline is TBD.&lt;/item&gt;
      &lt;item&gt;A slew of bug fixes are included.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The release is currently available on Flathub and in the Arch Linux AUR. Stable builds for Windows and macOS builds are not currently available. Other Linux distributions will likely be added, though we recommend using Flathub for the time being. We are not currently supporting Ubuntu 24.04 LTS due to it being stuck on the older Qt 6.4 series, while our new dependency, mpvqt, requires at least Qt 6.5.&lt;/p&gt;
    &lt;head rend="h3"&gt;Jellyfin for Android TV&lt;/head&gt;
    &lt;p&gt;Two versions of the Android TV app have been released: v0.19.5 and v0.19.6! These updates contain various improvements to music transcoding. The app now properly displays durations again and allows for seeking when music is transcoding. These changes also solve the issue of lyrics not scrolling in certain cases.&lt;/p&gt;
    &lt;p&gt;For video playback, we have improved the stability of Live TV and now support direct play for the VC-1 and AV1 codecs (if your device supports them). The AV1 support was already available on Android 10 and newer but now works on older Fire TV devices as well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Jellyfin for Xbox&lt;/head&gt;
    &lt;p&gt;The last two updates brought the long awaited full gamepad support and fixes for 4K and HDR.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gamepad support: Gamepad navigation is now the default navigation type for the Jellyfin for Xbox app and requires a server version of 10.11 or higher to work. However as we cannot switch the input mode type while the app is running, the Jellyfin for Xbox app can no longer connect to older versions than 10.11. As this is a fundamental change in how the app works, there are still a few hiccups like the app not loading correctly and users reporting that the gamepad does not work at all. In those instances we recommend uninstalling and reinstalling the app.&lt;/item&gt;
      &lt;item&gt;Web UI TV mode: For versions of Jellyfin earlier than 10.11.5 the web UI still runs in the desktop mode, which might look a bit odd. However, with Jellyfin 10.11.5, we have fixed a bug that now correctly sets the web UI to TV mode, so the UI should work a lot better.&lt;/item&gt;
      &lt;item&gt;4K and HDR: For the last few versions, we have been working on enabling 4K and HDR for the app. This is done by integrating with the web UI and switching the HDMI modes. Sadly, this also comes at the cost of not being allowed to run in the background. To enable 4K support, we had to use a feature flag that allocates more video memory to the Jellyfin for Xbox app, making it incompatible with running in the background.&lt;/item&gt;
      &lt;item&gt;General Improvements: Alongside the shiny new headline features, we have also been working on the code in general, adding small improvements and cleaning up a lot of code. The latest versions added log files and their upload to the Jellyfin server, tighter integrations with the web UI, a settings view that can be expanded for future features, version compatibility checking, a better server connection experience, and more.&lt;/item&gt;
      &lt;item&gt;Future: When I took over the for the previous maintainer almost a year ago, I made a rough plan for the general development of the app. I always planned on keeping the app as a web wrapper because while the app is certainly more popular than most think, it does not have enough support in development to be a full UWP app. Nevertheless, there are a few features left on my to-do list: &lt;list rend="ul"&gt;&lt;item&gt;Localization to other languages&lt;/item&gt;&lt;item&gt;Server discovery&lt;/item&gt;&lt;item&gt;Desktop support&lt;/item&gt;&lt;item&gt;Better decoder support&lt;/item&gt;&lt;item&gt;Subtitle storage on-device&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;- JPVenson&lt;/p&gt;
    &lt;head rend="h3"&gt;Jellyfin for Roku&lt;/head&gt;
    &lt;p&gt;3.0.15 was released on 2025-12-18 and is our last release before Roku's year-end publishing blackout. It fixes a bug with HDHomeRun Tuners.&lt;/p&gt;
    &lt;p&gt;- 1hitsong&lt;/p&gt;
    &lt;head rend="h3"&gt;Swiftfin&lt;/head&gt;
    &lt;head rend="h4"&gt;Swiftfin 1.4 is out now!&lt;/head&gt;
    &lt;p&gt;This is a large release with a lot of changes under the hood. Our three highlight changes are:&lt;/p&gt;
    &lt;head rend="h4"&gt;Swiftfin Roadmap&lt;/head&gt;
    &lt;p&gt;A roadmap / project board for Swiftfin is now available!&lt;/p&gt;
    &lt;p&gt;Follow this discussion for information about the next tvOS release.&lt;/p&gt;
    &lt;p&gt;To help organize Issues &amp;amp; PRs, Swiftfin now has milestones to help users identify which changes will be included in each release:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Version 1.5 &lt;list rend="ul"&gt;&lt;item&gt;Contains issues that should be resolved in version 1.5 of Swiftfin iOS.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;tvOS Resync &lt;list rend="ul"&gt;&lt;item&gt;Contains tvOS-specific issues that will be resolved as part of our next tvOS Release.&lt;/item&gt;&lt;item&gt;Issues that impact tvOS but are part of 1.4 or 1.5 will end up in the version milestone instead of this one. Once tvOS is released, it should mirror our existing 1.X structure and iOS.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A more detailed post about these changes can be found on GitHub!&lt;/p&gt;
    &lt;p&gt;- JPKribs&lt;/p&gt;
    &lt;head rend="h3"&gt;Other TV Platforms&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Tizen app was submitted for review, but unfortunately failed testing. Additional work is needed to replicate the reported issues and correct them.&lt;/item&gt;
      &lt;item&gt;Support for multiple new platforms is currently underway, and we will provide updates as progress is made.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wishing you all happy streaming in 2026 and beyond! We look forward to another year filled with exciting updates and features for Jellyfin.&lt;/p&gt;
    &lt;p&gt;- thornbill and the Jellyfin Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514282</guid><pubDate>Tue, 06 Jan 2026 16:17:24 +0000</pubDate></item><item><title>Vienam bans unskippable ads</title><link>https://saigoneer.com/vietnam-news/28652-vienam-bans-unskippable-ads,-requires-skip-button-to-appear-after-5-seconds</link><description>&lt;doc fingerprint="6f8928bd4270847c"&gt;
  &lt;main&gt;
    &lt;p&gt;If things go our way, YouTube’s notorious unskippable ads might be a thing of the past come this February.&lt;/p&gt;
    &lt;p&gt;As Phụ Nữ reports, Vietnam recently announced Decree No. 342, which details a number of provisions to the national Advertising Law, due to take effect from February 15, 2026. The adjustments are expected to place stricter control on Vietnam’s online advertising activities to protect consumers and curb illegal ads.&lt;/p&gt;
    &lt;p&gt;Amongst the decree articles, some standout stipulations include a hard cap on the waiting time before viewers can skip video and animated ads to no more than 5 seconds. Static ads must be immediately cancellable.&lt;/p&gt;
    &lt;p&gt;Additionally, the decree requires platforms to implement clear and straightforward ways for users to close ads with just one interaction. False or vague symbols designed to confuse viewers are forbidden.&lt;/p&gt;
    &lt;p&gt;Online platforms must add visible symbols and guidelines to help users report ads that violate the law and allow them to turn off, deny, or stop seeing inappropriate ads.&lt;/p&gt;
    &lt;p&gt;Beside rules about the user experience, the decree also seeks to tightly regulate ads for 11 groups of goods and services that directly impact the environment and human health, including: cosmetics; food and beverages; milk and formula for children; insecticidal chemicals and substances; medical supplies; healthcare services; plant pesticides and veterinary drugs; fertilizers; plant seeds and saplings; pharmaceuticals; and alcoholic drinks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514677</guid><pubDate>Tue, 06 Jan 2026 16:45:42 +0000</pubDate></item><item><title>Why is the Gmail app 700 MB?</title><link>https://akr.am/blog/posts/why-is-the-gmail-app-700-mb</link><description>&lt;doc fingerprint="bda994732ecfabe0"&gt;
  &lt;main&gt;
    &lt;p&gt;The Gmail app, on the App Store, is currently 760.7 MB in size. It is in the top three most bloated apps out of the top 100 free apps. I don’t use it on my phone, but I was prompted to compare it with the seemingly hefty one I (have to) use, Outlook, while clearing up some storage space. Its measly 428 MB size pales in comparison.&lt;/p&gt;
    &lt;p&gt;This isn’t new. In 2017, Axios reported that the top iPhone apps had been taking up an increasing amount of space over the period from 2013 to 2017. For most of that period, the size of the Gmail app hovered around 12 MB, with a sudden jump to more than 200 MB near the start of 2017. Other popular apps also saw a 10x or more increase in size over the same period.&lt;/p&gt;
    &lt;p&gt;Gmail isn’t even the worst offender, it’s just a more popular one. The Tesla and Crypto.com apps are around 1 GB each. So is Samsung’s SmartThings app. What about Google’s other popular apps? Google Home is another hefty one, at 630 MB, that I used for its remote feature, which I replaced with Google TV at almost a tenth the size. Their other popular apps average around 250 MB in size. This seems tame in comparison to Microsoft, with an average app size of around 330MB. For reference, the average size of an app in the top 100 free apps is 280 MB or, in a more expanded set (including games), 200MB.&lt;/p&gt;
    &lt;p&gt;Just to put this into perspective, on my device, apps (excluding their data) use up 35 GB, and the data is another 35 GB. iOS takes up another 25 GB. Let’s say, 100 GB for apps, data and the OS. That leaves me with 20 GB (leaving a margin of free space for updates) meant to be used for capturing 4K video and high-quality photos (why else get an iPhone), and storing music (don’t even think about lossless). The reality is that running out of space also slows things down, since most of my photos need to be fetched from the cloud before viewing them, and I need to re-download these hefty offloaded apps when I need them again. And good luck if you have a limited data bundle.&lt;/p&gt;
    &lt;p&gt;Maybe this doesn’t matter. The latest iPhones start at 256 GB, and surely I’ll have plenty of space when I get a new one (although I remember saying this when I upgraded to 64 GB from 32 GB). It’s not really about the space though. These apps don’t have 50x or even 10x the functionality. But now they’re 100x larger, and probably slower. Why?&lt;/p&gt;
    &lt;p&gt;Also, can someone explain why Microsoft Authenticator is 150 MB to show 6-digit codes?&lt;/p&gt;
    &lt;p&gt;It’s not clear if this is specifically an iOS problem. I don’t have an Android device and I could not find a way to get that information from the Play Store without a device. That said, I checked the size of Gmail on someone’s Android phone, and it’s around 185 MB, which certainly seems much better.&lt;/p&gt;
    &lt;p&gt;And if you’re considering switching from the default apps, this is what the installed size (which differs slightly from the App Store size) is of the alternatives on my iPhone running iOS 26.2:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;App&lt;/cell&gt;
        &lt;cell role="head"&gt;Apple&lt;/cell&gt;
        &lt;cell role="head"&gt;Microsoft&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Files - Drive - OneDrive&lt;/cell&gt;
        &lt;cell&gt;2.6 MB&lt;/cell&gt;
        &lt;cell&gt;370 MB&lt;/cell&gt;
        &lt;cell&gt;283 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Passwords - Authenticator&lt;/cell&gt;
        &lt;cell&gt;3.2 MB&lt;/cell&gt;
        &lt;cell&gt;35 MB&lt;/cell&gt;
        &lt;cell&gt;138 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FaceTime - Meet - Teams&lt;/cell&gt;
        &lt;cell&gt;3.4 MB&lt;/cell&gt;
        &lt;cell&gt;263 MB&lt;/cell&gt;
        &lt;cell&gt;423 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Photos&lt;/cell&gt;
        &lt;cell&gt;4.2 MB&lt;/cell&gt;
        &lt;cell&gt;372 MB&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Safari - Chrome - Edge&lt;/cell&gt;
        &lt;cell&gt;5.1 MB&lt;/cell&gt;
        &lt;cell&gt;313 MB&lt;/cell&gt;
        &lt;cell&gt;397 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Reminders - Tasks - To Do&lt;/cell&gt;
        &lt;cell&gt;7.7 MB&lt;/cell&gt;
        &lt;cell&gt;89 MB&lt;/cell&gt;
        &lt;cell&gt;132 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mail - Gmail - Outlook&lt;/cell&gt;
        &lt;cell&gt;8.7 MB&lt;/cell&gt;
        &lt;cell&gt;673 MB&lt;/cell&gt;
        &lt;cell&gt;376 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Home&lt;/cell&gt;
        &lt;cell&gt;14.1 MB&lt;/cell&gt;
        &lt;cell&gt;584 MB&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Notes - Keep - OneNote&lt;/cell&gt;
        &lt;cell&gt;17.3 MB&lt;/cell&gt;
        &lt;cell&gt;171 MB&lt;/cell&gt;
        &lt;cell&gt;315 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Maps&lt;/cell&gt;
        &lt;cell&gt;68 MB&lt;/cell&gt;
        &lt;cell&gt;385 MB&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Pages - Docs - Word&lt;/cell&gt;
        &lt;cell&gt;456 MB&lt;/cell&gt;
        &lt;cell&gt;311 MB&lt;/cell&gt;
        &lt;cell&gt;434 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Numbers - Sheets - Excel&lt;/cell&gt;
        &lt;cell&gt;500 MB&lt;/cell&gt;
        &lt;cell&gt;337 MB&lt;/cell&gt;
        &lt;cell&gt;370 MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Keynote - Slides - PowerPoint&lt;/cell&gt;
        &lt;cell&gt;516 MB&lt;/cell&gt;
        &lt;cell&gt;270 MB&lt;/cell&gt;
        &lt;cell&gt;376 MB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So, why is the Gmail app almost 80x the size of the native Mail app? My guess is as good as yours.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514692</guid><pubDate>Tue, 06 Jan 2026 16:46:13 +0000</pubDate></item><item><title>Raspberry Pi and mini PC home lab prices hit parity as DRAM costs skyrocket</title><link>https://www.tomshardware.com/raspberry-pi/raspberry-pi-and-mini-pc-home-lab-prices-hit-parity-as-dram-costs-skyrocket-price-hikes-force-hobbyists-to-weigh-up-performance-versus-power-consumption</link><description>&lt;doc fingerprint="93d3208d44bfd02e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Raspberry Pi and mini PC home lab prices hit parity as DRAM costs skyrocket — price hikes force hobbyists to weigh up performance versus power consumption&lt;/head&gt;
    &lt;p&gt;Building your own homelab has just become more expensive&lt;/p&gt;
    &lt;p&gt;The humble Raspberry Pi, the perennial leader of the low-power, single-board computing (SBC) world, has hit a price parity with its rival, the Intel N100-based mini PCs. An investigation by Jeff Geerling, which we’ve independently confirmed, shows that pricing for Pi’s is now within just a few cents with a similarly configured board from brands like GMKTec. Why does this matter? Hobbyists and homelab builders had a great 2024 / 2025 which saw low prices for their DIY setups.&lt;/p&gt;
    &lt;p&gt;If you’ve been keeping a close eye on the PC hardware market of late, you’ll have noticed prices only going one way: upward. Flash memory costs, along with tariff uncertainties last year, have forced mini PC manufacturers and retailers to raise prices across the board. As Geerling explains, an explosion in homelab builds using $100-150 mini PCs made those same PCs a better, or certainly cheaper, alternative to current-gen Raspberry Pi 5s which when bundled up with NVMe HATs, NVMe drives, cases etc, retailed for over $200 last year.&lt;/p&gt;
    &lt;p&gt;Now, the tables have turned. Geerling, who compared the prices of a GMKTec mini PC versus a Raspberry Pi 5 kit in March 2025, found in his updated investigation that the GMKTec machine he considered in his first investigation is now more expensive (albeit by only a few cents). Both systems feature 16GB RAM and a 512GB NVMe SSD, but given current market conditions, systems like these aren't be sold as cheaply as they were last year.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Model&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;2025 price&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;2026 price&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 5 16GB (with 512GB SSD, 27W PSU, Bumper Case, and RTC Battery)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$208.75 (Jeff Geerling - Jan 2025)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$246.95 (Jeff Geerling - Jan 2026)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;$156.87 (Aug 2025)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$246.99 (Jan 2026)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;$159 (Aug 2025)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$259 (Jan 2026)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;$158 (Aug 2025)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$217.54 (Jan 2026)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;$199&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$199&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our own comparison of rival mini PCs, based on Amazon’s current pricing whilst compared with Camelcamelcamel’s historical data, shows that this isn’t just a brand-specific issue. For instance, the Beelink S13 with the refreshed Intel N150 CPU, 16GB RAM, and a 512GB SSD is on sale for $269, from as low as $168.99 in August 2025.&lt;/p&gt;
    &lt;p&gt;Meanwhile the Acemagic V1, with similar specs, is available for $217.54, up from $158 in August 2025, or $180 in January 2025. Geekom does offer an N100 mini PC costing $199.99 that hasn't seen a price change on Amazon in the last year, but with only 8GB RAM and a 256GB SSD. &lt;lb/&gt;The Raspberry Pi is also not immune to the upward pricing trend. The cost of a Raspberry Pi has changed in recent months, and Raspberry Pi introduced a 1GB Pi 5 in order to keep a low $45 price point. A Raspberry Pi 5 with 16GB of RAM is now $145, $25 more expensive than in early 2025.&lt;lb/&gt;The cost of additional components, such as the SSD, have all added to the cost of creating your own DIY homelab.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Product&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;RAM&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Old Price&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;New Price&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 4&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;4GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$55&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$60&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 4&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;8GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$75&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$85&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;1GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;New product&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$45&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;2GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$50&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$55&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;4GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$60&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$70&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;8GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$80&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$95&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Raspberry Pi 5&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;16GB&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$120&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$145&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source: Raspberry Pi blog.&lt;/p&gt;
    &lt;p&gt;This now leaves prospective homelab builders with three variables to consider: overall cost, power usage, and performance. Intel mini PCs are more powerful than the Raspberry Pi, even if the Pi 5 did offer a significant speed boost over the Pi 4, as our Pi 5 review explains. However, the Raspberry Pi continues to be the superior option if you're looking for the lowest power draw, even compared to the otherwise power-efficient Intel N100 and refreshed N150 mini PCs on sale.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Geerling believes that, as a result of these price rises, repurposing old hardware will be the “theme” for this new year, and one that will save you far more money, given the alternatives. This might be the status quo for some time, too. There’s no end in sight for the price shocks affecting the market, with memory manufacturers warning that the crisis has only just started, and could roll on for years to come.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Ben Stockton is a deals writer at Tom’s Hardware. He's been writing about technology since 2018, with bylines at PCGamesN, How-To Geek, and Tom’s Guide, among others. When he’s not hunting down the best bargains, he’s busy tinkering with his homelab or watching old Star Trek episodes.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;I know it depends a lot on what you do with it, but I think 16 GB is overkill for a Pi 5. Sure, if you do heavy web-browsing, then ads and videos will quickly fill up even that much. But, a Pi 5 isn't going to provide a very good web browsing experience, no matter how much RAM it has. I definitely wouldn't use one as a primary desktop, if I could avoid it. For most other things, 8 GB on a 4-core/4-thread CPU is probably quite adequate.Reply&lt;lb/&gt;By contrast, a Alder Lake-N system is a much more viable desktop option. It does deliver a usable web experience, both by having significantly faster CPU cores and having proper driver support for video decode. As a desktop, it'd definitely benefit from having 16 GB - as well as DDR5, if you can manage. And here, regular DDR5 is best, since it's significantly lower-latency than LPDDR5. I think a lot of the Alder Lake-N boards with soldered-down DRAM probably use LPDDR4 or LPDDR5.&lt;lb/&gt;Heat and power consumption are where you can get burned by going the Alder Lake-N route. The Pi 5 hits a ceiling on power consumption, at a point where Alder Lake-N is just stretching its legs. I'm basing this on wall-power data I've seen from a quad-core N97, even with restricted package power limits. The specified TDP on those N97 or N100 chips is, by no means, the full story on how much power those systems actually use!&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;thestryker&lt;/header&gt;Reply&lt;quote/&gt;It doesn't support LPDDR4, but it's important to note it only supports 4800 in LPDDR5 which is the same as the DDR5 support.bit_user said:I think a lot of the Alder Lake-N boards with soldered-down DRAM probably use LPDDR4 or LPDDR5.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514794</guid><pubDate>Tue, 06 Jan 2026 16:52:08 +0000</pubDate></item><item><title>Loongarch Improvements with Box64</title><link>https://box86.org/2026/01/new-box64-v0-4-0-released/</link><description>&lt;doc fingerprint="b7c6381d6a23a46b"&gt;
  &lt;main&gt;
    &lt;p&gt;Happy new year, and happy new release of Box64!&lt;/p&gt;
    &lt;p&gt;The new version brings a ton of new enhancements and fixes to all 3 supported platforms, with Steam running not only on Arm64, but also on RiSC-V and on Loongarch! And this is the Linux version of Steam, not the Windows one (but the Windows one works too if you really prefer that one). While Box32 (used to run Steam) is still experimental and unstable, stability did improve. Still, expect some crashes when downloading things with steam. And it’s not all, Battle.net is also getting stable, and some games are working too. Not all unfortunately, and your success might depend on your geographical region, as program versions might differ. At least, you can try it on ARM64 &amp;amp; Loongarch. It’s still to be tested on RiSC-V.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights&lt;/head&gt;
    &lt;p&gt;So, what’s new in this release? Apart from the traditionals fixes, speedups, opcodes implementation and new wrapper functions, there has been a number of refactors. Some are more groundwork for later use, like the &lt;code&gt;libdl&lt;/code&gt; one. But some have immediate benefits. The prefix opcode decoder, now implemented on the interpreter and the 3 dynarec backend is one of those. This allows to more generically handle exotic opcode prefixes without doing hacks or adding duplicated code. This one lead to the removal of many source files, now useless, making the maintenance of box64 a bit easier. Also, this automatically added support for many rare (or less rare) occurrences of many opcodes in the Dynarec, with 0 extra coding.&lt;/p&gt;
    &lt;p&gt;There has been some work on the memory footprint of box64 too. Some applications use a lot of memory once launched with box64 (Steam for example, or other things that use &lt;code&gt;libcef&lt;/code&gt; or a variant of it). So some work was started to develop mechanisms to try to identify blocks of code that have been converted to native, but that seem to not be used anymore, so they can be deleted and the memory recycled for some fresh code. The work is still in progress on this subject, and more work will come in next release.&lt;/p&gt;
    &lt;head rend="h2"&gt;Capture card&lt;/head&gt;
    &lt;p&gt;I have (finally?) invested in a capture card (an Elgato HD60X) for all my video capture. I did all the Loongarch &amp;amp; RiSC-V game captures using OBS on my Ampere ARM64 machine (plug’n play), and I did the Ampere game footage captures using a Lenovo T14s (XElite/ARM64) running Ubuntu also with OBS. While the capture done on the Ampere (using the NVidia card and encoder) were fine, the ones done on the T14s (using software encoder) were a little too compressed for many captures. I have adjusted the settings, but not redone most of them, so you will see some compression artefacts in the ARM64 gameplay video. But at least the performances are not hurt by the capture anymore…&lt;/p&gt;
    &lt;p&gt;Side note: the editing was also done on the Ampere ARM64 machine (with OpenShot), so no x86 machine was involved in those videos at all!&lt;/p&gt;
    &lt;head rend="h2"&gt;Architectures&lt;/head&gt;
    &lt;p&gt;While there is not much news on the ARM64 side (there is added support for the GB10 cpu as a build profile for Box64), there was still some refactoring to improve performances of the Dynarec there. There is still ongoing work to detect and optimize more code loops; for instance, pre-loading used XMM/YMM register just before entering the loop, to avoid loading and saving these registers when it’s not needed. That can give some big speedup in some case. More speedup will be expected when more loop cases are getting detected and handled.&lt;/p&gt;
    &lt;p&gt;On the RiSC-V, the last months have brought the Dynarec to a very decent level of completion and performances, thanks to contribution of PLCT Labs and many things are working now. Steam, Proton and Wine are all running (even with the 39bits address space limitation of many hardware). But DRM protected stuff that need Windows Syscall emulation will not work for now unless the hardware support SV48 (48bits address space) or using a hacked Wine/Proton. The current hardware used for the video is a Pioneer Milk-V mini-server and its 64 cores CPU. But each core is still quite slow, at 1.5GHz, and the equipped graphic card, an old AMD RX550, is also showing it’s limitation in some of those videos…&lt;/p&gt;
    &lt;p&gt;The Loongarch backend is the one that saw the most progress in this cycle, and this CPU is starting to shine already, even if the Dynarec doesn’t yet have all the bells and whistles from the ARM64 version. Steam works now, but also Wine and Proton. But you need to use the 4K page size kernel. The default for this system is usually 16K, but most OS have a 4K option. I use AOSC on my side, but there is now a Debian option too. The modest 3A6000 is a 4 physical cores (8 logicals cores) CPU at 2.5 GHz. Sounds modest on paper, but equipped with a good AMD RX7600, the results are already quite surprising. And it seems to be even better on more powerful variants of the CPU, like the 3B6000 (12 cores) or 3C6000 (16 cores).&lt;/p&gt;
    &lt;head rend="h2"&gt;ESync / FSync / NTSync&lt;/head&gt;
    &lt;p&gt;Wine and Proton need some complex synchronisation mechanisms between processes. By default, Wine uses a basic system on the wineserver side. It works but can be quite slow depending on the number of processes / threads / CPU cores… So various mechanisms have been developed in the last few years. The first two are ESync and FSync. While they can be faster, they can also be not 100% accurate. Box64 supports both of them (and they get used automatically by Proton in Steam), but some games are not synchronising correctly when using them. Games that use the Rockstar launcher or Microsoft XBox services might not start correctly when using ESync and FSync on ARM64. So on steam, you might need to use&lt;/p&gt;
    &lt;code&gt;PROTON_NO_ESYNC=1 PROTON_NO_FSYNC=1 %command%&lt;/code&gt;
    &lt;p&gt;on the program launch option in steam. Or disable them using the UI if you are using Heroic.&lt;/p&gt;
    &lt;p&gt;NTSync on the other hand seems to work fine, but is very new. You need to have a kernel where NTSync is enabled to use it. Debian on ARM64 still hasn’t that, even on the Forky branch. Ubuntu on ARM64 seems the same. Both OS on X64 ship kernels with NTSync enabled it seems. Armbian, on the other hand, have kernel with NTSync enabled. On Loongarch, AOSC also has NTSync enabled. Note that you need a bleeding edge Wine or Proton-GE to actually use NTSync. The default Proton from Steam will not use it for now, so it’s very early, but some result on 3C6000 processor shows some impressive result with about 80% more FPS in heavy games.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Box64 is ready to be used, so go grab the source, build your own version and enjoy!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514807</guid><pubDate>Tue, 06 Jan 2026 16:52:56 +0000</pubDate></item><item><title>Why Big Companies Keep Failing: The Stack Fallacy (2016)</title><link>https://techcrunch.com/2016/01/18/why-big-companies-keep-failing-the-stack-fallacy/</link><description>&lt;doc fingerprint="7d0c11194f46e5ed"&gt;
  &lt;main&gt;
    &lt;p&gt;Stack fallacy has caused many companies to attempt to capture new markets and fail spectacularly. When you see a database company thinking apps are easy, or a VM company thinking big data is easy — they are suffering from stack fallacy.&lt;/p&gt;
    &lt;p&gt;Stack fallacy is the mistaken belief that it is trivial to build the layer above yours.&lt;/p&gt;
    &lt;p&gt;Comic credit: XKCD&lt;/p&gt;
    &lt;p&gt;Mathematicians often believe we can describe the entire natural world in mathematical terms. Hence, all of physics is just applied math. And so on and so forth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stack fallacy — “just an app”&lt;/head&gt;
    &lt;p&gt;In the business world, we have a similar illusion. Database companies believe that SaaS apps are “just a database app” — this gives them false confidence that they can easily build, compete and win in this new market.&lt;/p&gt;
    &lt;p&gt;As history has shown, Amazon is dominating the cloud IaaS market, even as the technology vendors that build ingredient, lower-layer technologies struggle to compete — VMware is nowhere close to winning against AWS, even though all of AWS runs on virtual machine technology, a core competency of VMware; Oracle has been unable to beat Salesforce in CRM SaaS, despite the fact that Oracle perceives Salesforce to be just a hosted database app. It even runs on their database!&lt;/p&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;head rend="h3"&gt;Join the Disrupt 2026 Waitlist&lt;/head&gt;
    &lt;head rend="h4"&gt;Add yourself to the Disrupt 2026 waitlist to be first in line when Early Bird tickets drop. Past Disrupts have brought Google Cloud, Netflix, Microsoft, Box, Phia, a16z, ElevenLabs, Wayve, Hugging Face, Elad Gil, and Vinod Khosla to the stages — part of 250+ industry leaders driving 200+ sessions built to fuel your growth and sharpen your edge. Plus, meet the hundreds of startups innovating across every sector.&lt;/head&gt;
    &lt;p&gt;Apple continues to successfully integrate vertically down — building chips, programming languages, etc., but again has found it very hard to go up the stack and build those simple apps — things like photo sharing apps and maps.&lt;/p&gt;
    &lt;p&gt;History is full of such examples. IBM thought nothing much of the software layer that ran their PC hardware layer and happily allowed Microsoft to own the OS market.&lt;/p&gt;
    &lt;p&gt;In the 1990s, Larry Ellison saw SAP make gargantuan sums of money selling process automation software (ERP) — to him, ERP was nothing more than a bunch of tables and workflows — so he spent hundreds of millions of dollars trying to own that market, with mixed results. Eventually, Oracle bought its way into the apps market by acquiring PeopleSoft and Siebel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why do we keep falling for the stack fallacy?&lt;/head&gt;
    &lt;p&gt;The stack fallacy is a result of human nature — we (over) value what we know. In real terms, imagine you work for a large database company and the CEO asks , “Can we compete with Intel or SAP?” Very few people will imagine they can build a computer chip just because they can build relational database software, but because of our familiarity with building blocks of the layer up, it is easy to believe you can build the ERP app. After all, we know tables and workflows.&lt;/p&gt;
    &lt;p&gt;The bottleneck for success often is not knowledge of the tools, but lack of understanding of the customer needs. Database engineers know almost nothing about what supply chain software customers want or need. They can hire for that, but it is not a core competency.&lt;/p&gt;
    &lt;p&gt;In a surprising way, it is far easier to innovate down the stack than up the stack.&lt;/p&gt;
    &lt;p&gt;The reason for this is that you are yourself a natural customer of the lower layers. Apple knew what it wanted from an ideal future microprocessor. It did not have the skills necessary to build it, but the customer needs were well understood. Technical skills can be bought/acquired, whereas it is very hard to buy a deep understanding of market needs.&lt;/p&gt;
    &lt;p&gt;It is therefore no surprise that Apple had an easier time building semiconductor chips than building Apple Maps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Google, Facebook, WhatsApp&lt;/head&gt;
    &lt;p&gt;Google is a great example. It owned our email graph and our interest data (search), yet found it very difficult to succeed in what looks like a “trivial to build” app — social networks.&lt;/p&gt;
    &lt;p&gt;In fact, this is the perfect irony of stack fallacy. You can build things higher up the stack. It is just that often it is not clear what to build.&lt;/p&gt;
    &lt;p&gt;Product management is the art of knowing what to build.&lt;/p&gt;
    &lt;p&gt;The stack fallacy provides insights into why companies keep failing at the obvious things — things so close to their reach that they can surely build. The answer may be that the what is 100 times more important than the how.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514816</guid><pubDate>Tue, 06 Jan 2026 16:53:27 +0000</pubDate></item><item><title>Show HN: Stash – Sync Markdown Files with Apple Notes via CLI</title><link>https://github.com/shakedlokits/stash</link><description>&lt;doc fingerprint="20c6dcb573341fc0"&gt;
  &lt;main&gt;
    &lt;code&gt;                                     
                                     
  ▄█████ ██████ ▄████▄ ▄█████ ██  ██ 
  ▀▀▀▄▄▄   ██   ██▄▄██ ▀▀▀▄▄▄ ██████ 
  █████▀   ██   ██  ██ █████▀ ██  ██ 
                                     
                                     
&lt;/code&gt;
    &lt;p&gt;Bidirectionally sync Markdown files with Apple Notes!&lt;/p&gt;
    &lt;code&gt;brew tap shakedlokits/stash https://github.com/shakedlokits/stash
brew install shakedlokits/stash/stash&lt;/code&gt;
    &lt;p&gt;Push a markdown file to Apple Notes:&lt;/p&gt;
    &lt;code&gt;stash push my-note.md&lt;/code&gt;
    &lt;p&gt;Pull changes back from Apple Notes:&lt;/p&gt;
    &lt;code&gt;stash pull my-note.md&lt;/code&gt;
    &lt;p&gt;That's it! The tool uses front-matter to track which Apple Note corresponds to your file.&lt;/p&gt;
    &lt;p&gt;Apple Notes has been my daily driver for years. I love its simplicity—it syncs fast, stays out of the way, and just lets me write.&lt;/p&gt;
    &lt;p&gt;I've explored the full spectrum of note-taking apps: &lt;code&gt;Workflowy&lt;/code&gt;, &lt;code&gt;Obsidian&lt;/code&gt;, &lt;code&gt;Bear&lt;/code&gt;, &lt;code&gt;Evernote&lt;/code&gt;, &lt;code&gt;Notion&lt;/code&gt;, &lt;code&gt;Google Keep&lt;/code&gt;, &lt;code&gt;GoodNotes&lt;/code&gt;, and others I've since forgotten. Each promised to revolutionize how I capture thoughts. But eventually, I realized something simple: note-taking is about writing things down, not managing a complex system. I came back to Apple Notes and haven't looked back.&lt;/p&gt;
    &lt;p&gt;There's just one friction point. When I'm building things—which is most days—I live in Markdown. At work, I sync those files to Notion or Confluence with CLI tools. For personal projects, everything goes into Git. But increasingly, I find myself writing quick notes that don't belong to any project—just ideas, experiments, small discoveries—and I want them on Apple Notes where I can read them anywhere. Right now, there's no clean path from my Markdown workflow to my notes.&lt;/p&gt;
    &lt;p&gt;I went searching for CLI tools to bridge this gap. What I found was disappointing: tools either pack in too many features, making them brittle and hard to maintain, or they offer so little functionality (read-only sync) that they're effectively useless.&lt;/p&gt;
    &lt;p&gt;So I built my own.&lt;/p&gt;
    &lt;p&gt;The requirements are straightforward:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run from the shell without configuration files&lt;/item&gt;
      &lt;item&gt;Use AppleScript for maximum compatibility and stability&lt;/item&gt;
      &lt;item&gt;Bidirectionally sync Markdown and Apple Notes, using front-matter to track state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Congratulations! You've written a new Markdown note, it's nice and tidy, and you've even run &lt;code&gt;vale&lt;/code&gt; on it. Now all that remains is getting it into Apple Notes. Here's what you need to do:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;push my-cool-note.md&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A new note will be created: &lt;code&gt;My Cool Note ...&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Front-matter with a unique identifier will be added to your markdown file: &lt;quote&gt;--- apple_notes_id: my-new-cool-note-identifier --- # My Cool Note ...&lt;/quote&gt;&lt;p&gt;NOTE: If you already have front-matter, it will be added to the existing front-matter.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Made changes to the Markdown file and now it's out of sync? Simply:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Rerun &lt;code&gt;push my-cool-note.md&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tool searches for the note matching your identifier (&lt;code&gt;id_my-new-cool-note-identifier&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;It rewrites the note's content with your updated Markdown. &lt;p&gt;NOTE: If no note was found (due to unexpected ID changes) you will be asked if you'd like to create a new note, which will overwrite your previous ID.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You've gone off for your coffee/potty/meeting break, and while skimming through your note on your phone, you realized you made a terrible mistake—which inevitably led you to rewrite half of it.&lt;/p&gt;
    &lt;p&gt;Now the panic has settled, you're back at your computer, and you're wondering: "What the hell have I done, and how can I possibly get all those changes back into my Markdown?"&lt;/p&gt;
    &lt;p&gt;Don't fret. Simply:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;pull my-cool-note.md&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tool searches for the note matching your identifier (&lt;code&gt;id_my-new-cool-note-identifier&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;It rewrites your local Markdown file with the content from Apple Notes. &lt;p&gt;NOTE: The front-matter is unchanged during pull operations.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS with Apple Notes&lt;/item&gt;
      &lt;item&gt;Pandoc for Markdown ↔ HTML conversion&lt;/item&gt;
      &lt;item&gt;pcregrep for frontmatter parsing (usually pre-installed on macOS, or install via &lt;code&gt;brew install pcre&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The tool is built in three layers:&lt;/p&gt;
    &lt;p&gt;AppleScript forms the core, handling all communication with Apple Notes—finding existing notes, updating content, and creating or deleting notes (the latter mostly for testing).&lt;/p&gt;
    &lt;p&gt;Shell scripts contain the business logic that orchestrates these AppleScript operations, managing the sync workflow and front-matter processing.&lt;/p&gt;
    &lt;p&gt;Pandoc handles the conversion between Markdown and HTML, ensuring content is properly formatted for Apple Notes.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Bashly&lt;/code&gt; ties it all together, providing a clean CLI interface, shell completions, and command scaffolding.&lt;/p&gt;
    &lt;p&gt;Clone the repository and build:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/shakedlokits/stash.git
cd stash
make build&lt;/code&gt;
    &lt;code&gt;# Run all tests (requires Apple Notes access)
make test

# Run unit tests only (no Apple Notes required)
make test-unit&lt;/code&gt;
    &lt;code&gt;src/
  lib/           # Utility functions (pure and integration)
  bashly.yml     # CLI configuration
  *_command.sh   # Command implementations
test/
  cases/         # Test specs (unit, integration, e2e)
  fixtures/      # Test fixture files
  approvals/     # Approval test snapshots
dist/
  stash          # Generated CLI (via bashly)
Formula/
  stash.rb       # Homebrew formula
&lt;/code&gt;
    &lt;code&gt;make release VERSION=x.y.z&lt;/code&gt;
    &lt;p&gt;This will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Update the version in &lt;code&gt;src/bashly.yml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Commit the change&lt;/item&gt;
      &lt;item&gt;Create and push a git tag&lt;/item&gt;
      &lt;item&gt;Trigger the release workflow (build, publish, update Homebrew formula)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This backlog contains both current and future development items, feel free to take some or add to it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Basic Functionality &lt;list rend="ul"&gt;&lt;item&gt; AppleScript notes access functions &lt;list rend="ul"&gt;&lt;item&gt;Find note&lt;/item&gt;&lt;item&gt;Create note&lt;/item&gt;&lt;item&gt;Delete note&lt;/item&gt;&lt;item&gt;Update note&lt;/item&gt;&lt;item&gt;Read note&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Markdown front-matter parser (get-id, extract, strip, update)&lt;/item&gt;&lt;item&gt;Push command shell script&lt;/item&gt;&lt;item&gt;Push command tests&lt;/item&gt;&lt;item&gt;Pull command shell script&lt;/item&gt;&lt;item&gt;Pull command tests&lt;/item&gt;&lt;item&gt;&lt;code&gt;Pandoc&lt;/code&gt;integration&lt;/item&gt;&lt;item&gt;&lt;code&gt;Bashly&lt;/code&gt;setup&lt;list rend="ul"&gt;&lt;item&gt;CLI interface&lt;/item&gt;&lt;item&gt;Shell completion&lt;/item&gt;&lt;item&gt;Documentation&lt;/item&gt;&lt;item&gt;Approval testing&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt; AppleScript notes access functions &lt;/item&gt;
      &lt;item&gt; Nice to have &lt;list rend="ul"&gt;&lt;item&gt;Diff changes (requires design)&lt;/item&gt;&lt;item&gt;Attachments support&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514874</guid><pubDate>Tue, 06 Jan 2026 16:57:15 +0000</pubDate></item><item><title>Volkswagen Brings Back Physical Buttons</title><link>https://www.caranddriver.com/news/a69916699/volkswagen-interior-physical-buttons-return/</link><description>&lt;doc fingerprint="bdad1cdd2a348da9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Volkswagen revealed a new generation of cockpit design with the refreshed ID. Polo.&lt;/item&gt;
      &lt;item&gt;The new design marks a big departure for VW and features a plethora of physical controls rather than the capacitive buttons on current models.&lt;/item&gt;
      &lt;item&gt;While the switchgear is currently only found on the new ID. Polo, which isn't sold in the United States, it could debut on the soon-to-be-refreshed ID.4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Volkswagen is making a drastic change to its interiors, or at least the interiors of its electric vehicles. The automaker recently unveiled a new cockpit generation with the refreshed ID. Polo—the diminutive electric hatchback that the brand sells in Europe—that now comes with physical buttons.&lt;/p&gt;
    &lt;p&gt;While VW certainly isn't the only automaker that pushed the envelope with haptic controls and digital buttons, it was a particularly egregious offender. Now, the company is doing a complete 180-degree shift, adding a full suite of physical buttons and switchgear to the Polo's interior.&lt;/p&gt;
    &lt;head rend="h2"&gt;For Sale Near You&lt;/head&gt;
    &lt;p&gt;See all results for Volkswagen for sale near 32202&lt;/p&gt;
    &lt;p&gt;The steering wheel gets new clusters of buttons for cruise control and interacting with music playback, while switches for the temperature and fan speed now live in a row along the dashboard. The move back to buttons doesn't come out of nowhere. Volkswagen already started the shift with the new versions of the Golf and Tiguan models in the United States. Unfortunately, some climate controls, such as those for the rear defrost and the heated seats, are still accessed through the touchscreen. Thankfully, they look to retain their dedicated spot at the bottom of the display.&lt;/p&gt;
    &lt;p&gt;Volkswagen hasn't announced which models will receive the new cockpit design. The redesigned interior also may be limited to the brand's electric vehicles, which would limit it to the upcoming refresh for the ID.4 SUV (and potentially the ID.Buzz), as the only VW EV models currently sold in America.&lt;/p&gt;
    &lt;p&gt;➡️ Skip the lot. Let Car and Driver help you find your next car.&lt;/p&gt;
    &lt;p&gt;Jack Fitzgerald’s love for cars stems from his as yet unshakable addiction to Formula 1. &lt;lb/&gt; After a brief stint as a detailer for a local dealership group in college, he knew he needed a more permanent way to drive all the new cars he couldn’t afford and decided to pursue a career in auto writing. By hounding his college professors at the University of Wisconsin-Milwaukee, he was able to travel Wisconsin seeking out stories in the auto world before landing his dream job at Car and Driver. His new goal is to delay the inevitable demise of his 2010 Volkswagen Golf.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46514913</guid><pubDate>Tue, 06 Jan 2026 16:59:44 +0000</pubDate></item><item><title>DatBench: Discriminative, faithful, and efficient VLM evaluations</title><link>https://arxiv.org/abs/2601.02316</link><description>&lt;doc fingerprint="ac1e2f29d9fdd48e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 5 Jan 2026]&lt;/p&gt;&lt;head rend="h1"&gt;Title:DatBench: Discriminative, Faithful, and Efficient VLM Evaluations&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515648</guid><pubDate>Tue, 06 Jan 2026 17:43:03 +0000</pubDate></item><item><title>Launch HN: Tamarind Bio (YC W24) – AI Inference Provider for Drug Discovery</title><link>https://news.ycombinator.com/item?id=46515777</link><description>&lt;doc fingerprint="6f51ad5b7d3645cb"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we're Deniz and Sherry from Tamarind Bio (&lt;/p&gt;https://www.tamarind.bio&lt;p&gt;). Tamarind is an inference provider for AI drug discovery, serving models like AlphaFold. Biopharma companies use our library of leading open-source models to design new medicines computationally.&lt;/p&gt;&lt;p&gt;Here’s a demo: https://youtu.be/luoMApPeglo&lt;/p&gt;&lt;p&gt;Two years ago, I was hired at a Stanford lab to run models for my labmates. Some post-doc would ask me to run a set of 1-5 models in sequence with tens of thousands inputs and I would email them back the result after setting up the workflow in the university cluster.&lt;/p&gt;&lt;p&gt;At some point, it became unreasonable that all of an organization's computational biology work would go through an undergrad, so we built Tamarind as a single place for all molecular AI tools, usable at massive scale with no technical background needed. Today, we are used by much of the top 20 pharma, dozens of biotechs and tens of thousands of scientists.&lt;/p&gt;&lt;p&gt;When we started getting adoption in the big pharma companies, we found that this problem also persisted. I know directors of data science, where half their job could be described as running scripts for other people.&lt;/p&gt;&lt;p&gt;Lots of companies have also deprecated their internally built solution to switch over, dealing with GPU infra and onboarding docker containers not being a very exciting problem when the company you work for is trying to cure cancer.&lt;/p&gt;&lt;p&gt;Unlike non-specialized inference providers, we build both a programmatic interface for developers along with a scientist-friendly web app, since most of our users are non-technical. Some of them used to extract proteins from animal blood before replacing that process with using AI to generate proteins on Tamarind.&lt;/p&gt;&lt;p&gt;Besides grinding out images for each of the models we serve, we’ve designed a standardized schema to be able to share each model’s data format. We’ve built a custom scheduler and queue optimized for horizontal scaling (each inference call takes minutes to hours, and runs on one GPU at a time), while splitting jobs across CPUs and GPUs for optimal timing.&lt;/p&gt;&lt;p&gt;As we've grown to handle a substantial portion of the biopharma R&amp;amp;D AI demand on behalf of our customers, we've expanded beyond just offering a library of open source protocols.&lt;/p&gt;&lt;p&gt;A common use case we saw from early on was the need to connect multiple models together into pipelines, and having reproducible, consistent protocols to replace physical experiments. Once we became the place to build internal tools for computational science, our users started asking if they could onboard their own models to the platform.&lt;/p&gt;&lt;p&gt;From there, we now support fine-tuning, building UIs for arbitrary docker containers, connecting to wet lab data sources and more!&lt;/p&gt;&lt;p&gt;Reach out to me at deniz[at]tamarind.bio if you’re interested in our work, we are hiring! Check out our product at https://app.tamarind.bio and let us know if you have any feedback to support how the biotech industry uses AI today.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515777</guid><pubDate>Tue, 06 Jan 2026 17:49:56 +0000</pubDate></item><item><title>Dude, where's my supersonic jet?</title><link>https://rationaloptimistsociety.substack.com/p/dude-wheres-my-supersonic-jet</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515936</guid><pubDate>Tue, 06 Jan 2026 17:59:41 +0000</pubDate></item><item><title>Locating a Photo of a Vehicle in 30 Seconds with GeoSpy</title><link>https://geospy.ai/blog/locating-a-photo-of-a-vehicle-in-30-seconds-with-geospy</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515948</guid><pubDate>Tue, 06 Jan 2026 18:00:27 +0000</pubDate></item><item><title>Hierarchical Autoregressive Modeling for Memory-Efficient Language Generation</title><link>https://arxiv.org/abs/2512.20687</link><description>&lt;doc fingerprint="889c3c0ad4d2528e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 22 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:PHOTON: Hierarchical Autoregressive Modeling for Lightspeed and Memory-Efficient Language Generation&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Transformers operate as horizontal token-by-token scanners; at each generation step, the model attends to an ever-growing sequence of token-level states. This access pattern increases prefill latency and makes long-context decoding increasingly memory-bound, as KV-cache reads and writes dominate inference throughput rather than arithmetic computation. We propose Parallel Hierarchical Operation for Top-down Networks (PHOTON), a hierarchical autoregressive model that replaces flat scanning with vertical, multi-resolution context access. PHOTON maintains a hierarchy of latent streams: a bottom-up encoder progressively compresses tokens into low-rate contextual states, while lightweight top-down decoders reconstruct fine-grained token representations. Experimental results show that PHOTON is superior to competitive Transformer-based language models regarding the throughput-quality trade-off, offering significant advantages in long-context and multi-query tasks. This reduces decode-time KV-cache traffic, yielding up to $10^{3}\times$ higher throughput per unit memory.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46515987</guid><pubDate>Tue, 06 Jan 2026 18:02:01 +0000</pubDate></item><item><title>Creating a Bespoke Data Diode for Air‑Gapped Networks</title><link>https://nelop.com/bespoke-data-diode-airgap/</link><description>&lt;doc fingerprint="753841970511f2af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Creating a Bespoke Data Diode for Air Gapped Networks&lt;/head&gt;
    &lt;p&gt;Published by Neil on&lt;/p&gt;
    &lt;p&gt;Air-gapped networks are physically isolated computer networks that do not connect to the internet or other external networks. They are widely used in industries where security is critical, such as finance, healthcare, and critical infrastructure. By design, these networks prevent remote access and reduce the risk of cyber attacks. However, while air gaps protect sensitive systems, they also create a challenge: how do you safely extract operational data for monitoring or analysis without compromising security?&lt;/p&gt;
    &lt;p&gt;We were approached by a client with precisely this challenge. Their crucial infrastructure was protected by an air gap, but they needed to extract syslog information and performance data to allow internal monitoring teams to track the system’s health and security posture.&lt;/p&gt;
    &lt;p&gt;After evaluating options, we chose to implement a bespoke data diode solution using two Raspberry Pi devices connected via an opto coupler. An opto coupler, also known as an opto isolator, allows an electrical signal to pass from one device to another using light, preventing direct electrical connection. This ensures data flows in a single direction, maintaining the integrity of the air gap.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;The system consists of a “send” Pi on the air-gapped network and a “receive” Pi on the external monitoring network. Both devices run custom scripts designed to handle data transmission reliably rather than quickly. This approach limits throughput, but reliability is paramount for critical monitoring, where losing data is unacceptable. The scripts are finely tuned to ensure that every log entry is transmitted securely without risk of cross-contamination between networks.&lt;/p&gt;
    &lt;p&gt;This method is particularly effective for syslog data. Our client can now extract performance and security information from their air-gapped network, providing their internal teams with actionable insights. The air gap remains intact, but vital operational intelligence passes safely to the outside world.&lt;/p&gt;
    &lt;head rend="h3"&gt;UART over Serial Ports&lt;/head&gt;
    &lt;p&gt;Initially, we explored using a standard serial port for data transmission. While functional, it introduced limitations in reliability and required additional hardware considerations. After testing, we switched to a UART interface on the Raspberry Pi, which provided a simpler, more reliable solution for one-way communication. This approach allowed the data diode to maintain a clean, stable signal with minimal risk of interference, further enhancing the integrity of the air-gapped network while keeping the setup streamlined and efficient.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bespoke Solutions for Critical Challenges&lt;/head&gt;
    &lt;p&gt;Every implementation we deliver is tailored to the specific requirements of the client’s environment. The Raspberry Pi data diode demonstrates our ability to provide bespoke solutions that meet strict security and operational needs. Whether your challenge involves syslogs, process data, or other critical information, we design systems that are reliable, secure, and efficient. This is precisely the type of project we enjoy: solving unique, complex problems with practical, secure technology solutions.&lt;/p&gt;
    &lt;p&gt;In summary, our Raspberry Pi-based data diode offers a secure and dependable method to extract critical information from air-gapped networks. It balances reliability and security, ensuring the client’s infrastructure remains protected while providing essential visibility for operational monitoring.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Choose Nelop Systems?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Proven experience Over 25 years in IT and systems integration, specialising in environments where uptime is essential.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;UK-based &amp;amp; on-site service For many legacy systems there is no substitute for someone who can bring the hardware, tools, and know-how to your premises.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Operational continuity We plan every intervention so that systems keep running, avoiding downtime where possible, and maintaining business critical performance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Email contact@nelop.com for assistance or information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46516117</guid><pubDate>Tue, 06 Jan 2026 18:10:31 +0000</pubDate></item><item><title>Passing of Joe Mancuso author of Masonite (Python web framework)</title><link>https://github.com/MasoniteFramework/masonite/discussions/853</link><description>&lt;doc fingerprint="2c52725b5bc5a72e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Passing of Joe Mancuso #853&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Good morning Masonite community,&lt;/p&gt;
          &lt;p&gt;I regret to inform you all that @josephmancuso has passed away due to health complications. Please keep his family in your thoughts during this time.&lt;/p&gt;
          &lt;p&gt;I had the privilege of working alongside Joe for many years, and it was clear as day how much Masonite meant to him. Even when fighting for his life, he continued doing everything he could to maintain and support this project.&lt;/p&gt;
          &lt;p&gt;One of the beautiful things about open source is that we build together. While Joe is no longer with us, Masonite can continue to grow and evolve through the contributions of this community. I hope we all continue working toward the vision he poured so much of himself into.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replies: 2 comments&lt;/head&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;So sad, my condolences.😞&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This is heartbreaking. I also had the privilege of working with Joe a couple of months ago, and he was always bringing new ideas to improve and share within the open-source community.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Beta Was this translation helpful? Give feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46516137</guid><pubDate>Tue, 06 Jan 2026 18:11:51 +0000</pubDate></item><item><title>Show HN: Jax-JS, array library in JavaScript targeting WebGPU</title><link>https://ss.ekzhang.com/p/jax-js-an-ml-library-for-the-web</link><description>&lt;doc fingerprint="3637085cea8c3ad8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;jax-js: an ML library for the web&lt;/head&gt;
    &lt;head rend="h3"&gt;JAX in pure JavaScript, as a flexible machine learning library and compiler.&lt;/head&gt;
    &lt;p&gt;I’m excited to release jax-js, a machine learning library for the web.&lt;/p&gt;
    &lt;p&gt;You can think of it as a reimplementation of Google DeepMind’s JAX framework (similar to PyTorch) in pure JavaScript.&lt;/p&gt;
    &lt;p&gt;jax-js runs completely in the browser by generating fast WebGPU and Wasm kernels.&lt;/p&gt;
    &lt;head rend="h2"&gt;Numerical computing on the web&lt;/head&gt;
    &lt;p&gt;Starting in February this year, I spent nights and weekends working on a new ML library for the browser. I wanted a cross-platform way to run numerical programs on the frontend web, so you can do machine learning.&lt;/p&gt;
    &lt;p&gt;Python and JavaScript are the most popular languages in the world:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;JavaScript is the language of the web.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Python is simple, expressive and now ubiquitous in ML thanks to frameworks like PyTorch and JAX.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But most developers would balk at running any number crunching in JavaScript. While the JavaScript JIT is really good, it’s not optimized for tight numerical loops. JavaScript doesn’t even have a fast, native integer data type! So how can you run fast numerical code on the web?&lt;/p&gt;
    &lt;p&gt;The answer is to rely on new browser technologies — WebAssembly and WebGPU, which allow you to run programs at near-native speeds. WebAssembly is a low-level portable bytecode, and WebGPU is GPU shaders on the web.&lt;/p&gt;
    &lt;p&gt;If we can use these native runtimes, then this lends itself to a programming model similar to JAX, where you trace programs and JIT compile them to GPU kernels. Here, instead of Nvidia CUDA, we write pure JavaScript to generate WebAssembly and WebGPU kernels. Then we can run them and execute instructions at near-native speed, skipping the JavaScript interpreter bottleneck.&lt;/p&gt;
    &lt;p&gt;That is what I ended up doing in jax-js, and now it “just works”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;p&gt;You can install jax-js as a library. It has 0 dependencies and is pure JS.&lt;/p&gt;
    &lt;code&gt;npm install @jax-js/jax&lt;/code&gt;
    &lt;p&gt;Then you can use it with an API almost identical to JAX.&lt;/p&gt;
    &lt;code&gt;import { numpy as np } from "@jax-js/jax";

const ar = np.array([1, 5, 6, 7]);
console.log(ar.mul(10).js());  // -&amp;gt; [10, 50, 60, 70]&lt;/code&gt;
    &lt;p&gt;Under the hood, this generates a WebAssembly kernel and dispatches it.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: There are some surface-level syntax differences here, versus JAX:&lt;/p&gt;&lt;p&gt;JavaScript doesn’t have operator overloading like Python. Instead of&lt;/p&gt;&lt;code&gt;ar * 10&lt;/code&gt;in Python, you have to call&lt;code&gt;ar.mul(10)&lt;/code&gt;.&lt;p&gt;The&lt;/p&gt;&lt;code&gt;.js()&lt;/code&gt;method converts a jax.Array object back into a plain JS array.&lt;p&gt;JS has no reference-counted destructor method to free memory, so array values in jax-js have move semantics like Rust, with&lt;/p&gt;&lt;code&gt;.ref&lt;/code&gt;incrementing their reference counts.&lt;/quote&gt;
    &lt;p&gt;If you’d like to use WebGPU, just start your program with:&lt;/p&gt;
    &lt;code&gt;import { init, setDevice } from "@jax-js/jax";

await init("webgpu");
setDevice("webgpu");&lt;/code&gt;
    &lt;p&gt;You can leverage grad, vmap, and other features of JAX. Here’s automatic differentiation with &lt;code&gt;grad()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import { grad, numpy as np } from “@jax-js/jax”;

const f = (x: np.Array) =&amp;gt; np.sqrt(x.ref.mul(x).sum());
const df = grad(f);

const x = np.array([1, 2, 3, 4]);
console.log(df(x).js());&lt;/code&gt;
    &lt;p&gt;And here’s an example the compiler fusing operations with &lt;code&gt;jit()&lt;/code&gt;. The following function gets translated into a compiled GPU compute kernel:&lt;/p&gt;
    &lt;code&gt;import { jit, numpy as np } from "@jax-js/jax";

const f = jit((x: np.Array) =&amp;gt; {
  return np.sqrt(x.add(2).mul(Math.PI)).sum();
});&lt;/code&gt;
    &lt;head rend="h2"&gt;Machine learning&lt;/head&gt;
    &lt;p&gt;With these simple building blocks, you can implement most machine learning algorithms and backpropagate through them.&lt;/p&gt;
    &lt;p&gt;Here is a runnable example of training a neural network from scratch on MNIST dataset in your browser. It learns to &amp;gt;99% accuracy in seconds, and everything from dataset loading to matmul kernels is pure frontend JavaScript code.&lt;/p&gt;
    &lt;p&gt;It’s remarkable to write ML programs with hot module reloading. You can edit code in real time while the model is training!&lt;/p&gt;
    &lt;p&gt;—&lt;/p&gt;
    &lt;p&gt;You can also build applications. Here’s a demo I built yesterday: download the whole text of Great Expectations (180,000 words), run it through a CLIP-based embedding model, and semantic search it in real time—all from your browser.&lt;/p&gt;
    &lt;p&gt;(The text embedding actually runs at a respectable ~500 GFLOP/s on my M1 Pro with just jax.jit(), despite me not having tried to optimize it at all yet. Not bad, crunching 500,000,000,000 calculations/second in browser on a 4-year-old laptop!)&lt;/p&gt;
    &lt;p&gt;For a lot of inference use cases, you might find a “model runtime” like ONNX to add prebuilt ML models to your browser, where the ML developers hand off pre-packaged weights to be used in product. With jax-js, it’s a bit different, and I’m imagining how a full ML framework, usually relegated to the backend, can run in a browser.&lt;/p&gt;
    &lt;p&gt;As for performance, it hasn’t been my primary focus so far, as just “getting the ML framework working” comes first. I have checked that jax-js’s generated kernels for matmuls are fast (&amp;gt;3 TFLOP on Macbook M4 Pro). But there’s a lot of room to improve (e.g., conv2d is slow), and I haven’t done much optimization work on transformer inference in particular yet. There’s plenty of low-hanging fruit.&lt;/p&gt;
    &lt;head rend="h2"&gt;Project release&lt;/head&gt;
    &lt;p&gt;I am open-sourcing jax-js today at ekzhang/jax-js.&lt;/p&gt;
    &lt;p&gt;There are rough edges in this initial release, but it’s ready to try out now.&lt;/p&gt;
    &lt;p&gt;Links:&lt;/p&gt;
    &lt;p&gt;I look forward to seeing what you create. 🥰&lt;/p&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;p/&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;This is a personal project and not related to Thinking Machines Lab. I started working on jax-js before starting my current job, and in a way, it’s partly how I ended up working in ML. Turns out this stuff is kind of fun!&lt;/p&gt;
    &lt;p&gt;If you’re still reading, hello—I have a bunch more details to share.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The authors of JAX for making an important ML library that’s a joy to use.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Thanks to Matthew Johnson, Dougal Maclaurin, and others for Autodidax, an instructive implementation of the JAX core from scratch.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;And thanks for all of the JAX ecosystem libraries as well.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tinygrad for a very excellent autograd library — you showed that code-generating kernels from scratch can’t really be that intrinsically complex!&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Many parts of jax-js in the backend internals follow Tinygrad’s design closely. The biggest example of this is ShapeTracker, which was directly ported.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Chrome, Safari, and Firefox for WebGPU, now used in 2% of all websites.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The open-source community, for inspiration and for showing that ML on the web is actually possible!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;How it works: An overview of internals&lt;/head&gt;
    &lt;p&gt;In general, I think there are roughly two parts to an ML library:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;“Frontend” (think JAX): The interface for creating and manipulating arrays, the autograd engine, JIT, typing and transformations. Also where you interact with a sync/async boundary and how you track memory allocations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“Backend” (think XLA): Actual kernels for executing operations. The frontend has some kind of representation of a kernel, it dispatches it to the backend, which then optimizes it, compiles it down to native code (CPU or GPU) and runs it very efficiently.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This dichotomy obviously isn’t perfect (e.g., where do Triton/Pallas fit in? how about warp-specialized cuTile?), and there are certainly concerns that span both parts. But it’s how jax-js works.&lt;/p&gt;
    &lt;p&gt;Let’s start with the backend and build our way up. In jax-js, the backend code is actually quite self-contained; they implement the Backend interface (abridged):&lt;/p&gt;
    &lt;code&gt;/** A device backend. */
export interface Backend {
  /** Allocate a new slot with reference count 1. */
  malloc(size: number, initialData?: Uint8Array): Slot;

  /** Increment the reference count of the slot. */
  incRef(slot: Slot): void;

  /**
   * Decrement the reference count of the slot. If the reference count reaches
   * zero, it is freed. This should throw if the slot was already freed.
   */
  decRef(slot: Slot): void;

  /** Read a range of bytes from a buffer. */
  read(
    slot: Slot,
    start?: number,
    count?: number,
  ): Promise&amp;lt;Uint8Array&amp;lt;ArrayBuffer&amp;gt;&amp;gt;;

  /** Prepare an expression to be executed later. */
  prepare(kernel: Kernel): Promise&amp;lt;Executable&amp;gt;;

  /**
   * Run a backend operation that was previously prepared.
   *
   * The operation may not run immediately, but operations are guaranteed to run
   * in the dispatch order. Also, `read()` will wait for all pending operations
   * on that slot to finish.
   */
  dispatch(exe: Executable, inputs: Slot[], outputs: Slot[]): void;
}&lt;/code&gt;
    &lt;p&gt;In other words, backends need to be able to malloc/free chunks of memory for tensors, and to execute &lt;code&gt;Kernel&lt;/code&gt; objects. Inside a &lt;code&gt;Kernel&lt;/code&gt; there is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A pointwise operation on one or more tensors, with&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lazy shape-tracking information for how to index the tensors, and&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;A reduction to be performed (optional).&lt;/p&gt;&lt;lb/&gt;Reductions can be any associative operation (add/multiply/max/min), and they can optionally have a fused epilogue as well.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The pointwise operation is constructed from a pure expression tree, an &lt;code&gt;AluExp&lt;/code&gt;, where each node is a symbolic &lt;code&gt;AluOp&lt;/code&gt;. There are 28 AluOps — you don’t need so many distinct operations when you can depend on kernel fusion!&lt;/p&gt;
    &lt;p&gt;Note that no automatic differentiation happens here; these are pure low-level operations, so we can introduce arbitrary building blocks this way.&lt;/p&gt;
    &lt;code&gt;/** Symbolic form for each mathematical operation. */
export enum AluOp {
  Add = “Add”,
  Sub = “Sub”,
  Mul = “Mul”,
  Idiv = “Idiv”,
  Mod = “Mod”,
  Min = “Min”,
  Max = “Max”,

  Sin = “Sin”,
  Cos = “Cos”,
  Asin = “Asin”,
  Atan = “Atan”,
  Exp = “Exp”,
  Log = “Log”,
  Erf = “Erf”,
  Erfc = “Erfc”,
  Sqrt = “Sqrt”,
  Reciprocal = “Reciprocal”,
  Cast = “Cast”,
  Bitcast = “Bitcast”,

  Cmplt = “Cmplt”,
  Cmpne = “Cmpne”,
  Where = “Where”, // Ternary operator: `cond ? a : b`

  Threefry2x32 = “Threefry2x32”, // PRNG operation, arg = ‘xor’ | 0 | 1

  // Const is a literal constant, while GlobalIndex takes data from an array
  // buffer. Special and Variable are distinguished since the former is for
  // indices like the global invocation, while the latter is a value.
  Const = “Const”, // arg = value
  Special = “Special”, // arg = [variable, n]
  Variable = “Variable”, // arg = variable
  GlobalIndex = “GlobalIndex”, // arg = [gid, len]; src = [bufidx]
  GlobalView = “GlobalView”, // arg = [gid, ShapeTracker], src = [indices...]
}&lt;/code&gt;
    &lt;p&gt;When auto-generating GPU kernels, they’re pretty simple for pointwise ops. The tricky part is if there’s a reduction (aka. tensor contraction), most commonly in matmuls and convolutions. These can be optimized pretty well on the web by unrolling judiciously and tiling the loads/stores.&lt;/p&gt;
    &lt;p&gt;An example WebGPU matmul kernel for &lt;code&gt;float32[4096,4096]&lt;/code&gt; matrices generated by jax-js is shown below.&lt;/p&gt;
    &lt;code&gt;@group(0) @binding(0) var&amp;lt;storage, read&amp;gt; in0 : array&amp;lt;f32&amp;gt;;
@group(0) @binding(1) var&amp;lt;storage, read&amp;gt; in1 : array&amp;lt;f32&amp;gt;;
@group(0) @binding(2) var&amp;lt;storage, read_write&amp;gt; result : array&amp;lt;f32&amp;gt;;

@compute @workgroup_size(256)
fn main(@builtin(global_invocation_id) id : vec3&amp;lt;u32&amp;gt;) {
  if (id.x &amp;gt;= 1048576) { return; }
  let gidx: i32 = i32(id.x);
  var acc0: f32 = f32(0);
  var acc1: f32 = f32(0);
  var acc2: f32 = f32(0);
  var acc3: f32 = f32(0);
  var acc4: f32 = f32(0);
  var acc5: f32 = f32(0);
  var acc6: f32 = f32(0);
  var acc7: f32 = f32(0);
  var acc8: f32 = f32(0);
  var acc9: f32 = f32(0);
  var acc10: f32 = f32(0);
  var acc11: f32 = f32(0);
  var acc12: f32 = f32(0);
  var acc13: f32 = f32(0);
  var acc14: f32 = f32(0);
  var acc15: f32 = f32(0);
  for (var ridx: i32 = 0; ridx &amp;lt; 1024; ridx++) {
    let x0: i32 = ((gidx / 8192) * 131072) + ((((gidx / 8) % 8) * 16384) + (ridx * 4));
    let x1: f32 = in0[x0];
    let x2: i32 = (((gidx / 64) % 128) * 32) + (((gidx % 8) * 4) + (ridx * 16384));
    let x3: f32 = in1[x2];
    let x4: f32 = in0[x0 + 1];
    let x6: f32 = in0[x0 + 2];
    let x8: f32 = in0[x0 + 3];
    let x10: f32 = in0[x0 + 4096];
    let x11: f32 = in0[x0 + 4097];
    let x12: f32 = in0[x0 + 4098];
    let x13: f32 = in0[x0 + 4099];
    let x14: f32 = in0[x0 + 8192];
    let x15: f32 = in0[x0 + 8193];
    let x16: f32 = in0[x0 + 8194];
    let x17: f32 = in0[x0 + 8195];
    let x18: f32 = in0[x0 + 12288];
    let x19: f32 = in0[x0 + 12289];
    let x20: f32 = in0[x0 + 12290];
    let x21: f32 = in0[x0 + 12291];
    let x22: f32 = in1[x2 + 1];
    let x26: f32 = in1[x2 + 2];
    let x30: f32 = in1[x2 + 3];
    let x5: f32 = in1[x2 + 4096];
    let x23: f32 = in1[x2 + 4097];
    let x27: f32 = in1[x2 + 4098];
    let x31: f32 = in1[x2 + 4099];
    let x7: f32 = in1[x2 + 8192];
    let x24: f32 = in1[x2 + 8193];
    let x28: f32 = in1[x2 + 8194];
    let x32: f32 = in1[x2 + 8195];
    let x9: f32 = in1[x2 + 12288];
    let x25: f32 = in1[x2 + 12289];
    let x29: f32 = in1[x2 + 12290];
    let x33: f32 = in1[x2 + 12291];
    acc0 += x1 * x3 + x4 * x5 + x6 * x7 + x8 * x9;
    acc1 += x10 * x3 + x11 * x5 + x12 * x7 + x13 * x9;
    acc2 += x14 * x3 + x15 * x5 + x16 * x7 + x17 * x9;
    acc3 += x18 * x3 + x19 * x5 + x20 * x7 + x21 * x9;
    acc4 += x1 * x22 + x4 * x23 + x6 * x24 + x8 * x25;
    acc5 += x10 * x22 + x11 * x23 + x12 * x24 + x13 * x25;
    acc6 += x14 * x22 + x15 * x23 + x16 * x24 + x17 * x25;
    acc7 += x18 * x22 + x19 * x23 + x20 * x24 + x21 * x25;
    acc8 += x1 * x26 + x4 * x27 + x6 * x28 + x8 * x29;
    acc9 += x10 * x26 + x11 * x27 + x12 * x28 + x13 * x29;
    acc10 += x14 * x26 + x15 * x27 + x16 * x28 + x17 * x29;
    acc11 += x18 * x26 + x19 * x27 + x20 * x28 + x21 * x29;
    acc12 += x1 * x30 + x4 * x31 + x6 * x32 + x8 * x33;
    acc13 += x10 * x30 + x11 * x31 + x12 * x32 + x13 * x33;
    acc14 += x14 * x30 + x15 * x31 + x16 * x32 + x17 * x33;
    acc15 += x18 * x30 + x19 * x31 + x20 * x32 + x21 * x33;
  }
  let x34: i32 = ((gidx / 8192) * 131072) + ((((gidx / 64) % 128) * 32) + ((((gidx / 8) % 8) * 16384) + ((gidx % 8) * 4)));
  result[x34] = acc0;
  result[x34 + 4096] = acc1;
  result[x34 + 8192] = acc2;
  result[x34 + 12288] = acc3;
  result[x34 + 1] = acc4;
  result[x34 + 4097] = acc5;
  result[x34 + 8193] = acc6;
  result[x34 + 12289] = acc7;
  result[x34 + 2] = acc8;
  result[x34 + 4098] = acc9;
  result[x34 + 8194] = acc10;
  result[x34 + 12290] = acc11;
  result[x34 + 3] = acc12;
  result[x34 + 4099] = acc13;
  result[x34 + 8195] = acc14;
  result[x34 + 12291] = acc15;
}&lt;/code&gt;
    &lt;p&gt;If you’re writing a native library, this isn’t good enough. For example, you have to at least use tensor cores &lt;code&gt;mma.sync.aligned.*&lt;/code&gt; on Nvidia GPUs! But on the web, it gets to pretty comparable performance with the best open-source libraries, and it seems that Dawn is alright at bridging any gaps with optimization.&lt;/p&gt;
    &lt;p&gt;Onto the frontend. This is the core of the library, and where the actual autograd and tracing happens. We follow the JAX design quite closely, where there is a set of primitives along with an ambient interpreter stack. This is… quite difficult, magical, and took me a while to figure out. To learn more see:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The simple essence of automatic differentiation (Elliott 2018)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(One particularly cool moment about this way of building an ML library is that you get reverse-mode AD “for free” by inverting/transposing the forward-mode rules. I found this really beautiful after I wrapped my head around it; it’s quite mathematically pleasing. Another cool moment is when you first get arbitrary 2nd, 3rd, … n-th order derivatives after just implementing the first-order derivative rules — GradientTape could never!)&lt;/p&gt;
    &lt;p&gt;Honestly this is probably the most lost I’ve ever felt in writing code. It’s like, nested mutually recursive interpreters to model functors in the “category of tensors.”&lt;/p&gt;
    &lt;p&gt;Anyway, once I reviewed my differential geometry notes from college and dusted off my understanding of tangents, pulling back cotangents, functors and so on, I think I eventually figured it out. Though I still had tiny bugs for the next 6 months. 😂&lt;/p&gt;
    &lt;p&gt;The list of high-level &lt;code&gt;Primitive&lt;/code&gt; in jax-js is below:&lt;/p&gt;
    &lt;code&gt;/**
 * Frontend primitive operations, which are lowered into Kernel objects before
 * being dispatched to the backend.
 *
 * Any operation between arrays can be described in these parts. This is also
 * the set of primitives that can occur in Jaxpr programs, and the level at
 * which transformations like vmap, grad, and jvp occur. They are loosely based
 * on [XLA](https://openxla.org/xla/operation_semantics).
 *
 * All n-ary operations support broadcasting, with NumPy semantics.
 */
export enum Primitive {
  Add = “add”,
  Mul = “mul”,
  Idiv = “idiv”,
  Neg = “neg”,
  Reciprocal = “reciprocal”,
  StopGradient = “stop_gradient”,
  Cast = “cast”,
  Bitcast = “bitcast”,
  RandomBits = “random_bits”,
  Sin = “sin”,
  Cos = “cos”,
  Asin = “asin”,
  Atan = “atan”,
  Exp = “exp”,
  Log = “log”,
  Erf = “erf”,
  Erfc = “erfc”,
  Sqrt = “sqrt”,
  Min = “min”,
  Max = “max”,
  Reduce = “reduce”,
  Dot = “dot”, // sum(x*y, axis=-1)
  Conv = “conv”, // see lax.conv_general_dilated
  Pool = “pool”,
  PoolTranspose = “pool_transpose”,
  Compare = “compare”,
  Where = “where”,
  Transpose = “transpose”,
  Broadcast = “broadcast”,
  Reshape = “reshape”,
  Flip = “flip”,
  Shrink = “shrink”,
  Pad = “pad”,
  Gather = “gather”,
  JitCall = “jit_call”,
}&lt;/code&gt;
    &lt;p&gt;Notice that many of these are similar to the backend operations above, but some are different. In particular, there are convolutions and matrix multiplications here. These are useful to see in the frontend IR (and for autograd) but can be lowered to a simpler form before the kernels are generated on the backend.&lt;/p&gt;
    &lt;p&gt;By default, an operation is just lowered directly to a backend kernel after passing through any necessary transformations (&lt;code&gt;vmap&lt;/code&gt;, &lt;code&gt;jvp&lt;/code&gt;, &lt;code&gt;grad&lt;/code&gt;). But if you’re using the &lt;code&gt;jit&lt;/code&gt;, jax-js will trace your program to produce a “Jaxpr” (list of operations) followed by automatic kernel fusion to generate kernels, specialized to each input shape.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bugs&lt;/head&gt;
    &lt;p&gt;It’s very hard to build an ML framework and a long task! So far, jax-js has implemented a lot of core functionality in JAX, but there’s still much more. If there’s an API or operation you want to see, please consider adding it or filing an issue (examples: np.split, FFT, AdamW).&lt;/p&gt;
    &lt;p&gt;I have a pretty varied, portable test suite that runs fast:&lt;/p&gt;
    &lt;p&gt;So we are in a good position to find bugs and fix them. But making an ML library is quite difficult, and WebGPU is a nascent technology (e.g., I somehow gave my MacBook kernel panics)—there will be bugs! Please report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Technical: Performance&lt;/head&gt;
    &lt;p&gt;We haven’t spent a ton of time optimizing yet, but performance is generally pretty good. &lt;code&gt;jit&lt;/code&gt; is very helpful for fusing operations together, and it’s a feature only available on the web in jax-js. The default kernel-tuning heuristics get about 3000 GFLOP/s for matrix multiplication on an M4 Pro chip (try it).&lt;/p&gt;
    &lt;p&gt;On that specific benchmark, it’s actually more GFLOP/s than both TensorFlow.js and ONNX, which both use handwritten libraries of custom kernels (versus jax-js, which generates kernels with an ML compiler).&lt;/p&gt;
    &lt;p&gt;Some particularly useful / low-hanging fruit to look at:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The WebAssembly backend currently is quite simple, I didn’t spend a ton of time optimizing it, but measurably it could be &amp;gt;150x faster on my MacBook Pro. This difference comes from a few things multiplying:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Don’t recompute loop indices each time, we could improve FLOPs by ~1-3x.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Do loop unrolling/tiling, will improve FLOPs by ~2-3x.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Use SIMD instructions. This would improve FLOPs by 4x.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Add multi-threading (10x on my laptop), to use all available cores. Requires SharedArrayBuffer (crossOriginIsolated) / there are some caveats here to sync/async handling, needs to be done carefully.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Running the forward pass of the MobileCLIP2 transformer model is only about 1/3 the FLOPs compared to pure 4096x4096 matmul. Maybe we can improve this, especially in the causal self-attention layer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Although WebGPU is rapidly gaining in popularity and support, it’s probably worth having a WebGL backend as well, as a fallback that’s guaranteed to work in pretty much all browsers and is still pretty fast. This isn’t a huge amount of work; the WebGPU backend is &amp;lt;700 lines of code for example.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Technical: Feature parity&lt;/head&gt;
    &lt;p&gt;jax-js strives for approximate API compatibility with the JAX python library (and through that, NumPy). But some features vary for a few reasons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Data model: jax-js has ownership of arrays using the&lt;/p&gt;&lt;code&gt;.ref&lt;/code&gt;system, which obviates the need for APIs like&lt;code&gt;jit()&lt;/code&gt;‘s&lt;code&gt;donate_argnums&lt;/code&gt;and&lt;code&gt;numpy.asarray()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Language primitives: JavaScript has no named arguments, so method call signatures may take objects instead of Python’s keyword arguments. Also, PyTrees are translated in spirit to “JsTree” in jax-js, but their specification is different.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Maturity: JAX has various types like&lt;/p&gt;&lt;code&gt;complex64&lt;/code&gt;, advanced functions like&lt;code&gt;hessenberg()&lt;/code&gt;, and advanced higher-order features like&lt;code&gt;lax.while_loop()&lt;/code&gt;that we haven’t implemented. Some of these are not easy to implement on GPU.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other features just aren’t implemented yet. But those can probably be added easily!&lt;/p&gt;
    &lt;p&gt;I’ve made a table of every JAX library feature and its implementation status in jax-js, see here. There are a couple big ones that stand out.&lt;/p&gt;
    &lt;p&gt;You’re welcome to contribute, though I’d also love if you could try using jax-js. :D&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46516267</guid><pubDate>Tue, 06 Jan 2026 18:19:31 +0000</pubDate></item></channel></rss>