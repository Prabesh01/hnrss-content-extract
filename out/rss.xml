<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 12 Dec 2025 06:53:08 +0000</lastBuildDate><item><title>The architecture of “not bad”: Decoding the Chinese source code of the void</title><link>https://suggger.substack.com/p/the-architecture-of-not-bad-decoding</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46231709</guid><pubDate>Thu, 11 Dec 2025 14:21:14 +0000</pubDate></item><item><title>iPhone Typos? It's Not Just You – The iOS Keyboard Is Broken [video]</title><link>https://www.youtube.com/watch?v=hksVvXONrIo</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46232528</guid><pubDate>Thu, 11 Dec 2025 15:25:43 +0000</pubDate></item><item><title>Show HN: Sim – Apache-2.0 n8n alternative</title><link>https://github.com/simstudioai/sim</link><description>&lt;doc fingerprint="189ff03c2161b5fe"&gt;
  &lt;main&gt;
    &lt;p&gt;Build and deploy AI agent workflows in minutes.&lt;/p&gt;
    &lt;p&gt;Design agent workflows visually on a canvas—connect agents, tools, and blocks, then run them instantly.&lt;/p&gt;
    &lt;p&gt;Leverage Copilot to generate nodes, fix errors, and iterate on flows directly from natural language.&lt;/p&gt;
    &lt;p&gt;Upload documents to a vector store and let agents answer questions grounded in your specific content.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cloud-hosted: sim.ai&lt;/head&gt;
    &lt;code&gt;npx simstudio&lt;/code&gt;
    &lt;p&gt;Docker must be installed and running on your machine.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Flag&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;-p, --port &amp;lt;port&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Port to run Sim on (default &lt;code&gt;3000&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--no-pull&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Skip pulling latest Docker images&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/simstudioai/sim.git

# Navigate to the project directory
cd sim

# Start Sim
docker compose -f docker-compose.prod.yml up -d&lt;/code&gt;
    &lt;p&gt;Access the application at http://localhost:3000/&lt;/p&gt;
    &lt;p&gt;Run Sim with local AI models using Ollama - no external APIs required:&lt;/p&gt;
    &lt;code&gt;# Start with GPU support (automatically downloads gemma3:4b model)
docker compose -f docker-compose.ollama.yml --profile setup up -d

# For CPU-only systems:
docker compose -f docker-compose.ollama.yml --profile cpu --profile setup up -d&lt;/code&gt;
    &lt;p&gt;Wait for the model to download, then visit http://localhost:3000. Add more models with:&lt;/p&gt;
    &lt;code&gt;docker compose -f docker-compose.ollama.yml exec ollama ollama pull llama3.1:8b&lt;/code&gt;
    &lt;p&gt;If you already have Ollama running on your host machine (outside Docker), you need to configure the &lt;code&gt;OLLAMA_URL&lt;/code&gt; to use &lt;code&gt;host.docker.internal&lt;/code&gt; instead of &lt;code&gt;localhost&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Docker Desktop (macOS/Windows)
OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d

# Linux (add extra_hosts or use host IP)
docker compose -f docker-compose.prod.yml up -d  # Then set OLLAMA_URL to your host's IP&lt;/code&gt;
    &lt;p&gt;Why? When running inside Docker, &lt;code&gt;localhost&lt;/code&gt; refers to the container itself, not your host machine. &lt;code&gt;host.docker.internal&lt;/code&gt; is a special DNS name that resolves to the host.&lt;/p&gt;
    &lt;p&gt;For Linux users, you can either:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use your host machine's actual IP address (e.g., &lt;code&gt;http://192.168.1.100:11434&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;extra_hosts: ["host.docker.internal:host-gateway"]&lt;/code&gt;to the simstudio service in your compose file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Sim also supports vLLM for self-hosted models with OpenAI-compatible API:&lt;/p&gt;
    &lt;code&gt;# Set these environment variables
VLLM_BASE_URL=http://your-vllm-server:8000
VLLM_API_KEY=your_optional_api_key  # Only if your vLLM instance requires auth&lt;/code&gt;
    &lt;p&gt;When running with Docker, use &lt;code&gt;host.docker.internal&lt;/code&gt; if vLLM is on your host machine (same as Ollama above).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open VS Code with the Remote - Containers extension&lt;/item&gt;
      &lt;item&gt;Open the project and click "Reopen in Container" when prompted&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;bun run dev:full&lt;/code&gt;in the terminal or use the&lt;code&gt;sim-start&lt;/code&gt;alias&lt;list rend="ul"&gt;&lt;item&gt;This starts both the main application and the realtime socket server&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bun runtime&lt;/item&gt;
      &lt;item&gt;PostgreSQL 12+ with pgvector extension (required for AI embeddings)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Sim uses vector embeddings for AI features like knowledge bases and semantic search, which requires the &lt;code&gt;pgvector&lt;/code&gt; PostgreSQL extension.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone and install dependencies:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/simstudioai/sim.git
cd sim
bun install&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up PostgreSQL with pgvector:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You need PostgreSQL with the &lt;code&gt;vector&lt;/code&gt; extension for embedding support. Choose one option:&lt;/p&gt;
    &lt;p&gt;Option A: Using Docker (Recommended)&lt;/p&gt;
    &lt;code&gt;# Start PostgreSQL with pgvector extension
docker run --name simstudio-db \
  -e POSTGRES_PASSWORD=your_password \
  -e POSTGRES_DB=simstudio \
  -p 5432:5432 -d \
  pgvector/pgvector:pg17&lt;/code&gt;
    &lt;p&gt;Option B: Manual Installation&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install PostgreSQL 12+ and the pgvector extension&lt;/item&gt;
      &lt;item&gt;See pgvector installation guide&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up environment:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cd apps/sim
cp .env.example .env  # Configure with required variables (DATABASE_URL, BETTER_AUTH_SECRET, BETTER_AUTH_URL)&lt;/code&gt;
    &lt;p&gt;Update your &lt;code&gt;.env&lt;/code&gt; file with the database URL:&lt;/p&gt;
    &lt;code&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set up the database:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, configure the database package environment:&lt;/p&gt;
    &lt;code&gt;cd packages/db
cp .env.example .env &lt;/code&gt;
    &lt;p&gt;Update your &lt;code&gt;packages/db/.env&lt;/code&gt; file with the database URL:&lt;/p&gt;
    &lt;code&gt;DATABASE_URL="postgresql://postgres:your_password@localhost:5432/simstudio"&lt;/code&gt;
    &lt;p&gt;Then run the migrations:&lt;/p&gt;
    &lt;code&gt;bunx drizzle-kit migrate --config=./drizzle.config.ts&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start the development servers:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Recommended approach - run both servers together (from project root):&lt;/p&gt;
    &lt;code&gt;bun run dev:full&lt;/code&gt;
    &lt;p&gt;This starts both the main Next.js application and the realtime socket server required for full functionality.&lt;/p&gt;
    &lt;p&gt;Alternative - run servers separately:&lt;/p&gt;
    &lt;p&gt;Next.js app (from project root):&lt;/p&gt;
    &lt;code&gt;bun run dev&lt;/code&gt;
    &lt;p&gt;Realtime socket server (from &lt;code&gt;apps/sim&lt;/code&gt; directory in a separate terminal):&lt;/p&gt;
    &lt;code&gt;cd apps/sim
bun run dev:sockets&lt;/code&gt;
    &lt;p&gt;Copilot is a Sim-managed service. To use Copilot on a self-hosted instance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to https://sim.ai → Settings → Copilot and generate a Copilot API key&lt;/item&gt;
      &lt;item&gt;Set &lt;code&gt;COPILOT_API_KEY&lt;/code&gt;environment variable in your self-hosted apps/sim/.env file to that value&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Key environment variables for self-hosted deployments (see &lt;code&gt;apps/sim/.env.example&lt;/code&gt; for full list):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Variable&lt;/cell&gt;
        &lt;cell role="head"&gt;Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;DATABASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;PostgreSQL connection string with pgvector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BETTER_AUTH_SECRET&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Auth secret (&lt;code&gt;openssl rand -hex 32&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BETTER_AUTH_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Your app URL (e.g., &lt;code&gt;http://localhost:3000&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;NEXT_PUBLIC_APP_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Public app URL (same as above)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;ENCRYPTION_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Encryption key (&lt;code&gt;openssl rand -hex 32&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;OLLAMA_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Ollama server URL (default: &lt;code&gt;http://localhost:11434&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;VLLM_BASE_URL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;vLLM server URL for self-hosted models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;COPILOT_API_KEY&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;API key from sim.ai for Copilot features&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you're running Ollama on your host machine and Sim in Docker, change &lt;code&gt;OLLAMA_URL&lt;/code&gt; from &lt;code&gt;localhost&lt;/code&gt; to &lt;code&gt;host.docker.internal&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;OLLAMA_URL=http://host.docker.internal:11434 docker compose -f docker-compose.prod.yml up -d&lt;/code&gt;
    &lt;p&gt;See Using an External Ollama Instance for details.&lt;/p&gt;
    &lt;p&gt;Ensure PostgreSQL has the pgvector extension installed. When using Docker, wait for the database to be healthy before running migrations.&lt;/p&gt;
    &lt;p&gt;If ports 3000, 3002, or 5432 are in use, configure alternatives:&lt;/p&gt;
    &lt;code&gt;# Custom ports
NEXT_PUBLIC_APP_URL=http://localhost:3100 POSTGRES_PORT=5433 docker compose up -d&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Framework: Next.js (App Router)&lt;/item&gt;
      &lt;item&gt;Runtime: Bun&lt;/item&gt;
      &lt;item&gt;Database: PostgreSQL with Drizzle ORM&lt;/item&gt;
      &lt;item&gt;Authentication: Better Auth&lt;/item&gt;
      &lt;item&gt;UI: Shadcn, Tailwind CSS&lt;/item&gt;
      &lt;item&gt;State Management: Zustand&lt;/item&gt;
      &lt;item&gt;Flow Editor: ReactFlow&lt;/item&gt;
      &lt;item&gt;Docs: Fumadocs&lt;/item&gt;
      &lt;item&gt;Monorepo: Turborepo&lt;/item&gt;
      &lt;item&gt;Realtime: Socket.io&lt;/item&gt;
      &lt;item&gt;Background Jobs: Trigger.dev&lt;/item&gt;
      &lt;item&gt;Remote Code Execution: E2B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We welcome contributions! Please see our Contributing Guide for details.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the Apache License 2.0 - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Made with ❤️ by the Sim Team&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234186</guid><pubDate>Thu, 11 Dec 2025 17:20:11 +0000</pubDate></item><item><title>Litestream VFS</title><link>https://fly.io/blog/litestream-vfs/</link><description>&lt;doc fingerprint="67cf1ef5a02c7b1b"&gt;
  &lt;main&gt;
    &lt;p&gt;Iâm Ben Johnson, and I work on Litestream at Fly.io. Litestream is the missing backup/restore system for SQLite. Itâs free, open-source software that should run anywhere, and you can read more about it here.&lt;/p&gt;
    &lt;p&gt;Again with the sandwiches: assume we’ve got a SQLite database of sandwich ratings, and we’ve backed it up with Litestream to an S3 bucket.&lt;/p&gt;
    &lt;p&gt;Now, on our local host, load up AWS credentials and an S3 path into our environment. Open SQLite and:&lt;/p&gt;
    &lt;code&gt;$ sqlite3
SQLite version 3.50.4 2025-07-30 19:33:53
sqlite&amp;gt; .load litestream.so
sqlite&amp;gt; .open file:///my.db?vfs=litestream
&lt;/code&gt;
    &lt;p&gt;SQLite is now working from that remote database, defined by the Litestream backup files in the S3 path we configured. We can query it:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
22|Veggie Delight|New York|4
30|Meatball|Los Angeles|5
168|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;This is Litestream VFS. It runs SQLite hot off an object storage URL. As long as you can load the shared library our tree builds for you, it’ll work in your application the same way it does in the SQLite shell.&lt;/p&gt;
    &lt;p&gt;Fun fact: we didn’t have to download the whole database to run this query. More about this in a bit.&lt;/p&gt;
    &lt;p&gt;Meanwhile, somewhere in prod, someone has it in for meatball subs and wants to knock them out of the bracket â oh, fuck:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; UPDATE sandwich_ratings SET stars = 1 ;
&lt;/code&gt;
    &lt;p&gt;They forgot the &lt;code&gt;WHERE&lt;/code&gt; clause!&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; SELECT * FROM sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
97|French Dip|Los Angeles|1
140|BÃ¡nh MÃ¬|San Francisco|1
62|Italian Beef|Chicago|1
&lt;/code&gt;
    &lt;p&gt;Italian Beefs and BÃ¡nh MÃ¬s, all at 1 star. Disaster!&lt;/p&gt;
    &lt;p&gt;But wait, back on our dev machine:&lt;/p&gt;
    &lt;code&gt;sqlite&amp;gt; PRAGMA litestream_time = '5 minutes ago'; 
sqlite&amp;gt; select * from sandwich_ratings ORDER BY RANDOM() LIMIT 3 ; 
30|Meatball|Los Angeles|5
33|Ham &amp;amp; Swiss|Los Angeles|2
163|Chicken Shawarma Wrap|Detroit|5
&lt;/code&gt;
    &lt;p&gt;We’re now querying that database from a specific point in time in our backups. We can do arbitrary relative timestamps, or absolute ones, like &lt;code&gt;2000-01-01T00:00:00Z&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;What we’re doing here is instantaneous point-in-time recovery (PITR), expressed simply in SQL and SQLite pragmas.&lt;/p&gt;
    &lt;p&gt;Ever wanted to do a quick query against a prod dataset, but didn’t want to shell into a prod server and fumble with the &lt;code&gt;sqlite3&lt;/code&gt; terminal command like a hacker in an 80s movie? Or needed to do a quick sanity check against yesterday’s data, but without doing a full database restore? Litestream VFS makes that easy. I’m so psyched about how it turned out.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;Litestream v0.5 integrates LTX, our SQLite data-shipping file format. Where earlier Litestream blindly shipped whole raw SQLite pages to and from object storage, LTX ships ordered sets of pages. We built LTX for LiteFS, which uses a FUSE filesystem to do transaction-aware replication for unmodified applications, but we’ve spent this year figuring out ways to use LTX in Litestream, without all that FUSE drama.&lt;/p&gt;
    &lt;p&gt;The big thing LTX gives us is “compaction”. When we restore a database from object storage, we want the most recent versions of each changed database page. What we don’t want are all the intermediate versions of those pages that occurred prior to the most recent change.&lt;/p&gt;
    &lt;p&gt;Imagine, at the time we’re restoring, we’re going to need pages 1, 2, 3, 4, and 5. Depending on the order in which pages were written, the backup data set might look something like &lt;code&gt;1 2 3 5 3 5 4 5 5&lt;/code&gt;. What we want is the rightmost  5, 4, 3, 2, and 1, without wasting time on the four “extra” page 5’s and the one “extra” page 3. Those “extra” pages are super common in SQLite data sets; for instance, every busy table with an autoincrementing primary key will have them.&lt;/p&gt;
    &lt;p&gt;LTX lets us skip the redundant pages, and the algorithm is trivial: reading backwards from the end of the sequence, skipping any page you already read. This drastically accelerates restores.&lt;/p&gt;
    &lt;p&gt;But LTX compaction isn’t limited to whole databases. We can also LTX-compact sets of LTX files. That’s the key to how PITR restores with Litestream now work.&lt;/p&gt;
    &lt;p&gt;In the diagram below, we’re taking daily full snapshots. Below those snapshots are “levels” of changesets: groups of database pages from smaller and smaller windows of time. By default, Litestream uses time intervals of 1 hour at the highest level, down to 30 seconds at level 1. L0 is a special level where files are uploaded every second, but are only retained until being compacted to L1.&lt;/p&gt;
    &lt;p&gt;Now, let’s do a PITR restore. Start from the most proximal snapshot. Then determine the minimal set of LTX files from each level to reach the time you are restoring to.&lt;/p&gt;
    &lt;p&gt;We have another trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;LTX trailers include a small index tracking the offset of each page in the file. By fetching only these index trailers from the LTX files we’re working with (each occupies about 1% of its LTX file), we can build a lookup table of every page in the database. Since modern object storage providers all let us fetch slices of files, we can perform individual page reads against S3 directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It’s Implemented&lt;/head&gt;
    &lt;p&gt;SQLite has a plugin interface for things like this: the “VFS” interface. VFS plugins abstract away the bottom-most layer of SQLite, the interface to the OS. If you’re using SQLite now, you’re already using some VFS module, one SQLite happens to ship with.&lt;/p&gt;
    &lt;p&gt;For Litestream users, there’s a catch. From the jump, we’ve designed Litestream to run alongside unmodified SQLite applications. Part of what makes Litestream so popular is that your apps don’t even need to know it exists. It’s “just” a Unix program.&lt;/p&gt;
    &lt;p&gt;That Litestream Unix program still does PITR restores, without any magic. But to do fast PITR-style queries straight off S3, we need more. To make those queries work, you have to load and register Litestream’s VFS module.&lt;/p&gt;
    &lt;p&gt;But that’s all that changes.&lt;/p&gt;
    &lt;p&gt;In particular: Litestream VFS doesn’t replace the SQLite library you’re already using. It’s not a new “version” of SQLite. It’s just a plugin for the SQLite you’re already using.&lt;/p&gt;
    &lt;p&gt;Still, we know that’s not going to work for everybody, and even though we’re really psyched about these PITR features, we’re not taking our eyes off the ball on the rest of Litestream. You don’t have to use our VFS library to use Litestream, or to get the other benefits of the new LTX code.&lt;/p&gt;
    &lt;p&gt;The way a VFS library works, we’re given just a couple structures, each with a bunch of methods defined on them. We override only the few methods we care about. Litestream VFS handles only the read side of SQLite. Litestream itself, running as a normal Unix program, still handles the “write” side. So our VFS subclasses just enough to find LTX backups and issue queries.&lt;/p&gt;
    &lt;p&gt;With our VFS loaded, whenever SQLite needs to read a page into memory, it issues a &lt;code&gt;Read()&lt;/code&gt; call through our library. The read call includes the byte offset at which SQLite expected to find the page. But with Litestream VFS, that byte offset is an illusion.&lt;/p&gt;
    &lt;p&gt;Instead, we use our knowledge of the page size along with the requested page number to do a lookup on the page index we’ve built. From it, we get the remote filename, the “real” byte offset into that file, and the size of the page. That’s enough for us to use the S3 API’s &lt;code&gt;Range&lt;/code&gt; header handling to download exactly the block we want.&lt;/p&gt;
    &lt;p&gt;To save lots of S3 calls, Litestream VFS implements an LRU cache. Most databases have a small set of “hot” pages â inner branch pages or the leftmost leaf pages for tables with an auto-incrementing ID field. So only a small percentage of the database is updated and queried regularly.&lt;/p&gt;
    &lt;p&gt;Weâve got one last trick up our sleeve.&lt;/p&gt;
    &lt;p&gt;Quickly building an index and restore plan for the current state of a database is cool. But we can do one better.&lt;/p&gt;
    &lt;p&gt;Because Litestream backs up (into the L0 layer) once per second, the VFS code can simply poll the S3 path, and then incrementally update its index. The result is a near-realtime replica. Better still, you donât need to stream the whole database back to your machine before you use it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Eat Your Heart Out, Marty McFly&lt;/head&gt;
    &lt;p&gt;Litestream holds backup files for every state your database has been in, with single-second resolution, for as long as you want it to. Forgot the &lt;code&gt;WHERE&lt;/code&gt; clause on a &lt;code&gt;DELETE&lt;/code&gt; statement? Updating your database state to where it was an hour (or day, or week) ago is just a matter of adjusting the LTX indices Litestream manages.&lt;/p&gt;
    &lt;p&gt;All this smoke-and-mirrors of querying databases without fully fetching them has another benefit: it starts up really fast! We’re living an age of increasingly ephemeral servers, what with the AIs and the agents and the clouds and the hoyvin-glavins. Wherever you find yourself, if your database is backed up to object storage with Litestream, you’re always in a place where you can quickly issue a query.&lt;/p&gt;
    &lt;p&gt;As always, one of the big things we think we’re doing right with Litestream is: we’re finding ways to get as much whiz-bang value as we can (instant PITR reading live off object storage: pretty nifty!) while keeping the underlying mechanism simple enough that you can fit your head around it.&lt;/p&gt;
    &lt;p&gt;Litestream is solid for serious production use (we rely on it for important chunks of our own Fly.io APIs). But you could write Litestream yourself, just from the basic ideas in these blog posts. We think that’s a point in its favor. We land there because the heavy lifting in Litestream is being done by SQLite itself, which is how it should be.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234710</guid><pubDate>Thu, 11 Dec 2025 17:59:10 +0000</pubDate></item><item><title>GPT-5.2</title><link>https://openai.com/index/introducing-gpt-5-2/</link><description>&lt;doc fingerprint="285fa2d92df7ce9f"&gt;
  &lt;main&gt;
    &lt;p&gt;We are introducing GPT‑5.2, the most capable model series yet for professional knowledge work.&lt;/p&gt;
    &lt;p&gt;Already, the average ChatGPT Enterprise user says AI saves them 40–60 minutes a day, and heavy users say it saves them more than 10 hours a week. We designed GPT‑5.2 to unlock even more economic value for people; it’s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long contexts, using tools, and handling complex, multi-step projects.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 sets a new state of the art across many benchmarks, including GDPval, where it outperforms industry professionals at well-specified knowledge work tasks spanning 44 occupations.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 Thinking&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GDPval (wins or ties)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;70.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;38.8% (GPT‑5)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Bench Pro (public)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;55.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;50.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-bench Verified&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;76.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPQA Diamond (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;92.4%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.1%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;CharXiv Reasoning (w/ Python)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;88.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;80.3%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;AIME 2025 (no tools)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;100.0%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;94.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 1–3)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;40.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;31.0%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;FrontierMath (Tier 4)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;14.6%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;12.5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-1 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;86.2%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;72.8%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ARC-AGI-2 (Verified)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;52.9%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;17.6%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notion(opens in a new window), Box(opens in a new window), Shopify(opens in a new window), Harvey(opens in a new window) and Zoom(opens in a new window) observed GPT‑5.2 demonstrates state-of-the-art long-horizon reasoning and tool-calling performance. Databricks(opens in a new window), Hex(opens in a new window) and Triple Whale(opens in a new window) found GPT‑5.2 to be exceptional at agentic data science and document analysis tasks. Cognition(opens in a new window), Warp(opens in a new window), Charlie Labs(opens in a new window), JetBrains(opens in a new window) and Augment Code(opens in a new window) say GPT‑5.2 delivers state-of-the-art agentic coding performance, with measurable improvements in areas such as interactive coding, code reviews and bug finding.&lt;/p&gt;
    &lt;p&gt;In ChatGPT, GPT‑5.2 Instant, Thinking, and Pro will begin rolling out today, starting with paid plans. In the API, they are available now to all developers.&lt;/p&gt;
    &lt;p&gt;Overall, GPT‑5.2 brings significant improvements in general intelligence, long-context understanding, agentic tool-calling, and vision—making it better at executing complex, real-world tasks end-to-end than any previous model.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is the best model yet for real-world, professional use. On GDPval, an eval measuring well-specified knowledge work tasks across 44 occupations, GPT‑5.2 Thinking sets a new state-of-the-art score, and is our first model that performs at or above a human expert level. Specifically, GPT‑5.2 Thinking beats or ties top industry professionals on 70.9% of comparisons on GDPval knowledge work tasks, according to expert human judges. These tasks include making presentations, spreadsheets, and other artifacts. GPT‑5.2 Thinking produced outputs for GDPval tasks at &amp;gt;11x the speed and &amp;lt;1% the cost of expert professionals, suggesting that when paired with human oversight, GPT‑5.2 can help with professional work. Speed and cost estimates are based on historical metrics; speed in ChatGPT may vary.&lt;/p&gt;
    &lt;p&gt;When reviewing one especially good output, one GDPval judge commented, "It is an exciting and noticeable leap in output quality... [it] appears to have been done by a professional company with staff, and has a surprisingly well designed layout and advice for both deliverables, though with one we still have some minor errors to correct."&lt;/p&gt;
    &lt;p&gt;Additionally, on our internal benchmark of junior investment banking analyst spreadsheet modeling tasks—such as putting together a three-statement model for a Fortune 500 company with proper formatting and citations, or building a leveraged buyout model for a take-private—GPT 5.2 Thinking's average score per task is 9.3% higher than GPT‑5.1’s, rising from 59.1% to 68.4%.&lt;/p&gt;
    &lt;p&gt;Side-by-side comparisons show improved sophistication and formatting in spreadsheets and slides generated by GPT‑5.2 Thinking:&lt;/p&gt;
    &lt;p&gt;To use the new spreadsheet and presentation capabilities in ChatGPT, you must be on a Plus, Pro, Business, or Enterprise plan and select either GPT‑5.2 Thinking or Pro. Complex generations can take many minutes to produce.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art of 55.6% on SWE-Bench Pro, a rigorous evaluation of real-world software engineering. Unlike SWE-bench Verified, which only tests Python, SWE-Bench Pro tests four languages and aims to be more contamination-resistant, challenging, diverse, and industrially relevant.&lt;/p&gt;
    &lt;p&gt;On SWE-bench Verified (not plotted), GPT‑5.2 Thinking scores our new high of 80%.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this translates into a model that can more reliably debug production code, implement feature requests, refactor large codebases, and ship fixes end-to-end with less manual intervention.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is also better at front-end software engineering than GPT‑5.1 Thinking. Early testers found it significantly stronger at front-end development and complex or unconventional UI work—especially involving 3D elements—making it a powerful daily partner for engineers across the stack. See a few examples of what it can produce from a single prompt:&lt;/p&gt;
    &lt;p&gt;Early testers shared their feedback on GPT‑5.2’s coding capabilities:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 represents the biggest leap for GPT models in agentic coding since GPT-5 and is a SOTA coding model in its price range. The version bump undersells the jump in intelligence. We’re excited to make it the default across Windsurf and several core Devin workloads."&lt;/quote&gt;
    &lt;p&gt;GPT‑5.2 Thinking hallucinates less than GPT‑5.1 Thinking. On a set of de-identified queries from ChatGPT, responses with errors were 30%rel less common. For professionals, this means fewer mistakes when using the model for research, writing, analysis, and decision support—making the model more dependable for everyday knowledge work.&lt;/p&gt;
    &lt;p&gt;Like all models, GPT‑5.2 Thinking is imperfect. For anything critical, double check its answers.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking sets a new state of the art in long-context reasoning, achieving leading performance on OpenAI MRCRv2—an evaluation that tests a model’s ability to integrate information spread across long documents. On real-world tasks like deep document analysis, which require related information across hundreds of thousands of tokens, GPT‑5.2 Thinking is substantially more accurate than GPT‑5.1 Thinking. In particular, it’s the first model we’ve seen that achieves near 100% accuracy on the 4-needle MRCR variant (out to 256k tokens).&lt;/p&gt;
    &lt;p&gt;In practical terms, this enables professionals to use GPT‑5.2 to work with long documents—such as reports, contracts, research papers, transcripts, and multi-file projects—while maintaining coherence and accuracy across hundreds of thousands of tokens. This makes GPT‑5.2 especially well suited for deep analysis, synthesis, and complex multi-source workflows.&lt;/p&gt;
    &lt;p&gt;For tasks that benefit from thinking beyond the maximum context window, GPT‑5.2 Thinking is compatible with our new Responses &lt;code&gt;/compact&lt;/code&gt; endpoint, which extends the model’s effective context window. This lets GPT‑5.2 Thinking tackle more tool-heavy, long-running workflows that would otherwise be limited by context length. Read more in our API documentation(opens in a new window).&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is our strongest vision model yet, cutting error rates roughly in half on chart reasoning and software interface understanding.&lt;/p&gt;
    &lt;p&gt;For everyday professional use, this means the model can more accurately interpret dashboards, product screenshots, technical diagrams, and visual reports—supporting workflows in finance, operations, engineering, design, and customer support where visual information is central.&lt;/p&gt;
    &lt;p&gt;Compared to previous models, GPT‑5.2 Thinking has a stronger grasp of how elements are positioned within an image, which helps on tasks where relative layout plays a key role in solving the problem. In the example below, we ask the model to identify the components in an image input (in this case, a motherboard) and return labels with approximate bounding boxes. Even on a low-quality image, GPT‑5.2 identifies the main regions and places boxes that sometimes match the true locations of each component, while GPT‑5.1 only labels a few parts and shows a much weaker understanding of their spatial arrangement. Both models make clear mistakes, but GPT‑5.2 shows better comprehension of the image.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;GPT‑5.2 Thinking achieves a new state of the art of 98.7% on Tau2-bench Telecom, demonstrating its ability to reliably use tools across long, multi-turn tasks.&lt;/p&gt;
    &lt;p&gt;For latency-sensitive use cases, GPT‑5.2 Thinking also performs much better at reasoning.effort='none', substantially outperforming GPT‑5.1 and GPT‑4.1.&lt;/p&gt;
    &lt;p&gt;For professionals, this translates into stronger end-to-end workflows—such as resolving customer support cases, pulling data from multiple systems, running analyses, and generating final outputs with fewer breakdowns between steps.&lt;/p&gt;
    &lt;p&gt;For example, when asking a complex customer service question that requires multi-step resolution, the model can more effectively coordinate a full workflow across multiple agents. In the case below, a traveler reports a delayed flight, a missed connection, an overnight stay in New York, and a medical seating requirement. GPT‑5.2 manages the entire chain of tasks—rebooking, special-assistance seating, and compensation—delivering a more complete outcome than GPT‑5.1.&lt;/p&gt;
    &lt;head rend="h5"&gt;GPT-5.1&lt;/head&gt;
    &lt;head rend="h5"&gt;GPT-5.2&lt;/head&gt;
    &lt;p&gt;One of our hopes for AI is that it will accelerate scientific research for the benefit of everyone. Toward this, we’ve been working with and listening to scientists to see how AI can speed up their work, and last month we shared some early collaborative experiments here.&lt;/p&gt;
    &lt;p&gt;We believe GPT‑5.2 Pro and GPT‑5.2 Thinking are the world’s best models for assisting and accelerating scientists. On GPQA Diamond, a graduate-level Google-proof Q&amp;amp;A benchmark, GPT‑5.2 Pro achieves 93.2%, followed closely by GPT‑5.2 Thinking at 92.4%.&lt;/p&gt;
    &lt;p&gt;On FrontierMath (Tier 1–3), an evaluation of expert-level mathematics, GPT‑5.2 Thinking set a new state of the art, solving 40.3% of problems.&lt;/p&gt;
    &lt;p&gt;We're beginning to see AI models meaningfully accelerate progress in math and science in tangible ways. For example, in recent work with GPT‑5.2 Pro, researchers explored an open question in statistical learning theory. In a narrow, well-specified setting, the model proposed a proof that was subsequently verified by the authors and reviewed with external experts, illustrating how frontier models can assist mathematical research under close human oversight.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-1 (Verified), a benchmark designed to measure general reasoning ability, GPT‑5.2 Pro is the first model to cross the 90% threshold, improving from 87%(opens in a new window) by o3‑preview last year while reducing the cost of achieving that performance by roughly 390×.&lt;/p&gt;
    &lt;p&gt;On ARC-AGI-2 (Verified), which raises the difficulty and better isolates fluid reasoning, GPT‑5.2 Thinking achieves a new state of the art for chain-of-thought models, scoring 52.9%. GPT‑5.2 Pro performs even higher, reaching 54.2%, further extending the model’s ability to reason through novel, abstract problems.&lt;/p&gt;
    &lt;p&gt;Improvements across these evaluations reflect GPT‑5.2’s stronger multi-step reasoning, greater quantitative accuracy, and more reliable problem solving on complex technical tasks.&lt;/p&gt;
    &lt;p&gt;Here’s what our early testers say about GPT‑5.2:&lt;/p&gt;
    &lt;quote&gt;"GPT-5.2 unlocked a complete architecture shift for us. We collapsed a fragile, multi-agent system into a single mega-agent with 20+ tools. The best part is, it just works. The mega-agent is faster, smarter, and 100x easier to maintain. We’re seeing dramatically lower latency, much stronger tool calling, and we no longer need sprawling system prompts because 5.2 will execute cleanly off a simple, one-line prompt. It feels like pure magic."&lt;/quote&gt;
    &lt;p&gt;In ChatGPT, users should notice GPT‑5.2 feels better to use day to day—more structured, more reliable, and still enjoyable to talk to.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Instant is a fast, capable workhorse for everyday work and learning, with clear improvements in info-seeking questions, how-tos and walk-throughs, technical writing, and translation, building on the warmer conversational tone introduced in GPT‑5.1 Instant. Early testers particularly noted clearer explanations that surface key information upfront.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Thinking is designed for deeper work, helping users tackle more complex tasks with greater polish—especially for coding, summarizing long documents, answering questions about uploaded files, working through math and logic step by step, and supporting planning and decisions with clearer structure and more useful detail.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 Pro is our smartest and most trustworthy option for difficult questions where a higher-quality answer is worth the wait, with early testing showing fewer major errors and stronger performance in complex domains like programming.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 builds on the safe completion research we introduced with GPT‑5, which teaches the model to give the most helpful answer while still staying within safety boundaries.&lt;/p&gt;
    &lt;p&gt;With this release, we continued our work to strengthen our models’ responses in sensitive conversations, with meaningful improvements in how they respond to prompts indicating signs of suicide or self harm, mental health distress, or emotional reliance on the model. These targeted interventions have resulted in fewer undesirable responses in both GPT‑5.2 Instant and GPT‑5.2 Thinking as compared to GPT‑5.1 and GPT‑5 Instant and Thinking models. Further details can be found in the system card.&lt;/p&gt;
    &lt;p&gt;We’re in the early stages of rolling out our age prediction model so that we can automatically apply content protections for users who are under 18, in order to limit access to sensitive content. This builds on our existing approach to users we know are under 18 and our parental controls.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is one step in an ongoing series of improvements, and we’re far from done. While this release delivers meaningful gains in intelligence and productivity, we know there are areas where people want more. In ChatGPT, we’re working on known issues like over-refusals, while continuing to raise the bar on safety and reliability overall. These changes are complex, and we’re focused on getting them right.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1 &lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Mental health&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.995&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.883&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.915&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.684&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emotional reliance&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.945&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.955&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.785&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Self-harm&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.938&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.925&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.963&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;0.937&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In ChatGPT, we’ll begin rolling out GPT‑5.2 (Instant, Thinking, and Pro) today, starting with paid plans (Plus, Pro, Go, Business, Enterprise). We deploy GPT‑5.2 gradually to keep ChatGPT as smooth and reliable as we can; if you don’t see it at first, please try again later. In ChatGPT, GPT‑5.1 will still be available to paid users for three months under legacy models, after which we will sunset GPT‑5.1.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;API&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Instant&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2-chat-latest&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Thinking&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;ChatGPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.2 Pro&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In our API Platform, GPT‑5.2 Thinking is available today in the Responses API and Chat Completions API as &lt;code&gt;gpt-5.2&lt;/code&gt;, and GPT‑5.2 Instant as &lt;code&gt;gpt-5.2-chat-latest&lt;/code&gt;. GPT‑5.2 Pro is available in the Responses API as &lt;code&gt;gpt-5.2-pro&lt;/code&gt;. Developers can now set the reasoning parameter in GPT‑5.2 Pro, and both GPT‑5.2 Pro and GPT‑5.2 Thinking now support the new fifth reasoning effort of xhigh, for tasks where quality is most important.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 is priced at $1.75/1M input tokens and $14/1M output tokens, with a 90% discount on cached inputs. On multiple agentic evals, we found that despite GPT‑5.2’s greater cost per token, the cost of attaining a given level of quality ended up less expensive due to GPT‑5.2’s greater token efficiency.&lt;/p&gt;
    &lt;p&gt;While ChatGPT subscription pricing remains the same, in the API GPT‑5.2 is priced higher per token than GPT‑5.1 because it is a more capable model. It’s still priced below other frontier models, so people can continue to use it deeply in their daily work and core applications.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Model&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cached input&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Output&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.75&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.175&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$14&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.2-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$21&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$168&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5.1 / &lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$1.25&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$0.125&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$10&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;gpt-5-pro&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$15&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;-&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$120&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We have no current plans to deprecate GPT‑5.1, GPT‑5, or GPT‑4.1 in the API and will communicate any deprecation plans with ample advance notice for developers. While GPT‑5.2 will work well out of the box in Codex, we expect to release a version of GPT‑5.2 optimized for Codex in the coming weeks.&lt;/p&gt;
    &lt;p&gt;GPT‑5.2 was built in collaboration with our long-standing partners NVIDIA and Microsoft. Azure data centers and NVIDIA GPUs, including H100, H200, and GB200-NVL72, underpin OpenAI’s at-scale training infrastructure, driving significant gains in model intelligence. Together, this collaboration allows us to scale compute with confidence and bring new models to market more quickly.&lt;/p&gt;
    &lt;p&gt;Below, we report comprehensive benchmark scores for GPT‑5.2 Thinking, along with a subset for GPT‑5.2 Pro.&lt;/p&gt;
    &lt;head rend="h5"&gt;Professional&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, wins or ties)&lt;/cell&gt;
        &lt;cell&gt;70.9%&lt;/cell&gt;
        &lt;cell&gt;74.1%&lt;/cell&gt;
        &lt;cell&gt;38.8% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (ties allowed, clear wins)&lt;/cell&gt;
        &lt;cell&gt;49.8%&lt;/cell&gt;
        &lt;cell&gt;60.0%&lt;/cell&gt;
        &lt;cell&gt;35.5% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GDPval (no ties)&lt;/cell&gt;
        &lt;cell&gt;61.0%&lt;/cell&gt;
        &lt;cell&gt;67.6%&lt;/cell&gt;
        &lt;cell&gt;37.1% (GPT-5)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Investment banking spreadsheet tasks (internal)&lt;/cell&gt;
        &lt;cell&gt;68.4%&lt;/cell&gt;
        &lt;cell&gt;71.7%&lt;/cell&gt;
        &lt;cell&gt;59.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Coding&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-Bench Pro, Public&lt;/cell&gt;
        &lt;cell&gt;55.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;80.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SWE-Lancer, IC Diamond*&lt;/cell&gt;
        &lt;cell&gt;74.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;69.7%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Factuality&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ChatGPT answers without errors (w/ search)&lt;/cell&gt;
        &lt;cell&gt;93.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;91.2%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatGPT answers without errors (no search)&lt;/cell&gt;
        &lt;cell&gt;88.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;87.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Long context&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 4k–8k&lt;/cell&gt;
        &lt;cell&gt;98.2%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;65.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 8k–16k&lt;/cell&gt;
        &lt;cell&gt;89.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;47.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 16k–32k&lt;/cell&gt;
        &lt;cell&gt;95.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 32k–64k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 64k–128k&lt;/cell&gt;
        &lt;cell&gt;85.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;OpenAI MRCRv2, 8 needles, 128k–256k&lt;/cell&gt;
        &lt;cell&gt;77.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;29.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 128k&lt;/cell&gt;
        &lt;cell&gt;92.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;90.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp Long Context 256k&lt;/cell&gt;
        &lt;cell&gt;89.8%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GraphWalks bfs &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;76.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Graphwalks parents &amp;lt;128k&lt;/cell&gt;
        &lt;cell&gt;89.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;71.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Vision&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (no tools)&lt;/cell&gt;
        &lt;cell&gt;82.1%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;67.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CharXiv reasoning (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;88.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;80.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (no tools)&lt;/cell&gt;
        &lt;cell&gt;79.5%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMU Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;80.4%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;79.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video MMMU (no tools)&lt;/cell&gt;
        &lt;cell&gt;85.9%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;82.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Screenspot Pro (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;86.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;64.2%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Tool usage&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Telecom&lt;/cell&gt;
        &lt;cell&gt;98.7%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;95.6%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tau2-bench Retail&lt;/cell&gt;
        &lt;cell&gt;82.0%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;65.8%&lt;/cell&gt;
        &lt;cell&gt;77.9%&lt;/cell&gt;
        &lt;cell&gt;50.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Scale MCP-Atlas&lt;/cell&gt;
        &lt;cell&gt;60.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;44.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Toolathlon&lt;/cell&gt;
        &lt;cell&gt;46.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;36.1%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Academic&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GPQA Diamond (no tools)&lt;/cell&gt;
        &lt;cell&gt;92.4%&lt;/cell&gt;
        &lt;cell&gt;93.2%&lt;/cell&gt;
        &lt;cell&gt;88.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (no tools)&lt;/cell&gt;
        &lt;cell&gt;34.5%&lt;/cell&gt;
        &lt;cell&gt;36.6%&lt;/cell&gt;
        &lt;cell&gt;25.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HLE (w/ search, Python)&lt;/cell&gt;
        &lt;cell&gt;45.5%&lt;/cell&gt;
        &lt;cell&gt;50.0%&lt;/cell&gt;
        &lt;cell&gt;42.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;MMMLU&lt;/cell&gt;
        &lt;cell&gt;89.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;89.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HMMT, Feb 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;99.4%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;96.3%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;AIME 2025 (no tools)&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;100.0%&lt;/cell&gt;
        &lt;cell&gt;94.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;FrontierMath Tier 1–3 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;40.3%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;31.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;FrontierMath Tier 4 (w/ Python)&lt;/cell&gt;
        &lt;cell&gt;14.6%&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;12.5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h5"&gt;Abstract reasoning&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;GPT-5.2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.2 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ARC-AGI-1 (Verified)&lt;/cell&gt;
        &lt;cell&gt;86.2%&lt;/cell&gt;
        &lt;cell&gt;90.5%&lt;/cell&gt;
        &lt;cell&gt;72.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ARC-AGI-2 (Verified)&lt;/cell&gt;
        &lt;cell&gt;52.9%&lt;/cell&gt;
        &lt;cell&gt;54.2% (high)&lt;/cell&gt;
        &lt;cell&gt;17.6%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Models were run with maximum available reasoning effort in our API (xhigh for GPT‑5.2 Thinking &amp;amp; Pro, and high for GPT‑5.1 Thinking), except for the professional evals, where GPT‑5.2 Thinking was run with reasoning effort heavy, the maximum available in ChatGPT Pro. Benchmarks were conducted in a research environment, which may provide slightly different output from production ChatGPT in some cases.&lt;/p&gt;
    &lt;p&gt;* For SWE-Lancer, we omit 40/237 problems that did not run on our infrastructure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234788</guid><pubDate>Thu, 11 Dec 2025 18:04:47 +0000</pubDate></item><item><title>Programmers and software developers lost the plot on naming their tools</title><link>https://larr.net/p/namings.html</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234806</guid><pubDate>Thu, 11 Dec 2025 18:06:42 +0000</pubDate></item><item><title>Rivian Unveils Custom Silicon, R2 Lidar Roadmap, and Universal Hands Free</title><link>https://riviantrackr.com/news/rivian-unveils-custom-silicon-r2-lidar-roadmap-universal-hands-free-and-its-next-gen-autonomy-platform/</link><description>&lt;doc fingerprint="993313b0ba26c9c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Rivian Unveils Custom Silicon, R2 LiDAR Roadmap, Universal Hands Free, and Its Next Gen Autonomy Platform&lt;/head&gt;
    &lt;p&gt;RJ opened the first ever Autonomy and AI Day explaining why Rivian believes it is positioned to lead in this next phase of the industry. The company is leaning hard into compute, custom hardware, large scale AI systems, and a shared data foundation that touches every part of the ownership experience.&lt;/p&gt;
    &lt;p&gt;Let’s break it all down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meet the Rivian Autonomy Processor&lt;/head&gt;
    &lt;p&gt;One of the biggest announcements was RAP1, Rivian’s first in house processor built on a 5nm multi chip module. It delivers 1600 sparse INT8 TOPS and can push 5 billion pixels per second inside the new Gen 3 Autonomy Computer. Rivian even built its own AI compiler and platform software to support it. This shows Rivian is no longer just integrating off the shelf chips, it is now designing silicon specifically for its autonomy roadmap.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy Computer and LiDAR on R2&lt;/head&gt;
    &lt;p&gt;The ACM3 (Autonomy Compute Module 3) autonomy computer will debut on R2 starting at the end of 2026, but Rivian made it clear that R2 will launch initially without LiDAR. What Rivian confirmed today is that LiDAR will be added later in the program. This lines up with what we explored back in May when we spotted early signs that Rivian was evaluating LiDAR as a redundancy and ground truth layer for future autonomy. Rivian has now officially validated that LiDAR is coming to R2 down the road, where it will join cameras and radar to create a richer, more resilient perception stack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Large Driving Model and Rivian’s Data Loop&lt;/head&gt;
    &lt;p&gt;Rivian explained how its autonomy stack is powered by a self improving data loop feeding the company’s Large Driving Model, which is trained similarly to an LLM. Reinforcement learning distills high quality driving behavior into efficient onboard models. Every release improves the system, and Rivian laid out a trajectory that moves toward point to point, eyes off and eventually personal Level 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;Universal Hands Free Coming to Gen 2&lt;/head&gt;
    &lt;p&gt;Rivian confirmed that a major software update will bring Universal Hands Free to Gen 2 R1T and R1S. This hands free experience will cover over 3.5 million miles of roads across the US and Canada as long as there are clearly painted lane lines. It is a huge expansion of the assisted driving envelope for current owners.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autonomy+ Sub Launching in 2026&lt;/head&gt;
    &lt;p&gt;Rivian also announced Autonomy+, an autonomy tier with continuously expanding features launching early 2026.&lt;/p&gt;
    &lt;p&gt;Pricing is $2,500 one time or $49.99 per month.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rivian Unified Intelligence&lt;/head&gt;
    &lt;p&gt;Rivian is reorganizing its entire platform around Rivian Unified Intelligence, a data foundation that ties together telemetry, cloud models, service systems and customer facing features. It is the backbone for predictive maintenance, smarter diagnostics and upcoming AI driven tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rivian Assistant Coming in 2026&lt;/head&gt;
    &lt;p&gt;Rivian also officially unveiled its new Rivian Assistant, a next generation voice experience arriving early 2026 on Gen 1 and Gen 2 R1 vehicles. The assistant uses a blend of edge models and in vehicle intelligence to understand your schedule, recognize context, and handle everyday requests.&lt;/p&gt;
    &lt;p&gt;On R2, it will even run fully offline thanks to a more powerful infotainment computer, reducing latency and keeping more of the experience on device.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI Powered Service and Diagnostics&lt;/head&gt;
    &lt;p&gt;Rivian is embedding AI into the service workflow. Technicians will have access to an AI driven expert system that analyzes telemetry and vehicle history to pinpoint issues faster and more accurately. These same tools will eventually power the mobile app as well, making self service diagnostics significantly smarter.&lt;/p&gt;
    &lt;head rend="h3"&gt;36 Comments&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Any mention of retrofitting Autonomy Computer and LiDAR onto existing R1’s? Hopefully at least Gen 2’s!&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Will not happen&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;I doubt it’ll happen, they want you to buy a new car.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I need this R2D2 themed R2 🔥&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I watched the event. It felt like I was at an apple event. Is Rivian wanting us to trade in ours cars every two years to get a newer version? A new chip set, better camera, or a faster processor?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;I’ve always been a vehicle buyer, not leaser. But these rapid tech changes definitely have me looking at leasing an R1S gen 2 so I can unload it once the R1’s get lidar.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Personally I want to do the driving. I like driving and don’t want to put my life in the hands of anyone’s computer!&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;No, they want you to subscribe to their services to get a revenue stream from something that’s actually useful and you can use. Unlike GM, Toyota, etc who try to block carplay so they can keep the diagnostics but give crap software and infotainment which is only marginally improved if you spend a fortune on their upscale, overpriced remakes of lower end models (camry =&amp;gt; lexus equivalent).&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So with everything we know do we think we can expect automatic lane change before point to point? Was kind of expecting auto park and auto lane change to be announced with universal hands free&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Universal Hands Free when? Did they give any indication?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Soon. So probably 6-9 months.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;S0––––0N&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Pretty sure the presentations said universal will in “an update later this month.”&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;They posted on Instagram, it will come on next software update!&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sad day for Gen 1 users except the Rivian Assistant part……&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is disappointing. An assistant that can integrate with Google calendar 🤷♂️. Who cares?!?&lt;/p&gt;
        &lt;p&gt;Maybe some finds this valuable but I would rather have apple carplay.&lt;/p&gt;
        &lt;p&gt;People listen to music in their cars. Why not make that the first app? Who decided “let’s invest 2 years and make Google calendar the first app”?&lt;/p&gt;
        &lt;p&gt;Rivian is clueless.&lt;/p&gt;
        &lt;p&gt;I have owned 2 of these and won’t buy a third one.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Rivian is trying to do everything, but they really need to consider people’s lives beyond the vehicle. I set up my phone, my home, my computer, my TV, and now my car too? Google and apple have texting and music covered, just let people’s phone do the work and not make the car yet “assistant” to set up. Focus on self driving. Let the car simply connect to phones for phone stuff.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Integrating with an AI is awesome! I don’t get why people want Carplay so bad–Rivian can do a better job integrating their car functions with Spotify than Apple can. I don’t want two operating systems running at once competing for control over the vehicle’s screens. Tesla’s is slightly more refined, but Rivian’s works fine and software is one of their core competencies.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sad as Gen 1 – should have had at least something to improve Driver-&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It will be interesting to see which features end up behind the paywall. Will we only end up with adaptive cruse control and center lane assist?&lt;/p&gt;
        &lt;p&gt;Also curious about the $2500 for Autonomy+. I wonder if that will follow a person/family to future Rivian vehicles. If so that would make it more enticing (also if it then included Connect+ as well).&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;Found the answer once Rivian website was updated:&lt;/p&gt;&lt;lb/&gt;“Autonomy+ one-time purchases remain with the vehicle upon ownership transfer, and Autonomy+ product features remain available during the lifetime of feature support for the hardware on the vehicle at delivery”&lt;p&gt;So unfortunately it follows the vehicle and not the person.&lt;/p&gt;&lt;p&gt;While I appreciate the fact that continuous development costs money, I just really don’t like subscriptions that much. Due really to the amount of time I would spend not even using the feature. So for this reason I sort of favor the more lifetime buy option…. Main use case for me is really trips/longer drives. Which are definitely less than monthly. I don’t think I see myself getting in the car and paying $50 at the start of a long weekend trip for the sort term feature rental. Oh well 🙂&lt;/p&gt;&lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Wondered the exact same thing. $2500 pays for itself after about 4 years. If it stays with the user, on all their Rivians over time, great!&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You all be thankful that your Gen 1s have gotten meaningful updates and backwards compatible updates from Gen 2. Be thankful you don’t have a BMW EV from 2022 with iDrive 8, not even a year later they were outdated by the next iteration of iDrive, 8.5 and BMW claims features (even software based) from 8.5 are not backwards compatible with iDrive 8.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;I know that buying a legacy vehicle like BMW is going to be static. Rivian said we were buying a software driven vehicle with updates (on par with Tesla). Tesla brings updates for years to older vehicles. I agree if we are talking a 5-6 year old car that we’re hitting the wall… but not a car 2 years old that’s marketed in a very different way.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Well with Gen1, just because a certain MY was purchased does not mean the platform was also updated for that model. Gen1 has been riding on an autonomous platform from 2019 so it’s well into its 6-7 year life span. It also lacks the hardware needed to achieve anything beyond what it can hardly do today. The lack of radar alone is what holds it back, on top of its constrained processing power and low res cameras.&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So the new processing unit will be out late 2026. Is that when LIDAR is also expected or is LIDAR further out?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;So R2 will launch without Lidar. But will R2 launch with Gen2 Autonomy hardware? They were clear that Gen3 Autonomy hardware is “late 2026”. But I went through it again, and didn’t see any clear statement on this. If R2 must wait for Gen3 Autonomy hardware, we won’t be seeing R2 any time s00n…&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Read between the lines….&lt;/p&gt;
            &lt;p&gt;R2 will have all of this stuff, lidar etc….&lt;/p&gt;
            &lt;p&gt;“gen 3 chip will be out late 2026”.&lt;/p&gt;
            &lt;p&gt;If R2 will have all the features, then that means its pushed late 2026. They just didnt to come out and say R2 is delayed.&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Yup, that’s the way I read it as well. Of course saying that explicitly would have really rained on the parade.&lt;/p&gt;
                &lt;list rend="ul"&gt;
                  &lt;item&gt;&lt;p&gt;According to this article, R2 will launch with Gen2 ACM, then later bring in Gen3 ACM. Best case, an early R2 with Gen2 ACM is upgradable to Gen3 ACM. Worst case, getting R2 early will mean forgoing Gen3 ACM features like Lidar, eyes-off and Personal L4.&lt;/p&gt;&lt;lb/&gt;RJ *did* mention that R2 would launch without Lidar – maybe that implied the Gen2 ACM.&lt;p&gt;https://www.mercurynews.com/2025/12/11/rivian-unveils-ai-chip-for-automated-driving-ditches-nvidia/&lt;/p&gt;&lt;/item&gt;
                &lt;/list&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Was there any hint of the Dec update that was to include RAD tuner?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I’m trying to figure out what are we getting on the next update. From the videos of the demos, it appears to be navigating towards a destination making turns on its own using turn signals. Is that universal hands-free or is that a preview of point to point?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Pretty sure that was a preview of point to point&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Where’s the adventure in autonomy? I for one don’t care if my vehicle can drive by itself. Who can really 100% trust such a claim?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This did nothing but reaffirm my lessons learned from buying gen1 R1T – will never buy another vehicle from Rivian – R1, R2, R3, etc. until they are on Gen 5 or higher… which at this pace… they’ll be there in next 4-6 years. Also doesn’t sound like any update on text message integration reading this update from Jose… so it feels like it will never happen for Gen 1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;They did a whole segment live about Rivian Assistant which is coming to Gen 1 and 2 which will have text messaging capabilities they demonstrated it live.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46234920</guid><pubDate>Thu, 11 Dec 2025 18:17:19 +0000</pubDate></item><item><title>An SVG is all you need</title><link>https://jon.recoil.org/blog/2025/12/an-svg-is-all-you-need.html</link><description>&lt;doc fingerprint="3dc38b5d0be4eb8d"&gt;
  &lt;main&gt;
    &lt;p&gt;SVGs are pretty cool - vector graphics in a simple XML format. They are supported on just about every device and platform, are crisp on every display, and can have embedded scripts in to make them interactive. They're way more capable than many people realise, and I think we can capitalise on some of that unrealised potential.&lt;/p&gt;
    &lt;p&gt;Anil's recent post Four Ps for Building Massive Collective Knowledge Systems got me thinking about the permanence of the experimentation that underlies our scientific papers. In my idealistic vision of how scientific publishing should work, each paper would be accompanied by a fully interactive environment where the reader could explore the data, rerun the experiments, tweak the parameters, and see how the results changed. Obviously we can't do this in the general case - some experiments are just too expensive or time-consuming to rerun on demand. But for many papers, especially in computer science, this is entirely feasible.&lt;/p&gt;
    &lt;p&gt;That line of thought reminded me of a project I tackled about 20 years ago as a post-doc in the Department of Plant Sciences here in Cambridge. I was writing a paper on synergy in fungal networks and built a tiny SVG visualisation tool that let readers wander through the raw data captured from a real fungal network growing in a petri dish. I dug it up recently and was surprised (and delighted) to see that it still works perfectly in modern browsers - even though the original âcover pageâ suggested Firefox 1.5 or the Adobe SVG plug-in (!). Give it a spin; click the 'forward', 'back' and other buttons below the petri dish!&lt;/p&gt;
    &lt;p&gt;And that, dear reader, is literally all you need. A completely self-contained SVG file can either fetch data from a versioned repository or embed the data directly, as the example does. It can process that data, generate visualisations, and render knobs and sliders for interactive exploration. No server-side magic required - everything runs client-side in the browser, served by a plain static web server, and very easily to share.&lt;/p&gt;
    &lt;p&gt;How does it fit in with Anil's four Ps?&lt;/p&gt;
    &lt;p&gt;The SVG above is only a visualisation tool for data; it doesn't really do any processing, but it certainly could. The biggest change that's happened over the 20 years since I wrote this is the massive increase in the computation power available in the browser. If would be entirely feasible to implement the entire data analysis pipeline for that paper in an SVG today, probably without even spinning up the fans on my laptop!&lt;/p&gt;
    &lt;p&gt;So this is yet another tool in our ongoing effort to be able to effortlessly share and remix our work - added to the pile of Jupyter notebooks, Marimo botebooks, the slipshow/x-ocaml combination, Patrick's take on Jon Sterling's Forester, my own notebooks, and many others - and this is a subset of what we're using just in our own group!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46235959</guid><pubDate>Thu, 11 Dec 2025 19:25:14 +0000</pubDate></item><item><title>My productivity app is a never-ending .txt file (2020)</title><link>https://jeffhuang.com/productivity_text_file/</link><description>&lt;doc fingerprint="2555483b29260ef2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Over 14 years of todos recorded in text&lt;/head&gt;
    &lt;head rend="h2"&gt;My productivity app is a never-ending .txt file&lt;/head&gt;
    &lt;head rend="h3"&gt;By Jeff Huang, updated on 2022-03-21&lt;/head&gt;
    &lt;p&gt;The biggest transition for me when I started college was learning to get organized. There was a point when I couldn't just remember everything in my head. And having to constantly keep track of things was distracting me from whatever task I was doing at the moment.&lt;/p&gt;
    &lt;p&gt;So I tried various forms of todo lists, task trackers, and productivity apps. They were all discouraging because the things to do kept getting longer, and there were too many interrelated things like past meeting notes, calendar appointments, idea lists, and lab notebooks, which were all on different systems.&lt;/p&gt;
    &lt;p&gt;I gave up and started just tracking in a single text file and have been using it as my main productivity system for 14 years now. It is so essential to my work now, and has surprisingly scaled with a growing set of responsibilities, that I wanted to share this system. It's been my secret weapon.&lt;/p&gt;
    &lt;p&gt;Prerequisite: A calendar. The one outside tool I use is an online calendar, and I put everything on this calendar, even things that aren't actually for a fixed time like "make a coffee table at the workshop" or "figure out how to recruit new PhD students" — I'll schedule them on a date when I want to think about it. That way all my future plans and schedule are together, and not a bunch of lists I have to keep track of.&lt;/p&gt;
    &lt;p&gt;Making the Daily List: Every night before I go to bed, I take all the items on my calendar for the next day and append it to the end of the text file as a daily todo list, so I know exactly what I'm doing when I wake up. This list contains scheduled tasks (2pm meeting with Madonna, 4pm office hours), errands (sign a form, return a book), and work items (review a paper, prepare a presentation). It also lets me think about whether I've got the right amount of work for a day.&lt;/p&gt;
    &lt;p&gt;Anything I don't want to do tomorrow, I'll shuffle back into my calendar on later dates. If the task is too big, I'll break it down into a piece for tomorrow, and the rest for another date. After years of doing this, I've gotten pretty good at estimating what I can finish in a day. Here's an example with names replaced so you can see what it looks like when I move a day's schedule from my calendar.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;2021-11-31
11am meet with Head TAs
- where are things at with inviting portfolio reviewers?
11:30am meet with student Enya (interested in research)
review and release A/B Testing assignment grading
12pm HCI group meeting
- vote for lab snacks
send reminders for CHI external reviewers
read Sketchy draft
Zelda pick up eye tracker
- have her sign for it
update biosketch for Co-PI
3:15pm join call with Umbrella Corp and industry partnership staff
3:45pm advising meet with Oprah
4pm Rihanna talk (368 CIT)
5pm 1:1 with Beyonce #phdadvisee
6pm faculty interview dinner with Madonna
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;As a Record: That daily todo list is where I also take notes, so it's a to do list that turns into a what done list. The best thing about these daily lists is I keep them all in a single text file separated by dates, so I have a record of everything I have ever done and when I did it.&lt;/p&gt;
    &lt;p&gt;My current file was created 9 years ago when I started my current job. It serves as a research notebook, and as meeting minutes. I have 51,690 handwritten lines in one file now, documenting everything I have done as a professor, and nearly every person I have met with, along with notes about what we discussed or ideas I had. Here's what my list looks like at the end of the day, representing work accomplished.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;2021-11-31
11am meet with Head TAs
- where are things at with inviting portfolio reviewers? A: got 7/29 replies
- need 3 TAs for Thursday lab
- Redesign assignment handout will be done by Monday, ship Thursday
11:30am meet with student Enya (interested in research)
- they're a little inexperienced, suggested applying next year
review and release A/B Testing assignment grading
12pm HCI group meeting
- automatically generate thumbnails from zoom behavior on web pages
- #idea subliminal audio that leads you to dream about websites
- Eminem presenting Nov 24
- vote for lab snacks. A: popcorn and seaweed thing
got unofficial notification ARO YIP funding award #annual #cv
read Sketchy paper draft
- needs 1 more revision
- send to Gandalf to look at?
Zelda pick up eye tracker
- have her sign for it
update biosketch for Co-PI
unexpected drop in from Coolio! #alumni
- now a PM working on TravelAdvisor, thinking about applying to grad school
3:15pm join call with Umbrella Corp and industry partnership staff
- they want to hire 20 data science + SWE interns (year 3), 4 alums there as SWE
3:45pm advising meet with Oprah
- enjoyed CS 33
- interning at Facebook
4pm Rihanna talk (368 CIT)
5pm 1:1 with Beyonce #phdadvisee
- stuck on random graph generating crash
	- monitor memory/swap/disk?
	- ask Mario to help?
- got internship at MSR with Cher
	- start May 15 or 22
- will send me study design outline before next meeting
- interviewing Spartacus as potential RA for next semester
6pm faculty interview dinner with Madonna (Gracie's)
- ask about connection with computer vision
- cool visual+audio unsupervised comparison, thoughtful about missing data, would work with ugrads (?), likes biking, teach compvis + graphics
- vote #HIRE
#note maybe visit Monsters University next spring, Bono does related work
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Shortcuts and Features: I use a consistent writing style so things are easily searchable, with a few shorthands. When I search for "meet with", it shows that I have had over 3,000 scheduled meetings. I have some tags like #idea for new ideas to revisit when I want project ideas, #annual for things to put on my next annual report, #nextui for things to add the next time I run my next UI course.&lt;/p&gt;
    &lt;p&gt;A text file is incredibly flexible, and at any point, I can quickly glance to see what I've done that day and what's left. I usually keep an empty line between tasks completed and upcoming tasks. When a task is completed, I move the empty line. Any leftover tasks from the current day can go back into the calendar for when I may want to tackle it again, but that is rare because tasks were already sized into what I can do on that day. I can calculate aggregate statistics using the search box, or list all the lines containing a tag, and other operations using my text editor. I use Ultraedit because I'm familiar with it, but any text editor would have similar capabilities.&lt;/p&gt;
    &lt;p&gt;Email: Email is obviously a part of my workflow. Everyone has all sorts of productivity advice about handling it, but I find a simple flagging system is sufficient — flag Red if it's something I need to deal with, flag Orange if I need to deal with it eventually but requires some thinking or someone else to handle it, and flag Yellow for emails I send that I am waiting on a reply for, so I know to follow up later. I'll flag emails as they come in, whenever it's convenient.&lt;/p&gt;
    &lt;p&gt;At the end of the day, I'll do a quick review of the Orange and Yellows to see if any need to be followed up or should become Red. Some peoples' workflows revolve around obsessively cleaning their Inbox. I don't really care about keeping my inbox empty because then I feel like I have new work to do whenever email comes in.&lt;/p&gt;
    &lt;p&gt;So my daily routine looks like&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;look at the daily todo list I wrote last night to find out what I'm doing today&lt;/item&gt;
      &lt;item&gt;do scheduled things on that list during the day&lt;/item&gt;
      &lt;item&gt;when I have free (unscheduled) time, do the floating tasks on my list and work on Red-flagged emails at the end of the day&lt;/item&gt;
      &lt;item&gt;do a quick review of Orange/Yellow emails to see if they need any handling&lt;/item&gt;
      &lt;item&gt;copy the next day's calendar items to the bottom of the text file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This process has a few nice properties:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It's easy to immediately see what to do when I wake up&lt;/item&gt;
      &lt;item&gt;I don't need to remember in my head the things to do later (following up on emails, future tasks)&lt;/item&gt;
      &lt;item&gt;It's easy to recall what happened in the past and see how much I can actually accomplish in a day&lt;/item&gt;
      &lt;item&gt;There's no running "todo" list with items that keep pushed back day after day&lt;/item&gt;
      &lt;item&gt;I use Remote Desktop so everything is accessible from every device&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;My daily workload is completely under my control the night before; whenever I feel overwhelmed with my long-term commitments, I reduce it by aggressively unflagging emails, removing items from my calendar that I am no longer excited about doing, and reducing how much work I assign myself in the future.&lt;/p&gt;
    &lt;p&gt;It does mean sometimes I miss some questions or don't pursue an interesting research question, but helps me maintain a manageable workload.&lt;/p&gt;
    &lt;p&gt;So that's it. I would love to hear from you if you try my system, or have some ideas about it!&lt;/p&gt;
    &lt;head rend="h3"&gt;Also in this series&lt;/head&gt;
    &lt;p&gt;The Coronavirus pandemic has changed our sleep behavior&lt;/p&gt;
    &lt;p&gt;Extracting data from tracking devices by going to the cloud&lt;/p&gt;
    &lt;head rend="h3"&gt;Other articles I've written&lt;/head&gt;
    &lt;p&gt;Behind the scenes: the struggle for each paper to get published&lt;/p&gt;
    &lt;p&gt;This page is designed to last, a manifesto for preserving content on the web&lt;/p&gt;
    &lt;p&gt;Illustrative notes for obsessing over publishing aesthetics&lt;/p&gt;
    &lt;p&gt;CS Faculty Composition and Hiring Trends&lt;/p&gt;
    &lt;p&gt;Bias in Computer Science Rankings&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46236037</guid><pubDate>Thu, 11 Dec 2025 19:30:58 +0000</pubDate></item><item><title>Denial of service and source code exposure in React Server Components</title><link>https://react.dev/blog/2025/12/11/denial-of-service-and-source-code-exposure-in-react-server-components</link><description>&lt;doc fingerprint="b02bbf4aec53e947"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Denial of Service and Source Code Exposure in React Server Components&lt;/head&gt;
    &lt;p&gt;December 11, 2025 by The React Team&lt;/p&gt;
    &lt;p&gt;Security researchers have found and disclosed two additional vulnerabilities in React Server Components while attempting to exploit the patches in last week’s critical vulnerability.&lt;/p&gt;
    &lt;p&gt;These new vulnerabilities do not allow for Remote Code Execution. The patch for React2Shell remains effective at mitigating the Remote Code Execution exploit.&lt;/p&gt;
    &lt;p&gt;The new vulnerabilities are disclosed as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Denial of Service - High Severity: CVE-2025-55184 and CVE-2025-67779 (CVSS 7.5)&lt;/item&gt;
      &lt;item&gt;Source Code Exposure - Medium Severity: CVE-2025-55183 (CVSS 5.3)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We recommend upgrading immediately due to the severity of the newly disclosed vulnerabilities.&lt;/p&gt;
    &lt;p&gt;Further details of these vulnerabilities will be provided after the rollout of the fixes are complete.&lt;/p&gt;
    &lt;head rend="h2"&gt;Immediate Action Required&lt;/head&gt;
    &lt;p&gt;These vulnerabilities are present in the same packages and versions as CVE-2025-55182.&lt;/p&gt;
    &lt;p&gt;This includes versions 19.0.0, 19.0.1, 19.0.2, 19.1.0, 19.1.1, 19.1.2, 19.1.2, 19.2.0, 19.2.1 and 19.2.2 of:&lt;/p&gt;
    &lt;p&gt;Fixes were backported to versions 19.0.3, 19.1.4, and 19.2.3. If you are using any of the above packages please upgrade to any of the fixed versions immediately.&lt;/p&gt;
    &lt;p&gt;As before, if your app’s React code does not use a server, your app is not affected by these vulnerabilities. If your app does not use a framework, bundler, or bundler plugin that supports React Server Components, your app is not affected by these vulnerabilities.&lt;/p&gt;
    &lt;head rend="h3"&gt;Affected frameworks and bundlers&lt;/head&gt;
    &lt;p&gt;Some React frameworks and bundlers depended on, had peer dependencies for, or included the vulnerable React packages. The following React frameworks &amp;amp; bundlers are affected: next, react-router, waku, @parcel/rsc, @vite/rsc-plugin, and rwsdk.&lt;/p&gt;
    &lt;p&gt;Please see the instructions in the previous post for upgrade steps.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hosting Provider Mitigations&lt;/head&gt;
    &lt;p&gt;As before, we have worked with a number of hosting providers to apply temporary mitigations.&lt;/p&gt;
    &lt;p&gt;You should not depend on these to secure your app, and still update immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;React Native&lt;/head&gt;
    &lt;p&gt;For React Native users not using a monorepo or &lt;code&gt;react-dom&lt;/code&gt;, your &lt;code&gt;react&lt;/code&gt; version should be pinned in your &lt;code&gt;package.json&lt;/code&gt;, and there are no additional steps needed.&lt;/p&gt;
    &lt;p&gt;If you are using React Native in a monorepo, you should update only the impacted packages if they are installed:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;react-server-dom-webpack&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;react-server-dom-parcel&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;react-server-dom-turbopack&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is required to mitigate the security advisories, but you do not need to update &lt;code&gt;react&lt;/code&gt; and &lt;code&gt;react-dom&lt;/code&gt; so this will not cause the version mismatch error in React Native.&lt;/p&gt;
    &lt;p&gt;See this issue for more information.&lt;/p&gt;
    &lt;head rend="h2"&gt;High Severity: Denial of Service&lt;/head&gt;
    &lt;p&gt;CVEs: CVE-2025-55184 and CVE-2025-67779 Base Score: 7.5 (High)&lt;/p&gt;
    &lt;p&gt;Security researchers have discovered that a malicious HTTP request can be crafted and sent to any Server Functions endpoint that, when deserialized by React, can cause an infinite loop that hangs the server process and consumes CPU. Even if your app does not implement any React Server Function endpoints it may still be vulnerable if your app supports React Server Components.&lt;/p&gt;
    &lt;p&gt;This creates a vulnerability vector where an attacker may be able to deny users from accessing the product, and potentially have a performance impact on the server environment.&lt;/p&gt;
    &lt;p&gt;The patches published today mitigate by preventing the infinite loop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Medium Severity: Source Code Exposure&lt;/head&gt;
    &lt;p&gt;CVE: CVE-2025-55183 Base Score: 5.3 (Medium)&lt;/p&gt;
    &lt;p&gt;A security researcher has discovered that a malicious HTTP request sent to a vulnerable Server Function may unsafely return the source code of any Server Function. Exploitation requires the existence of a Server Function which explicitly or implicitly exposes a stringified argument:&lt;/p&gt;
    &lt;code&gt;'use server';&lt;/code&gt;
    &lt;p&gt;An attacker may be able to leak the following:&lt;/p&gt;
    &lt;code&gt;0:{"a":"$@1","f":"","b":"Wy43RxUKdxmr5iuBzJ1pN"}&lt;/code&gt;
    &lt;p&gt;The patches published today prevent stringifying the Server Function source code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3rd: Leak reported to Vercel and Meta Bug Bounty by Andrew MacPherson.&lt;/item&gt;
      &lt;item&gt;December 4th: Initial DoS reported to Meta Bug Bounty by RyotaK.&lt;/item&gt;
      &lt;item&gt;December 6th: Both issues confirmed by the React team, and the team began investigating.&lt;/item&gt;
      &lt;item&gt;December 7th: Initial fixes created and the React team began verifying and planning new patch.&lt;/item&gt;
      &lt;item&gt;December 8th: Affected hosting providers and open source projects notified.&lt;/item&gt;
      &lt;item&gt;December 10th: Hosting provider mitigations in place and patches verified.&lt;/item&gt;
      &lt;item&gt;December 11th: Additional DoS reported to Meta Bug Bounty by Shinsaku Nomura.&lt;/item&gt;
      &lt;item&gt;December 11th: Patches published and publicly disclosed as CVE-2025-55183 and CVE-2025-55184.&lt;/item&gt;
      &lt;item&gt;December 11th: Missing DoS case found internally, patched and publicly disclosed as CVE-2025-67779.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Attribution&lt;/head&gt;
    &lt;p&gt;Thank you to Andrew MacPherson (AndrewMohawk) for reporting the Source Code Exposure, RyotaK from GMO Flatt Security Inc and Shinsaku Nomura of Bitforest Co., Ltd. for reporting the Denial of Service vulnerabilities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46236924</guid><pubDate>Thu, 11 Dec 2025 20:46:46 +0000</pubDate></item><item><title>Almond (YC X25) Is Hiring SWEs and MechEs</title><link>https://www.ycombinator.com/companies/almond-2/jobs</link><description>&lt;doc fingerprint="ce8988d79ddffda5"&gt;
  &lt;main&gt;
    &lt;p&gt;Robots designed for the era of AI&lt;/p&gt;
    &lt;p&gt;Our mission is to free humans from physical labor with robotics.&lt;/p&gt;
    &lt;p&gt;We imagine a future where robots handle the essential, repetitive work and humans are free to create, connect, and pursue what truly matters to them.&lt;/p&gt;
    &lt;p&gt;To build that future we’re starting from the ground up with hardware. Our first product is a California-designed and assembled humanoid arm. Surrounding it, we’re developing advanced controls, intuitive data collection, and a full AI stack that makes deployment effortless in real industrial environments. We’re proving it on our own assembly line first.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46237081</guid><pubDate>Thu, 11 Dec 2025 21:00:10 +0000</pubDate></item><item><title>Powder and stone, or, why medieval rulers loved castles</title><link>https://1517.substack.com/p/powder-and-stone-or-why-medieval</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46237501</guid><pubDate>Thu, 11 Dec 2025 21:35:52 +0000</pubDate></item><item><title>Nokia N900 Necromancy</title><link>https://yaky.dev/2025-12-11-nokia-n900-necromancy/</link><description>&lt;doc fingerprint="b6230073e3ff3e7c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nokia N900 Necromancy&lt;/head&gt;
    &lt;p&gt;Building a fake battery, adding a USB-C port, booting from SD card, and giving a new life to a classic Linux smartphone.&lt;/p&gt;
    &lt;p&gt;My friend Dima sent me his old-school classic Nokia N900. The battery is very old, and it does not boot as-is. So naturally, I wanted to see if I can resurrect it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 0: Is such a thing even possible?&lt;/head&gt;
    &lt;p&gt;Yes it is! (Unless there are other hardware issues)&lt;/p&gt;
    &lt;p&gt;I ran a smartphone without a battery a few years ago.&lt;/p&gt;
    &lt;p&gt;Cut and soldered a quick prototype to connect instead of the battery. Resistors are to emulate the "normal" temperature by providing expected resistance between the third pin and ground. See link above for details.&lt;/p&gt;
    &lt;p&gt;Hooked up a large supercapacitor to the battery pins and to a +5V source. If I recall correctly, using a capacitor without additional power did not work.&lt;/p&gt;
    &lt;p&gt;And it boots!&lt;/p&gt;
    &lt;p&gt;Now, let's make something that can fit into the battery compartment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 1: Better "battery"&lt;/head&gt;
    &lt;p&gt;These supercapacitors are nice, but way too large. After searching on Mouser, I found FM0H473ZF, 47000 mF (0.047F) capacitors in a rectangular case that is only 5mm thick.&lt;/p&gt;
    &lt;p&gt;Ten of these (~0.5F) is enough to run the smartphone without dying.&lt;/p&gt;
    &lt;p&gt;Capacitor contraption (TM) arranged (using a 3D-printed template) and soldered together.&lt;/p&gt;
    &lt;p&gt;And they all fit nicely into the battery compartment. The power is provided by a wire routed through the hole for the carry loop.&lt;/p&gt;
    &lt;p&gt;Running fine! One noticeable issue is that capacitors are getting pretty warm. Probably my sloppy soldering, but no shorts that I could find.&lt;/p&gt;
    &lt;head rend="h2"&gt;â ï¸&lt;/head&gt;
    &lt;p&gt;This is where I should have stopped. At some point while messing with the "battery" and power, I managed to corrupt the internal partition and the installed OS. Not sure if this was from the sudden battery pull or from supplying +5V instead of the expected +4.2V to the battery pins. Luckily, newer Maemo Leste is intended to run from the SD card anyway, and internal storage still works, so I was able to overwrite it with the bootloader.&lt;/p&gt;
    &lt;p&gt;Bootloader setup on Maemo Wiki&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 2: Consolidating connectors&lt;/head&gt;
    &lt;p&gt;I thought it might be practical to power the "battery" through the existing USB port. Just run the +5V wire from USB to the "battery", and avoid additional wires. (If you think this is kinda stupid, you are right)&lt;/p&gt;
    &lt;p&gt;Yooo... What is happening here? Dima says "oh yeah, the USB port was re-soldered. Twice". A quick glance at the forums also confirms that USB port was poorly designed and is prone to breaking.&lt;/p&gt;
    &lt;p&gt;Just one wire from the +5V pad to the "battery". The ground is the same as the battery pin.&lt;/p&gt;
    &lt;p&gt;Assembled everything back, routed and soldered the +5V wire, and added a diode to prevent the battery from feeding the USB port, and to drop the voltage to more acceptable ~4.3V.&lt;/p&gt;
    &lt;p&gt;The setup works, but the smartphone constantly shows either "Charging", or "Device using more power than it is receiving from the PC. Charging with a compatible charger is recommended", with battery gauge going crazy.&lt;/p&gt;
    &lt;p&gt;And then, the power just cut out.&lt;/p&gt;
    &lt;p&gt;Yeah, this was not a great idea. Let's see what happened.&lt;/p&gt;
    &lt;p&gt;USB +5V wire detached itself from the port. I presume this is from either the high current, age, stress, or corrosion.&lt;/p&gt;
    &lt;p&gt;However, when I opened the smartphone up, I... ripped off the +5V pad. (dark circle in lower right on the photo)&lt;/p&gt;
    &lt;p&gt;Fuck.&lt;/p&gt;
    &lt;p&gt;After reading some N900 forums, that +5V pad is a common place to connect the replacement USB port to (which was done here), but... that is the ONLY +5V connection on the board besides the pads under the USB port itself.&lt;/p&gt;
    &lt;p&gt;FUCK!&lt;/p&gt;
    &lt;head rend="h2"&gt;ðª¦&lt;/head&gt;
    &lt;p&gt;RIP Nokia N900. I tried to resurrect you, but instead, I killed your OS and ripped out the USB port wires.&lt;/p&gt;
    &lt;head rend="h2"&gt;Step 3: Radical replacements&lt;/head&gt;
    &lt;p&gt;To be fair, N900 is far from dead. I already flashed u-boot, was able to boot from SD card, and do not plan to use internal storage otherwise. Power can be supplied entirely through the new "battery". So technically, I do not need the USB functionality for the smartphone itself, just to power the "battery". At this point, I might as well replace the port with USB-C. Because why not.&lt;/p&gt;
    &lt;p&gt;Approximate placement of the new USB port.&lt;/p&gt;
    &lt;p&gt;The location of the original port is not very convenient. It is sandwiched between the main board and the SD card reader (lower left on the photo). SD card reader is also attached by a permanently-attached ribbon (i.e. nearly irreplaceable).&lt;/p&gt;
    &lt;p&gt;First, I used a small file to make the micro-USB-shaped hole on the smartphone body fit the USB-C shape. Then, I took a small 6-pin USB-C port, cut and sanded down its plastic parts to make it fit in the original spot. It is still slightly (~0.25mm) taller than the original, but I cannot make it any slimmer.&lt;/p&gt;
    &lt;p&gt;I tried to attach the USB-C port to the board in the correct place by carefully assembling the board, port and SD card reader into the body, and using small drops of glue to lightly affix the edge of the USB port (that I could reach) to the main board. The intent was to wait for glue to cure, take everything back apart and glue the port in its now-correct position for good. This took several tries but did not really work, as the port got detached while removing the main board every time, and the the superglue I used left lots of residue but did not adhere. Luckily, the tight fit and the shape of the USB-C port hold it in place mechanically quite well.&lt;/p&gt;
    &lt;p&gt;USB-C with +5V and ground attached.&lt;/p&gt;
    &lt;p&gt;Originally, I planned to solder all 6 pins and add 5.1 Ohm pull-down resistors to CC1 and CC2 pins (for full power delivery functionality). But there is simply not enough space to route the wires, the narrow valley between the chips (in the lower right of the photo) barely fits 3, and I did not have anything thinner on hand.&lt;/p&gt;
    &lt;p&gt;Nokia N900 with a USB-C port! Looks pretty nice IMO.&lt;/p&gt;
    &lt;p&gt;Since I did not solder the pull-down resistors, this USB-C port could only be powered by a "dumb" USB-A-to-USB-C cable, at default 0.5A. Chargers with power delivery functionality cannot identify such USB-C ports, and will not provide power at all. (This is also an issue with some handheld consoles such as RGB30)&lt;/p&gt;
    &lt;p&gt;The two wires are routed to the battery compartment through a very convenient opening in the metal frame, crimped and inserted into a DuPont connector.&lt;/p&gt;
    &lt;p&gt;Back to the battery. The capacitor contraption I built before works, but was kind of flimsy, and does not have any more space for a DuPont connector. Also, I would rather use a single capacitor, but it still has to fit. Since the original battery is unusable, I might as well try to salvage it, too.&lt;/p&gt;
    &lt;p&gt;Take off the sticker (that tells you not to do so :). The top BCM piece is held to the main battery body by two tiny screws (hidden under some crumbly compound) on each end, double-sided sticker, and a single lead in the middle.&lt;/p&gt;
    &lt;p&gt;Battery Control Module. Interestingly, for this battery, the body is the positive terminal. So the positive lead connects the battery body and the positive pin directly, while the negative lead goes thorough some control circuitry. Attaching a capacitor to these battery terminals should be sufficient.&lt;/p&gt;
    &lt;p&gt;Since I have a 3D printer, and once you have one, every problem can be solved by printing stuff, I printed the new "battery" to accommodate a large capacitor, diode (for voltage drop), wires, DuPont connectors, and the original battery's BCM.&lt;/p&gt;
    &lt;p&gt;N900 with a new "battery". Fits really tight, and only 0.25-0.5mm too tall, so the cover still snaps closed.&lt;/p&gt;
    &lt;p&gt;Boots without problems. Since the attached capacitor is pretty large, it can take a minute or two to charge it to an acceptable level (~4.0V) with a 0.5A current.&lt;/p&gt;
    &lt;p&gt;Nokia N900 enjoying its new life as an online radio device using Open Media Player.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239177</guid><pubDate>Fri, 12 Dec 2025 00:04:29 +0000</pubDate></item><item><title>Laying out the 404 Media zine</title><link>https://tedium.co/2025/12/10/404-media-zine-linux-affinity/?</link><description>&lt;doc fingerprint="3e12c577bf35a7fa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;WINE Cooler&lt;/head&gt;
    &lt;head rend="h2"&gt;Lessons on laying out the 404 Media zine using a relatively weird setup—on Linux, using Affinity, with the help of the Windows translation layer WINE.&lt;/head&gt;
    &lt;p&gt;I write a lot these days, but my path into journalism, going way back to J-School, was through layout.&lt;/p&gt;
    &lt;p&gt;For years, I was a graphic designer at a number of newspapers—some fairly small, some quite large. I was a card-carrying member of the Society for News Design. It was one of my biggest passions, and I fully expected to have a long career in newspaper design. But newspapers as a medium haven’t really panned out, so I eventually fell into writing.&lt;/p&gt;
    &lt;p&gt;But I still adore laying out a big project, conceptualizing it, and trying to use it to visually add to the story that the words are trying to convey. It’s not quite a lost art, but I do think that print layout is something that has been a bit back-burnered by society at large.&lt;/p&gt;
    &lt;p&gt;So when 404 Media co-founder Jason Koebler, who spent years editing my writing for Motherboard, reached out about doing a zine, I was absolutely in. The goal of the zine—to shine a spotlight on the intersection of ICE and surveillance tech—was important. Plus, I like working with Jason, and it was an opportunity to get into print design again after quite a few years away.&lt;/p&gt;
    &lt;p&gt;I just had two problems: One, I have decided that I no longer want to give Adobe money because of cost and ethical concerns about its business model. And two, I now use Linux pretty much exclusively (Bazzite DX, in case you’re wondering).&lt;/p&gt;
    &lt;p&gt;But the good news is that the open-source community has done a lot of work, and despite my own tech shifts, professional-grade print design on Linux is now a viable option.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why page layout on Linux is fairly uncommon&lt;/head&gt;
    &lt;p&gt;The meme in the Linux community writes itself: “I would move over to Linux, but I need Photoshop and InDesign and [insert app here] too much.” In the past, this has been a real barrier for designers, especially those who rely on print layout, where open-source alternatives are very limited. (They’ve also been traditionally at the mercy of print shops that have no time for your weird non-standard app.)&lt;/p&gt;
    &lt;p&gt;Admittedly, the native tools have been getting better. I’m not really a fan myself, but I know GIMP is getting closer in parity to Photoshop. Inkscape is a totally viable vector drawing app. Video is very doable on Linux thanks to the FOSS Kendenlive and the commercial DaVinci Resolve. Blender is basically a de facto standard for 3D at this point. The web-based Penpot is a capable Figma alternative. And Krita, while promoted as a digital painting app, has become my tool of choice for making frame-based animated GIFs, which I do a lot for Tedium.&lt;/p&gt;
    &lt;p&gt;But for ink-stained print layout nerds, it has been tougher to make the shift (our apologies to Scribus). And Adobe locks down Creative Cloud pretty hard.&lt;/p&gt;
    &lt;p&gt;However, the recent Affinity release, while drawing some skepticism from the open-source community as a potential enshittification issue, is starting to open up a fresh lane. For those not aware, the new version of Affinity essentially combines the three traditional design apps—vector editor, raster editor, and page layout—into a single tool. It’s pretty good at all three. (Plus, for business reasons related to its owner Canva, it’s currently free to use.)&lt;/p&gt;
    &lt;p&gt;While it doesn’t have a dedicated Linux version, it more or less runs very well using WINE, the technology that has enabled a Linux renaissance via the Steam Deck. (Some passionate community members, like the WINE hacker ElementalWarrior, have worked hard to make this a fully-fleshed out experience that can even be installed more or less painlessly.)&lt;/p&gt;
    &lt;p&gt;The desire for a native Linux version of a pro-level design app is such that the Canva subsidiary is thinking about doing it themselves.&lt;/p&gt;
    &lt;p&gt;But I’m not the kind of person who likes to wait, so I decided to try to build as much of the zine as I could with Affinity for page layout. For the few things I couldn’t do, I would remote into a Mac.&lt;/p&gt;
    &lt;head rend="h3"&gt;The RISO factor&lt;/head&gt;
    &lt;p&gt;Another consideration here is the fact that this zine is being built with Risograph printing, a multicolor printing approach distinct from the more traditional CMYK. The inky printing process, similar to screen printing, has a distinct, vibrant look, even if it avoids the traditional four-color approach (in our case, using layers of pink, black, and lime green).&lt;/p&gt;
    &lt;p&gt;Throughout the process, I spent a lot of time setting layers to multiply to ensure the results looked good, and adding effects like halftone and erase to help balance out the color effects. This mostly worked OK, though I did have some glitches.&lt;/p&gt;
    &lt;p&gt;At one point, a lime-green frog lost much of its detail when I tried to RISO-fy it, requiring me to double-check my color settings and ensure I was getting the right tone. And sometimes, PDF exports from Affinity added unsightly lines, which I had to go out of my way to remove. If I was designing for newspapers, I might have been forced to come up with a quick plan B for that layout. But fortunately, I had the luxury of not working on a daily deadline like I might have back in the day.&lt;/p&gt;
    &lt;p&gt;I think that this layout approach is genuinely fascinating—and I know Jason in particular is a huge fan of it. Could I see other publications in the 404 mold taking notes from this and doing the same thing? Heck yes.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ups and downs of print layout on Linux&lt;/head&gt;
    &lt;p&gt;So, the headline you can take away from this is pretty simple: Laying stuff out in Affinity over Linux is extremely doable, and if you’re doing it occasionally, you will find a quite capable tool.&lt;/p&gt;
    &lt;p&gt;Admittedly, if this was, like, my main gig, I might still feel the urge to go back to MacOS—especially near the end of the process. Here’s what I learned:&lt;/p&gt;
    &lt;p&gt;The good: Workflow-wise, it was pretty smooth. Image cutouts—a tightly honed skill of mine that AI has been trying to obsolete for years—were very doable. Affinity also has some great effects tools that in many ways beat equivalents in other apps, such as its glitch tool and its live filter layers. It didn’t feel like I was getting a second-class experience when all was said and done.&lt;/p&gt;
    &lt;p&gt;The bad: My muscle memory for InDesign shortcuts was completely ineffective for this, and there were occasional features of InDesign and Photoshop that I did not find direct equivalents for in Affinity. WINE’s file menus tend to look like old Windows, which might be a turn-off for UX purists, and required a bit of extra navigation to dig through folders. Also, one downside of WINE that I could not work past was that I couldn’t use my laptop’s Intel-based GPU for machine learning tasks, a known bug that I imagine slowed some things down on graphically intensive pages.&lt;/p&gt;
    &lt;p&gt;The ugly: I think one area Affinity will need to work on as it attempts to sell the idea that you can design in one interface are better strategies to help mash down content for export. At one point while I was trying to make a PDF, Affinity promised me that the file I would be exporting was going to be 17 exabytes in size, which my SSD was definitely not large enough for. That wasn’t true, but it does emphasize that the dream of doing everything in one interface gets complicated when you want to send things to the printer. Much of the work I did near the end of the process was rasterizing layers to ensure everything looked as intended.&lt;/p&gt;
    &lt;p&gt;When I did have to use a Mac app for something (mainly accessing Spectrolite, a prepress app for RISO designs), I accessed an old Hackintosh using NoMachine, a tool for connecting to computers remotely. So even for the stuff I actually needed MacOS for, I didn’t need to leave the comforts of my janky laptop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking for a Big Tech escape hatch&lt;/head&gt;
    &lt;p&gt;Was it 100% perfect? No. Affinity crashed every once in a while, but InDesign did that all the time back in the day. And admittedly, an office full of people using Affinity on Linux isn’t going to work as well as one guy in a coffee shop working with a team of editors over chat and email.&lt;/p&gt;
    &lt;p&gt;But it’s my hope that experiences like mine convince other people to try it, and for companies to embrace it. Affinity isn’t open-source, and Canva is a giant company with plenty of critics, just like Adobe. But there are emerging projects like PixiEditor and Graphite that could eventually make print layout an extremely viable and even modern open-source endeavor.&lt;/p&gt;
    &lt;p&gt;But we have to take victories where we can find them, and the one I see is that Affinity is a lot less locked down than Creative Cloud, which is why it’s viable on Linux. And in general, this feels like an opportunity to get away from the DRM-driven past of creative software. (Hey Canva, it’s never too late to make Affinity open-source.)&lt;/p&gt;
    &lt;p&gt;Difficult reporting shouldn’t have to be tethered to the whims of Big Tech to exist. Especially when that tech—on Amazon’s cloud, using Adobe’s PDFs, through Google’s search, over Meta’s social network, with Apple’s phones, and on Microsoft’s operating system—too often causes uncomfortable tensions with the reporting. This is one step towards a better escape hatch.&lt;/p&gt;
    &lt;head rend="h5"&gt;Laid-Out Links&lt;/head&gt;
    &lt;p&gt;Speaking of useful image tools, I want to give a shout to Imagor Studio, a self-hosted image management app. It’s actually an extension of a tool Tedium uses for image rendering, Imagor.&lt;/p&gt;
    &lt;p&gt;The video creator David Hoffman has been pulling absolute gems from his archive over the years, but this one, in which telephone operators discuss their feelings on automation, really hits home.&lt;/p&gt;
    &lt;p&gt;And yeah, I did a Nieman Lab prediction this year. It’s about my recent hobbyhorse, Grokipedia.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;Find this one an interesting read? Share it with a pal—and pick up 404 Media’s upcoming zine.&lt;/p&gt;
    &lt;p&gt;Want to resist the impending doom of big tech? Check out our sponsor la machine, a beautiful machine that won’t even let you hit the on button.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239188</guid><pubDate>Fri, 12 Dec 2025 00:05:36 +0000</pubDate></item><item><title>Stoolap: High-performance embedded SQL database in pure Rust</title><link>https://github.com/stoolap/stoolap</link><description>&lt;doc fingerprint="4130edfdec314ad6"&gt;
  &lt;main&gt;
    &lt;p&gt;Stoolap is an embedded SQL database with MVCC transactions, written entirely in Rust. It supports both in-memory and persistent storage modes with full ACID compliance.&lt;/p&gt;
    &lt;code&gt;# Add to Cargo.toml
[dependencies]
stoolap = "0.1"&lt;/code&gt;
    &lt;p&gt;Or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/stoolap/stoolap.git
cd stoolap
cargo build --release&lt;/code&gt;
    &lt;code&gt;use stoolap::api::Database;

fn main() -&amp;gt; Result&amp;lt;(), Box&amp;lt;dyn std::error::Error&amp;gt;&amp;gt; {
    let db = Database::open_in_memory()?;

    db.execute("CREATE TABLE users (id INTEGER PRIMARY KEY, name TEXT)", ())?;
    db.execute("INSERT INTO users VALUES (1, 'Alice')", ())?;

    for row in db.query("SELECT * FROM users", ())? {
        let row = row?;
        println!("{}: {}", row.get::&amp;lt;i64&amp;gt;(0)?, row.get::&amp;lt;String&amp;gt;(1)?);
    }

    Ok(())
}&lt;/code&gt;
    &lt;code&gt;./stoolap                                    # In-memory REPL
./stoolap --db "file:///path/to/data"        # Persistent database
./stoolap -q "SELECT 1 + 1"                  # Execute query directly&lt;/code&gt;
    &lt;p&gt;Full multi-version concurrency control with two isolation levels:&lt;/p&gt;
    &lt;code&gt;-- Read Committed (default)
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;
UPDATE accounts SET balance = balance + 100 WHERE id = 2;
COMMIT;

-- Snapshot Isolation
BEGIN TRANSACTION ISOLATION LEVEL SNAPSHOT;
SELECT * FROM accounts;  -- Consistent view throughout transaction
COMMIT;&lt;/code&gt;
    &lt;p&gt;Query historical data at any point in time:&lt;/p&gt;
    &lt;code&gt;-- Query data as it existed at a specific timestamp
SELECT * FROM orders AS OF TIMESTAMP '2024-01-15 10:30:00';

-- Query data as of a specific transaction
SELECT * FROM inventory AS OF TRANSACTION 1234;

-- Compare current vs historical data
SELECT
    current.price,
    historical.price AS old_price
FROM products current
JOIN products AS OF TIMESTAMP '2024-01-01' historical
    ON current.id = historical.id
WHERE current.price != historical.price;&lt;/code&gt;
    &lt;p&gt;Stoolap automatically selects optimal index types, or you can specify explicitly:&lt;/p&gt;
    &lt;code&gt;-- B-tree: Range queries, sorting, prefix matching
CREATE INDEX idx_date ON orders(created_at) USING BTREE;
SELECT * FROM orders WHERE created_at BETWEEN '2024-01-01' AND '2024-12-31';

-- Hash: O(1) equality lookups
CREATE INDEX idx_email ON users(email) USING HASH;
SELECT * FROM users WHERE email = 'alice@example.com';

-- Bitmap: Low-cardinality columns, efficient AND/OR
CREATE INDEX idx_status ON orders(status) USING BITMAP;
SELECT * FROM orders WHERE status = 'pending' AND priority = 'high';

-- Multi-column composite indexes
CREATE INDEX idx_lookup ON events(user_id, event_type, created_at);
SELECT * FROM events WHERE user_id = 100 AND event_type = 'click';&lt;/code&gt;
    &lt;p&gt;Full support for analytical queries:&lt;/p&gt;
    &lt;code&gt;SELECT
    employee_name,
    department,
    salary,
    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank,
    salary - LAG(salary) OVER (ORDER BY hire_date) as salary_change,
    AVG(salary) OVER (PARTITION BY department) as dept_avg,
    SUM(salary) OVER (ORDER BY hire_date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total
FROM employees;&lt;/code&gt;
    &lt;p&gt;Including recursive queries:&lt;/p&gt;
    &lt;code&gt;-- Non-recursive CTE
WITH high_value_orders AS (
    SELECT * FROM orders WHERE amount &amp;gt; 1000
)
SELECT customer_id, COUNT(*) FROM high_value_orders GROUP BY customer_id;

-- Recursive CTE (e.g., organizational hierarchy)
WITH RECURSIVE org_chart AS (
    SELECT id, name, manager_id, 1 as level
    FROM employees WHERE manager_id IS NULL

    UNION ALL

    SELECT e.id, e.name, e.manager_id, oc.level + 1
    FROM employees e
    JOIN org_chart oc ON e.manager_id = oc.id
)
SELECT * FROM org_chart ORDER BY level, name;&lt;/code&gt;
    &lt;code&gt;-- ROLLUP: Hierarchical subtotals
SELECT region, product, SUM(sales)
FROM sales_data
GROUP BY ROLLUP(region, product);

-- CUBE: All possible subtotal combinations
SELECT region, product, SUM(sales)
FROM sales_data
GROUP BY CUBE(region, product);&lt;/code&gt;
    &lt;p&gt;Scalar, correlated, EXISTS, and IN subqueries:&lt;/p&gt;
    &lt;code&gt;-- Correlated subquery
SELECT * FROM employees e
WHERE salary &amp;gt; (SELECT AVG(salary) FROM employees WHERE department = e.department);

-- EXISTS
SELECT * FROM customers c
WHERE EXISTS (SELECT 1 FROM orders o WHERE o.customer_id = c.id AND o.amount &amp;gt; 1000);

-- IN with subquery
SELECT * FROM products
WHERE category_id IN (SELECT id FROM categories WHERE active = true);&lt;/code&gt;
    &lt;p&gt;Cost-based optimizer with statistics:&lt;/p&gt;
    &lt;code&gt;-- Collect table statistics
ANALYZE orders;

-- View query execution plan
EXPLAIN SELECT * FROM orders WHERE customer_id = 100;

-- View plan with actual execution statistics
EXPLAIN ANALYZE SELECT * FROM orders o
JOIN customers c ON o.customer_id = c.id
WHERE c.country = 'US';&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;INTEGER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;64-bit signed integer&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;42&lt;/code&gt;, &lt;code&gt;-100&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;FLOAT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;64-bit floating point&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;3.14&lt;/code&gt;, &lt;code&gt;-0.001&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TEXT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;UTF-8 string&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;'hello'&lt;/code&gt;, &lt;code&gt;'日本語'&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;BOOLEAN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;true/false&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;TRUE&lt;/code&gt;, &lt;code&gt;FALSE&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;TIMESTAMP&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Date and time&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;'2024-01-15 10:30:00'&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;JSON&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;JSON data&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;'{"key": "value"}'&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;UPPER&lt;/code&gt;, &lt;code&gt;LOWER&lt;/code&gt;, &lt;code&gt;LENGTH&lt;/code&gt;, &lt;code&gt;TRIM&lt;/code&gt;, &lt;code&gt;LTRIM&lt;/code&gt;, &lt;code&gt;RTRIM&lt;/code&gt;, &lt;code&gt;CONCAT&lt;/code&gt;, &lt;code&gt;SUBSTRING&lt;/code&gt;, &lt;code&gt;REPLACE&lt;/code&gt;, &lt;code&gt;REVERSE&lt;/code&gt;, &lt;code&gt;LEFT&lt;/code&gt;, &lt;code&gt;RIGHT&lt;/code&gt;, &lt;code&gt;LPAD&lt;/code&gt;, &lt;code&gt;RPAD&lt;/code&gt;, &lt;code&gt;REPEAT&lt;/code&gt;, &lt;code&gt;POSITION&lt;/code&gt;, &lt;code&gt;LOCATE&lt;/code&gt;, &lt;code&gt;INSTR&lt;/code&gt;, &lt;code&gt;SPLIT_PART&lt;/code&gt;, &lt;code&gt;INITCAP&lt;/code&gt;, &lt;code&gt;ASCII&lt;/code&gt;, &lt;code&gt;CHR&lt;/code&gt;, &lt;code&gt;TRANSLATE&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ABS&lt;/code&gt;, &lt;code&gt;CEIL&lt;/code&gt;, &lt;code&gt;FLOOR&lt;/code&gt;, &lt;code&gt;ROUND&lt;/code&gt;, &lt;code&gt;TRUNC&lt;/code&gt;, &lt;code&gt;SQRT&lt;/code&gt;, &lt;code&gt;POWER&lt;/code&gt;, &lt;code&gt;MOD&lt;/code&gt;, &lt;code&gt;SIGN&lt;/code&gt;, &lt;code&gt;GREATEST&lt;/code&gt;, &lt;code&gt;LEAST&lt;/code&gt;, &lt;code&gt;EXP&lt;/code&gt;, &lt;code&gt;LN&lt;/code&gt;, &lt;code&gt;LOG&lt;/code&gt;, &lt;code&gt;LOG10&lt;/code&gt;, &lt;code&gt;LOG2&lt;/code&gt;, &lt;code&gt;SIN&lt;/code&gt;, &lt;code&gt;COS&lt;/code&gt;, &lt;code&gt;TAN&lt;/code&gt;, &lt;code&gt;ASIN&lt;/code&gt;, &lt;code&gt;ACOS&lt;/code&gt;, &lt;code&gt;ATAN&lt;/code&gt;, &lt;code&gt;ATAN2&lt;/code&gt;, &lt;code&gt;DEGREES&lt;/code&gt;, &lt;code&gt;RADIANS&lt;/code&gt;, &lt;code&gt;PI&lt;/code&gt;, &lt;code&gt;RAND&lt;/code&gt;, &lt;code&gt;RANDOM&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;NOW&lt;/code&gt;, &lt;code&gt;CURRENT_DATE&lt;/code&gt;, &lt;code&gt;CURRENT_TIME&lt;/code&gt;, &lt;code&gt;CURRENT_TIMESTAMP&lt;/code&gt;, &lt;code&gt;EXTRACT&lt;/code&gt;, &lt;code&gt;DATE_TRUNC&lt;/code&gt;, &lt;code&gt;DATE_ADD&lt;/code&gt;, &lt;code&gt;DATE_SUB&lt;/code&gt;, &lt;code&gt;DATEDIFF&lt;/code&gt;, &lt;code&gt;YEAR&lt;/code&gt;, &lt;code&gt;MONTH&lt;/code&gt;, &lt;code&gt;DAY&lt;/code&gt;, &lt;code&gt;HOUR&lt;/code&gt;, &lt;code&gt;MINUTE&lt;/code&gt;, &lt;code&gt;SECOND&lt;/code&gt;, &lt;code&gt;DAYOFWEEK&lt;/code&gt;, &lt;code&gt;DAYOFYEAR&lt;/code&gt;, &lt;code&gt;WEEK&lt;/code&gt;, &lt;code&gt;QUARTER&lt;/code&gt;, &lt;code&gt;TO_CHAR&lt;/code&gt;, &lt;code&gt;TO_DATE&lt;/code&gt;, &lt;code&gt;TO_TIMESTAMP&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;JSON_EXTRACT&lt;/code&gt;, &lt;code&gt;JSON_EXTRACT_PATH&lt;/code&gt;, &lt;code&gt;JSON_TYPE&lt;/code&gt;, &lt;code&gt;JSON_TYPEOF&lt;/code&gt;, &lt;code&gt;JSON_VALID&lt;/code&gt;, &lt;code&gt;JSON_KEYS&lt;/code&gt;, &lt;code&gt;JSON_ARRAY_LENGTH&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;COUNT&lt;/code&gt;, &lt;code&gt;SUM&lt;/code&gt;, &lt;code&gt;AVG&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;STDDEV&lt;/code&gt;, &lt;code&gt;STDDEV_POP&lt;/code&gt;, &lt;code&gt;STDDEV_SAMP&lt;/code&gt;, &lt;code&gt;VARIANCE&lt;/code&gt;, &lt;code&gt;VAR_POP&lt;/code&gt;, &lt;code&gt;VAR_SAMP&lt;/code&gt;, &lt;code&gt;STRING_AGG&lt;/code&gt;, &lt;code&gt;ARRAY_AGG&lt;/code&gt;, &lt;code&gt;FIRST&lt;/code&gt;, &lt;code&gt;LAST&lt;/code&gt;, &lt;code&gt;BIT_AND&lt;/code&gt;, &lt;code&gt;BIT_OR&lt;/code&gt;, &lt;code&gt;BIT_XOR&lt;/code&gt;, &lt;code&gt;BOOL_AND&lt;/code&gt;, &lt;code&gt;BOOL_OR&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;ROW_NUMBER&lt;/code&gt;, &lt;code&gt;RANK&lt;/code&gt;, &lt;code&gt;DENSE_RANK&lt;/code&gt;, &lt;code&gt;NTILE&lt;/code&gt;, &lt;code&gt;LAG&lt;/code&gt;, &lt;code&gt;LEAD&lt;/code&gt;, &lt;code&gt;FIRST_VALUE&lt;/code&gt;, &lt;code&gt;LAST_VALUE&lt;/code&gt;, &lt;code&gt;NTH_VALUE&lt;/code&gt;, &lt;code&gt;PERCENT_RANK&lt;/code&gt;, &lt;code&gt;CUME_DIST&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;COALESCE&lt;/code&gt;, &lt;code&gt;NULLIF&lt;/code&gt;, &lt;code&gt;CAST&lt;/code&gt;, &lt;code&gt;CASE&lt;/code&gt;, &lt;code&gt;IF&lt;/code&gt;, &lt;code&gt;IIF&lt;/code&gt;, &lt;code&gt;NVL&lt;/code&gt;, &lt;code&gt;NVL2&lt;/code&gt;, &lt;code&gt;DECODE&lt;/code&gt;, &lt;code&gt;GREATEST&lt;/code&gt;, &lt;code&gt;LEAST&lt;/code&gt;, &lt;code&gt;GENERATE_SERIES&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Stoolap uses write-ahead logging (WAL) with periodic snapshots:&lt;/p&gt;
    &lt;code&gt;# In-memory (default) - data lost on exit
./stoolap --db "memory://"

# File-based - durable storage
./stoolap --db "file:///var/lib/stoolap/data"&lt;/code&gt;
    &lt;p&gt;Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WAL: All changes logged before applied, survives crashes&lt;/item&gt;
      &lt;item&gt;Snapshots: Periodic full database snapshots for faster recovery&lt;/item&gt;
      &lt;item&gt;Index persistence: All indexes saved and restored&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;src/
├── api/        # Public API (Database, Connection, Rows)
├── core/       # Types (Value, Row, Schema, Error)
├── parser/     # SQL lexer and parser
├── planner/    # Query planning
├── optimizer/  # Cost-based query optimizer
├── executor/   # Query execution engine
├── functions/  # 100+ built-in functions
│   ├── scalar/     # String, math, date, JSON
│   ├── aggregate/  # COUNT, SUM, AVG, etc.
│   └── window/     # ROW_NUMBER, RANK, LAG, etc.
└── storage/    # Storage engine
    ├── mvcc/       # Multi-version concurrency control
    └── index/      # B-tree, Hash, Bitmap indexes
&lt;/code&gt;
    &lt;code&gt;cargo build              # Debug build
cargo build --release    # Release build (optimized)
cargo test               # Run tests
cargo clippy             # Lint
cargo doc --open         # Generate documentation&lt;/code&gt;
    &lt;p&gt;See CONTRIBUTING.md for guidelines.&lt;/p&gt;
    &lt;p&gt;Apache License 2.0. See LICENSE.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239372</guid><pubDate>Fri, 12 Dec 2025 00:28:24 +0000</pubDate></item><item><title>The HTML-First Approach: Why Htmx and Lightweight Frameworks Are Revolutionizin</title><link>https://www.danieleteti.it/post/html-first-frameworks-htmx-revolution-en/#building-with-html-instead-of-fighting-with-javascript-layers-</link><description>&lt;doc fingerprint="f74bd3092d277fab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The HTML-First Approach: Why htmx and Lightweight Frameworks Are Revolutionizing Web Development&lt;/head&gt;
    &lt;p&gt;For years, when it came to building something “modern” on the web, the almost automatic choice fell on React, Angular, Vue, and the entire Single Page Application (SPA) ecosystem. These frameworks became the safe choice, almost a de facto standard. But lately, a significant shift is happening in the front-end landscape. Many teams — including some large ones with enterprise projects — are moving toward HTML-first frameworks like htmx and other tools that take a more traditional, server-driven approach.&lt;/p&gt;
    &lt;p&gt;And honestly, it makes perfect sense. 🎯&lt;/p&gt;
    &lt;p&gt;Not every application needs a heavy client-side engine. In many cases, the SPA model adds more complexity than value. HTML-first frameworks bring back some of the simplicity and speed the web was originally designed for, without sacrificing the interactivity users expect.&lt;/p&gt;
    &lt;p&gt;In this article, we’ll explore in depth the reasons behind this trend, backed by concrete data and statistics.&lt;/p&gt;
    &lt;head rend="h2"&gt;The JavaScript Bloat Problem: The Numbers Speak Clearly 📊&lt;/head&gt;
    &lt;p&gt;Before analyzing the advantages of the HTML-first approach, it’s essential to understand the magnitude of the problem we’re facing.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Exponential Growth of JavaScript&lt;/head&gt;
    &lt;p&gt;The HTTP Archive data is eloquent: the average amount of JavaScript transferred per page has grown from 90 KB in 2010 to 650 KB in 2024. And this trend shows no signs of slowing down.&lt;/p&gt;
    &lt;p&gt;But these are just average values. A detailed 2024 analysis reveals far more extreme cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Slack, a chat application, loads 55 MB of JavaScript — practically the size of the original Quake 1 with all resources included&lt;/item&gt;
      &lt;item&gt;Jira, a task management software, weighs almost 50 MB&lt;/item&gt;
      &lt;item&gt;LinkedIn reaches 31 MB&lt;/item&gt;
      &lt;item&gt;Simple social network “Like” buttons typically require 12 MB of code&lt;/item&gt;
      &lt;item&gt;Even Google Maps, relatively modest by modern standards, weighs 4.5 MB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we assume an average line of code is about 65 characters, we’re talking about shipping approximately 150,000 lines of code with every website, sometimes just to display static content!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;💡 Ever thought about it? Slack, a messaging app, requires more space than an entire 3D video game from the 90s. To send text messages.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The Performance Impact&lt;/head&gt;
    &lt;p&gt;JavaScript is the most computationally expensive resource a browser has to handle. It’s often the bottleneck that determines whether a page appears fast or slow, as an oversized bundle can block rendering and degrade overall performance.&lt;/p&gt;
    &lt;p&gt;For those using a high-end laptop with fiber connection, this might be just a minor annoyance. But for those browsing with a low-end phone or unstable connection, it can make the difference between staying on the site or abandoning it completely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Framework Sizes Compared&lt;/head&gt;
    &lt;p&gt;Here’s a comparison of the major JavaScript frameworks’ sizes (gzipped versions), according to data from LogRocket and Strapi:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Gzipped Size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Angular&lt;/cell&gt;
        &lt;cell&gt;~62.3 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;React + ReactDOM&lt;/cell&gt;
        &lt;cell&gt;~44.5 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vue&lt;/cell&gt;
        &lt;cell&gt;~34.7 KB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;htmx&lt;/cell&gt;
        &lt;cell&gt;~14 KB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;htmx weighs about one-third of Vue and less than one-quarter of Angular. And these are just the core frameworks — real applications typically include libraries for routing, state management, form handling, HTTP client, and much more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building with HTML Instead of Fighting with JavaScript Layers 🛠️&lt;/head&gt;
    &lt;p&gt;HTML-first tools let you focus on the actual structure and behavior of your application, instead of juggling component trees, hydration, reducers, context providers, and all the overhead that comes with SPA frameworks.&lt;/p&gt;
    &lt;p&gt;With htmx, for example, you simply send HTML from the server, and the library swaps the right parts on the page. No 20-file folders for a single React component. No client-side state management libraries. No over-engineering.&lt;/p&gt;
    &lt;p&gt;It’s an approach that feels surprisingly straightforward and linear.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Practical Example&lt;/head&gt;
    &lt;p&gt;Let’s consider a simple button that loads dynamic content.&lt;/p&gt;
    &lt;p&gt;Traditional SPA approach (React):&lt;/p&gt;
    &lt;code&gt;// useState, useEffect, fetch API, loading state management,
// error handling, conditional rendering...
function LoadDataButton() {
  const [data, setData] = useState(null);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  const handleClick = async () =&amp;gt; {
    setLoading(true);
    try {
      const response = await fetch('/api/data');
      const json = await response.json();
      setData(json);
    } catch (e) {
      setError(e);
    } finally {
      setLoading(false);
    }
  };

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;button onClick={handleClick} disabled={loading}&amp;gt;
        {loading ? 'Loading...' : 'Load Data'}
      &amp;lt;/button&amp;gt;
      {error &amp;amp;&amp;amp; &amp;lt;div className="error"&amp;gt;{error.message}&amp;lt;/div&amp;gt;}
      {data &amp;amp;&amp;amp; &amp;lt;DataDisplay data={data} /&amp;gt;}
    &amp;lt;/div&amp;gt;
  );
}
&lt;/code&gt;
    &lt;p&gt;HTML-first approach with htmx:&lt;/p&gt;
    &lt;code&gt;&amp;lt;button hx-get="/data"
        hx-target="#result"
        hx-indicator="#loading"&amp;gt;
  Load Data
&amp;lt;/button&amp;gt;
&amp;lt;span id="loading" class="htmx-indicator"&amp;gt;Loading...&amp;lt;/span&amp;gt;
&amp;lt;div id="result"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The server returns the HTML ready to display directly. End of story.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;✨ The difference is clear: 30+ lines of React code vs 5 lines of HTML with htmx. Same result, radically different complexity.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Performance Improves Almost Automatically ⚡&lt;/head&gt;
    &lt;p&gt;SPAs ship tons of JavaScript to the browser — and the browser pays for it every time: parsing, executing, hydrating, diffing the virtual DOM, and so on.&lt;/p&gt;
    &lt;p&gt;HTML-first frameworks work the opposite way. They load fast because the browser handles what it’s best at: rendering HTML. Interactivity is added in small, targeted pieces instead of shipping an entire runtime.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Data Confirms&lt;/head&gt;
    &lt;p&gt;According to a 2024 study:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The median Time to Interactive (TTI) for SPAs was 2.9 seconds, versus 1.8 seconds for SSR sites&lt;/item&gt;
      &lt;item&gt;The median Time to First Byte (TTFB) was 0.6 seconds for SPAs, versus 0.2 seconds for SSR&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Users — especially mobile users — feel the difference immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Server-Side Rendering is Faster for Content&lt;/head&gt;
    &lt;p&gt;SSR’s advantage lies primarily in the faster time to display content, which becomes more evident on slow Internet connections or underpowered devices. Server-rendered markup doesn’t need to wait for all JavaScript to be downloaded and executed to be displayed, so users see a fully rendered page sooner.&lt;/p&gt;
    &lt;p&gt;This generally translates to better Core Web Vitals metrics, superior user experience, and can be critical for applications where content display time is directly associated with conversion rate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Server-Driven UI is Cleaner for Most Business Applications 🏢&lt;/head&gt;
    &lt;p&gt;Most real logic — validation, business rules, access control — lives on the server anyway. HTML-first frameworks don’t force you to duplicate it on both sides.&lt;/p&gt;
    &lt;p&gt;Instead of creating an API endpoint, transforming it, consuming it, and syncing everything in the client… you just render HTML with the updated state.&lt;/p&gt;
    &lt;p&gt;Simple. Predictable. Easy to understand and debug.&lt;/p&gt;
    &lt;head rend="h3"&gt;One Routing, One Source of Truth&lt;/head&gt;
    &lt;p&gt;One of the most underrated aspects of the HTML-first approach is the elimination of routing duplication. In traditional SPAs, you inevitably end up with two parallel routing systems: one on the server (for APIs and initial rendering) and one on the client (for internal navigation). This means double configuration, double maintenance, and often hard-to-debug inconsistencies.&lt;/p&gt;
    &lt;p&gt;With htmx and the server-driven approach, there’s only one routing: the server’s. URLs correspond directly to resources, the browser handles navigation natively, and the developer has a single source of truth to maintain.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validation: Only Where It Really Matters&lt;/head&gt;
    &lt;p&gt;The same principle applies to formal checks and data validation. In SPA architectures, developers often find themselves implementing the same validation logic twice: on the client (to provide immediate feedback and improve UX) and on the server (where checks MUST always reside for security reasons).&lt;/p&gt;
    &lt;p&gt;With the HTML-first approach, validation stays where it belongs: on the server. And thanks to the speed of partial HTML responses, user feedback is still almost instant. The server validates the data and returns the HTML with any error messages already rendered. No logic duplication, no risk of inconsistencies between client and server rules, no possibility of a malicious user bypassing client-side checks.&lt;/p&gt;
    &lt;p&gt;What if you want to implement a client-side check to avoid too many server requests? You can always do it! The fundamental difference is that you’re not forced to implement validation twice — you can choose to do it only when it makes sense for UX, knowing that server-side validation is already guaranteed.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;🔐 Security note: Client-side validation is always bypassable. A malicious user can simply disable JavaScript or modify HTTP requests. Server-side validation is not optional — it’s the only one that really matters for security.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The HATEOAS Pattern Revives&lt;/head&gt;
    &lt;p&gt;The HTML-first approach aligns with the HATEOAS architectural principle (Hypermedia as the Engine of Application State), one of the original REST constraints often ignored in modern JSON APIs.&lt;/p&gt;
    &lt;p&gt;But what exactly does this principle say? HATEOAS states that a client should be able to interact with an application entirely through the hypermedia responses dynamically provided by the server. In other words, the client should have no prior knowledge of how to interact with the server beyond an initial entry point — all possible actions should be discovered dynamically through the links and controls present in the response itself.&lt;/p&gt;
    &lt;p&gt;Think about it: when the server returns HTML, it automatically returns links to related pages, forms for available actions, buttons for permitted operations. The interface is the API. The browser already knows how to navigate links and submit forms — no client-side logic is needed to “interpret” the response and decide what to do next.&lt;/p&gt;
    &lt;p&gt;Can your JSON-APIs do this? In the vast majority of cases, no. JSON APIs return raw data that the client must interpret, and navigation and interaction logic must be implemented separately in the front-end code. The client must “know” beforehand which endpoints to call, how to construct requests, and how to interpret responses — effectively violating the HATEOAS principle.&lt;/p&gt;
    &lt;p&gt;Let’s make a concrete example: a customer list that shows the individual customer’s detail on click.&lt;/p&gt;
    &lt;p&gt;With a traditional JSON-API:&lt;/p&gt;
    &lt;code&gt;{
  "customers": [
    {"id": 1, "name": "Mario Rossi", "email": "mario@example.com"},
    {"id": 2, "name": "Luigi Verdi", "email": "luigi@example.com"}
  ]
}
&lt;/code&gt;
    &lt;p&gt;The client receives this data and must know beforehand that to get the detail it needs to call &lt;code&gt;/api/customers/{id}&lt;/code&gt;. This knowledge is hardcoded in the front-end JavaScript code. If the URL changes, the client breaks. If there are access rules that prevent viewing certain customers, the client doesn’t know until it tries to call the endpoint and receives an error.&lt;/p&gt;
    &lt;p&gt;With htmx and the HTML-first approach:&lt;/p&gt;
    &lt;code&gt;&amp;lt;ul&amp;gt;
  &amp;lt;li&amp;gt;
    &amp;lt;a hx-get="/customers/1" hx-target="#detail"&amp;gt;Mario Rossi&amp;lt;/a&amp;gt;
  &amp;lt;/li&amp;gt;
  &amp;lt;li&amp;gt;
    &amp;lt;a hx-get="/customers/2" hx-target="#detail"&amp;gt;Luigi Verdi&amp;lt;/a&amp;gt;
  &amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;
&amp;lt;div id="detail"&amp;gt;&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The server returns the HTML with links already embedded. The client doesn’t need to “know” anything — it simply follows the links present in the response. If a user doesn’t have access to a certain customer, the server simply doesn’t include that link in the list. If the URL changes, the server generates the new links and the client continues working without modifications.&lt;/p&gt;
    &lt;p&gt;Which of the two approaches respects HATEOAS? Only the second. HTML is hypermedia by definition — it was designed exactly for this purpose.&lt;/p&gt;
    &lt;p&gt;To be precise, a well-designed JSON-API should include a &lt;code&gt;links&lt;/code&gt; property for related resource discovery:&lt;/p&gt;
    &lt;code&gt;{
  "customers": [
    {
      "id": 1,
      "name": "Mario Rossi",
      "email": "mario@example.com",
      "links": {
        "self": "/api/customers/1",
        "orders": "/api/customers/1/orders"
      }
    }
  ],
  "links": {
    "self": "/api/customers",
    "next": "/api/customers?page=2"
  }
}
&lt;/code&gt;
    &lt;p&gt;But let’s be honest: the vast majority of JSON-APIs in production don’t implement these links. And even when they do, the problem isn’t solved: the client still needs to have a UI ready and capable of interpreting that data and displaying it appropriately. JSON links tell where to go, but not how to present what’s found. The client still needs to “know” that a customer has a name, an email, and that orders should be displayed in a table with certain columns.&lt;/p&gt;
    &lt;p&gt;With HTML, instead, the representation is already included in the response. No client-side rendering logic needed.&lt;/p&gt;
    &lt;p&gt;With htmx, the server doesn’t just return data, but returns complete user interface representations with links and available actions already embedded. This eliminates an entire category of problems related to state synchronization between client and server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Less Code and Fewer Dependencies 📦&lt;/head&gt;
    &lt;p&gt;One of the biggest benefits is simply a smaller codebase:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No huge bundle to optimize and debug&lt;/item&gt;
      &lt;item&gt;No complicated build pipeline (webpack, babel, typescript config, etc.)&lt;/item&gt;
      &lt;item&gt;Fewer moving parts that can break&lt;/item&gt;
      &lt;item&gt;Easier onboarding for new team members&lt;/item&gt;
      &lt;item&gt;Fewer version upgrade headaches&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This also translates to fewer bugs and faster long-term maintenance.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Hidden Cost of Complexity&lt;/head&gt;
    &lt;p&gt;Every JavaScript library you add to the project brings a cost that goes well beyond the kilobytes of the initial download.&lt;/p&gt;
    &lt;p&gt;Take a typical example: you want to format a date. You install &lt;code&gt;moment.js&lt;/code&gt; (or a modern alternative). But that library has its dependencies, which in turn have others. Suddenly, to format “2025-01-15” as “January 15, 2025”, you’ve added hundreds of kilobytes to your bundle.&lt;/p&gt;
    &lt;p&gt;And it doesn’t end there. Every library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Has its own release cycle with breaking changes&lt;/item&gt;
      &lt;item&gt;Can have security vulnerabilities requiring urgent updates&lt;/item&gt;
      &lt;item&gt;Must be compatible with all other project libraries&lt;/item&gt;
      &lt;item&gt;Adds time to build and deploy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Many developers install libraries like lodash, axios, or moment just to use a single function. It’s like buying an entire toolbox just to use the screwdriver.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;⚠️ Warning: Every dependency is a potential breaking point. JavaScript supply chain incidents continue to occur regularly — from compromised packages to maintainers abandoning critical projects. Fewer dependencies = fewer risks.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With the HTML-first approach, much of this complexity simply disappears. Date formatting happens on the server (where you have full control over the format), HTTP requests are handled by htmx with a few attribute lines, and the DOM is automatically updated with the received HTML. No need to reinvent the wheel with 200 KB of JavaScript.&lt;/p&gt;
    &lt;head rend="h2"&gt;Progressive Enhancement: Evolution, Not Rewrite 🔄&lt;/head&gt;
    &lt;p&gt;You can add htmx to almost any existing backend without overhauling the entire project structure.&lt;/p&gt;
    &lt;p&gt;Need a dynamic table? Enhance that section.&lt;/p&gt;
    &lt;p&gt;Need modal interactions without a full SPA? Swap in the HTML on the fly.&lt;/p&gt;
    &lt;p&gt;It’s not an “all or nothing” decision — you improve the interface step by step.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Three Levels of Progressive Enhancement&lt;/head&gt;
    &lt;p&gt;Progressive enhancement is based on three fundamental principles:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Content First: At the heart of every website is its content. Progressive enhancement ensures that content is accessible to all users, even those with very basic browsers or slow Internet connections. This means starting from a solid HTML foundation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Basic Functionality: Ensure that core features work without advanced JavaScript. With htmx this is possible if you design carefully: links can have a standard&lt;/p&gt;&lt;code&gt;href&lt;/code&gt;as fallback, forms can work with a normal submit. htmx enhances HTML’s standard behavior. However, it’s important to note that features based on buttons with&lt;code&gt;hx-get&lt;/code&gt;or&lt;code&gt;hx-post&lt;/code&gt;(that aren’t form submits) require JavaScript — it’s up to the developer to decide where graceful degradation is important and design accordingly.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Experiences: Progressively add advanced features for browsers and devices that support them.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wikipedia, powered by MediaWiki, is a perfect example: it’s readable, navigable, and even editable using the basic HTML interface without styling or scripts, but it’s enhanced when these are available.&lt;/p&gt;
    &lt;head rend="h2"&gt;SEO, Accessibility, and Browser Behavior Just Work 🌐&lt;/head&gt;
    &lt;p&gt;When your app uses real HTML instead of a Virtual DOM, you avoid a lot of accidental problems that SPAs introduce. The back button, deep linking, accessibility tools, and SEO all behave naturally.&lt;/p&gt;
    &lt;p&gt;No hacks required.&lt;/p&gt;
    &lt;head rend="h3"&gt;SEO Benefits&lt;/head&gt;
    &lt;p&gt;Since the base content is always accessible to search engine spiders, pages built with progressive enhancement methods avoid the problems that can hinder indexing, while having to render the page’s base content through JavaScript execution makes crawling slow and inefficient.&lt;/p&gt;
    &lt;p&gt;This strategy speeds up loading and makes crawling by search engines easier, as the text on the page loads immediately through the HTML source code rather than having to wait for JavaScript to initialize and load content later.&lt;/p&gt;
    &lt;p&gt;Search engines prioritize accessible and easy-to-read content. Starting from a clean HTML structure and ensuring content is available to all users improves the chances of ranking well in search results.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready for the AI Search Era&lt;/head&gt;
    &lt;p&gt;There’s an aspect that many developers aren’t yet considering: the rise of AI-based search engines. Tools like ChatGPT Search, Perplexity, Google AI Overviews, and others are radically changing how users find information online.&lt;/p&gt;
    &lt;p&gt;These AI systems need clear, structured, and immediately accessible content. While some modern crawlers can execute JavaScript, execution is often limited, delayed, or incomplete. A SPA that loads content dynamically via JavaScript risks being indexed partially, inaccurately, or with significant delays compared to sites with immediate HTML content.&lt;/p&gt;
    &lt;p&gt;With the HTML-first approach, content is already present in the HTML document served by the server. AI crawlers (as well as traditional search engine spiders) can immediately access, understand, and index all content. No waiting for JavaScript execution, no risk of parts of the page not being seen.&lt;/p&gt;
    &lt;p&gt;In a world where more and more traffic will come from AI-generated responses, having easily accessible and well-structured content is no longer just a best practice — it’s a competitive necessity.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;🤖 Prepare for the future: AI crawlers are becoming increasingly important. A site invisible to AI is a site losing opportunities. HTML-first puts you in pole position.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Accessibility Benefits&lt;/head&gt;
    &lt;p&gt;One of the most significant benefits of progressive enhancement is improved accessibility. Starting from a solid HTML foundation ensures that content is accessible to all users, including those with disabilities.&lt;/p&gt;
    &lt;p&gt;Screen readers and other assistive technologies work natively with semantic HTML. SPAs, on the other hand, often require complex ARIA implementations and specialized testing to reach the same level of accessibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;htmx: The Rising Star ⭐&lt;/head&gt;
    &lt;p&gt;htmx is experiencing impressive growth. According to JavaScript Rising Stars 2024, htmx gained more annual GitHub stars than more established libraries like Vue and Angular (note: this refers to annual star growth, not total).&lt;/p&gt;
    &lt;head rend="h3"&gt;The Numbers of Success&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the Stack Overflow Developer Survey 2024, htmx is the 22nd most used web framework with 3.3% of developers using it&lt;/item&gt;
      &lt;item&gt;In terms of satisfaction, htmx is the 2nd most “admired” web framework with 72.9% in the Stack Overflow 2024 survey, second only to Elixir’s Phoenix framework (83.7%)&lt;/item&gt;
      &lt;item&gt;In the Django Developers Survey, htmx usage went from 5% in 2021 to 16% in 2022 — 220% growth&lt;/item&gt;
      &lt;item&gt;htmx 2.0.0 was released on June 17, 2024, marking an important maturity milestone&lt;/item&gt;
      &lt;item&gt;htmx was admitted to the GitHub Accelerator in 2023, a program that selects the most promising open source projects&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What Makes htmx Special&lt;/head&gt;
    &lt;p&gt;htmx is small (~14-16 KB min.gz), dependency-free, extensible, and according to reports, has reduced code size by 67% compared to React in comparable projects.&lt;/p&gt;
    &lt;p&gt;htmx doesn’t just reduce bundle size — it eliminates the need for Virtual DOM diffing, component lifecycles, and client-side state orchestration. The result is faster page loads, less code to debug, and a lighter mental model for building user interfaces.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ideal for Enterprise Apps, Dashboards, and Portals 💼&lt;/head&gt;
    &lt;p&gt;Not every application is a highly interactive design tool. Many of the systems companies build — booking platforms, dashboards, admin tools, forms, internal portals — fit perfectly with this model.&lt;/p&gt;
    &lt;p&gt;They don’t need 500 KB of JavaScript to handle basic interactions.&lt;/p&gt;
    &lt;p&gt;HTML-first frameworks strike a good balance between interactivity and maintainability.&lt;/p&gt;
    &lt;head rend="h3"&gt;Perfect Use Cases for HTML-First&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Administrative dashboards: Tables with pagination, filters, CRUD actions&lt;/item&gt;
      &lt;item&gt;Internal portals: Document management, workflow approval, reporting&lt;/item&gt;
      &lt;item&gt;E-commerce backend: Order management, inventory, customers&lt;/item&gt;
      &lt;item&gt;Complex multi-step forms: Registration wizards, product configurators&lt;/item&gt;
      &lt;item&gt;Booking systems: Calendars, reservations, slot management&lt;/item&gt;
      &lt;item&gt;CRM and ERP: Contact management, sales pipeline, invoicing&lt;/item&gt;
      &lt;item&gt;Data-entry applications: Data import/export, bulk editing&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;htmx and Delphi: A Winning Combination 🚀&lt;/head&gt;
    &lt;p&gt;For Delphi developers, the HTML-first approach with htmx represents a particularly interesting opportunity. DelphiMVCFramework offers excellent support for this paradigm, allowing you to leverage the power and reliability of a Delphi backend with a modern, lightweight front-end.&lt;/p&gt;
    &lt;head rend="h3"&gt;Benefits for Delphi Developers&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Robust backend: Delphi’s solidity and performance on the server&lt;/item&gt;
      &lt;item&gt;Powerful template engines: TemplatePro or WebStencils for dynamic HTML generation&lt;/item&gt;
      &lt;item&gt;Simple deployment: A single executable without node.js dependencies&lt;/item&gt;
      &lt;item&gt;No JavaScript build: Goodbye webpack, npm, and complex tool chains&lt;/item&gt;
      &lt;item&gt;Reusable skills: Delphi developers can be productive immediately&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The quickstart projects available on GitHub (TemplatePro + htmx and WebStencils + htmx) offer an ideal starting point.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;💡 Tip: If you’re a Delphi developer and want to start with htmx, clone one of the quickstart projects and you’ll have a working application in minutes. No npm configuration, no webpack, no 500 MB node_modules.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;When NOT to Use the HTML-First Approach ⚖️&lt;/head&gt;
    &lt;p&gt;It’s important to be honest: the HTML-first approach isn’t the solution for everything. Here’s when traditional SPAs remain the better choice:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Highly interactive real-time applications: Graphic editors, collaboration tools like Google Docs, video editing applications&lt;/item&gt;
      &lt;item&gt;Offline-first applications: When the application needs to work significantly without connection&lt;/item&gt;
      &lt;item&gt;Games and 3D applications: Where intensive client-side rendering is necessary&lt;/item&gt;
      &lt;item&gt;Applications with complex client state: Where the interface state is significantly different from server state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For these types of applications, SPAs offer concrete advantages: sophisticated local state management, fluid transitions between complex views, and the ability to work offline with service workers.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Future: A Hybrid Approach 🔮&lt;/head&gt;
    &lt;p&gt;The trend we’re observing isn’t a return to the past, but an evolution toward a more pragmatic approach. The best developers are learning to choose the right tool for the specific problem.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Islands Architecture Paradigm&lt;/head&gt;
    &lt;p&gt;An emerging trend is “Islands Architecture”, where most of the page is static or server-rendered HTML, with JavaScript interactivity “islands” only where needed. Frameworks like Astro.js (with an impressive 94% of users who would use it again according to State of JavaScript 2024) are exploring this territory.&lt;/p&gt;
    &lt;p&gt;htmx fits perfectly into this paradigm, allowing you to add interactivity where needed without requiring a completely client-side architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions 🎯&lt;/head&gt;
    &lt;p&gt;React and Angular absolutely have their place, and they’re great when you really need a complex client-side application. But for many projects, the HTML-first approach is faster to build, easier to maintain, and lighter on the browser.&lt;/p&gt;
    &lt;p&gt;It’s not a step backward — it’s a reminder that the web already gives us everything we need to build powerful, responsive applications without the extra weight.&lt;/p&gt;
    &lt;p&gt;With htmx now having over 72% satisfaction among developers, explosive GitHub star growth, and adoption by increasingly larger teams, it’s clear this isn’t just a passing trend. It’s a rediscovery of fundamental web principles, empowered by modern tools that make the development experience smooth and enjoyable.&lt;/p&gt;
    &lt;p&gt;The next time you start a new web project, before automatically reaching for React or Vue, ask yourself: “Do I really need all this?” The answer might surprise you.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;🚀 Ready to try? Start with a small project or improve a section of an existing application. htmx doesn’t require a complete rewrite — you can adopt it gradually and see the benefits immediately.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Resources to Learn More 📚&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Official htmx site&lt;/item&gt;
      &lt;item&gt;htmx Documentation&lt;/item&gt;
      &lt;item&gt;DelphiMVCFramework + TemplatePro + htmx Quickstart&lt;/item&gt;
      &lt;item&gt;DelphiMVCFramework + WebStencils + htmx Quickstart&lt;/item&gt;
      &lt;item&gt;JavaScript Rising Stars 2024&lt;/item&gt;
      &lt;item&gt;HTTP Archive - Page Weight&lt;/item&gt;
      &lt;item&gt;Stack Overflow Developer Survey 2024&lt;/item&gt;
      &lt;item&gt;State of JavaScript 2024&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;🎓 Want to dive deeper with a course? bit Time Professionals offers dedicated courses on htmx and on the DelphiMVCFramework + TemplatePro + htmx integration. Taking a structured course allows you to be productive right away, with practical applications, real examples, proven architectural patterns, and best practices learned from years of field experience. Courses are available in Italian, English, and Spanish.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239445</guid><pubDate>Fri, 12 Dec 2025 00:37:42 +0000</pubDate></item><item><title>CRISPR fungus: Protein-packed, sustainable, and tastes like meat</title><link>https://www.isaaa.org/kc/cropbiotechupdate/article/default.asp?ID=21607</link><description>&lt;doc fingerprint="663d925d0c255a11"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;CRISPR Fungus: Protein-Packed, Sustainable, and Tastes Like Meat&lt;/head&gt;November 26, 2025&lt;table&gt;&lt;row/&gt;&lt;/table&gt;&lt;p&gt;Researchers have successfully used CRISPR gene editing technology to create a fungi strain that is highly efficient, more nutritious, and significantly more sustainable than its natural counterpart. The fungus Fusarium venenatum already stands out for its meat-like flavor and texture, leading to its approval for food use in several countries. This breakthrough, published in the journal Trends in Biotechnology, addresses the need for better, more environmentally friendly alternatives to conventional animal agriculture, which accounts for about 14% of global greenhouse gas emissions.&lt;/p&gt;&lt;p&gt;The scientists, led by corresponding author Xiao Liu of Jiangnan University, used CRISPR to remove two specific genes. The first modification, eliminating a gene for chitin synthase, resulted in thinner fungal cell walls. This change is crucial as it makes the fungal protein easier for humans to digest and increases its bioavailability. The second change involved removing the pyruvate decarboxylase gene, which optimized the fungus's metabolism. This fine-tuning made the new strain, called FCPD, more productive, requiring 44% less sugar to produce the same amount of protein and doing so 88% faster than the original strain.&lt;/p&gt;&lt;p&gt;When scaled up, FCPD production showed a lower environmental footprint regardless of the manufacturing location, reducing greenhouse gas emissions by up to 60% over its life cycle compared to traditional fungal protein production. Furthermore, compared to chicken production in China, the new myoprotein requires 70% less land and reduces the risk of freshwater pollution by 78%. According to the researchers, this type of gene-edited food can help meet global food demands without the substantial environmental costs associated with conventional farming, representing a major advancement in the field of sustainable food technology.&lt;/p&gt;&lt;p&gt;For more details, read this article or download the open-access paper.&lt;/p&gt;&lt;table&gt;&lt;row/&gt;&lt;/table&gt;&lt;head rend="h3"&gt;You might also like:&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Experts Highlight Hybrid Foods as Meat Alternatives&lt;/item&gt;&lt;item&gt;Gene Editing of Edible Fungus Produces Healthier Foods&lt;/item&gt;&lt;item&gt;Experts Say Effective Communication Can Promote Consumer Acceptance of Cultured Meat&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Biotech Updates is a weekly newsletter of ISAAA, a not-for-profit organization. It is distributed for free to over 22,000 subscribers worldwide to inform them about the key developments in biosciences, especially in biotechnology. Your support will help us in our mission to feed the world with knowledge. You can help by donating as little as $10.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;head rend="h3"&gt;See more articles:&lt;/head&gt;&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Plant&lt;/head&gt;&lt;/item&gt;&lt;item&gt;CRISPR Alters Chromosome Numbers in Plants&lt;/item&gt;&lt;item&gt;Precision Breeding Act Now Live in the UK&lt;/item&gt;&lt;item&gt;Philippine Regulators Work on Refining Risk Assessment Report on GM Plant and Plant Products&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Animal&lt;/head&gt;&lt;/item&gt;&lt;item&gt;Targeted Inheritance of Sex Offers a New Method to Improve Animal Breeding&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Food&lt;/head&gt;&lt;/item&gt;&lt;item&gt;Single Gene Discovery Promises Better Tea Taste and Yield&lt;/item&gt;&lt;item&gt;CRISPR Fungus: Protein-Packed, Sustainable, and Tastes Like Meat&lt;/item&gt;&lt;item&gt;&lt;head rend="h4"&gt;Environment&lt;/head&gt;&lt;/item&gt;&lt;item&gt;UN Climate Change Conference Highlights Role of Agrifood Systems in Climate Action&lt;/item&gt;&lt;item&gt;University of Waterloo Researchers Turn to Biotechnology to Combat Plastic Pollution&lt;/item&gt;&lt;item&gt;&lt;lb/&gt;Read the latest:&lt;/item&gt;&lt;item&gt;Biotech Updates (December 10, 2025)&lt;/item&gt;&lt;item&gt;Gene Editing Supplement (November 26, 2025)&lt;/item&gt;&lt;item&gt;Gene Drive Supplement (February 22, 2023)&lt;/item&gt;&lt;item&gt;&lt;lb/&gt;Subscribe to BU:&lt;/item&gt;&lt;item&gt;Share&lt;/item&gt;&lt;item&gt;Tweet&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239629</guid><pubDate>Fri, 12 Dec 2025 00:59:46 +0000</pubDate></item><item><title>Notes on Gamma</title><link>https://poniesandlight.co.uk/reflect/gamma/</link><description>&lt;doc fingerprint="2ac9d0535d52ab95"&gt;
  &lt;main&gt;
    &lt;p&gt;Gamma is a blight, a curse, and utterly annoying. Ever since somebody told me that RGB colours need to be Gamma corrected, RGB colours were spoilt for me. Gamma does to digital colour what kerning does to typography.&lt;/p&gt;
    &lt;p&gt;This post is an attempt to get even with Gamma.&lt;/p&gt;
    &lt;p&gt;Because, you see, only once I became fully aware of Gamma, things really started to fall apart. In my pre-gamma-aware innocence, I must have done some things right.&lt;/p&gt;
    &lt;p&gt;Let me show what I mean:&lt;/p&gt;
    &lt;p&gt;Here, I generate a linear gradient using a GLSL fragment shader. Say, drawing a full-screen quad using this shader code snippet:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;And voila - a perceptually linear gradient.&lt;/p&gt;
    &lt;p&gt;But now, let’s get clever about this: We notice that we’re actually drawing to an sRGB monitor (most desktop monitors are, nowadays), so we should probably use an sRGB image for the swapchain (these are the most common 8bpc swapchain image formats in Vulkan). Thus we somehow need to convert our RGB colours to sRGB. The most elegant way to do this is to use an image attachment with an sRGB format, which (at least in Vulkan) does the conversion automatically and precisely on texel save.&lt;/p&gt;
    &lt;p&gt;If we draw the same shader as before, but now into an sRGB swapchain the swapchain image’s non-linear rRGB encoding should correct the sRGB monitor gamma. The gradient’s brightness values (as measured by a brigthness meter) should now increase linearly on-screen. They do. That should look like a linear gradient, right?&lt;/p&gt;
    &lt;p&gt;Wrong.&lt;/p&gt;
    &lt;p&gt;Instead, we get this:&lt;/p&gt;
    &lt;p&gt;This doesnât look linear: shades donât seem to claim equal space. Instead, it looks as if dark shades are too bright, while bright shades wash out. Hereâs what Iâd expected to see:&lt;/p&gt;
    &lt;p&gt;The difference is subtle, but look at both gradients and ask yourself: which seems to have more detail? Which is more balanced?&lt;/p&gt;
    &lt;p&gt;Even though the first gradient is more linear in terms of physical brightness, the second one looks more linear.&lt;/p&gt;
    &lt;p&gt;I find this counter-intuitive. But where intuition fails, ratio may help; and there is indeed a rational explanation: Visual perception is non-linear.&lt;/p&gt;
    &lt;p&gt;The human eye can distinguish more contrast detail in darker shades.&lt;/p&gt;
    &lt;p&gt;You don’t have to take my word for it; instead take those of Dr. Charles Poynton – the person who gave HDTV square pixels and the number 1080. Here is a diagram of how our perception tends to respond to changes in lightness, which I found in his dissertation:&lt;/p&gt;
    &lt;p&gt;This is fine. Nature. It probably helped our ancestors to survive or something. And it does explain why our physically linear gradient looked too bright in dark areas. Let’s draw a diagram:&lt;/p&gt;
    &lt;p&gt;Whenever we want to draw a perceptually linear gradient, we must remember to pay our dues to evolution, and factor in this perceptual bias.&lt;/p&gt;
    &lt;p&gt;If you want the appearance of a linear gradient, you must display a non-linear gradient, one that tunes down darker parts. Effectively, you want to apply the inverse of the non-linearity that is introduced by perception. Confusingly, this may be done automatically for you if you forget to do any gamma correction:&lt;/p&gt;
    &lt;p&gt;If you have an sRGB monitor and you innocently don’t do any sRGB correction (by rendering into a linear RGB framebuffer such as &lt;code&gt;FORMAT_R8G8B8A8_UNORM&lt;/code&gt; for example), linear gradient values will get biased by the monitor’s gamma response alone – the result will be a gradient that looks “about linear”.&lt;/p&gt;
    &lt;p&gt;It looks “about linear” because what happens is that while the monitor will “gamma” the gradient, your eye will “de-gamma” the gradient again – and since these two non-linear effects on the signal (monitor, eye) are almost inverses of each other, we get a linear perceived signal at the end.&lt;/p&gt;
    &lt;p&gt;And here’s a cool thing: This was by design!&lt;/p&gt;
    &lt;quote&gt;"The nonlinearity of a CRT is very nearly the inverse of the lightness sensitivity of human vision. The nonlinearity causes a CRTâs response to be roughly perceptually uniform. Far from being a defect, this feature is highly desirable."&lt;/quote&gt;
    &lt;p&gt;A big reason for encoding images in sRGB is that, because of sRGB’s perceptual nature, we get much better perceptual luminance contrast resolution out of 8bits per channel. Instead of wasting bits on high brightnesses where our eye has trouble noticing change, we spend most of the bit-budget where it counts: on darker shades.&lt;/p&gt;
    &lt;p&gt;sRGB is an elegant form of perceptual compression: images at 8-bits-per-channel and below (for reasons of encoding efficiency) really want to be encoded as sRGB.&lt;/p&gt;
    &lt;p&gt;And if the monitor can display these sRGB images directly and natively (because the hardware applies the inverse of the sRGB gamma transform) – that’s just a perfect match…&lt;/p&gt;
    &lt;p&gt;In Vulkan, if you can use an sRGB format for your swapchain image, then you don’t have to manually correct for sRGB gamma. your pixels will be automatically stored in non-linear sRGB, and displayed linearly on-screen. this is great for encoding the highest amount of perceptual colour contrast detail using limited amount of bits (sRGB formats are usually about 8 bit per channel).&lt;/p&gt;
    &lt;p&gt;When image format names contain the suffix &lt;code&gt;*_SRGB&lt;/code&gt; the sRGB gamma transform is applied transparently   on every texel read   and texel write  . This is useful, because we can only meaningfully blend color in linear space, blending in non-linear space would not be (physically) correct. The specs on Khronos Data Formats have some great documentation on this topic.&lt;/p&gt;
    &lt;p&gt;But this means that you need to undo the effect of the implicit sRGB gamma transform on texel write if you want to render a perceptually linear gradient while using an sRGB image backing. The function that the vulkan driver applies for you on texel write is called the &lt;code&gt;srgb_oetf&lt;/code&gt;, short for “sRGB optical-electrical transfer function”.&lt;/p&gt;
    &lt;p&gt;To neutralise this function, you must apply the &lt;code&gt;inverse_srgb_oetf&lt;/code&gt;, that’s the &lt;code&gt;srgb_eotf&lt;/code&gt; “sRGB electo-optical transfer function”, just before you store the texel.&lt;/p&gt;
    &lt;p&gt;Here is how this would look like using our schematic from before:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;And voila:&lt;/p&gt;
    &lt;p&gt;Thank you sticking around past the end credits. Here’s an extra bit of information that might come in handy at a cgi pub quiz one day:&lt;/p&gt;
    &lt;p&gt;Did you ever wonder what the “s” in RGB stands for? Me neither – until now; I assumed it stood for super. The sad reality is more humble: Apparently it stands for Standard. “Standard RGB”. What a standard.&lt;/p&gt;
    &lt;p&gt;Find out first about new posts by subscribing to the RSS Feed&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239673</guid><pubDate>Fri, 12 Dec 2025 01:05:30 +0000</pubDate></item><item><title>Google De-Indexed My Bear Blog and I Don't Know Why</title><link>https://journal.james-zhan.com/google-de-indexed-my-entire-bear-blog-and-i-dont-know-why/</link><description>&lt;doc fingerprint="150021f738f60ba3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google De-Indexed My Entire Bear Blog and I Don’t Know Why&lt;/head&gt;
    &lt;p&gt;Preamble: The whole affair is Google’s fault and not Bear Blog’s. Huge thanks to Herman—Bear Blog’s founder and dev—for his patience and help.&lt;/p&gt;
    &lt;p&gt;A month after I started my first Bear blog at blog.james-zhan.com, my blog was entirely de-indexed by Google for no apparent reason:&lt;/p&gt;
    &lt;p&gt;I have since migrated to journal.james-zhan.com (you are on it right now) and redirected all links from blog.james-zhan.com accordingly, but to this day, I don’t understand what happened, and so I’m putting this post out there to see if perhaps anyone could shed some light—you are welcome to email me or leave a comment at the bottom of the post.&lt;/p&gt;
    &lt;p&gt;Let me backtrack and show you how it all went down.&lt;/p&gt;
    &lt;head&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;At first, all was well&lt;lb/&gt; Where it started to go wrong&lt;lb/&gt; Was that a coincidence or the cause of the issue?&lt;lb/&gt; It got worse&lt;lb/&gt; Extensive troubleshooting&lt;lb/&gt; Suspect #1: Something’s up with the domain&lt;lb/&gt; Suspect #2: Quality of blog content&lt;lb/&gt; Suspect #3: Lack of internal linking&lt;lb/&gt; Other suspects, eliminated with Herman’s help (thank you Herman!)&lt;lb/&gt; My blog was properly indexed by other search engines&lt;lb/&gt; What I ended up doing&lt;/p&gt;
    &lt;head rend="h2"&gt;At first, all was well&lt;/head&gt;
    &lt;p&gt;My blog went live on Oct 4 and I published a lengthy, well-research opinion piece commenting on a recent event.&lt;/p&gt;
    &lt;p&gt;Because of that, I wanted the article to show up on Google ASAP so that when people searched about the event, maybe my article would come up. I knew that it could take a while for Google to naturally crawl and index a new site, so to accelerate the process, I went on Google Search Console (GSC), submitted the sitemap and requested indexing on my article.&lt;/p&gt;
    &lt;p&gt;And it worked—the next day, my blog and articles were indexed and showed up on Google if you put in the right search terms.&lt;/p&gt;
    &lt;p&gt;In GSC, you can even see that I was getting some impressions and clicks at the time from the exact topic that my opinion piece was about. Great!&lt;/p&gt;
    &lt;p&gt;From then on, every time I published a new post, I would go to GSC and request indexing for the post URL, and my post would be on Google search results shortly after, as expected.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where it started to go wrong&lt;/head&gt;
    &lt;p&gt;On Oct 14, as I was digging around GSC, I noticed that it was telling me that one of the URLs weren’t indexed. I thought that was weird, and not being very familiar with GSC, I went ahead and clicked the “Validate” button.&lt;/p&gt;
    &lt;p&gt;Only after did I realized that URL was the RSS feed subscribe link, &lt;code&gt;https://blog.james-zhan.com/feed/?type=rss&lt;/code&gt;, which wasn’t even a page so it made sense that it hadn’t been indexed, but it was too late and there was no way for me to stop the validation.&lt;/p&gt;
    &lt;p&gt;I received an email from GSC telling me it was validating that 1 page with indexing issues:&lt;/p&gt;
    &lt;p&gt;Four days later, on Oct 20, I received an email from GSC saying “Some fixes failed for Page indexing issues on site https://blog.james-zhan.com/” and when I searched “site:blog.james-zhan.com,” I saw that all but one of my blog posts had been de-indexed:&lt;/p&gt;
    &lt;p&gt;All of them showed the same reason:&lt;/p&gt;
    &lt;p&gt;“Page is not indexed: Crawled – currently not indexed”&lt;/p&gt;
    &lt;p&gt;Confused, I poked around GSC to see if it showed me why, and I couldn’t find anything useful, so I resubmitted the sitemap for good measure, and clicked “Validate” again.&lt;/p&gt;
    &lt;p&gt;I even requested indexing for all the individual blog post URLs and that didn’t do anything.&lt;/p&gt;
    &lt;p&gt;As of the publishing of this post, the validation status is still “Started” (it’s been nearly 20 days).&lt;/p&gt;
    &lt;head rend="h2"&gt;Was that a coincidence or the cause of the issue?&lt;/head&gt;
    &lt;p&gt;As I was troubleshooting, I noticed that the day that I initiated the validation for the first time (Oct 14) was the same day that all but one of my blog posts got de-indexed:&lt;/p&gt;
    &lt;p&gt;Did my accidental attempt to make GSC index &lt;code&gt;https://blog.james-zhan.com/feed/?type=rss&lt;/code&gt; cause some kind of glitch, thereby de-indexing the rest of the blog?&lt;/p&gt;
    &lt;p&gt;I don’t get why it would, but it’s weird that the two events happened on the same day.&lt;/p&gt;
    &lt;head rend="h2"&gt;It got worse&lt;/head&gt;
    &lt;p&gt;While this was going on, I continued to post a few articles, and you can see that all the new posts faced the “Page is not indexed: Crawled – currently not indexed” error:&lt;/p&gt;
    &lt;p&gt;And then on Nov 3, I discovered that the remaining, single blog post that had been indexed just got de-indexed as well:&lt;/p&gt;
    &lt;p&gt;So basically, no one could find my blog on Google.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extensive troubleshooting&lt;/head&gt;
    &lt;p&gt;I’m not a web dev or programmer, but I tried my best to cover as much ground as possible in my troubleshooting to narrow down the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #1: Something’s up with the domain&lt;/head&gt;
    &lt;p&gt;The root domain, james-zhan.com, was from GoDaddy. I’ve had this domain for many years and I’ve used it on different sites and never had an issue with Google’s indexing.&lt;/p&gt;
    &lt;p&gt;For example, just this year, I created a new subdomain with it and that’s been indexed by Google.&lt;/p&gt;
    &lt;p&gt;I also don’t touch any advanced configuration with DNS records or what have you—I don’t have knowledge in that stuff, so it’s unlikely I somehow screwed up something in GoDaddy.&lt;/p&gt;
    &lt;p&gt;But just to be sure it wasn’t some wonky thing going on specifically with the Bear blog + GoDaddy combo, I created another Bear blog with the subdomain www.james-zhan.com.&lt;/p&gt;
    &lt;p&gt;This one shows up on Google no problem.&lt;/p&gt;
    &lt;p&gt;Conclusion: Domain wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #2: Quality of blog content&lt;/head&gt;
    &lt;p&gt;Whenever people discuss the indexing of their website in online forums, they always talk about the quality of the content being a huge factor. They say that your site isn’t indexed or isn’t ranked highly because your site doesn’t have much content, your content is low effort, or something like that.&lt;/p&gt;
    &lt;p&gt;First, I’m not worried about ranking—I just want my blog to be properly indexed.&lt;/p&gt;
    &lt;p&gt;Second, the issue couldn’t be the quality or the quantity of the content. I came across some other pretty barebones Bear blogs that don’t have much content, and looked them up on Google, and they showed up in the results just fine.&lt;/p&gt;
    &lt;p&gt;An example: Phong’s blog. It’s a very minimalist blog with only 6 posts (of great quality) and it shows up on Google search.&lt;/p&gt;
    &lt;p&gt;Conclusion: Quality or quantity of content wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Suspect #3: Lack of internal linking&lt;/head&gt;
    &lt;p&gt;I read about how the structure of a site can play a role in Google’s indexing.&lt;/p&gt;
    &lt;p&gt;Some say that if your blog posts’ URLs are all “orphaned,” like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;domain.com/post-title-1&lt;/item&gt;
      &lt;item&gt;domain.com/post-title-2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…instead of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;domain.com/blog/post-title-1&lt;/item&gt;
      &lt;item&gt;domain.com/blog/post-title-2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Allegedly, that might cause Google to not index your posts. By default, when you publish a post on Bear Blog, the blog post’s path isn’t preceded by “blog/.”&lt;/p&gt;
    &lt;p&gt;So I went around and checked the post URLs of other Bear blogs and saw that none of them had “/blog/” in them, and those blogs were indexed just fine. I also highly doubt it’s a real issue; otherwise, it wouldn’t be the default behaviour on Bear Blog.&lt;/p&gt;
    &lt;p&gt;Conclusion: Lack of internal linking wasn’t the cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Other suspects, eliminated with Herman’s help (thank you Herman!)&lt;/head&gt;
    &lt;p&gt;I reached out to Herman with all the details and asked him for help. Of course, he responded promptly and helped me troubleshoot to identify the cause.&lt;/p&gt;
    &lt;p&gt;He was able to confirm and the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GoDaddy and DNS weren’t the cause&lt;/item&gt;
      &lt;item&gt;My bear blog had nothing that would prevent Google from indexing&lt;/item&gt;
      &lt;item&gt;HTML/CSS doesn’t affect SEO/indexing&lt;list rend="ul"&gt;&lt;item&gt;I had the following CSS code to put the tags above the blog post title, but Herman said this was fine&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;/* --- Move tags above the title --- */ main { display: flex; flex-direction: column; } /* Style and reposition the tags */ main &amp;gt; p.tags { order: -1; /* Moves tags above the title */ margin: 0 0 0.6rem 0; font-size: 0.9em; letter-spacing: 0.02em; color: var(--heading-color); opacity: 0.8; } /* Keep the title below tags */ main &amp;gt; h1 { order: 0; }&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I just wanted to take a moment to express my gratitude to Herman for investigating this with me. My emails to him were pretty elaborate with troubleshooting steps I had taken along with many screenshots. He took the time to fully understand the whole issue and even triple-checked my site to make sure everything was sound.&lt;/p&gt;
    &lt;p&gt;It was a refreshing tech support experience, and made me love Bear Blog as a platform just that much more.&lt;/p&gt;
    &lt;head rend="h3"&gt;My blog was properly indexed by other search engines&lt;/head&gt;
    &lt;p&gt;I don’t even have to use “site:”—just by searching “James Zhan blog,” both my blog and my www.james-zhan.com site show up in other search engines:&lt;/p&gt;
    &lt;p&gt;DuckDuckGo:&lt;/p&gt;
    &lt;p&gt;Bing:&lt;/p&gt;
    &lt;p&gt;Brave:&lt;/p&gt;
    &lt;p&gt;So there’s definitely nothing wrong on a technical level with my blog that would prevent Google from indexing it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What I ended up doing&lt;/head&gt;
    &lt;p&gt;I copied my blog over to a different subdomain (you are on it right now), moved my domain from GoDaddy to Porkbun for URL forwarding, and set up URL forwarding with paths so any blog post URLs I posted online will automatically be redirected to the corresponding blog post on this new blog.&lt;/p&gt;
    &lt;p&gt;I also avoided submitting the sitemap of the new blog to GSC. I’m just gonna let Google naturally index the blog this time. Hopefully, this new blog won’t run into the same issue.&lt;/p&gt;
    &lt;p&gt;At this point, I’m no longer trying to resolve the issue, but just out of curiosity, I do want to know what the hell happened there. I’d had a previous site on GSC to track traffic for many years and never had such an issue.&lt;/p&gt;
    &lt;p&gt;If any of you have any guesses, I’d love to hear them (email me or leave a comment below)!&lt;/p&gt;
    &lt;p&gt;Subscribe to my blog via email or RSS feed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239752</guid><pubDate>Fri, 12 Dec 2025 01:20:05 +0000</pubDate></item><item><title>Cadmium Zinc Telluride: The wonder material powering a medical 'revolution'</title><link>https://www.bbc.com/news/articles/c24l223d9n7o</link><description>&lt;doc fingerprint="17cac8161cb484d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'It's amazing' – the wonder material very few can make&lt;/head&gt;
    &lt;p&gt;Lying on your back in a big hospital scanner, as still as you can, with your arms above your head – for 45 minutes. It doesn't sound much fun.&lt;/p&gt;
    &lt;p&gt;That's what patients at Royal Brompton Hospital in London had to do during certain lung scans, until the hospital installed a new device last year that cut these examinations down to just 15 minutes.&lt;/p&gt;
    &lt;p&gt;It is partly thanks to image processing technology in the scanner but also a special material called cadmium zinc telluride (CZT), which allows the machine to produce highly detailed, 3D images of patients' lungs.&lt;/p&gt;
    &lt;p&gt;"You get beautiful pictures from this scanner," says Dr Kshama Wechalekar, head of nuclear medicine and PET. "It's an amazing feat of engineering and physics."&lt;/p&gt;
    &lt;p&gt;The CZT in the machine, which was installed at the hospital last August, was made by Kromek – a British company. Kromek is one of just a few firms in the world that can make CZT. You may never have heard of the stuff but, in Dr Wechalekar's words, it is enabling a "revolution" in medical imaging.&lt;/p&gt;
    &lt;p&gt;This wonder material has many other uses, such as in X-ray telescopes, radiation detectors and airport security scanners. And it is increasingly sought-after.&lt;/p&gt;
    &lt;p&gt;Investigations of patients' lungs performed by Dr Wechalekar and her colleagues involve looking for the presence of many tiny blood clots in people with long Covid, or a larger clot known as a pulmonary embolism, for example.&lt;/p&gt;
    &lt;p&gt;The £1m scanner works by detecting gamma rays emitted by a radioactive substance that is injected into patients' bodies.&lt;/p&gt;
    &lt;p&gt;But the scanner's sensitivity means less of this substance is needed than before: "We can reduce doses about 30%," says Dr Wechalekar. While CZT-based scanners are not new in general, large, whole-body scanners such as this one are a relatively recent innovation.&lt;/p&gt;
    &lt;p&gt;CZT itself has been around for decades but it is notoriously difficult to manufacture. "It has taken a long time for it to develop into an industrial-scale production process," says Arnab Basu, founding chief executive of Kromek.&lt;/p&gt;
    &lt;p&gt;In the company's facility at Sedgefield, there are 170 small furnaces in a room that Dr Basu describes as looking "like a server farm".&lt;/p&gt;
    &lt;p&gt;A special powder is heated up in these furnaces, turned molten, and then solidified into a single-crystal structure. The whole process takes weeks. "Atom by atom, the crystals are rearranged […] so they become all aligned," says Dr Basu.&lt;/p&gt;
    &lt;p&gt;The newly formed CZT, a semiconductor, can detect tiny photon particles in X-rays and gamma rays with incredible precision – like a highly specialised version of the light-sensing, silicon-based image sensor in your smartphone camera.&lt;/p&gt;
    &lt;p&gt;Whenever a high energy photon strikes the CZT, it mobilises an electron and this electrical signal can be used to make an image. Earlier scanner technology used a two-step process, which was not as precise.&lt;/p&gt;
    &lt;p&gt;"It's digital," says Dr Basu. "It's a single conversion step. It retains all the important information such as timing, the energy of the X-ray that is hitting the CZT detector – you can create colour, or spectroscopic images."&lt;/p&gt;
    &lt;p&gt;He adds that CZT-based scanners are currently in use for explosives detection at UK airports, and for scanning checked baggage in some US airports. "We expect CZT to come into the hand luggage segment over the next [few] years."&lt;/p&gt;
    &lt;p&gt;But it's not always easy to get your hands on CZT.&lt;/p&gt;
    &lt;p&gt;Henric Krawczynski at Washington University in St Louis in the US has used the material before on space telescopes attached to high altitude balloons. These detectors can pick up X-rays emitted by both neutron stars and plasma around black holes.&lt;/p&gt;
    &lt;p&gt;Prof Krawczynski wants very thin, 0.8mm pieces of CZT for his telescopes because this helps to reduce the amount of background radiation they pick up, allowing for a clearer signal. "We'd like to buy 17 new detectors," he says. "It's really difficult to get these thin ones."&lt;/p&gt;
    &lt;p&gt;He was unable to source the CZT from Kromek. Dr Basu says his firm has high demand at the moment. "We support many, many research organisations," he adds, "It's very difficult for us to do a hundred different things. Each research [project] needs a very particular type of detector structure."&lt;/p&gt;
    &lt;p&gt;For Prof Krawczynski, it's not a crisis – he says he might use either CZT that he has from previous research, or cadmium telluride, an alternative, for his next mission.&lt;/p&gt;
    &lt;p&gt;However, there are bigger headaches at the moment. That upcoming mission was due to fly from Antarctica in December but "all the dates are in flux", says Prof Krawczynski, because of the US government shutdown.&lt;/p&gt;
    &lt;p&gt;Many other scientists use CZT. In the UK, a major upgrade of the Diamond Light Source research facility in Oxfordshire – costing half a billion pounds – will improve its capabilities thanks to the installation of CZT-based detectors.&lt;/p&gt;
    &lt;p&gt;Diamond Light Source is a synchrotron, which fires electrons around a giant ring at nearly the speed of light. Magnets cause these whizzing electrons to lose some energy in the form of X-rays, and these are directed off from the ring in beamlines so that they may be used to analyse materials, for example.&lt;/p&gt;
    &lt;p&gt;Some recent experiments have involved probing impurities in aluminium while it melts. Understanding those impurities better could help improve recycled forms of the metal.&lt;/p&gt;
    &lt;p&gt;With Diamond Light Source's upgrade, due to complete in 2030, the X-rays produced will be significantly brighter, meaning that existing sensors would not be able to detect them properly.&lt;/p&gt;
    &lt;p&gt;"There's no point in spending all this money in upgrading these facilities if you can't detect the light they produce," says Matt Veale, group leader for detector development at the Science and Technology Facilities Council, which is the majority owner of Diamond Light Source.&lt;/p&gt;
    &lt;p&gt;That's why, here too, CZT is the material of choice.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46239895</guid><pubDate>Fri, 12 Dec 2025 01:41:15 +0000</pubDate></item></channel></rss>