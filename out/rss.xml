<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 24 Nov 2025 13:06:27 +0000</lastBuildDate><item><title>We stopped roadmap work for a week and fixed bugs</title><link>https://lalitm.com/fixits-are-good-for-the-soul/</link><description>&lt;doc fingerprint="3623d0560ff62cdd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We stopped roadmap work for a week and fixed 189 bugs&lt;/head&gt;
    &lt;p&gt;It’s Friday at 4pm. I’ve just closed my 12th bug of the week. My brain is completely fried. And I’m staring at the bug leaderboard, genuinely sad that Monday means going back to regular work. Which is weird because I love regular work. But fixit weeks have a special place in my heart.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s a fixit, you ask?&lt;/head&gt;
    &lt;p&gt;Once a quarter, my org with ~45 software engineers stops all regular work for a week. That means no roadmap work, no design work, no meetings or standups.&lt;/p&gt;
    &lt;p&gt;Instead, we fix the small things that have been annoying us and our users:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;an error message that’s been unclear for two years&lt;/item&gt;
      &lt;item&gt;a weird glitch when the user scrolls and zooms at the same time&lt;/item&gt;
      &lt;item&gt;a test which runs slower than it should, slowing down CI for everyone&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The rules are simple: 1) no bug should take over 2 days and 2) all work should focus on either small end-user bugs/features or developer productivity.&lt;/p&gt;
    &lt;p&gt;We also have a “points system” for bugs and a leaderboard showing how many points people have. And there’s a promise of t-shirts for various achievements: first bug fix, most points, most annoying bug, etc. It’s a simple structure, but it works surprisingly well.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we achieved&lt;/head&gt;
    &lt;p&gt;Some stats from this fixit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;189 bugs fixed&lt;/item&gt;
      &lt;item&gt;40 people participated&lt;/item&gt;
      &lt;item&gt;4 was the median number of bugs closed per person&lt;/item&gt;
      &lt;item&gt;12 was maximum number of bugs closed by one person&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bug Burndown Chart for the Q4'25 Fixit&lt;/p&gt;
    &lt;p&gt;Here are some of the highlights (sadly many people in my org work in internal-facing things so I cannot share their work!):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I closed a feature request from 2021! It’s a classic fixit issue: a small improvement that never bubbled to the priority list. It took me one day to implement. One day for something that sat there for four years. And it’s going to provide a small but significant boost to every user’s experience of Perfetto.&lt;/item&gt;
      &lt;item&gt;My colleague made this small change to improve team productivity. Just ~25 lines of code in a GitHub Action to avoid every UI developer taking two extra clicks to open the CI’s build. The response from the team speaks for itself:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Such a simple change but the team loved it!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I also fixed this issue to provide a new “amalgamated” version of our SDK, allowing it to be easily integrated into projects. It’s one of those things that might be the difference between someone deciding to use us or not, but building it took just one hour of work (with liberal use of AI!).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The benefits of fixits&lt;/head&gt;
    &lt;head rend="h3"&gt;For the product: craftsmanship and care&lt;/head&gt;
    &lt;p&gt;I care deeply about any product I work on. That means asking big questions like “what should we build?” and “how do we make this fast?” But it also means asking smaller questions: “is this error message actually helpful?” or “would I be frustrated using this?”&lt;/p&gt;
    &lt;p&gt;A hallmark of any good product is attention to detail: a sense that someone has thought things through, and the pieces fit together to make a cohesive whole. And the opposite is true: a product with rough edges might be tolerated if there are no alternatives, but there will always be a sense of frustration and “I wish I could use something else”.&lt;/p&gt;
    &lt;p&gt;Fixits are a great chance to work on exactly those details that separate good products from great ones. The small things your average user might not consciously notice, but absolutely will notice if they’re wrong.&lt;/p&gt;
    &lt;head rend="h3"&gt;For the individual: doing, not thinking&lt;/head&gt;
    &lt;p&gt;I sometimes miss the feeling I had earlier in my career when I got to just fix things. See something broken, fix it, ship it the same day.&lt;/p&gt;
    &lt;p&gt;The more senior you get in a big company, the less you do that. Most of your time becomes thinking about what to build next, planning quarters ahead, navigating tradeoffs and getting alignment.&lt;/p&gt;
    &lt;p&gt;Fixits give me that early-career feeling back. You see the bug, you fix it, you ship it, you close it, you move on. There’s something deeply satisfying about work where the question isn’t “what should we do?” but rather “can I make this better?” And you get to answer that question multiple times in a week.&lt;/p&gt;
    &lt;head rend="h3"&gt;For the team: morale and spirit&lt;/head&gt;
    &lt;p&gt;People sharing live updates in the Fixit chatroom&lt;/p&gt;
    &lt;p&gt;Having 40 people across two time zones all fixing bugs together adds a whole other dimension.&lt;/p&gt;
    &lt;p&gt;The vibe of the office is different: normally we’re all heads-down on different projects, but during fixit the team spirit comes out strong. People share their bug fixes in chat rooms, post before-and-after screenshots and gather around monitors to demo a new feature or complain about a particularly nasty bug they’re wrestling.&lt;/p&gt;
    &lt;p&gt;The daily update from Friday&lt;/p&gt;
    &lt;p&gt;The leaderboard amplifies this energy. There’s a friendly sense of competition as people try and balance quick wins with meatier bugs they can share stories about.&lt;/p&gt;
    &lt;p&gt;There’s also a short update every morning about how the previous day went:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;total bugs fixed&lt;/item&gt;
      &lt;item&gt;how many people have fixed at least one bug&lt;/item&gt;
      &lt;item&gt;how many different products we’ve fixed things in&lt;/item&gt;
      &lt;item&gt;who’s currently at the top of the leaderboard&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of this creates real momentum, and people feel magnetically pulled into the effort.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to run a fixit&lt;/head&gt;
    &lt;p&gt;I’ve participated in 6 fixits over the years and I’ve learned a lot about what makes them successful. Here are a few things that matter more than you’d think.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparation is key&lt;/head&gt;
    &lt;p&gt;Most of what makes a fixit work happens before the week even starts.&lt;/p&gt;
    &lt;p&gt;All year round, we encourage everyone to tag bugs as “good fixit candidates” as they encounter them. Then the week before fixit, each subteam goes through these bugs and sizes them:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;small (less than half a day)&lt;/item&gt;
      &lt;item&gt;medium (less than a day)&lt;/item&gt;
      &lt;item&gt;large (less than 2 days).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They assign points accordingly: 1, 2, or 4.&lt;/p&gt;
    &lt;p&gt;We also create a shortlist of high-priority bugs we really want fixed. People start there and move to the full list once those are done. This pre-work is critical: it prevents wasting day one with people aimlessly searching for bugs to fix.&lt;/p&gt;
    &lt;head rend="h3"&gt;The 2-day hard limit&lt;/head&gt;
    &lt;p&gt;In one of our early fixits, someone picked up what looked like a straightforward bug. It should have been a few hours, maybe half a day. But it turned into a rabbit hole. Dependencies on other systems, unexpected edge cases, code that hadn’t been touched in years.&lt;/p&gt;
    &lt;p&gt;They spent the entire fixit week on it. And then the entire week after fixit trying to finish it. What started as a bug fix turned into a mini project. The work was valuable! But they missed the whole point of a fixit. No closing bugs throughout the week. No momentum. No dopamine hits from shipping fixes. Just one long slog.&lt;/p&gt;
    &lt;p&gt;That’s why we have the 2-day hard limit now. If something is ballooning, cut your losses. File a proper bug, move it to the backlog, pick something else. The limit isn’t about the work being worthless - it’s about keeping fixit feeling like fixit.&lt;/p&gt;
    &lt;head rend="h3"&gt;Number of people matters&lt;/head&gt;
    &lt;p&gt;We didn’t always do fixits with 40 people. Early on, this wasn’t an org-wide effort, just my subteam of 7 people. It worked okay: bugs got fixed and there was a sense of pride in making the product better. But it felt a bit hollow: in the bigger picture of our org, it didn’t feel like anyone else noticed or cared.&lt;/p&gt;
    &lt;p&gt;At ~40 people, it feels like a critical mass that changes things significantly. The magic number is probably somewhere between 7 and 40. And it probably varies based on the team. But whatever the number is, the collective energy matters. If you’re trying this with 5 people, it might still be worth doing, but it probably won’t feel the same.&lt;/p&gt;
    &lt;head rend="h3"&gt;Gamification&lt;/head&gt;
    &lt;p&gt;The points and leaderboard are more than a gimmick, but they have to be handled carefully.&lt;/p&gt;
    &lt;p&gt;What works for us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Points are coarse, not precise: We deliberately use 1/2/4 points instead of trying to measure exact effort; the goal is “roughly right and fun”, not accurate performance evaluation.&lt;/item&gt;
      &lt;item&gt;Celebrate breadth, not just volume. We give t-shirts for things like “first bug fix”, “most annoying bug fixed”, not just “most points”. That keeps newer or less experienced engineers engaged.&lt;/item&gt;
      &lt;item&gt;Visibility over prizes. A shout-out in the daily update or an internal post often matters more than the actual t-shirt.&lt;/item&gt;
      &lt;item&gt;No attachment to perf reviews. This is important: fixit scores do not feed into performance reviews. The moment they do, people will start gaming it and the good vibe will die.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ve had very little “gaming” in practice. Social norms do a good job of keeping people honest and 40 is still small enough that there’s a sense of “loyalty to the cause” from folks.&lt;/p&gt;
    &lt;head rend="h2"&gt;The AI factor&lt;/head&gt;
    &lt;p&gt;The big challenge with fixits is context switching. Constantly changing what you’re working on means constantly figuring out new parts of the codebase, thinking about new problems.&lt;/p&gt;
    &lt;p&gt;AI tools have mitigated this in a big way. The code they write is less important than their ability to quickly search through relevant files and summarize what needs to change. They might be right or wrong, but having that starting point really reduces the cognitive load. And sometimes (rarely) they one-shot a fix.&lt;/p&gt;
    &lt;p&gt;This docs change was a perfect example of the above: an update to our docs which catches out new contributors and AI was able to one-shot the fix.&lt;/p&gt;
    &lt;p&gt;On the other hand, in my record page change it was more useful for giving me prototypes of what the code should look like and I had to put in significant effort to correct the bad UX it generated and its tendency to “over-generate” code. Even so, it got me to the starting line much faster.&lt;/p&gt;
    &lt;head rend="h2"&gt;Criticisms of fixits (and why I still like them anyway)&lt;/head&gt;
    &lt;p&gt;I’ve definitely come across people who question whether fixits are actually a good idea. Some of the criticisms are fair but overall I still think it’s worth it.&lt;/p&gt;
    &lt;head rend="h3"&gt;“Isn’t this just admitting you ignore bugs the rest of the time?”&lt;/head&gt;
    &lt;p&gt;To some extent, yes, this is an admission of the fact that “papercut” bugs are underweighted in importance, both by managers and engineers. It’s all too easy to tunnel on making sure a big project is successful and easier to ignore the small annoyances for users and the team.&lt;/p&gt;
    &lt;p&gt;Fixits are a way of counterbalancing that somewhat and saying “actually those bugs matter too”. That’s not to say we don’t fix important bugs during regular work; we absolutely do. But fixits recognize that there should be a place for handling the “this is slightly annoying but never quite urgent enough” class of problems.&lt;/p&gt;
    &lt;p&gt;The whole reason we started fixits in the first place is that we observed these bugs never get actioned. Given this, I think carving out some explicit time for it is a good thing.&lt;/p&gt;
    &lt;head rend="h3"&gt;“Isn’t it a waste to pause roadmap work for a whole week?”&lt;/head&gt;
    &lt;p&gt;It’s definitely a tradeoff. 40 engineer-weeks is a lot of manpower and there’s an argument to be made it should be used for actually solving roadmap problems.&lt;/p&gt;
    &lt;p&gt;But I think this underweights the importance of polish of products to users. We’ve consistently found that the product feels noticeably better afterward (including positive comments from users about things they notice!) and there’s a sense of pride in having a well-functioning product.&lt;/p&gt;
    &lt;p&gt;A user’s response to my PR to improve the Perfetto UI “record” page&lt;/p&gt;
    &lt;p&gt;Also, many of the team productivity fixes compound (faster tests, clearer errors, smoother workflows) so the benefits carry forward well beyond the week itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;This only works at big companies!&lt;/head&gt;
    &lt;p&gt;I agree that a full week might be too much for tiny teams or startups. But you can still borrow the idea in smaller chunks: a “fixit Friday” once a month, or a 2-day mini-fixit each quarter. The core idea is the same: protected, collective time to fix the stuff people complain about but no one schedules time to address.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixits are good for the soul&lt;/head&gt;
    &lt;p&gt;The official justification for fixits is that they improve product quality and developer productivity. And of course they do this.&lt;/p&gt;
    &lt;p&gt;But the unofficial reason I love them is simpler: it just feels good to fix things. It takes me back to a simpler time, and putting thought and attention into building great products is a big part of my ethos for how software engineering should be done. I wouldn’t want to work like that all the time. But I also wouldn’t want to work somewhere that never makes time for it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46024541</guid><pubDate>Sun, 23 Nov 2025 16:06:46 +0000</pubDate></item><item><title>Calculus for Mathematicians, Computer Scientists, and Physicists [pdf]</title><link>https://mathcs.holycross.edu/~ahwang/print/calc.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46024773</guid><pubDate>Sun, 23 Nov 2025 16:31:50 +0000</pubDate></item><item><title>Native Secure Enclave backed SSH keys on macOS</title><link>https://gist.github.com/arianvp/5f59f1783e3eaf1a2d4cd8e952bb4acf</link><description>&lt;doc fingerprint="26c8495d3fe23961"&gt;
  &lt;main&gt;
    &lt;p&gt;It turns out that MacOS Tahoe can generate and use secure-enclave backed SSH keys! This replaces projects like https://github.com/maxgoedjen/secretive&lt;/p&gt;
    &lt;p&gt;There is a shared library &lt;code&gt;/usr/lib/ssh-keychain.dylib&lt;/code&gt; that traditionally has been used to add smartcard support
to ssh by implementing &lt;code&gt;PKCS11Provider&lt;/code&gt; interface. However since recently it also implements &lt;code&gt;SecurityKeyProivder&lt;/code&gt;
which supports loading keys directly from the secure enclave! &lt;code&gt;SecurityKeyProvider&lt;/code&gt; is what is normally used to talk to FIDO2 devices (e.g. &lt;code&gt;libfido2&lt;/code&gt; can be used to talk to your Yubikey). However you can now use it to talk to your Secure Enclave instead!&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;recording.mov&lt;/head&gt;
    &lt;p&gt;Seems this method was first discovered in https://lists.mindrot.org/pipermail/openssh-unix-dev/2024-July/041451.html&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;man sc_auth&lt;/code&gt; and &lt;code&gt;man ssh-keychain&lt;/code&gt; for all the options&lt;/p&gt;
    &lt;p&gt;To create a Secure Enclave backed key that requires biometrics, run the following command and press TouchID:&lt;/p&gt;
    &lt;code&gt;% sc_auth create-ctk-identity -l ssh -k p-256-ne -t bio
&lt;/code&gt;
    &lt;p&gt;You can confirm that the key was create with the &lt;code&gt;list-ctk-identities&lt;/code&gt; command:&lt;/p&gt;
    &lt;code&gt;arian@Mac ssh-keychain % sc_auth  list-ctk-identities       
Key Type Public Key Hash                          Prot Label Common Name Email Address Valid To        Valid 
p-256-ne A71277F0BC5825A7B3576D014F31282A866EF3BC bio  ssh   ssh                       23.11.26, 17:09 YES
&lt;/code&gt;
    &lt;p&gt;It also supports listing the ssh key fingerprints instead:&lt;/p&gt;
    &lt;code&gt;% sc_auth  list-ctk-identities -t ssh
Key Type Public Key Hash                                    Prot Label Common Name Email Address Valid To        Valid 
p-256-ne SHA256:vs4ByYo+T9M3V8iiDYONMSvx2k5Fj2ujVBWt1j6yzis bio  ssh   ssh                       23.11.26, 17:09 YES 
&lt;/code&gt;
    &lt;p&gt;Keys can be deleted with&lt;/p&gt;
    &lt;code&gt;% sc_auth delete-ctk-identity -h &amp;lt;Public Key Hash&amp;gt;
&lt;/code&gt;
    &lt;p&gt;You can "download" the public / private keypair from the secure enclave using the following command:&lt;/p&gt;
    &lt;code&gt;% ssh-keygen -w /usr/lib/ssh-keychain.dylib -K -N ""
Enter PIN for authenticator: 
You may need to touch your authenticator to authorize key download.
Saved ECDSA-SK key to id_ecdsa_sk_rk
% cat id_ecdsa_sk_rk.pub 
sk-ecdsa-sha2-nistp256@openssh.com AAAAInNrLWVjZHNhLXNoYTItbmlzdHAyNTZAb3BlbnNzaC5jb20AAAAIbmlzdHAyNTYAAABBBKiHAiAZhcsZ95n85dkNGs9GnbDt0aNOia2gnuknYV2wKL3y0u+d3QrE9cFkmWXIymHZMglL+uJA+6mShY8SeykAAAAEc3NoOg== ssh:
&lt;/code&gt;
    &lt;p&gt;You can just use the empty string for PIN. For some reason &lt;code&gt;openssh&lt;/code&gt; always asks for
it even if the authenticator in question does not use a PIN but a biometric.
Note that the "private" key here is just a reference to the FIDO credential. It does
not contain any secret key material. Hence I'm specifiyng &lt;code&gt;-N ""&lt;/code&gt; to skip an encryption
passphrase.&lt;/p&gt;
    &lt;p&gt;Now if you copy this public key to your authorized keys file, it should work!&lt;/p&gt;
    &lt;code&gt;% ssh-copy-id -i id_ecdsa_sk_rk localhost
% ssh -o SecurityKeyProvider=/usr/lib/ssh-keychain.dylib localhost
&lt;/code&gt;
    &lt;p&gt;Instead of downloading the public/private keypair to a file you can also directly make the keys available to &lt;code&gt;ssh-agent&lt;/code&gt;. For this you can use the following command:&lt;/p&gt;
    &lt;code&gt;% ssh-add -K -S /usr/lib/ssh-keychain.dylib
Enter PIN for authenticator: 
Resident identity added: ECDSA-SK SHA256:vs4ByYo+T9M3V8iiDYONMSvx2k5Fj2ujVBWt1j6yzis
% ssh-add -L
sk-ecdsa-sha2-nistp256@openssh.com AAAAInNrLWVjZHNhLXNoYTItbmlzdHAyNTZAb3BlbnNzaC5jb20AAAAIbmlzdHAyNTYAAABBBKiHAiAZhcsZ95n85dkNGs9GnbDt0aNOia2gnuknYV2wKL3y0u+d3QrE9cFkmWXIymHZMglL+uJA+6mShY8SeykAAAAEc3NoOg== 
% ssh-copy-id localhost
% ssh -o SecurityKeyProvider=/usr/lib/ssh-keychain.dylib localhost
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;SecurityKeyProvider&lt;/code&gt; can be configured in &lt;code&gt;.ssh/config&lt;/code&gt; but I recommend setting
&lt;code&gt;export SSH_SK_PROVIDER=/usr/lib/ssh-keychain.dylib&lt;/code&gt; in your &lt;code&gt;.zprofile&lt;/code&gt; instead as
that environment variable gets picked up by &lt;code&gt;ssh&lt;/code&gt;, &lt;code&gt;ssh-add&lt;/code&gt; and &lt;code&gt;ssh-keygen&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This means you can just do:&lt;/p&gt;
    &lt;code&gt;ssh-add -K
ssh my-server
&lt;/code&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;code&gt;ssh-keygen -K
ssh -i id_ecdsa_rk_sk my-server
&lt;/code&gt;
    &lt;p&gt;to ssh into your server&lt;/p&gt;
    &lt;p&gt;There's also an exportable variant where the private key is encrypted using the secure enclave as opposed to generated on the secure enclave. This is might be considered less secure but is convenient for key backup.&lt;/p&gt;
    &lt;code&gt;% sc_auth create-ctk-identity -l ssh-exportable -k p-256 -t bio
% sc_auth list-ctk-identities
p-256    A581E5404ED157C4C73FFDBDFC1339E0D873FCAE bio  ssh-exportable ssh-exportable               23.11.26, 19:50 YES  
% sc_auth export-ctk-identity -h A581E5404ED157C4C73FFDBDFC1339E0D873FCAE -f ssh-exportable.pem
Enter a password which will be used to protect the exported items:
Verify password:
&lt;/code&gt;
    &lt;p&gt;You can then re-import it on another device&lt;/p&gt;
    &lt;code&gt;% sc_auth import-ctk-identities -f ssh-exportable.pem.p12 -t bio
Enter PKCS12 file password:
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46025721</guid><pubDate>Sun, 23 Nov 2025 17:55:11 +0000</pubDate></item><item><title>Fran Sans – font inspired by San Francisco light rail displays</title><link>https://emilysneddon.com/fran-sans-essay</link><description>&lt;doc fingerprint="1e9811a28de60dc3"&gt;
  &lt;main&gt;
    &lt;p&gt;Published on 6TH NOVEMBER 2025&lt;/p&gt;
    &lt;p&gt;Fran Sans is a display font in every sense of the term. It’s an interpretation of the destination displays found on some of the light rail vehicles that service the city of San Francisco.&lt;/p&gt;
    &lt;p&gt;I say some because destination displays aren’t consistently used across the city’s transit system. In fact, SF has an unusually high number of independent public transit agencies. Unlike New York, Chicago or L.A., which each have one, maybe two, San Francisco and the greater Bay Area have over two dozen. Each agency, with its own models of buses and trains, use different destination displays, creating an eclectic patchwork of typography across the city.&lt;/p&gt;
    &lt;p&gt;Among them, one display in particular has always stood out to me: the LCD panel displays inside Muni’s Breda Light Rail Vehicles. I remember first noticing them on a Saturday in October on the N-Judah, heading to the Outer Sunset for a shrimp hoagie. This context is important, as anyone who’s spent an October weekend in SF knows this is the optimal vibe to really take in the beauty of the city.&lt;/p&gt;
    &lt;p&gt;What caught my eye was how the displays look mechanical and yet distinctly personal. Constructed on a 3×5 grid, the characters are made up of geometric modules: squares, quarter-circles, and angled forms. Combined, these modules create imperfect, almost primitive letterforms, revealing a utility and charm that feels distinctly like the San Francisco I’ve come to know.&lt;/p&gt;
    &lt;p&gt;This balance of utility and charm seems to show up everywhere in San Francisco and its history. The Golden Gate’s “International Orange” started as nothing more than a rust-proof primer, yet is now the city’s defining colour. The Painted Ladies became multicoloured icons after the 1960s Colourist movement covered decades of grey paint. Even the steepness of the streets was once an oversight in city planning but has since been romanticised in films and on postcards. So perhaps it is unsurprising that I would find this same utility and charm in a place as small and functional as a train sign.&lt;/p&gt;
    &lt;p&gt;To learn more about these displays, I visited the San Francisco Municipal Transportation Agency’s (SFMTA) Electronics Shop at Balboa Park. There, technician Armando Lumbad had set up one of the signs. They each feature one large LCD panel which displays the line name, and twenty-four smaller ones to display the destination. The loose spacing of the letters and fluorescent backlighting gives the sign a raw, analogue quality. Modern LED dot-matrix displays are far more efficient and flexible, but to me, they lack the awkwardness that makes these Breda signs so delightful.&lt;/p&gt;
    &lt;p&gt;Armando showed me how the signs work. He handed me a printed matrix table listing every line and destination, each paired with a three-digit code. On route, train operators punch the code into a control panel at the back of the display, and the LCD blocks light on specific segments of the grid to build each letter. I picked code 119, and Armando entered it for me. A few seconds later the panels revealed my own stop: the N-Judah at Church &amp;amp; Duboce. There in the workshop, devoid of the context of the trains and the commute, the display looked almost monolithic, or sculptural, and I have since fantasised whether it would be possible to ship one of these home to Australia.&lt;/p&gt;
    &lt;p&gt;Looking inside of the display, I found labels identifying the make and model. The signs were designed and manufactured by Trans-Lite, Inc., a company based in Milford, Connecticut that specialised in transport signage from 1959 until its acquisition by the Nordic firm Teknoware in 2012. After lots of amateur detective work, and with the help from an anonymous Reddit user in a Connecticut community group, I was connected with Gary Wallberg, Senior Engineer at Trans-Lite and the person responsible for the design of these very signs back in 1999.&lt;/p&gt;
    &lt;p&gt;Learning that the alphabet came from an engineer really explains its temperament and why I was drawn to it in the first place. The signs were designed for sufficiency: fixed segments, fixed grid, and no extras. Characters were created only as destinations required them, while other characters, like the Q, X, and much of the punctuation, were never programmed into the signs. In reducing everything to its bare essentials, somehow character emerged, and it’s what inspired me to design Fran Sans.&lt;/p&gt;
    &lt;p&gt;I shared some initial drawings with Dave Foster of Foster Type who encouraged me to get the font software Glyphs and turn it into my first working font. From there, I broke down the anatomy of the letters into modules, then used them like Lego to build out a full set: uppercase A–Z, numerals, core punctuation.&lt;/p&gt;
    &lt;p&gt;Some glyphs remain unsolved in this first version, for example the standard @ symbol refuses to squeeze politely into the 3×5 logic. Lowercase remains a question for the future, and would likely mean reconsidering the grid. But, as with the displays themselves, I am judging Fran Sans as sufficient for now.&lt;/p&gt;
    &lt;p&gt;Getting up close to these signs, you’ll notice Fran Sans’ gridlines are simplified even from its real‑life muse, but my hope is that its character remains. Specifically: the N and the zero, where the unusually thick diagonals close in on the counters; and the Z and 7, whose diagonals can feel uncomfortably thin. I’ve also noticed the centre of the M can scale strangely and read like an H at small sizes, but in fairness, this type was never designed for the kind of technical detail so many monospaced fonts aim for. Throughout the process I tried to protect these unorthodox moments, because to me, they determined the success of this interpretation.&lt;/p&gt;
    &lt;p&gt;Fran Sans comes in three styles: Solid, Tile, and Panel, each building in visual complexity. The decision to include variations, particularly the Solid style, was inspired by my time working at Christopher Doyle &amp;amp; Co. There, we worked with Bell Shakespeare, Australia’s national theatre company dedicated to the works of William Shakespeare. The equity of the Bell Shakespeare brand lies in its typography, which is a beautiful custom typeface called Hotspur, designed and produced by none other than Dave Foster.&lt;/p&gt;
    &lt;p&gt;Often, brand fonts are chosen or designed to convey a single feeling. Maybe it’s warmth and friendliness, or a sense of tech and innovation. But what I’ve always loved about the Bell typeface is how one weight could serve both Shakespeare’s comedies and tragedies, simply by shifting scale, spacing, or alignment. Hotspur has the gravity to carry the darkness of Titus Andronicus and the roundness to convey the humour of Much Ado About Nothing. And while Fran Sans Solid is technically no Hotspur, I wanted it to share that same versatility.&lt;/p&gt;
    &lt;p&gt;Further inspiration for Fran Sans came from the Letterform Archive, the world’s leading typography archive, based in San Francisco. Librarian and archivist Kate Long Stellar thoughtfully curated a research visit filled with modular typography spanning most of the past century. On the table were two pieces that had a significant impact on Fran Sans and are now personal must-sees at the archive. First, Joan Trochut’s Tipo Veloz “Fast Type” (1942) was created during the Second World War when resources were scarce. Tipo Veloz gave printers the ability to draw with type, rearranging modular pieces to form letters, ornaments and even illustrations.&lt;/p&gt;
    &lt;p&gt;Second, Zuzana Licko’s process work for Lo-Res (1985), an Emigre typeface, opened new ways of thinking about how ideas move between the physical and the digital and then back again. Seeing how Lo-Res was documented through iterations and variations gave the typeface a depth and richness that changed my understanding of how fonts are built. At some point I want to explore physical applications for Fran Sans out of respect for its origins, since it is impossible to fully capture the display’s charm on screen.&lt;/p&gt;
    &lt;p&gt;Back at the SFMTA, Armando told me the Breda vehicles are being replaced, and with them their destination displays will be swapped for newer LED dot-matrix units that are more efficient and easier to maintain. By the end of 2025 the signs that inspired Fran Sans will disappear from the city, taking with them a small but distinctive part of the city’s voice. That feels like a real loss. San Francisco is always reinventing itself, yet its charm lies in how much of its history still shows through. My hope is that Fran Sans can inspire a deeper appreciation for the imperfections that give our lives and our cities character. Life is so rich when ease and efficiency are not the measure.&lt;/p&gt;
    &lt;p&gt;OUTSIDE MY LIFE,&lt;/p&gt;
    &lt;p&gt;INSIDE THE DREAM.&lt;/p&gt;
    &lt;p&gt;FALLING UP THE STAIRS,&lt;/p&gt;
    &lt;p&gt;INTO THE STREET.&lt;/p&gt;
    &lt;p&gt;LET THE CABLE CAR&lt;/p&gt;
    &lt;p&gt;CARRY ME.&lt;/p&gt;
    &lt;p&gt;STRAIGHT OUT OF TOWN,&lt;/p&gt;
    &lt;p&gt;INTO THE SEA.&lt;/p&gt;
    &lt;p&gt;PAST THE DAHLIAS AND&lt;/p&gt;
    &lt;p&gt;THE SELF-DRIVING CARS.&lt;/p&gt;
    &lt;p&gt;THE CHURCH OF 8 WHEELS.&lt;/p&gt;
    &lt;p&gt;THE LOWER HAIGHT BARS.&lt;/p&gt;
    &lt;p&gt;THE PEAK HOUR SPRAWL.&lt;/p&gt;
    &lt;p&gt;THE KIDS IN THE PARK.&lt;/p&gt;
    &lt;p&gt;THE SLANTING HOUSES.&lt;/p&gt;
    &lt;p&gt;THE BAY AFTER DARK.&lt;/p&gt;
    &lt;p&gt;MY WINDOW, MY OWN&lt;/p&gt;
    &lt;p&gt;SILVER SCREEN.&lt;/p&gt;
    &lt;p&gt;I FOLLOW WHERE THE&lt;/p&gt;
    &lt;p&gt;FOG TAKES ME.&lt;/p&gt;
    &lt;p&gt;By MADDY CARRUCAN&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46025942</guid><pubDate>Sun, 23 Nov 2025 18:20:27 +0000</pubDate></item><item><title>µcad: New open source programming language that can generate 2D sketches and 3D</title><link>https://microcad.xyz/</link><description>&lt;doc fingerprint="db25b240da5dd2dc"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Alpha Release 0.2.14&lt;/head&gt;
    &lt;p&gt;What a day! This morning we just wanted to push a release and ended up in struggling with Issue #289…&lt;/p&gt;
    &lt;p&gt;Welcome to the website of µcad!&lt;lb/&gt;Microcad (or µcad) is a new open source programming language that can generate 2D sketches and 3D objects.&lt;lb/&gt;The project is still in its early stages, but µcad is becoming increasingly stable. New ideas are being added to the code every week.&lt;lb/&gt;In this blog, we want to keep you up to date on the latest developments at µcad.&lt;/p&gt;
    &lt;p&gt;What a day! This morning we just wanted to push a release and ended up in struggling with Issue #289…&lt;/p&gt;
    &lt;p&gt;What at first may look like a minimalist Christmas decoration, is a Spirograph consisting of five parts that we successfully…&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46027216</guid><pubDate>Sun, 23 Nov 2025 20:51:22 +0000</pubDate></item><item><title>Show HN: I wrote a minimal memory allocator in C</title><link>https://github.com/t9nzin/memory</link><description>&lt;doc fingerprint="a397485a6eb7beb9"&gt;
  &lt;main&gt;
    &lt;p&gt;A memory allocator written from scratch using &lt;code&gt;sbrk&lt;/code&gt; for small allocations and &lt;code&gt;mmap&lt;/code&gt; for large allocations. It includes optimizations like block splitting to reduce fragmentation and coalescing to merge adjacent free blocks. Please note that this allocator is not thread-safe. Concurrent calls to malloc/free/realloc will cause undefined behavior. I've also written a blog post (~20 minute read) explaining step by step the process behind writing this memory allocator project, if that's of interest, you can read it here!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;GCC compiler&lt;/item&gt;
      &lt;item&gt;Make&lt;/item&gt;
      &lt;item&gt;POSIX-compliant system (Linux, macOS) because we're using &lt;code&gt;sbrk()&lt;/code&gt;and&lt;code&gt;mmap()&lt;/code&gt;&amp;lt;- won't work on Windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;make           # build everything
make tests     # run tests
make bench     # run benchmark&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Build the static library:&lt;/p&gt;
        &lt;quote&gt;make lib&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Include the header in your C file:&lt;/p&gt;
        &lt;quote&gt;#include "allocator.h"&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Compile your program with the library:&lt;/p&gt;
        &lt;quote&gt;gcc -I./include examples/my_program.c -L./build -lallocator -o my_program&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run the compiled program&lt;/p&gt;
        &lt;quote&gt;./my_program&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;.
├── Makefile
├── README.md
├── examples
│   └── my_program.c        # an example program using implemented malloc()
├── include
│   └── allocator.h         # header file w/ function declarations
├── src
│   └── allocator.c         # allocator
└── tests
    ├── benchmark.c         # performance benchmarks
    ├── test_basic.c        # basic functionality tests
    └── test_edge_cases.c   # edge cases and stress tests 
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not thread-safe (no mutex protection)&lt;/item&gt;
      &lt;item&gt;fyi: &lt;code&gt;sbrk&lt;/code&gt;is deprecated on macOS but still functional&lt;/item&gt;
      &lt;item&gt;No defragmentation or compaction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License :D&lt;/p&gt;
    &lt;p&gt;Contributions are welcome. Please open an issue or submit a pull request.&lt;/p&gt;
    &lt;p&gt;I credit Dan Luu for his fantastic malloc() tutorial which I greatly enjoyed reading and served as a helpful reference for this project. If you would like to take a look at his tutorial (which I highly recommend), you can find it here.&lt;/p&gt;
    &lt;p&gt;I'd also like to thank Joshua Zhou and Abdul Fatir for reading over and giving me great feedback on the accompanying blog post for this project.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46027962</guid><pubDate>Sun, 23 Nov 2025 22:25:36 +0000</pubDate></item><item><title>Liva AI (YC S25) Is Hiring</title><link>https://www.ycombinator.com/companies/liva-ai/jobs/fYP8QP8-growth-intern</link><description>&lt;doc fingerprint="bb4e601ff5d377f7"&gt;
  &lt;main&gt;
    &lt;p&gt;Audio &amp;amp; Video Data&lt;/p&gt;
    &lt;p&gt;About the role:&lt;/p&gt;
    &lt;p&gt;Liva AI (YC S25) is focused on collecting high-quality and proprietary voice and video data. We're actively hiring for a community growth intern. You’ll help us foster a community at Liva AI, with an emphasis on building large online communities.&lt;/p&gt;
    &lt;p&gt;Job responsibilities:&lt;/p&gt;
    &lt;p&gt;You must:&lt;/p&gt;
    &lt;p&gt;No need to send resume, just send me the server/subreddit/community you made with a brief description.&lt;/p&gt;
    &lt;p&gt;Liva's mission is to make AI look and sound truly human. The AI voices and faces today feel off, and lack the capability to reflect diverse people across different ethnicities, races, accents, and career professions. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46028046</guid><pubDate>Sun, 23 Nov 2025 22:36:07 +0000</pubDate></item><item><title>Ego, empathy, and humility at work</title><link>https://matthogg.fyi/a-unified-theory-of-ego-empathy-and-humility-at-work/</link><description>&lt;doc fingerprint="be635b5ba3068937"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;A Unified Theory Of Ego, Empathy, And Humility At Work&lt;/head&gt;&lt;p&gt;In our daily lives empathy and humility are obvious virtues we aspire to. They keep our egos in check. Less obvious is that they’re practical skills in the workplace, too. I think, for developers and technical leaders in particular, that the absence of ego is the best way to further our careers and do great work.&lt;/p&gt;&lt;p&gt;In the simplest of terms the ego is the characteristic of personhood that enables us to practice self-reflection, self-awareness, and accountability for the actions or decisions we take.&lt;/p&gt;&lt;p&gt;However, the ego also motivates us to reframe our perception of the world in whatever way keeps us centered in it. Each of us is perpetually driven to justify our place in the world. This constant self-justification is like an engine that idles for our entire lives, and it requires constant fine-tuning. When it runs amok this is what we call a “big” ego.&lt;/p&gt;&lt;head rend="h2"&gt;Breaking News! Developers Have Egos!&lt;/head&gt;&lt;p&gt;I’m not thinking only of the 10x engineer stereotype, although I’ve worked with such folks in the past. Ego is more nuanced than that. Besides the most arrogant developer in the room throwing their weight around, our egos manifest in hundreds of ways that are much harder to detect.&lt;/p&gt;&lt;p&gt;As developers we’re more susceptible to letting our egos run free. The nature of our work is so technical that to others it can seem obscure, arcane, or even magical. Sometimes we don’t do enough to actively dispel that notion—and just like that half the work of self-justification is already done for us.&lt;/p&gt;&lt;p&gt;Very often it’s not intentional. The simplest example is the overuse of jargon and acronyms. We all do it, but as Jeremy Keith explains:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Still, I get why initialisms run rampant in technical discussions. You can be sure that most discussions of particle physics would be incomprehensible to outsiders, not necessarily because of the concepts, but because of the terminology.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Simply mashing a few letters together can be empowering for ourselves while being exclusionary for others. It’s an artifact—albeit a small one—of our egos. We know what the technobabble means. Our justified place in the universe is maintained.&lt;/p&gt;&lt;p&gt;Sometimes we express our egos more deliberately. Developers have a clear tendency towards gatekeeping. For most, it’s an honest mistake. There’s a fine line between holding others to a certain expectation versus actively keeping people on the outside. When we see ourselves doing this we can correct it easily enough.&lt;/p&gt;&lt;p&gt;Sadly there are developers who seemingly like to gatekeep. They get to feel like wizards in their towers with their dusty books and potions. But, it’s actually self-limiting. Gatekeeping by definition means you’re fixed in place and never moving, standing guard for eternity.&lt;/p&gt;&lt;p&gt;My point is our egos can “leak” in so many ways that it takes diligence to catch it let alone correct it. The following is a short, incomplete list of typical statements we as developers might say or hear at work. If you parse them more precisely each one is an attempt at self-justification:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;“That’s the way we’ve always done it.”&lt;/item&gt;&lt;item&gt;“It’s not that complicated! You just…”&lt;/item&gt;&lt;item&gt;“Yeah, I should be able to finish this in a day.”&lt;/item&gt;&lt;item&gt;“This legacy codebase is an absolute disaster.”&lt;/item&gt;&lt;item&gt;“Assign it to me. Nobody else will be able to fix it.”&lt;/item&gt;&lt;item&gt;“You can’t be a senior dev. You don’t know anything about…”&lt;/item&gt;&lt;item&gt;“Ugh, our morning standup is so useless.”&lt;/item&gt;&lt;item&gt;“This feature is too important to assign to the junior dev.”&lt;/item&gt;&lt;item&gt;“We should start using this new tool in our pipeline.”&lt;/item&gt;&lt;item&gt;“We should never use that new tool in our pipeline.”&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Everything Is Bigger Than You&lt;/head&gt;&lt;p&gt;The ego is concerned with the self but very easily becomes something harmful in the absence of new information or context. Indeed, the ego nudges us to self-justify so much that one could argue it actively resists new information when left unchecked.&lt;/p&gt;&lt;p&gt;You may have read one of the example statements above with some familiarity and thought, “But what if I’m right?”&lt;/p&gt;&lt;p&gt;To which I’d say: OK, but should that be your default stance? Why might you feel the need to immediately start a conversation with a self-justification? There are ways to adjust our approach, make our points, and accept new information all at the same time.&lt;/p&gt;&lt;p&gt;In any interaction—be it a meeting, Slack thread, or water cooler conversation—we must remember that the matter at hand is bigger than us in ways we don’t yet understand.&lt;/p&gt;&lt;p&gt;This is a simple enough heuristic but we need the skills to gain that understanding. We need empathy and humility. Empathy is the ability to recognize and comprehend what someone else is thinking or feeling. Humility is a resistance to our “competitive reflexes” through the practice of emotional neutrality and vulnerability. Both serve to counteract the ego.&lt;/p&gt;&lt;p&gt;To make these concepts more actionable I find it simpler to define them in terms of the purposes they serve. Specifically…&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Empathy is how we gather new information.&lt;/item&gt;&lt;item&gt;Humility is how we allow information to change our behavior.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This framing also helps remind us what empathy and humility are not. It’s not about putting yourself in another’s shoes, as the saying goes. It’s not about being submissive or a pushover. It’s not about altruism or self-sacrifice. We can easily practice empathy and humility without it ever being at our own expense.&lt;/p&gt;&lt;head rend="h2"&gt;The Pursuit Of Information&lt;/head&gt;&lt;p&gt;I don’t know about you but I go to work to solve problems, be creative, and build shit. I can’t think of a single instance where an unruly ego solved anything I’ve worked on. Ego just makes an existing challenge worse. Solutions require information I don’t have yet.&lt;/p&gt;&lt;p&gt;Empathy and humility are usually top of mind during situations of pain or distress, but they’re really aspects of emotional intelligence that should be activated at all times. Once you adjust your mindset to treat them as basic tools for the pursuit of information you’ll see opportunities to leverage them everywhere.&lt;/p&gt;&lt;p&gt;Developers can apply this mindset with almost anybody they come into contact with. Fellow developers, naturally. But also less technical teammates (e.g., QAs, designers, product owners, stakeholders) who have their own unique skills and context that our success depends on. And of course our users should be at the center of every problem we’re working to solve. Lastly, even executives and upper management have some insight to offer if you dare (but only up to a certain point).&lt;/p&gt;&lt;head rend="h2"&gt;“Be Curious, Not Judgmental”&lt;/head&gt;&lt;p&gt;I’ve been waiting years for a chance to work Ted Lasso into one of my essays. Today’s the day, readers.&lt;/p&gt;&lt;p&gt;The titular character is such an archetype for leadership that my jaw hit the floor when I first watched the show. The example Ted sets has spawned countless think pieces about leadership and management. Suffice it to say he exhibits all of my principles over the series’ 34 episodes. He’s empathy and humility sporting a mustache. He’s the absence of ego personified.&lt;/p&gt;&lt;p&gt;I highly recommend watching the show but to get a taste this 5 minute clip is worth your time. This is the famous “darts scene”…&lt;/p&gt;&lt;p&gt;There’s a common and derisive attitude that qualities like empathy or humility are signs of weakness. You have to get all up in your feelings. Ew! But they require enormous reserves of strength, patience, and determination. It’s those who follow their egos who are weak.&lt;/p&gt;&lt;p&gt;Letting your ego take control is the easiest thing in the world. Just ask any toddler throwing a temper tantrum. Resisting those impulses and remaining calm, on the other hand, has been a virtue humanity has aspired to for thousands of years. As the Roman emperor and Stoic philosopher Marcus Aurelius wrote: “The nearer a man comes to a calm mind, the closer he is to strength.”&lt;/p&gt;&lt;head rend="h2"&gt;You’re Neither Ted Lasso Nor A Roman Emperor&lt;/head&gt;&lt;p&gt;The practice of empathy, humility, and keeping your ego in check will test you daily. The feedback I’ve received the most from my coworkers is that I’m extraordinarily calm and even-keeled in any situation—even situations where I’d be right to freak out.&lt;/p&gt;&lt;p&gt;Is that just naturally my personality? Maybe in part, but remaining calm is a choice. I’m actively choosing to favor solutions over my own ego. To my colleagues past and present I confess to you now that any time you’ve seen me calm, cool, and collected I was very likely internally screaming.&lt;/p&gt;&lt;p&gt;If this sounds like a lot of work you might be wondering if it’s worth it. I think it is. At the very least your coworkers and colleagues will like you better. That’s no small thing.&lt;/p&gt;&lt;p&gt;In all seriousness, the positive feedback I get most about the developers I manage is when they’ve demonstrated empathy and humility while dialing back their egos. This is because they’re people we can work with—literally. Nobody wants to work with a narcissist or a rock star. Nobody is materially impressed by how many lines of code we wrote, or how fast we wrote it.&lt;/p&gt;&lt;p&gt;When people want to work with us—or even look forward to it—that means we have trust and respect. We’ll be on proper footing for working effectively as a group to solve problems. For developers this looks like coaching a junior developer, hopping on a quick call to pair with somebody, or understanding the business value of the next backlog item. For leaders this looks like people who feel empowered to do their work, who can proactively identify issues, or who can rally and adapt when circumstances change.&lt;/p&gt;&lt;p&gt;Anybody can do this! I can’t think of any other career advice that’s as universal as empathy and humility. Everybody is capable of, at any point in their lives, small yet impactful improvements.&lt;/p&gt;&lt;p&gt;So remember—watch your ego and look for opportunities to leverage empathy and humility in the pursuit of information so that you can solve problems together.&lt;/p&gt;&lt;p&gt;In my next essay on this subject I’ll get into the practical. What I like about this advice is that, while there’s much we can do, we don’t have to do it all to see some benefit. We can pick and choose and try something out. We can take your time and grow. Nobody’s perfect, not even Ted Lasso. Even if we take after a character like Roy Kent we can still call that a win. Just watch the show, OK?&lt;/p&gt;&lt;p&gt;In our daily lives empathy and humility are obvious virtues we aspire to. They keep our egos in check. Less obvious is that they're practical skills in the workplace, too. I think, for developers and technical leaders in particular, that the absence of ego is the best way to further our careers and do great work.&lt;/p&gt;https://matthogg.fyi/a-unified-theory-of-ego-empathy-and-humility-at-work/&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029534</guid><pubDate>Mon, 24 Nov 2025 02:01:02 +0000</pubDate></item><item><title>Passing the Torch – My Last Root DNSSEC KSK Ceremony as Crypto Officer 4</title><link>https://technotes.seastrom.com/2025/11/23/passing-the-torch.html</link><description>&lt;doc fingerprint="e483d9919623d7d9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sun 23 November 2025&lt;/item&gt;
      &lt;item&gt;misc&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Many years ago, when I was but an infant, the first computers were connected on the ARPANET - the seminal computer network that would eventually evolve to become the Internet. Computers at the time were large and expensive; indeed the first version of NCP - the predecessor of TCP/IP - only countenanced roughly 250 computers on the network.&lt;/p&gt;
    &lt;p&gt;The name (human friendly) to network address (computer friendly) mapping on this network was maintained via a "hosts file" - literally a flat file of ordered pairs, creating the connection between host (computer) name and address.&lt;/p&gt;
    &lt;p&gt;So it continued as computers got less expensive and proliferated, the Network Effect caused more institutions to want to be connected to the ARPANET. TCP/IP was developed in response to this, with support for orders of magnitude more connected computers. Along the way, the military users of the network got carved off into its own network, and by the early 1980s we had the beginnings of the Internet, or a "catenet" as it was sometimes called at the time - a network of networks.&lt;/p&gt;
    &lt;p&gt;Clearly, as we went from "a couple hundred computers" to "capacity for billions", a centrally managed host file wasn't going to scale, and by the early 1980s development had started on a distributed database to replace the centrally managed file. The name for this distributed database was the Domain Name System, or DNS.&lt;/p&gt;
    &lt;p&gt;It's important to realize that at the time, access to the network of networks was still restricted to a chosen few - higher education, research institutions, military organizations and the military-industrial complex (ARPA, later DARPA, was, after all, an activity of the United States Department of Defense), and a few companies that were tightly associated with one or more of those constituencies. Broad public commercial access to the Internet was many years in the future.&lt;/p&gt;
    &lt;p&gt;It was in this environment that the DNS sprang forth. Academics, military researchers, university students - a pretty collegial environment. Not to mention paleo-cybersecurity practices - indeed the word "cybersecurity" may not have even been coined yet, though the notion of "computer security" dates back to the early 1970s.&lt;/p&gt;
    &lt;p&gt;I've mentioned this brief "history of the early Internet" to preemptively answer the question which inevitably arises: why didn't DNS have better security built in? The answer is twofold: firstly it didn't have to based on the environment that it evolved in, and secondly, even if it had, the security practices would have been firmly rooted in 1980s best practices, which would certainly be inadequate by modern standards.&lt;/p&gt;
    &lt;p&gt;Discovery of security flaws in 1990 led the IETF to begin development on Domain Name System Security Extensions (DNSSEC) in 1995. Early versions were difficult to deploy. Later versions improved somewhat. But inertia is a thing, the status quo tends to prevail, and there was very real concern that DNSSEC would be a net reliability minus (security vs. availability can be a tricky circle to square), concentrate power in undesirable ways, and result in other unforeseen negative effects.&lt;/p&gt;
    &lt;p&gt;At the end of the day, as it so often does, it took a crisis to get the ball rolling for real. In 2008, Dan Kaminsky discovered a fundamental flaw in DNS, which simplified cache poisoning - essentially making it possible for an attacker to misdirect users to arbitrary web sites.&lt;/p&gt;
    &lt;p&gt;In less than two years, the DNS root would be cryptographically signed - allowing those who wished to sign their domains as well to create a cryptographic chain of trust authenticating their DNS lookups. This is non-repudiation, not non-disclosure - DNS queries and responses continued to happen in the clear. But this time, responses came back with a digital signature, courtesy of DNSSEC.&lt;/p&gt;
    &lt;p&gt;David Huberman at ICANN did a splendid slide deck explaining how it all works.&lt;/p&gt;
    &lt;p&gt;Trust in a system requires more than technical correctness. It involves trust in the execution of running the system itself. For that reason ICANN decided that it would build a framework to facilitate trust and transparency. Among other things it included:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Placing the cryptographic material in two highly secure sites, one near Washington DC and one in Los Angeles (geographic diversity)&lt;/item&gt;
      &lt;item&gt;Creating a multi-layered security regimen requiring several people to access the Key Management Facility&lt;/item&gt;
      &lt;item&gt;Storing cryptographic material in offline HSMs which utilize Shamir's Secret Sharing to require a quorum of at least 3 out of 7 Crypto Officers to be present in order to "wake them up"&lt;/item&gt;
      &lt;item&gt;Trusted Community Representatives with roles of Crypto Officer and Recovery Key Share Holder&lt;/item&gt;
      &lt;item&gt;Highly scripted (and therefore auditable) ceremonies surrounding handling the cryptographc material&lt;/item&gt;
      &lt;item&gt;Live streaming all events&lt;/item&gt;
      &lt;item&gt;Hosting External Witnesses from the community who have expressed interest in being present for a ceremony in person&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the call for volunteers to be Trusted Community Representatives came out, I was no stranger to community involvement, having served several years on the ARIN Advisory Council and done community work (and later Board work) for NANOG. I was employed by a Top Level Domain operator, and submitted my CV and expressed my interest.&lt;/p&gt;
    &lt;p&gt;That's how I found myself in Culpeper, Virginia in 2010 as Crypto Officer 4 at the first ceremony for signing the DNSSEC Root. I had no idea that I would still be doing it fifteen years later. I was the last of the original Crypto Officers for KMF-East, and the second last overall - outlasted only by Subramanian "SM" Moonesamy, who is Crypto Officer 7 for KMF-West.&lt;/p&gt;
    &lt;p&gt;It's been an adventure. I've been a small participant in root key rolls, put in a B-roll appearance on BBC Horizon (S53E13), become friends with many of the people I served with, overseen ceremonies remotely during the COVID lockdown, and witnessed an amazing pivot by ICANN staff who managed to get new HSMs selected, tested, integrated, and deployed on only 8 months' notice, a feat which I remain in awe of.&lt;/p&gt;
    &lt;p&gt;I was an early advocate for improving trust in our process by leveraging natural turnover and backfilling existing TCRs with people selected from a broader set of qualified individuals than just the fellowship of DNS protocol experts, operators, and developers. I'm grateful that our voices were heard.&lt;/p&gt;
    &lt;p&gt;On November 13th 2025, I passed the torch to Lodrina Cherne who is now Crypto Officer 4 for KMF-East. Lodrina is a security researcher, an educator with an emphasis on digital forensics, and works in security engineering at a large cloud provider. I'm honored to have her as my successor.&lt;/p&gt;
    &lt;p&gt;I've had several people reach out to me to ask what prompted me to step back from the ICANN volunteer work. Those who were hoping for some kind of salacious dirt or scandal were sorely disappointed - quite the opposite, this is a huge success story and I'm pleased to have been able to do my small part. A direct cut and paste from Slack logs with one of them follows:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;What led you to step back from ICANN?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Several things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It was understood to be a 5 year commitment. I've been doing it for more than 15.&lt;/item&gt;
      &lt;item&gt;It was broadly agreed among the cohort many years ago (over a decade ago) that more people from more diverse backgrounds than just DNS-old-boy-network (which was the original group of TCRs) was a Good Thing.&lt;/item&gt;
      &lt;item&gt;Many people cycled out earlier; I was happy to let the folks for whom travel was more odious go first. But it's only practical and only really good for the system to cycle out a single TCR per ceremony.&lt;/item&gt;
      &lt;item&gt;COVID delayed this. Kaminsky's untimely death and subsequent replacement as a recovery key shareholder (RKSH) delayed this.&lt;/item&gt;
      &lt;item&gt;A further delay was the AEP Keyper HSM getting abruptly EOLed, and the transition to the Thales Luna HSMs. It went off without a hitch after being researched, developed, and executed in 8 months - a record which I stand in awe of and which is a true testament to the skill and commitment of the ICANN PTI team. ICANN expressed the desire for continuity among long-serving COs past that milestone; Frederico Neves (fellow original Crypto Officer) and I were willing to extend our stay for that.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So in short it was time to pass the torch. Everyone has been doing everything right. I remarked at the end of the Ceremony 59 that when we started doing this 15 years ago, success was not guaranteed; it took the Kaminsky bug to get us over the line to actually deploy it. Today, the major Unix DNS resolvers ship with DNSSEC validation enabled. All of the major public DNS resolvers (google, quad9, cloudflare) do DNSSEC validation. I thanked everyone who has been responsible for and put their personal credibility on the line for the security, integrity, and credibility of this process and stated that I was honored to have been able to play a small part in doing that.&lt;/p&gt;
    &lt;p&gt;Epilogue:&lt;/p&gt;
    &lt;p&gt;I won't be participating in most East Coast ceremonies from here on out, but I don't rule out occasionally showing up as an external witness, particularly at KMF-West where I have never visited in person.&lt;/p&gt;
    &lt;p&gt;Here scans of our ceremony scripts from both Ceremony 59 and the previous day's administrative ceremonies.&lt;/p&gt;
    &lt;p&gt;Root DNSSEC KSK Administrative Ceremony 59 Backup HSM Acceptance Testing&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029649</guid><pubDate>Mon, 24 Nov 2025 02:16:42 +0000</pubDate></item><item><title>Ask HN: Hearing aid wearers, what's hot?</title><link>https://news.ycombinator.com/item?id=46029699</link><description>&lt;doc fingerprint="16e2875fc8f6a6a6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;One of my Phonak Audeo 90’s (RIC) died the other day after 5 years and I’m shopping for new. What’s your go to hearing aid currently if you’ve upgraded recently or have been thinking of doing so?&lt;/p&gt;
      &lt;p&gt;Moderate loss, have worn them for many years, enjoy listening to music and nature, but also need help in meetings and noisy environments.&lt;/p&gt;
      &lt;p&gt;Not worried about cost and wanting to get one more good deal out of work insurance before I retire.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029699</guid><pubDate>Mon, 24 Nov 2025 02:25:39 +0000</pubDate></item><item><title>Show HN: Stun LLMs with thousands of invisible Unicode characters</title><link>https://gibberifier.com</link><description>&lt;doc fingerprint="26bd1c36eda432b6"&gt;
  &lt;main&gt;
    &lt;p&gt;Block AIs from reading your text with invisible Unicode characters while preserving meaning for humans.&lt;/p&gt;
    &lt;p&gt;How it works: This tool inserts invisible zero-width Unicode characters between each character of your input text. The text will look the same but will be much longer and can help stop AI plagarism. It also helps to waste tokens, causing users to run into ratelimits faster.&lt;/p&gt;
    &lt;p&gt;How to use: This tool works best when gibberifying the most important parts of an essay prompt, up to about 500 characters. This makes it harder for the AI to detect while still functioning well in Google Docs. Some AI models will crash or fail to process the gibberified text, while others will respond with confusion or simply ignore everything inside the gibberified text.&lt;/p&gt;
    &lt;p&gt;Use cases: Anti-plagiarism, text obfuscation for LLM scrapers, or just for fun!&lt;/p&gt;
    &lt;p&gt;Even just one word's worth of gibberified text is enough to block something like Flint AI from grading a session.&lt;/p&gt;
    &lt;p&gt;🤖 Tested Against AI Models&lt;/p&gt;
    &lt;p&gt;🤖 ChatGPT&lt;/p&gt;
    &lt;p&gt;Result: Doesn't understand gibberified text - responds with confusion or completely ignores the invisible characters.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029889</guid><pubDate>Mon, 24 Nov 2025 03:00:31 +0000</pubDate></item><item><title>The Cloudflare outage might be a good thing</title><link>https://gist.github.com/jbreckmckye/32587f2907e473dd06d68b0362fb0048</link><description>&lt;doc fingerprint="eb10b5770b2785f9"&gt;
  &lt;main&gt;
    &lt;p&gt;Cloudflare, the CDN provider, suffered a massive outage today. Some of the world's most popular apps and web services were left inaccessible for serveral hours whilst the Cloudflare team scrambled to fix a whole swathe of the internet.&lt;/p&gt;
    &lt;p&gt;And that might be a good thing.&lt;/p&gt;
    &lt;p&gt;The proximate cause of the outage was pretty mundane: a bad config file triggered a latent bug in one of Cloudflare's services. The file was too large (details still hazy) and this led to a cascading failure across Cloudflare operations. Probably there is some useful post-morteming about canary releases and staged rollouts.&lt;/p&gt;
    &lt;p&gt;But the bigger problem, the ultimate cause, behind today's chaos is the creeping centralisation of the internet and a society that is sleepwalking into assuming the net is always on and always working.&lt;/p&gt;
    &lt;p&gt;It's not just "trivial" stuff like Twitter and League of Legends that were affected, either. A friend of mine remarked caustically about his experience this morning&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I couldn't get air for my tyres at two garages because of cloudflare going down. Bloody love the lack of resilience that goes into the design when the machine says "cash only" and there's no cash slot. So flat tires for everyone! Brilliant.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We are living in a society where every part of our lives is increasingly mediated through the internet: work, banking, retail, education, entertainment, dating, family, government ID and credit checks. And the internet is increasingly tied up in fewer and fewer points of failure.&lt;/p&gt;
    &lt;p&gt;It's ironic because the internet was actually designed for decentralisation, a system that governments could use to coordinate their response in the event of nuclear war. But due to the economics of the internet, the challenges of things like bots and scrapers, more of more web services are holed up in citadels like AWS or behind content distribution networks like Cloudflare.&lt;/p&gt;
    &lt;p&gt;Outages like today's are a good thing because they're a warning. They can force redundancy and resilience into systems. They can make the pillars of our society - governments, businesses, banks - provide reliable alternatives when things go wrong.&lt;/p&gt;
    &lt;p&gt;(Ideally ones that are completely offline)&lt;/p&gt;
    &lt;p&gt;You can draw a parallel to how COVID-19 shook up global supply chains: the logic up until 2020 was that you wanted your system to be as lean and efficient as possible, even if it meant relying totally on international supplies or keeping as little spare inventory as possible. After 2020 businesses realised they needed to diversify and build slack in the system to tolerate shocks.&lt;/p&gt;
    &lt;p&gt;In the same way that growing one kind of banana, nearly resulted in bananas going extinct, we're drifing towards a society that can't survive without digital infrastructure; and a digital infrastructure that can't operate without two or three key players. One day there's going to be an outage, a bug, or cyberattack from a hostile state, that demonstrates how fragile that system is.&lt;/p&gt;
    &lt;p&gt;Embrace outages, and build redundancy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029908</guid><pubDate>Mon, 24 Nov 2025 03:04:10 +0000</pubDate></item><item><title>Japan's gamble to turn island of Hokkaido into global chip hub</title><link>https://www.bbc.com/news/articles/c8676qpxgnqo</link><description>&lt;doc fingerprint="adf0807148ab5fef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Japan's high-stakes gamble to turn island of flowers into global chip hub&lt;/head&gt;
    &lt;p&gt;The island of Hokkaido has long been an agricultural powerhouse – now Japan is investing billions to turn it into a global hub for advanced semiconductors.&lt;/p&gt;
    &lt;p&gt;More than half of Japan's dairy produce comes from Hokkaido, the northernmost of its main islands. In winter, it's a wonderland of ski resorts and ice-sculpture festivals; in summer, fields bloom with bands of lavender, poppies and sunflowers.&lt;/p&gt;
    &lt;p&gt;These days, cranes are popping up across the island – building factories, research centres and universities focused on technology. It's part of Japan's boldest industrial push in a generation: an attempt to reboot the country's chip-making capabilities and reshape its economic future.&lt;/p&gt;
    &lt;p&gt;Locals say that beyond the cattle and tourism, Hokkaido has long lacked other industries. There's even a saying that those who go there do so only to leave.&lt;/p&gt;
    &lt;p&gt;But if the government succeeds in turning Hokkaido into Japan's answer to Silicon Valley - or "Hokkaido Valley", as some have begun to call it - the country could become a new contender in the $600bn (£458bn) race to supply the world's computer chips.&lt;/p&gt;
    &lt;head rend="h2"&gt;An unlikely player&lt;/head&gt;
    &lt;p&gt;At the heart of the plan is Rapidus, a little-known company backed by the government and some of Japan's biggest corporations including Toyota, Softbank and Sony.&lt;/p&gt;
    &lt;p&gt;Born out of a partnership with IBM, it has raised billions of dollars to build Japan's first cutting-edge chip foundry in decades.&lt;/p&gt;
    &lt;p&gt;The government has invested $12bn in the company, so that it can build a massive semiconductor factory or "fab" in the small city of Chitose.&lt;/p&gt;
    &lt;p&gt;In selecting the Hokkaido location, Rapidus CEO Atsuyoshi Koike points to Chitose's water, electricity infrastructure and its natural beauty.&lt;/p&gt;
    &lt;p&gt;Mr Koike oversaw the fab design, which will be completely covered in grass to harmonise with Hokkaido's landscape, he told the BBC.&lt;/p&gt;
    &lt;p&gt;Local authorities have also flagged the region as being at lower risk of earthquakes compared to other potential sites in Japan.&lt;/p&gt;
    &lt;p&gt;A key milestone for Rapidus came with the delivery of an extreme ultraviolet lithography (EUV) system from the Dutch company ASML.&lt;/p&gt;
    &lt;p&gt;The high-tech machinery helped bring about Rapidus' biggest accomplishment yet earlier this year – the successful production of prototype two nanometre (2nm) transistors.&lt;/p&gt;
    &lt;p&gt;These ultra-thin chips are at the cutting edge of semiconductor technology and allow devices to run faster and more efficiently.&lt;/p&gt;
    &lt;p&gt;It's a feat only rival chip makers TSMC and Samsung have accomplished. Intel is not pursuing 2nm, it is leapfrogging from 7nm straight to 1.8nm.&lt;/p&gt;
    &lt;p&gt;"We succeeded in manufacturing the 2nm prototype for the first time in Japan, and at an unprecedented speed in Japan and globally," Mr Koike said.&lt;/p&gt;
    &lt;p&gt;He credits the IBM partnership for helping achieve the breakthrough.&lt;/p&gt;
    &lt;p&gt;Tie-ups with global companies are essential to acquiring the technology needed for this level of chips, he added.&lt;/p&gt;
    &lt;head rend="h2"&gt;The sceptics&lt;/head&gt;
    &lt;p&gt;Rapidus is confident that it is on track to mass produce 2nm chips by 2027. The challenge will be achieving the yield and quality that is needed to survive in an incredibly competitive market – the very areas where Taiwan and South Korea have pulled ahead.&lt;/p&gt;
    &lt;p&gt;TSMC for example has achieved incredible success in mass production, but making high-end chips is costly and technically demanding.&lt;/p&gt;
    &lt;p&gt;In a 2024 report, the Asean+3 Macroeconomic Research Office highlighted that although Rapidus is receiving government subsidies and consortium members are contributing funds: "The financing falls short of the expected 5 trillion yen ($31.8bn; £24.4bn) needed to start mass production."&lt;/p&gt;
    &lt;p&gt;The Center for Security and International Studies (CSIS) has previously said: "Rapidus has no experience in manufacturing advanced chips, and to date there is no indication that it will be able to access actual know-how for such an endeavour from companies with the requisite experience (ie TSMC and Samsung)."&lt;/p&gt;
    &lt;p&gt;Finding customers may also be a challenge – Samsung and TSMC have established relationships with global companies that have been buying their chips for years.&lt;/p&gt;
    &lt;head rend="h2"&gt;The lost decades&lt;/head&gt;
    &lt;p&gt;Nevertheless, Japan's government is pouring money into the chip industry - $27bn between 2020 and early 2024 - a larger commitment relative to its gross domestic product (GDP) than the US made through the Biden-era CHIPS Act.&lt;/p&gt;
    &lt;p&gt;In late 2024, Tokyo unveiled a $65bn package for Artificial Intelligence (AI) and semiconductors that could further support Rapidus's expansion plans.&lt;/p&gt;
    &lt;p&gt;This comes after decades of decline. Forty years ago Japan made more than half of the world's semiconductors. Today, it produces just over 10%.&lt;/p&gt;
    &lt;p&gt;Many point to US-Japan trade tensions in the 1980s as a turning point.&lt;/p&gt;
    &lt;p&gt;Naoyuki Yoshino, professor emeritus at Keio University, said Japan lost out in the technology stakes to Taiwan and South Korea in the 1980s, leaving domestic companies weaker.&lt;/p&gt;
    &lt;p&gt;Unlike its rivals, Japan failed to sustain subsidies to keep its chipmakers competitive.&lt;/p&gt;
    &lt;p&gt;But Mr Koike says that mentality has changed.&lt;/p&gt;
    &lt;p&gt;"The [national] government and local government are united in supporting our industry to revive once again."&lt;/p&gt;
    &lt;p&gt;Japan's broader economic challenges also loom large. Its population is shrinking while the number of elderly citizens continues to surge. That has determined the national budget for years and has contributed to slowing growth.&lt;/p&gt;
    &lt;p&gt;More than a third of its budget now goes to social welfare for the elderly, and that squeezes the money available for research, education and technology, Prof Yoshino says.&lt;/p&gt;
    &lt;p&gt;Japan also faces a severe shortage of semiconductor engineers – an estimated 40,000 people in the coming years.&lt;/p&gt;
    &lt;p&gt;Rapidus is partnering with Hokkaido University and others to train new workers, but agrees it will have to rely heavily on foreigners, at a time when public support for workers coming into the country for employment is low.&lt;/p&gt;
    &lt;head rend="h2"&gt;Growing an ecosystem&lt;/head&gt;
    &lt;p&gt;The government's push is already attracting major global players.&lt;/p&gt;
    &lt;p&gt;TSMC is producing 12–28nm chips in Kumamoto, on the south-western island of Kyushu - a significant step for Japan, even if it lags behind the company's cutting-edge production in Taiwan.&lt;/p&gt;
    &lt;p&gt;The expansion has transformed the local economy, attracting suppliers, raising wages, and leading to infrastructure and service developments.&lt;/p&gt;
    &lt;p&gt;Japan's broader chip revival strategy appears to be following a playbook: establish a "fab", and an entire ecosystem will tend to follow.&lt;/p&gt;
    &lt;p&gt;TSMC started building a second plant on Kyushu in October this year, which is due to begin production by the end of 2027.&lt;/p&gt;
    &lt;p&gt;Beyond Rapidus and TSMC, local players like Kioxia and Toshiba are also getting government backing.&lt;/p&gt;
    &lt;p&gt;Kioxia has expanded fabs in Yokkaichi and Kitakami with state funds and Toshiba has built one in Ishikawa. Meanwhile, ROHM has been officially designated as a company that provides critical products under Tokyo's economic security framework.&lt;/p&gt;
    &lt;p&gt;American memory chipmaker Micron will also receive $3.63bn in subsidies from the Japanese government to grow facilities in Hiroshima, while Samsung is building a research and development facility in Yokohama.&lt;/p&gt;
    &lt;p&gt;Hokkaido is seeing similar momentum. Chipmaking equipment companies ASML and Tokyo Electron have both opened offices in Chitose, off the back of Rapidus building a production facility there.&lt;/p&gt;
    &lt;p&gt;"This will make a form of 'global ecosystem'," Mr Koike says, "where we work together to be able to produce semiconductors that contribute to the world."&lt;/p&gt;
    &lt;p&gt;Mr Koike said Rapidus's key selling point would be - as its name suggests - an ability to produce custom chips faster than competitors, rather than competing directly with other players.&lt;/p&gt;
    &lt;p&gt;"TSMC leads the world, with Intel and Samsung close behind. Our edge is speed - we can produce and deliver chips three to four times faster than anyone else. That speed is what gives us an edge in the global semiconductor race," Mr Koike said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big bet&lt;/head&gt;
    &lt;p&gt;Global demand for chips is surging with the rise of AI, while Japan's automakers - still recovering from pandemic-era supply shocks - are pressing for more reliable, domestically or regionally sourced production across the entire supply chain, from raw materials to finished chips.&lt;/p&gt;
    &lt;p&gt;Securing control over chip manufacturing is being seen as a national security priority, both in Japan and elsewhere, as recent trade frictions and geopolitical tensions between China and Taiwan raise concerns around the risks of relying on foreign suppliers.&lt;/p&gt;
    &lt;p&gt;"We'd like to provide products from Japan once again – products that are powerful and with great new value," Mr Koike said.&lt;/p&gt;
    &lt;p&gt;For Japan's government, investing in Rapidus is a high-stakes gamble to revive its semiconductor industry and more broadly its tech power.&lt;/p&gt;
    &lt;p&gt;And some analysts say it may be the country's best chance to build a domestic ecosystem to supply advanced chips to its many manufacturers, and one day become a formidable challenger in the global market.&lt;/p&gt;
    &lt;p&gt;Additional reporting by Jaltson Akkanath Chummar&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029929</guid><pubDate>Mon, 24 Nov 2025 03:07:07 +0000</pubDate></item><item><title>RuBee</title><link>https://computer.rip/2025-11-22-RuBee.html</link><description>&lt;doc fingerprint="3e20d99aa1e6f77f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;RuBee&lt;/head&gt;
    &lt;p&gt;I have at least a few readers for which the sound of a man's voice saying "government cell phone detected" will elicit a palpable reaction. In Department of Energy facilities across the country, incidences of employees accidentally carrying phones into secure areas are reduced through a sort of automated nagging. A device at the door monitors for the presence of a tag; when the tag is detected it plays an audio clip. Because this is the government, the device in question is highly specialized, fantastically expensive, and says "government cell phone" even though most of the phones in question are personal devices. Look, they already did the recording, they're not changing it now!&lt;/p&gt;
    &lt;p&gt;One of the things that I love is weird little wireless networks. Long ago I wrote about ANT+, for example, a failed personal area network standard designed mostly around fitness applications. There's tons of these, and they have a lot of similarities---so it's fun to think about the protocols that went down a completely different path. It's even better, of course, if the protocol is obscure outside of an important niche. And a terrible website, too? What more could I ask for.&lt;/p&gt;
    &lt;p&gt;The DoE's cell-phone nagging boxes, and an array of related but more critical applications, rely on an unusual personal area networking protocol called RuBee.&lt;/p&gt;
    &lt;p&gt;RuBee is a product of Visible Assets Inc., or VAI, founded in 2004 1 by John K. Stevens. Stevens seems a somewhat improbable founder, with a background in biophysics and eye health, but he's a repeat entrepreneur. He's particularly fond of companies called Visible: he founded Visible Assets after his successful tenure as CEO of Visible Genetics. Visible Genetics was an early innovator in DNA sequencing, and still provides a specialty laboratory service that sequences samples of HIV in order to detect vulnerabilities to antiretroviral medications.&lt;/p&gt;
    &lt;p&gt;Clinical trials in the early 2000s exposed Visible Genetics to one of the more frustrating parts of health care logistics: refrigeration. Samples being shipped to the lab and reagents shipped out to clinics were both temperature sensitive. Providers had to verify that these materials had stayed adequately cold throughout shipping and handling, otherwise laboratory results could be invalid or incorrect. Stevens became interested in technical solutions to these problems; he wanted some way to verify that samples were at acceptable temperatures both in storage and in transit.&lt;/p&gt;
    &lt;p&gt;Moreover, Stevens imagined that these sensors would be in continuous communication. There's a lot of overlap between this application and personal area networks (PANs), protocols like Bluetooth that provide low-power communications over short ranges. There is also clear overlap with RFID; you can buy RFID temperature sensors. VAI, though, coined the term visibility network to describe RuBee. That's visibility as in asset visibility: somewhat different from Bluetooth or RFID, RuBee as a protocol is explicitly designed for situations where you need to "keep tabs" on a number of different objects. Despite the overlap with other types of wireless communications, the set of requirements on a visibility network have lead RuBee down a very different technical path.&lt;/p&gt;
    &lt;p&gt;Visibility networks have to be highly reliable. When you are trying to keep track of an asset, a failure to communicate with it represents a fundamental failure of the system. For visibility networks, the ability to actually convey a payload is secondary: the main function is just reliably detecting that endpoints exist. Visibility networks have this in common with RFID, and indeed, despite its similarities to technologies like BLE RuBee is positioned mostly as a competitor to technologies like UHF RFID.&lt;/p&gt;
    &lt;p&gt;There are several differences between RuBee and RFID; for example, RuBee uses active (battery-powered) tags and the tags are generally powered by a complete 4-bit microcontroller. That doesn't necessarily sound like an advantage, though. While RuBee tags advertise a battery life of "5-25 years", the need for a battery seems mostly like a liability. The real feature is what active tags enable: RuBee operates in the low frequency (LF) band, typically at 131 kHz.&lt;/p&gt;
    &lt;p&gt;At that low frequency, the wavelength is very long, about 2.5 km. With such a long wavelength, RuBee communications all happen at much less than one wavelength in range. RF engineers refer to this as near-field operation, and it has some properties that are intriguingly different from more typical far-field RF communications. In the near-field, the magnetic field created by the antenna is more significant than the electrical field. RuBee devices are intentionally designed to emit very little electrical RF signal. Communications within a RuBee network are achieved through magnetic, not electrical fields. That's the core of RuBee's magic.&lt;/p&gt;
    &lt;p&gt;The idea of magnetic coupling is not unique to RuBee. Speaking of the near-field, there's an obvious comparison to NFC which works much the same way. The main difference, besides the very different logical protocols, is that NFC operates at 13.56 MHz. At this higher frequency, the wavelength is only around 20 meters. The requirement that near-field devices be much closer than a full wavelength leads naturally to NFC's very short range, typically specified as 4 cm.&lt;/p&gt;
    &lt;p&gt;At LF frequencies, RuBee can achieve magnetic coupling at ranges up to about 30 meters. That's a range comparable to, and often much better than, RFID inventory tracking technologies. Improved range isn't RuBee's only benefit over RFID. The properties of magnetic fields also make it a more robust protocol. RuBee promises significantly less vulnerability to shielding by metal or water than RFID.&lt;/p&gt;
    &lt;p&gt;There are two key scenarios where this comes up: the first is equipment stored in metal containers or on metal shelves, or equipment that is itself metallic. In that scenario, it's difficult to find a location for an RFID tag that won't suffer from shielding by the container. The case of water might seem less important, but keep in mind that people are made mostly of water. RFID reading is often unreliable for objects carried on a person, which are likely to be shielded from the reader by the water content of the body.&lt;/p&gt;
    &lt;p&gt;These problems are not just theoretical. WalMart is a major adopter of RFID inventory technology, and in early rollouts struggled with low successful read rates. Metal, moisture (including damp cardboard boxes), antenna orientation, and multipath/interference effects could cause read failure rates as high as 33% when scanning a pallet of goods. Low read rates are mostly addressed by using RFID "portals" with multiple antennas. Eight antennas used as an array greatly increase read rate, but at a cost of over ten thousand dollars per portal system. Even so, WalMart seems to now target a success rate of only 95% during bulk scanning.&lt;/p&gt;
    &lt;p&gt;95% might sound pretty good, but there are a lot of visibility applications where a failure rate of even a couple percent is unacceptable. These mostly go by the euphemism "high value goods," which depending on your career trajectory you may have encountered in corporate expense and property policies. High-value goods tend to be items that are both attractive to theft and where theft has particularly severe consequences. Classically, firearms and explosives. Throw in classified material for good measure.&lt;/p&gt;
    &lt;p&gt;I wonder if Stevens was surprised by RuBee's market trajectory. He came out of the healthcare industry and, it seems, originally developed RuBee for cold chain visibility... but, at least in retrospect, it's quite obvious that its most compelling application is in the armory.&lt;/p&gt;
    &lt;p&gt;Because RuBee tags are small and largely immune to shielding by metals, you can embed them directly in the frames of firearms, or as an aftermarket modification you can mill out some space under the grip. RuBee tags in weapons will read reliably when they are stored in metal cases or on metal shelving, as is often the case. They will even read reliably when a weapon is carried holstered, close to a person's body.&lt;/p&gt;
    &lt;p&gt;Since RuBee tags incorporate an active microcontroller, there are even more possibilities. Temperature logging is one thing, but firearm-embedded RuBee tags can incorporate an accelerometer (NIST-traceable, VAI likes to emphasize) and actually count the rounds fired.&lt;/p&gt;
    &lt;p&gt;Sidebar time: there is a long history of political hazard around "smart guns." The term "smart gun" is mostly used more specifically for firearms that identify their user, for example by fingerprint authentication or detection of an RFID fob. The idea has become vague enough, though, that mention of a firearm with any type of RFID technology embedded would probably raise the specter of the smart gun to gun-rights advocates.&lt;/p&gt;
    &lt;p&gt;Further, devices embedded in firearms that count the number of rounds fired have been proposed for decades, if not a century, as a means of accountability. The holder of a weapon could, in theory, be required to positively account for every round fired. That could eliminate incidents of unreported use of force by police, for example. In practice I think this is less compelling than it sounds, simple counting of rounds leaves too many opportunities to fudge the numbers and conceal real-world use of a weapon as range training, for example.&lt;/p&gt;
    &lt;p&gt;That said, the NRA has long been vehemently opposed to the incorporation of any sort of technology into weapons that could potentially be used as a means of state control or regulation. The concern isn't completely unfounded; the state of New Jersey did, for a time, have legislation that would have made user-identifying "smart guns" mandatory if they were commercially available. The result of the NRA's strident lobbying is that no such gun has ever become commercially available; "smart guns" have been such a political third rail that any firearms manufacturer that dared to introduce one would probably face a boycott by most gun stores. For better or worse, a result of the NRA's powerful political advocacy in this area is that the concept of embedding security or accountability technology into weapons has never been seriously pursued in the US. Even a tentative step in that direction can produce a huge volume of critical press for everyone involved.&lt;/p&gt;
    &lt;p&gt;I bring this up because I think it explains some of why VAI seems a bit vague and cagey about the round-counting capabilities of their tags. They position it as purely a maintenance feature, allowing the armorer to keep accurate tabs on the preventative maintenance schedule for each individual weapon (in armory environments, firearm users are often expected to report how many rounds they fired for maintenance tracking reasons). The resistance of RuBee tags to concealment is only positioned as a deterrent to theft, although the idea of RuBee-tagged firearms creates obvious potential for security screening. Probably the most profitable option for VAI would be to promote RuBee-tagged firearms as tool for enforcement of gun control laws, but this is a political impossibility and bringing it up at all could cause significant reputational harm, especially with the government as a key customer. The result is marketing copy that is a bit odd, giving a set of capabilities that imply an application that is never mentioned.&lt;/p&gt;
    &lt;p&gt;VAI found an incredible niche with their arms-tracking application. Institutional users of firearms, like the military, police, and security forces, are relatively price-insensitive and may have strict accounting requirements. By the mid-'00s, VAI was into the long sales cycle of proposing the technology to the military. That wasn't entirely unsuccessful. RuBee shot-counting weapon inventory tags were selected by the Naval Surface Warfare Center in 2010 for installation on SCAR and M4 rifles. That contract had a five-year term, it's unclear to me if it was renewed. Military contracting opened quite a few doors to VAI, though, and created a commercial opportunity that they eagerly pursued.&lt;/p&gt;
    &lt;p&gt;Perhaps most importantly, weapons applications required an impressive round of safety and compatibility testing. RuBee tags have the fairly unique distinction of military approval for direct attachment to ordnance, something called "zero separation distance" as the tags do not require a minimum separation from high explosives. Central to that certification are findings of intrinsic safety of the tags (that they do not contain enough energy to trigger explosives) and that the magnetic fields involved cannot convey enough energy to heat anything to dangerous temperatures.&lt;/p&gt;
    &lt;p&gt;That's not the only special certification that RuBee would acquire. The military has a lot of firearms, but military procurement is infamously slow and mercurial. Improved weapon accountability is, almost notoriously, not a priority for the US military which has often had stolen weapons go undetected until their later use in crime. The Navy's interest in RuBee does not seem to have translated to more widespread military applications.&lt;/p&gt;
    &lt;p&gt;Then you have police departments, probably the largest institutional owners of firearms and a very lucrative market for technology vendors. But here we run into the political hazard: the firearms lobby is very influential on police departments, as are police unions which generally oppose technical accountability measures. Besides, most police departments are fairly cash-poor and are not likely to make a major investment in a firearms inventory system.&lt;/p&gt;
    &lt;p&gt;That leaves us with institutional security forces. And there is one category of security force that are particularly well-funded, well-equipped, and beholden to highly R&amp;amp;D-driven, almost pedantic standards of performance: the protection forces of atomic energy facilities.&lt;/p&gt;
    &lt;p&gt;Protection forces at privately-operated atomic energy facilities, such as civilian nuclear power plants, are subject to licensing and scrutiny by the Nuclear Regulatory Commission. Things step up further at the many facilities operated by the National Nuclear Security Administration (NNSA). Protection forces for NNSA facilities are trained at the Department of Energy's National Training Center, at the former Manzano Base here in Albuquerque. Concern over adequate physical protection of NNSA facilities has lead Sandia National Laboratories to become one of the premier centers for R&amp;amp;D in physical security. Teams of scientists and engineers have applied sometimes comical scientific rigor to "guns, gates, and guards," the traditional articulation of physical security in the nuclear world.&lt;/p&gt;
    &lt;p&gt;That scope includes the evaluation of new technology for the management of protection forces, which is why Oak Ridge National Laboratory launched an evaluation program for the RuBee tagging of firearms in their armory. The white paper on this evaluation is curiously undated, but citations "retrieved 2008" lead me to assume that the evaluation happened right around the middle of the '00s. At the time, VAI seems to have been involved in some ultimately unsuccessful partnership with Oracle, leading to the branding of the RuBee system as Oracle Dot-Tag Server. The term "Dot-Tag" never occurs outside of very limited materials around the Oracle partnership, so I'm not sure if it was Oracle branding for RuBee or just some passing lark. In any case, Oracle's involvement seems to have mainly just been the use of the Oracle database for tracking inventory data---which was naturally replaced by PostgreSQL at Oak Ridge.&lt;/p&gt;
    &lt;p&gt;The Oak Ridge trial apparently went well enough, and around the same time, the Pantex Plant in Texas launched an evaluation of RuBee for tracking classified tools. Classified tools are a tricky category, as they're often metallic and often stored in metallic cases. During the trial period, Pantex tagged a set of sample classified tools with RuBee tags and then transported them around the property, testing the ability of the RuBee controllers to reliably detect them entering and exiting areas of buildings. Simultaneously, Pantex evaluated the use of RuBee tags to track containers of "chemical products" through the manufacturing lifecycle. Both seem to have produced positive results.&lt;/p&gt;
    &lt;p&gt;There are quite a few interesting and strange aspects of the RuBee system, a result of its purpose-built Visibility Network nature. A RuBee controller can have multiple antennas that it cycles through. RuBee tags remain in a deep-sleep mode for power savings until they detect a RuBee carrier during their periodic wake cycle. When a carrier is detected, they fully wake and listen for traffic. A RuBee controller can send an interrogate message and any number of tags can respond, with an interesting and novel collision detection algorithm used to ensure reliable reading of a large number of tags.&lt;/p&gt;
    &lt;p&gt;The actual RuBee protocol is quite simple, and can also be referred to as IEEE 1902.1 since the decision of VAI to put it through the standards process. Packets are small and contain basic addressing info, but they can also contain arbitrary payload in both directions, perfect for data loggers or sensors. RuBee tags are identified by something that VAI oddly refers to as an "IP address," causing some confusion over whether or not VAI uses IP over 1902.1. They don't, I am confident saying after reading a whole lot of documents. RuBee tags, as standard, have three different 4-byte addresses. VAI refers to these as "IP, subnet, and MAC," 2 but these names are more like analogies. Really, the "IP address" and "subnet" are both configurable arbitrary addresses, with the former intended for unicast traffic and the latter for broadcast. For example, you would likely give each asset a unique IP address, and use subnet addresses for categories or item types. The subnet address allows a controller to interrogate for every item within that category at once. The MAC address is a fixed, non-configurable address derived from the tag's serial number. They're all written in the formats we associate with IP networks, dotted-quad notation, as a matter of convenience.&lt;/p&gt;
    &lt;p&gt;And that's about it as far as the protocol specification, besides of course the physical details which are a 131,072 Hz carrier, 1024 Hz data clock, either ASK or BPSK modulation. The specification also describes an interesting mode called "clip," in which a set of multiple controllers interrogate in exact synchronization and all tags then reply in exact synchronization. Somewhat counter-intuitively, because of the ability of RuBee controllers to separate out multiple simultaneous tag transmissions using an anti-collision algorithm based on random phase shifts by each tag, this is ideal. It allows a room, say an armory, full of RuBee controllers to rapidly interrogate the entire contents of the room. I think this feature may have been added after the Oak Ridge trials...&lt;/p&gt;
    &lt;p&gt;RuBee is quite slow, typically 1,200 baud, so inventorying a large number of assets can take a while (Oak Ridge found that their system could only collect data on 2-7 tags per second per controller). But it's so robust that it an achieve a 100% read rate in some very challenging scenarios. Evaluation by the DoE and the military produced impressive results. You can read, for example, of a military experiment in which a RuBee antenna embedded in a roadway reliably identified rifles secured in steel containers in passing Humvees.&lt;/p&gt;
    &lt;p&gt;Paradoxically, then, one of the benefits of RuBee in the military/defense context is that it is also difficult to receive. Here is RuBee's most interesting trick: somewhat oversimplified, the strength of an electrical radio signal goes as 1/r, while the strength of a magnetic field goes as 1/r^3. RuBee equipment is optimized, by antenna design, to produce a minimal electrical field. The result is that RuBee tags can very reliably be contacted at short range (say, around ten feet), but are virtually impossible to contact or even detect at ranges over a few hundred feet. To the security-conscious buyer, this is a huge feature. RuBee tags are highly resistant to communications or electronic intelligence collection.&lt;/p&gt;
    &lt;p&gt;Consider the logical implications of tagging the military's rifles. With conventional RFID, range is limited by the size and sensitivity of the antenna. Particularly when tags are incidentally powered by a nearby reader, an adversary with good equipment can detect RFID tags at very long range. VAI heavily references a 2010 DEFCON presentation, for example, that demonstrated detection of RFID tags at a range of 80 miles. One imagines that opportunistic detection by satellite is feasible for a state intelligence agency. That means that your rifle asset tracking is also revealing the movements of soldiers in the field, or at least providing a way to detect their approach.&lt;/p&gt;
    &lt;p&gt;Most RuBee tags have their transmit power reduced by configuration, so even the maximum 100' range of the protocol is not achievable. VAI suggests that typical RuBee tags cannot be detected by radio direction finding equipment at ranges beyond 20', and that this range can be made shorter by further reducing transmit power.&lt;/p&gt;
    &lt;p&gt;Once again, we have caught the attention of the Department of Energy. Because of the short range of RuBee tags, they have generally been approved as not representing a COMSEC or TEMPEST hazard to secure facilities. And that brings us back to the very beginning: why does the DoE use a specialized, technically interesting, and largely unique radio protocol to fulfill such a basic function as nagging people that have their phones? Because RuBee's security properties have allowed it to be approved for use adjacent to and inside of secure facilities. A RuBee tag, it is thought, cannot be turned into a listening device because the intrinsic range limitation of magnetic coupling will make it impossible to communicate with the tag from outside of the building. It's a lot like how infrared microphones still see some use in secure facilities, but so much more interesting!&lt;/p&gt;
    &lt;p&gt;VAI has built several different product lines around RuBee, with names like Armory 20/20 and Shot Counting Allegro 20/20 and Store 20/20. The founder started his career in eye health, remember. None of them are that interesting, though. They're all pretty basic CRUD applications built around polling multiple RuBee controllers for tags in their presence.&lt;/p&gt;
    &lt;p&gt;And then there's the "Alert 20/20 DoorGuard:" a metal pedestal with a RuBee controller and audio announcement module, perfect for detecting government cell phones.&lt;/p&gt;
    &lt;p&gt;One of the strangest things about RuBee is that it's hard to tell if it's still a going concern. VAI's website has a press release section, where nothing has been posted since 2019. The whole website feels like it was last revised even longer ago. When RuBee was newer, back in the '00s, a lot of industry journals covered it with headlines like "the new RFID." I think VAI was optimistic that RuBee could displace all kinds of asset tracking applications, but despite some special certifications in other fields (e.g. approval to use RuBee controllers and tags around pacemakers in surgical suites), I don't think RuBee has found much success outside of military applications.&lt;/p&gt;
    &lt;p&gt;RuBee's resistance to shielding is impressive, but RFID read rates have improved considerably with new DSP techniques, antenna array designs, and the generally reduced cost of modern RFID equipment. RuBee's unique advantages, its security properties and resistance to even intentional exfiltration, are interesting but not worth much money to buyers other than the military.&lt;/p&gt;
    &lt;p&gt;So that's the fate of RuBee and VAI: defense contracting. As far as I can tell, RuBee and VAI are about as vital as they have ever been, but RuBee is now installed as just one part of general defense contracts around weapons systems, armory management, and process safety and security. IEEE standardization has opened the door to use of RuBee by federal contractors under license, and indeed, Lockheed Martin is repeatedly named as a licensee, as are firearms manufacturers with military contracts like Sig Sauer.&lt;/p&gt;
    &lt;p&gt;Besides, RuBee continues to grow closer to the DoE. In 2021, VAI appointed Lisa Gordon-Hagerty to it board of directors. Gordon-Hagerty was undersecretary of Energy and had lead the NNSA until the year before. This year, the New Hampshire Small Business Development Center wrote a glowing profile of VAI. They described it as a 25-employee company with a goal of hitting $30 million in annual revenue in the next two years.&lt;/p&gt;
    &lt;p&gt;Despite the outdated website, VAI claims over 1,200 RuBee sites in service. I wonder how many of those are Alert 20/20 DoorGuards? Still, I do believe there are military weapons inventory systems currently in use. RuBee probably has a bright future, as a niche technology for a niche industry. If nothing else, they have legacy installations and intellectual property to lean on. A spreadsheet of VAI-owned patents on RuBee, with nearly 200 rows, encourages would-be magnetically coupled visibility network inventors not to go it on their own. I just wish I could get my hands on a controller....&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;I have found some conflicting information on the date, it could have been as early as 2002. 2004 is the year I have the most confidence in.↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The documentation is confusing enough about these details that I am actually unclear on whether the RuBee "MAC address" is 4 bytes or 6. Examples show 6 byte addresses, but the actual 1902.1 specification only seems to allow 4 byte addresses in headers. Honestly all of the RuBee documentation is a mess like this. I suspect that part of the problem is that VAI has actually changed parts of the protocol and not all of their products are IEEE 1902.1 compliant.↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46029932</guid><pubDate>Mon, 24 Nov 2025 03:08:10 +0000</pubDate></item><item><title>Lambda Calculus – Animated Beta Reduction of Lambda Diagrams</title><link>https://cruzgodar.com/applets/lambda-calculus</link><description>&lt;doc fingerprint="962d55ae5cddd3a0"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript is required to use this site and many others. Consider enabling it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46030613</guid><pubDate>Mon, 24 Nov 2025 05:17:15 +0000</pubDate></item><item><title>General principles for the use of AI at CERN</title><link>https://home.web.cern.ch/news/official-news/knowledge-sharing/general-principles-use-ai-cern</link><description>&lt;doc fingerprint="ce5a81fa80f3ca86"&gt;
  &lt;main&gt;
    &lt;p&gt;Artificial intelligence (AI) can be found at CERN in many contexts: embedded in devices, software products and cloud services procured by CERN, brought on-site by individuals or developed in-house.&lt;/p&gt;
    &lt;p&gt;Following the approval of a CERN-wide AI strategy, these general principles are designed to promote the responsible and ethical use, development and deployment (collectively “use”) of AI at CERN.&lt;/p&gt;
    &lt;p&gt;They are technology neutral and apply to all AI technologies as they become available.&lt;/p&gt;
    &lt;p&gt;The principles apply across all areas of CERN’s activities, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI for scientific and technical research: data analysis, anomaly detection, simulation, predictive maintenance and optimisation of accelerator performance or detector operations, and&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI for productivity and administrative use: document drafting, note taking, automated translation, language correction and enhancement, coding assistants, and workflow automation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;General Principles&lt;/p&gt;
    &lt;p&gt;CERN, members of its personnel, and anyone using CERN computing facilities shall ensure that AI is used in accordance with the following principles:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transparency and explainability: Document and communicate when and how AI is used, and how AI contributes to specific tasks or decisions.&lt;/item&gt;
      &lt;item&gt;Responsibility and accountability: The use of AI, including its impact and resulting outputs throughout its lifecycle, must not displace ultimate human responsibility and accountability.&lt;/item&gt;
      &lt;item&gt;Lawfulness and conduct: The use of AI must be lawful, compliant with CERN’s internal legal framework and respect third-party rights and CERN’s Code of Conduct.&lt;/item&gt;
      &lt;item&gt;Fairness, non-discrimination, and “do no harm”: AI must be used in a way that promotes fairness and inclusiveness and prevents bias, discrimination and any other form of harm.&lt;/item&gt;
      &lt;item&gt;Security and safety: AI must be adequately protected to reduce the likelihood and impact of cybersecurity incidents. AI must be used in a way that is safe, respects confidentiality, integrity and availability requirements, and prevents negative outcomes.&lt;/item&gt;
      &lt;item&gt;Sustainability: The use of AI must be assessed with the goal of mitigating environmental and social risks and enhancing CERN's positive impact in relation to society and the environment.&lt;/item&gt;
      &lt;item&gt;Human oversight: The use of AI must always remain under human control. Its functioning and outputs must be consistently and critically assessed and validated by a human.&lt;/item&gt;
      &lt;item&gt;Data privacy: AI must be used in a manner that respects privacy and the protection of personal data.&lt;/item&gt;
      &lt;item&gt;Non-military purposes: Any use of AI at CERN must be for non-military purposes only.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46032513</guid><pubDate>Mon, 24 Nov 2025 10:37:05 +0000</pubDate></item><item><title>Shai-Hulud Returns: Over 300 NPM Packages Infected</title><link>https://helixguard.ai/blog/malicious-sha1hulud-2025-11-24</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46032539</guid><pubDate>Mon, 24 Nov 2025 10:40:22 +0000</pubDate></item><item><title>NSA and IETF, part 3: Dodging the issues at hand</title><link>https://blog.cr.yp.to/20251123-dodging.html</link><description>&lt;doc fingerprint="331298363f8401c8"&gt;
  &lt;main&gt;
    &lt;p&gt;Normal practice in deploying post-quantum cryptography is to deploy ECC+PQ. IETF's TLS working group is standardizing ECC+PQ. But IETF management is also non-consensually ramming a particular NSA-driven document through the IETF process, a "non-hybrid" document that adds just PQ as another TLS option.&lt;/p&gt;
    &lt;p&gt;Don't worry: we're standardizing cars with seatbelts. Also, recognizing generous funding from the National Morgue Association, we're going to standardize cars without seatbelts as another option, ignoring the safety objections. That's okay, right?&lt;/p&gt;
    &lt;p&gt;Last month I posted part 1 of this story. Today's part 2 highlighted the corruption. This blog post, part 3, highlights the dodging in a particular posting at the beginning of this month by an IETF "security area director". Part 4 will give an example of how dissent on this topic has been censored.&lt;/p&gt;
    &lt;p&gt;Consensus means whatever the people in power want to do. Recall from my previous blog post that "adoption" of a document is a preliminary step before an IETF "working group" works on, and decides whether to standardize, the document. In April 2025, the chairs of the IETF TLS WG called for "adoption" of this NSA-driven document. During the call period, 20 people expressed unequivocal support for adoption, 2 people expressed conditional support for adoption, and 7 people expressed unequivocal opposition to adoption. (Details for verification.)&lt;/p&gt;
    &lt;p&gt;The chairs claimed that "we have consensus to adopt this draft". I promptly asked for explanation.&lt;/p&gt;
    &lt;p&gt;Before the chairs could even reply, an "area director" interrupted, claiming, inter alia, the following: "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;After these lies by the "area director" were debunked, the chairs said that they had declared consensus "because there is clearly sufficient interest to work on this draft" specifically "enough people willing to review the draft".&lt;/p&gt;
    &lt;p&gt;I can understand not everybody being familiar with the specific definition of "consensus" that antitrust law requires standards-development organizations to follow. But it's astonishing to see chairs substituting a consensus-evaluation procedure that simply ignores objections.&lt;/p&gt;
    &lt;p&gt;Stonewalling. The chairs said I could escalate. IETF procedures say that an unresolved dispute can be brought "to the attention of the Area Director(s) for the area in which the Working Group is chartered", and then "The Area Director(s) shall attempt to resolve the dispute".&lt;/p&gt;
    &lt;p&gt;I filed a complaint with the "security area directors" in early June 2025. One of them never replied. The other, the same one who had claimed that there was "clearly consensus", sent a series of excuses for not handling the complaint. For example, one excuse was that the PDF format "discourages participation".&lt;/p&gt;
    &lt;p&gt;Do IETF procedures say "The Area Director(s) shall attempt to resolve the dispute unless the dispute is documented in a PDF"? No.&lt;/p&gt;
    &lt;p&gt;I sent email two days later systematically addressing the excuses. The "area director" never replied.&lt;/p&gt;
    &lt;p&gt;It isn't clear under IETF procedures whether a non-reply allows an appeal. It is, however, clear that an appeal can't be filed after two months. I escalated to the "Internet Engineering Steering Group" (IESG) in August 2025.&lt;/p&gt;
    &lt;p&gt;(These aren't even marginally independent groups. The "area directors" are the IESG members. IESG appoints the WG chairs.)&lt;/p&gt;
    &lt;p&gt;IESG didn't reply until October 2025. It rejected one of the "Area Director" excuses for having ignored my complaint, but endorsed another excuse. I promptly filed a revised complaint with the "area director", jumping through the hoops that IESG had set. There were then further runarounds.&lt;/p&gt;
    &lt;p&gt;The switch. Suddenly, on 1 November 2025, IESG publicly instructed the "area director" to address the following question: "Was rough consensus to adopt draft-connolly-tls-mlkem-key-agreement in the TLS Working Group appropriately called by the WG chairs?"&lt;/p&gt;
    &lt;p&gt;The "area director" posted his conclusion mere hours later: "I agree with the TLS WG Chairs that the Adoption Call result was that there was rough consensus to adopt the document".&lt;/p&gt;
    &lt;p&gt;Dodging procedural objections. Before looking at how the "area director" argued for this conclusion, I'd like to emphasize three things that the "area director" didn't do.&lt;/p&gt;
    &lt;p&gt;First, did the "area director" address my complaint about the chair action on this topic? No.&lt;/p&gt;
    &lt;p&gt;One reason this matters is that the law requires standards-development organizations to provide an "appeals process". Structurally, the "area director" isn't quoting and answering the points in my complaint; the "area director" puts the entire burden on the reader to try to figure out what's supposedly answering what, and to realize that many points remain unanswered.&lt;/p&gt;
    &lt;p&gt;Second, did the "area director" address the chairs claiming that "we have consensus to adopt this draft"? Or the previous claim from the "area director" that there was "clearly consensus"? No. Instead IESG and this "area director" quietly shifted from "consensus" to "rough consensus". (Did you notice this shift when I quoted IESG's "rough consensus" instruction?)&lt;/p&gt;
    &lt;p&gt;One reason this matters is that "consensus" is another of the legal requirements for standards-development organizations. The law doesn't allow "rough consensus". Also, IETF claims that "decision-making requires achieving broad consensus". "broad consensus" is even stronger than "consensus", since it's saying that there's consensus in a broad group.&lt;/p&gt;
    &lt;p&gt;Third, the way that my complaint had established the lack of consensus was, first, by reviewing the general definition of "consensus" (which I paraphrased from the definition in the law, omitting a citation only because the TLS chairs had threatened me with a list ban if I mentioned the law again), and then applying the components of that definition to the situation at hand. Did the area director follow this structure? Here's the definition of "consensus", or "rough consensus" if we're switching to that, and now let's apply that definition? No. Nobody reading this message from the "area director" can figure out what the "area director" believes these words mean.&lt;/p&gt;
    &lt;p&gt;Wow, look at that: "due process" is another of the legal requirements for standards-development organizations. Part of due process is simply making clear what procedures are being applied. Could it possibly be that the people writing the law were thinking through how standardization processes could be abused?&lt;/p&gt;
    &lt;p&gt;Numbers. Without further ado, let's look at what the "security area director" did write.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The IESG has requested that I evaluate the WG Adoption call results for ML-KEM Post-Quantum Key Agreement for TLS 1.3 (draft-connolly-tls-mlkem-key-agreement). Please see below.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As noted above, IESG had instructed the "area director" to answer the following question: "Was rough consensus to adopt draft-connolly-tls-mlkem-key-agreement in the TLS Working Group appropriately called by the WG chairs?"&lt;/p&gt;
    &lt;p&gt;Side note: Given that the "area director" posted all of the following on the same day that IESG instructed the "area director" to write this, presumably this was all written in advance and coordinated with the rest of IESG. I guess the real point of finally (on 1 November 2025) addressing the adoption decision (from 15 April 2025) was to try to provide cover for the "last call" a few days later (5 November 2025).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ExecSum&lt;/p&gt;
      &lt;p&gt;I agree with the TLS WG Chairs that the Adoption Call result was that there was rough consensus to adopt the document.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As noted above, the TLS WG chairs had claimed "consensus", and the "area director" had claimed that there was "clearly consensus". The "area director" is now quietly shifting to a weaker claim.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Timeline&lt;/p&gt;
      &lt;p&gt;April 1: Sean and Joe announce WG Adoption Call [ about 40 messages sent in the thread ]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"About 40"? What happened to the "area director" previously writing "There is clearly consensus based on the 67 responses to the adoption call"? And why is the number of messages supposed to matter in the first place?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;April 15: Sean announces the Adoption Call passed. [ another 50 messages are sent in the thread ]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Messages after the specified adoption-call deadline can't justify the claim that "the Adoption Call result was that there was rough consensus to adopt the document". The adoption call failed to reach consensus.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;April 18 to today: A chain of (attempted) Appeals by D. J. Bernstein to the AD(s), IESG and IAB, parts of which are still in process.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The fact that the ADs and IESG stonewalled in response to complaints doesn't mean that they were "attempted" complaints.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Outcome&lt;/p&gt;
      &lt;p&gt;30 people participated in the consensus call, 23 were in favour of adoption, 6 against and 1 ambivalent (names included at the bottom of this email).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;These numbers are much closer to reality than the "area director" previously writing "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions".&lt;/p&gt;
    &lt;p&gt;Also, given that the "area director" is continually making claims that aren't true (see examples below) and seems generally allergic to providing evidence (the text I'm quoting below has, amazingly, zero URLs), it's a relief to see the "area director" providing names to back up the claimed numbers here.&lt;/p&gt;
    &lt;p&gt;But somehow, even after being caught lying about the numbers before, the "area director" still can't resist shading the numbers a bit.&lt;/p&gt;
    &lt;p&gt;The actual numbers were 20 people unequivocally supporting adoption, 2 people conditionally supporting adoption, and 7 people unequivocally opposing adoption. Clearly 7 is close to 6, and 20+2 is close to 23, but, hmmm, not exactly. Let's check the details:&lt;/p&gt;
    &lt;p&gt;How does the "area director" end up with 6 negative votes rather than 7? By falsely listing Thomas Bellebaum as "ambivalent" and falsely attributing a "prefer not, but okay if we do" position to Bellebaum. In fact, Bellbaum had written "I agree with Stephen on this one and would not support adoption of non-hybrids." (This was in reply to Stephen Farrell, who had written "I'm opposed to adoption, at this time.")&lt;/p&gt;
    &lt;p&gt;How does the "area director" end up with 23 positive votes rather than 22? By falsely listing the document author (Deirdre Connolly) as having stated a pro-adoption position during the call. The "area director" seems generally clueless about conflict-of-interest issues and probably doesn't find it obvious that an author shouldn't vote, but the simple fact is that the author didn't vote. She sent three messages during the call period; all of those messages are merely commenting on specific points, not casting a vote on the adoption question.&lt;/p&gt;
    &lt;p&gt;The document author didn't object to the "area director" fudging the numbers. Bellebaum did politely object; the "area director" didn't argue, beyond trying to save face with comments such as "Thanks for the clarification".&lt;/p&gt;
    &lt;p&gt;More to the point, the "area director" has never explained whether or how the tallies of positive and negative votes are supposed to be relevant to the "rough consensus" claim. The "area director" also hasn't commented on IETF saying that IETF doesn't make decisions by voting.&lt;/p&gt;
    &lt;p&gt;Bogus arguments for the draft. I mentioned in my previous blog post that IETF claims that "IETF participants use their best engineering judgment to find the best solution for the whole Internet, not just the best solution for any particular network, technology, vendor, or user".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In favour argument summary&lt;/p&gt;
      &lt;p&gt;While there is a lack of substantiating why adoption is desired - which is typical&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Okay, the "area director" seems to have some basic awareness that this document flunks the "engineering judgment" criterion. The "area director" tries to defend this by saying that other documents flunk too. So confidence-inspiring!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;- the big use case seems to be to support those parties relying on NIST and FIPS for their security requirements.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Wrong. Anything+PQ, and in particular ECC+PQ, complies with NIST's standards when the PQ part does. See NIST SP 800-227: "This publication approves the use of the key combiner (14) for any t &amp;gt; 1 if at least one shared secret (i.e., Sj for some j) is generated from the key-establishment methods in SP 800-56A [1] or SP 800-56B [2] or an approved KEM." For example, if the PQ part is ML-KEM as per FIPS 203, then NIST allows ECC+PQ too.&lt;/p&gt;
    &lt;p&gt;What's next: claiming that using PQ in an Internet protocol would violate NIST standards unless NIST has standardized that particular Internet protocol?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This encompasses much more than just the US government as other certification bodies and other national governments have come to rely on the outcome of the NIST competition, which was the only public multi-year post-quantum cryptography effort to evaluate the security of proposed new post-quantum algorithms.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I won't bother addressing the errors here, since the bottom-line claim is orthogonal to the issue at hand. The TLS WG already has an ECC+PQ document using NIST-approved PQ; the question is whether to also have a document allowing the ECC seatbelt to be removed.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was also argued pure PQ has less complexity.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You know what would be even less complicated? Encrypting with the null cipher!&lt;/p&gt;
    &lt;p&gt;There was a claim that PQ is less complex than ECC+PQ. There was no response to Andrey Jivsov objecting that having a PQ option makes the ecosystem more complicated. The basic error in the PQ-less-complex claim is that it ignores ECC+PQ already being there.&lt;/p&gt;
    &lt;p&gt;How the "area director" described the objections.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Opposed argument summary&lt;/p&gt;
      &lt;p&gt;Most of the arguments against adoption are focused on the fact that a failsafe is better than no failsafe, irrespective of which post-quantum algorithm is used,&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is the closest that the "area director" comes to acknowledging the central security argument for ECC+PQ. Of course, the "area director" spends as little time as possible on security. Compare this to my own objection to adoption, which started with SIKE as a concrete example of the dangers and continued with "SIKE is not an isolated example: https://cr.yp.to/papers.html#qrcsp shows that 48% of the 69 round-1 submissions to the NIST competition have been broken by now".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;and that the practical costs for hybrids are negligible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Hmmm. By listing this as part of an "opposed argument summary", is the "area director" suggesting that this was disputed? When and where was the dispute?&lt;/p&gt;
    &lt;p&gt;As noted above, I've seen unquantified NSA/GCHQ fearmongering about costs, but that was outside IETF. If NSA and GCHQ tried the same arguments on a public mailing list then they'd end up being faced with questions that they can't answer.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was also argued that having an RFC gives too much promotion or sense of approval to a not recommended algorithm.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When I wrote my own summary of the objections, I provided a quote and link for each point. The "area director" doesn't do this. If the "area director" is accurately presenting an argument that was raised, why not provide a quote and a link? Is the "area director" misrepresenting the argument? Making up a strawman? The reader can't tell.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have expanded some of the arguments and my interpretation of the weight of these below.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This comment about "weight" is revealing. What we'll see again and again is that the "area director" is expressing the weight that he places on each argument (within the arguments selected and phrased by the "area director"), i.e., the extent to which he is convinced or not convinced by those arguments.&lt;/p&gt;
    &lt;p&gt;Given that IESG has power under IETF rules to unilaterally block publications approved by WGs, it's unsurprising that the "area directors", in their roles as IESG members, will end up evaluating the merits of WG-approved documents. But that isn't what this "area director" was instructed to do here. There isn't a WG-approved document at this point. Instead the "area director" was instructed to evaluate whether the chairs "appropriately" called "rough consensus" to "adopt" the document. The "area director" is supposed to be evaluating procedurally what the WG decision-makers did. Instead the "area director" is putting his thumb on the scale in favor of the document.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Non-hybrid as "basic flaw"&lt;/p&gt;
      &lt;p&gt;The argument by some opponents that non-hybrids are a "basic flaw" seems to miscategorize what a "basic flaw" is. There is currently no known "basic flaw" against MLKEM.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think that the "area director" is trying to make some sort of claim here about ML-KEM not having been attacked, but the wording is so unclear as to be unevaluatable. Why doesn't KyberSlash count? How about Clangover? How about the continuing advances in lattice attacks that have already reduced ML-KEM below its claimed security targets, the most recent news being from last month?&lt;/p&gt;
    &lt;p&gt;More importantly, claiming that ML-KEM isn't "known" to have problems is utterly failing to address the point of the ECC seatbelt. It's like saying "This car hasn't crashed, so the absence of seatbelts isn't a basic flaw".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As was raised, it is rather odd to be arguing we must immediately move to use post-quantum algorithms while at the same time argue these might contain fundamental basic flaws.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here the "area director" is reasonably capturing a statement from one document proponent (original wording: "I find it to be cognitive dissonance to simultaneously argue that the quantum threat requires immediate work, and yet we are also somehow uncertain of if the algorithms are totally broken. Both cannot be true at the same time").&lt;/p&gt;
    &lt;p&gt;But I promptly followed up explaining the error: "Rolling out PQ is trying to reduce the damage from an attacker having a quantum computer within the security lifetime of the user data. Doing that as ECC+PQ instead of just PQ is trying to reduce the damage in case the PQ part is broken. These actions are compatible, so how exactly do you believe they're contradictory?"&lt;/p&gt;
    &lt;p&gt;There was, of course, no reply at the time. The "area director" now simply repeats the erroneous argument.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As TLS (or IETF) is not phasing out all non-hybrid classics,&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"Non-hybrid classics" is weird terminology. Sometimes pre-quantum algorithms (ECC, RSA, etc.) are called "classical", so I guess the claim here is that using just ECC in TLS isn't being phased out. That's a bizarre claim. There are intensive efforts to roll out ECC+PQ in TLS to try to protect against quantum computers. Cloudflare reports the usage of post-quantum cryptography having risen to about 50% of all browsers that it sees (compared to 20% a year ago); within those connections, 95% use ECC+MLKEM768 and 5% use ECC+Kyber768.&lt;/p&gt;
    &lt;p&gt;The "area director" also gives no explanation of why the "not phasing out" claim is supposed to be relevant here.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I find this argument not strong enough&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;See how the "area director" is saying the weight that the "area director" places on each argument (within the arguments selected and phrased by the "area director"), rather than evaluating whether there was consensus to adopt the document?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;to override the consensus of allowing non-hybrid standards from being defined&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Circular argument. There wasn't consensus to adopt the document in the first place.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;especially in light of the strong consensus for marking these as "not recommended".&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I think many readers will be baffled by this comment. If something is "not recommended", wouldn't that be an argument against standardizing it, rather than an argument for standardizing it?&lt;/p&gt;
    &lt;p&gt;The answer is that "not recommended" doesn't mean what you think it means: the "area director" is resorting to confusing jargon. I don't think there's any point getting into the weeds on this.&lt;/p&gt;
    &lt;p&gt;Incompetent planning for the future.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Non-hybrids are a future end goal&lt;/p&gt;
      &lt;p&gt;Additionally, since if/when we do end up in an era with a CRQC, we are ultimately designing for a world where the classic components offer less to no value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If someone is trying to argue for removing ECC, there's a big difference between the plausible scenario of ECC having "less" value and the extreme scenario of ECC having "no" value. It's wrong for the "area director" to be conflating these possibilities.&lt;/p&gt;
    &lt;p&gt;As I put it almost two years ago: "Concretely, think about a demo showing that spending a billion dollars on quantum computation can break a thousand X25519 keys. Yikes! We should be aiming for much higher security than that! We don't even want a billion-dollar attack to be able to break one key! Users who care about the security of their data will be happy that we deployed post-quantum cryptography. But are the users going to say 'Let's turn off X25519 and make each session a million dollars cheaper to attack'? I'm skeptical. I think users will need to see much cheaper attacks before agreeing that X25519 has negligible security value."&lt;/p&gt;
    &lt;p&gt;Furthermore, let's think for a moment about the idea that one will eventually want to transition to just ML-KEM, the specific proposal that the "area director" is portraying as the future. Here are three ways that this can easily be wrong:&lt;/p&gt;
    &lt;p&gt;Maybe ML-KEM's implementation issues end up convincing the community to shift to a more robust option, analogously to what happened with ECC.&lt;/p&gt;
    &lt;p&gt;Maybe the advances in public attacks continue to the point of breaking ML-KEM outright.&lt;/p&gt;
    &lt;p&gt;Maybe the cliff stops crumbling and ML-KEM survives, but more efficient options also survive. At this point there are quite a few options more efficient than ML-KEM. (Random example: SMAUG. The current SMAUG software isn't as fast as the ML-KEM software, but this is outweighed by SMAUG using less network traffic than ML-KEM.) Probably some options will be broken, but ML-KEM would have to be remarkably lucky to end up as the most efficient remaining option.&lt;/p&gt;
    &lt;p&gt;Does this "area director" think that all of the more efficient options are going to be broken, while ML-KEM won't? Sounds absurdly overconfident. More likely is that the "area director" doesn't even realize that there are more efficient options. For anyone thinking "presumably those newer options have received less scrutiny than ML-KEM": we're talking about what to do long-term, remember?&lt;/p&gt;
    &lt;p&gt;Taking ML-KEM as the PQ component of ECC+PQ is working for getting something rolled out now. Hopefully ML-KEM will turn out to not be a security disaster (or a patent disaster). But, for guessing what will be best to do in 5 or 10 or 15 years, picking ML-KEM is premature.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When and where to exactly draw the line of still using a classic component safeguard is speculation at best.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here the "area director" is clearly attacking a strawman.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Already supporting pure post quantum algorithms now to gain experience&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How is rolling out PQ supposed to be gaining experience that isn't gained from the current rollout of ECC+PQ?&lt;/p&gt;
    &lt;p&gt;Also, I think it's important to call out the word "pure" here as incoherent, indefensible marketing. What we're actually talking about isn't modifying ML-KEM in any way; it's simply hashing the ML-KEM session key together with other inputs. Is ML-KEM no longer "pure" when it's plugged into TLS, which also hashes session keys? (The word "pure" also showed up in a few of the earlier quotes.)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;while not recommending it at this time seems a valid strategy for the future, allowing people and organizations their own timeline of deciding when/if to go from hybrid to pure PQ.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here we again see the area director making a decision to support the document, rather than evaluating whether there was consensus in the WG to adopt the document.&lt;/p&gt;
    &lt;p&gt;Again getting the complexity evaluation backwards.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Added complexity of hybrids&lt;/p&gt;
      &lt;p&gt;There was some discussion on whether or not hybrids add more complexity, and thus add risk, compared to non-hybrids. While arguments were made that proper classic algorithms add only a trivial amount of extra resources, it was also pointed out that there is a cost of implementation, deployment and maintenance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here the "area director" is again making the same mistake explained earlier: ignoring the fact that ECC+PQ is already there, and thus getting the complexity evaluation backwards.&lt;/p&gt;
    &lt;p&gt;The "thus add risk" logic is also wrong. Again, all of these options are more complex than the null cipher.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Additionally, the existence of draft-ietf-tls-hybrid-design and the extensive discussions around "chempat" vs "xwing" vs "kitchensink" shows that there is at least some complexity that is added by the hybrid solutions.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;No, the details of how to combine ECC with PQ in TLS are already settled and deployed.&lt;/p&gt;
    &lt;p&gt;Looking beyond TLS: Chempat hashes the transcript (similarly to TLS), making it robust for a wide range of protocols. The other options add fragility by hashing less for the sake of minor cost savings. Each of these options is under 10 lines of code. The "area director" exaggerates the complexity by mentioning "extensive discussions", and spends much more effort hyping this complexity as a risk than acknowledging the risks of further PQ attacks.&lt;/p&gt;
    &lt;p&gt;Anyway, it's not as if the presence of this document has eliminated the discussions of ECC+PQ details, nor is there any credible mechanism by which it could do so. Again, the actual choice at hand is whether to have PQ as an option alongside ECC+PQ. Adding that option adds complexity. The "area director" is getting the complexity comparison backwards by instead comparing (1) PQ in isolation to (2) ECC+PQ in isolation.&lt;/p&gt;
    &lt;p&gt;Botching the evaluation of human factors.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;RFCs being interpreted as IETF recommendation&lt;/p&gt;
      &lt;p&gt;It seems there is disagreement about whether the existence of an RFC itself qualifies as the IETF defacto "recommending" this in the view of IETF outsiders/ implemeners whom do not take into account any IANA registry RECOMMENDED setting or the Mandatory-To-Implement (MTI) reommendations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I would expect a purchasing manager to have instructions along the lines of "Buy only products complying with the standards", and to never see IETF's confusing jumble of further designations.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is an area where we recently found out there is little consensus on an IETF wide crypto policy statement via an RFC. The decision on whether an RFC adds value to a Code Point should therefor be taken independently of any such notion of how outsiders might interpret the existence of an RFC.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;From a security perspective, it's a big mistake to ignore the human factor, such as the impact of a purchasing manager saying "This is the most efficient standard so I'll pick that".&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In this case, while Section 3 could be considered informative, I believe Section 4 and Section 5 are useful (normative) content that assists implementers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Is this supposed to have something to do with the consensus question?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;And people have proposed extending the Security Considerations to more clearly state that this algorithm is not recommended at this point in time. Without an RFC, these recommendations cannot be published by the IETF in a way that implementers would be known to consume.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ah, yes, "known to consume"! There was, um, one of those, uh, studies showing the details of, um, how implementors use RFCs, which, uh, showed that 100% of the implementors diligently consumed the warnings in the RFCs. Yeah, that's the ticket. I'm sure the URL for this study is sitting around here somewhere.&lt;/p&gt;
    &lt;p&gt;Let's get back to the real world. Even if an implementor does see a "This document is a bad idea" warning, this simply doesn't matter when the implementors are chasing contracts issued by purchasing managers who simply care what's standardized and haven't seen the warning.&lt;/p&gt;
    &lt;p&gt;It's much smarter for the document to (1) eliminate making the proposal that it's warning about and (2) focus, starting in the title, on saying why such proposals are bad. This makes people more likely to see the warning, and at the same time it removes the core problem of the bad proposal being standardized.&lt;/p&gt;
    &lt;p&gt;Fictions regarding country actions.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Say no to Nation State algorithms&lt;/p&gt;
      &lt;p&gt;The history and birth of MLKEM from Kyber through a competition of the international Cryptographic Community, organized through US NIST can hardly be called or compared to unilateral dictated nation state algorithm selection.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;NIST repeatedly refused to designate the "NIST Post-Quantum Cryptography Standardization Process" as a "competition". It even wrote that the process "should not be treated as a competition".&lt;/p&gt;
    &lt;p&gt;Certainly there were competition-like aspects to the process. I tend to refer to it as a competition. But in the end the selection of algorithms to standardize was made by NIST, with input behind the scenes from NSA.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There has been no other comparable public effort to gather cryptographers and publicly discuss post-quantum crypto candidates in a multi-years effort.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Nonsense. The premier multi-year effort by cryptographers to "publicly discuss post-quantum crypto candidates" is the cryptographic literature.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In fact, other nation states are heavily relying on the results produced by this competition.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here's the objection from Stephen Farrell that the "area director" isn't quoting or linking to: "I don't see what criteria we might use in adopting this that wouldn't leave the WG open to accusations of favouritism if we don't adopt other pure PQ national standards that will certainly arise".&lt;/p&gt;
    &lt;p&gt;After reading this objection, you can see how the "area director" is sort of responding to it by suggesting that everybody is following NIST (i.e., that the "certainly arise" part is wrong).&lt;/p&gt;
    &lt;p&gt;But that's not true. NIST's selections are controversial. For example, ISO is considering not just ML-KEM but also&lt;/p&gt;
    &lt;p&gt;Classic McEliece, where NIST has said it's waiting for ISO ("After the ISO standardization process has been completed, NIST may consider developing a standard for Classic McEliece based on the ISO standard"), and&lt;/p&gt;
    &lt;p&gt;FrodoKEM, which NIST said "will not be considered further for standardization".&lt;/p&gt;
    &lt;p&gt;ISO is also now considering NTRU, where the advertisement includes "All patents related to NTRU have expired" (very different from the ML-KEM situation).&lt;/p&gt;
    &lt;p&gt;BSI, which sets cryptographic standards for Germany, recommends not just ML-KEM but also FrodoKEM (which it describes as "more conservative" than ML-KEM) and Classic McEliece ("conservative and very thoroughly analysed"). Meanwhile China has called for submissions of new post-quantum proposals for standardization.&lt;/p&gt;
    &lt;p&gt;I could keep going, but this is enough evidence to show that Farrell's prediction was correct; the "area director" is once again wrong.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The use of MLKEM in the IETF will not set a precedent for having to accept other nation state cryptography.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notice how the "area director" is dodging Farrell's point. If NSA can pressure the TLS WG into standardizing non-hybrid ML-KEM, why can't China pressure the TLS WG into standardizing something China wants? What criteria will IETF use to answer this question without leaving the WG "open to accusations of favouritism"? If you want people to believe that it isn't about the money then you need a really convincing alternative story.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Not recommending pure PQ right now&lt;/p&gt;
      &lt;p&gt;There was a strong consensus that pure PQ should not be recommended at this time, which is reflected in the document. There was some discussion on RECOMMENDED N vs D, which is something that can be discussed in the WG during the document's lifecycle before WGLC. It was further argued that adopting and publishing this document gives the WG control over the accompanying warning text, such as Security Considerations, that can reflect the current consensus of not recommending pure MLKEM over hybrid at publication time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is just rehashing earlier text, even if the detailed wording is a bit different.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Conclusion&lt;/p&gt;
      &lt;p&gt;The pure MLKEM code points exist.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Irrelevant. The question is whether they're being standardized.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An international market segment that wants to use pure MLKEM exists&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;"International"? Like Swedish company Ericsson setting up its "Ericsson Federal Technologies Group" in 2024 to receive U.S. military contracts?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;as can be seen by the consensus call outcome&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Um, how?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;along with existing implementations of the draft on mainstream devices and software.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, NSA waving around money has convinced some corporations to provide software. How is this supposed to justify the claim that "there was rough consensus to adopt the document"?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There is a rough consensus to adopt the document&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Repeating a claim doesn't make it true.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;with a strong consensus for RECOMMENDED N and not MTI, which is reflected in the draft.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Irrelevant. What matters is whether the document is standardized.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The reasons to not publish MLKEM as an RFC seem more based on personal opinions of risk and trust not shared amongst all participants as facts.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This sort of dismissal might be more convincing if it were coming from someone providing more URLs and fewer easily debunked claims. But it's in any case not addressing the consensus question.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Based on the above, I believe the WG Chairs made the correct call that there was rough consensus for adopting draft-connolly-tls-mlkem-key-agreement&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The chairs claimed that "we have consensus to adopt this draft" (based on claiming that "there were enough people willing to review the draft", never mind the number of objections). That claim is wrong. The call for adoption failed to reach consensus.&lt;/p&gt;
    &lt;p&gt;The "area director" claimed that "There is clearly consensus based on the 67 responses to the adoption call. ... The vast majority was in favour of adoption ... There were a few dissenting opinions". These statements still haven't been retracted; they were and are outright lies about what happened. Again, the actual tallies were 20 people unequivocally supporting adoption, 2 people conditionally supporting adoption, and 7 people unequivocally opposing adoption.&lt;/p&gt;
    &lt;p&gt;Without admitting error, the "area director" has retreated to a claim of "rough consensus". The mishmash of ad-hoc comments from the "area director" certainly doesn't demonstrate any coherent meaning of "rough consensus".&lt;/p&gt;
    &lt;p&gt;It's fascinating that IETF's advertising to the public claims that IETF's "decision-making requires achieving broad consensus", but IETF's WG procedures allow controversial documents to be pushed through on the basis of "rough consensus". To be clear, that's only if the "area director" approves of the documents, as you can see from the same "area director" issuing yet another mishmash of ad-hoc comments to overturn a separate chair decision in September 2025.&lt;/p&gt;
    &lt;p&gt;You would think that the WG procedures would define "rough consensus". They don't. All they say is that "51% of the working group does not qualify as 'rough consensus' and 99% is better than rough", not even making clear whether 51% of voters within a larger working group can qualify. This leaves a vast range of ambiguous intermediate cases up to the people in power.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46033151</guid><pubDate>Mon, 24 Nov 2025 12:00:55 +0000</pubDate></item><item><title>Bureau of Meteorology asked to examine $96.5M bill for website redesign</title><link>https://www.abc.net.au/news/2025-11-23/bureau-of-meteorology-new-website-cost-blowout-to-96-million/106042202</link><description>&lt;doc fingerprint="fb07803039b1571d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bureau of Meteorology's new boss asked to examine $96.5m bill for website redesign&lt;/head&gt;
    &lt;head rend="h2"&gt;In short:&lt;/head&gt;
    &lt;p&gt;The Bureau of Meteorology revealed its new website cost $96.5 million, not $4.1 million as the agency had originally said.&lt;/p&gt;
    &lt;p&gt;The weather agency said the website needed a "complete rebuild" to meet "modern security, usability and accessibility requirements".&lt;/p&gt;
    &lt;p&gt;The federal minister for the environment has asked the bureau's new boss to "to get on top" of the cost and functionality.&lt;/p&gt;
    &lt;p&gt;The Bureau of Meteorology's (BOM) flawed and expensive redesigned website will come under renewed scrutiny, with the federal environment minister asking the agency's new boss to closely examine how it all went so wrong, and report back to him.&lt;/p&gt;
    &lt;p&gt;It comes amid revelations that the new website cost more than $96 million to design — a far cry from the $4 million figure it originally claimed had been spent.&lt;/p&gt;
    &lt;p&gt;The national weather agency was flooded with complaints by the public after the website was launched a month ago.&lt;/p&gt;
    &lt;p&gt;Users found it difficult to navigate, and also criticised the changes to the radar map, which made place names hard to read.&lt;/p&gt;
    &lt;p&gt;Farmers were scathing, as they were unable to locate rainfall data.&lt;/p&gt;
    &lt;p&gt;The federal government was forced to intervene, ordering the agency to fix the website.&lt;/p&gt;
    &lt;p&gt;The site has since reverted to the old version of the radar map and other tweaks have been made to the site, with further changes to be rolled out.&lt;/p&gt;
    &lt;p&gt;In a statement provided to the ABC, the BOM admitted "the total cost of the website is approximately $96.5 million".&lt;/p&gt;
    &lt;head rend="h2"&gt;'Complete rebuild necessary'&lt;/head&gt;
    &lt;p&gt;It said the cost breakdown included $4.1 million for the redesign, $79.8 million for the website build, and the site's launch and security testing cost $12.6 million.&lt;/p&gt;
    &lt;p&gt;"A complete rebuild was necessary to ensure the website meets modern security, usability and accessibility requirements for the millions of Australians who reply on it every day," a spokesperson said.&lt;/p&gt;
    &lt;p&gt;The spokesperson also said it had "continued to listen to and analyse community feedback" since the launch of the new website on October 22.&lt;/p&gt;
    &lt;p&gt;Nine days after the launch it changed the radar map back to what it had previously been.&lt;/p&gt;
    &lt;quote&gt;"This brought back the visual style that the community said they found intuitive and reliable for interpreting weather conditions,"a spokesperson said.&lt;/quote&gt;
    &lt;p&gt;"This option was already available on the new site but not as the default setting when visiting the page.&lt;/p&gt;
    &lt;p&gt;"On 7 November we implemented changes to help the community find important fire behaviour index information."&lt;/p&gt;
    &lt;p&gt;Future changes were also in the pipeline in response to community feedback, according to the spokesperson, but some updates had been paused due to Severe Tropical Cyclone Fina in northern Australia.&lt;/p&gt;
    &lt;head rend="h2"&gt;Minister's expectations 'have been made very clear'&lt;/head&gt;
    &lt;p&gt;Environment Minister Murray Watt said he had met twice in the past week with the new CEO Stuart Minchin to reiterate his concerns about the bungled process and the cost.&lt;/p&gt;
    &lt;p&gt;He has asked Mr Minchin to report back to him on the issue.&lt;/p&gt;
    &lt;p&gt;"I don't think it's secret that I haven't been happy with the way the BOM has handled the transition to the new website," he told reporters on Sunday.&lt;/p&gt;
    &lt;p&gt;"I met with him on his first day and during the week just gone, to outline again that I think the BOM hasn't met public expectations, both in terms of the performance of the website and the cost of the website.&lt;/p&gt;
    &lt;p&gt;"So I've asked him as his first priority to make sure that he can get on top of the issues with the website — the functionality — and I'm pleased to see they've made changes.&lt;/p&gt;
    &lt;p&gt;"But I've also asked him to get on top of how we got to this position with this cost, with the problems.&lt;/p&gt;
    &lt;p&gt;"He's only been in the job for a week but I think my expectations have been made very clear."&lt;/p&gt;
    &lt;p&gt;However the minister stopped short of describing the website as a sheer waste of money, saying he would wait to hear back from Mr Minchin before commenting.&lt;/p&gt;
    &lt;p&gt;"Before leaping to judgement, I want to see what the new CEO of the BOM has been able to establish as to the reasons for those cost increases and I'll make my judgement at that point in time."&lt;/p&gt;
    &lt;head rend="h2"&gt;'Another Labor disaster'&lt;/head&gt;
    &lt;p&gt;Nationals leader David Littleproud said there should be "consequences" after the revelations about the true cost of the website.&lt;/p&gt;
    &lt;p&gt;"It is unbelievable a private consultancy was paid $78 million to redesign the website," Mr Littleproud said.&lt;/p&gt;
    &lt;p&gt;"But then security and system testing meant that Australian taxpayers actually paid $96 million for what was nothing more than another Labor disaster,.&lt;/p&gt;
    &lt;p&gt;"The seriousness of this cannot be understated. This isn't just about a clunky website, the changes actually put lives and safety at risk.&lt;/p&gt;
    &lt;p&gt;"The new platform did not allow people to enter GPS coordinates for their specific property locations, restricting searches to towns or postcodes.&lt;/p&gt;
    &lt;p&gt;"Families and farmers could not access vital, localised data such as river heights and rainfall information and this missing data created panic and fear across communities.&lt;/p&gt;
    &lt;p&gt;"But now, the fact the BOM has been hiding the true cost of its white elephant and initially lying about the total figure is deeply concerning, considering that the BOM should be all about trust."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46033435</guid><pubDate>Mon, 24 Nov 2025 12:35:15 +0000</pubDate></item><item><title>Trade Chaos Causes Businesses to Rethink Their Relationship with the U.S.</title><link>https://www.nytimes.com/2025/11/24/business/tariffs-trade-small-business.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46033536</guid><pubDate>Mon, 24 Nov 2025 12:47:38 +0000</pubDate></item></channel></rss>