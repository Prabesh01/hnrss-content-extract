<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 05 Sep 2025 14:38:18 +0000</lastBuildDate><item><title>Classic 8×8-pixel B&amp;W Mac patterns</title><link>https://www.pauladamsmith.com/blog/2025/09/classic-mac-patterns.html</link><description>&lt;doc fingerprint="3600717075e6a4ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Classic 8Ã8-pixel B&amp;amp;W Mac patterns&lt;/head&gt;
    &lt;p&gt;TL;DR: I made a website for the original classic Mac patterns&lt;/p&gt;
    &lt;p&gt;I was working on something and thought it would be fun to use one of the classic Mac black-and-white patterns in the project. I'm talking about the original 8Ã8-pixel ones that were in the original Control Panel for setting the desktop background and in MacPaint as fill patterns.&lt;/p&gt;
    &lt;p&gt;Screenshots via to Marcin's awesome interactive history&lt;/p&gt;
    &lt;p&gt;I figured there'd must be clean, pixel-perfect GIFs or PNGs of them somewhere on the web. And perhaps there are, but after poking around a bit, I ran out of energy for that, but by then had a head of steam for extracting the patterns en masse from the original source, somehow. Then I could produce whatever format I needed for them.&lt;/p&gt;
    &lt;p&gt;There are 38 patterns, introduced in the original System 1.0 in the 1984 debut of the Macintosh. They were unchanged in later versions, so I decided to get them from a System 6 disk, since that's a little easier with access to utility programs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preparation&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download Mini vMac.&lt;/item&gt;
      &lt;item&gt;Acquire "old world" Mac ROMs.&lt;/item&gt;
      &lt;item&gt;Download a System 6 startup disk image.&lt;/item&gt;
      &lt;item&gt;Download ExportFl disk image.&lt;/item&gt;
      &lt;item&gt;Download sitPack disk image.&lt;/item&gt;
      &lt;item&gt;Install "The Unarchiver" (&lt;code&gt;brew install --cask the-unarchiver&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Install the Xcode command-line tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Extraction process&lt;/head&gt;
    &lt;p&gt;Start System 6 (drag the ROM onto the Mini vMac icon, then drag the System 6 disk onto the window when you see the flashing floppy disk). Mount the ExportFl and sitPack disks by dragging their files and dropping on the classic Mac desktop.&lt;/p&gt;
    &lt;head rend="h3"&gt;In emulation&lt;/head&gt;
    &lt;p&gt;Double-click sitPack to launch the program. Command-O to open, then navigate to the startup disk by clicking "Drive". Scroll to find "System Folder" and double-click on it. Scroll to the bottom, select "System" and click "Open". Save the output file as "System.sit" in the top-level of the startup disk. Quit sitPack back to the Finder.&lt;/p&gt;
    &lt;p&gt;Start the ExportFl program. Command-O or pick "Open" from the "File" menu. Find the "System.sit" created in the last step and click "Open". A regular file save dialog will appear on the modern Mac, pick a location and save the file.&lt;/p&gt;
    &lt;head rend="h3"&gt;On the modern Mac&lt;/head&gt;
    &lt;p&gt;Drag the "System.sit" file onto The Unarchiver, or open the file from within it. This will produce a file called "System" (with no extension).&lt;/p&gt;
    &lt;p&gt;Run DeRez (part of the Xcode developer command-line tools) on the System file. I first added &lt;code&gt;/Library/Developer/CommandLineTools/usr/bin&lt;/code&gt; to my &lt;code&gt;$PATH&lt;/code&gt;, then
ran:&lt;/p&gt;
    &lt;code&gt;$ DeRez -only PAT\# System &amp;gt; patterns.r
&lt;/code&gt;
    &lt;p&gt;This produces a text representation of the &lt;code&gt;PAT#&lt;/code&gt; resource in the System file.
It's a series of bytes that comprise 38 8Ã8 patterns meant for QuickDraw
commands. There's a leading big-endian unsigned 16-bit number (&lt;code&gt;0026&lt;/code&gt;) to indicate the number of 8-byte patterns to follow.&lt;/p&gt;
    &lt;code&gt;data 'PAT#' (0, purgeable) {
	$"0026 FFFF FFFF FFFF FFFF DDFF 77FF DDFF"
	$"77FF DD77 DD77 DD77 DD77 AA55 AA55 AA55"
	$"AA55 55FF 55FF 55FF 55FF AAAA AAAA AAAA"
	$"AAAA EEDD BB77 EEDD BB77 8888 8888 8888"
	$"8888 B130 031B D8C0 0C8D 8010 0220 0108"
	$"4004 FF88 8888 FF88 8888 FF80 8080 FF08"
	$"0808 8000 0000 0000 0000 8040 2000 0204"
	$"0800 8244 3944 8201 0101 F874 2247 8F17"
	$"2271 55A0 4040 550A 0404 2050 8888 8888"
	$"0502 BF00 BFBF B0B0 B0B0 0000 0000 0000"
	$"0000 8000 0800 8000 0800 8800 2200 8800"
	$"2200 8822 8822 8822 8822 AA00 AA00 AA00"
	$"AA00 FF00 FF00 FF00 FF00 1122 4488 1122"
	$"4488 FF00 0000 FF00 0000 0102 0408 1020"
	$"4080 AA00 8000 8800 8000 FF80 8080 8080"
	$"8080 081C 22C1 8001 0204 8814 2241 8800"
	$"AA00 40A0 0000 040A 0000 0384 4830 0C02"
	$"0101 8080 413E 0808 14E3 1020 54AA FF02"
	$"0408 7789 8F8F 7798 F8F8 0008 142A 552A"
	$"1408"
};
&lt;/code&gt;
    &lt;p&gt;It would have been simple enough to parse this text, but I had Claude quickly make a Python program to do so and output them in .pbm format, which is part of the Netpbm image format class. This is a simple image format that is text-based, a '1' or a '0' indicating a black or white pixel in a row and column.&lt;/p&gt;
    &lt;p&gt;For example, this subway tile pattern is represented like this in .pbm:&lt;/p&gt;
    &lt;code&gt;P1
8 8
1 1 1 1 1 1 1 1
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 0 0 0 0 0 0 0
1 1 1 1 1 1 1 1
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
0 0 0 0 1 0 0 0
&lt;/code&gt;
    &lt;p&gt;From here, I can generate image files for the patterns in any format and resolution I want, using ImageMagick or similar. It's important when scaling the patterns to use &lt;code&gt;-filter point&lt;/code&gt;, so that ImageMagick doesn't try to interpolate
the pixels it needs to fill in, which would lead to blurry results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why do all this?&lt;/head&gt;
    &lt;p&gt;It's nostalgic, I have a fondness for these old patterns and the original B&amp;amp;W Mac aesthetic, it reminds me of playing games like Dark Castle and Glider, messing around with HyperCard, and using Tex-Edit and hoarding early shareware programs.&lt;/p&gt;
    &lt;p&gt;The whole point of the above is to get a copy of the System file out with the resource fork intact, that's where the desktop patterns live.&lt;/p&gt;
    &lt;p&gt;According to old classic Mac manuals, the patterns were QuickDraw bit-pattern resources, a simple bitmap of 8 bits per row packed into 8 bytes (columns). It was fast for QuickDraw to copy them over an area of the screen. For example the following pattern was used for the default gray desktop pattern on black-and-white Mac screens.&lt;/p&gt;
    &lt;p&gt;I could have extracted all 38 patterns other ways: I could have screenshotted each one, I could have looked at each one and hand-written .pbm files, both of which would have been tedious and error-prone.&lt;/p&gt;
    &lt;p&gt;Ultimately, I wanted to extract the exact original data from the source (or close enough copy thereof) and have the patterns in a format I considered archival for this limited purpose (.pbm files are trivial to parse and manipulate).&lt;/p&gt;
    &lt;p&gt;Head over to my pattern site to get the patterns for yourself.&lt;/p&gt;
    &lt;p&gt;(Credit for replica Geneva 9pt and Chicago 12pt fonts)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45131538</guid></item><item><title>What If OpenDocument Used SQLite?</title><link>https://www.sqlite.org/affcase1.html</link><description>&lt;doc fingerprint="f7cfb8164ca46ba1"&gt;
  &lt;main&gt;
    &lt;p&gt;Suppose the OpenDocument file format, and specifically the "ODP" OpenDocument Presentation format, were built around SQLite. Benefits would include:&lt;/p&gt;
    &lt;p&gt;Note that this is only a thought experiment. We are not suggesting that OpenDocument be changed. Nor is this article a criticism of the current OpenDocument design. The point of this essay is to suggest ways to improve future file format designs.&lt;/p&gt;
    &lt;p&gt;The OpenDocument file format is used for office applications: word processors, spreadsheets, and presentations. It was originally designed for the OpenOffice suite but has since been incorporated into other desktop application suites. The OpenOffice application has been forked and renamed a few times. This author's primary use for OpenDocument is building slide presentations with either NeoOffice on Mac, or LibreOffice on Linux and Windows.&lt;/p&gt;
    &lt;p&gt;An OpenDocument Presentation or "ODP" file is a ZIP archive containing XML files describing presentation slides and separate image files for the various images that are included as part of the presentation. (OpenDocument word processor and spreadsheet files are similarly structured but are not considered by this article.) The reader can easily see the content of an ODP file by using the "zip -l" command. For example, the following is the "zip -l" output from a 49-slide presentation about SQLite from the 2014 SouthEast LinuxFest conference:&lt;/p&gt;
    &lt;quote&gt;Archive: self2014.odp Length Date Time Name --------- ---------- ----- ---- 47 2014-06-21 12:34 mimetype 0 2014-06-21 12:34 Configurations2/statusbar/ 0 2014-06-21 12:34 Configurations2/accelerator/current.xml 0 2014-06-21 12:34 Configurations2/floater/ 0 2014-06-21 12:34 Configurations2/popupmenu/ 0 2014-06-21 12:34 Configurations2/progressbar/ 0 2014-06-21 12:34 Configurations2/menubar/ 0 2014-06-21 12:34 Configurations2/toolbar/ 0 2014-06-21 12:34 Configurations2/images/Bitmaps/ 54702 2014-06-21 12:34 Pictures/10000000000001F40000018C595A5A3D.png 46269 2014-06-21 12:34 Pictures/100000000000012C000000A8ED96BFD9.png ... 58 other pictures omitted... 13013 2014-06-21 12:34 Pictures/10000000000000EE0000004765E03BA8.png 1005059 2014-06-21 12:34 Pictures/10000000000004760000034223EACEFD.png 211831 2014-06-21 12:34 content.xml 46169 2014-06-21 12:34 styles.xml 1001 2014-06-21 12:34 meta.xml 9291 2014-06-21 12:34 Thumbnails/thumbnail.png 38705 2014-06-21 12:34 Thumbnails/thumbnail.pdf 9664 2014-06-21 12:34 settings.xml 9704 2014-06-21 12:34 META-INF/manifest.xml --------- ------- 10961006 78 files&lt;/quote&gt;
    &lt;p&gt;The ODP ZIP archive contains four different XML files: content.xml, styles.xml, meta.xml, and settings.xml. Those four files define the slide layout, text content, and styling. This particular presentation contains 62 images, ranging from full-screen pictures to tiny icons, each stored as a separate file in the Pictures folder. The "mimetype" file contains a single line of text that says:&lt;/p&gt;
    &lt;quote&gt;application/vnd.oasis.opendocument.presentation&lt;/quote&gt;
    &lt;p&gt;The purpose of the other files and folders is presently unknown to the author but is probably not difficult to figure out.&lt;/p&gt;
    &lt;p&gt;The use of a ZIP archive to encapsulate XML files plus resources is an elegant approach to an application file format. It is clearly superior to a custom binary file format. But using an SQLite database as the container, instead of ZIP, would be more elegant still.&lt;/p&gt;
    &lt;p&gt;A ZIP archive is basically a key/value database, optimized for the case of write-once/read-many and for a relatively small number of distinct keys (a few hundred to a few thousand) each with a large BLOB as its value. A ZIP archive can be viewed as a "pile-of-files" database. This works, but it has some shortcomings relative to an SQLite database, as follows:&lt;/p&gt;
    &lt;p&gt;Incremental update is hard.&lt;/p&gt;
    &lt;p&gt;It is difficult to update individual entries in a ZIP archive. It is especially difficult to update individual entries in a ZIP archive in a way that does not destroy the entire document if the computer loses power and/or crashes in the middle of the update. It is not impossible to do this, but it is sufficiently difficult that nobody actually does it. Instead, whenever the user selects "File/Save", the entire ZIP archive is rewritten. Hence, "File/Save" takes longer than it ought, especially on older hardware. Newer machines are faster, but it is still bothersome that changing a single character in a 50 megabyte presentation causes one to burn through 50 megabytes of the finite write life on the SSD.&lt;/p&gt;
    &lt;p&gt;Startup is slow.&lt;/p&gt;
    &lt;p&gt;In keeping with the pile-of-files theme, OpenDocument stores all slide content in a single big XML file named "content.xml". LibreOffice reads and parses this entire file just to display the first slide. LibreOffice also seems to read all images into memory as well, which makes sense seeing as when the user does "File/Save" it is going to have to write them all back out again, even though none of them changed. The net effect is that start-up is slow. Double-clicking an OpenDocument file brings up a progress bar rather than the first slide. This results in a bad user experience. The situation grows ever more annoying as the document size increases.&lt;/p&gt;
    &lt;p&gt;More memory is required.&lt;/p&gt;
    &lt;p&gt;Because ZIP archives are optimized for storing big chunks of content, they encourage a style of programming where the entire document is read into memory at startup, all editing occurs in memory, then the entire document is written to disk during "File/Save". OpenOffice and its descendants embrace that pattern.&lt;/p&gt;
    &lt;p&gt;One might argue that it is ok, in this era of multi-gigabyte desktops, to read the entire document into memory. But it is not ok. For one, the amount of memory used far exceeds the (compressed) file size on disk. So a 50MB presentation might take 200MB or more RAM. That still is not a problem if one only edits a single document at a time. But when working on a talk, this author will typically have 10 or 15 different presentations up all at the same time (to facilitate copy/paste of slides from past presentations) and so gigabytes of memory are required. Add in an open web browser or two and a few other desktop apps, and suddenly the disk is whirling and the machine is swapping. And even having just a single document is a problem when working on an inexpensive Chromebook retrofitted with Ubuntu. Using less memory is always better.&lt;/p&gt;
    &lt;p&gt;Crash recovery is difficult.&lt;/p&gt;
    &lt;p&gt;The descendants of OpenOffice tend to segfault more often than commercial competitors. Perhaps for this reason, the OpenOffice forks make periodic backups of their in-memory documents so that users do not lose all pending edits when the inevitable application crash does occur. This causes frustrating pauses in the application for the few seconds while each backup is being made. After restarting from a crash, the user is presented with a dialog box that walks them through the recovery process. Managing the crash recovery this way involves lots of extra application logic and is generally an annoyance to the user.&lt;/p&gt;
    &lt;p&gt;Content is inaccessible.&lt;/p&gt;
    &lt;p&gt;One cannot easily view, change, or extract the content of an OpenDocument presentation using generic tools. The only reasonable way to view or edit an OpenDocument document is to open it up using an application that is specifically designed to read or write OpenDocument (read: LibreOffice or one of its cousins). The situation could be worse. One can extract and view individual images (say) from a presentation using just the "zip" archiver tool. But it is not reasonable try to extract the text from a slide. Remember that all content is stored in a single "context.xml" file. That file is XML, so it is a text file. But it is not a text file that can be managed with an ordinary text editor. For the example presentation above, the content.xml file consist of exactly two lines. The first line of the file is just:&lt;/p&gt;
    &lt;quote&gt;&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;&lt;/quote&gt;
    &lt;p&gt;The second line of the file contains 211792 characters of impenetrable XML. Yes, 211792 characters all on one line. This file is a good stress-test for a text editor. Thankfully, the file is not some obscure binary format, but in terms of accessibility, it might as well be written in Klingon.&lt;/p&gt;
    &lt;p&gt;Let us suppose that instead of using a ZIP archive to store its files, OpenDocument used a very simple SQLite database with the following single-table schema:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE OpenDocTree( filename TEXT PRIMARY KEY, -- Name of file filesize BIGINT, -- Size of file after decompression content BLOB -- Compressed file content );&lt;/quote&gt;
    &lt;p&gt;For this first experiment, nothing else about the file format is changed. The OpenDocument is still a pile-of-files, only now each file is a row in an SQLite database rather than an entry in a ZIP archive. This simple change does not use the power of a relational database. Even so, this simple change shows some improvements.&lt;/p&gt;
    &lt;p&gt;Surprisingly, using SQLite in place of ZIP makes the presentation file smaller. Really. One would think that a relational database file would be larger than a ZIP archive, but at least in the case of NeoOffice that is not so. The following is an actual screen-scrape showing the sizes of the same NeoOffice presentation, both in its original ZIP archive format as generated by NeoOffice (self2014.odp), and as repacked as an SQLite database using the SQLAR utility:&lt;/p&gt;
    &lt;quote&gt;-rw-r--r-- 1 drh staff 10514994 Jun 8 14:32 self2014.odp -rw-r--r-- 1 drh staff 10464256 Jun 8 14:37 self2014.sqlar -rw-r--r-- 1 drh staff 10416644 Jun 8 14:40 zip.odp&lt;/quote&gt;
    &lt;p&gt;The SQLite database file ("self2014.sqlar") is about a half percent smaller than the equivalent ODP file! How can this be? Apparently the ZIP archive generator logic in NeoOffice is not as efficient as it could be, because when the same pile-of-files is recompressed using the command-line "zip" utility, one gets a file ("zip.odp") that is smaller still, by another half percent, as seen in the third line above. So, a well-written ZIP archive can be slightly smaller than the equivalent SQLite database, as one would expect. But the difference is slight. The key take-away is that an SQLite database is size-competitive with a ZIP archive.&lt;/p&gt;
    &lt;p&gt;The other advantage to using SQLite in place of ZIP is that the document can now be updated incrementally, without risk of corrupting the document if a power loss or other crash occurs in the middle of the update. (Remember that writes to SQLite databases are atomic.) True, all the content is still kept in a single big XML file ("content.xml") which must be completely rewritten if so much as a single character changes. But with SQLite, only that one file needs to change. The other 77 files in the repository can remain unaltered. They do not all have to be rewritten, which in turn makes "File/Save" run much faster and saves wear on SSDs.&lt;/p&gt;
    &lt;p&gt;A pile-of-files encourages content to be stored in a few large chunks. In the case of ODP, there are just four XML files that define the layout of all slides in a presentation. An SQLite database allows storing information in a few large chunks, but SQLite is also adept and efficient at storing information in numerous smaller pieces.&lt;/p&gt;
    &lt;p&gt;So then, instead of storing all content for all slides in a single oversized XML file ("content.xml"), suppose there was a separate table for storing the content of each slide separately. The table schema might look something like this:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE slide( pageNumber INTEGER, -- The slide page number slideContent TEXT -- Slide content as XML or JSON ); CREATE INDEX slide_pgnum ON slide(pageNumber); -- Optional&lt;/quote&gt;
    &lt;p&gt;The content of each slide could still be stored as compressed XML. But now each page is stored separately. So when opening a new document, the application could simply run:&lt;/p&gt;
    &lt;quote&gt;SELECT slideContent FROM slide WHERE pageNumber=1;&lt;/quote&gt;
    &lt;p&gt;This query will quickly and efficiently return the content of the first slide, which could then be speedily parsed and displayed to the user. Only one page needs to be read and parsed in order to render the first screen, which means that the first screen appears much faster and there is no longer a need for an annoying progress bar.&lt;/p&gt;
    &lt;p&gt;If the application wanted to keep all content in memory, it could continue reading and parsing the other pages using a background thread after drawing the first page. Or, since reading from SQLite is so efficient, the application might instead choose to reduce its memory footprint and only keep a single slide in memory at a time. Or maybe it keeps the current slide and the next slide in memory, to facilitate rapid transitions to the next slide.&lt;/p&gt;
    &lt;p&gt;Notice that dividing up the content into smaller pieces using an SQLite table gives flexibility to the implementation. The application can choose to read all content into memory at startup. Or it can read just a few pages into memory and keep the rest on disk. Or it can read just a single page into memory at a time. And different versions of the application can make different choices without having to make any changes to the file format. Such options are not available when all content is in a single big XML file in a ZIP archive.&lt;/p&gt;
    &lt;p&gt;Splitting content into smaller pieces also helps File/Save operations to go faster. Instead of having to write back the content of all pages when doing a File/Save, the application only has to write back those pages that have actually changed.&lt;/p&gt;
    &lt;p&gt;One minor downside of splitting content into smaller pieces is that compression does not work as well on shorter texts and so the size of the document might increase. But as the bulk of the document space is used to store images, a small reduction in the compression efficiency of the text content will hardly be noticeable, and is a small price to pay for an improved user experience.&lt;/p&gt;
    &lt;p&gt;Once one is comfortable with the concept of storing each slide separately, it is a small step to support versioning of the presentation. Consider the following schema:&lt;/p&gt;
    &lt;quote&gt;CREATE TABLE slide( slideId INTEGER PRIMARY KEY, derivedFrom INTEGER REFERENCES slide, content TEXT -- XML or JSON or whatever ); CREATE TABLE version( versionId INTEGER PRIMARY KEY, priorVersion INTEGER REFERENCES version, checkinTime DATETIME, -- When this version was saved comment TEXT, -- Description of this version manifest TEXT -- List of integer slideIds );&lt;/quote&gt;
    &lt;p&gt;In this schema, instead of each slide having a page number that determines its order within the presentation, each slide has a unique integer identifier that is unrelated to where it occurs in sequence. The order of slides in the presentation is determined by a list of slideIds, stored as a text string in the MANIFEST column of the VERSION table. Since multiple entries are allowed in the VERSION table, that means that multiple presentations can be stored in the same document.&lt;/p&gt;
    &lt;p&gt;On startup, the application first decides which version it wants to display. Since the versionId will naturally increase in time and one would normally want to see the latest version, an appropriate query might be:&lt;/p&gt;
    &lt;quote&gt;SELECT manifest, versionId FROM version ORDER BY versionId DESC LIMIT 1;&lt;/quote&gt;
    &lt;p&gt;Or perhaps the application would rather use the most recent checkinTime:&lt;/p&gt;
    &lt;quote&gt;SELECT manifest, versionId, max(checkinTime) FROM version;&lt;/quote&gt;
    &lt;p&gt;Using a single query such as the above, the application obtains a list of the slideIds for all slides in the presentation. The application then queries for the content of the first slide, and parses and displays that content, as before.&lt;/p&gt;
    &lt;p&gt;(Aside: Yes, that second query above that uses "max(checkinTime)" really does work and really does return a well-defined answer in SQLite. Such a query either returns an undefined answer or generates an error in many other SQL database engines, but in SQLite it does what you would expect: it returns the manifest and versionId of the entry that has the maximum checkinTime.)&lt;/p&gt;
    &lt;p&gt;When the user does a "File/Save", instead of overwriting the modified slides, the application can now make new entries in the SLIDE table for just those slides that have been added or altered. Then it creates a new entry in the VERSION table containing the revised manifest.&lt;/p&gt;
    &lt;p&gt;The VERSION table shown above has columns to record a check-in comment (presumably supplied by the user) and the time and date at which the File/Save action occurred. It also records the parent version to record the history of changes. Perhaps the manifest could be stored as a delta from the parent version, though typically the manifest will be small enough that storing a delta might be more trouble than it is worth. The SLIDE table also contains a derivedFrom column which could be used for delta encoding if it is determined that saving the slide content as a delta from its previous version is a worthwhile optimization.&lt;/p&gt;
    &lt;p&gt;So with this simple change, the ODP file now stores not just the most recent edit to the presentation, but a history of all historic edits. The user would normally want to see just the most recent edition of the presentation, but if desired, the user can now go backwards in time to see historical versions of the same presentation.&lt;/p&gt;
    &lt;p&gt;Or, multiple presentations could be stored within the same document.&lt;/p&gt;
    &lt;p&gt;With such a schema, the application would no longer need to make periodic backups of the unsaved changes to a separate file to avoid lost work in the event of a crash. Instead, a special "pending" version could be allocated and unsaved changes could be written into the pending version. Because only changes would need to be written, not the entire document, saving the pending changes would only involve writing a few kilobytes of content, not multiple megabytes, and would take milliseconds instead of seconds, and so it could be done frequently and silently in the background. Then when a crash occurs and the user reboots, all (or almost all) of their work is retained. If the user decides to discard unsaved changes, they simply go back to the previous version.&lt;/p&gt;
    &lt;p&gt;There are details to fill in here. Perhaps a screen can be provided that displays all historical changes (perhaps with a graph) allowing the user to select which version they want to view or edit. Perhaps some facility can be provided to merge forks that might occur in the version history. And perhaps the application should provide a means to purge old and unwanted versions. The key point is that using an SQLite database to store the content, rather than a ZIP archive, makes all of these features much, much easier to implement, which increases the possibility that they will eventually get implemented.&lt;/p&gt;
    &lt;p&gt;In the previous sections, we have seen how moving from a key/value store implemented as a ZIP archive to a simple SQLite database with just three tables can add significant capabilities to an application file format. We could continue to enhance the schema with new tables, with indexes added for performance, with triggers and views for programming convenience, and constraints to enforce consistency of content even in the face of programming errors. Further enhancement ideas include:&lt;/p&gt;
    &lt;p&gt;An SQLite database has a lot of capability, which this essay has only begun to touch upon. But hopefully this quick glimpse has convinced some readers that using an SQL database as an application file format is worth a second look.&lt;/p&gt;
    &lt;p&gt;Some readers might resist using SQLite as an application file format due to prior exposure to enterprise SQL databases and the caveats and limitations of those other systems. For example, many enterprise database engines advise against storing large strings or BLOBs in the database and instead suggest that large strings and BLOBs be stored as separate files and the filename stored in the database. But SQLite is not like that. Any column of an SQLite database can hold a string or BLOB up to about a gigabyte in size. And for strings and BLOBs of 100 kilobytes or less, I/O performance is better than using separate files.&lt;/p&gt;
    &lt;p&gt;Some readers might be reluctant to consider SQLite as an application file format because they have been inculcated with the idea that all SQL database schemas must be factored into Third Normal Form (3NF) and store only small primitive data types such as strings and integers. Certainly relational theory is important and designers should strive to understand it. But, as demonstrated above, it is often quite acceptable to store complex information as XML or JSON in text fields of a database. Do what works, not what your database professor said you ought to do.&lt;/p&gt;
    &lt;p&gt;In summary, the claim of this essay is that using SQLite as a container for an application file format like OpenDocument and storing lots of smaller objects in that container works out much better than using a ZIP archive holding a few larger objects. To wit:&lt;/p&gt;
    &lt;p&gt;An SQLite database file is approximately the same size, and in some cases smaller, than a ZIP archive holding the same information.&lt;/p&gt;
    &lt;p&gt;The atomic update capabilities of SQLite allow small incremental changes to be safely written into the document. This reduces total disk I/O and improves File/Save performance, enhancing the user experience.&lt;/p&gt;
    &lt;p&gt;Startup time is reduced by allowing the application to read in only the content shown for the initial screen. This largely eliminates the need to show a progress bar when opening a new document. The document just pops up immediately, further enhancing the user experience.&lt;/p&gt;
    &lt;p&gt;The memory footprint of the application can be dramatically reduced by only loading content that is relevant to the current display and keeping the bulk of the content on disk. The fast query capability of SQLite make this a viable alternative to keeping all content in memory at all times. And when applications use less memory, it makes the entire computer more responsive, further enhancing the user experience.&lt;/p&gt;
    &lt;p&gt;The schema of an SQL database is able to represent information more directly and succinctly than a key/value database such as a ZIP archive. This makes the document content more accessible to third-party applications and scripts and facilitates advanced features such as built-in document versioning, and incremental saving of work in progress for recovery after a crash.&lt;/p&gt;
    &lt;p&gt;These are just a few of the benefits of using SQLite as an application file format — the benefits that seem most likely to improve the user experience for applications like OpenOffice. Other applications might benefit from SQLite in different ways. See the Application File Format document for additional ideas.&lt;/p&gt;
    &lt;p&gt;Finally, let us reiterate that this essay is a thought experiment. The OpenDocument format is well-established and already well-designed. Nobody really believes that OpenDocument should be changed to use SQLite as its container instead of ZIP. Nor is this article a criticism of OpenDocument for not choosing SQLite as its container since OpenDocument predates SQLite. Rather, the point of this article is to use OpenDocument as a concrete example of how SQLite can be used to build better application file formats for future projects.&lt;/p&gt;
    &lt;p&gt;This page last modified on 2025-05-12 11:56:41 UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45132498</guid></item><item><title>io_uring is faster than mmap</title><link>https://www.bitflux.ai/blog/memory-is-slow-part2/</link><description>&lt;doc fingerprint="eeb8d08f7c322988"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Sourcing data directly from disk IS faster than caching in memory. I brought receipts. Because hardware got wider but not faster, the old methods don't get you there. You need new tools to use what is scaling and avoid what isn't.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In part 1 I showed how some computer performance factors are scaling exponentially while others have been stagnant for decades. I then asserted, without proof, that sourcing data from disk can be faster than from memory. What follows is the proof.&lt;/p&gt;
    &lt;p&gt;Computer Science dogma says that unused memory should be used to cache things from the filesystem because the disk is slow and memory is fast. Given that disk bandwidth is growing exponentially and memory access latency has stagnated this isn't always true anymore.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experimental set up&lt;/head&gt;
    &lt;p&gt;We need data and something straight forward to do with the data. I used my free will or the illusion thereof to create a benchmark I cleverly call "counting 10s". I write some pseudo random integers between 0 and 20 to a buffer and then count how many of the integers are 10. I want to make sure we are doing all the counting in a single thread to simulate an Amdahl's Law situation.&lt;/p&gt;
    &lt;p&gt;So how fast can we expect this to run? The upper limit would be the memory bandwidth.&lt;/p&gt;
    &lt;p&gt;My testing rig is a server with an old AMD EPYC 7551P 32-Core Processor on a Supermicro H11SSL-i and 96GB of DDR4 2133 MHz and a couple of 1.92TB Samsung PM983a PCIe 3.0 SSDs I pieced together from EBay parts. Given the way this server is configured, the upper limit for memory bandwidth can be calculated as 3 channels * 2133MT/s * 8B/T / 4 numa domains = ~13GB/s for a single thread. It's kind of an odd system but that just makes it more fun to optimize for!&lt;/p&gt;
    &lt;p&gt;The disks are rated at 3.1GB/s read BW each for an upper limit of 6.2GB/s. I made a raid0 volume with 4KB stripe size, formatted the the raid as ext4 with no journaling, and made sure it fully finished initializing the metadata before running the tests.&lt;/p&gt;
    &lt;code&gt;sudo mdadm --create /dev/md0 --level=0 --raid-devices=2 --chunk=4K /dev/nvme1n1 /dev/nvme2n1
sudo mkfs.ext4 -F -L data -O ^has_journal -E lazy_itable_init=0 /dev/md0
sudo mount -o noatime /dev/md0 mnt
&lt;/code&gt;
    &lt;p&gt;We'll use a 50GB dataset for most benchmarking here, because when I started this I thought the test system only had 64GB and it stuck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple Loop&lt;/head&gt;
    &lt;p&gt;The simple and cleanest way to do this in C would look like this.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;

// count_10_loop
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    int* data = (int*)mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    for (size_t i = 0; i &amp;lt; total_ints; ++i) {
        if (data[i] == 10) count++;
    }

    printf("Found %ld 10s\n", count);
}
&lt;/code&gt;
    &lt;p&gt;Just mmap() the file which will give us a buffer that we can read from. Then we just loop and count the 10s.&lt;/p&gt;
    &lt;p&gt;Because the point is to benchmark we will integrate some timing mechanisms before we move on.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;sys/time.h&amp;gt;

long get_time_us() {
    struct timeval tv;
    gettimeofday(&amp;amp;tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

// count_10_loop
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    int* data = (int*)mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    long start = get_time_us();
    for (size_t i = 0; i &amp;lt; total_ints; ++i) {
        if (data[i] == 10) count++;
    }
    long elapsed = get_time_us() - start;

    printf("simple loop found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}
&lt;/code&gt;
    &lt;p&gt;For the first run we're going to be reading from the disk. The disk/filesystem read is going to limit the performance before the memory bandwidth can.&lt;/p&gt;
    &lt;code&gt;â¯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
simple loop found 167802249 10s processed at 0.61 GB/s
&lt;/code&gt;
    &lt;p&gt;As expected, it's not anywhere near memory speeds because as everyone knows, disk is slow. We can look at the system and confirm that the first run cached the data to memory.&lt;/p&gt;
    &lt;p&gt;Our expectation is that the second run will be faster because the data is already in memory and as everyone knows, memory is fast.&lt;/p&gt;
    &lt;code&gt;â¯ sudo  ./count_10_loop ./mnt/datafile.bin 53687091200
simple loop found 167802249 10s processed at 3.71 GB/s
&lt;/code&gt;
    &lt;p&gt;It is faster, but clearly thatâs slower than the memory can feed it to the processor. What bottleneck might we be hitting? This speed does look possibly correlated to the instructions per second limit for this generation of CPU (between 2GHz * 1.5 IPC = 3G and 3GHz boost * 1.5 IPC = 4.5G instructions per second).&lt;/p&gt;
    &lt;p&gt;We can use perf to see if the CPU is using vector instructions, if not then the actual compute is the bottleneck.&lt;/p&gt;
    &lt;code&gt;Percentâ      test     %rbp,%rbp
       â    â je       84
       â      lea      (%rbx,%rbp,4),%rcx
       â      mov      %rbx,%rax
       â      xor      %ebp,%ebp
       â      nop
       â70:   xor      %edx,%edx
  1.31 â      cmpl     $0xa,(%rax)
 42.38 â      sete     %dl
 45.72 â      add      $0x4,%rax
  0.01 â      add      %rdx,%rbp
 10.42 â      cmp      %rax,%rcx
  0.16 â    â jne      70
       â84:   xor      %eax,%eax
       â      shr      $0x14,%r12
       â    â call     get_time_us
       â      pxor     %xmm0,%xmm0
       â      pxor     %xmm1,%xmm1
&lt;/code&gt;
    &lt;p&gt;Confirmed. We're running non-vectorized instructions, with a single thread counting that's as fast as it can go with a 2GHz CPU. Well crap. Weâve hit our first non-exponential limit. Even a brand new CPU running this machine code would probably struggle to do much better than a 50% improvement, still well below the memory bandwidth limit.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unrolling the loop&lt;/head&gt;
    &lt;p&gt;Good news is this code can definitely be vectorized if we help the compiler. Unroll the loop!&lt;/p&gt;
    &lt;p&gt;We're gonna make it very obvious to the compiler that it's safe to use vector instructions which could process our integers up to 8x faster.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;stdint.h&amp;gt;
#include &amp;lt;sys/time.h&amp;gt;

long get_time_us() {
    struct timeval tv;
    gettimeofday(&amp;amp;tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

// count_10_unrolled
int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY);
    void* buffer = mmap(NULL, size_bytes, PROT_READ, MAP_SHARED, fd, 0);
 
    // Get the compiler to align the buffer
    const int * __restrict data = (const int * __restrict)__builtin_assume_aligned(buffer, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    long start = get_time_us();
    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i &amp;lt; total_ints; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(data[i+ 0] == 10);
        c1  += (unsigned)(data[i+ 1] == 10);
        c2  += (unsigned)(data[i+ 2] == 10);
        c3  += (unsigned)(data[i+ 3] == 10);
        c4  += (unsigned)(data[i+ 4] == 10);
        c5  += (unsigned)(data[i+ 5] == 10);
        c6  += (unsigned)(data[i+ 6] == 10);
        c7  += (unsigned)(data[i+ 7] == 10);
        c8  += (unsigned)(data[i+ 8] == 10);
        c9  += (unsigned)(data[i+ 9] == 10);
        c10 += (unsigned)(data[i+10] == 10);
        c11 += (unsigned)(data[i+11] == 10);
        c12 += (unsigned)(data[i+12] == 10);
        c13 += (unsigned)(data[i+13] == 10);
        c14 += (unsigned)(data[i+14] == 10);
        c15 += (unsigned)(data[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    count = (t0 + t1) + (t2 + t3);
    long elapsed = get_time_us() - start;

    printf("unrolled loop found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}
&lt;/code&gt;
    &lt;p&gt;Check if we now have vectorized instructions with &lt;code&gt;perf&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;Percentâ       movq      %xmm0,%rcx
       â       movdqa    %xmm7,%xmm14
       â       pxor      %xmm0,%xmm0
       â       nop
       â e8:   movdqa    %xmm6,%xmm4
  0.30 â       movdqa    %xmm6,%xmm3
  0.12 â       movdqa    %xmm6,%xmm2
  0.35 â       add       $0x1,%rdx
  1.54 â       pcmpeqd   (%rax),%xmm4
 54.64 â       pcmpeqd   0x10(%rax),%xmm3
  1.62 â       movdqa    %xmm6,%xmm1
  0.99 â       add       $0x40,%rax
  0.12 â       pcmpeqd   -0x20(%rax),%xmm2
  3.03 â       pcmpeqd   -0x10(%rax),%xmm1
  1.32 â       pand      %xmm5,%xmm4
  1.25 â       pand      %xmm5,%xmm3
  1.55 â       movdqa    %xmm4,%xmm15
  0.24 â       punpckhdq %xmm0,%xmm4

&lt;/code&gt;
    &lt;p&gt;Confirmed. We're using 128bit vector instructions, this should be up to 4x faster than the original.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;NOTE: These are 128-bit vector instructions, but I expected 256-bit. I dug deeper here and found claims that Gen1 EPYC had unoptimized 256-bit instructions. I forced the compiler to use 256-bit instructions and found it was actually slower. Looks like the compiler was smart enough to know that here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Let's benchmark this unrolled version with the data as page cache in memory.&lt;/p&gt;
    &lt;code&gt;â¯ sudo  ./count_10_unrolled ./mnt/datafile.bin 53687091200
unrolled loop found 167802249 10s processed at 5.51 GB/s
&lt;/code&gt;
    &lt;p&gt;We're still nowhere close to hitting the memory bus speed limit of 13GB/s but 50% faster than the original is a win. There must be some other bottleneck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can the SSDs beat that?&lt;/head&gt;
    &lt;p&gt;5.51GB/s? On paper the SSDs can read at 6.2GB/s, but the first run from disk only did 0.61GB/s. How can I meet or beat this performance sourcing the data directly from disk?&lt;/p&gt;
    &lt;p&gt;Consider how the default mmap() mechanism works, it is a background IO pipeline to transparently fetch the data from disk. When you read the empty buffer from userspace it triggers a fault, the kernel handles the fault by reading the data from the filesystem, which then queues up IO from disk. Unfortunately these legacy mechanisms just aren't set up for serious high performance IO. Note that at 610MB/s it's faster than what a disk SATA can do. On the other hand, it only managed 10% of our disk's potential. Clearly we're going to have to do something else.&lt;/p&gt;
    &lt;p&gt;SSDs don't just automatically read data at multigigabyte speeds. You need to put some real effort into an IO pipeline to get serious performance.&lt;/p&gt;
    &lt;p&gt;I made a io_uring based IO engine, a kind of userspace driver, that can hit these speeds. The main thread will request data, the IO engine will handle the IO, then the main thread will do the counting when the data is in a buffer. We will use a set of queues to manage the IO requests, responses, and buffers. The IO engine will start 6 workers, target a queue depth of 8192, and have a buffer size of 16KB.&lt;/p&gt;
    &lt;p&gt;I wish I had tighter code here, but A) I didnât have time to clean it up B) some of the complexity is intractable. The IO engine code was a lot to scroll through so I moved it to github link&lt;/p&gt;
    &lt;code&gt;#include "io_engine.h"
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;getopt.h&amp;gt;
#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;sys/mman.h&amp;gt;
#include &amp;lt;stdint.h&amp;gt;
#include &amp;lt;sys/time.h&amp;gt;

#define DEFAULT_WORKERS 6
#define DEFAULT_BLOCK_SIZE 16384
#define DEFAULT_QUEUE_DEPTH 8192

// Count the number of "10" (int format) in the buffer
static inline size_t count_tens_unrolled(void* data, size_t size_bytes) {
    const size_t total = size_bytes / sizeof(int);
    // Get the compiler to align the buffer
    const int * __restrict p = (const int * __restrict)__builtin_assume_aligned(data, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i &amp;lt; total; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(p[i+ 0] == 10);
        c1  += (unsigned)(p[i+ 1] == 10);
        c2  += (unsigned)(p[i+ 2] == 10);
        c3  += (unsigned)(p[i+ 3] == 10);
        c4  += (unsigned)(p[i+ 4] == 10);
        c5  += (unsigned)(p[i+ 5] == 10);
        c6  += (unsigned)(p[i+ 6] == 10);
        c7  += (unsigned)(p[i+ 7] == 10);
        c8  += (unsigned)(p[i+ 8] == 10);
        c9  += (unsigned)(p[i+ 9] == 10);
        c10 += (unsigned)(p[i+10] == 10);
        c11 += (unsigned)(p[i+11] == 10);
        c12 += (unsigned)(p[i+12] == 10);
        c13 += (unsigned)(p[i+13] == 10);
        c14 += (unsigned)(p[i+14] == 10);
        c15 += (unsigned)(p[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    return (t0 + t1) + (t2 + t3);
}

int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);

    // Set up the io engine
    ioengine_t* na = ioengine_alloc(filename, size_bytes, DEFAULT_QUEUE_DEPTH, DEFAULT_BLOCK_SIZE, DEFAULT_WORKERS);

    sleep(1);

    // Use the background workers to read file directly
    size_t total_blocks = na-&amp;gt;file_size / na-&amp;gt;block_size;
    uint64_t uid = 1;
    size_t count = 0;

    long start = get_time_us();

    // Read all blocks
    size_t blocks_queued = 0;
    size_t blocks_read = 0;
    int buffer_queued = 0;
    while (blocks_read &amp;lt; total_blocks) {
        //// Queue IO phase //////
        //     Do we have more blocks to queue up?
        if (buffer_queued &amp;lt; na-&amp;gt;num_io_buffers/2 &amp;amp;&amp;amp; blocks_queued &amp;lt;= total_blocks) {
            // Calculate how many blocks on average we want our workers to queue up
            size_t free_buffers = (size_t)(na-&amp;gt;num_io_buffers - buffer_queued - 4); // hold back a few buffers
            size_t blocks_remaining = total_blocks - blocks_queued;  // how many blocks have we not queued
            size_t blocks_to_queue = free_buffers &amp;gt; blocks_remaining ? blocks_remaining : free_buffers;
            int blocks_to_queue_per_worker = (int) (blocks_to_queue + na-&amp;gt;num_workers - 1) / na-&amp;gt;num_workers;
            // Iterate through workers and assign work
            for (int i = 0; i &amp;lt; na-&amp;gt;num_workers; i++) {
                worker_thread_data_t* worker = &amp;amp;na-&amp;gt;workers[i];
                // Try to queue N blocks to this worker
                for (int j = 0; j &amp;lt; blocks_to_queue_per_worker; j++) {
                    if (blocks_queued == total_blocks) break;
                    int bgio_tail = worker-&amp;gt;bgio_tail;
                    int bgio_head = worker-&amp;gt;bgio_head;
                    int bgio_next = (bgio_tail + 1) % worker-&amp;gt;num_max_bgio;
                    int next_bhead = (worker-&amp;gt;buffer_head + 1) % worker-&amp;gt;num_max_bgio;
                    if (bgio_next == bgio_head) break;  // queue for send requests is full
                    if (next_bhead == worker-&amp;gt;buffer_tail) break; // queue for recieving completed IO is full
                    // Queue this block with the worker.  We have to track which buffer it's going to.
                    int buffer_idx = worker-&amp;gt;buffer_start_idx + worker-&amp;gt;buffer_head;
                    na-&amp;gt;buffer_state[buffer_idx] = BUFFER_PREFETCHING;
                    worker-&amp;gt;bgio_uids[bgio_tail] = (uid++)&amp;lt;&amp;lt;16; // unique id helps track IOs in io_uring, we encode 4 bytes later
                    worker-&amp;gt;bgio_buffer_idx[bgio_tail] = buffer_idx;
                    worker-&amp;gt;bgio_block_idx[bgio_tail] = blocks_queued++;  // block sized index into file
                    worker-&amp;gt;bgio_queued[bgio_tail] = -1;  // Requested but not yet queued
                    int next_tail = (bgio_tail + 1) % worker-&amp;gt;num_max_bgio;
                    worker-&amp;gt;bgio_tail = next_tail;
                    // Log the buffer in an ordered queue for us to read
                    worker-&amp;gt;complete_ring[worker-&amp;gt;buffer_head] = buffer_idx;
                    worker-&amp;gt;buffer_head = next_bhead;
                    buffer_queued++;
                }
                // Tell the worker to submit IOs as a group
                worker-&amp;gt;bgio_submit++;
            }
        }

        //// Completion Phase //////
        //     Iterate through worker and check if they have complete IOs
        for (int i = 0; i &amp;lt; na-&amp;gt;num_workers; i++) {
            worker_thread_data_t* worker = &amp;amp;na-&amp;gt;workers[i];
            int current = worker-&amp;gt;buffer_tail;
            // We know what IO's we're waiting on, but we have to poll
            //  to see if they are done.
            for (int scan = 0; scan &amp;lt; worker-&amp;gt;num_max_bgio; scan++) {
                // Scan until we get to the end of the list
                if (current == worker-&amp;gt;buffer_head) break;
                int buffer_idx = worker-&amp;gt;complete_ring[current];
                int state = na-&amp;gt;buffer_state[buffer_idx];
                if (state == BUFFER_PREFETCHED) {
                    // This buffer is completed - Process this buffer.
                    count += count_tens_unrolled(na-&amp;gt;io_buffers[buffer_idx], na-&amp;gt;block_size);
                    na-&amp;gt;buffer_state[buffer_idx] = BUFFER_UNUSED;
                    blocks_read++;
                    buffer_queued--;
                }
                current = (current + 1) % worker-&amp;gt;num_max_bgio;
            }
            // IO's might have been completed out of order, advance the tail when we can
            current = worker-&amp;gt;buffer_tail;
            while (current != worker-&amp;gt;buffer_head) {
                int buffer_idx = worker-&amp;gt;complete_ring[current];
                int state = na-&amp;gt;buffer_state[buffer_idx];
                if (state != BUFFER_UNUSED) break;
                current = (current + 1) % worker-&amp;gt;num_max_bgio;
            }
            worker-&amp;gt;buffer_tail = current;
            worker-&amp;gt;bgio_submit++;  // probably unnecessary
        }
    }
    long elapsed = get_time_us() - start;
    printf("diskbased found %ld 10s processed at %0.2f GB/s\n", count, (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));

    // Cleanup I/O system
    ioengine_free(na);

    return 0;
}
&lt;/code&gt;
    &lt;p&gt;I hope all this extra code makes it faster.&lt;/p&gt;
    &lt;code&gt;â¯ sudo ./diskbased/benchmark ./mnt/datafile.bin 53687091200
diskbased found 167802249 10s processed at 5.81 GB/s
&lt;/code&gt;
    &lt;p&gt;Boom! Disk is faster than memory! It takes several hundred lines of code but now we can source the data from my SSDs faster than the copy from the page cache in memory.&lt;/p&gt;
    &lt;head rend="h2"&gt;So what's going on here?&lt;/head&gt;
    &lt;p&gt;Of course my 6GB/s disk stripe isnât actually faster than the memory bus, even on this weird hack of a system. So what is happening? Where is the bottleneck? It's got to be the way the data is being read from the page cache in memory.&lt;/p&gt;
    &lt;p&gt;What if we replace the mmap() with a read() from disk into a preallocated buffer. That way we can measure the counting with the data in-memory without any page cache related overhead mmap() can introduce.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;stdlib.h&amp;gt;
#include &amp;lt;sys/time.h&amp;gt;
#include &amp;lt;sys/stat.h&amp;gt;
#include &amp;lt;fcntl.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;
#include &amp;lt;stdint.h&amp;gt;
#include &amp;lt;string.h&amp;gt;

long get_time_us() {
    struct timeval tv;
    gettimeofday(&amp;amp;tv, NULL);
    return tv.tv_sec * 1000000L + tv.tv_usec;
}

int main(int argc, char *argv[]) {
    char* filename = argv[1];
    size_t size_bytes = strtoull(argv[2], NULL, 10);
    size_t total_ints = size_bytes / sizeof(int);
    size_t count = 0;

    int fd = open(filename, O_RDONLY|O_DIRECT);
    void *buf;
    posix_memalign(&amp;amp;buf, 4096, size_bytes);
    int *data = buf;

    size_t off = 0;
    while (off &amp;lt; size_bytes) {
        ssize_t n = read(fd, (char*)data + off, size_bytes - off);
        off += (size_t)n;   // YOLO: assume n &amp;gt; 0 until done
    }

    long start = get_time_us();
    for (size_t i = 0; i &amp;lt; total_ints; ++i) {
        if (data[i] == 10) count++;
    }
    long elapsed = get_time_us() - start;

    printf("simple loop %ld 10s processed at %0.2f GB/s\n",
           count,
           (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));


    // Get the compiler to align the buffer
    const int * __restrict p = (const int * __restrict)__builtin_assume_aligned((void*)data, 4096);
    uint64_t c0=0, c1=0, c2=0, c3=0,
            c4=0, c5=0, c6=0, c7=0,
            c8=0, c9=0, c10=0, c11=0,
            c12=0, c13=0, c14=0, c15=0;

    start = get_time_us();
    // Unrolling the compiler knows it can use a vector unit like AVX2 to process
    for (size_t i = 0; i &amp;lt; total_ints; i += 16) {
        // removed 'if' to get it to be branchless: each compares to 10, adds 0 or 1
        c0  += (unsigned)(p[i+ 0] == 10);
        c1  += (unsigned)(p[i+ 1] == 10);
        c2  += (unsigned)(p[i+ 2] == 10);
        c3  += (unsigned)(p[i+ 3] == 10);
        c4  += (unsigned)(p[i+ 4] == 10);
        c5  += (unsigned)(p[i+ 5] == 10);
        c6  += (unsigned)(p[i+ 6] == 10);
        c7  += (unsigned)(p[i+ 7] == 10);
        c8  += (unsigned)(p[i+ 8] == 10);
        c9  += (unsigned)(p[i+ 9] == 10);
        c10 += (unsigned)(p[i+10] == 10);
        c11 += (unsigned)(p[i+11] == 10);
        c12 += (unsigned)(p[i+12] == 10);
        c13 += (unsigned)(p[i+13] == 10);
        c14 += (unsigned)(p[i+14] == 10);
        c15 += (unsigned)(p[i+15] == 10);
    }

    // pairwise reduce to help some compilers schedule better
    uint64_t s0 = c0 + c1,   s1 = c2 + c3,   s2 = c4 + c5,   s3 = c6 + c7;
    uint64_t s4 = c8 + c9,   s5 = c10 + c11, s6 = c12 + c13, s7 = c14 + c15;
    uint64_t t0 = s0 + s1,   t1 = s2 + s3,   t2 = s4 + s5,   t3 = s6 + s7;

    count = (t0 + t1) + (t2 + t3);
    elapsed = get_time_us() - start;

    printf("unrolled loop %ld 10s processed at %0.2f GB/s\n",
           count,
           (double)(size_bytes/1073741824)/((double)elapsed/1.0e6));
}
&lt;/code&gt;
    &lt;p&gt;If we keep the dataset smaller than a numa domain and we bind this to a single numa node to prevent numa overheads we see that the theoretical memory bandwidth we projected seems to be the primary bottleneck for the unrolled loop as we hoped to see at the outset.&lt;/p&gt;
    &lt;code&gt;â¯  sudo numactl --cpunodebind=0   ./in_ram mnt/datafile.bin 2147483648
simple loop 6709835 10s processed at 4.76 GB/s
unrolled loop 6709835 10s processed at 13.04 GB/s
&lt;/code&gt;
    &lt;p&gt;But this isn't useful to compare the with the other runs with the 50GB dataset. However if we do the full 50GB dataset the performance suffers. We have to get much of the data across numa domains which is going to be higher cost.&lt;/p&gt;
    &lt;code&gt;â¯ sudo ./in_ram ./mnt/datafile.bin 53687091200
simple loop 167802249 10s processed at 3.76 GB/s
unrolled loop 167802249 10s processed at 7.90 GB/s
&lt;/code&gt;
    &lt;p&gt;Comparing the results of "fully in-memory (50GB)" which is pre-loaded in memory before measuring against the "unrolled loop" that is only cached in memory we see 40% overhead. That's 2.75 seconds out of 9 seconds that was spent waiting on the caching system instead of counting. Why so much?&lt;/p&gt;
    &lt;p&gt;mmap()&lt;/p&gt;
    &lt;p&gt;The mmap() call presents the process with a buffer that is a blank slate even when the data is already in memory. The buffer is populated page by page as it's accessed from the page cache. This isn't a copy, it's just the operating system mapping the cached memory into the process. This costs more than it might seem. The worst case with mmap() the counting has to pause at every 4KB page boundary while the kernel processes a fault, tracks down the page of data in the page cache, then updates the page table of the process to insert the memory into the process. Fundamentally this is a process that is limited by the memory latency, not the CPU speed or memory bandwidth. With the potential for TLB walks and searching lists that track the page cache, weâre taking potentially dozens of CPU cache misses and several microseconds of waiting on memory for every 4KB page.&lt;/p&gt;
    &lt;p&gt;direct IO&lt;/p&gt;
    &lt;p&gt;Using our direct from disk approach uses pipelines and streams which avoids the kind of memory latency dominated bottleneck that mmap() has. In our case we're limited by the bandwidth of our disks yet because of the pipelining, the larger latency of the IOs doesn't get in the critical path of the counting very much. Allowing for higher throughput.&lt;/p&gt;
    &lt;head rend="h2"&gt;Scaling&lt;/head&gt;
    &lt;p&gt;Consider the implications of these experiments as we scale. The well vetted solution to get data from memory to a process is slower than using the disk directly. This isn't because the memory is slower than the disk. The memory has higher bandwidth than the disk, not by an order of magnitude, but a decent margin. But the latency of the memory is orders of magnitude lower than the disk. Nevertheless the way the data in memory is accessed is the culprit. Its a synchronous approach that assumes memory operations are cheap and low latency. These accesses add up and it ends up waiting on memory latencies. The disk method on the other hand is as a streaming approach built to leverage bandwidth and hide latencies.&lt;/p&gt;
    &lt;p&gt;extending the existing rig&lt;/p&gt;
    &lt;p&gt;If I got a few more of these disks I could push the IO bandwidth to be greater than the 13GB/s per thread memory bandwidth limit. IO is DMA'ed to buffers that are pretty small compared to the total dataset. These buffers scale with the throughput capabilities of the CPU and the disks, not the dataset size. The buffers can be located in a single numa domain allowing us to avoid the overhead of accessing the buffers between NUMA domains. Add more disks to this system I might be able to create a disk based solution to count at the full 13GB/s rather than be limited to the 7.90GB/s we see with the in memory example at the full 50GB dataset. With such a system our throughput would not be affected by the dataset size, unlike the in-memory case, which has numa overhead and eventually runs out of memory to scale.&lt;/p&gt;
    &lt;p&gt;faster than memory is possible&lt;/p&gt;
    &lt;p&gt;On a proper modern server the CPUs will let you do IO directly to the L3 cache, bypassing memory altogether. Because PCIe bandwidth is higher than memory bandwidth, on paper we could even get more max bandwidth than we can get from memory if we carefully pin the buffers into the CPU cache. I haven't confirm this works in practice, however, it could be made to work and is the sort of thing that CPU designs will be forced to lean into to push performance forward.&lt;/p&gt;
    &lt;p&gt;memory is changing too&lt;/p&gt;
    &lt;p&gt;This isn't just about disks vs memory. Similar techniques and principles apply to memory. Memory bandwidth is still scaling even if the latency is not. This means to take advantage of memory performance you have to actually treat it more like a disk and less like Random Access Memory. To scale performance with generational updates you have to make sure to stream data from memory into the CPU caches in blocks, similar to how data is streamed from disk to memory. If not you end up with 90s level memory throughput. A custom mechanism to cache data in memory could easily avoid the memory latency problems seen with the default mmap() solution with much less code than the io_uring solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Is this worth it?&lt;/head&gt;
    &lt;p&gt;I'm not going to say that going to the effort of implementing something like this is always worth it. The mmap() method is sure elegant from a coding perspective, especially when compared to all the code I had to write to get the io_uring setup working. Sometimes the simple way is the way to go.&lt;/p&gt;
    &lt;p&gt;Is using 6 cores of IO for 1 core of compute is always the right answer? Probably not. This was an extreme situation to prove a point. In realworld situations you'll need to look at the tradeoffs and decide what's best for your use case. Correctly understanding the strengths and weaknesses of the hardware can open up a number of possibilities where you can get a lot more performance for a lot less money.&lt;/p&gt;
    &lt;p&gt;The kind of overhead demonstrated with mmap() isnât going to go away, new hardware isn't going to fix it. At the same time disk bandwidth and the number of cores are scaling each generation. But doing things that scale performance with new technology is going to take extra code and effort.&lt;/p&gt;
    &lt;p&gt;But don't just blow this stuff off. Sure you can dedicate a server with 3TB of memory to serve 10K client connections. Memory in the cloud is like ~$5/GB/month, if you can afford it, then you do you. However it is worth considering that humanity doesn't have the silicon fabs or the power plants to support this for every moron vibe coder out there making an app. I figure either the karmic debt to the planet, or a vengeful AI demigod hungry for silicon and electricity will come for those that don't heed this warning, eventually. Either way my conscience is clear.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recap&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory is slow - when you use it oldschool.&lt;/item&gt;
      &lt;item&gt;Disk is fast - when you are clever with it.&lt;/item&gt;
      &lt;item&gt;Test the dogma - compounded exponentials are flipping somethings from true to false.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bad news is that this cleverness requires extra code and effort.&lt;/p&gt;
    &lt;p&gt;Good news is we now have AI to write and test the extra code this cleverness requires.&lt;/p&gt;
    &lt;p&gt;Better news is that, for those that are willing to learn, AI's don't do this unless you know how to ask them.&lt;/p&gt;
    &lt;p&gt;Lean into things that scale, avoid things that donât.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Time&lt;/head&gt;
    &lt;p&gt;What will be revealed in the next episode?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is O(ân) actually faster than O(log n)? Will the foundations of Computer Science survive this unveiling?&lt;/item&gt;
      &lt;item&gt;Will traditional code be consumed into the latent space of our AI overlords?&lt;/item&gt;
      &lt;item&gt;Is AI hiding these performance gains from me? Is AI even capable of writing optimized code?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Jared Hulbert&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A few notes for the "um actually" haters commenting on Hacker News:&lt;/p&gt;
      &lt;item&gt;This is not and does not claim to be an academic paper.&lt;/item&gt;
      &lt;item&gt;I do not intend to prove that NAND is a drop in replacement for DRAM.&lt;/item&gt;
      &lt;item&gt;Tis but a humble and hopefully fun exercise in exploring the limits and trends of modern hardware and the tradeoffs needed to maximize performance.&lt;/item&gt;
      &lt;item&gt;As I stated before I have no problem with your choice to ignore this and write lazy code that will perform just as fast on new hardware in 15 years as it does on todays hardware. In fact I applaud your choice. Jeff Bezos has an orbital yacht to build, someone has to pay for it, why not you?&lt;/item&gt;
      &lt;item&gt;I am not an AI. I am a human with a computer that don't write perfect.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;source code can be found here.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45132710</guid></item><item><title>What Is the Fourier Transform?</title><link>https://www.quantamagazine.org/what-is-the-fourier-transform-20250903/</link><description>&lt;doc fingerprint="b00b98134830c362"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What Is the Fourier Transform?&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;As we listen to a piece of music, our ears perform a calculation. The high-pitched flutter of the flute, the middle tones of the violin, and the low hum of the double bass fill the air with pressure waves of many different frequencies. When the combined sound wave descends through the ear canal and into the spiral-shaped cochlea, hairs of different lengths resonate to the different pitches, separating the messy signal into buckets of elemental sounds.&lt;/p&gt;
    &lt;p&gt;It took mathematicians until the 19th century to master this same calculation.&lt;/p&gt;
    &lt;p&gt;In the early 1800s, the French mathematician Jean-Baptiste Joseph Fourier discovered a way to take any function and decompose it into a set of fundamental waves, or frequencies. Add these constituent frequencies back together, and you’ll get your original function. The technique, today called the Fourier transform, allowed the mathematician — previously an ardent proponent of the French revolution — to spur a mathematical revolution as well.&lt;/p&gt;
    &lt;p&gt;Out of the Fourier transform grew an entire field of mathematics, called harmonic analysis, which studies the components of functions. Soon enough, mathematicians began to discover deep connections between harmonic analysis and other areas of math and physics, from number theory to differential equations to quantum mechanics. You can also find the Fourier transform at work in your computer, allowing you to compress files, enhance audio signals and more.&lt;/p&gt;
    &lt;p&gt;“It’s hard to overestimate the influence of Fourier analysis in math,” said Leslie Greengard of New York University and the Flatiron Institute. “It touches almost every field of math and physics and chemistry and everything else.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Flames of Passion&lt;/head&gt;
    &lt;p&gt;Fourier was born in 1768 amid the chaos of prerevolutionary France. Orphaned at 10 years old, he was educated at a convent in his hometown of Auxerre. He spent the next decade conflicted about whether to dedicate his life to religion or to math, eventually abandoning his religious training and becoming a teacher. He also promoted revolutionary efforts in France until, during the Reign of Terror in 1794, the 26-year-old was arrested and imprisoned for expressing beliefs that were considered anti-revolutionary. He was slated for the guillotine.&lt;/p&gt;
    &lt;p&gt;Before he could be executed, the Terror came to an end. And so, in 1795, he returned to teaching mathematics. A few years later, he was appointed as a scientific adviser to Napoleon Bonaparte and joined his army during the invasion of Egypt. It was there that Fourier, while also pursuing research into Egyptian antiquities, began the work that would lead him to develop his transform: He wanted to understand the mathematics of heat conduction. By the time he returned to France in 1801 — shortly before the French army was driven out of Egypt, the stolen Rosetta stone surrendered to the British — Fourier could think of nothing else.&lt;/p&gt;
    &lt;p&gt;If you heat one side of a metal rod, the heat will spread until the whole rod has the same temperature. Fourier argued that the distribution of heat through the rod could be written as a sum of simple waves. As the metal cools, these waves lose energy, causing them to smooth out and eventually disappear. The waves that oscillate more quickly — meaning they have more energy — decay first, followed eventually by the lower frequencies. It’s like a symphony that ends with each instrument fading to silence, from piccolos to tubas.&lt;/p&gt;
    &lt;p&gt;The proposal was radical. When Fourier presented it at a meeting of the Paris Institute in 1807, the renowned mathematician Joseph-Louis Lagrange reportedly declared the work “nothing short of impossible.”&lt;/p&gt;
    &lt;p&gt;What troubled his peers most were strange cases where the heat distribution might be sharply irregular — like a rod that is exactly half cold and half hot. Fourier maintained that the sudden jump in temperature could still be described mathematically: It would just require adding infinitely many simpler curves instead of a finite number. But most mathematicians at the time believed that no number of smooth curves could ever add up to a sharp corner.&lt;/p&gt;
    &lt;p&gt;Today, we know that Fourier was broadly right.&lt;/p&gt;
    &lt;p&gt;“You can represent anything as a sum of these very, very simple oscillations,” said Charles Fefferman, a mathematician at Princeton University. “It’s known that if you have a whole lot of tuning forks, and you set them perfectly, they can produce Beethoven’s Ninth Symphony.” The process only fails for the most bizarre functions, like those that oscillate wildly no matter how much you zoom in on them.&lt;/p&gt;
    &lt;p&gt;So how does the Fourier transform work?&lt;/p&gt;
    &lt;head rend="h2"&gt;A Well-Trained Ear&lt;/head&gt;
    &lt;p&gt;Performing a Fourier transform is akin to sniffing a perfume and distinguishing its list of ingredients, or hearing a complex jazzy chord and distinguishing its constituent notes.&lt;/p&gt;
    &lt;p&gt;Mathematically, the Fourier transform is a function. It takes a given function — which can look complicated — as its input. It then produces as its output a set of frequencies. If you write down the simple sine and cosine waves that have these frequencies, and then add them together, you’ll get the original function.&lt;/p&gt;
    &lt;p&gt;To achieve this, the Fourier transform essentially scans all possible frequencies and determines how much each contributes to the original function. Let’s look at a simple example.&lt;/p&gt;
    &lt;p&gt;Consider the following function:&lt;/p&gt;
    &lt;p&gt;The Fourier transform checks how much each frequency contributes to this original function. It does so by multiplying waves together. Here’s what happens if we multiply the original by a sine wave with a frequency of 3:&lt;/p&gt;
    &lt;p&gt;There are lots of large peaks, which means the frequency 3 contributes to the original function. The average height of the peaks reveals how large the contribution is.&lt;/p&gt;
    &lt;p&gt;Now let’s test if the frequency 5 is present. Here’s what you get when you multiply the original function by a sine wave with the frequency 5:&lt;/p&gt;
    &lt;p&gt;There are some large peaks but also large valleys. The new graph averages out to around zero. This indicates that the frequency 5 does not contribute to the original function.&lt;/p&gt;
    &lt;p&gt;The Fourier transform does this for all possible frequencies, multiplying the original function by both sine and cosine waves. (In practice, it runs this comparison on the complex plane, using a combination of real and imaginary numbers.)&lt;/p&gt;
    &lt;p&gt;In this way, the Fourier transform can decompose a complicated-looking function into just a few numbers. This has made it a crucial tool for mathematicians: If they are stumped by a problem, they can try transforming it. Often, the problem becomes much simpler when translated into the language of frequencies.&lt;/p&gt;
    &lt;p&gt;If the original function has a sharp edge, like the square wave below (which is often found in digital signals), the Fourier transform will produce an infinite set of frequencies that, when added together, approximate the edge as closely as possible. This infinite set is called the Fourier series, and — despite mathematicians’ early hesitation to accept such a thing — it is now an essential tool in the analysis of functions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Encore&lt;/head&gt;
    &lt;p&gt;The Fourier transform also works on higher-dimensional objects such as images. You can think of a grayscale image as a two-dimensional function that tells you how bright each pixel is. The Fourier transform decomposes this function into a set of 2D frequencies. The sine and cosine waves defined by these frequencies form striped patterns oriented in different directions. These patterns — and simple combinations of them that resemble checkerboards — can be added together to re-create any image.&lt;/p&gt;
    &lt;p&gt;Any 8-by-8 image, for example, can be built from some combination of the 64 building blocks below. A compression algorithm can then remove high-frequency information, which corresponds to small details, without drastically changing how the image looks to the human eye. This is how JPEGs compress complex images into much smaller amounts of data.&lt;/p&gt;
    &lt;p&gt;In the 1960s, the mathematicians James Cooley and John Tukey came up with an algorithm that could perform a Fourier transform much more quickly — aptly called the fast Fourier transform. Since then, the Fourier transform has been implemented practically every time there is a signal to process. “It’s now a part of everyday life,” Greengard said.&lt;/p&gt;
    &lt;p&gt;It has been used to study the tides, to detect gravitational waves, and to develop radar and magnetic resonance imaging. It allows us to reduce noise in busy audio files, and to compress and store all sorts of data. In quantum mechanics — the physics of the very small — it even provides the mathematical foundation for the uncertainty principle, which says that it’s impossible to know the precise position and momentum of a particle at the same time. You can write down a function that describes a particle’s possible positions; the Fourier transform of that function will describe the particle’s possible momenta. When your function can tell you where a particle will be located with high probability — represented by a sharp peak in the graph of the function — the Fourier transform will be very spread out. It will be impossible to determine what the particle’s momentum should be. The opposite is also true.&lt;/p&gt;
    &lt;p&gt;The Fourier transform has spread its roots throughout pure mathematics research, too. Harmonic analysis — which studies the Fourier transform, as well as how to reverse it to rebuild the original function — is a powerful framework for studying waves. Mathematicians have also found that harmonic analysis has deep and unexpected connections to number theory. They’ve used these connections to explore relationships among the integers, including the distribution of prime numbers, one of the greatest mysteries in mathematics.&lt;/p&gt;
    &lt;p&gt;“If people didn’t know about the Fourier transform, I don’t know what percent of math would then disappear,” Fefferman said. “But it would be a big percent.”&lt;/p&gt;
    &lt;p&gt;Editor’s note: The Flatiron Institute is funded by the Simons Foundation, which also funds this editorially independent magazine. Simons Foundation funding decisions have no influence on our coverage. More information about the relationship between Quanta Magazine and the Simons Foundation is available here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45132810</guid></item><item><title>Evolving the OCaml Programming Language (2025) [pdf]</title><link>https://kcsrk.info/slides/Evolution_Ashoka_2025.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45133652</guid></item><item><title>Forking Chrome to render in a terminal (2023)</title><link>https://fathy.fr/carbonyl</link><description>&lt;doc fingerprint="afa2e9556fb0038d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Forking Chrome to render in a terminal&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;January 27, 2023&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I wrote about forking Chrome to turn HTML to SVG two months ago, today we're going to do something similar by making it render into a terminal.&lt;/p&gt;
    &lt;p&gt;Let me introduce you to the Carbonyl web browser!&lt;/p&gt;
    &lt;head rend="h2"&gt;Drawing&lt;/head&gt;
    &lt;p&gt;There isn't much you can draw in a terminal, you're guaranteed to be able to render monospace characters in a fixed grid, and that's it. Escape sequences exist to perform actions like moving the cursor, changing the text color, or mouse tracking. Some came from the days of physical terminals like the DEC VT100, others came from the xterm project.&lt;/p&gt;
    &lt;p&gt;Assuming a popular terminal emulator, we can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Move the cursor&lt;/item&gt;
      &lt;item&gt;Write Unicode characters&lt;/item&gt;
      &lt;item&gt;Set a character's background and foreground color&lt;/item&gt;
      &lt;item&gt;Use a 6x6x6 RGB palette, or 24 bits RGB if &lt;code&gt;COLORTERM&lt;/code&gt;is set the&lt;code&gt;truecolor&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of the unicode characters we can render is the lower half block element &lt;code&gt;U+2584&lt;/code&gt;: &lt;code&gt;▄&lt;/code&gt;. Knowing that cells generally have an aspect ratio of 1:2, we can render perfectly square pixels by setting the background color to the top pixel color, and the foregound color to the bottom pixel color.&lt;/p&gt;
    &lt;p&gt;Let's hook &lt;code&gt;html2svg&lt;/code&gt;'s output into a Rust program:&lt;/p&gt;
    &lt;code&gt;fn move_cursor((x, y): (usize, usize)) {
    println!("\x1b[{};{}H", y + 1, x + 1)
}

fn set_foreground((r, g, b): (u8, u8, u8)) {
    println!("\x1b[38;2;{};{};{}m", r, g, b)
}

fn set_background((r, g, b): (u8, u8, u8)) {
    println!("\x1b[48;2;{};{};{}m", r, g, b)
}

fn print_pixels_pair(
    top: (u8, u8, u8),
    bottom: (u8, u8, u8),
    cursor: (usize, usize)
) {
    move_cursor(cursor);
    set_background(top);
    set_foreground(bottom);
    println!("▄");
}
&lt;/code&gt;
    &lt;p&gt;Not bad. To render text, we need to create a new Skia device using C++, lets call it &lt;code&gt;TextCaptureDevice&lt;/code&gt;. We'll make it call a &lt;code&gt;draw_text&lt;/code&gt; function written in Rust. Just like in &lt;code&gt;html2svg&lt;/code&gt;, we need to convert glyph IDs into unicode characters.&lt;/p&gt;
    &lt;code&gt;class TextCaptureDevice: public SkClipStackDevice {
  void onDrawGlyphRunList(SkCanvas*,
                          const sktext::GlyphRunList&amp;amp; glyphRunList,
                          const SkPaint&amp;amp;,
                          const SkPaint&amp;amp; paint) override {
    // Get the text position
    auto position = localToDevice().mapRect(glyphRunList.origin());

    for (auto&amp;amp; glyphRun : glyphRunList) {
      auto runSize = glyphRun.runSize();
      SkAutoSTArray&amp;lt;64, SkUnichar&amp;gt; unichars(runSize);

      // Convert glyph IDs to Unicode characters
      SkFontPriv::GlyphsToUnichars(glyphRun.font(),
                                  glyphRun.glyphsIDs().data(),
                                  runSize,
                                  unichars.get());

      // Draw that text on the terminal
      draw_text(unichars.data(), runSize, position, paint.getColor());
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Better! But the text is scrambled at the center. Our &lt;code&gt;TextCaptureDevice&lt;/code&gt; does not account for occlusion, drawing a rectangle does not clear the text behind it.&lt;/p&gt;
    &lt;p&gt;Let's add some code to the &lt;code&gt;drawRect&lt;/code&gt; and &lt;code&gt;drawRRect&lt;/code&gt; methods to clear the text if we're filling with a solid color:&lt;/p&gt;
    &lt;code&gt;void drawRRect(const SkRRect&amp;amp; rect, const SkPaint&amp;amp; paint) override {
    drawRect(rect.rect(), paint);
}

void drawRect(const SkRect&amp;amp; rect, const SkPaint&amp;amp; paint) override {
    if (
        paint.getStyle() == SkPaint::Style::kFill_Style &amp;amp;&amp;amp;
        paint.getAlphaf() == 1.0
    ) {
        clear_text(localToDevice().mapRect(rect));
    }
}
&lt;/code&gt;
    &lt;p&gt;The gray background behind text elements is caused by the software rasterizer rendering text in our bitmap. Let's remove it:&lt;/p&gt;
    &lt;code&gt;void SkBitmapDevice::onDrawGlyphRunList(SkCanvas* canvas,
                                        const sktext::GlyphRunList&amp;amp; glyphRunList,
                                        const SkPaint&amp;amp; initialPaint,
                                        const SkPaint&amp;amp; drawingPaint) {
-    SkASSERT(!glyphRunList.hasRSXForm());
-    LOOP_TILER( drawGlyphRunList(canvas, &amp;amp;fGlyphPainter, glyphRunList, drawingPaint), nullptr )
}
&lt;/code&gt;
    &lt;p&gt;That was the easy part, let's handle inputs!&lt;/p&gt;
    &lt;head rend="h2"&gt;Input&lt;/head&gt;
    &lt;code&gt;fn report_mouse_move((x, y): (usize, usize)) {
    write!(get_stdin(), "\x1b[&amp;lt;35;{};{}M", y + 1, x + 1)
}
fn report_mouse_down((x, y): (usize, usize)) {
    write!(get_stdin(), "\x1b[&amp;lt;0;{};{}M", y + 1, x + 1)
}
fn report_mouse_up((x, y): (usize, usize)) {
    write!(get_stdin(), "\x1b[&amp;lt;0;{};{}m", y + 1, x + 1)
}
&lt;/code&gt;
    &lt;p&gt;Some sequences exist to get a terminal emulator to track and report mouse events. For example, if you print &lt;code&gt;\x1b[?1003h&lt;/code&gt;, the terminal should start sending events using this format:&lt;/p&gt;
    &lt;p&gt;These are similar to the sequences we use for styling our output. The &lt;code&gt;\x1b[&lt;/code&gt; prefix is called the Control Sequence Introducer.&lt;/p&gt;
    &lt;code&gt;carbonyl::browser-&amp;gt;BrowserMainThread()-&amp;gt;PostTask(
    FROM_HERE,
    base::BindOnce(
        &amp;amp;HeadlessBrowserImpl::OnMouseDownInput,
        x,
        y
    )
);
&lt;/code&gt;
    &lt;p&gt;We need to notify the browser to wrap this up, but there is a catch: we need to block a thread to read stdin, but the browser methods should be called from the main thread. Thankfully, messages passing is available almost everywhere through the &lt;code&gt;TaskRunner&lt;/code&gt;
class.&lt;/p&gt;
    &lt;code&gt;for &amp;amp;key in input {
    sequence = match sequence {
        Sequence::Char =&amp;gt; match key {
            0x1b =&amp;gt; Sequence::Escape,
            0x03 =&amp;gt; emit!(Event::Exit),
            key =&amp;gt; emit!(Event::KeyPress { key }),
        },
        Sequence::Escape =&amp;gt; match key {
            b'[' =&amp;gt; Sequence::Control,
            b'P' =&amp;gt; Sequence::DeviceControl(DeviceControl::new()),
            0x1b =&amp;gt;
                emit!(Event::KeyPress { key: 0x1b }; continue),
            key =&amp;gt; {
                emit!(Event::KeyPress { key: 0x1b });
                emit!(Event::KeyPress { key })
            }
        },
        Sequence::Control =&amp;gt; match key {
            b'&amp;lt;' =&amp;gt; Sequence::Mouse(Mouse::new()),
            b'A' =&amp;gt; emit!(Event::KeyPress { key: 0x26 }),
            b'B' =&amp;gt; emit!(Event::KeyPress { key: 0x28 }),
            b'C' =&amp;gt; emit!(Event::KeyPress { key: 0x27 }),
            b'D' =&amp;gt; emit!(Event::KeyPress { key: 0x25 }),
            _ =&amp;gt; Sequence::Char,
        },
        Sequence::Mouse(ref mut mouse) =&amp;gt; parse!(mouse, key),
        Sequence::DeviceControl(ref mut dcs) =&amp;gt; parse!(dcs, key),
    }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Pipe&lt;/head&gt;
    &lt;p&gt;We have something that sorts of work, at the cost of a steady 400% CPU usage, and that's not counting iTerm2 which uses ~200%. We've got a few problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We need too much resources to render at 5 FPS&lt;/item&gt;
      &lt;item&gt;We render every time, even when nothing changes&lt;/item&gt;
      &lt;item&gt;We print all characters even if they didn't change on an individual level&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Modern browsers employ a multi-process architecture to improve security. It separates websites into different processes, reducing the potential damage caused by vulnerabilities. The renderer process is running in an OS-level sandboxed environment that blocks certain system calls, such as file-system access. The GPU process, is also considered unprivileged and cannot reach renderer process in order to protect against vulnerabilities in GPU APIs such as WebGL. In contrast, the browser process, considered privileged, can communicate freely with any process.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;CapturePaintPreview&lt;/code&gt; is great for &lt;code&gt;html2svg&lt;/code&gt;, but it's not designed for real-time rendering. It's using IPC calls to correctly support out-of-process iframes, making roundtrips between the browser, GPU, and renderer processes. It downloads hardware accelerated images from the GPU, explaining the surprising memory bandwidth usage. We can disable the fetching, and even disable hardware acceleration, but we still have an expensive IPC machinery holding us back.&lt;/p&gt;
    &lt;p&gt;Software rendering is still very common, it even used to be the default if you can believe it. It was fairly easy back in the single-process days, but nowadays shared memory regions are configured to efficiently render using multiple processes. If we can get our pixels into one of these memory regions, we would just have to notify our browser process using a simple IPC message.&lt;/p&gt;
    &lt;code&gt;void LayeredWindowUpdater::OnAllocatedSharedMemory(
    const gfx::Size&amp;amp; pixel_size,
    base::UnsafeSharedMemoryRegion region
) {
    if (region.IsValid())
        shm_mapping_ = region.Map();
}

void LayeredWindowUpdater::Draw(
    const gfx::Rect&amp;amp; damage_rect,
    DrawCallback draw_callback
) {
    carbonyl_draw_bitmap(
        shm_mapping_.GetMemoryAs&amp;lt;uint8_t&amp;gt;(),
        shm_mapping_.size()
    );

    std::move(draw_callback).Run();
}
&lt;/code&gt;
    &lt;p&gt;In order to setup this shared memory, we need to implement a &lt;code&gt;HostDisplayClient&lt;/code&gt; and a &lt;code&gt;SoftwareOutputDevice&lt;/code&gt; to manage a custom &lt;code&gt;LayeredWindowUpdater&lt;/code&gt; which implements &lt;code&gt;OnAllocatedSharedMemory()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;HostDisplayClient&lt;/code&gt; runs in the browser process and is called by the GPU process through IPC. To wrap this up we need to make the GPU process use our custom display client by adding the following to &lt;code&gt;VizProcessTransportFactory::OnEstablishedGpuChannel()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;compositor_data.display_client =
-      std::make_unique&amp;lt;HostDisplayClient&amp;gt;(compositor);
+      std::make_unique&amp;lt;carbonyl::HostDisplayClient&amp;gt;();
&lt;/code&gt;
    &lt;p&gt;We solved the bitmap problem, now how can we extract text data? This data lives in the renderer process, but our windowing code lives in the browser process. We need to make the renderer interact with the browser process.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mojo&lt;/head&gt;
    &lt;code&gt;// Our C++ bindings will be in the carbonyl::mojom namespace
module carbonyl.mojom;

// Import existing bindings to common structures
import "ui/gfx/geometry/mojom/geometry.mojom";
import "skia/public/mojom/skcolor.mojom";

// Define a structure to hold text to render
struct TextData {
    // An UTF-8 string with the contents
    string contents;
    // Bounds, size only defined for clearing
    gfx.mojom.RectF bounds;
    // Color of the text
    skia.mojom.SkColor color;
};

// The browser process runs this service
interface CarbonylRenderService {
    // The renderer process calls this method
    DrawText(array&amp;lt;TextData&amp;gt; data);
};
&lt;/code&gt;
    &lt;p&gt;Mojo is a library for inter-process communication. It defines an IDL for serializing data which supports native handles (i.e. file descriptors, shared memory regions, callbacks), and can be used to generate C++, Java (Android), and JavaScript (DevTools) bindings. It's extensively documented, and fairly simple to use.&lt;/p&gt;
    &lt;p&gt;We'll start by making an interface &lt;code&gt;CarbonylRenderService&lt;/code&gt; that runs on the browser process, with a method &lt;code&gt;DrawText&lt;/code&gt; called from the renderer process.&lt;/p&gt;
    &lt;p&gt;This &lt;code&gt;.mojom&lt;/code&gt; code generates C++ temporary files which we can then include to write the implementation code.&lt;/p&gt;
    &lt;p&gt;Mojo receivers such as our service are part of the native handles we can send between processes, to register the implementation we just need to add it to the &lt;code&gt;BrowserInterfaceBroker&lt;/code&gt;, which will get called by the renderer through &lt;code&gt;BrowserInterfaceBrokerProxy&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;map-&amp;gt;Add&amp;lt;carbonyl::mojom::CarbonylRenderService&amp;gt;(
    base::BindRepeating(&amp;amp;RenderFrameHostImpl::GetCarbonylRenderService,
                        base::Unretained(host)));
&lt;/code&gt;
    &lt;code&gt;GetBrowserInterfaceBroker().GetInterface(
  std::move(carbonyl_render_service_receiver_)
);
&lt;/code&gt;
    &lt;p&gt;Now, we need to get our text data without any expensive round-trip. Blink has a &lt;code&gt;GetPaintRecord()&lt;/code&gt; method to get the latest paint data for a page, but it's not behind a public API, which we need because our code runs in the content renderer. Ideally we should hook into the compositor (&lt;code&gt;cc&lt;/code&gt;), but it's way more involved. It's dirty but we can workaround this by casting to the private &lt;code&gt;blink::WebViewImpl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;auto* view = static_cast&amp;lt;blink::WebViewImpl*&amp;gt;(GetWebFrame()-&amp;gt;View());

view-&amp;gt;MainFrameImpl()-&amp;gt;GetFrame()-&amp;gt;View()-&amp;gt;GetPaintRecord().Playback(&amp;amp;canvas);
carbonyl_render_service_-&amp;gt;DrawText(std::move(data));
&lt;/code&gt;
    &lt;p&gt;Surprise after the first run: the text content doesn't follow the bitmap. Aaah, scrolling and animating is done on the compositor thread, which frees the main thread and makes everything smoother. Let's procastinate doing things right by adding &lt;code&gt;--disable-threaded-scrolling&lt;/code&gt; &lt;code&gt;--disable-threaded-animation&lt;/code&gt; to the command line arguments.&lt;/p&gt;
    &lt;p&gt;Pretty smooth, it'll be even smoother when threaded compositing is fixed! And we've fixed our biggest problem: we don't use any CPU when idling, and scrolling consumes ~15%.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layout&lt;/head&gt;
    &lt;code&gt;auto font = state.StyleBuilder().GetFontDescription();

font.SetStretch(ExtraExpandedWidthValue());
font.SetKerning(FontDescription::kNoneKerning);
font.SetComputedSize(11.75 / 7.0);
font.SetGenericFamily(FontDescription::kMonospaceFamily);
font.SetIsAbsoluteSize(true);
state.StyleBuilder().SetFontDescription(font);
state.StyleBuilder().SetLineHeight(Length::Fixed(14.0 / 7.0));
&lt;/code&gt;
    &lt;p&gt;Thing is, we can only render one font-size, but Blink doesn't know that. This causes the layout to be messed up, with text chunks overlapping or overly spaced. This is especially visible on websites with a lot of textual content and links like Wikipedia.&lt;/p&gt;
    &lt;p&gt;Another dirty - yet effective - hack we can use is forcing a monospaced font on every element. We can do that by adding some code to &lt;code&gt;StyleResolver::ResolveStyle&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;LoDPI&lt;/head&gt;
    &lt;code&gt;// static
float Display::GetForcedDeviceScaleFactor() {
    return 1.0 / 7.0;
}

// static
bool Display::HasForceDeviceScaleFactor() {
    return true;
}
&lt;/code&gt;
    &lt;p&gt;One expensive step in our rendering pipeline is downscaling: we need to resize the framebuffer from its virtual space to its physical space. What we're doing is kind of the opposite of HiDPI rendering, whose most common ratio is 2, which means 1 pixel on the web equals 4 pixels on the screen. Our ratio is &lt;code&gt;1 / 7&lt;/code&gt; which means 49 pixels on the web renders to 1 block on our terminal.&lt;/p&gt;
    &lt;p&gt;The annoying thing about HiDPI is that it can make rendering ~4x slower, whereas Carbonyl LoDPI® makes rendering run ~49x faster. We just need to force our scaling into the &lt;code&gt;Display&lt;/code&gt; class.&lt;/p&gt;
    &lt;head rend="h2"&gt;Color&lt;/head&gt;
    &lt;p&gt;I looked for examples of RGB color conversion to &lt;code&gt;xterm-256&lt;/code&gt; but the code I found was either wrong or slow because it did a nearest neighbor search. We're going to do it for every pixel so it should run fast.&lt;/p&gt;
    &lt;p&gt;The formula for the conversion is fairly simple, assuming color values between 0 and 1: &lt;code&gt;16 + r * 5 * 36 + g * 5 * 6 + b * 5&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;pub fn to_xterm(&amp;amp;self) -&amp;gt; u8 {
    let r = (self.r as f32 - (95.0 - 40.0)).max(0.0) / 40.0;
    let g = (self.g as f32 - (95.0 - 40.0)).max(0.0) / 40.0;
    let b = (self.b as f32 - (95.0 - 40.0)).max(0.0) / 40.0;

    (16.0 +
        r.round() * 36.0 +
        g.round() * 6.0 +
        b.round()) as u8
}
&lt;/code&gt;
    &lt;p&gt;The twist that most code online gets wrong is that the 6 color levels are not linear: 0, 95, 135, 175, 215, 255; there is a 95 gap between the first and second values, and 40 for the rest.&lt;/p&gt;
    &lt;p&gt;It makes sense to limit the dark range, color differences are more visible with bright colors. For us, it means that we can convert a value between 0 and 255 using &lt;code&gt;max(0, color - 95 - 40) / 40&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;pub fn to_xterm(&amp;amp;self) -&amp;gt; u8 {
    if self.max_val() - self.min_val() &amp;lt; 8 {
        match self.r {
            0..=4 =&amp;gt; 16,
            5..=8 =&amp;gt; 232,
            238..=246 =&amp;gt; 255,
            247..=255 =&amp;gt; 231,
            r =&amp;gt; 232 + (r - 8) / 10,
        }
    } else {
        let scale = 5.0 / 200.0;

        (16.0
            + self
                .cast::&amp;lt;f32&amp;gt;()
                .mul_add(scale, scale * -55.0)
                .max(0.0)
                .round()
                .dot((36.0, 6.0, 1.0))) as u8
    }
}
&lt;/code&gt;
    &lt;p&gt;The conversion itself can be thought of as a dot product of &lt;code&gt;(r, g, b)&lt;/code&gt; and &lt;code&gt;(36, 6, 1)&lt;/code&gt;. We can move the substraction to an &lt;code&gt;mul_add&lt;/code&gt; call to help the compiler use a fused multiply-add instruction.&lt;/p&gt;
    &lt;p&gt;The last step is grayscale: our xterm profile offers 256 colors, there are the 216 colors from the RGB cube (&lt;code&gt;6 * 6 * 6&lt;/code&gt;), the 16 configurable system colors, and 24 gray levels which go from &lt;code&gt;rgb(8,8,8)&lt;/code&gt; to &lt;code&gt;rgb(238,238,238)&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To find out if a color is on a grayscale, we can substract its minimal value to its maximum value and check if it's under a threshold, let's say 8.&lt;/p&gt;
    &lt;p&gt;We still have one tiny problem: how can you detect if a terminal supports true-color or 256 colors? A quick Google search leads us to the &lt;code&gt;COLORTERM&lt;/code&gt; environment variable,
which is &lt;code&gt;24bit&lt;/code&gt; or &lt;code&gt;truecolor&lt;/code&gt; if true-color is supported. But that won't work in
Docker or SSH, which are our primary targets.&lt;/p&gt;
    &lt;code&gt;    // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;    Code =&amp;gt; match key {
        b'0' | b'1' =&amp;gt; Type(key),
        _ =&amp;gt; control_flow!(break)?,
    },
&lt;/code&gt;
    &lt;code&gt;    // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;    Type(code) =&amp;gt; match key {
        b'$' =&amp;gt; Status(StatusParser::new(code)),
        b'+' =&amp;gt; Resource(ResourceParser::new(code)),
        _ =&amp;gt; control_flow!(break)?,
    },
&lt;/code&gt;
    &lt;code&gt;    // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;    Status(ref mut status) =&amp;gt; return status.parse(key),
    Resource(ref mut resource) =&amp;gt; return resource.parse(key),
};
&lt;/code&gt;
    &lt;code&gt;    // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;    Start =&amp;gt; match key {
        b'r' =&amp;gt; Value,
        _ =&amp;gt; control_flow!(break)?,
    },
&lt;/code&gt;
    &lt;code&gt;    // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;    Value =&amp;gt; match key {
&lt;/code&gt;
    &lt;code&gt;        // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;        0x1b =&amp;gt; self.terminate(),
&lt;/code&gt;
    &lt;code&gt;        // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;        b';' =&amp;gt; self.push_value(),
&lt;/code&gt;
    &lt;code&gt;        // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;        char =&amp;gt; self.push_char(char),
    },
&lt;/code&gt;
    &lt;code&gt;    // ^[P1$r0;48:2:1:13:37:42m^[\&lt;/code&gt;
    &lt;code&gt;    Terminator =&amp;gt; control_flow!(break self.parse_event(key))?,
};
&lt;/code&gt;
    &lt;p&gt;A trick we can use is a DCS (Device Control Sequence) to fetch a setting value, like the current background color. If we set an RGB value and get an RGB value back, we can enable true-color.&lt;/p&gt;
    &lt;p&gt;You can try it by running the following on your terminal:&lt;/p&gt;
    &lt;code&gt;$ printf "\e[48;2;13;37;42m\eP\$qm\e\\"; cat
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;\e&lt;/code&gt;: start escape sequence&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;[&lt;/code&gt;: introduce control sequence&lt;/item&gt;&lt;item&gt;&lt;code&gt;48&lt;/code&gt;: set foreground&lt;/item&gt;&lt;item&gt;&lt;code&gt;2&lt;/code&gt;: using an RGB color&lt;/item&gt;&lt;item&gt;&lt;code&gt;13&lt;/code&gt;: R is 13&lt;/item&gt;&lt;item&gt;&lt;code&gt;37&lt;/code&gt;: G is 37&lt;/item&gt;&lt;item&gt;&lt;code&gt;42&lt;/code&gt;: B is 42&lt;/item&gt;&lt;item&gt;&lt;code&gt;m&lt;/code&gt;: select graphic rendition&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;\e&lt;/code&gt;: start escape sequence&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;P&lt;/code&gt;: introduce device control sequence&lt;/item&gt;&lt;item&gt;&lt;code&gt;$&lt;/code&gt;: enter status mode&lt;/item&gt;&lt;item&gt;&lt;code&gt;q&lt;/code&gt;: query current setting&lt;/item&gt;&lt;item&gt;&lt;code&gt;m&lt;/code&gt;: select graphic rendition&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the commands are supported, you should get the following output with a dark turquoise background:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt; ^[P1$r0;48:2:1:13:37:42m^[\ &lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;This is what the terminal emulator sends to stdin, and what we can parse to toggle true-color on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Title&lt;/head&gt;
    &lt;p&gt;A few xterm sequences allow setting the terminal window title, we could use that to display the current page title.&lt;/p&gt;
    &lt;code&gt;fn set_title(title: &amp;amp;str) {
    // Set icon name and window title to string
    println!("\x1b]0;{}\x07", title);
    // Set icon name to string
    println!("\x1b]1;{}\x07", title);
    // Set window title to string
    println!("\x1b]2;{}\x07", title);
}
&lt;/code&gt;
    &lt;p&gt;To get notified when the title changes, we can simply implement &lt;code&gt;WebContentsObserver::TitleWasSet()&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;void HeadlessWebContentsImpl::TitleWasSet(content::NavigationEntry* entry) {
    carbonyl::Renderer::Main()-&amp;gt;SetTitle(
        base::UTF16ToUTF8(entry-&amp;gt;GetTitleForDisplay())
    );
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;That's all for today folks, check out Carbonyl on GitHub!&lt;/p&gt;
    &lt;p&gt;This was my first Rust project and I finally get the hype now. What a cool language!&lt;/p&gt;
    &lt;head rend="h3"&gt;Stay tuned&lt;/head&gt;
    &lt;p&gt;The post for next month will be a visual introduction to Fourier Analysis. After that, we'll look into a speculative JS VM in Rust.&lt;/p&gt;
    &lt;p&gt;Use the RSS feed to stay tuned, you can also watch the website repo for releases on GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45133935</guid></item><item><title>Fil's Unbelievable Garbage Collector</title><link>https://fil-c.org/fugc</link><description>&lt;doc fingerprint="da4929d184b21ed4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fil's Unbelievable Garbage Collector&lt;/head&gt;
    &lt;p&gt;Fil-C uses a parallel concurrent on-the-fly grey-stack Dijkstra accurate non-moving garbage collector called FUGC (Fil's Unbelievable Garbage Collector). You can find the source code for the collector itself in fugc.c, though be warned, that code cannot possibly work without lots of support logic in the rest of the runtime and in the compiler.&lt;/p&gt;
    &lt;p&gt;Let's break down FUGC's features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Parallel: marking and sweeping happen in multiple threads, in parallel. The more cores you have, the faster the collector finishes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Concurrent: marking and sweeping happen on some threads other than the mutator threads (i.e. your program's threads). Mutator threads don't have to stop and wait for the collector. The interaction between the collector thread and mutator threads is mostly non-blocking (locking is only used on allocation slow paths).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;On-the-fly: there is no global stop-the-world, but instead we use "soft handshakes" (aka "ragged safepoints"). This means that the GC may ask threads to do some work (like scan stack), but threads do this asynchronously, on their own time, without waiting for the collector or other threads. The only "pause" threads experience is the callback executed in response to the soft handshake, which does work bounded by that thread's stack height. That "pause" is usually shorter than the slowest path you might take through a typical&lt;/p&gt;&lt;code&gt;malloc&lt;/code&gt;implementation.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Grey-stack: the collector assumes it must rescan thread stacks to fixpoint. That is, GC starts with a soft handshake to scan stack, and then marks in a loop. If this loop runs out of work, then FUGC does another soft handshake. If that reveals more objects, then concurrent marking resumes. This prevents us from having a load barrier (no instrumentation runs when loading a pointer from the heap into a local variable). Only a store barrier is necessary, and that barrier is very simple. This fixpoint converges super quickly because all newly allocated objects during GC are pre-marked.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dijkstra: storing a pointer field in an object that's in the heap or in a global variable while FUGC is in its marking phase causes the newly pointed-to object to get marked. This is called a Dijkstra barrier and it is a kind of store barrier. Due to the grey stack, there is no load barrier like in the classic Dijkstra collector. The FUGC store barrier uses a compare-and-swap with relaxed memory ordering on the slowest path (if the GC is running and the object being stored was not already marked).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Accurate: the GC accurately (aka precisely, aka exactly) finds all pointers to objects, nothing more, nothing less.&lt;/p&gt;&lt;code&gt;llvm::FilPizlonator&lt;/code&gt;ensures that the runtime always knows where the root pointers are on the stack and in globals. The Fil-C runtime has a clever API and Ruby code generator for tracking pointers in low-level code that interacts with pizlonated code. All objects know where their outgoing pointers are - they can only be in the InvisiCap auxiliary allocation.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-moving: the GC doesn't move objects. This makes concurrency easy to implement and avoids a lot of synchronization between mutator and collector. However, FUGC will "move" pointers to free objects (it will repoint the capability pointer to the free singleton so it doesn't have to mark the freed allocation).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes FUGC an advancing wavefront garbage collector. Advancing wavefront means that the mutator cannot create new work for the collector by modifying the heap. Once an object is marked, it'll stay marked for that GC cycle. It's also an incremental update collector, since some objects that would have been live at the start of GC might get freed if they become free during the collection cycle.&lt;/p&gt;
    &lt;p&gt;FUGC relies on safepoints, which comprise:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Pollchecks emitted by the compiler. The&lt;/p&gt;&lt;code&gt;llvm::FilPizlonator&lt;/code&gt;compiler pass emits pollchecks often enough that only a bounded amount of progress is possible before a pollcheck happens. The fast path of a pollcheck is just a load-and-branch. The slow path runs a pollcheck callback, which does work for FUGC.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Soft handshakes, which request that a pollcheck callback is run on all threads and then waits for this to happen.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enter/exit functionality. This is for allowing threads to block in syscalls or long-running runtime functions without executing pollchecks. Threads that are in the exited state will have pollcheck callbacks executed by the collector itself (when it does the soft handshake). The only way for a Fil-C program to block is either by looping while entered (which means executing a pollcheck at least once per loop iteration, often more) or by calling into the runtime and then exiting.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Safepointing is essential for supporting threading (Fil-C supports pthreads just fine) while avoiding a large class of race conditions. For example, safepointing means that it's safe to load a pointer from the heap and then use it; the GC cannot possibly delete that memory until the next pollcheck or exit. So, the compiler and runtime just have to ensure that the pointer becomes tracked for stack scanning at some point between when it's loaded and when the next pollcheck/exit happens, and only if the pointer is still live at that point.&lt;/p&gt;
    &lt;p&gt;The safepointing functionality also supports stop-the-world, which is currently used to implement &lt;code&gt;fork(2)&lt;/code&gt; and for debugging FUGC (if you set the &lt;code&gt;FUGC_STW&lt;/code&gt; environment variable to &lt;code&gt;1&lt;/code&gt; then the
collector will stop the world and this is useful for triaging GC bugs; if the bug reproduces in STW
then it means it's not due to issues with the store barrier). The safepoint infrastructure also allows
safe signal delivery; Fil-C makes it possible to use signal handling in a practical way. Safepointing is
a common feature of virtual machines that support multiple threads and accurate garbage collection,
though usually, they are only used to stop the world rather than to request asynchronous activity from all
threads. See here for a write-up about
how OpenJDK does it. The Fil-C implementation is in &lt;code&gt;filc_runtime.c&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's the basic flow of the FUGC collector loop:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wait for the GC trigger.&lt;/item&gt;
      &lt;item&gt;Turn on the store barrier, then soft handshake with a no-op callback.&lt;/item&gt;
      &lt;item&gt;Turn on black allocation (new objects get allocated marked), then soft handshake with a callback that resets thread-local caches.&lt;/item&gt;
      &lt;item&gt;Mark global roots.&lt;/item&gt;
      &lt;item&gt;Soft handshake with a callback that requests stack scan and another reset of thread-local caches. If all collector mark stacks are empty after this, go to step 7.&lt;/item&gt;
      &lt;item&gt;Tracing: for each object in the mark stack, mark its outgoing references (which may grow the mark stack). Do this until the mark stack is empty. Then go to step 5.&lt;/item&gt;
      &lt;item&gt;Turn off the store barrier and prepare for sweeping, then soft handshake to reset thread-local caches again.&lt;/item&gt;
      &lt;item&gt;Perform the sweep. During the sweep, objects are allocated black if they happen to be allocated out of not-yet-swept pages, or white if they are allocated out of alraedy-swept pages.&lt;/item&gt;
      &lt;item&gt;Victory! Go back to step 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're familiar with the literature, FUGC is sort of like the DLG (Doligez-Leroy-Gonthier) collector (published in two papers because they had a serious bug in the first one), except it uses the Dijkstra barrier and a grey stack, which simplifies everything but isn't as academically pure (FUGC fixpoints, theirs doesn't). I first came up with the grey-stack Dijkstra approach when working on Fiji VM's CMR and Schism garbage collectors. The main advantage of FUGC over DLG is that it has a simpler (cheaper) store barrier and it's a slightly more intuitive algorithm. While the fixpoint seems like a disadvantage, in practice it converges after a few iterations.&lt;/p&gt;
    &lt;p&gt;Additionally, FUGC relies on a sweeping algorithm based on bitvector SIMD. This makes sweeping insanely fast compared to marking. This is made thanks to the Verse heap config that I added to libpas. FUGC typically spends &amp;lt;5% of its time sweeping.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus Features&lt;/head&gt;
    &lt;p&gt;FUGC supports a most of C-style, Java-style, and JavaScript-style memory management. Let's break down what that means.&lt;/p&gt;
    &lt;head rend="h3"&gt;Freeing Objects&lt;/head&gt;
    &lt;p&gt;If you call &lt;code&gt;free&lt;/code&gt;, the runtime will flag the object as free and all subsequent accesses to the object will trap. Additionally, FUGC will not scan outgoing references from the object (since they cannot be accessed anymore).&lt;/p&gt;
    &lt;p&gt;Also, FUGC will redirect all capability pointers (lowers in InvisiCaps jargon) to free objects to point at the free singleton object instead. This allows freed object memory to really be reclaimed.&lt;/p&gt;
    &lt;p&gt;This means that freeing objects can be used to prevent GC-induced leaks. Surprisingly, a program that works fine with &lt;code&gt;malloc&lt;/code&gt;/&lt;code&gt;free&lt;/code&gt; (no leaks, no crashes) that gets converted to GC the naive way (&lt;code&gt;malloc&lt;/code&gt; allocates from the GC and &lt;code&gt;free&lt;/code&gt; is a no-op) may end up leaking due to dangling pointers that the program never accesses. Those dangling pointers will be treated as live by the GC. In FUGC, if you freed those pointers, then FUGC will really kill them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Finalizers&lt;/head&gt;
    &lt;p&gt;FUGC supports finalizer queues using the &lt;code&gt;zgc_finq&lt;/code&gt; API in stdfil.h. This feature allows you to implement finalizers in the style of Java, except that you get to set up your own finalizer queues and choose which thread processes them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Weak References&lt;/head&gt;
    &lt;p&gt;FUGC supports weak references using the &lt;code&gt;zweak&lt;/code&gt; API in stdfil.h. Weak references work just like the weak references in Java, except there are no reference queues. Fil-C does not support phantom or soft references.&lt;/p&gt;
    &lt;head rend="h3"&gt;Weak Maps&lt;/head&gt;
    &lt;p&gt;FUGC supports weak maps using the &lt;code&gt;zweak_map&lt;/code&gt; API in stdfil.h. This API works almost exactly like the JavaScript WeakMap, except that Fil-C's weak maps allow you to iterate all of their elements and get a count of elements.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;FUGC allows Fil-C to give the strongest possible guarantees on misuse of &lt;code&gt;free&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Freeing an object and then accessing it is guaranteed to result in a trap. Unlike tag-based approaches, which will trap on use after free until until memory reclamation is forced, FUGC means you will trap even after memory is reclaimed (due to lower repointing to the free singleton).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Freeing an object twice is guaranteed to result in a trap.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Failing to free an object means the object gets reclaimed for you.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45133938</guid></item><item><title>Swimming in Tech Debt</title><link>https://helpthisbook.com/lou-franco/swimming-in-tech-debt</link><description>&lt;doc fingerprint="43255b5b50b7438"&gt;
  &lt;main&gt;
    &lt;p&gt;F e t c h i n g y o u r b o o k . . . Swimming in Tech Debt - Help This Book&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45135263</guid></item><item><title>Fiber Concurrency</title><link>https://honeyryderchuck.gitlab.io/httpx/wiki/Fiber-Concurrency</link><description>&lt;doc fingerprint="7e50fa1a0e18b6d0"&gt;
  &lt;main&gt;
    &lt;p&gt;The &lt;code&gt;:fiber_concurrency&lt;/code&gt; plugin enables connections a session to be used seamlessly across fibers managed by a fiber scheduler. This is of particular relevance if the connections are long-lived/persistent.&lt;/p&gt;
    &lt;p&gt;Note that, if you’re using the &lt;code&gt;:persistent&lt;/code&gt; plugin, this plugin is required by default.&lt;/p&gt;
    &lt;code&gt;
http = HTTPX.plugin(:fiber_concurrency)

Thread.start do
  # assuming fiber scheduler is set here
  10.times.each do
    Fiber.schedule do
      http.get("https://example.com")
    end
  end
end
&lt;/code&gt;
    &lt;p&gt;This plugin is a requirement if you’re using &lt;code&gt;httpx&lt;/code&gt; in programs with a fiber scheduler. This includes, for example, programs developed using the async gem.&lt;/p&gt;
    &lt;p&gt;Next: Custom Plugins&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45136008</guid></item><item><title>Interview with Japanese Demoscener – 0b5vr</title><link>https://6octaves.com/2025/09/interview-with-demoscener-0b5vr.html</link><description>&lt;doc fingerprint="27325121b8a72b9e"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to “Interviews with Demosceners”! This time, we welcome Japanese demoscener 0b5vr, who mainly creates 64K and 4K intros.&lt;/p&gt;
    &lt;p&gt;For many, 0b5vr is best remembered for his 64K demo “0b5vr GLSL Techno Live Set”, released at Revision 2023. In this interview, he talks about how this piece was created, as well as his recent live music performance.&lt;/p&gt;
    &lt;p&gt;He also talks about trends around the Japanese demoscene, like music production with GLSL, machine live, and generative VJ. I also took the chance to ask how he feels about sceners like me—that is, people who know nothing about programming or technology! Happy reading!&lt;/p&gt;
    &lt;p&gt;Note: If you don’t know what demoscene is, you may want to start from here!&lt;/p&gt;
    &lt;p&gt;First of all, could you introduce yourself?&lt;/p&gt;
    &lt;p&gt;I’m 0b5vr, and I don’t belong to any particular group. I mainly work on 64k intros and 4k intros/exegfx using WebGL. I also compete in Shader Jam and perform live coding and VJ sets at club events and similar venues.&lt;/p&gt;
    &lt;p&gt;Your demo “0b5vr GLSL Techno Live Set” had a strong impact on me. I was curious about this. It says “Live Set,” but was released in the 64K category. What is this exactly? Is this live coding?&lt;/p&gt;
    &lt;p&gt;0b5vr GLSL Techno Live Set (“0mix”) is indeed a 64K intro demo. Just like any other 64K intro, this audiovisual piece is generated from a 64KB file―an HTML file, in this case.&lt;/p&gt;
    &lt;p&gt;That said, as described in the title, its format is “Live Set.” It can be somewhat tricky, because it looks like a recorded video of a live performance at an event, but it’s actually a 64K intro.&lt;/p&gt;
    &lt;p&gt;Hmm… I’m still not sure if I understood correctly. Could you elaborate a bit more?&lt;/p&gt;
    &lt;p&gt;0mix was inspired by three different scenes: techno demos, live coding, and 64K intros.&lt;/p&gt;
    &lt;p&gt;Let me start with techno demos. There are many techno-themed demos in the history of the demoscene. If you look at the demos such as “Medium” by Einklang.net, “X-MIX 2004: Ion Traxx” by Kewlers &amp;amp; mfx, and “Emix” by Epoch, they use multiple tracks mixed together like a DJ set, rather than a single techno soundtrack. They also use VJ-style visuals to create an atmosphere similar to a club event. Emix has black-and-white visuals with unique textures that fit perfectly with cold, mechanical techno, and it’s one of my favorites.&lt;/p&gt;
    &lt;p&gt;Next is live coding. Live coding is a live performance where visuals and music are generated with programming in real time. On the screen, you’ll see the visuals and sound waveforms being generated alongside the code you’re writing. This highlights that the artwork is generated by code. In the demoscene, live coding sessions focus mostly on visuals in GLSL (eg, Shader Showdown, Shader Jam). But in live coding events like Algorave and Eulerroom, music live coding is as popular as, or even more popular than, visual coding. From what I see, Tidal Cycles and Sonic Pi are the most commonly used tools in those environments. (Reference video)&lt;/p&gt;
    &lt;p&gt;Finally, there’s the 64K intro. It’s a category where you create visuals and audio with an executable file of just 64KB. This is the most challenging category since every element has to be procedurally generated within the intro. Most 64K creators build their own engines and tools from scratch. This category requires a broad range of knowledge and skills including modeling, animation, rendering, post-processing, music, and compression.&lt;/p&gt;
    &lt;p&gt;If I managed to merge all three inspirations and create a 64K techno demo with music generated by live coding, I knew I could present it to demosceners and other creators around the scene with confidence. I came up with the idea about a year before Revision 2023. Over the course of that year, I refined a demo engine, built a live coding environment, composed music, and created visual assets almost entirely on my own.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Here’s the working environment for 0mix. The top screen shows the preview, timeline, etc., while the bottom screen is the code editor. Basically, I spend most of the time in the code editor.&lt;/p&gt;
    &lt;p&gt;So, you climbed the highest mountain by yourself. What was the process like?&lt;/p&gt;
    &lt;p&gt;It was extremely tough and painful to spend a year working on a challenging 64K project by myself. My advice is to collaborate with others. At the very least, you should find someone you can discuss the progress with. It was indeed fun to surprise many friends at demoparties, but at the end of the day, completing the project is more important.&lt;/p&gt;
    &lt;p&gt;You entered 64K compo, but it ended up being released in the PC Demo compo. Did that bother you?&lt;/p&gt;
    &lt;p&gt;It’s true that 0mix was released in the PC Demo Compo at Revision 2023. That was because it was the only entry in the PC 64K intro, which wasn’t enough to hold a separate compo. So the two compos were merged. The same thing happened at Revision 2022. PC 64K intro compo was incorporated into the 8K intro compo because there were only two entries. Nevertheless, I’ve always pursued uncompromising quality, so I was down with it. Along with the works of other demo groups (such as Fairlight, mfx, and Still), I think I could contribute to making that compo interesting.&lt;/p&gt;
    &lt;p&gt;Ah, you’re right. That felt like a never-ending compo!&lt;/p&gt;
    &lt;p&gt;There were so many entries for Revision 2023, and from the chat I got the impression that many participants and viewers were exhausted after the compo. Still, it was a great compo. All of the top works featured demoscene-style visuals built with their own engines, and their narratives were also impressive. So I’m happy with my result. When there’s a big entry in the compo I’m in, I feel more accomplished because it means I helped make that compo exciting together with those great pieces.&lt;/p&gt;
    &lt;p&gt;That’s right, I remember some big names rushing in at the end. Nevertheless, this demo stood out for its originality.&lt;/p&gt;
    &lt;p&gt;Thank you. Revision has an award called “Crowd Favorite” where viewers can vote for their favorite demo in any category, and 0mix received first prize. 0mix is a piece that reflects what I love, so I felt happy that everyone else enjoyed it, too.&lt;/p&gt;
    &lt;p&gt;Congratulations! It was indeed a cool demo.&lt;/p&gt;
    &lt;p&gt;Oh, I have a question for you. How do you feel about the code constantly shown in 0mix? What kind of impression does it give you? &lt;lb/&gt; (Interviewer’s note: I’m not from the programming field. I’m the type of person who chooses a laptop by its color.)&lt;/p&gt;
    &lt;p&gt;Maybe it’s more like a design or typography? It says “live coding,” so I figured this code is for its visuals, but I have absolutely no idea if the code itself is cool or not. If I didn’t know what live coding is, I’d probably just look at it as part of the design, just like seeing the typography in a language I don’t understand.&lt;/p&gt;
    &lt;p&gt;Ah, that’s interesting! Actually, the code displayed on the screen is not for visuals but for music. I use a programming language called GLSL, which is normally used to generate visuals. But 0mix is a live performance-themed demo where I use GLSL for music, and that’s why it’s called “GLSL Techno Live.” If you look at the code closely, you’ll see the parts for instruments, like “KICK,” “HIHAT,” and “BASS.” And by adding and subtracting these elements, I shaped the flow of music.&lt;/p&gt;
    &lt;p&gt;Ohh, so that was code for music! But even after knowing this fact, my impression of this piece hasn’t really changed. I guess that shows I interpreted the code as part of the design. Is it okay if a viewer like me sees it that way? (laughs)&lt;/p&gt;
    &lt;p&gt;In my post about this production on Scrapbox, I wrote, “for viewers without coding knowledge, it feels like music-making magic. And for viewers who know programming languages and environments, it’s a hint to guess the next move.” So I expected that some people would see it as part of the design.&lt;/p&gt;
    &lt;p&gt;To reveal a bit more about my understanding, now I do understand that “demo is generated from an executable file” and that “a 64K piece has a 64KB file.” But I still don’t see things like “this is real-time rendering, so it’s more impressive than live-action” or “it’s great quality considering this is 64KB.” Basically, I watch demos like I watch music videos, and the only thing that matters to me is whether I find it cool or not.&lt;/p&gt;
    &lt;p&gt;Ryoji Ikeda has a work that presents data including planets and genes using 5×5 pixel fonts. Of course, only experts can truly understand such data, so most of us simply enjoy the visual design that comes out of it. Even if we try to find deeper meaning in it, we probably just end up saying something like, “Wow, the world is huge.” I’ve read that Ikeda actually intended for viewers to see it that way.&lt;/p&gt;
    &lt;p&gt;Oh, then I’m actually one of his intended viewers. When I first saw his installation video, I knew him as a musician, so I thought, “Wow, that’s his MV? Cool! Very futuristic!” I later realized that it wasn’t just design. It’s nice to know that creators and demosceners expected viewers like me, and personally, I feel relieved. I’d always thought they might be annoyed to hear a clueless person like me commenting on their piece. (laughs)&lt;/p&gt;
    &lt;p&gt;To me, how others first got interested in a piece or in the culture is as fascinating as the motives behind its creation. So I do appreciate sceners who are not from the tech side!&lt;/p&gt;
    &lt;p&gt;Thank you! That’s really nice and reassuring to hear!&lt;lb/&gt; OK, let’s go back to that music code. You wrote in your post on Scrapbox that you put a lot of time and effort into the music.&lt;/p&gt;
    &lt;p&gt;Actually, I had never really made this type of techno music before, so I watched a lot of live performances of this style and tutorials on YouTube. I also bought and tried hardware for “machine live” performances, like the Elektron Syntakt and Dirtywave M8, for research.&lt;/p&gt;
    &lt;p&gt;What is “machine live”?&lt;/p&gt;
    &lt;p&gt;“Machine live” is a type of music performance similar to live coding. Performers use music equipment like grooveboxes and modular synths in real-time to control the sound during the performance. What you can do depends on the features of the equipment, so performers always have to be aware of limitations—something somewhat similar to the demoscene. It’s a fascinating culture. There’s even a “DAWless Live” category where you perform without using a DAW, the standard PC-based music production system. For 0mix, I drew a lot of inspiration from the philosophy and methods of machine live and applied them to GLSL live coding. (Reference video)&lt;/p&gt;
    &lt;p&gt;I just watched the reference video you sent me. Does everyone in this scene really use that much gear?&lt;/p&gt;
    &lt;p&gt;Of course not. Not everyone uses this much equipment, or equipment of this size, for live performance. Lately, it seems like the palm-sized Dirtywave M8 is trending for live sets. The Dirtywave M8 uses a tracker-style UI, and it’s fun to compose with. Plus, it fits well with the demoscene aesthetic.&lt;/p&gt;
    &lt;p&gt;I did a lot of research on machine live and live coding performances, and this gave me ideas about how to create sound and how to evolve live performance. But that only covered the technical side. When it comes to making techno, especially abstract sounds, I had to learn through trial and error and trust my feelings. Even after I learned how to make sounds on standard hardware or software, GLSL follows a completely different set of rules, and I had to be really fired up to tackle it.&lt;/p&gt;
    &lt;p&gt;I heard that you did a live performance recently. What kind of event was it?&lt;/p&gt;
    &lt;p&gt;I performed a live coding set at “draw(tokyo); #2” in March 2025. “draw(); ” is a club event focused on audiovisuals, especially live coding and generative VJ (the so-called “gene-kei” performances). It takes place from time to time in VRChat and at physical venues.&lt;/p&gt;
    &lt;p&gt;At draw(tokyo); #2, I performed using Wavenerd, my custom GLSL live coding environment. For my 40-minute live set, I mainly used techno patterns created for 0mix. It was a really memorable experience, since it was my first time doing a live music performance with Wavenerd. I’d love to do more live performances in the future.&lt;/p&gt;
    &lt;p&gt;The “Wavenerd” system I used for my live coding performance at draw(tokyo); #2. Since we were chroma keying with VJ visuals, the background is blue. The performers are always lit up in blue.&lt;/p&gt;
    &lt;p&gt;When a coder does a live music performance, aren’t you too busy typing code in front of the PC to even look at the audience’s reaction?&lt;/p&gt;
    &lt;p&gt;During the performances, I rewrite parts of prewritten code, so I don’t need to constantly keep typing. But I’m busy adding and removing parts, changing parameters, and doing some DJ-style mixing, so basically I completely zone in on the screen. That said, I can still see the audience’s reactions to some extent, and I felt really happy when they reacted at the moments I expected.&lt;/p&gt;
    &lt;p&gt;Do you know who the primary audience is? I guess this kind of live performance requires some knowledge to really enjoy it.&lt;/p&gt;
    &lt;p&gt;I still don’t know what kind of audience it attracts. From what I saw, I got the impression that many of them are interested in musical experiences and visual production at least. But I’m not sure how many are interested in coding, or actually create things with code. How technical it should get, how strictly you stick to the technical restrictions, and how much you make the audience dance—I think performers are expected to balance these elements well. Probably, this is something gene-kei performers constantly have to tackle. In fact, quite a few performers change their set depending on the tone of the event.&lt;/p&gt;
    &lt;p&gt;Did you have VJs for your live performance?&lt;/p&gt;
    &lt;p&gt;Yes, I asked fellow demosceners, ukonpower and Renard, and they generated visuals that matched the techno. I just told them, “I’m going to do 0mix,” and they both knew what it meant, so everything went very smoothly. (laughs) They created visuals in my style, but their own personalities also shone through. It was really cool.&lt;/p&gt;
    &lt;p&gt;Oh, that’s really cool! &lt;lb/&gt; According to your discography, you also have 4K as well as 64K works. Is there a reason for that?&lt;/p&gt;
    &lt;p&gt;For the 4K intros I’ve released lately, I can usually create them in one or two weeks. But 64K is my soul, so I want to keep making 64K intros. The thing is, 64K requires hundreds of times more work than 4K. So, when I don’t have the time or motivation but still want to contribute to a demoparty, I just make a 4K intro.&lt;/p&gt;
    &lt;p&gt;I must say that the production environment for 4K intros is well-supported in the current demoscene. Recently I’ve been using 0x4015’s minimalGL. With this demotool, I can easily create 4K intros just by writing GLSL. That being said, I wouldn’t recommend it to everyone, because you also have to write the music in GLSL.&lt;/p&gt;
    &lt;p&gt;In 2023, I released a 4K intro called “Architectural Shapeshifter” with Renard. For this piece, Renard was in charge of the concept and visuals, while I was in charge of the music and direction. We used minimalGL for this piece as well. It was the first time for Renard to create a 4K intro, but he was able to create it easily. We collaborated by tweaking the source code on GitHub and communicating via Discord. We exchanged ideas and suggestions on each other’s code, and it turned out to be a very efficient workflow.&lt;/p&gt;
    &lt;p&gt;There are many coders who can write GLSL in Japan, but not many of them take on 4K. So I’d love to collaborate more using minimalGL.&lt;/p&gt;
    &lt;p&gt;What’s hot in the Japanese demoscene these days? What category is popular? I noticed there was a demoparty called SESSIONS in Japan last year.&lt;/p&gt;
    &lt;p&gt;It seems like a lot of people are coming into the demoscene from shader culture centered around VRChat. The people I got to know at demoparties like SESSIONS were mostly active in VRChat. In particular, the event draw(); seems to have a strong influence, and many of the people who got interested in live coding or generative VJ through draw();’s audiovisual experience also developed an interest in the demoscene.&lt;/p&gt;
    &lt;p&gt;Live coding and generative VJ becoming a gateway into the demoscene sounds like a new path to me.&lt;/p&gt;
    &lt;p&gt;Yes, indeed. draw();’s main crew, Saina-san, purposefully aims for a crossover with demoscene culture, like SESSIONS, and this accelerates the influx. We’re really grateful for that.&lt;/p&gt;
    &lt;p&gt;I’m sure a person like that is supporting the demoscene in Japan and around the world. &lt;lb/&gt; OK, let’s go back to the production. Is there anything you do in everyday life to get inspired for your creations?&lt;/p&gt;
    &lt;p&gt;I check Pouet and Demozoo as much as possible to stay in the know about recent demoscene productions. If I ever stopped checking Pouet and Demozoo, I think that would be the end of me as a demoscener.&lt;/p&gt;
    &lt;p&gt;I also try to take in other cultures as well. Recently, I’ve been fascinated by the flashy audiovisual productions in pachinko and pachislot machines. They use dazzling visuals and music to stir up the spirit of gambling. These productions thoroughly pursue how to exploit the human reward system, all within machines that operate under very strict legal restrictions. In a way, I think this represents the highest peak of visual entertainment.&lt;/p&gt;
    &lt;p&gt;I also go for walks frequently. Especially walking around Tokyo late at night gives me a strong sense of urban life and social activity, and it inspires me a lot. “Domain“, a 64K intro I released at Tokyo Demo Fest 2021, was heavily inspired by night Tokyo. I find the concept of the night city very interesting, and I’d like to explore it further.&lt;/p&gt;
    &lt;p&gt;Which areas do you usually walk around?&lt;/p&gt;
    &lt;p&gt;I mainly walk around downtown. I can feel the rhythm of social activity through people’s movements, clothing, and buildings. It’s also very fun to walk around residential areas. When I imagine that this is someone’s everyday life, I can sense their presence through the scenery.&lt;/p&gt;
    &lt;p&gt;Do you have anything you always keep in mind when you create, like a routine or your own personal rule?&lt;/p&gt;
    &lt;p&gt;For my demo source code, I use Git for version control and share as much of the code as possible on GitHub. Basically, I publish my source code under the Creative Commons BY-NC 4.0 license, and users can adapt and use it freely for non-profit purposes. By publishing my source code, I allow other people to refer to my production methods. In fact, I’ve often heard that people have made demos based on my code. Getting more chances to discover other demosceners’ great works is valuable for me too, so I’ll continue to publish my source code.&lt;/p&gt;
    &lt;p&gt;Also, when I do version control on Git, I try to write commit logs—comments you can add to each version—as detailed as possible. Commit logs explain which part of the code I changed, and they also serve as a kind of production journal. In addition to information like what type of change I made and for what purpose, they help me recall my state of mind or what I was thinking during the creative process.&lt;/p&gt;
    &lt;p&gt;For programmers, is it a hassle to write detailed commit logs?&lt;/p&gt;
    &lt;p&gt;Commit logs aren’t considered a direct contribution to a program, just like READMEs or documentation. So, engineers who want to focus on coding and dislike communicating often don’t write them at all. Usually, detailed commit logs are recommended when you work with other people on business projects. However, even for a one-off piece of code written by a single person, I think we should consider how detailed we make the commit logs, because someone else—or even yourself—may end up reading them like archaeology.&lt;/p&gt;
    &lt;p&gt;Archaeology… that’s interesting. &lt;lb/&gt; Okay, let me go to the classic question: your favorite demo, a memorable demo, or a demo that changed your life… anything. Tell us about a demo, or demos that are special to you.&lt;/p&gt;
    &lt;p&gt;As I mentioned, “Emix” by Epoch is the demo I like the most. From the theme of each effect to the color grading, glitch effects, music, and direction, this piece defined what a demo should have, for me. Other pieces that helped define my standards include “cdak” by Quite &amp;amp; Orange, “Transformer 3” by Limp Ninja, and “Clean Slate” by Conspiracy. I put them together in my Pouet playlist “0b5vr’s bible”, if you’re interested.&lt;/p&gt;
    &lt;p&gt;Among many other forms of self-expression, why did you choose the demoscene? Or are you trapped by this culture? Tell me what’s so attractive about it.&lt;/p&gt;
    &lt;p&gt;The demoscene is a creative activity free from art as a capital asset or from commercial value. We mostly create and present pieces in a format that has little value in today’s society, and we purely inspire one another’s technical curiosity and the craving for expression. Also, the demoscene ecosystem is cooperative. Anyone can access demotools, ask questions to veterans, and start creating a piece. I respect the works, workflows, and ideas of active demosceners in the community, and that’s what motivates me to create something that earns their recognition.&lt;/p&gt;
    &lt;p&gt;On the other hand, due to the methods used in the demoscene, a lot of pieces look similar, and that’s clearly a weak point of the scene. If I only keep exploring the demoscene, I can’t expand my range of expression. As a creator, I think it’s important to look at various cultures and absorb many different methods of expression. The easy exchange of fresh inspiration is one of the features of the demoscene, so I’d like to take in many forms of expression both inside and outside the scene, and keep inspiring each other.&lt;/p&gt;
    &lt;p&gt;Is there anything you want to do in the future?&lt;/p&gt;
    &lt;p&gt;What I want to do most is live music performance using GLSL, as I mentioned. Seemingly, this format of live music with GLSL is currently performed only by me and “Rakuto-ice” san. So I want to perform more to develop my style further, and I hope more people will enjoy it.&lt;/p&gt;
    &lt;p&gt;And of course, I want to create demos like 64K, but right now I don’t have enough motivation or ideas. To find more motivation and inspiration, I think it’s about time I formed a demogroup.&lt;/p&gt;
    &lt;p&gt;Sounds like there’s much to look forward to! &lt;lb/&gt; Finally, your message for demosceners and demo fans out there, please.&lt;/p&gt;
    &lt;p&gt;For those of you who are not yet demosceners: &lt;lb/&gt; I’ve seen many people who have an interest in the demoscene but also fears about the culture itself. And it’s not just Japanese people, people in other countries have reacted that way too. Please don’t be afraid of us. If you are interested in creating something with a computer and having fun at a demoparty, then you are a demoscener. Whether you already have a medium of expression or not, if you join the party, you may naturally feel inspired to think, “I want to express myself too.” Demoparties like Tokyo Demo Fest, SESSIONS, and Revision have various compos, including simple programs, illustration, photography, music, along with the demo compo. Of course, if you want to create a demo, fellow creators will help you. We demosceners hope you will have fun in this scene.&lt;/p&gt;
    &lt;p&gt;For those who are already demosceners (including me): &lt;lb/&gt; Make 64K!&lt;/p&gt;
    &lt;p&gt;Thank you very much for answering my question, 0b5vr!&lt;/p&gt;
    &lt;p&gt;0b5vr’s works can be found on Pouet and Demozoo. Also, be sure to check his essay on the production of 0mix on Scrapbox, where he goes deeper into his thoughts on the demoscene and the creative process.&lt;/p&gt;
    &lt;p&gt;Thank you very much for reading this to the end!&lt;/p&gt;
    &lt;p&gt;—————-&lt;/p&gt;
    &lt;p&gt;In case you’re wondering what “demo” or “demoscene” is, better check out the well-made documentary called Moleman2. (and the director, M. Szilárd Matusik’s interview can be read in here.)&lt;/p&gt;
    &lt;p&gt;#1: q from nonoil/gorakubu is here. &lt;lb/&gt; #2: Gargaj from Conspiracy, Ümlaüt Design is here. &lt;lb/&gt; #3: Preacher from Brainstorm, Traction is here. &lt;lb/&gt; #4: Zavie from Ctrl-Alt-Test is here. &lt;lb/&gt; #5: Smash from Fairlight is here. &lt;lb/&gt; #6: Gloom from Excess, Dead Roman is here. &lt;lb/&gt; #7: kioku from System K is here. &lt;lb/&gt; #8: kb from Farbrausch is here. &lt;lb/&gt; #9: iq from RGBA is here.&lt;lb/&gt; #10: Navis from Andromeda Software Development is here.&lt;lb/&gt; #11: Pixtur from Still, LKCC is here.&lt;lb/&gt; #12: Cryptic from Approximate is here.&lt;lb/&gt; #13: 0x4015 aka Yosshin is here.&lt;lb/&gt; #14: Flopine from Cookie Collective is here. &lt;lb/&gt; #15: noby from Epoch, Prismbeings is here.&lt;/p&gt;
    &lt;p&gt;Why I’m interested in demoscene is explained in this article.&lt;lb/&gt; And for some of my other posts related to “demo and “demoscene” culture is here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137245</guid></item><item><title>Nepal moves to block Facebook, X, YouTube and others</title><link>https://www.aljazeera.com/news/2025/9/4/nepal-moves-to-block-facebook-x-youtube-and-others</link><description>&lt;doc fingerprint="9894c6d400e0d759"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Nepal moves to block Facebook, X, YouTube and others&lt;/head&gt;&lt;p&gt;The restrictions come after the social media giants failed to meet state registration requirements, says government.&lt;/p&gt;&lt;p&gt;Nepal’s government has said it will shut off access to major social media platforms, including Facebook and X, after they failed to comply with authorities’ registration requirements.&lt;/p&gt;&lt;p&gt;The move, announced on Thursday, is part of what the government says is an effort to curb online hate, rumours and cybercrime.&lt;/p&gt;&lt;head rend="h2"&gt;Recommended Stories&lt;/head&gt;list of 3 items&lt;list rend="ul"&gt;&lt;item&gt;list 1 of 3‘Everest Man’ breaks own record for climbing world’s highest mountain&lt;/item&gt;&lt;item&gt;list 2 of 3Dozens missing after monsoon triggers Nepal-China floods&lt;/item&gt;&lt;item&gt;list 3 of 3Photos: The last nomads of Nepal&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Companies were given a deadline of Wednesday to register with the Ministry of Communications and Information Technology and provide a local contact, grievance handler and person responsible for self-regulation – or face shutdown.&lt;/p&gt;&lt;p&gt;“Unregistered social media platforms will be deactivated today onwards,” ministry spokesman Gajendra Kumar Thakur told AFP.&lt;/p&gt;&lt;p&gt;Communications and IT Minister Prithvi Subba Gurung said, “We gave them enough time to register and repeatedly requested them to comply with our request, but they ignored [this], and we had to shut their operations in Nepal.”&lt;/p&gt;&lt;p&gt;Meta, which owns Facebook, Instagram and WhatsApp, YouTube parent Alphabet, X, Reddit, and LinkedIn were asked to register by Wednesday’s deadline.&lt;/p&gt;&lt;p&gt;AFP reported that the platforms remained accessible on Thursday.&lt;/p&gt;&lt;head rend="h2"&gt;‘Directly hits fundamental rights’&lt;/head&gt;&lt;p&gt;The online restrictions follow a 2023 directive requiring social media platforms – which have millions of users in Nepal with accounts for entertainment, news and business – to register and establish a local presence.&lt;/p&gt;&lt;p&gt;Only five, including TikTok and Viber, have since formally registered, while two others are in the process.&lt;/p&gt;&lt;p&gt;Bhola Nath Dhungana, president of Digital Rights Nepal, said that the sudden closure shows the “controlling” approach of the government.&lt;/p&gt;&lt;p&gt;“This directly hits the fundamental rights of the public,” Dhungana said. “It is not wrong to regulate social media, but we first need to have the legal infrastructure to enforce it. A sudden closure like this is controlling.”&lt;/p&gt;&lt;p&gt;Nepal has restricted access to popular online platforms in the past.&lt;/p&gt;&lt;p&gt;Access was blocked to the Telegram messaging app in July, with the government citing a rise in online fraud and money laundering.&lt;/p&gt;&lt;p&gt;In August last year, Nepal lifted a nine-month ban on TikTok after the platform’s South Asia division agreed to comply with Nepali regulations.&lt;/p&gt;&lt;p&gt;Governments worldwide, including the United States, European Union, Brazil and Australia, are also tightening oversight of social media and big tech, citing concerns over misinformation, data privacy, online harm and national security. India has mandated local compliance officers and takedown mechanisms, while China maintains strict censorship and licensing controls.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137363</guid></item><item><title>Why ML Needs a New Programming Language</title><link>https://signalsandthreads.com/why-ml-needs-a-new-programming-language/</link><description>&lt;doc fingerprint="272a001512c6a995"&gt;
  &lt;main&gt;
    &lt;p&gt;Listen in on Jane Street’s Ron Minsky as he has conversations with engineers working on everything from clock synchronization to reliable multicast, build systems to reconfigurable hardware. Get a peek at how Jane Street approaches problems, and how those ideas relate to tech more broadly.&lt;/p&gt;
    &lt;p&gt;Chris Lattner is the creator of LLVM and led the development of the Swift language at Apple. With Mojo, he’s taking another big swing: How do you make the process of getting the full power out of modern GPUs productive and fun? In this episode, Ron and Chris discuss how to design a language that’s easy to use while still providing the level of control required to write state of the art kernels. A key idea is to ask programmers to fully reckon with the details of the hardware, but making that work manageable and shareable via a form of type-safe metaprogramming. The aim is to support both specialization to the computation in question as well as to the hardware platform. “Somebody has to do this work,” Chris says, “if we ever want to get to an ecosystem where one vendor doesn’t control everything.”&lt;/p&gt;
    &lt;p&gt;Chris Lattner is the creator of LLVM and led the development of the Swift language at Apple. With Mojo, he’s taking another big swing: How do you make the process of getting the full power out of modern GPUs productive and fun? In this episode, Ron and Chris discuss how to design a language that’s easy to use while still providing the level of control required to write state of the art kernels. A key idea is to ask programmers to fully reckon with the details of the hardware, but making that work manageable and shareable via a form of type-safe metaprogramming. The aim is to support both specialization to the computation in question as well as to the hardware platform. “Somebody has to do this work,” Chris says, “if we ever want to get to an ecosystem where one vendor doesn’t control everything.”&lt;/p&gt;
    &lt;p&gt;Some links to topics that came up in the discussion:&lt;/p&gt;
    &lt;p&gt;Welcome to Signals and Threads, in-depth conversations about every layer of the tech stack, from Jane Street. I’m Ron Minsky. It is my great pleasure to have Chris Lattner on the show. Typically on Signals and Threads, we end up talking to engineers who work here at Jane Street, but sometimes we like to grab outside folk, and Chris is an amazing figure to bring on because he’s been so involved in a bunch of really foundational pieces of computing that we all use—LLVM, and Clang, and MLIR, and OpenCL, and Swift, and now Mojo. And this has happened at a bunch of different storied institutions—Apple, and Tesla, and Google, and SiFive, and now Modular. So anyway, it’s a pleasure to have you joining us, Chris.&lt;/p&gt;
    &lt;p&gt;Thank you, Ron. I’m so happy to be here.&lt;/p&gt;
    &lt;p&gt;I guess I want to start by just hearing a little bit more about your origin story. How did you get into computing and how did you get into this world of both compiler engineering and programming language design?&lt;/p&gt;
    &lt;p&gt;So I grew up in the ’80s and back before computers were really a thing. We had PCs, but they weren’t considered cool. And so I fell in love with understanding how the computer worked. And back then, things were way simpler. I started with a BASIC interpreter, for example, and you’d get a book from the store. Remember when we had books? [laughs] And you’d learn things from books?&lt;/p&gt;
    &lt;p&gt;Did you do the thing where you’d get the hobbyist magazine and copy out the listing of the program?&lt;/p&gt;
    &lt;p&gt;That’s exactly right. And so we didn’t have vibe coding, but we did have books. And so just by typing things in, you could understand how things work, and then when you broke it—because inevitably you’re typing something in and you don’t really know what you’re doing—you have to figure out what went wrong and so it encouraged a certain amount of debugging. I really love computer games. Again, back then, things were a little bit simpler. Computer games drove graphics and performance and things like this. And so I spent some time on these things called bulletin board systems and the early internet reading about how game programmers are trying to push the limits of the hardware. And so that’s where I got interested in performance and computers and systems. I went on to college and had an amazing professor at my school, shout out to University of Portland in Portland, Oregon, and he was a compiler nerd.&lt;/p&gt;
    &lt;p&gt;And so, I think that his love for compilers was infectious. His name was Steven Vegdahl, and that caused me to go on to pursue compilers at University of Illinois. And there again, continue to fall down this rabbit hole of compilers and systems, and build LLVM. And ever since I got into the compiler world, I loved it. I love compilers because they’re large-scale systems, there’s multiple different components that all work together. And in the university setting, it was really cool in the compiler class, because unlike most of the assignments where you do an assignment, turn it in, forget about it—in compilers, you would do an assignment, turn it in, get graded, and then build on it. And it felt much more realistic like software engineering, rather than just doing a project to get graded.&lt;/p&gt;
    &lt;p&gt;Yeah, I think for a lot of people, the OS class is their first real experience of doing a thing where you really are building layer on top of layer. I think it’s an incredibly important experience for people as they start engineering.&lt;/p&gt;
    &lt;p&gt;It’s also one where you get to use some of those data structures. I took this, almost academic, here’s what a binary tree is, and here’s what a graph is. And particularly when I went through it, it was taught from a very math-forward perspective, but it really made it useful. And so that was actually really cool. I’m like, ‘Oh, this is why I learned this stuff.’&lt;/p&gt;
    &lt;p&gt;So one thing that strikes me about your career is that you’ve ended up going back and forth between compiler engineering and language design space, whereas I feel like a lot of people are on one side or the other—they’re mostly compilers people and they don’t care that much about the language, and just, how do we make this thing go fast? And there are some people who are really focusing on language design and the work on the compiler is a secondary thing towards that design. And you’ve both popped back and forth. And then also a lot of your compiler engineering work, really starting with LLVM, in some sense is itself, very language-forward. With LLVM, there’s a language in there that’s this intermediate language that you’re surfacing as a tool for people to use. So I’m just curious to hear more about how you think about the back and forth between compiler engineering and language design.&lt;/p&gt;
    &lt;p&gt;The reason I do this is that effectively, my career is following my own interests. And so my interests are not static. I want to work on different kinds of problems and solve useful problems and build into things. And so the more technology and capability you have, the higher you can reach. And so with LLVM, for example, built and learned a whole bunch of cool stuff about deep code generation for an X86 chip and that category of technology with register allocation, stuff like this. But then it made it possible to go, say, let’s go tackle C++ and let’s go use this to build the world’s best implementation of something that lots more people use and understand than deep backend code generation technology. And then with Swift, it was, build even higher and say, ‘Okay, well C++, maybe some people like it, but I think we can do better and let’s reach higher.’ I’ve also been involved in AI systems, been involved in building an iPad app to help teach kids how to code. And so, lots of different things over time. And so for me, the place I think I’m most useful and where a lot of my experience is valuable ends up being at this hardware-software boundary.&lt;/p&gt;
    &lt;p&gt;I’m curious how you ended up making the leap to working on Swift. From my perspective, Swift looks from the outside, like one of these points of arrival in mainstream programming contexts of a bunch of ideas that I have long thought are really great ideas in other programming languages. And I’m curious, in some ways a step away from like, oh, I’m going to work on really low-level stuff and compiler optimization, and then we will go much higher level and do a C++ implementation, which is still a pretty low level. How did the whole Swift thing happen?&lt;/p&gt;
    &lt;p&gt;Great question. I mean, the timeframe for people that aren’t familiar is that LLVM started in 2000. So by 2005, I had exited university and I joined Apple. And so LLVM was an advanced research project at that point. By the 2010 timeframe, LLVM was much more mature and we had just shipped C++ support in Clang, and so it could bootstrap itself, which means the compiler could compile itself. It’s all written in C++, it could build advanced libraries like the Boost template library, which is super crazy advanced template stuff. And so the C++ implementation that I and the team had built was real. Now, C++ in my opinion, is not a beautiful programming language. And so implementing it is a very interesting technical challenge. For me, a lot of problem-solving ends up being, how do you factor the system the right way?&lt;/p&gt;
    &lt;p&gt;And so Clang has some really cool stuff that allowed it to scale and things like that, but I was also burned out. We had just shipped it. It was amazing. I’m like, there has to be something better. And so, Swift really came starting in 2010. It was a nights and weekends project. It wasn’t like top-down management said, ‘Let’s go build a new programming language.’ It was ‘Chris being burned out’—I was running a 20 to 40 person team at the time, being an engineer during the day, and being a technical leader, but then needing an escape hatch. And so I said, ‘Okay, well, I think we can have something better. I have a lot of good ideas. Turns out, programming languages are a mature space. It’s not like you need to invent pattern matching at this point. It’s embarrassing that C++ doesn’t have good pattern matching.&lt;/p&gt;
    &lt;p&gt;We should just pause for a second, because I think this is like a small but really essential thing. I think the single best feature coming out of language like ML in the mid-seventies is, first of all, this notion of an algebraic data type, meaning every programming language on earth has a way of saying this and that and the other, a record, or a class, or a tuple.&lt;/p&gt;
    &lt;p&gt;A weird programming language, I think it was Barbara Liskov?&lt;/p&gt;
    &lt;p&gt;Yeah. And she did a lot of the early theorizing about, ‘What are abstract data types?’ But the ability to do this or that or the other, to have data types that are a union of different possible shapes of the data—and then having this pattern matching facility that lets you basically in a reliable way do the case analysis so you can break down what the possibilities are—is just incredibly useful. And very few mainstream languages have picked it up. I mean Swift again is an example, but languages like ML, SML, and Haskell, and OCaml—&lt;/p&gt;
    &lt;p&gt;Standard!&lt;/p&gt;
    &lt;p&gt;That’s right. SML. Standard ML. It’s been there for a long time.&lt;/p&gt;
    &lt;p&gt;I mean pattern matching, it is not an exotic feature. Here we’re talking about 2010. C# didn’t have it. C++ didn’t have it. Obviously Java didn’t have it. I don’t think JavaScript had it. None of these mainstream languages had it, but it’s obvious. And so part of my opinion about that—and so by the way, I represent as engineer, I’m not actually a mathematician, and so type theory goes way over my head. I don’t really understand this. The thing that gets me frustrated about the academic approach to programming languages is that people approach it by saying there’s sum types, and there’s intersection types, and there’s these types, and they don’t start from utility forward. And so pattern matching, when I learned OCaml, it’s so beautiful. It makes it so easy and expressive to build very simple things. And so to me, I always identify to the utility and then yes, there’s amazing formal type theory behind it, and that’s great and that’s why it actually works and composes. But bringing that stuff forward and focusing on utility and the problems it solves, and how it makes people happy, ends up being the thing that I think moves the needle in terms of adoption, at least in mainstream.&lt;/p&gt;
    &lt;p&gt;Yeah, I mean I think that’s right. My approach also, and my interest in language is also very much not from the mathematical perspective, although my undergraduate degree is in math. I like math a lot, but I mostly approach these things as a practitioner. But the thing I’ve been struck by over the years is the value of having these features have a really strong mathematical foundation is they generalize, and as you were saying, compose much better. If they are in the end mathematically simple, you’re way more likely to have a feature that actually pans out as it gets used way beyond your initial view as to what the thing was for.&lt;/p&gt;
    &lt;p&gt;That’s right. This is actually a personal defect because I don’t understand the math in the way that maybe theoretically would be ideal. I end up having to rediscover certain truths that are obvious. The cliche, ‘If the Russian mathematician invented it 50 years ago…’ And so a lot of what I find is that I can find truth and beauty when things compose and things fit together, and often I’ll find out it’s already been discovered because everything in programming language has been done. There’s almost nothing novel, but still that design process of saying, let’s pull things together, let’s reason about why it doesn’t quite fit together. Let’s go figure out how to better factor this. Let’s figure out how to make it simpler these days. That process to me, I think is kind of like people working on physics, [from what] I hear. The simpler the outcome becomes, the more close to truth it feels like it is. And so I share that—and maybe it’s more design gene or engineer-design combination, but it’s probably what you mathematicians actually know inherently, and I just haven’t figured it out yet.&lt;/p&gt;
    &lt;p&gt;Do you find yourself doing things after you come to it from an engineering perspective, trying to figure out whether there are useful mathematical insights? Do you go back and read the papers? Do you have other PL people who are more mathematically oriented who you talk to? How do you extend your thinking to cover some of that other stuff?&lt;/p&gt;
    &lt;p&gt;See, the problem is math is scary to me. So I see Greek letters and I run away. I do follow arXiv and things like this, and there’s a programming language section on that. And so I get into some of it, but what I get attracted to in that is the examples and the results section and the future-looking parts of it. And so it’s not necessarily the ‘how,’ it’s the ‘what it means.’ And so I think a lot of that really speaks to me. The other thing that really speaks to me when you talk about language design and things like this is blog posts from some obscure academic programming language that I’ve never heard of. You just have somebody talking about algebraic effect systems for this and that and the other thing, or something really fancy, but they figure out how to explain it in a way that’s useful. And so when it’s not just, ‘Let me explain to you the type system,’ but it’s, ‘Let me explain this problem this fancy feature enables,’ that’s where I get excited. That’s where it speaks to me because, again, I’m problem-oriented, and having a beautiful way to express and solve problems, I appreciate.&lt;/p&gt;
    &lt;p&gt;I think there’s a lot of value in the work that’s done in papers of really working out in detail the theory and the math and how it all fits together. [And] I think the fact that the world has been filled with a lot of interesting blog posts from the same people has been great because I think it’s another modality where it often encourages you to pull out the simpler and easier-to-consume versions of those ideas. And I think that is just a different kind of insight and it’s valuable to surface that too.&lt;/p&gt;
    &lt;p&gt;And also when I look at those blog posts, sometimes they design smell. Particularly the C++ community, there’s a lot of really good work to fix C++. They’re adding a lot of stuff to it, and C++ will never get simpler—you can’t really remove things, right? And so a lot of the challenge there is, it’s constrained problem-solving. And so when I look at that, often what I’ll see when I’m reading one of those posts, and again, these are brilliant people and they’re doing God’s work trying to solve problems with C++, best of luck with that. But you look at that and you realize there’s a grain of sand in the system that didn’t need to be there. And so to me, it’s like if you remove that grain of sand, then the entire system gets relaxed and suddenly all these constraints fall away and you can get to something much simpler. Swift, for example, it’s a wonderful language and it’s grown really well and the community is amazing, but it has a few grains of sand in it that cause it to be a lot more complicated. And so this is where I’m not just happy with things that got built. LLVM is amazing, it’s very practical, but it has lots of problems. That’s why when I get a chance to build a next generation system, I want to learn from that and actually try to solve these problems.&lt;/p&gt;
    &lt;p&gt;So this is the great privilege of getting to work on a new language, which is a thing you’re doing now. There’s this new language called Mojo, and it’s being done by this company that you co-founded called Modular. Maybe just so we understand the context a little bit, can you tell me a little bit about, what is Modular? What’s the basic offering? What’s the business model?&lt;/p&gt;
    &lt;p&gt;Before I even get there, I’ll share more of how I got here. If you oversimplify my background, I did this LLVM thing and its foundational compiler technology for CPUs. It helped unite a lot of CPU-era infrastructure and it provided a platform for languages like Swift, but also Rust, and Julia, and many different systems that all got built on top of, and I think it really catalyzed and enabled a lot of really cool applications of accelerated compiler technology. People use LLVM in databases and for query engine optimization, lots of cool stuff. Maybe you use it for trading or something. I mean, there can be tons of different applications for this kind of technology—and then [I] did programming language stuff with Swift. But in the meantime, AI happened. And so with AI brought this entirely new generation of compute: GPUs, tensor processing units, large-scale AI training systems, FPGAs, and ASICs and all this complexity for compute, and LLVM never really worked in that system.&lt;/p&gt;
    &lt;p&gt;And so one of the things that I built when I was at Google was a bunch of foundational compiler technology for that category of systems. And there’s this compiler technology called MLIR. MLIR is basically LLVM 2.0. And so take everything you learn from building LLVM and helping solve this, but then bring it forward into this next generation of compiler technology so that you can go hopefully unify the world’s compute for this GPU and AI and ASIC kind of world. MLIR has been amazingly successful, and I think it’s used in roughly every one of these AI systems and GPUs. It’s used by Nvidia, it’s used by Google, it’s used by roughly everybody in this space. But one of the challenges is that there hasn’t been unification. And so you have these very large-scale AI software platforms. You have CUDA from Nvidia, you have XLA from Google, you have ROCm from AMD.&lt;/p&gt;
    &lt;p&gt;It’s countless. Every company has their own software stack. And one of the things that I discovered and encountered, and I think the entire world sees, is that there’s this incredible fragmentation driven by the fact that each of these software stacks built by a hardware maker are just all completely different. And some of them work better than others, but regardless, it’s a gigantic mess. And there’s these really cool high-level technologies like PyTorch that we all love and we want to use. But if PyTorch is built on completely different stacks and schooling together these megalithic worlds from different vendors, it’s very difficult to get something that works.&lt;/p&gt;
    &lt;p&gt;Right. They’re both complicated trade-offs around the performance that you get out of different tools and then also a different set of complicated trade-offs around how hard they are to use, how complicated it is to write something in them, and then what hardware you can target from each individual one. And each of these ecosystems is churning just incredibly fast. There’s always new hardware coming out and new vendors in new places, and there’s also new little languages popping up into existence, and it makes the whole thing pretty hard to wrangle.&lt;/p&gt;
    &lt;p&gt;Exactly. And AI is moving so fast. There’s a new model every week. It’s crazy. And new applications, new research, the amount of money being dumped into this by everybody is just incredible. And so how does anybody keep up? It’s a structural problem in the industry. And so the structural problem is that the people doing this kind of work, the people doing code generation for advanced GPUs and things like this, they’re all at hardware companies. And the hardware companies, every single one of them is building their own stack because they have to. There is nothing to plug into. There’s nothing like ‘LLVM but for AI,’ that doesn’t exist. And so as they go and build their own vertical software stack, of course they’re focused on their hardware, they got advanced roadmaps, they have a new chip coming out next year, they’re plowing their energy and time into solving for their hardware. But we, out in the industry, we actually want something else. We want to be able to have software that runs across multiple pieces of hardware. And so, if everybody doing the work is at a hardware company, it’s very natural that you get this fragmentation across vendors because nobody’s incentivized to go work together. And even if they’re incentivized, they don’t have time to go work on somebody else’s chip. AMD is not going to pay to work on Nvidia GPUs or something like this.&lt;/p&gt;
    &lt;p&gt;That’s true when you think about this, kind of, a split between low-level and high-level languages. So Nvidia has CUDA and AMD has ROCm, which is mostly a clone of CUDA, and then the XLA tools from Google work incredibly well on TPUs, and so on and so forth. Different vendors have different things. Then there’s the high-level tools, PyTorch, and JAX, and Triton, and various things like that. And those are typically actually not made by the hardware vendors. Those are made by different kinds of users—I guess Google is responsible for some of these and they’re also sometimes a hardware vendor—but a lot of the time it’s more stepped back. Although even there, the cross-platform support is complicated and messy and incomplete.&lt;/p&gt;
    &lt;p&gt;Because they’re built on top of fundamentally incompatible things. And so that’s the fundamental nature. And so again, you go back to Chris’s dysfunction and my weird career choices, I always end up back at the hardware-software boundary, and there’s a lot of other folks that are really good at adding very high-level abstractions. If you go back a few years ago, MLOps was the cool thing, and it was, ‘Let’s build a layer of Python on top of TensorFlow and PyTorch and build a unified AI platform.’ But the problem with that, is that building abstractions on top of two things that don’t work very well, can’t solve performance, or liability, or management, or these other problems. You can only add a layer of duct tape, but as soon as something goes wrong, you end up having to debug this entire crazy stack of stuff that you really didn’t want to have to know about.&lt;/p&gt;
    &lt;p&gt;And so it’s a leaky abstraction. And so the genesis of Modular (bringing it back to this) was realizing there are structural problems in the industry. There is nobody that’s incentivized to go build a unifying software platform and do that work at the bottom level. And so what we set off to do is we said, ‘Okay, let’s go build…’—and there’s different ways of explaining this. You could say ‘a replacement for CUDA,’ that’s like a flamboyant way to say this, but ‘let’s go build a successor to all of this technology that is better than what the hardware makers are building, and is portable.’ And so what this takes, is doing the work that these hardware companies are doing, and I set the goal for the team of saying, let’s do it better than, for example, Nvidia is doing it for their own hardware.&lt;/p&gt;
    &lt;p&gt;Which is no easy feat, right? They’ve got a lot of very strong engineers and they understand their hardware better than anyone does. Beating them on their own hardware is tough.&lt;/p&gt;
    &lt;p&gt;That is really hard. And they’ve got a 20-year head start, because CUDA is about 20 years old. They’ve got all the momentum. They’re a pretty big company. As you say, lots of smart people. And so that was a ridiculous goal. Why did I do that? Well, I mean a certain amount of confidence in understanding how the technology worked, having a bet on what I thought we could build and the approach, and some insight and intuition, but also realizing that it’s actually destiny. Somebody has to do this work. If we ever want to get to an ecosystem where one vendor doesn’t control everything, if we want to get the best out of the hardware, if we want to get new programming language technologies, if we want pattern matching on a GPU—I mean, come on, this isn’t rocket science—then we need at some point to do this. And if nobody else is going to do it, I’ll step up and do that. That’s where Modular came from—saying, ‘Let’s go crack this thing open. I don’t know how long it will take, but sometimes it’s worthwhile doing really hard things if they’re valuable to the world.’ And the belief was it could be profoundly impactful and hopefully get more people into even just being able to use this new form of compute with GPUs and accelerators and all this stuff, and just really redemocratize AI compute.&lt;/p&gt;
    &lt;p&gt;So you pointed out that there’s a real structural problem here, and I’m actually wondering how, at a business model level, do you want to solve the structural problem? Which is, the history of computing is these days littered with the bodies of companies that try to sell a programming language. It’s a really hard business. How is Modular set up so that it’s incented to build this platform in a way that can be a shared platform that isn’t subject to just one other vendor’s lock-in?&lt;/p&gt;
    &lt;p&gt;First answer is, don’t sell a programming language. As you say, that’s very difficult. So we’re not doing that. Go take Mojo, go use it for free. We’re not selling a programming language. What we’re doing is we’re investing in this foundational technology to unify hardware. Our view is, as we’ve seen in many other domains, once you fix the foundation, now you can build high-value services for enterprises. And so our enterprise layer, often what we talk to, you end up with these groups where you have hundreds or thousands of GPUs. Often it’s rented from a cloud on a three-year commit. You have a platform team that’s carrying pagers and they need to keep all this stuff running and all the production workloads running. And then you have these product teams that are inventing new stuff all the time, and there’s new research, there’s a new model that comes out and they want to get it on the production infrastructure, but none of this stuff actually works.&lt;/p&gt;
    &lt;p&gt;And so the software ecosystem we have with all these brilliant but crazy open source tools that are thrashing around, all these different versions of CUDA and libraries, all this different hardware happening, is just a gigantic mess. And so, helping solve this for the platform engineering team that actually needs to have stuff work, and want to be able to reason about it, and want good observability and manageability and scalability and things like this is actually, we think, very interesting. We’ve gotten a lot of good responses from people on that. The cost of doing this is we want to actually make it work, that’s where we do fundamental language compiler underlying systems technology and help bring together these accelerators so that we can get, for example, the best performance on an AMD GPU and get it so that the software comes out in the same release train as support for an Nvidia GPU. And being able to pull that together, again, it just multiplicatively reduces complexity, which then leads to a product that actually works, which is really cool and very novel in AI.&lt;/p&gt;
    &lt;p&gt;So the way that Mojo plays in here, is it basically lets you provide the best possible performance and I guess the best possible performance across multiple different hardware platforms. Are you primarily thinking about this as an inference platform, or, how does the training world fit in?&lt;/p&gt;
    &lt;p&gt;So let me zoom in and I’ll explain our technology components. I have a blog post series I encourage you and any viewers or listeners to check out, called, ‘Democratizing AI Compute.’ It goes through the history of all the systems and the problems and challenges that they’ve run into, and it gets to, ‘What is Modular doing about it?’ So Part 11 talks about our architecture and the inside is Mojo, which is a programming language. I’ll explain Mojo in a second. Next level out is called MAX. And so you can think of MAX as being a PyTorch replacement or a vLLM replacement, something that you can run on a single node and then get high performance LLM surveying, that kind of use case. And then the next level out is called Mammoth, and this is the cluster management Kubernetes layer. And so if you zoom in all the way back to Mojo, you say—your experience, you know what programming languages are, they’re incredibly difficult and expensive to build.&lt;/p&gt;
    &lt;p&gt;Why would you do that in the first place? And the answer is, we had to. In fact, when we started Modular, I was like, ‘I’m not going to invent a programming language.’ I know that’s a bad idea, it takes too long, it’s too much work. You can’t convince people to adopt a new language. I know all the reasons why creating language is actually a really bad idea. But it turns out, we were forced to do this because there is no good way to solve the problem. And the problem is, how do you write code that is portable across accelerators? So, that problem, I want portability across—for example, make it simple AMD and Nvidia GPUs, but then you layer on the fact that you’re using a GPU because you want performance. And so I don’t want a simplified, watered down—I want Java that runs on a GPU.&lt;/p&gt;
    &lt;p&gt;I want the full power of the GPU. I want to be able to deliver performance that meets and beats Nvidia on their own hardware. I want to have portability and unify this crazy compute where you have these really fancy heterogeneous systems and you have tensor cores and you have this explosion of complexity and innovation happening in this hardware platform layer. Most programming languages don’t even know that there’s an 8-bit floating point that exists. And so we looked around and I really did not want to have to do this, but it turns out that there really is no good answer. And again, we decided that, hey, the stakes are high, we want to do something impactful. We’re willing to invest. I know what it takes to build a programming language. It’s not rocket science, it’s just a lot of really hard work and you need to set the team up to be incentivized the right way. But we decided that, yeah, let’s do that.&lt;/p&gt;
    &lt;p&gt;So I want to talk more about Mojo and its design, but before we do, maybe let’s talk a little bit more about the pre-existing environment. I did actually read that blog post series. I recommended it to everyone. I think it’s really great, and I want to talk a little bit about what the existing ecosystem of languages looks like, but even before then, can we talk more about the hardware? What does the space of hardware look like that people want to run these ML models on?&lt;/p&gt;
    &lt;p&gt;Yeah, so the one that most people zero in on is the GPU. And so GPUs are, I think, getting better understood now. And so if you go back before that though, you have CPUs. So, modern CPUs in a data center, often you’ll have—I mean today you guys are probably riding quite big iron, but you got 100 cores in a CPU and you got a server with two-to-four CPUs on a motherboard, and then you go and you scale that. And so, you’ve got traditional threaded workloads that have to run on CPUs, and we know how to scale that for internet servers and things like this. If you get to a GPU, the architecture shifts. And so they have basically these things called SMs. And now the programming model is that you have effectively much more medium-sized compute that’s now put together on much higher performance memory fabrics and the programming model shifts. And one of the things that really broke CUDA, for example, was when GPUs got this thing called a tensor core—and the way to think about a tensor core is it’s a dedicated piece of hardware for matrix multiplication. And so, why’d we get that? Well, a lot of AI is matrix multiplication. And so, if you design the hardware to be good at a specific workload, you can have dedicated silicon for that and you can make things go really fast.&lt;/p&gt;
    &lt;p&gt;There are really these two quite different models sitting inside of the GPU space. Of course, the name itself is weird. GPU is ‘graphics processing unit,’ which is what they were originally for. And then this SM model is really interesting. They have this notion of a warp. A warp is a collection of typically 32 threads that are operating together in lockstep, always doing the same thing—a slight variation on what’s called the SIMD model, same instruction, multiple data. It’s a little more general than that, but more or less, you can think of it as the same thing. And you just have to run a lot of them. And then there’s a ton of hardware inside of these systems basically to make switching between threads incredibly cheap. So you pay a lot of silicon to add extra registers. So the context switch is super cheap, so you can do a ton of stuff in parallel.&lt;/p&gt;
    &lt;p&gt;Each thing you’re doing is itself 32-wise parallel. And then because you can do all this very fast context switching, you can hide a lot of latency. And that worked for a while. And then we’re like, actually, we need way more of this matrix multiplication stuff. And you can sort of do reasonably efficient matrix multiplication through this warp model, but not really that good. And then there’s a bunch of quite idiosyncratic hardware, which changes its performing characteristics from generation to generation, just for doing these matrix multiplications. So that’s the Nvidia GPU story, and Volta is like V100 and A100 and H100. They just keep on going and changing, pretty materially from generation to generation in terms of the performance characteristics, and then also the memory model, which keeps on changing.&lt;/p&gt;
    &lt;p&gt;You go back to intuition, CUDA was never designed for this world. CUDA was not designed for modern GPUs. It was designed for a much simpler world. And CUDA being 20 years old, it hasn’t really caught up. And it’s very difficult because, as you say, the hardware keeps changing. And so CUDA was designed from a world where—almost like C is designed for a very simple programming model that it expected to scale, but then as the hardware changed, it couldn’t adapt. Now, if you get beyond GPUs, you get to Google TPU and many other dedicated AI systems. They blow this way out and they say, ‘Okay, well, let’s get rid of the threads that you have on a GPU and let’s just have matrix multiplication units and have really big matrix multiplication units and build the entire chip around that. And you get much more specialization, but you get a much higher throughput for those AI workloads.&lt;/p&gt;
    &lt;p&gt;Going back to, ‘Why Mojo?’ Well, Mojo was designed from first principles to support this kind of system. Each of these chips, as you’re saying, even within Nvidia’s family, from Volta, to Ampere, to Hopper, to Blackwell, these things are not compatible with each other. Actually, Blackwell just broke compatibility with Hopper, so it can’t run Hopper kernels always on Blackwell. Oops, well, why are they doing that? Well, AI software is moving so fast. They decided that was the right trade-off to make. And meanwhile, we all software people need the ability to target this. When you look at other existing systems, with Triton for example, their goal was, ‘Let’s make it easier to program a GPU,’ which I love, that’s awesome. But then they said, ‘We’ll just give up 20% of the performance of the silicon to do it.’ Wait a second. I want all the performance. And so if I’m using a GPU—GPUs are quite expensive by the way—&lt;/p&gt;
    &lt;p&gt;I want all the performance. And if it’s not going to be able to deliver the same quality of results you get by writing CUDA, well then, you’re always going to run to this head room, where you get going quickly, but then you run into a ceiling and then have to switch to a different system to get full performance. And so this is where Mojo is really trying to solve this problem where we can get more usability, more portability, and full performance of the silicon because it’s designed for these wacky architectures like tensor cores.&lt;/p&gt;
    &lt;p&gt;And if we look at the other languages that are out there, there’s languages like CUDA, and OpenCL, which are low level, typically look like variations on C++, in that tradition are unsafe languages, which means that there’s a lot of rules you have to follow. And if you don’t exactly follow the rules, you’re in undefined behavior land, it’s very hard to reason about your program.&lt;/p&gt;
    &lt;p&gt;And just let me make fun of my C++ heritage because I’ve spent so many years, like, you just have a variable that you forget to initialize, it just shoots your foot off. [laughs] Like, it’s just unnecessary violence to programmers.&lt;/p&gt;
    &lt;p&gt;Right. And it’s done in the interest of making performance better because the idea is C++ and its related languages don’t really give you enough information to know when you’re making a mistake, and they want to have as much space as they can to optimize the programs they get. So the stance is just, if you do anything that’s not allowed, we have no obligation to maintain any kind of reasonable semantics or debug ability around that behavior. And we’re just going to try really, really hard to optimize correct programs, which is a super weird stance to take, because nobody’s programs are correct. There are bugs and undefined behavior in almost any C++ program of any size. And so, you’re in a very strange position in terms of the guarantees that you get from the compiler system you’re using.&lt;/p&gt;
    &lt;p&gt;Well, so I mean, I can be dissatisfied. I can also be sympathetic with people that work on C++. So again, I’ve spent decades in this language and around this ecosystem, and building compilers for it. I know quite a lot about it. The challenge is that C++ is established, and so there’s tons of code out there. By far, the code that’s already written is the code that’s the most valuable. And so if you’re building a compiler, or you have a new chip, or you have an optimizer, your goal is to get value out of the existing software. And so you can’t invent a new programming paradigm that’s a better way of doing things and defines away the problem. Instead, you have to work with what you’ve got. You have a SPEC benchmark you’re trying to make go fast, and so you invent some crazy heroic hack that makes some important benchmark work because you can’t go change the code.&lt;/p&gt;
    &lt;p&gt;In my experience, particularly for AI, but also I’m sure within Jane Street, if something’s going slow, go change the code. You have control over the architecture of the system. And so, what I think the world really benefits from, unlike benchmark hacking, is languages that give control and power and expressivity to the programmer. And this is something where I think that, again, you take a step back and you realize history is the way it is for lots of structural and very valid reasons, but the reasons don’t apply to this new age of compute. Nobody has a workload that they can pull forward to next year’s GPU—doesn’t exist. Nobody solved this problem. I don’t know the timeframe, but once we solve that problem, once we solve portability, you can start this new era of software that can actually go forward. And so now, to me, the burden is—make sure it’s actually good. And so, to your point about memory safety, don’t make it so that forgetting to initialize a variable is just going to shoot your foot off. [Instead] produce a good compiler error saying, ‘Hey, you forgot to initialize a variable,’ right? These basic things are actually really profound and important, and the tooling and all this usability and this DNA, these feelings and thoughts, are what flow into Mojo.&lt;/p&gt;
    &lt;p&gt;And GPU programming is just a very different world from traditional CPU programming just in terms of the basic economics and how humans are involved. You end up dealing with much smaller programs. You have these very small but very high-value programs whose performance is super critical, and in the end, a relatively small coterie of experts who end up programming in it. And so it pushes you ever in the direction, you’re saying, of performance engineering, right? You want to give people the control they need to make the thing behave as it should, and you want to do it in a way that allows people to be highly productive. And the idea that you have an enormous amount of legacy code that you need to bring over, it’s like, actually you kind of don’t. The entire universe of software is actually shockingly small, and it’s really about how to write these small programs as well as possible.&lt;/p&gt;
    &lt;p&gt;And also there’s another huge change. And so this is something that I don’t think that the programming language community has recognized yet, but AI coding has massively changed the game because now you can take a CUDA kernel and say, ‘Hey, Claude, go make that into Mojo.’&lt;/p&gt;
    &lt;p&gt;And actually, how good have you guys found the experience of that? Of doing translation?&lt;/p&gt;
    &lt;p&gt;Well, we do hackathons and people do amazing things, having never touched Mojo, having never done GPU programming, and within a day they can make things happen that are just shocking. Now, AI coding tools are not magic. You cannot just vibe code DeepSeek-R1 or something, right? But it’s amazing what that can do in terms of learning new languages, learning new tools, and getting into and catalyzing ecosystems. And so this is one of the things where, again, you go back five or 10 years—everybody knows nobody can learn a new language, and nobody’s willing to adopt new things. But the entire system has changed.&lt;/p&gt;
    &lt;p&gt;So let’s talk a little bit more in detail about the architecture of Mojo. What kind of language is Mojo, and what are the design elements that you chose in order to make it be able to address this set of problems?&lt;/p&gt;
    &lt;p&gt;Yeah, again, just to relate how different the situation is—back when I was working on Swift, one of the major problems to solve was, objective C was very difficult for people to use, and you had pointers, and you had square brackets, and it was very weird. And so the goal in the game of the day was, invent new syntax and bring together modern programming language features to build a new language. Fast forward to today, actually, some of that is true. AI people don’t like C++. C++ has pointers, and it’s ugly, and it’s a 40-year-old-plus language, and has actually the same problem that Swift had to solve back in the day. But today there’s something different, which is that AI people do actually love a thing. It’s called Python. And so, one of the really important things about Mojo is, it’s a member of the Python family. And so, this is polarizing to some, because yes—I get it that some people love curly braces, but it’s hugely powerful because so much of the AI community is Pythonic already.&lt;/p&gt;
    &lt;p&gt;And so we started out by saying, let’s keep the syntax like Python and only diverge from that if there’s a really good reason. But then what are the good reasons? Well, the good reasons are, we want—as we were talking about—performance, power, full control over the system. And for GPUs, there’s these very important things you want to do that require metaprogramming. And so Mojo has a very fancy metaprogramming system, kind of inspired by this language called Zig, that brings runtime and compile time together to enable really powerful library designs. And the way you crack open this problem with tensor cores and things like this, is you enable really powerful libraries to be built in the language as libraries, instead of hard coding into the compiler.&lt;/p&gt;
    &lt;p&gt;Let’s take it a little bit to the metaprogramming idea. What is metaprogramming and why does it matter for performance in particular?&lt;/p&gt;
    &lt;p&gt;Yeah, it’s a great question, and I think you know the answer to this too, and I know you, but—&lt;/p&gt;
    &lt;p&gt;[Laughs] We are also working on metaprogramming features in our own world.&lt;/p&gt;
    &lt;p&gt;Exactly. And so the observation here is, when you’re writing a for loop in a programming language, for example, typically that for loop executes at runtime, so you’re writing code that when you execute the program, it’s the instructions that the computer will follow to execute the algorithm within your code. But when you get into designing higher level type systems, suddenly you want to be able to run code at compile time as well. And so there’s many languages out there. Some of them have macro systems, C++ has templates. What you end up getting is, you end up getting, in many languages, this duality between what happens at runtime, and then a different language almost that happens at compile time. And C++ is the most egregious, because templates that you have a for loop in runtime, but then you have unrolled recursive templates, or something like that at compile time.&lt;/p&gt;
    &lt;p&gt;Well, so the insight is, hey, these two problems are actually the same. They just run at different times. And so what Mojo does is says, let’s allow the use of effectively any code that you would use at runtime to also work at compile time. And so you can have a list, or a string, or whatever you want in the algorithms—go do memory allocation, deallocation—and you can run those at compile time, enabling you to build really powerful high-level abstractions and put them into libraries. So why is this cool? Well, the reason it’s cool is that on a GPU, for example, you’ll have a tensor core. Tensor cores are weird. We probably don’t need to deep dive into all the reasons why, but the indexing and the layout that tensor cores use is very specific and very vendor different. And so the tensor core you have on AMD, or the tensor cores you have on different versions of Nvidia GPUs are all very different.&lt;/p&gt;
    &lt;p&gt;And so what you want, is you want to build as a GP programmer a set of abstractions so you can reason about all of these things in one common ecosystem and have the layouts much higher level. And so what this enables, it enables very powerful libraries—and very powerful libraries where a lot of the logic is actually done at compile time, but you can debug it because it’s the same language that you use at runtime. And it makes the language much more simpler, much more powerful, and just be able to scale into these complexities in a way that’s possible with C++. But in C++, you get some crazy template stack trace that is maddening and impossible to understand. In Mojo, you can get a very simple error message. You can actually debug your code, and debugger things like this.&lt;/p&gt;
    &lt;p&gt;So maybe an important point here is that metaprogramming is really an old solution to this performance problem. Maybe a good way of thinking about this is, imagine you have some piece of data that you have that represents a little embedded domain-specific language that you’ve written, that you want to execute via a program that you wrote. You can, in a nice high-level way, write a little interpreter for that language that just—you know, I have maybe a Boolean expression language or who knows what else. Maybe it’s a language for computing on tensors in a GPU. And you could write a program that just executes that mini domain-specific language and does the thing that you want and you can do it, but it’s really slow. Writing an interpreter is just inherently slow because of all this interpretation overhead where you are dynamically making decisions about what the behavior of the program is. And sometimes what you want, is, you just want to actually emit exactly the code that you want and boil away the control structure and just get the direct lines of machine code that you want to do the thing that’s necessary.&lt;/p&gt;
    &lt;p&gt;And various forms of code generation let you get past in a simpler way, lets you get past all of this control structure that you have to execute at runtime and instead be able to execute it at compile time and get this minified program that just does exactly the thing that you want. So that’s a really old idea. It goes back to all sorts of programming languages. There’s a lot of Lisps that did a lot of this metaprogramming stuff, but then the problem is this stuff is super hard to think about and reason about and debug. And that’s certainly true if you think about in C, all this macro language, if you use the various C preprocessors to do this kind of stuff in C, it’s pretty painful to reason about. And then C++ made it richer and more expressive, but still really hard to reason about. And you write a C++ template and you don’t really know what it’s going to do or if it’s going to compile until you give it all the inputs and let it go and it—&lt;/p&gt;
    &lt;p&gt;Feels good in the simple case. But then when you get to more advanced cases, suddenly the complexity compounds and it gets out of hand.&lt;/p&gt;
    &lt;p&gt;And it sounds like the thing that you’re going for in Mojo is it feels like one language. It has one type system that covers both the stuff you’re generating statically and the stuff that you’re doing at runtime. It sounds like debugging works in the same way across both of these layers, but you still get the actual runtime behavior you want from a language that you could more explicitly just be like, here’s exactly the code that I want to generate.&lt;/p&gt;
    &lt;p&gt;[…] metaprogramming is one of the fancy features. One of the cool features is it feels and looks like Python, but with actual types.&lt;/p&gt;
    &lt;p&gt;Right.&lt;/p&gt;
    &lt;p&gt;And let’s not forget the basics. Having something that looks and feels like Python but it’s a thousand times faster or something is actually pretty cool. For example, if you’re on a CPU, you have access to SIMD, the SIMD registers that allow you to do multiple operations at a time and [to] be able to get the full power of your hardware even without using the fancy features is also really cool. And so the challenge with any of these systems is, how do you make something that’s powerful, but it’s also easy to use? I think your team’s been playing with Mojo and doing some cool stuff. I mean, what have you seen and what’s your experience been?&lt;/p&gt;
    &lt;p&gt;We’re all still pretty new to it, but I think it’s got a lot of exciting things going for it. I mean, the first thing is, yeah, it gives you the kind of programming model you want to get the performance that you need. And actually, in many ways the same kind of programming model that you get out of something like CUTLASS or CuTe DSL, which are these Nvidia-specific, some at the C++ level, some at the Python DSL level—and by the way, every tool you can imagine nowadays is done once in C++ and once in Python. We don’t need to implement programming languages in any other way anymore. They’re all either skins on C++ or skins on Python. But depending on which path you go down, whether you go the C++ path or the Python path, you get all sorts of complicated trade-offs.&lt;/p&gt;
    &lt;p&gt;Like in the C++ path in particular, you get very painful compilation times. The thing you said about template metaprogramming is absolutely true. The error messages are super bad. If you look at these more Python-embedded DSLs, the compile times tend to be better. It still can be hard to reason about though. One nice thing about Mojo is the overall discipline seems very explicit when you want to understand: Is this a value that’s happening at execution time at the end, or is it a value that is going to be dealt with at compile time? It’s just very explicit in the syntax, you can look and understand. Whereas in some of these DSLs, you have to actively go and poke the value and ask it what kind of value it is. And I think that kind of explicitness is actually really important for performance engineering, making it easy to understand just what precisely you’re doing.&lt;/p&gt;
    &lt;p&gt;You actually see this a ton, not even with these very low-level things, but if you look at PyTorch, which is a much higher level tool, PyTorch does this thing where you get to write a thing that looks like an ordinary Python program, but really it’s got a much trickier execution model. Python’s an amazing and terrible ecosystem in which to do this kind of stuff, because what guarantees do you have when you’re using Python? None. What can you do? Anything. You have an enormous amount of freedom. The PyTorch people in particular have leveraged this freedom in a bunch of very clever ways, where you can write a Python program that looks like it’s doing something very simple and straightforward that would be really slow, but no—it’s very carefully delaying and making some operations lazy so it can overlap compute on the GPU and CPU and make stuff go really fast. And that’s really nice, except sometimes it just doesn’t work.&lt;/p&gt;
    &lt;p&gt;This is the trap again, this is my decades of battle scars now. So as a compiler guy, I can make fun of other compiler people. There’s this trap and it’s an attractive trap, which is called the ‘sufficiently smart compiler.’ And so what you can do is you can take something and you can make it look good on a demo and you can say, ‘Look! I make it super easy and I’m going to make my compiler super smart, and it’s going to take care of all this and make it easy through magic.’ But magic doesn’t exist. And so anytime you have one of those ‘sufficiently smart compilers,’ if you go back in the days, it was like auto-parallelization, just write C code is sequential logic, and then we’re going to automatically map it into running on 100 cores on a supercomputer or something like that.&lt;/p&gt;
    &lt;p&gt;They often actually do work, they work in very simple cases and they work in the demos. But the problem is that you go and you’re using them and then you change one thing and suddenly everything breaks. Maybe the compiler crashes, it just doesn’t work. Or you go and fix a bug and now instead of 100-times speedup, you get 100-times slowdown because it foiled the compiler. A lot of AI tools, a lot of these systems, particularly these DSLs, have this design point of, let me pretend like it’s easy and then I will take care of it behind the scenes. But then when something breaks, you have to end up looking at compiler dumps, right? And this is because magic doesn’t exist. And so this is where predictability and control is really, I think, the name of the game, particularly if you want to get the most out of a piece of hardware, which is how we ended up here.&lt;/p&gt;
    &lt;p&gt;It’s funny, the same issue of, “How clever is the underlying system you’re using?” comes up when you look at the difference between CPUs and GPUs. CPUs themselves are trying to do a weird thing where a chip is a fundamentally parallel substrate. It’s got all of these circuits that in principle could be running in parallel and then it is yoked to running this extremely sequential programming language, which is just trying to do one thing after another. And then how does that actually work with any reasonable efficiency? Well, there’s all sorts of clever dirty tricks happening under the covers where it’s trying to predict what you’re going to do, this speculation that allows it to dispatch multiple instructions in a row by guessing what you’re going to do in the future. There’s things like memory prefetching where it has heuristics to estimate what memory you’re going to ask in the future so it can dispatch multiple memory requests at the same time.&lt;/p&gt;
    &lt;p&gt;And then if you look at things like GPUs, and I think even more, TPUs, and then also totally other things like FPGAs, the field-programmable gate arrays where you put basically a circuit design on it. It’s a very different kind of software system. But all of them are in some sense simpler and more deterministic and more explicitly parallel. Like when you write down your program, you have to write an explicitly parallel program—that’s actually harder to write. I don’t want to complain too much about CPUs. The great thing about CPUs is they’re extremely flexible and incredibly easy to use and all of that dark magic actually works a pretty large fraction of the time.&lt;/p&gt;
    &lt;p&gt;Yeah, remarkably well. But your point here, I think it’s really great, and what you’re saying is, you’re saying CPUs are the magic box that makes sequential code go in parallel pretty fast. And then we have new, more explicit machines, somewhat harder to program because they’re not a magic box, but you get something from it. You get performance and power because that magic box doesn’t come without a cost. It comes with a very significant cost, often the amount of power that your machine dissipates. And so it’s not efficient. And so a lot of the reasons we’re getting these new accelerators is because people really do care about it being a hundred times faster, or using way less power, or things like this. And I’d never thought about it, but your analogy of Triton to Mojo kind of follows a similar pattern, right? Triton is trying to be the magic box, and it doesn’t give you the full performance, and it burns more power, and all that kind of stuff. And so Mojo is saying, look, let’s go back to being simple. Let’s give the programmer more control. And that more explicit approach, I think, is a good fit for people that are building crazy advanced hardware like you’re talking about—but also people that want to get the best performance out of the existing hardware we have.&lt;/p&gt;
    &lt;p&gt;So we talked about how metaprogramming lets you write faster programs by boiling away this control structure that you don’t really need. So that part’s good. How does it give you portable performance? How does it help you on the portability front?&lt;/p&gt;
    &lt;p&gt;Yeah, so this is another great question. So in this category of ‘sufficiently smart compilers,’ and particularly for AI compilers, there’s been years of work and MLIR has catalyzed a lot of this work building these magic AI compilers that take TensorFlow or even the new PyTorch stuff and trying to generate optimal code for some chip. So take some PyTorch model and put it through a compiler, and magically get out high performance. And so there’s tons of these things, and there’s a lot of great work done here, and a lot of people have shown that you can take kernels and accelerate them with compilers. The challenge with this is that people don’t ever measure—what is the full performance of the chip? And so people always measure from a somewhat unfortunate baseline and then try to climb higher instead of saying—what is the speed of light? And so if you measure from speed of light, suddenly you say, okay, how do I achieve several different things?&lt;/p&gt;
    &lt;p&gt;Even if you zero into one piece of silicon, how do I achieve the best performance for one use case? And then how do I make it so the software I write can generalize even within the domain? And so for example, take a matrix multiplication, well, you want to work on maybe float32, but then you want to generalize it to float16. Okay, well, templates and things like this are easy ways to do this. Then programming allows you to say, okay, I will tackle that. And then the next thing that happens is, because you went from float32 to float16, your effective cache size has doubled, because twice as many elements fit into cache if there’s 16 bits than if there are 32 bits. Well, if that’s the case, now suddenly the access pattern needs to change. And so you get a whole bunch of this conditional logic that now changes in a very parametric way as a result of one simple change that happened with float32 to float16.&lt;/p&gt;
    &lt;p&gt;Now you play that forward and you say, okay, well actually matrix multiplication is a recursive hierarchical problem. There’s specializations for tall and skinny matrices, and a dimension is one or something. There’s all these special cases. Just one algorithm for one chip becomes this very complicated subsystem that you end up wanting to do a lot of transformations to so you can go specialize it for different use cases. And so Mojo with the metaprogramming allows you to tackle that. Now you bring in other hardware, and so think of matrix multiplication these days as being almost an operating system, and there’s so many different subsystems, and special cases, and different D types, and crazy float4 and six and other stuff going on.&lt;/p&gt;
    &lt;p&gt;At some point they’re going to come out with a floating point number so small that it will be a joke. But every time I think that they’re just kidding, it turns out it’s real.&lt;/p&gt;
    &lt;p&gt;Seriously, I heard somebody talking about 1.2-bit floating point, right? It’s exactly like you’re saying, is that a joke? You can’t be serious. And so now when you bring in other hardware, other hardware brings in more complexity because suddenly the tensor core has a different layout in AMD than it does on Nvidia. Or maybe to your point about warps, you have 64 threads in a warp on one and 32 threads in a warp on the other. But what you realize is, wait a second—this really has nothing to do with hardware vendors. This is actually true even within, for example, the Nvidia line, because across these different data types, the tensor cores are changing. The way the tensor core works for float32 is different from the way it works for float4 or something. And so you already—within one vendor—have to have this very powerful metaprogramming to be able to handle the complexity and do so in the scaffolding of a single algorithm like matrix multiplication.&lt;/p&gt;
    &lt;p&gt;And so now as you bring in other vendors, well it turns out hey, they all have things that look roughly like tensor cores. And so we’re coming at this with a software engineering perspective, and so we’re forced to build abstractions. We have this powerful metaprogramming system so we can actually achieve this. And so even for one vendor, we get this thing called LayoutTensor. LayoutTensor is saying, okay, well I have the ability to reason about not just an array of numbers or a multidimensional array of numbers, but also how it’s laid out in memory and how it gets accessed. And so now we can declaratively map these things onto the hardware that you have and these abstractions stack. And so it’s this really amazing triumvirate between having a type system that works well and this very important basis. I know you’re a fan of type systems also.&lt;/p&gt;
    &lt;p&gt;You then bring in metaprogramming, and so you can build powerful abstractions and run a compile time so you get no runtime overhead. And then you bring in the most important part of this entire equation, which is programmers who understand the domain. I am not going to write a fast matrix multiplication. I’m sorry, that’s not my experience. But there are people in that space that are just fricking brilliant. They understand exactly how the hardware works, they understand the use cases and the latest research and the new crazy quantized format of the day, but they’re not compiler people. And so the magic of Mojo is it says, ‘Hey, you have a type system, you have metaprogramming, you have effectively the full power of a compiler that you have when you’re building libraries.’ And so now these people that are brilliant at unlocking the power of the hardware can actually do this. And now they can write software that scales both across the complexity of the domain but also across hardware. And to me, that’s what I find so exciting and so powerful about this. It’s unlocking the power of the Mojo programmer instead of trying to put it into the compiler, which is what a lot of earlier systems have tried to do.&lt;/p&gt;
    &lt;p&gt;So maybe the key point here is that you get to build these abstractions that allow you to represent different kinds of hardware, and then you can conditionally have your code execute based on the kind of hardware that it’s on. It’s not like an #ifdef where you’re picking between different hardware platforms. There are complicated data structures like these layout values that tell you how you traverse data.&lt;/p&gt;
    &lt;p&gt;Which is kind of a tree. This isn’t just a simple int that you’re passing around. This is like a recursive hierarchical tree that you need at compile time.&lt;/p&gt;
    &lt;p&gt;The critical thing is you get to write a thing that feels like one synthetic program with one understandable behavior, but then parts of it are actually going to execute at compile time, so that the thing that you generate is in fact specialized for the particular platform that you’re going to run it on. So one concern I have over this is it sounds like the configuration space of your programs is going to be massive, and I feel like there are two directions where this seems potentially hard to do from an engineering perspective. One is, can you really create abstractions that within the context of the program hide the relevant complexity? So it’s possible for people to think in a modular way about the program they’re building, so their brains don’t explode with the 70 different kinds of hardware that they might be running it on. And then the other question is, how do you think about testing? Because there’s just so many configurations. How do you know whether it’s working in all the places? Because it sounds like it has an enormous amount of freedom to do different things, including wrong things in some cases. How do you deal with those two problems, both controlling the complexity of the abstractions and then having a testing story that works out?&lt;/p&gt;
    &lt;p&gt;Okay, Ron, I’m going to blow your mind. I know you’re going to be resistant to this, but let me convince you that types are cool.&lt;/p&gt;
    &lt;p&gt;Okay!&lt;/p&gt;
    &lt;p&gt;I know you’re going to fight me on this. Well, so this is again, you go back to the challenges and opportunities of working with either Python or C++. Python doesn’t have types really. I mean it has some stuff, but it doesn’t really have a type system. C++ has a type system, but it’s just incredibly painful to work with. And so what Mojo does is it says, again, it’s not rocket science. We see it all around us. Let’s bring in traits. Let’s bring in a reasonable way to write code so that we can build abstractions that are domain-specific and they can be checked modularly. And so one of the big problems with C++ is that you get error messages when you instantiate layers and layers and layers and layers of templates. And so if you get some magic number wrong, it explodes spectacularly in a way that you can’t reason about. And so what Mojo does, it says, cool, let’s bring in traits that feel very much like protocols in Swift, or traits in Rust, or type classes in Haskell. Like, this isn’t novel.&lt;/p&gt;
    &lt;p&gt;This is like a mechanism for what’s called ad hoc polymorphism, meaning I want to have some operation or function that has some meaning, but actually it’s going to get implemented in different ways for different types. And these are basically all mechanisms of a way of, given the thing that you’re doing and the types involved, looking up the right implementation that’s going to do the thing that you want.&lt;/p&gt;
    &lt;p&gt;Yeah, I mean a very simple case is an iterator. So Mojo has an iterator trait and you can say, ‘Hey, what is an iterator over a collection?’ Well, you can either check, see if there’s an element, or you can get the value at the current element. And then as you keep pulling things out of an iterator, it will eventually decide to stop. And so this concept can be applied to things like a linked list, or an array, or a dictionary, or an unbounded sequence of packets coming off a network. And so you can write code that’s generic across these different—call them “backends” or “models”—that implement this trait. And what the compiler will do for you is it will check to make sure when you’re writing that generic code, you’re not using something that won’t work. And so what that does, is it means that you can check the generic code without having to instantiate it, which is good for compile time. It’s good for user experience, because if you get something wrong as a programmer, that’s important. It’s good for reasoning about the modularity of these different subsystems, because now you have an interface that connects the two components.&lt;/p&gt;
    &lt;p&gt;I think it’s an underappreciated problem with the C++ templates approach to the world, where C++ templates seem like a deep language feature, but really they’re just a code generation feature.&lt;/p&gt;
    &lt;p&gt;They’re like C macros.&lt;/p&gt;
    &lt;p&gt;That’s right. It both means they’re hard to think about and reason about because it sort of seems at first glance not to be so bad—this property that you don’t really know when your template expands, if it’s actually going to compile. But as you start composing things more deeply, it gets worse and worse because something somewhere is going to fail, and it’s just going to be hard to reason about and understand. Whereas when you have type-level notions of genericity that are guaranteed to compose correctly and won’t just blow up, you just drive that error right down. So that’s one thing that’s nice about getting past templates as a language feature. And then the other thing is it’s just crushingly slow. You’re generating the code, almost exactly the same code, over and over and over again. And so that just means you can’t save any of the compilation work. You just have to redo the whole thing from scratch.&lt;/p&gt;
    &lt;p&gt;That’s exactly right. And so this is where again, we were talking about the sand in the system—these little things that if you get wrong, they play forward and they cause huge problems. The metaprogramming approach in Mojo is cool, both for usability and compile time and correctness. Coming back to your point about portability, it’s also valuable for portability because what it means is that the compiler parses your code, and it parses it generically and has no idea what the target is. And so when Mojo generates the first level of intermediate representation, the compiler representation for the code, it’s not hard coding and the pointers are 32 bit or 64 bit, or that you’re on a x86 or whatever. And what this means is that you can take generic code in Mojo and you can put it on a CPU and you can put it on a GPU. Same code, same function. And again, these crazy compilery things that Chris gets obsessed about, it means that you can slice out the chunk of code that you want to put onto your GPU in a way that it looks like a distributed system, but it’s a distributed system where the GPU is actually a crazy embedded device that wants this tiny snippet of code and it wants it fully self-contained. These worlds of things that normal programming languages haven’t even thought about.&lt;/p&gt;
    &lt;p&gt;So does that mean when I compile a Mojo program, I get a shippable executable that contains within it another little compiler that can take the Mojo code and specialize it to get the actual machine code for the final destination that you need? Do I bundle together all the compilers for all the possible platforms in every Mojo executable?&lt;/p&gt;
    &lt;p&gt;The answer is no. The world’s not ready for that. And there are use cases for JIT compilers and things like this, and that’s cool, but the default way of building, if you just run mojo build, then it will give you just an a.out executable, a normal thing. But if you build a Mojo package, the Mojo package retains portability. This is a big difference. This is what Java does. If you think about Java in a completely different way and for different reasons in a different ecosystem universe, it parses all your source code without knowing what the target is, and it generates Java bytecode. And so it’s not 1995 anymore. The way we do this is completely different. And we’re not Java obviously, and we have a type system that’s very different. But this concept is something that’s been well known, and is something that at least the world of compiled languages like Swift, and C++, and Rust have kind of forgotten.&lt;/p&gt;
    &lt;p&gt;So the Mojo package is kind of shipped with the compiler technology required to specialize to the different domains.&lt;/p&gt;
    &lt;p&gt;Yes. And so again, by default, if you’re a user, you’re sitting on your laptop and you say, ‘Compile a Mojo program,’ you just want an executable. But the compiler technology has all of these powerful features and they can be used in different ways. This is similar to LLVM, where LLVM had a just-in-time compiler, and that’s really important if you’re Sony Pictures and you’re rendering shaders for some fancy movie, but that’s not what you’d want to use if you’re just running a C++ code that needs to be ahead-of-time compiled.&lt;/p&gt;
    &lt;p&gt;I mean, there’s some echoes here also of the PTX story with Nvidia. Nvidia has this thing that they sort of hide that it’s an intermediate representation, but this thing called PTX, which is a portable bytecode essentially. And they for many years maintained compatibility across many, many different generations of GPUs. They have a thing called the assembler that’s part of the driver thing for loading on, and it’s really not an assembler. It’s like a real compiler that takes the PTX and compiles it down to SASS, the accelerator-specific machine code, which they very carefully do not fully document because they don’t want to give away all of their secrets. And so there’s a built-in portability story there where it’s meant to actually be portable in the future across new generations. Although as you were pointing out before, it in fact doesn’t always succeed. And there are now some programs that will not actually make the transition to Blackwell.&lt;/p&gt;
    &lt;p&gt;So that’s in the category that I’d consider to be like a virtual machine, a very low-level virtual machine by the way. And so when you’re looking at these systems, the thing I’d ask is, what is the type system? And so if you look at PTX, because as you’re saying, you’re totally right, it’s an abstraction between a whole bunch of source code on the top end and then that specific SASS hardware thing on the backend, but the type system isn’t very interesting. It’s pointers and registers and memory. And so Java, what is the type system? Well, Java achieves portability by making the type system in its bytecode expose objects. And so it’s a much higher level abstraction, dynamic virtual dispatch, that’s all part of the Java ecosystem. It’s not a bytecode, but the representation that’s portable maintains the full generic system. And so this is what makes it possible to say, ‘Okay, well I’m going to take this code, compile it once to a package, and now go specialize and instantiate this for a device.’ So the way that works is a little bit different, but it enables, coming back to your original question of safety and correctness, it enables all the checking to happen the right way.&lt;/p&gt;
    &lt;p&gt;Right, there’s also a huge shift in control. With PTX, the machine-specific details of how it’s compiled are totally out of the programmer’s control. You can generate the best PTX you can, and then it’s going to get compiled. How? Somehow, don’t ask too many questions, it’s going to do what it’s going to do. Whereas here, you’re preserving in the portable object, the programmer-driven instructions about how the specialization is going to work. You’ve just partially executed your compilation, you’ve got partway down, and then there’s some more that’s going to be done at the end when you pick actually where you’re going to run it.&lt;/p&gt;
    &lt;p&gt;Exactly. And so these are all very nerdy pieces that go into the stack, but the thing that I like is if you bubble out of that, it’s easy to use. It works. It gives good error messages, right? I don’t understand the Greek letters, but I do understand a lot of the engineering that goes into this. The way this technology stack builds up, the whole purpose is to unlock compute, and we want new programmers to be able to get into the system. And if they know Python, if they understand some of the basics of the hardware, they can be effective and then they don’t get limited to 80% of the performance. They can keep driving and keep growing in sophistication, and maybe not everybody wants to do that. They can stop at 80%, but if you do want to go all the way, then you can get there.&lt;/p&gt;
    &lt;p&gt;One thing I’m curious about is, how do you actually manage to keep it simple? You said that Mojo is meant to be Pythonic and you talked a bunch about the syntax, but actually one of the nice things about Python is it’s simple in some ways in a deeper sense. The fact that there isn’t by default a complicated type system with complicated type errors to think about—there’s a lot of problems with that, but it’s also a real source of simplicity for users who are trying to learn the system. Dynamic errors at runtime are in some ways easier to understand. ‘I wrote a program and it tried to do a thing and it tripped over this particular thing and you can see it tripping over,’ and in some ways that’s easier to understand when you’re going to a language which, for both safety and performance reasons, needs much more precise type level control. How do you do that in a way that still feels Pythonic in terms of the base simplicity that you’re exposing to users?&lt;/p&gt;
    &lt;p&gt;I can’t give you the perfect answer, but I can tell you my current thoughts. So again, learn from history. Swift had a lot of really cool features, but it spiraled and got a lot of complexity that got layered in over time. And also one of the challenges with Swift is it had a team that was paid to add features to swift.&lt;/p&gt;
    &lt;p&gt;It’s never a good thing.&lt;/p&gt;
    &lt;p&gt;Well, you have a C++ committee, what is the C++ committee going to do? They’re going to keep adding features to C++. Don’t expect C++ to get smaller. It’s common sense. And so with Mojo, there’s a couple of different things. So one of which is, start from Python. So Python being the surface-level syntax enables me as management to be able to push back and say, ‘Look, let’s make sure we’re implementing the full power of the Python ecosystem. Let’s have lists, and for-comprehensions, and all this stuff before just inventing random stuff because it might be useful.’ But there’s also, for me personally, a significant back pressure on complexity. How can we factor these things? How can we get, for example, the metaprogramming system to subsume a lot of complexity that would otherwise exist? And there are fundamental things that I want us to add.&lt;/p&gt;
    &lt;p&gt;For example, checked generics, things like this because they have a better UX, they’re part of the metaprogramming system, they’re part of the core addition that we’re adding, but I don’t want Mojo to turn into a ‘add every language feature’ that every other language has just because it’s useful to somebody. I was actually inspired by and learned a lot from Go, and it’s a language that people are probably surprised to hear me talk about. Go, I think, did a really good job of intentionally constraining the language with Go 1. And they took a lot of heat for that. They didn’t add a generic system, and everybody, myself included, were like, ‘Ha ha ha, why doesn’t this language even have a generic system? You’re not even a modern language.’ But they held the line, they understood how far people could get, and then they did a really good job of adding generics to Go 2, and I thought they did a great job.&lt;/p&gt;
    &lt;p&gt;There was a recent blog post I was reading, talking about Go, and apparently they have an 80-20 rule, and they say they want to have 80% of the features with 20% of the complexity, something like that. And the observation is that that’s a point in the space that annoys everybody, because everybody wants 81% of the features, but 81% of the features maybe gives you 35% of the complexity. And so, figuring out where to draw that line and figuring out where to say no—for example, we have people in the community that are asking for very reasonable things that exist in Rust. And Rust is a wonderful language. I love it. There’s a lot of great ideas and we shamelessly pull good ideas from everywhere. But I don’t want the complexity.&lt;/p&gt;
    &lt;p&gt;I often like to say that one of the most critical things about a language design is maintaining the power-to-weight ratio.&lt;/p&gt;
    &lt;p&gt;You want to get an enormous amount of good functionality, and power, and good user experience while minimizing that complexity. I think it is a very challenging thing to manage, and it’s actually a thing that we are seeing a lot as well. We are also doing a lot to extend OCaml in all sorts of ways, pulling from all sorts of languages, including Rust, and again, doing it in a way where the language maintains its basic character and maintains its simplicity is a real challenge. And it’s kind of hard to know if you’re hitting the actual right point on that. And it’s easier to do in a world where you can take things back, try things out and decide that maybe they don’t work, and then adjust your behavior. And we’re trying to iterate a lot in that mode, which is a thing you can do under certain circumstances. It gets harder as you have a big open-source language that lots of people are using.&lt;/p&gt;
    &lt;p&gt;That’s a really great point. And so one of the other lessons I’ve learned with Swift, is that with Swift, I pushed very early to have an open design process where anybody could come in, write a proposal, and then it would be evaluated by the language committee, and then if it was good, it would be implemented and put into Swift. Again, be careful what you wish for. That enabled a lot of people with really good ideas to add a bunch of features to Swift. And so with Mojo as a counterbalance, I really want the core team to be small. I want the core team not just to be able to add a whole bunch of stuff because it might be useful someday, but to be really deliberate about how we add things, how we evolve things.&lt;/p&gt;
    &lt;p&gt;How are you thinking about maintaining backwards compatibility guarantees as you evolve it forward?&lt;/p&gt;
    &lt;p&gt;We’re actively debating and discussing what Mojo 1.0 looks like. And so I’m not going to give you a timeframe, but it will hopefully not be very far away. And what I am fond of is this notion of semantic versioning, and saying we’re going to have a 1.0, and then we’re going to have a 2.0, and we’re going to have a 3.0, and we’re going to have a 4.0, et cetera. And each of these will be able to be incompatible, but they can link together. And so one of the big challenges and a lot of the damage in the Python ecosystem was from the Python two-to-three conversion. It took 15 years and it was a heroic mess for many different reasons. The reason it took so long is because you have to convert the entire package ecosystem before you can be 3.0. And so if you contrast that to something like C++, let me say good things about C++, they got the ABI right.&lt;/p&gt;
    &lt;p&gt;And so once the ABI was set, then you could have one package built in C++ 98, and one package built in C++ 23, and these things would interoperate and be compatible even if you took new keywords or other things in the future language version. And so what I see for Mojo is much more similar to the—maybe the C++ ecosystem or something like this, but that allows us to be a little bit more aggressive in terms of migrating code, in terms of fixing bugs, and in moving language forward. But I want to make sure that Mojo 2.0 and Mojo 1.0 packages work together and that there’s good tooling, probably AI-driven, but good tooling to move from 1.0 to 2.0 and be able to manage the ecosystem that way.&lt;/p&gt;
    &lt;p&gt;I think the type system also helps an enormous amount. I think one of the reasons the Python migration was so hard is that you couldn’t be like, ‘And then let me try and build this with Python 3 and see what’s broken.’ You could only see what’s broken by actually walking all of the execution paths of your program. And if you didn’t have enough testing, that would be very hard. And even if you did, it wasn’t that easy. Whereas with a strong type system, you can get an enormous amount of very precise guidance. And actually the combination of a strong type system and an agentic coding system is awesome. We actually have a bunch of experience of just trying these things out now, where you make some small change to the type of something and then you’re like, ‘Hey, AI system, please run down all the type errors, fix them all.’ And it does surprisingly well.&lt;/p&gt;
    &lt;p&gt;I absolutely agree. There’s other components to it. So Rust has done a very good job with the stabilization approach with crates and APIs. And I think that’s a really good thing. And so I think we’ll take good ideas from many of these different ecosystems and hopefully do something that works well, and works well for the ecosystem, and allows us to scale without being completely constrained by never being able to fix something once you ship a 1.0.&lt;/p&gt;
    &lt;p&gt;I’m actually curious, just to go to the agentic programming thing for a second, which is having AI agents that write good kernels is actually pretty hard. And I’m curious what your experience is of how things work with Mojo. Mojo is obviously not a language deeply embedded in the training set that these models were built on, but on the other hand, you have this very strong type structure that can guide the process of the AI agent trying to write and modify code. I’m curious how that pans out in practice as you try and use these tools.&lt;/p&gt;
    &lt;p&gt;So this is why Mojo being open source, and—so we have hundreds of thousands of lines of Mojo code that are public with all these GPU kernels, and like, all this other cool stuff. And we have a community of people writing more code. Having hundreds of thousand lines of Mojo code is fantastic. You can point your coding tool cursor, or whatever it is, at that repo and say, ‘Go learn about this repo and index it.’ So it’s not that you have to train the model to know the language, just having access to it—that enables it to do good work. And these tools are phenomenal. And so that’s been very, very, very important. And so we have instructions on our webpage for how to set up these tools, and there’s a huge difference if you set it up right, so that it can index that, or if you don’t, and make sure to follow that markdown file that explains how to set up the tool.&lt;/p&gt;
    &lt;p&gt;So, I want to talk a little bit about the future of Mojo. I think that the current way that Modular and you have been talking about Mojo, these days at least—it’s a replacement for CUDA, an alternate full top-to-bottom stack for building GPU kernels, for writing programs that execute on GPUs. But that’s not the only way you’ve ever talked about Mojo. You’ve also, especially earlier on I think, there was more discussion of Mojo as an extension, and maybe evolution of, and maybe eventually replacement of Python. And I’m curious, how do you think about that now? To what degree do you think of Mojo as its own new language that takes inspiration and syntax from Python, and to what degree do you want something that’s more deeply integrated over time?&lt;/p&gt;
    &lt;p&gt;So today, to pull it back to, ‘What is Mojo useful for today, and how do we explain it?’ Mojo is useful if you want code to go fast. If you have code on a CPU or a GPU and you want it to go fast, Mojo is a great thing. One of the really cool things that is available now—but it’s in preview and it’ll solidify in the next month or something—is it’s also the best way to extend Python. And so if you have a large-scale Python code base, again, tell me if this sounds familiar, you are coding away and you’re doing cool stuff in Python and then it starts to get slow. Typically what people do is, they have to either go rewrite the whole thing in Rust or C++, or they carve out some chunk of it and move some chunk of that package to C++ or Rust. This is what NumPy, or PyTorch, or all modern large-scale Python code bases end up doing.&lt;/p&gt;
    &lt;p&gt;If you look up on the mirrors and look at the percentage of programs that have C extensions in them, it’s shockingly high. A really large fraction of Python stuff is actually part Python and part some other language, almost always C and C++, a little bit of Rust.&lt;/p&gt;
    &lt;p&gt;That’s right. And so today—this isn’t distant future—today, you can take your Python package and you can create a Mojo file and you can say, ‘Okay, well these for loops are slow, move it over to Mojo.’ And we have people, for example, doing bioinformatics and other crazy stuff I know nothing about, saying, ‘Okay, well I’m just taking my Python code, I move it over to Mojo. Wow, now I get types, I get these benefits, but there’s no bindings. The pip experience is beautiful. It’s super simple.’ You don’t have to have FFI’s and nanobind and all this complexity to be able to do this. You also are not moving from Python with its syntax to curly braces and borrow checkers and other craziness. You now get a very simple and seamless way to extend your Python package. And we have people that say, okay, well I did that and I got it first 10x, and 100x, and 1000x faster on CPU.&lt;/p&gt;
    &lt;p&gt;But then because it was easy, I just put it on a GPU. And so to me, this is amazing because these are people that didn’t even think and would never have gotten it on a GPU if they switched to Rust or something like that. Again, the way I explain it is, Mojo is good for performance. It’s good if you want to go fast on a GPU, on a CPU, if you want to make Python go fast, or if you want to—I mean, some people are crazy enough to go whole hog and just write entirely from scratch Mojo programs, and that’s super cool. If you fast forward six, nine months, something, I think that Mojo will be a very credible top-to-bottom replacement for Rust.&lt;/p&gt;
    &lt;p&gt;And so we need a few more extensions to the generic system. And there’s a few things I want to bake out a little bit. Some of the dynamic features that Rust has for the existentials, the ability to make a runtime trait is missing in Mojo. And so we’ll add a few of those kinds of features. And as we do that, I think that’ll be really interesting as an applications-level programming language for people who care about this kind of stuff. You fast forward, I might even project a timeframe, maybe a year, 18 months from now, it depends on how we prioritize things, and we’ll add classes. And so as we add classes, suddenly it will look and feel to a Python programmer much more familiar. The classes in Mojo will be intentionally designed to be very similar to Python, and at that point we’ll have something that looks and feels kind of like a Python 4.&lt;/p&gt;
    &lt;p&gt;It’s very much cut from the same mold as Python. It integrates really well from Python. It’s really easy to extend Python, and so it’s very much a member of the Python family, but it’s not compatible with Python. And so what we’ll do over the course of N years, and I can’t predict exactly how long that is, is continue to run down the line of, okay, well how much compatibility do we want to add to this thing? And then I think that at some point people will consider it to be a Python superset, and effectively it will feel just like the best way to do Python in general. And I think that that will come in time. But to bring it all the way back, I want us to be very focused on, ‘What is Mojo useful for today?’ Great claims require great proof.&lt;/p&gt;
    &lt;p&gt;We have no proof that we can do this. I have a vision and a future in my brain, and I’ve built a few languages and some scale things before, and so I have quite high confidence that we can do this. But I want people to zero back into, okay, if you’re writing performance code, if you’re writing GPU kernels or AI, if you have Python code, you don’t want it to go slow, a few of us have that problem, then Mojo can be very useful. And hopefully it’ll be even more useful to more people in the future.&lt;/p&gt;
    &lt;p&gt;And I think that already, the practical short-term thing is already plenty ambitious and exciting on its own. Seems like a great thing to focus on.&lt;/p&gt;
    &lt;p&gt;Yeah, let’s solve heterogeneous compute and AI. That’s actually a pretty useful thing, right?&lt;/p&gt;
    &lt;p&gt;Alright, that seems like a great place to stop. Thank you so much for joining me.&lt;/p&gt;
    &lt;p&gt;Yeah, well thank you for having me. I love nerding out with you and I hope it’s useful and interesting to other people too. But even if not, I had a lot of fun with you.&lt;/p&gt;
    &lt;p&gt;You’ll find a complete transcript of the episode along with show notes and links at signalsandthreads.com. Thanks for joining us. See you next time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137373</guid></item><item><title>I Ditched Docker for Podman (and You Should Too)</title><link>https://codesmash.dev/why-i-ditched-docker-for-podman-and-you-should-too</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137525</guid></item><item><title>Relace (YC W23) Is Hiring for Code LLM's (SF)</title><link>https://news.ycombinator.com/item?id=45137554</link><description>&lt;doc fingerprint="25e92f79f76accfa"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hey, we're a highly technical team building code generation models, and growing fast. We're looking for people who are down to scrap and love to build -- on both technical and GTM/Devrel roles.&lt;/p&gt;
      &lt;p&gt;If you have a Physics, Math, CS degree; and training fast codegen models is something that piques your interest, please email me directly at pzhou@relace.ai.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137554</guid></item><item><title>Using Your Phone on Toilet May Give You Hemorrhoids: Study</title><link>https://www.nbcnews.com/health/health-news/phone-use-hemorrhoids-bathroom-social-media-scrolling-rcna228080</link><description>&lt;doc fingerprint="7f4703d92f8711d0"&gt;
  &lt;main&gt;
    &lt;p&gt;Of all the crappy ways smartphones have affected our health, this one is a real kick in the pants.&lt;/p&gt;
    &lt;p&gt;A first-of-its-kind study links excessive scrolling on the phone while sitting on the toilet with hemorrhoids.&lt;/p&gt;
    &lt;p&gt;(Insert poo emojis.)&lt;/p&gt;
    &lt;p&gt;But, seriously. Sitting on an open bowl offers no support for the pelvic floor. That puts pressure on veins in the rectum, making them swollen and inflamed.&lt;/p&gt;
    &lt;p&gt;“The longer you sit on the toilet, the worse it is for you,” said Dr. Trisha Pasricha, director of the Beth Israel Deaconess Medical Center’s Institute for Gut-Brain Research Institute in Boston. Pasricha is also an author of the study, which was published Wednesday in PLOS One.&lt;/p&gt;
    &lt;p&gt;And smartphones are designed to keep people fixated for as long as possible. “They’re completely consuming to us in ways that wasn’t happening to the casual bathroom reader in the 80s,” Pasricha said. “They could much more easily put the newspaper down and get up and leave.”&lt;/p&gt;
    &lt;p&gt;Pasricha and colleagues surveyed 125 adults just before they were about to have a routine colonoscopy to screen for colorectal cancer.&lt;/p&gt;
    &lt;p&gt;Eighty-three (66%) of the participants admitted to using their phones in the bathroom — mostly to catch up on news of the day and scroll through social media.&lt;/p&gt;
    &lt;p&gt;Gastroenterologists performing the colonoscopies looked for evidence of inflamed veins, or hemorrhoids. People who said they took their phone into the bathroom were 46% more likely to have hemorrhoids compared to the others.&lt;/p&gt;
    &lt;p&gt;The risk remained even when researchers accounted for other factors associated with hemorrhoids, including dietary fiber, exercise and constipation or straining while using the toilet.&lt;/p&gt;
    &lt;p&gt;Hemorrhoids aren’t necessarily dangerous, but they can be bothersome, itchy and even painful. They also bleed sometimes, understandably causing concern and leading to nearly 4 million doctor’s office and emergency department visits a year.&lt;/p&gt;
    &lt;p&gt;Over time, “pelvic floor dysfunction can also lead to incontinence, worsen constipation and be associated with rectal pain,” said Dr. Reezwana Chowdhury, an inflammatory bowel disorder specialist at the Johns Hopkins University School of Medicine. Chowdhury was not involved with the new research.&lt;/p&gt;
    &lt;p&gt;What’s more, microscopic particles from urine and feces are sent flying through the air when a toilet is flushed. Taking a phone into the bathroom, Chowdhury said, “is kind of gross.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Younger patients&lt;/head&gt;
    &lt;p&gt;In the new study, smartphone users in the bathroom tended to be younger, meaning adults in their 40s and 50s, versus people over age 60.&lt;/p&gt;
    &lt;p&gt;Dr. Robert Cima, a colorectal surgeon at the Mayo Clinic in Rochester, Minnesota, said he’s noticed an uptick in recent years of people coming in with hemorrhoids.&lt;/p&gt;
    &lt;p&gt;“I am seeing younger, earlier- and middle-aged people having more hemorrhoidal complaints, but I can’t tie it to smartphones,” said Cima, who was not involved with the new study. “Maybe it’s because they’re using smartphones or they have better access to care or they’re not eating appropriately.”&lt;/p&gt;
    &lt;head rend="h2"&gt;The 5-minute rule&lt;/head&gt;
    &lt;p&gt;The experts agreed that business on the toilet should take no longer than 5 minutes.&lt;/p&gt;
    &lt;p&gt;More than 37% of study participants who used a smartphone in the bathroom stayed for longer than that, compared to 7% of people who kept their phones out of the bathroom.&lt;/p&gt;
    &lt;p&gt;Pasricha and other experts do not advocate for taking a phone into the bathroom. If you absolutely must, set a timer.&lt;/p&gt;
    &lt;p&gt;“If the magic is not happening within five minutes, it’s not going to happen,” Pasricha said. “Take a breather and try again later.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137656</guid></item><item><title>OpenAI eats jobs, then offers to help you find a new one at Walmart</title><link>https://www.theregister.com/2025/09/05/openai_jobs_board/</link><description>&lt;doc fingerprint="ee551855906695f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI eats jobs, then offers to help you find a new one at Walmart&lt;/head&gt;
    &lt;head rend="h2"&gt;Move over LinkedIn, Altman's crew wants a piece of the action&lt;/head&gt;
    &lt;p&gt;For those worried that AI is going to disrupt their jobs, OpenAI has the solution – take its certification and use a newly announced jobs board to find a new role.&lt;/p&gt;
    &lt;p&gt;On Thursday, Fidji Simo, OpenAI's head of applications (and former CEO of Instacart), announced the plan for workers to advertise themselves to the company's customers for new jobs. She said that while AI is going to shake up the employment market, who better to solve that problem than the people doing the shaking?&lt;/p&gt;
    &lt;p&gt;"AI will be disruptive. Jobs will look different, companies will have to adapt, and all of us – from shift workers to CEOs – will have to learn how to work in new ways," she said in a blog post.&lt;/p&gt;
    &lt;p&gt;"At OpenAI, we can't eliminate that disruption. But what we can do is help more people become fluent in AI and connect them with companies that need their skills, to give people more economic opportunities."&lt;/p&gt;
    &lt;p&gt;Simo's plan is that workers should take courses in tech literacy at its OpenAI Academy and then advertise themselves on a forthcoming jobs platform. She said the company has already signed up some big names to the scheme, although maybe the choice of Walmart as an early adopter might not encourage IT admins in their future career paths.&lt;/p&gt;
    &lt;p&gt;OpenAI declined to comment further on the plans.&lt;/p&gt;
    &lt;p&gt;"At Walmart, we know the future of retail won't be defined by technology alone – it will be defined by people who know how to use it," Walmart US CEO John Furner said in a canned statement.&lt;/p&gt;
    &lt;p&gt;"By bringing AI training directly to our associates, we're putting the most powerful technology of our time in their hands – giving them the skills to rewrite the playbook and shape the future of retail."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Biased bots: AI hiring managers shortlist candidates with AI resumes&lt;/item&gt;
      &lt;item&gt;OpenAI wants to bend copyright rules. Study suggests it isn't waiting for permission&lt;/item&gt;
      &lt;item&gt;White House bans 'woke' AI, but LLMs don't know the truth&lt;/item&gt;
      &lt;item&gt;Microsoft unveils home-made ML models amid OpenAI negotiations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The OpenAI Academy has had some big-name sign-ups, particularly the respected computer science teachers at Georgia Tech, but Simo says that the business is pushing hard to build on a White House plan to make AI a core skill for American workers – so long as the engines they use aren't too woke.&lt;/p&gt;
    &lt;p&gt;What Simo didn't mention directly is that getting into the jobs market would bring the company into competition with Microsoft, one of its biggest backers. LinkedIn is the primary Western jobs site and OpenAI setting up a competitor might get in the way of cordial relations.&lt;/p&gt;
    &lt;p&gt;Microsoft had no comment on the matter, but OpenAI appears to be only scooping the AI cream, and whatever else floats to the top of the market, on its proposed employment register. There's also the question of whether or not the skills OpenAI is shilling will have any validity in the actual jobs market.&lt;/p&gt;
    &lt;p&gt;Meanwhile, CEO Sam Altman and most of the tech glitterati attended a dinner hosted by First Lady Melania Trump to discuss AI last night. Elon Musk wasn't there, but insists he was invited. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137658</guid></item><item><title>You're absolutely Right!</title><link>https://absolutelyright.lol/</link><description>&lt;doc fingerprint="5367b53d7a58fab5"&gt;
  &lt;main&gt;
    &lt;p&gt;I'm absolutely right! Claude Code said it 0 times today Absolutely right Just right&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137802</guid></item><item><title>Lava RGB</title><link>https://amaiorano.io/2025/09/03/lava-rgb.html</link><description>&lt;doc fingerprint="3a061475512553f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Lava RGB&lt;/head&gt;
    &lt;p&gt;Back in 2021, I installed an NESRGB on a front loader, which has been working great. For years now, NESRGB was pretty much the only mod available to get RGB out of the NES; but recently, a new mod known as Lava RGB came on the scene from a company in China. I bought one, and in this post I go over how I installed it on another front loader.&lt;/p&gt;
    &lt;p&gt;I bought the Lava RGB mod off AliExpress from Lava FC Store, specifically from this listing (although I suspect these links will not survive very long). The “color” I bought is the one named “NES senior VER 2.0”, which includes both the mod board itself, along with a replacement power A/V module. Version 2.0 adds a bunch of new features on top of the previous version 1.2, such as 24-bit color output, 8 integrated palettes, the ability to reset the console and change palettes using controller 1, an OSD that displays the palette name when switching them, and a micro usb port that allows for firmware upgrades in the future.&lt;/p&gt;
    &lt;p&gt;Compared to the NESRGB, Lava RGB 2.0 is definitely a worthy contender. The palette switching OSD, and ability to easily update firmware, are improvements, and I wonder if future firmware updates might add more OSD-based options. However, one thing Lava RGB does not do is process and output audio like the NESRGB. Although not a huge deal, it does mean audio needs to be tapped from the main audio output circuit. This is probably why the latest version now offers a power A/V replacement PCB which routes audio output to its Saturn-style DIN connector.&lt;/p&gt;
    &lt;p&gt;Although the power module includes a Saturn-style DIN connector, I don’t own a compatible cable, and prefer to use VGA cables. So for my install, I decided to add a SNES multiout port.&lt;/p&gt;
    &lt;head rend="h2"&gt;Parts #&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lava RGB 2.0 mod board and power module - $81.37 CAD&lt;/item&gt;
      &lt;item&gt;Two 20 rounded pin headers - $16 CAD for ten on Amazon&lt;/item&gt;
      &lt;item&gt;SNES multiout parts (more on that later)&lt;/item&gt;
      &lt;item&gt;47K resistor for expansion audio&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Lava RGB kit took about a week to arrive, and was packaged decently well:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Build #&lt;/head&gt;
    &lt;head rend="h3"&gt;Prepare NES main board #&lt;/head&gt;
    &lt;head rend="h4"&gt;Remove PPU #&lt;/head&gt;
    &lt;p&gt;I picked a front loader with an NES-CPU-10 main board that was in excellent shape. I took it apart, and extracted the main board from the shell:&lt;/p&gt;
    &lt;p&gt;First order of business was to desolder the PPU:&lt;/p&gt;
    &lt;p&gt;I started by adding fresh solder, using flux to make it flow into the existing solder:&lt;/p&gt;
    &lt;p&gt;I then used my trusty desoldering pump to remove all the solder from the pins:&lt;/p&gt;
    &lt;p&gt;Using a trick I learned from Voultar, I used my finger nails to move each pin back and forth until they moved freely. For the usual more stubborn pins on the thicker ground plane, I added more solder and desoldered again until I was able to move the pins. With that, I extracted the PPU:&lt;/p&gt;
    &lt;head rend="h4"&gt;Remove power module #&lt;/head&gt;
    &lt;p&gt;Next I needed to remove the original power module to replace it with the new one:&lt;/p&gt;
    &lt;p&gt;This can be pretty challenging, but with the right tools and technique, it’s not too bad. I started by adding flux to the four large pins that anchor the power module to the main board and use my desoldering gun to remove most of the solder:&lt;/p&gt;
    &lt;p&gt;Then I use wicking braid to remove the solder lodged around the pins. I added a little more solder and flux to the pins until I was able to wick away most of the remaining solder:&lt;/p&gt;
    &lt;p&gt;At this point, the gaps around the pins are pretty clear:&lt;/p&gt;
    &lt;p&gt;The next trick is to desolder the 5 pins visible above - but not from this side, but rather from inside the power module. This is actually necessary for properly installing the new power module that comes with the Lava RGB kit. I pried off the metal plate:&lt;/p&gt;
    &lt;p&gt;I switched tips on my desoldering gun to one with a larger pitch, and desoldered the 5 pins:&lt;/p&gt;
    &lt;p&gt;With that done, I grabbed hold of the power module with one hand, while holding the main board with the other, and rocked the power module back and forth until it came free:&lt;/p&gt;
    &lt;p&gt;We’re left with the 5 pins still attached to the main board, ready to be soldered to the new power module later:&lt;/p&gt;
    &lt;head rend="h4"&gt;Push main board capacitors flat #&lt;/head&gt;
    &lt;p&gt;As with the NESRGB, the caps on the main board need to be pushed down flat for the Lava RGB PCB to fit:&lt;/p&gt;
    &lt;p&gt;Normally this is simply a matter of pushing the caps down while heating the vias with a soldering iron. However, as I have a bunch of NES cap kits that I ordered from Console5, I decided to replace them:&lt;/p&gt;
    &lt;p&gt;I desoldered the three caps:&lt;/p&gt;
    &lt;p&gt;And replaced them with the new ones, making sure to match the capacitance values, and ensuring the voltage rating is equal or above the original one. When placing the caps, I laid them flat before soldering them in:&lt;/p&gt;
    &lt;head rend="h3"&gt;Solder wires #&lt;/head&gt;
    &lt;p&gt;At this point, I decided to solder the ends of the Lava RGB wire connector to the power module and to the main board:&lt;/p&gt;
    &lt;p&gt;What’s nice is that the order of the pads on the power module and on the mod board match, so it’s easy to solder the first eight wires:&lt;/p&gt;
    &lt;p&gt;I soldered the eight wires to the pads:&lt;/p&gt;
    &lt;p&gt;The remaining four wires are used to allow controller 1 to perform an in-game reset (IGR) as well as palette swapping using key combos. IGR is performed by holding Select for about 2 seconds, then pressing A; while for palette swapping, you hold Select for 2 seconds and press Up on the dpad. To make this work, the RST wire needs to be wired to the reset line on the main board (where the actual Reset button is wired to), while the CLK, DATA, and LATCH pins need to be wired to their respective pins on the player 1 controller input lines.&lt;/p&gt;
    &lt;p&gt;I put this handy image together to identify the pins to solder to:&lt;/p&gt;
    &lt;p&gt;Noting the colors of each pin, I first soldered the RST line:&lt;/p&gt;
    &lt;p&gt;Then I soldered CLK, DATA, and LATCH. Note that DATA and LATCH are not in the same order as on the Lava RGB PCB:&lt;/p&gt;
    &lt;head rend="h3"&gt;Power module #&lt;/head&gt;
    &lt;p&gt;As already mentioned, for my install I planned to add a SNES-style multiout. Rather than grab these connections from the Lava RGB mod board itself, I decided to get them from the power module since it’s closer to where the port would be installed. For this, I used an 8 wire ribbon cable:&lt;/p&gt;
    &lt;p&gt;In retrospect, I should have made this cable longer, as I was limited to where I could place the multiout. Anyway, I soldered the cable to the power module. Conveniently, the power module PCB offers two ways to connect wires to it, pads and vias, so I used the vias for this:&lt;/p&gt;
    &lt;p&gt;Note that two of the wires, the grey and the purple, are not soldered yet. One of these will be used for 5V, and the other for audio, both of which will be wired later to the PV and PA pins of the 5 pin connector.&lt;/p&gt;
    &lt;p&gt;I also soldered a wire to GND, and soldered it to the ground plane of the main board:&lt;/p&gt;
    &lt;p&gt;At this point, I loosely positioned the PCBs to get a sense for where I was heading:&lt;/p&gt;
    &lt;p&gt;Next, I soldered the power module to the five pins coming from the main board:&lt;/p&gt;
    &lt;p&gt;And now I could solder the 5V and audio wires to PV and PA respectively:&lt;/p&gt;
    &lt;p&gt;This is what everything looked like at this point:&lt;/p&gt;
    &lt;head rend="h3"&gt;Mod board #&lt;/head&gt;
    &lt;p&gt;Next up was installing the mod board:&lt;/p&gt;
    &lt;p&gt;As can be seen above, the kit includes two 20 square pin headers. Their official instructions expect you to solder the mod board PCB directly to the NES main board using these square pin headers, making sure to first solder the round pin socket onto the mod board itself so that the PPU can be inserted into it. The apparent advantage here is being able to swap out the PPU, but with the major disadvantage that the mod board cannot be removed from the main board, making it difficult to service the mod in the future. So instead of this, I did as is usually done with the NESRGB mod: solder the socket to the main board, and the PPU directly to the mod board. For this, I needed to either get a socket that works with the included square pin headers, or rounded pin headers - I chose the latter.&lt;/p&gt;
    &lt;p&gt;I soldered the socket to the main board, making sure to line up the notch:&lt;/p&gt;
    &lt;p&gt;I inserted the two rounded pin headers I purchased separately into the sockets, longer pins down:&lt;/p&gt;
    &lt;p&gt;Then laid the PCB over the pins, making sure to insert them into the right vias, and soldered them in place:&lt;/p&gt;
    &lt;p&gt;I carefully detached the mod board from the socket by rocking each edge back and forth. Some care has to be taken here as these rounded pins can bend and break very easily:&lt;/p&gt;
    &lt;p&gt;Before soldering the PPU to the mod board, I use my flush cutters to cut down the one set of pins that would be underneath the PPU, and then touched them up with the soldering iron. Although not strictly necessary, this allows the PPU to lay flat on the PCB:&lt;/p&gt;
    &lt;p&gt;I inserted the PPU onto the mod board, making sure to line up the notch, and soldered it in place:&lt;/p&gt;
    &lt;p&gt;Next, I soldered the wire connector to the mod board:&lt;/p&gt;
    &lt;p&gt;Now I carefully inserted the mod board back into the socket, making sure to line up the socket pins, while also routing the wires down and to the side, as there isn’t much space between the connector and the expansion port (note that we’re seeing the reflection of the wires on the edge of the expansion port):&lt;/p&gt;
    &lt;head rend="h3"&gt;First test #&lt;/head&gt;
    &lt;p&gt;At this point, I could finally test the mod out. I hadn’t soldered the multiout yet, and I didn’t have a way to hook up the Saturn-style DIN, so I couldn’t test RGB, but I could test composite. So I loosely put everything together and composite video and audio worked perfectly:&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiout #&lt;/head&gt;
    &lt;p&gt;For the SNES multiout, I 3D printed Laser Bear’s Multiout Panel Mount Snap In Connector, and used PCBWay to print The Real Pheonix’s PCB. I also needed to order the right #2 x 1/2” screw. Alternatively, one could order the parts directly from Laser Bear and The Real Phoenix. Anyway, here are the parts:&lt;/p&gt;
    &lt;head rend="h4"&gt;Solder ribbon to multiout PCB #&lt;/head&gt;
    &lt;p&gt;I mapped out the pins I’d need to solder to, along with the specific color wires from my ribbon cable:&lt;/p&gt;
    &lt;p&gt;I soldered the ribbon cable to the multiout PCB:&lt;/p&gt;
    &lt;p&gt;For testing, I inserted the PCB into the 3D printed frame and screwed it in:&lt;/p&gt;
    &lt;head rend="h4"&gt;Second test #&lt;/head&gt;
    &lt;p&gt;Using an old cable I made, I gave it a quick test:&lt;/p&gt;
    &lt;p&gt;It worked! Although note that the colors aren’t quite right. It turned out that my cable was defective - green wasn’t being passed through. I later used a better cable, and all was fine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Cut shell for multiout connector #&lt;/head&gt;
    &lt;p&gt;Using the 3D printed bracket, I marked off where I would need to cut into the shell to fit the 3D printed connector:&lt;/p&gt;
    &lt;p&gt;I made sure to position the hole as to not interfere with the posts. I actually wanted to position it more to the right, but as already mentioned, I had cut the ribbon cable too short, and didn’t feel like redoing it.&lt;/p&gt;
    &lt;p&gt;Next came the most annoying part for me, since I still don’t own a proper dremel: I used a drill, flush cutters, and filing tools to cut out the hole:&lt;/p&gt;
    &lt;p&gt;Finally, after way too long, the connector fit in perfectly:&lt;/p&gt;
    &lt;p&gt;I used the 3D printed clip to hold the connector in place:&lt;/p&gt;
    &lt;p&gt;The fit was a little snug considering that raised portion inside, but thankfully it was fine:&lt;/p&gt;
    &lt;head rend="h3"&gt;Expansion audio #&lt;/head&gt;
    &lt;p&gt;Before closing everything up, I also wanted to enable expansion audio. I decided to use Voultar’s PCB because this is what I had done for my NESRGB install. What I didn’t realize at the time was that this wasn’t necessary at all since the Lava RGB doesn’t process audio like the NESRGB. I’ll show my misteps here, and how I corrected it, but this could definitely have been simpler!&lt;/p&gt;
    &lt;p&gt;I soldered a 1K and 47K resistor to the PCB:&lt;/p&gt;
    &lt;p&gt;I positioned the PCB on pins 2 and 9 on the expansion port pins and soldered the PCB in place:&lt;/p&gt;
    &lt;p&gt;It was at this point that I tested expansion audio and realized it wasn’t working. After chatting with Toxic_Tripod0 on Discord, I realized my mistake. To enable expansion audio without NESRGB, all that’s needed is a 47K resistor between pins 3 and 9. This PCB is expressly designed to route expansion audio to the NESRGB for processing. At this point, I could have removed the PCB, but I realized I could fix this relatively easily by removing the 1K resistor, and soldering a wire from the exposed pad to pin 3:&lt;/p&gt;
    &lt;p&gt;As required, pins 3 and 9 were now connected via a 47K resistor:&lt;/p&gt;
    &lt;p&gt;I tested expansion audio, and it worked! If I were to redo this, I’d probably just connect a 47K throughole resistor between pins 3 and 9 instead.&lt;/p&gt;
    &lt;head rend="h3"&gt;Closing everything up #&lt;/head&gt;
    &lt;p&gt;It was finally time to put everything back together:&lt;/p&gt;
    &lt;p&gt;I attached the power and controller cables to the main board:&lt;/p&gt;
    &lt;p&gt;I inserted the multiout PCB into the connector and screwed it in:&lt;/p&gt;
    &lt;p&gt;I reattached the cartridge slot to the main board, and carefully inserted the main board into the bottom shell. The new power module is only held by those 5 pins to the main board, so some care has to be taken when fitting it in place:&lt;/p&gt;
    &lt;p&gt;I decided not to put in the RF shielding as it’s not really necessary, and would pinch and possibly short the reset and controller 1 wires used for IGR and palette switching. I put in all the screws, including the ones in the posts used for the RF shielding:&lt;/p&gt;
    &lt;p&gt;I screwed on the top shell, and was finally done! Here it is in all its glory:&lt;/p&gt;
    &lt;head rend="h3"&gt;Final test #&lt;/head&gt;
    &lt;p&gt;With everything back together, I hooked it up using my SNES2VGA with a VGA cable to my gbs-control:&lt;/p&gt;
    &lt;p&gt;Looking good in RGB:&lt;/p&gt;
    &lt;p&gt;Interestingly, the colors don’t exactly match the composite output:&lt;/p&gt;
    &lt;p&gt;Using the palette switching control (hold select + Up on dpad), I cycled through the palettes, which displays the name of the palette for a few seconds. Here are some samples:&lt;/p&gt;
    &lt;head rend="h2"&gt;Thoughts #&lt;/head&gt;
    &lt;p&gt;I’ve been playing with this modded NES for a few days now, and it works and looks great. The Lava RGB 2.0 is definitely a worthy contender to the NESRGB, especially for the price. Although it doesn’t process audio, tapping the NES-produced audio out from the new power module works fine, and the sound is very clean.&lt;/p&gt;
    &lt;p&gt;It’s worth noting that this mod doesn’t require the new power module at all, especially in my case as I added the SNES-style multiout. However, for those who don’t want the multiout, and want a no-cut mod, this is a nice way to do it. In my case, the power module made it easier to wire up the multiout, and possibly improved the audio output as the older power modules are known to add interference to the audio signal.&lt;/p&gt;
    &lt;p&gt;I just wanted to give a quick shout out to the folks on the ConsoleMods discord, especially Toxic_Tripod0, manadream, and RobStrange for their help.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137914</guid></item><item><title>Data Modeling Guide for Real-Time Analytics with ClickHouse</title><link>https://www.ssp.sh/blog/practical-data-modeling-clickhouse/</link><description>&lt;doc fingerprint="77d1aa33699f5292"&gt;
  &lt;main&gt;
    &lt;p&gt;Querying billions of weather records and getting results in under 200 milliseconds isn’t theory; it’s what real-time analytics solutions provide. Processing streaming IoT data from thousands of sensors while delivering real-time dashboards with no lag is what certain business domains need. That’s what you’ll learn at the end of this guide through building a ClickHouse-modeled analytics use case.&lt;/p&gt;
    &lt;p&gt;You’ll learn how to land data in ClickHouse that is optimized for real-time data applications, going from basic ingestion to advanced techniques like statistical sampling, pre-aggregation strategies, and multi-level optimization. I’ve included battle-tested practices from Rill’s years of implementing real-time analytics for customers processing everything from financial transactions and programmatic advertising to IoT telemetry.&lt;/p&gt;
    &lt;p&gt;This article is for data engineers and practitioners who want to build analytics that deliver sub-second query responses, and who want to unlock ClickHouse’s full potential for real-time analytics demands. By the end, you’ll have a playbook for ClickHouse data modeling plus a working example that ingests NOAA weather data from S3 and visualizes it with a single configuration file.&lt;/p&gt;
    &lt;p&gt;If you haven’t heard of ClickHouse or are wondering why it’s becoming the go-to choice for real-time analytics, here’s what sets it apart from traditional data warehouses.&lt;/p&gt;
    &lt;p&gt;ClickHouse achieves blazingly fast analytical performance through column-oriented storage that reads only relevant data, advanced compression (LZ4/ZSTD), and vectorized query execution that maximizes CPU capabilities. Its sparse primary indexing with data skipping eliminates irrelevant data blocks, while the C++ implementation avoids JVM overhead for bare-metal performance.&lt;/p&gt;
    &lt;p&gt;These innovations enable sub-second query responses on billions of rows, performance that would take minutes or hours in traditional data warehouses. Storage efficiency has a direct impact on both cost and speed at scale, making ClickHouse the ideal foundation for the real-time analytics modeling strategies covered in this article.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data Flow for Real-time Analytics&lt;/head&gt;
    &lt;p&gt;Before we see a concrete example of modeling data with ClickHouse, specifically for real-time and online analytical processing (OLAP) cubes, it’s important to understand the flow of data, its trade-offs, and payoffs. Where Does Data Come From, and Where Does It Go?&lt;/p&gt;
    &lt;head rend="h3"&gt;Data Flow is Knowing the Requirements&lt;/head&gt;
    &lt;p&gt;Data flows from sources to analytics. In the simplest terms, we have sources of data, a transformation with aggregations, and the visualization. Most often, the data should travel from source to visualization as quickly as possible and respond fast to queries.&lt;/p&gt;
    &lt;p&gt;Most data flow modeling is handled in the transformation phase. Connecting to a source, whether it is an S3 or R2 bucket, a relational database like Postgres or others, or visualization on an analytical tool. We need to aggregate and combine the data to extract business insights out of masses of information to answer the questions our business needs to answer.&lt;/p&gt;
    &lt;p&gt;Obviously data modeling can get much more involvedâlooking at modeling open data stack, or looking at The State of Data Engineering and its challenges. However, modeling data has nothing to do with choosing tools in the first place. If we have the best tools but a bad data flow, it’s not worth much.&lt;/p&gt;
    &lt;p&gt;The below illustration shows where the modeling part actually happens:&lt;/p&gt;
    &lt;p&gt;Most often, modeling is more about offline, off-computer, and real conversations with the business people involved than figuring it out ourselves. We have to answer the questions “What’s needed on a dashboard?” “Which numbers are even possible with the data at hand?” and “How can we get them, join and aggregate them with other data from the company to get the best possible insights?”&lt;/p&gt;
    &lt;p&gt;Shifting Left is another important concept related to data modeling. It means that the better we model and structure data at the source (left side of the data pipeline), the more efficient and accurate our analytics become downstream (right side). When raw data is properly typed, deduplicated, and structured early in the pipeline, we avoid expensive transformations later and reduce the risk of data quality issues propagating through our entire analytics stack. This is especially critical for real-time systems where you can’t afford lengthy batch cleanup processes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Real-Time Analytics: A Tradeoff&lt;/head&gt;
    &lt;p&gt;Real-time analytics specifics are always a tradeoff between data freshness and accuracy.&lt;/p&gt;
    &lt;p&gt;The moment the data is loaded, it is outdated. But to avoid pulling the latest all the time, we need to make sure the data is consistent across tables, meaning related data is pulled too when we refresh, so that it’s cohesive and accurate.&lt;/p&gt;
    &lt;p&gt;In the end, you need a set of metrics that are business critical for your organization. Some businesses like IoT and e-commerce don’t need all data, but specific data such as IP or location to identify quickly where users come from. Use cases like this especially need and benefit from low-latency query responses. Data needs to load near real-time and needs to deliver fast, flexible access to core analytics.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Payoff of Great Data Flow&lt;/head&gt;
    &lt;p&gt;The payoffs of modeling are higher performance, insights on consistent data, and lower cost as we do not need to query production with a reduced aggregated data set and without the need for heavy overnight ETL jobs. We need less storage for aggregated data and get even faster query responses.&lt;/p&gt;
    &lt;p&gt;Imagine a fast river that flows constantly with great volume. This is what good data will look like when new data is coming in steady and accurate.&lt;/p&gt;
    &lt;p&gt;Let’s see that in action with ClickHouse real-time modeling.&lt;/p&gt;
    &lt;head rend="h2"&gt;ClickHouse Modeling Strategies: From Theory to Practice&lt;/head&gt;
    &lt;p&gt;Now that we understand the data flow requirements for real-time analytics such as fast ingestion, efficient transformation, and sub-second query responses, let’s explore how ClickHouse specifically addresses these challenges through its modeling approaches.&lt;/p&gt;
    &lt;p&gt;Remember our data flow: &lt;code&gt;Sources â Transformation &amp;amp; Aggregation â ClickHouse â Visualization&lt;/code&gt;. The key insight is that ClickHouse doesn’t only serve as storage but can handle much of the transformation and aggregation work directly, eliminating traditional ETL bottlenecks.&lt;/p&gt;
    &lt;p&gt;ClickHouse offers several strategies to optimize this flow, each addressing different aspects of the freshness-accuracy tradeoff we discussed:&lt;/p&gt;
    &lt;p&gt;For Minimizing Query-Time Complexity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Denormalizing data: Move joins from query time to insert time by flattening related tables into a single structure (One Big Table, approach). This trades some storage efficiency for dramatic query performance gains. Especially recommended for tables that change infrequently and not for high-cardinality or many-to-many relationships.&lt;/item&gt;
      &lt;item&gt;Dictionaries: Handle dimension lookups through in-memory key-value structures, perfect for enriching streaming data with relatively static reference information.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Real-Time Aggregation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Incremental Materialized Views: Shift computational cost from query time to insert time, computing aggregates as data arrives rather than when users request it. Most suitable for real-time aggregations and transformations, especially for single-table aggregations or simple enrichments with static dimension tables.&lt;/item&gt;
      &lt;item&gt;Refreshable Materialized Views: Handle complex multi-table joins and transformations on a scheduled basis, suitable when real-time freshness isn’t critical. They are also useful for batch denormalization and building view dependencies (like DAGs) and can be scheduled with dbt, Airflow, and other data orchestrators. Refreshable MVs are similar to materialized views in traditional OLTP databases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The fundamental principle underlying all these approaches is minimizing joins at query time. In traditional OLAP cubes, much of this complexity is handled by pre-built logical modeling layers. ClickHouse takes a different approach where you explicitly choose where in the pipeline to handle complexity based on your specific performance and freshness requirements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modeling Data with ClickHouse&lt;/head&gt;
    &lt;p&gt;An interesting new dimension is modeling multi-dimensional cubes. What’s the difference, you might ask? Besides the difference between traditional OLAP cubes and modern OLAP cubes, which first stores measures and joins within the cube and pre-processes, whereas modern real-time databases systems like ClickHouse, Pinot, Druid, and StarRocks do not. This is at first glance a disadvantage, but on the other hand an advantage, that we can change our queries at query time without re-processing needed.&lt;/p&gt;
    &lt;p&gt;What else do we need to know about OLAP data modeling? We need to understand that OLAP cubes store data in a column-oriented (or columnar) way. This is important to the ClickHouse architecture. Unlike traditional row-oriented databases that store all values in a row together, ClickHouse stores all values that belong to a single column together. This also influences how we model our data and enables fast analytical queries based on a few columns out of potentially hundreds. ClickHouse only needs to read the data files for those specific columns, drastically reducing disk I/O compared to reading entire rows.&lt;/p&gt;
    &lt;p&gt;Usually when we model a multi-dimensional cube, we deal with facts and dimensions. The queries are optimized for sub-second response times and the users might be our clients or business users; there might only be one visualization layer in between such as a BI tool or Excel. This means it’s mission-critical.&lt;/p&gt;
    &lt;p&gt;In ClickHouse and in general with cubes, we are working with dimensions, measures, and operators that operate on time aggregations and dimensions. You want rollups and drill-downs along multiple axes, with subtotals and potentially pivots.&lt;/p&gt;
    &lt;p&gt;SQL can sometimes be hard work to get right as we constantly pivot along different dimensions, and there are joins involved, different granularity, and all of a sudden, you accidentally duplicate your counting by adding a wrong dimension.&lt;/p&gt;
    &lt;p&gt;So how do we effectively model ClickHouse to get real-time data from start to end with no more than needed effort?&lt;/p&gt;
    &lt;p&gt;In the following example, we’ll see several of these strategies in action: denormalization through data transformation during ingestion, partitioning for query optimization, and incremental processing for real-time updates.&lt;/p&gt;
    &lt;p&gt;There’s no logical modeling layer like in SQL Server Analysis Services (SSAS), meaning we need to model our data outside of ClickHouse to create pre-defined optimized tables to query with the methods explained above such as materialized views, small lookup tables, or denormalized tables.&lt;/p&gt;
    &lt;head rend="h2"&gt;Demo: Using S3 -&amp;gt; ClickHouse -&amp;gt; Rill&lt;/head&gt;
    &lt;p&gt;But we can design and model the data flow easily to source data from an S3/R2 bucket, load from Kafka, or other streaming data sources.&lt;/p&gt;
    &lt;p&gt;Let’s have a look at a practical example where we ingest data from S3, using ClickHouse as the engine to do transformation and aggregation, ingesting the data incrementally with the built-in refresh by ClickHouse, and visualizing with Rill.&lt;/p&gt;
    &lt;p&gt;Watch the short video for the interactive version - below we are going to explain each config step by step.&lt;/p&gt;
    &lt;p&gt;Find everything shown in this demo at clickhouse-modeling-rill-example.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ingest and Transformation&lt;/head&gt;
    &lt;p&gt;This example represents an end-to-end data project, loading NOAA weather data that gets updated from S3 via ClickHouse and visualized in Rill. All within a single YAML shown here (expand to see the full code):&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Source code and full project can be found on GitHub at clickhouse-modeling-rill-example&lt;/p&gt;
    &lt;p&gt;So what happens here?&lt;/p&gt;
    &lt;p&gt;This YAML configuration demonstrates how ClickHouse can serve as both your data transformation engine and storage layer, eliminating the need for traditional ETL tools.&lt;/p&gt;
    &lt;p&gt;Data ingestion and transformation in one step: The &lt;code&gt;sql&lt;/code&gt; section directly reads compressed CSV files from S3 using ClickHouse’s native &lt;code&gt;s3()&lt;/code&gt; function. Rather than requiring a separate ETL process to extract, clean, and load the data, ClickHouse performs all transformations during the ingestion process itself. The query handles data type conversions (like converting temperature readings from tenths to actual values with &lt;code&gt;toFloat32(c4) / 10.0&lt;/code&gt;), creates derived fields for analytics (such as extracting year, month, and day components), and applies data quality measures using &lt;code&gt;COALESCE&lt;/code&gt; to handle null values.&lt;/p&gt;
    &lt;p&gt;MergeTree is your built-in ETL engine: The &lt;code&gt;engine: MergeTree&lt;/code&gt; specification transforms ClickHouse into what you can think of as “local ETL without the need for an ETL tool.” MergeTree engines are specifically designed for high data ingest rates and massive data volumes. When new data arrives, ClickHouse creates table parts that are automatically merged by background processes, maintaining optimal query performance without manual intervention. This means your data pipeline becomes very lightweight and self-managing â new weather data gets ingested, transformed, and optimized automatically based on defined cron triggers.&lt;/p&gt;
    &lt;p&gt;Multi-level optimization strategy: This example demonstrates ClickHouse’s ability to optimize at multiple levels simultaneously. At the query level, the &lt;code&gt;order_by: (measurement_date, station_id, measurement_type)&lt;/code&gt; ensures that data is physically sorted for optimal access patterns typical in weather analytics. This is very important to your end query and how your response will perform. At the storage level, the &lt;code&gt;partition_by: __partition&lt;/code&gt; creates year-based partitions that enable ClickHouse to skip entire data segments when querying specific time ranges. The incremental strategy with &lt;code&gt;partition_overwrite&lt;/code&gt; means only changed partitions are reprocessed, not the entire dataset.&lt;/p&gt;
    &lt;p&gt;Real-time processing without complexity: The &lt;code&gt;refresh: cron: "0 * * * *"&lt;/code&gt; configuration creates an automated pipeline that updates hourly without requiring external orchestration tools like Airflow or Dagster. ClickHouse handles the scheduling, dependency management, and incremental processing internally.&lt;/p&gt;
    &lt;p&gt;Further optimizations are TTL (time-to-live), which deletes data after a defined retention period such as &lt;code&gt;hour + INTERVAL 90 DAY DELETE&lt;/code&gt;, or we can apply further table features such as:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;These settings optimize both deduplication behavior with projections and accelerate automatic data lifecycle management through more frequent TTL merges, ensuring expired data is cleaned up promptly rather than waiting for the default 4-hour intervals.&lt;/p&gt;
    &lt;p&gt;ClickHouse provides built-in insert deduplication for retry scenarios by creating unique &lt;code&gt;block_id&lt;/code&gt; hashes for each inserted block. Duplicate blocks are skipped automatically.&lt;/p&gt;
    &lt;p&gt;Key settings are &lt;code&gt;insert_deduplicate=1&lt;/code&gt; enables block-level deduplication (default for replicated tables) and &lt;code&gt;insert_deduplication_token&lt;/code&gt; provides custom deduplication keys for explicit control. This is block-level deduplication at insert time, unlike ReplacingMergeTree’s row-level deduplication during merges. For more details, see the deduplication token documentation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Visualizing in Rill&lt;/head&gt;
    &lt;p&gt;The above YAML is the source &lt;code&gt;noaa-weather.yaml&lt;/code&gt; and when you start rill after cloning the example above with:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;You can click on the source, and the data will be automatically loaded from the S3 source, and the above-defined transformations and conversions will be made:&lt;/p&gt;
    &lt;head rend="h3"&gt;What Did We Learn so Far?&lt;/head&gt;
    &lt;p&gt;To recap this example, ClickHouse offers a fundamentally different approach compared to other real-time databases like Druid, where most heavy lifting must be done ahead of ingestion using Spark or other compute engines. With ClickHouse, the engine itself handles complex aggregations and optimizations at ingestion time, during query execution, and even post-ingestion.&lt;/p&gt;
    &lt;p&gt;Interestingly, Rill automatically spawns up ClickHouse and orchestrates the incremental loads and ingests data. If you will, Rill is doing orchestration work.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;ClickHouse provides multiple levels of optimization that can be applied independently or combined:&lt;/p&gt;
    &lt;p&gt;Query-Level Optimizations:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simple GROUP BY aggregations that process data from milliseconds to hours on the fly.&lt;/item&gt;
      &lt;item&gt;Data partitioning: Data is organized into directories based on partition keys for parallel processing.&lt;/item&gt;
      &lt;item&gt;Filter and partition pushdown: ClickHouse’s optimizer pushes filters closer to the data source and skips irrelevant partitions, dramatically reducing I/O.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Storage-Level Pre-Aggregation Optimizations:&lt;lb/&gt; 4. Incremental materialized views shift computation cost from query time to insert time for faster SELECT queries.&lt;lb/&gt; 5. AggregatingMergeTree stores partial aggregation states directly in the table engine, merging rows with the same primary key into single rows containing combined aggregate statesâenabling orders of magnitude data reduction and sub-second query performance.&lt;/p&gt;
    &lt;p&gt;This flexibility allows you to choose the right optimization strategy based on your specific use case, query patterns, and performance requirements.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; Querying stackoverflow data in ClickHouse locally | X Post&lt;/p&gt;
    &lt;p&gt;ClickPipes is ClickHouse Cloud’s managed integration platform that makes ingesting data from diverse sources as simple as clicking a few buttons, providing a scalable, serverless ingestion experience with high throughput and low latency. Beyond object storage, ClickPipes supports Kafka/Confluent, database CDC from MySQL and Postgres, and streaming platforms like Kinesis and Event Hubs.&lt;/p&gt;
    &lt;p&gt;The platform includes fully managed operations with built-in error handling, automatic retries, schema evolution, and monitoring through dedicated error tables, plus enterprise features like API/Terraform integration and Prometheus metrics). For object storage specifically, ClickPipes supports &lt;code&gt;continuous ingestion&lt;/code&gt; with configurable polling where new files must be lexically ordered (e.g., &lt;code&gt;file1&lt;/code&gt;, &lt;code&gt;file2&lt;/code&gt;, &lt;code&gt;file3&lt;/code&gt;) for proper ingestion sequencing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Applicable Tips &amp;amp; Tricks&lt;/head&gt;
    &lt;p&gt;In this chapter we look at practical strategies for data modeling with ClickHouse with practical tips and tricks for real-time analytics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deduplication Strategies&lt;/head&gt;
    &lt;p&gt;Why it matters: Real-time data streams often contain duplicate records due to network retries, system failures, or multiple data sources. Without deduplication, your analytics might show inflated metrics and incorrect insights.&lt;/p&gt;
    &lt;p&gt;How to implement: ClickHouse offers several deduplication approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ReplacingMergeTree: Automatically deduplicates rows based on the sorting key during background merges.&lt;/item&gt;
      &lt;item&gt;Refreshable Materialized Views: Use &lt;code&gt;GROUP BY&lt;/code&gt;with&lt;code&gt;argMax()&lt;/code&gt;to keep the latest version of each record.&lt;/item&gt;
      &lt;item&gt;Custom Deduplication Logic: Implement application-level deduplication before insertion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Best Practice: For high-throughput real-time scenarios, use ReplacingMergeTree with a proper sorting key that includes your natural deduplication fields (e.g., &lt;code&gt;user_id&lt;/code&gt;, &lt;code&gt;event_id&lt;/code&gt;, &lt;code&gt;timestamp&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Optimization&lt;/head&gt;
    &lt;p&gt;ClickHouse is all about performance and speed out of the gate. But here are some tips and practical examples to optimize even more.&lt;/p&gt;
    &lt;head rend="h4"&gt;Partitioning Strategy&lt;/head&gt;
    &lt;p&gt;Why it matters: Proper partitioning enables query pruning and parallel processing, dramatically reducing query times from minutes to seconds.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Partition by time (daily/monthly) for time-series data.&lt;/item&gt;
      &lt;item&gt;Use secondary partitioning for high-cardinality dimensions. This means adding additional partition keys beyond just time to handle columns with many distinct values (&lt;code&gt;region&lt;/code&gt;in the example below).&lt;/item&gt;
      &lt;item&gt;Design partitions to match your most common query patterns.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt;Predicate Pushdown Optimization&lt;/head&gt;
    &lt;p&gt;Why it matters: Moving filters closer to the data source reduces the amount of data processed at each query stage.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Structure your &lt;code&gt;WHERE&lt;/code&gt;clauses to match your sorting key order.&lt;/item&gt;
      &lt;item&gt;Use low-cardinality columns early in filtering.&lt;/item&gt;
      &lt;item&gt;Leverage ClickHouse’s automatic index usage for range queries with sparse index.&lt;/item&gt;
      &lt;item&gt;Advanced tip: Combine with materialized views to push aggregations to insert time, not just filters to data source.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Pre-Aggregation with AggregatingMergeTree&lt;/head&gt;
    &lt;p&gt;When to use: High-volume time-series data where the same aggregation queries run frequently.&lt;/p&gt;
    &lt;p&gt;Implementation: Use &lt;code&gt;-State&lt;/code&gt; functions during INSERT and &lt;code&gt;-Merge&lt;/code&gt; functions during SELECT to work with pre-computed aggregate states rather than raw data. More Information&lt;/p&gt;
    &lt;head rend="h3"&gt;Storage Efficiency&lt;/head&gt;
    &lt;p&gt;Data modeling has a real impact on cost when done correctly. Here are some strategies to reduce storage, therefore save cost, and speed up query responses by an order of magnitude.&lt;/p&gt;
    &lt;head rend="h4"&gt;Data Sketches for Approximation&lt;/head&gt;
    &lt;p&gt;Why it matters: Exact distinct counts and percentiles on billions of rows are very expensive and time-consuming. Data sketches use clever algorithms to deliver 99%+ accuracy for 1% of the cost and storage.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Impact: The above example has an accuracy of 99%+ and a memory footprint of &amp;lt;2KB with a speedboost of 100x by reducing storage.&lt;/p&gt;
    &lt;head rend="h4"&gt;Rollup to Optimal Time Granularity&lt;/head&gt;
    &lt;p&gt;Why it matters: Storing every millisecond-level event creates significant storage overhead. Most business analytics work at hourly or daily granularity.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Aggregate raw events to hourly summaries using materialized views or SQL aggregations.&lt;/item&gt;
      &lt;item&gt;Keep detailed data for recent periods (last 30 days) and aggregated monthly data for historical analysis, for example.&lt;/item&gt;
      &lt;item&gt;Use different retention policies per granularity level.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Sampling Strategies&lt;/head&gt;
    &lt;p&gt;Sampling is a statistical way to reduce data without compromising on getting the right insights.&lt;/p&gt;
    &lt;head rend="h4"&gt;Statistical Sampling for Large Datasets&lt;/head&gt;
    &lt;p&gt;Why it matters: When dealing with billions of events, sometimes a representative sample provides sufficient accuracy for analytics while dramatically reducing processing time and storage costs.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Best Practice: Use stratified sampling when you need to maintain proportional representation across important business dimensions (customer segments, product categories, geographic regions). Use consistent hash functions to ensure reproducible samples.&lt;/p&gt;
    &lt;p&gt;Impact: Can reduce data volumes by 90-99% while maintaining statistical significance for trend analysis and aggregate metrics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Schema Management&lt;/head&gt;
    &lt;head rend="h4"&gt;Table Projections for Query Optimization&lt;/head&gt;
    &lt;p&gt;Table projections are ClickHouse’s native feature for pre-computed, physically stored copies of your table data with different sort orders or pre-aggregations. Think “same table, multiple indexes on steroids”.&lt;/p&gt;
    &lt;p&gt;Why it matters: Different queries need different sort orders or aggregations. Projections let you maintain multiple optimized access patterns without duplicating tables, and the query optimizer automatically picks the projection with the least data to scan.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Use dbt to create a denormalized One Big Table (OBT) in ClickHouse, then leverage ClickHouse projections for different query patterns instead of maintaining separate OLAP cubes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Schema Evolution Best Practices&lt;/head&gt;
    &lt;p&gt;Why it matters: Real-time systems need to handle schema changes without breaking existing queries or requiring full data reloads.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use nullable columns for new fields to maintain backward compatibility.&lt;/item&gt;
      &lt;item&gt;Implement “latest state” modeling for slowly changing dimensions.&lt;/item&gt;
      &lt;item&gt;Leverage ClickHouse’s automatic schema detection for JSON fields.&lt;/item&gt;
      &lt;item&gt;Snapshot approach: Daily/weekly full snapshots of dimensional data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Time Series Optimization&lt;/head&gt;
    &lt;p&gt;When working with time series, dates are an important part of how we query and store data.&lt;/p&gt;
    &lt;head rend="h4"&gt;Always Store in UTC&lt;/head&gt;
    &lt;p&gt;Why it matters: Mixed timezones in analytical data lead to incorrect aggregations and confusing results when data spans multiple regions.&lt;/p&gt;
    &lt;p&gt;How to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Convert all timestamps to UTC at ingestion time.&lt;/item&gt;
      &lt;item&gt;Store the original timezone as a separate column if needed for display.&lt;/item&gt;
      &lt;item&gt;Use ClickHouse’s timezone functions for display conversion only.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Besides all the strengths, some limitations can’t be neglected. For example, it’s more difficult to do updates and deletes (Mutations). Joins are limited in performance and functionality and there’s no full ACID transactions support. There’s also no notion of foreign keys. This means referential integrity is left to the user to manage at an application level. Read more about this on ClickHouse Architecture 101 as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Choosing the Right ClickHouse Modeling Strategy&lt;/head&gt;
    &lt;p&gt;After exploring ClickHouse’s capabilities for real-time analytics, the key question becomes: How do you choose the right modeling approach for your specific use case? As always, the answer depends on your data volume, latency requirements, complexity needs, and team capabilities. But we can say that ClickHouse lets us handle powerful use cases without the need for expensive ETL pipelines or an additional semantic layer.&lt;/p&gt;
    &lt;p&gt;For straightforward real-time scenarios, ClickHouse’s native features shine. You can deduplicate within ClickHouse to land consistent data in your cube, and use the FINAL modifier to let ClickHouse fully merge data before returning results. This performs all data transformations that happen during merges for the given table engine, eliminating the complexity of external processing.&lt;/p&gt;
    &lt;p&gt;The ETL Pipeline Approach&lt;lb/&gt; However, for more complex data projects, you can always handle execution through external ETL performed outside of ClickHouse using tools like dbt, Airflow, Dagster, Kestra, Flink, BladePipe, or dlt. These tools can orchestrate batch or streaming transformations before loading data into ClickHouse, which is especially useful for complex pipelines or when you want to manage schema evolution, data quality, or referential integrity outside the database. The ClickHouse integration for dbt ensures this is performed atomically with a new version of the target table created and then atomically swapped with the version receiving queries via the EXCHANGE command.&lt;/p&gt;
    &lt;p&gt;Modeling outside of ClickHouse is a common approach with more complex landscapes, but if we want real-time analytics, batch ETL can break the flow of continuously updated streams. That’s why this shouldn’t be the first choice if you want real-time data, quickly updated.&lt;/p&gt;
    &lt;p&gt;The BI Approach There’s also a tradeoff with storing metrics within the OLAP cube versus outside of it. Because SQL aggregations and measures can be queried on the fly but can’t be stored within ClickHouse easily, data modeling often happens outside ClickHouse or gets stored within BI tools. The advantage is you can change metrics at any time without running an ETL pipeline. The downside is you can’t easily store or manage them except in your UI, whether it’s a web app with an OLAP-ORM, notebooks, or Business Intelligence tools.&lt;/p&gt;
    &lt;p&gt;This is one reason why Rill pairs so well with ClickHouseâit has a full-blown metrics layer built-in with all its capabilities out of the box. You can store metrics declaratively based on YAML, version control them, and update them in a governed way. For example, put them in a git repository and let users collaborate on these metrics, which then get blazingly fast query returns on ClickHouse. Rill gives you another layer of data modeling while using ClickHouse as a sub-second response query engine.&lt;/p&gt;
    &lt;p&gt;Ultimately, the choice between native ClickHouse modeling, external ETL pipelines, or BI tool integration comes down to balancing three key factors: data freshness requirements, transformation complexity, and team capabilities. ClickHouse’s native approach eliminates traditional ETL overhead for most real-time use cases, but the flexibility to layer additional tools when needed ensures your analytics architecture can evolve with your business requirements.&lt;/p&gt;
    &lt;p&gt;To get started, check out the practical example that demonstrates ClickHouse ETL with NOAA weather data, or explore ClickHouse’s comprehensive Schema Design documentation, which guides you through all the steps including querying large datasets like StackOverflow’s 60+ million records locally within seconds.&lt;/p&gt;
    &lt;quote&gt;Written as part of my services&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45137927</guid></item><item><title>Development Speed Has Never Been a Bottleneck</title><link>https://pawelbrodzinski.substack.com/p/development-speed-is-not-a-bottleneck</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45138156</guid></item></channel></rss>